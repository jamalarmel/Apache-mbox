sequoiadb <mailing-list-recv@sequoiadb.com>,"Wed, 1 Apr 2015 09:12:55 +0800",should we add a start-masters.sh script in sbin?,dev <dev@spark.apache.org>,"Hey,

start-slaves.sh script is able to read from slaves file and start slaves node in multiple boxes.
However in standalone mode if I want to use multiple masters, I¡¯ll have to start masters in each individual box, and also need to provide the list of masters¡¯ hostname+port to each worker. ( start-slaves.sh only take 1 master ip+port for now)
I wonder should we create a new script called start-masters.sh to read conf/masters file? Also start-slaves.sh script may need to change a little bit so that master list can be passed to worker nodes.

Thanks

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Tue, 31 Mar 2015 18:20:58 -0700",Re: should we add a start-masters.sh script in sbin?,sequoiadb <mailing-list-recv@sequoiadb.com>,"Sounds good to me.


l have to
f
e 1 master
e
"
Debasish Das <debasish.das83@gmail.com>,"Tue, 31 Mar 2015 20:15:25 -0700",ADMM based proximal flow,"dev <dev@spark.apache.org>, David Hall <david.lw.hall@gmail.com>","Hi,

We recently added ADMM based proximal algorithm in
breeze.optimize.proximal.NonlinearMinimizer which uses a combination of
BFGS and proximal algorithms (soft thresholding for L1 for example) to
solve large scale constrained optimization problem of form f(x) + g(z). Its
usage is similar to current lbfgs flow. We also added a library of commonly
used proximal algorithms in breeze.

I am experimenting with L1 but I remember there were various admm based PR
before. Please feel free to use the solver for your usecases and let us
know if it was helpful.

The solver is available in mllib master as we already integrated breeze
0.11.2 to mllib.

Thanks.
Deb
"
Niranda Perera <niranda.perera@gmail.com>,"Wed, 1 Apr 2015 12:10:17 +0530",Migrating from 1.2.1 to 1.3.0 - org.apache.spark.sql.api.java.Row,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

previously in 1.2.1, the result row from a Spark SQL query was
a org.apache.spark.sql.api.java.Row.

In 1.3.0 I do not see a sql.api.java package. so does it mean that even the
SQL query result row is an implementation of org.apache.spark.sql.Row such
as GenericRow etc?

-- 
Niranda
"
Reynold Xin <rxin@databricks.com>,"Tue, 31 Mar 2015 23:54:03 -0700",Re: Migrating from 1.2.1 to 1.3.0 - org.apache.spark.sql.api.java.Row,Niranda Perera <niranda.perera@gmail.com>,"Yup - we merged the Java and Scala API so there is now a single set of API
to support both languages.

See more at
http://spark.apache.org/docs/latest/sql-programming-guide.html#unification-of-the-java-and-scala-apis




"
Reynold Xin <rxin@databricks.com>,"Wed, 1 Apr 2015 00:11:19 -0700","Spark 2.0: Rearchitecting Spark for Mobile, Local, Social","""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Spark devs,

I've spent the last few months investigating the feasibility of
re-architecting Spark for mobile platforms, considering the growing
population of Android/iOS users. I'm happy to share with you my findings at
https://issues.apache.org/jira/browse/SPARK-6646

The tl;dr is that we should support running Spark on Android/iOS, and the
best way to do this at the moment is to use Scala.js to compile Spark code
into JavaScript, and then run it in Safari or Chrome (and even node.js
potentially for servers).

If you are on your phones right now and prefer reading a blog post rather
than a PDF file, you can read more about the design doc at
https://databricks.com/blog/2015/04/01/spark-2-rearchitecting-spark-for-mobile.html


This is done in collaboration with TD, Xiangrui, Patrick. Look forward to
your feedback!
"
Akhil Das <akhil@sigmoidanalytics.com>,"Wed, 1 Apr 2015 13:01:51 +0530","Re: Spark 2.0: Rearchitecting Spark for Mobile, Local, Social",Reynold Xin <rxin@databricks.com>,"Nice try :)

Thanks
Best Regards


"
Kushal Datta <kushal.datta@gmail.com>,"Wed, 1 Apr 2015 00:34:26 -0700","Re: Spark 2.0: Rearchitecting Spark for Mobile, Local, Social",Akhil Das <akhil@sigmoidanalytics.com>,"Reynold, what's the idea behind using LLVM?


"
Tathagata Das <tathagata.das1565@gmail.com>,"Wed, 1 Apr 2015 03:37:44 -0400","Re: Spark 2.0: Rearchitecting Spark for Mobile, Local, Social",Reynold Xin <rxin@databricks.com>,"This is a significant effort that Reynold has undertaken, and I am super
glad to see that it's finally taking a concrete form. Would love to see
what the community thinks about the idea.

TD


"
Gil Vernik <GILV@il.ibm.com>,"Wed, 1 Apr 2015 12:58:24 +0300",,dev <dev@spark.apache.org>,"I actually saw the same issue, where we analyzed some container with few 
hundreds of GBs zip files - one was corrupted and Spark exit with 
Exception on the entire job.
I like SPARK-6593, since it  can cover also additional cases, not just in 
case of corrupted zip files.



From:   Dale Richardson <dale__r@hotmail.com>
To:     ""dev@spark.apache.org"" <dev@spark.apache.org>
Date:   29/03/2015 11:48 PM



Recently had an incident reported to me where somebody was analysing a 
directory of gzipped log files, and was struggling to load them into spark 
because one of the files was corrupted - calling 
sc.textFiles('hdfs:///logs/*.gz') caused an IOException on the particular 
executor that was reading that file, which caused the entire job to be 
cancelled after the retry count was exceeded, without any way of catching 
and recovering from the error.  While normally I think it is entirely 
appropriate to stop execution if something is wrong with your input, 
sometimes it is useful to analyse what you can get (as long as you are 
aware that input has been skipped), and treat corrupt files as acceptable 
losses.
To cater for this particular case I've added SPARK-6593 (PR at 
https://github.com/apache/spark/pull/5250). Which adds an option 
(spark.hadoop.ignoreInputErrors) to log exceptions raised by the hadoop 
Input format, but to continue on with the next task.
Ideally in this case you would want to report the corrupt file paths back 
to the master so they could be dealt with in a particular way (eg moved to 
a separate directory), but that would require a public API 
change/addition. I was pondering on an addition to Spark's hadoop API that 
could report processing status back to the master via an optional 
accumulator that collects filepath/Option(exception message) tuples so the 
user has some idea of what files are being processed, and what files are 
being skipped.
Regards,Dale.  
"
Romi Kuntsman <romi@totango.com>,"Wed, 1 Apr 2015 18:34:48 +0300",,Gil Vernik <GILV@il.ibm.com>,"What about communication errors and not corrupted files?
Both when reading input and when writing output.
We currently experience a failure of the entire process, if the last stage
of writing the output (to Amazon S3) failed because of a very temporary DNS
resolution issue (easily resolved by retrying).

*Romi Kuntsman*, *Big Data Engineer*
 http://www.totango.com


"
Ted Yu <yuzhihong@gmail.com>,"Wed, 1 Apr 2015 08:46:05 -0700",,Romi Kuntsman <romi@totango.com>,"bq. writing the output (to Amazon S3) failed

What's the value of ""fs.s3.maxRetries"" ?
Increasing the value should help.

Cheers


"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Wed, 1 Apr 2015 17:09:05 +0000",RE: Stochastic gradient descent performance,"""dev@spark.apache.org"" <dev@spark.apache.org>","Sorry for bothering you again, but I think that it is an important issue for applicability of SGD in Spark MLlib. Could Spark developers please comment on it.


It seems to me that there is an overhead in ""runMiniBatchSGD"" function of MLlib's ""GradientDescent"". In particular, ""sample"" and ""treeAggregate"" might take time that is order of magnitude greater than the actual gradient computation. In particular, for mnist dataset of 60K instances, minibatch size = 0.001 (i.e. 60 samples) it take 0.15 s to sample and 0.3 to aggregate in local mode with 1 data partition on Core i5 processor. The actual gradient computation takes 0.002 s. I searched through Spark Jira and found that there was recently an update for more efficient sampling (SPARK-3250) that is already included in Spark codebase. Is there a way to reduce the sampling time and local treeRedeuce by order of magnitude?

Best regards, Alexander

---------------------------------------------------------------------


"
Hector Yee <hector.yee@gmail.com>,"Wed, 1 Apr 2015 11:35:40 -0700",Re: Storing large data for MLlib machine learning,"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","I use Thrift and then base64 encode the binary and save it as text file
lines that are snappy or gzip encoded.

It makes it very easy to copy small chunks locally and play with subsets of
the data and not have dependencies on HDFS / hadoop for server stuff for
example.





-- 
Yee Yang Li Hector <http://google.com/+HectorYee>
*google.com/+HectorYee <http://google.com/+HectorYee>*
"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Wed, 1 Apr 2015 19:32:27 +0000",RE: Storing large data for MLlib machine learning,Hector Yee <hector.yee@gmail.com>,"Thanks, sounds interesting! How do you load files to Spark? Did you consider having multiple files instead of file lines?

From: Hector Yee [mailto:hector.yee@gmail.com]
Sent: Wednesday, April 01, 2015 11:36 AM
To: Ulanov, Alexander
Cc: Evan R. Sparks; Stephen Boesch; dev@spark.apache.org
Subject: Re: Storing large data for MLlib machine learning

I use Thrift and then base64 encode the binary and save it as text file lines that are snappy or gzip encoded.

It makes it very easy to copy small chunks locally and play with subsets of the data and not have dependencies on HDFS / hadoop for server stuff for example.


On Thu, Mar 26, 2015 at 2:51 PM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
Thanks, Evan. What do you think about Protobuf? Twitter has a library to manage protobuf files in hdfs https://github.com/twitter/elephant-bird


From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>]
Sent: Thursday, March 26, 2015 2:34 PM
To: Stephen Boesch
Cc: Ulanov, Alexander; dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: Storing large data for MLlib machine learning

On binary file formats - I looked at HDF5+Spark a couple of years ago and found it barely JVM-friendly and very Hadoop-unfriendly (e.g. the APIs needed filenames as input, you couldn't pass it anything like an InputStream). I don't know if it has gotten any better.

Parquet plays much more nicely and there are lots of spark-related projects using it already. Keep in mind that it's column-oriented which might impact performance - but basically you're going to want your features in a byte array and deser should be pretty straightforward.

On Thu, Mar 26, 2015 at 2:26 PM, Stephen Boesch <javadba@gmail.com<mailto:javadba@gmail.com><mailto:javadba@gmail.com<mailto:javadba@gmail.com>>> wrote:
There are some convenience methods you might consider including:

           MLUtils.loadLibSVMFile

and   MLUtils.loadLabeledPoint

2015-03-26 14:16 GMT-07:00 Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>>:

> Hi,
>
> Could you suggest what would be the reasonable file format to store
> feature vector data for machine learning in Spark MLlib? Are there any best
> practices for Spark?
>
> My data is dense feature vectors with labels. Some of the requirements are
> that the format should be easy loaded/serialized, randomly accessible, with
> a small footprint (binary). I am considering Parquet, hdf5, protocol buffer
> (protobuf), but I have little to no experience with them, so any
> suggestions would be really appreciated.
>
> Best regards, Alexander
>



--
Yee Yang Li Hector<http://google.com/+HectorYee>
google.com/+HectorYee<http://google.com/+HectorYee>
"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Wed, 1 Apr 2015 19:11:03 +0000",RE: Using CUDA within Spark / boosting linear algebra,"Xiangrui Meng <mengxr@gmail.com>, Sean Owen <sowen@cloudera.com>","FYI, I've added instructions to Netlib-java wiki, Sam added the link to them from the project's readme.md
https://github.com/fommil/netlib-java/wiki/NVBLAS

Best regards, Alexander
-----Original Message-----
From: Xiangrui Meng [mailto:mengxr@gmail.com] 
Sent: Monday, March 30, 2015 2:43 PM
To: Sean Owen
Cc: Evan R. Sparks; Sam Halliday; dev@spark.apache.org; Ulanov, Alexander; jfcanny
Subject: Re: Using CUDA within Spark / boosting linear algebra

Hi Alex,

Since it is non-trivial to make nvblas work with netlib-java, it would be great if you can send the instructions to netlib-java as part of the README. Hopefully we don't need to modify netlib-java code to use nvblas.

Best,
Xiangrui

On Thu, Mar 26, 2015 at 9:54 AM, Sean Owen <sowen@cloudera.com> wrote:
> The license issue is with libgfortran, rather than OpenBLAS.
>
> (FWIW I am going through the motions to get OpenBLAS set up by default 
> on CDH in the near future, and the hard part is just handling
> libgfortran.)
>
> On Thu, Mar 26, 2015 at 4:07 PM, Evan R. Sparks <evan.sparks@gmail.com> wrote:
>> Alright Sam - you are the expert here. If the GPL issues are 
>> unavoidable, that's fine - what is the exact bit of code that is GPL?
>>
>> The suggestion to use OpenBLAS is not to say it's the best option, 
>> but that it's a *free, reasonable default* for many users - keep in 
>> mind the most common deployment for Spark/MLlib is on 64-bit linux on EC2[1].
>> Additionally, for many of the problems we're targeting, this 
>> reasonable default can provide a 1-2 orders of magnitude improvement 
>> in performance over the f2jblas implementation that netlib-java falls back on.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For 
> additional commands, e-mail: dev-help@spark.apache.org
>
"
Hector Yee <hector.yee@gmail.com>,"Wed, 1 Apr 2015 12:52:09 -0700",Re: Storing large data for MLlib machine learning,"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Just using sc.textfile then a .map(decode)
Yes by default it is multiple files .. our training data is 1TB gzipped
into 5000 shards.





-- 
Yee Yang Li Hector <http://google.com/+HectorYee>
*google.com/+HectorYee <http://google.com/+HectorYee>*
"
Marcelo Vanzin <vanzin@cloudera.com>,"Wed, 1 Apr 2015 13:01:34 -0700",Unit test logs in Jenkins?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey all,

Is there a way to access unit test logs in jenkins builds? e.g.,
core/target/unit-tests.log

That would be really helpful to debug build failures. The scalatest
output isn't all that helpful.

If that's currently not available, would it be possible to add those
logs as build artifacts?

-- 
Marcelo

---------------------------------------------------------------------


"
Jeremy Freeman <freeman.jeremy@gmail.com>,"Wed, 1 Apr 2015 16:36:43 -0400",Re: Storing large data for MLlib machine learning,Hector Yee <hector.yee@gmail.com>,"@Alexander, re: using flat binary and metadata, you raise excellent points! At least in our case, we decided on a specific endianness, but do end up storing some extremely minimal specification in a JSON file, and have written importers and exporters with"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Wed, 1 Apr 2015 20:41:22 +0000",RE: Storing large data for MLlib machine learning,"Jeremy Freeman <freeman.jeremy@gmail.com>, Hector Yee
	<hector.yee@gmail.com>","Jeremy, thanks for explanation!
What if instead you've used Parquet file format? You can still write a number of small files as you do, but you don't have to implement a writer/reader, because they are available for Parquet in various languages.

From: Jeremy Freeman [mailto:freeman.jeremy@gmail.com]
Sent: Wednesday, April 01, 2015 1:37 PM
To: Hector Yee
Cc: Ulanov, Alexander; Evan R. Sparks; Stephen Boesch; dev@spark.apache.org
Subject: Re: Storing large data for MLlib machine learning

@Alexander, re: using flat binary and metadata, you raise excellent points! At least in our case, we decided on a specific endianness, but do end up storing some extremely minimal specification in a JSON file, and have written importers and exporters within our library to parse it. While it does feel a little like reinvention, it's fast, direct, and scalable, and seems pretty sensible if you know your data will be dense arrays of numerical features.

-------------------------
jeremyfreeman.net<http://jeremyfreeman.net>
@thefreemanlab



Just using sc.textfile then a .map(decode)
Yes by default it is multiple files .. our training data is 1TB gzipped
into 5000 shards.

<mailto:alexander.ulanov@hp.com>>


Thanks, sounds interesting! How do you load files to Spark? Did you
consider having multiple files instead of file lines?



*From:* Hector Yee [mailto:hector.yee@gmail.com]
*Sent:* Wednesday, April 01, 2015 11:36 AM
*To:* Ulanov, Alexander
*Cc:* Evan R. Sparks; Stephen Boesch; dev@spark.apache.org<mailto:dev@spark.apache.org>

*Subject:* Re: Storing large data for MLlib machine learning



I use Thrift and then base64 encode the binary and save it as text file
lines that are snappy or gzip encoded.



It makes it very easy to copy small chunks locally and play with subsets
of the data and not have dependencies on HDFS / hadoop for server stuff for
example.






Thanks, Evan. What do you think about Protobuf? Twitter has a library to
manage protobuf files in hdfshttps://github.com/twitter/elephant-bird


From: Evan R. Sparks [mailto:evan.sparks@gmail.com]
Sent: Thursday, March 26, 2015 2:34 PM
To: Stephen Boesch
Cc: Ulanov, Alexander; dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: Storing large data for MLlib machine learning

found it barely JVM-friendly and very Hadoop-unfriendly (e.g. the APIs
needed filenames as input, you couldn't pass it anything like an
InputStream). I don't know if it has gotten any better.

Parquet plays much more nicely and there are lots of spark-related
projects using it already. Keep in mind that it's column-oriented which
might impact performance - but basically you're going to want your features
in a byte array and deser should be pretty straightforward.

avadba@gmail.com><mailto:
There are some convenience methods you might consider including:

          MLUtils.loadLibSVMFile

and   MLUtils.loadLabeledPoint

2015-03-26 14:16 GMT-07:00 Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>
<mailto:alexander.ulanov@hp.com>>:



Hi,

Could you suggest what would be the reasonable file format to store
feature vector data for machine learning in Spark MLlib? Are there any
best

practices for Spark?

My data is dense feature vectors with labels. Some of the requirements
are

that the format should be easy loaded/serialized, randomly accessible,
with

a small footprint (binary). I am considering Parquet, hdf5, protocol
buffer

(protobuf), but I have little to no experience with them, so any
suggestions would be really appreciated.

Best regards, Alexander





--

Yee Yang Li Hector <http://google.com/+HectorYee>

*google.com/+HectorYee<http://google.com/+HectorYee> <http://google.com/+HectorYee>*



--
Yee Yang Li Hector <http://google.com/+HectorYee>
*google.com/+HectorYee<http://google.com/+HectorYee> <http://google.com/+HectorYee>*

"
Burak Yavuz <brkyvz@gmail.com>,"Wed, 1 Apr 2015 14:48:18 -0700","Re: Spark 2.0: Rearchitecting Spark for Mobile, Local, Social",Tathagata Das <tathagata.das1565@gmail.com>,"This is awesome! I can write the apps for it, to make the Web UI more
functional!


"
Ameet Talwalkar <atalwalkar@gmail.com>,"Wed, 1 Apr 2015 16:12:28 -0700",Volunteers for Spark MOOCs,dev <dev@spark.apache.org>,"Dear Spark Devs,

Anthony Joseph and I are teaching two large MOOCs
<http://en.wikipedia.org/wiki/Massive_open_online_course> this summer on
Apache Spark and we are looking for participants from the community who
would like to help us administer the course.


Anthony is a Professor in Computer Science at UC Berkeley in the AMPLab,
and his course
<https://www.edx.org/course/v2/introduction-big-data-apache-spark-uc-berkeleyx-cs100-1x>
will be an introduction to big data analysis using Spark.  I am an
Assistant Professor in Computer Science at UCLA (and a former AMPLab
post-doc), and my course
<https://www.edx.org/course/scalable-machine-learning-uc-berkeleyx-cs190-1x>
will be about using Spark for Machine Learning pipelines.  The courses will
be taught in Python, and will be freely available via edX, a non-profit
MOOC provider that partners with many top universities across the world.

We're looking for volunteer Teaching Assistants (TAs) with at least two of
the following skills: ability to deal with Spark or Python setup issues,
basic Spark programming and debugging experience, basic ML knowledge, and
basic operations skills (writing and using scripts, helping with
username/password issues, etc.).  TAing is a great opportunity to interact
with a wide audience of Spark enthusiasts, and TAs will be formerly listed
as part of the course staff on the edX website.

We are looking for a time-commitment of roughly 10hrs per week in May and
at least 20 hours a week in June and July, though we are quite flexible
about specific working hours and location (since we will have students
around the world). We can also offer a stipend.

Please contact Anthony and me directly if you are interested.

Thanks,

Ameet and Anthony
"
Reynold Xin <rxin@databricks.com>,"Wed, 1 Apr 2015 17:11:10 -0700",Re: Can I call aggregate UDF in DataFrame?,Haopu Wang <HWang@qilinsoft.com>,"You totally can.

https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/DataFrame.scala#L792

There is also an attempt at adding stddev here already:
https://github.com/apache/spark/pull/5228




"
"""Haopu Wang"" <HWang@qilinsoft.com>","Thu, 2 Apr 2015 13:13:21 +0800",RE: Can I call aggregate UDF in DataFrame?,"""Reynold Xin"" <rxin@databricks.com>","Great! Thank you!

 

________________________________

From: Reynold Xin [mailto:rxin@databricks.com] 
Sent: Thursday, April 02, 2015 8:11 AM
To: Haopu Wang
Cc: user; dev@spark.apache.org
Subject: Re: Can I call aggregate UDF in DataFrame?

 

You totally can.

 

https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/
apache/spark/sql/DataFrame.scala#L792

 

There is also an attempt at adding stddev here already:
https://github.com/apache/spark/pull/5228

 

 

 


Specifically there are only 5 aggregate functions in class
org.apache.spark.sql.GroupedData: sum/max/min/mean/count.

Can I plugin a function to calculate stddev?

Thank you!


---------------------------------------------------------------------

 

"
Patrick Wendell <pwendell@gmail.com>,"Wed, 1 Apr 2015 22:31:31 -0700",Re: Unit test logs in Jenkins?,Marcelo Vanzin <vanzin@cloudera.com>,"Hey Marcelo,

Great question. Right now, some of the more active developers have an
account that allows them to log into this cluster to inspect logs (we
copy the logs from each run to a node on that cluster). The
infrastructure is maintained by the AMPLab.

I will put you in touch the someone there who can get you an account.

This is a short term solution. The longer term solution is to have
these scp'd regularly to an S3 bucket or somewhere people can get
access to them, but that's not ready yet.

- Patrick




---------------------------------------------------------------------


"
Steve Loughran <stevel@hortonworks.com>,"Thu, 2 Apr 2015 10:01:18 +0000",Re: Unit test logs in Jenkins?,Apache Spark Dev <dev@spark.apache.org>,"


ASF Jenkins is always there to play with; committers/PMC members should just need to file a BUILD JIRA to get access.

Its main limitation is more cultural than technical: you need to get people to care about intermittent test runs, otherwise you can end up with failures that nobody keeps on top of
https://builds.apache.org/view/H-L/view/Hadoop/

Someone really needs to own the ""keep the builds working"" problem -and have the ability to somehow kick others into fixing things. The latter is pretty hard cross-organisation



Potentially an issue with the test runner, rather than the tests themselves.

---------------------------------------------------------------------


"
Niranda Perera <niranda.perera@gmail.com>,"Thu, 2 Apr 2015 16:29:46 +0530",org.spark-project.jetty and guava repo locations,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

I am looking for the org.spark-project.jetty and org.spark-project.guava
repo locations but I'm unable to find it in the maven repository.

are these publicly available?

rgds

-- 
Niranda
"
anshu shukla <anshushukla0@gmail.com>,"Thu, 2 Apr 2015 17:23:27 +0530",Support for cycles in spark streaming topology ?????,dev@spark.apache.org,"I  didn't  find any documentation  regarding support for  cycles in spark
topology , although storm supports  this using manual  configuration in
acker function logic (setting it to a particular count) .By cycles  i
 doesn't mean infinite loops .

Can any body please help me in that .

-- 
Thanks & Regards,
Anshu Shukla
"
Peter Rudenko <petro.rudenko@gmail.com>,"Thu, 2 Apr 2015 14:54:00 +0300",[sql] Dataframe how to check null values,dev@spark.apache.org,"Hi i need to implement MeanImputor - impute missing values with mean. If 
i set missing values to null - then dataframe aggregation works 
properly, but in UDF it treats null values to 0.0. Hereâ€™s example:

|val df = sc.parallelize(Array(1.0,2.0, null, 3.0, 5.0, null)).toDF 
df.agg(avg(""_1"")).first //res45: org.apache.spark.sql.Row = [2.75] 
df.withColumn(""d2"", callUDF({(value: Double) => value}, DoubleType, 
df(""d""))),show() d d2 1.0 1.0 2.0 2.0 null 0.0 3.0 3.0 5.0 5.0 null 0.0 
val df = sc.parallelize(Array(1.0,2.0, Double.NaN, 3.0, 5.0, 
Double.NaN)).toDF df.agg(avg(""_1"")).first //res46: 
org.apache.spark.sql.Row = [Double.NaN] |

In UDF i cannot compare scalaâ€™s Double to null:

|comparing values of types Double and Null using `==' will always yield 
false [warn] if (value==null) meanValue else value |

With Double.NaN instead of null i can compare in UDF, but aggregation 
doesnâ€™t work properly. Maybe itâ€™s related to : 
https://issues.apache.org/jira/browse/SPARK-6573

Thanks,
Peter Rudenko

â€‹
"
Ted Yu <yuzhihong@gmail.com>,"Thu, 2 Apr 2015 05:00:48 -0700",Re: org.spark-project.jetty and guava repo locations,Niranda Perera <niranda.perera@gmail.com>,"Take a look at the maven-shade-plugin in pom.xml.
Here is the snippet for org.spark-project.jetty :

            <relocation>
              <pattern>org.eclipse.jetty</pattern>
              <shadedPattern>org.spark-project.jetty</shadedPattern>
              <includes>
                <include>org.eclipse.jetty.**</include>
              </includes>
            </relocation>


"
Dean Wampler <deanwampler@gmail.com>,"Thu, 2 Apr 2015 08:51:34 -0400",Re: [sql] Dataframe how to check null values,Peter Rudenko <petro.rudenko@gmail.com>,"I'm afraid you're a little stuck. In Scala, the types Int, Long, Float,
Double, Byte, and Boolean look like reference types in source code, but
they are compiled to the corresponding JVM primitive types, which can't be
null. That's why you get the warning about ==.

It might be your best choice is to use NaN as the placeholder for null,
then create one DF using a filter that removes those values. Use that DF to
compute the mean. Then apply a map step to the original DF to translate the
NaN's to the mean.

dean

Dean Wampler, Ph.D.
Author: Programming Scala, 2nd Edition
<http://shop.oreilly.com/product/0636920033073.do> (O'Reilly)
Typesafe <http://typesafe.com>
@deanwampler <http://twitter.com/deanwampler>
http://polyglotprogramming.com


i
ut
al
oDF
] |
ld
sues.apache.org/
"
Sean Owen <sowen@cloudera.com>,"Thu, 2 Apr 2015 14:27:38 +0100",Re: Support for cycles in spark streaming topology ?????,anshu shukla <anshushukla0@gmail.com>,"You can have diamonds but not cycles in the dependency graph.

But what you are describing really sounds like simple iteration, since
presumably you mean that the state of each element in the 'cycle'
changes each time, and so isn't really the same element each time, and
eventually you decide to stop. That is quite possible.



---------------------------------------------------------------------


"
Romi Kuntsman <romi@totango.com>,"Thu, 2 Apr 2015 17:33:07 +0300",,Ted Yu <yuzhihong@gmail.com>,"Hi Ted,
Not sure what's the config value, I'm using s3n filesystem and not s3.

The error that I get is the following:
(so does that mean it's 4 retries?)

 Caused by: org.apache.spark.SparkException: Job aborted due to stage
failure: Task 2 in stage 0.0 failed 4 times, most recent failure: Lost task
2.3 in stage 0.0 (TID 11, ip.ec2.internal): java.net.UnknownHostException:
mybucket.s3.amazonaws.com
        at
java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:178)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
        at java.net.Socket.connect(Socket.java:579)
        at sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:618)
        at sun.security.ssl.SSLSocketImpl.<init>(SSLSocketImpl.java:451)
        at
sun.security.ssl.SSLSocketFactoryImpl.createSocket(SSLSocketFactoryImpl.java:140)
        at
org.apache.commons.httpclient.protocol.SSLProtocolSocketFactory.createSocket(SSLProtocolSocketFactory.java:82)
        at
org.apache.commons.httpclient.protocol.ControllerThreadSocketFactory$1.doit(ControllerThreadSocketFactory.java:91)
        at
org.apache.commons.httpclient.protocol.ControllerThreadSocketFactory$SocketTask.run(ControllerThreadSocketFactory.java:158)
        at java.lang.Thread.run(Thread.java:745)

*Romi Kuntsman*, *Big Data Engineer*
 http://www.totango.com


"
Ted Yu <yuzhihong@gmail.com>,"Thu, 2 Apr 2015 08:01:25 -0700",,Romi Kuntsman <romi@totango.com>,"S3n is governed by the same config parameter. 

Cheers



ure: Task 2 in stage 0.0 failed 4 times, most recent failure: Lost task 2.3 in stage 0.0 (TID 11, ip.ec2.internal): java.net.UnknownHostException: mybucket.s3.amazonaws.com
l.java:178)
toryImpl.java:140)
.createSocket(SSLProtocolSocketFactory.java:82)
ctory$1.doit(ControllerThreadSocketFactory.java:91)
ctory$SocketTask.run(ControllerThreadSocketFactory.java:158)
ge
NS
ew
 in

park
lar

ing

ble
p
ack
d to
hat
 the
re
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 02 Apr 2015 15:54:07 +0000",Re: Unit test logs in Jenkins?,"Steve Loughran <stevel@hortonworks.com>, Apache Spark Dev <dev@spark.apache.org>","This is secondary to Marceloâ€™s question, but I wanted to comment on this:

Its main limitation is more cultural than technical: you need to get people
to care about intermittent test runs, otherwise you can end up with
failures that nobody keeps on top of

This is a problem that plagues Spark as well, but there *is* a technical
solution.

The solution is simple: *All* the builds that we care about run for *every*
proposed change. If *any* build fails, the change doesnâ€™t make it into the
repository.

Spark already has a pull request builder that tests and reports back on
PRs. Committers donâ€™t merge in PRs when this builder reports that it failed
some tests. Thatâ€™s a good thing.

The problem is that there are several other builds that we run on a fixed
interval, independent of the pull request builder. These builds test
different configurations, dependency versions, and environments than what
the PR builder covers. If one of those builds fails, it fails on its own
little island, with no-one to hear it scream. The build failure is detached
from the PR that caused it to fail.

What should happen is that the whole matrix of stuff we care to test gets
run for every PR. No PR goes in if any build we care about fails for that
PR, and every build we care about runs for every commit of every PR.

Really, this is just an extension of the basic idea of the PR builder. It
doesnâ€™t make much sense to test stuff *after* it has been committed and
potentially broken things. And it becomes exponentially more difficult to
find and fix a problem the longer it has been festering in the repo. Itâ€™s
best to keep such problems out in the first place.

With some more work on our CI infrastructure, I think this can be done.
Maybe even later this year.

Nick



h
"
shane knapp <sknapp@berkeley.edu>,"Thu, 2 Apr 2015 08:59:37 -0700",Re: Unit test logs in Jenkins?,Nicholas Chammas <nicholas.chammas@gmail.com>,"i agree with all of this.  but can we please break up the tests and make
them shorter?  :)

m

on this:
le
y*
 into the
 it failed
ed
ed and
â€™s
is
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 02 Apr 2015 16:21:39 +0000",Test all the things (Was: Unit test logs in Jenkins?),shane knapp <sknapp@berkeley.edu>,"(Renaming thread so as to un-hijack Marcelo's request.)

Sure, we definitely want tests running faster.

Part of ""testing all the things"" will be factoring out stuff from the
various builds that can be run just once.

We've also tried in the past (with little success) to parallelize test
execution <https://issues.apache.org/jira/browse/SPARK-3431>. That still
needs work before it becomes possible.

Nick



 on this:
t into the
t it
d
t
s
t
t
ed and
o
â€™s
n
e
.
d
"
Imran Rashid <irashid@cloudera.com>,"Thu, 2 Apr 2015 11:57:06 -0500",Re: Spark config option 'expression language' feedback request,Reynold Xin <rxin@databricks.com>,"IMO, spark's config is kind of a mess right now.  I completely agree with
Reynold that Spark's handling of config ought to be super-simple, its not
the kind of thing we want to put much effort in spark itself.  It sounds so
trivial that everyone wants to redo it, but then all these additional
features start to get thrown in, it starts to get complicated.  This is one
of many reasons our config handling is inadequate.  It would be better if
we could outsource it to other libraries, or even better yet, let users
bring their own.

The biggest problem, in my mind, is that there isn't a definitive,
strongly-typed, modular listing of all the parameters.  This makes it
really hard to put your own thing on top -- you've got to manually go
through all the options and put them into your own config library.  And
then make sure its up-to-date with every new release of spark.

Just as a small example of how the options are hard to track down, some of
the options for event logging are listed in SparkContext:
https://github.com/apache/spark/blob/424e987dfebbbaa37f4496d44090d469a931ce76/core/src/main/scala/org/apache/spark/SparkContext.scala#L229

and some others are listed in EventLoggingListener:
https://github.com/apache/spark/blob/424e987dfebbbaa37f4496d44090d469a931ce76/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala#L60

this also makes it a headache while developing & trying to keep the
documentation up-to-date.

There are a handful of different libraries that might help out with this:
scopt, argot, scallop, sumac.  I'm biased to sumac [since I wrote it ], but
probably any of these would let me do whatever customizations I wanted on
top, without needing to manually keep every option in sync.  That said, I
do think sumac is especially well suited to the way Spark uses
configuration -- the nested structure directly maps to the way we have
things organized currently.  so eg. everything related to event logging
would get placed in a class like:

class EventLoggingOpts {
  var enabled = false
  var compress = false
  var testing = false
  var overwrite = false
  var buffer: Bytes = 100.kilobytes
}


Another plus is that you get fail-fast behavior -- if you put in some
unparseable value, the job will fail immediately, rather than 1 hour in
when you first try to access the value.

In any case, my main point is just that I think we should try to make our
config more compatible with external config tools, rather than trying to
build own.  And after that, I'd just like to throw Sumac into the ring as a
contender :)



"
Joseph Bradley <joseph@databricks.com>,"Thu, 2 Apr 2015 10:50:52 -0700",Re: Stochastic gradient descent performance,"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","It looks like SPARK-3250 was applied to the sample() which GradientDescent
uses, and that should kick in for your minibatchFraction <= 0.4.  Based on
your numbers, aggregation seems like the main issue, though I hesitate to
optimize aggregation based on local tests for data sizes that small.

The first thing I'd check for is unnecessary object creation, and to
profile in a cluster or larger data setting.


"
Reynold Xin <rxin@databricks.com>,"Thu, 2 Apr 2015 11:18:00 -0700",Re: [sql] Dataframe how to check null values,Dean Wampler <deanwampler@gmail.com>,"Incidentally, we were discussing this yesterday. Here are some thoughts on
null handling in SQL/DataFrames. Would be great to get some feedback.

1. Treat floating point NaN and null as the same ""null"" value. This would
be consistent with most SQL databases, and Pandas. This would also require
some inbound conversion.

2. Internally, when we see a NaN value, we should mark the null bit as
true, and keep the NaN value. When we see a null value for a floating point
field, we should mark the null bit as true, and update the field to store
NaN.

3. Externally, for floating point values, return NaN when the value is null.

4. For all other types, return null for null values.

5. For UDFs, if the argument is primitive type only (i.e. does not handle
null) and not a floating point field, simply evaluate the expression to
null. This is consistent with most SQL UDFs and most programming languages'
treatment of NaN.


Any thoughts on this semantics?



e
to
he
f
aN]
ield
"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Thu, 2 Apr 2015 18:33:07 +0000",RE: Stochastic gradient descent performance,Joseph Bradley <joseph@databricks.com>,"Hi Joseph,

Thank you for suggestion!
It seems that instead of sample it is better to shuffle data and then access it sequentially by mini-batches. Could you suggest how to implement it?

With regards to aggregate (reduce), I am wondering why it works so slow in local mode? Could you elaborate on this? I do understand that in cluster mode the network speed will kick in and then one can blame it.

Best regards, Alexander

From: Joseph Bradley [mailto:joseph@databricks.com]
Sent: Thursday, April 02, 2015 10:51 AM
To: Ulanov, Alexander
Cc: dev@spark.apache.org
Subject: Re: Stochastic gradient descent performance

It looks like SPARK-3250 was applied to the sample() which GradientDescent uses, and that should kick in for your minibatchFraction <= 0.4.  Based on your numbers, aggregation seems like the main issue, though I hesitate to optimize aggregation based on local tests for data sizes that small.

The first thing I'd check for is unnecessary object creation, and to profile in a cluster or larger data setting.

On Wed, Apr 1, 2015 at 10:09 AM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
Sorry for bothering you again, but I think that it is an important issue for applicability of SGD in Spark MLlib. Could Spark developers please comment on it.

-----Original Message-----
From: Ulanov, Alexander
Sent: Monday, March 30, 2015 5:00 PM
To: dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Stochastic gradient descent performance

Hi,

It seems to me that there is an overhead in ""runMiniBatchSGD"" function of MLlib's ""GradientDescent"". In particular, ""sample"" and ""treeAggregate"" might take time that is order of magnitude greater than the actual gradient computation. In particular, for mnist dataset of 60K instances, minibatch size = 0.001 (i.e. 60 samples) it take 0.15 s to sample and 0.3 to aggregate in local mode with 1 data partition on Core i5 processor. The actual gradient computation takes 0.002 s. I searched through Spark Jira and found that there was recently an update for more efficient sampling (SPARK-3250) that is already included in Spark codebase. Is there a way to reduce the sampling time and local treeRedeuce by order of magnitude?

Best regards, Alexander
---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org>
For additional commands, e-mail: dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>

"
Marcelo Vanzin <vanzin@cloudera.com>,"Thu, 2 Apr 2015 11:44:25 -0700",Re: Unit test logs in Jenkins?,Steve Loughran <stevel@hortonworks.com>,"
Sorry, that was me over-generalizing. The output is generally fine,
except for certain classes of tests (especially those that need to
fork a child process or even multiple processes). In those cases, most
of the interesting output ends up in logs, and the error reported by
scalatest is very generic.

-- 
Marcelo

---------------------------------------------------------------------


"
Joseph Bradley <joseph@databricks.com>,"Thu, 2 Apr 2015 12:00:20 -0700",Re: Stochastic gradient descent performance,"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","When you say ""It seems that instead of sample it is better to shuffle data
and then access it sequentially by mini-batches,"" are you sure that holds
true for a big dataset in a cluster?  As far as implementing it, I haven't
looked carefully at GapSamplingIterator (in RandomSampler.scala) myself,
but that looks like it could be modified to be deterministic.

Hopefully someone else can comment on aggregation in local mode.  I'm not
sure how much effort has gone into optimizing for local mode.

Joseph


"
shane knapp <sknapp@berkeley.edu>,"Thu, 2 Apr 2015 12:34:04 -0700",Re: Test all the things (Was: Unit test logs in Jenkins?),Nicholas Chammas <nicholas.chammas@gmail.com>,"cool.  FYI, i'm at databricks today and talked w/patrick, josh and davies
about this.  we have some great ideas to actually make this happen and will
be pushing over the next few weeks to get it done.  :)

m

t on this:
l
it into
at it
ed
at
n
ts
at
It
ted and
to
â€™s
:
an
we
t.
ld
d
r
t
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Thu, 2 Apr 2015 13:25:53 -0700",Re: Stochastic gradient descent performance,Joseph Bradley <joseph@databricks.com>,"I haven't looked closely at the sampling issues, but regarding the
aggregation latency, there are fixed overheads (in local and distributed
mode) with the way aggregation is done in Spark. Launching a stage of
tasks, fetching outputs from the previous stage etc. all have overhead, so
I would say its not efficient / recommended to run stages where computation
is less than 500ms or so. You could increase your batch size based on this
and hopefully that will help.

Regarding reducing these overheads by an order of magnitude it is a
challenging problem given the architecture in Spark -- I have some ideas
for this, but they are very much at a research stage.

Thanks
Shivaram


"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Thu, 2 Apr 2015 21:19:27 +0000",RE: Stochastic gradient descent performance,"""shivaram@eecs.berkeley.edu"" <shivaram@eecs.berkeley.edu>, Joseph Bradley
	<joseph@databricks.com>","Hi Shivaram,

It sounds really interesting! With this time we can estimate if it worth considering to run an iterative algorithm on Spark. For example, for SGD on Imagenet (450K samples) we will spend 450K*50ms=62.5 hours to traverse all data by one example not considering the data loading, computation and update times. One may need to traverse all data a number of times to converge. Letâ€™s say this number is equal to the batch size. So, we remain with 62.5 hours overhead. Is it reasonable?

Best regardram@eecs.berkeley.edu]
Sent: Thursday, April 02, 2015 1:26 PM
To: Joseph Bradley
Cc: Ulanov, Alexander; dev@spark.apache.org
Subject: Re: Stochastic gradient descent performance

I haven't looked closely at the sampling issues, but regarding the aggregation latency, there are fixed overheads (in local and distributed mode) with the way aggregation is done in Spark. Launching a stage of tasks, fetching outputs from the previous stage etc. all have overhead, so I would say its not efficient / recommended to run stages where computation is less than 500ms or so. You could increase your batch size based on this and hopefully that will help.

Regarding reducing these overheads by an order of magnitude it is a challenging problem given the architecture in Spark -- I have some ideas for this, but they are very much at a research stage.

Thanks
Shivaram

On Thu, Apr 2, 2015 at 12:00 PM, Joseph Bradley <joseph@databricks.com<mailto:joseph@databricks.com>> wrote:
When you say ""It seems that instead of sample it is better to shuffle data
and then access it sequentially by mini-batches,"" are you sure that holds
true for a big dataset in a cluster?  As far as implementing it, I haven't
looked carefully at GapSamplingIterator (in RandomSampler.scala) myself,
but that looks like it could be modified to be deterministic.

Hopefully someone else can comment on aggregation in local mode.  I'm not
sure how much effort has gone into optimizing for local mode.

Joseph

On Thu, Apr 2, 2015 at 11:33 AM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>
wrote:

>  Hi Joseph,
>
>
>
> Thank you for suggestion!
>
> It seems that instead of sample it is better to shuffle data and then
> access it sequentially by mini-batches. Could you suggest how to implement
> it?
>
>
>
> With regards to aggregate (reduce), I am wondering why it works so slow in
> local mode? Could you elaborate on this? I do understand that in cluster
> mode the network speed will kick in and then one can blame it.
>
>
>
> Best regards, Alexander
>
>
>
> *From:* Joseph Bradley [mailto:joseph@databricks.com<mailto:joseph@databricks.com>]
> *Sent:* Thursday, April 02, 2015 10:51 AM
> *To:* Ulanov, Alexander
> *Cc:* dev@spark.apache.org<mailto:dev@spark.apache.org>
> *Subject:* Re: Stochastic gradient descent performance
>
>
>
> It looks like SPARK-3250 was applied to the sample() which GradientDescent
> uses, and that should kick in for your minibatchFraction <= 0.4.  Based on
> your numbers, aggregation seems like the main issue, though I hesitate to
> optimize aggregation based on local tests for data sizes that small.
>
>
>
> The first thing I'd check for is unnecessary object creation, and to
> profile in a cluster or larger data setting.
>
>
>
> On Wed, Apr 1, 2015 at 10:09 AM, Ulanov, Alexander <
> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
>
> Sorry for bothering you again, but I think that it is an important issue
> for applicability of SGD in Spark MLlib. Could Spark developers please
> comment on it.
>
>
> -----Original Message-----
> From: Ulanov, Alexander
> Sent: Monday, March 30, 2015 5:00 PM
> To: dev@spark.apache.org<mailto:dev@spark.apache.org>
> Subject: Stochastic gradient descent performance
>
> Hi,
>
> It seems to me that there is an overhead in ""runMiniBatchSGD"" function of
> MLlib's ""GradientDescent"". In particular, ""sample"" and ""treeAggregate""
> might take time that is order of magnitude greater than the actual gradient
> computation. In particular, for mnist dataset of 60K instances, minibatch
> size = 0.001 (i.e. 60 samples) it take 0.15 s to sample and 0.3 to
> aggregate in local mode with 1 data partition on Core i5 processor. The
> actual gradient computation takes 0.002 s. I searched through Spark Jira
> and found that there was recently an update for more efficient sampling
> (SPARK-3250) that is already included in Spark codebase. Is there a way to
> reduce the sampling time and local treeRedeuce by order of magnitude?
>
> Best regards, Alexander
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org>
> For additional commands, e-mail: dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>
>
>
>

"
Xiangrui Meng <mengxr@gmail.com>,"Thu, 2 Apr 2015 17:05:44 -0700",Re: Using CUDA within Spark / boosting linear algebra,"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","This is great! Thanks! -Xiangrui


---------------------------------------------------------------------


"
"""Evan R. Sparks"" <evan.sparks@gmail.com>","Thu, 2 Apr 2015 17:07:26 -0700",Re: Using CUDA within Spark / boosting linear algebra,Xiangrui Meng <mengxr@gmail.com>,"Yeah, thanks Alex!


"
Tathagata Das <tdas@databricks.com>,"Thu, 2 Apr 2015 18:21:04 -0700",Re: Support for cycles in spark streaming topology ?????,Sean Owen <sowen@cloudera.com>,"Just to add to that, DStream.transform allows you do to arbitrary
RDD-to-RDD function. Inside that you can do iterative RDD operations as
well.



"
Steve Loughran <stevel@hortonworks.com>,"Fri, 3 Apr 2015 12:04:51 +0000",Re: Unit test logs in Jenkins?,Apache Spark Dev <dev@spark.apache.org>,"
rote:
ves.


I've been seeing (and remember, I've only recently started playing with funsuite), that the logs go to target/unit.log; edit log4j.properties to go to stdout and they go to stdout -but they don't go into the surefire logs. That is, the test runner isn't generating the classic <junit> XML report with its sections of stdout and stderr. While that format has its limitations, and you can certainly do better (ideally every log level event should be captured; let you turn them on/off, pull in threads, correlate timestamps across machines, etc, etc), having Junit & jenkins reports which include all the output logs testcase-by-testcase is pretty foundational. Maybe there's some way to turn it on that I haven't seen. 

---------------------------------------------------------------------


"
Michael Malak <michaelmalak@yahoo.com.INVALID>,"Fri, 3 Apr 2015 15:41:56 +0000 (UTC)",Wrong initial bias in GraphX SVDPlusPlus?,Dev <dev@spark.apache.org>,"I believe that in the initialization portion of GraphX SVDPlusPluS, the initialization of biases is incorrect. Specifically, in line 
https://github.com/apache/spark/blob/master/graphx/src/main/scala/org/apache/spark/graphx/lib/SVDPlusPlus.scala#L96 
instead of 
(vd._1, vd._2, msg.get._2 / msg.get._1, 1.0 / scala.math.sqrt(msg.get._1)) 
it should be 
(vd._1, vd._2, msg.get._2 / msg.get._1 - u, 1.0 / scala.math.sqrt(msg.get._1)) 

That is, the biases bu and bi (both represented as the third component of the Tuple4[] above, depending on whether the vertex is a user or an item), described in equation (1) of the Koren paper, are supposed to be small offsets to the mean (represented by the variable u, signifying the Greek letter mu) to account for peculiarities of individual users and items. 

Initializing these biases to wrong values should theoretically not matter given enough iterations of the algorithm, but some quick empirical testing shows it has trouble converging at all, even after many orders of magnitude additional iterations. 

This perhaps could be the source of previously reported trouble with SVDPlusPlus. 
http://apache-spark-user-list.1001560.n3.nabble.com/GraphX-SVDPlusPlus-problem-td12885.html 

If after a day, no one tells me I'm crazy here, I'll go ahead and create a Jira ticket. 

---------------------------------------------------------------------


"
sara mustafa <eng.sara.mustafa@gmail.com>,"Fri, 3 Apr 2015 09:22:13 -0700 (MST)",IntelliJ Runtime error,dev@spark.apache.org,"Hi,

I have built Spark 1.3.0 successfully on IntelliJ IDEA 14, but when i try to
SparkPi example under the examples module i face this error:
Exception in thread ""main"" java.lang.NoClassDefFoundError:
org/apache/spark/SparkConf
	at org.apache.spark.examples.SparkPi$.main(SparkPi.scala:27)
	at org.apache.spark.examples.SparkPi.main(SparkPi.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:134)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.SparkConf
	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
	... 7 more

Could anyone help me please?



--

---------------------------------------------------------------------


"
java8964 <java8964@hotmail.com>,"Fri, 3 Apr 2015 12:40:51 -0400",RE: IntelliJ Runtime error,"sara mustafa <eng.sara.mustafa@gmail.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","You have to change most of dependences in the spark-example model from ""provided"" to ""compile"", so you can run the example in Intellij.
Yong

ry to
:57)
mpl.java:43)
.n3.nabble.com/IntelliJ-Runtime-error-tp11383.html
.com.
 		 	   		  "
sara mustafa <eng.sara.mustafa@gmail.com>,"Fri, 3 Apr 2015 09:54:45 -0700 (MST)",RE: IntelliJ Runtime error,dev@spark.apache.org,"Thank you, it works with me when I changed the dependencies from provided to
compile. 



--

---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Fri, 3 Apr 2015 09:59:54 -0700","extended jenkins downtime, thursday april 9th 7am-noon PDT (moving to
 anaconda python & more)","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","welcome to python2.7+, java 8 and more!  :)

i'll be doing a major upgrade to our build system next thursday morning.
 here's a quick list of what's going on:

* installation of anaconda python on all worker nodes

* installation of pypy 2.5.1 (python 2.7) on all nodes

* matching installation of python modules for the current system python
(2.6), and anaconda python (2.6, 2.7 and 3.4)
  - anaconda python 2.7 will be the default for all workers (this has
stealthily been the case on amp-jenkins-worker-01 for the past two weeks,
and i've noticed no test failures)
  - you can now use anaconda environments to specify which version of
python to use in your tests:  http://www.continuum.io/blog/conda

* installation of new python 2.7 modules:  pymongo requests six pymongo
requests six python-crontab

* bare-bones mongodb installation on all workers

* installation of java 1.6 and 1.8 internal to jenkins
  - jobs will default to the system java, which is 1.7.0_75
  - if you want to run your tests w/java 6 or 8, you can select the JDK
version of your choice in the job configuration page (it'll be towards the
top)

these changes have actually all been tested against a variety of builds
(yay staging!) and while i'm certain that i have all of the kinks worked
out, i'm going to schedule a longer downtime so that i have a chance to
identify and squash any problems that surface.

thanks to josh rosen, k. shankari and davies liu for helping me test all of
this and get it working.

shane
"
Davies Liu <davies@databricks.com>,"Fri, 3 Apr 2015 10:24:36 -0700",Re: Haskell language Spark support,danilo2 <wojciech.danilo@gmail.com>,"The PR for integrate SparkR into Spark may help: https://github.com/apache/spark/pull/5096 

-- 
Davies Liu
Sent with Sparrow (http://www.sparrowmailapp.com/?sig)





"
"""York, Brennon"" <Brennon.York@capitalone.com>","Fri, 3 Apr 2015 15:02:36 -0400",Spark PR builder now with dependency reports!,"""dev@spark.apache.org"" <dev@spark.apache.org>","All, recently Spark has updated the PR builder in an attempt to better help the committers understand what, if anything, has changed wrt dependencies for new PR’s. This comes in the form of a fourth bullet posted on Github that I’m sure you’ve noticed when Jenkins is done building and running the Spark tests (language like “This patch does not change any dependencies.”). Right now there are sparse issues that, I believe, are related to timing problems (see build #29576 at https://github.com/apache/spark/pull/5286 for an example), but I’m not 100%.

So, if anyone sees an issue like the above where dependencies are reported as added or removed when nothing has been changed in any pom.xml file, please CC me on the Github PR (@brennonyork) to help me dive into this issue and figure out what’s going on.

Thanks everyone!
________________________________________________________

The information contained in this e-mail is confidential and/or proprietary is intended only for use by the individual or entity to which it is addressed.  If the reader of this message is not the intended recipient, you are hereby notified that any review, retransmission, dissemination, distribution, copying or other use of, or taking of any action in reliance upon this information is strictly prohibited. If you have received this communication in error, please contact the sender and delete the material from your computer.
"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Fri, 3 Apr 2015 19:21:56 +0000",Running LocalClusterSparkContext,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

I am trying to execute unit tests with LocalClusterSparkContext on Windows 7. I am getting a bunch of error in the log saying that: ""Cannot find any assembly build directories."" Below is the part from the log where it brakes. Could you suggest what's happening? In addition the application has created a folder ""app-20150403121631-0000"" and is keeping to create empty folders there named with numbers until I stop the application.


15/04/03 12:16:31.239 sparkWorker2-akka.actor.default-dispatcher-3 INFO Worker: Connecting to master akka.tcp://sparkMaster@Mynetwork.net:51990/user/Master...
15/04/03 12:16:31.277 sparkDriver-akka.actor.default-dispatcher-2 INFO AppClient$ClientActor: Connecting to master akka.tcp://sparkMaster@Mynetwork.net:51990/user/Master...
15/04/03 12:16:31.549 sparkMaster-akka.actor.default-dispatcher-7 INFO Master: Registering worker Mynetwork.net:52006 with 1 cores, 512.0 MB RAM
15/04/03 12:16:31.556 sparkMaster-akka.actor.default-dispatcher-7 INFO Master: Registering worker Mynetwork.net:52020 with 1 cores, 512.0 MB RAM
15/04/03 12:16:31.561 sparkMaster-akka.actor.default-dispatcher-7 INFO Master: Registering app test-cluster
15/04/03 12:16:31.568 sparkMaster-akka.actor.default-dispatcher-7 INFO Master: Registered app test-cluster with ID app-20150403121631-0000
15/04/03 12:16:31.573 sparkWorker1-akka.actor.default-dispatcher-3 INFO Worker: Successfully registered with master spark://Mynetwork.net:51990
15/04/03 12:16:31.588 sparkMaster-akka.actor.default-dispatcher-7 INFO Master: Launching executor app-20150403121631-0000/0 on worker worker-20150403121631-Mynetwork.net-52020
15/04/03 12:16:31.589 sparkMaster-akka.actor.default-dispatcher-7 INFO Master: Launching executor app-20150403121631-0000/1 on worker worker-20150403121631-Mynetwork.net-52006
15/04/03 12:16:31.590 sparkWorker2-akka.actor.default-dispatcher-3 INFO Worker: Successfully registered with master spark://Mynetwork.net:51990
15/04/03 12:16:31.595 sparkDriver-akka.actor.default-dispatcher-3 INFO SparkDeploySchedulerBackend: Connected to Spark cluster with app ID app-20150403121631-0000
15/04/03 12:16:31.608 sparkWorker1-akka.actor.default-dispatcher-4 INFO Worker: Asked to launch executor app-20150403121631-0000/1 for test-cluster
15/04/03 12:16:31.624 sparkWorker2-akka.actor.default-dispatcher-5 INFO Worker: Asked to launch executor app-20150403121631-0000/0 for test-cluster
15/04/03 12:16:31.639 sparkDriver-akka.actor.default-dispatcher-3 INFO AppClient$ClientActor: Executor added: app-20150403121631-0000/0 on worker-20150403121631-Mynetwork.net-52020 (Mynetwork.net:52020) with 1 cores
15/04/03 12:16:31.683 sparkDriver-akka.actor.default-dispatcher-3 INFO SparkDeploySchedulerBackend: Granted executor ID app-20150403121631-0000/0 on hostPort Mynetwork.net:52020 with 1 cores, 512.0 MB RAM
15/04/03 12:16:31.683 sparkDriver-akka.actor.default-dispatcher-3 INFO AppClient$ClientActor: Executor added: app-20150403121631-0000/1 on worker-20150403121631-Mynetwork.net-52006 (Mynetwork.net:52006) with 1 cores
15/04/03 12:16:31.684 sparkDriver-akka.actor.default-dispatcher-3 INFO SparkDeploySchedulerBackend: Granted executor ID app-20150403121631-0000/1 on hostPort Mynetwork.net:52006 with 1 cores, 512.0 MB RAM
15/04/03 12:16:31.688 sparkDriver-akka.actor.default-dispatcher-3 INFO AppClient$ClientActor: Executor updated: app-20150403121631-0000/1 is now LOADING
15/04/03 12:16:31.688 sparkDriver-akka.actor.default-dispatcher-3 INFO AppClient$ClientActor: Executor updated: app-20150403121631-0000/0 is now LOADING
15/04/03 12:16:31.688 ExecutorRunner for app-20150403121631-0000/1 ERROR ExecutorRunner: Error running executor
java.lang.IllegalStateException: Cannot find any assembly build directories.
                at org.apache.spark.launcher.CommandBuilderUtils.checkState(CommandBuilderUtils.java:228)
                at org.apache.spark.launcher.AbstractCommandBuilder.getScalaVersion(AbstractCommandBuilder.java:283)
                at org.apache.spark.launcher.AbstractCommandBuilder.buildClassPath(AbstractCommandBuilder.java:150)
                at org.apache.spark.launcher.AbstractCommandBuilder.buildJavaCommand(AbstractCommandBuilder.java:111)
                at org.apache.spark.launcher.WorkerCommandBuilder.buildCommand(WorkerCommandBuilder.scala:39)
                at org.apache.spark.launcher.WorkerCommandBuilder.buildCommand(WorkerCommandBuilder.scala:48)
                at org.apache.spark.deploy.worker.CommandUtils$.buildCommandSeq(CommandUtils.scala:61)
                at org.apache.spark.deploy.worker.CommandUtils$.buildProcessBuilder(CommandUtils.scala:49)
                at org.apache.spark.deploy.worker.ExecutorRunner.org$apache$spark$deploy$worker$ExecutorRunner$$fetchAndRunExecutor(ExecutorRunner.scala:132)
                at org.apache.spark.deploy.worker.ExecutorRunner$$anon$1.run(ExecutorRunner.scala:68)
15/04/03 12:16:31.712 sparkDriver-akka.actor.default-dispatcher-3 INFO AppClient$ClientActor: Executor updated: app-20150403121631-0000/0 is now RUNNING
15/04/03 12:16:31.725 sparkDriver-akka.actor.default-dispatcher-3 INFO AppClient$ClientActor: Executor updated: app-20150403121631-0000/1 is now RUNNING
15/04/03 12:16:31.726 sparkWorker1-akka.actor.default-dispatcher-4 INFO Worker: Executor app-20150403121631-0000/1 finished with state FAILED message java.lang.IllegalStateException: Cannot find any assembly build directories.
15/04/03 12:16:31.689 ExecutorRunner for app-20150403121631-0000/0 ERROR ExecutorRunner: Error running executor
java.lang.IllegalStateException: Cannot find any assembly build directories.

Best regards, Alexander
"
Marcelo Vanzin <vanzin@cloudera.com>,"Fri, 3 Apr 2015 12:31:09 -0700",Re: Running LocalClusterSparkContext,"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","When was the last time you pulled? That should have been fixed as part
of SPARK-6473.

Notice latest master suffers from SPARK-6673 on Windows.

s 7. I am getting a bunch of error in the log saying that: ""Cannot find any assembly build directories."" Below is the part from the log where it brakes. Could you suggest what's happening? In addition the application has created a folder ""app-20150403121631-0000"" and is keeping to create empty folders there named with numbers until I stop the application.
orker: Connecting to master akka.tcp://sparkMaster@Mynetwork.net:51990/user/Master...
pClient$ClientActor: Connecting to master akka.tcp://sparkMaster@Mynetwork.net:51990/user/Master...
ster: Registering worker Mynetwork.net:52006 with 1 cores, 512.0 MB RAM
ster: Registering worker Mynetwork.net:52020 with 1 cores, 512.0 MB RAM
ster: Registering app test-cluster
ster: Registered app test-cluster with ID app-20150403121631-0000
orker: Successfully registered with master spark://Mynetwork.net:51990
ster: Launching executor app-20150403121631-0000/0 on worker worker-20150403121631-Mynetwork.net-52020
ster: Launching executor app-20150403121631-0000/1 on worker worker-20150403121631-Mynetwork.net-52006
orker: Successfully registered with master spark://Mynetwork.net:51990
arkDeploySchedulerBackend: Connected to Spark cluster with app ID app-20150403121631-0000
orker: Asked to launch executor app-20150403121631-0000/1 for test-cluster
orker: Asked to launch executor app-20150403121631-0000/0 for test-cluster
pClient$ClientActor: Executor added: app-20150403121631-0000/0 on worker-20150403121631-Mynetwork.net-52020 (Mynetwork.net:52020) with 1 cores
arkDeploySchedulerBackend: Granted executor ID app-20150403121631-0000/0 on hostPort Mynetwork.net:52020 with 1 cores, 512.0 MB RAM
pClient$ClientActor: Executor added: app-20150403121631-0000/1 on worker-20150403121631-Mynetwork.net-52006 (Mynetwork.net:52006) with 1 cores
arkDeploySchedulerBackend: Granted executor ID app-20150403121631-0000/1 on hostPort Mynetwork.net:52006 with 1 cores, 512.0 MB RAM
pClient$ClientActor: Executor updated: app-20150403121631-0000/1 is now LOADING
pClient$ClientActor: Executor updated: app-20150403121631-0000/0 is now LOADING
ExecutorRunner: Error running executor
es.
te(CommandBuilderUtils.java:228)
alaVersion(AbstractCommandBuilder.java:283)
ClassPath(AbstractCommandBuilder.java:150)
JavaCommand(AbstractCommandBuilder.java:111)
mmand(WorkerCommandBuilder.scala:39)
mmand(WorkerCommandBuilder.scala:48)
andSeq(CommandUtils.scala:61)
essBuilder(CommandUtils.scala:49)
he$spark$deploy$worker$ExecutorRunner$$fetchAndRunExecutor(ExecutorRunner.scala:132)
run(ExecutorRunner.scala:68)
pClient$ClientActor: Executor updated: app-20150403121631-0000/0 is now RUNNING
pClient$ClientActor: Executor updated: app-20150403121631-0000/1 is now RUNNING
orker: Executor app-20150403121631-0000/1 finished with state FAILED message java.lang.IllegalStateException: Cannot find any assembly build directories.
ExecutorRunner: Error running executor
es.



-- 
Marcelo

---------------------------------------------------------------------


"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Fri, 3 Apr 2015 19:48:39 +0000",RE: Running LocalClusterSparkContext,Marcelo Vanzin <vanzin@cloudera.com>,"Hi Marcelo,

Thank you for quick response!
It seems that I get the issue 6673. If I set set SPARK_SCALA_VERSION=2.10 as suggested in https://issues.apache.org/jira/browse/SPARK-6673, I get instead:

15/04/03 12:46:24.510 ExecutorRunner for app-20150403124624-0000/0 ERROR ExecutorRunner: Error running executor
java.lang.IllegalStateException: No assemblies found in 'C:\ulanov\dev\spark\mllib\.\assembly\target\scala-2.10'.

-----Original Message-----
From: Marcelo Vanzin [mailto:vanzin@cloudera.com] 
Sent: Friday, April 03, 2015 12:31 PM
To: Ulanov, Alexander
Cc: dev@spark.apache.org
Subject: Re: Running LocalClusterSparkContext

When was the last time you pulled? That should have been fixed as part of SPARK-6473.

Notice latest master suffers from SPARK-6673 on Windows.

On Fri, Apr 3, 2015 at 12:21 PM, Ulanov, Alexander <alexander.ulanov@hp.com> wrote:
> Hi,
>
> I am trying to execute unit tests with LocalClusterSparkContext on Windows 7. I am getting a bunch of error in the log saying that: ""Cannot find any assembly build directories."" Below is the part from the log where it brakes. Could you suggest what's happening? In addition the application has created a folder ""app-20150403121631-0000"" and is keeping to create empty folders there named with numbers until I stop the application.
>
>
> 15/04/03 12:16:31.239 sparkWorker2-akka.actor.default-dispatcher-3 INFO Worker: Connecting to master akka.tcp://sparkMaster@Mynetwork.net:51990/user/Master...
> 15/04/03 12:16:31.277 sparkDriver-akka.actor.default-dispatcher-2 INFO AppClient$ClientActor: Connecting to master akka.tcp://sparkMaster@Mynetwork.net:51990/user/Master...
> 15/04/03 12:16:31.549 sparkMaster-akka.actor.default-dispatcher-7 INFO 
> Master: Registering worker Mynetwork.net:52006 with 1 cores, 512.0 MB 
> RAM
> 15/04/03 12:16:31.556 sparkMaster-akka.actor.default-dispatcher-7 INFO 
> Master: Registering worker Mynetwork.net:52020 with 1 cores, 512.0 MB 
> RAM
> 15/04/03 12:16:31.561 sparkMaster-akka.actor.default-dispatcher-7 INFO 
> Master: Registering app test-cluster
> 15/04/03 12:16:31.568 sparkMaster-akka.actor.default-dispatcher-7 INFO 
> Master: Registered app test-cluster with ID app-20150403121631-0000
> 15/04/03 12:16:31.573 sparkWorker1-akka.actor.default-dispatcher-3 
> INFO Worker: Successfully registered with master 
> spark://Mynetwork.net:51990
> 15/04/03 12:16:31.588 sparkMaster-akka.actor.default-dispatcher-7 INFO 
> Master: Launching executor app-20150403121631-0000/0 on worker 
> worker-20150403121631-Mynetwork.net-52020
> 15/04/03 12:16:31.589 sparkMaster-akka.actor.default-dispatcher-7 INFO 
> Master: Launching executor app-20150403121631-0000/1 on worker 
> worker-20150403121631-Mynetwork.net-52006
> 15/04/03 12:16:31.590 sparkWorker2-akka.actor.default-dispatcher-3 
> INFO Worker: Successfully registered with master 
> spark://Mynetwork.net:51990
> 15/04/03 12:16:31.595 sparkDriver-akka.actor.default-dispatcher-3 INFO 
> SparkDeploySchedulerBackend: Connected to Spark cluster with app ID 
> app-20150403121631-0000
> 15/04/03 12:16:31.608 sparkWorker1-akka.actor.default-dispatcher-4 
> INFO Worker: Asked to launch executor app-20150403121631-0000/1 for 
> test-cluster
> 15/04/03 12:16:31.624 sparkWorker2-akka.actor.default-dispatcher-5 
> INFO Worker: Asked to launch executor app-20150403121631-0000/0 for 
> test-cluster
> 15/04/03 12:16:31.639 sparkDriver-akka.actor.default-dispatcher-3 INFO 
> AppClient$ClientActor: Executor added: app-20150403121631-0000/0 on 
> worker-20150403121631-Mynetwork.net-52020 (Mynetwork.net:52020) with 1 
> cores
> 15/04/03 12:16:31.683 sparkDriver-akka.actor.default-dispatcher-3 INFO 
> SparkDeploySchedulerBackend: Granted executor ID 
> app-20150403121631-0000/0 on hostPort Mynetwork.net:52020 with 1 
> cores, 512.0 MB RAM
> 15/04/03 12:16:31.683 sparkDriver-akka.actor.default-dispatcher-3 INFO 
> AppClient$ClientActor: Executor added: app-20150403121631-0000/1 on 
> worker-20150403121631-Mynetwork.net-52006 (Mynetwork.net:52006) with 1 
> cores
> 15/04/03 12:16:31.684 sparkDriver-akka.actor.default-dispatcher-3 INFO 
> SparkDeploySchedulerBackend: Granted executor ID 
> app-20150403121631-0000/1 on hostPort Mynetwork.net:52006 with 1 
> cores, 512.0 MB RAM
> 15/04/03 12:16:31.688 sparkDriver-akka.actor.default-dispatcher-3 INFO 
> AppClient$ClientActor: Executor updated: app-20150403121631-0000/1 is 
> now LOADING
> 15/04/03 12:16:31.688 sparkDriver-akka.actor.default-dispatcher-3 INFO 
> AppClient$ClientActor: Executor updated: app-20150403121631-0000/0 is 
> now LOADING
> 15/04/03 12:16:31.688 ExecutorRunner for app-20150403121631-0000/1 
> ERROR ExecutorRunner: Error running executor
> java.lang.IllegalStateException: Cannot find any assembly build directories.
>                 at org.apache.spark.launcher.CommandBuilderUtils.checkState(CommandBuilderUtils.java:228)
>                 at org.apache.spark.launcher.AbstractCommandBuilder.getScalaVersion(AbstractCommandBuilder.java:283)
>                 at org.apache.spark.launcher.AbstractCommandBuilder.buildClassPath(AbstractCommandBuilder.java:150)
>                 at org.apache.spark.launcher.AbstractCommandBuilder.buildJavaCommand(AbstractCommandBuilder.java:111)
>                 at org.apache.spark.launcher.WorkerCommandBuilder.buildCommand(WorkerCommandBuilder.scala:39)
>                 at org.apache.spark.launcher.WorkerCommandBuilder.buildCommand(WorkerCommandBuilder.scala:48)
>                 at org.apache.spark.deploy.worker.CommandUtils$.buildCommandSeq(CommandUtils.scala:61)
>                 at org.apache.spark.deploy.worker.CommandUtils$.buildProcessBuilder(CommandUtils.scala:49)
>                 at org.apache.spark.deploy.worker.ExecutorRunner.org$apache$spark$deploy$worker$ExecutorRunner$$fetchAndRunExecutor(ExecutorRunner.scala:132)
>                 at 
> org.apache.spark.deploy.worker.ExecutorRunner$$anon$1.run(ExecutorRunn
> er.scala:68)
> 15/04/03 12:16:31.712 sparkDriver-akka.actor.default-dispatcher-3 INFO 
> AppClient$ClientActor: Executor updated: app-20150403121631-0000/0 is 
> now RUNNING
> 15/04/03 12:16:31.725 sparkDriver-akka.actor.default-dispatcher-3 INFO 
> AppClient$ClientActor: Executor updated: app-20150403121631-0000/1 is 
> now RUNNING
> 15/04/03 12:16:31.726 sparkWorker1-akka.actor.default-dispatcher-4 INFO Worker: Executor app-20150403121631-0000/1 finished with state FAILED message java.lang.IllegalStateException: Cannot find any assembly build directories.
> 15/04/03 12:16:31.689 ExecutorRunner for app-20150403121631-0000/0 
> ERROR ExecutorRunner: Error running executor
> java.lang.IllegalStateException: Cannot find any assembly build directories.
>
> Best regards, Alexander



--
Marcelo
"
Marcelo Vanzin <vanzin@cloudera.com>,"Fri, 3 Apr 2015 12:52:22 -0700",Re: Running LocalClusterSparkContext,"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","That looks like another bug on top of 6673; can you point that out in
the PR to make sure it's covered?

In the meantime, could you try running the command from $SPARK_HOME
instead of from inside the mllib directory?

10 as suggested in https://issues.apache.org/jira/browse/SPARK-6673, I get instead:
ExecutorRunner: Error running executor
ark\mllib\.\assembly\target\scala-2.10'.
 SPARK-6473.
ws 7. I am getting a bunch of error in the log saying that: ""Cannot find any assembly build directories."" Below is the part from the log where it brakes. Could you suggest what's happening? In addition the application has created a folder ""app-20150403121631-0000"" and is keeping to create empty folders there named with numbers until I stop the application.
Worker: Connecting to master akka.tcp://sparkMaster@Mynetwork.net:51990/user/Master...
ppClient$ClientActor: Connecting to master akka.tcp://sparkMaster@Mynetwork.net:51990/user/Master...
ies.
ate(CommandBuilderUtils.java:228)
calaVersion(AbstractCommandBuilder.java:283)
dClassPath(AbstractCommandBuilder.java:150)
dJavaCommand(AbstractCommandBuilder.java:111)
ommand(WorkerCommandBuilder.scala:39)
ommand(WorkerCommandBuilder.scala:48)
mandSeq(CommandUtils.scala:61)
cessBuilder(CommandUtils.scala:49)
che$spark$deploy$worker$ExecutorRunner$$fetchAndRunExecutor(ExecutorRunner.scala:132)
Worker: Executor app-20150403121631-0000/1 finished with state FAILED message java.lang.IllegalStateException: Cannot find any assembly build directories.
ies.



-- 
Marcelo

---------------------------------------------------------------------


"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Fri, 3 Apr 2015 20:01:11 +0000",RE: Running LocalClusterSparkContext,Marcelo Vanzin <vanzin@cloudera.com>,"Thanks! It worked. I've updated the PR and now start the test with working directory in SPARK_HOME

Now I get:
ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.

Any ideas?

-----Original Message-----
From: Marcelo Vanzin [mailto:vanzin@cloudera.com] 
Sent: Friday, April 03, 2015 12:52 PM
To: Ulanov, Alexander
Cc: dev@spark.apache.org
Subject: Re: Running LocalClusterSparkContext

That looks like another bug on top of 6673; can you point that out in the PR to make sure it's covered?

In the meantime, could you try running the command from $SPARK_HOME instead of from inside the mllib directory?

On Fri, Apr 3, 2015 at 12:48 PM, Ulanov, Alexander <alexander.ulanov@hp.com> wrote:
> Hi Marcelo,
>
> Thank you for quick response!
> It seems that I get the issue 6673. If I set set SPARK_SCALA_VERSION=2.10 as suggested in https://issues.apache.org/jira/browse/SPARK-6673, I get instead:
>
> 15/04/03 12:46:24.510 ExecutorRunner for app-20150403124624-0000/0 
> ERROR ExecutorRunner: Error running executor
> java.lang.IllegalStateException: No assemblies found in 'C:\ulanov\dev\spark\mllib\.\assembly\target\scala-2.10'.
>
> -----Original Message-----
> F Friday, April 03, 2015 12:31 PM
> To: Ulanov, Alexander
> Cc: dev@spark.apache.org
> Subject: Re: Running LocalClusterSparkContext
>
> When was the last time you pulled? That should have been fixed as part of SPARK-6473.
>
> Notice latest master suffers from SPARK-6673 on Windows.
>
> On Fri, Apr 3, 2015 at 12:21 PM, Ulanov, Alexander <alexander.ulanov@hp.com> wrote:
>> Hi,
>>
>> I am trying to execute unit tests with LocalClusterSparkContext on Windows 7. I am getting a bunch of error in the log saying that: ""Cannot find any assembly build directories."" Below is the part from the log where it brakes. Could you suggest what's happening? In addition the application has created a folder ""app-20150403121631-0000"" and is keeping to create empty folders there named with numbers until I stop the application.
>>
>>
>> 15/04/03 12:16:31.239 sparkWorker2-akka.actor.default-dispatcher-3 INFO Worker: Connecting to master akka.tcp://sparkMaster@Mynetwork.net:51990/user/Master...
>> 15/04/03 12:16:31.277 sparkDriver-akka.actor.default-dispatcher-2 INFO AppClient$ClientActor: Connecting to master akka.tcp://sparkMaster@Mynetwork.net:51990/user/Master...
>> 15/04/03 12:16:31.549 sparkMaster-akka.actor.default-dispatcher-7 
>> INFO
>> Master: Registering worker Mynetwork.net:52006 with 1 cores, 512.0 MB 
>> RAM
>> 15/04/03 12:16:31.556 sparkMaster-akka.actor.default-dispatcher-7 
>> INFO
>> Master: Registering worker Mynetwork.net:52020 with 1 cores, 512.0 MB 
>> RAM
>> 15/04/03 12:16:31.561 sparkMaster-akka.actor.default-dispatcher-7 
>> INFO
>> Master: Registering app test-cluster
>> 15/04/03 12:16:31.568 sparkMaster-akka.actor.default-dispatcher-7 
>> INFO
>> Master: Registered app test-cluster with ID app-20150403121631-0000
>> 15/04/03 12:16:31.573 sparkWorker1-akka.actor.default-dispatcher-3
>> INFO Worker: Successfully registered with master
>> spark://Mynetwork.net:51990
>> 15/04/03 12:16:31.588 sparkMaster-akka.actor.default-dispatcher-7 
>> INFO
>> Master: Launching executor app-20150403121631-0000/0 on worker
>> worker-20150403121631-Mynetwork.net-52020
>> 15/04/03 12:16:31.589 sparkMaster-akka.actor.default-dispatcher-7 
>> INFO
>> Master: Launching executor app-20150403121631-0000/1 on worker
>> worker-20150403121631-Mynetwork.net-52006
>> 15/04/03 12:16:31.590 sparkWorker2-akka.actor.default-dispatcher-3
>> INFO Worker: Successfully registered with master
>> spark://Mynetwork.net:51990
>> 15/04/03 12:16:31.595 sparkDriver-akka.actor.default-dispatcher-3 
>> INFO
>> SparkDeploySchedulerBackend: Connected to Spark cluster with app ID
>> app-20150403121631-0000
>> 15/04/03 12:16:31.608 sparkWorker1-akka.actor.default-dispatcher-4
>> INFO Worker: Asked to launch executor app-20150403121631-0000/1 for 
>> test-cluster
>> 15/04/03 12:16:31.624 sparkWorker2-akka.actor.default-dispatcher-5
>> INFO Worker: Asked to launch executor app-20150403121631-0000/0 for 
>> test-cluster
>> 15/04/03 12:16:31.639 sparkDriver-akka.actor.default-dispatcher-3 
>> INFO
>> AppClient$ClientActor: Executor added: app-20150403121631-0000/0 on
>> worker-20150403121631-Mynetwork.net-52020 (Mynetwork.net:52020) with 
>> 1 cores
>> 15/04/03 12:16:31.683 sparkDriver-akka.actor.default-dispatcher-3 
>> INFO
>> SparkDeploySchedulerBackend: Granted executor ID
>> app-20150403121631-0000/0 on hostPort Mynetwork.net:52020 with 1 
>> cores, 512.0 MB RAM
>> 15/04/03 12:16:31.683 sparkDriver-akka.actor.default-dispatcher-3 
>> INFO
>> AppClient$ClientActor: Executor added: app-20150403121631-0000/1 on
>> worker-20150403121631-Mynetwork.net-52006 (Mynetwork.net:52006) with 
>> 1 cores
>> 15/04/03 12:16:31.684 sparkDriver-akka.actor.default-dispatcher-3 
>> INFO
>> SparkDeploySchedulerBackend: Granted executor ID
>> app-20150403121631-0000/1 on hostPort Mynetwork.net:52006 with 1 
>> cores, 512.0 MB RAM
>> 15/04/03 12:16:31.688 sparkDriver-akka.actor.default-dispatcher-3 
>> INFO
>> AppClient$ClientActor: Executor updated: app-20150403121631-0000/1 is 
>> now LOADING
>> 15/04/03 12:16:31.688 sparkDriver-akka.actor.default-dispatcher-3 
>> INFO
>> AppClient$ClientActor: Executor updated: app-20150403121631-0000/0 is 
>> now LOADING
>> 15/04/03 12:16:31.688 ExecutorRunner for app-20150403121631-0000/1 
>> ERROR ExecutorRunner: Error running executor
>> java.lang.IllegalStateException: Cannot find any assembly build directories.
>>                 at org.apache.spark.launcher.CommandBuilderUtils.checkState(CommandBuilderUtils.java:228)
>>                 at org.apache.spark.launcher.AbstractCommandBuilder.getScalaVersion(AbstractCommandBuilder.java:283)
>>                 at org.apache.spark.launcher.AbstractCommandBuilder.buildClassPath(AbstractCommandBuilder.java:150)
>>                 at org.apache.spark.launcher.AbstractCommandBuilder.buildJavaCommand(AbstractCommandBuilder.java:111)
>>                 at org.apache.spark.launcher.WorkerCommandBuilder.buildCommand(WorkerCommandBuilder.scala:39)
>>                 at org.apache.spark.launcher.WorkerCommandBuilder.buildCommand(WorkerCommandBuilder.scala:48)
>>                 at org.apache.spark.deploy.worker.CommandUtils$.buildCommandSeq(CommandUtils.scala:61)
>>                 at org.apache.spark.deploy.worker.CommandUtils$.buildProcessBuilder(CommandUtils.scala:49)
>>                 at org.apache.spark.deploy.worker.ExecutorRunner.org$apache$spark$deploy$worker$ExecutorRunner$$fetchAndRunExecutor(ExecutorRunner.scala:132)
>>                 at
>> org.apache.spark.deploy.worker.ExecutorRunner$$anon$1.run(ExecutorRun
>> n
>> er.scala:68)
>> 15/04/03 12:16:31.712 sparkDriver-akka.actor.default-dispatcher-3 
>> INFO
>> AppClient$ClientActor: Executor updated: app-20150403121631-0000/0 is 
>> now RUNNING
>> 15/04/03 12:16:31.725 sparkDriver-akka.actor.default-dispatcher-3 
>> INFO
>> AppClient$ClientActor: Executor updated: app-20150403121631-0000/1 is 
>> now RUNNING
>> 15/04/03 12:16:31.726 sparkWorker1-akka.actor.default-dispatcher-4 INFO Worker: Executor app-20150403121631-0000/1 finished with state FAILED message java.lang.IllegalStateException: Cannot find any assembly build directories.
>> 15/04/03 12:16:31.689 ExecutorRunner for app-20150403121631-0000/0 
>> ERROR ExecutorRunner: Error running executor
>> java.lang.IllegalStateException: Cannot find any assembly build directories.
>>
>> Best regards, Alexander
>
>
>
> --
> Marcelo



--
Marcelo
"
Marcelo Vanzin <vanzin@cloudera.com>,"Fri, 3 Apr 2015 13:03:31 -0700",Re: Running LocalClusterSparkContext,"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","SPARK-2356

g directory in SPARK_HOME
th
 the Hadoop binaries.
 PR to make sure it's covered?
ad of from inside the mllib directory?
.10 as suggested in https://issues.apache.org/jira/browse/SPARK-6673, I get instead:
park\mllib\.\assembly\target\scala-2.10'.
f SPARK-6473.
ows 7. I am getting a bunch of error in the log saying that: ""Cannot find any assembly build directories."" Below is the part from the log where it brakes. Could you suggest what's happening? In addition the application has created a folder ""app-20150403121631-0000"" and is keeping to create empty folders there named with numbers until I stop the application.
 Worker: Connecting to master akka.tcp://sparkMaster@Mynetwork.net:51990/user/Master...
AppClient$ClientActor: Connecting to master akka.tcp://sparkMaster@Mynetwork.net:51990/user/Master...
ries.
tate(CommandBuilderUtils.java:228)
ScalaVersion(AbstractCommandBuilder.java:283)
ldClassPath(AbstractCommandBuilder.java:150)
ldJavaCommand(AbstractCommandBuilder.java:111)
Command(WorkerCommandBuilder.scala:39)
Command(WorkerCommandBuilder.scala:48)
mmandSeq(CommandUtils.scala:61)
ocessBuilder(CommandUtils.scala:49)
ache$spark$deploy$worker$ExecutorRunner$$fetchAndRunExecutor(ExecutorRunner.scala:132)
 Worker: Executor app-20150403121631-0000/1 finished with state FAILED message java.lang.IllegalStateException: Cannot find any assembly build directories.
ries.



-- 
Marcelo

---------------------------------------------------------------------


"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Fri, 3 Apr 2015 22:40:25 +0000",RE: Running LocalClusterSparkContext,Marcelo Vanzin <vanzin@cloudera.com>,"Thank you so much! It allows me to proceed and Spark started to execute my code. I use sc.textFile and Spark crashes on it: 

15/04/03 15:15:21.572 sparkDriver-akka.actor.default-dispatcher-4 INFO SparkDeploySchedulerBackend: Registered executor: Actor[akka.tcp://sparkExecutor@Mynetwork.net:57652/user/Executor#1137816732] with ID 1
15/04/03 15:15:21.601 sparkDriver-akka.actor.default-dispatcher-4 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, Mynetwork.net, PROCESS_LOCAL, 1301 bytes)
15/04/03 15:15:22.285 sparkDriver-akka.actor.default-dispatcher-2 INFO BlockManagerMasterActor: Registering block manager Mynetwork.net:57687 with 265.1 MB RAM, BlockManagerId(1, Mynetwork.net, 57687)
15/04/03 15:15:22.436 sparkDriver-akka.actor.default-dispatcher-2 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, Mynetwork.net, PROCESS_LOCAL, 1301 bytes)
15/04/03 15:15:22.455 task-result-getter-0 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, Mynetwork.net): java.io.EOFException
	at java.io.ObjectInputStream$BlockDataInputStream.readFully(ObjectInputStream.java:2747)
	at java.io.ObjectInputStream.readFully(ObjectInputStream.java:1033)

Could you suggest?

(it seems that new version of Spark was not tested for Windows. Previous versions worked more or less fine for me)

-----Original Message-----
From: Marcelo Vanzin [mailto:vanzin@cloudera.com] 
Sent: Friday, April 03, 2015 1:04 PM
To: Ulanov, Alexander
Cc: dev@spark.apache.org
Subject: Re: Running LocalClusterSparkContext

SPARK-2356

On Fri, Apr 3, 2015 at 1:01 PM, Ulanov, Alexander <alexander.ulanov@hp.com> wrote:
> Thanks! It worked. I've updated the PR and now start the test with 
> working directory in SPARK_HOME
>
> Now I get:
> ERROR Shell: Failed to locate the winutils binary in the hadoop binary 
> path
> java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
>
> Any ideas?
>
> -----Original Message-----
> From: Marcelo Vanzin [mailto:vanzin@cloudera.com]
> Sent: Friday, April 03, 2015 12:52 PM
> To: Ulanov, Alexander
> Cc: dev@spark.apache.org
> Subject: Re: Running LocalClusterSparkContext
>
> That looks like another bug on top of 6673; can you point that out in the PR to make sure it's covered?
>
> In the meantime, could you try running the command from $SPARK_HOME instead of from inside the mllib directory?
>
> On Fri, Apr 3, 2015 at 12:48 PM, Ulanov, Alexander <alexander.ulanov@hp.com> wrote:
>> Hi Marcelo,
>>
>> Thank you for quick response!
>> It seems that I get the issue 6673. If I set set SPARK_SCALA_VERSION=2.10 as suggested in https://issues.apache.org/jira/browse/SPARK-6673, I get instead:
>>
>> 15/04/03 12:46:24.510 ExecutorRunner for app-20150403124624-0000/0 
>> ERROR ExecutorRunner: Error running executor
>> java.lang.IllegalStateException: No assemblies found in 'C:\ulanov\dev\spark\mllib\.\assembly\target\scala-2.10'.
>>
>> -----Original Message-----
>> From: Marcelo Vanzin [mailto:31 PM
>> To: Ulanov, Alexander
>> Cc: dev@spark.apache.org
>> Subject: Re: Running LocalClusterSparkContext
>>
>> When was the last time you pulled? That should have been fixed as part of SPARK-6473.
>>
>> Notice latest master suffers from SPARK-6673 on Windows.
>>
>> On Fri, Apr 3, 2015 at 12:21 PM, Ulanov, Alexander <alexander.ulanov@hp.com> wrote:
>>> Hi,
>>>
>>> I am trying to execute unit tests with LocalClusterSparkContext on Windows 7. I am getting a bunch of error in the log saying that: ""Cannot find any assembly build directories."" Below is the part from the log where it brakes. Could you suggest what's happening? In addition the application has created a folder ""app-20150403121631-0000"" and is keeping to create empty folders there named with numbers until I stop the application.
>>>
>>>
>>> 15/04/03 12:16:31.239 sparkWorker2-akka.actor.default-dispatcher-3 INFO Worker: Connecting to master akka.tcp://sparkMaster@Mynetwork.net:51990/user/Master...
>>> 15/04/03 12:16:31.277 sparkDriver-akka.actor.default-dispatcher-2 INFO AppClient$ClientActor: Connecting to master akka.tcp://sparkMaster@Mynetwork.net:51990/user/Master...
>>> 15/04/03 12:16:31.549 sparkMaster-akka.actor.default-dispatcher-7
>>> INFO
>>> Master: Registering worker Mynetwork.net:52006 with 1 cores, 512.0 
>>> MB RAM
>>> 15/04/03 12:16:31.556 sparkMaster-akka.actor.default-dispatcher-7
>>> INFO
>>> Master: Registering worker Mynetwork.net:52020 with 1 cores, 512.0 
>>> MB RAM
>>> 15/04/03 12:16:31.561 sparkMaster-akka.actor.default-dispatcher-7
>>> INFO
>>> Master: Registering app test-cluster
>>> 15/04/03 12:16:31.568 sparkMaster-akka.actor.default-dispatcher-7
>>> INFO
>>> Master: Registered app test-cluster with ID app-20150403121631-0000
>>> 15/04/03 12:16:31.573 sparkWorker1-akka.actor.default-dispatcher-3
>>> INFO Worker: Successfully registered with master
>>> spark://Mynetwork.net:51990
>>> 15/04/03 12:16:31.588 sparkMaster-akka.actor.default-dispatcher-7
>>> INFO
>>> Master: Launching executor app-20150403121631-0000/0 on worker
>>> worker-20150403121631-Mynetwork.net-52020
>>> 15/04/03 12:16:31.589 sparkMaster-akka.actor.default-dispatcher-7
>>> INFO
>>> Master: Launching executor app-20150403121631-0000/1 on worker
>>> worker-20150403121631-Mynetwork.net-52006
>>> 15/04/03 12:16:31.590 sparkWorker2-akka.actor.default-dispatcher-3
>>> INFO Worker: Successfully registered with master
>>> spark://Mynetwork.net:51990
>>> 15/04/03 12:16:31.595 sparkDriver-akka.actor.default-dispatcher-3
>>> INFO
>>> SparkDeploySchedulerBackend: Connected to Spark cluster with app ID
>>> app-20150403121631-0000
>>> 15/04/03 12:16:31.608 sparkWorker1-akka.actor.default-dispatcher-4
>>> INFO Worker: Asked to launch executor app-20150403121631-0000/1 for 
>>> test-cluster
>>> 15/04/03 12:16:31.624 sparkWorker2-akka.actor.default-dispatcher-5
>>> INFO Worker: Asked to launch executor app-20150403121631-0000/0 for 
>>> test-cluster
>>> 15/04/03 12:16:31.639 sparkDriver-akka.actor.default-dispatcher-3
>>> INFO
>>> AppClient$ClientActor: Executor added: app-20150403121631-0000/0 on
>>> worker-20150403121631-Mynetwork.net-52020 (Mynetwork.net:52020) with
>>> 1 cores
>>> 15/04/03 12:16:31.683 sparkDriver-akka.actor.default-dispatcher-3
>>> INFO
>>> SparkDeploySchedulerBackend: Granted executor ID
>>> app-20150403121631-0000/0 on hostPort Mynetwork.net:52020 with 1 
>>> cores, 512.0 MB RAM
>>> 15/04/03 12:16:31.683 sparkDriver-akka.actor.default-dispatcher-3
>>> INFO
>>> AppClient$ClientActor: Executor added: app-20150403121631-0000/1 on
>>> worker-20150403121631-Mynetwork.net-52006 (Mynetwork.net:52006) with
>>> 1 cores
>>> 15/04/03 12:16:31.684 sparkDriver-akka.actor.default-dispatcher-3
>>> INFO
>>> SparkDeploySchedulerBackend: Granted executor ID
>>> app-20150403121631-0000/1 on hostPort Mynetwork.net:52006 with 1 
>>> cores, 512.0 MB RAM
>>> 15/04/03 12:16:31.688 sparkDriver-akka.actor.default-dispatcher-3
>>> INFO
>>> AppClient$ClientActor: Executor updated: app-20150403121631-0000/1 
>>> is now LOADING
>>> 15/04/03 12:16:31.688 sparkDriver-akka.actor.default-dispatcher-3
>>> INFO
>>> AppClient$ClientActor: Executor updated: app-20150403121631-0000/0 
>>> is now LOADING
>>> 15/04/03 12:16:31.688 ExecutorRunner for app-20150403121631-0000/1 
>>> ERROR ExecutorRunner: Error running executor
>>> java.lang.IllegalStateException: Cannot find any assembly build directories.
>>>                 at org.apache.spark.launcher.CommandBuilderUtils.checkState(CommandBuilderUtils.java:228)
>>>                 at org.apache.spark.launcher.AbstractCommandBuilder.getScalaVersion(AbstractCommandBuilder.java:283)
>>>                 at org.apache.spark.launcher.AbstractCommandBuilder.buildClassPath(AbstractCommandBuilder.java:150)
>>>                 at org.apache.spark.launcher.AbstractCommandBuilder.buildJavaCommand(AbstractCommandBuilder.java:111)
>>>                 at org.apache.spark.launcher.WorkerCommandBuilder.buildCommand(WorkerCommandBuilder.scala:39)
>>>                 at org.apache.spark.launcher.WorkerCommandBuilder.buildCommand(WorkerCommandBuilder.scala:48)
>>>                 at org.apache.spark.deploy.worker.CommandUtils$.buildCommandSeq(CommandUtils.scala:61)
>>>                 at org.apache.spark.deploy.worker.CommandUtils$.buildProcessBuilder(CommandUtils.scala:49)
>>>                 at org.apache.spark.deploy.worker.ExecutorRunner.org$apache$spark$deploy$worker$ExecutorRunner$$fetchAndRunExecutor(ExecutorRunner.scala:132)
>>>                 at
>>> org.apache.spark.deploy.worker.ExecutorRunner$$anon$1.run(ExecutorRu
>>> n
>>> n
>>> er.scala:68)
>>> 15/04/03 12:16:31.712 sparkDriver-akka.actor.default-dispatcher-3
>>> INFO
>>> AppClient$ClientActor: Executor updated: app-20150403121631-0000/0 
>>> is now RUNNING
>>> 15/04/03 12:16:31.725 sparkDriver-akka.actor.default-dispatcher-3
>>> INFO
>>> AppClient$ClientActor: Executor updated: app-20150403121631-0000/1 
>>> is now RUNNING
>>> 15/04/03 12:16:31.726 sparkWorker1-akka.actor.default-dispatcher-4 INFO Worker: Executor app-20150403121631-0000/1 finished with state FAILED message java.lang.IllegalStateException: Cannot find any assembly build directories.
>>> 15/04/03 12:16:31.689 ExecutorRunner for app-20150403121631-0000/0 
>>> ERROR ExecutorRunner: Error running executor
>>> java.lang.IllegalStateException: Cannot find any assembly build directories.
>>>
>>> Best regards, Alexander
>>
>>
>>
>> --
>> Marcelo
>
>
>
> --
> Marcelo



--
Marcelo
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 03 Apr 2015 22:59:32 +0000",Windows CI infra,Spark dev list <dev@spark.apache.org>,"I've seen other projects use Appveyor <http://www.appveyor.com/> for CI on
Windows.

Has anyone used them before?

I've seen on more than one occasion something break on Windows without us
knowing, so it might be worth looking into using something like this if
it's relatively straightforward.

Nick
"
Cheng Lian <lian.cs.zju@gmail.com>,"Sat, 04 Apr 2015 20:46:27 +0800",Re: IntelliJ Runtime error,"sara mustafa <eng.sara.mustafa@gmail.com>, dev@spark.apache.org","I found in general it's a pain to build/run Spark inside IntelliJ IDEA. 
I guess most people resort to this approach so that they can leverage 
the integrated debugger to debug and/or learn Spark internals. A more 
convenient way I'm using recently is resorting to the remote debugging 
feature. In this way, by adding driver/executor Java options, you may 
build and start the Spark applications/tests/daemons in the normal way 
and attach the debugger to it. I was using this to debug the 
HiveThriftServer2, and it worked perfectly.

Steps to enable remote debugging:

1. Menu ""Run / Edit configurations...""
2. Click the ""+"" button, choose ""Remote""
3. Choose ""Attach"" or ""Listen"" in ""Debugger mode"" according to your 
actual needs
4. Copy, edit, and add Java options suggested in the dialog to 
`--driver-java-options` or `--executor-java-options`
5. If you're using attaching mode, first start your Spark program, then 
start remote debugging in IDEA
6. If you're using listening mode, first start remote debugging in IDEA, 
and then start your Spark program.

Hope this can be helpful.

Cheng



---------------------------------------------------------------------


"
Stephen Boesch <javadba@gmail.com>,"Sat, 4 Apr 2015 11:20:55 -0700",Re: IntelliJ Runtime error,Cheng Lian <lian.cs.zju@gmail.com>,"Thanks Cheng. Yes, the problem is that the way to set up to run inside
Intellij changes v frequently.  It is unfortunately not simply a one-time
investment to get IJ debugging working properly: the steps required are a
moving target approximately monthly to bi-monthly.

Doing remote debugging is probably a good choice to reduce the dev
environment volatility/maintenance.



2015-04-04 5:46 GMT-07:00 Cheng Lian <lian.cs.zju@gmail.com>:

"
Patrick Wendell <pwendell@gmail.com>,"Sat, 4 Apr 2015 20:09:30 -0400",[VOTE] Release Apache Spark 1.3.1,"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version 1.3.1!

The tag to be voted on is v1.3.1-rc1 (commit 0dcb5d9f):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=0dcb5d9f31b713ed90bcec63ebc4e530cbb69851

The list of fixes present in this release can be found at:
http://bit.ly/1C2nVPY

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-1.3.1-rc1/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1080

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-1.3.1-rc1-docs/

Please vote on releasing this package as Apache Spark 1.3.1!

The vote is open until Wednesday, April 08, at 01:10 UTC and passes
if a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.3.1
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

- Patrick

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Sat, 4 Apr 2015 17:13:36 -0700",Re: [VOTE] Release Apache Spark 1.3.1,Patrick Wendell <pwendell@gmail.com>,"+1

Tested some DataFrame functions locally on Mac OS X.


"
Krishna Sankar <ksankar42@gmail.com>,"Sat, 4 Apr 2015 20:03:33 -0700",Re: [VOTE] Release Apache Spark 1.3.1,Reynold Xin <rxin@databricks.com>,"+1 (non-binding, of course)

1. Compiled OSX 10.10 (Yosemite) OK Total time: 15:04 min
     mvn clean package -Pyarn -Dyarn.version=2.6.0 -Phadoop-2.4
-Dhadoop.version=2.6.0 -Phive -DskipTests -Dscala-2.11
2. Tested pyspark, mlib - running as well as comp"
"""=?utf-8?B?R3VvUWlhbmcgTGk=?="" <witgo@qq.com>","Sun, 5 Apr 2015 11:35:55 +0800",Re: [VOTE] Release Apache Spark 1.3.1,"""=?utf-8?B?UGF0cmljayBXZW5kZWxs?="" <pwendell@gmail.com>","+1 (non-binding)


I have a simple test of ALS(Implicit), LR(SGD,L-BFGS) algorithm. Looks no problemâ€





------------------ Original ------------------
From:  ""Patrick Wendell"";<pwendell@gmail.com>;
Date:  Sun, Apr 5, 2015 08:09 AM
To:  ""d"
Priya Ch <learnings.chitturi@gmail.com>,"Sun, 5 Apr 2015 12:00:42 +0530",Spark streaming with Kafka- couldnt find KafkaUtils,"""user@spark.apache.org"" <user@spark.apache.org>, dev@spark.apache.org","Hi All,

  I configured Kafka  cluster on a  single node and I have streaming
application which reads data from kafka topic using KafkaUtils. When I
execute the code in local mode from the IDE, the application runs fine.

But when I submit the same to spark cluster in standalone mode, I end up
with the following exception:
java.lang.ClassNotFoundException:
org/apache/spark/streaming/kafka/KafkaUtils.

I am using spark-1.2.1 version. when i checked the source files of
streaming, the source files related to kafka are missing. Are these not
included in spark-1.3.0 and spark-1.2.1 versions ?

Have to manually include these ??

Regards,
Padma Ch
"
Akhil Das <akhil@sigmoidanalytics.com>,"Sun, 5 Apr 2015 12:21:06 +0530",Re: Spark streaming with Kafka- couldnt find KafkaUtils,Priya Ch <learnings.chitturi@gmail.com>,"How are you submitting the application? Use a standard build tool like
maven or sbt to build your project, it will download all the dependency
jars, when you submit your application (if you are using spark-submit, then
use --jars option to add those jars which are causing
classNotFoundException). If you are running as a standalone application
without using spark-submit, then while creating the SparkContext, use
sc.addJar() to add those dependency jars.

For Kafka streaming, when you use sbt, these will be jars that are required:

    sc.addJar(""/root/.ivy2/cache/org.apache.spark/spark-streaming-kafka_2.10/jars/spark-streaming-kafka_2.10-1.1.0.jar"")
   sc.addJar(""/root/.ivy2/cache/com.yammer.metrics/metrics-core/jars/metrics-core-2.2.0.jar"")
   sc.addJar(""/root/.ivy2/cache/org.apache.kafka/kafka_2.10/jars/kafka_2.10-0.8.0.jar"")
   sc.addJar(""/root/.ivy2/cache/com.101tec/zkclient/jars/zkclient-0.3.jar"")




Thanks
Best Regards


"
Sean Owen <sowen@cloudera.com>,"Sun, 5 Apr 2015 09:24:13 +0100",Re: [VOTE] Release Apache Spark 1.3.1,Patrick Wendell <pwendell@gmail.com>,"Signatures and hashes are good.
LICENSE, NOTICE still check out.
Compiles for a Hadoop 2.6 + YARN + Hive profile.

I still see the UISeleniumSuite test failure observed in 1.3.0, which
is minor and already fixed. I don't know why I didn't back-port it:
https://issues.apache.org/jira/browse/SPARK-6205

If we roll another, let's get this easy fix in, but it is only an
issue with tests.


all look legitimate (e.g. reopened or in progress)


There is 1 open Blocker for 1.3.1 per Andrew:
https://issues.apache.org/jira/browse/SPARK-6673 spark-shell.cmd can't
start even when spark was built in Windows

I believe this can be resolved quickly but as a matter of hygiene
should be fixed or demoted before release.


FYI there are 16 Critical issues marked for 1.3.0 / 1.3.1; worth
examining before release to see how critical they are:

SPARK-6701,Flaky test: o.a.s.deploy.yarn.YarnClusterSuite Python
application,,Open,4/3/15
SPARK-6484,""Ganglia metrics xml reporter doesn't escape
correctly"",Josh Rosen,Open,3/24/15
SPARK-6270,Standalone Master hangs when streaming job completes,,Open,3/11/15
SPARK-6209,ExecutorClassLoader can leak connections after failing to
load classes from the REPL class server,Josh Rosen,In Progress,4/2/15
SPARK-5113,Audit and document use of hostnames and IP addresses in
Spark,,Open,3/24/15
SPARK-5098,Number of running tasks become negative after tasks
lost,,Open,1/14/15
SPARK-4925,Publish Spark SQL hive-thriftserver maven artifact,Patrick
Wendell,Reopened,3/23/15
SPARK-4922,Support dynamic allocation for coarse-grained Mesos,,Open,3/31/15
SPARK-4888,""Spark EC2 doesn't mount local disks for i2.8xlarge
instances"",,Open,1/27/15
SPARK-4879,Missing output partitions after job completes with
speculative execution,Josh Rosen,Open,3/5/15
SPARK-4751,Support dynamic allocation for standalone mode,Andrew
Or,Open,12/22/14
SPARK-4454,Race condition in DAGScheduler,Josh Rosen,Reopened,2/18/15
SPARK-4452,Shuffle data structures can starve others on the same
thread for memory,Tianshuo Deng,Open,1/24/15
SPARK-4352,Incorporate locality preferences in dynamic allocation
requests,,Open,1/26/15
SPARK-4227,Document external shuffle service,,Open,3/23/15
SPARK-3650,Triangle Count handles reverse edges incorrectly,,Open,2/23/15


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Sun, 5 Apr 2015 14:40:05 +0100",Github auth problems => some test results not posting,dev <dev@spark.apache.org>,"I noticed recent pull request build results weren't posting results of
MiMa checks, etc.

I think it's due to Github auth issues:

Attempting to post to Github...
 > http_code: 401.
 > api_response: {
  ""message"": ""Bad credentials"",
  ""documentation_url"": ""https://developer.github.com/v3""
}

I've heard another colleague say they're having trouble with
credentials today. Anyone else?

I don't know if it's transient or what, but for today, just be aware
you'll have to look at the end of the Jenkins output to see if these
other checks passed.

---------------------------------------------------------------------


"
Josh Rosen <rosenville@gmail.com>,"Sun, 5 Apr 2015 12:02:14 -0700",Re: Github auth problems => some test results not posting,Sean Owen <sowen@cloudera.com>,"Thanks for catching this.  It looks like a recent Jenkins job configuration
change inadvertently renamed the GITHUB_OAUTH_KEY environment variable to
something else, causing this to break.  I've rolled back that change, so
hopefully the GitHub posting should start working again.

- Josh


"
Patrick Wendell <pwendell@gmail.com>,"Sun, 5 Apr 2015 19:24:50 -0400",[VOTE] Release Apache Spark 1.2.2,"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version 1.2.2!

The tag to be voted on is v1.2.2-rc1 (commit 7531b50):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=7531b50e406ee2e3301b009ceea7c684272b2e27

The list of fixes present in this release can be found at:
http://bit.ly/1DCNddt

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-1.2.2-rc1/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1082/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-1.2.2-rc1-docs/

Please vote on releasing this package as Apache Spark 1.2.2!

The vote is open until Thursday, April 08, at 00:30 UTC and passes
if a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.2.2
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

---------------------------------------------------------------------


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Sun, 5 Apr 2015 19:13:13 -0700",Re: Stochastic gradient descent performance,"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Yeah, a simple way to estimate the time for an iterative algorithms is
number of iterations required * time per iteration. The time per iteration
will depend on the batch size, computation required and the fixed overheads
I mentioned before. The number of iterations of course depends on the
convergence rate for the problem being solved.

Thanks
Shivaram


on
all
e remain
o
on
s
a
t
r
ed
to
e
of
ch
a
"
Reynold Xin <rxin@databricks.com>,"Sun, 5 Apr 2015 20:27:57 -0700",Re: Wrong initial bias in GraphX SVDPlusPlus?,Michael Malak <michaelmalak@yahoo.com>,"Adding Jianping Wang to the thread, since he contributed the SVDPlusPlus
implementaiton.

Jianping,

Can you take a look at this message? Thanks.



"
Xiangrui Meng <mengxr@gmail.com>,"Sun, 5 Apr 2015 21:14:05 -0700",Re: [VOTE] Release Apache Spark 1.3.1,Sean Owen <sowen@cloudera.com>,"+1 Verified some MLlib bug fixes on OS X. -Xiangrui


---------------------------------------------------------------------


"
Denny Lee <denny.g.lee@gmail.com>,"Mon, 06 Apr 2015 05:40:43 +0000",Re: [VOTE] Release Apache Spark 1.3.1,"Xiangrui Meng <mengxr@gmail.com>, Sean Owen <sowen@cloudera.com>","+1 (non-binding)  Verified various DataFrame functions, Hive integration,
MLlib, etc. on OSX.


"
Chunnan Yao <yaochunnan@gmail.com>,"Sun, 5 Apr 2015 23:48:33 -0700 (MST)","Support parallelized online matrix factorization for Collaborative
 Filtering",dev@spark.apache.org,"re-train a CF model from scratch every time when new data comes in is very
inefficient
(http://stackoverflow.com/questions/27734329/apache-spark-incremental-training-of-als-model).
However, in Spark community we see few discussion about collaborative
filtering on streaming data. Given streaming k-means, streaming logistic
regression, and the on-going incremental model training of Naive Bayes
Classifier (SPARK-4144), we think it is meaningful to consider streaming
Collaborative Filtering support on MLlib.

I've created an issue on JIRA (SPARK-6711) for possible discussions. We
suggest to refer to this paper
(https://www.cs.utexas.edu/~cjohnson/ParallelCollabFilt.pdf). It is based on
SGD instead of ALS, which is easier to be tackled under streaming data.

Fortunately, the authors of this paper have implemented their algorithm as a
Github Project, based on Storm:
https://github.com/MrChrisJohnson/CollabStream

Please don't hesitate to give your opinions on this issue and our planned
approach. We'd like to work on this in the next few weeks. 



-----
Feel the sparking Spark!
--

---------------------------------------------------------------------


"
=?UTF-8?Q?Grega_Ke=C5=A1pret?= <grega@celtra.com>,"Mon, 6 Apr 2015 09:50:24 +0200","Approximate rank-based statistics (median, 95-th percentile, etc.)
 for Spark","""dev@spark.apache.org"" <dev@spark.apache.org>","Hi!

I'd like to get community's opinion on implementing a generic quantile
approximation algorithm for Spark that is O(n) and requires limited memory.
I would find it useful and I haven't found any existing implementation. The
plan was basically to wrap t-digest <https://github.com/tdunning/t-digest>,
implement the serialization/deserialization boilerplate and provide

def cdf(x: Double): Double
def quantile(q: Double): Double


on RDD[Double] and RDD[(K, Double)].

Let me know what you think. Any other ideas/suggestions also welcome!

Best,
Grega
--
[image: Inline image 1]*Grega KeÅ¡pret*
Senior Software Engineer, Analytics

Skype: gregakespret
celtra.com <http://www.celtra.com/> | @celtramobile
<http://www.twitter.com/celtramobile>
"
Reynold Xin <rxin@databricks.com>,"Mon, 6 Apr 2015 00:59:47 -0700","Re: Approximate rank-based statistics (median, 95-th percentile,
 etc.) for Spark",=?UTF-8?Q?Grega_Ke=C5=A1pret?= <grega@celtra.com>,"I think those are great to have. I would put them in the DataFrame API
though, since this is applying to structured data. Many of the advanced
functions on the PairRDDFunctions should really go into the DataFrame API
now we have it.

alternatives are out there. I did a quick google scholar search using the
keyword ""approximate quantile"" and found some older papers. Just the first
few I found:

http://www.softnet.tuc.gr/~minos/Papers/sigmod05.pdf  by bell labs

http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.6.6513&rep=rep1&type=pdf
 by Bruce Lindsay, IBM

http://infolab.stanford.edu/~datar/courses/cs361a/papers/quantiles.pdf





e:

y.
he
"
Sean Owen <sowen@cloudera.com>,"Mon, 6 Apr 2015 09:23:29 +0100",Re: Wrong initial bias in GraphX SVDPlusPlus?,Reynold Xin <rxin@databricks.com>,"See now: https://issues.apache.org/jira/browse/SPARK-6710


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Mon, 6 Apr 2015 13:21:22 +0100",Re: [VOTE] Release Apache Spark 1.3.1,Patrick Wendell <pwendell@gmail.com>,"SPARK-6673 is not, in the end, relevant for 1.3.x I believe; we just
resolved it for 1.4 anyway. False alarm there.

I back-ported SPARK-6205 into the 1.3 branch for next time. We'll pick
it up if there's another RC, but by itself is not something that needs
a new RC. (I will give the same treatment to branch 1.2 if needed in
light of the 1.2.2 release.)

I applied the simple change in SPARK-6205 in order to continue
executing tests and all was well. I still see a few failures in Hive
tests:

- show_create_table_serde *** FAILED ***
- show_tblproperties *** FAILED ***
- udf_std *** FAILED ***
- udf_stddev *** FAILED ***

with ...

mvn -Phadoop-2.4 -Pyarn -Phive -Phive-0.13.1 -Dhadoop.version=2.6.0
-DskipTests clean package; mvn -Phadoop-2.4 -Pyarn -Phive
-Phive-0.13.1 -Dhadoop.version=2.6.0 test

... but these are not regressions from 1.3.0.

+1 from me at this point on the current artifacts.


---------------------------------------------------------------------


"
Vadim Bichutskiy <vadim.bichutskiy@gmail.com>,"Mon, 6 Apr 2015 12:23:41 -0400",Re: Spark + Kinesis,Tathagata Das <tdas@databricks.com>,"Hi all,

I am wondering, has anyone on this list been able to successfully implement
Spark on top of Kinesis?

Best,
Vadim
á§

m

ream-Feeding-and-Eating-Amazon-Kinesis-Streams-with-Python
,
.
t
t
ge:
ven
he
s()
his
h
g and
 of
al
y.
e
l
5XZs653q_MN8rBNbzRbv22W8r4TLx56dCDWf13Gc8R02?t=http%3A%2F%2Fspark.apache.org%2Fdocs%2Flatest%2Fstreaming-kinesis-integration.html&si=5533377798602752&pi=8898f7f0-ede6-4e6c-9bfd-00cf2101f0e9>
in/scala/org/apache/spark/examples/streaming/KinesisWordCountASL.scala
5XZs653q_MN8rBNbzRbv22W8r4TLx56dCDWf13Gc8R02?t=https%3A%2F%2Fgithub.com%2Fapache%2Fspark%2Fblob%2Fmaster%2Fextras%2Fkinesis-asl%2Fsrc%2Fmain%2Fscala%2Forg%2Fapache%2Fspark%2Fexamples%2Fstreaming%2FKinesisWordCountASL.scala&si=5533377798602752&pi=8898f7f0-ede6-4e6c-9bfd-00cf2101f0e9>
""
5, you
n
er the
e just
gns
0""
ad:
ther
:=
ore_2.10""
%
.3.0"")*
"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Mon, 6 Apr 2015 17:38:08 +0000",RE: Stochastic gradient descent performance,"""shivaram@eecs.berkeley.edu"" <shivaram@eecs.berkeley.edu>","Batch size impacts convergence, so bigger batch means more iterations. There are some approaches to deal with it (such as http://www.cs.cmu.edu/~muli/file/minibatch_sgd.pdf), but they need to be implemented and tested.

Nonetheless, could you share your thoughts regarding reducing this overhead in Spark (or probably a workaround)? Sorry for repeating it, but I think this is crucial for MLlib in Spark, because Spark is intended for bigger amounts of data. Machine learning with bigger data usually requires SGD (vs batch GD), SGD requires a lot of updates, and â€œSpark overheadâ€ times â€œmany updatesâ€ equals impractical time needed for learning.


From: Shivaram Venkataraman [mailto:shivaram@eecs.berkeley.edu]
Sent: Sunday, April 05, 2015 7:13 PM
To: Ulanov, Alexander
Cc: shivaram@eecs.berkeley.edu; Joseph Bradley; dev@spark.apache.org
Subject: Re: Stochastic gradient descent performance

Yeah, a simple way to estimate the time for an iterative algorithms is number of iterations required * time per iteration. The time per iteration will depend on the batch size, computation required and the fixed overheads I mentioned before. The number of iterations of course depends on the convergence rate for the problem being solved.

Thanks
Shivaram

On Thu, Apr 2, 2015 at 2:19 PM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
Hi Shivaram,

It sounds really interesting! With this time we can estimate if it worth considering to run an iterative algorithm on Spark. For example, for SGD on Imagenet (450K samples) we will spend 450K*50ms=62.5 hours to traverse all data by one example not considering the data loading, computation and update times. One may need to traverse all data a number of times to converge. Letâ€™s say this number is equal to the batch size. So, we remain with 62.5 hours overhead. Is it reasonable?

Best regards, Alexander

From: Shivaram Vearam@eecs.berkeley.edu>]
Sent: Thursday, April 02, 2015 1:26 PM
To: Joseph Bradley
Cc: Ulanov, Alexander; dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: Stochastic gradient descent performance

I haven't looked closely at the sampling issues, but regarding the aggregation latency, there are fixed overheads (in local and distributed mode) with the way aggregation is done in Spark. Launching a stage of tasks, fetching outputs from the previous stage etc. all have overhead, so I would say its not efficient / recommended to run stages where computation is less than 500ms or so. You could increase your batch size based on this and hopefully that will help.

Regarding reducing these overheads by an order of magnitude it is a challenging problem given the architecture in Spark -- I have some ideas for this, but they are very much at a research stage.

Thanks
Shivaram

On Thu, Apr 2, 2015 at 12:00 PM, Joseph Bradley <joseph@databricks.com<mailto:joseph@databricks.com>> wrote:
When you say ""It seems that instead of sample it is better to shuffle data
and then access it sequentially by mini-batches,"" are you sure that holds
true for a big dataset in a cluster?  As far as implementing it, I haven't
looked carefully at GapSamplingIterator (in RandomSampler.scala) myself,
but that looks like it could be modified to be deterministic.

Hopefully someone else can comment on aggregation in local mode.  I'm not
sure how much effort has gone into optimizing for local mode.

Joseph

On Thu, Apr 2, 2015 at 11:33 AM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>
wrote:

>  Hi Joseph,
>
>
>
> Thank you for suggestion!
>
> It seems that instead of sample it is better to shuffle data and then
> access it sequentially by mini-batches. Could you suggest how to implement
> it?
>
>
>
> With regards to aggregate (reduce), I am wondering why it works so slow in
> local mode? Could you elaborate on this? I do understand that in cluster
> mode the network speed will kick in and then one can blame it.
>
>
>
> Best regards, Alexander
>
>
>
> *From:* Joseph Bradley [mailto:joseph@databricks.com<mailto:joseph@databricks.com>]
> *Sent:* Thursday, April 02, 2015 10:51 AM
> *To:* Ulanov, Alexander
> *Cc:* dev@spark.apache.org<mailto:dev@spark.apache.org>
> *Subject:* Re: Stochastic gradient descent performance
>
>
>
> It looks like SPARK-3250 was applied to the sample() which GradientDescent
> uses, and that should kick in for your minibatchFraction <= 0.4.  Based on
> your numbers, aggregation seems like the main issue, though I hesitate to
> optimize aggregation based on local tests for data sizes that small.
>
>
>
> The first thing I'd check for is unnecessary object creation, and to
> profile in a cluster or larger data setting.
>
>
>
> On Wed, Apr 1, 2015 at 10:09 AM, Ulanov, Alexander <
> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
>
> Sorry for bothering you again, but I think that it is an important issue
> for applicability of SGD in Spark MLlib. Could Spark developers please
> comment on it.
>
>
> -----Original Message-----
> From: Ulanov, Alexander
> Sent: Monday, March 30, 2015 5:00 PM
> To: dev@spark.apache.org<mailto:dev@spark.apache.org>
> Subject: Stochastic gradient descent performance
>
> Hi,
>
> It seems to me that there is an overhead in ""runMiniBatchSGD"" function of
> MLlib's ""GradientDescent"". In particular, ""sample"" and ""treeAggregate""
> might take time that is order of magnitude greater than the actual gradient
> computation. In particular, for mnist dataset of 60K instances, minibatch
> size = 0.001 (i.e. 60 samples) it take 0.15 s to sample and 0.3 to
> aggregate in local mode with 1 data partition on Core i5 processor. The
> actual gradient computation takes 0.002 s. I searched through Spark Jira
> and found that there was recently an update for more efficient sampling
> (SPARK-3250) that is already included in Spark codebase. Is there a way to
> reduce the sampling time and local treeRedeuce by order of magnitude?
>
> Best regards, Alexander
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org>
> For additional commands, e-mail: dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>
>
>
>


"
"""York, Brennon"" <Brennon.York@capitalone.com>","Mon, 6 Apr 2015 14:03:24 -0400",Re: [VOTE] Release Apache Spark 1.3.1,"Sean Owen <sowen@cloudera.com>, Patrick Wendell <pwendell@gmail.com>","+1 (non-binding)

Tested GraphX, build infrastructure, & core test suite on OSX 10.9 w/ Java
1.7/1.8



b5d9f3

________________________________________________________

The information contained in this e-mail is confidential and/or proprietary is intend"
"""Hari Shreedharan"" <hshreedharan@cloudera.com>","Mon, 06 Apr 2015 11:10:56 -0700 (PDT)",Re: [VOTE] Release Apache Spark 1.3.1,"""YorkBrennon"" <brennon.york@capitalone.com>","It does not look like https://issues.apache.org/jira/browse/SPARK-6222 made it. It was targeted towards this release.Â 




Thanks,Â Hari


Java
git;a=commit;h=0dcb5d9f3
at:

________________________________
transmitted herewith is intended only for use by the individual or entity to which it is addressed.  If the reader of this message is not the intended recipient, you are hereby notified that any review, retransmission, dissemination, distribution, copying or other use of, or taking of any action in reliance upon this information is strictly prohibited. If you have received this communication in error, please contact the sender and delete the material from your computer.
org"
Mark Hamstra <mark@clearstorydata.com>,"Mon, 6 Apr 2015 11:31:32 -0700",Re: [VOTE] Release Apache Spark 1.3.1,Hari Shreedharan <hshreedharan@cloudera.com>,"Is that correct, or is the JIRA just out of sync, since TD's PR was merged?
https://github.com/apache/spark/pull/5008


"
Patrick Wendell <pwendell@gmail.com>,"Mon, 6 Apr 2015 14:36:28 -0400",Re: [VOTE] Release Apache Spark 1.3.1,"Mark Hamstra <mark@clearstorydata.com>, Tathagata Das <tathagata.das1565@gmail.com>","I believe TD just forgot to set the fix version on the JIRA. There is
a fix for this in 1.3:

https://github.com/apache/spark/commit/03e263f5b527cf574f4ffcd5cd886f7723e3756e

- Patrick


---------------------------------------------------------------------


"
"""Hari Shreedharan"" <hshreedharan@cloudera.com>","Mon, 06 Apr 2015 11:43:09 -0700 (PDT)",Re: [VOTE] Release Apache Spark 1.3.1,"""Patrick Wendell"" <pwendell@gmail.com>","Ah, ok. It was missing in the list of jiras. So +1.




Thanks,Â Hari


e3756e
merged?
w/
pick
needs
0
which
it:
and
can't
to
4/2/15
Patrick
2/18/15
com>
git;a=commit;h=0dcb5d9f3
found
1080
passes
---

_________________________________
entity to
intended
any
you have
delete
"
Mark Hamstra <mark@clearstorydata.com>,"Mon, 6 Apr 2015 11:51:52 -0700",Re: [VOTE] Release Apache Spark 1.2.2,Patrick Wendell <pwendell@gmail.com>,1
Mark Hamstra <mark@clearstorydata.com>,"Mon, 6 Apr 2015 11:51:19 -0700",Re: [VOTE] Release Apache Spark 1.3.1,Patrick Wendell <pwendell@gmail.com>,1
Reynold Xin <rxin@databricks.com>,"Mon, 6 Apr 2015 11:52:41 -0700",Re: [VOTE] Release Apache Spark 1.2.2,Patrick Wendell <pwendell@gmail.com>,"+1 too


"
Krishna Sankar <ksankar42@gmail.com>,"Mon, 6 Apr 2015 12:20:06 -0700",Re: [VOTE] Release Apache Spark 1.2.2,Patrick Wendell <pwendell@gmail.com>,1
Sean McNamara <Sean.McNamara@Webtrends.com>,"Mon, 6 Apr 2015 19:30:13 +0000",Re: [VOTE] Release Apache Spark 1.3.1,Patrick Wendell <pwendell@gmail.com>,"+1

1.3.1!
5d9f31b713ed90bcec63ebc4e530cbb69851


---------------------------------------------------------------------


"
Tathagata Das <tdas@databricks.com>,"Mon, 6 Apr 2015 12:39:39 -0700",Re: Spark + Kinesis,"Vadim Bichutskiy <vadim.bichutskiy@gmail.com>, Chris Fregly <cfregly@databricks.com>","Cc'ing Chris Fregly, who wrote the Kinesis integration. Maybe he can help.

m

d
tream-Feeding-and-Eating-Amazon-Kinesis-Streams-with-Python
b
st
st
=>*
s
age:
e>
iven
the
ds()
this
ch
ig and
r of
val
cy.
he
ml
t5XZs653q_MN8rBNbzRbv22W8r4TLx56dCDWf13Gc8R02?t=http%3A%2F%2Fspark.apache.org%2Fdocs%2Flatest%2Fstreaming-kinesis-integration.html&si=5533377798602752&pi=8898f7f0-ede6-4e6c-9bfd-00cf2101f0e9>
ain/scala/org/apache/spark/examples/streaming/KinesisWordCountASL.scala
t5XZs653q_MN8rBNbzRbv22W8r4TLx56dCDWf13Gc8R02?t=https%3A%2F%2Fgithub.com%2Fapache%2Fspark%2Fblob%2Fmaster%2Fextras%2Fkinesis-asl%2Fsrc%2Fmain%2Fscala%2Forg%2Fapache%2Fspark%2Fexamples%2Fstreaming%2FKinesisWordCountASL.scala&si=5533377798602752&pi=8898f7f0-ede6-4e6c-9bfd-00cf2101f0e9>
:
0""
.5, you
an
ter the
se just
igns
ead:
other
 :=
core_2.10""
 %
1.3.0"")*
""
"
Xiangrui Meng <mengxr@gmail.com>,"Mon, 6 Apr 2015 12:42:42 -0700",Re: Stochastic gradient descent performance,"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","The gap sampling is triggered when the sampling probability is small
and the directly underlying storage has constant time lookups, in
particular, ArrayBuffer. This is a very strict requirement. If rdd is
cached in memory, we use ArrayBuffer to store its elements and
rdd.sample will trigger gap sampling. However, if we call rdd2 =
rdd.map(x => x), we can no longer tell whether the storage is backed
by an ArrayBuffer and hence gaps sampling is not enabled. We should
use Scala's drop(k) and let Scala decides whether this is an O(1)
operation or an O(k) operation. But unfortunately, due to a Scala bug,
this could become an O(k^2) operation. So we didn't use this approach.
Please check the comments in the gap sampling PR.

For SGD, I think we should either assume the input data is randomized
or randomize the input data (and eat this one-time cost), then do
min-batch sequentially. The key is the balance the batch size and the
communication cost of model update.

Best,
Xiangrui

ere are some approaches to deal with it (such as http://www.cs.cmu.edu/~muli/file/minibatch_sgd.pdf), but they need to be implemented and tested.
ad in Spark (or probably a workaround)? Sorry for repeating it, but I think this is crucial for MLlib in Spark, because Spark is intended for bigger amounts of data. Machine learning with bigger data usually requires SGD (vs batch GD), SGD requires a lot of updates, and â€œSpark overheadâ€ times â€œmany updatesâ€ equals impractical time needed for learning.
mber of iterations required * time per iteration. The time per iteration will depend on the batch size, computation required and the fixed overheads I mentioned before. The number of iterations of course depends on the convergence rate for the problem being solved.
considering to run an iterative algorithm on Spark. For example, for SGD on Imagenet (450K samples) we will spend 450K*50ms=62.5 hours to traverse all data by one example not considering the data loading, computation and up. Letâ€™s say this number is equal to the batch size. So, we remain with 62.5 hours overhead. Is it reasonable?
varam@eecs.berkeley.edu>]
ation latency, there are fixed overheads (in local and distributed mode) with the way aggregation is done in Spark. Launching a stage of tasks, fetching outputs from the previous stage etc. all have overhead, so I would say its not efficient / recommended to run stages where computation is less than 500ms or so. You could increase your batch size based on this and hopefully that will help.
enging problem given the architecture in Spark -- I have some ideas for this, but they are very much at a research stage.
a
t
om<mailto:alexander.ulanov@hp.com>>
nt
in
ricks.com>]
nt
d on
o
f
ent
h
to
bscribe@spark.apache.org>
lp@spark.apache.org>

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Mon, 6 Apr 2015 12:44:22 -0700",Re: Stochastic gradient descent performance,Xiangrui Meng <mengxr@gmail.com>,"Note that we can do this in DataFrames and use Catalyst to push Sample down
beneath Projection :)


 I
s
headâ€
earning.
n
ds
h
on
all
e remain
o
on
s
ds
,
ot
w
er
sed
ue
e
ra
g
y
"
Xiangrui Meng <mengxr@gmail.com>,"Mon, 6 Apr 2015 12:47:56 -0700","Re: Support parallelized online matrix factorization for
 Collaborative Filtering",Chunnan Yao <yaochunnan@gmail.com>,"This is being discussed in
https://issues.apache.org/jira/browse/SPARK-6407. Let's move the
discussion there. Thanks for providing references! -Xiangrui


---------------------------------------------------------------------


"
Dean Chen <dean@ocirs.com>,"Mon, 6 Apr 2015 14:38:14 -0700",Re: Experience using binary packages on various Hadoop distros,Marcelo Vanzin <vanzin@cloudera.com>,"This would be great for those of us running on HDP. At eBay we recently ran
in to few problems using the generic Hadoop lib. Two off of the top of my
head:

* Needed to included our custom Hadoop client due to custom keberos
integration
* Minor difference in HDFS protocol causing the following error. Was fixed
by removing the HDFS jar from the Spark assembly.

Exception in thread ""Driver"" java.lang.IllegalStateException
        at
org.spark-project.guava.common.base.Preconditions.checkState(Preconditions.java:133)
        at
org.apache.hadoop.hdfs.protocolPB.PBHelper.convert(PBHelper.java:673)
        at
org.apache.hadoop.hdfs.protocolPB.PBHelper.convertLocatedBlock(PBHelper.java:1100)
        at
org.apache.hadoop.hdfs.protocolPB.PBHelper.convert(PBHelper.java:1118)
        at
org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:221)
        at
org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1142)
        at
org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1132)
        at
org.apache.hadoop.hdfs.DFSClient.getBlockLocations(DFSClient.java:1182)
        at
org.apache.hadoop.hdfs.DistributedFileSystem$1.doCall(DistributedFileSystem.java:218)
        at
org.apache.hadoop.hdfs.DistributedFileSystem$1.doCall(DistributedFileSystem.java:214)
        at
org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
        at
org.apache.hadoop.hdfs.DistributedFileSystem.getFileBlockLocations(DistributedFileSystem.java:214)
        at
org.apache.hadoop.hdfs.DistributedFileSystem.getFileBlockLocations(DistributedFileSystem.java:206)
        at
org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)
        at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:201)
        at
org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:205)
        at
org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:203)
        at scala.Option.getOrElse(Option.scala:120)
        at org.apache.spark.rdd.RDD.partitions(RDD.scala:203)
        at org.apache.spark.rdd.MappedRDD.getPartitions(MappedRDD.scala:28)
        at
org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:205)
        at
org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:203)
        at scala.Option.getOrElse(Option.scala:120)
        at org.apache.spark.rdd.RDD.partitions(RDD.scala:203)
        at org.apache.spark.rdd.RDD.take(RDD.scala:1060)
        at org.apache.spark.rdd.RDD.first(RDD.scala:1093)
        at
com.ebay.ss.niffler.miner.acronym.AcronymMinerSparkLaunchOld$.test10(AcronymMiner.scala:664)
        at
com.ebay.ss.niffler.miner.acronym.AcronymMinerSparkLaunchOld$.main(AcronymMiner.scala:611)
        at
com.ebay.ss.niffler.miner.acronym.AcronymMinerSparkLaunchOld.main(AcronymMiner.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at
org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:427)


--
Dean Chen


"
mjhb <spark@mjhb.com>,"Mon, 6 Apr 2015 17:40:49 -0700 (MST)",Zinc now required?,dev@spark.apache.org,"Today I cannot build the 1.2 branch:

[INFO]
------------------------------------------------------------------------
[INFO] Building Spark Project Networking 1.2.3-SNAPSHOT
[INFO]
------------------------------------------------------------------------

  <snipped>

[INFO] --- scala-maven-plugin:3.2.0:compile (scala-compile-first) @
spark-network-common_2.10 ---
[INFO] Using zinc server for incremental compilation
[INFO] compiler plugin:
BasicArtifact(org.scalamacros,paradise_2.10.4,2.0.1,null)
[error] Required file not found: sbt-interface.jar
[error] See zinc -help for information about locating necessary files

I don't have zinc installed, and have built successfully before.



--

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Mon, 6 Apr 2015 21:02:21 -0400",Re: Zinc now required?,mjhb <spark@mjhb.com>,"I don't think it's required. This looks like zinc is running (it seems
to find the process on port 3030), but, something is wrong with zinc
then. If you aren't running your own zinc, then it's the copy
downloaded by Spark. Maybe try deleting that and shutting down the
zinc process, and trying a clean build?


---------------------------------------------------------------------


"
Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"Mon, 6 Apr 2015 18:46:59 -0700 (MST)","[mllib] Deprecate static train and use builder instead for
 Scala/Java",dev@spark.apache.org,"Hi all, 

Joseph proposed an idea about using just builder methods, instead of static
train() 
methods for Scala/Java. I agree with that idea. Because we have many
duplicated 
static train() method. If you have any thoughts on that please share it with
us.

[SPARK-6682] Deprecate static train and use builder instead for Scala/Java
https://issues.apache.org/jira/browse/SPARK-6682

Thanks
Yu Ishikawa




-----
-- Yu Ishikawa
--

---------------------------------------------------------------------


"
mjhb <spark@mjhb.com>,"Mon, 6 Apr 2015 18:54:03 -0700 (MST)",Re: Zinc now required?,dev@spark.apache.org,"Killing zinc resolved the problem building with scala-2.10 - thank you.

(adding that to my build script)

Having problems building with scala-2.11 - will post separately for that if
reproducible.



--

---------------------------------------------------------------------


"
Marcelo Vanzin <vanzin@cloudera.com>,"Mon, 6 Apr 2015 18:54:33 -0700",Re: Zinc now required?,mjhb <spark@mjhb.com>,"I ran into this recently and I had to upgrade my version of zinc to fix it...




-- 
Marcelo

---------------------------------------------------------------------


"
mjhb <spark@mjhb.com>,"Mon, 6 Apr 2015 19:20:15 -0700 (MST)",1.3 Build Error with Scala-2.11,dev@spark.apache.org,"$dev/change-version-to-2.11.sh
$build/mvn -e -DskipTests clean package

[ERROR] Failed to execute goal on project spark-core_2.11: Could not resolve
dependencies for project
org.apache.spark:spark-core_2.11:jar:1.3.2-SNAPSHOT: The following artifacts
could not be resolved:
org.apache.spark:spark-network-common_2.10:jar:1.3.2-SNAPSHOT,
org.apache.spark:spark-network-shuffle_2.10:jar:1.3.2-SNAPSHOT: Failure to
find org.apache.spark:spark-network-common_2.10:jar:1.3.2-SNAPSHOT in
http://repository.apache.org/snapshots was cached in the local repository,
resolution will not be reattempted until the update interval of
apache.snapshots has elapsed or updates are forced -> [Help 1]
org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute
goal on project spark-core_2.11: Could not resolve dependencies for project
org.apache.spark:spark-core_2.11:jar:1.3.2-SNAPSHOT: The following artifacts
could not be resolved:
org.apache.spark:spark-network-common_2.10:jar:1.3.2-SNAPSHOT,
org.apache.spark:spark-network-shuffle_2.10:jar:1.3.2-SNAPSHOT: Failure to
find org.apache.spark:spark-network-common_2.10:jar:1.3.2-SNAPSHOT in
http://repository.apache.org/snapshots was cached in the local repository,
resolution will not be reattempted until the update interval of
apache.snapshots has elapsed or updates are forced
	at
org.apache.maven.lifecycle.internal.LifecycleDependencyResolver.getDependencies(LifecycleDependencyResolver.java:210)
	at
org.apache.maven.lifecycle.internal.LifecycleDependencyResolver.resolveProjectDependencies(LifecycleDependencyResolver.java:117)
	at
org.apache.maven.lifecycle.internal.MojoExecutor.ensureDependenciesAreResolved(MojoExecutor.java:258)
	at
org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:201)
	at
org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at
org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at
org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:84)
	at
org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:59)
	at
org.apache.maven.lifecycle.internal.LifecycleStarter.singleThreadedBuild(LifecycleStarter.java:183)
	at
org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:161)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:320)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:156)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:537)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:196)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:141)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at
org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at
org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at
org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at
org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
Caused by: org.apache.maven.project.DependencyResolutionException: Could not
resolve dependencies for project
org.apache.spark:spark-core_2.11:jar:1.3.2-SNAPSHOT: The following artifacts
could not be resolved:
org.apache.spark:spark-network-common_2.10:jar:1.3.2-SNAPSHOT,
org.apache.spark:spark-network-shuffle_2.10:jar:1.3.2-SNAPSHOT: Failure to
find org.apache.spark:spark-network-common_2.10:jar:1.3.2-SNAPSHOT in
http://repository.apache.org/snapshots was cached in the local repository,
resolution will not be reattempted until the update interval of
apache.snapshots has elapsed or updates are forced
	at
org.apache.maven.project.DefaultProjectDependenciesResolver.resolve(DefaultProjectDependenciesResolver.java:189)
	at
org.apache.maven.lifecycle.internal.LifecycleDependencyResolver.getDependencies(LifecycleDependencyResolver.java:185)
	... 22 more
Caused by: org.sonatype.aether.resolution.DependencyResolutionException: The
following artifacts could not be resolved:
org.apache.spark:spark-network-common_2.10:jar:1.3.2-SNAPSHOT,
org.apache.spark:spark-network-shuffle_2.10:jar:1.3.2-SNAPSHOT: Failure to
find org.apache.spark:spark-network-common_2.10:jar:1.3.2-SNAPSHOT in
http://repository.apache.org/snapshots was cached in the local repository,
resolution will not be reattempted until the update interval of
apache.snapshots has elapsed or updates are forced
	at
org.sonatype.aether.impl.internal.DefaultRepositorySystem.resolveDependencies(DefaultRepositorySystem.java:375)
	at
org.apache.maven.project.DefaultProjectDependenciesResolver.resolve(DefaultProjectDependenciesResolver.java:183)
	... 23 more
Caused by: org.sonatype.aether.resolution.ArtifactResolutionException: The
following artifacts could not be resolved:
org.apache.spark:spark-network-common_2.10:jar:1.3.2-SNAPSHOT,
org.apache.spark:spark-network-shuffle_2.10:jar:1.3.2-SNAPSHOT: Failure to
find org.apache.spark:spark-network-common_2.10:jar:1.3.2-SNAPSHOT in
http://repository.apache.org/snapshots was cached in the local repository,
resolution will not be reattempted until the update interval of
apache.snapshots has elapsed or updates are forced
	at
org.sonatype.aether.impl.internal.DefaultArtifactResolver.resolve(DefaultArtifactResolver.java:538)
	at
org.sonatype.aether.impl.internal.DefaultArtifactResolver.resolveArtifacts(DefaultArtifactResolver.java:216)
	at
org.sonatype.aether.impl.internal.DefaultRepositorySystem.resolveDependencies(DefaultRepositorySystem.java:358)
	... 24 more
Caused by: org.sonatype.aether.transfer.ArtifactNotFoundException: Failure
to find org.apache.spark:spark-network-common_2.10:jar:1.3.2-SNAPSHOT in
http://repository.apache.org/snapshots was cached in the local repository,
resolution will not be reattempted until the update interval of
apache.snapshots has elapsed or updates are forced
	at
org.sonatype.aether.impl.internal.DefaultUpdateCheckManager.newException(DefaultUpdateCheckManager.java:230)
	at
org.sonatype.aether.impl.internal.DefaultUpdateCheckManager.checkArtifact(DefaultUpdateCheckManager.java:204)
	at
org.sonatype.aether.impl.internal.DefaultArtifactResolver.resolve(DefaultArtifactResolver.java:427)
	... 26 more




--

---------------------------------------------------------------------


"
mjhb <spark@mjhb.com>,"Mon, 6 Apr 2015 19:26:50 -0700 (MST)",Re: 1.3 Build Error with Scala-2.11,dev@spark.apache.org,"Similar problem on 1.2 branch:

[ERROR] Failed to execute goal on project spark-core_2.11: Could not resolve
dependencies for project
org.apache.spark:spark-core_2.11:jar:1.2.3-SNAPSHOT: The following artifacts
could not be resolved:
org.apache.spark:spark-network-common_2.10:jar:1.2.3-SNAPSHOT,
org.apache.spark:spark-network-shuffle_2.10:jar:1.2.3-SNAPSHOT: Failure to
find org.apache.spark:spark-network-common_2.10:jar:1.2.3-SNAPSHOT in
http://repository.apache.org/snapshots was cached in the local repository,
resolution will not be reattempted until the update interval of
apache.snapshots has elapsed or updates are forced -> [Help 1]
org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute
goal on project spark-core_2.11: Could not resolve dependencies for project
org.apache.spark:spark-core_2.11:jar:1.2.3-SNAPSHOT: The following artifacts
could not be resolved:
org.apache.spark:spark-network-common_2.10:jar:1.2.3-SNAPSHOT,
org.apache.spark:spark-network-shuffle_2.10:jar:1.2.3-SNAPSHOT: Failure to
find org.apache.spark:spark-network-common_2.10:jar:1.2.3-SNAPSHOT in
http://repository.apache.org/snapshots was cached in the local repository,
resolution will not be reattempted until the update interval of
apache.snapshots has elapsed or updates are forced




--

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Mon, 6 Apr 2015 22:31:16 -0400",Re: 1.3 Build Error with Scala-2.11,mjhb <spark@mjhb.com>,"What if you don't run zinc? I.e. just download maven and run that ""mvn
package..."". It might take longer, but I wonder if it will work.


---------------------------------------------------------------------


"
Marty Bower <spark@mjhb.com>,"Tue, 07 Apr 2015 02:43:17 +0000",Re: 1.3 Build Error with Scala-2.11,"Patrick Wendell <pwendell@gmail.com>, dev@spark.apache.org","I'm killing zinc (if it's running) before running each build attempt.

Trying to build as ""clean"" as possible.



"
Patrick Wendell <pwendell@gmail.com>,"Mon, 6 Apr 2015 22:54:06 -0400",Re: 1.3 Build Error with Scala-2.11,Marty Bower <spark@mjhb.com>,"The issue is that if you invoke ""build/mvn"" it will start zinc again
if it sees that it is killed.

The absolute most ""sterile"" thing to do is this:
1. Kill any zinc processes.
2. Clean up spark ""git clean -fdx"" (WARNING: this will delete any
staged changes you have, if you have code modifications or extra files
around)
3. Run the 2.11 script to change the versions.
4. Run ""mvn package"" with maven that you installed on your machine.



---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Mon, 6 Apr 2015 22:55:19 -0400",Re: 1.3 Build Error with Scala-2.11,Marty Bower <spark@mjhb.com>,"Scala 2.10, then try to run it with 2.11, since I think we may store
some downloaded jars relating to zinc that will get screwed up. Not
sure that's what is happening, just an idea.


---------------------------------------------------------------------


"
mjhb <spark@mjhb.com>,"Mon, 6 Apr 2015 20:02:21 -0700 (MST)",Re: 1.3 Build Error with Scala-2.11,dev@spark.apache.org,"I resorted to deleting the spark directory between each build earlier today
(attempting maximum sterility) and then re-cloning from github and switching
to the 1.2 or 1.3 branch.

Does anything persist outside of the spark directory?

Are you able to build either 1.2 or 1.3 w/ Scala-2.11?



--

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Tue, 7 Apr 2015 00:46:36 -0400",Re: 1.3 Build Error with Scala-2.11,mjhb <spark@mjhb.com>,"The only think that can persist outside of Spark is if there is still
a live Zinc process. We took care to make sure this was a generally
stateless mechanism.

Both the 1.2.X and 1.3.X releases are built with Scala 2.11 for
packaging purposes. And these have been built as recently as in the
last few days, since we are voting on 1.2.2 and 1.3.1. However there
could be issues that only affect certain environments.

- Patrick


---------------------------------------------------------------------


"
mjhb <spark@mjhb.com>,"Mon, 6 Apr 2015 22:00:11 -0700 (MST)",Re: 1.3 Build Error with Scala-2.11,dev@spark.apache.org,"I even deleted my local maven repository (.m2) but still stuck when
attempting to build w/ Scala-2.11:

[ERROR] Failed to execute goal on project spark-core_2.11: Could not resolve
dependencies for project
org.apache.spark:spark-core_2.11:jar:1.3.2-SNAPSHOT: The following artifacts
could not be resolved:
org.apache.spark:spark-network-common_2.10:jar:1.3.2-SNAPSHOT,
org.apache.spark:spark-network-shuffle_2.10:jar:1.3.2-SNAPSHOT: Could not
find artifact org.apache.spark:spark-network-common_2.10:jar:1.3.2-SNAPSHOT
in apache.snapshots (http://repository.apache.org/snapshots) -> [Help 1]
org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute
goal on project spark-core_2.11: Could not resolve dependencies for project
org.apache.spark:spark-core_2.11:jar:1.3.2-SNAPSHOT: The following artifacts
could not be resolved:
org.apache.spark:spark-network-common_2.10:jar:1.3.2-SNAPSHOT,
org.apache.spark:spark-network-shuffle_2.10:jar:1.3.2-SNAPSHOT: Could not
find artifact org.apache.spark:spark-network-common_2.10:jar:1.3.2-SNAPSHOT
in apache.snapshots (http://repository.apache.org/snapshots)




--

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Tue, 7 Apr 2015 01:05:06 -0400",Re: 1.3 Build Error with Scala-2.11,mjhb <spark@mjhb.com>,"Hmm..  Make sure you are building with the right flags. I think you need to
pass -Dscala-2.11 to maven. Take a look at the upstream docs - on my phone
now so can't easily access.

"
Felix C <felixcheung_m@hotmail.com>,"Tue, 7 Apr 2015 02:06:37 -0700",Re: Spark streaming with Kafka- couldnt find KafkaUtils,"Akhil Das <akhil@sigmoidanalytics.com>, Priya Ch
	<learnings.chitturi@gmail.com>","Or you could build an uber jar ( you could google that )

https://eradiating.wordpress.com/2015/02/15/getting-spark-streaming-on-kafka-to-work/


How are you submitting the application? Use a standard build tool like
maven or sbt to build your project, it will download all the dependency
jars, when you submit your application (if you are using spark-submit, then
use --jars option to add those jars which are causing
classNotFoundException). If you are running as a standalone application
without using spark-submit, then while creating the SparkContext, use
sc.addJar() to add those dependency jars.

For Kafka streaming, when you use sbt, these will be jars that are required:

    sc.addJar(""/root/.ivy2/cache/org.apache.spark/spark-streaming-kafka_2.10/jars/spark-streaming-kafka_2.10-1.1.0.jar"")
   sc.addJar(""/root/.ivy2/cache/com.yammer.metrics/metrics-core/jars/metrics-core-2.2.0.jar"")
   sc.addJar(""/root/.ivy2/cache/org.apache.kafka/kafka_2.10/jars/kafka_2.10-0.8.0.jar"")
   sc.addJar(""/root/.ivy2/cache/com.101tec/zkclient/jars/zkclient-0.3.jar"")




Thanks
Best Regards

om>

p

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Tue, 7 Apr 2015 07:36:20 -0400",Re: [VOTE] Release Apache Spark 1.2.2,Patrick Wendell <pwendell@gmail.com>,"I think that's close enough for a +1:

Signatures and hashes are good.
LICENSE, NOTICE still check out.
Compiles for a Hadoop 2.6 + YARN + Hive profile.

JIRAs with target version = 1.2.x look legitimate; no blockers.

I still observe several Hive test failures with:
mvn -Phadoop-2.4 -Pyarn -Phive -Phive-0.13.1 -Dhadoop.version=2.6.0
-DskipTests clean package; mvn -Phadoop-2.4 -Pyarn -Phive
-Phive-0.13.1 -Dhadoop.version=2.6.0 test
.. though again I think these are not regressions but known issues in
older branches.

FYI there are 16 Critical issues still open for 1.2.x:

SPARK-6209,ExecutorClassLoader can leak connections after failing to
load classes from the REPL class server,Josh Rosen,In Progress,4/5/15
SPARK-5098,Number of running tasks become negative after tasks
lost,,Open,1/14/15
SPARK-4888,""Spark EC2 doesn't mount local disks for i2.8xlarge
instances"",,Open,1/27/15
SPARK-4879,Missing output partitions after job completes with
speculative execution,Josh Rosen,Open,3/5/15
SPARK-4568,Publish release candidates under $VERSION-RCX instead of
$VERSION,Patrick Wendell,Open,11/24/14
SPARK-4520,SparkSQL exception when reading certain columns from a
parquet file,sadhan sood,Open,1/21/15
SPARK-4514,SparkContext localProperties does not inherit property
updates across thread reuse,Josh Rosen,Open,3/31/15
SPARK-4454,Race condition in DAGScheduler,Josh Rosen,Reopened,2/18/15
SPARK-4452,Shuffle data structures can starve others on the same
thread for memory,Tianshuo Deng,Open,1/24/15
SPARK-4356,Test Scala 2.11 on Jenkins,Patrick Wendell,Open,11/12/14
SPARK-4258,NPE with new Parquet Filters,Cheng Lian,Reopened,4/3/15
SPARK-4194,Exceptions thrown during SparkContext or SparkEnv
construction might lead to resource leaks or corrupted global
state,,In Progress,4/2/15
SPARK-4159,""Maven build doesn't run JUnit test suites"",Sean Owen,Open,1/11/15
SPARK-4106,Shuffle write and spill to disk metrics are incorrect,,Open,10/28/14
SPARK-3492,Clean up Yarn integration code,Andrew Or,Open,9/12/14
SPARK-3461,Support external groupByKey using
repartitionAndSortWithinPartitions,Sandy Ryza,Open,11/10/14
SPARK-2984,FileNotFoundException on _temporary directory,,Open,12/11/14
SPARK-2532,Fix issues with consolidated shuffle,,Open,3/26/15
SPARK-1312,Batch should read based on the batch interval provided in
the StreamingContext,Tathagata Das,Open,12/24/14


---------------------------------------------------------------------


"
prabeesh k <prabsmails@gmail.com>,"Tue, 7 Apr 2015 16:31:03 +0400",not in gzip format,dev <dev@spark.apache.org>,"Please check the apache mirror
http://www.apache.org/dyn/closer.cgi/spark/spark-1.3.0/spark-1.3.0.tgz
file. It is not in the gzip format.
"
Sean Owen <sowen@cloudera.com>,"Tue, 7 Apr 2015 08:35:07 -0400",Re: not in gzip format,prabeesh k <prabsmails@gmail.com>,"Er, click the link? It is indeed a redirector HTML page. This is how all
Apache releases are served.

"
prabeesh k <prabsmails@gmail.com>,"Tue, 7 Apr 2015 16:36:42 +0400",Re: not in gzip format,Sean Owen <sowen@cloudera.com>,"but name just confusing


"
Marcelo Vanzin <vanzin@cloudera.com>,"Tue, 7 Apr 2015 11:34:31 -0700",Re: [VOTE] Release Apache Spark 1.3.1,"""dev@spark.apache.org"" <dev@spark.apache.org>","+1 (non-binding)

Ran standalone and yarn tests on the hadoop-2.6 tarball, with and
without the external shuffle service in yarn mode.




-- 
Marcelo

---------------------------------------------------------------------


"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Tue, 7 Apr 2015 22:03:33 +0000",Regularization in MLlib,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

Could anyone elaborate on the regularization in Spark? I've found that L1 and L2 are implemented with Updaters (L1Updater, SquaredL2Updater).
1)Why the loss reported by L2 is (0.5 * regParam * norm * norm) where norm is Norm(weights, 2.0)? It should be 0.5*regParam*norm (0.5 to disappear after differentiation). It seems that it is mixed up with mean squared error.
2)Why all weights are regularized? I think we should leave the bias weights (aka free or intercept) untouched if we don't assume that the data is centralized.
3)Are there any short-term plans to move regularization from updater to a more convenient place?

Best regards, Alexander

"
DB Tsai <dbtsai@dbtsai.com>,"Tue, 7 Apr 2015 15:28:04 -0700",Re: Regularization in MLlib,"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","1)  Norm(weights, N) will return (w_1^N + w_2^N +....)^(1/N), so norm
* norm is required.

2) This is bug as you said. I intend to fix this using weighted
regularization, and intercept term will be regularized with weight
zero. https://github.com/apache/spark/pull/1518 But I never actually
have time to finish it. In the meantime, I'm fixing this without this
framework in new ML pipeline framework.

3) I think in the long term, we need weighted regularizer instead of
updater which couples regularization and adaptive step size update for
GD which is not needed in other optimization package.

Sincerely,

DB Tsai
-------------------------------------------------------
Blog: https://www.dbtsai.com



---------------------------------------------------------------------


"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Tue, 7 Apr 2015 23:02:41 +0000",RE: Regularization in MLlib,DB Tsai <dbtsai@dbtsai.com>,"Hi DB,

Thank you!

In general case (not only for regression), I think that Regularizer should be tightly coupled with Gradient otherwise it will have no idea which weights are bias (intercept).

Best regards, Alexander

-----Original Message-----
From: DB Tsai [mailto:dbtsai@dbtsai.com] 
Sent: Tuesday, April 07, 2015 3:28 PM
To: Ulanov, Alexander
Cc: dev@spark.apache.org
Subject: Re: Regularization in MLlib

1)  Norm(weights, N) will return (w_1^N + w_2^N +....)^(1/N), so norm
* norm is required.

2) This is bug as you said. I intend to fix this using weighted regularization, and intercept term will be regularized with weight zero. https://github.com/apache/spark/pull/1518 But I never actually have time to finish it. In the meantime, I'm fixing this without this framework in new ML pipeline framework.

3) I think in the long term, we need weighted regularizer instead of updater which couples regularization and adaptive step size update for GD which is not needed in other optimization package.

Sincerely,

DB Tsai
-------------------------------------------------------
Blog: https://www.dbtsai.com


On Tue, Apr 7, 2015 at 3:03 PM, Ulanov, Alexander <alexander.ulanov@hp.com> wrote:
> Hi,
>
> Could anyone elaborate on the regularization in Spark? I've found that L1 and L2 are implemented with Updaters (L1Updater, SquaredL2Updater).
> 1)Why the loss reported by L2 is (0.5 * regParam * norm * norm) where norm is Norm(weights, 2.0)? It should be 0.5*regParam*norm (0.5 to disappear after differentiation). It seems that it is mixed up with mean squared error.
> 2)Why all weights are regularized? I think we should leave the bias weights (aka free or intercept) untouched if we don't assume that the data is centralized.
> 3)Are there any short-term plans to move regularization from updater to a more convenient place?
>
> Best regards, Alexander
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org
"
Patrick Wendell <pwendell@gmail.com>,"Tue, 7 Apr 2015 19:13:30 -0400",Re: [VOTE] Release Apache Spark 1.3.1,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey All,

Today SPARK-6737 came to my attention. This is a bug that causes a
memory leak for any long running program that repeatedly saves data
out to a Hadoop FileSystem. For that reason, it is problematic for
Spark Streaming.

My sense is that this is severe enough to cut another RC once the fix
is merged (which is imminent):

https://issues.apache.org/jira/browse/SPARK-6737

I'll leave a bit of time for others to comment, in particular if
people feel we should not wait for this fix.

- Patrick


---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Tue, 7 Apr 2015 16:38:53 -0700","Re: extended jenkins downtime, thursday april 9th 7am-noon PDT
 (moving to anaconda python & more)","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","reminder!  this is happening thurday morning.


"
Josh Rosen <rosenville@gmail.com>,"Tue, 7 Apr 2015 17:13:04 -0700",Re: [VOTE] Release Apache Spark 1.3.1,Patrick Wendell <pwendell@gmail.com>,"The leak will impact long running streaming jobs even if they don't write Hadoop files, although the problem may take much longer to manifest itself for those jobs.

I think we currently leak an empty HashMap per stage submitted in the common other hand, the worst case behavior is quite bad for streaming jobs, so we should probably fix this so that 1.2.x streaming users can more safely upgrade to 1.3.x.

- Josh

Sent from my phone

e:
te:
 1.3.1!
b5d9f31b713ed90bcec63ebc4e530cbb69851

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 08 Apr 2015 00:59:41 +0000",Contributor CLAs,Spark dev list <dev@spark.apache.org>,"I've seen many other OSS projects ask contributors to sign CLAs. I've never
seen us do that.

I assume it's not an issue, since people opening PRs generally understand
what it means. But legally I'm sure there's some danger in taking an
implied vs. explicit license to do something.

So: Do we need to make people sign contributor CLAs?

I'm betting Sean Owen knows something about this... :)

Nick
"
Sean Owen <sowen@cloudera.com>,"Tue, 7 Apr 2015 21:11:21 -0400",Re: Contributor CLAs,Nicholas Chammas <nicholas.chammas@gmail.com>,"Yeah, this is why this pops up when you open a PR:
https://github.com/apache/spark/blob/master/CONTRIBUTING.md

Mostly, I want to take all reasonable steps to ensure that when
somebody offers a code contribution, that they are fine with the ways
in which it actually used (redistributed under the terms of the AL2),
whether or not they understand the intricacies. In good faith, I'm all
but sure that all contributors either think they're giving the
contribution to the project anyway, or at least, do understand it to
be their own work licensed under the same terms as all of the project
contributions are.

IANAL, but in stricter legal terms, the project license is plain and
clear, and the intricacies are signposted and easy to read when you
contribute. You would have a very hard time arguing that you made a
contribution, didn't state anything about the license, but did not
intend somehow that the work could be licensed as the rest of the
project is. For reference Apache projects do not in general require a
CLA.


---------------------------------------------------------------------


"
Vadim Bichutskiy <vadim.bichutskiy@gmail.com>,"Tue, 7 Apr 2015 21:17:27 -0400",Re: Spark + Kinesis,Tathagata Das <tdas@databricks.com>,"Hey y'all,

While I haven't been able to get Spark + Kinesis integration working, I
pivoted to plan B: I now push data to S3 where I set up a DStream to
monitor an S3 bucket with textFileStream, and that works great.

I <3 Spark!

Best,
Vadim


á§


d
tream-Feeding-and-Eating-Amazon-Kinesis-Streams-with-Python
b
st
st
=>*
s
age:
e>
iven
the
ds()
this
ch
ig and
r of
val
cy.
he
ml
t5XZs653q_MN8rBNbzRbv22W8r4TLx56dCDWf13Gc8R02?t=http%3A%2F%2Fspark.apache.org%2Fdocs%2Flatest%2Fstreaming-kinesis-integration.html&si=5533377798602752&pi=8898f7f0-ede6-4e6c-9bfd-00cf2101f0e9>
ain/scala/org/apache/spark/examples/streaming/KinesisWordCountASL.scala
t5XZs653q_MN8rBNbzRbv22W8r4TLx56dCDWf13Gc8R02?t=https%3A%2F%2Fgithub.com%2Fapache%2Fspark%2Fblob%2Fmaster%2Fextras%2Fkinesis-asl%2Fsrc%2Fmain%2Fscala%2Forg%2Fapache%2Fspark%2Fexamples%2Fstreaming%2FKinesisWordCountASL.scala&si=5533377798602752&pi=8898f7f0-ede6-4e6c-9bfd-00cf2101f0e9>
:
0""
.5, you
an
ter the
se just
igns
ead:
other
 :=
core_2.10""
 %
1.3.0"")*
""
"
Imran Rashid <irashid@cloudera.com>,"Tue, 7 Apr 2015 21:04:09 -0500",Re: 1.3 Build Error with Scala-2.11,mjhb <spark@mjhb.com>,"did you run

dev/change-version-to-2.11.sh

before compiling?  When I ran this on current master, it mostly worked:

dev/change-version-to-2.11.sh
mvn -Pyarn -Phadoop-2.4 -Pscala-2.11 -DskipTests clean package

There was a failure in building catalyst, but core built just fine for me.
The error I got was:

[INFO]
------------------------------------------------------------------------

[INFO] Building Spark Project Catalyst 1.4.0-SNAPSHOT

[INFO]
------------------------------------------------------------------------

[WARNING] The POM for org.scalamacros:quasiquotes_2.11:jar:2.0.1 is
missing, no dependency information available


I'm not sure if catalyst is supposed to work w/ scala-2.11 or not ... I
wouldn't be surprised if the way macros should be used has changed, but its
not listed explicitly in the docs as being incompatible:

http://spark.apache.org/docs/latest/building-spark.html#building-for-scala-211





"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 08 Apr 2015 02:19:15 +0000",Re: Contributor CLAs,Sean Owen <sowen@cloudera.com>,"SGTM.


"
Matei Zaharia <matei.zaharia@gmail.com>,"Tue, 7 Apr 2015 23:10:29 -0400",Re: Contributor CLAs,Nicholas Chammas <nicholas.chammas@gmail.com>,"You do actually sign a CLA when you become a committer, and in general, we should ask for CLAs from anyone who contributes a large piece of code. This is the individual CLA: https://www.apache.org/licenses/icla.txt. Some people have sent them proactively because their employer asks them too.

Matei

all
I've
understand


---------------------------------------------------------------------


"
Marty Bower <spark@mjhb.com>,"Wed, 08 Apr 2015 03:54:34 +0000",Re: 1.3 Build Error with Scala-2.11,"Imran Rashid <irashid@cloudera.com>, mjhb <spark@mjhb.com>","Yes - ran dev/change-version-to-2.11.sh

But was missing -Dscala-2.11 on mvn command after a -2.10 build. Building
successfully again now after adding that.


"
Patrick Wendell <pwendell@gmail.com>,"Wed, 8 Apr 2015 01:38:57 -0400",[RESULT] [VOTE] Release Apache Spark 1.3.1,"""dev@spark.apache.org"" <dev@spark.apache.org>","This vote is cancelled in favor of RC2.

 Hadoop files, although the problem may take much longer to manifest itself for those jobs.
 the other hand, the worst case behavior is quite bad for streaming jobs, so we should probably fix this so that 1.2.x streaming users can more safely upgrade to 1.3.x.
ote:
rote:
on 1.3.1!
dcb5d9f31b713ed90bcec63ebc4e530cbb69851
:

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Wed, 8 Apr 2015 01:46:09 -0400",[VOTE] Release Apache Spark 1.3.1 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version 1.3.1!

The tag to be voted on is v1.3.1-rc2 (commit 7c4473a):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=7c4473aa5a7f5de0323394aaedeefbf9738e8eb5

The list of fixes present in this release can be found at:
http://bit.ly/1C2nVPY

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-1.3.1-rc2/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1083/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-1.3.1-rc2-docs/

The patches on top of RC1 are:

[SPARK-6737] Fix memory leak in OutputCommitCoordinator
https://github.com/apache/spark/pull/5397

[SPARK-6636] Use public DNS hostname everywhere in spark_ec2.py
https://github.com/apache/spark/pull/5302

[SPARK-6205] [CORE] UISeleniumSuite fails for Hadoop 2.x test with
NoClassDefFoundError
https://github.com/apache/spark/pull/4933

Please vote on releasing this package as Apache Spark 1.3.1!

The vote is open until Saturday, April 11, at 07:00 UTC and passes
if a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.3.1
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

---------------------------------------------------------------------


"
Emre Sevinc <emre.sevinc@gmail.com>,"Wed, 8 Apr 2015 10:44:43 +0200",Which method do you think is better for making MIN_REMEMBER_DURATION configurable?,dev@spark.apache.org,"Hello,

This is about SPARK-3276 and I want to make MIN_REMEMBER_DURATION (that is
now a constant) a variable (configurable, with a default value). Before
spending effort on developing something and creating a pull request, I
wanted to consult with the core developers to see which approach makes most
sense, and has the higher probability of being accepted.

The constant MIN_REMEMBER_DURATION can be seen at:


https://github.com/apache/spark/blob/master/streaming/src/main/scala/org/apache/spark/streaming/dstream/FileInputDStream.scala#L338

it is marked as private member of private[streaming] object
FileInputDStream.

Approach 1: Make MIN_REMEMBER_DURATION a variable, with a new name of
minRememberDuration, and then  add a new fileStream method to
JavaStreamingContext.scala :


https://github.com/apache/spark/blob/master/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingContext.scala

such that the new fileStream method accepts a new parameter, e.g.
minRememberDuration: Int (in seconds), and then use this value to set the
private minRememberDuration.


Approach 2: Create a new, public Spark configuration property, e.g. named
spark.rememberDuration.min (with a default value of 60 seconds), and then
set the private variable minRememberDuration to the value of this Spark
property.


Approach 1 would mean adding a new method to the public API, Approach 2
would mean creating a new public Spark property. Right now, approach 2
seems more straightforward and simpler to me, but nevertheless I wanted to
have the opinions of other developers who know the internals of Spark
better than I do.

Kind regards,
Emre SevinÃ§
"
Steve Loughran <stevel@hortonworks.com>,"Wed, 8 Apr 2015 10:16:25 +0000",finding free ports for tests,"""dev@spark.apache.org"" <dev@spark.apache.org>","
I'm writing some functional tests for the SPARK-1537 JIRA, Yarn timeline service integration, for which I need to allocate some free ports.

I don't want to hard code them in as that can lead to unreliable tests, especially on Jenkins. 

Before I implement the logic myself -Is there a utility class/trait for finding ports for tests?

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Wed, 8 Apr 2015 07:13:31 -0400",Re: [VOTE] Release Apache Spark 1.3.1 (RC2),Patrick Wendell <pwendell@gmail.com>,"Still a +1 from me; same result (except that now of course the
UISeleniumSuite test does not fail)


---------------------------------------------------------------------


"
=?UTF-8?Q?Zolt=C3=A1n_Zvara?= <zoltan.zvara@gmail.com>,"Wed, 8 Apr 2015 15:19:51 +0200",RDD firstParent,"""dev@spark.apache.org"" <dev@spark.apache.org>","Is does not seem to be safe to call RDD.firstParent from anywhere, as it
might throw a java.util.NoSuchElementException: ""head of empty list"". This
seems to be a bug for a consumer of the RDD API.

Zvara ZoltÃ¡n



mail, hangout, skype: zoltan.zvara@gmail.com

mobile, viber: +36203129543

bank: 10918001-00000021-50480008

address: Hungary, 2475 KÃ¡polnÃ¡snyÃ©k, Kossuth 6/a

elte: HSKSJZ (ZVZOAAI.ELTE)
"
Nathan Kronenfeld <nkronenfeld@uncharted.software>,"Wed, 8 Apr 2015 11:03:10 -0400",PR 5140,dev <dev@spark.apache.org>,"Could I get someone to look at PR 5140 please? It's been languishing more
than two weeks.
"
Matei Zaharia <matei.zaharia@gmail.com>,"Wed, 8 Apr 2015 11:55:19 -0400",Re: [VOTE] Release Apache Spark 1.3.1 (RC2),Sean Owen <sowen@cloudera.com>,"+1. Tested on Mac OS X and verified that some of the bugs were fixed.

Matei

version 1.3.1!
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=7c4473aa5a7f5de0323394aaedeefbf9738e8eb5
at:
https://repository.apache.org/content/repositories/org"
Tom Graves <tgraves_cs@yahoo.com.INVALID>,"Wed, 8 Apr 2015 15:57:41 +0000 (UTC)",Re: [VOTE] Release Apache Spark 1.3.1 (RC2),"Sean Owen <sowen@cloudera.com>, Patrick Wendell <pwendell@gmail.com>","+1. Tested spark on yarn against hadoop 2.6.
Tom 


   

 Still a +1 from me; same result (except that now of course the
UISeleniumSuite test does not fail)


---------------------------------------------------------------------



   "
Denny Lee <denny.g.lee@gmail.com>,"Wed, 08 Apr 2015 16:32:25 +0000",Re: [VOTE] Release Apache Spark 1.3.1 (RC2),"Tom Graves <tgraves_cs@yahoo.com>, Sean Owen <sowen@cloudera.com>, 
	Patrick Wendell <pwendell@gmail.com>","The RC2 bits are lacking Hadoop 2.4 and Hadoop 2.6 - was that intended
(they were included in RC1)?



"
Tathagata Das <tdas@databricks.com>,"Wed, 8 Apr 2015 10:16:36 -0700","Re: Which method do you think is better for making
 MIN_REMEMBER_DURATION configurable?",Emre Sevinc <emre.sevinc@gmail.com>,"Approach 2 is definitely better  :)
Can you tell us more about the use case why you want to do this?

TD


s
st
apache/spark/streaming/dstream/FileInputDStream.scala#L338
apache/spark/streaming/api/java/JavaStreamingContext.scala
o
"
Timothy Chen <tnachen@gmail.com>,"Wed, 8 Apr 2015 10:53:34 -0700",Re: [VOTE] Release Apache Spark 1.3.1 (RC2),Denny Lee <denny.g.lee@gmail.com>,"+1 Tested on 4 nodes Mesos cluster with fine-grain and coarse-grain mode.

Tim


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Wed, 8 Apr 2015 11:38:20 -0700",Re: RDD firstParent,=?UTF-8?Q?Zolt=C3=A1n_Zvara?= <zoltan.zvara@gmail.com>,"Why is this a bug? Each RDD implementation should know whether they have a
parent or not.

For example, if you are a MapPartitionedRDD, there is always a parent since
it is a unary operator.



s
"
Emre Sevinc <emre.sevinc@gmail.com>,"Wed, 8 Apr 2015 20:51:33 +0200","Re: Which method do you think is better for making
 MIN_REMEMBER_DURATION configurable?",Tathagata Das <tdas@databricks.com>,"Tathagata,

Thanks for stating your preference for Approach 2.

My use case and motivation are similar to the concerns raised by others in
SPARK-3276. In previous versions of Spark, e.g. 1.1.x we had the ability
for Spark Streaming applications to process the files in an input directory
that existed before the streaming application began, and for some projects
that we did for our customers, we relied on that feature. Starting from
1.2.x series, we are limited in this respect to the files whose time stamp
is not older than 1 minute. The only workaround is to 'touch' those files
before starting a streaming application.

Moreover, this MIN_REMEMBER_DURATION is set to an arbitrary value of 1
minute, and I don't see any argument why it cannot be set to another
arbitrary value (keeping the default value of 1 minute, if nothing is set
by the user).

Putting all this together, my plan is to create a Pull Request that is like

  1- Convert ""private val MIN_REMEMBER_DURATION"" into ""private val
minRememberDuration"" (to reflect the change that it is not a constant in
the sense that it can be set via configuration)

  2- Set its value by using something like
getConf(""spark.streaming.minRememberDuration"", Minutes(1))

  3- Document the spark.streaming.minRememberDuration in Spark Streaming
Programming Guide

If the above sounds fine, then I'll go on implementing this small change
and submit a pull request for fixing SPARK-3276.

What do you say?

Kind regards,

Emre SevinÃ§
http://www.bigindustries.be/



:
is
/apache/spark/streaming/dstream/FileInputDStream.scala#L338
/apache/spark/streaming/api/java/JavaStreamingContext.scala
e
d
n
to


-- 
Emre Sevinc
"
"""Hari Shreedharan"" <hshreedharan@cloudera.com>","Wed, 08 Apr 2015 12:19:06 -0700 (PDT)",Re: finding free ports for tests,"""Sean Owen"" <sowen@cloudera.com>","to an ephemeral port and then query it to find the port it is running on. This ensures that race conditions donâ€™t cause test failures.




Thanks,Â Hari


 service integration, for which I need to allocate some free ports.
especially on Jenkins.
finding ports for tests?
org"
Patrick Wendell <pwendell@gmail.com>,"Wed, 8 Apr 2015 15:30:45 -0400",Re: [VOTE] Release Apache Spark 1.3.1 (RC2),Timothy Chen <tnachen@gmail.com>,"Hey Denny,

I beleive the 2.4 bits are there. The 2.6 bits I had done specially
(we haven't merge that into our upstream build script). I'll do it
again now for RC2.

- Patrick


---------------------------------------------------------------------


"
Denny Lee <denny.g.lee@gmail.com>,"Wed, 08 Apr 2015 19:41:34 +0000",Re: [VOTE] Release Apache Spark 1.3.1 (RC2),"Patrick Wendell <pwendell@gmail.com>, Timothy Chen <tnachen@gmail.com>","Oh, it appears the 2.4 bits without hive are there but not the 2.4 bits
with hive. Cool stuff on the 2.6.

"
Patrick Wendell <pwendell@gmail.com>,"Wed, 8 Apr 2015 15:49:54 -0400",Re: [VOTE] Release Apache Spark 1.3.1 (RC2),Denny Lee <denny.g.lee@gmail.com>,"Oh I see - ah okay I'm guessing it was a transient build error and
I'll get it posted ASAP.


---------------------------------------------------------------------


"
Andrew Or <andrew@databricks.com>,"Wed, 8 Apr 2015 13:26:39 -0700",Re: PR 5140,Nathan Kronenfeld <nkronenfeld@uncharted.software>,"Hey Nathan, thanks for bringing this up I will look at this within the next
day or two.

2015-04-08 8:03 GMT-07:00 Nathan Kronenfeld <nkronenfeld@uncharted.software>
:

"
Sandy Ryza <sandy.ryza@cloudera.com>,"Wed, 8 Apr 2015 14:20:23 -0700",Re: [VOTE] Release Apache Spark 1.3.1 (RC2),Patrick Wendell <pwendell@gmail.com>,"+1

Built against Hadoop 2.6 and ran some jobs against a pseudo-distributed
YARN cluster.

-Sandy


"
Joseph Bradley <joseph@databricks.com>,"Wed, 8 Apr 2015 18:34:07 -0400",Re: [mllib] Deprecate static train and use builder instead for Scala/Java,Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"I'll add a note that this is just for ML, not other parts of Spark.  (We
can discuss more on the JIRA.)
Thanks!
Joseph


"
Jeremy Freeman <freeman.jeremy@gmail.com>,"Wed, 8 Apr 2015 19:53:38 -0400",Re: Which method do you think is better for making MIN_REMEMBER_DURATION configurable?,Emre Sevinc <emre.sevinc@gmail.com>,"+1 for this feature

In our use case, we probably wouldn’t use this feature in production, but it can be useful during prototyping and algorithm development to repeatedly perform the same streaming operation on a fixed, already existing set of files.

---"
Krishna Sankar <ksankar42@gmail.com>,"Wed, 8 Apr 2015 16:59:00 -0700",Re: [VOTE] Release Apache Spark 1.3.1 (RC2),Patrick Wendell <pwendell@gmail.com>,"+1 (non-binding, of course)

1. Compiled OSX 10.10 (Yosemite) OK Total time: 14:16 min
     mvn clean package -Pyarn -Dyarn.version=2.6.0 -Phadoop-2.4
-Dhadoop.version=2.6.0 -Phive -DskipTests -Dscala-2.11
2. Tested pyspark, mlib - running as well as comp"
Joseph Bradley <joseph@databricks.com>,"Wed, 8 Apr 2015 20:34:59 -0400",Re: [VOTE] Release Apache Spark 1.3.1 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1 tested ML-related items on Mac OS X


"
Denny Lee <denny.g.lee@gmail.com>,"Thu, 09 Apr 2015 04:59:08 +0000",Re: [VOTE] Release Apache Spark 1.3.1 (RC2),"Joseph Bradley <joseph@databricks.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","+1 (non-binding)

Tested Scala, SparkSQL, and MLLib on OSX against Hadoop 2.6


"
=?UTF-8?Q?Zolt=C3=A1n_Zvara?= <zoltan.zvara@gmail.com>,"Thu, 9 Apr 2015 10:15:44 +0200",Spark remote communication pattern,"""dev@spark.apache.org"" <dev@spark.apache.org>","Dear Developers,

I'm trying to investigate the communication pattern regarding data-flow
during execution of a Spark program defined by an RDD chain. I'm
investigating from the Task point of view, and found out that the task type
ResultTask (as retrieving the iterator for its RDD for a given partition),
effectively asks the BlockManager to get the block from local or remote
location. What I do there is to include actual location data in BlockResult
so the task can tell where it retrieved the data from. I've found out that
ResultTask can issue a data-flow only in this case.

What's the case with the ShuffleMapTask? What happens there? I'm trying to
log locations which are included in the shuffle process. I would be happy
to receive a few hints regarding where remote communication is managed in
case of ShuffleMapTask.

Thanks!

ZoltÃ¡n
"
Reynold Xin <rxin@databricks.com>,"Thu, 9 Apr 2015 01:24:21 -0700",Re: Spark remote communication pattern,=?UTF-8?Q?Zolt=C3=A1n_Zvara?= <zoltan.zvara@gmail.com>,"Take a look at the following two files:

https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/shuffle/hash/BlockStoreShuffleFetcher.scala

and

https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala


pe
,
lt
t
o
"
=?UTF-8?Q?Zolt=C3=A1n_Zvara?= <zoltan.zvara@gmail.com>,"Thu, 9 Apr 2015 13:57:13 +0200",Connect to remote YARN cluster,"""dev@spark.apache.org"" <dev@spark.apache.org>","cluster everything works fine, but the remote YARN resource manager throws
away my request because of authentication error. I'm running IntelliJ 14 on
Ubuntu and the driver tries to connect to YARN with my local user name. How
can I force IntelliJ to run my code with a different user? Or how can I set
up the connection to YARN RM with auth data?

Thanks!

Zvara ZoltÃ¡n



mail, hangout, skype: zoltan.zvara@gmail.com

mobile, viber: +36203129543

bank: 10918001-00000021-50480008

address: Hungary, 2475 KÃ¡polnÃ¡snyÃ©k, Kossuth 6/a

elte: HSKSJZ (ZVZOAAI.ELTE)
"
=?UTF-8?Q?Zolt=C3=A1n_Zvara?= <zoltan.zvara@gmail.com>,"Thu, 9 Apr 2015 16:27:56 +0200",Re: Spark remote communication pattern,Reynold Xin <rxin@databricks.com>,"Thanks! I've found the fetcher! Is there any other places and cases where
blocks are traveled through network?

Zvara ZoltÃ¡n



mail, hangout, skype: zoltan.zvara@gmail.com

mobile, viber: +36203129543

bank: 10918001-00000021-50480008

address: Hungary, 2475 KÃ¡polnÃ¡snyÃ©k, Kossuth 6/a

elte: HSKSJZ (ZVZOAAI.ELTE)

2015-04-09 10:24 GMT+02:00 Reynold Xin <rxin@databricks.com>:

e/spark/shuffle/hash/BlockStoreShuffleFetcher.scala
e/spark/storage/ShuffleBlockFetcherIterator.scala
),
at
to
y
n
"
shane knapp <sknapp@berkeley.edu>,"Thu, 9 Apr 2015 07:29:00 -0700","Re: extended jenkins downtime, thursday april 9th 7am-noon PDT
 (moving to anaconda python & more)","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","and this is now happening.


"
shane knapp <sknapp@berkeley.edu>,"Thu, 9 Apr 2015 08:38:11 -0700","Re: extended jenkins downtime, thursday april 9th 7am-noon PDT
 (moving to anaconda python & more)","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","things are looking pretty good and i expect to be done within an hour.
 i've got some test builds running right now, and will give the green light
when they successfully complete.


"
Sean McNamara <Sean.McNamara@Webtrends.com>,"Thu, 9 Apr 2015 15:43:00 +0000",Re: [VOTE] Release Apache Spark 1.3.1 (RC2),Patrick Wendell <pwendell@gmail.com>,"+1 tested on OS X

Sean

1.3.1!
73aa5a7f5de0323394aaedeefbf9738e8eb5


---------------------------------------------------------------------


"
Marcelo Vanzin <vanzin@cloudera.com>,"Thu, 9 Apr 2015 09:42:34 -0700",Re: Connect to remote YARN cluster,=?UTF-8?Q?Zolt=C3=A1n_Zvara?= <zoltan.zvara@gmail.com>,"If YARN is authenticating users it's probably running on kerberos, so
you need to log in with your kerberos credentials (kinit) before
submitting an application.

s
on
ow
et



-- 
Marcelo

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Thu, 9 Apr 2015 10:04:11 -0700",Re: Spark remote communication pattern,=?UTF-8?Q?Zolt=C3=A1n_Zvara?= <zoltan.zvara@gmail.com>,"For torrent broadcast, data are read directly through the block manager:

https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala#L167




he/spark/shuffle/hash/BlockStoreShuffleFetcher.scala
he/spark/storage/ShuffleBlockFetcherIterator.scala
m>
py
in
"
shane knapp <sknapp@berkeley.edu>,"Thu, 9 Apr 2015 10:18:54 -0700","Re: extended jenkins downtime, thursday april 9th 7am-noon PDT
 (moving to anaconda python & more)","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","ok, we're looking good.  i'll keep an eye on this for the rest of the day,
and if you happen to notice any infrastructure failures before i do (i
updated a LOT), please let me know immediately!  :)


"
Imran Rashid <irashid@cloudera.com>,"Thu, 9 Apr 2015 12:59:48 -0500",Re: enum-like types in Spark,Patrick Wendell <pwendell@gmail.com>,"any update here?  This is relevant for a currently open PR of mine -- I've
got a bunch of new public constants defined w/ format #4, but I'd gladly
switch to java enums.  (Even if we are just going to postpone this
decision, I'm still inclined to switch to java enums ...)

just to be clear about the existing problem with enums & scaladoc: right
now, the scaladoc knows about the enum class, and generates a page for it,
but it does not display the enum constants.  It is at least labeled as a
java enum, though, so a savvy user could switch to the javadocs to see the
constants.




"
Steve Loughran <stevel@hortonworks.com>,"Thu, 9 Apr 2015 19:46:48 +0000",Re: finding free ports for tests,,"
On 8 Apr 2015, at 20:19, Hari Shreedharan <hshreedharan@cloudera.com<mailto:hshreedharan@cloudera.com>> wrote:

One good way to guarantee your tests will work is to have your server bind to an ephemeral port and then query it to find the port it is running on. This ensures that race conditions donâ€™t cause test failures.


yes, that's what I'm doing; the classic tactic. Find the tests fail if the laptop doesn't know its own name, but so do others


Thanks,
Hari



On Wed, Apr 8, 2015 at 3:24 AM, Sean Owen <sowen@cloudera.com<mailto:sowen@cloudera.com>> wrote:

Utils.startServiceOnPort?

On Wed, Apr 8, 2015 at 6:16 AM, Steve Loughran <stevel@hortonworks.com<mailto:stevel@hortonworks.com>> wrote:
>
> I'm writing some functional tests for the SPARK-1537 JIRA, Yarn timeline service integration, for which I need to allocate some free ports.
>
> I don't want to hard code them in as that can lead to unreliable tests, especially on Jenkins.
>
> Before I implement the logic myself -Is there a utility class/trait for finding ports for tests?
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org>
> For additional commands, e-mail: dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org>
For additional commands, e-mail: dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>



"
Steve Loughran <stevel@hortonworks.com>,"Thu, 9 Apr 2015 19:48:07 +0000",Re: Connect to remote YARN cluster,Marcelo Vanzin <vanzin@cloudera.com>,"

also: make sure that you have the full JCE and not the crippled crypto; every time you upgrade the JDK you are likely to have to re-install it. Java gives no useful error messages on this or any other Kerberos problem

---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Thu, 9 Apr 2015 16:12:58 -0700","update, was extended jenkins downtime, thursday april 9th 7am-noon
 PDT (moving to anaconda python & more)","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>, 
	Marcelo Vanzin <vanzin@cloudera.com>, Josh Rosen <joshrosen@databricks.com>","fail due to the tests requiring JAVA_HOME being set, and the JDK
installation jenkins plugin not setting that variable if the JDK was set to
'Default'.

thanks to marcelo vanzin for noticing, and josh for helping me come up
w/the fix...

many more details here and here:
https://github.com/apache/spark/pull/5432
https://github.com/apache/spark/pull/5441

have i mentioned that i hate jenkins plugins?  :(

shane
"
"""York, Brennon"" <Brennon.York@capitalone.com>","Thu, 9 Apr 2015 19:34:01 -0400",Re: [VOTE] Release Apache Spark 1.3.1 (RC2),"Sean McNamara <Sean.McNamara@Webtrends.com>, Patrick Wendell
	<pwendell@gmail.com>","+1 (non-binding)

Still +1 for me





73aa5a
















________________________________________________________

The information contained in this e-mail is confidential and/or proprietary is intended only for use by the individual or entity to whi"
Xiangrui Meng <mengxr@gmail.com>,"Thu, 9 Apr 2015 17:16:00 -0700",Re: enum-like types in Spark,Imran Rashid <irashid@cloudera.com>,"Using Java enums sound good. We can list the values in the JavaDoc and
hope Scala will be able to correctly generate docs for Java enums in
the future. -Xiangrui


---------------------------------------------------------------------


"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Fri, 10 Apr 2015 00:23:15 +0000",Access to hdfs FileSystem through Spark,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

Is there a way to access hdfs FileSystem through Spark? For example, I need to check the file size before opening it with sc.binaryFile(""hdfs://mynetwork.com:9000/myfile""). Can I do it without creating hadoop FileSystem by myself ?

val fs = FileSystem.get(new URI(""hdfs://mynetwork.com:9000""), new Configuration())

Best regards, Alexander
"
Sean Owen <sowen@cloudera.com>,"Thu, 9 Apr 2015 20:38:25 -0400",Re: Access to hdfs FileSystem through Spark,Alexander Ulanov <alexander.ulanov@hp.com>,"What you have there is how to do it although you want to use
sc.hadoopConfiguration IIRC.

"
"""=?utf-8?B?R3VvUWlhbmcgTGk=?="" <witgo@qq.com>","Fri, 10 Apr 2015 10:21:21 +0800",Re: [VOTE] Release Apache Spark 1.3.1 (RC2),"""=?utf-8?B?UGF0cmljayBXZW5kZWxs?="" <pwendell@gmail.com>","+1 (non-binding)â€




------------------ Original ------------------
From:  ""Patrick Wendell"";<pwendell@gmail.com>;
Date:  Wed, Apr 8, 2015 01:46 PM
To:  ""dev@spark.apache.org""<dev@spark.apache.org>; 

Subject:  [VOTE] Release Apache Spark 1.3"
jay vyas <jayunit100.apache@gmail.com>,"Thu, 9 Apr 2015 22:41:13 -0400",Re: Access to hdfs FileSystem through Spark,Sean Owen <sowen@cloudera.com>,"if already on a hadoop cluster, it should just work ootb.
Spark is smart enough to work on hadoop filesystems... it reads hadoop conf
on an existing normal HCFS
cluster and sc.textFile will just use whatever your default hadoop fs uri
is.

In general any this is quite easy to test... once spark is setup properly,
it should naturally
load text files useing spark context from the

1) then put a file into your HCFS file system.

hadoop fs -put /etc/passwd /etc/passwd

2) Then just confirm spark sees it...

val lines = sc.textFile(""/tmp/passwd"")

lines.collect can print this out for you.

As a test of this... you can just use ASF BigTop's spark vagrant recipes :
we dont do anything special, and I found hdfs integration ""just worked"",
since by default we deploy with hadoop configuration for HDFS.







-- 
jay vyas
"
jay vyas <jayunit100.apache@gmail.com>,"Thu, 9 Apr 2015 22:43:40 -0400",Re: Access to hdfs FileSystem through Spark,Sean Owen <sowen@cloudera.com>,"whoa ! sorry about the typos above, i tried to refactor the email and it
sent. anyway, you get the idea :).....

basically spark context will read hadoop settings in the same way yarn
does... so if already on a hadoop cluster, it should work quite naturally
without needing to explicitly set anything at all.





-- 
jay vyas
"
Prashant Sharma <scrapcodes@gmail.com>,"Fri, 10 Apr 2015 12:20:47 +0530",Re: [VOTE] Release Apache Spark 1.3.1 (RC2),GuoQiang Li <witgo@qq.com>,"+1 tested locally on linux.

Prashant Sharma




73aa5a7f5de0323394aaedeefbf9738e8eb5
"
=?UTF-8?Q?Zolt=C3=A1n_Zvara?= <zoltan.zvara@gmail.com>,"Fri, 10 Apr 2015 10:53:21 +0200",Re: Connect to remote YARN cluster,Steve Loughran <stevel@hortonworks.com>,"Wow, very useful comments! Thanks! Now, just HDFS permission problems.

Zvara ZoltÃ¡n



mail, hangout, skype: zoltan.zvara@gmail.com

mobile, viber: +36203129543

bank: 10918001-00000021-50480008

address: Hungary, 2475 KÃ¡polnÃ¡snyÃ©k, Kossuth 6/a

elte: HSKSJZ (ZVZOAAI.ELTE)

2015-04-09 21:48 GMT+02:00 Steve Loughran <stevel@hortonworks.com>:

"
=?UTF-8?Q?Zolt=C3=A1n_Zvara?= <zoltan.zvara@gmail.com>,"Fri, 10 Apr 2015 13:15:08 +0200",Re: Spark remote communication pattern,Reynold Xin <rxin@databricks.com>,"Thank you for the hint!

I've found a HTTP and torrent type broadcast. It seems that
TorrentBroadcast is used. Was HTTP implemented earlier?
Broadcasting is done through SparkContext by user code as I see. But what
other events can trigger a TorrentBroadcast?

Zvara ZoltÃ¡n



mail, hangout, skype: zoltan.zvara@gmail.com

mobile, viber: +36203129543

bank: 10918001-00000021-50480008

address: Hungary, 2475 KÃ¡polnÃ¡snyÃ©k, Kossuth 6/a

elte: HSKSJZ (ZVZOAAI.ELTE)

2015-04-09 19:04 GMT+02:00 Reynold Xin <rxin@databricks.com>:

e/spark/broadcast/TorrentBroadcast.scala#L167
e
che/spark/shuffle/hash/BlockStoreShuffleFetcher.scala
che/spark/storage/ShuffleBlockFetcherIterator.scala
om>
w
e
g
"
Devl Devel <devl.development@gmail.com>,"Fri, 10 Apr 2015 13:21:48 +0100",Integration with Apache Ignite,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi

Having evaluated the ignite project and it's relationship to spark: Ignite
vs. Spark

https://wiki.apache.org/incubator/IgniteProposal

The scope of the project are defined. However are there any current or
future projects planned to have an Ignite file system RDD (like the Hadoop
RDD) so that Spark can leverage the in-memory file system as an
alternative/complement to Tachyon for example?

The idea being to use MLLib and other Spark components but with the option
to use ignite fs.

I'm not an expert on Ignite FS but would appreciate any comments on the
matter.

Thanks

Devl
"
Corey Nolet <cjnolet@gmail.com>,"Fri, 10 Apr 2015 09:10:10 -0400",Re: [VOTE] Release Apache Spark 1.3.1 (RC2),Joseph Bradley <joseph@databricks.com>,"+1 (non-binding)

- Verified signatures
- built on Mac OSX
- built on Fedora 21

All builds were done using profiles: hive, hive-thriftserver, hadoop-2.4,
yarn

+1 tested ML-related items on Mac OS X


itertools
https://git-wip-us.apache.org/repos/asf?p=s"
Reynold Xin <rxin@databricks.com>,"Fri, 10 Apr 2015 11:07:30 -0700",Re: Spark remote communication pattern,=?UTF-8?Q?Zolt=C3=A1n_Zvara?= <zoltan.zvara@gmail.com>,"You can grep for ""broadcast("" in the code for that.


he/spark/broadcast/TorrentBroadcast.scala#L167
m>
ache/spark/shuffle/hash/BlockStoreShuffleFetcher.scala
ache/spark/storage/ShuffleBlockFetcherIterator.scala
com>
ow
k
te
d
"
Reynold Xin <rxin@databricks.com>,"Fri, 10 Apr 2015 11:25:01 -0700",Re: Integration with Apache Ignite,Devl Devel <devl.development@gmail.com>,"There is some work to create an off-heap storage API for Spark. I think
with it, it will be easier to support different storage backends.

https://issues.apache.org/jira/browse/SPARK-6479

With that API in place, rest of the integration should probably just live
outside of Spark in Ignite or as a 3rd party package.



"
Michael Armbrust <michael@databricks.com>,"Fri, 10 Apr 2015 13:38:20 -0700",Re: [VOTE] Release Apache Spark 1.3.1 (RC2),Corey Nolet <cjnolet@gmail.com>,"-1 (binding)

We just were alerted to a pretty serious regression since 1.3.0 (
https://issues.apache.org/jira/browse/SPARK-6851).  Should have a fix
shortly.

Michael


"
Nitin Mathur <ntnmathur@gmail.com>,"Fri, 10 Apr 2015 16:28:55 -0700",Guidance for becoming Spark contributor,dev@spark.apache.org,"Hi Spark Dev Team,

I want to start contributing to Spark Open source. This is the first time I
will be doing any open source contributions.

It would be great if I can get some guidance on where I can start with.

Thanks,
- Nitin
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 10 Apr 2015 23:35:56 +0000",Re: Guidance for becoming Spark contributor,"Nitin Mathur <ntnmathur@gmail.com>, dev@spark.apache.org","Have you reviewed this guide?

https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark

Nick


"
Suraj Shetiya <surajshetiya@gmail.com>,"Sat, 11 Apr 2015 07:27:46 +0530",Query regarding infering data types in pyspark,dev@spark.apache.org,"Hi,

In pyspark when if I read a json file using sqlcontext I find that the date
field is not infered as date instead it is converted to string. And when I
try to convert it to date using df.withColumn(df.DateCol.cast(""timestamp""))
it does not parse it successfuly and adds a null instead there. Should I
use UDF to convert the date ? Is this expected behaviour (not throwing an
error after failure to cast all fields)?

-- 
Regards,
Suraj
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sat, 11 Apr 2015 03:24:58 +0000",wait time between start master and start slaves,Spark dev list <dev@spark.apache.org>,"Check this out
<https://github.com/mesos/spark-ec2/blob/f0a48be1bb5aaeef508619a46065648beb8f1d92/spark-standalone/setup.sh#L26-L33>
(from spark-ec2):

# Start Master$BIN_FOLDER/start-master.sh
# Pause
sleep 20
# Start Workers$BIN_FOLDER/start-slaves.sh

I know this was probably done defensively, but is there a more direct way
to know when the master is ready?

Nick
â€‹
"
Patrick Wendell <pwendell@gmail.com>,"Fri, 10 Apr 2015 23:03:31 -0700",[RESULT] [VOTE] Release Apache Spark 1.3.1 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","This vote is cancelled in favor of RC3, due to SPARK-6851.


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Fri, 10 Apr 2015 23:05:07 -0700",[VOTE] Release Apache Spark 1.3.1 (RC3),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version 1.3.1!

The tag to be voted on is v1.3.1-rc2 (commit 3e83913):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=3e8391327ba586eaf54447043bd526d919043a44

The list of fixes present in this release can be found at:
http://bit.ly/1C2nVPY

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-1.3.1-rc3/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1088/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-1.3.1-rc3-docs/

The patches on top of RC2 are:
[SPARK-6851] [SQL] Create new instance for each converted parquet relation
[SPARK-5969] [PySpark] Fix descending pyspark.rdd.sortByKey.
[SPARK-6343] Doc driver-worker network reqs
[SPARK-6767] [SQL] Fixed Query DSL error in spark sql Readme
[SPARK-6781] [SQL] use sqlContext in python shell
[SPARK-6753] Clone SparkConf in ShuffleSuite tests
[SPARK-6506] [PySpark] Do not try to retrieve SPARK_HOME when not needed...

Please vote on releasing this package as Apache Spark 1.3.1!

The vote is open until Tuesday, April 14, at 07:00 UTC and passes
if a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.3.1
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

---------------------------------------------------------------------


"
Davies Liu <davies@databricks.com>,"Fri, 10 Apr 2015 23:26:08 -0700",Re: Query regarding infering data types in pyspark,Suraj Shetiya <surajshetiya@gmail.com>,"What's the format you have in json file?


---------------------------------------------------------------------


"
Suraj Shetiya <surajshetiya@gmail.com>,"Sat, 11 Apr 2015 12:16:45 +0530",Re: Query regarding infering data types in pyspark,Davies Liu <davies@databricks.com>,"Hi,

Below is one line from the json file.
I have highlighted the field that represents the date.

""YEAR"":2015,""QUARTER"":1,""MONTH"":1,""DAY_OF_MONTH"":31,""DAY_OF_WEEK"":6,
*""FL_DATE"":""2015-01-31""*,""UNIQUE_CARRIER"":""NK"",""AI
 RLINE_ID"":20416,""CARRIER"":""NK"",""TAIL_NUM"":""N614NK"",""FL_
NUM"":126,""ORIGIN_AIRPORT_ID"":11697,""ORIGIN_AIRPORT_SEQ_ID"":1169
 703,""ORIGIN_CITY_MARKET_ID"":32467,""ORIGIN"":""FLL"",""ORIGIN_CITY_NAME"":""Fort
Lauderdale, FL"",""ORIGIN_STATE_ABR"":""FL"",""ORI    GIN_STATE_FIPS"":12,""ORIGIN_
STATE_NM"":""Florida"",""ORIGIN_WAC"":33,""DEST_AIRPORT_ID"":
13577,""DEST_AIRPORT_SEQ_ID"":1357702,""    DEST_CITY_MARKET_ID"":31135,""
DEST"":""MYR"",""DEST_CITY_NAME"":""Myrtle Beach,
SC"",""DEST_STATE_ABR"":""SC"",""DEST_STATE_FIPS"":45
   ,""DEST_STATE_NM"":""South Carolina"",""DEST_WAC"":37,""CRS_
DEP_TIME"":2010,""DEP_TIME"":2009.0,""DEP_DELAY"":-1.0,""DEP_DELAY_NEW""
 :0.0,""DEP_DEL15"":0.0,""DEP_DELAY_GROUP"":-1.0,""DEP_TIME_
BLK"":""2000-2059"",""TAXI_OUT"":17.0,""WHEELS_OFF"":2026.0,""WHEELS_ON""
 :2147.0,""TAXI_IN"":5.0,""CRS_ARR_TIME"":2149,""ARR_TIME"":
2152.0,""ARR_DELAY"":3.0,""ARR_DELAY_NEW"":3.0,""ARR_DEL15"":0.
0,""ARR_DELAY_GROUP"":0.0,""ARR_TIME_BLK"":""2100-2159"",""Unnamed: 47"":null}

Please let me know if you need access to the dataset.





-- 
Regards,
Suraj
"
Dmitriy Setrakyan <dsetrakyan@apache.org>,"Sat, 11 Apr 2015 01:14:12 -0700",Integrating Spark with Ignite File System,dev@spark.apache.org,"Hello Everyone,

I am one of the committers to Apache Ignite and have noticed some talks on
this dev list about integrating Ignite In-Memory File System (IgniteFS)
with Spark. We definitely like the idea. If you have any questions about
Apache Ignite at all, feel free to forward them to the Ignite dev list. We
are going to be monitoring this list as well.

Ignite mailing list: dev-subscribe@ignite.incubator.apache.org

Regards,
Dmitriy
"
Reynold Xin <rxin@databricks.com>,"Sat, 11 Apr 2015 01:28:53 -0700",Re: Integrating Spark with Ignite File System,Dmitriy Setrakyan <dsetrakyan@apache.org>,"Welcome, Dmitriy, to the Spark dev list!



"
Devl Devel <devl.development@gmail.com>,"Sat, 11 Apr 2015 10:13:30 +0100",Re: Integrating Spark with Ignite File System,Reynold Xin <rxin@databricks.com>,"Hi Dmitriy,

Thanks for the input, I think as per my previous email it would be good to
have a bridge project that for example, creates a IgniteFS RDD, similar to
the JDBC or HDFS one in which we can extract blocks and populate RDD
partitions, I'll post this proposal on your list.

Thanks
Devl




"
Sean Owen <sowen@cloudera.com>,"Sat, 11 Apr 2015 16:08:50 +0100",Re: [VOTE] Release Apache Spark 1.3.1 (RC3),Patrick Wendell <pwendell@gmail.com>,"+1 same result as last time.


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Sat, 11 Apr 2015 17:08:01 +0000 (UTC)",Re: [VOTE] Release Apache Spark 1.3.1 (RC3),"Patrick Wendell <pwendell@gmail.com>,  <dev@spark.apache.org>","+1
















Please vote on releasing the following candidate as Apache Spark version 1.3.1!

The tag to be voted on is v1.3.1-rc2 (commit 3e83913):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=3e8391327ba586eaf54447043bd526d919043"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Sat, 11 Apr 2015 11:41:58 -0700",Re: wait time between start master and start slaves,Nicholas Chammas <nicholas.chammas@gmail.com>,"Yeah from what I remember it was set defensively. I don't know of a good
way to check if the master is up though. I guess we could poll the Master
Web UI and see if we get a 200/ok response

Shivaram


b8f1d92/spark-standalone/setup.sh#L26-L33
"
Krishna Sankar <ksankar42@gmail.com>,"Sat, 11 Apr 2015 11:48:23 -0700",Re: [VOTE] Release Apache Spark 1.3.1 (RC3),Patrick Wendell <pwendell@gmail.com>,"+1. All tests OK (same as RC2)
Cheers
<k/>


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sat, 11 Apr 2015 21:38:35 +0000",Re: wait time between start master and start slaves,shivaram@eecs.berkeley.edu,"So basically, to tell if the master is ready to accept slaves, just poll
http://master-node:4040 for an HTTP 200 response?
â€‹


eb8f1d92/spark-standalone/setup.sh#L26-L33
y
"
Denny Lee <denny.g.lee@gmail.com>,"Sat, 11 Apr 2015 21:52:05 +0000",Re: [VOTE] Release Apache Spark 1.3.1 (RC3),"Krishna Sankar <ksankar42@gmail.com>, Patrick Wendell <pwendell@gmail.com>","+1 (non-binding)



"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Sat, 11 Apr 2015 16:33:20 -0700",Re: wait time between start master and start slaves,Nicholas Chammas <nicholas.chammas@gmail.com>,"Yeah thats the best I can think ok -- Not sure if there is a better way to
do it.


r
beb8f1d92/spark-standalone/setup.sh#L26-L33
ay
"
Ted Yu <yuzhihong@gmail.com>,"Sat, 11 Apr 2015 16:52:30 -0700",Re: wait time between start master and start slaves,Nicholas Chammas <nicholas.chammas@gmail.com>,"
  def getUIPort(conf: SparkConf): Int = {
    conf.getInt(""spark.ui.port"", SparkUI.DEFAULT_PORT)
  }
Better retrieve effective UI port before probing.

Cheers


d
er
b8f1d92/spark-standalone/setup.sh#L26-L33
"
shroffpradyumn <shroffpradyumn@berkeley.edu>,"Sat, 11 Apr 2015 16:54:20 -0700 (MST)",Integrating D3 with Spark,dev@spark.apache.org,"I'm working on adding a data-graph to the Spark jobs page (rendered by
stagePage.scala) to help users analyze the different job phases visually.

I've already made a mockup using ""dummy data"" and D3.js but I'm having some
difficulties integrating my JavaScript code with the Scala code of Spark.
Essentially, I'm not sure on how I can access the validTasks variable in
stagePage.scala from within my Javascript code such that I can use it along
with D3.js to render the data-graph.

Any help would be greatly appreciated!



--

---------------------------------------------------------------------


"
Yijie Shen <henry.yijieshen@gmail.com>,"Sun, 12 Apr 2015 13:50:21 +0800",Parquet File Binary column statistics error when reuse byte[] among rows,dev@spark.apache.org,"Hi,

Suppose I create a dataRDD which extends RDD[Row], and each row is
GenericMutableRow(Array(Int, Array[Byte])). A same Array[Byte] object is
reused among rows but has different content each time. When I convert it to
a dataFrame and save it as Parquet File, the file's row group statistic(max
& min) of Binary column would be wrong.



Here is the reason: In Parquet, BinaryStatistic just keep max & min as
parquet.io.api.Binary references, Spark sql would generate a new Binary
backed by the same Array[Byte] passed from row.
 reference backed max: Binary---------->ByteArrayBackedBinary---------->
Array[Byte]

Therefore, each time parquet updating row group's statistic, max & min
would always refer to the same Array[Byte], which has new content each
time. When parquet decides to save it into file, the last row's content
would be saved as both max & min.



It seems it is a parquet bug because it's parquet's responsibility to
update statistics correctly.
But not quite sure. Should I report it as a bug in parquet JIRA?


The spark JIRA is https://issues.apache.org/jira/browse/SPARK-6859
"
Suraj Shetiya <surajshetiya@gmail.com>,"Sun, 12 Apr 2015 11:27:28 +0530",Re: Query regarding infering data types in pyspark,Davies Liu <davies@databricks.com>,"Humble reminder





-- 
Regards,
Suraj
"
Cheng Lian <lian.cs.zju@gmail.com>,"Sun, 12 Apr 2015 23:48:52 +0800","Re: Parquet File Binary column statistics error when reuse byte[]
 among rows","Yijie Shen <henry.yijieshen@gmail.com>, dev@spark.apache.org","Thanks for reporting this! Would you mind to open JIRA tickets for both 
Spark and Parquet?

I'm not sure whether Parquet declares somewhere the user mustn't reuse 
byte arrays when using binary type. If it does, then it's a Spark bug. 
Anyway, this should be fixed.

Cheng



---------------------------------------------------------------------


"
Kay Ousterhout <kayousterhout@gmail.com>,"Sun, 12 Apr 2015 09:36:45 -0700",Re: Integrating D3 with Spark,shroffpradyumn <shroffpradyumn@berkeley.edu>,"Hi Pradyumn,

Take a look at this pull request, which does something similar: https://github.com/apache/spark/pull/2342/files 

You can put JavaScript in <script> tags in Scala. That code takes a nice approach of putting most of the JavaScript in a new file, and then just calling into it from the HTML generated by the Scala files.

-Kay

rote:
e
g
n3.nabble.com/Integrating-D3-with-Spark-tp11544.html
com.

---------------------------------------------------------------------


"
Sree V <sree_at_chess@yahoo.com.INVALID>,"Sun, 12 Apr 2015 18:08:16 +0000 (UTC)",Re: SPARK-5364,Dev <dev@spark.apache.org>,"Hi,
I was browsing through the JIRAs and found this can be closed.If anyone who has edit permissions on Spark JIRA, please close this.
https://issues.apache.org/jira/browse/SPARK-5364 
It is OpenIts Pull Request already merged
Its parent and grand parent Resolved


Thanking you.

With Regards
Sree Vaddi
"
Reynold Xin <rxin@databricks.com>,"Sun, 12 Apr 2015 11:17:46 -0700",Re: SPARK-5364,Sree V <sree_at_chess@yahoo.com>,"I closed it. Thanks.



"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sun, 12 Apr 2015 21:56:14 +0000",Re: wait time between start master and start slaves,Ted Yu <yuzhihong@gmail.com>,"Oh, good point. So I guess I should be able to query the master via code
like this before any slaves are started.


od
eb8f1d92/spark-standalone/setup.sh#L26-L33
"
Mark Hamstra <mark@clearstorydata.com>,"Sun, 12 Apr 2015 15:11:44 -0700",Re: [VOTE] Release Apache Spark 1.3.1 (RC3),Patrick Wendell <pwendell@gmail.com>,1
anshu shukla <anshushukla0@gmail.com>,"Mon, 13 Apr 2015 09:20:16 +0530",Re: Integrating D3 with Spark,Kay Ousterhout <kayousterhout@gmail.com>,"Hey  Ousterhout ,

I found its amazing .Before  this  i used to use  my own  D3.js files that
 subscribes  to the redis pub-shub  database  where  output  tuples are
being published  to the DB . So  it was already including latency  to push
 data to redis ,although  it was very less.




-- 
Thanks & Regards,
Anshu Shukla
"
Dean Chen <dean@ocirs.com>,"Sun, 12 Apr 2015 22:41:41 -0700",How is hive-site.xml loaded?,"""dev@spark.apache.org"" <dev@spark.apache.org>","The docs state that:
Configuration of Hive is done by placing your `hive-site.xml` file in
`conf/`.

I've searched the codebase for hive-site.xml and didn't find code that
specifically loaded it anywhere so it looks like there is some magic to
autoload *.xml files in /conf? I've skimmed through HiveContext
<https://github.com/apache/spark/blob/master/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveContext.scala>
and didn't see anything obvious in there.

The reason I'm asking is that I am working on a feature that needs config
in hbase-site.xml to be available in the spark context and would prefer to
follow the convention set by hive-site.xml.

--
Dean Chen
"
Reynold Xin <rxin@databricks.com>,"Sun, 12 Apr 2015 22:45:05 -0700",Re: How is hive-site.xml loaded?,Dean Chen <dean@ocirs.com>,"It is loaded by Hive's HiveConf, which simply searches for hive-site.xml on
the classpath.



"
Dean Chen <dean@ocirs.com>,"Sun, 12 Apr 2015 22:52:08 -0700",Re: How is hive-site.xml loaded?,Reynold Xin <rxin@databricks.com>,"Ah ok, thanks!

--
Dean Chen


It is loaded by Hive's HiveConf, which simply searches for hive-site.xml on
the classpath.



"
Raunak Jhawar <raunak.jhawar@gmail.com>,"Mon, 13 Apr 2015 11:40:03 +0530",Re: How is hive-site.xml loaded?,Dean Chen <dean@ocirs.com>,"The most obvious path being /etc/hive/conf, but this can be changed to
lookup for any other path.

--
Thanks,
Raunak Jhawar







"
Reynold Xin <rxin@databricks.com>,"Sun, 12 Apr 2015 23:12:14 -0700",style checker will be turned on for test code,"""dev@spark.apache.org"" <dev@spark.apache.org>","FYI once this pull request goes in, scalastyle will be run against all test
code as well. Previously we didn't run the style checker on test code.

https://github.com/apache/spark/pull/5486

We might see the master build breaking tomorrow if a PR has style
violations in test code and is tested by Jenkins before the above PR is
merged.
"
Paolo Platter <paolo.platter@agilelab.it>,"Mon, 13 Apr 2015 07:26:58 +0000",R: Integrating D3 with Spark,"anshu shukla <anshushukla0@gmail.com>, Kay Ousterhout
	<kayousterhout@gmail.com>","Hi,

I integrated charts on spark-notebook, very similar task. In order to reduce D3 boiler plate I suggest to use dimple.js.
It provides out of the box d3 based charts.

Bye

Paolo

Inviata dal mio Windows Phone
________________________________
Da: anshu shukla<mailto:anshushukla0@gmail.com>
Inviato: ý13/ý04/ý2015 05:50
A: Kay Ousterhout<mailto:kayousterhout@gmail.com>
Cc: shroffpradyumn<mailto:shroffpradyumn@berkeley.edu>; dev@spark.apache.org<mailto:dev@spark.apache.org>
Oggetto: Re: Integrating D3 with Spark

Hey  Ousterhout ,

I found its amazing .Before  this  i used to use  my own  D3.js files that
 subscribes  to the redis pub-shub  database  where  output  tuples are
being published  to the DB . So  it was already including latency  to push
 data to redis ,although  it was very less.


u>
y.
k.
n
with-Spark-tp11544.html


--
Thanks & Regards,
Anshu Shukla
"
Reynold Xin <rxin@databricks.com>,"Mon, 13 Apr 2015 00:32:47 -0700",Manning looking for a co-author for the GraphX in Action book,"""dev@spark.apache.org"" <dev@spark.apache.org>, user <user@spark.apache.org>","Hi all,

Manning (the publisher) is looking for a co-author for the GraphX in Action
book. The book currently has one author (Michael Malak), but they are
looking for a co-author to work closely with Michael and improve the
writings and make it more consumable.

Early access page for the book: http://www.manning.com/malak/

Let me know if you are interested in that. Cheers.
"
zhangxiongfei <zhangxiongfei0815@163.com>,"Mon, 13 Apr 2015 19:13:32 +0800 (CST)","=?GBK?Q?Spark_SQL_1.3.1=A1=A1""saveAsParquetFile""=A1=A1will__ou?=
 =?GBK?Q?tput__tachyon_file_with_different_block_size?=","dev@spark.apache.org, user@spark.apache.org","Hi experts
I run below code  in Spark Shell to access parquet files in Tachyon.
1.First,created a DataFrame by loading a bunch of Parquet Files in Tachyon
val ta3 =sqlContext.parquetFile(""tachyon://tachyonserver:19998/apps/tachyon/zhangxf/parquetAdClick-6p-256m"");
2.Second, set the ""fs.local.block.size"" to 256M to make sure that block size of output files in Tachyon is 256M.
   sc.hadoopConfiguration.setLong(""fs.local.block.size"",268435456)
3.Third,saved above DataFrame into Parquet files that is stored in Tachyon
  ta3.saveAsParquetFile(""tachyon://tachyonserver:19998/apps/tachyon/zhangxf/parquetAdClick-6p-256m-test"");
After above code run successfully, the output parquet files were stored in Tachyon,but these files have different block size,below is the information of those files in the path ""tachyon://tachyonserver:19998/apps/tachyon/zhangxf/parquetAdClick-6p-256m-test"":
    File Name                     Size              Block Size     In-Memory     Pin     Creation Time
 _SUCCESS                      0.00 B           256.00 MB     100%         NO     04-13-2015 17:48:23:519
_common_metadata      1088.00 B      256.00 MB     100%         NO     04-13-2015 17:48:23:741
_metadata                       22.71 KB       256.00 MB     100%         NO     04-13-2015 17:48:23:646
part-r-00001.parquet     177.19 MB     32.00 MB      100%         NO     04-13-2015 17:46:44:626
part-r-00002.parquet     177.21 MB     32.00 MB      100%         NO     04-13-2015 17:46:44:636
part-r-00003.parquet     177.02 MB     32.00 MB      100%         NO     04-13-2015 17:46:45:439
part-r-00004.parquet     177.21 MB     32.00 MB      100%         NO     04-13-2015 17:46:44:845
part-r-00005.parquet     177.40 MB     32.00 MB      100%         NO     04-13-2015 17:46:44:638
part-r-00006.parquet     177.33 MB     32.00 MB      100%         NO     04-13-2015 17:46:44:648

It seems that the API saveAsParquetFile does not distribute/broadcast the hadoopconfiguration to executors like the other API such as saveAsTextFile.The configutation ""fs.local.block.size"" only take effects on Driver.
If I set that configuration before loading parquet files,the problem is gone.
Could anyone help me verify this problem?

Thanks
Zhang Xiongfei
"
Sean McNamara <Sean.McNamara@Webtrends.com>,"Mon, 13 Apr 2015 15:22:59 +0000",Re: [VOTE] Release Apache Spark 1.3.1 (RC3),Patrick Wendell <pwendell@gmail.com>,"+1

Sean

1.3.1!
91327ba586eaf54447043bd526d919043a44
n
..


---------------------------------------------------------------------


"
Andrew Lee <alee526@hotmail.com>,"Mon, 13 Apr 2015 09:47:05 -0700","RE: Spark ThriftServer encounter
 java.lang.IllegalArgumentException: Unknown auth type: null Allowed values
 are: [auth-int, auth-conf, auth]","Cheng Lian <lian.cs.zju@gmail.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","Hi Cheng,
I couldn't find the component for Spark ThriftServer, will that be 'SQL' component?
JIRA created.https://issues.apache.org/jira/browse/SPARK-6882

ion: Unknown auth type: null Allowed values are: [auth-int, auth-conf, auth]
 
1 git commit
p 2.4.1) when starting Spark ThriftServer.
iveconf hive.server2.thrift.bind.host=$(hostname) --master yarn-client
ftCLIService (ThriftBinaryCLIService.java:run(93)) - Error:
ues are: [auth-int, auth-conf, auth]
a:56)
ies(HiveAuthFactory.java:118)
ctory(HiveAuthFactory.java:133)
un(ThriftBinaryCLIService.java:43)
 HIVE-7620 due to an older code based for the Spark ThriftServer?
er to run against a Kerberos cluster (Apache 2.4.1).
-conf'</description>
 		 	   		  "
Steve Loughran <stevel@hortonworks.com>,"Mon, 13 Apr 2015 20:04:21 +0000",Re: How is hive-site.xml loaded?,"""dev@spark.apache.org"" <dev@spark.apache.org>","There's some magic in the process that is worth knowing/being cautious of

Those special HDFSConfiguration, YarnConfiguration, HiveConf objects are all doing work in their class initializer to call Configuration.addDefaultResource

this puts their -default and -site XML files onto the list of default configuration. Hadoop then runs through the list of configuration instances it is tracking in a WeakHashmap, and, if created with the useDefaults=true option in their constructor, tells them to reload all their ""default"" config props (preserving anything set explicitly).

This means you can use/abuse this feature to force in properties onto all Hadoop Configuration instances that asked for the default values -though this doesn't guarantee the changes will be picked up.

It's generally considered best practice for apps to create an instance of the configuration classes whose defaults & site they want picked up as soon as they can. Even if you discard the instance itself. Your goal is to get those settings in, so that the defaults don't get picked up elsewhere.
-steve

 on
apache/spark/sql/hive/HiveContext.scala
ig


---------------------------------------------------------------------


"
Sree V <sree_at_chess@yahoo.com.INVALID>,"Mon, 13 Apr 2015 20:11:24 +0000 (UTC)",Re: SPARK-5364,Reynold Xin <rxin@databricks.com>,"Thank you, Raynold.Â 

Thanking you.

With Regards
Sree 


   

 I closed it. Thanks.





  "
Sree V <sree_at_chess@yahoo.com.INVALID>,"Mon, 13 Apr 2015 20:09:40 +0000 (UTC)",Re: [VOTE] Release Apache Spark 1.3.1 (RC3),"Patrick Wendell <pwendell@gmail.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","+1builds - checktests - checkinstalls and sample run - check

Thanking you.

With Regards
Sree 


   

 Please vote on releasing the following candidate as Apache Spark version 1.3.1!

The tag to be voted on is v1.3.1-rc2 (commit 3e83913):
https://git-wip"
Marcelo Vanzin <vanzin@cloudera.com>,"Mon, 13 Apr 2015 13:37:37 -0700",Re: [VOTE] Release Apache Spark 1.3.1 (RC3),Patrick Wendell <pwendell@gmail.com>,"+1 (non-binding)

Tested 2.6 build with standalone and yarn (no external shuffle service
this time, although it does come up).




-- 
Marcelo

---------------------------------------------------------------------


"
Tom Graves <tgraves_cs@yahoo.com.INVALID>,"Mon, 13 Apr 2015 21:53:38 +0000 (UTC)",Spark Sql reading hive partitioned tables?,Dev <dev@spark.apache.org>,"Hey,
I was trying out spark sql using the HiveContext and doing a select on a partitioned table with lots of partitions (16,000+). It took over 6 minutes before it even started the job. It looks like it was querying the Hive metastore and got a good chunk of data back. Â Which I'm guessing is info on the partitions. Â Running the same query using hive takes 45 seconds for the entire job.Â 
I know spark sql doesn't support all the hive optimization. Â Is this a known limitation currently? Â 
Thanks,Tom"
Michael Armbrust <michael@databricks.com>,"Mon, 13 Apr 2015 15:27:46 -0700",Re: Spark Sql reading hive partitioned tables?,Tom Graves <tgraves_cs@yahoo.com>,"Yeah, we don't currently push down predicates into the metastore.  Though,
we do prune partitions based on predicates (so we don't read the data).


"
Sean Owen <sowen@cloudera.com>,"Mon, 13 Apr 2015 23:59:00 +0100","Streamline contribution process with update to Contribution wiki,
 JIRA rules",dev <dev@spark.apache.org>,"Pardon, I wanted to call attention to a JIRA I just created...

https://issues.apache.org/jira/browse/SPARK-6889

... in which I propose what I hope are some changes to the
contribution process wiki that could help a bit with the flood of
reviews and PRs. I'd be grateful for your thoughts and comments there,
as it's my current pet issue.

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Mon, 13 Apr 2015 16:05:33 -0700","Re: Streamline contribution process with update to Contribution wiki,
 JIRA rules",Sean Owen <sowen@cloudera.com>,"Would just like to encourage everyone who is active in day-to-day
development to give feedback on this (and I will do same). Sean has
spent a lot of time looking through different ways we can streamline
our dev process.

- Patrick


---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 13 Apr 2015 23:18:58 +0000","Re: Streamline contribution process with update to Contribution wiki,
 JIRA rules","Patrick Wendell <pwendell@gmail.com>, Sean Owen <sowen@cloudera.com>","Wow, I had an open email draft to whine (yet again) about our open PR count
and provide some suggestions.

Will redirect that to the JIRA Sean created. Sweet!

Nick


"
Davies Liu <davies@databricks.com>,"Mon, 13 Apr 2015 16:29:27 -0700",Re: Query regarding infering data types in pyspark,Suraj Shetiya <surajshetiya@gmail.com>,"Hey Suraj,

You should use ""date"" for DataType:

df.withColumn(df.DateCol.cast(""date""))

Davies


---------------------------------------------------------------------


"
"""=?ISO-8859-1?B?R3VvUWlhbmcgTGk=?="" <witgo@qq.com>","Tue, 14 Apr 2015 11:35:08 +0800",Re: [VOTE] Release Apache Spark 1.3.1 (RC3),"""=?ISO-8859-1?B?UGF0cmljayBXZW5kZWxs?="" <pwendell@gmail.com>","+1 (non-binding)





------------------ Original ------------------
From:  ""Patrick Wendell"";<pwendell@gmail.com>;
Date:  Sat, Apr 11, 2015 02:05 PM
To:  ""dev@spark.apache.org""<dev@spark.apache.org>; 

Subject:  [VOTE] Release Apache Spark 1.3"
Yijie Shen <henry.yijieshen@gmail.com>,"Tue, 14 Apr 2015 11:57:27 +0800","Eliminate partition filters in execution.Filter after filter
 pruning",dev@spark.apache.org,"Hi,

Suppose I have a table t(id: String, event: String) saved as parquet file, and have directory hierarchy: Â 
hdfs://path/to/data/root/dt=2015-01-01/hr=00
After partition discovery, the result schema should be (id: String,Â event: String, dt: String, hr: Int)

If I have a query like:

df.select($â€œidâ€).filter(event match).filter($â€œdtâ€ > â€œ2015-01-01â€).filter($â€hrâ€ > 13)

In current implementation, after (dt > 2015-01-01 && hr >13) is used to filter partitions,Â 
these two filters remains in execution plan and result in each row returned from parquet add two fields dt & hr each time, Â 
which I think is useless, if we could rewrite execution.Filterâ€™s predicate and eliminate them.

Whatâ€™s your opinion? Is it a general assumption or itâ€™s just my jobâ€™s specific requirement? Â 

If itâ€™s a general one, I would love to discuss further about the implementations.Â 
If specific, I would just make my own workaround :)

â€”Â 
Best Regards!
Yijie Shen"
Kannan Rajah <krajah@maprtech.com>,"Mon, 13 Apr 2015 22:01:24 -0700",Using memory mapped file for shuffle,dev <dev@spark.apache.org>,"DiskStore.getBytes uses memory mapped files if the length is more than a
configured limit. This code path is used during map side shuffle in
ExternalSorter. I want to know if its possible for the length to exceed the
limit in the case of shuffle. The reason I ask is in the case of Hadoop,
each map task is supposed to produce only data that can fit within the
task's configured max memory. Otherwise it will result in OOM. Is the
behavior same in Spark or the size of data generated by a map task can
exceed what can be fitted in memory.

  if (length < minMemoryMapBytes) {
    val buf = ByteBuffer.allocate(length.toInt)
    ....
  } else {
    Some(channel.map(MapMode.READ_ONLY, offset, length))
  }

--
Kannan
"
<doovsaid@sina.com>,"Tue, 14 Apr 2015 13:48:25 +0800 ",How to connect JDBC DB based on Spark Sql,"""dev"" <dev@spark.apache.org>","Hi all,
According to the official document, SparkContext can load datatable to dataframe using the DataSources API. However, it just supports the following properties:Property NameMeaningurlThe JDBC URL to connect to.dbtableThe JDBC table that should be read. Note that anything that is valid in a `FROM` clause of a SQL query can be used. For example, instead of a full table you could also use a subquery in parentheses.driverThe class name of the JDBC driver needed to connect to this URL. This class with be loaded on the master and workers before running an JDBC commands to allow the driver to register itself with the JDBC subsystem.partitionColumn, lowerBound, upperBound, numPartitionsThese options must all be specified if any of them is specified. They describe how to partition the table when reading in parallel from multiple workers. partitionColumn must be a numeric column from the table in question.It lets me confused how to pass the username, password or other info? BTW, I am connecting to Postgresql like this:    val dataFrame = sqlContext.load(""jdbc"", Map(      ""url"" -> ""jdbc:postgresql://192.168.1.110:5432/demo"",  //how to pass username and password?      ""driver"" -> ""org.postgresql.Driver"",      ""dbtable"" -> ""schema.tab_users""    ))
Thanks.
RegardsYi



"
Augustin Borsu <augustin@sagacify.com>,"Tue, 14 Apr 2015 08:14:55 +0200",Re: How to connect JDBC DB based on Spark Sql,doovsaid@sina.com,"Hello Yi,

You can actually pass the username and password in the url. E.g.

val url = ""
jdbc:postgresql://ip.ip.ip.ip/ow-feeder?user=MY_LOGIN&password=MY_PASSWORD""
val query = ""(SELECT * FROM \""YadaYada\"" WHERE type='item' LIMIT 100) as
MY_DB""

val jdbcDF = sqlContext.load(""jdbc"", Map( ""url"" -> url, dbtable"" -> query))


"
Sree V <sree_at_chess@yahoo.com.INVALID>,"Tue, 14 Apr 2015 06:25:19 +0000 (UTC)","Re: Streamline contribution process with update to Contribution
 wiki, JIRA rules","Nicholas Chammas <nicholas.chammas@gmail.com>, 
	Patrick Wendell <pwendell@gmail.com>, Sean Owen <sowen@cloudera.com>","Hi Sean,
This is not the first time, I am hearing it.
I agree with JIRA suggestion.
In most of the companies that I worked, we have 'no status', 'no type', when a jira is created. And we set both, in sprint planning meetings.

I am not sure, how easy it would be for apache jira.Â  As any change might effect every apache project.Â 
Thanking you.

With Regards
Sree 


   

 Wow, I had an open email draft to whine (yet again) about our open PR count
and provide some suggestions.

Will redirect that to the JIRA Sean created. Sweet!

Nick




  "
<doovsaid@sina.com>,"Tue, 14 Apr 2015 14:31:33 +0800 ",=?GBK?B?u9i4tKO6UmU6IEhvdyB0byBjb25uZWN0IEpEQkMgREIgYmFzZWQgb24gU3BhcmsgU3Fs?=,"""Augustin Borsu"" <augustin@sagacify.com>","Great! It works. Thanks.
Best,Yi




----- Ô­Ê¼ÓÊ¼þ -----
·¢¼þÈË£ºAugustin Borsu <augustin@sagacify.com>
ÊÕ¼þÈË£ºdoovsaid@sina.com
³­ËÍÈË£ºdev <dev@spark.apache.org>
Ö÷Ìâ£ºRe: How to connect JDBC DB based on Spark Sql
ÈÕÆÚ£º2015Äê04ÔÂ14ÈÕ 14µã14·Ö

Hello Yi,
You can actually pass the username and password in the url. E.g.
val url = ""
jdbc:postgresql://ip.ip.ip.ip/ow-feeder?user=MY_LOGIN&password=MY_PASSWORD""
val query = ""(SELECT * FROM \""YadaYada\"" WHERE type='item' LIMIT 100) as
MY_DB""
val jdbcDF = sqlContext.load(""jdbc"", Map( ""url"" -> url, dbtable"" -> query))
On Tue, Apr 14, 2015 at 7:48 AM, <doovsaid@sina.com> wrote:
> Hi all,
> According to the official document, SparkContext can load datatable to
> dataframe using the DataSources API. However, it just supports the
> following properties:Property NameMeaningurlThe JDBC URL to connect
> to.dbtableThe JDBC table that should be read. Note that anything that is
> valid in a `FROM` clause of a SQL query can be used. For example, instead
> of a full table you could also use a subquery in parentheses.driverThe
> class name of the JDBC driver needed to connect to this URL. This class
> with be loaded on the master and workers before running an JDBC commands to
> allow the driver to register itself with the JDBC
> subsystem.partitionColumn, lowerBound, upperBound, numPartitionsThese
> options must all be specified if any of them is specified. They describe
> how to partition the table when reading in parallel from multiple workers.
> partitionColumn must be a numeric column from the table in question.It lets
> me confused how to pass the username, password or other info? BTW, I am
> connecting to Postgresql like this:    val dataFrame =
> sqlContext.load(""jdbc"", Map(      ""url"" -> ""jdbc:postgresql://
> 192.168.1.110:5432/demo"",  //how to pass username and password?
> ""driver"" -> ""org.postgresql.Driver"",      ""dbtable"" -> ""schema.tab_users""
>   ))
> Thanks.
> RegardsYi
>
>
>
>
"
"""A.M.Chan"" <kaka_1992@163.com>","Tue, 14 Apr 2015 19:34:44 +0800 (CST)",SQL: Type mismatch when using codegen,dev@spark.apache.org,"When I run tests in DataFrameSuite with codegen on, some type mismatched error occured.
test(""average"")

{ checkAnswer( decimalData.agg(avg('a)), Row(new java.math.BigDecimal(2.0))) }

type mismatch;
found : Int(0)


required: org.apache.spark.sql.types.DecimalType#JvmType


JIRA: https://issues.apache.org/jira/browse/SPARK-6899





--

A.M.Chan"
Imran Rashid <irashid@cloudera.com>,"Tue, 14 Apr 2015 08:47:22 -0500",Re: Using memory mapped file for shuffle,Kannan Rajah <krajah@maprtech.com>,"That limit doesn't have anything to do with the amount of available
memory.  Its just a tuning parameter, as one version is more efficient for
smaller files, the other is better for bigger files.  I suppose the comment
is a little better in FileSegmentManagedBuffer:

https://github.com/apache/spark/blob/master/network/common/src/main/java/org/apache/spark/network/buffer/FileSegmentManagedBuffer.java#L62


"
Theodore Vasiloudis <theodoros.vasiloudis@gmail.com>,"Tue, 14 Apr 2015 06:54:59 -0700 (MST)",RE: Regularization in MLlib,dev@spark.apache.org,"Hello DB, 

could you elaborate a bit on how you are currently fixing this for the new
ML pipeline framework?
Are there any JIRAs/PR we could follow?

Regards,
Theodore



--

---------------------------------------------------------------------


"
JaeSung Jun <jaesjun@gmail.com>,"Wed, 15 Apr 2015 00:13:19 +1000",DDL parser class parsing DDL in spark-sql cli,dev@spark.apache.org,"Hi,

Wile I've been walking through spark-sql source code, I typed the following
HiveQL:

CREATE EXTERNAL TABLE user (uid STRING, age INT, gender STRING, job STRING,
ts STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LOCATION
'/hive/user';

, and I finally came across ddl.scala after analysing a couple of scala
files.

What I found here is createTable method, which doesn't parse the above
statement.
Please let me know which class is responsible for parsing the above
statement.

Thanks
Jason
"
Yijie Shen <henry.yijieshen@gmail.com>,"Tue, 14 Apr 2015 22:29:46 +0800","Re: Eliminate partition filters in execution.Filter after
 filter pruning",dev@spark.apache.org,"Iâ€™ve opened a PR on this:Â https://github.com/apache/spark/pull/5509


Hi,

Suppose I have a table t(id: String, event: String) saved as parquet file, and have directory hierarchy: Â 
hdfs://path/to/data/root/dt=2015-01-01/hr=00
After partition discovery, the result schema should be (id: String,Â event: String, dt: String, hr: Int)

If I have a query like:

df.select($â€œidâ€).filter(event match).filter($â€œdtâ€ > â€œ2015-01-01â€).filter($â€hrâ€ > 13)

In current implementation, after (dt > 2015-01-01 && hr >13) is used to filter partitions,Â 
these two filters remains in execution plan and result in each row returned from parquet add two fields dt & hr each time, Â 
which I think is useless, if we could rewrite execution.Filterâ€™s predicate and eliminate them.

Whatâ€™s your opinion? Is it a general assumption or itâ€™s just my jobâ€™s specific requirement? Â 

If itâ€™s a general one, I would love to discuss further about the implementations.Â 
If specific, I would just make my own workaround :)

â€”Â 
Best Regards!
Yijie Shen"
Sean Owen <sowen@cloudera.com>,"Tue, 14 Apr 2015 16:02:55 +0100","Fwd: [jira] [Commented] (SPARK-6889) Streamline contribution process
 with update to Contribution wiki, JIRA rules",dev <dev@spark.apache.org>,"Bringing a discussion to dev@. I think the general questions on the table are:

- Should more changes be rejected? What are the pros/cons of that?
- If no, how do you think about the very large backlog of PRs and JIRAs?
- What should be rejected and why?
- How much support is there for proactively cleaning house now? What
would you close and why?
- What steps can be taken to prevent people from wasting time on JIRAs
/ PRs that will be rejected?
- What if anything does this tell us about the patterns of project
planning to date and what can we learn?

This overlaps with other discussion on SPARK-6889 but per Nicholas
wanted to surface this

---------- Forwarded message ----------
From: Nicholas Chammas (JIRA) <jira@apache.org>
Date: Tue, Apr 14, 2015 at 3:38 PM
Subject: [jira] [Commented] (SPARK-6889) Streamline contribution
process with update to Contribution wiki, JIRA rules
To: issues@spark.apache.org


Nicholas Chammas commented on SPARK-6889:
-----------------------------------------

{quote}
I also agree that most projects don't say ""no"" enough and it's
actually bad for everyone. Yes, one goal was to also set more
expectation that lots of changes are rejected. If there is widespread
agreement, I'd also like firmer language in the guide. As you say it
is also a matter of taste and culture, but, I'd personally favor a lot
more ""no"".
{quote}

Regarding this point about culture, should we have some kind of
discussion on the dev list to nudge people in the right direction?

---------------------------------------------------------------------


"
Burak Yavuz <brkyvz@gmail.com>,"Tue, 14 Apr 2015 08:38:51 -0700","Re: Fwd: [jira] [Commented] (SPARK-6889) Streamline contribution
 process with update to Contribution wiki, JIRA rules",Sean Owen <sowen@cloudera.com>,"Hi Sean and fellow devs,
I also wanted to chime in and remind people of <spark-packages.org>. Just
because the work of someone doesn't fit into the broader scope of things,
devs should be encouraged to showcase their hard work in Spark Packages.
We have been working hard to make it easier for devs to share their work
and users to access it in Spark Packages. There are some great datasource
connectors, ml algorithms, streaming connectors in there already. I would
urge devs and users to check it out!

Best,
Burak

"
Sandy Ryza <sandy.ryza@cloudera.com>,"Tue, 14 Apr 2015 09:07:21 -0700",Re: Using memory mapped file for shuffle,Imran Rashid <irashid@cloudera.com>,"Hi Kannan,

Both in MapReduce and Spark, the amount of shuffle data a task produces can
exceed the tasks memory without risk of OOM.

-Sandy


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 14 Apr 2015 16:40:13 +0000","Re: Fwd: [jira] [Commented] (SPARK-6889) Streamline contribution
 process with update to Contribution wiki, JIRA rules","Burak Yavuz <brkyvz@gmail.com>, Sean Owen <sowen@cloudera.com>","That's a great point, Burak. I think we are still figuring out when and how
to redirect people when their work doesn't fit the main Spark repo, and
Spark Packages should be a good destination in some cases.

Btw, my comment on JIRA (which Sean referenced) regarding rejecting more
patches is here
<https://issues.apache.org/jira/browse/SPARK-6889?focusedCommentId=14493394&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14493394>
.

Nick


"
Cheng Lian <lian.cs.zju@gmail.com>,"Wed, 15 Apr 2015 00:50:33 +0800","Re: Spark ThriftServer encounter java.lang.IllegalArgumentException:
 Unknown auth type: null Allowed values are: [auth-int, auth-conf, auth]","Andrew Lee <alee526@hotmail.com>, 
 ""dev@spark.apache.org"" <dev@spark.apache.org>","Yeah, SQL is the right component. Thanks!

Cheng


"
Cheng Lian <lian.cs.zju@gmail.com>,"Wed, 15 Apr 2015 00:57:24 +0800","Re: Spark SQL =?gbk?Q?1=2E3=2E1=A1=A1=22saveAsParquetFile=22=A1=A1?=
 =?gbk?Q?will__output__tachyon_file_with_different_block_?=
 =?gbk?Q?size?=","zhangxiongfei <zhangxiongfei0815@163.com>, dev@spark.apache.org, 
 user@spark.apache.org","Would you mind to open a JIRA for this?

I think your suspicion makes sense. Will have a look at this tomorrow. 
Thanks for reporting!

Cheng



---------------------------------------------------------------------


"
Cheolsoo Park <piaozhexiu@gmail.com>,"Tue, 14 Apr 2015 09:58:16 -0700",Re: Spark Sql reading hive partitioned tables?,Michael Armbrust <michael@databricks.com>,"Is there a plan to fix this? I also ran into this issue with a *""select *
from tbl where ... limit 10""* query. Spark SQL is 100x slower than Presto
in worst case (1.6M partitions table). This is a serious blocker for us
since we have many tables with near (and over) 1M partitions, and any query
against these big tables wastes 5 minutes to get full partitions info.

I briefly looked at the code, and it looks like resolving metastore
relations is the first thing that the analyzer does prior to any other
optimization rules such as partition pruning. So in the Hive metastore
client, it ends up calling getAllPartitions() with no filter expression. I
am wondering how much work will be involved to fix this issue. Can you
please advise what you think should be done?



"
Kannan Rajah <krajah@maprtech.com>,"Tue, 14 Apr 2015 10:15:03 -0700",Re: Using memory mapped file for shuffle,Sandy Ryza <sandy.ryza@cloudera.com>,"Sandy,
Can you clarify how it won't cause OOM? Is it anyway to related to memory
being allocated outside the heap - native space? The reason I ask is that I
have a use case to store shuffle data in HDFS. Since there is no notion of
memory mapped files, I need to store it as a byte buffer. I want to make
sure this will not cause OOM when the file size is large.


--
Kannan


"
Michael Armbrust <michael@databricks.com>,"Tue, 14 Apr 2015 11:11:45 -0700",Re: Spark Sql reading hive partitioned tables?,Cheolsoo Park <piaozhexiu@gmail.com>,"We can try to add this as part of some hive refactoring we are doing for
1.4.  I've created a JIRA: https://issues.apache.org/jira/browse/SPARK-6910


"
Michael Armbrust <michael@databricks.com>,"Tue, 14 Apr 2015 11:15:22 -0700",Re: DDL parser class parsing DDL in spark-sql cli,JaeSung Jun <jaesjun@gmail.com>,"HiveQL
<https://github.com/apache/spark/blob/master/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveQl.scala>


"
Michael Armbrust <michael@databricks.com>,"Tue, 14 Apr 2015 11:18:21 -0700",Re: Eliminate partition filters in execution.Filter after filter pruning,Yijie Shen <henry.yijieshen@gmail.com>,"The contract of the DataSources API is that filters are advisory and you
are allowed to ignore them
<https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala#L158>.
This is why we always evaluate them ourselves.  Have you benchmarked you
change?  Does it result in a noticeable speed up?


09
,
:
â€ >
predicate
ust my jobâ€™s
"
Davies Liu <davies@databricks.com>,"Tue, 14 Apr 2015 11:41:15 -0700","Re: extended jenkins downtime, thursday april 9th 7am-noon PDT
 (moving to anaconda python & more)",shane knapp <sknapp@berkeley.edu>,"Hey Shane,

Have you updated all the jenkins slaves?

There is a run with old configurations (no Python 3, with 130 minutes
timeout), see https://amplab.cs.berkeley.edu/jenkins/job/NewSparkPullRequestBuilder/666/consoleFull

Davies


---------------------------------------------------------------------


"
DB Tsai <dbtsai@dbtsai.com>,"Tue, 14 Apr 2015 11:44:00 -0700",Re: Regularization in MLlib,Theodore Vasiloudis <theodoros.vasiloudis@gmail.com>,"Hi Theodore,

I'm currently working on elastic-net regression in ML framework, and I
decided not to have any extra layer of abstraction for now but focus
on accuracy and performance. We may come out with proper solution
later. Any idea is welcome.

Sincerely,

DB Tsai
-------------------------------------------------------
Blog: https://www.dbtsai.com



---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Tue, 14 Apr 2015 12:35:04 -0700","Re: extended jenkins downtime, thursday april 9th 7am-noon PDT
 (moving to anaconda python & more)",Davies Liu <davies@databricks.com>,"yep, everything is installed (and i just checked again).  the path for
python 3.4 is /home/anaconda/bin/envs/py3k/bin, which you can find by
either manually prepending it to the PATH variable or running 'source
activate py3k' in the test.


"
Sree V <sree_at_chess@yahoo.com.INVALID>,"Tue, 14 Apr 2015 21:35:19 +0000 (UTC)","Re: start-slaves.sh uses local path from master on remote slave
 nodes",Dev <dev@spark.apache.org>,"https://issues.apache.org/jira/browse/SPARK-967
 Hi Team,
The reporter hasn't replied for the suggested change for this issue.
Also, there is a work around suggested.
Change or Work-A-Round, so we can close this issue ?

Thanking you.

With Regards
Sree"
Imran Rashid <irashid@cloudera.com>,"Tue, 14 Apr 2015 16:59:51 -0500",Re: Catching executor exception from executor in driver,Justin Yip <yipjustin@prediction.io>,"(+dev)

Hi Justin,

short answer: no, there is no way to do that.

I'm just guessing here, but I imagine this was done to eliminate
serialization problems (eg., what if we got an error trying to serialize
the user exception to send from the executors back to the driver?).
Though, actually that isn't a great explanation either, since even when the
info gets back to the driver, its broken into a few string fields (eg., we
have the class name of the root exception), but eventually it just gets
converted to one big string.

I've cc'ed dev b/c I think this is an oversight in Spark.  It makes it
really hard to write an app to deal gracefully with various exceptions --
all you can do is look at the string in SparkException (which could change
arbitrarily between versions, in addition to just being a pain to work
with).  We should probably add much more fine-grained subclasses of
SparkException, at the very least distinguishing errors in user code vs.
errors in spark.  I could imagine there might be a few other cases we'd
like to distinguish more carefully as well.

Any thoughts from other devs?

thanks
Imran






"
JaeSung Jun <jaesjun@gmail.com>,"Wed, 15 Apr 2015 10:45:57 +1000",Re: DDL parser class parsing DDL in spark-sql cli,Michael Armbrust <michael@databricks.com>,"Thanks Michael,

I was wondering how HiveContext.sql() is hooked up HiveQL..I'll have a look
at it.
much appreciated.

Thanks
Jason


"
Imran Rashid <irashid@cloudera.com>,"Tue, 14 Apr 2015 20:44:38 -0500","Re: [jira] [Commented] (SPARK-6889) Streamline contribution process
 with update to Contribution wiki, JIRA rules",Sean Owen <sowen@cloudera.com>,"These are great questions -- I dunno the answer to most of them, but I'll
try to at least give my take on ""What should be rejected and why?""

For new features, I'm often really confused by our guidelines on what to
include and what to exclude.  Maybe we should ask that all new features
make it clear why they should *not* just be a separate package.

them -- everyone wants all the bugs fixed.  But I think its actually a lot
harder for someone that isn't experienced with spark to fix a bug in a
clean way, when they don't know the code base.  Often the proposed fixes
are just kludges tacked on somewhere rather than addressing the real
problem.  It might help to clearly say that the most useful thing they can
do is submit bug reports with simple steps to reproduce, or even better to
submit a failing test case.  Of course submitting a patch is great too, but
we could be clear that patches would only be accepted if they fit in the
long-term design for spark.

I really feel that saying ""no"" more directly would be very helpful.
Actually I think one of the most discouraging things we can do is give a
""soft no"" -- say ""oh that sounds interesting"", but then let the PR languish.

thanks for pushing on this Sean, really useful to have this discussion.


"
zhangxiongfei  <zhangxiongfei0815@163.com>,"Wed, 15 Apr 2015 10:22:02 +0800 (CST)","=?GBK?Q?Re:Re:_Spark_SQL_1.3.1=A1=A1""saveAsParquetFile""=A1=A1will?=
 =?GBK?Q?__output__tachyon_file_with_different_block_size?=","""Cheng Lian"" <lian.cs.zju@gmail.com>","
JIRA opened:https://issues.apache.org/jira/browse/SPARK-6921




At 2015-04-15 00:57:24, ""Cheng Lian"" <lian.cs.zju@gmail.com> wrote: >Would you mind to open a JIRA for this? > >I think your suspicion makes sense. Will have a look at this tomorrow. >Thanks for reporting! > >Cheng > >On 4/13/15 7:13 PM, zhangxiongfei wrote: >> Hi experts >> I run below code in Spark Shell to access parquet files in Tachyon. >> 1.First,created a DataFrame by loading a bunch of Parquet Files in Tachyon >> val ta3 =sqlContext.parquetFile(""tachyon://tachyonserver:19998/apps/tachyon/zhangxf/parquetAdClick-6p-256m""); >> 2.Second, set the ""fs.local.block.size"" to 256M to make sure that block size of output files in Tachyon is 256M. >> sc.hadoopConfiguration.setLong(""fs.local.block.size"",268435456) >> 3.Third,saved above DataFrame into Parquet files that is stored in Tachyon >> ta3.saveAsParquetFile(""tachyon://tachyonserver:19998/apps/tachyon/zhangxf/parquetAdClick-6p-256m-test""); >> After above code run successfully, the output parquet files were stored in Tachyon,but these files have different block size,below is the information of those files in the path ""tachyon://tachyonserver:19998/apps/tachyon/zhangxf/parquetAdClick-6p-256m-test"": >> File Name Size Block Size In-Memory Pin Creation Time >> _SUCCESS 0.00 B 256.00 MB 100% NO 04-13-2015 17:48:23:519 >> _common_metadata 1088.00 B 256.00 MB 100% NO 04-13-2015 17:48:23:741 >> _metadata 22.71 KB 256.00 MB 100% NO 04-13-2015 17:48:23:646 >> part-r-00001.parquet 177.19 MB 32.00 MB 100% NO 04-13-2015 17:46:44:626 >> part-r-00002.parquet 177.21 MB 32.00 MB 100% NO 04-13-2015 17:46:44:636 >> part-r-00003.parquet 177.02 MB 32.00 MB 100% NO 04-13-2015 17:46:45:439 >> part-r-00004.parquet 177.21 MB 32.00 MB 100% NO 04-13-2015 17:46:44:845 >> part-r-00005.parquet 177.40 MB 32.00 MB 100% NO 04-13-2015 17:46:44:638 >> part-r-00006.parquet 177.33 MB 32.00 MB 100% NO 04-13-2015 17:46:44:648 >> >> It seems that the API saveAsParquetFile does not distribute/broadcast the hadoopconfiguration to executors like the other API such as saveAsTextFile.The configutation ""fs.local.block.size"" only take effects on Driver. >> If I set that configuration before loading parquet files,the problem is gone. >> Could anyone help me verify this problem? >> >> Thanks >> Zhang Xiongfei > > >--------------------------------------------------------------------- >To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org >For additional commands, e-mail: dev-help@spark.apache.org "
Patrick Wendell <pwendell@gmail.com>,"Tue, 14 Apr 2015 21:45:52 -0700",Re: [VOTE] Release Apache Spark 1.3.1 (RC3),GuoQiang Li <witgo@qq.com>,"+1 from myself as well


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Tue, 14 Apr 2015 21:55:12 -0700",Re: [VOTE] Release Apache Spark 1.2.2,Sean Owen <sowen@cloudera.com>,"+1 from me ass well.


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Tue, 14 Apr 2015 21:54:47 -0700",[RESULT] [VOTE] Release Apache Spark 1.3.1 (RC3),"""dev@spark.apache.org"" <dev@spark.apache.org>","This vote passes with 10 +1 votes (5 binding) and no 0 or -1 votes.

+1:
Sean Owen*
Reynold Xin*
Krishna Sankar
Denny Lee
Mark Hamstra*
Sean McNamara*
Sree V
Marcelo Vanzin
GuoQiang Li
Patrick Wendell*

0:

-1:

I will work on packaging this release in the next 48 hours.

- Patrick

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Tue, 14 Apr 2015 21:56:33 -0700",Re: [VOTE] Release Apache Spark 1.2.2,Sean Owen <sowen@cloudera.com>,"I'd like to close this vote to coincide with the 1.3.1 release,
however, it would be great to have more people test this release
first. I'll leave it open for a bit longer and see if others can give
a +1.


---------------------------------------------------------------------


"
Chester Chen <chester@alpinenow.com>,"Tue, 14 Apr 2015 22:05:09 -0700",Question regarding some of the changes in [SPARK-3477],"""dev@spark.apache.org"" <dev@spark.apache.org>","While working on upgrading to Spark 1.3.x, notice that the Client and
ClientArgument classes in yarn module are now defined as private[spark]. I
know that these code are mostly used by spark-submit code; but we call Yarn
client directly ( without going through spark-submit) in our spark
integration. This change essentially makes us either 1) to fork out the
code and un-change the  private prefix and build the yarn component ourself
or 2) to move the our code to org.apache.spark packages. Currently we are
using the #1) approach.

So I am curious to know if there is compelling reason to make these Yarn
Client related class private ? Any possibilities make these Client classes
non-private ?

thanks
Chester
"
Gil Vernik <GILV@il.ibm.com>,"Wed, 15 Apr 2015 08:33:16 +0300",saveAsTextFile and tmp files generations in tasks,dev <dev@spark.apache.org>,"Hi,

I run very simple operation via ./spark-shell (version 1.3.0 ):

val data = Array(1, 2, 3, 4)
val distd = sc.parallelize(data)
distd.saveAsTextFile(.. )

When i executed it, I saw that 4 tasks very created in Spark.  Each task 
created 2 temp files at different stages, there was 1st tmp file ( with 
some long name ) that at some point it was renamed to 2nd tmp file with 
another name. 
By task completion the 2nd tmp file was renamed to PART-XXXX file.  So in 
totally for 4 tasks i had about 8 tmp files..

I have some questions related those tmp files generations.
What is the logic and algorithm in tasks to generate those tmp files. Can 
someone explain it to me?  Why there were 2 tmp files ( one after another 
) and not a single tmp file? 
Is this something configurable in Spark? I mean can i run saveAsTextFile 
so tasks will run without tmp files creations? Can this tmp data be 
created in memory?

And the last one, where is the code that responsible for this?

Thanks a lot,
Gil Vernik.
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 15 Apr 2015 05:35:58 +0000",Re: wait time between start master and start slaves,Ted Yu <yuzhihong@gmail.com>,"For the record, this is what I came up with (ignoring the configurable port
for now):

spark/sbin/start-master.sh

master_ui_response_code=0
while [ ""$master_ui_response_code"" -ne 200 ]; do
    sleep 1
    master_ui_response_code=""$(
        curl --head --silent --output /dev/null \
             --write-out ""%{http_code}"" localhost:8080
    )""done

spark/sbin/start-slaves.sh

Turns out that the master typically takes 3-4 seconds to come up.

Thatâ€™s 15 seconds saved. Hurray for yak shaving!

Nick
â€‹

m>

l
beb8f1d92/spark-standalone/setup.sh#L26-L33
t
"
Imran Rashid <irashid@cloudera.com>,"Wed, 15 Apr 2015 10:18:15 -0500",Re: saveAsTextFile and tmp files generations in tasks,Gil Vernik <GILV@il.ibm.com>,"The temp file creation is controlled by a hadoop OutputCommitter, which is
normally FileOutputCommitter by default.  Its used in SparkHadoopWriter
(which in turn is used by PairRDDFunctions.saveAsHadoopDataset).

You could change the output committer to not use tmp files (eg. use this
from Aaron Davidson: https://gist.github.com/aarondav/c513916e72101bbe14ec).



"
Suraj Shetiya <surajshetiya@gmail.com>,"Wed, 15 Apr 2015 21:59:25 +0530",Re: Query regarding infering data types in pyspark,Davies Liu <davies@databricks.com>,"Thank you :)

That worked. I had another query regarding date being used as filter.

With the new df which has the column cast as date I am unable to apply a
filter that compares the dates.
The query I am using is :
df.filter(df.Datecol > datetime.date(2015,1,1)).show()

I do not want to use date as a string to compare them. Please suggest.






-- 
Regards,
Suraj
"
Davies Liu <davies@databricks.com>,"Wed, 15 Apr 2015 10:48:23 -0700",Re: Query regarding infering data types in pyspark,Suraj Shetiya <surajshetiya@gmail.com>,"It does not work now, could you file a jira for it?


---------------------------------------------------------------------


"
Tom Graves <tgraves_cs@yahoo.com.INVALID>,"Wed, 15 Apr 2015 21:40:14 +0000 (UTC)",Re: [VOTE] Release Apache Spark 1.2.2,"Patrick Wendell <pwendell@gmail.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","+1 tested on spark on yarn on hadoop 2.6 cluster with security.
Tom 


   

 Please vote on releasing the following candidate as Apache Spark version 1.2.2!

The tag to be voted on is v1.2.2-rc1 (commit 7531b50):
https://git-wip-us.apache.org/repos/asf?p="
Joseph Bradley <joseph@databricks.com>,"Wed, 15 Apr 2015 17:55:52 -0400",Re: [VOTE] Release Apache Spark 1.2.2,"""dev@spark.apache.org"" <dev@spark.apache.org>",1
Sean McNamara <Sean.McNamara@Webtrends.com>,"Wed, 15 Apr 2015 22:31:59 +0000",Re: [VOTE] Release Apache Spark 1.2.2,Patrick Wendell <pwendell@gmail.com>,"Ran tests on OS X

+1

Sean


te:
/11/15
10/28/14
ote:
on 1.2.2!
531b50e406ee2e3301b009ceea7c684272b2e27
:
/


---------------------------------------------------------------------


"
Gil Vernik <GILV@il.ibm.com>,"Thu, 16 Apr 2015 06:59:30 +0300",Re: saveAsTextFile and tmp files generations in tasks,Imran Rashid <irashid@cloudera.com>,"Thanks a lot for the info on it.
Does this explains 2 temp file generation per each task ( one temp that is 
renamed to another )? 
I understand why there is one temp file per task, but still not sure why 
there were 2 per each task,

Thanks
Gil.





From:   Imran Rashid <irashid@cloudera.com>
To:     Gil Vernik/Haifa/IBM@IBMIL
Cc:     dev <dev@spark.apache.org>
Date:   15/04/2015 06:20 PM
Subject:        Re: saveAsTextFile and tmp files generations in tasks



The temp file creation is controlled by a hadoop OutputCommitter, which is
normally FileOutputCommitter by default.  Its used in SparkHadoopWriter
(which in turn is used by PairRDDFunctions.saveAsHadoopDataset).

You could change the output committer to not use tmp files (eg. use this
from Aaron Davidson: https://gist.github.com/aarondav/c513916e72101bbe14ec
).



in
Can
another

"
Suraj Shetiya <surajshetiya@gmail.com>,"Thu, 16 Apr 2015 15:41:48 +0530",Dataframe from mysql database in pyspark,dev@spark.apache.org,"Hi,

Is there any means of transforming mysql databases into dataframes from
pyspark.
Iwas about to find a document that converts mysql database to dataframe in
spark-shell(http://www.infoobjects.com/spark-sql-jdbcrdd/) using jdbc. I
had been through the official documentation and can't find any pointers in
this regard.

-- 
Regards,
Suraj
"
vinodkc <vinod.kc.in@gmail.com>,"Thu, 16 Apr 2015 03:36:38 -0700 (MST)",spark shell paste mode is not consistent,dev@spark.apache.org,"Hi All,

I faced below issue while working with spark. It seems spark shell paste
mode is not consistent

Example code 
---------------
val textFile = sc.textFile(""README.md"")
textFile.count() 
textFile.first() 
val linesWithSpark = textFile.filter(line => line.contains(""Spark""))
textFile.filter(line => line.contains(""Spark"")).count() 

Step 1 : Run above code in spark-shell
--------

scala> val textFile = sc.textFile(""README.md"")
textFile: org.apache.spark.rdd.RDD[String] = README.md MapPartitionsRDD[1]
at textFile at <console>:21

scala> textFile.count() 
res0: Long = 98

scala> textFile.first() 
res1: String = # Apache Spark

scala> val linesWithSpark = textFile.filter(line => line.contains(""Spark""))
linesWithSpark: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at
filter at <console>:23

scala> textFile.filter(line => line.contains(""Spark"")).count() 
res2: Long = 19

Result 1: Following actions are evaluated properly
textFile.count() ,textFile.first() ,textFile.filter(line =>
line.contains(""Spark"")).count() 
res0: Long = 98,res1: String = # Apache Spark,res2: Long = 19

Step 2 : Run above code in spark-shell paste mode
scala> :p
// Entering paste mode (ctrl-D to finish)

val textFile = sc.textFile(""README.md"")
textFile.count() 
textFile.first() 
val linesWithSpark = textFile.filter(line => line.contains(""Spark""))
textFile.filter(line => line.contains(""Spark"")).count()

// Exiting paste mode, now interpreting.

textFile: org.apache.spark.rdd.RDD[String] = README.md MapPartitionsRDD[1]
at textFile at <console>:21
linesWithSpark: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at
filter at <console>:24
res0: Long = 19

scala>

textFile.filter(line => line.contains(""Spark"")).count()Â  
res0: Long = 19

Expected result : Result 1 and Result 2 should be same

I feel this is an issue with spark shell . I fixed and verified it
locally.If community also think that it need to be handled, I can make a PR.

Thanks
Vinod KC



--
3.nabble.com/spark-shell-paste-mode-is-not-consistent-tp11621.html
om.

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Thu, 16 Apr 2015 11:46:43 +0100",Re: spark shell paste mode is not consistent,vinodkc <vinod.kc.in@gmail.com>,"I'm not sure I understand what you are suggesting is wrong. It prints the
result of the last command. In the second case that is the whole pasted
block so you see 19.

"
Vinod KC <vinod.kc.in@gmail.com>,"Thu, 16 Apr 2015 16:35:02 +0530",Re: spark shell paste mode is not consistent,Sean Owen <sowen@cloudera.com>,"Hi Sean,

In paste mode , shell is evaluating only last action.It ignores previous
actions

.ie , it is  not executing  actions
textFile.count()  and textFile.first

Thanks
Vinod
 I'm not sure I understand what you are suggesting is wrong. It prints the
result of the last command. In the second case that is the whole pasted
block so you see 19.

MapPartitionsRDD[1]
line.contains(""Spark""))
at
MapPartitionsRDD[1]
at
http://apache-spark-developers-list.1001551.n3.nabble.com/spark-shell-paste-mode-is-not-consistent-tp11621.html
<http:///user/SendEmail.jtp?type=node&node=11622&i=1>
<http:///user/SendEmail.jtp?type=node&node=11622&i=2>


------------------------------
 If you reply to this email, your message will be added to the discussion
below:
http://apache-spark-developers-list.1001551.n3.nabble.com/spark-shell-paste-mode-is-not-consistent-tp11621p11622.html
 To start a new topic under Apache Spark Developers List, email
ml-node+s1001551n1h4@n3.nabble.com
<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=1&code=dmlub2Qua2MuaW5AZ21haWwuY29tfDF8MTk2Mjg4MTAzOA==>
.
NAML
<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
"
Sean Owen <sowen@cloudera.com>,"Thu, 16 Apr 2015 12:50:31 +0100",Re: spark shell paste mode is not consistent,Vinod KC <vinod.kc.in@gmail.com>,"No, look at the Spark UI. You can see all three were executed.


---------------------------------------------------------------------


"
vinodkc <vinod.kc.in@gmail.com>,"Thu, 16 Apr 2015 05:23:02 -0700 (MST)",Re: spark shell paste mode is not consistent,dev@spark.apache.org,"Yes I could see all actions in Spark UI.

Paste command is returning last action result to console, that is why I got
confused.
Thank you for the help

Vinod





--"
Reynold Xin <rxin@databricks.com>,"Thu, 16 Apr 2015 08:45:32 -0500",Re: Dataframe from mysql database in pyspark,Suraj Shetiya <surajshetiya@gmail.com>,"There is a jdbc in the SQLContext scala doc:

https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SQLContext

Note that this is more of a user list question ....


"
Reynold Xin <rxin@databricks.com>,"Thu, 16 Apr 2015 13:44:03 -0500",Re: Spark SQL queries hang forever,Michael Allman <michael@videoamp.com>,"Thanks for the information.

Adding the dev list.

Do you mind trying 1.3.1 to see if the issue is fixed?  I could be wrong
but it migtht be related to https://issues.apache.org/jira/browse/SPARK-6578




"
Sree V <sree_at_chess@yahoo.com.INVALID>,"Thu, 16 Apr 2015 18:56:22 +0000 (UTC)",how long does it takes for full build ?,Dev <dev@spark.apache.org>,"Hi Team,
How long does it takes for a full build 'mvn clean package' on spark 1.2.2-rc1 ?


Thanking you.

With Regards
Sree"
Ted Yu <yuzhihong@gmail.com>,"Thu, 16 Apr 2015 12:01:28 -0700",Re: how long does it takes for full build ?,Sree V <sree_at_chess@yahoo.com>,"You can get some idea by looking at the builds here:

https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-with-YARN/HADOOP_PROFILE=hadoop-2.4,label=centos/

Cheers


"
Kushal Datta <kushal.datta@gmail.com>,"Thu, 16 Apr 2015 12:02:08 -0700",Re: how long does it takes for full build ?,Sree V <sree_at_chess@yahoo.com>,"15-20mins.


"
Sree V <sree_at_chess@yahoo.com.INVALID>,"Thu, 16 Apr 2015 19:42:41 +0000 (UTC)",Re: how long does it takes for full build ?,Ted Yu <yuzhihong@gmail.com>,"1.
40 min+ to 1hr+, from jenkins.I didn't find the commands of the job. Does it require a login ?
Part of the console output:
 > git checkout -f 3ae37b93a7c299bd8b22a36248035bca5de3422f
 > git rev-list de4fa6b6d12e2bee0307ffba2abfca0c33f15e45 # timeout=10
Triggering Spark-Master-Maven-pre-YARN ? 2.0.0-mr1-cdh4.1.2,centos
Triggering Spark-Master-Maven-pre-YARN ? 1.0.4,centosHow to find the commands of these 'triggers' ?I am interested, whether these named triggers use -DskipTests or not.

2.
This page, gives examples all with -DskipTests only.
http://spark.apache.org/docs/1.2.0/building-spark.html

 
3.For casting VOTE to release 1.2.2-rc1,
I am running 'mvn clean package' on spark 1.2.2-rc1 with oralce jdk8_40 on centos7.This is stuck at, from last night. i.e. almost 12 hours.
...ExternalSorterSuite:
- empty data stream
- few elements per partition
- empty partitions with spilling
- empty partitions with spilling, bypass merge-sort

Any pointers ?

Thanking you.

With Regards
Sree 


   

 You can get some idea by looking at the builds here:
https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-with-YARN/HADOOP_PROFILE=hadoop-2.4,label=centos/

Cheers

Hi Team,
How long does it takes for a full build 'mvn clean package' on spark 1.2.2-rc1 ?


Thanking you.

With Regards
Sree



   "
Sree V <sree_at_chess@yahoo.com.INVALID>,"Thu, 16 Apr 2015 19:56:44 +0000 (UTC)",Re: how long does it takes for full build ?,"Ted Yu <yuzhihong@gmail.com>, 
	""sknapp@berkeley.edu"" <sknapp@berkeley.edu>","+ ShaneHi Shane,
Would you address 1. please ?Â 

Thanking you.

With Regards
Sree 


   

 1.
40 min+ to 1hr+, from jenkins.I didn't find the commands of the job. Does it require a login ?
Part of the console output:
 > git checkout -f 3ae37b93a7c299bd8b"
Ted Yu <yuzhihong@gmail.com>,"Thu, 16 Apr 2015 13:02:23 -0700",Re: how long does it takes for full build ?,Sree V <sree_at_chess@yahoo.com>,"You can find the command at the beginning of the console output:

[centos] $ /home/jenkins/tools/hudson.tasks.Maven_MavenInstallation/Maven_3.0.5/bin/mvn
-DHADOOP_PROFILE=hadoop-2.4 -Dlabel=centos -DskipTests -Phadoop-2.4
-Pyarn -Phive clean package



"
Sree V <sree_at_chess@yahoo.com.INVALID>,"Thu, 16 Apr 2015 21:51:41 +0000 (UTC)",Re: how long does it takes for full build ?,Ted Yu <yuzhihong@gmail.com>,"Found it, Ted. Thank you.https://amplab.cs.berkeley.edu/jenkins/job/Spark-1.2-Maven-pre-YARN/hadoop.version=2.0.0-mr1-cdh4.1.2,label=centos/354/consoleFull
We locally build with ""-DskipTests"" and on our jenkins as well.

Thanking you.

With Regards
Sree 


e:
   

 You can find the command at the beginning of the console output:

[centos] $ /home/jenkins/tools/hudson.tasks.Maven_MavenInstallation/Maven_3.0.5/bin/mvn
-DHADOOP_PROFILE=hadoop-2.4 -Dlabel=centos -DskipTests -Phadoop-2.4
-Pyarn -Phive clean package



=10
://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-Maven-pre-YARN/hadoop.version=2.0.0-mr1-cdh4.1.2,label=centos/>
berkeley.edu/jenkins/job/Spark-Master-Maven-pre-YARN/hadoop.version=1.0.4,label=centos/>
n
h-YARN/HADOOP_PROFILE=hadoop-2.4,label=centos/


  "
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 16 Apr 2015 22:03:26 +0000",Gitter chat room for Spark,Spark dev list <dev@spark.apache.org>,"Would we be interested in having a public chat room?

Gitter <http://gitter.im> offers them for free for open source projects.
It's like web-based IRC.

Check out the Docker room for example:

https://gitter.im/docker/docker

And if people prefer to use actual IRC, Gitter offers a bridge for that
<https://irc.gitter.im/> to their service.

All we need is someone who's a member of the Apache GitHub group to create
a room for us.

It should show up under

https://gitter.im/apache/spark

when it's ready.

What do y'all think?

Nick
"
Sree V <sree_at_chess@yahoo.com.INVALID>,"Thu, 16 Apr 2015 22:09:33 +0000 (UTC)",Re: [VOTE] Release Apache Spark 1.2.2,"Sean McNamara <Sean.McNamara@Webtrends.com>, 
	Patrick Wendell <pwendell@gmail.com>","In my effort to vote for this release, I found these along:

This is from jenkins.  It uses ""-DskipTests"".

[centos] $ /home/jenkins/tools/hudson.tasks.Maven_MavenInstallation/Maven_3.0.5/bin/mvn -Dhadoop.version=2.0.0-mr1-cdh4.1.2 -Dlabel=centos -DskipTests clean packageWe build on our locals / servers using same flag.


Usually, for releases we build with running all the tests, as well. and at some level of code coverage.

Are we by-passing it ?


Thanking you.

With Regards
Sree 


   

 Ran tests on OS X

+1

Sean




---------------------------------------------------------------------


   "
Sean Owen <sowen@cloudera.com>,"Thu, 16 Apr 2015 23:27:10 +0100",Re: [VOTE] Release Apache Spark 1.2.2,Sree V <sree_at_chess@yahoo.com>,"No, of course Jenkins runs tests. The way some of the tests work, they
need the build artifacts to have been created first. So it runs ""mvn
... -DskipTests package"" then ""mvn ... test""


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Thu, 16 Apr 2015 15:41:35 -0700",[RESULT] [VOTE] Release Apache Spark 1.2.2,"""dev@spark.apache.org"" <dev@spark.apache.org>","I'm gonna go ahead and close this now - thanks everyone for voting!

This vote passes with 7 +1 votes (6 binding) and no 0 or -1 votes.

+1:
Mark Hamstra*
Reynold Xin
Kirshna Sankar
Sean Owen*
Tom Graves*
Joseph Bradley*
Sean McNamara*

0:

-1:

Thanks!
- Patrick


---------------------------------------------------------------------


"
Michael Allman <michael@videoamp.com>,"Thu, 16 Apr 2015 16:43:33 -0700",Re: Spark SQL queries hang forever,Reynold Xin <rxin@databricks.com>,"Hi Reynold,

Our build is based off of the 1.3 branch as of commit 6d3c4d8b04b2738a821dfcc3df55a5635b89e506. Unless your fix is on another branch it appears this problem still exists.

LMK if there's anything else I can help with.

Thanks,

Michael

wrong but it migtht be related to https://issues.apache.org/jira/browse/SPARK-6578 <https://issues.apache.org/jira/browse/SPARK-6578>
that *sometimes* triggers the hang I found the following:
I can sometimes reproduce the problem.
default), I have not yet been able to reproduce the problem.
for now is setting ""spark.shuffle.blockTransferService nio"" in possible trigger was the fact that its default value changed starting in Spark 1.2. We haven't experienced a similar problem in our current Spark 1.1 cluster, and we're skipping directly to Spark 1.3. Has anyone else tried this apparent workaround?
seeing this exact problem in ShuffleBlockFetcherIterator in the query takeOrdered stage. Sometimes a single task hangs, sometimes a couple dozen. The final stage has 200 tasks. I'm using the Spark standalone scheduler, parquet files. Here's the stack trace of the stuck thread:
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:278)
org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:53)
org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1$$anonfun$7.apply(Aggregate.scala:154)
org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1$$anonfun$7.apply(Aggregate.scala:149)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
myself. If I find something that works I'll post back.
are sitting with 4 active tasks, the other has no active tasks.  Not sure if it makes a difference, but I only see an ExecutorLauncher instance via JPS on the worker that has no active tasks.  
mode):
tid=0x00007f91ec12e000 nid=0x43a3 waiting on condition [0x0000000000000000]
os_prio=0 tid=0x00007f91d02fd000 nid=0x42ee waiting on condition [0x00007f91bad9a000]
akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
tid=0x00007f91d8379000 nid=0x42e2 runnable [0x00007f91b4c54000]
sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
io.netty.channel.nio.SelectedSelectionKeySet)
java.util.Collections$UnmodifiableSet)
io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
tid=0x00007f91d80ae800 nid=0x42e1 runnable [0x00007f91b5d56000]
sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
io.netty.channel.nio.SelectedSelectionKeySet)
java.util.Collections$UnmodifiableSet)
io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
tid=0x00007f91d80ac000 nid=0x42e0 runnable [0x00007f91b5e57000]
sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
io.netty.channel.nio.SelectedSelectionKeySet)
java.util.Collections$UnmodifiableSet)
io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
tid=0x00007f91d01f7800 nid=0x42df runnable [0x00007f91b5f58000]
sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
io.netty.channel.nio.SelectedSelectionKeySet)
java.util.Collections$UnmodifiableSet)
io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
tid=0x00007f91d0773800 nid=0x427b waiting on condition [0x00007f91b6258000]
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:278)
org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:53)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1$$anonfun$7.apply(Aggregate.scala:152)
org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1$$anonfun$7.apply(Aggregate.scala:147)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
os_prio=0 tid=0x00007f91d0c86000 nid=0x41f7 waiting on condition [0x00007f91b6b5a000]
org.apache.hadoop.hdfs.PeerCache.access$000(PeerCache.java:41)
nid=0x41f2 waiting on condition [0x00007f91b6c5b000]
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
nid=0x41ef runnable [0x00007f91baf9c000]
Method)
org.apache.hadoop.net.unix.DomainSocketWatcher.access$800(DomainSocketWatcher.java:52)
org.apache.hadoop.net.unix.DomainSocketWatcher$1.run(DomainSocketWatcher.java:457)
tid=0x00007f91d8118800 nid=0x41ee waiting on condition [0x00007f91bae9b000]
io.netty.util.ThreadDeathWatcher$Watcher.run(ThreadDeathWatcher.java:137)
io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)
tid=0x0000000000961000 nid=0x41ed runnable [0x00007f91baa97000]
sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
io.netty.channel.nio.SelectedSelectionKeySet)
java.util.Collections$UnmodifiableSet)
io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
tid=0x00007f91d0019000 nid=0x41ea waiting on condition [0x00007f91bac98000]
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:278)
org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:53)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1$$anonfun$7.apply(Aggregate.scala:152)
org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1$$anonfun$7.apply(Aggregate.scala:147)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
tid=0x00007f91d0016800 nid=0x41e9 waiting on condition [0x00007f91ba16d000]
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:278)
org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:53)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1$$anonfun$7.apply(Aggregate.scala:152)
org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1$$anonfun$7.apply(Aggregate.scala:147)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
tid=0x00007f91d0015000 nid=0x41e8 waiting on condition [0x00007f91b9f6b000]
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:278)
org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:53)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1$$anonfun$7.apply(Aggregate.scala:152)
org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1$$anonfun$7.apply(Aggregate.scala:147)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
tid=0x000000000144e800 nid=0x41c7 waiting on condition [0x00007f91bb9c6000]
org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:437)
tid=0x0000000000a53800 nid=0x41c6 runnable [0x00007f91bbac7000]
sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
io.netty.channel.nio.SelectedSelectionKeySet)
java.util.Collections$UnmodifiableSet)
io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
tid=0x00007f91edcb7000 nid=0x41c3 waiting on condition [0x00007f91bbbc8000]
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
tid=0x00007f91eda6e000 nid=0x41c2 in Object.wait() [0x00007f91bbdca000]
tid=0x00007f91ed9b3000 nid=0x41c1 in Object.wait() [0x00007f91bbcc9000]
tid=0x00007f91ed92d800 nid=0x41bf waiting on condition [0x00007f91bab98000]
scala.concurrent.forkjoin.ForkJoinPool)
scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
tid=0x00007f91ed92c800 nid=0x41be waiting on condition [0x00007f91ba370000]
org.jboss.netty.util.HashedWheelTimer$Worker.waitForNextTick(HashedWheelTimer.java:467)
org.jboss.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:376)
org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
tid=0x0000000000a02000 nid=0x41bd runnable [0x00007f91b9564000]
sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
java.util.Collections$UnmodifiableSet)
org.jboss.netty.channel.socket.nio.NioServerBoss.select(NioServerBoss.java:163)
org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:206)
org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)
org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
tid=0x0000000000a00000 nid=0x41bc runnable [0x00007f91b9665000]
sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
java.util.Collections$UnmodifiableSet)
org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:64)
org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:409)
org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:206)
org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
tid=0x00000000009fe800 nid=0x41bb runnable [0x00007f91b9766000]
sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
java.util.Collections$UnmodifiableSet)
org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:64)
org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:409)
org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:206)
org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
tid=0x0000000000d00800 nid=0x41ba runnable [0x00007f91b9867000]
sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
java.util.Collections$UnmodifiableSet)
org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:64)
org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:409)
org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:206)
org.jboss.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:41)
org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
tid=0x0000000000cfe800 nid=0x41b9 runnable [0x00007f91b9968000]
sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
java.util.Collections$UnmodifiableSet)
org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:64)
org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:409)
org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:206)
org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
tid=0x00000000009b5000 nid=0x41b8 runnable [0x00007f91b9c69000]
sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
java.util.Collections$UnmodifiableSet)
org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:64)
org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:409)
org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:206)
org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
prio=5 os_prio=0 tid=0x00000000009b3800 nid=0x41b7 waiting on condition [0x00007f91b9d6a000]
akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
scala.concurrent.forkjoin.ForkJoinPool.idleAwaitWork(ForkJoinPool.java:2135)
scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2067)
scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
prio=5 os_prio=0 tid=0x0000000000c0c800 nid=0x41b6 waiting on condition [0x00007f91b9e6b000]
akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
tid=0x00007f91ed8f9800 nid=0x41b2 sleeping[0x00007f91ba26f000]
akka.actor.LightArrayRevolverScheduler.waitNanos(Scheduler.scala:226)
akka.actor.LightArrayRevolverScheduler$$anon$8.nextTick(Scheduler.scala:405)
akka.actor.LightArrayRevolverScheduler$$anon$8.run(Scheduler.scala:375)
tid=0x00007f91ec126800 nid=0x419e runnable [0x0000000000000000]
tid=0x00007f91ec109000 nid=0x419d waiting on condition [0x0000000000000000]
tid=0x00007f91ec107000 nid=0x419c waiting on condition [0x0000000000000000]
tid=0x00007f91ec105000 nid=0x419b waiting on condition [0x0000000000000000]
tid=0x00007f91ec102800 nid=0x419a runnable [0x0000000000000000]
os_prio=0 tid=0x00007f91ec101000 nid=0x4199 waiting on condition [0x0000000000000000]
nid=0x4198 in Object.wait() [0x00007f91d4254000]
java.lang.ref.ReferenceQueue$Lock)
java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:209)
tid=0x00007f91ec0c9800 nid=0x4197 in Object.wait() [0x00007f91d4355000]
java.lang.ref.Reference$ReferenceHandler.run(Reference.java:157)
waiting on condition [0x00007f91f1fdf000]
java.util.concurrent.CountDownLatch$Sync)
java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997)
java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
java.util.concurrent.CountDownLatch.await(CountDownLatch.java:231)
akka.actor.ActorSystemImpl$TerminationCallbacks.ready(ActorSystem.scala:817)
akka.actor.ActorSystemImpl$TerminationCallbacks.ready(ActorSystem.scala:786)
scala.concurrent.Await$$anonfun$ready$1.apply(package.scala:86)
scala.concurrent.Await$$anonfun$ready$1.apply(package.scala:86)
ala:53)
akka.actor.ActorSystemImpl.awaitTermination(ActorSystem.scala:642)
akka.actor.ActorSystemImpl.awaitTermination(ActorSystem.scala:643)
org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$run$1.apply$mcV$sp(CoarseGrainedExecutorBackend.scala:174)
org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:60)
org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:59)
org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
org.apache.spark.deploy.SparkHadoopUtil.runAsSparkUser(SparkHadoopUtil.scala:59)
org.apache.spark.executor.CoarseGrainedExecutorBackend$.run(CoarseGrainedExecutorBackend.scala:128)
org.apache.spark.executor.CoarseGrainedExecutorBackend$.main(CoarseGrainedExecutorBackend.scala:224)
org.apache.spark.executor.CoarseGrainedExecutorBackend.main(CoarseGrainedExecutorBackend.scala)
runnable
tid=0x00007f91ec02a000 nid=0x4191 runnable
tid=0x00007f91ec02b800 nid=0x4192 runnable
tid=0x00007f91ec02d800 nid=0x4193 runnable
tid=0x00007f91ec02f000 nid=0x4194 runnable
 nid=0x4195 runnable
nid=0x419f waiting on condition
mode):
tid=0x00007fcbfa3b6000 nid=0x44f1 waiting on condition [0x0000000000000000]
os_prio=0 tid=0x00007fcbe488b000 nid=0x4464 waiting on condition [0x00007fcbbf87f000]
akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
tid=0x00007fcbe4028800 nid=0x445f waiting on condition [0x00007fcbbf97f000]
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:278)
org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:53)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1$$anonfun$7.apply(Aggregate.scala:152)
org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1$$anonfun$7.apply(Aggregate.scala:147)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
tid=0x00007fcbfa0c0800 nid=0x4459 runnable [0x00007fcbc88c7000]
sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
io.netty.channel.nio.SelectedSelectionKeySet)
java.util.Collections$UnmodifiableSet)
io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
tid=0x00007fcbfa0c0000 nid=0x4458 runnable [0x00007fcbc7897000]
sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
io.netty.channel.nio.SelectedSelectionKeySet)
java.util.Collections$UnmodifiableSet)
io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
tid=0x00007fcbf8113800 nid=0x4457 runnable [0x00007fcbc7998000]
sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
io.netty.channel.nio.SelectedSelectionKeySet)
java.util.Collections$UnmodifiableSet)
io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
tid=0x00007fcbf810e000 nid=0x4456 runnable [0x00007fcbc7b9a000]
sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
io.netty.channel.nio.SelectedSelectionKeySet)
java.util.Collections$UnmodifiableSet)
io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
tid=0x00007fcbe48a5000 nid=0x4455 waiting on condition [0x00007fcbc4886000]
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:278)
org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:53)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1$$anonfun$7.apply(Aggregate.scala:152)
org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1$$anonfun$7.apply(Aggregate.scala:147)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
tid=0x00007fcbe4165000 nid=0x4453 waiting on condition [0x00007fcbca620000]
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:278)
org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:53)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1$$anonfun$7.apply(Aggregate.scala:152)
org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1$$anonfun$7.apply(Aggregate.scala:147)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
tid=0x00007fcbe4164000 nid=0x4452 waiting on condition [0x00007fcbc87c5000]
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:278)
org.apache.spark.sto"
spread out over roughly 2,"000 Parquet files and my queries frequently hang. Simple queries like """"select count(*) from ..."""" on the entire data set work ok.  Slightly more demanding ones with group by's and some aggregate functions (percentile_approx", avg, etc.) work ok as well, as long as I have some criteria in my where clause to keep the number of rows down.  
"
Josh Rosen <rosenville@gmail.com>,Thu"," 16 Apr 2015 16:46:26 -0700""",Python 3 support for PySpark has been merged into master,"dev <dev@spark.apache.org>, ""user@spark.apache.org"" <user@spark.apache.org>","Hi everyone,

We just merged Python 3 support for PySpark into Spark's master branch
(which will become Spark 1.4.0).  This means that PySpark now supports
Python 2.6+, PyPy 2.5+, and Python 3.4+.

To run with Python 3, download and build Spark from the master branch then
configure the PYSPARK_PYTHON environment variable to point to a Python 3.4
executable.  For example:

PYSPARK_PYTHON=python3.4 ./bin/pyspark


For more details on this feature, see the pull request and JIRA:

- https://github.com/apache/spark/pull/5173
- https://issues.apache.org/jira/browse/SPARK-4897

For Spark contributors, this change means that any open PySpark pull
requests are now likely to have merge conflicts.  If a pull request does
not have merge conflicts, we should still re-test it with Jenkins to check
that it still works under Python 3.  When backporting Python patches,
committers may wish to run the PySpark unit tests locally to make sure that
the change still work correctly in older branches.  I can also help with
backports / fixing conflicts.

Thanks to Davies Liu, Shane Knapp, Thom Neale, Xiangrui Meng, and everyone
else who helped with this patch.

- Josh
"
Akhil Das <akhil@sigmoidanalytics.com>,"Fri, 17 Apr 2015 12:11:48 +0530",Re: Gitter chat room for Spark,Nicholas Chammas <nicholas.chammas@gmail.com>,"Freenode already has a bit active channel under #Apache-spark, I think Josh
idle there sometimes.

Thanks
Best Regards


"
Sree V <sree_at_chess@yahoo.com.INVALID>,"Fri, 17 Apr 2015 07:53:56 +0000 (UTC)",Re: [RESULT] [VOTE] Release Apache Spark 1.2.2,"Patrick Wendell <pwendell@gmail.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","Sorry, I couldn't catch up before closing the voting.If it still counts, mvn package fails (1).Â  And didn't run test (2).Â  So, -1.1.mvn -Phadoop-2.4 -Pyarn -Phive -Phive-0.13.1 -Dhadoop.version=2.6.0 -DskipTests clean package
2. mvn -Phadoop-2.4 -Pyarn -Phive -Phive-0.13.1 -Dhadoop.version=2.6.0 test
Error:
[INFO] Spark Project External Flume Sink .................. SUCCESS [ 39.561 s]
[INFO] Spark Project External Flume ....................... FAILURE [ 11.212 s]
[INFO] Spark Project External MQTT ........................ SKIPPED
[INFO] Spark Project External ZeroMQ ...................... SKIPPED
[INFO] Spark Project External Kafka ....................... SKIPPED
[INFO] Spark Project Examples ............................. SKIPPED
[INFO] Spark Project YARN Shuffle Service ................. SKIPPED


Thanking you.

With Regards
Sree 


   

 I'm gonna go ahead and close this now - thanks everyone for voting!

This vote passes with 7 +1 votes (6 binding) and no 0 or -1 votes.

+1:
Mark Hamstra*
Reynold Xin
Kirshna Sankar
Sean Owen*
Tom Graves*
Joseph Bradley*
Sean McNamara*

0:

-1:

Thanks!
- Patrick

/mvn
 package
at
e:
14
=7531b50e406ee2e3301b009ceea7c684272b2e27
at:
82/
-

---------------------------------------------------------------------



  "
Sean Owen <sowen@cloudera.com>,"Fri, 17 Apr 2015 09:07:20 +0100",Re: [RESULT] [VOTE] Release Apache Spark 1.2.2,Sree V <sree_at_chess@yahoo.com>,"Sree that doesn't show any error, so it doesn't help. I built with the
same flags when I tested and it succeeded.


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Fri, 17 Apr 2015 09:38:12 +0100",Re: Gitter chat room for Spark,Akhil Das <akhil@sigmoidanalytics.com>,"There are N chat options out there, and of course there's no need or
way to stop people from using them. If 1 is blessed as 'best', it
excludes others who prefer a different one. Tomorrow there will be a
New Best Chat App. If a bunch are blessed, the conversation fractures.

There's also a principle that important-ish discussions should take
place on official project discussion forums, i.e., the mailing lists.
Chat is just chat but sometimes discussions appropriate to the list
happen there.

For this reason I've always thought it best to punt on official-ish
chat and let it happen organically as it will, with a request to port
any important discussions to the list.



---------------------------------------------------------------------


"
zhangxiongfei <zhangxiongfei0815@163.com>,"Fri, 17 Apr 2015 17:51:20 +0800 (CST)","Why does the HDFS parquet file generated by Spark SQL  have
 different size with those on Tachyon?","user <user@spark.apache.org>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,
I did some tests on Parquet Files with Spark SQL DataFrame API.
I generated 36 gzip compressed parquet files by Spark SQL and stored them on Tachyon,The size of each file is about  222M.Then read them with below code.
val tfs =sqlContext.parquetFile(""tachyon://datanode8.bitauto.dmp:19998/apps/tachyon/adClick"");
Next,I just save this DataFrame onto HDFS with below code.It will generate 36 parquet files too,but the size of each file is about 265M
tfs.repartition(36).saveAsParquetFile(""/user/zhangxf/adClick-parquet-tachyon"");
My question is Why the files on HDFS has different size with those on Tachyon even though they come from the same original data?


Thanks
Zhang Xiongfei

"
Archit Thakur <archit279thakur@gmail.com>,"Fri, 17 Apr 2015 16:07:54 +0530",Addition of new Metrics for killed executors.,"user@spark.incubator.apache.org, user@spark.apache.org, 
	dev@spark.incubator.apache.org","Hi,

We are planning to add new Metrics in Spark for the executors that got
killed during the execution. Was just curious, why this info is not already
present. Is there some reason for not adding it.?
Any ideas around are welcome.

Thanks and Regards,
Archit Thakur.
"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Fri, 17 Apr 2015 15:07:06 +0200","[Spark SQL] Java map/flatMap api broken with DataFrame in 1.3.{0,1}","""dev@spark.apache.org"" <dev@spark.apache.org>","Hi everyone,
I had an issue trying to use Spark SQL from Java (8 or 7), I tried to
reproduce it in a small test case close to the actual documentation
<https://spark.apache.org/docs/latest/sql-programming-guide.html#inferring-the-schema-using-reflection>,
so sorry for the long mail, but this is ""Java"" :

import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.sql.DataFrame;
import org.apache.spark.sql.SQLContext;

import java.io.Serializable;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;

class Movie implements Serializable {
    private int id;
    private String name;

    public Movie(int id, String name) {
        this.id = id;
        this.name = name;
    }

    public int getId() {
        return id;
    }

    public void setId(int id) {
        this.id = id;
    }

    public String getName() {
        return name;
    }

    public void setName(String name) {
        this.name = name;
    }
}

public class SparkSQLTest {
    public static void main(String[] args) {
        SparkConf conf = new SparkConf();
        conf.setAppName(""My Application"");
        conf.setMaster(""local"");
        JavaSparkContext sc = new JavaSparkContext(conf);

        ArrayList<Movie> movieArrayList = new ArrayList<Movie>();
        movieArrayList.add(new Movie(1, ""Indiana Jones""));

        JavaRDD<Movie> movies = sc.parallelize(movieArrayList);

        SQLContext sqlContext = new SQLContext(sc);
        DataFrame frame = sqlContext.applySchema(movies, Movie.class);
        frame.registerTempTable(""movies"");

        sqlContext.sql(""select name from movies"")

*                .map(row -> row.getString(0)) // this is what i would
expect to work *                .collect();
    }
}


But this does not compile, here's the compilation error :

[ERROR]
/Users/ogirardot/Documents/spark/java-project/src/main/java/org/apache/spark/MainSQL.java:[37,47]
method map in class org.apache.spark.sql.DataFrame cannot be applied to
given types;
[ERROR] *required:
scala.Function1<org.apache.spark.sql.Row,R>,scala.reflect.ClassTag<R> *
[ERROR]* found: (row)->""Na[...]ng(0) *
[ERROR] *reason: cannot infer type-variable(s) R *
[ERROR] *(actual and formal argument lists differ in length) *
[ERROR]
/Users/ogirardot/Documents/spark/java-project/src/main/java/org/apache/spark/SampleSHit.java:[56,17]
method map in class org.apache.spark.sql.DataFrame cannot be applied to
given types;
[ERROR] required:
scala.Function1<org.apache.spark.sql.Row,R>,scala.reflect.ClassTag<R>
[ERROR] found: (row)->row[...]ng(0)
[ERROR] reason: cannot infer type-variable(s) R
[ERROR] (actual and formal argument lists differ in length)
[ERROR] -> [Help 1]

Because in the DataFrame the *map *method is defined as :

[image: Images intÃ©grÃ©es 1]

And once this is translated to bytecode the actual Java signature uses a
Function1 and adds a ClassTag parameter.
I can try to go around this and use the scala.reflect.ClassTag$ like that :

ClassTag$.MODULE$.apply(String.class)

To get the second ClassTag parameter right, but then instantiating a
java.util.Function or using the Java 8 lambdas fail to work, and if I
try to instantiate a proper scala Function1... well this is a world of
pain.

This is a regression introduced by the 1.3.x DataFrame because
JavaSchemaRDD used to be JavaRDDLike but DataFrame's are not (and are
not callable with JFunctions), I can open a Jira if you want ?

Regards,

-- 
*Olivier Girardot* | AssociÃ©
o.girardot@lateral-thoughts.com
+33 6 24 09 17 94
"
Ted Yu <yuzhihong@gmail.com>,"Fri, 17 Apr 2015 07:20:19 -0700","Re: [Spark SQL] Java map/flatMap api broken with DataFrame in 1.3.{0,1}",Olivier Girardot <o.girardot@lateral-thoughts.com>,"The image didn't go through.

I think you were referring to:
  override def map[R: ClassTag](f: Row => R): RDD[R] = rdd.map(f)

Cheers


g-the-schema-using-reflection>,
pect to work *                .collect();
ark/MainSQL.java:[37,47]
ark/SampleSHit.java:[56,17]
 :
.util.Function or using the Java 8 lambdas fail to work, and if I try to instantiate a proper scala Function1... well this is a world of pain.
RDD used to be JavaRDDLike but DataFrame's are not (and are not callable with JFunctions), I can open a Jira if you want ?
"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Fri, 17 Apr 2015 14:22:34 +0000","Re: [Spark SQL] Java map/flatMap api broken with DataFrame in 1.3.{0,1}","Ted Yu <yuzhihong@gmail.com>, Olivier Girardot <o.girardot@lateral-thoughts.com>","Yes thanks !

Le ven. 17 avr. 2015 Ã  16:20, Ted Yu <yuzhihong@gmail.com> a Ã©crit :

-the-schema-using-reflection
;
ark/MainSQL.java:[37,47]
ark/SampleSHit.java:[56,17]
a
"
Reynold Xin <rxin@databricks.com>,"Fri, 17 Apr 2015 11:13:53 -0500","Re: [Spark SQL] Java map/flatMap api broken with DataFrame in 1.3.{0,1}",Olivier Girardot <o.girardot@lateral-thoughts.com>,"I think in 1.3 and above, you'd need to do

.sql(...).javaRDD().map(..)


rit :
-the-schema-using-reflection
s);
d
ark/MainSQL.java:[37,47]
to
 *
ark/SampleSHit.java:[56,17]
to
s
ry
.
ot
"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Fri, 17 Apr 2015 19:36:45 +0000","Re: [Spark SQL] Java map/flatMap api broken with DataFrame in 1.3.{0,1}","Reynold Xin <rxin@databricks.com>, Olivier Girardot <o.girardot@lateral-thoughts.com>","Ok, do you want me to open a pull request to fix the dedicated
documentation ?

Le ven. 17 avr. 2015 Ã  18:14, Reynold Xin <rxin@databricks.com> a Ã©crit :

crit :
o
g-the-schema-using-reflection
ss);
ld
park/MainSQL.java:[37,47]
park/SampleSHit.java:[56,17]
n.
"
Reynold Xin <rxin@databricks.com>,"Fri, 17 Apr 2015 14:38:43 -0500","Re: [Spark SQL] Java map/flatMap api broken with DataFrame in 1.3.{0,1}",Olivier Girardot <o.girardot@lateral-thoughts.com>,"Please do! Thanks.



Ã©crit :
©crit :
to
ng-the-schema-using-reflection
spark/MainSQL.java:[37,47]
d
spark/SampleSHit.java:[56,17]
d
R>
e
a
"
Nipun Batra <bnipun@gmail.com>,"Fri, 17 Apr 2015 12:52:03 -0700",BUG: 1.3.0 org.apache.spark.sql.Row Does not exist in Java API,dev@spark.apache.org,"Hi

The example given in SQL document
https://spark.apache.org/docs/latest/sql-programming-guide.html

org.apache.spark.sql.Row Does not exist in Java API or atleast I was not
able to find it.

Build Info - Downloaded from spark website

Dependency
                <dependency>
<groupId>org.apache.spark</groupId>
<artifactId>spark-sql_2.10</artifactId>
<version>1.3.0</version>
<scope>provided</scope>
</dependency>

Code in documentation

// Import factory methods provided by DataType.import
org.apache.spark.sql.types.DataType;// Import StructType and
StructFieldimport org.apache.spark.sql.types.StructType;import
org.apache.spark.sql.types.StructField;// Import Row.import
org.apache.spark.sql.Row;
// sc is an existing JavaSparkContext.SQLContext sqlContext = new
org.apache.spark.sql.SQLContext(sc);
// Load a text file and convert each line to a
JavaBean.JavaRDD<String> people =
sc.textFile(""examples/src/main/resources/people.txt"");
// The schema is encoded in a stringString schemaString = ""name age"";
// Generate the schema based on the string of schemaList<StructField>
fields = new ArrayList<StructField>();for (String fieldName:
schemaString.split("" "")) {
  fields.add(DataType.createStructField(fieldName,
DataType.StringType, true));}StructType schema =
DataType.createStructType(fields);
// Convert records of the RDD (people) to Rows.JavaRDD<Row> rowRDD = people.map(
  new Function<String, Row>() {
    public Row call(String record) throws Exception {
      String[] fields = record.split("","");
      return Row.create(fields[0], fields[1].trim());
    }
  });
// Apply the schema to the RDD.DataFrame peopleDataFrame =
sqlContext.createDataFrame(rowRDD, schema);
// Register the DataFrame as a
table.peopleDataFrame.registerTempTable(""people"");
// SQL can be run over RDDs that have been registered as
tables.DataFrame results = sqlContext.sql(""SELECT name FROM people"");
// The results of SQL queries are DataFrames and support all the
normal RDD operations.// The columns of a row in the result can be
accessed by ordinal.List<String> names = results.map(new Function<Row,
String>() {
  public String call(Row row) {
    return ""Name: "" + row.getString(0);
  }

}).collect();


Thanks
Nipun
"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Fri, 17 Apr 2015 20:36:28 +0000","Re: [Spark SQL] Java map/flatMap api broken with DataFrame in 1.3.{0,1}","Reynold Xin <rxin@databricks.com>, Olivier Girardot <o.girardot@lateral-thoughts.com>","Is there any convention *not* to show java 8 versions in the documentation ?

Le ven. 17 avr. 2015 Ã  21:39, Reynold Xin <rxin@databricks.com> a Ã©crit :

Ã©crit :
©crit :
)
n
ing-the-schema-using-reflection
;
/spark/MainSQL.java:[37,47]
*
/spark/SampleSHit.java:[56,17]
ke
 a
I
e
"
Reynold Xin <rxin@databricks.com>,"Fri, 17 Apr 2015 15:55:53 -0500","Re: Why does the HDFS parquet file generated by Spark SQL have
 different size with those on Tachyon?",zhangxiongfei <zhangxiongfei0815@163.com>,"It's because you did a repartition -- which rearranges all the data.

Parquet uses all kinds of compression techniques such as dictionary
encoding and run-length encoding, which would result in the size difference
when the data is ordered different.


"
Reynold Xin <rxin@databricks.com>,"Fri, 17 Apr 2015 15:58:51 -0500",Re: dataframe can not find fields after loading from hive,Cesar Flores <cesar7@gmail.com>,"This is strange. cc the dev list since it might be a bug.




"
Reynold Xin <rxin@databricks.com>,"Fri, 17 Apr 2015 15:59:50 -0500","Re: [Spark SQL] Java map/flatMap api broken with DataFrame in 1.3.{0,1}",Olivier Girardot <o.girardot@lateral-thoughts.com>,"No there isn't a convention. Although if you want to show java 8, you
should also show java 6/7 syntax since there are still more 7 users than 8.



n
Ã©crit :
©crit :
f)
d
on
ring-the-schema-using-reflection
);
e/spark/MainSQL.java:[37,47]
 *
e/spark/SampleSHit.java:[56,17]
g
"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Fri, 17 Apr 2015 21:16:57 +0000","Re: [Spark SQL] Java map/flatMap api broken with DataFrame in 1.3.{0,1}","Reynold Xin <rxin@databricks.com>, Olivier Girardot <o.girardot@lateral-thoughts.com>","another PR I guess :) here's the associated Jira
https://issues.apache.org/jira/browse/SPARK-6988

Le ven. 17 avr. 2015 Ã  23:00, Reynold Xin <rxin@databricks.com> a Ã©crit :

8.
Ã©crit :
Ã©crit :
(f)
rring-the-schema-using-reflection
();
;
he/spark/MainSQL.java:[37,47]
he/spark/SampleSHit.java:[56,17]
e
f
"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Fri, 17 Apr 2015 21:21:52 +0000","Re: [Spark SQL] Java map/flatMap api broken with DataFrame in 1.3.{0,1}","Reynold Xin <rxin@databricks.com>, Olivier Girardot <o.girardot@lateral-thoughts.com>","and the PR: https://github.com/apache/spark/pull/5564

Thank you !

Olivier.

Le ven. 17 avr. 2015 Ã  23:00, Reynold Xin <rxin@databricks.com> a Ã©crit :

8.
Ã©crit :
Ã©crit :
(f)
rring-the-schema-using-reflection
();
;
he/spark/MainSQL.java:[37,47]
he/spark/SampleSHit.java:[56,17]
e
f
"
Olivier Girardot <ssaboum@gmail.com>,"Fri, 17 Apr 2015 21:50:46 +0000",Re: BUG: 1.3.0 org.apache.spark.sql.Row Does not exist in Java API,"Nipun Batra <bnipun@gmail.com>, dev@spark.apache.org","Hi Nipun,
I'm sorry but I don't understand exactly what your problem is ?
Regarding the org.apache.spark.sql.Row, it does exists in the Spark SQL
dependency.
Is it a compilation problem ?
Are you trying to run a main method using the pom you've just described ?
or are you trying to spark-submit the jar ?
If you're trying to run a main method, the scope provided is not designed
for that and will make your program fail.

Regards,

Olivier.

Le ven. 17 avr. 2015 Ã  21:52, Nipun Batra <bnipun@gmail.com> a Ã©crit :

"
Sree V <sree_at_chess@yahoo.com.INVALID>,"Fri, 17 Apr 2015 22:14:39 +0000 (UTC)",Re: [RESULT] [VOTE] Release Apache Spark 1.2.2,Sean Owen <sowen@cloudera.com>,"Hi Sean,
This is from build log.Â  I made a master branch build earlier on this machine.Do you think, it needs a clean up of .m2 folder, that you suggested in onetime earlier ?Giving it another try, while you take a look at this.

[INFO] --- scala-maven-plugin:3.2.0:compile (scala-compile-first) @ spark-streaming-flume_2.10 ---
[WARNING] Zinc server is not available at port 3030 - reverting to normal incremental compile
[INFO] Using incremental compilation
[INFO] compiler plugin: BasicArtifact(org.scalamacros,paradise_2.10.4,2.0.1,null)
[INFO] Compiling 6 Scala sources and 1 Java source to /root/sources/github/spark/external/flume/target/scala-2
.10/classes...
[ERROR] /root/sources/github/spark/external/flume/src/main/scala/org/apache/spark/streaming/flume/FlumeBatchFe
tcher.scala:22: object Throwables is not a member of package com.google.common.base
[ERROR] import com.google.common.base.Throwables
[ERROR]Â Â Â Â Â Â Â  ^
[ERROR] /root/sources/github/spark/external/flume/src/main/scala/org/apache/spark/streaming/flume/FlumeBatchFe
tcher.scala:59: not found: value Throwables
[ERROR]Â Â Â Â Â Â Â Â Â Â  Throwables.getRootCause(e) match {
[ERROR]Â Â Â Â Â Â Â Â Â Â  ^
[ERROR] /root/sources/github/spark/external/flume/src/main/scala/org/apache/spark/streaming/flume/FlumePolling
InputDStream.scala:26: object util is not a member of package com.google.common
[ERROR] import com.google.common.util.concurrent.ThreadFactoryBuilder
[ERROR]Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  ^
[ERROR] /root/sources/github/spark/external/flume/src/main/scala/org/apache/spark/streaming/flume/FlumePolling
InputDStream.scala:69: not found: type ThreadFactoryBuilder
[ERROR]Â Â Â Â  Executors.newCachedThreadPool(new ThreadFactoryBuilder().setDaemon(true).
[ERROR]Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  ^
[ERROR] /root/sources/github/spark/external/flume/src/main/scala/org/apache/spark/streaming/flume/FlumePolling
InputDStream.scala:76: not found: type ThreadFactoryBuilder
[ERROR]Â Â Â Â  new ThreadFactoryBuilder().setDaemon(true).setNameFormat(""Flume Receiver Thread - %d"").build())
[ERROR]Â Â Â Â Â Â Â Â  ^
[ERROR] 5 errors found
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Spark Project Parent POM ........................... SUCCESS [ 15.894 s]
[INFO] Spark Project Networking ........................... SUCCESS [ 20.801 s]
[INFO] Spark Project Shuffle Streaming Service ............ SUCCESS [ 18.111 s]
[INFO] Spark Project Core ................................. SUCCESS [08:09 min]
[INFO] Spark Project Bagel ................................ SUCCESS [ 43.592 s]
[INFO] Spark Project GraphX ............................... SUCCESS [01:55 min]
[INFO] Spark Project Streaming ............................ SUCCESS [03:02 min]
[INFO] Spark Project Catalyst ............................. SUCCESS [02:59 min]
[INFO] Spark Project SQL .................................. SUCCESS [03:09 min]
[INFO] Spark Project ML Library ........................... SUCCESS [03:24 min]
[INFO] Spark Project Tools ................................ SUCCESS [ 24.816 s]
[INFO] Spark Project Hive ................................. SUCCESS [02:14 min]
[INFO] Spark Project REPL ................................. SUCCESS [01:12 min]
[INFO] Spark Project YARN Parent POM ...................... SUCCESS [Â  6.080 s]
[INFO] Spark Project YARN Stable API ...................... SUCCESS [01:27 min]
[INFO] Spark Project Assembly ............................. SUCCESS [01:22 min]
[INFO] Spark Project External Twitter ..................... SUCCESS [ 35.881 s]
[INFO] Spark Project External Flume Sink .................. SUCCESS [ 39.561 s]
[INFO] Spark Project External Flume ....................... FAILURE [ 11.212 s]
[INFO] Spark Project External MQTT ........................ SKIPPED
[INFO] Spark Project External ZeroMQ ...................... SKIPPED
[INFO] Spark Project External Kafka ....................... SKIPPED
[INFO] Spark Project Examples ............................. SKIPPED
[INFO] Spark Project YARN Shuffle Service ................. SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 32:36 min
[INFO] Finished at: 2015-04-16T23:02:18-07:00
[INFO] Final Memory: 91M/2043M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal net.alchim31.maven:scala-maven-plugin:3.2.0:compile (scala-compile-first) on pr
oject spark-streaming-flume_2.10: Execution scala-compile-first of goal net.alchim31.maven:scala-maven-plugin:
3.2.0:compile failed. CompileFailed -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/PluginExecutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]Â Â  mvn <goals> -rf :spark-streaming-flume_2.10Â 

Thanking you.

With Regards
Sree 


e:
   

 Sree that doesn't show any error, so it doesn't help. I built with the
same flags when I tested and it succeeded.

rote:
mvn package fails (1).Â  And didn't run test (2).Â  So, -1.1.mvn -Phadoop-2.4 -Pyarn -Phive -Phive-0.13.1 -Dhadoop.version=2.6.0 -DskipTests clean package
test
561 s]
212 s]


  "
Sree V <sree_at_chess@yahoo.com.INVALID>,"Fri, 17 Apr 2015 22:18:28 +0000 (UTC)",Re: [RESULT] [VOTE] Release Apache Spark 1.2.2,Sean Owen <sowen@cloudera.com>,"Hi Sean,
This is from build log.Â  I made a master branch build earlier on this machine.Do you think, it needs a clean up of .m2 folder, that you suggested in onetime earlier ?Giving it another try, while you take a look at this.

[INFO] --- scala-maven-plugin:3.2.0:compile (scala-compile-first) @ spark-streaming-flume_2.10 ---
[WARNING] Zinc server is not available at port 3030 - reverting to normal incremental compile
[INFO] Using incremental compilation
[INFO] compiler plugin: BasicArtifact(org.scalamacros,paradise_2.10.4,2.0.1,null)
[INFO] Compiling 6 Scala sources and 1 Java source to /root/sources/github/spark/external/flume/target/scala-2
.10/classes...
[ERROR] /root/sources/github/spark/external/flume/src/main/scala/org/apache/spark/streaming/flume/FlumeBatchFe
tcher.scala:22: object Throwables is not a member of package com.google.common.base
[ERROR] import com.google.common.base.Throwables
[ERROR]Â Â Â Â Â Â Â  ^
[ERROR] /root/sources/github/spark/external/flume/src/main/scala/org/apache/spark/streaming/flume/FlumeBatchFe
tcher.scala:59: not found: value Throwables
[ERROR]Â Â Â Â Â Â Â Â Â Â  Throwables.getRootCause(e) match {
[ERROR]Â Â Â Â Â Â Â Â Â Â  ^
[ERROR] /root/sources/github/spark/external/flume/src/main/scala/org/apache/spark/streaming/flume/FlumePolling
InputDStream.scala:26: object util is not a member of package com.google.common
[ERROR] import com.google.common.util.concurrent.ThreadFactoryBuilder
[ERROR]Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  ^
[ERROR] /root/sources/github/spark/external/flume/src/main/scala/org/apache/spark/streaming/flume/FlumePolling
InputDStream.scala:69: not found: type ThreadFactoryBuilder
[ERROR]Â Â Â Â  Executors.newCachedThreadPool(new ThreadFactoryBuilder().setDaemon(true).
[ERROR]Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  ^
[ERROR] /root/sources/github/spark/external/flume/src/main/scala/org/apache/spark/streaming/flume/FlumePolling
InputDStream.scala:76: not found: type ThreadFactoryBuilder
[ERROR]Â Â Â Â  new ThreadFactoryBuilder().setDaemon(true).setNameFormat(""Flume Receiver Thread - %d"").build())
[ERROR]Â Â Â Â Â Â Â Â  ^
[ERROR] 5 errors found
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Spark Project Parent POM ........................... SUCCESS [ 15.894 s]
[INFO] Spark Project Networking ........................... SUCCESS [ 20.801 s]
[INFO] Spark Project Shuffle Streaming Service ............ SUCCESS [ 18.111 s]
[INFO] Spark Project Core ................................. SUCCESS [08:09 min]
[INFO] Spark Project Bagel ................................ SUCCESS [ 43.592 s]
[INFO] Spark Project GraphX ............................... SUCCESS [01:55 min]
[INFO] Spark Project Streaming ............................ SUCCESS [03:02 min]
[INFO] Spark Project Catalyst ............................. SUCCESS [02:59 min]
[INFO] Spark Project SQL .................................. SUCCESS [03:09 min]
[INFO] Spark Project ML Library ........................... SUCCESS [03:24 min]
[INFO] Spark Project Tools ................................ SUCCESS [ 24.816 s]
[INFO] Spark Project Hive ................................. SUCCESS [02:14 min]
[INFO] Spark Project REPL ................................. SUCCESS [01:12 min]
[INFO] Spark Project YARN Parent POM ...................... SUCCESS [Â  6.080 s]
[INFO] Spark Project YARN Stable API ...................... SUCCESS [01:27 min]
[INFO] Spark Project Assembly ............................. SUCCESS [01:22 min]
[INFO] Spark Project External Twitter ..................... SUCCESS [ 35.881 s]
[INFO] Spark Project External Flume Sink .................. SUCCESS [ 39.561 s]
[INFO] Spark Project External Flume ....................... FAILURE [ 11.212 s]
[INFO] Spark Project External MQTT ........................ SKIPPED
[INFO] Spark Project External ZeroMQ ...................... SKIPPED
[INFO] Spark Project External Kafka ....................... SKIPPED
[INFO] Spark Project Examples ............................. SKIPPED
[INFO] Spark Project YARN Shuffle Service ................. SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 32:36 min
[INFO] Finished at: 2015-04-16T23:02:18-07:00
[INFO] Final Memory: 91M/2043M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal net.alchim31.maven:scala-maven-plugin:3.2.0:compile (scala-compile-first) on pr
oject spark-streaming-flume_2.10: Execution scala-compile-first of goal net.alchim31.maven:scala-maven-plugin:
3.2.0:compile failed. CompileFailed -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/PluginExecutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]Â Â  mvn <goals> -rf :spark-streaming-flume_2.10Â 

Thanking you.

With Regards
Sree 


e:
   

 Sree that doesn't show any error, so it doesn't help. I built with the
same flags when I tested and it succeeded.

rote:
mvn package fails (1).Â  And didn't run test (2).Â  So, -1.1.mvn -Phadoop-2.4 -Pyarn -Phive -Phive-0.13.1 -Dhadoop.version=2.6.0 -DskipTests clean package
test
561 s]
212 s]
:
n/mvn
n package
 at
te:
0
n
5
5
/14
=7531b50e406ee2e3301b009ceea7c684272b2e27
 at:
082/
--

---------------------------------------------------------------------



  "
Nathan Kronenfeld <nkronenfeld@uncharted.software>,"Fri, 17 Apr 2015 18:18:27 -0400",Re: Spark streaming vs. spark usage,dev <dev@spark.apache.org>,"I finally got this compiling and working, I think, but since (as Reynold
points out) it involves a little API refactoring, I was hoping to get some
discussion about it going as soon as possible.

I have the changes necessary to give RDD, DStream, and DataFrame some level
of common interface, in https://github.com/apache/spark/pull/5565, and
would very much appreciate comments.

Thanks,
                                Nathan


"
Patrick Wendell <pwendell@gmail.com>,"Fri, 17 Apr 2015 15:53:47 -0700",Announcing Spark 1.3.1 and 1.2.2,"""dev@spark.apache.org"" <dev@spark.apache.org>, ""user@spark.apache.org"" <user@spark.apache.org>","Hi All,

I'm happy to announce the Spark 1.3.1 and 1.2.2 maintenance releases.
We recommend all users on the 1.3 and 1.2 Spark branches upgrade to
these releases, which contain several important bug fixes.

Download Spark 1.3.1 or 1.2.2:
http://spark.apache.org/downloads.html

Release notes:
1.3.1: http://spark.apache.org/releases/spark-release-1-3-1.html
1.2.2:  http://spark.apache.org/releases/spark-release-1-2-2.html

Comprehensive list of fixes:
1.3.1: http://s.apache.org/spark-1.3.1
1.2.2: http://s.apache.org/spark-1.2.2

Thanks to everyone who worked on these releases!

- Patrick

---------------------------------------------------------------------


"
Sree V <sree_at_chess@yahoo.com.INVALID>,"Fri, 17 Apr 2015 23:01:53 +0000 (UTC)",Re: [RESULT] [VOTE] Release Apache Spark 1.2.2,"Sree V <sree_at_chess@yahoo.com>, Sean Owen <sowen@cloudera.com>","cleaned up ~/.m2 and ~/.zinc.
received exact same error, again. So, -1 from me.

[INFO] ------------------------------------------------------------------------
[INFO] Building Spark Project External Flume 1.2.2
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ spark-streaming-flume_2.10 ---
[INFO] Deleting /root/sources/github/spark/external/flume/target
[INFO] 
[INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-versions) @ spark-streaming-flume_2.10 ---
[INFO] 
[INFO] --- build-helper-maven-plugin:1.8:add-source (add-scala-sources) @ spark-streaming-flume_2.10 ---
[INFO] Source directory: /root/sources/github/spark/external/flume/src/main/scala added.
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ spark-streaming-flume_2.10 ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ spark-streaming-flume_2.10 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /root/sources/github/spark/external/flume/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- scala-maven-plugin:3.2.0:compile (scala-compile-first) @ spark-streaming-flume_2.10 ---
[WARNING] Zinc server is not available at port 3030 - reverting to normal incremental compile
[INFO] Using incremental compilation
[INFO] compiler plugin: BasicArtifact(org.scalamacros,paradise_2.10.4,2.0.1,null)
[INFO] Compiling 6 Scala sources and 1 Java source to /root/sources/github/spark/external/flume/target/scala-2.10/classes...
[ERROR] /root/sources/github/spark/external/flume/src/main/scala/org/apache/spark/streaming/flume/FlumeBatchFetcher.scala:22: object Throwables is not a member of package com.google.common.base
[ERROR] import com.google.common.base.Throwables
[ERROR]Â Â Â Â Â Â Â  ^
[ERROR] /root/sources/github/spark/external/flume/src/main/scala/org/apache/spark/streaming/flume/FlumeBatchFetcher.scala:59: not found: value Throwables
[ERROR]Â Â Â Â Â Â Â Â Â Â  Throwables.getRootCause(e) match {
[ERROR]Â Â Â Â Â Â Â Â Â Â  ^
[ERROR] /root/sources/github/spark/external/flume/src/main/scala/org/apache/spark/streaming/flume/FlumePollingInputDStream.scala:26: object util is not a member of package com.google.common
[ERROR] import com.google.common.util.concurrent.ThreadFactoryBuilder
[ERROR]Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  ^
[ERROR] /root/sources/github/spark/external/flume/src/main/scala/org/apache/spark/streaming/flume/FlumePollingInputDStream.scala:69: not found: type ThreadFactoryBuilder
[ERROR]Â Â Â Â  Executors.newCachedThreadPool(new ThreadFactoryBuilder().setDaemon(true).
[ERROR]Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  ^
[ERROR] /root/sources/github/spark/external/flume/src/main/scala/org/apache/spark/streaming/flume/FlumePollingInputDStream.scala:76: not found: type ThreadFactoryBuilder
[ERROR]Â Â Â Â  new ThreadFactoryBuilder().setDaemon(true).setNameFormat(""Flume Receiver Thread - %d"").build())
[ERROR]Â Â Â Â Â Â Â Â  ^
[ERROR] 5 errors found
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Spark Project Parent POM ........................... SUCCESS [01:44 min]
[INFO] Spark Project Networking ........................... SUCCESS [ 49.128 s]
[INFO] Spark Project Shuffle Streaming Service ............ SUCCESS [Â  8.503 s]
[INFO] Spark Project Core ................................. SUCCESS [05:22 min]
[INFO] Spark Project Bagel ................................ SUCCESS [ 25.647 s]
[INFO] Spark Project GraphX ............................... SUCCESS [01:13 min]
[INFO] Spark Project Streaming ............................ SUCCESS [01:29 min]
[INFO] Spark Project Catalyst ............................. SUCCESS [01:51 min]
[INFO] Spark Project SQL .................................. SUCCESS [01:57 min]
[INFO] Spark Project ML Library ........................... SUCCESS [02:25 min]
[INFO] Spark Project Tools ................................ SUCCESS [ 16.665 s]
[INFO] Spark Project Hive ................................. SUCCESS [02:03 min]
[INFO] Spark Project REPL ................................. SUCCESS [ 50.294 s]
[INFO] Spark Project YARN Parent POM ...................... SUCCESS [Â  5.777 s]
[INFO] Spark Project YARN Stable API ...................... SUCCESS [ 53.803 s]
[INFO] Spark Project Assembly ............................. SUCCESS [ 59.515 s]
[INFO] Spark Project External Twitter ..................... SUCCESS [ 40.038 s]
[INFO] Spark Project External Flume Sink .................. SUCCESS [ 32.779 s]
[INFO] Spark Project External Flume ....................... FAILURE [Â  7.936 s]
[INFO] Spark Project External MQTT ........................ SKIPPED
[INFO] Spark Project External ZeroMQ ...................... SKIPPED
[INFO] Spark Project External Kafka ....................... SKIPPED
[INFO] Spark Project Examples ............................. SKIPPED
[INFO] Spark Project YARN Shuffle Service ................. SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 23:58 min
[INFO] Finished at: 2015-04-17T15:58:27-07:00
[INFO] Final Memory: 151M/1924M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal net.alchim31.maven:scala-maven-plugin:3.2.0:compile (scala-compile-first) on project spark-streaming-flume_2.10: Execution scala-compile-first of goal net.alchim31.maven:scala-maven-plugin:3.2.0:compile failed. CompileFailed -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/PluginExecutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]Â Â  mvn <goals> -rf :spark-streaming-flume_2.10

Â 

Thanking you.

With Regards
Sree 


   

 Hi Sean,
This is from build log.Â  I made a master branch build earlier on this machine.Do you think, it needs a clean up of .m2 folder, that you suggested in onetime earlier ?Giving it another try, while you take a look at this.

[INFO] --- scala-maven-plugin:3.2.0:compile (scala-compile-first) @ spark-streaming-flume_2.10 ---
[WARNING] Zinc server is not available at port 3030 - reverting to normal incremental compile
[INFO] Using incremental compilation
[INFO] compiler plugin: BasicArtifact(org.scalamacros,paradise_2.10.4,2.0.1,null)
[INFO] Compiling 6 Scala sources and 1 Java source to /root/sources/github/spark/external/flume/target/scala-2
.10/classes...
[ERROR] /root/sources/github/spark/external/flume/src/main/scala/org/apache/spark/streaming/flume/FlumeBatchFe
tcher.scala:22: object Throwables is not a member of package com.google.common.base
[ERROR] import com.google.common.base.Throwables
[ERROR]Â Â Â Â Â Â Â  ^
[ERROR] /root/sources/github/spark/external/flume/src/main/scala/org/apache/spark/streaming/flume/FlumeBatchFe
tcher.scala:59: not found: value Throwables
[ERROR]Â Â Â Â Â Â Â Â Â Â  Throwables.getRootCause(e) match {
[ERROR]Â Â Â Â Â Â Â Â Â Â  ^
[ERROR] /root/sources/github/spark/external/flume/src/main/scala/org/apache/spark/streaming/flume/FlumePolling
InputDStream.scala:26: object util is not a member of package com.google.common
[ERROR] import com.google.common.util.concurrent.ThreadFactoryBuilder
[ERROR]Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  ^
[ERROR] /root/sources/github/spark/external/flume/src/main/scala/org/apache/spark/streaming/flume/FlumePolling
InputDStream.scala:69: not found: type ThreadFactoryBuilder
[ERROR]Â Â Â Â  Executors.newCachedThreadPool(new ThreadFactoryBuilder().setDaemon(true).
[ERROR]Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  ^
[ERROR] /root/sources/github/spark/external/flume/src/main/scala/org/apache/spark/streaming/flume/FlumePolling
InputDStream.scala:76: not found: type ThreadFactoryBuilder
[ERROR]Â Â Â Â  new ThreadFactoryBuilder().setDaemon(true).setNameFormat(""Flume Receiver Thread - %d"").build())
[ERROR]Â Â Â Â Â Â Â Â  ^
[ERROR] 5 errors found
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Spark Project Parent POM ........................... SUCCESS [ 15.894 s]
[INFO] Spark Project Networking ........................... SUCCESS [ 20.801 s]
[INFO] Spark Project Shuffle Streaming Service ............ SUCCESS [ 18.111 s]
[INFO] Spark Project Core ................................. SUCCESS [08:09 min]
[INFO] Spark Project Bagel ................................ SUCCESS [ 43.592 s]
[INFO] Spark Project GraphX ............................... SUCCESS [01:55 min]
[INFO] Spark Project Streaming ............................ SUCCESS [03:02 min]
[INFO] Spark Project Catalyst ............................. SUCCESS [02:59 min]
[INFO] Spark Project SQL .................................. SUCCESS [03:09 min]
[INFO] Spark Project ML Library ........................... SUCCESS [03:24 min]
[INFO] Spark Project Tools ................................ SUCCESS [ 24.816 s]
[INFO] Spark Project Hive ................................. SUCCESS [02:14 min]
[INFO] Spark Project REPL ................................. SUCCESS [01:12 min]
[INFO] Spark Project YARN Parent POM ...................... SUCCESS [Â  6.080 s]
[INFO] Spark Project YARN Stable API ...................... SUCCESS [01:27 min]
[INFO] Spark Project Assembly ............................. SUCCESS [01:22 min]
[INFO] Spark Project External Twitter ..................... SUCCESS [ 35.881 s]
[INFO] Spark Project External Flume Sink .................. SUCCESS [ 39.561 s]
[INFO] Spark Project External Flume ....................... FAILURE [ 11.212 s]
[INFO] Spark Project External MQTT ........................ SKIPPED
[INFO] Spark Project External ZeroMQ ...................... SKIPPED
[INFO] Spark Project External Kafka ....................... SKIPPED
[INFO] Spark Project Examples ............................. SKIPPED
[INFO] Spark Project YARN Shuffle Service ................. SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 32:36 min
[INFO] Finished at: 2015-04-16T23:02:18-07:00
[INFO] Final Memory: 91M/2043M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal net.alchim31.maven:scala-maven-plugin:3.2.0:compile (scala-compile-first) on pr
oject spark-streaming-flume_2.10: Execution scala-compile-first of goal net.alchim31.maven:scala-maven-plugin:
3.2.0:compile failed. CompileFailed -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/PluginExecutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]Â Â  mvn <goals> -rf :spark-streaming-flume_2.10Â 

Thanking you.

With Regards
Sree 


Â  

 Sree that doesn't show any error, so it doesn't help. I built with the
same flags when I tested and it succeeded.

rote:
mvn package fails (1).Â  And didn't run test (2).Â  So, -1.1.mvn -Phadoop-2.4 -Pyarn -Phive -Phive-0.13.1 -Dhadoop.version=2.6.0 -DskipTests clean package
test
561 s]
212 s]
:
n/mvn
n package
 at
te:
0
n
5
5
/14
=7531b50e406ee2e3301b009ceea7c684272b2e27
 at:
082/
--

---------------------------------------------------------------------





  "
tanejagagan <tanejagagan@yahoo.com>,"Fri, 17 Apr 2015 17:49:13 -0700 (MST)",Re: Spark development with IntelliJ,dev@spark.apache.org,"<http://apache-spark-developers-list.1001551.n3.nabble.com/file/n11668/Screenshot-1.png>
This was little bit of frustration to get this working with Intellij IDEA
14.1 Clearing the Additional Compiler option was not fixing the issueI had
to add following to Settings->Build Execution, Deployment-> Scala
CompilerAdditional compiler option -P:/Your home
directory/.m2/repository/org/scalamacros/  >
paradise_2.10.4/2.0.1/paradise_2.10.4-2.0.1.jar



--"
Olivier Girardot <ssaboum@gmail.com>,"Sat, 18 Apr 2015 07:26:33 +0000",Re: BUG: 1.3.0 org.apache.spark.sql.Row Does not exist in Java API,Nipun Batra <batranipun@gmail.com>,"Hi Nipun,
you're right, I created the pull request fixing the documentation:
https://github.com/apache/spark/pull/5569
and the corresponding issue:
https://issues.apache.org/jira/browse/SPARK-6992
Thank you for your time,

Olivier.

Le sam. 18 avr. 2015 Ã  01:11, Nipun Batra <batranipun@gmail.com> a Ã©crit :

e-frame.html).
?
d
©crit :
t
,
"
Justin Uang <justin.uang@gmail.com>,"Sun, 19 Apr 2015 17:56:49 -0400","Infinite recursion when using SQLContext#createDataFrame(JavaRDD[Row],
 java.util.List[String])",dev@spark.apache.org,"Hi,

I have a question regarding SQLContext#createDataFrame(JavaRDD[Row],
java.util.List[String]). It looks like when I try to call it, it results in
an infinite recursion that overflows the stack. I filed it here:
https://issues.apache.org/jira/browse/SPARK-6999.

What is the best way to fix this? Is the intention that it indeed calls a
scala implementation that infers the schema using the datatypes of the Rows
as well as using the provided column names?

Thanks!

Justin
"
Reynold Xin <rxin@databricks.com>,"Sun, 19 Apr 2015 15:09:23 -0700","Re: Infinite recursion when using SQLContext#createDataFrame(JavaRDD[Row],
 java.util.List[String])",Justin Uang <justin.uang@gmail.com>,"Definitely a bug. I just checked and it looks like we don't actually have a
function that takes a Scala RDD and Seq[String].

cc Davies who added this code a while back.



"
Yin Huai <yhuai@databricks.com>,"Sun, 19 Apr 2015 18:19:32 -0700",Re: dataframe can not find fields after loading from hive,Reynold Xin <rxin@databricks.com>,"Hi Cesar,

Can you try 1.3.1 (
https://spark.apache.org/releases/spark-release-1-3-1.html) and see if it
still shows the error?

Thanks,

Yin


"
"""wyphao.2007"" <wyphao.2007@163.com>","Mon, 20 Apr 2015 10:40:27 +0800 (CST)",Question about recovery from checkpoint exception[SPARK-6892],"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi, 
   When I recovery from checkpoint in yarn-cluster mode using Spark Streaming, I found it will reuse the application id (In my case is application_1428664056212_0016) before falied to write spark eventLog, But now my application id is application_1428664056212_0017,then spark write eventLog will falied, the stacktrace as follow:
15/04/14 10:14:01 WARN util.ShutdownHookManager: ShutdownHook '$anon$3' failed, java.io.IOException: Target log file already exists (hdfs://mycluster/spark-logs/eventLog/application_1428664056212_0016)
java.io.IOException: Target log file already exists (hdfs://mycluster/spark-logs/eventLog/application_1428664056212_0016)
	at org.apache.spark.scheduler.EventLoggingListener.stop(EventLoggingListener.scala:201)
	at org.apache.spark.SparkContext$$anonfun$stop$4.apply(SparkContext.scala:1388)
	at org.apache.spark.SparkContext$$anonfun$stop$4.apply(SparkContext.scala:1388)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1388)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$3.run(ApplicationMaster.scala:107)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
Is someone can help me, The issue is SPARK-6892.
thanks



"
Sean Owen <sowen@cloudera.com>,"Mon, 20 Apr 2015 04:46:12 +0100",Re: Question about recovery from checkpoint exception[SPARK-6892],"""wyphao.2007"" <wyphao.2007@163.com>","This is why spark.hadoop.validateOutputSpecs exists, really:
https://spark.apache.org/docs/latest/configuration.html

ming, I found it will reuse the application id (In my case is application_1428664056212_0016) before falied to write spark eventLog, But now my application id is application_1428664056212_0017,then spark write eventLog will falied, the stacktrace as follow:
ailed, java.io.IOException: Target log file already exists (hdfs://mycluster/spark-logs/eventLog/application_1428664056212_0016)
rk-logs/eventLog/application_1428664056212_0016)
ingListener.scala:201)
xt.scala:1388)
xt.scala:1388)
licationMaster.scala:107)
anager.java:54)

---------------------------------------------------------------------


"
"""wyphao.2007"" <wyphao.2007@163.com>","Mon, 20 Apr 2015 11:53:48 +0800 (CST)","Re:Re: Question about recovery from checkpoint
 exception[SPARK-6892]","""Sean Owen"" <sowen@cloudera.com>","Hi Sean Owen, Thank you for your attention.


I know spark.hadoop.validateOutputSpecs. 


I restart the job, the application id is application_1428664056212_0017 and it recovery from checkpoint, and it will write eventLog into application_1428664056212_0016 dir, I think it shoud write to application_1428664056212_0017 not application_1428664056212_0016.



At 2015-04-20 11:46:12,""Sean Owen"" <sowen@cloudera.com> wrote:
>This is why spark.hadoop.validateOutputSpecs exists, really:
>https://spark.apache.org/docs/latest/configuration.html
>
>On Mon, Apr 20, 2015 at 3:40 AM, wyphao.2007 <wyphao.2007@163.com> wrote:
>> Hi,
>>    When I recovery from checkpoint in yarn-cluster mode using Spark Streaming, I found it will reuse the application id (In my case is application_1428664056212_0016) before falied to write spark eventLog, But now my application id is application_1428664056212_0017,then spark write eventLog will falied, the stacktrace as follow:
>> 15/04/14 10:14:01 WARN util.ShutdownHookManager: ShutdownHook '$anon$3' failed, java.io.IOException: Target log file already exists (hdfs://mycluster/spark-logs/eventLog/application_1428664056212_0016)
>> java.io.IOException: Target log file already exists (hdfs://mycluster/spark-logs/eventLog/application_1428664056212_0016)
>>         at org.apache.spark.scheduler.EventLoggingListener.stop(EventLoggingListener.scala:201)
>>         at org.apache.spark.SparkContext$$anonfun$stop$4.apply(SparkContext.scala:1388)
>>         at org.apache.spark.SparkContext$$anonfun$stop$4.apply(SparkContext.scala:1388)
>>         at scala.Option.foreach(Option.scala:236)
>>         at org.apache.spark.SparkContext.stop(SparkContext.scala:1388)
>>         at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$3.run(ApplicationMaster.scala:107)
>>         at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
>> Is someone can help me, The issue is SPARK-6892.
>> thanks
>>
>>
>>
>
>---------------------------------------------------------------------
>To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>For additional commands, e-mail: dev-help@spark.apache.org
>
"
twinkle sachdeva <twinkle.sachdeva@gmail.com>,"Mon, 20 Apr 2015 13:31:36 +0530",Re: Addition of new Metrics for killed executors.,Archit Thakur <archit279thakur@gmail.com>,"Hi Archit,

What is your use case and what kind of metrics are you planning to add?

Thanks,
Twinkle


"
Emre Sevinc <emre.sevinc@gmail.com>,"Mon, 20 Apr 2015 10:58:54 +0200","How to use Spark Streaming .jar file that I've built using a
 different branch than master?",dev@spark.apache.org,"Hello,

I'm building a different version of Spark Streaming (based on a different
branch than master) in my application for testing purposes, but it seems
like spark-submit is ignoring my newly built Spark Streaming .jar, and
using an older version.

Here's some context:

I'm on a different branch:

$ git branch
* SPARK-3276
  master

Then I build the Spark Streaming that I've changed:

âœ” ~/code/spark [SPARK-3276 L|âœš 1]
$ mvn --projects streaming/ -DskipTests install

it builds without problems, and then when I check my local Maven
repository, I see that I have newly generated Spark Streaming jars:

$ ls -lh
~/.m2/repository/org/apache/spark/spark-streaming_2.10/1.4.0-SNAPSHOT/
total 3.3M
-rw-rw-r-- 1 emre emre 1.6K Apr 20 10:43 maven-metadata-local.xml
-rw-rw-r-- 1 emre emre  421 Apr 20 10:43 _remote.repositories
-rw-rw-r-- 1 emre emre 1.3M Apr 20 10:42
spark-streaming_2.10-1.4.0-SNAPSHOT.jar
-rw-rw-r-- 1 emre emre 622K Apr 20 10:43
spark-streaming_2.10-1.4.0-SNAPSHOT-javadoc.jar
-rw-rw-r-- 1 emre emre 6.7K Apr 20 10:42
spark-streaming_2.10-1.4.0-SNAPSHOT.pom
-rw-rw-r-- 1 emre emre 181K Apr 20 10:42
spark-streaming_2.10-1.4.0-SNAPSHOT-sources.jar
-rw-rw-r-- 1 emre emre 1.2M Apr 20 10:42
spark-streaming_2.10-1.4.0-SNAPSHOT-tests.jar
-rw-rw-r-- 1 emre emre  82K Apr 20 10:42
spark-streaming_2.10-1.4.0-SNAPSHOT-test-sources.jar

Then I build and run an application (in Java) that uses Spark Streaming. In
that test project's pom.xml I have

...
 <properties>
    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
    <hadoop.version>2.4.0</hadoop.version>
    <spark.version>1.4.0-SNAPSHOT</spark.version>
  </properties>
...
 <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-streaming_2.10</artifactId>
      <version>${spark.version}</version>
      <scope>provided</scope>
    </dependency>


And then I use

  ~/code/spark/bin/spark-submit

to submit my application. It starts fine, and continues to run on my local
filesystem but when I check the log messages on the console, I don't see
the changes I have made, and I *did* make changes, e.g. changed some
logging messages. It is like when I submit my application, it is not using
the Spark Streaming from *branch SPARK-3276* but from the master branch.

Any ideas what might be causing this? Is there some form of caching? Or is
spark-submit using a different .jar for streaming? (Where?)

How can I see the effects of my changes that I did to Spark Streaming in my
SPARK-3276 branch?

-- 
Emre SevinÃ§
"
Akhil Das <akhil@sigmoidanalytics.com>,"Mon, 20 Apr 2015 14:36:38 +0530","Re: How to use Spark Streaming .jar file that I've built using a
 different branch than master?",Emre Sevinc <emre.sevinc@gmail.com>,"I think you can override the SPARK_CLASSPATH with your newly built jar.

Thanks
Best Regards


In
l
g
s
my
"
Emre Sevinc <emre.sevinc@gmail.com>,"Mon, 20 Apr 2015 11:14:29 +0200","Re: How to use Spark Streaming .jar file that I've built using a
 different branch than master?",Akhil Das <akhil@sigmoidanalytics.com>,"I thought it was spark-submit that was configuring and arranging everything
related to classpath (am I wrong?), e.g. that's how I used Spark so far. Is
there a way to do it using spark-submit?

--
Emre


t
al
ng
is


-- 
Emre Sevinc
"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Mon, 20 Apr 2015 09:17:05 +0000",Dataframe.fillna from 1.3.0,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi everyone,
let's assume I'm stuck in 1.3.0, how can I benefit from the *fillna* API in
PySpark, is there any efficient alternative to mapping the records myself ?

Regards,

Olivier.
"
Archit Thakur <archit279thakur@gmail.com>,"Mon, 20 Apr 2015 14:47:03 +0530",Re: Addition of new Metrics for killed executors.,twinkle sachdeva <twinkle.sachdeva@gmail.com>,"Hi Twinkle,

We have a use case in where we want to debug the reason of how n why an
executor got killed.
Could be because of stackoverflow, GC or any other unexpected scenario.
If I see the driver UI there is no information present around killed
executors, So was just curious how do people usually debug those things
apart from scanning logs and understanding it. The metrics we are planning
to add are similar to what we have for non killed executors - [data per
stage specifically] - numFailedTasks, executorRunTime, inputBytes,
memoryBytesSpilled .. etc.

Apart from that we also intend to add all information present in an
executor tabs for running executors.

Thanks,
Archit Thakur.


"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Mon, 20 Apr 2015 09:48:31 +0000",Re: Dataframe.fillna from 1.3.0,"Olivier Girardot <o.girardot@lateral-thoughts.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","a UDF might be a good idea no ?

Le lun. 20 avr. 2015 Ã  11:17, Olivier Girardot <
o.girardot@lateral-thoughts.com> a Ã©crit :

"
Peter Rudenko <petro.rudenko@gmail.com>,"Mon, 20 Apr 2015 14:29:55 +0300",Re: [sql] Dataframe how to check null values,"Reynold Xin <rxin@databricks.com>, Dean Wampler <deanwampler@gmail.com>","Sounds very good. Is there a jira for this? Would be cool to have in 
1.4, because currently cannot use dataframe.describe function with NaN 
values, need to filter manually all the columns.

Thanks,
Peter Rudenko


"
Ted Yu <yuzhihong@gmail.com>,"Mon, 20 Apr 2015 05:24:55 -0700",Re: [sql] Dataframe how to check null values,Peter Rudenko <petro.rudenko@gmail.com>,"I found:
https://issues.apache.org/jira/browse/SPARK-6573



:
ecause currently cannot use dataframe.describe function with NaN values, need to filter manually all the columns.
n null handling in SQL/DataFrames. Would be great to get some feedback.
 be consistent with most SQL databases, and Pandas. This would also require some inbound conversion.
ue, and keep the NaN value. When we see a null value for a floating point field, we should mark the null bit as true, and update the field to store NaN.
ll.
 null) and not a floating point field, simply evaluate the expression to null. This is consistent with most SQL UDFs and most programming languages' treatment of NaN.
F

,

---------------------------------------------------------------------


"
Emre Sevinc <emre.sevinc@gmail.com>,"Mon, 20 Apr 2015 14:43:25 +0200","Re: How to use Spark Streaming .jar file that I've built using a
 different branch than master?",dev <dev@spark.apache.org>,"Apparently, after *only* building Spark Streaming, I also have to:

   mvn --projects assembly/ -DskipTests clean install

so that my test project uses the new version when I pass it to spark-submit.

--
Emre SevinÃ§


:

l
g
s



-- 
Emre Sevinc
"
Reynold Xin <rxin@databricks.com>,"Mon, 20 Apr 2015 13:22:15 -0700",Re: Dataframe.fillna from 1.3.0,Olivier Girardot <o.girardot@lateral-thoughts.com>,"You can just create fillna function based on the 1.3.1 implementation of
fillna, no?



I
"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Mon, 20 Apr 2015 20:44:11 +0000",Re: Dataframe.fillna from 1.3.0,"Reynold Xin <rxin@databricks.com>, Olivier Girardot <o.girardot@lateral-thoughts.com>","that's why I was wondering.
Thank you for answering :)

Le lun. 20 avr. 2015 Ã  22:22, Reynold Xin <rxin@databricks.com> a Ã©crit :

PI
"
Reynold Xin <rxin@databricks.com>,"Mon, 20 Apr 2015 13:47:56 -0700",Re: Dataframe.fillna from 1.3.0,Olivier Girardot <o.girardot@lateral-thoughts.com>,"Ah ic. You can do something like


df.select(coalesce(df(""a""), lit(0.0)))


,
Ã©crit :
"
JaeSung Jun <jaesjun@gmail.com>,"Tue, 21 Apr 2015 16:55:25 +1000",Can't find postgresql jdbc driver when using external datasource,dev@spark.apache.org,"Hi,

I tried to get external data base table running sitting on postgresql.
i've got java.lang.ClassNotFoundException even if i added driver jar using
--jars option like followings :

is it class loader hierarchy problem or any idea?

thanks

-----------------

spark-sql --jars ../lib/postgresql-9.4-1200.jdbc41.jar

spark-sql> CREATE TEMPORARY TABLE uusr

         > USING org.apache.spark.sql.jdbc

         > OPTIONS (

         > url
""jdbc:postgresql://localhost/spark_db?user=postgres&password=password"",

         > driver ""org.postgresql.Driver"",

         > dbtable ""uusr""

         > );

..

java.lang.ClassNotFoundException: org.postgresql.Driver

at java.net.URLClassLoader$1.run(URLClassLoader.java:366)

at java.net.URLClassLoader$1.run(URLClassLoader.java:355)

at java.security.AccessController.doPrivileged(Native Method)

at java.net.URLClassLoader.findClass(URLClassLoader.java:354)

at java.lang.ClassLoader.loadClass(ClassLoader.java:425)

at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)

at java.lang.ClassLoader.loadClass(ClassLoader.java:358)

at java.lang.Class.forName0(Native Method)

at java.lang.Class.forName(Class.java:191)

 at
org.apache.spark.sql.jdbc.DefaultSource.createRelation(JDBCRelation.scala:97)

-----------------------------
"
Hrishikesh Subramonian <hrishikesh.subramonian@flytxt.com>,"Tue, 21 Apr 2015 14:00:22 +0530",python/run-tests fails at spark master branch,dev@spark.apache.org,"Hi,

I cloned spark master branch from github and was built successfully 
using the mvn -DskipTests clean package command.
But the python/run-tests command fails. Please see the log below:

Running PySpark tests. Output is in python/unit-tests.log.
Testing with Python version:
Python 2.7.3
Run core tests ...
Running test: pyspark/rdd.py
Running test: pyspark/context.py
Running test: pyspark/conf.py
Running test: pyspark/broadcast.py
Running test: pyspark/accumulators.py
Running test: pyspark/serializers.py
Running test: pyspark/profiler.py
Running test: pyspark/shuffle.py
Running test: pyspark/tests.py
Run sql tests ...
Running test: pyspark/sql/_types.py
Running test: pyspark/sql/context.py
Running test: pyspark/sql/dataframe.py
Running test: pyspark/sql/functions.py
Running test: pyspark/sql/tests.py
Run mllib tests ...
Running test: pyspark/mllib/classification.py
Running test: pyspark/mllib/clustering.py
Running test: pyspark/mllib/evaluation.py
Running test: pyspark/mllib/feature.py
Running test: pyspark/mllib/fpm.py
Running test: pyspark/mllib/linalg.py
Running test: pyspark/mllib/rand.py
Running test: pyspark/mllib/recommendation.py
Running test: pyspark/mllib/regression.py
Running test: pyspark/mllib/stat/_statistics.py
Running test: pyspark/mllib/tree.py
Running test: pyspark/mllib/util.py
Running test: pyspark/mllib/tests.py
Run ml tests ...
Running test: pyspark/ml/feature.py
Running test: pyspark/ml/classification.py
Running test: pyspark/ml/tests.py
Run streaming tests ...
Failed to find Spark Streaming Kafka assembly jar in 
/home/xyz/spark/external/kafka-assembly
You need to build Spark with  'build/sbt assembly/assembly 
streaming-kafka-assembly/assembly' or 'build/mvn package' before running 
this program


Is anybody facing the same problem?

Regards,

Hrishikesh
"
hrishikesh91 <hrishikesh.subramonian@flytxt.com>,"Tue, 21 Apr 2015 02:52:31 -0700 (MST)",python/run-tests fails at spark master branch,dev@spark.apache.org,"Hi,

I cloned spark master branch from github and was built successfully using
the mvn -DskipTests clean package command.
But the python/run-tests command fails. Please see the log below:

/Running PySpark tests. Output is in python/unit-tests.log.
Testing with Python version:
Python 2.7.3
Run core tests ...
Running test: pyspark/rdd.py
Running test: pyspark/context.py
Running test: pyspark/conf.py
Running test: pyspark/broadcast.py
Running test: pyspark/accumulators.py
Running test: pyspark/serializers.py
Running test: pyspark/profiler.py
Running test: pyspark/shuffle.py
Running test: pyspark/tests.py
Run sql tests ...
Running test: pyspark/sql/_types.py
Running test: pyspark/sql/context.py
Running test: pyspark/sql/dataframe.py
Running test: pyspark/sql/functions.py
Running test: pyspark/sql/tests.py
Run mllib tests ...
Running test: pyspark/mllib/classification.py
Running test: pyspark/mllib/clustering.py
Running test: pyspark/mllib/evaluation.py
Running test: pyspark/mllib/feature.py
Running test: pyspark/mllib/fpm.py
Running test: pyspark/mllib/linalg.py
Running test: pyspark/mllib/rand.py
Running test: pyspark/mllib/recommendation.py
Running test: pyspark/mllib/regression.py
Running test: pyspark/mllib/stat/_statistics.py
Running test: pyspark/mllib/tree.py
Running test: pyspark/mllib/util.py
Running test: pyspark/mllib/tests.py
Run ml tests ...
Running test: pyspark/ml/feature.py
Running test: pyspark/ml/classification.py
Running test: pyspark/ml/tests.py
Run streaming tests ...
Failed to find Spark Streaming Kafka assembly jar in
/home/xyz/spark/external/kafka-assembly
You need to build Spark with  'build/sbt assembly/assembly
streaming-kafka-assembly/assembly' or 'build/mvn package' before running
this program/


Is anybody facing the same problem?

Regards,

Hrishikesh




--

---------------------------------------------------------------------


"
Felix C <felixcheung_m@hotmail.com>,"Tue, 21 Apr 2015 05:27:33 -0700","RE: Can't find postgresql jdbc driver when using external
 datasource","JaeSung Jun <jaesjun@gmail.com>, <dev@spark.apache.org>","It works with --driver-class-path?

Please see https://eradiating.wordpress.com/2015/04/17/using-spark-data-sources-to-load-data-from-postgresql/

Hi,

I tried to get external data base table running sitting on postgresql.
i've got java.lang.ClassNotFoundException even if i added driver jar using
--jars option like followings :

is it class loader hierarchy problem or any idea?

thanks

-----------------

spark-sql --jars ../lib/postgresql-9.4-1200.jdbc41.jar

spark-sql> CREATE TEMPORARY TABLE uusr

         > USING org.apache.spark.sql.jdbc

         > OPTIONS (

         > url
""jdbc:postgresql://localhost/spark_db?user=postgres&password=password"",

         > driver ""org.postgresql.Driver"",

         > dbtable ""uusr""

         > );

..

java.lang.ClassNotFoundException: org.postgresql.Driver

at java.net.URLClassLoader$1.run(URLClassLoader.java:366)

at java.net.URLClassLoader$1.run(URLClassLoader.java:355)

at java.security.AccessController.doPrivileged(Native Method)

at java.net.URLClassLoader.findClass(URLClassLoader.java:354)

at java.lang.ClassLoader.loadClass(ClassLoader.java:425)

at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)

at java.lang.ClassLoader.loadClass(ClassLoader.java:358)

at java.lang.Class.forName0(Native Method)

at java.lang.Class.forName(Class.java:191)

 at
org.apache.spark.sql.jdbc.DefaultSource.createRelation(JDBCRelation.scala:97)

-----------------------------

---------------------------------------------------------------------


"
Felix C <felixcheung_m@hotmail.com>,"Tue, 21 Apr 2015 05:27:33 -0700","RE: Can't find postgresql jdbc driver when using external
 datasource","JaeSung Jun <jaesjun@gmail.com>, <dev@spark.apache.org>","It works with --driver-class-path?

Please see https://eradiating.wordpress.com/2015/04/17/using-spark-data-sources-to-load-data-from-postgresql/

Hi,

I tried to get external data base table running sitting on postgresql.
i've got java.lang.ClassNotFoundException even if i added driver jar using
--jars option like followings :

is it class loader hierarchy problem or any idea?

thanks

-----------------

spark-sql --jars ../lib/postgresql-9.4-1200.jdbc41.jar

spark-sql> CREATE TEMPORARY TABLE uusr

         > USING org.apache.spark.sql.jdbc

         > OPTIONS (

         > url
""jdbc:postgresql://localhost/spark_db?user=postgres&password=password"",

         > driver ""org.postgresql.Driver"",

         > dbtable ""uusr""

         > );

..

java.lang.ClassNotFoundException: org.postgresql.Driver

at java.net.URLClassLoader$1.run(URLClassLoader.java:366)

at java.net.URLClassLoader$1.run(URLClassLoader.java:355)

at java.security.AccessController.doPrivileged(Native Method)

at java.net.URLClassLoader.findClass(URLClassLoader.java:354)

at java.lang.ClassLoader.loadClass(ClassLoader.java:425)

at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)

at java.lang.ClassLoader.loadClass(ClassLoader.java:358)

at java.lang.Class.forName0(Native Method)

at java.lang.Class.forName(Class.java:191)

 at
org.apache.spark.sql.jdbc.DefaultSource.createRelation(JDBCRelation.scala:97)

-----------------------------

---------------------------------------------------------------------


"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Tue, 21 Apr 2015 13:06:59 +0000",Spark 1.2.2 prebuilt release for Hadoop 2.4 didn't get deployed,"""dev@spark.apache.org"" <dev@spark.apache.org>, pwendell@gmail.com","Hi everyone,
It seems the some of the Spark 1.2.2 prebuilt versions (I tested mainly for
Hadoop 2.4 and later) didn't get deploy on all the mirrors and cloudfront.
Both the direct download and apache mirrors fails with dead links, for
example : http://d3kbcqa49mib13.cloudfront.net/spark-1.2.2-bin-hadoop2.4.tgz

Regards,

Olivier.
"
Karlson <ksonspark@siberie.de>,"Tue, 21 Apr 2015 17:13:26 +0200",[pyspark] Drop =?UTF-8?Q?=5F=5Fgetattr=5F=5F=20on=20DataFrame?=,dev@spark.apache.org,"I think the __getattr__ method should be removed from the DataFrame API 
in pyspark.

May I draw the Python folk's attention to the issue 
https://issues.apache.org/jira/browse/SPARK-7035 and invite comments?

Thank you!

---------------------------------------------------------------------


"
Marcelo Vanzin <vanzin@cloudera.com>,"Tue, 21 Apr 2015 10:15:07 -0700",Re: python/run-tests fails at spark master branch,Hrishikesh Subramonian <hrishikesh.subramonian@flytxt.com>,"

Have you built the assemblies before running the tests? (mvn package
-DskipTests, or sbt assembly)


-- 
Marcelo

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Tue, 21 Apr 2015 11:47:15 -0700",Re: Spark 1.2.2 prebuilt release for Hadoop 2.4 didn't get deployed,Olivier Girardot <o.girardot@lateral-thoughts.com>,"Good catch Olivier - I'll take care of it. Tracking this on SPARK-7027.


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Tue, 21 Apr 2015 12:04:06 -0700",Re: [pyspark] Drop __getattr__ on DataFrame,Karlson <ksonspark@siberie.de>,"I replied on JIRA. Let's move the discussion there.



"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 21 Apr 2015 19:05:23 +0000",Is spark-ec2 for production use?,Spark dev list <dev@spark.apache.org>,"Is spark-ec2 intended for spinning up production Spark clusters?

I think the answer is no.

However, the docs for spark-ec2
<https://spark.apache.org/docs/latest/ec2-scripts.html> very much leave
that possibility open, and indeed I see many people asking questions or
opening issues that stem from some production use case they are trying to
fit spark-ec2 to.

Here's the latest example
<https://issues.apache.org/jira/browse/SPARK-6900?focusedCommentId=14504236&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14504236>
of
someone using spark-ec2 to power their (presumably) production service.

Shouldn't we actively discourage people from using spark-ec2 in this way?

I understand there's no stopping people from doing what they want with it,
and certainly the questions and issues we receive about spark-ec2 are still
valid, even if they stem from discouraged use cases.

one-off jobs, prototypes, and so forth.

If that's the case, it's best to stress this in the docs.

Nick
"
Patrick Wendell <pwendell@gmail.com>,"Tue, 21 Apr 2015 12:46:04 -0700",Re: Is spark-ec2 for production use?,Nicholas Chammas <nicholas.chammas@gmail.com>,"It could be a good idea to document this a bit. The original goals
were to give people an easy way to get started with Spark and also to
provide a consistent environment for our own experiments and
benchmarking of Spark at the AMPLab. Over time I've noticed a huge
amount of scope increase in terms of what people want to do and I do
know that many companies run production infrastructure based on
launching the EC2 scripts.

My feeling is that the general problem of deploying Spark with other
applications and frameworks is fairly well covered by projects which
specifically focus on packaging and automation (e.g. Whirr, BigTop,
etc). So I'd like to see a narrower focus on just getting a vanilla
Spark cluster up and running and make it clear that customization and
extension of that functionality is really not in scope.

This doesn't mean discouraging people from using it for production use
cases, but more that they shouldn't expect us to merge and maintain
things that seek to do broader integration with other technologies,
automation, etc.

- Patrick


---------------------------------------------------------------------


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Tue, 21 Apr 2015 12:48:00 -0700",Re: Is spark-ec2 for production use?,Nicholas Chammas <nicholas.chammas@gmail.com>,"could stress is that spark-ec2 is meant to be run manually (i.e. it outputs
errors, asks for prompts etc.) and that automating it is not in our scope
right now.

Shivaram


"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Tue, 21 Apr 2015 19:50:31 +0000",Re: Spark 1.2.2 prebuilt release for Hadoop 2.4 didn't get deployed,"Patrick Wendell <pwendell@gmail.com>, Olivier Girardot <o.girardot@lateral-thoughts.com>","Thanks Patrick ! I'll update
https://registry.hub.docker.com/u/ogirardot/spark-docker-shell/ when you're
done.

Regards,

Olivier.

Le mar. 21 avr. 2015 Ã  20:47, Patrick Wendell <pwendell@gmail.com> a Ã©crit :

"
Olivier Girardot <ssaboum@gmail.com>,"Tue, 21 Apr 2015 20:02:25 +0000",Re: Spark Streaming updatyeStateByKey throws OutOfMemory Error,"Sourav Chandra <sourav.chandra@livestream.com>, user@spark.apache.org, 
	dev@spark.apache.org","Hi Sourav,
Can you post your updateFunc as well please ?

Regards,

Olivier.

Le mar. 21 avr. 2015 Ã  12:48, Sourav Chandra <sourav.chandra@livestream.com>
a Ã©crit :

s
to
)*
ateByKey(updateFunc).foreachRDD(_.foreachPartition(RedisHelper.update(_)))*
ils
?
· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â·
"
Olivier Girardot <ssaboum@gmail.com>,"Tue, 21 Apr 2015 19:54:07 +0000",Spark build time,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi everyone,
I was just wandering about the Spark full build time (including tests),
1h48 seems to me quite... spacious. What's taking most of the time ? Is the
build mainly integration tests ? Is there any roadmap or jiras dedicated to
that we can chip in ?

Regards,

Olivier.
"
Reynold Xin <rxin@databricks.com>,"Tue, 21 Apr 2015 13:19:49 -0700",Re: Spark build time,Olivier Girardot <ssaboum@gmail.com>,"It runs tons of integration tests. I think most developers just let Jenkins
run the full suite of them.


"
Reynold Xin <rxin@databricks.com>,"Tue, 21 Apr 2015 13:34:23 -0700",[discuss] new Java friendly InputSource API,"""dev@spark.apache.org"" <dev@spark.apache.org>","I created a pull request last night for a new InputSource API that is
essentially a stripped down version of the RDD API for providing data into
Spark. Would be great to hear the community's feedback.

Spark currently has two de facto input source API:
1. RDD
2. Hadoop MapReduce InputFormat

Neither of the above is ideal:

1. RDD: It is hard for Java developers to implement RDD, given the implicit
class tags. In addition, the RDD API depends on Scala's runtime library,
which does not preserve binary compatibility across Scala versions. If a
developer chooses Java to implement an input source, it would be great if
that input source can be binary compatible in years to come.

2. Hadoop InputFormat: The Hadoop InputFormat API is overly restrictive.
For example, it forces key-value semantics, and does not support running
arbitrary code on the driver side (an example of why this is useful is
broadcast). In addition, it is somewhat awkward to tell developers that in
order to implement an input source for Spark, they should learn the Hadoop
MapReduce API first.


My patch creates a new InputSource interface, described by:

- an array of InputPartition that specifies the data partitioning
- a RecordReader that specifies how data on each partition can be read

This interface is similar to Hadoop's InputFormat, except that there is no
explicit key/value separation.


JIRA ticket: https://issues.apache.org/jira/browse/SPARK-7025
Pull request: https://github.com/apache/spark/pull/5603
"
Alex <lxvava@gmail.com>,"Tue, 21 Apr 2015 16:46:05 -0400",RE: Spark build time,"Reynold Xin <rxin@databricks.com>, Olivier Girardot <ssaboum@gmail.com>","If you are using MVN there are some parameters (MAVEN_OPTS) which need to be set in order to give the underlying environment enough memory.  See the instructions here: https://spark.apache.org/docs/latest/building-spark.html

It runs tons of integration tests. I think most developers just let Jenkins
run the full suite of them.


he
to
"
<nate@reactor8.com>,"Tue, 21 Apr 2015 14:20:51 -0700",RE: Is spark-ec2 for production use?,"""'Spark dev list'"" <dev@spark.apache.org>","Several of the Bigtop folks got together last week at ApacheCon, this was
popular topic for next enhancements with spark related components after
getting 1.0 out the door.  Some leading topics were:

-deployment of spark specific clusters
     -spark standalone, hdfs
     -spark over yarn, hdfs
     -spark on mesos (talked to mesos folk about working to include in
bigtop post 1.0)
     -the above plus variants of other bigtop components (ie: kafka,
zeppelin, demo data generators)

things can be validated post build/deploy and enhance CI process so if you
choose to deploy via bigtop in test/prod/etc you know things have gone
through a certain amount of rigor beforehand

Nate

It could be a good idea to document this a bit. The original goals were to
give people an easy way to get started with Spark and also to provide a
consistent environment for our own experiments and benchmarking of Spark at
the AMPLab. Over time I've noticed a huge amount of scope increase in terms
of what people want to do and I do know that many companies run production
infrastructure based on launching the EC2 scripts.

My feeling is that the general problem of deploying Spark with other
applications and frameworks is fairly well covered by projects which
specifically focus on packaging and automation (e.g. Whirr, BigTop, etc). So
I'd like to see a narrower focus on just getting a vanilla Spark cluster up
and running and make it clear that customization and extension of that
functionality is really not in scope.

This doesn't mean discouraging people from using it for production use
cases, but more that they shouldn't expect us to merge and maintain things
that seek to do broader integration with other technologies, automation,
etc.

- Patrick


---------------------------------------------------------------------
commands, e-mail: dev-help@spark.apache.org



---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 21 Apr 2015 21:32:30 +0000",Re: Is spark-ec2 for production use?,"nate@reactor8.com, Spark dev list <dev@spark.apache.org>","Nate, could you point us to an example of how one would use Big Top as a
""more production-ish"" replacement for spark-ec2? I look a look at the project
page <http://bigtop.apache.org/index.html>, but couldn't find any usage
examples. Perhaps we can link to them from the spark-ec2 docs.

Regarding tests to validate that Spark was set up correctly, I am
using the JSON
feed from the Spark master web UI
<http://stackoverflow.com/a/29659630/877069> for starters. Y'all might find
it useful for the same purpose.

Nick


"
Punyashloka Biswal <punya.biswal@gmail.com>,"Tue, 21 Apr 2015 23:06:26 +0000",Re: [discuss] new Java friendly InputSource API,"Reynold Xin <rxin@databricks.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Reynold, thanks for this! At Palantir we're heavy users of the Java APIs
and appreciate being able to stop hacking around with fake ClassTags :)

Regarding this specific proposal, is the contract of RecordReader#get
intended to be that it returns a fresh object each time? Or is it allowed
to mutate a fixed object and return a pointer to it each time?

Put another way, is a caller supposed to clone the output of get() if they
want to use it later?

Punya

"
Reynold Xin <rxin@databricks.com>,"Tue, 21 Apr 2015 16:10:09 -0700",Re: [discuss] new Java friendly InputSource API,Punyashloka Biswal <punya.biswal@gmail.com>,"It can reuse. That's a good point and we should document it in the API
contract.



"
Soren Macbeth <soren@yieldbot.com>,"Tue, 21 Apr 2015 16:33:00 -0700",Re: [discuss] new Java friendly InputSource API,Reynold Xin <rxin@databricks.com>,"I'm also super interested in this. Flambo (our clojure DSL) wraps the java
api and it would be great to have this.


"
JaeSung Jun <jaesjun@gmail.com>,"Wed, 22 Apr 2015 10:09:26 +1000",Re: Can't find postgresql jdbc driver when using external datasource,Felix C <felixcheung_m@hotmail.com>,"Thanks Felix,
It worked with spark class path variable as follows :
SPARK_CLASSPATH=postgresql-9.3-1102-jdbc41.jar

I think it should be working with driver class path.

Thanks
Jason


"
Saisai Shao <sai.sai.shao@gmail.com>,"Wed, 22 Apr 2015 10:08:59 +0800",Re: python/run-tests fails at spark master branch,Marcelo Vanzin <vanzin@cloudera.com>,"Hi Hrishikesh,

Now we add Kafka unit test for python which relies on Kafka assembly jar,
so you need to run `sbt assembly` or mvn `package` at first to get an
assemble jar.



2015-04-22 1:15 GMT+08:00 Marcelo Vanzin <vanzin@cloudera.com>:

"
Punyashloka Biswal <punya.biswal@gmail.com>,"Wed, 22 Apr 2015 02:38:28 +0000",Graphical display of metrics on application UI page,"""dev@spark.apache.org"" <dev@spark.apache.org>","Dear Spark devs,

Would people find it useful to have a graphical display of metrics (such as
duration, GC time, etc) on the application UI page? Has anybody worked on
this before?

Punya
"
Hrishikesh Subramonian <hrishikesh.subramonian@flytxt.com>,"Wed, 22 Apr 2015 11:07:13 +0530",Re: python/run-tests fails at spark master branch,"Saisai Shao <sai.sai.shao@gmail.com>, 
 Marcelo Vanzin <vanzin@cloudera.com>","Hi,

The /python/run-tests/ executes successfully after I ran /'build/sbt 
assembly/' command. But the tests fail if I run it after /'mvn 
-Dskiptests clean package'/ command. Why does it run in /sbt assembly/ 
and not in/mvn package/?

--
Hrishikesh


"
Saisai Shao <sai.sai.shao@gmail.com>,"Wed, 22 Apr 2015 14:36:40 +0800",Re: python/run-tests fails at spark master branch,Hrishikesh Subramonian <hrishikesh.subramonian@flytxt.com>,"Hi Hrishikesh,

Seems the behavior of Kafka-assembly is a little different when using Maven
to sbt. The assembly jar name and location is different while using `mvn
package`. This is a actually bug, I'm fixing this now.

Thanks
Jerry


2015-04-22 13:37 GMT+08:00 Hrishikesh Subramonian <
hrishikesh.subramonian@flytxt.com>:

"
Akhil Das <akhil@sigmoidanalytics.com>,"Wed, 22 Apr 2015 12:19:02 +0530",Re: Graphical display of metrics on application UI page,Punyashloka Biswal <punya.biswal@gmail.com>,"â€‹There were some PR's about graphical representation with D3.js, you can
possibly see it on the github. Here's a few of them
https://github.com/apache/spark/pulls?utf8=%E2%9C%93&q=d3â€‹

Thanks
Best Regards


as
"
Sourav Chandra <sourav.chandra@livestream.com>,"Wed, 22 Apr 2015 12:29:46 +0530",Re: Spark Streaming updatyeStateByKey throws OutOfMemory Error,Olivier Girardot <ssaboum@gmail.com>,"Hi Olivier,

*the update function is as below*:

*val updateFunc = (values: Seq[IConcurrentUsers], state: Option[(Long,
Long)]) => {*
*      val previousCount = state.getOrElse((0L, 0L))._2*
*      var startValue: IConcurrentUsers = ConcurrentViewers(0)*
*      var currentCount = 0L*
*      val lastIndexOfConcurrentUsers =*
*        values.lastIndexWhere(_.isInstanceOf[ConcurrentViewers])*
*      val subList = values.slice(0, lastIndexOfConcurrentUsers)*
*      val currentCountFromSubList = subList.foldLeft(startValue)(_ op
_).count + previousCount*
*      val lastConcurrentViewersCount =
values(lastIndexOfConcurrentUsers).count*

*      if (math.abs(lastConcurrentViewersCount - currentCountFromSubList)
*        logger.error(*
*          s""Count using state updation $currentCountFromSubList, "" +*
*            s""ConcurrentUsers count $lastConcurrentViewersCount"" +*
*            s"" resetting to $lastConcurrentViewersCount""*
*        )*
*        currentCount = lastConcurrentViewersCount*
*      }*
*      val remainingValuesList = values.diff(subList)*
*      startValue = ConcurrentViewers(currentCount)*
*      currentCount = remainingValuesList.foldLeft(startValue)(_ op
_).count*

*      if (currentCount < 0) {*

*        logger.error(*
*          s""ERROR: Got new count $currentCount < 0, value:$values,
state:$state, resetting to 0""*
*        )*
*        currentCount = 0*
*      }*
*      // to stop pushing subsequent 0 after receiving first 0*
*      if (currentCount == 0 && previousCount == 0) None*
*      else Some(previousCount, currentCount)*
*    }*

*trait IConcurrentUsers {*
*  val count: Long*
*  def op(a: IConcurrentUsers): IConcurrentUsers =
IConcurrentUsers.op(this, a)*
*}*

*object IConcurrentUsers {*
*  def op(a: IConcurrentUsers, b: IConcurrentUsers): IConcurrentUsers = (a,
b) match {*
*    case (_, _: ConcurrentViewers) => *
*      ConcurrentViewers(b.count)*
*    case (_: ConcurrentViewers, _: IncrementConcurrentViewers) => *
*      ConcurrentViewers(a.count + b.count)*
*    case (_: ConcurrentViewers, _: DecrementConcurrentViewers) => *
*      ConcurrentViewers(a.count - b.count)*
*  }*
*}*

*case class IncrementConcurrentViewers(count: Long) extends
IConcurrentUsers*
*case class DecrementConcurrentViewers(count: Long) extends
IConcurrentUsers*
*case class ConcurrentViewers(count: Long) extends IConcurrentUsers*


*also the error stack trace copied from executor logs is:*

*java.lang.OutOfMemoryError: Java heap space*
*        at
org.apache.hadoop.io.WritableUtils.readCompressedStringArray(WritableUtils.java:183)*
*        at
org.apache.hadoop.conf.Configuration.readFields(Configuration.java:2564)*
*        at
org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:285)*
*        at
org.apache.hadoop.io.ObjectWritable.readFields(ObjectWritable.java:77)*
*        at
org.apache.spark.SerializableWritable$$anonfun$readObject$1.apply$mcV$sp(SerializableWritable.scala:43)*
*        at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:927)*
*        at
org.apache.spark.SerializableWritable.readObject(SerializableWritable.scala:39)*
*        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)*
*        at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)*
*        at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)*
*        at java.lang.reflect.Method.invoke(Method.java:601)*
*        at
java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1004)*
*        at
java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1866)*
*        at
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1771)*
*        at
java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1347)*
*        at
java.io.ObjectInputStream.readObject(ObjectInputStream.java:369)*
*        at
org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:62)*
*        at
org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:236)*
*        at
org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readObject$1.apply$mcV$sp(TorrentBroadcast.scala:169)*
*        at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:927)*
*        at
org.apache.spark.broadcast.TorrentBroadcast.readObject(TorrentBroadcast.scala:155)*
*        at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)*
*        at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)*
*        at java.lang.reflect.Method.invoke(Method.java:601)*
*        at
java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1004)*
*        at
java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1866)*
*        at
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1771)*
*        at
java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1347)*
*        at
java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1964)*
*        at
java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1888)*
*        at
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1771)*
*        at
java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1347)*
*15/04/21 15:51:23 ERROR ExecutorUncaughtExceptionHandler: Uncaught
exception in thread Thread[Executor task launch worker-1,5,main]*



:

res
g
p)*
tateByKey(updateFunc).foreachRDD(_.foreachPartition(RedisHelper.update(_)))*
*
s?
Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â·


-- 

Sourav Chandra

Senior Software Engineer

Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â·

sourav.chandra@livestream.com

o: +91 80 4121 8723

m: +91 988 699 3746

skype: sourav.chandra

Livestream

""Ajmera Summit"", First Floor, #3/D, 68 Ward, 3rd Cross, 7th C Main, 3rd
Block, Koramangala Industrial Area,

Bangalore 560034

www.livestream.com
"
twinkle sachdeva <twinkle.sachdeva@gmail.com>,"Wed, 22 Apr 2015 12:30:06 +0530",Re: Addition of new Metrics for killed executors.,"Archit Thakur <archit279thakur@gmail.com>, Patrick Wendell <pwendell@gmail.com>","Hi,

Looks interesting.

It is quite interesting to know about what could have been the reason for
not showing these stats in UI.

As per the description of Patrick W in
https://spark-project.atlassian.net/browse/SPARK-999, it does not mention
any exception w.r.t failed tasks/executors.

Can somebody please comment if it is a bug or some intended behaviour w.r.t
performance or some other bottleneck.

--Twinkle





"
<nate@reactor8.com>,"Wed, 22 Apr 2015 02:16:52 -0700",RE: Is spark-ec2 for production use?,"""'Nicholas Chammas'"" <nicholas.chammas@gmail.com>,
	""'Spark dev list'"" <dev@spark.apache.org>","""Replacement for production-ish"" is beyond a stretch phrasing, UX just isnâ€™t there yet for average end user wanting push-button.

Up until a bit ago focus was heavily focused on infrastructure folks and people building their own distros.  Project is turning towards ""end users"" so anyone from ops to dev/data-hacker will be able to extract value and get moving easily.

If you are brave enough to give it a go and start playing around with it in its current state you can start here looking at puppet modules readme:

https://github.com/apache/bigtop/tree/master/bigtop-deploy/puppet

Currently limited (ie: no yarn, mesos variants, orchestration not added yet), things will be stepping up a great detail heading out of 1.0 release.  If you do and run into stuff hop on mailing list, docs are another area updating is needed.

Thanks for pointers on the json feed link, definitely handy for some smoke tests


""more production-ish"" replacement for spark-ec2? I look a look at the project page <http://bigtop.apache.org/index.html>, but couldn't find any usage examples. Perhaps we can link to them from the spark-ec2 docs.

Regarding tests to validate that Spark was set up correctly, I am using the JSON feed from the Spark master web UI <http://stackoverflow.com/a/29659630/877069> for starters. Y'all might find it useful for the same purpose.

Nick



launching the EC2 scripts.
etc).

<https://issues.apache.org/jira/browse/SPARK-6900?focusedCommentId=1
04236&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-t
service.
way?
cases.


---------------------------------------------------------------------


"
Olivier Girardot <ssaboum@gmail.com>,"Wed, 22 Apr 2015 09:24:43 +0000",Re: Spark build time,Reynold Xin <rxin@databricks.com>,"I agree, it's what I did :)
I was just wondering if it was considered a ""problem"" or something to work
on, I personally think so because the feedback loop should be as quick as
possible, and therefore if there was someone I could help.

Le mar. 21 avr. 2015 Ã  22:20, Reynold Xin <rxin@databricks.com> a Ã©crit :

"
Sourav Chandra <sourav.chandra@livestream.com>,"Wed, 22 Apr 2015 15:04:22 +0530",Re: Spark Streaming updatyeStateByKey throws OutOfMemory Error,Olivier Girardot <ssaboum@gmail.com>,"Anyone?


s.java:183)*
SerializableWritable.scala:43)*
)*
la:39)*
:57)*
mpl.java:43)*
*
alizer.scala:62)*
dcast.scala:236)*
cV$sp(TorrentBroadcast.scala:169)*
)*
cala:155)*
mpl.java:43)*
*
*
ores
ap)*
,
StateByKey(updateFunc).foreachRDD(_.foreachPartition(RedisHelper.update(_)))*
)*
e
Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â·
· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â·



-- 

Sourav Chandra

Senior Software Engineer

Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â·

sourav.chandra@livestream.com

o: +91 80 4121 8723

m: +91 988 699 3746

skype: sourav.chandra

Livestream

""Ajmera Summit"", First Floor, #3/D, 68 Ward, 3rd Cross, 7th C Main, 3rd
Block, Koramangala Industrial Area,

Bangalore 560034

www.livestream.com
"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Wed, 22 Apr 2015 09:56:03 +0000",Re: Dataframe.fillna from 1.3.0,"Reynold Xin <rxin@databricks.com>, Olivier Girardot <o.girardot@lateral-thoughts.com>","Where should this *coalesce* come from ? Is it related to the partition
manipulation coalesce method ?
Thanks !

Le lun. 20 avr. 2015 Ã  22:48, Reynold Xin <rxin@databricks.com> a Ã©crit :

Ã©crit :
f
s
"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Wed, 22 Apr 2015 10:05:04 +0000",Re: Dataframe.fillna from 1.3.0,"Olivier Girardot <o.girardot@lateral-thoughts.com>, Reynold Xin <rxin@databricks.com>","I think I found the Coalesce you were talking about, but this is a catalyst
class that I think is not available from pyspark

Regards,

Olivier.

Le mer. 22 avr. 2015 Ã  11:56, Olivier Girardot <
o.girardot@lateral-thoughts.com> a Ã©crit :

Ã©crit :
*
ds
"
Punyashloka Biswal <punya.biswal@gmail.com>,"Wed, 22 Apr 2015 10:14:49 +0000",Re: Graphical display of metrics on application UI page,Akhil Das <akhil@sigmoidanalytics.com>,"Thanks for the pointers! It looks like others are pretty active on this so
I'll comment on those PRs and try to coordinate before starting any new
work.

Punya

you can
n
"
Suraj Shetiya <surajshetiya@gmail.com>,"Wed, 22 Apr 2015 17:37:54 +0530",Pipeline in pyspark,dev@spark.apache.org,"Hi,

I came across documentation for creating a pipeline in mlib library of
pyspark. I wanted to know if something similar exists for pyspark input
transformations. I have a use case where I have my input files in different
formats and would like to convert them to rdd and store them in memory and
perform certain custom tasks in a pipeline without storing it back to disc
in any step. I came across luigi(http://luigi.readthedocs.org/en/latest/),
but I found that it stores the contents onto disc and reloads it for the
next phase of the pipeline.

-- 
Thanks and regards,
Suraj
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 22 Apr 2015 14:12:57 +0000",Re: Spark build time,"Olivier Girardot <ssaboum@gmail.com>, Reynold Xin <rxin@databricks.com>","I suggest searching the archives for this list as there were several
previous discussions about this problem. JIRA also has several issues
related to this.

Some pointers:

   - SPARK-3431 <https://issues.apache.org/jira/browse/SPARK-3431>:
   Parallelize Scala/Java test execution
   -
   http://apache-spark-developers-list.1001551.n3.nabble.com/Unit-tests-in-lt-5-minutes-td7757.html
   - SPARK-4746 <https://issues.apache.org/jira/browse/SPARK-4746>:
   integration tests should be separated from faster unit tests
   -
   http://apache-spark-developers-list.1001551.n3.nabble.com/Building-Spark-with-Pants-td10397.html

Summary is, everyone agrees the long times are a problem and wants the
build and tests to run faster. There are several things that can be done,
but they all require a lot of work.

Nick
â€‹


k
Ã©crit :
,
s
ed
"
Chunnan Yao <yaochunnan@gmail.com>,"Wed, 22 Apr 2015 08:29:31 -0700 (MST)",Indices of SparseVector must be ordered while computing SVD,dev@spark.apache.org,"Hi all, 
I am using Spark 1.3.1 to write a Spectral Clustering algorithm. This really
confused me today. At first I thought my implementation is wrong. It turns
out it's an issue in MLlib. Fortunately, I've figured it out. 

I suggest to add a hint on user document of MLlib ( as far as I know, there
have not been such hints yet) that  indices of Local Sparse Vector must be
ordered in ascending manner. Because of ignorance of this point, I spent a
lot of time looking for reasons why computeSVD of RowMatrix did not run
correctly on Sparse data. I don't know the influence of Sparse Vector
without ordered indices on other functions, but I believe it is necessary to
let the users know or fix it. Actually, it's very easy to fix. Just add a
sortBy function in internal construction of SparseVector. 

Here is an example to reproduce the affect of unordered Sparse Vector on
computeSVD. 
================================================ 
//in spark-shell, Spark 1.3.1 
 import org.apache.spark.mllib.linalg.distributed.RowMatrix 
 import org.apache.spark.mllib.linalg.{SparseVector, DenseVector, Vector,
Vectors} 

  val sparseData_ordered = Seq( 
    Vectors.sparse(3, Array(1, 2), Array(1.0, 2.0)), 
    Vectors.sparse(3, Array(0,1,2), Array(3.0, 4.0, 5.0)), 
    Vectors.sparse(3, Array(0,1,2), Array(6.0, 7.0, 8.0)), 
    Vectors.sparse(3, Array(0,2), Array(9.0, 1.0)) 
  ) 
  val sparseMat_ordered = new RowMatrix(sc.parallelize(sparseData_ordered,
2)) 

  val sparseData_not_ordered = Seq( 
    Vectors.sparse(3, Array(1, 2), Array(1.0, 2.0)), 
    Vectors.sparse(3, Array(2,1,0), Array(5.0,4.0,3.0)), 
    Vectors.sparse(3, Array(0,1,2), Array(6.0, 7.0, 8.0)), 
    Vectors.sparse(3, Array(2,0), Array(1.0,9.0)) 
  ) 
 val sparseMat_not_ordered = new
RowMatrix(sc.parallelize(sparseData_not_ordered, 2)) 

//apparently, sparseMat_ordered and sparseMat_not_ordered are essentially
the same matirx 
//however, the computeSVD result of these two matrixes are different. Users
should be notified about this situation. 
  println(sparseMat_ordered.computeSVD(2,
true).U.rows.collect.mkString(""\n"")) 
  println(""==================="") 
  println(sparseMat_not_ordered.computeSVD(2,
true).U.rows.collect.mkString(""\n"")) 
====================================================== 
The results are: 
ordered: 
[-0.10972870132786407,-0.18850811494220537] 
[-0.44712472003608356,-0.24828866611663725] 
[-0.784520738744303,-0.3080692172910691] 
[-0.4154110101064339,0.8988385762953358] 

not ordered: 
[-0.10830447119599484,-0.1559341848984378] 
[-0.4522713511277327,-0.23449829541447448] 
[-0.7962382310594706,-0.3130624059305111] 
[-0.43131320303494614,0.8453864703362308] 

Looking into this issue, I can see it's reason locates in
RowMatrix.scala(line 629). The implementation of Sparse dspr here requires
ordered indices. Because it is scanning the indices consecutively to skip
empty columns. 



-----
Feel the sparking Spark!
--

---------------------------------------------------------------------


"
Tathagata Das <tdas@databricks.com>,"Wed, 22 Apr 2015 11:10:51 -0700",Re: Spark Streaming updatyeStateByKey throws OutOfMemory Error,Sourav Chandra <sourav.chandra@livestream.com>,"It could very well be that your executor memory is not enough to store the
state RDDs AND operate on the data. 1G per executor is quite low.
Definitely give more memory. And have you tried increasing the number of
partitions (specify number of partitions in updateStateByKey) ?


,
p
t)
=
s.java:183)*
)*
*
SerializableWritable.scala:43)*
la:39)*
*
:57)*
mpl.java:43)*
*
*
alizer.scala:62)*
dcast.scala:236)*
cV$sp(TorrentBroadcast.scala:169)*
cala:155)*
*
mpl.java:43)*
*
*
)*
*
ng,
ateByKey(updateFunc).foreachRDD(_.foreachPartition(RedisHelper.update(_)))*
f
B*
""*
 Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â·
rd
Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â·
· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â·
"
Sean Owen <sowen@cloudera.com>,"Wed, 22 Apr 2015 19:13:15 +0100",Should we let everyone set Assignee?,dev <dev@spark.apache.org>,"Anecdotally, there are a number of people asking to set the Assignee
field. This is currently restricted to Committers in JIRA. I know the
logic was to prevent people from Assigning a JIRA and then leaving it;
it also matters a bit for questions of ""credit"".

Still I wonder if it's best to just let people go ahead and set it, as
the lesser ""evil"". People can already do a lot like resolve JIRAs and
set shepherd and critical priority and all that.

I think the intent was to let ""Developers"" set this, but maybe due to
an error, that's not how the current JIRA permission is implemented.

I ask because I'm about to ping INFRA to update our scheme.

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Wed, 22 Apr 2015 11:30:00 -0700",Re: Dataframe.fillna from 1.3.0,Olivier Girardot <o.girardot@lateral-thoughts.com>,"It is actually different.

coalesce expression is to pick the first value that is not null:
https://msdn.microsoft.com/en-us/library/ms190349.aspx

Would be great to update the documentation for it (both Scala and Java) to
explain that it is different from coalesce function on a DataFrame/RDD. Do
you want to submit a pull request?




Ã©crit :
"
Patrick Wendell <pwendell@gmail.com>,"Wed, 22 Apr 2015 12:32:49 -0700",Re: Should we let everyone set Assignee?,Sean Owen <sowen@cloudera.com>,"X"" in JIAR means from a process perspective. Personally I actually
feel it's better for this to be more historical - i.e. who ended up
submitting a patch for this feature that was merged - rather than
creating an exclusive reservation for a particular user to work on
something.

If an issue is ""assigned"" to person X, but some other person Y submits
a great patch for it, I think we have some obligation to Spark users
and to the community to merge the better patch. So the idea of
reserving the right to add a feature, it just seems overall off to me.
IMO, its fine if multiple people want to submit competing patches for
something, provided everyone comments on JIRA saying they are
intending to submit a patch, and everyone understands there is
duplicate effort. So commenting with an intention to submit a patch,
IMO seems like the healthiest workflow since it is non exclusive.

To me the main benefit of ""assigning"" something ahead of time is if
you have a committer that really wants to see someone specific work on
a patch, it just acts as a strong signal that there is someone
endorsed to work on that patch. That doesn't mean no one else can
submit a patch, but it is IMO more of a warning that there may be
existing work which is likely to be high quality, to avoid duplicated
effort.

When it was really easy to assign features to themselves, I saw a lot
of anti-patterns in the community that seemed unhealthy, specifically:

- It was really unclear what it means semantically if someone is
assigned to a JIRA.
- People assign JIRA's to themselves that aren't a good fit, given the
authors level of experience.
- People expect if they assign JIRA's to themselves that others won't
submit patches, and become upset if they do.
- People are discouraged from working on a patch because someone else
was officially assigned.

- Patrick


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Wed, 22 Apr 2015 21:04:19 +0100",Re: Should we let everyone set Assignee?,Patrick Wendell <pwendell@gmail.com>,"I can get behind that point of view too. That's what I've told people
who expect Assignee is a necessary part of workflow. The existence of
a PR link is a signal someone's working on it.

In that case we need not do anything.


---------------------------------------------------------------------


"
Vinod Kumar Vavilapalli <vinodkv@hortonworks.com>,"Wed, 22 Apr 2015 20:11:11 +0000",Re: Should we let everyone set Assignee?,Patrick Wendell <pwendell@gmail.com>,"Actually what this community got away with is pretty much an anti-pattern compared to every other Apache project I have seen. And may I say in a not so Apache way.

Waiting for a committer to assign a patch to someone leaves it as a privilege to a committer. Not alluding to anything fishy in practice, but this also leaves a lot of open ground for self-interest. Committers defining notions of good fit / level of experience do not work, highly subjective and lead to group control.

In terms of semantics, here is what most other projects (dare I say every Apache project?) that I have seen do
 - A new contributor comes in who is not yet added to the JIRA project. He/she requests one of the project's JIRA admins to add him/her.
 - After that, he or she is free to assign tickets to themselves.
 - What this means
    -- Assigning a ticket to oneself is a signal to the rest of the community that he/she is actively working on the said patch.
    -- If multiple contributors want to work on the same patch, it needs to. Not by the whim of a committer.
 - Common issues
    -- Land grabbing: Other contributors can nudge him/her in case of inactivity and take them over. Again, amicably instead of a committer making subjective decisions.
self is actively debating but with no real code/docs contribution or with any real intention of making progress. Here workable, reviewable code for review usually wins.

Assigning patches is not a privilege. Contributors at Apache are a bunch of volunteers, the PMC should let volunteers contribute as they see fit. We do not assign work at Apache.

+Vinod




---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Wed, 22 Apr 2015 13:18:37 -0700",Re: Should we let everyone set Assignee?,Vinod Kumar Vavilapalli <vinodkv@hortonworks.com>,"Woh hold on a minute.

Spark has been among the projects that are the most welcoming to new
contributors. And thanks to this, the sheer number of activities in Spark
is much larger than other projects, and our workflow has to accommodate
this fact.

In practice, people just create pull requests on github, which is a newer &
friendlier & better model given the constraints. We even have tools that
automatically tags a ticket with a link to the pull requests.



"
Patrick Wendell <pwendell@gmail.com>,"Wed, 22 Apr 2015 13:20:15 -0700",Re: Should we let everyone set Assignee?,Vinod Kumar Vavilapalli <vinodkv@hortonworks.com>,"Hi Vinod,

Thanks for you thoughts - However, I do not agree with your sentiment
and implications. Spark is broadly quite an inclusive project and we
spend a lot of effort culturally to help make newcomers feel welcome.

- Patrick

 compared to every other Apache project I have seen. And may I say in a not so Apache way.
lege to a committer. Not alluding to anything fishy in practice, but this also leaves a lot of open ground for self-interest. Committers defining notions of good fit / level of experience do not work, highly subjective and lead to group control.
 Apache project?) that I have seen do
e/she requests one of the project's JIRA admins to add him/her.
nity that he/she is actively working on the said patch.
ts. Not by the whim of a committer.
ctivity and take them over. Again, amicably instead of a committer making subjective decisions.
erself is actively debating but with no real code/docs contribution or with any real intention of making progress. Here workable, reviewable code for review usually wins.
of volunteers, the PMC should let volunteers contribute as they see fit. We do not assign work at Apache.

---------------------------------------------------------------------


"
Mark Hamstra <mark@clearstorydata.com>,"Wed, 22 Apr 2015 13:25:04 -0700",Re: Should we let everyone set Assignee?,Patrick Wendell <pwendell@gmail.com>,"Agreed.  The Spark project and community that Vinod describes do not
resemble the ones with which I am familiar.


"
Vinod Kumar Vavilapalli <vinodkv@hortonworks.com>,"Wed, 22 Apr 2015 20:29:00 +0000",Re: Should we let everyone set Assignee?,Reynold Xin <rxin@databricks.com>,"
If it is true what you say, what is the reason for this committer-only-assigns-JIRA tickets policy? If anyone can send a pull request, anyone should be able to assign tickets to himself/herself too.

+Vinod


Woh hold on a minute.

Spark has been among the projects that are the most welcoming to new contributors. And thanks to this, the sheer number of activities in Spark is much larger than other projects, and our workflow has to accommodate this fact.

In practice, people just create pull requests on github, which is a newer & friendlier & better model given the constraints. We even have tools that automatically tags a ticket with a link to the pull requests.


Actually what this community got away with is pretty much an anti-pattern compared to every other Apache project I have seen. And may I say in a not so Apache way.

Waiting for a committer to assign a patch to someone leaves it as a privilege to a committer. Not alluding to anything fishy in practice, but this also leaves a lot of open ground for self-interest. Committers defining notions of good fit / level of experience do not work, highly subjective and lead to group control.

In terms of semantics, here is what most other projects (dare I say every Apache project?) that I have seen do
 - A new contributor comes in who is not yet added to the JIRA project. He/she requests one of the project's JIRA admins to add him/her.
 - After that, he or she is free to assign tickets to themselves.
 - What this means
    -- Assigning a ticket to oneself is a signal to the rest of the community that he/she is actively working on the said patch.
    -- If multiple contributors want to work on the same patch, it needs to. Not by the whim of a committer.
 - Common issues
    -- Land grabbing: Other contributors can nudge him/her in case of inactivity and take them over. Again, amicably instead of a committer making subjective decisions.
self is actively debating but with no real code/docs contribution or with any real intention of making progress. Here workable, reviewable code for review usually wins.

Assigning patches is not a privilege. Contributors at Apache are a bunch of volunteers, the PMC should let volunteers contribute as they see fit. We do not assign work at Apache.

+Vinod


bscribe@spark.apache.org>
lp@spark.apache.org>
scribe@spark.apache.org>
p@spark.apache.org>


---------------------------------------------------------------------
ribe@spark.apache.org>
spark.apache.org>



"
Sean Owen <sowen@cloudera.com>,"Wed, 22 Apr 2015 21:28:51 +0100",Re: Should we let everyone set Assignee?,Vinod Kumar Vavilapalli <vinodkv@hortonworks.com>,"I think you misread the thread, since that's the opposite of what
Patrick suggested. He's suggesting that *nobody ever waits* to be
assigned a JIRA to work on it; that anyone may work on a JIRA without
waiting for it to be assigned.

The point is: assigning JIRAs discourages others from doing work and
we don't want to do that. So the pattern so far has been to not use it
(except retroactively to credit the major contributor to the
resolution.)

The cost of this policy is -- oops, maybe you work on something that's
already being worked on. That isn't a problem in practice. We already
have a way to signal that you're working on a patch: you open a PR. It
automatically links to JIRA. Or you can just comment.

I suppose you could also use Assignee as a strong signal that your'e
working on it, and some people want to do that, and so I was floating
the idea of just letting people use it as they like. But I also back
the idea of not having a notion of ""owner"" of working on a JIRA.

 compared to every other Apache project I have seen. And may I say in a not so Apache way.
lege to a committer. Not alluding to anything fishy in practice, but this also leaves a lot of open ground for self-interest. Committers defining notions of good fit / level of experience do not work, highly subjective and lead to group control.
 Apache project?) that I have seen do
e/she requests one of the project's JIRA admins to add him/her.
nity that he/she is actively working on the said patch.
ts. Not by the whim of a committer.
ctivity and take them over. Again, amicably instead of a committer making subjective decisions.
erself is actively debating but with no real code/docs contribution or with any real intention of making progress. Here workable, reviewable code for review usually wins.
of volunteers, the PMC should let volunteers contribute as they see fit. We do not assign work at Apache.

---------------------------------------------------------------------


"
Ram Sriharsha <harshars@yahoo-inc.com.INVALID>,"Wed, 22 Apr 2015 20:29:34 +0000 (UTC)",Re: Should we let everyone set Assignee?,"Mark Hamstra <mark@clearstorydata.com>,
        Patrick Wendell <pwendell@gmail.com>","agreed, what Patrick suggests seems very reasonable. 


   

 Agreed.Â  The Spark project and community that Vinod describes do not
resemble the ones with which I am familiar.

:

I
ject.
f the
, it needs
case of
o
,
h
We
:
;
s


  "
"""Ganelin, Ilya"" <Ilya.Ganelin@capitalone.com>","Wed, 22 Apr 2015 16:30:45 -0400",Re: Should we let everyone set Assignee?,"Mark Hamstra <mark@clearstorydata.com>, Patrick Wendell
	<pwendell@gmail.com>","As a contributor, I¹ve never felt shut out from the Spark community, nor
have I seen any examples of territorial behavior. A few times I¹ve
expressed interest in more challenging work and the response I received
was generally ³go ahead and give it a shot, just understand that this is
sensitive code so we may end up modifying the PR substantially.² Honestly,
that seems fine, and in general, I think it¹s completely fair to go with
the PR model - e.g. If a JIRA has an open PR then it¹s an active effort,
otherwise it¹s fair game unless otherwise stated. At the end of the day,
it¹s about moving the project forward and the only way to do that is to
have actual code in the pipes -speculation and intent don¹t really help,
and there¹s nothing preventing an interested party from submitting a PR
against an issue. 

Thank you, 
Ilya Ganelin









________________________________________________________

The information contained in this e-mail is confidential and/or proprietary is intended only for use by the individual or entity to which it is addressed.  If the reader of this message is not the intended recipient, you are hereby notified that any review, retransmission, dissemination, distribution, copying or other use of, or taking of any action in reliance upon this information is strictly prohibited. If you have received this communication in error, please contact the sender and delete the material from your computer.


---------------------------------------------------------------------


"
Sandy Ryza <sandy.ryza@cloudera.com>,"Wed, 22 Apr 2015 14:08:09 -0700",Re: Should we let everyone set Assignee?,"""Ganelin, Ilya"" <Ilya.Ganelin@capitalone.com>","I think one of the benefits of assignee fields that I've seen in other
projects is their potential to coordinate and prevent duplicate work.  It's
really frustrating to put a lot of work into a patch and then find out that
someone has been doing the same.  It's helpful for the project etiquette to
include a way to signal to others that you are working or intend to work on
a patch.  Obviously there are limits to how long someone should be able to
hold on to a JIRA without making progress on it, but a signal is still
useful.  Historically, in other projects, the assignee field serves as this
signal.  If we don't want to use the assignee field for this, I think it's
important to have some alternative, even if it's just encouraging
contributors to comment ""I'm planning to work on this"" on JIRA.

-Sandy




 nor
s is
estly,
with
ort,
day,
 to
elp,
 PR
g
f
o
s
or
,
ed
ot
't
se
ee
,
d.
--
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 22 Apr 2015 21:33:39 +0000",Re: Should we let everyone set Assignee?,"""Ganelin, Ilya"" <Ilya.Ganelin@capitalone.com>, Mark Hamstra <mark@clearstorydata.com>, 
	Patrick Wendell <pwendell@gmail.com>","To repeat what Patrick said (literally):

If an issue is â€œassignedâ€ to person X, but some other person Y submits
a great patch for it, I think we have some obligation to Spark users
and to the community to merge the better patch. So the idea of
reserving the right to add a feature, it just seems overall off to me.

No-one in the Spark community dictates who gets to do work. When an issue
is assigned to someone in JIRA, itâ€™s either because a) they did the work
and the issue is now resolved, or b) they are signaling to others that they
are working on it.

In the case of b), nothing stops other people from working on the issue and
itâ€™s quite normal for other people to complete issues that were technically
assigned to someone else. There is no land grabbing or stalling. Anyone who
has contributed to Spark for any amount of time knows this.

Vinod,

I want to take this opportunity to call out the approach to communication
you took here.

As a random contributor to Spark and active participant on this list, my
reaction when I read your email was this:

   - You do not know how the Spark community actually works.
   - You read a thread that contains some trigger phrases.
   - You wrote a lengthy response as a knee-jerk reaction.

Iâ€™m not trying to mock, but I want to be direct and honest about how you
came off in this thread to me and probably many others.

Why not ask questions firstâ€”many questions? Why not make doubly sure that
you understand the situation correctly before responding?

In many ways this is much like filing a bug report. â€œIâ€™m seeing this. It
seems wrong to me. Is this expected?â€ I think we all know from experience
that this kind of bug report is polite and will likely lead to a productive
 This is
obviously wrong! And, boy, lemme tell you how wrong you are!!!â€ No-one
likes to deal with bug reports like this. More importantly, they get in the
way of fixing the actual problem, if there is one.

This is not about the Apache Way or not. Itâ€™s about basic etiquette and
effective communication.

I understand that there are legitimate potential concerns here, and itâ€™s
important that, as an Apache project, Spark work according to Apache
principles. But when some person who has never participated on this list
pops up out of nowhere with a lengthy lecture on the Apache Way and
whatnot, I have to say that that is not an effective way to communicate.
Pretty much the same thing happened with Greg Stein on an earlier thread
some months ago about designating maintainers for components.

The concerns are legitimate, Iâ€™m sure, and we want to keep Spark in line
with the Apache Way. And certainly, there have been many times when a
project veered off course and needed to corrected.

But when we want to make things right, I hope we can do it in a way that
respectfully and tactfully engages the community. These â€œlectures delivered
from aboveâ€ â€” which is how they come off â€” are not helpful.

Nick
â€‹


 nor
s is
estly,
with
ort,
day,
 to
elp,
 PR
g
f
o
s
or
,
ed
ot
't
se
ee
,
d.
--
"
Patrick Wendell <pwendell@gmail.com>,"Wed, 22 Apr 2015 14:37:31 -0700",Re: Should we let everyone set Assignee?,Nicholas Chammas <nicholas.chammas@gmail.com>,"Sandy - I definitely agree with that. We should have a convention of
signaling someone intends to work - for instance by commenting on the
JIRA and we should document this on the contribution guide. The nice
thing about having that convention is that multiple people can say
they are going to work on something, whereas only one person can be
given the assignee slot on a JIRA.


 is
re
nd
ly
ho
t
ve
kes
 of
ot,
ch
ago
ed
m>
or
is
tly,
th
t,
y,
o
p,
R
y
ng
of
to
p
rs
h,
f
k
g
t,
e
---
y to
d
have
te

---------------------------------------------------------------------


"
Vinod Kumar Vavilapalli <vinodkv@hortonworks.com>,"Wed, 22 Apr 2015 21:51:38 +0000",Re: Should we let everyone set Assignee?,Nicholas Chammas <nicholas.chammas@gmail.com>,"
I watch these lists, so I have a fair understanding of how things work around here. I don't give direct input in the day to day activities though, like Greg Stein on the other thread, so I can understand if it looks like it came from up above. Apache Members come around and give opinions time to time, you don't need to take it as somebody up above forcing things down.

Thanks
+Vinod


I want to take this opportunity to call out the approach to communication you took here.

As a random contributor to Spark and active participant on this list, my reaction when I read your email was this:

  *   You do not know how the Spark community actually works.
  *   You read a thread that contains some trigger phrases.
  *   You wrote a lengthy response as a knee-jerk reaction.

I’m not trying to mock, but I want to be direct and honest about how you came off in this thread to me and probably many others.

Why not ask questions first—many questions? Why not make doubly sure that you understand the situation correctly before responding?

In many ways this is much like filing a bug report. “I’m seeing this. It seems wrong to me. Is this expected?” I think we all know from experience that this kind of bug report is polite and will likely lead to a product obviously wrong! And, boy, lemme tell you how wrong you are!!!” No-one likes to deal with bug reports like this. More importantly, they get in the way of fixing the actual problem, if there is one.

This is not about the Apache Way or not. It’s about basic etiquette and effective communication.

I understand that there are legitimate potential concerns here, and it’s important that, as an Apache project, Spark work according to Apache principles. But when some person who has never participated on this list pops up out of nowhere with a lengthy lecture on the Apache Way and whatnot, I have to say that that is not an effective way to communicate. Pretty much the same thing happened with Greg Stein on an earlier thread some months ago about designating maintainers for components.

The concerns are legitimate, I’m sure, and we want to keep Spark in line with the Apache Way. And certainly, there have been many times when a project veered off course and needed to corrected.

But when we want to make things right, I hope we can do it in a way that respectfully and tactfully engages the community. These “lectures delivered from above” — which is how they come off — are not helpful.

Nick
"
Vinod Kumar Vavilapalli <vinodkv@hortonworks.com>,"Wed, 22 Apr 2015 21:58:36 +0000",Re: Should we let everyone set Assignee?,Patrick Wendell <pwendell@gmail.com>,"
Last one for the day.

Everyone, as I said clearly, I was ""not alluding to anything fishy in practice"", I was describing how things go wrong in such an environment. Sandy's email lays down some of these problems.

Assigning a JIRA in other projects is not a reservation. It is a clear intention of working on design or code.

You don't need a new convention of signaling. In almost all other projects, it is assigning tickets - that's how it is used.

+Vinod


e is
d
are
and
lly
who
n
at
e
ive
ikes
y of
not,
uch
 ago
red
om>
nor
 is
stly,
ith
rt,
ay,
to
lp,
PR
y
ng
o
s
,
,
--
ty to
ed
 have
ete


---------------------------------------------------------------------


"
jimfcarroll <jimfcarroll@gmail.com>,"Wed, 22 Apr 2015 18:00:00 -0700 (MST)",GradientBoostTrees leaks a persisted RDD,dev@spark.apache.org,"Hi all,

It appears GradientBoostedTrees.scala can call 'persist' on an RDD and never
unpersist it. In the master branch it's here:

https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/tree/GradientBoostedTrees.scala#L181

In 1.3.1 it's here:

https://github.com/apache/spark/blob/v1.3.1/mllib/src/main/scala/org/apache/spark/mllib/tree/GradientBoostedTrees.scala#L138

Let me know if you want a fix for this.

Jim




--

---------------------------------------------------------------------


"
Joseph Bradley <joseph@databricks.com>,"Wed, 22 Apr 2015 19:18:39 -0700",Re: GradientBoostTrees leaks a persisted RDD,jimfcarroll <jimfcarroll@gmail.com>,"Hi Jim,

You're right; that should be unpersisted.  Could you please create a JIRA
and submit a patch?

Thanks!
Joseph


"
Joseph Bradley <joseph@databricks.com>,"Wed, 22 Apr 2015 19:31:37 -0700",Re: Indices of SparseVector must be ordered while computing SVD,Chunnan Yao <yaochunnan@gmail.com>,"Hi Chunnan,

There is currently Scala documentation for the constructor parameters:
https://github.com/apache/spark/blob/04525c077c638a7e615c294ba988e35036554f5f/mllib/src/main/scala/org/apache/spark/mllib/linalg/Vectors.scala#L515

There is one benefit to not checking for validity (ordering) within the
constructor: If you need to translate between SparseVector and some other
library's type (e.g., Breeze), you can do so with a few reference copies,
rather than iterating through or copying the actual data.  It might be good
to provide this check within Vectors.sparse(), but we'd need to check
through MLlib for uses of Vectors.sparse which expect it to be a cheap
operation.  What do you think?

It is documented in the programming guide too:
https://github.com/apache/spark/blob/04525c077c638a7e615c294ba988e35036554f5f/docs/mllib-data-types.md
But perhaps that should be more prominent.

If you think it would be helpful, then please do make a JIRA about adding a
check to Vectors.sparse().

Joseph


"
"""daniel.mescheder"" <daniel.mescheder@realimpactanalytics.com>","Thu, 23 Apr 2015 01:30:18 -0700 (MST)","In Spark-SQL, is there support for distributed execution of native
 Hive UDAFs?",dev@spark.apache.org,"Hi everyone,

I was playing with the integration of Hive UDAFs in Spark-SQL and noticed that the terminatePartial and merge methods of custom UDAFs were not called. This made me curious as those two methods are the ones responsible for distributing the UDAF execution in Hive.
Looking at the code of HiveUdafFunction which seems to be the wrapper for all native Hive functions for which there exists no spark-sql specific implementation, I noticed that it

a) extends AggregateFunction and not PartialAggregate
b) only contains calls to iterate and evaluate, but never to merge of the underlying UDAFEvaluator object

My question is thus twofold: Is my observation correct, that to achieve distributed execution of a UDAF I have to add a custom implementation at the spark-sql layer (like the examples in aggregates.scala)? If that is the case, how difficult would it be to use the terminatePartial and merge functions provided by the UDAFEvaluator to make Hive UDAFs distributed by default?


Cheers,

Daniel



--"
Reynold Xin <rxin@databricks.com>,"Thu, 23 Apr 2015 01:35:16 -0700","Re: In Spark-SQL, is there support for distributed execution of
 native Hive UDAFs?","""daniel.mescheder"" <daniel.mescheder@realimpactanalytics.com>","Your understanding is correct -- there is no partial aggregation currently
for Hive UDAF.

However, there is a PR to fix that:
https://github.com/apache/spark/pull/5542




"
Sean Owen <sowen@cloudera.com>,"Thu, 23 Apr 2015 11:05:51 +0100",Re: Indices of SparseVector must be ordered while computing SVD,Joseph Bradley <joseph@databricks.com>,"I think we discussed this a while ago (?) and the problem was the
overhead of even verifying the sorted state took too long.


---------------------------------------------------------------------


"
Sourav Chandra <sourav.chandra@livestream.com>,"Thu, 23 Apr 2015 15:46:25 +0530",Re: Spark Streaming updatyeStateByKey throws OutOfMemory Error,Tathagata Das <tdas@databricks.com>,"HI TD,

Some observations:

1. If I submit the application using spark-submit tool with *client as
deploy mode* it works fine with single master and worker (driver, master
and worker are running in same machine)
2. If I submit the application using spark-submit tool with client as
deploy mode it *crashes after some time with  StackOverflowError* *single
master and 2 workers* (driver, master and 1 worker is running in same
machine, other
worker is in different machine)
     *15/04/23 05:42:04 Executor: Exception in task 0.0 in stage 23153.0
(TID 5412)*
*java.lang.StackOverflowError*
*        at
java.io.ObjectInputStream$BlockDataInputStream.readUTF(ObjectInputStream.java:2864)*
*        at java.io.ObjectInputStream.readUTF(ObjectInputStream.java:1072)*
*        at
java.io.ObjectStreamClass.readNonProxy(ObjectStreamClass.java:671)*
*        at
java.io.ObjectInputStream.readClassDescriptor(ObjectInputStream.java:830)*
*        at
java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1601)*
*        at
java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1517)*
*        at
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1771)*
*        at
java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)*
*        at
java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)*
*        at
java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)*
*        at
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)*
*        at
java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)*
*        at
java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)*
*        at
java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)*
*        at
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)*
*        at
java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)*
*        at
java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)*
*        at
scala.collection.immutable.$colon$colon.readObject(List.scala:362)*
*        at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)*
*        at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)*
*        at java.lang.reflect.Method.invoke(Method.java:606)*
*        at
java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)*
*        at
java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)*
*        at
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)*
*        at
java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)*
*        at
java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)*
*        at
java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)*
*        at
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)*
*        at
java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)*
*        at
java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)*
*        at
java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)*
*        at
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)*
*        at
java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)*
*        at
java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)*
*        at
scala.collection.immutable.$colon$colon.readObject(List.scala:362)*
*        at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)*
*        at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)*
*        at java.lang.reflect.Method.invoke(Method.java:606)*
*        at
java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)*
*        at
java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)*
*        at
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)*
*        at
java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)*
*        at
java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)*
*        at
java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)*
*        at
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)*
*        at
java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)*
*        at
java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)*
*        at
java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)*
*        at
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)*
*        at
java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)*
*        at
java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)*
*        at
scala.collection.immutable.$colon$colon.readObject(List.scala:366)*
*        at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)*


3. If I submit the application using spark-submit tool with *cluster as
deploy mode* it *crashes after some time with Out Of Memory error  *with
single master and worker (driver, master and worker are running in same
machine)

*   15/04/23 05:47:49 Executor: Exception in task 0.0 in stage 858.0 (TID
1464)*
*java.lang.OutOfMemoryError: Java heap space*
*        at
org.apache.hadoop.io.WritableUtils.readCompressedStringArray(WritableUtils.java:183)*
*        at
org.apache.hadoop.conf.Configuration.readFields(Configuration.java:2564)*
*        at
org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:285)*
*        at
org.apache.hadoop.io.ObjectWritable.readFields(ObjectWritable.java:77)*
*        at
org.apache.spark.SerializableWritable$$anonfun$readObject$1.apply$mcV$sp(SerializableWritable.scala:43)*
*        at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:927)*
*        at
org.apache.spark.SerializableWritable.readObject(SerializableWritable.scala:39)*
*        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)*
*        at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)*
*        at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)*
*        at java.lang.reflect.Method.invoke(Method.java:606)*
*        at
java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)*
*        at
java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)*
*        at
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)*
*        at
java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)*
*        at
java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)*
*        at
com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:40)*
*        at
com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)*
*        at
org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:133)*
*        at
org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:236)*
*        at
org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readObject$1.apply$mcV$sp(TorrentBroadcast.scala:169)*
*        at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:927)*
*        at
org.apache.spark.broadcast.TorrentBroadcast.readObject(TorrentBroadcast.scala:155)*
*        at sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)*
*        at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)*
*        at java.lang.reflect.Method.invoke(Method.java:606)*
*        at
java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)*
*        at
java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)*
*        at
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)*
*        at
java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)*
*        at
java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)*
*        at
java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)*

Thanks,
Sourav

:

e
g,
op
=
ls.java:183)*
*
)*
*
(SerializableWritable.scala:43)*
ala:39)*
)*
a:57)*
Impl.java:43)*
)*
)*
ializer.scala:62)*
adcast.scala:236)*
mcV$sp(TorrentBroadcast.scala:169)*
scala:155)*
)*
Impl.java:43)*
)*
)*
*
)*
,
ong,
tateByKey(updateFunc).foreachRDD(_.foreachPartition(RedisHelper.update(_)))*
*
of
MB*
r""*
d
t
· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â·
Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â·
d
Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â·


-- 

Sourav Chandra

Senior Software Engineer

Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â·

sourav.chandra@livestream.com

o: +91 80 4121 8723

m: +91 988 699 3746

skype: sourav.chandra

Livestream

""Ajmera Summit"", First Floor, #3/D, 68 Ward, 3rd Cross, 7th C Main, 3rd
Block, Koramangala Industrial Area,

Bangalore 560034

www.livestream.com
"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Thu, 23 Apr 2015 11:31:22 +0000",Re: Dataframe.fillna from 1.3.0,"Olivier Girardot <o.girardot@lateral-thoughts.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","Yep no problem, but I can't seem to find the coalesce fonction in
pyspark.sql.{*, functions, types or whatever :) }

Olivier.

Le lun. 20 avr. 2015 Ã  11:48, Olivier Girardot <
o.girardot@lateral-thoughts.com> a Ã©crit :

"
Sean Owen <sowen@cloudera.com>,"Thu, 23 Apr 2015 07:47:59 -0400","Contributors, read me! Updated Contributing to Spark wiki","""user@spark.apache.org"" <user@spark.apache.org>, dev <dev@spark.apache.org>","Following several discussions about how to improve the contribution
process in Spark, I've overhauled the guide to contributing. Anyone
who is going to contribute needs to read it, as it has more formal
guidance about the process:

https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark

We may push back harder now on pull requests and JIRAs that don't
follow this guidance. It will help everyone spend less time to get
changes in, and spend less time on duplicated effort, or changes that
won't.

A summary of key points is found in CONTRIBUTING.md, a prompt
presented before opening pull requests
(https://github.com/apache/spark/blob/master/CONTRIBUTING.md):

- Is the change important and ready enough to ask the community to
spend time reviewing?
- Have you searched for existing, related JIRAs and pull requests?
- Is this a new feature that can stand alone as a package on
http://spark-packages.org ?
- Is the change being proposed clearly explained and motivated?

---------------------------------------------------------------------


"
jimfcarroll <jimfcarroll@gmail.com>,"Thu, 23 Apr 2015 07:53:09 -0700 (MST)",Re: GradientBoostTrees leaks a persisted RDD,dev@spark.apache.org,"Hi Joe,

Do you want a PR per branch (one for master, one for 1.3)? Are you still
maintaining 1.2? Do you need a Jira ticket per PR or can I submit them all
under the same ticket?

Or should I just submit it to master and let you guys back-port it?

Jim




--

---------------------------------------------------------------------


"
jimfcarroll <jimfcarroll@gmail.com>,"Thu, 23 Apr 2015 08:12:53 -0700 (MST)",Re: GradientBoostTrees leaks a persisted RDD,dev@spark.apache.org,"Hi Sean and Joe,

I have another question. 

GradientBoostedTrees.run iterates over the RDD calling DecisionTree.run on
each iteration with a new random sample from the input RDD. DecisionTree.run
calls RandomForest.run. which also calls persist.


Should I simply remove the persist call at the GradientBoostedTrees level?

Thanks
Jim




--

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Thu, 23 Apr 2015 12:16:59 -0400",Re: GradientBoostTrees leaks a persisted RDD,jimfcarroll <jimfcarroll@gmail.com>,"Those are different RDDs that DecisionTree persists, though. It's not redundant.


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Thu, 23 Apr 2015 10:31:07 -0700",Re: Dataframe.fillna from 1.3.0,Olivier Girardot <o.girardot@lateral-thoughts.com>,"Ah damn. We need to add it to the Python list. Would you like to give it a
shot?



PI
"
Mingyu Kim <mkim@palantir.com>,"Thu, 23 Apr 2015 18:02:58 +0000",Re: [discuss] new Java friendly InputSource API,"Soren Macbeth <soren@yieldbot.com>, Reynold Xin <rxin@databricks.com>","Hi Reynold,

You mentioned that the new API allows arbitrary code to be run on the
driver side, but it¹s not very clear to me how this is different from what
Hadoop API provides. In your example of using broadcast, did you mean
broadcasting something in InputSource.getPartitions() and having
InputPartitions use the broadcast variables? Isn¹t that already possible
with Hadoop's InputFormat.getSplits()?

Thanks,
Mingyu






ji
b6oO
NAKk
=
sp
r=en
qD


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Thu, 23 Apr 2015 11:09:29 -0700",Re: [discuss] new Java friendly InputSource API,Mingyu Kim <mkim@palantir.com>,"In the ctor of InputSource (I'm also considering adding an explicit
initialize call), the implementation of InputSource can execute arbitrary
code. The state in it will also be serialized and passed onto the executors.

Yes - technically you can hijack getSplits in Hadoop InputFormat to do the
same thing, and then put a reference of the state into every Split. But
that's kind of awkward. Hadoop relies on the giant Configuration object to
pass state over.




m what
ible
va
:
t
f
is
ta
e
ji
tFb6oO
CHNAKk
&e=
sp
8&r=en
MrqD
"
jimfcarroll <jimfcarroll@gmail.com>,"Thu, 23 Apr 2015 12:22:45 -0700 (MST)",Re: GradientBoostTrees leaks a persisted RDD,dev@spark.apache.org,"
Okay.

PR: https://github.com/apache/spark/pull/5669

Jira: https://issues.apache.org/jira/browse/SPARK-7100

Hope that helps.

Let me know if you need anything else.

Jim




--

---------------------------------------------------------------------


"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Thu, 23 Apr 2015 20:06:36 +0000",Re: Dataframe.fillna from 1.3.0,"Reynold Xin <rxin@databricks.com>, Olivier Girardot <o.girardot@lateral-thoughts.com>","yep :) I'll open the jira when I've got the time.
Thanks

Le jeu. 23 avr. 2015 Ã  19:31, Reynold Xin <rxin@databricks.com> a Ã©crit :

a
"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Thu, 23 Apr 2015 20:17:33 +0000",Re: Dataframe.fillna from 1.3.0,"Olivier Girardot <o.girardot@lateral-thoughts.com>, Reynold Xin <rxin@databricks.com>","What is the way of testing/building the pyspark part of Spark ?

Le jeu. 23 avr. 2015 Ã  22:06, Olivier Girardot <
o.girardot@lateral-thoughts.com> a Ã©crit :

Ã©crit :
s
"
Reynold Xin <rxin@databricks.com>,"Thu, 23 Apr 2015 13:20:17 -0700",Re: Dataframe.fillna from 1.3.0,Olivier Girardot <o.girardot@lateral-thoughts.com>,"You need to first have the Spark assembly jar built with ""sbt/sbt
assembly/assembly""

Then usually I go into python/run-tests and comment out the non-SQL tests:

#run_core_tests
run_sql_tests
#run_mllib_tests
#run_ml_tests
#run_streaming_tests

And then you can run ""python/run-tests""





Ã©crit :
t
*
ds
"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Thu, 23 Apr 2015 20:59:42 +0000",Re: Dataframe.fillna from 1.3.0,"Olivier Girardot <o.girardot@lateral-thoughts.com>, Reynold Xin <rxin@databricks.com>","I found another way setting a SPARK_HOME on a released version and
launching an ipython to load the contexts.
I may need your insight however, I found why it hasn't been done at the
same time, this method (like some others) uses a varargs in Scala and for
now the way functions are called only one parameter is supported.

So at first I tried to just generalise the helper function ""_"" in the
functions.py file to multiple arguments, but py4j's handling of varargs
forces me to create an Array[Column] if the target method is expecting
varargs.

But from Python's perspective, we have no idea of whether the target method
will be expecting varargs or just multiple arguments (to un-tuple).
I can create a special case for ""coalesce"" or ""for method that takes of
list of columns as arguments"" considering they will be varargs based (and
therefore needs an Array[Column] instead of just a list of arguments)

But this seems very specific and very prone to future mistakes.
Is there any way in Py4j to know before calling it the signature of a
method ?


Le jeu. 23 avr. 2015 Ã  22:17, Olivier Girardot <
o.girardot@lateral-thoughts.com> a Ã©crit :

Ã©crit :
t
*
ds
"
Joseph Bradley <joseph@databricks.com>,"Thu, 23 Apr 2015 14:33:54 -0700",Re: GradientBoostTrees leaks a persisted RDD,jimfcarroll <jimfcarroll@gmail.com>,"I saw the PR already, but only saw this just now.  I think both persists
are useful based on my experience, but it's very hard to say in general.


"
Reynold Xin <rxin@databricks.com>,"Thu, 23 Apr 2015 15:09:07 -0700",Re: Dataframe.fillna from 1.3.0,Olivier Girardot <o.girardot@lateral-thoughts.com>,"You can do it similar to the way countDistinct is done, can't you?

https://github.com/apache/spark/blob/master/python/pyspark/sql/functions.py#L78




.
"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Thu, 23 Apr 2015 22:12:23 +0000",RE: Should we let everyone set Assignee?,"Vinod Kumar Vavilapalli <vinodkv@hortonworks.com>, Patrick Wendell
	<pwendell@gmail.com>","My thinking is that current way of assigning a contributor after the patch is done (or almost done) is OK. Parallel efforts are also OK until they are discussed in the issue's thread. Ilya Ganelin made a good point that it is about moving the project forward. It also adds means of competition ""who make it faster/better"" which is also good for the project and community. My only concern is about the throughput of Databricks folks who monitor issues, check patches and assign a contributor. Monitoring should be done on a constant basis (weekly?).

Best regards, Alexander


Last one for the day.

Everyone, as I said clearly, I was ""not alluding to anything fishy in practice"", I was describing how things go wrong in such an environment. Sandy's email lays down some of these problems.

Assigning a JIRA in other projects is not a reservation. It is a clear intention of working on design or code.

You don't need a new convention of signaling. In almost all other projects, it is assigning tickets - that's how it is used.

+Vinod


to me.
knows this.
here is one.
onents.
 helpful.
 
 
 
 
 
issue.
s feel welcome.
uter.


---------------------------------------------------------------------
mands, e-mail: dev-help@spark.apache.org


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Thu, 23 Apr 2015 18:31:01 -0400",RE: Should we let everyone set Assignee?,Alexander Ulanov <alexander.ulanov@hp.com>,"The merge script automatically updates the linked JIRA after merging the PR
(why it is important to put the JIRA in the title). It can't auto assign
the JIRA since usernames dont match up but it is an easy reminder to set
the Assignee. I do right after and I think other committers do too.

I'll search later for Fixed and Unassigned JIRAs in case there are any.
Feel free to flag any.

In practice I think it is pretty rare that 2 people work on one JIRA
accidentally and can't remember a case where there was disagreement about
how to proceed. So I dont think a 'lock' is necessary in practice and don't
think even signaling has been a problem.

h
re
s
My
f
e
ity,
e I
tand
nk
s an
e unless
eÂ¹s
n
"
Sean Owen <sowen@cloudera.com>,"Thu, 23 Apr 2015 20:26:30 -0400",Let's set Assignee for Fixed JIRAs,dev <dev@spark.apache.org>,"Following my comment earlier that ""I think we set Assignee for Fixed
JIRAs consistently"", I found there are actually 880 counter examples.
Lots of them are old, and I'll try to fix as many that are recent (for
the 1.4.0 release credits) as I can stand to click through.

Let's set Assignee after resolving consistently though. In various
ways I've heard that people do really like the bit of credit, and I
don't think anybody disputed setting Assignee *after* it was resolved
as a way of giving credit.

People who know they're missing a credit are welcome to ping me
directly to get it fixed.

---------------------------------------------------------------------


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Thu, 23 Apr 2015 17:31:39 -0700",Re: Let's set Assignee for Fixed JIRAs,Sean Owen <sowen@cloudera.com>,"A related question that has affected me in the past: If we get a PR from a
new developer I sometimes find that I am not able to assign an issue to
them after merging the PR. Is there a process we need follow to get new
contributors on to a particular group in JIRA ? Or does it somehow happen
automatically ?

Thanks
Shivaram


"
"""Hari Shreedharan"" <hshreedharan@cloudera.com>","Thu, 23 Apr 2015 17:47:20 -0700 (PDT)",Re: Let's set Assignee for Fixed JIRAs,shivaram@eecs.berkeley.edu,"Youâ€™d need to add them as a contributor in the JIRA admin page. 



Thanks,Â Hari


a
happen"
Luciano Resende <luckbr1975@gmail.com>,"Thu, 23 Apr 2015 18:08:51 -0700",Re: Let's set Assignee for Fixed JIRAs,Hari Shreedharan <hshreedharan@cloudera.com>,"m

nce you
Is this documented, and does every PMC (or committer) have access to do
that ?


m
en



-- 
Luciano Resende
http://people.apache.org/~lresende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
Luciano Resende <luckbr1975@gmail.com>,"Thu, 23 Apr 2015 18:07:43 -0700",Re: Let's set Assignee for Fixed JIRAs,Sean Owen <sowen@cloudera.com>,"
+1, this will help with giving people the ""bit of credit"", but I guess it
also helps on recognizing the community contributors towards becoming
committers much easier.



-- 
Luciano Resende
http://people.apache.org/~lresende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
Sean Owen <sowen@cloudera.com>,"Thu, 23 Apr 2015 21:16:45 -0400",Re: Let's set Assignee for Fixed JIRAs,Luciano Resende <luckbr1975@gmail.com>,"Permission to change project roles is restricted to admins / PMC,
naturally. I don't know if it's documented beyond this that Gavin
helpfully pasted:
https://cwiki.apache.org/confluence/display/SPARK/Jira+Permissions+Scheme

Another option is to make the list of people who you can assign to
include ""Anyone"". You'd get all of Apache in the list, so I assume
that's why it isn't the default, but might be the easier thing at this
point in Spark. I had to add 6 new people just now while assigning ~40
JIRAs.

te:
hat
om
o
w
r

---------------------------------------------------------------------


"
Justin Uang <justin.uang@gmail.com>,"Fri, 24 Apr 2015 03:23:43 +0000",First-class support for pip/virtualenv in pyspark,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

I have been trying to figure out how to ship a python package that I have
been working on, and this has brought up a couple questions to me. Please
note that I'm fairly new to python package management, so any
feedback/corrections is welcome =)

It looks like the --py-files support we have merely adds the .py, .zip, or
.egg to the sys.path, and therefore only supports ""built"" distributions
that only needed to be added to the path. Because of this, it looks like
wheels won't work as well, since they involve an installation process (
https://www.python.org/dev/peps/pep-0427/#is-it-possible-to-import-python-code-directly-from-a-wheel-file
).

In addition, any type of distribution that has shared libraries, such as
pandas and numpy wheels will fail because ""ZIP import of dynamic modules
(.pyd, .so) is disallowed"" (https://docs.python.org/2/library/zipimport.html
).

The only way to support wheels or other types of source distributions that
require an ""installation"" step, is to use an installer like pip, in which
case, the natural extension is to use virtualenv. Have we considered having
pyspark manage virtualenvs, and to use pip install to install packages that
are sent across the cluster? I feel like first class support of using pip
install will

- allow us to ship packages that require an install step (numpy, pandas,
etc)
- help users not have to provision the cluster with all the dependencies
- allow multiple applications run with different environments at the same
time
- allow a user just to specify a top level dependency or requirements.txt,
and have pip install all the transitive dependencies automatically

Thanks!

Justin
"
Sourav Chandra <sourav.chandra@livestream.com>,"Fri, 24 Apr 2015 10:07:26 +0530",Re: Spark Streaming updatyeStateByKey throws OutOfMemory Error,Tathagata Das <tdas@databricks.com>,"*bump*


java:2864)*
)*
*
*
*
*
mpl.java:43)*
*
*
*
mpl.java:43)*
*
*
*
s.java:183)*
SerializableWritable.scala:43)*
)*
la:39)*
:57)*
mpl.java:43)*
*
java:40)*
alizer.scala:133)*
dcast.scala:236)*
cV$sp(TorrentBroadcast.scala:169)*
)*
cala:155)*
mpl.java:43)*
*
ng,
 op
*
 =
*
*
ils.java:183)*
)*
*
)*
p(SerializableWritable.scala:43)*
cala:39)*
va:57)*
rImpl.java:43)*
*
*
1)*
rializer.scala:62)*
oadcast.scala:236)*
$mcV$sp(TorrentBroadcast.scala:169)*
.scala:155)*
rImpl.java:43)*
*
*
1)*
)*
*
1)*
a,
y
StateByKey(updateFunc).foreachRDD(_.foreachPartition(RedisHelper.update(_)))*
)*
rof
 MB*
er""*
nd
nt
x
· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â·
 Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â·
rd
Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â·
· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â·



-- 

Sourav Chandra

Senior Software Engineer

Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â·

sourav.chandra@livestream.com

o: +91 80 4121 8723

m: +91 988 699 3746

skype: sourav.chandra

Livestream

""Ajmera Summit"", First Floor, #3/D, 68 Ward, 3rd Cross, 7th C Main, 3rd
Block, Koramangala Industrial Area,

Bangalore 560034

www.livestream.com
"
madhu phatak <phatak.dev@gmail.com>,"Fri, 24 Apr 2015 10:27:37 +0530",Contributing Documentation Changes,dev@spark.apache.org,"Hi,
 As I was reading contributing to Spark wiki, it was mentioned that we can
contribute external links to spark tutorials. I have written many
<http://blog.madhukaraphatak.com/categories/spark/> of them in my blog. It
will be great if someone can add it to the spark website.



Regards,
Madhukara Phatak
http://datamantra.io/
"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Fri, 24 Apr 2015 05:34:01 +0000",Re: Dataframe.fillna from 1.3.0,"Reynold Xin <rxin@databricks.com>, Olivier Girardot <o.girardot@lateral-thoughts.com>","I'll try thanks

Le ven. 24 avr. 2015 Ã  00:09, Reynold Xin <rxin@databricks.com> a Ã©crit :

py#L78
r
).
d
"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Fri, 24 Apr 2015 09:38:14 +0000",Re: Dataframe.fillna from 1.3.0,"Olivier Girardot <o.girardot@lateral-thoughts.com>, Reynold Xin <rxin@databricks.com>","done : https://github.com/apache/spark/pull/5683 and
https://issues.apache.org/jira/browse/SPARK-7118
thx

Le ven. 24 avr. 2015 Ã  07:34, Olivier Girardot <
o.girardot@lateral-thoughts.com> a Ã©crit :

Ã©crit :
.py#L78
or
e).
nd
a
e
"
Steve Loughran <stevel@hortonworks.com>,"Fri, 24 Apr 2015 10:02:18 +0000",Re: Should we let everyone set Assignee?,dev <dev@spark.apache.org>,"
I actually think the assignee JIRA issue is a minor detail; what really matters is do things get in and how.

So far, in the bits I've worked on, I've not encountered any problems. And as I've stated in the hadoop-dev lists, my main concern there is long-standing patches that languish because nobody invests the time to look at other people's patches unless/until they are on the critical path or part of a late-night-emergency-patch event (e.g. HADOOP-11730).  I'm as guilty there as everyone else -and I know that a reason is that a lot of those external patches come without good test coverage; getting something in usually involves dealing with that.

So far, so good -and I'd like to praise Sean Owen here, as not only has he put in effort, being in the same TZ means I get feedback faster. Sean, I owe you beer the next time you are in Bristol. 

If some JIRA has someone say ""I'm working on it"" and then nothing happens, it's moot whether its in a drop-down list or a comment on the bottom. If someone else wants to take it up, unless they like duplicating effort, starting off other people's work -collaborating- is the best way to produce quality code.

The only thing I would change is somehow get newly created JIRAs posted onto a list (dev?) that doesn't have the firehose of every other JIRA; issues@ is too noisy.

-Steve


PR
't
ch
are
is
 My
e


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Fri, 24 Apr 2015 07:47:16 -0400",Re: Contributing Documentation Changes,madhu phatak <phatak.dev@gmail.com>,"I think that your own tutorials and such should live on your blog. The
goal isn't to pull in a bunch of external docs to the site.


---------------------------------------------------------------------


"
Punyashloka Biswal <punya.biswal@gmail.com>,"Fri, 24 Apr 2015 12:12:41 +0000",Design docs: consolidation and discoverability,"""dev@spark.apache.org"" <dev@spark.apache.org>","Dear Spark devs,

Right now, design docs are stored on Google docs and linked from tickets.
For someone new to the project, it's hard to figure out what subjects are
being discussed, what organization to follow for new feature proposals, etc.

Would it make sense to consolidate future design docs in either a
designated area on the Apache Confluence Wiki, or on GitHub's Wiki pages?
If people have a strong preference to keep the design docs on Google Docs,
then could we have a top-level page on the confluence wiki that lists all
active and archived design docs?

Punya
"
Sean Owen <sowen@cloudera.com>,"Fri, 24 Apr 2015 08:21:33 -0400",Re: Design docs: consolidation and discoverability,Punyashloka Biswal <punya.biswal@gmail.com>,"That would require giving wiki access to everyone or manually adding people
any time they make a doc.

I don't see how this helps though. They're still docs on the internet and
they're still linked from the central project JIRA, which is what you
should follow.

"
Jan-Paul Bultmann <janpaulbultmann@me.com>,"Fri, 24 Apr 2015 14:40:34 +0200",Stackoverflow in createDataFrame.,dev@spark.apache.org,"Hey,
I get a stack overflow when calling the following method on SQLContext.

defÂ createDataFrame(rowRDD:Â JavaRDD[Row],Â columns: java.util.List[String]):Â DataFrameÂ =Â {
Â  Â  createDataFrame(rowRDD.rdd, columns.toSeq)
Â  } <https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/SQLContext.scala#L441>

The function will just call itself over and over.

If you consider this a bug as well I'll gladly open an JIRA issue :).

Cheers Jan"
yash datta <saucam@gmail.com>,"Fri, 24 Apr 2015 18:47:09 +0530",Re: Stackoverflow in createDataFrame.,Jan-Paul Bultmann <janpaulbultmann@me.com>,"This is already reported :

https://issues.apache.org/jira/browse/SPARK-6999

"
Jan-Paul Bultmann <janpaulbultmann@me.com>,"Fri, 24 Apr 2015 15:22:29 +0200",Re: Stackoverflow in createDataFrame.,dev@spark.apache.org,"Sorry, missed that issue :)


<https://issues.apache.org/jira/browse/SPARK-6999>
SQLContext.
java.util.List[String]): DataFrame = {
<https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/SQLContext.scala#L441 <https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/SQLContext.scala#L441>>

"
Cody Koeninger <cody@koeninger.org>,"Fri, 24 Apr 2015 09:09:02 -0500",Re: Design docs: consolidation and discoverability,Sean Owen <sowen@cloudera.com>,"My 2 cents - I'd rather see design docs in github pull requests (using
plain text / markdown).  That doesn't require changing access or adding
people, and github PRs already allow for conversation / email notifications.

Conversation is already split between jira and github PRs.  Having a third
stream of conversation in Google Docs just leads to things being ignored.


"
madhu phatak <phatak.dev@gmail.com>,"Fri, 24 Apr 2015 20:58:25 +0530",Re: Contributing Documentation Changes,Sean Owen <sowen@cloudera.com>,"Hi,
I understand that. The following page

http://spark.apache.org/documentation.html has a external tutorials,blogs
section which points to other blog pages. I wanted to add there.




Regards,
Madhukara Phatak
http://datamantra.io/


"
Ted Yu <yuzhihong@gmail.com>,"Fri, 24 Apr 2015 09:50:42 -0700",Re: Should we let everyone set Assignee?,Steve Loughran <stevel@hortonworks.com>,"bq. get newly created JIRAs posted onto a list (dev?)

+1


"
Patrick Wendell <pwendell@gmail.com>,"Fri, 24 Apr 2015 11:08:49 -0700",Re: Should we let everyone set Assignee?,Ted Yu <yuzhihong@gmail.com>,"It's a bit of a digression - but Steve's suggestion that we have a
mailing list for new issues is a great idea and we can do it easily.
We could nave new-issues@s.a.o or something (we already have
issues@s.a.o).

- Patrick


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Fri, 24 Apr 2015 11:12:08 -0700",Re: Should we let everyone set Assignee?,Patrick Wendell <pwendell@gmail.com>,"I like that idea (having a new-issues list instead of directly forwarding
them to dev).



"
shane knapp <sknapp@berkeley.edu>,"Fri, 24 Apr 2015 11:24:20 -0700",Jenkins down,"amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","jenkins is currently unreachable.  i'm not entirely sure why, as i can't
ssh in to the box and see what's going on.  i've filed a ticket and will
let everyone know when i have more information.

shane
"
Reynold Xin <rxin@databricks.com>,"Fri, 24 Apr 2015 11:30:54 -0700",Re: Design docs: consolidation and discoverability,Cody Koeninger <cody@koeninger.org>,"I'd love to see more design discussions consolidated in a single place as
well. That said, there are many practical challenges to overcome. Some of
them are out of our control:

1. For large features, it is fairly common to open a PR for discussion,
close the PR taking some feedback into account, and reopen another one. You
sort of lose the discussions that way.

2. With the way Jenkins is setup currently, Jenkins testing introduces a
lot of noise to GitHub pull requests, making it hard to differentiate
legitimate comments from noise. This is unfortunately due to the fact that
ASF won't allow our Jenkins bot to have API privilege to post messages.

3. The Apache Way is that all development discussions need to happen on ASF
property, i.e. dev lists and JIRA. As a result, technically we are not
allowed to have development discussions on GitHub.



"
shane knapp <sknapp@berkeley.edu>,"Fri, 24 Apr 2015 11:33:09 -0700",Re: Jenkins down,"amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","looks like we had a power failure on campus, and our datacenter is working
to bring things back up:

http://systemstatus.berkeley.edu/


"
Mingyu Kim <mkim@palantir.com>,"Fri, 24 Apr 2015 18:45:41 +0000",Re: [discuss] new Java friendly InputSource API,Reynold Xin <rxin@databricks.com>,"I see. So, the difference is that the InputSource is instantiated on the driver side and gets sent to the executors, whereas Hadoop¡¯s InputFormats are instantiated via reflection on the executors. That makes sense. Thanks for the clarification!

Mingyu

From: Reynold Xin <rxin@databricks.com<mailto:rxin@databricks.com>>
Date: Thursday, April 23, 2015 at 11:09 AM
To: Mingyu Kim <mkim@palantir.com<mailto:mkim@palantir.com>>
Cc: Soren Macbeth <soren@yieldbot.com<mailto:soren@yieldbot.com>>, Punyashloka Biswal <punya.biswal@gmail.com<mailto:punya.biswal@gmail.com>>, ""dev@spark.apache.org<mailto:dev@spark.apache.org>"" <dev@spark.apache.org<mailto:dev@spark.apache.org>>
Subject: Re: [discuss] new Java friendly InputSource API

In the ctor of InputSource (I'm also considering adding an explicit initialize call), the implementation of InputSource can execute arbitrary code. The state in it will also be serialized and passed onto the executors.

Yes - technically you can hijack getSplits in Hadoop InputFormat to do the same thing, and then put a reference of the state into every Split. But that's kind of awkward. Hadoop relies on the giant Configuration object to pass state over.



On Thu, Apr 23, 2015 at 11:02 AM, Mingyu Kim <mkim@palantir.com<mailto:mkim@palantir.com>> wrote:
Hi Reynold,

You mentioned that the new API allows arbitrary code to be run on the
driver side, but it©ös not very clear to me how this is different from what
Hadoop API provides. In your example of using broadcast, did you mean
broadcasting something in InputSource.getPartitions() and having
InputPartitions use the broadcast variables? Isn©öt that already possible
with Hadoop's InputFormat.getSplits()?

Thanks,
Mingyu





On 4/21/15, 4:33 PM, ""Soren Macbeth"" <soren@yieldbot.com<mailto:soren@yieldbot.com>> wrote:

>I'm also super interested in this. Flambo (our clojure DSL) wraps the java
>api and it would be great to have this.
>
>On Tue, Apr 21, 2015 at abricks.com>> wrote:
>
>> It can reuse. That's a good point and we should document it in the API
>> contract.
>>
>>
>> On Tue, Apr 21, 2015 at 4:06 PM, Punyashloka Biswal <
>> punya.biswal@gmail.com<mailto:punya.biswal@gmail.com>>
>> wrote:
>>
>> > Reynold, thanks for this! At Palantir we're heavy users of the Java
>>APIs
>> > and appreciate being able to stop hacking around with fake ClassTags
>>:)
>> >
>> > Regarding this specific proposal, is the contract of RecordReader#get
>> > intended to be that it returns a fresh object each time? Or is it
>>allowed
>> > to mutate a fixed object and return a pointer to it each time?
>> >
>> > Put another way, is a caller supposed to clone the output of get() if
>> they
>> > want to use it later?
>> >
>> > Punya
>> >
>> > On Tue, Apr 21, 2015 at 4:35 PM Reynold Xin <rxin@databr >> I created a pull request last night for a new InputSource API that is
>> >> essentially a stripped down version of the RDD API for providing data
>> into
>> >> Spark. Would be great to hear the community's feedback.
>> >>
>> >> Spark currently has two de facto input source API:
>> >> 1. RDD
>> >> 2. Hadoop MapReduce InputFormat
>> >>
>> >> Neither of the above is ideal:
>> >>
>> >> 1. RDD: It is hard for Java developers to implement RDD, given the
>> >> implicit
>> >> class tags. In addition, the RDD API depends on Scala's runtime
>>library,
>> >> which does not preserve binary compatibility across Scala versions.
>>If a
>> >> developer chooses Java to implement an input source, it would be
>>great
>> if
>> >> that input source can be binary compatible in years to come.
>> >>
>> >> 2. Hadoop InputFormat: The Hadoop InputFormat API is overly
>>restrictive.
>> >> For example, it forces key-value semantics, and does not support
>>running
>> >> arbitrary code on the driver side (an example of why this is useful
>>is
>> >> broadcast). In addition, it is somewhat awkward to tell developers
>>that
>> in
>> >> order to implement an input source for Spark, they should learn the
>> Hadoop
>> >> MapReduce API first.
>> >>
>> >>
>> >> My patch creates a new InputSource interface, described by:
>> >>
>> >> - an array of InputPartition that specifies the data partitioning
>> >> - a RecordReader that specifies how data on each partition can be
>>read
>> >>
>> >> This interface is similar to Hadoop's InputFormat, except that there
>>is
>> no
>> >> explicit key/value separation.
>> >>
>> >>
>> >> JIRA ticket:
>>https://urldefense.proofpoint.com/v2/url?u=https-3A__issues.apache.org_ji
>>ra_browse_SPARK-2D7025&d=AwIBaQ&c=izlc9mHr637UR4lpLEZLFFS3Vn2UXBrZ4tFb6oO
>>nmz8&r=ennQJq47pNnObsDh-88a9YUrUulcYQoV8giPASqXB84&m=WO8We1dUoSercyCHNAKk
>>tWH_nMrqD5TUhek8mTSCfFs&s=xUHYpQoU3NlV__I37IUkVwf94zzgAvtIj6N6uy2vwnc&e=
>> >> Pull request:
>>https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_apache_sp
>>ark_pull_5603&d=AwIBaQ&c=izlc9mHr637UR4lpLEZLFFS3Vn2UXBrZ4tFb6oOnmz8&r=en
>>nQJq47pNnObsDh-88a9YUrUulcYQoV8giPASqXB84&m=WO8We1dUoSercyCHNAKktWH_nMrqD
>>5TUhek8mTSCfFs&s=qoAlpURPOSkRgXxtlXHChqVHjm3yiPFgERk4LwKHLpg&e=
>> >>
>> >
>>


"
Sean Owen <sowen@cloudera.com>,"Fri, 24 Apr 2015 14:49:19 -0400",Re: Design docs: consolidation and discoverability,Reynold Xin <rxin@databricks.com>,"I think it's OK to have design discussions on github, as emails go to
ASF lists. After all, loads of PR discussions happen there. It's easy
for anyone to follow.

I also would rather just discuss on Github, except for all that noise.

It's not great to put discussions in something like Google Docs
actually; the resulting doc needs to be pasted back to JIRA promptly
if so. I suppose it's still better than a private conversation or not
talking at all, but the principle is that one should be able to access
any substantive decision or conversation by being tuned in to only the
project systems of record -- mailing list, JIRA.




---------------------------------------------------------------------


"
Punyashloka Biswal <punya.biswal@gmail.com>,"Fri, 24 Apr 2015 19:01:18 +0000",Re: Design docs: consolidation and discoverability,"Sean Owen <sowen@cloudera.com>, Reynold Xin <rxin@databricks.com>","The Gradle dev team keep their design documents  *checked into* their Git
repository -- see
https://github.com/gradle/gradle/blob/master/design-docs/build-comparison.md
for example. The advantages I see to their approach are:

   - design docs stay on ASF property (since Github is synced to the
   Apache-run Git repository)
   - design docs have a lifetime across PRs, but can still be modified and
   commented on through the mechanism of PRs
   - keeping a central location helps people to find good role models and
   converge on conventions

Sean, I find it hard to use the central Jira as a jumping-off point for
understanding ongoing design work because a tiny fraction of the tickets
actually relate to design docs, and it's not easy from the outside to
figure out which ones are relevant.

Punya


"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Fri, 24 Apr 2015 19:16:51 +0000",Re: Should we let everyone set Assignee?,Reynold Xin <rxin@databricks.com>,"+1 for new issues in dev list

ïÔÐÒÁ×ÌÅÎÏ Ó iPhone

ÁÐÉÓÁÌ(Á):
y
r
s
of
s
 I
If
d

---------------------------------------------------------------------


"
Mridul Muralidharan <mridul@gmail.com>,"Fri, 24 Apr 2015 12:26:11 -0700",Re: Should we let everyone set Assignee?,Patrick Wendell <pwendell@gmail.com>,"This is a great suggestion - definitely makes sense to have it.

Regards,
Mridul


---------------------------------------------------------------------


"
Sandy Ryza <sandy.ryza@cloudera.com>,"Fri, 24 Apr 2015 12:53:23 -0700",Re: Design docs: consolidation and discoverability,Punyashloka Biswal <punya.biswal@gmail.com>,"I think there are maybe two separate things we're talking about?

1. Design discussions and in-progress design docs.

My two cents are that JIRA is the best place for this.  It allows tracking
the progression of a design across multiple PRs and contributors.  A piece
of useful feedback that I've gotten in the past is to make design docs
immutable.  When updating them in response to feedback, post a new version
rather than editing the existing one.  This enables tracking the history of
a design and makes it possible to read comments about previous designs in
context.  Otherwise it's really difficult to understand why particular
approaches were chosen or abandoned.

2. Completed design docs for features that we've implemented.

Perhaps less essential to project progress, but it would be really lovely
to have a central repository to all the projects design doc.  If anyone
wants to step up to maintain it, it would be cool to have a wiki page with
links to all the final design docs posted on JIRA.

-Sandy


"
Punyashloka Biswal <punya.biswal@gmail.com>,"Fri, 24 Apr 2015 19:56:22 +0000",Re: Design docs: consolidation and discoverability,Sandy Ryza <sandy.ryza@cloudera.com>,"Sandy, doesn't keeping (in-progress) design docs in Git satisfy the history
requirement? Referring back to my Gradle example, it seems that
https://github.com/gradle/gradle/commits/master/design-docs/build-comparison.md
is a really good way to see why the design doc evolved the way it did. When
keeping the doc in Jira (presumably as an attachment) it's not easy to see
what changed between successive versions of the doc.

Punya


"
Sean Owen <sowen@cloudera.com>,"Fri, 24 Apr 2015 15:57:50 -0400",Re: Design docs: consolidation and discoverability,Punyashloka Biswal <punya.biswal@gmail.com>,"way for people who aren't committers to write and collaborate (for
point #1)


---------------------------------------------------------------------


"
Cody Koeninger <cody@koeninger.org>,"Fri, 24 Apr 2015 15:19:47 -0500",Re: Design docs: consolidation and discoverability,Sean Owen <sowen@cloudera.com>,"Why can't pull requests be used for design docs in Git if people who aren't
committers want to contribute changes (as opposed to just comments)?


"
Reynold Xin <rxin@databricks.com>,"Fri, 24 Apr 2015 13:51:55 -0700",Re: Dataframe.fillna from 1.3.0,Olivier Girardot <o.girardot@lateral-thoughts.com>,"The changes look good to me. Jenkins is somehow not responding. Will merge
once Jenkins comes back happy.



Ã©crit :
s.py#L78
e
for
s
le).
f
and
 a
"
Patrick Wendell <pwendell@gmail.com>,"Fri, 24 Apr 2015 14:01:48 -0700",Re: Design docs: consolidation and discoverability,Cody Koeninger <cody@koeninger.org>,"Using our ASF git repository as a working area for design docs, it
seems potentially concerning to me. It's difficult process wise
because all commits need to go through committers and also, we'd
pollute our git history a lot with random incremental design updates.

The git history is used a lot by downstream packagers, us during our
QA process, etc... we really try to keep it oriented around code
patches:

https://git-wip-us.apache.org/repos/asf?p=spark.git;a=shortlog

Committing a polished design doc along with a feature, maybe that's
something we could consider. But I still think JIRA is the best
location for these docs, consistent with what most other ASF projects
do that I know.


---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Fri, 24 Apr 2015 14:17:09 -0700",Re: Jenkins down,"amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","ok, power has been restored and jenkins is back up.  we might be taking
things down again to fix up some power mis-cabling (jon and i are in the
colo, and the jenkins master wasn't on the UPS and needs to be).

more updates as they come.  sorry for the inconvenience.


"
"""Shuai Zheng"" <szheng.code@gmail.com>","Fri, 24 Apr 2015 17:24:27 -0400",[SQL][Feature] Access row by column name instead of index,"""'dev'"" <dev@spark.apache.org>","Hi All,

I want to ask whether there is a plan to implement the feature to access the Row in sql by name? Currently we can only allow to access a row by index (there is a python version api of access by name, but none for java)

Regards,

Shuai


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Fri, 24 Apr 2015 14:37:11 -0700",Re: [SQL][Feature] Access row by column name instead of index,Shuai Zheng <szheng.code@gmail.com>,"Can you elaborate what you mean by that? (what's already available in
Python?)



"
Yang Lei <genially@gmail.com>,"Fri, 24 Apr 2015 17:38:02 -0400","Re: Issue of running partitioned loading (RDD) in Spark External
 Datasource on Mesos",dev <dev@spark.apache.org>,"forward to dev.


"
Michael Armbrust <michael@databricks.com>,"Fri, 24 Apr 2015 14:57:13 -0700",Re: [SQL][Feature] Access row by column name instead of index,Reynold Xin <rxin@databricks.com>,"Already done :)

https://github.com/apache/spark/commit/2e8c6ca47df14681c1110f0736234ce76a3eca9b


"
"""Shuai Zheng"" <szheng.code@gmail.com>","Fri, 24 Apr 2015 18:02:10 -0400",RE: [SQL][Feature] Access row by column name instead of index,"""'Michael Armbrust'"" <michael@databricks.com>,
	""'Reynold Xin'"" <rxin@databricks.com>","Great, 

 

That is exactly what I want. Now I will wait patiently for 1.4.0 J

 

Regards,

 

Shuai

 

From: Michael Armbrust [mailto:michael@databricks.com] 
Sent: Friday, April 24, 2015 5:57 PM
To: Reynold Xin
Cc: Shuai Zheng; dev
Subject: Re: [SQL][Feature] Access row by column name instead of index

 

Already done :)

 

https://github.com/apache/spark/commit/2e8c6ca47df14681c1110f0736234ce76a3eca9b

 


Can you elaborate what you mean by that? (what's already available in
Python?)




access
java)

 

"
shane knapp <sknapp@berkeley.edu>,"Fri, 24 Apr 2015 15:18:09 -0700",Re: Jenkins down,"amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","ok, jenkins is back up and building.  we have a few things to mop up here
(ganglia is sad), but i think we'll be good for the afternoon.

shane


"
Reynold Xin <rxin@databricks.com>,"Fri, 24 Apr 2015 15:25:55 -0700",Re: Jenkins down,shane knapp <sknapp@berkeley.edu>,"Thanks for looking into this, Shane.


"
"""York, Brennon"" <Brennon.York@capitalone.com>","Fri, 24 Apr 2015 18:37:42 -0400",Re: Jenkins down,"Reynold Xin <rxin@databricks.com>, shane knapp <sknapp@berkeley.edu>","Ditto to Reynold. Thanks a bunch for all the updates and work Shane!



________________________________________________________

The information contained in this e-mail is confidential and/or proprietary is intended only for use by the individual or entity to which it is addressed.  If the reader of this message is not the intended recipient, you are hereby notified that any review, retransmission, dissemination, distribution, copying or other use of, or taking of any action in reliance upon this information is strictly prohibited. If you have received this communication in error, please contact the sender and delete the material from your computer.


---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Fri, 24 Apr 2015 16:14:14 -0700",Re: Jenkins down,amp-infra <amp-infra@googlegroups.com>,"thanks everyone!  happy friday!  :)


"
Punyashloka Biswal <punya.biswal@gmail.com>,"Fri, 24 Apr 2015 23:42:16 +0000",Re: Design docs: consolidation and discoverability,"Patrick Wendell <pwendell@gmail.com>, Cody Koeninger <cody@koeninger.org>","Okay, I can understand wanting to keep Git history clean, and avoid
bottlenecking on committers. Is it reasonable to establish a convention of
having a label, component or (best of all) an issue type for issues that
are associated with design docs? For example, if we used the existing
""Brainstorming"" issue type, and people put their design doc in the
description of the ticket, it would be relatively easy to figure out what
designs are in progress.

Given the push-back against design docs in Git or on the wiki and the
strong preference for keeping docs on ASF property, I'm a bit surprised
that all the existing design docs are on Google Docs. Perhaps Apache should
consider opening up parts of the wiki to a larger group, to better serve
this use case.

Punya


"
Sean Owen <sowen@cloudera.com>,"Fri, 24 Apr 2015 19:57:36 -0400",Re: Design docs: consolidation and discoverability,Punyashloka Biswal <punya.biswal@gmail.com>,"I know I recently used Google Docs from a JIRA, so am guilty as
charged. I don't think there are a lot of design docs in general, but
the ones I've seen have simply pushed docs to a JIRA. (I did the same,
mirroring PDFs of the Google Doc.) I don't think this is hard to
follow.

I think you can do what you like: make a JIRA and attach files. Make a
WIP PR and attach your notes. Make a Google Doc if you're feeling
transgressive.

I don't see much of a problem to solve here. In practice there are
plenty of workable options, all of which are mainstream, and so I do
not see an argument that somehow this is solved by letting people make
wikis.


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Fri, 24 Apr 2015 17:14:11 -0700","Re: Issue of running partitioned loading (RDD) in Spark External
 Datasource on Mesos",Yang Lei <genially@gmail.com>,"This looks like a specific Spray configuration issue (or how Spray reads
config files). Maybe Spray is reading some local config file that doesn't
exist on your executors?

You might need to email the Spray list.



"
Konstantin Boudnik <cos@apache.org>,"Fri, 24 Apr 2015 19:52:57 -0700",Call for papers at In-Memory Computing Summit SF 2015,dev@spark.apache.org,"Guys,

I wanted to reach out to make sure you're aware about the coming
industry first in-memory computing summit (http://imcsummit.org) that takes
please in San Francisco on June 29-30, 2015.

CFP closes on April 30th, so if you want to participate in the - submit your
proposal by the deadline. It's going to be fun - see you there!

-- 
Take care,
	Cos
2CAC 8312 4870 D885 8616  6115 220F 6980 1F27 E622
Cos' pubkey: http://people.apache.org/~cos/cos.asc

         ---- Wisdom of the hour ----

You will attract cultured and artistic people to your home.
"
Yang Lei <genially@gmail.com>,"Fri, 24 Apr 2015 22:57:46 -0400",Re: Issue of running partitioned loading (RDD) in Spark External Datasource on Mesos,Reynold Xin <rxin@databricks.com>,"The configure is in the jar I passed in.  And if I do not create my own RDD for partitioned loading, everything is fine, in which case the task is run in executor right? So it seems some special call path before triggering my RDD compute makes the configure 'lost'. 

I will try to see if I can debug further. But any  insight in this special call path will be appreciated. 

Yang

Sent from my iPhone

onfig files). Maybe Spray is reading some local config file that doesn't exist on your executors? 


 the

"",

ava_gateway.py"",
rotocol.py"",
sk
tage
ing:

8)
4)
)
)
AccessorImpl.java:57)
structorAccessorImpl.java:45)
ynamicAccess.scala:78)
a:73)
ataAccess.scala:118)
Access.scala:71)
)
)
va:1145)
ava:615)
he
"
Akhil Das <akhil@sigmoidanalytics.com>,"Sat, 25 Apr 2015 22:45:30 +0530",Re: Contributing Documentation Changes,madhu phatak <phatak.dev@gmail.com>,"I also want to add mine :/
Everyone wants to add it seems.

Thanks
Best Regards


"
Patrick Wendell <pwendell@gmail.com>,"Sat, 25 Apr 2015 11:09:19 -0700",Re: Contributing Documentation Changes,Akhil Das <akhil@sigmoidanalytics.com>,"It is true that in the past we've posted community tutorials on the
site. Spark has grown a lot since then and it might be a better fit at
this point to curate community tutorials on the wiki (something like
the powered by page) and link to them from the documentation website.

The documentation page overall could use some love, as it's pretty outdated.

- Patrick


---------------------------------------------------------------------


"
Denny Lee <denny.g.lee@gmail.com>,"Sat, 25 Apr 2015 19:23:44 +0000",Re: Contributing Documentation Changes,"Patrick Wendell <pwendell@gmail.com>, Akhil Das <akhil@sigmoidanalytics.com>","Concur with the sentiment - we have a lot of solid external blog posts
ranging on a wide variety of topics. Whether it is a curated community wiki
or something ala spark-packages.org (but for content), what are your
thoughts about having curators to help with this?

"
Yin Huai <yhuai@databricks.com>,"Sat, 25 Apr 2015 13:54:10 -0700",[Spark SQL] Generating new golden answer files for HiveComparisonTest,"""dev@spark.apache.org"" <dev@spark.apache.org>","Spark SQL developers,

If you are trying to add new tests based on HiveComparisonTest and want to
generate golden answer files with Hive 0.13.1, unfortunately, the setup
work is quite different from that for Hive 0.12. We have updated SQL readme
to include the new instruction for Hive 0.13.1. You can find it at the the
section of ""Other dependencies for developers""
<https://github.com/apache/spark/tree/master/sql>.

Please let me know if you still see any issue after setup your environment
based on this instruction.

Thanks,

Yin
"
Patrick Wendell <pwendell@gmail.com>,"Sat, 25 Apr 2015 16:02:30 -0700",Reminder about Spark 1.4.0 deadline of May 1st,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey All,

Just a friendly reminder that May 1st is the feature freeze for Spark
1.4, meaning major outstanding changes will need to land in the next
week.

After May 1st we'll package a release for testing and then go into the
normal triage process where bugs are prioritized and some smaller
features are allowed on a case by case basis (if they are
additive/feature flagged/etc).

As always, I'll invite the community to help participate in code
review of patches in the next week, since review bandwidth is the
single biggest determinant of how many features will get in. Please
also keep in mind that most active committers are working overtime
(nights/weekends) during this period and will try their best to help
usher in as many patches as possible, along with their own code.

As a reminder, release window dates are always maintained on the wiki
and are updated after each release according to our 3 month release
cadence:

https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage

Thanks - and happy coding!
- Patrick

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Sat, 25 Apr 2015 20:00:09 -0400",Possible lead on Jenkins <-> Github issue,dev <dev@spark.apache.org>,"I know Jenkins hasn't been responding to requests to test today. I
don't know if it's related, but I also noted that test results aren't
quite posting:

...
Archiving unit tests logs...

Attempting to post to Github...
 > http_code: 400.
 > api_response: {
  ""message"": ""Problems parsing JSON"",
  ""documentation_url"": ""https://developer.github.com/v3""
}
 > data: {""body"": ""  [Test build #701 has
finished](https://amplab.cs.berkeley.edu/jenkins/job/NewSparkPullRequestBuilder/701/consoleFull)
for   PR 5672 at commit
[`0f1abd0`](https://github.com/apache/spark/commit/0f1abd01287cd33aa73a0b5574f95369b8d42910).\n
* This patch **passes all tests**.\n * This patch merges cleanly.\n *
This patch adds no public classes.\nYour branch is ahead of
'origin/master' by 200 commits.
Your branch is ahead of 'origin/master' by 200 commits.
 * This patch **adds the following new dependencies:**\n   *
`tachyon-0.6.4.jar`\n   * `tachyon-client-0.6.4.jar`\n\n * This patch
**removes the following dependencies:**\n   * `tachyon-0.5.0.jar`\n
* `tachyon-client-0.5.0.jar`\n""}
Recording test results

Finished: SUCCESS

---------------------------------------------------------------------


"
"""York, Brennon"" <Brennon.York@capitalone.com>","Sat, 25 Apr 2015 21:55:34 -0400",Re: Possible lead on Jenkins <-> Github issue,"Sean Owen <sowen@cloudera.com>, dev <dev@spark.apache.org>","Sean, re: why Jenkins isn¹t posting, looking at that output it looks like
the issue is with the output of the following text:

\nYour branch is ahead of'origin/master' by 200 commits.
Your branch is ahead of 'origin/master' by 200 commits.


Not sure where its coming from (can¹t find it in the master branch), but
after the first line there is an actual newline character (not escaped to
print \n) which is destroying the JSON string from being uploaded to
Github for the post. Still looking into the code to see if I can pinpoint
it, but hope that helps.



________________________________________________________

The information contained in this e-mail is confidential and/or proprietary is intended only for use by the individual or entity to which it is addressed.  If the reader of this message is not the intended recipient, you are hereby notified that any review, retransmission, dissemination, distribution, copying or other use of, or taking of any action in reliance upon this information is strictly prohibited. If you have received this communication in error, please contact the sender and delete the material from your computer.


---------------------------------------------------------------------


"
eric wong <win19999@gmail.com>,"Sun, 26 Apr 2015 16:12:30 +0800",WebUI shows poor locality when task scheduling,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi developers,

I have sent to user mail list but no response...

When running a exprimental KMeans job for expriment, the Cached RDD is
original Points data.

I saw poor locality in Task details from WebUI. Almost one half of the
input of task is Network instead of Memory.

And Task with network input consumes almost the same time compare with the
task with  Hadoop(Disk) input, and twice with task(Memory input).
e.g
Task(Memory): 16s
Task(Network): 9s
Task(Hadoop): 9s


I see fectching RDD with 30MB form remote node consumes 5 seconds in
executor logs like below:

15/03/31 04:08:52 INFO CoarseGrainedExecutorBackend: Got assigned task 58
15/03/31 04:08:52 INFO Executor: Running task 15.0 in stage 1.0 (TID 58)
15/03/31 04:08:52 INFO HadoopRDD: Input split:
hdfs://master:8000/kmeans/data-Kmeans-5.3g:2013265920+134217728
15/03/31 04:08:52 INFO BlockManager: Found block rdd_3_15 locally
15/03/31 04:08:58 INFO Executor: Finished task 15.0 in stage 1.0 (TID 58).
1920 bytes result sent to driver
15/03/31 04:08:58 INFO CoarseGrainedExecutorBackend: Got assigned task 60
-----------------Task60
15/03/31 04:08:58 INFO Executor: Running task 17.0 in stage 1.0 (TID 60)
15/03/31 04:08:58 INFO HadoopRDD: Input split:
hdfs://master:8000/kmeans/data-Kmeans-5.3g:2281701376+134217728
15/03/31 04:09:02 INFO BlockManager: Found block rdd_3_17 remotely
15/03/31 04:09:12 INFO Executor: Finished task 17.0 in stage 1.0 (TID 60).
1920 bytes result sent to driver


So
1)is that means i should use RDD with cache(MEMORY_AND_DISK) instead of
Memory only?

2)And should i expand Network capacity or turn Schduling locality parameter?
  i set spark.locality.wait up to 15000, but no effect seems to increase
the Memory input percentage

Any suggestion will be appreciated.


------Env info-----------

Cluster: 4 worker, with 1 Cores and 2G executor memory

Spark version: 1.1.0

Network: 30MB/s

-----Submit shell-------
bin/spark-submit --class org.apache.spark.examples.mllib.JavaKMeans
--master spark://master:7077 --executor-memory 1g
lib/spark-examples-1.1.0-hadoop2.3.0.jar
hdfs://master:8000/kmeans/data-Kmeans-7g 8 1


Thanks very much and forgive for my poor English.

-- 
Wang Haihua






-- 
çŽ‹æµ·åŽ

---------------------------------------------------------------------"
Deepak Gopalakrishnan <dgkris@gmail.com>,"Mon, 27 Apr 2015 01:12:04 +0530",Re: Spark timeout issue,dev@spark.apache.org,"Hello All,

I'm trying to process a 3.5GB file on standalone mode using spark. I could
run my spark job succesfully on a 100MB file and it works as expected. But,
when I try to run it on the 3.5GB file, I run into the below error :


15/04/26 12:45:50 INFO BlockManagerMaster: Updated info of block taskresult_83
15/04/26 12:46:46 WARN AkkaUtils: Error sending message [message =
Heartbeat(2,[Lscala.Tuple2;@790223d3,BlockManagerId(2,
master.spark.com, 39143))] in 1 attempts
java.util.concurrent.TimeoutException: Futures timed out after [30 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
	at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)
	at scala.concurrent.Await$.result(package.scala:107)
	at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:195)
	at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:427)
15/04/26 12:47:15 INFO MemoryStore: ensureFreeSpace(26227673) called
with curMem=265897, maxMem=5556991426
15/04/26 12:47:15 INFO MemoryStore: Block taskresult_92 stored as
bytes in memory (estimated size 25.0 MB, free 5.2 GB)
15/04/26 12:47:16 INFO MemoryStore: ensureFreeSpace(26272879) called
with curMem=26493570, maxMem=5556991426
15/04/26 12:47:16 INFO MemoryStore: Block taskresult_94 stored as
bytes in memory (estimated size 25.1 MB, free 5.1 GB)
15/04/26 12:47:18 INFO MemoryStore: ensureFreeSpace(26285327) called
with curMem=52766449, maxMem=5556991426


and the job fails.


I'm on AWS and have opened all ports. Also, since the 100MB file works, it
should not be a connection issue.  I've a r3 xlarge and 2 m3 large.

Can anyone suggest a way to fix this?
"
Patrick Wendell <pwendell@gmail.com>,"Sun, 26 Apr 2015 14:27:46 -0700",Re: WebUI shows poor locality when task scheduling,"eric wong <win19999@gmail.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Eric - please direct this to the user@ list. This list is for
development of Spark itself.

put
e
.
.
er?
the
ter

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Sun, 26 Apr 2015 14:34:13 -0700",Re: Design docs: consolidation and discoverability,Sean Owen <sowen@cloudera.com>,"I actually don't totally see why we can't use Google Docs provided it
is clearly discoverable from the JIRA. It was my understanding that
many projects do this. Maybe not (?).

If it's a matter of maintaining public record on ASF infrastructure,
perhaps we can just automate that if an issue is closed we capture the
doc content and attach it to the JIRA as a PDF.

My sense is that in general the ASF infrastructure policy is becoming
more and more lenient with regards to using third party services,
provided the are broadly accessible (such as a public google doc) and
can be definitively archived on ASF controlled storage.

- Patrick


---------------------------------------------------------------------


"
Manku Timma <manku.timma1@gmail.com>,"Mon, 27 Apr 2015 10:34:37 +0530",hive initialization on executors,dev@spark.apache.org,"I am facing an exception ""Hive.get() called without a hive db setup"" in the
executor. I wanted to understand how Hive object is initialized in the
executor threads? I only see Hive.get(hiveconf) in two places in spark 1.3
code.

In HiveContext.scala - I dont think this is created on the executor
In HiveMetastoreCatalog.scala - I am not sure if it is created on the
executor

Any information on how the hive code is bootstrapped on the executor will
be really helpful and I can do the debugging. I have compiled spark-1.3
with -Phive-provided.

In case you are curious the stacktrace is:-
java.lang.RuntimeException:
org.apache.hadoop.hive.ql.metadata.HiveException: Hive.get() called without
a hive db setup
  at
org.apache.hadoop.hive.ql.plan.PlanUtils.configureJobPropertiesForStorageHandler(PlanUtils.java:841)
  at
org.apache.hadoop.hive.ql.plan.PlanUtils.configureInputJobPropertiesForStorageHandler(PlanUtils.java:776)
  at
org.apache.spark.sql.hive.HadoopTableReader$.initializeLocalJobConfFunc(TableReader.scala:253)
  at
org.apache.spark.sql.hive.HadoopTableReader$$anonfun$11.apply(TableReader.scala:229)
  at
org.apache.spark.sql.hive.HadoopTableReader$$anonfun$11.apply(TableReader.scala:229)
  at
org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$6.apply(HadoopRDD.scala:172)
  at
org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$6.apply(HadoopRDD.scala:172)
  at scala.Option.map(Option.scala:145)
  at org.apache.spark.rdd.HadoopRDD.getJobConf(HadoopRDD.scala:172)
  at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:216)
  at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:212)
  at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
  at
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
  at
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
  at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:87)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
  at
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
  at
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
  at
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
  at org.apache.spark.scheduler.Task.run(Task.scala:64)
  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:206)
  at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
  at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
  at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive.get()
called without a hive db setup
  at org.apache.hadoop.hive.ql.metadata.Hive.get(Hive.java:211)
  at
org.apache.hadoop.hive.ql.plan.PlanUtils.configureJobPropertiesForStorageHandler(PlanUtils.java:797)
"
Emre Sevinc <emre.sevinc@gmail.com>,"Mon, 27 Apr 2015 10:34:04 +0200","Is there any particular reason why there's no Java counterpart in
 Streaming Guide's ""Design Patterns for using foreachRDD"" section?",dev <dev@spark.apache.org>,"Hello,

Is there any particular reason why there's no Java counterpart in Streaming
Guide's ""Design Patterns for using foreachRDD"" section?

   https://spark.apache.org/docs/latest/streaming-programming-guide.html

Up to that point, each source code example includes corresponding Java (and
sometimes Python) source code for the Scala examples, but in section
""Design Patterns for using foreachRDD"", the code examples are only in Scala
and Python.

After that section comes ""DataFrame and SQL Operations"", and it continues
giving examples in Scala, Java, and Python.

The reason I'm asking: if there's no particular reason, maybe I can open a
JIRA ticket and contribute to that part of the documentation?

-- 
Emre SevinÃ§
"
Manku Timma <manku.timma1@gmail.com>,"Mon, 27 Apr 2015 15:23:32 +0530",creating hive packages for spark,dev@spark.apache.org,"Hello Spark developers,
I want to understand the procedure to create the org.spark-project.hive
jars. Is this documented somewhere? I am having issues with -Phive-provided
with my private hive13 jars and want to check if using spark's procedure
helps.
"
yash datta <saucam@gmail.com>,"Mon, 27 Apr 2015 15:39:44 +0530",Re: creating hive packages for spark,Manku Timma <manku.timma1@gmail.com>,"Hi,

you can build spark-project hive from here :

https://github.com/pwendell/hive/tree/0.13.1-shaded-protobuf

Hope this helps.






-- 
When events unfold with calm and ease
When the winds that blow are merely breeze
Learn from nature, from birds and bees
Live your life in love, and let joy not cease.
"
Sean Owen <sowen@cloudera.com>,"Mon, 27 Apr 2015 07:20:35 -0400","Re: Is there any particular reason why there's no Java counterpart in
 Streaming Guide's ""Design Patterns for using foreachRDD"" section?",Emre Sevinc <emre.sevinc@gmail.com>,"My guess is since it says ""for example (in Scala)"" that this started
as Scala-only and then Python was tacked on as a one-off, and Java
never got added. I think you'd be welcome to add it. It's not an
obscure example and one people might want to see in Java.

ng
nd
la
a

---------------------------------------------------------------------


"
"""=?ISO-8859-1?B?U2Vh?="" <261810726@qq.com>","Mon, 27 Apr 2015 21:32:57 +0800",Exception in using updateStateByKey,"""=?ISO-8859-1?B?ZGV2?="" <dev@spark.apache.org>","Hi, all:
I use function updateStateByKey in Spark Streaming, I need to store the states for one minite,  I set ""spark.cleaner.ttl"" to 120, the duration is 2 seconds, but it throws Exception 




Caused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: spark/ck/hdfsaudit/receivedData/0/log-1430139541443-1430139601443
        at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)
        at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:51)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsUpdateTimes(FSNamesystem.java:1499)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1448)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1428)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1402)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:468)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:269)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java:59566)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2048)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2042)


        at org.apache.hadoop.ipc.Client.call(Client.java:1347)
        at org.apache.hadoop.ipc.Client.call(Client.java:1300)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
        at com.sun.proxy.$Proxy14.getBlockLocations(Unknown Source)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:188)
        at sun.reflect.GeneratedMethodAccessor21.invoke(Unknown Source)



Why?


my code is 


    ssc = StreamingContext(sc,2)
    kvs = KafkaUtils.createStream(ssc, zkQuorum, group, {topic: 1})
    kvs.window(60,2).map(lambda x: analyzeMessage(x[1]))\
        .filter(lambda x: x[1] != None).updateStateByKey(updateStateFunc) \
        .filter(lambda x: x[1]['isExisted'] != 1) \
        .foreachRDD(lambda rdd: rdd.foreachPartition(insertIntoDb))"
Steve Loughran <stevel@hortonworks.com>,"Mon, 27 Apr 2015 14:54:09 +0000",Re: Design docs: consolidation and discoverability,dev <dev@spark.apache.org>,"
riginal proposal, that's not the place to keep living specifications. That stuff needs to live in SCM, in a format which can be easily maintained, can generate readable documents, and, in an unrealistically ideal world, even be used by machines to validate compliance with the design. Test suites tend to be the implicit machine-readable part of the specification, though they aren't usually viewed as such.

PDFs of word docs in JIRAs are not the place for ongoing work, even if the early drafts can contain them. Given it's just as easy to point to markdown docs in github by commit ID, that could be an alternative way to publish docs, with the document itself being viewed as one of the deliverables. When the time comes to update a document, then its there in the source tree to edit.

If there's a flaw here, its that design docs are that: the design. The implementation may not match, ongoing work will certainly diverge. If the design docs aren't kept in sync, then they can mislead people. Accordingly, once the design docs are incorporated into the source tree, keeping them in sync with changes has be viewed as essential as keeping tests up to date

 of
t are
at
trong
ll
ider
e
ote:
:
mparison.md
 A


---------------------------------------------------------------------


"
Sandy Ryza <sandy.ryza@cloudera.com>,"Mon, 27 Apr 2015 10:20:21 -0700",Re: Design docs: consolidation and discoverability,Steve Loughran <stevel@hortonworks.com>,"My only issue with Google Docs is that they're mutable, so it's difficult
to follow a design's history through its revisions and link up JIRA
comments with the relevant version.

-Sandy


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 27 Apr 2015 17:29:45 +0000",Re: Design docs: consolidation and discoverability,"Sandy Ryza <sandy.ryza@cloudera.com>, Steve Loughran <stevel@hortonworks.com>","I like the idea of having design docs be kept up to date and tracked in
git.

If the Apache repo isn't a good fit, perhaps we can have a separate repo
just for design docs? Maybe something like github.com/spark-docs/spark-docs/
?

If there's other stuff we want to track but haven't, perhaps we can
generalize the purpose of the repo a bit and rename it accordingly (e.g.
spark-misc/spark-misc).

Nick


"
Punyashloka Biswal <punya.biswal@gmail.com>,"Mon, 27 Apr 2015 17:47:17 +0000",Re: Design docs: consolidation and discoverability,"Nicholas Chammas <nicholas.chammas@gmail.com>, Sandy Ryza <sandy.ryza@cloudera.com>, 
	Steve Loughran <stevel@hortonworks.com>","Nick, I like your idea of keeping it in a separate git repository. It seems
to combine the advantages of the present Google Docs approach with the
crisper history, discoverability, and text format simplicity of GitHub
wikis.

Punya

"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 27 Apr 2015 17:50:33 +0000",Re: Design docs: consolidation and discoverability,"Punyashloka Biswal <punya.biswal@gmail.com>, Sandy Ryza <sandy.ryza@cloudera.com>, 
	Steve Loughran <stevel@hortonworks.com>","Oh, a GitHub wiki (which is separate from having docs in a repo) is yet
another approach we could take, though if we want to do that on the main
Spark repo we'd need permission from Apache, which may be tough to get...


"
shane knapp <sknapp@berkeley.edu>,"Mon, 27 Apr 2015 11:18:51 -0700","github pull request builder FAIL, now WIN(-ish)","dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>","somehow, the power outage on friday caused the pull request builder to lose
it's config entirely...  i'm not sure why, but after i added the oauth
token back, we're now catching up on the weekend's pull request builds.

have i mentioned how much i hate this plugin?  ;)

sorry for the inconvenience...

shane
"
shane knapp <sknapp@berkeley.edu>,"Mon, 27 Apr 2015 11:23:35 -0700","Re: github pull request builder FAIL, now WIN(-ish)","dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>","anyways, the build queue is SLAMMED...  we're going to need at least a day
to catch up w/this.  i'll be keeping an eye on system loads and whatnot all
day today.

whee!


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 27 Apr 2015 18:30:04 +0000","Re: github pull request builder FAIL, now WIN(-ish)","shane knapp <sknapp@berkeley.edu>, dev <dev@spark.apache.org>, 
	amp-infra <amp-infra@googlegroups.com>","And unfortunately, many Jenkins executor slots are being taken by stale
Spark PRs...


"
Punyashloka Biswal <punya.biswal@gmail.com>,"Mon, 27 Apr 2015 18:30:57 +0000",Re: Design docs: consolidation and discoverability,"Nicholas Chammas <nicholas.chammas@gmail.com>, Sandy Ryza <sandy.ryza@cloudera.com>, 
	Steve Loughran <stevel@hortonworks.com>","Github's wiki is just another Git repo. If we use a separate repo, it's
probably easiest to use the wiki git repo rather than the ""primary"" git
repo.

Punya


"
Reynold Xin <rxin@databricks.com>,"Mon, 27 Apr 2015 11:34:09 -0700","Re: github pull request builder FAIL, now WIN(-ish)",Nicholas Chammas <nicholas.chammas@gmail.com>,"Shane - can we purge all the outstanding builds so we are not running stuff
against stale PRs?



"
shane knapp <sknapp@berkeley.edu>,"Mon, 27 Apr 2015 11:35:44 -0700","Re: github pull request builder FAIL, now WIN(-ish)",Reynold Xin <rxin@databricks.com>,"sure, i'll kill all of the current spark prb build...


"
shane knapp <sknapp@berkeley.edu>,"Mon, 27 Apr 2015 11:37:47 -0700","Re: github pull request builder FAIL, now WIN(-ish)",Reynold Xin <rxin@databricks.com>,"never mind, looks like you guys are already on it.  :)


"
Punyashloka Biswal <punya.biswal@gmail.com>,"Mon, 27 Apr 2015 18:58:10 +0000",Plans for upgrading Hive dependency?,dev <dev@spark.apache.org>,"Dear Spark devs,

Is there a plan for staying up-to-date with current (and future) versions
of Hive? Spark currently supports version 0.13 (June 2014), but the latest
version of Hive is 1.1.0 (March 2015). I don't see any Jira tickets about
updating beyond 0.13, so I was wondering if this was intentional or it was
just that nobody had started work on this yet.

I'd be happy to work on a PR for the upgrade if one of the core developers
can tell me what pitfalls to watch out for.

Punya
"
Marcelo Vanzin <vanzin@cloudera.com>,"Mon, 27 Apr 2015 12:47:42 -0700",Re: Plans for upgrading Hive dependency?,Punyashloka Biswal <punya.biswal@gmail.com>,"That's a lot more complicated than you might think.

We've done some basic work to get HiveContext to compile against Hive
1.1.0. Here's the code:
https://github.com/cloudera/spark/commit/00e2c7e35d4ac236bcfbcd3d2805b483060255ec

We didn't sent that upstream because that only solves half of the
problem; the hive-thriftserver is disabled in our CDH build because it
uses a lot of Hive APIs that have been removed in 1.1.0, so even
getting it to compile is really complicated.

If there's interest in getting the HiveContext part fixed up I can
send a PR for that code. But at this time I don't really have plans to
look at the thrift server.





-- 
Marcelo

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Mon, 27 Apr 2015 12:56:07 -0700",Re: Plans for upgrading Hive dependency?,Marcelo Vanzin <vanzin@cloudera.com>,"Hey Punya,

There is some ongoing work to help make Hive upgrades more manageable
will be much easier for us to upgrade.

https://issues.apache.org/jira/browse/SPARK-6906

- Patrick


---------------------------------------------------------------------


"
Punyashloka Biswal <punya.biswal@gmail.com>,"Mon, 27 Apr 2015 20:03:02 +0000",Re: Plans for upgrading Hive dependency?,"Patrick Wendell <pwendell@gmail.com>, Marcelo Vanzin <vanzin@cloudera.com>","Thanks Marcelo and Patrick - I don't know how I missed that ticket in my
Jira search earlier. Is anybody working on the sub-issues yet, or is there
a design doc I should look at before taking a stab?

Regards,
Punya


"
"""wyphao.2007"" <wyphao.2007@163.com>","Tue, 28 Apr 2015 10:05:45 +0800 (CST)","java.lang.StackOverflowError when recovery from checkpoint in
 Streaming",dev@spark.apache.org," Hi everyone, I am using val messages = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, topicsSet) to read data from kafka(1k/second), and store the data in windows,the code snippets as follow:        val windowedStreamChannel = streamChannel.combineByKey[TreeSet[Obj]](TreeSet[Obj](_), _ += _, _ ++= _, new HashPartitioner(numPartition))
          .reduceByKeyAndWindow((x: TreeSet[Obj], y: TreeSet[Obj]) => x ++= y,
            (x: TreeSet[Obj], y: TreeSet[Obj]) => x --= y, Minutes(60), Seconds(2), numPartition,
            (item: (String, TreeSet[Obj])) => item._2.size != 0)after the application  run for an hour,  I kill the application and restart it from checkpoint directory, but I  encountered an exception:2015-04-27 17:52:40,955 INFO  [Driver] - Slicing from 1430126222000 ms to 1430126222000 ms (aligned to 1430126222000 ms and 1430126222000 ms)
2015-04-27 17:52:40,958 ERROR [Driver] - User class threw exception: null
java.lang.StackOverflowError
	at java.io.UnixFileSystem.getBooleanAttributes0(Native Method)
	at java.io.UnixFileSystem.getBooleanAttributes(UnixFileSystem.java:242)
	at java.io.File.exists(File.java:813)
	at sun.misc.URLClassPath$FileLoader.getResource(URLClassPath.java:1080)
	at sun.misc.URLClassPath.getResource(URLClassPath.java:199)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:358)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:190)
	at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:122)
	at org.apache.spark.SparkContext.clean(SparkContext.scala:1623)
	at org.apache.spark.rdd.RDD.filter(RDD.scala:303)
	at org.apache.spark.streaming.dstream.FilteredDStream$$anonfun$compute$1.apply(FilteredDStream.scala:35)
	at org.apache.spark.streaming.dstream.FilteredDStream$$anonfun$compute$1.apply(FilteredDStream.scala:35)
	at scala.Option.map(Option.scala:145)
	at org.apache.spark.streaming.dstream.FilteredDStream.compute(FilteredDStream.scala:35)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:300)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:300)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:299)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:287)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:284)
	at org.apache.spark.streaming.dstream.FlatMappedDStream.compute(FlatMappedDStream.scala:35)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:300)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:300)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:299)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:287)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:284)
	at org.apache.spark.streaming.dstream.FilteredDStream.compute(FilteredDStream.scala:35)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:300)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:300)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:299)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:287)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:284)
	at org.apache.spark.streaming.dstream.ShuffledDStream.compute(ShuffledDStream.scala:41)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:300)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:300)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:299)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:287)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:284)
	at org.apache.spark.streaming.dstream.ShuffledDStream.compute(ShuffledDStream.scala:41)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:300)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:300)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:299)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:287)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:284)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$slice$4.apply(DStream.scala:778)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$slice$4.apply(DStream.scala:777)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:105)
	at org.apache.spark.streaming.dstream.DStream.slice(DStream.scala:777)
	at org.apache.spark.streaming.dstream.ReducedWindowedDStream.compute(ReducedWindowedDStream.scala:116)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:300)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:300)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:299)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:287)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:284)
	at org.apache.spark.streaming.dstream.ReducedWindowedDStream.compute(ReducedWindowedDStream.scala:121)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:300)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:300)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:299)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:287)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:284)
	at org.apache.spark.streaming.dstream.ReducedWindowedDStream.compute(ReducedWindowedDStream.scala:121)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:300)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:300)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:299)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:287)
	at scala.Option.orElse(Option.scala:257)        (many log like above)

the full log is in https://gist.githubusercontent.com/397090770/bb53fb2bf01447cefc2e/raw/gistfile1.txt



"
Stephen Boesch <javadba@gmail.com>,"Mon, 27 Apr 2015 22:09:22 -0700",Pickling error when attempting to add a method in pyspark,"""dev@spark.apache.org"" <dev@spark.apache.org>","My intention is to add pyspark support for certain mllib spark methods.  I
have been unable to resolve pickling errors of the form

   Pyspark py4j PickleException: â€œexpected zero arguments for construction
of ClassDictâ€
<http://stackoverflow.com/questions/29910708/pyspark-py4j-pickleexception-expected-zero-arguments-for-construction-of-class>

These are occurring during python to java conversion of python named
tuples.  The details are rather hard to provide here so I have created an
SOF question

http://stackoverflow.com/questions/29910708/pyspark-py4j-pickleexception-expected-zero-arguments-for-construction-of-class

In any case I have included the text here. The SOF is easier to read though
;)

--------------

This question is directed towards persons familiar with py4j - and can help
to resolve a pickling error. I am trying to add a method to the pyspark
PythonMLLibAPI that accepts an RDD of a namedtuple, does some work, and
returns a result in the form of an RDD.

This method is modeled after the PYthonMLLibAPI.trainALSModel() method,
whose analogous *existing* relevant portions are:

  def trainALSModel(
    ratingsJRDD: JavaRDD[Rating],
    .. )

The *existing* python Rating class used to model the new code is:

class Rating(namedtuple(""Rating"", [""user"", ""product"", ""rating""])):
    def __reduce__(self):
        return Rating, (int(self.user), int(self.product), float(self.rating))

Here is the attempt So here are the relevant classes:

*New* python class pyspark.mllib.clustering.MatrixEntry:

from collections import namedtupleclass
MatrixEntry(namedtuple(""MatrixEntry"", [""x"",""y"",""weight""])):
    def __reduce__(self):
        return MatrixEntry, (long(self.x), long(self.y), float(self.weight))

*New* method *foobarRDD* In PythonMLLibAPI:

  def foobarRdd(
    data: JavaRDD[MatrixEntry]): RDD[FooBarResult] = {
    val rdd = data.rdd.map { d => FooBarResult(d.i, d.j, d.value, d.i
* 100 + d.j * 10 + d.value)}
    rdd
  }

Now let us try it out:

from pyspark.mllib.clustering import MatrixEntry
def convert_to_MatrixEntry(tuple):
  return MatrixEntry(*tuple)
from pyspark.mllib.clustering import *
pic = PowerIterationClusteringModel(2)
tups = [(1,2,3),(4,5,6),(12,13,14),(15,7,8),(16,17,16.5)]
trdd = sc.parallelize(map(convert_to_MatrixEntry,tups))
# print out the RDD on python side just for validationprint ""%s""
%(repr(trdd.collect()))
from pyspark.mllib.common import callMLlibFunc
pic = callMLlibFunc(""foobar"", trdd)

Relevant portions of results:

[(1,2)=3.0, (4,5)=6.0, (12,13)=14.0, (15,7)=8.0, (16,17)=16.5]

which shows the input rdd is 'whole'. However the pickling was unhappy:

5/04/27 21:15:44 ERROR Executor: Exception in task 6.0 in stage 1.0 (TID 14)
net.razorvine.pickle.PickleException: expected zero arguments for
construction of ClassDict(for pyspark.mllib.clustering.MatrixEntry)
    at net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)
    at net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:617)
    at net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:170)
    at net.razorvine.pickle.Unpickler.load(Unpickler.java:84)
    at net.razorvine.pickle.Unpickler.loads(Unpickler.java:97)
    at org.apache.spark.mllib.api.python.SerDe$$anonfun$pythonToJava$1$$anonfun$apply$1.apply(PythonMLLibAPI.scala:1167)
    at org.apache.spark.mllib.api.python.SerDe$$anonfun$pythonToJava$1$$anonfun$apply$1.apply(PythonMLLibAPI.scala:1166)
    at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
    at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
    at scala.collection.Iterator$class.foreach(Iterator.scala:727)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
    at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
    at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
    at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
    at scala.collection.AbstractIterator.to(Iterator.scala:1157)
a:265)
    at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
:252)
    at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
    at org.apache.spark.rdd.RDD$$anonfun$17.apply(RDD.scala:819)
    at org.apache.spark.rdd.RDD$$anonfun$17.apply(RDD.scala:819)
    at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1523)
    at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1523)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
    at org.apache.spark.scheduler.Task.run(Task.scala:64)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:212)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:724)


Here is the python invocation stack trace:

Py4JJavaError                             Traceback (most recent call last)
<ipython-input-2-3589950a5c09> in <module>()
     12
     13 from pyspark.mllib.common import callMLlibFunc
---> 14 pic = callMLlibFunc(""foobar"", trdd)

/shared/picpy/python/pyspark/mllib/common.pyc in callMLlibFunc(name, *args)
    119     sc = SparkContext._active_spark_context
    120     api = getattr(sc._jvm.PythonMLLibAPI(), name)
--> 121     return callJavaFunc(sc, api, *args)
    122
    123

/shared/picpy/python/pyspark/mllib/common.pyc in callJavaFunc(sc, func,
*args)
    112     """""" Call Java Function """"""
    113     args = [_py2java(sc, a) for a in args]
--> 114     return _java2py(sc, func(*args))
    115
    116

/Library/Python/2.7/site-packages/py4j-0.8.2.1-py2.7.egg/py4j/java_gateway.pyc
in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539
    540         for temp_arg in temp_args:

/Library/Python/2.7/site-packages/py4j-0.8.2.1-py2.7.egg/py4j/protocol.pyc
in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling o31.foobar.
"
Manku Timma <manku.timma1@gmail.com>,"Tue, 28 Apr 2015 14:25:46 +0530",Re: creating hive packages for spark,yash datta <saucam@gmail.com>,"Yash,
This is exactly what I wanted! Thanks a bunch.


"
Akhil Das <akhil@sigmoidanalytics.com>,"Tue, 28 Apr 2015 15:38:32 +0530",Re: java.lang.StackOverflowError when recovery from checkpoint in Streaming,"""wyphao.2007"" <wyphao.2007@163.com>","There's a similar issue reported over here
https://issues.apache.org/jira/browse/SPARK-6847

Thanks
Best Regards


"
"""wyphao.2007"" <wyphao.2007@163.com>","Tue, 28 Apr 2015 18:25:01 +0800 (CST)","Re:Re: java.lang.StackOverflowError when recovery from checkpoint
 in Streaming","""Akhil Das"" <akhil@sigmoidanalytics.com>","Hi Akhil Das, Thank you for your reply.
It is very similar to my problem, I will focus on it.
Thanks
Best Regards

At 2015-04-28 18:08:32,""Akhil Das"" <akhil@sigmoidanalytics.com> wrote:
>There's a similar issue reported over here
>https://issues.apache.org/jira/browse/SPARK-6847
>
>Thanks
>Best Regards
>
>On Tue, Apr 28, 2015 at 7:35 AM, wyphao.2007 <wyphao.2007@163.com> wrote:
>
>>  Hi everyone, I am using val messages =
>> KafkaUtils.createDirectStream[String, String, StringDecoder,
>> StringDecoder](ssc, kafkaParams, topicsSet) to read data from
>> kafka(1k/second), and store the data in windows,the code snippets as
>> follow:        val windowedStreamChannel =
>> streamChannel.combineByKey[TreeSet[Obj]](TreeSet[Obj](_), _ += _, _ ++= _,
>> new HashPartitioner(numPartition))
>>           .reduceByKeyAndWindow((x: TreeSet[Obj], y: TreeSet[Obj]) => x
>> ++= y,
>>             (x: TreeSet[Obj], y: TreeSet[Obj]) => x --= y, Minutes(60),
>> Seconds(2), numPartition,
>>             (item: (String, TreeSet[Obj])) => item._2.size != 0)after the
>> application  run for an hour,  I kill the application and restart it from
>> checkpoint directory, but I  encountered an exception:2015-04-27
>> 17:52:40,955 INFO  [Driver] - Slicing from 1430126222000 ms to
>> 1430126222000 ms (aligned to 1430126222000 ms and 1430126222000 ms)
>> 2015-04-27 17:52:40,958 ERROR [Driver] - User class threw exception: null
>> java.lang.StackOverflowError
>>         at java.io.UnixFileSystem.getBooleanAttributes0(Native Method)
>>         at
>> java.io.UnixFileSystem.getBooleanAttributes(UnixFileSystem.java:242)
>>         at java.io.File.exists(File.java:813)
>>         at
>> sun.misc.URLClassPath$FileLoader.getResource(URLClassPath.java:1080)
>>         at sun.misc.URLClassPath.getResource(URLClassPath.java:199)
>>         at java.net.URLClassLoader$1.run(URLClassLoader.java:358)
>>         at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
>>         at java.security.AccessController.doPrivileged(Native Method)
>>         at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
>>         at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
>>         at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
>>         at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
>>         at java.lang.Class.forName0(Native Method)
>>         at java.lang.Class.forName(Class.java:190)
>>         at
>> org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:122)
>>         at org.apache.spark.SparkContext.clean(SparkContext.scala:1623)
>>         at org.apache.spark.rdd.RDD.filter(RDD.scala:303)
>>         at
>> org.apache.spark.streaming.dstream.FilteredDStream$$anonfun$compute$1.apply(FilteredDStream.scala:35)
>>         at
>> org.apache.spark.streaming.dstream.FilteredDStream$$anonfun$compute$1.apply(FilteredDStream.scala:35)
>>         at scala.Option.map(Option.scala:145)
>>         at
>> org.apache.spark.streaming.dstream.FilteredDStream.compute(FilteredDStream.scala:35)
>>         at
>> org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:300)
>>         at
>> org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:300)
>>         at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
>>         at
>> org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:299)
>>         at
>> org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:287)
>>         at scala.Option.orElse(Option.scala:257)
>>         at
>> org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:284)
>>         at
>> org.apache.spark.streaming.dstream.FlatMappedDStream.compute(FlatMappedDStream.scala:35)
>>         at
>> org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:300)
>>         at
>> org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:300)
>>         at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
>>         at
>> org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:299)
>>         at
>> org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:287)
>>         at scala.Option.orElse(Option.scala:257)
>>         at
>> org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:284)
>>         at
>> org.apache.spark.streaming.dstream.FilteredDStream.compute(FilteredDStream.scala:35)
>>         at
>> org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:300)
>>         at
>> org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:300)
>>         at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
>>         at
>> org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:299)
>>         at
>> org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:287)
>>         at scala.Option.orElse(Option.scala:257)
>>         at
>> org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:284)
>>         at
>> org.apache.spark.streaming.dstream.ShuffledDStream.compute(ShuffledDStream.scala:41)
>>         at
>> org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:300)
>>         at
>> org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:300)
>>         at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
>>         at
>> org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:299)
>>         at
>> org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:287)
>>         at scala.Option.orElse(Option.scala:257)
>>         at
>> org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:284)
>>         at
>> org.apache.spark.streaming.dstream.ShuffledDStream.compute(ShuffledDStream.scala:41)
>>         at
>> org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:300)
>>         at
>> org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:300)
>>         at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
>>         at
>> org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:299)
>>         at
>> org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:287)
>>         at scala.Option.orElse(Option.scala:257)
>>         at
>> org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:284)
>>         at
>> org.apache.spark.streaming.dstream.DStream$$anonfun$slice$4.apply(DStream.scala:778)
>>         at
>> org.apache.spark.streaming.dstream.DStream$$anonfun$slice$4.apply(DStream.scala:777)
>>         at
>> scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
>>         at
>> scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
>>         at scala.collection.Iterator$class.foreach(Iterator.scala:727)
>>         at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
>>         at
>> scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
>>         at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
>>         at
>> scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251)
>>         at
>> scala.collection.AbstractTraversable.flatMap(Traversable.scala:105)
>>         at
>> org.apache.spark.streaming.dstream.DStream.slice(DStream.scala:777)
>>         at
>> org.apache.spark.streaming.dstream.ReducedWindowedDStream.compute(ReducedWindowedDStream.scala:116)
>>         at
>> org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:300)
>>         at
>> org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:300)
>>         at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
>>         at
>> org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:299)
>>         at
>> org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:287)
>>         at scala.Option.orElse(Option.scala:257)
>>         at
>> org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:284)
>>         at
>> org.apache.spark.streaming.dstream.ReducedWindowedDStream.compute(ReducedWindowedDStream.scala:121)
>>         at
>> org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:300)
>>         at
>> org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:300)
>>         at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
>>         at
>> org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:299)
>>         at
>> org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:287)
>>         at scala.Option.orElse(Option.scala:257)
>>         at
>> org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:284)
>>         at
>> org.apache.spark.streaming.dstream.ReducedWindowedDStream.compute(ReducedWindowedDStream.scala:121)
>>         at
>> org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:300)
>>         at
>> org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:300)
>>         at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
>>         at
>> org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:299)
>>         at
>> org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:287)
>>         at scala.Option.orElse(Option.scala:257)        (many log like
>> above)
>>
>> the full log is in
>> https://gist.githubusercontent.com/397090770/bb53fb2bf01447cefc2e/raw/gistfile1.txt
>>
>>
>>
>>
"
Bo Fu <bof@uchicago.edu>,"Tue, 28 Apr 2015 16:13:38 +0000",How to deploy self-build spark source code on EC2,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,

I have an issue. I added some timestamps in Spark source code and built it using:

mvn package -DskipTests

I checked the new version in my own computer and it works. However, when I ran spark on EC2, the spark code EC2 machines ran is the original version.

Anyone knows how to deploy the changed spark source code into EC2?
Thx a lot


Bo Fu

---------------------------------------------------------------------


"
art1i <a.topchyan@reply.de>,"Tue, 28 Apr 2015 09:20:37 -0700 (MST)",SPARK_SUBMIT_CLASSPATH Windows PYSPARK,dev@spark.apache.org,"Hi,

So I was trying to get the Kafka Streaming working in a standalone python
application.

I needed to add the dependencies for this to work. The suggested way for
this is doing --jars using spark-submit, which is not practical considering
I wanted to launch and debug an application. Also you still have to set
-driver-class-path for this to work. 

So after searching around I found a JIRA, which said, that setting
SPARK_SUBMIT_CLASSPATH is workaround. This works on Linux, but on Windows
just setting the environmental var did not seem to work. So I looked into
spark-submit.cmd and it seems SPARK_SUBMIT_CLASSPATH is being set to:

set SPARK_SUBMIT_CLASSPATH= 

This is then overridden if  --driver-class-path is supplied. But  did not
want to supply the  argsument, I just wanted my enviromental variable to
persist like on Linux. So instead i changed the line

set SPARK_SUBMIT_CLASSPATH=%SPARK_SUBMIT_CLASSPATH%

Now everything works as expected and I can just inject the var in my python
script before importing  pypsark

os.environ[""SPARK_SUBMIT_CLASSPATH""]=""path1.jar;path2.jar""

So I was wondering if there is any reason this is not the default behavior
or I am just doing something wrong?

This is my first time posting, please excuse me if this is the wrong place
to post this.

Cheers,

Artyom
 
 



--

---------------------------------------------------------------------


"
"""Shuai Zheng"" <szheng.code@gmail.com>","Tue, 28 Apr 2015 15:52:45 -0400",RE: [SQL][Feature] Access row by column name instead of index,"""'Michael Armbrust'"" <michael@databricks.com>,
	""'Reynold Xin'"" <rxin@databricks.com>","Hi, 

 

I add a few helper method on this, to make java developer life easier, because we canâ€™t benefit from the generic feature on getAs[T].

 

Please let me know if I should not do that.

 

Regards,

 

Shuai

 

From: Michael Armbrust [mailto:michael@databricks.com] 
Sent: Friday, April 24, 2015 5:57 PM
To: Reynold Xin
Cc: Shuai Zheng; dev
Subject: Re: [SQL][Feature] Access row by column name instead of index

 

Already done :)

 

https://github.com/apache/spark/commit/2e8c6ca47df14681c1110f0736234ce76a3eca9b

 


Can you elaborate what you mean by that? (what's already available in
Python?)




access
java)

 

"
sara mustafa <eng.sara.mustafa@gmail.com>,"Tue, 28 Apr 2015 13:19:20 -0700 (MST)","=?UTF-8?Q?Re:_Spark_SQL_1.3.1=E3=80=80""saveAsParquetFile""=E3=80=80will__?=
 =?UTF-8?Q?output__tachyon_file_with_different_block_size?=",dev@spark.apache.org,"Hi Zhang,

How did you compile Spark 1.3.1 with Tachyon? when i changed Tachyon version
to 0.6.3 in core/pom.xml, make-distribution.sh and try to compile again,
many compilation errors raised.

Thanks,




--

---------------------------------------------------------------------


"
Calvin Jia <jia.calvin@gmail.com>,"Tue, 28 Apr 2015 14:23:42 -0700","Re: Spark SQL 1.3.1 ""saveAsParquetFile"" will output tachyon file with
 different block size",sara mustafa <eng.sara.mustafa@gmail.com>,"Hi,

You can apply this patch <https://github.com/apache/spark/pull/5354> and
recompile.

Hope this helps,
Calvin


"
zhangxiongfei  <zhangxiongfei0815@163.com>,"Wed, 29 Apr 2015 09:33:36 +0800 (CST)","=?GBK?Q?Re:Re:_Spark_SQL_1.3.1=A1=A1""saveAsParquetFile""=A1=A1will?=
 =?GBK?Q?__output__tachyon_file_with_different_block_size?=","""sara mustafa"" <eng.sara.mustafa@gmail.com>","HiActually I did not use Tachyon 0.6.3,just compiled it with 0.5.0 by make-distribution.sh. When I pulled the spark code from github,the Tachyon version was still 0.5.0 in pom,xml.

Regards
Zhang




At 2015-04-29 04:19:20, ""sara mustafa"" <eng.sara.mustafa@gmail.com> wrote:
>Hi Zhang,
>
>How did you compile Spark 1.3.1 with Tachyon? when i changed Tachyon version
>to 0.6.3 in core/pom.xml, make-distribution.sh and try to compile again,
>many compilation errors raised.
>
>Thanks,
>
>
>
>
>--
>View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-SQL-1-3-1-saveAsParquetFile-will-output-tachyon-file-with-different-block-size-tp11561p11870.html
>Sent from the Apache Spark Developers List mailing list archive at Nabble.com.
>
>---------------------------------------------------------------------
>To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>For additional commands, e-mail: dev-help@spark.apache.org
>
"
lonely Feb <lonely8658@gmail.com>,"Wed, 29 Apr 2015 16:44:15 +0800",Spark SQL cannot tolerate regexp with BIGINT,dev@spark.apache.org,"Hi all, we are transfer our HIVE job into SparkSQL, but we found a litter
difference between HIVE and Spark SQL that our sql has a statement like:

select A from B where id regexp '^12345$'

in HIVE it works fine but in Spark SQL we got a:

java.lang.ClassCastException: java.lang.Long cannot be cast to
java.lang.String

Can this statement be handled with Spark SQL?
"
Manku Timma <manku.timma1@gmail.com>,"Wed, 29 Apr 2015 12:05:59 +0530",Re: hive initialization on executors,dev@spark.apache.org,"The problem was in my hive-13 branch. So ignore this.


"
Ewan Higgs <ewan.higgs@ugent.be>,"Wed, 29 Apr 2015 10:18:02 +0200",Tungsten + Flink,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,
A quick question about Tungsten. The announcement of the Tungsten 
project is on the back of Hadoop Summit in Brussels where some of the 
Flink devs were giving talks [1] on how Flink manages memory using byte 
arrays and the like to avoid the overhead of all the Java types[2]. Is 
there an opportunity for code reuse here? Spark and Flink may have 
different needs in some respects, but they work fundamentally towards 
the same goal so I imagine there could be come worthwhile collaboration.

-Ewan

[1] http://2015.hadoopsummit.org/brussels/speaker/?speaker=MrtonBalassi
http://2015.hadoopsummit.org/brussels/speaker/?speaker=AljoschaKrettek

[2] 
https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=53741525
https://flink.apache.org/news/2015/03/13/peeking-into-Apache-Flinks-Engine-Room.html

---------------------------------------------------------------------


"
"""wyphao.2007"" <wyphao.2007@163.com>","Wed, 29 Apr 2015 18:42:04 +0800 (CST)",Driver memory leak?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi, Dear developer, I am using Spark Streaming to read data from kafka, the program already run about 120 hours, but today the program failed because of driver's OOM as follow:


Container [pid=49133,containerID=container_1429773909253_0050_02_000001] is running beyond physical memory limits. Current usage: 2.5 GB of 2.5 GB physical memory used; 3.2 GB of 50 GB virtual memory used. Killing container.


I set --driver-memory to 2g, In my mind, driver is responsibility for job scheduler and job monitor(Please correct me If I'm wrong), Why it using so much memory?


So I using jmap to monitor other program(already run about 48 hours): 
sudo /home/q/java7/jdk1.7.0_45/bin/jmap -histo:live 31256, the result as follow:
the java.util.HashMap$Entry and java.lang.Long  object using about 600Mb memory!


and I also using jmap to monitor other program(already run about 1 hours),  the result as follow:
the java.util.HashMap$Entry and java.lang.Long object doesn't using so many memory, But I found, as time goes by, the java.util.HashMap$Entry and java.lang.Long object will occupied more and more memory,
It is driver's memory leak question? or other reason?
Thanks
Best Regards









"
=?UTF-8?B?U8OpYmFzdGllbiBTb3VicsOpLUxhbmFiw6hyZQ==?= <s.soubre@gmail.com>,"Wed, 29 Apr 2015 12:56:16 +0200",RDD split into multiple RDDs,dev@spark.apache.org,"Hello,

I'm facing a problem with custom RDD transformations.

I would like to transform a RDD[K, V] into a Map[K, RDD[V]], meaning a map
of RDD by key.

This would be great, for example, in order to process mllib clustering on V
values grouped by K.

I know I could do it using filter() on my RDD as many times I have keys,
but I'm afraid this would not be efficient (the entire RDD would be read
each time, right ?). Then, I could mapByPartition my RDD before filtering,
but the code is finally huge...

So, I tried to create a CustomRDD to implement a splitByKey(rdd: RDD[K,
V]): Map[K, RDD[V]] method, which would iterate on the RDD once time only,
but I cannot achieve my development.

Please, could you tell me first if this is really faisable, and then, could
you give me some pointers ?

Thank you,
Regards,
Sebastien
"
"""wyphao.2007"" <wyphao.2007@163.com>","Wed, 29 Apr 2015 18:59:28 +0800 (CST)",Re:Re:Driver memory leak?,"zhangxiongfei <zhangxiongfei0815@163.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","No, I am not collect  the result to driver,I sample send the result to kafka.


BTW, the image address are:
https://cloud.githubusercontent.com/assets/5170878/7389463/ac03bf34-eea0-11e4-9e6b-1d2fba170c1c.png
and 
https://cloud.githubusercontent.com/assets/5170878/7389480/c629d236-eea0-11e4-983a-dc5aa97c2554.png



At 2015-04-29 18:48:33,zhangxiongfei <zhangxiongfei0815@163.com> wrote:



The mount of memory that the driver consumes depends on your program logic,did you try to collect the result of Spark job?




At 2015-04-29 18:42:04, ""wyphao.2007"" <wyphao.2007@163.com> wrote:

Hi, Dear developer, I am using Spark Streaming to read data from kafka, the program already run about 120 hours, but today the program failed because of driver's OOM as follow:


Container [pid=49133,containerID=container_1429773909253_0050_02_000001] is running beyond physical memory limits. Current usage: 2.5 GB of 2.5 GB physical memory used; 3.2 GB of 50 GB virtual memory used. Killing container.


I set --driver-memory to 2g, In my mind, driver is responsibility for job scheduler and job monitor(Please correct me If I'm wrong), Why it using so much memory?


So I using jmap to monitor other program(already run about 48 hours): 
sudo /home/q/java7/jdk1.7.0_45/bin/jmap -histo:live 31256, the result as follow:
the java.util.HashMap$Entry and java.lang.Long  object using about 600Mb memory!


and I also using jmap to monitor other program(already run about 1 hours),  the result as follow:
the java.util.HashMap$Entry and java.lang.Long object doesn't using so many memory, But I found, as time goes by, the java.util.HashMap$Entry and java.lang.Long object will occupied more and more memory,
It is driver's memory leak question? or other reason?
Thanks
Best Regards















"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Wed, 29 Apr 2015 11:26:08 +0000",Pandas' Shift in Dataframe,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,
Is there any plan to add the ""shift"" method from Pandas to Spark Dataframe,
not that I think it's an easy task...

c.f.
http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.shift.html

Regards,

Olivier.
"
=?UTF-8?B?SnVhbiBSb2Ryw61ndWV6IEhvcnRhbMOh?= <juan.rodriguez.hortala@gmail.com>,"Wed, 29 Apr 2015 14:10:26 +0200",Re: RDD split into multiple RDDs,=?UTF-8?B?U8OpYmFzdGllbiBTb3VicsOpLUxhbmFiw6hyZQ==?= <s.soubre@gmail.com>,"Hi SÃ©bastien,

I came with a similar problem some time ago, you can see the discussion in
the Spark users mailing list at
http://markmail.org/message/fudmem4yy63p62ar#query:+page:1+mid:qv4gw6czf6lb6hpq+state:results
. My experience was that when you create too many RDDs the Spark scheduler
gets stuck, so if you have many keys in the map you are creating you'll
that mailing thread was a batch job in which we start from a single RDD of
time tagged data, transform the RDD in a list of RDD corresponding to
generating windows according to the time tag of the records, and then apply
a transformation of RDD to each window RDD, like for example KMeans.run of
MLlib. This is very similar to what you propose.
So in my humble opinion the approach of generating thousands of RDDs by
filtering doesn't work, and a new RDD class should be implemented for this.
I have never implemented a custom RDD, but if you want some help I would be
happy to join you in this task

Greetings,

Juan



2015-04-29 12:56 GMT+02:00 SÃ©bastien SoubrÃ©-LanabÃ¨re <s.soubre@gmail.com>:

p
 V
,
,
ld
"
Daniel Darabos <daniel.darabos@lynxanalytics.com>,"Wed, 29 Apr 2015 14:20:06 +0200",Re: RDD split into multiple RDDs,juan.rodriguez.hortala@gmail.com,"Check out http://stackoverflow.com/a/26051042/3318517. It's a nice method
for saving the RDD into separate files by key in a single pass. Then you
can read the files into separate RDDs.


n
lb6hpq+state:results
r
in
f
ly
f
s.
be

Sebastien said nothing about thousands of keys. This is a valid problem
even if you only have two different keys.

Greetings,
ubre@gmail.com>:
,
d
"
=?UTF-8?B?SnVhbiBSb2Ryw61ndWV6IEhvcnRhbMOh?= <juan.rodriguez.hortala@gmail.com>,"Wed, 29 Apr 2015 15:02:41 +0200",Re: RDD split into multiple RDDs,Daniel Darabos <daniel.darabos@lynxanalytics.com>,"Hi Daniel,

I understood SÃ©bastien was talking having having a high number of keys, I
guess I was prejudiced by my own problem! :) Anyway I don't think you need
to use disk or a database to generate a RDD per key, you can use filter
which I guess would be more efficient because IO is avoided, especially if
the RDD was cached. For example:

// in the spark shell
import org.apache.spark.rdd.RDD
import org.apache.spark.rdd.RDD._
import scala.reflect.ClassTag

// generate a map from key to rdd of values
def groupByKeyToRDDs[K, V](pairRDD: RDD[(K, V)]) (implicit kt: ClassTag[K],
vt: ClassTag[V], ord: Ordering[K]): Map[K, RDD[V]] = {
    val keys = pairRDD.keys.distinct.collect
    (for (k <- keys) yield
        k -> (pairRDD filter(_._1 == k) values)
    ) toMap
}

// simple demo
val xs = sc.parallelize(1 to 1000)
val ixs = xs map(x => (x % 10, x))
val gs = groupByKeyToRDDs(ixs)
gs(1).collect

Just an idea.

Greetings,

Juan Rodriguez



2015-04-29 14:20 GMT+02:00 Daniel Darabos <daniel.darabos@lynxanalytics.com:

in
6lb6hpq+state:results
er
of
of
oubre@gmail.com
s,
ad
,
"
=?UTF-8?B?U8OpYmFzdGllbiBTb3VicsOpLUxhbmFiw6hyZQ==?= <s.soubre@gmail.com>,"Wed, 29 Apr 2015 15:57:09 +0200",Re: RDD split into multiple RDDs,dev@spark.apache.org,"Hi Juan, Daniel,

thank you for your explanations. Indeed, I don't have a big number of keys,
at least not enough to stuck the scheduler.

I was using a method quite similar as what you post, Juan, and yes it
works, but I think this would be more efficient to not call filter on each
key. So, I was thinking something like :
- get the iterator of the KV rdd
- distribute each value into a subset by key and then recreate a rdd from
this subset

Because spark context parallelize method cannot be used inside a
transformation, I wonder if I could do it by creating a custom RDD and then
try to implement something like PairRDDFunctions.lookup method, but
remplacing Seq[V] of course by a RDD

def lookup(key: K): Seq[V] = {
    self.partitioner match {
      case Some(p) =>
        val index = p.getPartition(key)
        val process = (it: Iterator[(K, V)]) => {
          val buf = new ArrayBuffer[V]
          for (pair <- it if pair._1 == key) {
            buf += pair._2
          }
          buf
        } : Seq[V]
        val res = self.context.runJob(self, process, Array(index), false)
        res(0)
      case None =>
        self.filter(_._1 == key).map(_._2).collect()
    }
  }


2015-04-29 15:02 GMT+02:00 Juan RodrÃ­guez HortalÃ¡ <
juan.rodriguez.hortala@gmail.com>:

ys, I
d
f
hen
f6lb6hpq+state:results
d
d
soubre@gmail.com
a
g
K,
"
Tom Hubregtsen <thubregtsen@gmail.com>,"Wed, 29 Apr 2015 10:59:04 -0700 (MST)",Re: Spilling when not expected,dev@spark.apache.org,"Hi reynold,

It took me some time, but I've finally found that there is a difference
between spilling on the map-side and spilling on the reduce-side for a
shuffle. Spilling to disk on the map-side happens by default (with the
spillToPartitionFiles call from insertAll in ExternalSorter; don't know yet
why there is a difference in number of calls though), spilling on the reduce
side (with the maybeSpillCollection call from insertAll in ExternalSorter)
is optional and based on the available memory set by
spark.shuffle.memoryFraction and the total memory available. In my case, I
was just seeing the spilling on the map-side, but did not realize that this
is supposed to happen, regardless of the memory settings.

Thanks for your help,

Tom



--

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 29 Apr 2015 19:09:47 +0000",Re: Pandas' Shift in Dataframe,"Olivier Girardot <o.girardot@lateral-thoughts.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","You can check JIRA for any existing plans. If there isn't any, then feel
free to create a JIRA and make the case there for why this would be a good
feature to add.

Nick


"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Wed, 29 Apr 2015 19:43:03 +0000",Re: Pandas' Shift in Dataframe,"Nicholas Chammas <nicholas.chammas@gmail.com>, 
	Olivier Girardot <o.girardot@lateral-thoughts.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Nicholas,
yes I've already checked, and I've just created the
https://issues.apache.org/jira/browse/SPARK-7247
I'm not even sure why this would be a good feature to add except the fact
that some of the data scientists I'm working with are using it, and it
would be therefore useful for me to translate Pandas code to Spark...

Isn't the goal of Spark Dataframe to allow all the features of Pandas/R
Dataframe using Spark ?

Regards,

Olivier.

Le mer. 29 avr. 2015 Ã  21:09, Nicholas Chammas <nicholas.chammas@gmail.com>
a Ã©crit :

d
hift.html
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 29 Apr 2015 20:08:40 +0000",Re: Pandas' Shift in Dataframe,"Olivier Girardot <o.girardot@lateral-thoughts.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","I can't comment on the direction of the DataFrame API (that's more for
Reynold or Michael I guess), but I just wanted to point out that the JIRA
would be the recommended way to create a central place for discussing a
feature add like that.

Nick


il.com>
od
shift.html
"
Reynold Xin <rxin@databricks.com>,"Wed, 29 Apr 2015 13:25:01 -0700",Re: Pandas' Shift in Dataframe,Nicholas Chammas <nicholas.chammas@gmail.com>,"In this case it's fine to discuss whether this would fit in Spark
DataFrames' high level direction before putting it in JIRA. Otherwise we
might end up creating a lot of tickets just for querying whether something
might be a good idea.

About this specific feature -- I'm not sure what it means in general given
we don't have axis in Spark DataFrames. But I think it'd probably be good
to be able to shift a column by one so we can support the end time / begin
time case, although it'd require two passes over the data.




ct
el
ift.html
"
"""Evan R. Sparks"" <evan.sparks@gmail.com>","Wed, 29 Apr 2015 13:34:23 -0700",Re: Pandas' Shift in Dataframe,Reynold Xin <rxin@databricks.com>,"In general there's a tension between ordered data and set-oriented data
model underlying DataFrames. You can force a total ordering on the data,
but it may come at a high cost with respect to performance.

It would be good to get a sense of the use case you're trying to support,
but one suggestion would be to apply I can imagine achieving a similar
result by applying a datetime.timedelta (in Python terms) to a time
attribute (your ""axis"") and then performing join between the base table and
this derived table to merge the data back together. This type of join could
then be optimized if the use case is frequent enough to warrant it.

- Evan


g
n
n
RA
t
/R
a
ift.html
"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Wed, 29 Apr 2015 20:51:35 +0000",Re: Pandas' Shift in Dataframe,"""Evan R. Sparks"" <evan.sparks@gmail.com>, Reynold Xin <rxin@databricks.com>","To give you a broader idea of the current use case, I have a few
transformations (sort and column creations) oriented towards a simple goal.
My data is timestamped and if two lines are identical, that time difference
will have to be more than X days in order to be kept, so there are a few
shifts done but very locally : only -1 or +1.

FYI regarding JIRA, i created one -
https://issues.apache.org/jira/browse/SPARK-7247 - associated to this
discussion.
@rxin considering, in my use case, the data is sorted beforehand, there
might be a better way - but I guess some shuffle would needed anyway...


Le mer. 29 avr. 2015 Ã  22:34, Evan R. Sparks <evan.sparks@gmail.com> a
Ã©crit :

nd
ld
ng
en
d
in
a
it
.
 a
hift.html
"
Olivier Girardot <ssaboum@gmail.com>,"Wed, 29 Apr 2015 21:06:25 +0000",Re: Spark SQL cannot tolerate regexp with BIGINT,"lonely Feb <lonely8658@gmail.com>, dev@spark.apache.org","I guess you can use cast(id as String) instead of just id in your where
clause ?

Le mer. 29 avr. 2015 Ã  12:13, lonely Feb <lonely8658@gmail.com> a Ã©crit :

"
Sandy Ryza <sandy.ryza@cloudera.com>,"Wed, 29 Apr 2015 14:40:19 -0700",Re: Using memory mapped file for shuffle,Kannan Rajah <krajah@maprtech.com>,"Spark currently doesn't allocate any memory off of the heap for shuffle
objects.  When the in-memory data gets too large, it will write it out to a
file, and then merge spilled filed later.

What exactly do you mean by store shuffle data in HDFS?

-Sandy


"
Reynold Xin <rxin@databricks.com>,"Wed, 29 Apr 2015 15:21:47 -0700",[discuss] DataFrame function namespacing,"""dev@spark.apache.org"" <dev@spark.apache.org>","Before we make DataFrame non-alpha, it would be great to decide how we want
to namespace all the functions. There are 3 alternatives:

1. Put all in org.apache.spark.sql.functions. This is how SQL does it,
since SQL doesn't have namespaces. I estimate eventually we will have ~ 200
functions.

2. Have explicit namespaces, which is what master branch currently looks
like:

- org.apache.spark.sql.functions
- org.apache.spark.sql.mathfunctions
- ...

3. Have explicit namespaces, but restructure them slightly so everything is
under functions.

package object functions {

  // all the old functions here -- but deprecated so we keep source
compatibility
  def ...
}

package org.apache.spark.sql.functions

object mathFunc {
  ...
}

object basicFuncs {
  ...
}
"
Reynold Xin <rxin@databricks.com>,"Wed, 29 Apr 2015 15:30:59 -0700",Re: [discuss] DataFrame function namespacing,"""dev@spark.apache.org"" <dev@spark.apache.org>","To add a little bit more context, some pros/cons I can think of are:

Option 1: Very easy for users to find the function, since they are all in
org.apache.spark.sql.functions. However, there will be quite a large number
of them.

Option 2: I can't tell why we would want this one over Option 3, since it
has all the problems of Option 3, and not as nice of a hierarchy.

Option 3: Opposite of Option 1. Each ""package"" or static class has a small
number of functions that are relevant to each other, but for some functions
it is unclear where they should go (e.g. should ""min"" go into basic or
math?)





"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Wed, 29 Apr 2015 15:38:36 -0700",Re: [discuss] DataFrame function namespacing,Reynold Xin <rxin@databricks.com>,"My feeling is that we should have a handful of namespaces (say 4 or 5). It
becomes too cumbersome to import / remember more package names and having
everything in one package makes it hard to read scaladoc etc.

Thanks
Shivaram


"
Reynold Xin <rxin@databricks.com>,"Wed, 29 Apr 2015 15:51:59 -0700",Re: [discuss] DataFrame function namespacing,Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Scaladoc isn't much of a problem because scaladocs are grouped. Java/Python
is the main problem ...

See
https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$


"
Reynold Xin <rxin@databricks.com>,"Wed, 29 Apr 2015 15:53:58 -0700",Re: Spark SQL cannot tolerate regexp with BIGINT,Olivier Girardot <ssaboum@gmail.com>,"We added ExpectedInputConversion rule recently in analysis:
https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/HiveTypeCoercion.scala#L647

With this rule, the analyzer automatically adds cast for expressions that
inherit ExpectsInputTypes. We can make all string functions inherit
ExpectsInputTypes and specify input types, so the casts are added
automatically. Would you like to submit a PR?



:

Ã©crit :
er
:
"
Sree V <sree_at_chess@yahoo.com.INVALID>,"Thu, 30 Apr 2015 00:01:07 +0000 (UTC)",Re: Tungsten + Flink,"Ewan Higgs <ewan.higgs@ugent.be>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","I agree, Ewan.
We should also look into combining both Flink and Spark into one.This eases the industry adaptation instead.

Thanking you.

With Regards
Sree 


   

 Hi all,
A quick question about Tungsten. The announcement of the Tungsten 
project is on the back of Hadoop Summit in Brussels where some of the 
Flink devs were giving talks [1] on how Flink manages memory using byte 
arrays and the like to avoid the overhead of all the Java types[2]. Is 
there an opportunity for code reuse here? Spark and Flink may have 
different needs in some respects, but they work fundamentally towards 
the same goal so I imagine there could be come worthwhile collaboration.

-Ewan

[1] http://2015.hadoopsummit.org/brussels/speaker/?speaker=MrtonBalassi
http://2015.hadoopsummit.org/brussels/speaker/?speaker=AljoschaKrettek

[2] 
https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=53741525
https://flink.apache.org/news/2015/03/13/peeking-into-Apache-Flinks-Engine-Room.html

---------------------------------------------------------------------



   "
lonely Feb <lonely8658@gmail.com>,"Thu, 30 Apr 2015 08:10:40 +0800",Re: Spark SQL cannot tolerate regexp with BIGINT,"Reynold Xin <rxin@databricks.com>, dev@spark.apache.org","OK, I'll try.

rg/apache/spark/sql/catalyst/analysis/HiveTypeCoercion.scala#L647
Ã©crit :
e:
"
Reynold Xin <rxin@databricks.com>,"Wed, 29 Apr 2015 17:29:28 -0700",Re: Spark SQL cannot tolerate regexp with BIGINT,lonely Feb <lonely8658@gmail.com>,"Actually I'm doing some cleanups related to type coercion, and I will take
care of this.



org/apache/spark/sql/catalyst/analysis/HiveTypeCoercion.scala#L647
t
Ã©crit
"
Michael Armbrust <michael@databricks.com>,"Wed, 29 Apr 2015 17:52:33 -0700",Re: Plans for upgrading Hive dependency?,Punyashloka Biswal <punya.biswal@gmail.com>,"I am working on it.  Here is the (very rough) version:
https://github.com/apache/spark/compare/apache:master...marmbrus:multiHiveVersions


"
Punyashloka Biswal <punya.biswal@gmail.com>,"Thu, 30 Apr 2015 05:01:50 +0000",Re: [discuss] DataFrame function namespacing,"Reynold Xin <rxin@databricks.com>, Shivaram Venkataraman <shivaram@eecs.berkeley.edu>","Do we still have to keep the names of the functions distinct to avoid
collisions in SQL? Or is there a plan to allow ""importing"" a namespace into
SQL somehow?

I ask because if we have to keep worrying about name collisions then I'm
not sure what the added complexity of #2 and #3 buys us.

Punya

"
Reynold Xin <rxin@databricks.com>,"Wed, 29 Apr 2015 22:03:04 -0700",Re: [discuss] DataFrame function namespacing,Punyashloka Biswal <punya.biswal@gmail.com>,"We definitely still have the name collision problem in SQL.


"
anshu shukla <anshushukla0@gmail.com>,"Thu, 30 Apr 2015 10:43:44 +0530",Event generator for SPARK-Streaming from csv,"dev@spark.apache.org, dev@storm.apache.org, user@spark.apache.org","I have the real DEBS-TAxi data in csv file , in order to operate over it
how to simulate a ""Spout"" kind  of thing as event generator using the
timestamps in CSV file.

-- 
SERC-IISC
Thanks & Regards,
Anshu Shukla
"
Niranda Perera <niranda.perera@gmail.com>,"Thu, 30 Apr 2015 11:32:00 +0530",Custom PersistanceEngine and LeaderAgent implementation in Java,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

this follows the following feature in this feature [1]

I'm trying to implement a custom persistence engine and a leader agent in
the Java environment.

vis-a-vis scala, when I implement the PersistenceEngine trait in java, I
would have to implement methods such as readPersistedData, removeDriver,
etc together with read, persist and unpersist methods.

but the issue here is, methods such as readPersistedData etc are 'final
def's, hence can not be overridden in the java environment.

I am new to scala, but is there any workaround to implement the above
traits in java?

look forward to hear from you.

[1] https://issues.apache.org/jira/browse/SPARK-1830

-- 
Niranda
"
Reynold Xin <rxin@databricks.com>,"Wed, 29 Apr 2015 23:04:24 -0700",Re: Custom PersistanceEngine and LeaderAgent implementation in Java,Niranda Perera <niranda.perera@gmail.com>,"We should change the trait to abstract class, and then your problem will go
away.

Do you want to submit a pull request?



"
"""Haopu Wang"" <HWang@qilinsoft.com>","Thu, 30 Apr 2015 18:50:10 +0800",RE: Is SQLContext thread-safe?,"""Cheng, Hao"" <hao.cheng@intel.com>,
	""user"" <user@spark.apache.org>,
	<dev@spark.apache.org>","Hi, in a test on SparkSQL 1.3.0, multiple threads are doing select on a
same SQLContext instance, but below exception is thrown, so it looks
like SQLContext is NOT thread safe? I think this is not the desired
behavior.

======

java.lang.RuntimeException: [1.1] failure: ``insert'' expected but
identifier select found

select id ,ext.d from UNIT_TEST
^
         at scala.sys.package$.error(package.scala:27)
         at
org.apache.spark.sql.catalyst.AbstractSparkSQLParser.apply(AbstractSpark
SQLParser.scala:40)
         at
org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:130)
         at
org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:130)
         at
org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkS
QLParser$$others$1.apply(SparkSQLParser.scala:96)
         at
org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkS
QLParser$$others$1.apply(SparkSQLParser.scala:95)
         at
scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
         at
scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
         at
scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parser
s.scala:242)
         at
scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parser
s.scala:242)
         at
scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
         at
scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$
apply$2.apply(Parsers.scala:254)
         at
scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$
apply$2.apply(Parsers.scala:254)
         at
scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
         at
scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Par
sers.scala:254)
         at
scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Par
sers.scala:254)
         at
scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
         at
scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Pa
rsers.scala:891)
         at
scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Pa
rsers.scala:891)
         at
scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
         at
scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
         at
scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParser
s.scala:110)
         at
org.apache.spark.sql.catalyst.AbstractSparkSQLParser.apply(AbstractSpark
SQLParser.scala:38)
         at
org.apache.spark.sql.SQLContext$$anonfun$parseSql$1.apply(SQLContext.sca
la:134)
         at
org.apache.spark.sql.SQLContext$$anonfun$parseSql$1.apply(SQLContext.sca
la:134)
         at scala.Option.getOrElse(Option.scala:120)
         at
org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:134)
         at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:915)


different threads at the same time? Thank you very much!

---------------------------------------------------------------------
commands, e-mail: user-help@spark.apache.org


---------------------------------------------------------------------


"
"""Wangfei (X)"" <wangfei1@huawei.com>","Thu, 30 Apr 2015 11:07:44 +0000",Re: Is SQLContext thread-safe?,Haopu Wang <HWang@qilinsoft.com>,"actually this is a sql parse exception, are you sure your sql is right?

·¢×ÔÎÒµÄ iPhone

> ÔÚ 2015Äê4ÔÂ30ÈÕ£¬18:50£¬""Haopu Wang"" <HWang@qilinsoft.com> Ð´µÀ£º
> 
> Hi, in a test on SparkSQL 1.3.0, multiple threads are doing select on a
> same SQLContext instance, but below exception is thrown, so it looks
> like SQLContext is NOT thread safe? I think this is not the desired
> behavior.
> 
> ======
> 
> java.lang.RuntimeException: [1.1] failure: ``insert'' expected but
> identifier select found
> 
> select id ,ext.d from UNIT_TEST
> ^
>         at scala.sys.package$.error(package.scala:27)
>         at
> org.apache.spark.sql.catalyst.AbstractSparkSQLParser.apply(AbstractSpark
> SQLParser.scala:40)
>         at
> org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:130)
>         at
> org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:130)
>         at
> org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkS
> QLParser$$others$1.apply(SparkSQLParser.scala:96)
>         at
> org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkS
> QLParser$$others$1.apply(SparkSQLParser.scala:95)
>         at
> scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
>         at
> scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
>         at
> scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parser
> s.scala:242)
>         at
> scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parser
> s.scala:242)
>         at
> scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
>         at
> scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$
> apply$2.apply(Parsers.scala:254)
>         at
> scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$
> apply$2.apply(Parsers.scala:254)
>         at
> scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
>         at
> scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Par
> sers.scala:254)
>         at
> scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Par
> sers.scala:254)
>         at
> scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
>         at
> scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Pa
> rsers.scala:891)
>         at
> scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Pa
> rsers.scala:891)
>         at
> scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
>         at
> scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
>         at
> scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParser
> s.scala:110)
>         at
> org.apache.spark.sql.catalyst.AbstractSparkSQLParser.apply(AbstractSpark
> SQLParser.scala:38)
>         at
> org.apache.spark.sql.SQLContext$$anonfun$parseSql$1.apply(SQLContext.sca
> la:134)
>         at
> org.apache.spark.sql.SQLContext$$anonfun$parseSql$1.apply(SQLContext.sca
> la:134)
>         at scala.Option.getOrElse(Option.scala:120)
>         at
> org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:134)
>         at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:915)
> 
> -----Original Message-----
> From: Cheng, Hao [mailto:hao.cheng@intel.com] 
> Sent: Monday, March 02, 2015 9:05 PM
> To: Haopu Wang; user
> Subject: RE: Is SQLContext thread-safe?
> 
> Yes it is thread safe, at least it's supposed to be.
> 
> -----Original Message-----
> From: Haopu Wang [mailto:HWang@qilinsoft.com] 
> Sent: Monday, March 2, 2015 4:43 PM
> To: user
> Subject: Is SQLContext thread-safe?
> 
> Hi, is it safe to use the same SQLContext to do Select operations in
> different threads at the same time? Thank you very much!
> 
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: user-unsubscribe@spark.apache.org For additional
> commands, e-mail: user-help@spark.apache.org
> 
> 
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
> 
"
twinkle sachdeva <twinkle.sachdeva@gmail.com>,"Thu, 30 Apr 2015 16:43:58 +0530",Regarding KryoSerialization in Spark,dev@spark.incubator.apache.org,"Hi,

As per the code, KryoSerialization used writeClassAndObject method, which
internally calls writeClass method, which will write the class of the
object while serilization.

As per the documentation in tuning page of spark, it says that registering
the class will avoid that.

Am I missing something or there is some issue with the documentation???

Thanks,
Twinkle
"
alexandre Clement <a.p.clement@gmail.com>,"Thu, 30 Apr 2015 16:36:59 +0200",withColumn is very slow with datasets with large number of columns,dev@spark.apache.org,"Hi all,


I'm experimenting serious performance problem when using withColumn and
dataset with large number of columns. It is very slow: on a dataset with
100 columns it takes a few seconds.


The code snippet demonstrates the problem.


val custs = Seq(
Row(1, ""Bob"", 21, 80.5),
Row(2, ""Bobby"", 21, 80.5),
Row(3, ""Jean"", 21, 80.5),
Row(4, ""Fatime"", 21, 80.5)
)

var fields = List(
StructField(""id"", IntegerType, true),
StructField(""a"", IntegerType, true),
StructField(""b"", StringType, true),
StructField(""target"", DoubleType, false))
val schema = StructType(fields)

var rdd = sc.parallelize(custs)
var df = sqlContext.createDataFrame(rdd, schema)

for (i <- 1 to 200)
{ val now = System.currentTimeMillis df = df.withColumn(""a_new_col_"" + i,
df(""a"") + i) println(s""$i -> "" + (System.currentTimeMillis - now)) }

df.show()
"
alexandre Clement <a.p.clement@gmail.com>,"Thu, 30 Apr 2015 16:37:56 +0200",Re: withColumn is very slow with datasets with large number of columns,dev@spark.apache.org,"I have reported the issue on JIRA:
https://issues.apache.org/jira/browse/SPARK-7276


"
Akshat Aranya <aaranya@gmail.com>,"Thu, 30 Apr 2015 09:11:12 -0700",Spark standalone cluster mode operation,dev@spark.apache.org,"Hi,

I'm trying to figure out how the Spark standalone cluster mode works.
Specifically, I'm looking at the code to see how the user's application jar
makes it from the submission node, when it is on the local file system, to
the driver and the executors.

somewhere, and then the driver spins up executors.  I couldn't figure out
how the application jar goes through though.  Could someone explain how
this mechanism works?

Thanks!
"
Michael Armbrust <michael@databricks.com>,"Thu, 30 Apr 2015 09:45:47 -0700",Re: Is SQLContext thread-safe?,"""Wangfei (X)"" <wangfei1@huawei.com>","Unfortunately, I think the SQLParser is not threadsafe.  I would recommend
using HiveQL.


pu Wang"" <HWang@qilinsoft.com> å†™é“ï¼š
k
S
S
r
r
$
$
r
r
a
a
r
k
a
a
l
"
Ted Yu <yuzhihong@gmail.com>,"Thu, 30 Apr 2015 10:04:00 -0700",Re: [discuss] DataFrame function namespacing,Reynold Xin <rxin@databricks.com>,"IMHO I would go with choice #1

Cheers


"
badgerpants <mark.stewart@tapjoy.com>,"Thu, 30 Apr 2015 10:10:24 -0700 (MST)","practical usage of the new ""exactly-once"" supporting
 DirectKafkaInputDStream",dev@spark.apache.org,"We're a group of experienced backend developers who are fairly new to Spark
Streaming (and Scala) and very interested in using the new (in 1.3)
DirectKafkaInputDStream impl as part of the metrics reporting service we're
building.

Our flow involves reading in metric events, lightly modifying some of the
data values, and then creating aggregates via reduceByKey. We're following
the approach in Cody Koeninger's blog on exactly-once streaming
(https://github.com/koeninger/kafka-exactly-once/blob/master/blogpost.md) in
which the Kakfa OffsetRanges are grabbed from the RDD and persisted to a
tracking table within the same db transaction as the data within said
ranges. 

Within a short time frame the offsets in the table fall out of synch with
the offsets. It appears that the writeOffsets method (see code below)
occasionally doesn't get called which also indicates that some blocks of
data aren't being processed either; the aggregate operation makes this
difficult to eyeball from the data that's written to the db.

Note that we do understand that the reduce operation alters that
size/boundaries of the partitions we end up processing. Indeed, without the
reduceByKey operation our code seems to work perfectly. But without the
reduceByKey operation the db has to perform *a lot* more updates. It's
certainly a significant restriction to place on what is such a promising
approach. I'm hoping there simply something we're missing.

Any workarounds or thoughts are welcome. Here's the code we've got:

def run(stream: DStream[Input], conf: Config, args: List[String]): Unit = {
    ...
    val sumFunc: (BigDecimal, BigDecimal) => BigDecimal = (_ + _)

    val transformStream = stream.transform { rdd =>
      val offsets = rdd.asInstanceOf[HasOffsetRanges].offsetRanges
      printOffsets(offsets) // just prints out the offsets for reference
      rdd.mapPartitionsWithIndex { case (i, iter) =>
        iter.flatMap { case (name, msg) => extractMetrics(msg) }
          .map { case (k,v) => ( ( keyWithFlooredTimestamp(k), offsets(i) ),
v ) }
      }
    }.reduceByKey(sumFunc, 1)

    transformStream.foreachRDD { rdd =>
      rdd.foreachPartition { partition =>
        val conn = DriverManager.getConnection(dbUrl, dbUser, dbPass)
        val db = DB(conn)
        db.autoClose(false)

        db.autoCommit { implicit session =>
          var currentOffset: OffsetRange = null
          partition.foreach { case (key, value) =>
            currentOffset = key._2
            writeMetrics(key._1, value, table)
          }
          writeOffset(currentOffset) // updates the offset positions
        }
        db.close()
      }
    }

Thanks,
Mark




--

---------------------------------------------------------------------


"
rakeshchalasani <vnit.rakesh@gmail.com>,"Thu, 30 Apr 2015 10:22:04 -0700 (MST)",Drop column/s in DataFrame,dev@spark.apache.org,"Hi All:

Is there any plan to add ""drop"" column/s functionality in the data frame?
one or two columns in large dataframe are to be dropped. 

Pandas has this functionality, which I find handy when constructing feature
vectors from DataFrame, allowing me to carry only useful information
forward.
http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.drop.html

Rakesh



--

---------------------------------------------------------------------


"
Cody Koeninger <cody@koeninger.org>,"Thu, 30 Apr 2015 12:29:14 -0500","Re: practical usage of the new ""exactly-once"" supporting DirectKafkaInputDStream",badgerpants <mark.stewart@tapjoy.com>,"What's your schema for the offset table, and what's the definition of
writeOffset ?

What key are you reducing on?  Maybe I'm misreading the code, but it looks
like the per-partition offset is part of the key.  If that's true then you
could just do your reduction on each partition, rather than after the fact
on the whole stream.


"
Reynold Xin <rxin@databricks.com>,"Thu, 30 Apr 2015 10:41:45 -0700",Re: Drop column/s in DataFrame,rakeshchalasani <vnit.rakesh@gmail.com>,"I filed a ticket: https://issues.apache.org/jira/browse/SPARK-7280

Would you like to give it a shot?



"
Rakesh Chalasani <vnit.rakesh@gmail.com>,"Thu, 30 Apr 2015 17:43:28 +0000",Re: Drop column/s in DataFrame,Reynold Xin <rxin@databricks.com>,"Sure, I will try sending a PR soon.


"
badgerpants <mark.stewart@tapjoy.com>,"Thu, 30 Apr 2015 11:38:38 -0700 (MST)","Re: practical usage of the new ""exactly-once"" supporting
 DirectKafkaInputDStream",dev@spark.apache.org,"Cody Koeninger-2 wrote

The schema is the same as the one in your post: topic | partition| offset
The writeOffset is nearly identical:

  def writeOffset(osr: OffsetRange)(implicit session: DBSession): Unit = {
    logWarning(Thread.currentThread().toString + ""writeOffset: "" + osr)
    if(osr==null) {
      logWarning(""no offset provided"")
      return
    }

    val updated = sql""""""
update txn_offsets set off = ${osr.untilOffset}
  where topic = ${osr.topic} and part = ${osr.partition} and off =
${osr.fromOffset}
"""""".update.apply()
    if (updated != 1) {
      throw new Exception( Thread.currentThread().toString + s""""""failed to
write offset:
${osr.topic},${osr.partition},${osr.fromOffset}-${osr.untilOffset}"""""")
    } else {
      logWarning(Thread.currentThread().toString + ""offsets updated to "" +
osr.untilOffset)
    }

  }


Cody Koeninger-2 wrote

Yes, the key is a duple comprised of a case class called Key and the
partition's OffsetRange. We piggybacked the OffsetRange in this way so it
would be available within the scope of the partition.

I have tried moving the reduceByKey from the end of the .transform block
into the partition level (at the end of the mapPartitionsWithIndex block.)
This is what you're suggesting, yes? The results didn't correct the offset
update behavior; they still get out of sync pretty quickly.

Some details: I'm using the kafka-console-producer.sh tool to drive the
process, calling it three or four times in succession and piping in 100-1000
the output of the printOffsets method to stop changing and compare it to the
txn_offsets table. (When no data is getting processed the printOffsets
method yields something like the following: [ OffsetRange(topic:
'testmulti', partition: 1, range: [23602 -> 23602] OffsetRange(topic:
'testmulti', partition: 2, range: [32503 -> 32503] OffsetRange(topic:
'testmulti', partition: 0, range: [26100 -> 26100] OffsetRange(topic:
'testmulti', partition: 3, range: [20900 -> 20900]])

Thanks,
Mark






--

---------------------------------------------------------------------


"
Sandy Ryza <sandy.ryza@cloudera.com>,"Thu, 30 Apr 2015 11:40:12 -0700",Re: Regarding KryoSerialization in Spark,twinkle sachdeva <twinkle.sachdeva@gmail.com>,"Hi Twinkle,

Registering the class makes it so that writeClass only writes out a couple
bytes, instead of a full String of the class name.

-Sandy


"
Cody Koeninger <cody@koeninger.org>,"Thu, 30 Apr 2015 13:58:55 -0500","Re: practical usage of the new ""exactly-once"" supporting DirectKafkaInputDStream",badgerpants <mark.stewart@tapjoy.com>,"This is what I'm suggesting, in pseudocode

rdd.mapPartitionsWithIndex { case (i, iter) =>
   offset = offsets(i)
   result = yourReductionFunction(iter)
   transaction {
      save(result)
      save(offset)
   }
}.foreach { (_: Nothing) => () }

where yourReductionFunction is just normal scala code.

The code you posted looks like you're only saving offsets once per
partition, but you're doing it after reduceByKey.  Reduction steps in spark
imply a shuffle.  After a shuffle you no longer have a guaranteed 1:1
correspondence between spark partiion and kafka partition.  If you want to
verify that's what the problem is, log the value of currentOffset whenever
it changes.




"
Reynold Xin <rxin@databricks.com>,"Thu, 30 Apr 2015 12:02:55 -0700",[discuss] ending support for Java 6?,"""dev@spark.apache.org"" <dev@spark.apache.org>","This has been discussed a few times in the past, but now Oracle has ended
support for Java 6 for over a year, I wonder if we should just drop Java 6
support.

There is one outstanding issue Tom has brought to my attention: PySpark on
YARN doesn't work well with Java 7/8, but we have an outstanding pull
request to fix that.

https://issues.apache.org/jira/browse/SPARK-6869
https://issues.apache.org/jira/browse/SPARK-1920
"
Cody Koeninger <cody@koeninger.org>,"Thu, 30 Apr 2015 14:06:25 -0500","Re: practical usage of the new ""exactly-once"" supporting DirectKafkaInputDStream",badgerpants <mark.stewart@tapjoy.com>,"In fact, you're using the 2 arg form of reduce by key to shrink it down to
1 partition

 reduceByKey(sumFunc, 1)

But you started with 4 kafka partitions?  So they're definitely no longer
1:1



"
Stephen Boesch <javadba@gmail.com>,"Thu, 30 Apr 2015 12:07:12 -0700",Re: Pickling error when attempting to add a method in pyspark,"""dev@spark.apache.org"" <dev@spark.apache.org>","Bumping this.  Anyone of you having some familiarity with py4j interface in
pyspark?

thanks


2015-04-27 22:09 GMT-07:00 Stephen Boesch <javadba@gmail.com>:

I
-expected-zero-arguments-for-construction-of-class>
expected-zero-arguments-for-construction-of-class
k,
ing))
ry"", [""x"",""y"",""weight""])):
t))
 * 100 + d.j * 10 + d.value)}
rdd.collect()))
14)
ction of ClassDict(for pyspark.mllib.clustering.MatrixEntry)
ictConstructor.java:23)
nonfun$apply$1.apply(PythonMLLibAPI.scala:1167)
nonfun$apply$1.apply(PythonMLLibAPI.scala:1166)
la:48)
la:103)
la:47)
3)
ala:265)
la:252)
.scala:1523)
.scala:1523)
12)
or.java:1145)
tor.java:615)
t)
s)
y.pyc
c
'.
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 30 Apr 2015 19:18:53 +0000",Re: [discuss] ending support for Java 6?,"Reynold Xin <rxin@databricks.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","I understand the concern about cutting out users who still use Java 6, and
I don't have numbers about how many people are still using Java 6.

But I want to say at a high level that I support deprecating older versions
of stuff to reduce our maintenance burden and let us use more modern
patterns in our code.

Maintenance always costs way more than initial development over the
lifetime of a project, and for that reason ""anti-support"" is just as
important as support.

sometime later this year, but that's for another thread.)

Nick



"
badgerpants <mark.stewart@tapjoy.com>,"Thu, 30 Apr 2015 12:41:44 -0700 (MST)","Re: practical usage of the new ""exactly-once"" supporting
 DirectKafkaInputDStream",dev@spark.apache.org,"Cody Koeninger-2 wrote

True. I added the second arg because we were seeing multiple threads
attempting to update the same offset. Setting it to 1 prevented that but
doesn't fix the core issue.


Cody Koeninger-2 wrote

I'll give this a try. Thanks, Cody.





--

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 30 Apr 2015 19:48:02 +0000",Re: [discuss] ending support for Java 6?,"Reynold Xin <rxin@databricks.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","sometime later this year, but thatâ€™s for another thread.)

(To continue the parenthetical, Python 2.6 was in fact EOL-ed in October of
2013. <https://www.python.org/download/releases/2.6.9/>)
â€‹

m>

d
d
 6
on
"
Koert Kuipers <koert@tresata.com>,"Thu, 30 Apr 2015 15:53:31 -0400",Re: [discuss] ending support for Java 6?,Nicholas Chammas <nicholas.chammas@gmail.com>,"i am not sure eol means much if it is still actively used. we have a lot of
clients with centos 5 (for which we still support python 2.4 in some form
or another, fun!). most of them are on centos 6, which means python 2.6. by
cutting out python 2.6 you would cut out the majority of the actual
clusters i am aware of. unless you intention is to truly make something
academic i dont think that is wise.


of
:
k
"
Reynold Xin <rxin@databricks.com>,"Thu, 30 Apr 2015 12:57:23 -0700",Re: [discuss] ending support for Java 6?,Koert Kuipers <koert@tresata.com>,"Guys thanks for chiming in, but please focus on Java here. Python is an
entirely separate issue.



al
"
Koert Kuipers <koert@tresata.com>,"Thu, 30 Apr 2015 16:00:55 -0400",Re: [discuss] ending support for Java 6?,Reynold Xin <rxin@databricks.com>,"nicholas started it! :)

for java 6 i would have said the same thing about 1 year ago: it is foolish
to drop it. but i think the time is right about now.
about half our clients are on java 7 and the other half have active plans
to migrate to it within 6 months.


:
ual
r
,
e
k
l
"
Patrick Wendell <pwendell@gmail.com>,"Thu, 30 Apr 2015 13:12:02 -0700",Re: [discuss] ending support for Java 6?,Koert Kuipers <koert@tresata.com>,"I'd also support this. In general, I think it's good that we try to
have Spark support different versions of things (Hadoop, Hive, etc).
But at some point you need to weigh the costs of doing so against the
number of users affected.

In the case of Java 6, we are seeing increasing cost from this. Some
of the newer unsafe code is not supported in Java 6 (and it's a pretty
large internal initiative). And the ability to upgrade dependencies is
starting to cause pain for users. Sean and I had to ""wontfix"" an
important bug fix for users because the library requires JRE 7.


---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Thu, 30 Apr 2015 13:15:48 -0700",Re: [discuss] ending support for Java 6?,Koert Kuipers <koert@tresata.com>,"something to keep in mind:  we can easily support java 6 for the build
environment, particularly if there's a definite EOL.

i'd like to fix our java versioning 'problem', and this could be a big
instigator...  right now we're hackily setting java_home in test invocation
on jenkins, which really isn't the best.  if i decide, within jenkins, to
reconfigure every build to 'do the right thing' WRT java version, then i
will clean up the old mess and pay down on some technical debt.

or i can just install java 6 and we use that as JAVA_HOME on a
build-by-build basis.

this will be a few days of prep and another morning-long downtime if i do
the right thing (within jenkins), and only a couple of hours the hacky way
(system level).

either way, we can test on java 6.  :)


sh
ot
e
on
g
k
s
s
p
"
Sean Owen <sowen@cloudera.com>,"Thu, 30 Apr 2015 21:32:24 +0100",Re: [discuss] ending support for Java 6?,Reynold Xin <rxin@databricks.com>,"I'm firmly in favor of this.

It would also fix https://issues.apache.org/jira/browse/SPARK-7009 and
avoid any more of the long-standing 64K file limit thing that's still
a problem for PySpark.

As a point of reference, CDH5 has never supported Java 6, and it was
released over a year ago.


---------------------------------------------------------------------


"
Punyashloka Biswal <punya.biswal@gmail.com>,"Thu, 30 Apr 2015 20:34:23 +0000",Re: [discuss] ending support for Java 6?,"Koert Kuipers <koert@tresata.com>, shane knapp <sknapp@berkeley.edu>","I'm in favor of ending support for Java 6. We should also articulate a
policy on how long we want to support current and future versions of Java
after Oracle declares them EOL (Java 7 will be in that bucket in a matter
of days).

Punya

on
y
ns
an
va
.
r
he
"
Ted Yu <yuzhihong@gmail.com>,"Thu, 30 Apr 2015 13:37:30 -0700",Re: [discuss] ending support for Java 6?,Punyashloka Biswal <punya.biswal@gmail.com>,"+1 on ending support for Java 6.

BTW from https://www.java.com/en/download/faq/java_7.xml :
After April 2015, Oracle will no longer post updates of Java SE 7 to its
public download sites.


to
i
do
s
 a
he
se
st
g
m
e
:
ng
"
Marcelo Vanzin <vanzin@cloudera.com>,"Thu, 30 Apr 2015 13:40:33 -0700",Re: [discuss] ending support for Java 6?,Sean Owen <sowen@cloudera.com>,"As for the idea, I'm +1. Spark is the only reason I still have jdk6
around - exactly because I don't want to cause the issue that started
this discussion (inadvertently using JDK7 APIs). And as has been
pointed out, even J7 is about to go EOL real soon.

Even Hadoop is moving away (I think 2.7 will be j7-only). Hive 1.1 is
already j7-only. And when Hadoop moves away from something, it's an
event worthy of headlines. They're still on Jetty 6!

As for pyspark, https://github.com/apache/spark/pull/5580 should get
rid of the last incompatibility with large assemblies, by keeping the
python files in separate archives. If we remove support for Java 6,
then we don't need to worry about the size of the assembly anymore.


-- 
Marcelo

---------------------------------------------------------------------


"
Sree V <sree_at_chess@yahoo.com.INVALID>,"Thu, 30 Apr 2015 21:21:16 +0000 (UTC)",Re: [discuss] ending support for Java 6?,"Marcelo Vanzin <vanzin@cloudera.com>, Sean Owen <sowen@cloudera.com>","Hi Team,
Should we take this opportunity to layout and evangelize a pattern for EOL of dependencies.I propose, we follow the official EOL of java, python, scala, .....And add say 6-12-24 months depending on the popularity.
Java 6 official EOL Feb 2013Add 6-12 monthsAug 2013 - Feb 2014 official End of Support for Java 6 in SparkAnnounce 3-6 months prior to EOS.

Thanking you.

With Regards
Sree 


   

 As for the idea, I'm +1. Spark is the only reason I still have jdk6
around - exactly because I don't want to cause the issue that started
this discussion (inadvertently using JDK7 APIs). And as has been
pointed out, even J7 is about to go EOL real soon.

Even Hadoop is moving away (I think 2.7 will be j7-only). Hive 1.1 is
already j7-only. And when Hadoop moves away from something, it's an
event worthy of headlines. They're still on Jetty 6!

As for pyspark, https://github.com/apache/spark/pull/5580 should get
rid of the last incompatibility with large assemblies, by keeping the
python files in separate archives. If we remove support for Java 6,
then we don't need to worry about the size of the assembly anymore.


-- 
Marcelo

---------------------------------------------------------------------



   "
Sree V <sree_at_chess@yahoo.com.INVALID>,"Thu, 30 Apr 2015 21:26:10 +0000 (UTC)",Re: [discuss] ending support for Java 6?,"Sree V <sree_at_chess@yahoo.com>, Marcelo Vanzin <vanzin@cloudera.com>, 
	Sean Owen <sowen@cloudera.com>","If there is any possibility of getting the download counts,then we can use it as EOS criteria as well.Say, if download counts are lower than 30% (or another number) of Life time highest,then it qualifies for EOS.

Thanking you.

With Regards
Sree 


   

 Hi Team,
Should we take this opportunity to layout and evangelize a pattern for EOL of dependencies.I propose, we follow the official EOL of java, python, scala, .....And add say 6-12-24 months depending on the popularity.
Java 6 official EOL Feb 2013Add 6-12 monthsAug 2013 - Feb 2014 official End of Support for Java 6 in SparkAnnounce 3-6 months prior to EOS.

Thanking you.

With Regards
Sree 


Â  

 As for the idea, I'm +1. Spark is the only reason I still have jdk6
around - exactly because I don't want to cause the issue that started
this discussion (inadvertently using JDK7 APIs). And as has been
pointed out, even J7 is about to go EOL real soon.

Even Hadoop is moving away (I think 2.7 will be j7-only). Hive 1.1 is
already j7-only. And when Hadoop moves away from something, it's an
event worthy of headlines. They're still on Jetty 6!

As for pyspark, https://github.com/apache/spark/pull/5580 should get
rid of the last incompatibility with large assemblies, by keeping the
python files in separate archives. If we remove support for Java 6,
then we don't need to worry about the size of the assembly anymore.


-- 
Marcelo

---------------------------------------------------------------------



Â  

  "
Ted Yu <yuzhihong@gmail.com>,"Thu, 30 Apr 2015 14:34:45 -0700",Re: [discuss] ending support for Java 6?,Sree V <sree_at_chess@yahoo.com>,"But it is hard to know how long customers stay with their most recent
download.

Cheers


"
Marcelo Vanzin <vanzin@cloudera.com>,"Thu, 30 Apr 2015 14:36:20 -0700",Uninitialized session in HiveContext?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey all,

We ran into some test failures in our internal branch (which builds
against Hive 1.1), and I narrowed it down to the fix below. I'm not
super familiar with the Hive integration code, but does this look like
a bug for other versions of Hive too?

This caused an error where some internal Hive configuration that is
initialized by the session were not available.

diff --git a/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveContext.scala
b/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveContext.scala
index dd06b26..6242745 100644
--- a/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveContext.scala
+++ b/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveContext.scala
@@ -93,6 +93,10 @@ class HiveContext(sc: SparkContext) extends SQLContext(sc) {
     if (conf.dialect == ""sql"") {
       super.sql(substituted)
     } else if (conf.dialect == ""hiveql"") {
+      // Make sure Hive session state is initialized.
+      if (SessionState.get() != sessionState) {
+        SessionState.start(sessionState)
+      }
       val ddlPlan = ddlParserWithHiveQL.parse(sqlText,
       DataFrame(this, ddlPlan.getOrElse(HiveQl.parseSql(substituted)))
     }  else {



-- 
Marcelo

---------------------------------------------------------------------


"
Vinod Kumar Vavilapalli <vinodkv@hortonworks.com>,"Thu, 30 Apr 2015 22:07:21 +0000",Re: [discuss] ending support for Java 6?,Reynold Xin <rxin@databricks.com>,"FYI, after enough consideration, we the Hadoop community dropped support for JDK 6 starting release Apache Hadoop 2.7.x.

Thanks
+Vinod


6
n


---------------------------------------------------------------------


"
Ram Sriharsha <harshars@yahoo-inc.com.INVALID>,"Thu, 30 Apr 2015 22:13:36 +0000 (UTC)",Re: [discuss] ending support for Java 6?,"Vinod Kumar Vavilapalli <vinodkv@hortonworks.com>,
        Reynold Xin <rxin@databricks.com>","+1 for end of support for Java 6 


   

 FYI, after enough consideration, we the Hadoop community dropped support for JDK 6 starting release Apache Hadoop 2.7.x.

Thanks
+Vinod




---------------------------------------------------------------------


 "
Yang Lei <genially@gmail.com>,"Thu, 30 Apr 2015 18:26:13 -0400","Re: Issue of running partitioned loading (RDD) in Spark External
 Datasource on Mesos",Reynold Xin <rxin@databricks.com>,"I finally isolated the issue to be related to the ActorSystem I reuse from
SparkEnv.get.actorSystem. This ActorSystem will contain the configuration
defined in my application jar's reference.conf in both local cluster case,
and in the case I use it directly in an extension to BaseRelation's buildScan
method. However if used in my RDD which is returned in the buildScan, it
loses the configuration.

I solve / bypass the problem by checking if my configuration exists in the
SparkEnv.get.actorSystem(settings.config) .If it does not exist, I will
create a new ActorSystem using my class's classLoader to force config
reading from my application jar:

            val classLoader = this.getClass.getClassLoader

            val myconfig = ConfigFactory.load(classLoader)// force config
reading from my classloader

            ActorSystem(""somename.."",myconfig,classLoader)


I wonder if this different behavior of SparkEnv.get.actorSystem is
working-as-designed, or something is missing in executor setup for this
custom RDD driven execution case.


Thanks.


Yang
"
