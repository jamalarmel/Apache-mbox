Koert Kuipers <koert@tresata.com>,"Thu, 1 Dec 2016 00:02:12 -0500",Re: [VOTE] Apache Spark 2.1.0 (RC1),Reynold Xin <rxin@databricks.com>,"after seeing Hyukjin Kwon's comment in SPARK-17583 i think its safe to say
that what i am seeing with csv is not bug or regression. it was unintended
and/or unreliable behavior in spark 2.0.x


"
"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Wed, 30 Nov 2016 23:56:35 -0700 (MST)","RE: [SPARK-17845] [SQL][PYTHON] More self-evident window function
 frame boundary API",dev@spark.apache.org,"I may be mistaken but if I remember correctly spark behaves differently when it is bounded in the past and when it is not. Specifically I seem to recall a fix which made sure that when there is no lower bound then the aggregation is done one by one instead of doing the whole range for each window. So I believe it should be configured exactly the same as in scala/java so the optimization would take place.
Assaf.

From: rxin [via Apache Spark Developers List] [mailto:ml-node+s1001551n20069h28@n3.nabble.com]
Sent: Wednesday, November 30, 2016 8:35 PM
To: Mendelson, Assaf
Subject: Re: [SPARK-17845] [SQL][PYTHON] More self-evident window function frame boundary API

Yes I'd define unboundedPreceding to -sys.maxsize, but also any value less than min(-sys.maxsize, _JAVA_MIN_LONG) are considered unboundedPreceding too. We need to be careful with long overflow when transferring data over to Java.



It is platform specific so theoretically can be larger, but 2**63 - 1 is a standard on 64 bit platform and 2**31 - 1 on 32bit platform. I can submit a patch but I am not sure how to proceed. Personally I would set

unboundedPreceding = -sys.maxsize

unboundedFollowing = sys.maxsize

to keep backwards compatibility.
Ah ok for some reason when I did the pull request sys.maxsize was much larger than 2^63. Do you want to submit a patch to fix this?



The problem is that -(1 << 63) is -(sys.maxsize + 1) so the code which used to work before is off by one.
Can you give a repro? Anything less than -(1 << 63) is considered negative infinity (i.e. unbounded preceding).

Hi,

I've been looking at the SPARK-17845 and I am curious if there is any
reason to make it a breaking change. In Spark 2.0 and below we could use:

    Window().partitionBy(""foo"").orderBy(""bar"").rowsBetween(-sys.maxsize,
sys.maxsize))

In 2.1.0 this code will silently produce incorrect results (ROWS BETWEEN
-1 PRECEDING AND UNBOUNDED FOLLOWING) Couldn't we use
Window.unboundedPreceding equal -sys.maxsize to ensure backward
compatibility?

--

Maciej Szymkiewicz


---------------------------------------------------------------------



--

Maciej Szymkiewicz



--

Maciej Szymkiewicz


________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/SPARK-17845-SQL-PYTHON-More-self-evident-window-function-frame-boundary-API-tp20064p20069.html
To start a new topic under Apache Spark Developers List, email ml-node+s1001551n1h20@n3.nabble.com<mailto:ml-node+s1001551n1h20@n3.nabble.com>
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>




--"
Robert Kruszewski <robertk@palantir.com>,"Thu, 1 Dec 2016 13:51:02 +0000",Re: [VOTE] Apache Spark 2.1.0 (RC1),"Reynold Xin <rxin@databricks.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","-1 since https://issues.apache.org/jira/browse/SPARK-17213 is a correctness regression from 2.0 release. The commit that caused it is 776d183c82b424ef7c3cae30537d8afe9b9eee83. 

 

Robert

 

From: Reynold Xin <rxin@databricks.com>
Date: Tuesday, November"
Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Thu, 1 Dec 2016 18:38:35 +0100","Re: [SPARK-17845] [SQL][PYTHON] More self-evident window function
 frame boundary API",Reynold Xin <rxin@databricks.com>,"It could be something like this
https://github.com/zero323/spark/commit/b1f4d8218629b56b0982ee58f5b93a40305985e0 
but I am not fully satisfied.


-- 
Maciej Szymkiewicz

"
Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Thu, 1 Dec 2016 18:39:25 +0100","Re: [SPARK-17845] [SQL][PYTHON] More self-evident window function
 frame boundary API",dev@spark.apache.org,"This doesn't affect that. The only concern is what we consider to
UNBOUNDED on Python side.



-- 
Maciej Szymkiewicz

"
Reynold Xin <rxin@databricks.com>,"Thu, 1 Dec 2016 15:51:01 -0800","Re: [SPARK-17845] [SQL][PYTHON] More self-evident window
 function frame boundary API","dev@spark.apache.org, Maciej Szymkiewicz <mszymkiewicz@gmail.com>","Can you submit a pull request with test cases based on that change?


NDED on Python side.
ly when it is bounded in the past and when it is not. Specifically I seem to recall a fix which made sure that when there is no lower bound then the aggregation is done one by one instead of doing the whole range for each window. So I believe it should be configured exactly the same as in scala/java so the optimization would take place.
+[hidden email]]
t window function frame boundary API
 less than min(-sys.maxsize,¬†_JAVA_MIN_LONG) are considered unboundedPreceding too. We need to be careful with long overflow when transferring data over to Java.
 is a standard on 64 bit platform and 2**31 - 1 on 32bit platform. I can submit a patch but I am not sure how to proceed. Personally I would set
uch larger than 2^63. Do you want to submit a patch to fix this?
ich used to work before is off by one.
ed negative infinity (i.e. unbounded preceding).
 any
uld use:
rowsBetween(-sys.maxsize,
BETWEEN
----
ion below:
-SQL-PYTHON-More-self-evident-window-function-frame-boundary-API-tp20064p20069.html
den email]
N] More self-evident window function frame boundary API
bble.com.
"
Alex153 <alexander153490@gmail.com>,"Wed, 30 Nov 2016 13:30:16 -0700 (MST)",Hidden Markov Model or Bayes Networks in Spark - MS Thesis theme,dev@spark.apache.org,"As part of my MS Thesis (in computer science) project I am looking for chance
to implement some machine learning or data mining algorithms. Are there good
ideas for this - are there some unrealised algorithms that can be great
contribution to the project?

I am thinking about Hidden Markov Models and/or Bayes Networs - can here be
interest in them? I can consider ideas for other projects or algorithms as
well.

A,



--

---------------------------------------------------------------------


"
Chan Chor Pang <chin-sh@indetail.co.jp>,"Fri, 2 Dec 2016 11:19:02 +0900",Re: REST api for monitoring Spark Streaming,dev@spark.apache.org,"hi everyone

I have done the coding and create the PR
the implementation is straightforward and similar to the api in spark-core
but we still need someone with streaming background to verify the patch
just to make sure everything is OK

so, please anyone can help?
https://github.com/apache/spark/pull/16000



-- 
---*------------------------------------------------*---*---*---*---
Ê†™Âºè‰ºöÁ§æINDETAIL
„Éã„Ç¢„Ç∑„Éß„Ç¢Á∑èÂêà„Çµ„Éº„Éì„Çπ‰∫ãÊ•≠Êú¨ÈÉ®
„Ç≤„Éº„É†„Çµ„Éº„Éì„Çπ‰∫ãÊ•≠ÈÉ®
Èô≥„ÄÄÊ•öÈµ¨
E-mail :chin-sh@indetail.co.jp
URL : http://www.indetail.co.jp

„ÄêÊú≠ÂπåÊú¨Á§æÔºèLABOÔºèLABO2„Äë
„Äí060-0042
Êú≠ÂπåÂ∏Ç‰∏≠Â§ÆÂå∫Â§ßÈÄöË•ø9‰∏ÅÁõÆ3Áï™Âú∞33
„Ç≠„Çø„Ç≥„Éº„Çª„É≥„Çø„Éº„Éì„É´„Éá„Ç£„É≥„Ç∞
ÔºàÊú≠ÂπåÊú¨Á§æÔºèLABO2Ôºö2Èöé„ÄÅLABOÔºö9ÈöéÔºâ
TELÔºö011-206-9235 FAXÔºö011-206-9236

„ÄêÊù±‰∫¨ÊîØÂ∫ó„Äë
„Äí108-0014
Êù±‰∫¨ÈÉΩÊ∏ØÂå∫Ëäù5‰∏ÅÁõÆ29Áï™20Âè∑ „ÇØ„É≠„Çπ„Ç™„Éï„Ç£„Çπ‰∏âÁî∞
TELÔºö03-6809-6502 FAXÔºö03-6809-6504

„ÄêÂêçÂè§Â±ã„Çµ„ÉÜ„É©„Ç§„Éà„Äë
„Äí460-0002
ÊÑõÁü•ÁúåÂêçÂè§Â±ãÂ∏Ç‰∏≠Âå∫‰∏∏„ÅÆÂÜÖ3‰∏ÅÁõÆ17Áï™24Âè∑ NAYUTA BLD
TELÔºö052-971-0086

"
Steve Loughran <stevel@hortonworks.com>,"Fri, 2 Dec 2016 11:57:24 +0000",getting PRs into the spark hive dependency,Apache Spark Dev <dev@spark.apache.org>,"
What's the process for PR review for the Hive JAR?

I ask as I've had a PR for fixing a kerberos problem outstanding for a while, without much response

https://github.com/pwendell/hive/pull/2

I'm now looking at the one line it would take for the JAR to consider Hadoop 3.x compatible at the API level with Hives 2.x, so that Dataframes work against Hadoop 3.x; without that ASF Spark is not going to work on Hadoop 3.

Where should I be submitting those PRs, and what'st the review process?

thx

-steve
"
Marcelo Vanzin <vanzin@cloudera.com>,"Fri, 2 Dec 2016 10:03:37 -0800",Re: getting PRs into the spark hive dependency,Steve Loughran <stevel@hortonworks.com>,"I believe the latest one is actually in Josh's repository. Which kinda
raises a more interesting question:

Should we create a repository managed by the Spark project, using the
Apache infrastructure, to handle that fork? It seems not very optimal
to have this lie in some random person's github account. Given how
often it changes, it shouldn't really be too much overhead to maintain
it.





-- 
Marcelo

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Fri, 2 Dec 2016 11:09:04 -0800",Re: getting PRs into the spark hive dependency,Marcelo Vanzin <vanzin@cloudera.com>,"ThriftHttpCLIService.java code is actually in Spark. That pull request is
basically no-op. Overall we are moving away from Hive dependency by
implementing almost everything in Spark, so the need to change that repo is
getting less and less.




"
Fritz Budiyanto <fbudiyan@icloud.com>,"Fri, 02 Dec 2016 11:20:40 -0800",Flink event session window in Spark,dev@spark.apache.org,"Hi All,

I need help on how to implement Flink event session window in Spark. Is this possible?

For instance, I wanted to create a session window with a timeout of 10 minutes (see Flink snippet below)
Continues event will make the session window alive. If there are no activity for 10 minutes, the session window shall close and forward the data to a sink function.

// event-time session windows
input                                                                           
    .keyBy(<key selector>)                                                      
    .window(EventTimeSessionWindows.withGap(Time.minutes(10)))                  
    .<windowed transformation>(<window function>);                              
                                                                                
Any idea ?

Thanks,
Fritz"
Michael Armbrust <michael@databricks.com>,"Fri, 2 Dec 2016 12:13:15 -0800",Re: Flink event session window in Spark,Fritz Budiyanto <fbudiyan@icloud.com>,"Here is the JIRA for adding this feature:
https://issues.apache.org/jira/browse/SPARK-10816


"
Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Fri, 2 Dec 2016 21:23:29 +0100","Re: [SPARK-17845] [SQL][PYTHON] More self-evident window function
 frame boundary API","Reynold Xin <rxin@databricks.com>, dev@spark.apache.org","Sure, here you are: https://issues.apache.org/jira/browse/SPARK-18690

To be fair I am not fully convinced it is worth it.



-- 
Maciej Szymkiewicz

"
"""Hegner, Travis"" <THegner@trilliumit.com>","Fri, 2 Dec 2016 20:27:31 +0000","SPARK-18689: A proposal for priority based app scheduling utilizing
 linux cgroups.","""dev@spark.apache.org"" <dev@spark.apache.org>","Hello,


I've just created a JIRA to open up discussion of a new feature that I'd like to propose.


https://issues.apache.org/jira/browse/SPARK-18689


I'd love to get some feedback on the idea. I know that normally anything related to scheduling or queuing automatically throws up the ""hard to implement"" red flags, but the proposal contains a rather simple way to implement the concept, which delegates the scheduling logic to the actual kernel of each worker, rather than in any spark core code. I believe this to be more flexible and simpler to set up and maintain than dynamic allocation, and avoids the need for any preemption type of logic.


The proposal does not contain any code. I am not (yet) familiar enough with the core spark code to confidently create an implementation.


I appreciate your time and am looking forward to your feedback!


Thanks,


Travis
"
Miguel Morales <therevoltingx@gmail.com>,"Fri, 2 Dec 2016 12:45:18 -0800",Re: Flink event session window in Spark,Michael Armbrust <michael@databricks.com>,"Although this may not be natively supported, you can mimic this behavior.
By having a micro batch time of 1 minute.  Then on your
updateStateByKey check how long the session has been running.  If it's
longer than 10 minutes, return an empty key so that it's removed from
the stream.


---------------------------------------------------------------------


"
Jim Hughes <jnh5y@ccri.com>,"Fri, 2 Dec 2016 16:14:58 -0500",Issues using Hive JDBC,Apache Spark Dev <dev@spark.apache.org>,"Hi all,

I'm investigating adding geospatial user-defined functions and types to 
Spark SQL in Spark 2.0.x.  That is going rather well; I've seen how to 
add geospatial UDT and UDFs (and even UDAFs!).

As part of the investigation, I tried out the Thrift JDBC server, and I 
have encountered two general issues.

First, there are a few places in the Hive Thrift Server module which 
aren't general.  In two places, a SessionState is cast to a 
HiveSessionState.  Additionally, in SparkExecuteStatementOperation, some 
serialization code for handling DataTypes doesn't consider 
UserDefinedTypes.

For those issues, would a JIRA and pull request be appropriate?

My second issue was connecting existing JDBC code to the Hive JDBC 
connector.  The code calls connection.getMetaData.getTables().  I tried 
various options to see that call work, and I was unable to get the list 
of tables back.

It looks like these tickets are related: 
https://issues.apache.org/jira/browse/SPARK-9686 
https://issues.apache.org/jira/browse/SPARK-9333

Has there been any more work on the JDBC metadata issues?

Thanks in advance,

Jim


---------------------------------------------------------------------


"
Erik LaBianca <erik.labianca@gmail.com>,"Fri, 2 Dec 2016 19:03:48 -0500",ability to provide custom serializers,dev@spark.apache.org,"Hi All,

Apologies in advance for any confusing terminology, I‚Äôm still pretty new to Spark.

I‚Äôve got a bunch of Scala case class ‚Äúdomain objects‚Äù from an existing application. Many of them contain simple, but unsupported-by-spark types in them, such as case class Foo(timestamp: java.time.Instant). I‚Äôd like to be able to use these case classes directly in a DataSet, but can‚Äôt, since there‚Äôs no encoder available for java.time.Instant. I‚Äôd like to resolve that.

I asked around on the gitter channel, and was pointed to the ScalaReflections class, which handles creating Encoder[T] for a variety of things, including case classes and their members. Barring a better solution, what I‚Äôd like is to be able to add some additional case statements to the serializerFor and deserializeFor methods, dispatching to something along the lines of the Slick MappedColumnType[1]. In an ideal scenario, I could provide these mappings via implicit search, but I‚Äôd be happy to settle for a registry of some sort too.

Does this idea make sense, in general? I‚Äôm interested in taking a stab at the implementation, but Jakob recommended I surface it here first to see if there were any plans around this sort of functionality already.

Thanks!

‚Äîerik

1. http://slick.lightbend.com/doc/3.0.0/userdefined.html#using-custom-scalar-types-in-queries


---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Fri, 2 Dec 2016 16:29:51 -0800",Re: ability to provide custom serializers,Erik LaBianca <erik.labianca@gmail.com>,"I would love to see something like this.  The closest related ticket is
probably https://issues.apache.org/jira/browse/SPARK-7768 (though maybe
there are enough people using UDTs in their current form that we should
just make a new ticket)

A few thoughts:
 - even if you can do implicit search, we probably also want a registry for
Java users.
 - what is the output of the serializer going to be? one challenge here is
that encoders write directly into the tungsten format, which is not a
stable public API. Maybe this is more obvious if I understood MappedColumnType
better?

Either way, I'm happy to give further advice if you come up with a more
concrete proposal and put it on JIRA.


tty new
Äù from an existing
in
ike to be
since
ike to resolve
f
o
ôd be
 stab at
if
"
Shuai Lin <linshuai2012@gmail.com>,"Sat, 3 Dec 2016 19:52:34 +0800","Re: SPARK-18689: A proposal for priority based app scheduling
 utilizing linux cgroups.","""Hegner, Travis"" <THegner@trilliumit.com>","Sorry but I don't get the scope of the problem from your description. Seems
it's an improvement for spark standalone scheduler (i.e. not for yarn or
mesos)?


"
Steve Loughran <stevel@hortonworks.com>,"Sat, 3 Dec 2016 12:02:57 +0000",Re: getting PRs into the spark hive dependency,Reynold Xin <rxin@databricks.com>,"

ThriftHttpCLIService.java code is actually in Spark. That pull request is basically no-op. Overall we are moving away from Hive dependency by implementing almost everything in Spark, so the need to change that repo is getting less and less.



that's good, but currently there is still a hive dependency, and as it's a fork of hive, no easy way to get anything needed for Hadoop 3.x in.

I suspect Imran Rashid owns that problem...


I believe the latest one is actually in Josh's repository. Which kinda
raises a more interesting question:

Should we create a repository managed by the Spark project, using the
Apache infrastructure, to handle that fork? It seems not very optimal
to have this lie in some random person's github account. Given how
often it changes, it shouldn't really be too much overhead to maintain
it.


oop
 3.



--
Marcelo

---------------------------------------------------------------------
ibe@spark.apache.org>



"
Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Sun, 4 Dec 2016 16:59:41 +0100",Future of the Python 2 support.,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

I am aware there was a previous discussion about dropping support for
different platforms
(http://apache-spark-developers-list.1001551.n3.nabble.com/Straw-poll-dropping-support-for-things-like-Scala-2-10-td19553.html)
but somehow it has been dominated by Scala and JVM and never touched the
subject of Python 2.

Some facts:

  * Python 2 End Of Life is scheduled for 2020
    (http://legacy.python.org/dev/peps/pep-0373/) without with ""no
    guarantee that bugfix releases will be made on a regular basis""
    until then.
  * Almost all commonly used libraries already support Python 3
    (https://python3wos.appspot.com/). A single exception that can be
    important for Spark is thrift (Python 3 support is already present
    on the master) and transitively PyHive and Blaze.
  * Supporting both Python 2 and Python 3 introduces significant
    technical debt. In practice Python 3 is a different language with
    backward incompatible syntax and growing number of features which
    won't be backported to 2.x.

Suggestions:

  * We need a public discussion about possible date for dropping Python
    2 support.
  * Early 2018 should give enough time for a graceful transition.

-- 
Best,
Maciej

"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sun, 04 Dec 2016 17:29:27 +0000",Re: Future of the Python 2 support.,"Maciej Szymkiewicz <mszymkiewicz@gmail.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","I don't think it makes sense to deprecate or drop support for Python 2.7
until at least 2020, when 2.7 itself will be EOLed. (As of Spark 2.0,
Python 2.6 support is deprecated and will be removed by Spark 2.2. Python
2.7 is only version of Python 2 that's still fully supported.)

Given the widespread industry use of Python 2.7, and the fact that it is
supported upstream by the Python core developers until 2020, I don't see
why Spark should even consider dropping support for it before then. There
is, of course, additional ongoing work to support Python 2.7, but it seems
more than justified by its level of use and popularity in the broader
community. And I say that as someone who almost exclusively develops in
Python 3.5+ these days.

Perhaps by 2018 the industry usage of Python 2 will drop precipitously and
merit a discussion about dropping support, but I think at this point it's
premature to discuss that and we should just wait and see.

Nick



"
Reynold Xin <rxin@databricks.com>,"Sun, 04 Dec 2016 18:28:24 +0000",Re: Future of the Python 2 support.,"Maciej Szymkiewicz <mszymkiewicz@gmail.com>, Nicholas Chammas <nicholas.chammas@gmail.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","Echoing Nick. I don't see any strong reason to drop Python 2 support.

We typically drop support for X when it is rarely used and support for X is
long past EOL. Python 2 is still very popular, and depending on the
statistics it might be more popular than Python 3.


"
Erik LaBianca <erik.labianca@gmail.com>,"Sun, 4 Dec 2016 20:39:50 -0500",Re: ability to provide custom serializers,Michael Armbrust <michael@databricks.com>,"Thanks Michael!

is probably https://issues.apache.org/jira/browse/SPARK-7768 <https://issues.apache.org/jira/browse/SPARK-7768> (though maybe there are enough people using UDTs in their current form that we should just make a new ticket)

I‚Äôm not very familiar with UDT‚Äôs. Is this something I should research or just leave it be and create a new ticket? I did notice the presence of a registry in the source code but it seemed like it was targeted at a different use case.

registry for Java users.

That‚Äôs fine. I‚Äôm not 100% sure I can get the right implicit in scope as things stand anyway, so let‚Äôs table that idea for now and do the registry.

here is that encoders write directly into the tungsten format, which is not a stable public API. Maybe this is more obvious if I understood MappedColumnType better?

My assumption was that the output would be existing scalar data types. So string, long, double, etc. What I‚Äôd like to do is just ‚Äúlayer‚Äù the new ones on top already existing ones, kinda like the case case encoder does.

more concrete proposal and put it on JIRA.

Great, let me know and I‚Äôll create a ticket, or we can re-use SPARK-7768 and we can move the discussion there.

Thanks!

‚Äîerik

"
Koert Kuipers <koert@tresata.com>,"Sun, 4 Dec 2016 23:16:57 -0500",Re: [VOTE] Apache Spark 2.1.0 (RC1),Reynold Xin <rxin@databricks.com>,"somewhere between rc1 and the current head of branch-2.1 i started seeing
an NPE in our in-house unit tests for Dataset + Aggregator. i created
SPARK-18711 <https://issues.apache.org/jira/browse/SPARK-18711> for this.
<https://issues.apache.org/jira/browse/SPARK-18711>


"
Koert Kuipers <koert@tresata.com>,"Sun, 4 Dec 2016 23:23:37 -0500",Re: [VOTE] Apache Spark 2.1.0 (RC1),Reynold Xin <rxin@databricks.com>,"with the current branch-2.1 after rc1 i am now also seeing this error in
our unit tests:

 java.lang.UnsupportedOperationException: Cannot create encoder for Option
of Product type, because Product type is represented as a row, and the
entire row can not be null in Spark SQL like normal databases. You can wrap
your type with Tuple1 if you do want top level null Product objects, e.g.
instead of creating `Dataset[Option[MyClass]]`, you can do something like
`val ds: Dataset[Tuple1[MyClass]] = Seq(Tuple1(MyClass(...)),
Tuple1(null)).toDS`

the issue is that we have Aggregator[String, Option[SomeCaseClass], String]
and it doesn't like creating the Encoder for that Option[SameCaseClass]
anymore.

this is related to SPARK-18251
<https://issues.apache.org/jira/browse/SPARK-18251>
we have a workaround for this: we will wrap all buffer encoder types in
Tuple1. a little inefficient but its okay with me.


"
tenglong <longteng.cq@gmail.com>,"Sun, 4 Dec 2016 22:47:22 -0700 (MST)",Can I add a new method to RDD class?,dev@spark.apache.org,"Hi,

Apparently, I've already tried adding a new method to RDD,

for example,

class RDD {
  def foo() // this is the one I added

  def map()

  def collect()
}

I can build Spark successfully, but I can't compile my application code
which calls rdd.foo(), and the error message says

value foo is not a member of org.apache.spark.rdd.RDD[String]

So I am wondering if there is any mechanism prevents me from doing this or
something I'm doing wrong?




--

---------------------------------------------------------------------


"
trsell@gmail.com,"Mon, 05 Dec 2016 05:59:29 +0000",Re: Can I add a new method to RDD class?,"tenglong <longteng.cq@gmail.com>, dev@spark.apache.org","How does your application fetch the spark dependency? Perhaps list your
project dependencies and check it's using your dev build.


"
Tarun Kumar <tarunk1407@gmail.com>,"Mon, 05 Dec 2016 06:07:19 +0000",Re: Can I add a new method to RDD class?,"dev@spark.apache.org, tenglong <longteng.cq@gmail.com>, trsell@gmail.com","Hi Tenglong,

In addition to trsell's reply, you can add any method to an rdd without
making changes to spark code.

This can be achieved by using implicit class in your own client code:

implicit class extendRDD[T](rdd: RDD[T]){

def foo()

}

Then you basically nees to import this implicit class in scope where you
want to use the new foo method.

Thanks
Tarun Kumar


"
long <longteng.cq@gmail.com>,"Sun, 4 Dec 2016 23:12:10 -0700 (MST)",Re: Can I add a new method to RDD class?,dev@spark.apache.org,"So im my sbt build script, I have the same line as instructed in the 
quickstart guide <http://spark.apache.org/docs/latest/quick-start.html>  ,

libraryDependencies += ""org.apache.spark"" %% ""spark-core"" % ""2.0.2""

And since I was able to see all the other logs I added into the spark source
code, so I'm pretty sure the application is using the one I just built.

Thanks!



--

---------------------------------------------------------------------


"
long <longteng.cq@gmail.com>,"Sun, 4 Dec 2016 23:15:06 -0700 (MST)",Re: Can I add a new method to RDD class?,dev@spark.apache.org,"So is there documentation of this I can refer to? 





--"
Tarun Kumar <tarunk1407@gmail.com>,"Mon, 05 Dec 2016 06:43:33 +0000",Re: Can I add a new method to RDD class?,"dev@spark.apache.org, long <longteng.cq@gmail.com>","Not sure if that's documented in terms of Spark but this is a fairly common
pattern in scala known as ""pimp my library"" pattern, you can easily find
many generic example of using this pattern.

If you want I can quickly cook up a short conplete example with
rdd(although there is nothing really more to my example in earlier mail) ?

Thanks
Tarun Kumar


]
0>>
=0""
=1""
r
-method-to-RDD-class-tp20100.html
=2""
il]
-method-to-RDD-class-tp20100p20102.html
ervlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
w-method-to-RDD-class-tp20100p20104.html>
"
=?UTF-8?B?TWljaGFsIMWgZW5rw73FmQ==?= <binarek@gmail.com>,"Mon, 5 Dec 2016 09:15:14 +0100",Re: Can I add a new method to RDD class?,dev@spark.apache.org,"A simple Scala example of implicit classes:

implicit  class  EnhancedString(str:String) {
   def  prefix(prefix:String)=  prefix+  str
}

println(""World"".prefix(""Hello ""))

As Tarun said, you have to import it if it's not in the same class where 
you use it.

Hope this makes it clearer,

Michal Senkyr



"
long <longteng.cq@gmail.com>,"Mon, 5 Dec 2016 03:06:04 -0700 (MST)",Re: Can I add a new method to RDD class?,dev@spark.apache.org,"Thank you very much! But why can‚Äôt I just add new methods in to the source code of RDD?

you use it.
mon pattern in scala known as ""pimp my library"" pattern, you can easily find many generic example of using this pattern.
ough there is nothing really more to my example in earlier mail) ?
st] <[hidden email] <http://user/SendEmail.jtp?type=node&node=20104&i making changes to spark code.
u want to use the new foo method.
p?type=node&amp;node=20102&amp;i=0"" target=""_top"" rel=""nofollow""  project dependencies and check it's using your dev build.
il.jtp?type=node&amp;node=20102&amp;i=1"" target=""_top"" rel=""nofol or
51.n3.nabble.com/Can-I-add-a-new-method-to-RDD-class-tp20100.html <http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-add-a-new-method-to-RDD-class-tp20100.html>
le.com <http://nabble.com/>.
node&amp;node=20102&amp;i=2"" target=""_top"" rel=""nofollow"" link=""external"" class="""">[hidden email]
n below:
ew-method-to-RDD-class-tp20100p20102.html <http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-add-a-new-method-to-RDD-class-tp20100p20102.html>
.
e/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-add-a-new-method-to-RDD-class-tp20100p20104.html>
apache-spark-developers-list.1001551.n3.nabble.com/> at Nabble.com.
below:
-method-to-RDD-class-tp20100p20106.html <http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-add-a-new-method-to-RDD-class-tp20100p20106.html>
p://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=20100&code=bG9uZ3RlbmcuY3FAZ21haWwuY29tfDIwMTAwfC0xNzQ1MzUzNzE=>.
NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>




--
3.nabble.com/Can-I-add-a-new-method-to-RDD-class-tp20100p20107.html
om."
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 05 Dec 2016 14:11:56 +0000",java.lang.IllegalStateException: There is no space for new record,Spark dev list <dev@spark.apache.org>,"I was testing out a new project at scale on Spark 2.0.2 running on YARN,
and my job failed with an interesting error message:

TaskSetManager: Lost task 37.3 in stage 31.0 (TID 10684,
server.host.name): java.lang.IllegalStateException: There is no space
for new record
05:27:09.573     at
org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter.insertRecord(UnsafeInMemorySorter.java:211)
05:27:09.574     at
org.apache.spark.sql.execution.UnsafeKVExternalSorter.<init>(UnsafeKVExternalSorter.java:127)
05:27:09.574     at
org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap.destructAndCreateExternalSorter(UnsafeFixedWidthAggregationMap.java:244)
05:27:09.575     at
org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithKeys$(Unknown
Source)
05:27:09.575     at
org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown
Source)
05:27:09.576     at
org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
05:27:09.576     at
org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)
05:27:09.577     at
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
05:27:09.577     at
org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
05:27:09.577     at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
05:27:09.578     at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
05:27:09.578     at org.apache.spark.scheduler.Task.run(Task.scala:86)
05:27:09.578     at
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
05:27:09.579     at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
05:27:09.579     at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
05:27:09.579     at java.lang.Thread.run(Thread.java:745)

I‚Äôve never seen this before, and searching on Google/DDG/JIRA doesn‚Äôt yield
any results. There are no other errors coming from that executor, whether
related to memory, storage space, or otherwise.

Could this be a bug? If so, how would I narrow down the source? Otherwise,
how might I work around the issue?

Nick
‚Äã
"
Holden Karau <holden@pigscanfly.ca>,"Mon, 05 Dec 2016 14:19:25 +0000",Re: Can I add a new method to RDD class?,"dev@spark.apache.org, long <longteng.cq@gmail.com>","Doing that requires publishing a custom version of Spark, you can edit the
version number do do a publishLocal - but maintaining that change is going
to be difficult. The other approaches suggested are probably better, but
also does your method need to be defined on the RDD class? Could you
instead make a helper object or class to expose whatever functionality you
need?


he source
k Developers
y
y
y
=0""
]
s
p;amp;i=0""
;i=0""
p;amp;i=1""
;i=1""
r
-method-to-RDD-class-tp20100.html
p;amp;i=2""
;i=2""
il]
-method-to-RDD-class-tp20100p20102.html
ervlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
w-method-to-RDD-class-tp20100p20104.html>
m
-method-to-RDD-class-tp20100p20106.html
ervlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
w-method-to-RDD-class-tp20100p20107.html>
"
"""Hegner, Travis"" <THegner@trilliumit.com>","Mon, 5 Dec 2016 15:36:58 +0000","Re: SPARK-18689: A proposal for priority based app scheduling
 utilizing linux cgroups.",Shuai Lin <linshuai2012@gmail.com>,"My apologies, in my excitement of finding a rather simple way to accomplish the scheduling goal I have in mind, I hastily jumped straight into a technical solution, without explaining that goal, or the problem it's attempting to solve.


You are correct that I'm looking for an additional running mode for the standalone scheduler. Perhaps you could/should classify it as a different scheduler, but I don't want to give the impression that this will be as difficult to implement as most schedulers are. Initially, from a memory perspective, we would still allocate in a FIFO manner. This new scheduling mode (or new scheduler, if you'd rather) would mostly benefit any users with small-ish clusters, both on-premise and cloud based. Essentially, my end goal is to be able to run multiple *applications* simultaneously with each application having *access* to the entire core count of the cluster.


I have a very cpu intensive application that I'd like to run weekly. I have a second application that I'd like to run hourly. The hourly application is more time critical (higher priority), so I'd like it to finish in a small amount of time. If I allow the first app to run with all cores (this takes several days on my 64 core cluster), then nothing else can be executed when running with the default FIFO scheduler. All of the cores have been allocated to the first application, and it will not release them until it is finished. Dynamic allocation does not help in this case, as there is always a backlog of tasks to run until the first application is nearing the end anyway. Naturally, I could just limit the number of cores that the first application has access to, but then I have idle cpu time when the second app is not running, and that is not optimal. Secondly in that case, the second application only has access to the *leftover* cores that the first app has not allocated, and will take a considerably longer amount of time to run.


You could also imagine a scenario where a developer has a spark-shell running without specifying the number of cores they want to utilize (whether intentionally or not). As I'm sure you know, the default is to allocate the entire cluster to this application. The cores allocated to this shell are unavailable to other applications, even if they are just sitting idle while a developer is getting their environment set up to run a very big job interactively. Other developers that would like to launch interactive shells are stuck waiting for the first one to exit their shell.


My proposal would eliminate this static nature of core counts and allow as many simultaneous applications to be running as the cluster memory (still statically partitioned, at least initially) will allow. Applications could be configured with a ""cpu shares"" parameter (just an arbitrary integer relative only to other applications) which is essentially just passed through to the linux cgroup cpu.shares setting. Since each executor of an application on a given worker runs in it's own process/jvm, then that process could be easily be placed into a cgroup created and dedicated for that application.


Linux cgroups cpu.shares are pretty well documented, but the gist is that processes competing for cpu time are allocated a percentage of time equal to their share count as a percentage of all shares in that level of the cgroup hierarchy. If two applications are both scheduled on the same core with the same weight, each will get to utilize 50% of the time on that core. This is all built into the kernel, and the only thing the spark worker has to do is create a cgroup for each application, set the cpu.shares parameter, and assign the executors for that application to the new cgroup. If multiple executors are running on a single worker, for a single application, the cpu time available to that application is divided among each of those executors equally. The default for cpu.shares is that they are not limiting in any way. A process can consume all available cpu time if it would otherwise be idle anyway.


Another benefit to passing cpu.shares directly to the kernel (as opposed to some abstraction) is that cpu share allocations are heterogeneous to all processes running on a machine. An admin could have very fine grained control over which processes get priority access to cpu time, depending on their needs.


To continue my personal example above, my long running cpu intensive application could utilize 100% of all cluster cores if they are idle. Then my time sensitive app could be launched with nine times the priority and the linux kernel would scale back the first application to 10% of all cores (completely seemlessly and automatically: no pre-emption, just fewer time slices of cpu allocated by the kernel to the first application), while the second application gets 90% of all the cores until it completes.


The only downside that I can think of currently is that this scheduling mode would create an increase in context switching on each host. This issue is somewhat mitigated by still statically allocating memory however, since there wouldn't typically be an exorbitant number of applications running at once.


In my opinion, this would allow the most optimal usage of cluster resources. Linux cgroups allow you to control access to more than just cpu shares. You can apply the same concept to other resources (memory, disk io). You can also set up hard limits so that an application will never get more than is allocated to it. I know that those limitations are important for some use cases involving predictability of application execution times. Eventually, this idea could be expanded to include many more of the features that cgroups provide.


Thanks again for any feedback on this idea. I hope that I have explained it a bit better now. Does anyone else can see value in it?


Travis

________________________________
From: Shuai Lin <linshuai2012@gmail.com>
Sent: Saturday, December 3, 2016 06:52
To: Hegner, Travis
Cc: dev@spark.apache.org
Subject: Re: SPARK-18689: A proposal for priority based app scheduling utilizing linux cgroups.

Sorry but I don't get the scope of the problem from your description. Seems it's an improvement for spark standalone scheduler (i.e. not for yarn or mesos)?


Hello,


I've just created a JIRA to open up discussion of a new feature that I'd like to propose.


https://issues.apache.org/jira/browse/SPARK-18689


I'd love to get some feedback on the idea. I know that normally anything related to scheduling or queuing automatically throws up the ""hard to implement"" red flags, but the proposal contains a rather simple way to implement the concept, which delegates the scheduling logic to the actual kernel of each worker, rather than in any spark core code. I believe this to be more flexible and simpler to set up and maintain than dynamic allocation, and avoids the need for any preemption type of logic.


The proposal does not contain any code. I am not (yet) familiar enough with the core spark code to confidently create an implementation.


I appreciate your time and am looking forward to your feedback!


Thanks,


Travis

"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 05 Dec 2016 16:53:21 +0000",Difference between netty and netty-all,Spark dev list <dev@spark.apache.org>,"I‚Äôm looking at the list of dependencies here:

https://github.com/apache/spark/search?l=Groff&q=netty&type=Code&utf8=%E2%9C%93

What‚Äôs the difference between netty and netty-all?

The reason I ask is because I‚Äôm looking at a Netty PR
<https://github.com/netty/netty/pull/5345> and trying to figure out if
Spark 2.0.2 is using a version of Netty that includes that PR or not.

Nick
‚Äã
"
=?UTF-8?B?TWljaGFsIMWgZW5rw73FmQ==?= <mike.senkyr@gmail.com>,"Mon, 5 Dec 2016 18:52:21 +0100","Re: SPARK-18689: A proposal for priority based app scheduling
 utilizing linux cgroups.",dev@spark.apache.org,"Hello Travis,


I am just a short-time member of this list but I can definitely see the 
benefit of using built-in OS resource management facilities to 
dynamically manage cluster resources on the node level in this manner. 
At our company we often fight for resources on our development cluster 
as well as sometimes cancel running jobs in production to free up 
immediately needed resources. If I understand it correctly, this would 
solve a lot of our problems.


The only downside I see with this is that it is Linux-specific.


Michal



"
Ted Yu <yuzhihong@gmail.com>,"Mon, 5 Dec 2016 10:57:04 -0800",Re: Difference between netty and netty-all,Nicholas Chammas <nicholas.chammas@gmail.com>,"This should be in netty-all :

$ jar tvf
/home/x/.m2/repository/io/netty/netty-all/4.0.29.Final/netty-all-4.0.29.Final.jar
| grep ThreadLocalRandom
   967 Tue Jun 23 11:10:30 UTC 2015
io/netty/util/internal/ThreadLocalRandom$1.class
  1079 Tue Jun 23 11:10:30 UTC 2015
io/netty/util/internal/ThreadLocalRandom$2.class
  5973 Tue Jun 23 11:10:30 UTC 2015
io/netty/util/internal/ThreadLocalRandom.class

m

"
Reynold Xin <rxin@databricks.com>,"Mon, 5 Dec 2016 11:00:31 -0800",Re: Please limit commits for branch-2.1,"Joseph Bradley <joseph@databricks.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","I would like to re-iterate that committers please be very conservative now
in merging patches into branch-2.1.

Spark is a very sophisticated (compiler, optimizer) project and sometimes
one-line changes can have huge consequences and introduce regressions. If
it is just a tiny optimization, don't merge it into branch-2.1.



"
Sean Owen <sowen@cloudera.com>,"Mon, 05 Dec 2016 19:18:43 +0000",Re: Difference between netty and netty-all,"Nicholas Chammas <nicholas.chammas@gmail.com>, Spark dev list <dev@spark.apache.org>","netty should be Netty 3.x. It is all but unused but I couldn't manage to
get rid of it: https://issues.apache.org/jira/browse/SPARK-17875

netty-all should be 4.x, actually used.

m>

f8=%E2%9C%93
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 05 Dec 2016 19:46:03 +0000",Re: Difference between netty and netty-all,"Sean Owen <sowen@cloudera.com>, Spark dev list <dev@spark.apache.org>","So if I'm running Spark 2.0.2 built against Hadoop 2.6, I should be running
[Netty 4.0.29.Final](
https://github.com/apache/spark/blob/v2.0.2/dev/deps/spark-deps-hadoop-2.6#L141
<https://github.com/apache/spark/blob/553aac56bd5284e84391c05e2ef54d8bd7ad3a12/dev/deps/spark-deps-hadoop-2.6#L141>),
right?

And since [the Netty PR I'm interested in](
https://github.com/netty/netty/pull/5345) is tagged 4.0.37.Final, then I
guess Spark 2.0.2 isn't using a version of Netty that includes that PR.
This correlates with what I'm seeing in my environment (warnings related to
low entropy followed by executor failures).

OK cool! Thanks for the pointers.

Nick


f8=%E2%9C%93
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 05 Dec 2016 19:47:14 +0000",Re: Difference between netty and netty-all,Ted Yu <yuzhihong@gmail.com>,"That file is in Netty 4.0.29, but I believe the PR I referenced is not.
It's only in Netty 4.0.37 and up.


inal.jar
f8=%E2%9C%93
"
Michael Armbrust <michael@databricks.com>,"Mon, 5 Dec 2016 13:02:48 -0800",Re: ability to provide custom serializers,Erik LaBianca <erik.labianca@gmail.com>,"Lets start with a new ticket, link them and we can merge if the solution
ends up working out for both cases.


uld research or
it in scope as
registry.
s
nType
ayer‚Äù the new ones
RK-7768
"
Teng Long <longteng.cq@gmail.com>,"Mon, 5 Dec 2016 16:04:59 -0500",Re: Can I add a new method to RDD class?,"Holden Karau <holden@pigscanfly.ca>,
 dev@spark.apache.org","Thank you for providing another answer, Holden.

So I did what Tarun and Michal suggested, and it didn‚Äôt work out as I want to have a new transformation method in RDD class, and need to use that RDD‚Äôs spark context which is private. So I guess the only thing I can do now is to sbt publishLocal?

the version number do do a publishLocal - but maintaining that change is going to be difficult. The other approaches suggested are probably better, but also does your method need to be defined on the RDD class? Could you instead make a helper object or class to expose whatever functionality you need?
to the source code of RDD?
Spark Developers List] <[hidden email] where you use it.
common pattern in scala known as ""pimp my library"" pattern, you can easily find many generic example of using this pattern.
rdd(although there is nothing really more to my example in earlier mail) ?
href=""x-msg://22/user/SendEmail.jtp?type=node&amp;node=20106&amp;i=0"" target=""_top"" rel=""nofollow"" link=""external"" class="""">[hidden Developers List] <[hidden email] without making changes to spark code.
code:
where you want to use the new foo method.
href=""x-msg://19/user/SendEmail.jtp?type=node&amp;amp;node=20102&amp;amp;i=0"" class="""">x-msg://19/user/SendEmail.jtp?type=node&amp;node=20102&amp;i=0"" target=""_top"" rel=""nofollow"" link=""external"" your project dependencies and check it's using your dev build.
href=""x-msg://19/user/SendEmail.jtp?type=node&amp;amp;node=20102&amp;amp;i=1"" class="""">x-msg://19/user/SendEmail.jtp?type=node&amp;node=20102&amp;i=1"" target=""_top"" rel=""nofollow"" link=""external"" code
this or
http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-add-a-new-method-to-RDD-class-tp20100.html <http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-add-a-new-method-to-RDD-class-tp20100.html>
Nabble.com <http://nabble.com/>.
---------------------------------------------------------------------
href=""x-msg://19/user/SendEmail.jtp?type=node&amp;amp;node=20102&amp;amp;i=2"" class="""">x-msg://19/user/SendEmail.jtp?type=node&amp;node=20102&amp;i=2"" target=""_top"" rel=""nofollow"" link=""external"" class="""">[hidden email]
discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-add-a-new-method-to-RDD-class-tp20100p20102.html <http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-add-a-new-method-to-RDD-class-tp20100p20102.html>
here <>.
<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
class? <http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-add-a-new-method-to-RDD-class-tp20100p20104.html>
<http://apache-spark-developers-list.1001551.n3.nabble.com/> at Nabble.com <http://nabble.com/>.
discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-add-a-new-method-to-RDD-class-tp20100p20106.html <http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-add-a-new-method-to-RDD-class-tp20100p20106.html>
<>.
<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
<http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-add-a-new-method-to-RDD-class-tp20100p20107.html>
<http://apache-spark-developers-list.1001551.n3.nabble.com/> at Nabble.com.

"
"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Mon, 5 Dec 2016 13:10:54 -0800",Re: Can I add a new method to RDD class?,Teng Long <longteng.cq@gmail.com>,"RDD.sparkContext is public:
http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD@sparkContext:org.apache.spark.SparkContext


as I want
 can do
e
g
u
the
rk Developers
ly
ly
ly
ss
er/
or
ail]
Servlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
ew-method-to-RDD-class-tp20100p20104.html>
Servlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
ew-method-to-RDD-class-tp20100p20107.html>
"
Teng Long <longteng.cq@gmail.com>,"Mon, 5 Dec 2016 16:19:59 -0500",Re: Can I add a new method to RDD class?,"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>,
 dev@spark.apache.org","Thank you, Ryan. Didn‚Äôt there is a method for that!

http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD@sparkContext:org.apache.spark.SparkContext <http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD@sparkContext:org.apache.spark.SparkContext>
out as I want to have a new transformation method in RDD class, and need to use that RDD‚Äôs spark context which is private. So I guess the only thing I can do now is to sbt publishLocal?
edit the version number do do a publishLocal - but maintaining that change is going to be difficult. The other approaches suggested are probably better, but also does your method need to be defined on the RDD class? Could you instead make a helper object or class to expose whatever functionality you need?
to the source code of RDD?
Spark Developers List] <[hidden email] where you use it.
fairly common pattern in scala known as ""pimp my library"" pattern, you can easily find many generic example of using this pattern.
rdd(although there is nothing really more to my example in earlier mail) ?
href=""x-msg://22/user/SendEmail.jtp?type=node&amp;node=20106&amp;i=0 <>"" target=""_top"" rel=""nofollow"" link=""external"" Developers List] <[hidden email] without making changes to spark code.
code:
where you want to use the new foo method.
href=""x-msg://19/user/SendEmail.jtp?type=node&amp;amp;node=20102&amp;amp;i=0 <>"" class="""">x-msg://19/user/SendEmail.jtp?type=node&amp;node=20102&amp;i=0 <>"" target=""_top"" rel=""nofollow"" link=""external"" your project dependencies and check it's using your dev build.
href=""x-msg://19/user/SendEmail.jtp?type=node&amp;amp;node=20102&amp;amp;i=1 <>"" class="""">x-msg://19/user/SendEmail.jtp?type=node&amp;node=20102&amp;i=1 <>"" target=""_top"" rel=""nofollow"" link=""external"" code
this or
http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-add-a-new-method-to-RDD-class-tp20100.html <http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-add-a-new-method-to-RDD-class-tp20100.html>
Nabble.com <http://nabble.com/>.
---------------------------------------------------------------------
href=""x-msg://19/user/SendEmail.jtp?type=node&amp;amp;node=20102&amp;amp;i=2 <>"" class="""">x-msg://19/user/SendEmail.jtp?type=node&amp;node=20102&amp;i=2 <>"" target=""_top"" rel=""nofollow"" link=""external"" class="""">[hidden email]
discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-add-a-new-method-to-RDD-class-tp20100p20102.html <http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-add-a-new-method-to-RDD-class-tp20100p20102.html>
here <>.
<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
class? <http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-add-a-new-method-to-RDD-class-tp20100p20104.html>
<http://apache-spark-developers-list.1001551.n3.nabble.com/> at Nabble.com <http://nabble.com/>.
discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-add-a-new-method-to-RDD-class-tp20100p20106.html <http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-add-a-new-method-to-RDD-class-tp20100p20106.html>
<>.
<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
class? <http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-add-a-new-method-to-RDD-class-tp20100p20107.html>
<http://apache-spark-developers-list.1001551.n3.nabble.com/> at Nabble.com <http://nabble.com/>.

"
"""Thakrar, Jayesh"" <jthakrar@conversantmedia.com>","Mon, 5 Dec 2016 21:33:48 +0000",Re: Can I add a new method to RDD class?,"Teng Long <longteng.cq@gmail.com>, Holden Karau <holden@pigscanfly.ca>,
	""dev@spark.apache.org"" <dev@spark.apache.org>","Teng,

Before you go down creating your own custom Spark system, do give some thought to what Holden and others are suggesting, viz. using implicit methods.

If you want real concrete examples, have a look at the Spark Cassandra Connector -

Here you will see an example of ""extending"" SparkContext - https://github.com/datastax/spark-cassandra-connector/blob/master/doc/2_loading.md

// validation is deferred, so it is not triggered during rdd creation
val rdd = sc.cassandraTable[SomeType](""ks"", ""not_existing_table"")
val emptyRDD = rdd.toEmptyCassandraRDD

val emptyRDD2 = sc.emptyCassandraTable[SomeType](""ks"", ""not_existing_table""))


And here you will se an example of ""extending"" RDD - https://github.com/datastax/spark-cassandra-connector/blob/master/doc/5_saving.md

case class WordCount(word: String, count: Long)
val collection = sc.parallelize(Seq(WordCount(""dog"", 50), WordCount(""cow"", 60)))
collection.saveToCassandra(""test"", ""words"", SomeColumns(""word"", ""count""))

Hope that helps‚Ä¶
Jayesh


From: Teng Long <longteng.cq@gmail.com>
Date: Monday, December 5, 2016 at 3:04 PM
To: Holden Karau <holden@pigscanfly.ca>, <dev@spark.apache.org>
Subject: Re: Can I add a new method to RDD class?

Thank you for providing another answer, Holden.

So I did what Tarun and Michal suggested, and it didn‚Äôt work out as I want to have a new transformation method in RDD class, and need to use that RDD‚Äôs spark context which is private. So I guess the only thing I can do now is to sbt publishLocal?

On Dec 5, 2016, at 9:19 AM, Holden Karau <holden@pigscanfly.ca<mailto:holden@pigscanfly.ca>> wrote:

Doing that requires publishing a custom version of Spark, you can edit the version number do do a publishLocal - but maintaining that change is going to be difficult. The other approaches suggested are probably better, but also does your method need to be defined on the RDD class? Could you instead make a helper object or class to expose whatever functionality you need?

On Mon, Dec 5, 2016 at 6:06 PM long <longteng.cq@gmail.com<mailto:longteng.cq@gmail.com>> wrote:
Thank you very much! But why can‚Äôt I just add new methods in to the source code of RDD?

On Dec 5, 2016, at 3:15 AM, Michal ≈†enk√Ω≈ô [via Apache Spark Developers List] <[hidden email]<http://user/SendEmail.jtp?type=node&node=20107&i=0>> wrote:


A simple Scala example of implicit classes:

implicit class EnhancedString(str: String) {

  def prefix(prefix: String) = prefix + str

}



println(""World"".prefix(""Hello ""))

As Tarun said, you have to import it if it's not in the same class where you use it.

Hope this makes it clearer,

Michal Senkyr

On 5.12.2016 07:43, Tarun Kumar wrote:
Not sure if that's documented in terms of Spark but this is a fairly common pattern in scala known as ""pimp my library"" pattern, you can easily find many generic example of using this pattern. If you want I can quickly cook up a short conplete example with rdd(although there is nothing really more to my example in earlier mail) ? Thanks Tarun Kumar

On Mon, 5 Dec 2016 at 7:15 AM, long <<a href=""x-msg://22/user/SendEmail.jtp?type=node&amp;node=20106&amp;i=0"" target=""_top"" rel=""nofollow"" link=""external"" class="""">[hidden email]> wrote:
So is there documentation of this I can refer to?

On Dec 5, 2016, at 1:07 AM, Tarun Kumar [via Apache Spark Developers List] <[hidden email]<http://user/SendEmail.jtp?type=node&node=20104&i=0>> wrote:

Hi Tenglong, In addition to trsell's reply, you can add any method to an rdd without making changes to spark code. This can be achieved by using implicit class in your own client code: implicit class extendRDD[T](rdd: RDD[T]){ def foo() } Then you basically nees to import this implicit class in scope where you want to use the new foo method. Thanks Tarun Kumar

On Mon, 5 Dec 2016 at 6:59 AM, <<a href=""<a href=""x-msg://19/user/SendEmail.jtp?type=node&amp;amp;node=20102&amp;amp;i=0"" class="""">x-msg://19/user/SendEmail.jtp?type=node&amp;node=20102&amp;i=0"" target=""_top"" rel=""nofollow"" link=""external"" class="""">[hidden email]> wrote:

How does your application fetch the spark dependency? Perhaps list your project dependencies and check it's using your dev build.

On Mon, 5 Dec 2016, 08:47 tenglong, <<a href=""<a href=""x-msg://19/user/SendEmail.jtp?type=node&amp;amp;node=20102&amp;amp;i=1"" class="""">x-msg://19/user/SendEmail.jtp?type=node&amp;node=20102&amp;i=1"" target=""_top"" rel=""nofollow"" link=""external"" class="""">[hidden email]> wrote:
Hi,

Apparently, I've already tried adding a new method to RDD,

for example,

class RDD {
  def foo() // this is the one I added

  def map()

  def collect()
}

I can build Spark successfully, but I can't compile my application code
which calls rdd.foo(), and the error message says

value foo is not a member of org.apache.spark.rdd.RDD[String]

So I am wondering if there is any mechanism prevents me from doing this or
something I'm doing wrong?




--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-add-a-new-method-to-RDD-class-tp20100.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com<http://nabble.com/>.

---------------------------------------------------------------------
To unsubscribe e-mail: <a href=""<a href=""x-msg://19/user/SendEmail.jtp?type=node&amp;amp;node=20102&amp;amp;i=2"" class="""">x-msg://19/user/SendEmail.jtp?type=node&amp;node=20102&amp;i=2"" target=""_top"" rel=""nofollow"" link=""external"" class="""">[hidden email]

________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-add-a-new-method-to-RDD-class-tp20100p20102.html
To unsubscribe from Can I add a new method to RDD class?, click here.
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>

________________________________
View this message in context: Re: Can I add a new method to RDD class?<http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-add-a-new-method-to-RDD-class-tp20100p20104.html>
Sent from the Apache Spark Developers List mailing list archive<http://apache-spark-developers-list.1001551.n3.nabble.com/> at Nabble.com<http://nabble.com/>.
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-add-a-new-method-to-RDD-class-tp20100p20106.html
To unsubscribe from Can I add a new method to RDD class?, click here.
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>

________________________________
View this message in context: Re: Can I add a new method to RDD class?<http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-add-a-new-method-to-RDD-class-tp20100p20107.html>
Sent from the Apache Spark Developers List mailing list archive<http://apache-spark-developers-list.1001551.n3.nabble.com/> at Nabble.com<http://Nabble.com>.

"
Tarun Kumar <tarunk1407@gmail.com>,"Mon, 5 Dec 2016 23:07:01 +0100",Re: Can I add a new method to RDD class?,"""Thakrar, Jayesh"" <jthakrar@conversantmedia.com>","Teng,

Can you please share the details of transformation that you want to
implement in your method foo?

I have created a gist of one dummy transformation for your method foo ,
this foo method transforms from an RDD[T] to RDD[(T,T)]. Many such more
transformations can easily be achieved.

https://gist.github.com/fidato13/3b46fe1c96b37ae0dd80c275fbe90e92

Thanks
Tarun Kumar


as I want
 can do
e
g
u
he source
k Developers
&i=0>>
y
y
y
]
s
r/
r
il]
-
ervlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
w-method-to-RDD-class-tp20100p20104.html>
m
-
ervlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
w-method-to-RDD-class-tp20100p20107.html>
m
"
Teng Long <longteng.cq@gmail.com>,"Mon, 5 Dec 2016 17:14:32 -0500",Re: Can I add a new method to RDD class?,"Tarun Kumar <tarunk1407@gmail.com>,
 dev@spark.apache.org","I‚Äôm trying to implement a transformation that can merge partitions (to align with GPU specs) and move them onto GPU memory, for example rdd.toGPU() and later transformations like map can automatically be performed on GPU. And another transformation rdd.offGPU() to move partitions off GPU memory and repartition them to the way they were on CPU before.

Thank you, Tarun, for creating that gist. I‚Äôll look at it and see if it meets my needs.

implement in your method foo?
, this foo method transforms from an RDD[T] to RDD[(T,T)]. Many such more transformations can easily be achieved.
<https://gist.github.com/fidato13/3b46fe1c96b37ae0dd80c275fbe90e92>
<jthakrar@conversantmedia.com <mailto:jthakrar@conversantmedia.com>> thought to what Holden and others are suggesting, viz. using implicit methods.
Connector -
https://github.com/datastax/spark-cassandra-connector/blob/master/doc/2_loading.md <https://github.com/datastax/spark-cassandra-connector/blob/master/doc/2_loading.md>
""not_existing_table""))
https://github.com/datastax/spark-cassandra-connector/blob/master/doc/5_saving.md <https://github.com/datastax/spark-cassandra-connector/blob/master/doc/5_saving.md>
WordCount(""cow"", 60)))
""count""))
<dev@spark.apache.org <mailto:dev@spark.apache.org>>
out as I want to have a new transformation method in RDD class, and need to use that RDD‚Äôs spark context which is private. So I guess the only thing I can do now is to sbt publishLocal?
the version number do do a publishLocal - but maintaining that change is going to be difficult. The other approaches suggested are probably better, but also does your method need to be defined on the RDD class? Could you instead make a helper object or class to expose whatever functionality you need?
to the source code of RDD?
Spark Developers List] <[hidden email] where you use it.
common pattern in scala known as ""pimp my library"" pattern, you can easily find many generic example of using this pattern. If you want I can quickly cook up a short conplete  example with rdd(although there is nothing really more to my example in earlier mail) ? Thanks Tarun Kumar
href=""x-msg://22/user/SendEmail.jtp?type=node&amp;node=20106&amp;i=0 <>"" target=""_top"" rel=""nofollow"" link=""external"" List] <[hidden email] an rdd without making changes to spark code. This can be achieved by using implicit class in your own client code: implicit class extendRDD[T](rdd: RDD[T]){ def foo() } Then you basically nees to import this implicit class in scope where you want to use the new foo method. Thanks Tarun Kumar
href=""x-msg://19/user/SendEmail.jtp?type=node&amp;amp;node=20102&amp;amp;i=0 <>"" class="""">x-msg://19/user/SendEmail.jtp?type=node&amp;node=20102&amp;i=0 <>"" target=""_top"" rel=""nofollow"" link=""external"" your project dependencies and check it's using your dev build.
href=""x-msg://19/user/SendEmail.jtp?type=node&amp;amp;node=20102&amp;amp;i=1 <>"" class="""">x-msg://19/user/SendEmail.jtp?type=node&amp;node=20102&amp;i=1 <>"" target=""_top"" rel=""nofollow"" link=""external"" code
this or
http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-add-a-new-method-to-RDD-class-tp20100.html <http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-add-a-new-method-to-RDD-class-tp20100.html>
Nabble.com <http://nabble.com/>.
href=""x-msg://19/user/SendEmail.jtp?type=node&amp;amp;node=20102&amp;amp;i=2 <>"" class="""">x-msg://19/user/SendEmail.jtp?type=node&amp;node=20102&amp;i=2 <>"" target=""_top"" rel=""nofollow"" link=""external"" class="""">[hidden email]
discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-add-a-new-method-to-RDD-class-tp20100p20102.html <http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-add-a-new-method-to-RDD-class-tp20100p20102.html>
<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
<http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-add-a-new-method-to-RDD-class-tp20100p20104.html>
<http://apache-spark-developers-list.1001551.n3.nabble.com/> at Nabble.com <http://nabble.com/>.
discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-add-a-new-method-to-RDD-class-tp20100p20106.html <http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-add-a-new-method-to-RDD-class-tp20100p20106.html>
<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
<http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-add-a-new-method-to-RDD-class-tp20100p20107.html>
<http://apache-spark-developers-list.1001551.n3.nabble.com/> at Nabble.com <http://nabble.com/>.

"
Liren Ding <sky.gonna.bright@gmail.com>,"Mon, 5 Dec 2016 14:18:01 -0800",Back-pressure to Spark Kafka Streaming?,"dev@spark.apache.org, user@spark.apache.org","Hey all,

Does backressure actually work on spark kafka streaming? According to the
latest spark streaming document:
*http://spark.apache.org/docs/latest/streaming-programming-guide.html
<http://spark.apache.org/docs/latest/streaming-programming-guide.html>*
""*In Spark 1.5, we have introduced a feature called backpressure that
eliminate the need to set this rate limit, as Spark Streaming automatically
figures out the rate limits and dynamically adjusts them if the processing
conditions change. This backpressure can be enabled by setting the
configuration parameter spark.streaming.backpressure.enabled to true.*""
But I also see a few open spark jira tickets on this option:

*https://issues.apache.org/jira/browse/SPARK-7398
<https://issues.apache.org/jira/browse/SPARK-7398>*
*https://issues.apache.org/jira/browse/SPARK-18371
<https://issues.apache.org/jira/browse/SPARK-18371>*

The case in the second ticket describes a similar issue as we have here. We
use Kafka to send large batches (10~100M) to spark streaming, and the spark
streaming interval is set to 1~4 minutes. With the backpressure set to
true, the queued active batches still pile up when average batch processing
time takes longer than default interval. After the spark driver is
restarted, all queued batches turn to a giant batch, which block subsequent
batches and also have a great chance to fail eventually. The only config we
found that might help is ""*spark.streaming.kafka.maxRatePerPartition*"". It
does limit the incoming batch size, but not a perfect solution since it
depends on size of partition as well as the length of batch interval. For
our case, hundreds of partitions X minutes of interval still produce a
number that is too large for each batch. So we still want to figure out how
to make the backressure work in spark kafka streaming, if it is supposed to
work there. Thanks.


Liren
"
Tarun Kumar <tarunk1407@gmail.com>,"Mon, 5 Dec 2016 23:55:35 +0100",Why is there no flatten method on RDD?,dev <dev@spark.apache.org>,"Hi,

Although a flatMap would yield me the results which I look from map
followed by flatten, but just out of curiosity, Why there is no flatten
method provided on RDD.

Would it make sense to submit a PR adding flatten on RDD?

Thanks
Tarun
"
Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Tue, 6 Dec 2016 00:23:44 +0100",Re: Future of the Python 2 support.,"Reynold Xin <rxin@databricks.com>,
 Nicholas Chammas <nicholas.chammas@gmail.com>,
 ""dev@spark.apache.org"" <dev@spark.apache.org>","Fair enough. I have to admit I am bit disappointed but that's life :)



-- 
Maciej Szymkiewicz

"
Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Tue, 6 Dec 2016 00:30:02 +0100",[MLLIB] RankingMetrics.precisionAt,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

Could I ask fora fresh pair of eyes on this piece of code:

https://github.com/apache/spark/blob/f830bb9170f6b853565d9dd30ca7418b93a54fe3/mllib/src/main/scala/org/apache/spark/mllib/evaluation/RankingMetrics.scala#L59-L80

  @Since(""1.2.0"")
  def precisionAt(k: Int): Double = {
    require(k > 0, ""ranking position k should be positive"")
    predictionAndLabels.map { case (pred, lab) =>
      val labSet = lab.toSet

      if (labSet.nonEmpty) {
        val n = math.min(pred.length, k)
        var i = 0
        var cnt = 0
        while (i < n) {
          if (labSet.contains(pred(i))) {
            cnt += 1
          }
          i += 1
        }
        cnt.toDouble / k
      } else {
        logWarning(""Empty ground truth set, check input data"")
        0.0
      }
    }.mean()
  }


Am I the only one who thinks this doesn't do what it claims? Just for
reference:

  * https://web.archive.org/web/20120415101144/http://sas.uwaterloo.ca/stats_navigation/techreports/04WorkingPapers/2004-09.pdf
  * https://github.com/benhamner/Metrics/blob/master/Python/ml_metrics/average_precision.py

-- 
Best,
Maciej

"
Saikat Kanjilal <sxk1969@hotmail.com>,"Mon, 5 Dec 2016 23:33:25 +0000","Re: Spark-9487, Need some insight","""dev@spark.apache.org"" <dev@spark.apache.org>","Hello again dev community,

Ping on this, apologies for rerunning this thread but never heard from anyone, based on this link:  https://wiki.jenkins-ci.org/display/JENKINS/Installing+Jenkins  I can try to install jenkins locally but is that really needed?


Thanks in advance.


________________________________
From: Saikat Kanjilal <sxk1969@hotmail.com>
Sent: Tuesday, November 29, 2016 8:14 PM
To: dev@spark.apache.org
Subject: Spark-9487, Need some insight


Hello Spark dev community,

I took this the following jira item (https://github.com/apache/spark/pull/15848) and am looking for some general pointers, it seems that I am running into issues where things work successfully doing local development on my macbook pro but fail on jenkins for a multitiude of reasons and errors, here's an example,  if you see this build output report: https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/69297/ you will see the DataFrameStatSuite, now locally I am running these individual tests with this command: ./build/mvn test -P... -DwildcardSuites=none -Dtest=org.apache.spark.sql.DataFrameStatSuite.     It seems that I need to emulate a jenkins like environment locally, this seems sort of like an untenable hurdle, granted that my changes involve changing the total number of workers in the sparkcontext and if so should I be testing my changes in an environment that more closely resembles jenkins.  I really want to work on/complete this PR but I keep getting hamstrung by a dev environment that is not equivalent to our CI environment.



I'm guessing/hoping I'm not the first one to run into this so some insights. pointers to get past this would be very appreciated , would love to keep contributing and hoping this is a hurdle that's overcomeable with some tweaks to my dev environment.



Thanks in advance.
"
Teng Long <longteng.cq@gmail.com>,"Mon, 5 Dec 2016 18:42:38 -0500",Re: Can I add a new method to RDD class?,"Tarun Kumar <tarunk1407@gmail.com>,
 dev@spark.apache.org","Tarun,

I want to access some private methods e.g. withScope, so I add a similar implicit class compiled with spark. But I can‚Äôt import that into my application? 

for example in I have added org/apache/spark/rdd/RDDExtensions.scala, in there I defined a implicit class inside the RDDExtensions object, and I successfully compiled spark with it.

Then in my application code, when I try to import that implicit class by using,

import org.apache.spark.rdd.RDDExtensions._

I can‚Äôt compile my application, and error says ""object RDDExtensions is not a member of package org.apache.spark.rdd‚Äù. It seems like my import statement is wrong, but I don‚Äôt know how?

Thanks!

partitions (to align with GPU specs) and move them onto GPU memory, for example rdd.toGPU() and later transformations like map can automatically be performed on GPU. And another transformation rdd.offGPU() to move partitions off GPU memory and repartition them to the way they were on CPU before.
see if it meets my needs.
implement in your method foo?
, this foo method transforms from an RDD[T] to RDD[(T,T)]. Many such more transformations can easily be achieved.
some thought to what Holden and others are suggesting, viz. using implicit methods.
Cassandra Connector -
https://github.com/datastax/spark-cassandra-connector/blob/master/doc/2_loading.md
""not_existing_table""))
https://github.com/datastax/spark-cassandra-connector/blob/master/doc/5_saving.md
WordCount(""cow"", 60)))
""count""))
out as I want to have a new transformation method in RDD class, and need to use that RDD‚Äôs spark context which is private. So I guess the only thing I can do now is to sbt publishLocal?
edit the version number do do a publishLocal - but maintaining that change is going to be difficult. The other approaches suggested are probably better, but also does your method need to be defined on the RDD class? Could you instead make a helper object or class to expose whatever functionality you need?
to the source code of RDD?
where you use it.
common pattern in scala known as ""pimp my library"" pattern, you can easily find many generic example of using this pattern. If you want I can quickly cook up a short conplete example with rdd(although there is nothing really more to my example in earlier mail) ? Thanks Tarun Kumar
href=""x-msg://22/user/SendEmail.jtp?type=node&amp;node=20106&amp;i=0"" target=""_top"" rel=""nofollow"" link=""external"" class="""">[hidden an rdd without making changes to spark code. This can be achieved by using implicit class in your own client code: implicit class extendRDD[T](rdd: RDD[T]){ def foo() } Then you basically nees to import this implicit class in scope where you want to use the new foo method. Thanks Tarun Kumar
href=""x-msg://19/user/SendEmail.jtp?type=node&amp;amp;node=20102&amp;amp;i=0"" class="""">x-msg://19/user/SendEmail.jtp?type=node&amp;node=20102&amp;i=0"" target=""_top"" rel=""nofollow"" link=""external"" your project dependencies and check it's using your dev build.
href=""x-msg://19/user/SendEmail.jtp?type=node&amp;amp;node=20102&amp;amp;i=1"" class="""">x-msg://19/user/SendEmail.jtp?type=node&amp;node=20102&amp;i=1"" target=""_top"" rel=""nofollow"" link=""external"" code
this or
http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-add-a-new-method-to-RDD-class-tp20100.html
Nabble.com.
href=""x-msg://19/user/SendEmail.jtp?type=node&amp;amp;node=20102&amp;amp;i=2"" class="""">x-msg://19/user/SendEmail.jtp?type=node&amp;node=20102&amp;i=2"" target=""_top"" rel=""nofollow"" link=""external"" class="""">[hidden email]
discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-add-a-new-method-to-RDD-class-tp20100p20102.html
class?
Nabble.com.
discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-add-a-new-method-to-RDD-class-tp20100p20106.html
class?
Nabble.com.


---------------------------------------------------------------------


"
"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Mon, 5 Dec 2016 17:35:04 -0800",Re: Difference between netty and netty-all,Nicholas Chammas <nicholas.chammas@gmail.com>,"Hey Nick,

It should be safe to upgrade Netty to the latest 4.0.x version. Could you
submit a PR, please?


ll-4.0.29.Final.jar
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 06 Dec 2016 01:39:26 +0000",Re: Difference between netty and netty-all,"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","You mean just for branch-2.0, right?
‚Äã


inal.jar
f8=%E2%9C%93
"
Sean Owen <sowen@cloudera.com>,"Tue, 06 Dec 2016 01:45:19 +0000",Re: [MLLIB] RankingMetrics.precisionAt,"Maciej Szymkiewicz <mszymkiewicz@gmail.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","I read it again and that looks like it implements mean precision@k as I
would expect. What is the issue?


"
"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Mon, 5 Dec 2016 17:57:19 -0800",Re: Difference between netty and netty-all,Nicholas Chammas <nicholas.chammas@gmail.com>,"No. I meant only updating master. It's not worth to update a maintenance
branch unless there are critical issues.

m

m>
u
ll-4.0.29.Final.jar
"
Jakob Odersky <jakob@odersky.com>,"Mon, 5 Dec 2016 18:12:17 -0800",Re: Can I add a new method to RDD class?,Teng Long <longteng.cq@gmail.com>,"It looks like you're having issues with including your custom spark
version (with the extensions) in your test project. To use your local
spark version:
1) make sure it has a custom version (let's call it 2.1.0-CUSTOM)
2) publish it to your local machine with `sbt publishLocal`
3) include the modified version of spark in your test project with
`libraryDependencies += ""org.apache.spark"" %% ""spark-core"" %
""2.1.0-CUSTOM""`

However, as others have said, it can be quite a lot of work to
maintain a custom fork of spark. If you're planning on contributing
these changes back to spark, than forking is the way to go (although I
would recommend to keep an ongoing discussion with the maintainers, to
make sure your work will be merged back). Otherwise, I would recommend
to use ""implicit extensions"" to enrich your rdds instead. An easy
workaround to access spark-private fields is to simply define your
custom rdds in an ""org.apache.spark"" package as well ;)

---------------------------------------------------------------------


"
Cody Koeninger <cody@koeninger.org>,"Mon, 5 Dec 2016 20:18:29 -0600",Re: Back-pressure to Spark Kafka Streaming?,Liren Ding <sky.gonna.bright@gmail.com>,"If you want finer-grained max rate setting, SPARK-17510 got merged a
while ago.  There's also SPARK-18580 which might help address the
issue of starting backpressure rate for the first batch.


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Mon, 5 Dec 2016 18:44:22 -0800","Re: Spark-9487, Need some insight",Saikat Kanjilal <sxk1969@hotmail.com>,"Honestly it is pretty difficult. Given the difficulty, would it still make
sense to do that change? (the one that sets the same number of
workers/parallelism across different languages in testing)



"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 06 Dec 2016 03:45:38 +0000",Re: Difference between netty and netty-all,"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Latest 4.0.x is Netty 4.0.42 and master is already on that version.

My comments earlier were about the version of Netty that Spark 2.0.2 uses,
which is why I thought you were talking about branch-2.0.

Anyway, I don‚Äôt think we need to do anything here. If someone is having
serious Netty problems caused by low entropy, they can install Haveged
<https://wiki.archlinux.org/index.php/Haveged> (which is what I did!) or
just wait and upgrade to Spark 2.1.0.

Nick


No. I meant only updating master. It's not worth to update a maintenance
branch unless there are critical issues.

m

You mean just for branch-2.0, right?
‚Äã


Hey Nick,

It should be safe to upgrade Netty to the latest 4.0.x version. Could you
submit a PR, please?


That file is in Netty 4.0.29, but I believe the PR I referenced is not.
It's only in Netty 4.0.37 and up.



This should be in netty-all :

$ jar tvf
/home/x/.m2/repository/io/netty/netty-all/4.0.29.Final/netty-all-4.0.29.Final.jar
| grep ThreadLocalRandom
   967 Tue Jun 23 11:10:30 UTC 2015
io/netty/util/internal/ThreadLocalRandom$1.class
  1079 Tue Jun 23 11:10:30 UTC 2015
io/netty/util/internal/ThreadLocalRandom$2.class
  5973 Tue Jun 23 11:10:30 UTC 2015
io/netty/util/internal/ThreadLocalRandom.class

m

I‚Äôm looking at the list of dependencies here:

https://github.com/apache/spark/search?l=Groff&q=netty&type=Code&utf8=%E2%9C%93

What‚Äôs the difference between netty and netty-all?

The reason I ask is because I‚Äôm looking at a Netty PR
<https://github.com/netty/netty/pull/5345> and trying to figure out if
Spark 2.0.2 is using a version of Netty that includes that PR or not.

Nick
‚Äã




‚Äã
"
Yu Wei <yu2003w@hotmail.com>,"Tue, 6 Dec 2016 06:05:06 +0000",driver in queued state and not started,"dev <dev@spark.apache.org>, ""user@spark.apache.org""
	<user@spark.apache.org>","Hi Guys,


I tried to run spark on mesos cluster.

However, when I tried to submit jobs via spark-submit. The driver is in ""Queued state"" and not started.


Which should I check?



Thanks,

Jared, (??)
Software developer
Interested in open source software, big data, Linux
"
Prasann modi <prasannmodi@gmail.com>,"Tue, 6 Dec 2016 09:25:51 +0530",SparkR Function for Step Wise Regression,user@spark.apache.org,"Hello,

I have an issue related to SparkR. I want to build step wise regression model

using SparkR, is any function is there in SparkR to build those kind of model.

In R function is available for step wise regression, code is given below:

step(glm(formula,data,family),direction = ""forward""))

If SparkR have function for step wise then please let me know.


Thanks & Regard
Prasann Modi

---------------------------------------------------------------------


"
Richard Startin <richardstartin@outlook.com>,"Mon, 5 Dec 2016 22:58:47 +0000",Re: Back-pressure to Spark Kafka Streaming?,"Liren Ding <sky.gonna.bright@gmail.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>, ""user@spark.apache.org"" <user@spark.apache.org>","I've seen the feature work very well. For tuning, you've got:

spark.streaming.backpressure.pid.proportional (defaults to 1, non-negative) - weight for response to ""error"" (change between last batch and this batch)
spark.streaming.backpressure.pid.integral (defaults to 0.2, non-negative) - weight for the response to the accumulation of error. This has a dampening effect.
spark.streaming.backpressure.pid.derived (defaults to zero, non-negative) - weight for the response to the trend in error. This can cause arbitrary/noise-induced fluctuations in batch size, but can also help react quickly to increased/reduced capacity.
spark.streaming.backpressure.pid.minRate - the default value is 100 (must be positive), batch size won't go below this.

spark.streaming.receiver.maxRate - batch size won't go above this.


Cheers,

Richard


https://richardstartin.com/


________________________________
From: Liren Ding <sky.gonna.bright@gmail.com>
Sent: 05 December 2016 22:18
To: dev@spark.apache.org; user@spark.apache.org
Subject: Back-pressure to Spark Kafka Streaming?

Hey all,

Does backressure actually work on spark kafka streaming? According to the latest spark streaming document:
http://spark.apache.org/docs/latest/streaming-programming-guide.html
""In Spark 1.5, we have introduced a feature called backpressure that eliminate the need to set this rate limit, as Spark Streaming automatically figures out the rate limits and dynamically adjusts them if the processing conditions change. This backpressure can be enabled by setting the configuration parameter spark.streaming.backpressure.enabled to true.""
But I also see a few open spark jira tickets on this option:
https://issues.apache.org/jira/browse/SPARK-7398
https://issues.apache.org/jira/browse/SPARK-18371

The case in the second ticket describes a similar issue as we have here. We use Kafka to send large batches (10~100M) to spark streaming, and the spark streaming interval is set to 1~4 minutes. With the backpressure set to true, the queued active batches still pile up when average batch processing time takes longer than default interval. After the spark driver is restarted, all queued batches turn to a giant batch, which block subsequent batches and also have a great chance to fail eventually. The only config we found that might help is ""spark.streaming.kafka.maxRatePerPartition"". It does limit the incoming batch size, but not a perfect solution since it depends on size of partition as well as the length of batch interval. For our case, hundreds of partitions X minutes of interval still produce a number that is too large for each batch. So we still want to figure out how to make the backressure work in spark kafka streaming, if it is supposed to work there. Thanks.


Liren







"
Teng Long <longteng.cq@gmail.com>,"Tue, 6 Dec 2016 05:50:32 -0500",Re: Can I add a new method to RDD class?,"Jakob Odersky <jakob@odersky.com>,
 dev@spark.apache.org","Thank you Jokob for clearing things up for me. 

Before, I thought my application was compiled against my local build since I can get all the logs I just added in spark-core. But it was all along using spark downloaded from remote maven repository, and that‚Äôs why I ‚Äúcannot"" add new RDD methods in. 

How can I specify a custom version? modify version numbers in all the pom.xml file?

 

"
Steve Loughran <stevel@hortonworks.com>,"Tue, 6 Dec 2016 11:18:19 +0000","Re: Spark-9487, Need some insight",Saikat Kanjilal <sxk1969@hotmail.com>,"jenkins uses SBT, so you need to do the test run there. They are different, and have different test runners in particular.



Hello Spark dev community,
I took this the following jira item (https://github.com/apache/spark/pull/15848) and am looking for some general pointers, it seems that I am running into issues where things work successfully doing local development on my macbook pro but fail on jenkins for a multitiude of reasons and errors, here's an example,  if you see this build output report: https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/69297/ you will see the DataFrameStatSuite, now locally I am running these individual tests with this command: ./build/mvn test -P... -DwildcardSuites=none -Dtest=org.apache.spark.sql.DataFrameStatSuite.     It seems that I need to emulate a jenkins like environment locally, this seems sort of like an untenable hurdle, granted that my changes involve changing the total number of workers in the sparkcontext and if so should I be testing my changes in an environment that more closely resembles jenkins.  I really want to work on/complete this PR but I keep getting hamstrung by a dev environment that is not equivalent to our CI environment.

There's always the option of creating a linux VM/container with jenkins in it; there's a nice trick there in which you can have it watch a git branch, and have it kick off a run whenever you push up to it. That way, you have your own personal jenkins to do the full regression tests, while you yourself work on a small bit.



I'm guessing/hoping I'm not the first one to run into this so some insights. pointers to get past this would be very appreciated , would love to keep contributing and hoping this is a hurdle that's overcomeable with some tweaks to my dev environment.


Thanks in advance.

"
chris snow <chsnow123@gmail.com>,"Tue, 6 Dec 2016 11:36:14 +0000","unhelpful exception thrown on predict() when ALS trained model
 doesn't contain user or product?",dev@spark.apache.org,"I'm using the MatrixFactorizationModel.predict() method and encountered the
following exception:

Name: java.util.NoSuchElementException
Message: next on empty iterator
StackTrace: scala.collection.Iterator$$anon$2.next(Iterator.scala:39)
scala.collection.Iterator$$anon$2.next(Iterator.scala:37)
scala.collection.IndexedSeqLike$Elements.next(IndexedSeqLike.scala:64)
scala.collection.IterableLike$class.head(IterableLike.scala:91)
scala.collection.mutable.ArrayBuffer.scala$collection$IndexedSeqOptimized$$super$head(ArrayBuffer.scala:47)
scala.collection.IndexedSeqOptimized$class.head(IndexedSeqOptimized.scala:120)
scala.collection.mutable.ArrayBuffer.head(ArrayBuffer.scala:47)
org.apache.spark.mllib.recommendation.MatrixFactorizationModel.predict(MatrixFactorizationModel.scala:81)
$line78.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:74)
$line78.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:79)
$line78.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:81)
$line78.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:83)
$line78.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:85)
$line78.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:87)
$line78.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:89)
$line78.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:91)
$line78.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:93)
$line78.$read$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:95)
$line78.$read$$iwC$$iwC$$iwC$$iwC.<init>(<console>:97)
$line78.$read$$iwC$$iwC$$iwC.<init>(<console>:99)
$line78.$read$$iwC$$iwC.<init>(<console>:101)
$line78.$read$$iwC.<init>(<console>:103)
$line78.$read.<init>(<console>:105)
$line78.$read$.<init>(<console>:109)
$line78.$read$.<clinit>(<console>)
$line78.$eval$.<init>(<console>:7)
$line78.$eval$.<clinit>(<console>)
$line78.$eval.$print(<console>)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)
java.lang.reflect.Method.invoke(Method.java:507)
org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)
org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)
org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)
org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)
org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)
com.ibm.spark.interpreter.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:296)
com.ibm.spark.interpreter.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:291)
com.ibm.spark.global.StreamState$.withStreams(StreamState.scala:80)
com.ibm.spark.interpreter.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:290)
com.ibm.spark.interpreter.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:290)
com.ibm.spark.utils.TaskManager$$anonfun$add$2$$anon$1.run(TaskManager.scala:123)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1153)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
java.lang.Thread.run(Thread.java:785)

This took some debugging to figure out why I received the Exception, but
when looking at the predict() implementation, I seems to assume that there
will always be features found for the provided user and product ids:


  /** Predict the rating of one user for one product. */
  @Since(""0.8.0"")
  def predict(user: Int, product: Int): Double = {
    val userVector = userFeatures.lookup(user).head
    val productVector = productFeatures.lookup(product).head
    blas.ddot(rank, userVector, 1, productVector, 1)
  }

It would be helpful if a more useful exception was raised, e.g.

MissingUserFeatureException : ""User ID ${user} not found in model""
MissingProductFeatureException : ""Product ID ${product} not found in model""

WDYT?
"
Steve Loughran <stevel@hortonworks.com>,"Tue, 6 Dec 2016 11:54:00 +0000","Re: SPARK-18689: A proposal for priority based app scheduling
 utilizing linux cgroups.","""Hegner, Travis"" <THegner@trilliumit.com>","This is essentially what the cluster schedulers do: allow different people to submit work with different credentials and priority; cgroups & equivalent to limit granted resources to requested ones. If you have pre-emption enabled, you can even have one job kill work off the others. Spark does recognise pre-emption failures and doesn't treat it as a sign of problems in the executor, that is: it doesn't over-react.

cluster scheduling is one of the cutting edge bits of datacentre-scale computing ‚Äîif you are curious about what is state of the art, look at the Morning Paper https://blog.acolyer.org/ for coverage last week of MS and google work there. YARN, Mesos, Borg, whatever Amazon use, at scale it's not just meeting SLAs, its about how much idle CPU costs, and how expensive even a 1-2% drop in throughput would be.

I would strongly encourage you to avoid this topic, unless you want dive deep into the whole world of cluster scheduling, the debate over centralized vs decentralized, the idelogical one of ""should services ever get allocated RAM/CPU in times of low overall load?"", the challenge of swap, or more specifically, ""how do you throttle memory consumption"", as well as what to do when the IO load of a service is actually incurred on a completely different host from the one your work is running on.

There's also a fair amount of engineering work; to get a hint download the Hadoop tree and look at hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux for the cgroup support, and then hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl for the native code needed alongside this. Then consider that it's not just a matter of writing something similar, it's getting an OSS project to actually commit to maintaining such code after you provide that initial contribution.

Instead? Use a multi-user cluster scheduler and spin up different spark instances for the different workloads, with different CPU & memory limits, queue priorities, etc. Other people have done the work, written the tests, deployed it in production, met their own SLAs *and are therefore committed to maintaining this stuff*.

-Steve

On 5 Dec 2016, at 15:36, Hegner, Travis <THegner@trilliumit.com<mailto:THegner@trilliumit.com>> wrote:

My apologies, in my excitement of finding a rather simple way to accomplish the scheduling goal I have in mind, I hastily jumped straight into a technical solution, without explaining that goal, or the problem it's attempting to solve.

You are correct that I'm looking for an additional running mode for the standalone scheduler. Perhaps you could/should classify it as a different scheduler, but I don't want to give the impression that this will be as difficult to implement as most schedulers are. Initially, from a memory perspective, we would still allocate in a FIFO manner. This new scheduling mode (or new scheduler, if you'd rather) would mostly benefit any users with small-ish clusters, both on-premise and cloud based. Essentially, my end goal is to be able to run multiple *applications* simultaneously with each application having *access* to the entire core count of the cluster.

I have a very cpu intensive application that I'd like to run weekly. I have a second application that I'd like to run hourly. The hourly application is more time critical (higher priority), so I'd like it to finish in a small amount of time. If I allow the first app to run with all cores (this takes several days on my 64 core cluster), then nothing else can be executed when running with the default FIFO scheduler. All of the cores have been allocated to the first application, and it will not release them until it is finished. Dynamic allocation does not help in this case, as there is always a backlog of tasks to run until the first application is nearing the end anyway. Naturally, I could just limit the number of cores that the first application has access to, but then I have idle cpu time when the second app is not running, and that is not optimal. Secondly in that case, the second application only has access to the *leftover* cores that the first app has not allocated, and will take a considerably longer amount of time to run.

You could also imagine a scenario where a developer has a spark-shell running without specifying the number of cores they want to utilize (whether intentionally or not). As I'm sure you know, the default is to allocate the entire cluster to this application. The cores allocated to this shell are unavailable to other applications, even if they are just sitting idle while a developer is getting their environment set up to run a very big job interactively. Other developers that would like to launch interactive shells are stuck waiting for the first one to exit their shell.

My proposal would eliminate this static nature of core counts and allow as many simultaneous applications to be running as the cluster memory (still statically partitioned, at least initially) will allow. Applications could be configured with a ""cpu shares"" parameter (just an arbitrary integer relative only to other applications) which is essentially just passed through to the linux cgroup cpu.shares setting. Since each executor of an application on a given worker runs in it's own process/jvm, then that process could be easily be placed into a cgroup created and dedicated for that application.

Linux cgroups cpu.shares are pretty well documented, but the gist is that processes competing for cpu time are allocated a percentage of time equal to their share count as a percentage of all shares in that level of the cgroup hierarchy. If two applications are both scheduled on the same core with the same weight, each will get to utilize 50% of the time on that core. This is all built into the kernel, and the only thing the spark worker has to do is create a cgroup for each application, set the cpu.shares parameter, and assign the executors for that application to the new cgroup. If multiple executors are running on a single worker, for a single application, the cpu time available to that application is divided among each of those executors equally. The default for cpu.shares is that they are not limiting in any way. A process can consume all available cpu time if it would otherwise be idle anyway.


That's the issue that surfaces in google papers: should jobs get idle capacity. Current consensus is ""no"". Why not? Because you may end up writing an SLA-sensitive app which just happens to meet it's SLAs in times of light cluster load, but precisely when the cluster is busy, it suddenly slows down, leading to stress all round, in the ""why is this service suddenly unusable"" kind of stress. Instead you keep the cluster busy with low priority preemptible work, use labels to allocate specific hosts to high-SLA apps, etc.


Another benefit to passing cpu.shares directly to the kernel (as opposed to some abstraction) is that cpu share allocations are heterogeneous to all processes running on a machine. An admin could have very fine grained control over which processes get priority access to cpu time, depending on their needs.



To continue my personal example above, my long running cpu intensive application could utilize 100% of all cluster cores if they are idle. Then my time sensitive app could be launched with nine times the priority and the linux kernel would scale back the first application to 10% of all cores (completely seemlessly and automatically: no pre-emption, just fewer time slices of cpu allocated by the kernel to the first application), while the second application gets 90% of all the cores until it completes.


FWIW, it's often memory consumption that's most problematic here. If one process starts to swap, it hurts everything else. But Java isn't that good at handling limited heap/memory size; you have to spec that heap up front.


The only downside that I can think of currently is that this scheduling mode would create an increase in context switching on each host. This issue is somewhat mitigated by still statically allocating memory however, since there wouldn't typically be an exorbitant number of applications running at once.

In my opinion, this would allow the most optimal usage of cluster resources. Linux cgroups allow you to control access to more than just cpu shares. You can apply the same concept to other resources (memory, disk io). You can also set up hard limits so that an application will never get more than is allocated to it. I know that those limitations are important for some use cases involving predictability of application execution times. Eventually, this idea could be expanded to include many more of the features that cgroups provide.

Thanks again for any feedback on this idea. I hope that I have explained it a bit better now. Does anyone else can see value in it?



I'm not saying ""don't get involved in the scheduling problem""; I'm trying to show just how complex it gets in a large system. Before you begin to write a line of code, I'd recommend

-you read as much of the published work as you can, including the google and microsoft papers, Facebook's FairScheduler work, etc, etc.
-have a look at the actual code inside those schedulers whose source is public, that's YARN and Mesos.
-try using these schedulers for your own workloads.

really: scheduling work across a datacentre a complex problem that is still considered a place for cutting-edge research. Avoid unless you want to do that.

-Steve


"
Steve Loughran <stevel@hortonworks.com>,"Tue, 6 Dec 2016 12:05:47 +0000",Re: Difference between netty and netty-all,Nicholas Chammas <nicholas.chammas@gmail.com>,"Nicholas,

FYI, there's some patch for Hadoop 2.8? 2.9? to move up to Netty

https://issues.apache.org/jira/browse/HADOOP-13866
https://issues.apache.org/jira/browse/HADOOP-12854



On 5 Dec 2016, at 19:46, Nicholas Chammas <nicholas.chammas@gmail.com<mailto:nicholas.chammas@gmail.com>> wrote:

So if I'm running Spark 2.0.2 built against Hadoop 2.6, I should be running [Netty 4.0.29.Final](https://github.com/apache/spark/blob/v2.0.2/dev/deps/spark-deps-hadoop-2.6#L141<https://github.com/apache/spark/blob/553aac56bd5284e84391c05e2ef54d8bd7ad3a12/dev/deps/spark-deps-hadoop-2.6#L141>), right?

And since [the Netty PR I'm interested in](https://github.com/netty/netty/pull/5345) is tagged 4.0.37.Final, then I guess Spark 2.0.2 isn't using a version of Netty that includes that PR. This correlates with what I'm seeing in my environment (warnings related to low entropy followed by executor failures).

OK cool! Thanks for the pointers.

Nick

On Mon, Dec 5, 2016 at 2:18 PM Sean Owen <sowen@cloudera.com<mailto:sowen@cloudera.com>> wrote:
netty should be Netty 3.x. It is all but unused but I couldn't manage to get rid of it: https://issues.apache.org/jira/browse/SPARK-17875


I don't know why there's also a dependency on netty 3.x there, except to note that HADOOP-12928 covers keeping it in sync with ZK. Cutting it entirely would achieve that. Patches there welcome; anything that cuts a dependency is less traumatic than those which increment them, which is why I'm happy Hadoop had just got rid of Jackson 1.9.x entirely.

netty-all should be 4.x, actually used.


On Tue, Dec 6, 2016 at 12:54 AM Nicholas Chammas <nicholas.chammas@gmail.com<mailto:nicholas.chammas@gmail.com>> wrote:

I‚Äôm looking at the list of dependencies here:

https://github.com/apache/spark/search?l=Groff&q=netty&type=Code&utf8=%E2%9C%93

What‚Äôs the difference between netty and netty-all?

The reason I ask is because I‚Äôm looking at a Netty PR<https://github.com/netty/netty/pull/5345> and trying to figure out if Spark 2.0.2 is using a version of Netty that includes that PR or not.

Nick

‚Äã

"
Nick Pentreath <nick.pentreath@gmail.com>,"Tue, 06 Dec 2016 12:25:43 +0000","Re: unhelpful exception thrown on predict() when ALS trained model
 doesn't contain user or product?",dev@spark.apache.org,"Indeed, it's being tracked here:
https://issues.apache.org/jira/browse/SPARK-18230 though no Pr has been
opened yet.


"
chris snow <chsnow123@gmail.com>,"Tue, 6 Dec 2016 12:36:32 +0000","Re: unhelpful exception thrown on predict() when ALS trained model
 doesn't contain user or product?",dev@spark.apache.org,"Ah cool, thanks for the link!


"
Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Tue, 6 Dec 2016 14:43:43 +0100",Re: [MLLIB] RankingMetrics.precisionAt,"Sean Owen <sowen@cloudera.com>,
 ""dev@spark.apache.org"" <dev@spark.apache.org>","Thank you Sean.

Maybe I am just confused about the language. When I read that it returns
""the average precision at the first k ranking positions"" I somehow
expect there will ap@k there and a the final output would be MAP@k not
average precision at the k-th position.

I guess it is not enough sleep.


-- 
Maciej Szymkiewicz

"
"""Hegner, Travis"" <THegner@trilliumit.com>","Tue, 6 Dec 2016 15:49:27 +0000","Re: SPARK-18689: A proposal for priority based app scheduling
 utilizing linux cgroups.",Steve Loughran <stevel@hortonworks.com>,"
Steve,

I appreciate your experience and insight when dealing with large clusters at the data-center scale. I'm also well aware of the complex nature of schedulers, and that it is an area of ongoing research being done by people/companies with many more resources than I have. This might explain my apprehension in even calling this idea a *scheduler*: I wanted to avoid this exact kind of debate over what I want to accomplish. This is also why I mentioned that this idea will mostly benefit users with small clusters.

I've used many of the big named ""cluster schedulers"" (YARN, Mesos, and Kubernetes) and the main thing that they have in common is that they don't work well for my use case. Those systems are designed for large scale 1000+ node clusters, and become painful to manage in the small cluster range. Most of the tools that we've attempted to use don't work well for us, so we've written several of our own: https://github.com/trilliumit/.

It can be most easily stated by the fact that *we are not* Google, Facebook, or Amazon: we don't have a *data-center* of servers to manage, we barely have half of a rack. *We are not trying to solve the problem that you are referring to*. We are operating at a level that if we aren't meeting SLAs, then we could just buy another server to add to the cluster. I imagine that we are not alone in that fact either, I've seen that many of the questions on SO and on the user list are from others operating at a level similar to ours.

I understand that pre-emption isn't inherently a bad thing, and that these multi-node systems typically handle it gracefully. However, if idle CPU is expensive, then how much more does wasted CPU cost when a nearly complete task is pre-empted and has to be started over? Fortunately for me, that isn't a problem that I have to solve at the moment.

stances for the different workloads

See my above comment on how well these cluster schedulers work for us. I have considered the avenue of multiple spark clusters, and in reality the infrastructure we have set up would allow me to do this relatively easily. In fact, in my environment, this is a similar solution to what I'm proposing, just managed one layer up the stack and with less flexibility. I am trying to avoid this solution however because it does require more overhead and maintenance. What if I want two spark apps to run on the same cluster at the same time, sharing the available CPU capacity equally? I can't accomplish that easily with multiple spark clusters. Also, we are a 1 to 2 man operation at this point, I don't have teams of ops people to task with managing as many spark clusters as I feel like launching.


Perhaps in the use-cases you have experience with, but not currently in mine. In fact, my initial proposal is net yet changing the allocation of memory as a resource. This would still be statically allocated in a FIFO manner as long as memory is available on the cluster, the same way it is now.


Thanks for the suggestion, but I will choose how I spend my time. If I can find a simple solution to a problem that I face, and I'm willing to share that solution, I'd hope one would encourage that instead.


Perhaps I haven't yet clearly communicated what I'm trying to do. In short, *I am not trying to write a scheduler*: I am trying to slightly (and optionally) tweak the way executors are allocated and launched, so that I can more intuitively and more optimally utilize my small spark cluster.

Thanks,

Travis

________________________________
From: Steve Loughran <stevel@hortonworks.com>
Sent: Tuesday, December 6, 2016 06:54
To: Hegner, Travis
Cc: Shuai Lin; Apache Spark Dev
Subject: Re: SPARK-18689: A proposal for priority based app scheduling utilizing linux cgroups.

This is essentially what the cluster schedulers do: allow different people to submit work with different credentials and priority; cgroups & equivalent to limit granted resources to requested ones. If you have pre-emption enabled, you can even have one job kill work off the others. Spark does recognise pre-emption failures and doesn't treat it as a sign of problems in the executor, that is: it doesn't over-react.

cluster scheduling is one of the cutting edge bits of datacentre-scale computing óif you are curious about what is state of the art, look at the Morning Paper https://blog.acolyer.org/ for coverage last week of MS and google work there. YARN, Mesos, Borg, whatever Amazon use, at scale it's not just meeting SLAs, its about how much idle CPU costs, and how expensive even a 1-2% drop in throughput would be.


I would strongly encourage you to avoid this topic, unless you want dive deep into the whole world of cluster scheduling, the debate over centralized vs decentralized, the idelogical one of ""should services ever get allocated RAM/CPU in times of low overall load?"", the challenge of swap, or more specifically, ""how do you throttle memory consumption"", as well as what to do when the IO load of a service is actually incurred on a completely different host from the one your work is running on.

There's also a fair amount of engineering work; to get a hint download the Hadoop tree and look at hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux for the cgroup support, and then hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl for the native code needed alongside this. Then consider that it's not just a matter of writing something similar, it's getting an OSS project to actually commit to maintaining such code after you provide that initial contribution.

Instead? Use a multi-user cluster scheduler and spin up different spark instances for the different workloads, with different CPU & memory limits, queue priorities, etc. Other people have done the work, written the tests, deployed it in production, met their own SLAs *and are therefore committed to maintaining this stuff*.

-Steve


My apologies, in my excitement of finding a rather simple way to accomplish the scheduling goal I have in mind, I hastily jumped straight into a technical solution, without explaining that goal, or the problem it's attempting to solve.

You are correct that I'm looking for an additional running mode for the standalone scheduler. Perhaps you could/should classify it as a different scheduler, but I don't want to give the impression that this will be as difficult to implement as most schedulers are. Initially, from a memory perspective, we would still allocate in a FIFO manner. This new scheduling mode (or new scheduler, if you'd rather) would mostly benefit any users with small-ish clusters, both on-premise and cloud based. Essentially, my end goal is to be able to run multiple *applications* simultaneously with each application having *access* to the entire core count of the cluster.

I have a very cpu intensive application that I'd like to run weekly. I have a second application that I'd like to run hourly. The hourly application is more time critical (higher priority), so I'd like it to finish in a small amount of time. If I allow the first app to run with all cores (this takes several days on my 64 core cluster), then nothing else can be executed when running with the default FIFO scheduler. All of the cores have been allocated to the first application, and it will not release them until it is finished. Dynamic allocation does not help in this case, as there is always a backlog of tasks to run until the first application is nearing the end anyway. Naturally, I could just limit the number of cores that the first application has access to, but then I have idle cpu time when the second app is not running, and that is not optimal. Secondly in that case, the second application only has access to the *leftover* cores that the first app has not allocated, and will take a considerably longer amount of time to run.

You could also imagine a scenario where a developer has a spark-shell running without specifying the number of cores they want to utilize (whether intentionally or not). As I'm sure you know, the default is to allocate the entire cluster to this application. The cores allocated to this shell are unavailable to other applications, even if they are just sitting idle while a developer is getting their environment set up to run a very big job interactively. Other developers that would like to launch interactive shells are stuck waiting for the first one to exit their shell.

My proposal would eliminate this static nature of core counts and allow as many simultaneous applications to be running as the cluster memory (still statically partitioned, at least initially) will allow. Applications could be configured with a ""cpu shares"" parameter (just an arbitrary integer relative only to other applications) which is essentially just passed through to the linux cgroup cpu.shares setting. Since each executor of an application on a given worker runs in it's own process/jvm, then that process could be easily be placed into a cgroup created and dedicated for that application.

Linux cgroups cpu.shares are pretty well documented, but the gist is that processes competing for cpu time are allocated a percentage of time equal to their share count as a percentage of all shares in that level of the cgroup hierarchy. If two applications are both scheduled on the same core with the same weight, each will get to utilize 50% of the time on that core. This is all built into the kernel, and the only thing the spark worker has to do is create a cgroup for each application, set the cpu.shares parameter, and assign the executors for that application to the new cgroup. If multiple executors are running on a single worker, for a single application, the cpu time available to that application is divided among each of those executors equally. The default for cpu.shares is that they are not limiting in any way. A process can consume all available cpu time if it would otherwise be idle anyway.


That's the issue that surfaces in google papers: should jobs get idle capacity. Current consensus is ""no"". Why not? Because you may end up writing an SLA-sensitive app which just happens to meet it's SLAs in times of light cluster load, but precisely when the cluster is busy, it suddenly slows down, leading to stress all round, in the ""why is this service suddenly unusable"" kind of stress. Instead you keep the cluster busy with low priority preemptible work, use labels to allocate specific hosts to high-SLA apps, etc.


Another benefit to passing cpu.shares directly to the kernel (as opposed to some abstraction) is that cpu share allocations are heterogeneous to all processes running on a machine. An admin could have very fine grained control over which processes get priority access to cpu time, depending on their needs.



To continue my personal example above, my long running cpu intensive application could utilize 100% of all cluster cores if they are idle. Then my time sensitive app could be launched with nine times the priority and the linux kernel would scale back the first application to 10% of all cores (completely seemlessly and automatically: no pre-emption, just fewer time slices of cpu allocated by the kernel to the first application), while the second application gets 90% of all the cores until it completes.


FWIW, it's often memory consumption that's most problematic here. If one process starts to swap, it hurts everything else. But Java isn't that good at handling limited heap/memory size; you have to spec that heap up front.


The only downside that I can think of currently is that this scheduling mode would create an increase in context switching on each host. This issue is somewhat mitigated by still statically allocating memory however, since there wouldn't typically be an exorbitant number of applications running at once.

In my opinion, this would allow the most optimal usage of cluster resources. Linux cgroups allow you to control access to more than just cpu shares. You can apply the same concept to other resources (memory, disk io). You can also set up hard limits so that an application will never get more than is allocated to it. I know that those limitations are important for some use cases involving predictability of application execution times. Eventually, this idea could be expanded to include many more of the features that cgroups provide.

Thanks again for any feedback on this idea. I hope that I have explained it a bit better now. Does anyone else can see value in it?



I'm not saying ""don't get involved in the scheduling problem""; I'm trying to show just how complex it gets in a large system. Before you begin to write a line of code, I'd recommend

-you read as much of the published work as you can, including the google and microsoft papers, Facebook's FairScheduler work, etc, etc.
-have a look at the actual code inside those schedulers whose source is public, that's YARN and Mesos.
-try using these schedulers for your own workloads.

really: scheduling work across a datacentre a complex problem that is still considered a place for cutting-edge research. Avoid unless you want to do that.

-Steve


"
Jakob Odersky <jakob@odersky.com>,"Tue, 6 Dec 2016 10:07:03 -0800",Re: Can I add a new method to RDD class?,Teng Long <longteng.cq@gmail.com>,"Yes, I think changing the <version> property (line 29) in spark's root
pom.xml should be sufficient. However, keep in mind that you'll also
need to publish spark locally before you can access it in your test
application.

e I
ng
‚Äúcannot"" add

---------------------------------------------------------------------


"
Saikat Kanjilal <sxk1969@hotmail.com>,"Tue, 6 Dec 2016 18:17:33 +0000","Re: Spark-9487, Need some insight",Reynold Xin <rxin@databricks.com>,"Well  other than making the code consistent whats the high level goal in doing this and why does it matter so much how many workers we have in different scenarios (pyspark versus different components of spark).  I'm ok not making the change and working on something else to be honest but spending hours troubleshooting issues in a local dev environment that doesnt resemble jenkins closely enough is not a productive use of time.  Would love to get input on next logical steps.


________________________________
From: Reynold Xin <rxin@databricks.com>
Sent: Monday, December 5, 2016 6:44 PM
To: Saikat Kanjilal
Cc: dev@spark.apache.org
Subject: Re: Spark-9487, Need some insight

Honestly it is pretty difficult. Given the difficulty, would it still make sense to do that change? (the one that sets the same number of workers/parallelism across different languages in testing)



Hello again dev community,

Ping on this, apologies for rerunning this thread but never heard from anyone, based on this link:  https://wiki.jenkins-ci.org/display/JENKINS/Installing+Jenkins  I can try to install jenkins locally but is that really needed?


Thanks in advance.


________________________________
From: Saikat Kanjilal <sxk1969@hotmail.com<mailto:sxk1969@hotmail.com>>
Sent: Tuesday, November 29, 2016 8:14 PM
To: dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Spark-9487, Need some insight


Hello Spark dev community,

I took this the following jira item (https://github.com/apache/spark/pull/15848) and am looking for some general pointers, it seems that I am running into issues where things work successfully doing local development on my macbook pro but fail on jenkins for a multitiude of reasons and errors, here's an example,  if you see this build output report: https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/69297/ you will see the DataFrameStatSuite, now locally I am running these individual tests with this command: ./build/mvn test -P... -DwildcardSuites=none -Dtest=org.apache.spark.sql.DataFrameStatSuite.     It seems that I need to emulate a jenkins like environment locally, this seems sort of like an untenable hurdle, granted that my changes involve changing the total number of workers in the sparkcontext and if so should I be testing my changes in an environment that more closely resembles jenkins.  I really want to work on/complete this PR but I keep getting hamstrung by a dev environment that is not equivalent to our CI environment.



I'm guessing/hoping I'm not the first one to run into this so some insights. pointers to get past this would be very appreciated , would love to keep contributing and hoping this is a hurdle that's overcomeable with some tweaks to my dev environment.



Thanks in advance.

"
Michael Gummelt <mgummelt@mesosphere.io>,"Tue, 6 Dec 2016 11:31:36 -0800",Re: driver in queued state and not started,Yu Wei <yu2003w@hotmail.com>,"Client mode or cluster mode?





-- 
Michael Gummelt
Software Engineer
Mesosphere
"
Mr rty ff <yasha32@yahoo.com.INVALID>,"Tue, 6 Dec 2016 20:23:46 +0000 (UTC)",Seeing bytecode of each task ececuted.,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi
If there are some way to see the bytecode in each task that is executed by spark.
Thanks

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Tue, 06 Dec 2016 20:52:10 +0000",Re: [MLLIB] RankingMetrics.precisionAt,"Maciej Szymkiewicz <mszymkiewicz@gmail.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","As I understand, this might best be called ""mean precision@k"", not ""mean
average precision, up to k"".


"
Teng Long <longteng.cq@gmail.com>,"Tue, 6 Dec 2016 16:02:16 -0500",Re: Can I add a new method to RDD class?,"""Jakob Odersky-2 [via Apache Spark Developers List]"" <ml-node+s1001551n20151h82@n3.nabble.com>,
 dev@spark.apache.org","Hi Jakob, 

It seems like I‚Äôll have to either replace the version with my custom version in all the pom.xml files in every subdirectory that has one and publish locally, or keep the version (i.e. 2.0.2) and manually remove the spark repository cache in ~/.ivy2 and ~/.m2 and publish spark locally, then compile my application with the correct version respectively to make it work. I think there has to be an elegant way to do this. 



since I 
along using 
why I ‚Äúcannot"" add 
the 


<x-msg://50/user/SendEmail.jtp?type=node&node=20151&i=2> 
discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-add-a-new-method-to-RDD-class-tp20100p20151.html <http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-add-a-new-method-to-RDD-class-tp20100p20151.html>
<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=20100&code=bG9uZ3RlbmcuY3FAZ21haWwuY29tfDIwMTAwfC0xNzQ1MzUzNzE=>.
<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
"
Teng Long <longteng.cq@gmail.com>,"Tue, 6 Dec 2016 14:02:21 -0700 (MST)",Re: Can I add a new method to RDD class?,dev@spark.apache.org,"Hi Jakob, 

It seems like I‚Äôll have to either replace the version with my custom version in all the pom.xml files in every subdirectory that has one and publish locally, or keep the version (i.e. 2.0.2) and manually remove the spark repository cache in ~/.ivy2 and ~/.m2 and publish spark locally, then compile my application with the correct version respectively to make it work. I think there has to be an elegant way to do this. 

nce I 
sing 
 ‚Äúcannot"" add 
 
=node&node=20151&i=2> 
below:
-method-to-RDD-class-tp20100p20151.html <http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-add-a-new-method-to-RDD-class-tp20100p20151.html>
p://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=20100&code=bG9uZ3RlbmcuY3FAZ21haWwuY29tfDIwMTAwfC0xNzQ1MzUzNzE=>.
NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>




--
3.nabble.com/Can-I-add-a-new-method-to-RDD-class-tp20100p20157.html
om."
Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Wed, 7 Dec 2016 01:02:33 +0100",Re: [MLLIB] RankingMetrics.precisionAt,"Sean Owen <sowen@cloudera.com>,
 ""dev@spark.apache.org"" <dev@spark.apache.org>","This sounds much better.

Follow up question is if we should provide MAP@k, which I believe is
wider used metric.



-- 
Maciej Szymkiewicz

"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 07 Dec 2016 02:11:47 +0000",Reduce memory usage of UnsafeInMemorySorter,Spark dev list <dev@spark.apache.org>,"[Re-titling thread.]

OK, I see that the exception from my original email is being triggered from
this part of UnsafeInMemorySorter:

https://github.com/apache/spark/blob/v2.0.2/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java#L209-L212

So I can ask a more refined question now: How can I ensure that
UnsafeInMemorySorter has room to insert new records? In other words, how
can I ensure that hasSpaceForAnotherRecord() returns a true value?

Do I need:

   - More, smaller partitions?
   - More memory per executor?
   - Some Java or Spark option enabled?
   - etc.

I‚Äôm running Spark 2.0.2 on Java 7 and YARN. Would Java 8 help here?
(Unfortunately, I cannot upgrade at this time, but it would be good to know
regardless.)

This is morphing into a user-list question, so accept my apologies. Since I
can‚Äôt find any information anywhere else about this, and the question is
about internals like UnsafeInMemorySorter, I hope this is OK here.

Nick


I was testing out a new project at scale on Spark 2.0.2 running on YARN,
): java.lang.IllegalStateException: There is no space for new record
MemorySorter.insertRecord(UnsafeInMemorySorter.java:211)
.<init>(UnsafeKVExternalSorter.java:127)
ationMap.destructAndCreateExternalSorter(UnsafeFixedWidthAggregationMap.java:244)
ass$GeneratedIterator.agg_doAggregateWithKeys$(Unknown Source)
ass$GeneratedIterator.processNext(Unknown Source)
sNext(BufferedRowIterator.java:43)
anonfun$8$anon$1.hasNext(WholeStageCodegenExec.scala:370)
ala:408)
riter.write(BypassMergeSortShuffleWriter.java:125)
ffleMapTask.scala:79)
ffleMapTask.scala:47)
cutor.scala:274)
eadPoolExecutor.java:1145)
readPoolExecutor.java:615)
sn‚Äôt
,
‚Äã
"
Reynold Xin <rxin@databricks.com>,"Tue, 6 Dec 2016 21:26:03 -0800",Re: Reduce memory usage of UnsafeInMemorySorter,Nicholas Chammas <nicholas.chammas@gmail.com>,"This is not supposed to happen. Do you have a repro?


m

e?
ow
estion is
m
e): java.lang.IllegalStateException: There is no space for new record
nMemorySorter.insertRecord(UnsafeInMemorySorter.java:211)
r.<init>(UnsafeKVExternalSorter.java:127)
gationMap.destructAndCreateExternalSorter(UnsafeFixedWidthAggregationMap.java:244)
lass$GeneratedIterator.agg_doAggregateWithKeys$(Unknown Source)
lass$GeneratedIterator.processNext(Unknown Source)
asNext(BufferedRowIterator.java:43)
$anonfun$8$anon$1.hasNext(WholeStageCodegenExec.scala:370)
cala:408)
Writer.write(BypassMergeSortShuffleWriter.java:125)
uffleMapTask.scala:79)
uffleMapTask.scala:47)
ecutor.scala:274)
readPoolExecutor.java:1145)
hreadPoolExecutor.java:615)
esn‚Äôt
"
Mr rty ff <yasha32@yahoo.com.INVALID>,"Wed, 7 Dec 2016 08:53:31 +0000 (UTC)",Re: Seeing bytecode of each task ececuted.,"Mr rty ff <yasha32@yahoo.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","If not the bytecode the code in each task executed will suffice.



Hi
If there are some way to see the bytecode in each task that is executed by spark.
Thanks

---------------------------------------------------------------------

---------------------------------------------------------------------


"
"""Kazuaki Ishizaki"" <ISHIZAKI@jp.ibm.com>","Wed, 7 Dec 2016 21:31:25 +0900",Re: Reduce memory usage of UnsafeInMemorySorter,"Reynold Xin <rxin@databricks.com>,
        Nicholas Chammas
 <nicholas.chammas@gmail.com>","I do not have a repro, too.
But, when I took a quick browse at the file 'UnsafeInMemorySort.java', I 
am afraid about the similar cast issue like 
https://issues.apache.org/jira/browse/SPARK-18458 at the following line.
https://github.com/apache/spark/blob/master/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java#L156

Regards,
Kazuaki Ishizaki



From:   Reynold Xin <rxin@databricks.com>
To:     Nicholas Chammas <nicholas.chammas@gmail.com>
Cc:     Spark dev list <dev@spark.apache.org>
Date:   2016/12/07 14:27
Subject:        Re: Reduce memory usage of UnsafeInMemorySorter



This is not supposed to happen. Do you have a repro?


On Tue, Dec 6, 2016 at 6:11 PM, Nicholas Chammas <
nicholas.chammas@gmail.com> wrote:
[Re-titling thread.]
OK, I see that the exception from my original email is being triggered 
from this part of UnsafeInMemorySorter:
https://github.com/apache/spark/blob/v2.0.2/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java#L209-L212
So I can ask a more refined question now: How can I ensure that 
UnsafeInMemorySorter has room to insert new records? In other words, how 
can I ensure that hasSpaceForAnotherRecord() returns a true value?
Do I need:
More, smaller partitions?
More memory per executor?
Some Java or Spark option enabled?
etc.
I‚Äôm running Spark 2.0.2 on Java 7 and YARN. Would Java 8 help here? 
(Unfortunately, I cannot upgrade at this time, but it would be good to 
know regardless.)
This is morphing into a user-list question, so accept my apologies. Since 
I can‚Äôt find any information anywhere else about this, and the question 
is about internals like UnsafeInMemorySorter, I hope this is OK here.
Nick
On Mon, Dec 5, 2016 at 9:11 AM Nicholas Chammas nicholas.chammas@gmail.com 
wrote:
I was testing out a new project at scale on Spark 2.0.2 running on YARN, 
and my job failed with an interesting error message:
TaskSetManager: Lost task 37.3 in stage 31.0 (TID 10684, server.host.name
): java.lang.IllegalStateException: There is no space for new record
05:27:09.573     at 
org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter.insertRecord(UnsafeInMemorySorter.java:211)
05:27:09.574     at 
org.apache.spark.sql.execution.UnsafeKVExternalSorter.<init>(UnsafeKVExternalSorter.java:127)
05:27:09.574     at 
org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap.destructAndCreateExternalSorter(UnsafeFixedWidthAggregationMap.java:244)
05:27:09.575     at 
org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithKeys$(Unknown 
Source)
05:27:09.575     at 
org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown 
Source)
05:27:09.576     at 
org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
05:27:09.576     at 
org.apache.spark.sql.execution.WholeStageCodegenExec$anonfun$8$anon$1.hasNext(WholeStageCodegenExec.scala:370)
05:27:09.577     at 
scala.collection.Iterator$anon$11.hasNext(Iterator.scala:408)
05:27:09.577     at 
org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
05:27:09.577     at 
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
05:27:09.578     at 
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
05:27:09.578     at org.apache.spark.scheduler.Task.run(Task.scala:86)
05:27:09.578     at 
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
05:27:09.579     at 
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
05:27:09.579     at 
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
05:27:09.579     at java.lang.Thread.run(Thread.java:745)

I‚Äôve never seen this before, and searching on Google/DDG/JIRA doesn‚Äôt 
yield any results. There are no other errors coming from that executor, 
whether related to memory, storage space, or otherwise.
Could this be a bug? If so, how would I narrow down the source? Otherwise, 
how might I work around the issue?
Nick
‚Äã
‚Äã



"
Teng Long <longteng.cq@gmail.com>,"Wed, 7 Dec 2016 14:31:35 -0500",Re: Can I add a new method to RDD class?,"dev@spark.apache.org, Holden Karau <holden@pigscanfly.ca>","Hi Holden,

Can you please tell me how to edit version numbers efficiently? the correct way? I'm really struggling with this and don't know where to look.

Thanks,
Teng


ersion in all the pom.xml files in every subdirectory that has one and publish locally, or keep the version (i.e. 2.0.2) and manually remove the spark repository cache in ~/.ivy2 and ~/.m2 and publish spark locally, then compile my application with the correct version respectively to make it work. I think there has to be an elegant way to do this. 

dEmail.jtp?type=node&amp;node=20151&amp;i=0"" target=""_top"" rel=""nonce I 
sing 
∞cannot"" add 

endEmail.jtp?type=node&amp;node=20151&amp;i=1"" target=""_top"" rel=""de&amp;node=20151&amp;i=2"" target=""_top"" rel=""nofollow"" link=""external"" class="""">[hidden email] 
elow:
-method-to-RDD-class-tp20100p20151.html
com.
"
Mark Hamstra <mark@clearstorydata.com>,"Wed, 7 Dec 2016 11:49:29 -0800",Re: Can I add a new method to RDD class?,Teng Long <longteng.cq@gmail.com>,"The easiest way is probably with:

mvn versions:set -DnewVersion=your_new_version


.
tom
en
 ‚Äúcannot""
-
ervlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
w-method-to-RDD-class-tp20100p20157.html>
m
"
Teng Long <longteng.cq@gmail.com>,"Wed, 7 Dec 2016 15:03:40 -0500",Re: Can I add a new method to RDD class?,"""Mark Hamstra [via Apache Spark Developers List]"" <ml-node+s1001551n20164h85@n3.nabble.com>,
 dev@spark.apache.org","Thank you so much, Mark and everyone else responded for putting up with my ignorance.

correct way? I'm really struggling with this and don't know where to look.
custom version in all the pom.xml files in every subdirectory that has one and publish locally, or keep the version (i.e. 2.0.2) and manually remove the spark repository cache in ~/.ivy2 and ~/.m2 and publish spark locally, then compile my application with the correct version respectively to make it work. I think there has to be an elegant way to do this. 
Developers List] <[hidden email] root 


href=""x-msg://50/user/SendEmail.jtp?type=node&amp;node=20151&amp;i=0"" target=""_top"" rel=""nofollow"" link=""external"" class="""">[hidden build since I 
along using 
why I ‚Äúcannot"" add 
the 
href=""x-msg://50/user/SendEmail.jtp?type=node&amp;node=20151&amp;i=1"" target=""_top"" rel=""nofollow"" link=""external"" class="""">[hidden --------------------------------------------------------------------- 
href=""x-msg://50/user/SendEmail.jtp?type=node&amp;node=20151&amp;i=2"" target=""_top"" rel=""nofollow"" link=""external"" class="""">[hidden email] 
discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-add-a-new-method-to-RDD-class-tp20100p20151.html <http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-add-a-new-method-to-RDD-class-tp20100p20151.html>
<>.
<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
class? <http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-add-a-new-method-to-RDD-class-tp20100p20157.html>
<http://apache-spark-developers-list.1001551.n3.nabble.com/> at Nabble.com <http://nabble.com/>.
discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-add-a-new-method-to-RDD-class-tp20100p20164.html <http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-add-a-new-method-to-RDD-class-tp20100p20164.html>
<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=20100&code=bG9uZ3RlbmcuY3FAZ21haWwuY29tfDIwMTAwfC0xNzQ1MzUzNzE=>.
<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
"
Teng Long <longteng.cq@gmail.com>,"Wed, 7 Dec 2016 13:04:01 -0700 (MST)",Re: Can I add a new method to RDD class?,dev@spark.apache.org,"Thank you so much, Mark and everyone else responded for putting up with my ignorance.

ct way? I'm really struggling with this and don't know where to look.
stom version in all the pom.xml files in every subdirectory that has one and publish locally, or keep the version (i.e. 2.0.2) and manually remove the spark repository cache in ~/.ivy2 and ~/.m2 and publish spark locally, then compile my application with the correct version respectively to make it work. I think there has to be an elegant way to do this. 
s List] <[hidden email] <http://user/SendEmail.jtp?type=node&node=20157 
endEmail.jtp?type=node&amp;node=20151&amp;i=0"" target=""_top"" rel=since I 
 using 
 I ‚Äúcannot"" add 
 
/SendEmail.jtp?type=node&amp;node=20151&amp;i=1"" target=""_top"" rel 
node&amp;node=20151&amp;i=2"" target=""_top"" rel=""nofollow"" link=""external"" class="""">[hidden email] 
n below:
ew-method-to-RDD-class-tp20100p20151.html <http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-add-a-new-method-to-RDD-class-tp20100p20151.html>
.
e/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-add-a-new-method-to-RDD-class-tp20100p20157.html>
apache-spark-developers-list.1001551.n3.nabble.com/> at Nabble.com <http://nabble.com/>.
below:
-method-to-RDD-class-tp20100p20164.html <http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-add-a-new-method-to-RDD-class-tp20100p20164.html>
p://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=20100&code=bG9uZ3RlbmcuY3FAZ21haWwuY29tfDIwMTAwfC0xNzQ1MzUzNzE=>.
NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>




--
3.nabble.com/Can-I-add-a-new-method-to-RDD-class-tp20100p20165.html
om."
Jencir Lee <jencirl@uci.edu>,"Tue, 6 Dec 2016 17:53:28 +0000",Publishing of the Spectral LDA model on Spark Packages,"dev@spark.apache.org, user@spark.apache.org","Hello,

We just published the Spectral LDA model on Spark Packages. It‚Äôs an alternative approach to the LDA modelling based on tensor decompositions. We first build the 2nd, 3rd-moment tensors from empirical word counts, then orthogonalise them and perform decomposition on the 3rd-moment tensor. The convergence is guaranteed by theory, in contrast to most current approaches. We achieve comparable log-perplexity in much shorter running time. 

You could find the package at 

https://spark-packages.org/package/FurongHuang/SpectralLDA-TensorSpark <https://spark-packages.org/package/FurongHuang/SpectralLDA-TensorSpark>


We‚Äôd welcome any thoughts or feedback on it.

Thanks very much,

Furong Huang
Jencir Lee
Anima Anandkumar"
harini <philly.harini@gmail.com>,"Wed, 7 Dec 2016 16:00:23 -0700 (MST)",modifications to ALS.scala,dev@spark.apache.org,"Hi all,I am trying to implement ALS with a slightly modified objective
function, which will require minor changes to fit -> train -> computeFactors
within  ALS.scala
<https://github.com/apache/spark/blob/v1.6.2/mllib/src/main/scala/org/apache/spark/ml/recommendation/ALS.scala>  
- Is there a way to do this without having to build spark in its entirety? 



--"
Georg Heiler <georg.kf.heiler@gmail.com>,"Wed, 07 Dec 2016 23:10:38 +0000",Re: modifications to ALS.scala,"harini <philly.harini@gmail.com>, dev@spark.apache.org","What about putting a custom als implementation into sparks name space?
harini <philly.harini@gmail.com> schrieb am Do. 8. Dez. 2016 um 00:01:

"
harini <philly.harini@gmail.com>,"Wed, 7 Dec 2016 16:23:21 -0700 (MST)",Re: modifications to ALS.scala,dev@spark.apache.org,"I am new to development with spark, how do I do that? Can I write up a custom
implementation under package org.apache.spark.ml.recommendation, and specify
""spark-mllib"" along with others as a library dependency?



--

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 08 Dec 2016 01:55:02 +0000",Re: Reduce memory usage of UnsafeInMemorySorter,"Kazuaki Ishizaki <ISHIZAKI@jp.ibm.com>, Reynold Xin <rxin@databricks.com>","Unfortunately, I don't have a repro, and I'm only seeing this at scale. But
I was able to get around the issue by fiddling with the distribution of my
data before asking GraphFrames to process it. (I think that's where the
error was being thrown from.)

:

/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java#L156
e/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java#L209-L212*
e/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java#L209-L212>
here?
 know
nd the
rtRecord(UnsafeInMemorySorter.java:211)
xternalSorter.java:127)
AndCreateExternalSorter(UnsafeFixedWidthAggregationMap.java:244)
ator.agg_doAggregateWithKeys$(Unknown
ator.processNext(Unknown
Iterator.java:43)
hasNext(WholeStageCodegenExec.scala:370)
sMergeSortShuffleWriter.java:125)
:79)
:47)
ava:1145)
java:615)
doesn‚Äôt
,
"
Reynold Xin <rxin@databricks.com>,"Wed, 7 Dec 2016 23:20:17 -0800","2.1.0-rc2 cut; committers please set fix version for branch-2.1 to
 2.1.1 instead","""dev@spark.apache.org"" <dev@spark.apache.org>","Thanks.
"
Nick Pentreath <nick.pentreath@gmail.com>,"Thu, 08 Dec 2016 07:34:12 +0000","Re: 2.1.0-rc2 cut; committers please set fix version for branch-2.1
 to 2.1.1 instead","""dev@spark.apache.org"" <dev@spark.apache.org>","I went ahead and re-marked all the existing 2.1.1 fix version JIRAs (that
had gone into branch-2.1 since RC1 but before RC2) for Spark ML to 2.1.0


"
Georg Heiler <georg.kf.heiler@gmail.com>,"Thu, 08 Dec 2016 08:32:03 +0000",Re: modifications to ALS.scala,"harini <philly.harini@gmail.com>, dev@spark.apache.org","You can write some code e.g. A custom estimator transformer in sparks
namespace.
http://stackoverflow.com/a/40785438/2587904 might help you get started.
Be aware that using private e.g. Spark internal api might be subjected to
change from release to release.

You definitely will require spark -mllib dependency.

Currently for my usage I was not required to build a separate version of
mllib.
harini <philly.harini@gmail.com> schrieb am Do. 8. Dez. 2016 um 00:23:

"
Moriarty <zhouyf0621@dtdream.com>,"Thu, 8 Dec 2016 01:24:22 -0700 (MST)",[Spark SQL]: How does Spark HiveThriftServer handle idle sessions ?,dev@spark.apache.org,"In org.apache.spark.sql.hive.thriftserver.SparkSQLSessionManager.init(),
SparkSQLSessionManager inits ‚ÄúbackgroundOperationPool‚Äù by creating a
ThreadPool directly instead of invoking
super.createBackgroundOperationPool(). 

This results in the idle-session-check-thread in
org.apache.hive.service.cli.session.SessionManager not created.

So does Spark HiveThriftServer have any mechanism to clean idle sessions, or
it is a bug that we should fix ?



--
3.nabble.com/Spark-SQL-How-does-Spark-HiveThriftServer-handle-idle-sessions-tp20173.html
om.

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Thu, 8 Dec 2016 00:39:52 -0800",[VOTE] Apache Spark 2.1.0 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version
2.1.0. The vote is open until Sun, December 11, 2016 at 1:00 PT and passes
if a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 2.1.0
[ ] -1 Do not release this package because ...


To learn more about Apache Spark, please see http://spark.apache.org/

The tag to be voted on is v2.1.0-rc2
(080717497365b83bc202ab16812ced93eb1ea7bd)

List of JIRA tickets resolved are:
https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20fixVersion%20%3D%202.1.0

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.1.0-rc2-bin/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1217

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.1.0-rc2-docs/


(Note that the docs and staging repo are still being uploaded and will be
available soon)


=======================================
How can I help test this release?
=======================================
If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions.

===============================================================
What should happen to JIRA tickets still targeting 2.1.0?
===============================================================
Committers should look at those and triage. Extremely important bug fixes,
documentation, and API tweaks that impact compatibility should be worked on
immediately. Everything else please retarget to 2.1.1 or 2.2.0.
"
Reynold Xin <rxin@databricks.com>,"Thu, 8 Dec 2016 00:42:23 -0800",Re: [VOTE] Apache Spark 2.1.0 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","This vote is closed in favor of rc2.



"
=?UTF-8?Q?Fran=C3=A7ois_Garillot?= <francois@garillot.net>,"Thu, 08 Dec 2016 10:32:14 +0000",Re: Publishing of the Spectral LDA model on Spark Packages,"Jencir Lee <jencirl@uci.edu>, dev@spark.apache.org, user@spark.apache.org","This is very cool ! Thanks a lot for making this more accessible !

Best,
-- 
FG


an
en
e
"
"""Kazuaki Ishizaki"" <ISHIZAKI@jp.ibm.com>","Fri, 9 Dec 2016 01:49:42 +0900",Re: Reduce memory usage of UnsafeInMemorySorter,Nicholas Chammas <nicholas.chammas@gmail.com>,"The line where I pointed out would work correctly. This is because a type 
of this division is double. d2i correctly handles overflow cases.

Kazuaki Ishizaki



From:   Nicholas Chammas <nicholas.chammas@gmail.com>
To:     Kazuaki Ishizaki/Japan/IBM@IBMJP, Reynold Xin 
<rxin@databricks.com>
Cc:     Spark dev list <dev@spark.apache.org>
Date:   2016/12/08 10:56
Subject:        Re: Reduce memory usage of UnsafeInMemorySorter



Unfortunately, I don't have a repro, and I'm only seeing this at scale. 
But I was able to get around the issue by fiddling with the distribution 
of my data before asking GraphFrames to process it. (I think that's where 
the error was being thrown from.)

I do not have a repro, too.
But, when I took a quick browse at the file 'UnsafeInMemorySort.java', I 
am afraid about the similar cast issue like 
https://issues.apache.org/jira/browse/SPARK-18458at the following line.
https://github.com/apache/spark/blob/master/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java#L156


Regards,
Kazuaki Ishizaki



From:        Reynold Xin <rxin@databricks.com>
To:        Nicholas Chammas <nicholas.chammas@gmail.com>
Cc:        Spark dev list <dev@spark.apache.org>
Date:        2016/12/07 14:27
Subject:        Re: Reduce memory usage of UnsafeInMemorySorter



This is not supposed to happen. Do you have a repro?


[Re-titling thread.]
OK, I see that the exception from my original email is being triggered 
from this part of UnsafeInMemorySorter:
https://github.com/apache/spark/blob/v2.0.2/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java#L209-L212

So I can ask a more refined question now: How can I ensure that 
UnsafeInMemorySorterhas room to insert new records? In other words, how 
can I ensure that hasSpaceForAnotherRecord()returns a true value?
Do I need:
More, smaller partitions?
More memory per executor?
Some Java or Spark option enabled?
etc.
I$B!G(Bm running Spark 2.0.2 on Java 7 and YARN. Would Java 8 help here? 
(Unfortunately, I cannot upgrade at this time, but it would be good to 
know regardless.)
This is morphing into a user-list question, so accept my apologies. Since 
I can$B!G(Bt find any information anywhere else about this, and the question 
is about internals like UnsafeInMemorySorter, I hope this is OK here.
Nick
I was testing out a new project at scale on Spark 2.0.2 running on YARN, 
and my job failed with an interesting error message:
TaskSetManager: Lost task 37.3 in stage 31.0 (TID 10684, server.host.name
): java.lang.IllegalStateException: There is no space for new record
05:27:09.573     at 
org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter.insertRecord(UnsafeInMemorySorter.java:211)
05:27:09.574     at 
org.apache.spark.sql.execution.UnsafeKVExternalSorter.<init>(UnsafeKVExternalSorter.java:127)
05:27:09.574     at 
org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap.destructAndCreateExternalSorter(UnsafeFixedWidthAggregationMap.java:244)
05:27:09.575     at 
org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithKeys$(Unknown 
Source)
05:27:09.575     at 
org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown 
Source)
05:27:09.576     at 
org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
05:27:09.576     at 
org.apache.spark.sql.execution.WholeStageCodegenExec$anonfun$8$anon$1.hasNext(WholeStageCodegenExec.scala:370)
05:27:09.577     at 
scala.collection.Iterator$anon$11.hasNext(Iterator.scala:408)
05:27:09.577     at 
org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
05:27:09.577     at 
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
05:27:09.578     at 
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
05:27:09.578     at org.apache.spark.scheduler.Task.run(Task.scala:86)
05:27:09.578     at 
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
05:27:09.579     at 
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
05:27:09.579     at 
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
05:27:09.579     at java.lang.Thread.run(Thread.java:745)

I$B!G(Bve never seen this before, and searching on Google/DDG/JIRA doesn$B!G(Bt 
yield any results. There are no other errors coming from that executor, 
whether related to memory, storage space, or otherwise.
Could this be a bug? If so, how would I narrow down the source? Otherwise, 
how might I work around the issue?
Nick
?
?




"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Thu, 8 Dec 2016 15:25:23 -0800",Re: [VOTE] Apache Spark 2.1.0 (RC2),Reynold Xin <rxin@databricks.com>,"+0

I am not sure how much of a problem this is but the pip packaging
seems to have changed the size of the hadoop-2.7 artifact. As you can
see in http://people.apache.org/~pwendell/spark-releases/spark-2.1.0-rc2-bin/,
the Hadoop 2.7 build is 359M almost "
"""Dongjoon Hyun""<dongjoon@apache.org>","Thu, 08 Dec 2016 23:59:55 -0000",Question about SPARK-11374 (skip.header.line.count),<dev@spark.apache.org>,"Hi, All.

Could you give me some opinion?

There is an old SPARK issue, SPARK-11374, about removing header lines from text file.
Currently, Spark supports removing CSV header lines by the following way.

```
scala> spark.read.option(""header"",""true"").csv(""/data"").show
+---+---+
| c1| c2|
+---+---+
|  1|  a|
|  2|  b|
+---+---+
```

In SQL world, we can support that like the Hive way, `skip.header.line.count`.

```
scala> sql(""CREATE TABLE t1 (id INT, value VARCHAR(10)) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE LOCATION '/data' TBLPROPERTIES('skip.header.line.count'='1')"")
scala> sql(""SELECT * FROM t1"").show
+---+-----+
| id|value|
+---+-----+
|  1|    a|
|  2|    b|
+---+-----+
```

Although I made a PR for this based on the JIRA issue, I want to know this is really needed feature.
Is it need for your use cases? Or, it's enough for you to remove them in a preprocessing stage.
If this is too old and not proper in these days, I'll close the PR and JIRA issue as WON'T FIX.

Thank you for all in advance!

Bests,
Dongjoon.

---------------------------------------------------------------------


"
Dongjoon Hyun <dongjoon@apache.org>,"Fri, 09 Dec 2016 00:01:11 +0000",Fwd: Question about SPARK-11374 (skip.header.line.count),"dev <dev@spark.apache.org>, ""user@spark.apache.org"" <user@spark.apache.org>","+dev

I forget to add @user.

Dongjoon.

---------- Forwarded message ---------
From: Dongjoon Hyun <dongjoon@apache.org>
Date: Thu, Dec 8, 2016 at 16:00
Subject: Question about SPARK-11374 (skip.header.line.count)
To: <dev@spark.apache.org>


Hi, All.


"
satyajit vegesna <satyajit.apasprk@gmail.com>,"Thu, 8 Dec 2016 18:42:35 -0800","Issue in using DenseVector in RowMatrix, error could be due to ml and
 mllib package changes","user@spark.apache.org, dev@spark.apache.org","Hi All,

PFB code.


import org.apache.spark.ml.feature.{HashingTF, IDF}
import org.apache.spark.ml.linalg.SparseVector
import org.apache.spark.mllib.linalg.distributed.RowMatrix
import org.apache.spark.sql.SparkSession
import org.apache.spark.{SparkConf, SparkContext}

/**
  * Created by satyajit on 12/7/16.
  */
object DIMSUMusingtf extends App {

  val conf = new SparkConf()
    .setMaster(""local[1]"")
    .setAppName(""testColsim"")
  val sc = new SparkContext(conf)
  val spark = SparkSession
    .builder
    .appName(""testColSim"").getOrCreate()

  import org.apache.spark.ml.feature.Tokenizer

  val sentenceData = spark.createDataFrame(Seq(
    (0, ""Hi I heard about Spark""),
    (0, ""I wish Java could use case classes""),
    (1, ""Logistic regression models are neat"")
  )).toDF(""label"", ""sentence"")

  val tokenizer = new Tokenizer().setInputCol(""sentence"").setOutputCol(""words"")

  val wordsData = tokenizer.transform(sentenceData)


  val hashingTF = new HashingTF()
    .setInputCol(""words"").setOutputCol(""rawFeatures"").setNumFeatures(20)

  val featurizedData = hashingTF.transform(wordsData)


  val idf = new IDF().setInputCol(""rawFeatures"").setOutputCol(""features"")
  val idfModel = idf.fit(featurizedData)
  val rescaledData = idfModel.transform(featurizedData)
  rescaledData.show()
  rescaledData.select(""features"", ""label"").take(3).foreach(println)
  val check = rescaledData.select(""features"")

  val row = check.rdd.map(row => row.getAs[SparseVector](""features""))

  val mat = new RowMatrix(row) //i am basically trying to use
Dense.vector as a direct input to

rowMatrix, but i get an error that RowMatrix Cannot resolve constructor

  row.foreach(println)
}

Any help would be appreciated.

Regards,
Satyajit.
"
"""John Fang"" <xiaojian.fxj@alibaba-inc.com>","Fri, 09 Dec 2016 11:20:45 +0800","=?UTF-8?B?aG93IGNhbiBJIHNldCB0aGUgIGxvZyBjb25maWd1cmF0aW9uIGZpbGUgZm9yICBzcGFyayBo?=
  =?UTF-8?B?aXN0b3J5IHNlcnZlciAgPw==?=","""spark-dev"" <dev@spark.apache.org>,
  ""spark-user"" <user@spark.apache.org>","./start-history-server.sh
starting¬†org.apache.spark.deploy.history.HistoryServer,¬†logging¬†to¬†/home/admin/koala/data/versions/0/SPARK/2.0.2/spark-2.0.2-bin-hadoop2.6/logs/spark-admin-org.apache.spark.deploy.history.HistoryServer-1-v069166214.sqa.zmf.out
Then the history will print all log to the XXX.sqa.zmf.outÔºå so i can't limit the file max size. ¬†I want limit the size of the log file"
Don Drake <dondrake@gmail.com>,"Thu, 8 Dec 2016 22:03:08 -0600",Re: how can I set the log configuration file for spark history server ?,John Fang <xiaojian.fxj@alibaba-inc.com>,"You can update $SPARK_HOME/spark-env.sh by setting the environment
variable SPARK_HISTORY_OPTS.

See
http://spark.apache.org/docs/latest/monitoring.html#spark-configuration-options
for options (spark.history.fs.logDirectory) you can set.

There is log rotation built in (by time, not size) to the history server,
you need to enable/configure it.

Hope that helps.

-Don


can't



-- 
Donald Drake
Drake Consulting
http://www.drakeconsulting.com/
https://twitter.com/dondrake <http://www.MailLaunder.com/>
800-733-2143
"
Michael Allman <michael@videoamp.com>,"Thu, 8 Dec 2016 21:13:12 -0800",Re: [VOTE] Apache Spark 2.1.0 (RC2),Reynold Xin <rxin@databricks.com>,"I believe https://github.com/apache/spark/pull/16122 <https://github.com/apache/spark/pull/16122> needs to be included in Spark 2.1. It's a simple bug fix to some functionality that is introduced in 2.1. Unfortunately, it's been manually verified only. There's no unit test that covers it, and building one is far from trivial.

Michael



version 2.1.0. The vote is open until Sun, December 11, 2016 at 1:00 PT and passes if a majority of at least 3 +1 PMC votes are cast.
<http://spark.apache.org/>
(080717497365b83bc202ab16812ced93eb1ea7bd)
https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20fixVersion%20%3D%202.1.0 <https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20fixVersion%20%3D%202.1.0>
at:
<http://people.apache.org/~pwendell/spark-releases/spark-2.1.0-rc2-bin/>
<https://people.apache.org/keys/committer/pwendell.asc>
<https://repository.apache.org/content/repositories/orgapachespark-1217>
http://people.apache.org/~pwendell/spark-releases/spark-2.1.0-rc2-docs/ <http://people.apache.org/~pwendell/spark-releases/spark-2.1.0-rc2-docs/>
be available soon)
===============
===============
an existing Spark workload and running on this release candidate, then reporting any regressions.
=======================================
=======================================
fixes, documentation, and API tweaks that impact compatibility should be worked on immediately. Everything else please retarget to 2.1.1 or 2.2.0.

"
Nick Pentreath <nick.pentreath@gmail.com>,"Fri, 09 Dec 2016 05:24:28 +0000","Re: Issue in using DenseVector in RowMatrix, error could be due to ml
 and mllib package changes","satyajit vegesna <satyajit.apasprk@gmail.com>, user@spark.apache.org, dev@spark.apache.org","Yes most likely due to hashing tf returns ml vectors while you need mllib
vectors for row matrix.

I'd recommend using the vector conversion utils (I think in
mllib.linalg.Vectors but I'm on mobile right now so can't recall exactly).
There are until methods for converting single vectors as well as vector
rows of DF. Check the mllib user guide for 2.0 for details.

"
Dongjin Lee <dongjin@apache.org>,"Fri, 9 Dec 2016 14:55:57 +0900",Re: Question about SPARK-11374 (skip.header.line.count),dev@spark.apache.org,"+1 For this idea. I need it also.

Regards,
Dongjin




-- 
*Dongjin Lee*


*Software developer in Line+.So interested in massive-scale machine
learning.facebook: www.facebook.com/dongjin.lee.kr
<http://www.facebook.com/dongjin.lee.kr>linkedin:
kr.linkedi"
viirya <viirya@gmail.com>,"Fri, 9 Dec 2016 00:11:13 -0700 (MST)","Re: Issue in using DenseVector in RowMatrix, error could be due to
 ml and mllib package changes",dev@spark.apache.org,"Hi Satyajit,

You can just import mllib's Vectors (org.apache.spark.mllib.linalg.Vectors)
and call its fromML method to convert ml's Vector to mllib's Vector.

For example:

import org.apache.spark.mllib.linalg.{Vectors => OldVectors}

val row = check.rdd.map(row =>
OldVectors.fromML(row.getAs[SparseVector](""features"")))
val mat = new RowMatrix(row)

 



-----
Liang-Chi Hsieh | @viirya
Spark Technology Center
--

---------------------------------------------------------------------


"
Prashant Sharma <scrapcodes@gmail.com>,"Fri, 9 Dec 2016 13:12:05 +0530",Re: [VOTE] Apache Spark 2.1.0 (RC2),Michael Allman <michael@videoamp.com>,"I am getting 404 for Link
https://repository.apache.org/content/repositories/orgapachespark-1217.

--Prashant



"
Liang-Chi Hsieh <viirya@gmail.com>,"Fri, 9 Dec 2016 01:25:43 -0700 (MST)","Re: java.lang.IllegalStateException: There is no space for new
 record",dev@spark.apache.org,"
Hi Nick,

I think it is due to a bug in UnsafeKVExternalSorter. I created a Jira and a
PR for this bug:

https://issues.apache.org/jira/browse/SPARK-18800







-----
Liang-Chi Hsieh | @viirya
Spark Technology Center
--

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Fri, 9 Dec 2016 00:33:50 -0800",Re: [VOTE] Apache Spark 2.1.0 (RC2),Prashant Sharma <scrapcodes@gmail.com>,"I uploaded a new one:
https://repository.apache.org/content/repositories/orgapachespark-1219/




"
Sean Owen <sowen@cloudera.com>,"Fri, 09 Dec 2016 10:55:10 +0000",Re: [VOTE] Apache Spark 2.1.0 (RC2),"Reynold Xin <rxin@databricks.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","As usual, the sigs / hashes are fine and licenses look fine.

I am still seeing some test failures. A few I've seen over time and aren't
repeatable, but a few seem persistent. ANyone else observed these? I'm on
Ubuntu 16 / Java 8 building for -Pyarn -Phadoop-2.7 -Phive

If anyone can confirm I'll investigate the cause if I can. I'd hesitate to
support the release yet unless the build is definitely passing for others.


udf3Test(test.org.apache.spark.sql.JavaUDFSuite)  Time elapsed: 0.281 sec
 <<< ERROR!
java.lang.NoSuchMethodError:
org.apache.spark.sql.catalyst.JavaTypeInference$.inferDataType(Lcom/google/common/reflect/TypeToken;)Lscala/Tuple2;
at test.org.apache.spark.sql.JavaUDFSuite.udf3Test(JavaUDFSuite.java:107)



- caching on disk *** FAILED ***
  java.util.concurrent.TimeoutException: Can't find 2 executors before
30000 milliseconds elapsed
  at
org.apache.spark.ui.jobs.JobProgressListener.waitUntilExecutorsUp(JobProgressListener.scala:584)
  at org.apache.spark.DistributedSuite.org
$apache$spark$DistributedSuite$$testCaching(DistributedSuite.scala:154)
  at
org.apache.spark.DistributedSuite$$anonfun$32$$anonfun$apply$1.apply$mcV$sp(DistributedSuite.scala:191)
  at
org.apache.spark.DistributedSuite$$anonfun$32$$anonfun$apply$1.apply(DistributedSuite.scala:191)
  at
org.apache.spark.DistributedSuite$$anonfun$32$$anonfun$apply$1.apply(DistributedSuite.scala:191)
  at
org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
  at org.scalatest.Transformer.apply(Transformer.scala:22)
  at org.scalatest.Transformer.apply(Transformer.scala:20)
  ...


  org.apache.spark.sql.streaming.StreamingQueryException: Query [id =
3b191b78-7f30-46d3-93f8-5fbeecce94a2, runId =
0cab93b6-19d8-47a7-88ad-d296bea72405] terminated with exception: null
  at org.apache.spark.sql.execution.streaming.StreamExecution.org
$apache$spark$sql$execution$streaming$StreamExecution$$runBatches(StreamExecution.scala:262)
  at
org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:160)
  ...
  Cause: java.lang.NullPointerException:
  ...




"
Dongjoon Hyun <dongjoon@apache.org>,"Fri, 09 Dec 2016 17:42:58 +0000",Re: Question about SPARK-11374 (skip.header.line.count),"Dongjin Lee <dongjin@apache.org>, dev@spark.apache.org","Thank you for the opinion, Dongjin!



"
"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Fri, 9 Dec 2016 10:09:16 -0800",Re: [VOTE] Apache Spark 2.1.0 (RC2),Sean Owen <sowen@cloudera.com>,"be thrown NPE when a topic is deleted. I added some logic to retry on such
failure, however, it may still fail when topic deletion is too frequent
(the stress test). Just reopened
https://issues.apache.org/jira/browse/SPARK-18588.

Anyway, this is just a best effort to deal with Kafka issue, and in
practice, people won't delete topic frequently, so this is not a release
blocker.


"
Cody Koeninger <cody@koeninger.org>,"Fri, 9 Dec 2016 13:21:38 -0600",Re: [VOTE] Apache Spark 2.1.0 (RC2),"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Agree that frequent topic deletion is not a very Kafka-esque thing to do


---------------------------------------------------------------------


"
satyajit vegesna <satyajit.apasprk@gmail.com>,"Fri, 9 Dec 2016 11:52:42 -0800",Document Similarity -Spark Mllib,"dev@spark.apache.org, user@spark.apache.org","Hi ALL,

I am trying to implement a mlllib spark job, to find the similarity between
documents(for my case is basically home addess).

i believe i cannot use DIMSUM for my use case as, DIMSUM is works well only
with matrix with thin columns and more rows in matrix.

matrix example format, for my use case:

                         doc1(address1)  doc2(address2) .......... m is
going to be huge as i have more add.
      san mateo         0.73462                 0
      san fransico       ..                           ..
      san bruno           ..                            ..
       .
       .
       .
       .
     and n is going to be thin compared to m

I would like to know if there is way to leverage DIMSUM to work on my use
case, and if not what other alogrithm i can try that is available in spark
mlllib.

Regards,
Satyajit.
"
Sean Owen <sowen@cloudera.com>,"Fri, 09 Dec 2016 21:09:38 +0000",Re: [VOTE] Apache Spark 2.1.0 (RC2),"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Sure, it's only an issue insofar as it may be a flaky test. If it's fixable
or disable-able for a possible next RC that could be helpful.


"
Liang-Chi Hsieh <viirya@gmail.com>,"Sat, 10 Dec 2016 04:44:19 -0700 (MST)",Re: Document Similarity -Spark Mllib,dev@spark.apache.org,"Hi Satyajit,

I am not sure why you think DIMSUM cannot apply for your use case. Or you've
tried it but encountered some problems.

Although in the paper[1] the authors mentioned they concentrate on the
regime where the number of rows is very large, and the number of columns is
not too large. But I think it doesn't prevent you applying it on the dataset
of large columns. By the way, in another paper[2], they experimented it on a
dataset of 10^7 columns.

Even the number of column is very large, if your dataset is very sparse, and
you use SparseVector, DIMSUM should work well too. You can also adjust the
threshold when using DIMSUM.


[1] Reza Bosagh Zadeh and Gunnar Carlsson, ""Dimension Independent Matrix
Square using MapReduce (DIMSUM)""
[2] Reza Bosagh Zadeh and Ashish Goel, ""Dimension Independent Similarity
Computation""




-----
Liang-Chi Hsieh | @viirya
Spark Technology Center
--

---------------------------------------------------------------------


"
Felix Cheung <felixcheung_m@hotmail.com>,"Sat, 10 Dec 2016 19:00:47 +0000",Re: Question about SPARK-11374 (skip.header.line.count),"Dongjin Lee <dongjin@apache.org>, Dongjoon Hyun <dongjoon@apache.org>,
	""dev@spark.apache.org"" <dev@spark.apache.org>","+1 I think it's useful to always have a pure SQL way and skip header for plain text / csv that lots of companies have.


________________________________
From: Dongjoon Hyun <dongjoon@apache.org>
Sent: Friday, December 9, 2016 9:42:58 AM
To: Dongjin Lee; "
Dongjoon Hyun <dongjoon@apache.org>,"Sat, 10 Dec 2016 18:28:19 -0800",Re: Question about SPARK-11374 (skip.header.line.count),Felix Cheung <felixcheung_m@hotmail.com>,"Thank you for the opinion, Felix.

Bests,
Dongjoon.


"
Mingjie Tang <tangrock@gmail.com>,"Sat, 10 Dec 2016 21:45:49 -0500",Re: Question about SPARK-11374 (skip.header.line.count),Dongjoon Hyun <dongjoon@apache.org>,"+1, it is useful.


"
kant kodali <kanth909@gmail.com>,"Sun, 11 Dec 2016 06:31:19 -0800",Re: Wrting data from Spark streaming to AWS Redshift?,ayan guha <guha.ayan@gmail.com>,"@shyla  a side question: What does Redshift can do that Spark cannot do?
Trying to understand your use case.


"
Liang-Chi Hsieh <viirya@gmail.com>,"Sun, 11 Dec 2016 18:42:48 -0700 (MST)",Re: Question about SPARK-11374 (skip.header.line.count),dev@spark.apache.org,"Hi Dongjoon,

I know some people only use Spark SQL with SQL syntax not Dataset API. So I
think it should be useful to provide a way to do this in SQL.



-----
Liang-Chi Hsieh | @viirya
Spark Technology Center
--

---------------------------------------------------------------------


"
Dongjoon Hyun <dongjoon@apache.org>,"Sun, 11 Dec 2016 18:20:38 -0800",Re: Question about SPARK-11374 (skip.header.line.count),Liang-Chi Hsieh <viirya@gmail.com>,"Thank you for the opinion, Mingjie and Liang-Chi.

Dongjoon.


"
Liang-Chi Hsieh <viirya@gmail.com>,"Sun, 11 Dec 2016 23:44:22 -0700 (MST)","Re: Why don't we imp some adaptive learning rate methods, such as
 adadelat, adam?",dev@spark.apache.org,"
Hi,

There is a plan to add this into Spark ML. Please check out
https://issues.apache.org/jira/browse/SPARK-18023. You can also follow this
jira to get the latest update.



-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
nsyca <nsy.can@gmail.com>,"Mon, 12 Dec 2016 11:48:16 -0700 (MST)",Re: Aggregating over sorted data,dev@spark.apache.org,"Hi,

SPARK-18591 <https://issues.apache.org/jira/browse/SPARK-18591>   might be a
solution to your problem but making assuming in your UDAF logic on how Spark
will process the aggregation is really a risky thing. Is there a way to do
it using Windows function with ORDER BY clause to enforce the processing in
this case?




-----
Nattavut Sutyanyong | @nsyca
Spark Technology Center
http://www.spark.tc/
--

---------------------------------------------------------------------


"
Marcelo Vanzin <vanzin@cloudera.com>,"Mon, 12 Dec 2016 14:03:55 -0800",Re: [VOTE] Apache Spark 2.1.0 (RC2),Reynold Xin <rxin@databricks.com>,"I'm running into this when building / testing on 1.7 (haven't tried 1.8):

udf3Test(test.org.apache.spark.sql.JavaUDFSuite)  Time elapsed: 0.079
sec  <<< ERROR!
java.lang.NoSuchMethodError:
org.apache.spark.sql.catalyst.JavaTypeInference$.inferDataType(Lcom/google/common/reflect/TypeToken;)Lsc
ala/Tuple2;
       at test.org.apache.spark.sql.JavaUDFSuite.udf3Test(JavaUDFSuite.java:107)


Results :

Tests in error:
 JavaUDFSuite.udf3Test:107 ¬ª NoSuchMethod
org.apache.spark.sql.catalyst.JavaTyp...


Given the error I'm mostly sure it's something easily fixable by
adding Guava explicitly in the pom, so probably shouldn't block
anything.


s
20fixVersion%20%3D%202.1.0
===============
===============
=======================================
=======================================
,
on



-- 
Marcelo

---------------------------------------------------------------------


"
Marcelo Vanzin <vanzin@cloudera.com>,"Mon, 12 Dec 2016 15:02:05 -0800",Re: [VOTE] Apache Spark 2.1.0 (RC2),Reynold Xin <rxin@databricks.com>,"Actually this is not a simple pom change. The code in
UDFRegistration.scala calls this method:

          if (returnType == null) {
           returnType =
JavaTypeInference.inferDataType(TypeToken.of(udfReturnType))._1
         }

Because we shade guava, it's generally not very safe to call methods
in different modules that expose shaded APIs. Can this code be
modified to call the variant that just takes a java.lang.Class instead
of Guava's TypeToken? It seems like that would work, since that method
basically just wraps the argument with ""TypeToken.of"".



:
e/common/reflect/TypeToken;)Lsc
va:107)
es
%20fixVersion%20%3D%202.1.0
e
===============
===============
=======================================
=======================================
s,
 on



-- 
Marcelo

---------------------------------------------------------------------


"
Yin Huai <yhuai@databricks.com>,"Mon, 12 Dec 2016 17:19:48 -0800",Re: [VOTE] Apache Spark 2.1.0 (RC2),Marcelo Vanzin <vanzin@cloudera.com>,"-1

I hit https://issues.apache.org/jira/browse/SPARK-18816, which prevents
executor page from showing the log links if an application does not have
executors initially.

:

):
on
:
/
================
================
n
==================================="
Marcelo Vanzin <vanzin@cloudera.com>,"Mon, 12 Dec 2016 17:52:25 -0800",Re: [VOTE] Apache Spark 2.1.0 (RC2),Reynold Xin <rxin@databricks.com>,"Another failing test is ""ReplSuite:should clone and clean line object
in ClosureCleaner"". It never passes for me, just keeps spinning until
the JVM eventually starts throwing OOM errors. Anyone seeing that?




-- 
Marcelo

---------------------------------------------------------------------


"
Mark Hamstra <mark@clearstorydata.com>,"Mon, 12 Dec 2016 18:39:51 -0800",Re: [VOTE] Apache Spark 2.1.0 (RC2),Marcelo Vanzin <vanzin@cloudera.com>,"Yes, I see the same.


"
"""Chawla,Sumit "" <sumitkchawla@gmail.com>","Tue, 13 Dec 2016 08:31:16 -0800",Output Side Effects for different chain of operations,"User <user@spark.apache.org>, dev@spark.apache.org","Hi All

I have a workflow with different steps in my program. Lets say these are
steps A, B, C, D.  Step B produces some temp files on each executor node.
How can i add another step E which consumes these files?

I understand the easiest choice is  to copy all these temp files to any
shared location, and then step E can create another RDD from it and work on
that.  But i am trying to avoid this copy.  I was wondering if there is any
way i can queue up these files for E as they are getting generated on
executors.  Is there any possibility of creating a dummy RDD in start of
program, and then push these files into this RDD from each executor.

I take my inspiration from the concept of Side Outputs in Google Dataflow:

https://cloud.google.com/dataflow/model/par-do#emitting-to-side-outputs-in-your-dofn



Regards
Sumit Chawla
"
nseggert <nicholas.eggert@target.com>,"Tue, 13 Dec 2016 09:31:23 -0700 (MST)",SPARK-17455 Isotonic Regression fix languishing,dev@spark.apache.org,"I have PR that has been sitting untouched for months. Could someone please
take a look at it?

https://github.com/apache/spark/pull/15018 (SPARK-17455) in JIRA.

This PR fixes problems with the way isotonic regression was implemented that
caused it to take exponential time for some inputs. This problem made it
unusable for me, because I occasionally hit those cases. I'd assume it's
very likely to affect others as well. 

My patch just translates the scikit-learn implementation into Scala, so I
think it should be fairly uncontroversial.

Also note that although this is a change to the mllib code, this is one of
those cases where the ml implementation just calls the mllib version.



--

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Tue, 13 Dec 2016 18:13:04 +0000",Re: SPARK-17455 Isotonic Regression fix languishing,"nseggert <nicholas.eggert@target.com>, dev@spark.apache.org","straightforward so I will review today.


"
akchin <akchin@us.ibm.com>,"Tue, 13 Dec 2016 12:50:43 -0700 (MST)",Re: [VOTE] Apache Spark 2.1.0 (RC2),dev@spark.apache.org,"Hello, 

I am seeing this error as well except during ""define case class and create
Dataset together with paste mode *** FAILED ***"" 
Starts throwing OOM and GC errors after running for several minutes. 





-----
Alan Chin 
IBM Spark Technology Center 
--

---------------------------------------------------------------------


"
Adam Roberts <AROBERTS@uk.ibm.com>,"Tue, 13 Dec 2016 20:04:30 +0000",Re: [VOTE] Apache Spark 2.1.0 (RC2),akchin <akchin@us.ibm.com>,"I've never seen the ReplSuite test OoMing with IBM's latest SDK for Java 
but have always noticed this particular test failing with the following 
instead:

java.lang.AssertionError: assertion failed: deviation too large: 
0.8506807397223823, first size: 180392, second size: 333848

This particular test could be improved and I don't think it should hold up 
releases, I've commented on [SPARK-14558] already a while back and the 
discussion ended with: 

""A better check would be to run with and without the closure cleaner 
change
-> Yea, this is what I did locally, but how to write a test for it?""

It will fail in this particular way reliably with Open/Oracle JDK as well 
if you were to use Kryo.

We don't see this test failing (either OoM or the above failure) with 
OpenJDK 8 in our test farm, this is with OpenJDK 1.8.0_51-b16 and I'm 
running with -Xmx4g -Xss2048k -Dspark.buffer.pageSize 1048576.

All other Spark unit tests pass (we see a grand total of 11980 tests) 
except for the Kafka stress test already mentioned, various 
platforms/operating systems including big-endian.

I've never seen the NoSuchMethod error mentioned in JavaUDFSuite and 
haven't seen the failure Alan mentions below either.

I also have performance data to share (HiBench and SparkSqlPerf with 
TPC-DS queries) comparing this release to Spark 2.0.2, I'll wait until the 
next RC before commenting (it is positive), looks like we'll have another 
as this RC2 vote should be closed by now and in RC3 we'd also have the 
[SPARK-18091] fix included to prevent a test's generated code exceeding 
the 64k constant pool size limit.




From:   akchin <akchin@us.ibm.com>
To:     dev@spark.apache.org
Date:   13/12/2016 19:51
Subject:        Re: [VOTE] Apache Spark 2.1.0 (RC2)



Hello, 

I am seeing this error as well except during ""define case class and create
Dataset together with paste mode *** FAILED ***"" 
Starts throwing OOM and GC errors after running for several minutes. 





-----
Alan Chin 
IBM Spark Technology Center 
--
View this message in context: 
http://apache-spark-developers-list.1001551.n3.nabble.com/VOTE-Apache-Spark-2-1-0-RC2-tp20175p20215.html

Nabble.com.

---------------------------------------------------------------------



Unless stated otherwise above:
IBM United Kingdom Limited - Registered in England and Wales with number 
741598. 
Registered office: PO Box 41, North Harbour, Portsmouth, Hampshire PO6 3AU
"
satyajit vegesna <satyajit.apasprk@gmail.com>,"Tue, 13 Dec 2016 14:10:06 -0800",Re: Document Similarity -Spark Mllib,Liang-Chi Hsieh <viirya@gmail.com>,"Hi Liang,

The problem is that when i take a huge data set , i get a matrix size
1616160 * 1616160.

PFB code,

 val exact = mat.columnSimilarities(0.5)
 val exactEntries = exact.entries.map { case MatrixEntry(i, j, u) => ((i,
j), u) }
case class output(label1:Long,label2:Long,score:Double)
val fin = exactEntries.map(x => output(x._1._1,x._1._2,x._2)).toDF
val fin2 = fin.persist(StorageLevel.MEMORY_AND_DISK_SER)

finally when i try to write the data into parquet from
fin2.(fin2.write.parquet(""/somelocation""))

it takes forever and i do not see any progress.

But the same code works good with smaller dataset.

Any suggestion on how to deal with the above situation , is highly
appreciated.

Regards,
Satyajit.


"
"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Wed, 14 Dec 2016 00:57:52 +0000",Belief propagation algorithm is open sourced,"""dev@spark.apache.org"" <dev@spark.apache.org>, user
	<user@spark.apache.org>","Dear Spark developers and users,


HPE has open sourced the implementation of the belief propagation (BP) algorithm for Apache Spark, a popular message passing algorithm for performing inference in probabilistic graphical models. It provides exact inference for graphical models without loops. While inference for graphical models with loops is approximate, in practice it is shown to work well. The implementation is generic and operates on factor graph representation of graphical models. It handles factors of any order, and variable domains of any size. It is implemented with Apache Spark GraphX, and thus can scale to large scale models. Further, it supports computations in log scale for numerical stability. Large scale applications of BP include fraud detection in banking transactions and malicious site detection in computer networks.


Source code: https://github.com/HewlettPackard/sandpiper


Best regards, Alexander
"
Liang-Chi Hsieh <viirya@gmail.com>,"Tue, 13 Dec 2016 19:14:32 -0700 (MST)",Re: Document Similarity -Spark Mllib,dev@spark.apache.org,"
Hi Satyajit,

Have you tried to adjust a higher threshold for columnSimilarities to lower
the computation cost?

BTW, can you also comment out most of other codes and just run
columnSimilarities and do a simple computation like counting for the entries
of returned CoordinateMatrix? So we can make sure the problem is exactly at
columnSimilarities?

E.g,

val exact = mat.columnSimilarities(0.5)
val exactCount = exact.entries.count





-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Tue, 13 Dec 2016 23:55:25 -0800",Re: [VOTE] Apache Spark 2.1.0 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","I'm going to -1 this myself: https://issues.apache.org/jira/browse/
SPARK-18856 <https://issues.apache.org/jira/browse/SPARK-18856>



"
"""John Fang"" <xiaojian.fxj@alibaba-inc.com>","Wed, 14 Dec 2016 21:14:52 +0800","=?UTF-8?B?Y2FuIHdlIHVuaXRlIHRoZSBVSSBhbW9uZyBkaWZmZXJlbnQgc3RhbmRhb25lIGNsdXN0ZXJz?=
  =?UTF-8?B?JyBVST8=?=","""spark-dev"" <dev@spark.apache.org>,
  ""spark-user"" <user@spark.apache.org>","As we know, each standaone cluster has itself UI. Then we will have more than one UI if we have many standalone cluster. How can I only have a UI which can access different standaone clusters?"
Bryan Cutler <cutlerb@gmail.com>,"Wed, 14 Dec 2016 09:46:26 -0800",Re: Why is there no flatten method on RDD?,Tarun Kumar <tarunk1407@gmail.com>,"Hi Tarun,

I think there just hasn't been a strong need for it when you can accomplish
the same with just rdd.flatMap(identity).  I see a JIRA was just opened for
this https://issues.apache.org/jira/browse/SPARK-18855


"
Bryan Cutler <cutlerb@gmail.com>,"Wed, 14 Dec 2016 20:21:21 -0800",Re: Belief propagation algorithm is open sourced,"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","I'll check it out, thanks for sharing Alexander!


"
Anton Okolnychyi <anton.okolnychyi@gmail.com>,"Thu, 15 Dec 2016 09:28:58 +0100",Expand the Spark SQL programming guide?,dev@spark.apache.org,"Hi,

I am wondering whether it makes sense to expand the Spark SQL programming
guide with examples of aggregations (including user-defined via the
Aggregator API) and window functions.  For instance, there might be a
separate subsection under ""Getting Started"" for each functionality.

SPARK-16046 seems to be related but there is no activity for more than 4
months.

Best regards,
Anton
"
Bertrand Dechoux <dechouxb@gmail.com>,"Thu, 15 Dec 2016 10:03:49 +0100",Re: Belief propagation algorithm is open sourced,Bryan Cutler <cutlerb@gmail.com>,"Nice! I am especially interested in Bayesian Networks, which are only one
of the many models that can be expressed by a factor graph representation.
Do you do Bayesian Networks learning at scale (parameters and structure)
with latent variables? Are you using publicly available tools for that?
Which ones?

LibDAI, which created the supported format, ""supports parameter learning of
conditional probability tables by Expectation Maximization"" according to
the documentation. Is it your reference tool?

Bertrand


"
Liang-Chi Hsieh <viirya@gmail.com>,"Thu, 15 Dec 2016 02:05:32 -0700 (MST)",Re: Document Similarity -Spark Mllib,dev@spark.apache.org,"
OK. I go to check the DIMSUM implementation in Spark MLlib. The probability
a column is sampled is decided by math.sqrt(10 * math.log(nCol) / threshold)
/ colMagnitude. The most influential parameter is colMagnitude. If in your
dataset, the colMagnitude for most columns is very low, then looks like it
might not work much better than brute-force even you set a higher threshold.





-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
"""Hegner, Travis"" <THegner@trilliumit.com>","Thu, 15 Dec 2016 16:37:22 +0000","Re: SPARK-18689: A proposal for priority based app scheduling
 utilizing linux cgroups.",Apache Spark Dev <dev@spark.apache.org>,"Hello Spark Devs,


I have finally completed a mostly working proof of concept. I do not want to create a pull request for this code, as I don't believe it's production worthy at the moment. My intent is to better communicate what I'd like to accomplish. Please review the following patch: https://github.com/apache/spark/compare/branch-2.0...travishegner:cgroupScheduler.


What the code does:


Currently, it exposes two options ""spark.cgroups.enabled"", which defaults to false, and ""spark.executor.shares"" which defaults to None. When cgroups mode is enabled, a single executor is created on each worker, with access to all cores. The worker will create a parent cpu cgroup (on first executor launch) called ""spark-worker"" to house any executors that it launches. Each executor is put into it's own cgroup named with the app id, under the parent cgroup. The cpu.shares parameter is set to the value in ""spark.executor.shares"", if this is ""None"", it inherits the value from the parent cgroup.


Tested on Ubuntu 16:04 (docker containers), kernel 4.4.0-53-generic: I have not run unit tests. I do not know if/how cgroups v2 (kernel 4.5) is going to change this code base, but it looks like the kernel interface is the same for the most part.


I was able to launch a spark shell which consumed all cores in the cluster, but sat idle. I was then able to launch an application (client deploy-mode) which was also allocated all cores in the cluster, and ran to completion unhindered. Each of the executors on each worker was properly placed into it's respective cgroup, which in turn had the correct cpu.shares value allocated.


What the code still needs:


* Documentation (assuming the community moves forward with some kind of implementation)

* Sometimes the cgroups get destroyed after app completion, sometimes they don't. (need to put `.destroy()` call in a `finally` block., or in the `maybeCleanupApplication()` method; what do you think?)

* Proper handling of drivers's resources when running `--deploy-mode cluster`

* Better web UI indication of cgroup mode or core sharing (currently just shows up as an over allocation of cores per worker)

* Better environment/os/platform detection and testing (I won't be surprised if there is something broken if trying to run this on a different OS)

* Security/permissions for cgroups if running worker as non-root (perhaps creating the parent cgroup with correct permissions before launching the worker is all that is necessary)

  - running the worker in a container currently requires --privileged mode (I haven't figured out if/what capability makes cgroups writable, or if it's possible to use a new cgroup mount point)

* More user defined options

  - cgroup root path (currently hard coded)

  - driver cpu.shares (for cluster deploy-mode: would require a specially named cgroup... s""$appId-driver"" ? default same #shares as executor? default double shares?

  - parent cpu.shares (currently os default)

  - parent cgroup name (currently hard coded)


I tried to structure the initial concept to make it easy to add support for more cgroup features (cpuset, mem, etc...), should the community feel there is value in adding them. Linux cgroups are an extremely powerful resource allocation and isolation tool, and this patch is only scratching the surface of their general capabilities. Of course, as Mr. Loughran's points out, expanding into these features will require more code maintenance, but not enough that we should shy away from it.


<opinion>

I personally believe that any multi-node resource allocation system should offload as much of the scheduling and resource allocation as possible to the underlying kernel within the node level. Each node's own kernel is the beocate a few seconds worth of cpu to the low priority app, while the high priority app is waiting on disk I/O, and instantly give it back to the high priority app when it needs it, with (near) real-time granularity


The multi-node system should set up a proper framework to give each node's kernel the information it needs to allocate the resources correctly. Naturally, the system should allow resource reservations, and even limits, for the purposes of meeting and testing for SLAs and worst case scenarios as well. Linux cgroups are capable of doing those things in a near real-time fashion.


With a proper convention of priorities/shares for applications within an organization, I believe that everyone can get better throughput out of their hardware, at any cluster size. But, alas, *that* is not a problem I'm trying to solve currently.

</opinion>

Sorry that the patch is pretty rough still, as I'm still getting my head wrapped around spark's code base structure. Looking forward to any feedback.

Thanks,

Travis

________________________________
From: Hegner, Travis <THegner@trilliumit.com>
Sent: Tuesday, December 6, 2016 10:49
To: Steve Loughran
Cc: Shuai Lin; Apache Spark Dev
Subject: Re: SPARK-18689: A proposal for priority based app scheduling utilizing linux cgroups.



Steve,

I appreciate your experience and insight when dealing with large clusters at the data-center scale. I'm also well aware of the complex nature of schedulers, and that it is an area of ongoing research being done by people/companies with many more resources than I have. This might explain my apprehension in even calling this idea a *scheduler*: I wanted to avoid this exact kind of debate over what I want to accomplish. This is also why I mentioned that this idea will mostly benefit users with small clusters.

I've used many of the big named ""cluster schedulers"" (YARN, Mesos, and Kubernetes) and the main thing that they have in common is that they don't work well for my use case. Those systems are designed for large scale 1000+ node clusters, and become painful to manage in the small cluster range. Most of the tools that we've attempted to use don't work well for us, so we've written several of our own: https://github.com/trilliumit/.

It can be most easily stated by the fact that *we are not* Google, Facebook, or Amazon: we don't have a *data-center* of servers to manage, we barely have half of a rack. *We are not trying to solve the problem that you are referring to*. We are operating at a level that if we aren't meeting SLAs, then we could just buy another server to add to the cluster. I imagine that we are not alone in that fact either, I've seen that many of the questions on SO and on the user list are from others operating at a level similar to ours.

I understand that pre-emption isn't inherently a bad thing, and that these multi-node systems typically handle it gracefully. However, if idle CPU is expensive, then how much more does wasted CPU cost when a nearly complete task is pre-empted and has to be started over? Fortunately for me, that isn't a problem that I have to solve at the moment.

stances for the different workloads

See my above comment on how well these cluster schedulers work for us. I have considered the avenue of multiple spark clusters, and in reality the infrastructure we have set up would allow me to do this relatively easily. In fact, in my environment, this is a similar solution to what I'm proposing, just managed one layer up the stack and with less flexibility. I am trying to avoid this solution however because it does require more overhead and maintenance. What if I want two spark apps to run on the same cluster at the same time, sharing the available CPU capacity equally? I can't accomplish that easily with multiple spark clusters. Also, we are a 1 to 2 man operation at this point, I don't have teams of ops people to task with managing as many spark clusters as I feel like launching.


Perhaps in the use-cases you have experience with, but not currently in mine. In fact, my initial proposal is net yet changing the allocation of memory as a resource. This would still be statically allocated in a FIFO manner as long as memory is available on the cluster, the same way it is now.


Thanks for the suggestion, but I will choose how I spend my time. If I can find a simple solution to a problem that I face, and I'm willing to share that solution, I'd hope one would encourage that instead.


Perhaps I haven't yet clearly communicated what I'm trying to do. In short, *I am not trying to write a scheduler*: I am trying to slightly (and optionally) tweak the way executors are allocated and launched, so that I can more intuitively and more optimally utilize my small spark cluster.

Thanks,

Travis

________________________________
From: Steve Loughran <stevel@hortonworks.com>
Sent: Tuesday, December 6, 2016 06:54
To: Hegner, Travis
Cc: Shuai Lin; Apache Spark Dev
Subject: Re: SPARK-18689: A proposal for priority based app scheduling utilizing linux cgroups.

This is essentially what the cluster schedulers do: allow different people to submit work with different credentials and priority; cgroups & equivalent to limit granted resources to requested ones. If you have pre-emption enabled, you can even have one job kill work off the others. Spark does recognise pre-emption failures and doesn't treat it as a sign of problems in the executor, that is: it doesn't over-react.

cluster scheduling is one of the cutting edge bits of datacentre-scale computing óif you are curious about what is state of the art, look at the Morning Paper https://blog.acolyer.org/ for coverage last week of MS and google work there. YARN, Mesos, Borg, whatever Amazon use, at scale it's not just meeting SLAs, its about how much idle CPU costs, and how expensive even a 1-2% drop in throughput would be.


I would strongly encourage you to avoid this topic, unless you want dive deep into the whole world of cluster scheduling, the debate over centralized vs decentralized, the idelogical one of ""should services ever get allocated RAM/CPU in times of low overall load?"", the challenge of swap, or more specifically, ""how do you throttle memory consumption"", as well as what to do when the IO load of a service is actually incurred on a completely different host from the one your work is running on.

There's also a fair amount of engineering work; to get a hint download the Hadoop tree and look at hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux for the cgroup support, and then hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl for the native code needed alongside this. Then consider that it's not just a matter of writing something similar, it's getting an OSS project to actually commit to maintaining such code after you provide that initial contribution.

Instead? Use a multi-user cluster scheduler and spin up different spark instances for the different workloads, with different CPU & memory limits, queue priorities, etc. Other people have done the work, written the tests, deployed it in production, met their own SLAs *and are therefore committed to maintaining this stuff*.

-Steve


My apologies, in my excitement of finding a rather simple way to accomplish the scheduling goal I have in mind, I hastily jumped straight into a technical solution, without explaining that goal, or the problem it's attempting to solve.

You are correct that I'm looking for an additional running mode for the standalone scheduler. Perhaps you could/should classify it as a different scheduler, but I don't want to give the impression that this will be as difficult to implement as most schedulers are. Initially, from a memory perspective, we would still allocate in a FIFO manner. This new scheduling mode (or new scheduler, if you'd rather) would mostly benefit any users with small-ish clusters, both on-premise and cloud based. Essentially, my end goal is to be able to run multiple *applications* simultaneously with each application having *access* to the entire core count of the cluster.

I have a very cpu intensive application that I'd like to run weekly. I have a second application that I'd like to run hourly. The hourly application is more time critical (higher priority), so I'd like it to finish in a small amount of time. If I allow the first app to run with all cores (this takes several days on my 64 core cluster), then nothing else can be executed when running with the default FIFO scheduler. All of the cores have been allocated to the first application, and it will not release them until it is finished. Dynamic allocation does not help in this case, as there is always a backlog of tasks to run until the first application is nearing the end anyway. Naturally, I could just limit the number of cores that the first application has access to, but then I have idle cpu time when the second app is not running, and that is not optimal. Secondly in that case, the second application only has access to the *leftover* cores that the first app has not allocated, and will take a considerably longer amount of time to run.

You could also imagine a scenario where a developer has a spark-shell running without specifying the number of cores they want to utilize (whether intentionally or not). As I'm sure you know, the default is to allocate the entire cluster to this application. The cores allocated to this shell are unavailable to other applications, even if they are just sitting idle while a developer is getting their environment set up to run a very big job interactively. Other developers that would like to launch interactive shells are stuck waiting for the first one to exit their shell.

My proposal would eliminate this static nature of core counts and allow as many simultaneous applications to be running as the cluster memory (still statically partitioned, at least initially) will allow. Applications could be configured with a ""cpu shares"" parameter (just an arbitrary integer relative only to other applications) which is essentially just passed through to the linux cgroup cpu.shares setting. Since each executor of an application on a given worker runs in it's own process/jvm, then that process could be easily be placed into a cgroup created and dedicated for that application.

Linux cgroups cpu.shares are pretty well documented, but the gist is that processes competing for cpu time are allocated a percentage of time equal to their share count as a percentage of all shares in that level of the cgroup hierarchy. If two applications are both scheduled on the same core with the same weight, each will get to utilize 50% of the time on that core. This is all built into the kernel, and the only thing the spark worker has to do is create a cgroup for each application, set the cpu.shares parameter, and assign the executors for that application to the new cgroup. If multiple executors are running on a single worker, for a single application, the cpu time available to that application is divided among each of those executors equally. The default for cpu.shares is that they are not limiting in any way. A process can consume all available cpu time if it would otherwise be idle anyway.


That's the issue that surfaces in google papers: should jobs get idle capacity. Current consensus is ""no"". Why not? Because you may end up writing an SLA-sensitive app which just happens to meet it's SLAs in times of light cluster load, but precisely when the cluster is busy, it suddenly slows down, leading to stress all round, in the ""why is this service suddenly unusable"" kind of stress. Instead you keep the cluster busy with low priority preemptible work, use labels to allocate specific hosts to high-SLA apps, etc.


Another benefit to passing cpu.shares directly to the kernel (as opposed to some abstraction) is that cpu share allocations are heterogeneous to all processes running on a machine. An admin could have very fine grained control over which processes get priority access to cpu time, depending on their needs.



To continue my personal example above, my long running cpu intensive application could utilize 100% of all cluster cores if they are idle. Then my time sensitive app could be launched with nine times the priority and the linux kernel would scale back the first application to 10% of all cores (completely seemlessly and automatically: no pre-emption, just fewer time slices of cpu allocated by the kernel to the first application), while the second application gets 90% of all the cores until it completes.


FWIW, it's often memory consumption that's most problematic here. If one process starts to swap, it hurts everything else. But Java isn't that good at handling limited heap/memory size; you have to spec that heap up front.


The only downside that I can think of currently is that this scheduling mode would create an increase in context switching on each host. This issue is somewhat mitigated by still statically allocating memory however, since there wouldn't typically be an exorbitant number of applications running at once.

In my opinion, this would allow the most optimal usage of cluster resources. Linux cgroups allow you to control access to more than just cpu shares. You can apply the same concept to other resources (memory, disk io). You can also set up hard limits so that an application will never get more than is allocated to it. I know that those limitations are important for some use cases involving predictability of application execution times. Eventually, this idea could be expanded to include many more of the features that cgroups provide.

Thanks again for any feedback on this idea. I hope that I have explained it a bit better now. Does anyone else can see value in it?



I'm not saying ""don't get involved in the scheduling problem""; I'm trying to show just how complex it gets in a large system. Before you begin to write a line of code, I'd recommend

-you read as much of the published work as you can, including the google and microsoft papers, Facebook's FairScheduler work, etc, etc.
-have a look at the actual code inside those schedulers whose source is public, that's YARN and Mesos.
-try using these schedulers for your own workloads.

really: scheduling work across a datacentre a complex problem that is still considered a place for cutting-edge research. Avoid unless you want to do that.

-Steve


"
"""Dongjoon Hyun""<dongjoon@apache.org>","Thu, 15 Dec 2016 17:18:17 -0000",Forking or upgrading Apache Parquet in Spark,<dev@spark.apache.org>,"Hi, All.

I made a PR to upgrade Parquet to 1.9.0 for Apache Spark 2.2 on Late March.

- https://github.com/apache/spark/pull/16281

Currently, there occurs some important options about that. Here is the summary.

1. Forking Parquet 1.8.X and maintaining like Spark Hive.
2. Wait and see for Parquet 1.9.x adoption in the other community.
3. Make stronger integration tests including both feature and performance tests

I think we had better inform all of you on dev mailing list because it has an option forking.
If you have any opinion, please reply here or on the PR.

BTW, the default decision is always number 2 because we will use Apache Parquet 1.8.1 for a while.

Bests,
Dongjoon.

---------------------------------------------------------------------


"
Ryan Blue <rblue@netflix.com.INVALID>,"Thu, 15 Dec 2016 09:34:41 -0800",Re: Forking or upgrading Apache Parquet in Spark,Dongjoon Hyun <dongjoon@apache.org>,"Thanks for the heads-up Dongjoon. I just noticed that thread this morning
and was planning on responding. I'll do that today.

rb




-- 
Ryan Blue
Software Engineer
Netflix
"
=?utf-8?Q?J=C3=B6rn_Franke?= <jornfranke@gmail.com>,"Thu, 15 Dec 2016 18:48:23 +0100",Re: SPARK-18689: A proposal for priority based app scheduling utilizing linux cgroups.,"""Hegner, Travis"" <THegner@trilliumit.com>","Hi,

What about yarn or mesos used in combination with Spark. The have also cgroups. Or a kubernetes etc deployment.

o create a pull request for this code, as I don't believe it's production worthy at the moment. My intent is to better communicate what I'd like to accomplish. Please review the following patch: https://github.com/apache/spark/compare/branch-2.0...travishegner:cgroupScheduler.
o false, and ""spark.executor.shares"" which defaults to None. When cgroups mode is enabled, a single executor is created on each worker, with access to all cores. The worker will create a parent cpu cgroup (on first executor launch) called ""spark-worker"" to house any executors that it launches. Each executor is put into it's own cgroup named with the app id, under the parent cgroup. The cpu.shares parameter is set to the value in ""spark.executor.shares"", if this is ""None"", it inherits the value from the parent cgroup.
e not run unit tests. I do not know if/how cgroups v2 (kernel 4.5) is going to change this code base, but it looks like the kernel interface is the same for the most part.
, but sat idle. I was then able to launch an application (client deploy-mode) which was also allocated all cores in the cluster, and ran to completion unhindered. Each of the executors on each worker was properly placed into it's respective cgroup, which in turn had the correct cpu.shares value allocated.
plementation)
 don't. (need to put `.destroy()` call in a `finally` block., or in the `maybeCleanupApplication()` method; what do you think?)
er`
hows up as an over allocation of cores per worker)
ed if there is something broken if trying to run this on a different OS)
reating the parent cgroup with correct permissions before launching the worker is all that is necessary)
 (I haven't figured out if/what capability makes cgroups writable, or if it's possible to use a new cgroup mount point)
amed cgroup... s""$appId-driver"" ? default same #shares as executor? default double shares?
r more cgroup features (cpuset, mem, etc...), should the community feel there is value in adding them. Linux cgroups are an extremely powerful resource allocation and isolation tool, and this patch is only scratching the surface of their general capabilities. Of course, as Mr. Loughran's points out, expanding into these features will require more code maintenance, but not enough that we should shy away from it.
 offload as much of the scheduling and resource allocation as possible to the underlying kernel within the node level. Each node's own kernel is the besate a few seconds worth of cpu to the low priority app, while the high priority app is waiting on disk I/O, and instantly give it back to the high priority app when it needs it, with (near) real-time granularity
 kernel the information it needs to allocate the resources correctly. Naturally, the system should allow resource reservations, and even limits, for the purposes of meeting and testing for SLAs and worst case scenarios as well. Linux cgroups are capable of doing those things in a near real-time fashion.
rganization, I believe that everyone can get better throughput out of their hardware, at any cluster size. But, alas, *that* is not a problem I'm trying to solve currently.
rapped around spark's code base structure. Looking forward to any feedback.
lizing linux cgroups.
t the data-center scale. I'm also well aware of the complex nature of schedulers, and that it is an area of ongoing research being done by people/companies with many more resources than I have. This might explain my apprehension in even calling this idea a *scheduler*: I wanted to avoid this exact kind of debate over what I want to accomplish. This is also why I mentioned that this idea will mostly benefit users with small clusters.
ernetes) and the main thing that they have in common is that they don't work well for my use case. Those systems are designed for large scale 1000+ node clusters, and become painful to manage in the small cluster range. Most of the tools that we've attempted to use don't work well for us, so we've written several of our own: https://github.com/trilliumit/.
k, or Amazon: we don't have a *data-center* of servers to manage, we barely have half of a rack. *We are not trying to solve the problem that you are referring to*. We are operating at a level that if we aren't meeting SLAs, then we could just buy another server to add to the cluster. I imagine that we are not alone in that fact either, I've seen that many of the questions on SO and on the user list are from others operating at a level  similar to ours.
 multi-node systems typically handle it gracefully. However, if idle CPU is expensive, then how much more does wasted CPU cost when a nearly complete task is pre-empted and has to be started over? Fortunately for me, that isn't a problem that I have to solve at the moment.
nstances for the different workloads
ave considered the avenue of multiple spark clusters, and in reality the infrastructure we have set up would allow me to do this relatively easily. In fact, in my environment, this is a similar solution to what I'm proposing, just managed one layer up the stack and with less flexibility. I am trying to avoid this solution however because it does require more overhead and maintenance. What if I want two spark apps to run on the same cluster at the same time, sharing the available CPU capacity equally? I can't accomplish that easily with multiple spark clusters. Also, we are a 1 to 2 man operation at this point, I don't have teams of ops people to task with managing as many spark clusters as I feel like launching.
ne. In fact, my initial proposal is net yet changing the allocation of memory as a resource. This would still be statically allocated in a FIFO manner as long as memory is available on the cluster, the same way it is now.
 find a simple solution to a problem that I face, and I'm willing to share that solution, I'd hope one would encourage that instead.
, *I am not trying to write a scheduler*: I am trying to slightly (and optionally) tweak the way executors are allocated and launched, so that I can more intuitively and more optimally utilize my small spark cluster.
lizing linux cgroups.
 to submit work with different credentials and priority; cgroups & equivalent to limit granted resources to requested ones. If you have pre-emption enabled, you can even have one job kill work off the others. Spark does recognise pre-emption failures and doesn't treat it as a sign of problems in the executor, that is: it doesn't over-react.
puting ‚Äîif you are curious about what is state of the art, look at the Morning Paper https://blog.acolyer.org/ for coverage last week of MS and google work there. YARN, Mesos, Borg, whatever Amazon use, at scale it's not just meeting SLAs, its about how much idle CPU costs, and how expensive even a 1-2% drop in throughput would be.
eep into the whole world of cluster scheduling, the debate over centralized vs decentralized, the idelogical one of ""should services ever get allocated RAM/CPU in times of low overall load?"", the challenge of swap, or more specifically, ""how do you throttle memory consumption"", as well as what to do when the IO load of a service is actually incurred on a completely different host from the one your work is running on. 
 Hadoop tree and look at hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux for the cgroup support, and then hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl for the native code needed alongside this. Then consider that it's not just a matter of writing something similar, it's getting an OSS project to actually commit to maintaining such code after you provide that initial contribution.
stances for the different workloads, with different CPU & memory limits, queue priorities, etc. Other people have done the work, written the tests, deployed it in production, met their own SLAs *and are therefore committed to maintaining this stuff*.
sh the scheduling goal I have in mind, I hastily jumped straight into a technical solution, without explaining that goal, or the problem it's attempting to solve.
tandalone scheduler. Perhaps you could/should classify it as a different scheduler, but I don't want to give the impression that this will be as difficult to implement as most schedulers are. Initially, from a memory perspective, we would still allocate in a FIFO manner. This new scheduling mode (or new scheduler, if you'd rather) would mostly benefit any users with small-ish clusters, both on-premise and cloud based. Essentially, my end goal is to be able to run multiple *applications* simultaneously with each application having *access* to the entire core count of the cluster.
ve a second application that I'd like to run hourly. The hourly application is more time critical (higher priority), so I'd like it to finish in a small amount of time. If I allow the first app to run with all cores (this takes several days on my 64 core cluster), then nothing else can be executed when running with the default FIFO scheduler. All of the cores have been allocated to the first application, and it will not release them until it is finished. Dynamic allocation does not help in this case, as there is always a backlog of tasks to run until the first application is nearing the end anyway. Naturally, I could just limit the number of cores that the first application has access to, but then I have idle cpu time when the second app is not running, and that is not optimal. Secondly in that case, the second application only has access to the *leftover* cores that the first app has not allocated, and will take a considerably longer amount of time to run.
ning without specifying the number of cores they want to utilize (whether intentionally or not). As I'm sure you know, the default is to allocate the entire cluster to this application. The cores allocated to this shell are unavailable to other applications, even if they are just sitting idle while a developer is getting their environment set up to run a very big job interactively. Other developers that would like to launch interactive shells are stuck waiting for the first one to exit their shell.
s many simultaneous applications to be running as the cluster memory (still statically partitioned, at least initially) will allow. Applications could be configured with a ""cpu shares"" parameter (just an arbitrary integer relative only to other applications) which is essentially just passed through to the linux cgroup cpu.shares setting. Since each executor of an application on a given worker runs in it's own process/jvm, then that process could be easily be placed into a cgroup created and dedicated for that application.
 processes competing for cpu time are allocated a percentage of time equal to their share count as a percentage of all shares in that level of the cgroup hierarchy. If two applications are both scheduled on the same core with the same weight, each will get to utilize 50% of the time on that core. This is all built into the kernel, and the only thing the spark worker has to do is create a cgroup for each application, set the cpu.shares parameter, and assign the executors for that application to the new cgroup. If multiple executors are running on a single worker, for a single application, the cpu time available to that application is divided  among each of those executors equally. The default for cpu.shares is that they are not limiting in any way. A process can consume all available cpu time if it would otherwise be idle anyway.
city. Current consensus is ""no"". Why not? Because you may end up writing an SLA-sensitive app which just happens to meet it's SLAs in times of light cluster load, but precisely when the cluster is busy, it suddenly slows down, leading to stress all round, in the ""why is this service suddenly unusable"" kind of stress. Instead you keep the cluster busy with low priority preemptible work, use labels to allocate specific hosts to high-SLA apps, etc.
o some abstraction) is that cpu share allocations are heterogeneous to all processes running on a machine. An admin could have very fine grained control over which processes get priority access to cpu time, depending on their needs.
ication could utilize 100% of all cluster cores if they are idle. Then my time sensitive app could be launched with nine times the priority and the linux kernel would scale back the first application to 10% of all cores (completely seemlessly and automatically: no pre-emption, just fewer time slices of cpu allocated by the kernel to the first application), while the second application gets 90% of all the cores until it completes.
rocess starts to swap, it hurts everything else. But Java isn't that good at handling limited heap/memory size; you have to spec that heap up front. 
ode would create an increase in context switching on each host. This issue is somewhat mitigated by still statically allocating memory however, since there wouldn't typically be an exorbitant number of applications running at once.
es. Linux cgroups allow you to control access to more than just cpu shares. You can apply the same concept to other resources (memory, disk io). You can also set up hard limits so that an application will never get more than is allocated to it. I know that those limitations are important for some use cases involving predictability of application execution times. Eventually, this idea could be expanded to include many more of the features that cgroups provide.
t a bit better now. Does anyone else can see value in it?
o show just how complex it gets in a large system. Before you begin to write a line of code, I'd recommend
nd microsoft papers, Facebook's FairScheduler work, etc, etc.
blic, that's YARN and Mesos.
l considered a place for cutting-edge research. Avoid unless you want to do that.
"
"""Hegner, Travis"" <THegner@trilliumit.com>","Thu, 15 Dec 2016 18:38:46 +0000","Re: SPARK-18689: A proposal for priority based app scheduling
 utilizing linux cgroups.",=?Windows-1252?Q?J=F6rn_Franke?= <jornfranke@gmail.com>,"Thanks for the response Jˆrn,

This patch is intended only for spark standalone.

My understanding of the YARN cgroup support is that it only limits cpu, rather than allocates it based on the priority or shares system. This could be old documentation that I'm remembering, however. Another issue with YARN is that it has a lot more overhead than standalone mode, and always seemed a bit less responsive in general. Lastly, I remember struggling greatly with yet another resource abstraction layer (as if spark doesn't have enough already), it still statically allocated cores (albeit virtual ones), and it was much more cumbersome to find a proper balance of resources to request for an app.

My experience in trying to accomplish something like this in Mesos was always met with frustration because the system still statically allocated cores away to be reserved by individual apps. Trying to adjust the priority of individual applications was only possible by increasing the core count, further starving other apps of available cores. It was impossible to give a priority lower than the default to an app. The cpu.shares parameter was abstracted away as a multiple of the number of requested cores, which had a double down affect on the app: not only was it given more cores, it was also given a higher priority to run on them. Perhaps this has changed in more recent versions, but this was my experience when testing it.

I'm not familiar with a spark scheduler for kubernetes, unless you mean to launch a standalone cluster in containers with kubernetes? In that case, this patch would simply divvy up the resources allocated to the spark-worker container among each of it's executors, based on the shares that each executor is given. This is similar to how my current environment works, I'm just not using kubernetes as a container launcher. I found kubernetes was quite limiting in the way we wanted our network to be structured, and it also seemed quite difficult to get new functionality exposed in the form of their yaml API system.

My goal with this patch is to essentially eliminate the static allocation of cpu cores at all. Give each app time on the cpu equal to the number of shares it has as a percentage of the total pool.

Thanks,

Travis

________________________________
From: Jˆrn Franke <jornfranke@gmail.com>
Sent: Thursday, December 15, 2016 12:48
To: Hegner, Travis
Cc: Apache Spark Dev
Subject: Re: SPARK-18689: A proposal for priority based app scheduling utilizing linux cgroups.

Hi,

What about yarn or mesos used in combination with Spark. The have also cgroups. Or a kubernetes etc deployment.



Hello Spark Devs,


I have finally completed a mostly working proof of concept. I do not want to create a pull request for this code, as I don't believe it's production worthy at the moment. My intent is to better communicate what I'd like to accomplish. Please review the following patch: https://github.com/apache/spark/compare/branch-2.0...travishegner:cgroupScheduler.


What the code does:


Currently, it exposes two options ""spark.cgroups.enabled"", which defaults to false, and ""spark.executor.shares"" which defaults to None. When cgroups mode is enabled, a single executor is created on each worker, with access to all cores. The worker will create a parent cpu cgroup (on first executor launch) called ""spark-worker"" to house any executors that it launches. Each executor is put into it's own cgroup named with the app id, under the parent cgroup. The cpu.shares parameter is set to the value in ""spark.executor.shares"", if this is ""None"", it inherits the value from the parent cgroup.


Tested on Ubuntu 16:04 (docker containers), kernel 4.4.0-53-generic: I have not run unit tests. I do not know if/how cgroups v2 (kernel 4.5) is going to change this code base, but it looks like the kernel interface is the same for the most part.


I was able to launch a spark shell which consumed all cores in the cluster, but sat idle. I was then able to launch an application (client deploy-mode) which was also allocated all cores in the cluster, and ran to completion unhindered. Each of the executors on each worker was properly placed into it's respective cgroup, which in turn had the correct cpu.shares value allocated.


What the code still needs:


* Documentation (assuming the community moves forward with some kind of implementation)

* Sometimes the cgroups get destroyed after app completion, sometimes they don't. (need to put `.destroy()` call in a `finally` block., or in the `maybeCleanupApplication()` method; what do you think?)

* Proper handling of drivers's resources when running `--deploy-mode cluster`

* Better web UI indication of cgroup mode or core sharing (currently just shows up as an over allocation of cores per worker)

* Better environment/os/platform detection and testing (I won't be surprised if there is something broken if trying to run this on a different OS)

* Security/permissions for cgroups if running worker as non-root (perhaps creating the parent cgroup with correct permissions before launching the worker is all that is necessary)

  - running the worker in a container currently requires --privileged mode (I haven't figured out if/what capability makes cgroups writable, or if it's possible to use a new cgroup mount point)

* More user defined options

  - cgroup root path (currently hard coded)

  - driver cpu.shares (for cluster deploy-mode: would require a specially named cgroup... s""$appId-driver"" ? default same #shares as executor? default double shares?

  - parent cpu.shares (currently os default)

  - parent cgroup name (currently hard coded)


I tried to structure the initial concept to make it easy to add support for more cgroup features (cpuset, mem, etc...), should the community feel there is value in adding them. Linux cgroups are an extremely powerful resource allocation and isolation tool, and this patch is only scratching the surface of their general capabilities. Of course, as Mr. Loughran's points out, expanding into these features will require more code maintenance, but not enough that we should shy away from it.


<opinion>

I personally believe that any multi-node resource allocation system should offload as much of the scheduling and resource allocation as possible to the underlying kernel within the node level. Each node's own kernel is the beocate a few seconds worth of cpu to the low priority app, while the high priority app is waiting on disk I/O, and instantly give it back to the high priority app when it needs it, with (near) real-time granularity


The multi-node system should set up a proper framework to give each node's kernel the information it needs to allocate the resources correctly. Naturally, the system should allow resource reservations, and even limits, for the purposes of meeting and testing for SLAs and worst case scenarios as well. Linux cgroups are capable of doing those things in a near real-time fashion.


With a proper convention of priorities/shares for applications within an organization, I believe that everyone can get better throughput out of their hardware, at any cluster size. But, alas, *that* is not a problem I'm trying to solve currently.

</opinion>

Sorry that the patch is pretty rough still, as I'm still getting my head wrapped around spark's code base structure. Looking forward to any feedback.

Thanks,

Travis

________________________________
From: Hegner, Travis <THegner@trilliumit.com<mailto:THegner@trilliumit.com>Sent: Tuesday, December 6, 2016 10:49
To: Steve Loughran
Cc: Shuai Lin; Apache Spark Dev
Subject: Re: SPARK-18689: A proposal for priority based app scheduling utilizing linux cgroups.



Steve,

I appreciate your experience and insight when dealing with large clusters at the data-center scale. I'm also well aware of the complex nature of schedulers, and that it is an area of ongoing research being done by people/companies with many more resources than I have. This might explain my apprehension in even calling this idea a *scheduler*: I wanted to avoid this exact kind of debate over what I want to accomplish. This is also why I mentioned that this idea will mostly benefit users with small clusters.

I've used many of the big named ""cluster schedulers"" (YARN, Mesos, and Kubernetes) and the main thing that they have in common is that they don't work well for my use case. Those systems are designed for large scale 1000+ node clusters, and become painful to manage in the small cluster range. Most of the tools that we've attempted to use don't work well for us, so we've written several of our own: https://github.com/trilliumit/.

It can be most easily stated by the fact that *we are not* Google, Facebook, or Amazon: we don't have a *data-center* of servers to manage, we barely have half of a rack. *We are not trying to solve the problem that you are referring to*. We are operating at a level that if we aren't meeting SLAs, then we could just buy another server to add to the cluster. I imagine that we are not alone in that fact either, I've seen that many of the questions on SO and on the user list are from others operating at a level similar to ours.

I understand that pre-emption isn't inherently a bad thing, and that these multi-node systems typically handle it gracefully. However, if idle CPU is expensive, then how much more does wasted CPU cost when a nearly complete task is pre-empted and has to be started over? Fortunately for me, that isn't a problem that I have to solve at the moment.

stances for the different workloads

See my above comment on how well these cluster schedulers work for us. I have considered the avenue of multiple spark clusters, and in reality the infrastructure we have set up would allow me to do this relatively easily. In fact, in my environment, this is a similar solution to what I'm proposing, just managed one layer up the stack and with less flexibility. I am trying to avoid this solution however because it does require more overhead and maintenance. What if I want two spark apps to run on the same cluster at the same time, sharing the available CPU capacity equally? I can't accomplish that easily with multiple spark clusters. Also, we are a 1 to 2 man operation at this point, I don't have teams of ops people to task with managing as many spark clusters as I feel like launching.


Perhaps in the use-cases you have experience with, but not currently in mine. In fact, my initial proposal is net yet changing the allocation of memory as a resource. This would still be statically allocated in a FIFO manner as long as memory is available on the cluster, the same way it is now.


Thanks for the suggestion, but I will choose how I spend my time. If I can find a simple solution to a problem that I face, and I'm willing to share that solution, I'd hope one would encourage that instead.


Perhaps I haven't yet clearly communicated what I'm trying to do. In short, *I am not trying to write a scheduler*: I am trying to slightly (and optionally) tweak the way executors are allocated and launched, so that I can more intuitively and more optimally utilize my small spark cluster.

Thanks,

Travis

________________________________
From: Steve Loughran <stevel@hortonworks.com<mailto:stevel@hortonworks.com>Sent: Tuesday, December 6, 2016 06:54
To: Hegner, Travis
Cc: Shuai Lin; Apache Spark Dev
Subject: Re: SPARK-18689: A proposal for priority based app scheduling utilizing linux cgroups.

This is essentially what the cluster schedulers do: allow different people to submit work with different credentials and priority; cgroups & equivalent to limit granted resources to requested ones. If you have pre-emption enabled, you can even have one job kill work off the others. Spark does recognise pre-emption failures and doesn't treat it as a sign of problems in the executor, that is: it doesn't over-react.

cluster scheduling is one of the cutting edge bits of datacentre-scale computing óif you are curious about what is state of the art, look at the Morning Paper https://blog.acolyer.org/ for coverage last week of MS and google work there. YARN, Mesos, Borg, whatever Amazon use, at scale it's not just meeting SLAs, its about how much idle CPU costs, and how expensive even a 1-2% drop in throughput would be.


I would strongly encourage you to avoid this topic, unless you want dive deep into the whole world of cluster scheduling, the debate over centralized vs decentralized, the idelogical one of ""should services ever get allocated RAM/CPU in times of low overall load?"", the challenge of swap, or more specifically, ""how do you throttle memory consumption"", as well as what to do when the IO load of a service is actually incurred on a completely different host from the one your work is running on.

There's also a fair amount of engineering work; to get a hint download the Hadoop tree and look at hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux for the cgroup support, and then hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl for the native code needed alongside this. Then consider that it's not just a matter of writing something similar, it's getting an OSS project to actually commit to maintaining such code after you provide that initial contribution.

Instead? Use a multi-user cluster scheduler and spin up different spark instances for the different workloads, with different CPU & memory limits, queue priorities, etc. Other people have done the work, written the tests, deployed it in production, met their own SLAs *and are therefore committed to maintaining this stuff*.

-Steve


My apologies, in my excitement of finding a rather simple way to accomplish the scheduling goal I have in mind, I hastily jumped straight into a technical solution, without explaining that goal, or the problem it's attempting to solve.

You are correct that I'm looking for an additional running mode for the standalone scheduler. Perhaps you could/should classify it as a different scheduler, but I don't want to give the impression that this will be as difficult to implement as most schedulers are. Initially, from a memory perspective, we would still allocate in a FIFO manner. This new scheduling mode (or new scheduler, if you'd rather) would mostly benefit any users with small-ish clusters, both on-premise and cloud based. Essentially, my end goal is to be able to run multiple *applications* simultaneously with each application having *access* to the entire core count of the cluster.

I have a very cpu intensive application that I'd like to run weekly. I have a second application that I'd like to run hourly. The hourly application is more time critical (higher priority), so I'd like it to finish in a small amount of time. If I allow the first app to run with all cores (this takes several days on my 64 core cluster), then nothing else can be executed when running with the default FIFO scheduler. All of the cores have been allocated to the first application, and it will not release them until it is finished. Dynamic allocation does not help in this case, as there is always a backlog of tasks to run until the first application is nearing the end anyway. Naturally, I could just limit the number of cores that the first application has access to, but then I have idle cpu time when the second app is not running, and that is not optimal. Secondly in that case, the second application only has access to the *leftover* cores that the first app has not allocated, and will take a considerably longer amount of time to run.

You could also imagine a scenario where a developer has a spark-shell running without specifying the number of cores they want to utilize (whether intentionally or not). As I'm sure you know, the default is to allocate the entire cluster to this application. The cores allocated to this shell are unavailable to other applications, even if they are just sitting idle while a developer is getting their environment set up to run a very big job interactively. Other developers that would like to launch interactive shells are stuck waiting for the first one to exit their shell.

My proposal would eliminate this static nature of core counts and allow as many simultaneous applications to be running as the cluster memory (still statically partitioned, at least initially) will allow. Applications could be configured with a ""cpu shares"" parameter (just an arbitrary integer relative only to other applications) which is essentially just passed through to the linux cgroup cpu.shares setting. Since each executor of an application on a given worker runs in it's own process/jvm, then that process could be easily be placed into a cgroup created and dedicated for that application.

Linux cgroups cpu.shares are pretty well documented, but the gist is that processes competing for cpu time are allocated a percentage of time equal to their share count as a percentage of all shares in that level of the cgroup hierarchy. If two applications are both scheduled on the same core with the same weight, each will get to utilize 50% of the time on that core. This is all built into the kernel, and the only thing the spark worker has to do is create a cgroup for each application, set the cpu.shares parameter, and assign the executors for that application to the new cgroup. If multiple executors are running on a single worker, for a single application, the cpu time available to that application is divided among each of those executors equally. The default for cpu.shares is that they are not limiting in any way. A process can consume all available cpu time if it would otherwise be idle anyway.


That's the issue that surfaces in google papers: should jobs get idle capacity. Current consensus is ""no"". Why not? Because you may end up writing an SLA-sensitive app which just happens to meet it's SLAs in times of light cluster load, but precisely when the cluster is busy, it suddenly slows down, leading to stress all round, in the ""why is this service suddenly unusable"" kind of stress. Instead you keep the cluster busy with low priority preemptible work, use labels to allocate specific hosts to high-SLA apps, etc.


Another benefit to passing cpu.shares directly to the kernel (as opposed to some abstraction) is that cpu share allocations are heterogeneous to all processes running on a machine. An admin could have very fine grained control over which processes get priority access to cpu time, depending on their needs.



To continue my personal example above, my long running cpu intensive application could utilize 100% of all cluster cores if they are idle. Then my time sensitive app could be launched with nine times the priority and the linux kernel would scale back the first application to 10% of all cores (completely seemlessly and automatically: no pre-emption, just fewer time slices of cpu allocated by the kernel to the first application), while the second application gets 90% of all the cores until it completes.


FWIW, it's often memory consumption that's most problematic here. If one process starts to swap, it hurts everything else. But Java isn't that good at handling limited heap/memory size; you have to spec that heap up front.


The only downside that I can think of currently is that this scheduling mode would create an increase in context switching on each host. This issue is somewhat mitigated by still statically allocating memory however, since there wouldn't typically be an exorbitant number of applications running at once.

In my opinion, this would allow the most optimal usage of cluster resources. Linux cgroups allow you to control access to more than just cpu shares. You can apply the same concept to other resources (memory, disk io). You can also set up hard limits so that an application will never get more than is allocated to it. I know that those limitations are important for some use cases involving predictability of application execution times. Eventually, this idea could be expanded to include many more of the features that cgroups provide.

Thanks again for any feedback on this idea. I hope that I have explained it a bit better now. Does anyone else can see value in it?



I'm not saying ""don't get involved in the scheduling problem""; I'm trying to show just how complex it gets in a large system. Before you begin to write a line of code, I'd recommend

-you read as much of the published work as you can, including the google and microsoft papers, Facebook's FairScheduler work, etc, etc.
-have a look at the actual code inside those schedulers whose source is public, that's YARN and Mesos.
-try using these schedulers for your own workloads.

really: scheduling work across a datacentre a complex problem that is still considered a place for cutting-edge research. Avoid unless you want to do that.

-Steve


"
Reynold Xin <rxin@databricks.com>,"Thu, 15 Dec 2016 11:07:52 -0800","Re: SPARK-18689: A proposal for priority based app scheduling
 utilizing linux cgroups.","""Hegner, Travis"" <THegner@trilliumit.com>","In general this falls directly into the domain of external cluster managers
(YARN, Mesos, Kub). The standalone thing was meant as a simple way to
deploy Spark, and we gotta be careful with introducing a lot more features
to it because then it becomes just a full fledged cluster manager and is
duplicating the work of the other more mature ones.

Have you thought about contributing specific changes to these cluster
managers to address the gaps you have seen?




ld
RN
d
ty
,
 a
 a
o
t
n
s
or
ch
e
o
y
nt
e
d
e
gh
gh
s
rally,
'm
d
y
le
we
.
of
e
s
.
 I
e
 1
k
n
nd
e
of
k at the
ve
a
e
ntainer-executor/impl
st
,
,
d
y
g
l
se
is
 a
l.
s
d
e
s
y
ll
n
r
le
d
.
t.
u
t
e
"
Mario Fernandez Villa <mario.fernandez.villa@everis.com>,"Thu, 15 Dec 2016 15:37:38 +0000",Mistake in Apache Spark Java.,"""user@spark.apache.org"" <user@spark.apache.org>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","Hello,

My name is Mario Fern·ndez and I¥m a Big Data developer, I usually program in Apache Spark in Java, and we have a big problem to read properly a csv file. The issue is that:

When I want to read csv file, for instance, with semicolon delimiter, the dataframe take semicolon like delimiter ant that¥s correct, but also take comma like delimiter and that¥s the problem.
I check this problem in Apache Spark 2.10, 1.6.2 DataFrame and also in Apache Spark 2.11 2.0.2 Dataset, and troubles are the same.

dfFile1 = sqlContext.read()       .format(""com.databricks.spark.csv"")
                                  .schema(customSchema)
                                  .option(""charset"", ""Cp1252"")
                                  .option(""header"", ""true"")
                                  .option(""delimiter"", "";"")
                                  .load(path);

When I read a csv file like that, dataFrame take like delimiter the yellow letters:
Number;Name;Surname;Category
129.363;Mathew, Thomas;Johnson;Centers Technician

And the comma between Mathew and Thomas, shouldn¥t be take like delimiter.

I would like to know if that¥s problem is a bug and you are going to correct or the way to read simply is like that.

Thank you so much in advance.

Kind Regards.




________________________________

AVISO DE CONFIDENCIALIDAD.
Este correo y la informaciÛn contenida o adjunta al mismo es privada y confidencial y va dirigida exclusivamente a su destinatario. everis informa a quien pueda haber recibido este correo por error que contiene informaciÛn confidencial cuyo uso, copia, reproducciÛn o distribuciÛn est· expresamente prohibida. Si no es Vd. el destinatario del mismo y recibe este correo por error, le rogamos lo ponga en conocimiento del emisor y proceda a su eliminaciÛn sin copiarlo, imprimirlo o utilizarlo de ning˙n modo.

CONFIDENTIALITY WARNING.
This message and the information contained in or attached to it are private and confidential and intended exclusively for the addressee. everis informs to whom it may receive it in error that it contains privileged information and its use, copy, reproduction or distribution is prohibited. If you are not an intended recipient of this E-mail, please notify the sender, delete it and do not read, act upon, print, disclose, copy, retain or redistribute any portion of this E-mail.
"
"""Chawla,Sumit "" <sumitkchawla@gmail.com>","Thu, 15 Dec 2016 11:33:37 -0800",Re: Output Side Effects for different chain of operations,"User <user@spark.apache.org>, dev@spark.apache.org","Any suggestions on this one?

Regards
Sumit Chawla



"
Reynold Xin <rxin@databricks.com>,"Thu, 15 Dec 2016 11:42:15 -0800",Re: Output Side Effects for different chain of operations,"""Chawla,Sumit"" <sumitkchawla@gmail.com>","You can just write some files out directly (and idempotently) in your
map/mapPartitions functions. It is just a function that you can run
arbitrary code after all.



"
Jim Hughes <jnh5y@ccri.com>,"Thu, 15 Dec 2016 14:48:24 -0500",Re: Expand the Spark SQL programming guide?,dev@spark.apache.org,"Hi Anton,

I'd like to see this as well.  I've been working on implementing 
geospatial user-defined types and functions.  Having examples of 
aggregations and window functions would be awesome!

I did test out implementing a distributed convex hull as a 
UserDefinedAggregateFunction, and that seemed to work sensibly.

Cheers,

Jim



"
Michael Armbrust <michael@databricks.com>,"Thu, 15 Dec 2016 13:21:29 -0800",Re: Expand the Spark SQL programming guide?,Jim Hughes <jnh5y@ccri.com>,"Pull requests would be welcome for any major missing features in the guide:
https://github.com/apache/spark/blob/master/docs/sql-programming-guide.md


"
"""Hegner, Travis"" <THegner@trilliumit.com>","Thu, 15 Dec 2016 21:32:08 +0000","Re: SPARK-18689: A proposal for priority based app scheduling
 utilizing linux cgroups.",Reynold Xin <rxin@databricks.com>,"
ta be careful with introducing a lot more features to it because then it becomes just a full fledged cluster manager and is duplicating the work of the other more mature ones.

Understood. There seems to be a very fine line between making ""standalone"" better, or more intuitive, versus just not using it. The only thing I'm after personally is the ability to run more than one application at once, on the same spark standalone cluster, sharing all available cores with a predetermined level of priority. To me, that is a more intuitive way for it to run. This patch accomplishes that in a little over 100 lines added to the code base. Wanting to leave it at that point without any further expansion to avoid duplicate work is understandable.

gers to address the gaps you have seen?

Somewhat. I've tested each of those cluster managers and even contributed to Mesos for some docker functionality. The basic foundation in each of the designs is through static core allocation, which doesn't work well when you want to share workloads on a small cluster, while still giving those workloads full access to all resources when they are available. I haven't studied the YARN code base closely, so I can't comment on how difficult it would be to contribute similar changes there. I don't currently run a YARN cluster, I only intend to run spark jobs, and the standalone mode was better for those in my tests.

Studying the way spark standalone launches its executors led me to realize that there was a shortcut, and a very small amount of code, to accomplish my goal. Granted, my goal seems to be a completely different concept in the way cpu time is allocated, compared to how any of the cluster managers are doing it. But it is the way the kernel manages cpu time within a host. Everything that runs in linux is already running in a cgroup, and is allocated cpu time based on it's shares. Imagine if your workstation could only one run process at a time, all resources reserved for just that process whether it needed them or not.

I'm not trying to turn spark standalone into a cluster manager. I know I keep mentioning ways to expand cgroup functionality, but that is because I see a lot of value in allowing the kernel to manage the resources there. That is not my short term goal, however. My short term goal is allowing multiple simultaneous spark applications to run on the same cluster. Does anyone see value in just that short term goal?

Thanks,

Travis

________________________________
From: Reynold Xin <rxin@databricks.com>
Sent: Thursday, December 15, 2016 14:07
To: Hegner, Travis
Cc: Jˆrn Franke; Apache Spark Dev
Subject: Re: SPARK-18689: A proposal for priority based app scheduling utilizing linux cgroups.

In general this falls directly into the domain of external cluster managers (YARN, Mesos, Kub). The standalone thing was meant as a simple way to deploy Spark, and we gotta be careful with introducing a lot more features to it because then it becomes just a full fledged cluster manager and is duplicating the work of the other more mature ones.

Have you thought about contributing specific changes to these cluster managers to address the gaps you have seen?




Thanks for the response Jˆrn,

This patch is intended only for spark standalone.

My understanding of the YARN cgroup support is that it only limits cpu, rather than allocates it based on the priority or shares system. This could be old documentation that I'm remembering, however. Another issue with YARN is that it has a lot more overhead than standalone mode, and always seemed a bit less responsive in general. Lastly, I remember struggling greatly with yet another resource abstraction layer (as if spark doesn't have enough already), it still statically allocated cores (albeit virtual ones), and it was much more cumbersome to find a proper balance of resources to request for an app.

My experience in trying to accomplish something like this in Mesos was always met with frustration because the system still statically allocated cores away to be reserved by individual apps. Trying to adjust the priority of individual applications was only possible by increasing the core count, further starving other apps of available cores. It was impossible to give a priority lower than the default to an app. The cpu.shares parameter was abstracted away as a multiple of the number of requested cores, which had a double down affect on the app: not only was it given more cores, it was also given a higher priority to run on them. Perhaps this has changed in more recent versions, but this was my experience when testing it.

I'm not familiar with a spark scheduler for kubernetes, unless you mean to launch a standalone cluster in containers with kubernetes? In that case, this patch would simply divvy up the resources allocated to the spark-worker container among each of it's executors, based on the shares that each executor is given. This is similar to how my current environment works, I'm just not using kubernetes as a container launcher. I found kubernetes was quite limiting in the way we wanted our network to be structured, and it also seemed quite difficult to get new functionality exposed in the form of their yaml API system.

My goal with this patch is to essentially eliminate the static allocation of cpu cores at all. Give each app time on the cpu equal to the number of shares it has as a percentage of the total pool.

Thanks,

Travis

________________________________
From: Jˆrn Franke <jornfranke@gmail.com<mailto:jornfranke@gmail.com>>
Sent: Thursday, December 15, 2016 12:48
To: Hegner, Travis
Cc: Apache Spark Dev

Subject: Re: SPARK-18689: A proposal for priority based app scheduling utilizing linux cgroups.

Hi,

What about yarn or mesos used in combination with Spark. The have also cgroups. Or a kubernetes etc deployment.



Hello Spark Devs,


I have finally completed a mostly working proof of concept. I do not want to create a pull request for this code, as I don't believe it's production worthy at the moment. My intent is to better communicate what I'd like to accomplish. Please review the following patch: https://github.com/apache/spark/compare/branch-2.0...travishegner:cgroupScheduler.


What the code does:


Currently, it exposes two options ""spark.cgroups.enabled"", which defaults to false, and ""spark.executor.shares"" which defaults to None. When cgroups mode is enabled, a single executor is created on each worker, with access to all cores. The worker will create a parent cpu cgroup (on first executor launch) called ""spark-worker"" to house any executors that it launches. Each executor is put into it's own cgroup named with the app id, under the parent cgroup. The cpu.shares parameter is set to the value in ""spark.executor.shares"", if this is ""None"", it inherits the value from the parent cgroup.


Tested on Ubuntu 16:04 (docker containers), kernel 4.4.0-53-generic: I have not run unit tests. I do not know if/how cgroups v2 (kernel 4.5) is going to change this code base, but it looks like the kernel interface is the same for the most part.


I was able to launch a spark shell which consumed all cores in the cluster, but sat idle. I was then able to launch an application (client deploy-mode) which was also allocated all cores in the cluster, and ran to completion unhindered. Each of the executors on each worker was properly placed into it's respective cgroup, which in turn had the correct cpu.shares value allocated.


What the code still needs:


* Documentation (assuming the community moves forward with some kind of implementation)

* Sometimes the cgroups get destroyed after app completion, sometimes they don't. (need to put `.destroy()` call in a `finally` block., or in the `maybeCleanupApplication()` method; what do you think?)

* Proper handling of drivers's resources when running `--deploy-mode cluster`

* Better web UI indication of cgroup mode or core sharing (currently just shows up as an over allocation of cores per worker)

* Better environment/os/platform detection and testing (I won't be surprised if there is something broken if trying to run this on a different OS)

* Security/permissions for cgroups if running worker as non-root (perhaps creating the parent cgroup with correct permissions before launching the worker is all that is necessary)

  - running the worker in a container currently requires --privileged mode (I haven't figured out if/what capability makes cgroups writable, or if it's possible to use a new cgroup mount point)

* More user defined options

  - cgroup root path (currently hard coded)

  - driver cpu.shares (for cluster deploy-mode: would require a specially named cgroup... s""$appId-driver"" ? default same #shares as executor? default double shares?

  - parent cpu.shares (currently os default)

  - parent cgroup name (currently hard coded)


I tried to structure the initial concept to make it easy to add support for more cgroup features (cpuset, mem, etc...), should the community feel there is value in adding them. Linux cgroups are an extremely powerful resource allocation and isolation tool, and this patch is only scratching the surface of their general capabilities. Of course, as Mr. Loughran's points out, expanding into these features will require more code maintenance, but not enough that we should shy away from it.


<opinion>

I personally believe that any multi-node resource allocation system should offload as much of the scheduling and resource allocation as possible to the underlying kernel within the node level. Each node's own kernel is the beocate a few seconds worth of cpu to the low priority app, while the high priority app is waiting on disk I/O, and instantly give it back to the high priority app when it needs it, with (near) real-time granularity


The multi-node system should set up a proper framework to give each node's kernel the information it needs to allocate the resources correctly. Naturally, the system should allow resource reservations, and even limits, for the purposes of meeting and testing for SLAs and worst case scenarios as well. Linux cgroups are capable of doing those things in a near real-time fashion.


With a proper convention of priorities/shares for applications within an organization, I believe that everyone can get better throughput out of their hardware, at any cluster size. But, alas, *that* is not a problem I'm trying to solve currently.

</opinion>

Sorry that the patch is pretty rough still, as I'm still getting my head wrapped around spark's code base structure. Looking forward to any feedback.

Thanks,

Travis

________________________________
From: Hegner, Travis <THegner@trilliumit.com<mailto:THegner@trilliumit.com>Sent: Tuesday, December 6, 2016 10:49
To: Steve Loughran
Cc: Shuai Lin; Apache Spark Dev
Subject: Re: SPARK-18689: A proposal for priority based app scheduling utilizing linux cgroups.



Steve,

I appreciate your experience and insight when dealing with large clusters at the data-center scale. I'm also well aware of the complex nature of schedulers, and that it is an area of ongoing research being done by people/companies with many more resources than I have. This might explain my apprehension in even calling this idea a *scheduler*: I wanted to avoid this exact kind of debate over what I want to accomplish. This is also why I mentioned that this idea will mostly benefit users with small clusters.

I've used many of the big named ""cluster schedulers"" (YARN, Mesos, and Kubernetes) and the main thing that they have in common is that they don't work well for my use case. Those systems are designed for large scale 1000+ node clusters, and become painful to manage in the small cluster range. Most of the tools that we've attempted to use don't work well for us, so we've written several of our own: https://github.com/trilliumit/.

It can be most easily stated by the fact that *we are not* Google, Facebook, or Amazon: we don't have a *data-center* of servers to manage, we barely have half of a rack. *We are not trying to solve the problem that you are referring to*. We are operating at a level that if we aren't meeting SLAs, then we could just buy another server to add to the cluster. I imagine that we are not alone in that fact either, I've seen that many of the questions on SO and on the user list are from others operating at a level similar to ours.

I understand that pre-emption isn't inherently a bad thing, and that these multi-node systems typically handle it gracefully. However, if idle CPU is expensive, then how much more does wasted CPU cost when a nearly complete task is pre-empted and has to be started over? Fortunately for me, that isn't a problem that I have to solve at the moment.

stances for the different workloads

See my above comment on how well these cluster schedulers work for us. I have considered the avenue of multiple spark clusters, and in reality the infrastructure we have set up would allow me to do this relatively easily. In fact, in my environment, this is a similar solution to what I'm proposing, just managed one layer up the stack and with less flexibility. I am trying to avoid this solution however because it does require more overhead and maintenance. What if I want two spark apps to run on the same cluster at the same time, sharing the available CPU capacity equally? I can't accomplish that easily with multiple spark clusters. Also, we are a 1 to 2 man operation at this point, I don't have teams of ops people to task with managing as many spark clusters as I feel like launching.


Perhaps in the use-cases you have experience with, but not currently in mine. In fact, my initial proposal is net yet changing the allocation of memory as a resource. This would still be statically allocated in a FIFO manner as long as memory is available on the cluster, the same way it is now.


Thanks for the suggestion, but I will choose how I spend my time. If I can find a simple solution to a problem that I face, and I'm willing to share that solution, I'd hope one would encourage that instead.


Perhaps I haven't yet clearly communicated what I'm trying to do. In short, *I am not trying to write a scheduler*: I am trying to slightly (and optionally) tweak the way executors are allocated and launched, so that I can more intuitively and more optimally utilize my small spark cluster.

Thanks,

Travis

________________________________
From: Steve Loughran <stevel@hortonworks.com<mailto:stevel@hortonworks.com>Sent: Tuesday, December 6, 2016 06:54
To: Hegner, Travis
Cc: Shuai Lin; Apache Spark Dev
Subject: Re: SPARK-18689: A proposal for priority based app scheduling utilizing linux cgroups.

This is essentially what the cluster schedulers do: allow different people to submit work with different credentials and priority; cgroups & equivalent to limit granted resources to requested ones. If you have pre-emption enabled, you can even have one job kill work off the others. Spark does recognise pre-emption failures and doesn't treat it as a sign of problems in the executor, that is: it doesn't over-react.

cluster scheduling is one of the cutting edge bits of datacentre-scale computing óif you are curious about what is state of the art, look at the Morning Paper https://blog.acolyer.org/ for coverage last week of MS and google work there. YARN, Mesos, Borg, whatever Amazon use, at scale it's not just meeting SLAs, its about how much idle CPU costs, and how expensive even a 1-2% drop in throughput would be.


I would strongly encourage you to avoid this topic, unless you want dive deep into the whole world of cluster scheduling, the debate over centralized vs decentralized, the idelogical one of ""should services ever get allocated RAM/CPU in times of low overall load?"", the challenge of swap, or more specifically, ""how do you throttle memory consumption"", as well as what to do when the IO load of a service is actually incurred on a completely different host from the one your work is running on.

There's also a fair amount of engineering work; to get a hint download the Hadoop tree and look at hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux for the cgroup support, and then hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl for the native code needed alongside this. Then consider that it's not just a matter of writing something similar, it's getting an OSS project to actually commit to maintaining such code after you provide that initial contribution.

Instead? Use a multi-user cluster scheduler and spin up different spark instances for the different workloads, with different CPU & memory limits, queue priorities, etc. Other people have done the work, written the tests, deployed it in production, met their own SLAs *and are therefore committed to maintaining this stuff*.

-Steve


My apologies, in my excitement of finding a rather simple way to accomplish the scheduling goal I have in mind, I hastily jumped straight into a technical solution, without explaining that goal, or the problem it's attempting to solve.

You are correct that I'm looking for an additional running mode for the standalone scheduler. Perhaps you could/should classify it as a different scheduler, but I don't want to give the impression that this will be as difficult to implement as most schedulers are. Initially, from a memory perspective, we would still allocate in a FIFO manner. This new scheduling mode (or new scheduler, if you'd rather) would mostly benefit any users with small-ish clusters, both on-premise and cloud based. Essentially, my end goal is to be able to run multiple *applications* simultaneously with each application having *access* to the entire core count of the cluster.

I have a very cpu intensive application that I'd like to run weekly. I have a second application that I'd like to run hourly. The hourly application is more time critical (higher priority), so I'd like it to finish in a small amount of time. If I allow the first app to run with all cores (this takes several days on my 64 core cluster), then nothing else can be executed when running with the default FIFO scheduler. All of the cores have been allocated to the first application, and it will not release them until it is finished. Dynamic allocation does not help in this case, as there is always a backlog of tasks to run until the first application is nearing the end anyway. Naturally, I could just limit the number of cores that the first application has access to, but then I have idle cpu time when the second app is not running, and that is not optimal. Secondly in that case, the second application only has access to the *leftover* cores that the first app has not allocated, and will take a considerably longer amount of time to run.

You could also imagine a scenario where a developer has a spark-shell running without specifying the number of cores they want to utilize (whether intentionally or not). As I'm sure you know, the default is to allocate the entire cluster to this application. The cores allocated to this shell are unavailable to other applications, even if they are just sitting idle while a developer is getting their environment set up to run a very big job interactively. Other developers that would like to launch interactive shells are stuck waiting for the first one to exit their shell.

My proposal would eliminate this static nature of core counts and allow as many simultaneous applications to be running as the cluster memory (still statically partitioned, at least initially) will allow. Applications could be configured with a ""cpu shares"" parameter (just an arbitrary integer relative only to other applications) which is essentially just passed through to the linux cgroup cpu.shares setting. Since each executor of an application on a given worker runs in it's own process/jvm, then that process could be easily be placed into a cgroup created and dedicated for that application.

Linux cgroups cpu.shares are pretty well documented, but the gist is that processes competing for cpu time are allocated a percentage of time equal to their share count as a percentage of all shares in that level of the cgroup hierarchy. If two applications are both scheduled on the same core with the same weight, each will get to utilize 50% of the time on that core. This is all built into the kernel, and the only thing the spark worker has to do is create a cgroup for each application, set the cpu.shares parameter, and assign the executors for that application to the new cgroup. If multiple executors are running on a single worker, for a single application, the cpu time available to that application is divided among each of those executors equally. The default for cpu.shares is that they are not limiting in any way. A process can consume all available cpu time if it would otherwise be idle anyway.


That's the issue that surfaces in google papers: should jobs get idle capacity. Current consensus is ""no"". Why not? Because you may end up writing an SLA-sensitive app which just happens to meet it's SLAs in times of light cluster load, but precisely when the cluster is busy, it suddenly slows down, leading to stress all round, in the ""why is this service suddenly unusable"" kind of stress. Instead you keep the cluster busy with low priority preemptible work, use labels to allocate specific hosts to high-SLA apps, etc.


Another benefit to passing cpu.shares directly to the kernel (as opposed to some abstraction) is that cpu share allocations are heterogeneous to all processes running on a machine. An admin could have very fine grained control over which processes get priority access to cpu time, depending on their needs.



To continue my personal example above, my long running cpu intensive application could utilize 100% of all cluster cores if they are idle. Then my time sensitive app could be launched with nine times the priority and the linux kernel would scale back the first application to 10% of all cores (completely seemlessly and automatically: no pre-emption, just fewer time slices of cpu allocated by the kernel to the first application), while the second application gets 90% of all the cores until it completes.


FWIW, it's often memory consumption that's most problematic here. If one process starts to swap, it hurts everything else. But Java isn't that good at handling limited heap/memory size; you have to spec that heap up front.


The only downside that I can think of currently is that this scheduling mode would create an increase in context switching on each host. This issue is somewhat mitigated by still statically allocating memory however, since there wouldn't typically be an exorbitant number of applications running at once.

In my opinion, this would allow the most optimal usage of cluster resources. Linux cgroups allow you to control access to more than just cpu shares. You can apply the same concept to other resources (memory, disk io). You can also set up hard limits so that an application will never get more than is allocated to it. I know that those limitations are important for some use cases involving predictability of application execution times. Eventually, this idea could be expanded to include many more of the features that cgroups provide.

Thanks again for any feedback on this idea. I hope that I have explained it a bit better now. Does anyone else can see value in it?



I'm not saying ""don't get involved in the scheduling problem""; I'm trying to show just how complex it gets in a large system. Before you begin to write a line of code, I'd recommend

-you read as much of the published work as you can, including the google and microsoft papers, Facebook's FairScheduler work, etc, etc.
-have a look at the actual code inside those schedulers whose source is public, that's YARN and Mesos.
-try using these schedulers for your own workloads.

really: scheduling work across a datacentre a complex problem that is still considered a place for cutting-edge research. Avoid unless you want to do that.

-Steve



"
"""Chawla,Sumit "" <sumitkchawla@gmail.com>","Thu, 15 Dec 2016 13:44:14 -0800",Re: Output Side Effects for different chain of operations,Reynold Xin <rxin@databricks.com>,"I am already creating these files on slave.  How can i create an RDD from
these slaves?

Regards
Sumit Chawla



"
Alexey Klimov <klimov@megaputer.ru>,"Thu, 15 Dec 2016 14:46:00 -0700 (MST)",Is restarting of SparkContext allowed?,dev@spark.apache.org,"Hello, my question is the continuation of a problem I described  here
<http://stackoverflow.com/questions/40834482/delegation-token-can-be-issued-only-with-kerberos-or-web-authentication-when-r> 
. 

I've done some investigation and found out that nameNode.getDelegationToken
is called during constructing SparkContext even if delegation token is
already presented in token list of current logged user in object of
UserGroupInforation class. The problem doesn't occur when waiting time
before constructing a new context is less than 10 seconds, because rpc
connection to namenode just isn't resetting because of
ipc.client.connection.maxidletime property.

As a workaround of this problem I do login from keytab before every
constructing of SparkContext, which basically just resets token list of
current logged user (as well as whole user structure) and the problem seems
to be gone. Still I'm not really sure that it is correct way to deal with
SparkContext. 

Having found a reason of the problem, I've got 2 assumptions now: 
First - SparkContext was designed to be restarted during JVM run and
behaviour above is just a bug. 
Second - it wasn't and I'm just using SparkContext in a wrong manner.

Since I haven't found any related bug in Jira and any solution on the
internet (as well as too many users facing this error) I tend to think that
it is rather a not allowed usage of SparkContext. 

Is that correct?  



--

---------------------------------------------------------------------


"
"""Thakrar, Jayesh"" <jthakrar@conversantmedia.com>","Thu, 15 Dec 2016 21:48:37 +0000",Re: Expand the Spark SQL programming guide?,"Michael Armbrust <michael@databricks.com>, Jim Hughes <jnh5y@ccri.com>","I too am interested in expanding the documentation for Spark SQL.
For my work I needed to get some info/examples/guidance on window functions and have been using https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html .
How about divide and conquer?


From: Michael Armbrust <michael@databricks.com>
Date: Thursday, December 15, 2016 at 3:21 PM
To: Jim Hughes <jnh5y@ccri.com>
Cc: ""dev@spark.apache.org"" <dev@spark.apache.org>
Subject: Re: Expand the Spark SQL programming guide?

Pull requests would be welcome for any major missing features in the guide: https://github.com/apache/spark/blob/master/docs/sql-programming-guide.md

On Thu, Dec 15, 2016 at 11:48 AM, Jim Hughes <jnh5y@ccri.com<mailto:jnh5y@ccri.com>> wrote:
Hi Anton,

I'd like to see this as well.  I've been working on implementing geospatial user-defined types and functions.  Having examples of aggregations and window functions would be awesome!

I did test out implementing a distributed convex hull as a UserDefinedAggregateFunction, and that seemed to work sensibly.

Cheers,

Jim

On 12/15/2016 03:28 AM, Anton Okolnychyi wrote:
Hi,

I am wondering whether it makes sense to expand the Spark SQL programming guide with examples of aggregations (including user-defined via the Aggregator API) and window functions.  For instance, there might be a separate subsection under ""Getting Started"" for each functionality.

SPARK-16046 seems to be related but there is no activity for more than 4 months.

Best regards,
Anton



"
Alexey Klimov <klimov@megaputer.ru>,"Thu, 15 Dec 2016 14:50:43 -0700 (MST)",Re: Is restarting of SparkContext allowed?,dev@spark.apache.org,"I also wanted to ask that if it's not design way to use SparkContext how much
would it take to get it work completely correct. (e.g. are there any other
singletones that can preserve other state between running different
SparkContext's). 



--

---------------------------------------------------------------------


"
Anton Okolnychyi <anton.okolnychyi@gmail.com>,"Thu, 15 Dec 2016 23:34:25 +0100",Re: Expand the Spark SQL programming guide?,"""Thakrar, Jayesh"" <jthakrar@conversantmedia.com>","I think it will make sense to show a sample implementation of
UserDefinedAggregateFunction
for DataFrames, and an example of the Aggregator API for typed Datasets.

Jim, what if I submit a PR and you join the review process? I also do not
mind to split this if you want, but it seems to be an overkill for this
part.

Jayesh, shall I skip the window functions part since you are going to work
on that?

2016-12-15 22:48 GMT+01:00 Thakrar, Jayesh <jthakrar@conversantmedia.com>:

"
Reynold Xin <rxin@databricks.com>,"Thu, 15 Dec 2016 14:47:11 -0800",Spark 2.1.0-rc3 cut,"""dev@spark.apache.org"" <dev@spark.apache.org>","Committers please use 2.1.1 as the fix version for patches merged into the
branch. I will post a voting email once the packaging is done.
"
Ryan Williams <ryan.blake.williams@gmail.com>,"Fri, 16 Dec 2016 00:15:46 +0000","spark-core ""compile""-scope transitive-dependency on scalatest","""dev@spark.apache.org"" <dev@spark.apache.org>","spark-core depends on spark-tags (compile scope) which depends on scalatest
(compile scope), so spark-core leaks test-deps into downstream libraries'
""compile""-scope classpath.

The cause is that spark-core has logical ""test->test"" and
""compile->compile"" dependencies on spark-tags, but spark-tags publishes
both its test-oriented and non-test-oriented bits in its default
(""compile"") artifact.

spark-tags' test-bits should be in a ""-tests""-JAR that spark-core can
""test""-scope depend on (in addition to ""compile""-scope depending on
spark-tags as it does today).

SPARK-17807 <https://issues.apache.org/jira/browse/SPARK-17807> was ""Not a
Problem""d but I don't think that's the right outcome; spark-core should not
be leaking test-deps into downstream libraries' classpaths when depended on
in ""compile"" scope.
"
Marcelo Vanzin <vanzin@cloudera.com>,"Thu, 15 Dec 2016 16:41:37 -0800","Re: spark-core ""compile""-scope transitive-dependency on scalatest",Ryan Williams <ryan.blake.williams@gmail.com>,"You're right; we had a discussion here recently about this.

I'll re-open that bug, if you want to send a PR. (I think it's just a
matter of making the scalatest dependency ""provided"" in spark-tags, if
I remember the discussion.)




-- 
Marcelo

---------------------------------------------------------------------


"
Ryan Williams <ryan.blake.williams@gmail.com>,"Fri, 16 Dec 2016 00:46:57 +0000","Re: spark-core ""compile""-scope transitive-dependency on scalatest",Marcelo Vanzin <vanzin@cloudera.com>,"ah I see this thread
<http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-has-a-compile-dependency-on-scalatest-td19639.html>
now, thanks; interestingly I don't think the solution I've proposed here
(splitting spark-tags' test-bits into a ""-tests"" JAR and having spark-core
""test""-depend on that) is discussed there.

thanks for re-opening the JIRA; I can't promise a PR for it atm but I will
think about it :)


"
Marcelo Vanzin <vanzin@cloudera.com>,"Thu, 15 Dec 2016 17:32:53 -0800","Re: spark-core ""compile""-scope transitive-dependency on scalatest",Ryan Williams <ryan.blake.williams@gmail.com>,"I posted a PR; the solution I suggested seems to work (and is simpler
than breaking spark-tags into multiple artifacts).




-- 
Marcelo

---------------------------------------------------------------------


"
"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Fri, 16 Dec 2016 01:38:56 +0000",Re: Belief propagation algorithm is open sourced,"Bertrand Dechoux <dechouxb@gmail.com>, Bryan Cutler <cutlerb@gmail.com>","Hi Bertrand,


We only do inference. We do not do structure or parameter estimation (or learning) - that for the MRF would be estimation of the factors, and the structure of the graphical model. The parameters can be estimated using maximum likelihood if data is available for all the nodes, or by EM if there are hidden nodes. We of course don't implement MLE, or EM.


Assuming the model parameters are already available, we can do inference for both Bayesian and Markov models.

So to answer the question below, we don't do ""learning"", we do ""inference"" using BP.

We were using both LibDAI and our own implementation of BP for GraphLab and as a reference.


Best regards, Manish Marwah & Alexander

________________________________
From: Bertrand Dechoux <dechouxb@gmail.com>
Sent: Thursday, December 15, 2016 1:03:49 AM
To: Bryan Cutler
Cc: Ulanov, Alexander; user; dev
Subject: Re: Belief propagation algorithm is open sourced

Nice! I am especially interested in Bayesian Networks, which are only one of the many models that can be expressed by a factor graph representation. Do you do Bayesian Networks learning at scale (parameters and structure) with latent variables? Are you using publicly available tools for that? Which ones?

LibDAI, which created the supported format, ""supports parameter learning of conditional probability tables by Expectation Maximization"" according to the documentation. Is it your reference tool?

Bertrand

I'll check it out, thanks for sharing Alexander!


Dear Spark developers and users,


HPE has open sourced the implementation of the belief propagation (BP) algorithm for Apache Spark, a popular message passing algorithm for performing inference in probabilistic graphical models. It provides exact inference for graphical models without loops. While inference for graphical models with loops is approximate, in practice it is shown to work well. The implementation is generic and operates on factor graph representation of graphical models. It handles factors of any order, and variable domains of any size. It is implemented with Apache Spark GraphX, and thus can scale to large scale models. Further, it supports computations in log scale for numerical stability. Large scale applications of BP include fraud detection in banking transactions and malicious site detection in computer networks.


Source code: https://github.com/HewlettPackard/sandpiper


Best regards, Alexander

"
Reynold Xin <rxin@databricks.com>,"Thu, 15 Dec 2016 21:16:41 -0800",[VOTE] Apache Spark 2.1.0 (RC5),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version
2.1.0. The vote is open until Sun, December 18, 2016 at 21:30 PT and passes
if a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 2.1.0
[ ] -1 Do not release this package because ...


To learn more about Apache Spark, please see http://spark.apache.org/

The tag to be voted on is v2.1.0-rc5
(cd0a08361e2526519e7c131c42116bf56fa62c76)

List of JIRA tickets resolved are:
https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20fixVersion%20%3D%202.1.0

The release files, including signatures, digests, etc. can be found at:
http://home.apache.org/~pwendell/spark-releases/spark-2.1.0-rc5-bin/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1223/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.1.0-rc5-docs/


*FAQ*

*How can I help test this release?*

If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions.

*What should happen to JIRA tickets still targeting 2.1.0?*

Committers should look at those and triage. Extremely important bug fixes,
documentation, and API tweaks that impact compatibility should be worked on
immediately. Everything else please retarget to 2.1.1 or 2.2.0.

*What happened to RC3/RC5?*

They had issues withe release packaging and as a result were skipped.
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Thu, 15 Dec 2016 21:42:00 -0800",Re: [VOTE] Apache Spark 2.1.0 (RC5),Reynold Xin <rxin@databricks.com>,"In addition to usual binary artifacts, this is the first release where
we have installable packages for Python [1] and R [2] that are part of
the release.  I'm including instructions to test the R package below.
Holden / other Python developers can chime in if there are special
instructions to test the pip package.

To test the R source package you can follow the following commands.
1. Download the SparkR source package from
http://people.apache.org/~pwendell/spark-releases/spark-2.1.0-rc5-bin/SparkR_2.1.0.tar.gz
2. Install the source package with R CMD INSTALL SparkR_2.1.0.tar.gz
3. As the SparkR package doesn't contain Spark JARs (this is due to
package size limits from CRAN), we'll need to run [3]
export SPARKR_RELEASE_DOWNLOAD_URL=""http://people.apache.org/~pwendell/spark-releases/spark-2.1.0-rc5-bin/spark-2.1.0-bin-hadoop2.6.tgz""
4. Launch R. You can now use include SparkR with `library(SparkR)` and
test it with your applications.
5. Note that the first time a SparkSession is created the binary
artifacts will the downloaded.

Thanks
Shivaram

[1] https://issues.apache.org/jira/browse/SPARK-18267
[2] https://issues.apache.org/jira/browse/SPARK-18590
[3] Note that this isn't required once 2.1.0 has been released as
SparkR can automatically resolve and download releases.


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Thu, 15 Dec 2016 21:44:42 -0800",Re: [VOTE] Apache Spark 2.1.0 (RC5),Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"I'm going to start this with a +1!



"
Holden Karau <holden@pigscanfly.ca>,"Fri, 16 Dec 2016 00:48:50 -0800",Re: [VOTE] Apache Spark 2.1.0 (RC5),Reynold Xin <rxin@databricks.com>,"Thanks for the specific mention of the new PySpark packaging Shivaram,

For *nix (Linux, Unix, OS X, etc.) Python users interested in helping test
the new artifacts you can do as follows:

Setup PySpark with pip by:

1. Download the artifact from
http://home.apache.org/~pwendell/spark-releases/spark-2.1.0-rc5-bin/pyspark-2.1.0+hadoop2.7.tar.gz
2. (Optional): Create a virtual env (e.g. virtualenv /tmp/pysparktest;
source /tmp/pysparktest/bin/activate)
3. (Possibly required depending on pip version): Upgrade pip to a recent
version (e.g. pip install --upgrade pip)
3. Install the package with pip install pyspark-2.1.0+hadoop2.7.tar.gz
4. If you have SPARK_HOME set to any specific path unset it to force the
pip installed pyspark to run with its provided jars

In the future we hope to publish to PyPI allowing you to skip the download
step, but there just wasn't a chance to get that part included for this
release. If everything goes smoothly hopefully we can add that soon (see
SPARK-18128 <https://issues.apache.org/jira/browse/SPARK-18128>) :)

Some things to verify:
1) Verify you can start the PySpark shell (e.g. run pyspark)
2) Verify you can start PySpark from python (e.g. run python, verify you
can import pyspark and construct a SparkContext).
3) Verify you PySpark programs works with pip installed PySpark as well as
regular spark (e.g. spark-submit my-workload.py)
4) Have a different version of Spark downloaded locally as well? Verify
that launches and runs correctly & pip installed PySpark is not taking
precedence (make sure to use the fully qualified path when executing).

Some things that are explicitly not supported in pip installed PySpark:
1) Starting a new standalone cluster with pip installed PySpark (connecting
to an existing standalone cluster is expected to work)
2) non-Python Spark interfaces (e.g. don't pip install pypsark for SparkR,
use the SparkR packaging instead :)).
3) PyPi - if things go well coming in a future release (track the progress
on https://issues.apache.org/jira/browse/SPARK-18128)
4) Python versions prior to 2.7
5) Full Windows support - later follow up task (if your interested in this
please chat with me or see https://issues.apache.org/jira/browse/SPARK-18136
)

Post verification cleanup:
1. Uninstall the pip installed PySpark since it is just an RC and you don't
want it getting in the way later (e.g. pip uninstall pypsark-2.1.0 )
2 (Optional). deactivate your pip environment

If anyone has any questions about the new PySpark packaging I'm more than
happy to chat :)

Cheers,

Holden :)






-- 
Twitter: https://twitter.com/holdenkarau
"
Liang-Chi Hsieh <viirya@gmail.com>,"Fri, 16 Dec 2016 01:59:30 -0700 (MST)",Re: Mistake in Apache Spark Java.,dev@spark.apache.org,"
Hi,

I tried your example with latest Spark master branch and branch-2.0. It
works well.





-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
"""Thakrar, Jayesh"" <jthakrar@conversantmedia.com>","Fri, 16 Dec 2016 13:39:08 +0000",Re: Expand the Spark SQL programming guide?,Anton Okolnychyi <anton.okolnychyi@gmail.com>,"Yes - that sounds good Anton, I can work on documenting the window functions.

From: Anton Okolnychyi <anton.okolnychyi@gmail.com>
Date: Thursday, December 15, 2016 at 4:34 PM
To: Conversant <jthakrar@conversantmedia.com>
Cc: Michael Armbrust <michael@databricks.com>, Jim Hughes <jnh5y@ccri.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>
Subject: Re: Expand the Spark SQL programming guide?

I think it will make sense to show a sample implementation of UserDefinedAggregateFunction for DataFrames, and an example of the Aggregator API for typed Datasets.

Jim, what if I submit a PR and you join the review process? I also do not mind to split this if you want, but it seems to be an overkill for this part.

Jayesh, shall I skip the window functions part since you are going to work on that?

2016-12-15 22:48 GMT+01:00 Thakrar, Jayesh <jthakrar@conversantmedia.com<mailto:jthakrar@conversantmedia.com>>:
I too am interested in expanding the documentation for Spark SQL.
For my work I needed to get some info/examples/guidance on window functions and have been using https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html .
How about divide and conquer?


From: Michael Armbrust <michael@databricks.com<mailto:michael@databricks.com>>
Date: Thursday, December 15, 2016 at 3:21 PM
To: Jim Hughes <jnh5y@ccri.com<mailto:jnh5y@ccri.com>>
Cc: ""dev@spark.apache.org<mailto:dev@spark.apache.org>"" <dev@spark.apache.org<mailto:dev@spark.apache.org>>
Subject: Re: Expand the Spark SQL programming guide?

Pull requests would be welcome for any major missing features in the guide: https://github.com/apache/spark/blob/master/docs/sql-programming-guide.md

On Thu, Dec 15, 2016 at 11:48 AM, Jim Hughes <jnh5y@ccri.com<mailto:jnh5y@ccri.com>> wrote:
Hi Anton,

I'd like to see this as well.  I've been working on implementing geospatial user-defined types and functions.  Having examples of aggregations and window functions would be awesome!

I did test out implementing a distributed convex hull as a UserDefinedAggregateFunction, and that seemed to work sensibly.

Cheers,

Jim

On 12/15/2016 03:28 AM, Anton Okolnychyi wrote:
Hi,

I am wondering whether it makes sense to expand the Spark SQL programming guide with examples of aggregations (including user-defined via the Aggregator API) and window functions.  For instance, there might be a separate subsection under ""Getting Started"" for each functionality.

SPARK-16046 seems to be related but there is no activity for more than 4 months.

Best regards,
Anton




"
"""Chawla,Sumit "" <sumitkchawla@gmail.com>","Fri, 16 Dec 2016 08:41:16 -0800",Mesos Spark Fine Grained Execution - CPU count,"user@mesos.apache.org, Dev <dev@mesos.apache.org>, 
	User <user@spark.apache.org>, dev@spark.apache.org","Hi

I am using Spark 1.6. I have one query about Fine Grained model in Spark.
I have a simple Spark application which transforms A -> B.  Its a single
stage application.  To begin the program, It starts with 48 partitions.
When the program starts running, in mesos UI it shows 48 tasks and 48 CPUs
allocated to job.  Now as the tasks get done, the number of active tasks
number starts decreasing.  How ever, the number of CPUs does not decrease
propotionally.  When the job was about to finish, there was a single
remaininig task, however CPU count was still 20.

My questions, is why there is no one to one mapping between tasks and cpus
in Fine grained?  How can these CPUs be released when the job is done, so
that other jobs can start.


Regards
Sumit Chawla
"
Sean Owen <sowen@cloudera.com>,"Fri, 16 Dec 2016 17:57:15 +0000",Re: [VOTE] Apache Spark 2.1.0 (RC5),"Reynold Xin <rxin@databricks.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","(If you have a template for these emails, maybe update it to use https
links. They work for apache.org domains. After all we are asking people to
verify the integrity of release artifacts, so it might as well be secure.)

(Also the new archives use .tar.gz instead of .tgz like the others. No big
deal, my OCD eye just noticed it.)

I don't see an Apache license / notice for the Pyspark or SparkR artifacts.
It would be good practice to include this in a convenience binary. I'm not
sure if it's strictly mandatory, but something to adjust in any event. I
think that's all there is to do for SparkR. For Pyspark, which packages a
bunch of dependencies, it does include the licenses (good) but I think it
should include the NOTICE file.

This is the first time I recall getting 0 test failures off the bat!
I'm using Java 8 / Ubuntu 16 and yarn/hive/hadoop-2.7 profiles.

I think I'd +1 this therefore unless someone knows that the license issue
above is real and a blocker.


"
"""Dongjoon Hyun""<dongjoon@apache.org>","Fri, 16 Dec 2016 18:54:34 -0000",Re: [VOTE] Apache Spark 2.1.0 (RC5),<dev@spark.apache.org>,"RC5 is also tested on CentOS 6.8, OpenJDK 1.8.0_111, R 3.3.2 with profiles `-Pyarn -Phadoop-2.7 -Pkinesis-asl -Phive -Phive-thriftserver -Psparkr`.

BTW, there still exist five on-going issues in JIRA (with target version 2.1.0).

1. SPARK-16845  org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificOrdering"" grows beyond 64 KB
2. SPARK-18669 Update Apache docs regard watermarking in Structured Streaming
3. SPARK-18894 Event time watermark delay threshold specified in months or years gives incorrect results
4. SPARK-18899 append data to a bucketed table with mismatched bucketing should fail

+1 with known issues for now.

Bests,
Dongjoon.


---------------------------------------------------------------------


"
Jim Hughes <jnh5y@ccri.com>,"Fri, 16 Dec 2016 14:54:54 -0500",Re: Expand the Spark SQL programming guide?,"""Thakrar, Jayesh"" <jthakrar@conversantmedia.com>,
 Anton Okolnychyi <anton.okolnychyi@gmail.com>","I'd be happy to review a PR.  At the minute, I'm still learning Spark 
SQL, so writing documentation might be a bit of a stretch, but reviewing 
would be fine.

Thanks!


"
Felix Cheung <felixcheung_m@hotmail.com>,"Fri, 16 Dec 2016 20:19:28 +0000",Re: [VOTE] Apache Spark 2.1.0 (RC5),"Reynold Xin <rxin@databricks.com>, Sean Owen <sowen@cloudera.com>,
	""dev@spark.apache.org"" <dev@spark.apache.org>","For R we have a license field in the DESCRIPTION, and this is standard practice (and requirement) for R packages.

https://cran.r-project.org/doc/manuals/R-exts.html#Licensing

________________________________
From: Sean Owen <sowen@cloudera.com>
Sent: Friday, December 16, 2016 9:57:15 AM
To: Reynold Xin; dev@spark.apache.org
Subject: Re: [VOTE] Apache Spark 2.1.0 (RC5)

(If you have a template for these emails, maybe update it to use https links. They work for apache.org<http://apache.org> domains. After all we are asking people to verify the integrity of release artifacts, so it might as well be secure.)

(Also the new archives use .tar.gz instead of .tgz like the others. No big deal, my OCD eye just noticed it.)

I don't see an Apache license / notice for the Pyspark or SparkR artifacts. It would be good practice to include this in a convenience binary. I'm not sure if it's strictly mandatory, but something to adjust in any event. I think that's all there is to do for SparkR. For Pyspark, which packages a bunch of dependencies, it does include the licenses (good) but I think it should include the NOTICE file.

This is the first time I recall getting 0 test failures off the bat!
I'm using Java 8 / Ubuntu 16 and yarn/hive/hadoop-2.7 profiles.

I think I'd +1 this therefore unless someone knows that the license issue above is real and a blocker.

Please vote on releasing the following candidate as Apache Spark version 2.1.0. The vote is open until Sun, December 18, 2016 at 21:30 PT and passes if a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 2.1.0
[ ] -1 Do not release this package because ...


To learn more about Apache Spark, please see http://spark.apache.org/

The tag to be voted on is v2.1.0-rc5 (cd0a08361e2526519e7c131c42116bf56fa62c76)

List of JIRA tickets resolved are:  https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20fixVersion%20%3D%202.1.0

The release files, including signatures, digests, etc. can be found at:
http://home.apache.org/~pwendell/spark-releases/spark-2.1.0-rc5-bin/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1223/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.1.0-rc5-docs/


FAQ

How can I help test this release?

If you are a Spark user, you can help us test this release by taking an existing Spark workload and running on this release candidate, then reporting any regressions.

What should happen to JIRA tickets still targeting 2.1.0?

Committers should look at those and triage. Extremely important bug fixes, documentation, and API tweaks that impact compatibility should be worked on immediately. Everything else please retarget to 2.1.1 or 2.2.0.

What happened to RC3/RC5?

They had issues withe release packaging and as a result were skipped.

"
Xiao Li <gatorsmile@gmail.com>,"Fri, 16 Dec 2016 15:14:38 -0800",Re: [VOTE] Apache Spark 2.1.0 (RC5),Felix Cheung <felixcheung_m@hotmail.com>,"+1

Xiao Li

2016-12-16 12:19 GMT-08:00 Felix Cheung <felixcheung_m@hotmail.com>:

"
=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@databricks.com>,"Sat, 17 Dec 2016 00:21:16 +0100",Re: [VOTE] Apache Spark 2.1.0 (RC5),Xiao Li <gatorsmile@gmail.com>,"+1


 in
ich
t I
e
n
sses
e
0.


-- 

Herman van H√∂vell

Software Engineer

Databricks Inc.

hvanhovell@databricks.com

+31 6 420 590 27

databricks.com

[image: http://databricks.com] <http://databricks.com/>
"
Joseph Bradley <joseph@databricks.com>,"Fri, 16 Dec 2016 18:23:38 -0800",Re: [VOTE] Apache Spark 2.1.0 (RC5),=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@databricks.com>,"+1


t in
hich
ut I
:
PT
:
/
/
n
be
.0.



-- 

Joseph Bradley

Software Engineer - Machine Learning

Databricks, Inc.

[image: http://databricks.com] <http://databricks.com/>
"
Yuming Wang <wgyumg@gmail.com>,"Sat, 17 Dec 2016 10:29:19 +0800",Re: [VOTE] Apache Spark 2.1.0 (RC5),Joseph Bradley <joseph@databricks.com>,"I hope https://github.com/apache/spark/pull/16252 can be fixed until
release 2.1.0. It's a fix for broadcast cannot fit in memory.


ll be
st in
which
but I
 PT
0
t:
n
 be
2.0.
"
Prashant Sharma <scrapcodes@gmail.com>,"Sun, 18 Dec 2016 11:34:04 +0530",Re: Kafka Spark structured streaming latency benchmark.,"""dev@spark.apache.org"" <dev@spark.apache.org>","Furthermore, I ran the same thing with 26 GB as the memory, which would
mean 1.3GB per thread of memory. My jmap
<https://github.com/ScrapCodes/KafkaProducer/blob/master/data/26GB/t11_jmap-histo>
results and jstat
<https://github.com/ScrapCodes/KafkaProducer/blob/master/data/26GB/t11_jstat>
results collected after running the job for more than 11h, again show a
memory constraint. The same gradual slowdown, but a bit more gradual as
memory is considerably more than the previous runs.




This situation sounds like a memory leak ? As the byte array objects are
more than 13GB, and are not garbage collected.

--Prashant



"
Liwei Lin <lwlin7@gmail.com>,"Sun, 18 Dec 2016 15:44:57 +0800",Re: [VOTE] Apache Spark 2.1.0 (RC5),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1

Cheers,
Liwei


<
d
s
ell be
o
ust in
 which
 but I
0 PT
/
.0
en
d be
.2.0.
.
"
Anton Okolnychyi <anton.okolnychyi@gmail.com>,"Sun, 18 Dec 2016 15:08:33 +0100",Re: Expand the Spark SQL programming guide?,Jim Hughes <jnh5y@ccri.com>,"Here is the pull request: https://github.com/apache/spark/pull/16329



2016-12-16 20:54 GMT+01:00 Jim Hughes <jnh5y@ccri.com>:

"
Anton Okolnychyi <anton.okolnychyi@gmail.com>,"Sun, 18 Dec 2016 15:09:29 +0100",Re: Expand the Spark SQL programming guide?,Jim Hughes <jnh5y@ccri.com>,"Any comments/suggestions are more than welcome.

Thanks,
Anton

2016-12-18 15:08 GMT+01:00 Anton Okolnychyi <anton.okolnychyi@gmail.com>:

"
Denny Lee <denny.g.lee@gmail.com>,"Sun, 18 Dec 2016 19:42:32 +0000",Re: [VOTE] Apache Spark 2.1.0 (RC5),"Liwei Lin <lwlin7@gmail.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","+1 (non-binding)



g
in
ch
 I
es
20fixVersion%20%3D%202.1.0
,
on
"
Holden Karau <holden@pigscanfly.ca>,"Sun, 18 Dec 2016 20:04:35 +0000",Re: [VOTE] Apache Spark 2.1.0 (RC5),"Denny Lee <denny.g.lee@gmail.com>, Liwei Lin <lwlin7@gmail.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","+1 (non-binding) - checked Python artifacts with virtual env.


g
in
es
20fixVersion%20%3D%202.1.0
,
on
"
Adam Roberts <AROBERTS@uk.ibm.com>,"Sun, 18 Dec 2016 20:33:04 +0000",Re: [VOTE] Apache Spark 2.1.0 (RC5),Holden Karau <holden@pigscanfly.ca>,"+1 (non-binding)

Functional: looks good, tested with OpenJDK 8 (1.8.0_111) and IBM's latest 
SDK for Java (8 SR3 FP21).

Tests run clean on Ubuntu 16 04, 14 04, SUSE 12, CentOS 7.2 on x86 and IBM 
failing but nothing to be concerned over (timeouts):

org"
vaquar khan <vaquar.khan@gmail.com>,"Sun, 18 Dec 2016 16:33:01 -0600",Re: [VOTE] Apache Spark 2.1.0 (RC5),Adam Roberts <AROBERTS@uk.ibm.com>,"+1 (non-binding)

Regards,
vaquar khan


M
y
y
yo
g
in
es
%20fixVersion%20%3D%202.1.0*
%20fixVersion%20%3D%202.1.0>
,
on
U


-- 
Regards,
Vaquar Khan
+1 -224-436-0783

IT Architect / Lead Consultant
Greater Chicago
"
Liang-Chi Hsieh <viirya@gmail.com>,"Sun, 18 Dec 2016 21:07:15 -0700 (MST)",Re: Aggregating over sorted data,dev@spark.apache.org,"
Hi,

As I know, Spark SQL doesn't provide native support for this feature now.
After searching, I found only few database systems support it, e.g.,
PostgreSQL.

Actually based on the Spark SQL's aggregate system, I think it is not very
difficult to add the support for this feature. The problem is how frequently
this feature is needed for Spark SQL users and if it is worth adding this,
because as I see, this feature is not very common.

Alternative possible to achieve this in current Spark SQL, is to use
Aggregator with Dataset API. You can write your custom Aggregator which has
an user-defined JVM object as buffer to hold the input data into your
aggregate function. But you may need to write necessary encoder for the
buffer object.

If you really need this feature, you may open a Jira to ask others' opinion
about this feature.






-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
Felix Cheung <felixcheung_m@hotmail.com>,"Mon, 19 Dec 2016 04:55:24 +0000",Re: [VOTE] Apache Spark 2.1.0 (RC5),"""dev@spark.apache.org"" <dev@spark.apache.org>","0/+1

Tested a bunch of R package/install cases.
Unfortunately we are still working on SPARK-18817, which looks to be a change when going from Spark 1.6 to 2.0. In that case it won't be a blocker.


_____________________________
From: vaquar khan <vaquar.khan@gmail.com<mailto:vaquar.khan@gmail.com>>
Sent: Sunday, December 18, 2016 2:33 PM
Subject: Re: [VOTE] Apache Spark 2.1.0 (RC5)
To: Adam Roberts <aroberts@uk.ibm.com<mailto:aroberts@uk.ibm.com>>
Cc: Denny Lee <denny.g.lee@gmail.com<mailto:denny.g.lee@gmail.com>>, Holden Karau <holden@pigscanfly.ca<mailto:holden@pigscanfly.ca>>, Liwei Lin <lwlin7@gmail.com<mailto:lwlin7@gmail.com>>, <dev@spark.apache.org<mailto:dev@spark.apache.org>>


+1 (non-binding)

Regards,
vaquar khan

+1 (non-binding)

Functional: looks good, tested with OpenJDK 8 (1.8.0_111) and IBM's latest SDK for Java (8 SR3 FP21).

Tests run clean on Ubuntu 16 04, 14 04, SUSE 12, CentOS 7.2 on x86 and IBM ling but nothing to be concerned over (timeouts):

org.apache.spark.DistributedSuite.caching on disk
org.apache.spark.rdd.LocalCheckpointSuite.missing checkpoint block fails with informative message
org.apache.spark.sql.streaming.StreamingAggregationSuite.prune results by current_time, complete mode
org.apache.spark.sql.streaming.StreamingAggregationSuite.prune results by current_date, complete mode
org.apache.spark.sql.hive.HiveSparkSubmitSuite.set hive.metastore.warehouse.dir

Performance vs 2.0.2: lots of improvements seen using the HiBench and SparkSqlPerf benchmarks, tested with a 48 core Intel machine using the Kryo serializer, controlled test environment. These are all open source benchmarks anyone can use and experiment with. Elapsed times measured, + scores are an improvement (so it's that much percent faster) and- scores are used for regressions I'm seeing.

  *   K-means: Java API +22% (100 sec to 78 sec), Scala API+30% (34 seconds to 24 seconds), Python API unchanged
  *   PageRank: minor improvement from 40 seconds to 38 seconds,+5%
  *   Sort: minor improvement, 10.8 seconds to 9.8 seconds,+10%
  *   WordCount: unchanged
  *   Bayes: mixed bag, sometimes much slower (95 sec to 140 sec) which is-47%, other times marginally faster by 15%, something to keep an eye on
  *   Terasort: +18% (39 seconds to 32 seconds) with the Java/Scala APIs

For TPC-DS SQL queries the results are a mixed bag again, I see > 10% boosts for q9,  q68, q75, q96 and > 10% slowdowns for q7, q39a, q43, q52, q57, q89. Five iterations, average times compared, only changing which version of Spark we're using



From:        Holden Karau <holden@pigscanfly.ca<mailto:holden@pigscanfly.caTo:        Denny Lee <denny.g.lee@gmail.com<mailto:denny.g.lee@gmail.com>>, Liwei Lin <lwlin7@gmail.com<mailto:lwlin7@gmail.com>>, ""dev@spark.apache.org<mailto:dev@spark.apache.org>"" <dev@spark.apache.org<mailto:dev@spark.apache.org>>
Date:        18/12/2016 20:05
Subject:        Re: [VOTE] Apache Spark 2.1.0 (RC5)
________________________________



+1 (non-binding) - checked Python artifacts with virtual env.

+1 (non-binding)


+1

Cheers,
Liwei



I hope https://github.com/apache/spark/pull/16252 can be fixed until release 2.1.0. It's a fix for broadcast cannot fit in memory.

+1

+1

+1

Xiao Li

2016-12-16 12:19 GMT-08:00 Felix Cheung <felixcheung_m@hotmail.com<mailto:felixcheung_m@hotmail.com>>:












For R we have a license field in the DESCRIPTION, and this is standard practice (and requirement) for R packages.







https://cran.r-project.org/doc/manuals/R-exts.html#Licensing







________________________________


From: Sean Owen <sowen@cloudera.com<mailto:sowen@cloudera.com>>


Sent: Friday, December 16, 2016 9:57:15 AM


To: Reynold Xin; dev@spark.apache.org<mailto:dev@spark.apache.org>


Subject: Re: [VOTE] Apache Spark 2.1.0 (RC5)










(If you have a template for these emails, maybe update it to use https links. They work for

apache.org<http://apache.org/> domains. After all we are asking people to verify the integrity of release artifacts, so it might as well be secure.)







(Also the new archives use .tar.gz instead of .tgz like the others. No big deal, my OCD eye just noticed it.)







I don't see an Apache license / notice for the Pyspark or SparkR artifacts. It would be good practice to include this in a convenience binary. I'm not sure if it's strictly mandatory, but something to adjust in any event. I think that's all there is to

do for SparkR. For Pyspark, which packages a bunch of dependencies, it does include the licenses (good) but I think it should include the NOTICE file.







This is the first time I recall getting 0 test failures off the bat!


I'm using Java 8 / Ubuntu 16 and yarn/hive/hadoop-2.7 profiles.







I think I'd +1 this therefore unless someone knows that the license issue above is real and a blocker.















Please vote on releasing the following candidate as Apache Spark version 2.1.0. The vote is open until Sun, December 18, 2016 at 21:30 PT and passes if a majority of at least 3 +1 PMC votes are cast.







[ ] +1 Release this package as Apache Spark 2.1.0


[ ] -1 Do not release this package because ...












To learn more about Apache Spark, please see

http://spark.apache.org/







The tag to be voted on is v2.1.0-rc5 (cd0a08361e2526519e7c131c42116bf56fa62c76)







List of JIRA tickets resolved are:  https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20fixVersion%20%3D%202.1.0







The release files, including signatures, digests, etc. can be found at:


http://home.apache.org/~pwendell/spark-releases/spark-2.1.0-rc5-bin/







Release artifacts are signed with the following key:


https://people.apache.org/keys/committer/pwendell.asc







The staging repository for this release can be found at:


https://repository.apache.org/content/repositories/orgapachespark-1223/







The documentation corresponding to this release can be found at:


http://people.apache.org/~pwendell/spark-releases/spark-2.1.0-rc5-docs/












FAQ







How can I help test this release?







If you are a Spark user, you can help us test this release by taking an existing Spark workload and running on this release candidate, then reporting any regressions.







What should happen to JIRA tickets still targeting 2.1.0?







Committers should look at those and triage. Extremely important bug fixes, documentation, and API tweaks that impact compatibility should be worked on immediately. Everything else please retarget to 2.1.1 or 2.2.0.







What happened to RC3/RC5?







They had issues withe release packaging and as a result were skipped.



























--
Herman van Hˆvell
Software Engineer
Databricks Inc.
hvanhovell@databricks.com<mailto:hvanhovell@databricks.com>
+31 6 420 590 27
databricks.com<http://databricks.com/>
<http://databricks.com/>







--
Joseph Bradley
Software Engineer - Machine Learning
Databricks, Inc.
<http://databricks.com/>











Unless stated otherwise above:
IBM United Kingdom Limited - Registered in England and Wales with number 741598.
Registered office: PO Box 41, North Harbour, Portsmouth, Hampshire PO6 3AU



--
Regards,
Vaquar Khan
+1 -224-436-0783

IT Architect / Lead Consultant
Greater Chicago


"
Robin East <robin.east@xense.co.uk>,"Mon, 19 Dec 2016 08:14:20 +0000",Re: Aggregating over sorted data,Liang-Chi Hsieh <viirya@gmail.com>,"This is also a feature we need for our time-series processing 




ly

s
n
n3.nabble.com/Aggregating-over-sorted-data-tp19999p20273.html
com.


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Mon, 19 Dec 2016 09:37:11 +0000",Re: [VOTE] Apache Spark 2.1.0 (RC5),"Reynold Xin <rxin@databricks.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","PS, here are the open issues for 2.1.0. Forgot this one. No Blockers, but
one ""Critical"":

SPARK-16845
org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificOrdering""
grows beyond 64 KB

SPARK-18669 Update Apache docs regard watermarking in Structured Streaming

SPARK-18894 Event time watermark delay threshold specified in months or
years gives incorrect results

SPARK-18899 append data to a bucketed table with mismatched bucketing
should fail

SPARK-18909 The error message in `ExpressionEncoder.toRow` and `fromRow` is
too verbose

SPARK-18912 append to a non-file-based data source table should detect
columns number mismatch

SPARK-18913 append to a table with special column names should work

SPARK-18921 check database existence with Hive.databaseExists instead of
getDatabase



"
Martin Le <martin.lequoc@gmail.com>,"Mon, 19 Dec 2016 14:17:06 +0100",stratified sampling scales poorly,user@spark.apache.org,"Hi all,

I perform sampling on a DStream by taking samples from RDDs in the DStream.
I have used two sampling mechanisms: simple random sampling and stratified
sampling.

Simple random sampling: inputStream.transform(x => x.sample(false,
fraction)).

Stratified sampling: inputStream.transform(x => x.sampleByKeyExact(false,
fractions))

where fractions = Map(‚Äúkey1‚Äù-> fraction,  ‚Äúkey2‚Äù-> fraction, ‚Ä¶, ‚Äúkeyn‚Äù->
fraction).

I have a question is that why stratified sampling scales poorly with
different sampling fractions in this context? meanwhile simple random
sampling scales well with different sampling fractions (I ran experiments
on 4 nodes cluster )?

Thank you,

Martin
"
"""Franklyn D'souza"" <franklyn.dsouza@shopify.com>","Mon, 19 Dec 2016 12:36:08 -0500",Re: [VOTE] Apache Spark 2.1.0 (RC5),"""dev@spark.apache.org"" <dev@spark.apache.org>","-1 https://issues.apache.org/jira/browse/SPARK-18589 hasn't been resolved
by this release and is a blocker in our adoption of spark 2.0. I've updated
the issue with some steps to reproduce the error.


"
Michael Gummelt <mgummelt@mesosphere.io>,"Mon, 19 Dec 2016 10:06:06 -0800",Re: Mesos Spark Fine Grained Execution - CPU count,user@mesos.apache.org,"Hi

I don't have a lot of experience with the fine-grained scheduler.  It's
deprecated and fairly old now.  CPUs should be relinquished as tasks
complete, so I'm not sure why you're seeing what you're seeing.  There have
been a few discussions on the spark list regarding deprecating the
fine-grained scheduler, and no one seemed too dead-set on keeping it.  I'd
recommend you move over to coarse-grained.




-- 
Michael Gummelt
Software Engineer
Mesosphere
"
"""Chawla,Sumit "" <sumitkchawla@gmail.com>","Mon, 19 Dec 2016 10:35:51 -0800",Re: Mesos Spark Fine Grained Execution - CPU count,Michael Gummelt <mgummelt@mesosphere.io>,"But coarse grained does the exact same thing which i am trying to avert
here.  At the cost of lower startup, it keeps the resources reserved till
the entire duration of the job.

Regards
Sumit Chawla



"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 19 Dec 2016 18:38:16 +0000",Re: [VOTE] Apache Spark 2.1.0 (RC5),"""Franklyn D'souza"" <franklyn.dsouza@shopify.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","Since it‚Äôs not a regression from 2.0 (I believe the same issue affects both
2.0 and 2.1) it doesn‚Äôt merit a -1 vote according to the voting guidelines.

Of course, it would be nice if we could fix the various optimizer issues
that all seem to have a workaround that involves persist() (another one is
SPARK-18492 <https://issues.apache.org/jira/browse/SPARK-18492>) but I
don‚Äôt think this should block the release.
‚Äã


ed
""
g
es
20fixVersion%20%3D%202.1.0
,
on
"
Michael Gummelt <mgummelt@mesosphere.io>,"Mon, 19 Dec 2016 11:16:10 -0800",Re: Mesos Spark Fine Grained Execution - CPU count,Mehdi Meziane <mehdi.meziane@ldmobile.net>,"Yea, the idea is to use dynamic allocation.  I can't speak to how well it
works with Mesos, though.


k
Berlin /
ave
'd
 a
asks
es
s a
one,


-- 
Michael Gummelt
Software Engineer
Mesosphere
"
Timothy Chen <tnachen@gmail.com>,"Mon, 19 Dec 2016 11:26:24 -0800",Re: Mesos Spark Fine Grained Execution - CPU count,"""dev@mesos.apache.org"" <dev@mesos.apache.org>","Hi Chawla,

to run the executor per host, so if you have 20 agents running Fine
grained executor it will take up 20 cores while it's still running.

Tim


---------------------------------------------------------------------


"
Mehdi Meziane <mehdi.meziane@ldmobile.net>,"Mon, 19 Dec 2016 20:01:01 +0100 (CET)",Re: Mesos Spark Fine Grained Execution - CPU count,Sumit Chawla <sumitkchawla@gmail.com>,"I think that what you are looking for is Dynamic resource allocation: 
http://spark.apache.org/docs/latest/job-scheduling.html#dynamic-resource-allocation 


Spark provides a mechanism to dynamically adjust the resources your application occupies based on the workload. This means that your application may give resources back to the cluster if they are no longer used and request them again later when there is demand. This feature is particularly useful if multiple applications share resources in your Spark cluster. 

----- Mail Original ----- 
De: ""Sumit Chawla"" <sumitkchawla@gmail.com> 
√Ä: ""Michael Gummelt"" <mgummelt@mesosphere.io> 
Cc: user@mesos.apache.org, ""Dev"" <dev@mesos.apache.org>, ""User"" <user@spark.apache.org>, ""dev"" <dev@spark.apache.org> 
Envoy√©: Lundi 19 D√©cembre 2016 19h35:51 GMT +01:00 Amsterdam / Berlin / Berne / Rome / Stockholm / Vienne 
Objet: Re: Mesos Spark Fine Grained Execution - CPU count 


But coarse grained does the exact same thing which i am trying to avert here. At the cost of lower startup, it keeps the resources reserved till the entire duration of the job. 



Regards 
Sumit Chawla 







Hi 

I don't have a lot of experience with the fine-grained scheduler. It's deprecated and fairly old now. CPUs should be relinquished as tasks complete, so I'm not sure why you're seeing what you're seeing. There have been a few discussions on the spark list regarding deprecating the fine-grained scheduler, and no one seemed too dead-set on keeping it. I'd recommend you move over to coarse-grained. 





ote: 



Hi 


I am using Spark 1.6. I have one query about Fine Grained model in Spark. I have a simple Spark application which transforms A -> B. Its a single stage application. To begin the program, It starts with 48 partitions. When the program starts running, in mesos UI it shows 48 tasks and 48 CPUs allocated to job. Now as the tasks get done, the number of active tasks number starts decreasing. How ever, the number of CPUs does not decrease propotionally. When the job was about to finish, there was a single remaininig task, however CPU count was still 20. 


My questions, is why there is no one to one mapping between tasks and cpus in Fine grained? How can these CPUs be released when the job is done, so that other jobs can start. 






Regards 
Sumit Chawla 




-- 







Michael Gummelt 
Software Engineer 
Mesosphere 

"
samkum <sameer2kk@gmail.com>,"Mon, 19 Dec 2016 05:02:46 -0700 (MST)",java.lang.AssertionError: assertion failed,dev@spark.apache.org,"I am using Apache Spark 2.0.2 and facing following issue while using
cartesian product in Spark Streaming module.

I am using compression codec as snappy but facing the same issue while using
the default one:LZ4, also using kryo for serialization.

I also see ample memory available in the executor section.

Please find the stacktrace below:-

java.lang.AssertionError: assertion failed at
scala.Predef$.assert(Predef.scala:156) at
at org.apache.spark.util.collection.Spillable.spill(Spillable.scala:111) at
org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:150)
at
org.apache.spark.memory.MemoryConsumer.acquireMemory(MemoryConsumer.java:147)
at org.apache.spark.util.collection.Spillable.maybeSpill(Spillable.scala:86)
at
at org.apache.spark.Aggregator.combineCombinersByKey(Aggregator.scala:50) at
org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:85)
at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:109) at
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319) at
org.apache.spark.rdd.RDD.iterator(RDD.scala:283) at
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319) at
org.apache.spark.rdd.RDD.iterator(RDD.scala:283) at
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319) at
org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:332) at
org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:330) at
org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:951)
at
org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:926)
at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866) at
org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:926)
at
org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:670)
at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330) at
org.apache.spark.rdd.RDD.iterator(RDD.scala:281) at
org.apache.spark.rdd.PartitionerAwareUnionRDD$$anonfun$compute$1.apply(PartitionerAwareUnionRDD.scala:100)
at
org.apache.spark.rdd.PartitionerAwareUnionRDD$$anonfun$compute$1.apply(PartitionerAwareUnionRDD.scala:99)
at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434) at
scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440) at
at org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:41) at
org.apache.spark.rdd.PairRDDFunctions$$anonfun$combineByKeyWithClassTag$1$$anonfun$apply$11.apply(PairRDDFunctions.scala:96)
at
org.apache.spark.rdd.PairRDDFunctions$$anonfun$combineByKeyWithClassTag$1$$anonfun$apply$11.apply(PairRDDFunctions.scala:94)
at
org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785)
at
org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319) at
org.apache.spark.rdd.RDD.iterator(RDD.scala:283) at
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319) at
org.apache.spark.rdd.RDD.iterator(RDD.scala:283) at
org.apache.spark.rdd.PartitionerAwareUnionRDD$$anonfun$compute$1.apply(PartitionerAwareUnionRDD.scala:100)
at
org.apache.spark.rdd.PartitionerAwareUnionRDD$$anonfun$compute$1.apply(PartitionerAwareUnionRDD.scala:99)
at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434) at
scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440) at
scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439) at
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) at
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461) at
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) at
org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:192)
at
org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)
at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
at org.apache.spark.scheduler.Task.run(Task.scala:86) at
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274) at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)



--

---------------------------------------------------------------------


"
"""Chawla,Sumit "" <sumitkchawla@gmail.com>","Mon, 19 Dec 2016 12:09:54 -0800",Re: Mesos Spark Fine Grained Execution - CPU count,user@mesos.apache.org,"Ah thanks. looks like i skipped reading this *""Neither will executors
terminate when they‚Äôre idle.""*

So in my job scenario,  I should preassume that No of executors should be
less than number of tasks. Ideally one executor should execute 1 or more
tasks.  But i am observing something strange instead.  I start my job with
48 partitions for a spark job. In mesos ui i see that number of tasks is
48, but no. of CPUs is 78 which is way more than 48.  Here i am assuming
that 1 CPU is 1 executor.   I am not specifying any configuration to set
number of cores per executor.

Regards
Sumit Chawla



y
ory
‚Äôre
e
le
.
ks
"
Michael Gummelt <mgummelt@mesosphere.io>,"Mon, 19 Dec 2016 12:45:05 -0800",Re: Mesos Spark Fine Grained Execution - CPU count,"""Chawla,Sumit"" <sumitkchawla@gmail.com>","tasks.

No.  Each executor runs 0 or more tasks.

Each executor consumes 1 CPU, and each task running on that executor
consumes another CPU.  You can customize this via
spark.mesos.mesosExecutor.cores (
https://github.com/apache/spark/blob/v1.6.3/docs/running-on-mesos.md) and
spark.task.cpus (
https://github.com/apache/spark/blob/v1.6.3/docs/configuration.md)


h
ve
hen
uld
:
s.
,


-- 
Michael Gummelt
Software Engineer
Mesosphere
"
"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Mon, 19 Dec 2016 12:50:19 -0800",Re: Kafka Spark structured streaming latency benchmark.,Prashant Sharma <scrapcodes@gmail.com>,"Hey, Prashant. Could you track the GC root of byte arrays in the heap?


"
Joris Van Remoortere <joris@mesosphere.io>,"Mon, 19 Dec 2016 11:35:55 -0800",Re: Mesos Spark Fine Grained Execution - CPU count,"""dev@mesos.apache.org"" <dev@mesos.apache.org>","That makes sense. From the documentation it looks like the executors are
not supposed to terminate:
http://spark.apache.org/docs/latest/running-on-mesos.html#fine-grained-deprecated

ry
‚Äôre


I suppose your task to executor CPU ratio is low enough that it looks like
most of the resources are not being reclaimed. If your tasks were using
significantly more CPU the amortized cost of the idle executors would not
be such a big deal.


‚Äî
*Joris Van Remoortere*
Mesosphere


k.
e
s
se
so
"
"""Chawla,Sumit "" <sumitkchawla@gmail.com>","Mon, 19 Dec 2016 13:32:48 -0800",Re: Mesos Spark Fine Grained Execution - CPU count,Michael Gummelt <mgummelt@mesosphere.io>,"Great.  Makes much better sense now.  What will be reason to have
spark.mesos.mesosExecutor.cores more than 1, as this number doesn't include
the number of cores for tasks.

So in my case it seems like 30 CPUs are allocated to executors.  And there
are 48 tasks so 48 + 30 =  78 CPUs.  And i am noticing this gap of 30 is
maintained till the last task exits.  This explains the gap.   Thanks
everyone.  I am still not sure how this number 30 is calculated.  ( Is it
dynamic based on current resources, or is it some configuration.  I have 32
nodes in my cluster).

Is this problem of idle executors sticking around solved in Dynamic
Resource Allocation?  Is there some timeout after which Idle executors can
just shutdown and cleanup its resources.


Regards
Sumit Chawla



or.cores
e
th
e
ive
when
ould
8
d
"
Michael Gummelt <mgummelt@mesosphere.io>,"Mon, 19 Dec 2016 13:42:55 -0800",Re: Mesos Spark Fine Grained Execution - CPU count,"""Chawla,Sumit"" <sumitkchawla@gmail.com>","Resource Allocation?  Is there some timeout after which Idle executors can
just shutdown and cleanup its resources.

Yes, that's exactly what dynamic allocation does.  But again I have no idea
what the state of dynamic allocation + mesos is.


e
s
32
n
more
ith
s
g
t
give
 when
would
e


-- 
Michael Gummelt
Software Engineer
Mesosphere
"
"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Mon, 19 Dec 2016 14:35:44 -0800",Re: Kafka Spark structured streaming latency benchmark.,Prashant Sharma <scrapcodes@gmail.com>,"Hey Prashant. Thanks for your codes. I did some investigation and it turned
out that ContextCleaner is too slow and its ""referenceQueue"" keeps growing.
My hunch is cleaning broadcast is very slow since it's a blocking call.


"
Timothy Chen <tnachen@gmail.com>,"Mon, 19 Dec 2016 15:11:56 -0800",Re: Mesos Spark Fine Grained Execution - CPU count,Mehdi Meziane <mehdi.meziane@ldmobile.net>,"Dynamic allocation works with Coarse grain mode only, we wasn't aware
a need for Fine grain mode after we enabled dynamic allocation support
on the coarse grain mode.

What's the reason you're running fine grain mode instead of coarse
grain + dynamic allocation?

Tim

ion
Berlin /
an
ea
ude
re
is
t
 32
an
o>
f
and
 more
with
is 48,
hat 1
er of
d-deprecated
 give
e when
e using
 not be
s
m>
le

---------------------------------------------------------------------


"
"""Chawla,Sumit "" <sumitkchawla@gmail.com>","Mon, 19 Dec 2016 15:23:52 -0800",Re: Mesos Spark Fine Grained Execution - CPU count,Timothy Chen <tnachen@gmail.com>,"Tim,

We will try to run the application in coarse grain mode, and share the
findings with you.

Regards
Sumit Chawla



/ Berlin /
0 is
)
m
ld
b
s
s
as
ks
e
.
in
a
nd
ve
s
"
Reynold Xin <rxin@databricks.com>,"Mon, 19 Dec 2016 15:27:51 -0800",Re: [VOTE] Apache Spark 2.1.0 (RC5),"""dev@spark.apache.org"" <dev@spark.apache.org>","The vote passed with the following +1 and -1:


+1

Reynold Xin*
Sean Owen*
Dongjoon Hyun
Xiao Li
Herman van H√∂vell tot Westerflier
Joseph Bradley*
Liwei Lin
Denny Lee
Holden Karau
Adam Roberts
vaquar khan


0/+1 (not sure what this means but putting it here just in case)
Felix Cheung

-1
Franklyn D'souza (due to a bug that's not a regression)


I will work on packaging the release.

















es
,
on
"
Mehdi Meziane <mehdi.meziane@ldmobile.net>,"Mon, 19 Dec 2016 23:45:31 +0100 (CET)",Re: Mesos Spark Fine Grained Execution - CPU count,Michael Gummelt <mgummelt@mesosphere.io>,"We will be interested by the results if you give a try to Dynamic allocation with mesos ! 



----- Mail Original ----- 
De: ""Michael Gummelt"" <mgummelt@mesosphere.io> 
√Ä: ""Sumit Chawla"" <sumitkchawla@gmail.com> 
Cc: user@mesos.apache.org, dev@mesos.apache.org, ""User"" <user@spark.apache.org>, dev@spark.apache.org 
Envoy√©: Lundi 19 D√©cembre 2016 22h42:55 GMT +01:00 Amsterdam / Berlin / Berne / Rome / Stockholm / Vienne 
Objet: Re: Mesos Spark Fine Grained Execution - CPU count 



rce Allocation? Is there some timeout after which Idle executors can just shutdown and cleanup its resources. 

Yes, that's exactly what dynamic allocation does. But again I have no idea what the state of dynamic allocation + mesos is. 



ote: 



Great. Makes much better sense now. What will be reason to have spark.mesos.mesosExecutor. cores more than 1, as this number doesn't include the number of cores for tasks. 


So in my case it seems like 30 CPUs are allocated to executors. And there are 48 tasks so 48 + 30 = 78 CPUs. And i am noticing this gap of 30 is maintained till the last task exits. This explains the gap. Thanks everyone. I am still not sure how this number 30 is calculated. ( Is it dynamic based on current resources, or is it some configuration. I have 32 nodes in my cluster). 


Is this problem of idle executors sticking around solved in Dynamic Resource Allocation? Is there some timeout after which Idle executors can just shutdown and cleanup its resources. 





Regards 
Sumit Chawla 










ks. 

No. Each executor runs 0 or more tasks. 

Each executor consumes 1 CPU, and each task running on that executor consumes another CPU. You can customize this via spark.mesos.mesosExecutor.cores ( https://github.com/apache/spark/blob/v1.6.3/docs/running-on-mesos.md ) and spark.task.cpus ( https://github.com/apache/spark/blob/v1.6.3/docs/configuration.md ) 





rote: 



Ah thanks. looks like i skipped reading this "" Neither will executors terminate when they‚Äôre idle."" 


So in my job scenario, I should preassume that No of executors should be less than number of tasks. Ideally one executor should execute 1 or more tasks. But i am observing something strange instead. I start my job with 48 partitions for a spark job. In mesos ui i see that number of tasks is 48, but no. of CPUs is 78 which is way more than 48. Here i am assuming that 1 CPU is 1 executor. I am not specifying any configuration to set number of cores per executor. 



Regards 
Sumit Chawla 








That makes sense. From the documentation it looks like the executors are not supposed to terminate: 
http://spark.apache.org/docs/latest/running-on-mesos.html#fine-grained-deprecated 


Note that while Spark tasks in fine-grained will relinquish cores as they terminate, they will not relinquish memory, as the JVM does not give memory back to the Operating System. Neither will executors terminate when they‚Äôre idle. 


I suppose your task to executor CPU ratio is low enough that it looks like most of the resources are not being reclaimed. If your tasks were using significantly more CPU the amortized cost of the idle executors would not be such a big deal. 






‚Äî 
Joris Van Remoortere 
Mesosphere 

 


Hi Chawla, 

to run the executor per host, so if you have 20 agents running Fine 
grained executor it will take up 20 cores while it's still running. 

Tim 

ote: 


 
 
s 
 
 
s 
 





-- 







Michael Gummelt 
Software Engineer 
Mesosphere 




-- 







Michael Gummelt 
Software Engineer 
Mesosphere 
"
Koert Kuipers <koert@tresata.com>,"Mon, 19 Dec 2016 22:32:01 -0500",Re: Aggregating over sorted data,Robin East <robin.east@xense.co.uk>,"take a look at:
https://issues.apache.org/jira/browse/SPARK-15798



This is also a feature we need for our time-series processing



frequently
has
opinion
developers-list.1001551.n3.nabble.com/Aggregating-over-
sorted-data-tp19999p20273.html
Nabble.com.


---------------------------------------------------------------------
"
Liang-Chi Hsieh <viirya@gmail.com>,"Tue, 20 Dec 2016 01:54:33 -0700 (MST)",Re: Reduce memory usage of UnsafeInMemorySorter,dev@spark.apache.org,"
Hi Nick,

The scope of the PR I submitted is reduced because we can't make sure if it
is really the root cause of the error you faced. You can check out the
discussion on the PR. So I can just change the assert in the code as shown
in the PR.

If you can have a repro, we can go back to see if it is the root cause and
fix it then.




-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Tue, 20 Dec 2016 13:32:41 +0100",Re: Kafka Spark structured streaming latency benchmark.,"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Hi,

(what a timing. Just reviewed CC yesterday!)

In ALS they trigger cleaning up shufflemapstages themselves so if I
understood the issue the streaming part could do it too.

Jacek


"
Prashant Sharma <scrapcodes@gmail.com>,"Tue, 20 Dec 2016 18:16:52 +0530",Re: Kafka Spark structured streaming latency benchmark.,"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Hi Shixiong,

Thanks for taking a look, I am trying to run and see if making
ContextCleaner run more frequently and/or making it non blocking will help.

--Prashant



"
Jim Hughes <jnh5y@ccri.com>,"Tue, 20 Dec 2016 10:56:50 -0500",Re: Expand the Spark SQL programming guide?,dev@spark.apache.org,"Hi Anton,

Your example and documentation looks great!  I left some comments 
suggesting a few additions, but the PR in its current state is a great 
improvement!

Thanks,

Jim


"
Ricardo Almeida <ricardo.almeida@actnowib.com>,"Tue, 20 Dec 2016 17:20:09 +0100",Re: Expand the Spark SQL programming guide?,dev@spark.apache.org,"The examples look great indeed. Seems a good addition to the existing
documentation.
I understand the UDAF examples don't apply to Python but is there any
relevant reason to skip Python API altogether from this window functions
documentation?


"
Sivanesan Govindaraj <nesan.committer@gmail.com>,"Tue, 20 Dec 2016 18:18:37 +0530",Reg: Any Dev member in and around Chennai / Tamilnadu,dev@spark.apache.org,"HI Dev,

   Sorry to bother with non-technical query. I wish to connect with any
active contributor / committer in and around Chennai / TamilNadu. I wish to
connect in person. Is there a list of all committer details in any location?

Regs,
Siva.
"
"""Triones,Deng(vip.com)"" <triones.deng@vipshop.com>","Tue, 20 Dec 2016 12:36:54 +0000",question about the data frame save mode to make the data exactly one,"""'dev@spark.apache.org'"" <dev@spark.apache.org>","
Hi spark dev,

         I am using spark 2 to write orc file to hdfs. I have one questions about the savemode.

         My use case is this. When I write data into hdfs. If one task failed I hope the file that the task created should be delete and the retry task can write all data, that is to say,
If I have the data 1 to 100 in this task, when the task which write 1 to 100 failed at first, then the task scheduler reschedule the partition task , the data in hdfs should only have the data 1 to 100. Not double 1 and so on.

If so which kind  of savemode should I use. I the FileFormatWriter.scala the file name rule contains one UUID,so I am in mistake..


Thanks


Triones

Êú¨ÁîµÂ≠êÈÇÆ‰ª∂ÂèØËÉΩ‰∏∫‰øùÂØÜÊñá‰ª∂„ÄÇÂ¶ÇÊûúÈòÅ‰∏ãÈùûÁîµÂ≠êÈÇÆ‰ª∂ÊâÄÊåáÂÆö‰πãÊî∂‰ª∂‰∫∫ÔºåË∞®ËØ∑Á´ãÂç≥ÈÄöÁü•Êú¨‰∫∫„ÄÇÊï¨ËØ∑ÈòÅ‰∏ã‰∏çË¶Å‰ΩøÁî®„ÄÅ‰øùÂ≠ò„ÄÅÂ§çÂç∞„ÄÅÊâìÂç∞„ÄÅÊï£Â∏ÉÊú¨ÁîµÂ≠êÈÇÆ‰ª∂ÂèäÂÖ∂ÂÜÖÂÆπÔºåÊàñÂ∞ÜÂÖ∂Áî®‰∫éÂÖ∂‰ªñ‰ªª‰ΩïÁõÆÁöÑÊàñÂêë‰ªª‰Ωï‰∫∫Êä´Èú≤„ÄÇË∞¢Ë∞¢ÊÇ®ÁöÑÂêà‰ΩúÔºÅ This communication is intended only for the addressee(s) and may contain information that is privileged and confidential. You are hereby notified that, if you are not an intended recipient listed above, or an authorized employee or agent of an addressee of this communication responsible for delivering e-mail messages to an intended recipient, any dissemination, distribution or reproduction of this communication (including any attachments hereto) is strictly prohibited. If you have received this communication in error, please notify us immediately by a reply e-mail addressed to the sender and permanently delete the original e-mail communication and any attachments from all storage devices without making or otherwise retaining a copy.
"
"""Triones,Deng(vip.com)"" <triones.deng@vipshop.com>","Tue, 20 Dec 2016 12:35:50 +0000","=?utf-8?B?562U5aSNOiBIb3cgdG8gZGVhbCB3aXRoIHN0cmluZyBjb2x1bW4gZGF0YSBm?=
 =?utf-8?Q?or_spark_mlib=3F?=","""dev@spark.apache.org"" <dev@spark.apache.org>","Hi spark dev,

         I am using spark 2 to write orc file to hdfs. I have one questions about the savemode.

         My use case is this. When I write data into hdfs. If one task failed I hope the file that the task created should be delete and the retry task can write all data, that is to say,
If I have the data 1 to 100 in this task, when the task which write 1 to 100 failed at first, then the task scheduler reschedule the partition task , the data in hdfs should only have the data 1 to 100. Not double 1 and so on.

If so which kind  of savemode should I use. I the FileFormatWriter.scala the file name rule contains one UUID,so I am in mistake..


Thanks


Triones

Êú¨ÁîµÂ≠êÈÇÆ‰ª∂ÂèØËÉΩ‰∏∫‰øùÂØÜÊñá‰ª∂„ÄÇÂ¶ÇÊûúÈòÅ‰∏ãÈùûÁîµÂ≠êÈÇÆ‰ª∂ÊâÄÊåáÂÆö‰πãÊî∂‰ª∂‰∫∫ÔºåË∞®ËØ∑Á´ãÂç≥ÈÄöÁü•Êú¨‰∫∫„ÄÇÊï¨ËØ∑ÈòÅ‰∏ã‰∏çË¶Å‰ΩøÁî®„ÄÅ‰øùÂ≠ò„ÄÅÂ§çÂç∞„ÄÅÊâìÂç∞„ÄÅÊï£Â∏ÉÊú¨ÁîµÂ≠êÈÇÆ‰ª∂ÂèäÂÖ∂ÂÜÖÂÆπÔºåÊàñÂ∞ÜÂÖ∂Áî®‰∫éÂÖ∂‰ªñ‰ªª‰ΩïÁõÆÁöÑÊàñÂêë‰ªª‰Ωï‰∫∫Êä´Èú≤„ÄÇË∞¢Ë∞¢ÊÇ®ÁöÑÂêà‰ΩúÔºÅ This communication is intended only for the addressee(s) and may contain information that is privileged and confidential. You are hereby notified that, if you are not an intended recipient listed above, or an authorized employee or agent of an addressee of this communication responsible for delivering e-mail messages to an intended recipient, any dissemination, distribution or reproduction of this communication (including any attachments hereto) is strictly prohibited. If you have received this communication in error, please notify us immediately by a reply e-mail addressed to the sender and permanently delete the original e-mail communication and any attachments from all storage devices without making or otherwise retaining a copy.
"
Mark Hamstra <mark@clearstorydata.com>,"Tue, 20 Dec 2016 10:16:58 -0800",Re: Reg: Any Dev member in and around Chennai / Tamilnadu,Sivanesan Govindaraj <nesan.committer@gmail.com>,"http://spark.apache.org/committers.html


"
satyajit vegesna <satyajit.apasprk@gmail.com>,"Tue, 20 Dec 2016 16:46:05 -0800",,"dev@spark.apache.org, user@spark.apache.org","Hi All,

PFB sample code ,

val df = spark.read.parquet(....)
df.registerTempTable(""df"")
val zip = df.select(""zip_code"").distinct().as[String].rdd


def comp(zipcode:String):Unit={

val zipval = ""SELECT * FROM df WHERE
zip_code='$zipvalrepl'"".replace(""$zipvalrepl"", zipcode)
val data = spark.sql(zipval) //Throwing null pointer exception with RDD
data.write.parquet(......)

}

val sam = zip.map(x => comp(x))
sam.count

But when i do val zip =
df.select(""zip_code"").distinct().as[String].rdd.collect and call the
function, then i get data computer, but in sequential order.

I would like to know, why when tried running map with rdd, i get null
pointer exception and is there a way to compute the comp function for each
zipcode in parallel ie run multiple zipcode at the same time.

Any clue or inputs are appreciated.

Regards.
"
satyajit vegesna <satyajit.apasprk@gmail.com>,"Tue, 20 Dec 2016 16:47:26 -0800","Null pointer exception with RDD while computing a method, creating dataframe.","dev@spark.apache.org, user@spark.apache.org","Hi All,

PFB sample code ,

val df = spark.read.parquet(....)
df.registerTempTable(""df"")
val zip = df.select(""zip_code"").distinct().as[String].rdd


def comp(zipcode:String):Unit={

val zipval = ""SELECT * FROM df WHERE
zip_code='$zipvalrepl'"".replace(""$zipvalrepl"",
zipcode)
val data = spark.sql(zipval) //Throwing null pointer exception with RDD
data.write.parquet(......)

}

val sam = zip.map(x => comp(x))
sam.count

But when i do val zip = df.select(""zip_code"").distinct().as[String].rdd.collect
and call the function, then i get data computer, but in sequential order.

I would like to know, why when tried running map with rdd, i get null
pointer exception and is there a way to compute the comp function for each
zipcode in parallel ie run multiple zipcode at the same time.

Any clue or inputs are appreciated.

Regards.
"
Liang-Chi Hsieh <viirya@gmail.com>,"Tue, 20 Dec 2016 18:46:00 -0700 (MST)","Re: Null pointer exception with RDD while computing a method,
 creating dataframe.",dev@spark.apache.org,"
Hi,

You can't invoke any RDD actions/transformations inside another
transformations. They must be invoked by the driver.

If I understand your purpose correctly, you can partition your data (i.e.,
`partitionBy`) when writing out to parquet files.



-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
Liang-Chi Hsieh <viirya@gmail.com>,"Tue, 20 Dec 2016 19:58:21 -0700 (MST)",Re: Aggregating over sorted data,dev@spark.apache.org,"Hi,

Can you try the combination of `repartition` + `sortWithinPartitions` on the
dataset?

E.g.,

    val df = Seq((2, ""b c a""), (1, ""c a b""), (3, ""a c b"")).toDF(""number"",
""letters"")
    val df2 =
      df.explode('letters) {
        case Row(letters: String) => letters.split("" "").map(Tuple1(_)).toSeq
      }

    df2
      .select('number, '_1 as 'letter)
      .repartition('number)
      .sortWithinPartitions('number, 'letter)
      .groupBy('number)
      .agg(collect_list('letter))
      .show()

    +------+--------------------+
    |number|collect_list(letter)|
    +------+--------------------+
    |     3|           [a, b, c]|
    |     1|           [a, b, c]|
    |     2|           [a, b, c]|
    +------+--------------------+

I think it should let you do aggregate on sorted data per key.




-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
Naveen <hadoopstack@gmail.com>,"Wed, 21 Dec 2016 12:47:36 +0530",Launching multiple spark jobs within a main spark job.,"dev@spark.apache.org, user@spark.apache.org","Hi Team,

Is it ok to spawn multiple spark jobs within a main spark job, my main
spark job's driver which was launched on yarn cluster, will do some
preprocessing and based on it, it needs to launch multilple spark jobs on
yarn cluster. Not sure if this right pattern.

Please share your thoughts.
Sample code i ve is as below for better understanding..
---------------------

Object Mainsparkjob {

main(...){

val sc=new SparkContext(..)

Fetch from hive..using hivecontext
Fetch from hbase

//spawning multiple Futures..
Val future1=Future{
Val sparkjob= SparkLauncher(...).launch; spark.waitFor
}

Similarly, future2 to futureN.

future1.onComplete{...}
}

}// end of mainsparkjob
----------------------
"
Liang-Chi Hsieh <viirya@gmail.com>,"Wed, 21 Dec 2016 01:38:59 -0700 (MST)",Re: Launching multiple spark jobs within a main spark job.,dev@spark.apache.org,"Hi,

As you launch multiple Spark jobs through `SparkLauncher`, I think it
actually works like you run multiple Spark applications with `spark-submit`.

By default each application will try to use all available nodes. If your
purpose is to share cluster resources across those Spark jobs/applications,
you may need to set some configs properly.

Please check out:

http://spark.apache.org/docs/latest/job-scheduling.html#scheduling-across-applications

As you said you launch the main Spark job on yarn cluster, if you are using
cluster mode, actually you will submit those Spark jobs/applications on the
node which the driver runs. It looks weird.

Looks like you try to fetch some data first and do some jobs on the data.
Can't you just do those jobs in the main driver as Spark actions with its
API?



-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
Chetan Khatri <chetan.opensource@gmail.com>,"Wed, 21 Dec 2016 15:58:58 +0530",Approach: Incremental data load from HBASE,"dev@spark.apache.org, user@spark.apache.org","Hello Guys,

I would like to understand different approach for Distributed Incremental
load from HBase, Is there any *tool / incubactor tool* which satisfy
requirement ?

*Approach 1:*

Write Kafka Producer and maintain manually column flag for events and
ingest it with Linkedin Gobblin to HDFS / S3.

*Approach 2:*

Run Scheduled Spark Job - Read from HBase and do transformations and
maintain flag column at HBase Level.

In above both approach, I need to maintain column level flags. such as 0 -
by default, 1-sent,2-sent and acknowledged. So next time Producer will take
another 1000 rows of batch where flag is 0 or 1.

I am looking for best practice approach with any distributed tool.

Thanks.

- Chetan Khatri
"
Naveen <hadoopstack@gmail.com>,"Wed, 21 Dec 2016 17:14:13 +0530",Re: Launching multiple spark jobs within a main spark job.,David Hodeffi <David.Hodeffi@niceactimize.com>,"Hi Team,

Thanks for your responses.
Let me give more details in a picture of how I am trying to launch jobs.

Main spark job will launch other spark-job similar to calling multiple
spark-submit within a Spark driver program.
These spawned threads for new jobs will be totally different components, so
these cannot be implemented using spark actions.

sample code:
---------------------

Object Mainsparkjob {

main(...){

val sc=new SparkContext(..)

Fetch from hive..using hivecontext
Fetch from hbase

//spawning multiple Futures..
Val future1=Future{
Val sparkjob= SparkLauncher(...).launch; spark.waitFor
}

Similarly, future2 to futureN.

future1.onComplete{...}
}

}// end of mainsparkjob
----------------------


[image: Inline image 1]


"
Liang-Chi Hsieh <viirya@gmail.com>,"Wed, 21 Dec 2016 06:04:33 -0700 (MST)",Re: Launching multiple spark jobs within a main spark job.,dev@spark.apache.org,"
OK.

I think it is little unusual use pattern, but it should work.

As I said before, if you want those Spark applications to share cluster
resources, proper configs is needed for Spark.

If you submit the main driver and all other Spark applications in client
mode under yarn, you should make sure the node running the driver has enough
resources to run them.

I am not sure if you can use `SparkLauncher` to submit them in different
mode, e.g., main driver in client mode, others in cluster mode. Worth
trying.





-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
Naveen <hadoopstack@gmail.com>,"Wed, 21 Dec 2016 18:43:45 +0530",Re: Launching multiple spark jobs within a main spark job.,Sebastian Piu <sebastian.piu@gmail.com>,"Hi Sebastian,

Yes, for fetching the details from Hive and HBase, I would want to use
Spark's HiveContext etc.
However, based on your point, I might have to check if JDBC based driver
connection could be used to do the same.

Main reason for this is to avoid a client-server architecture design.

If we go by a normal scala app without creating a sparkcontext as per your
suggestion, then
1. it turns out to be a client program on cluster on a single node, and for
any multiple invocation through xyz scheduler , it will be invoked always
from that same node
2. Having client program on a single data node might create a hotspot for
that data node which might create a bottleneck as all invocations might
create JVMs on that node itself.
3. With above, we will loose the Spark on YARN's feature of dynamically
allocating a driver on any available data node through RM and NM
co-ordination. With YARN and Cluster mode of invoking a spark-job, it will
help distribute multiple application(main one) in cluster uniformly.

Thanks and please let me know your views.



"
Naveen <hadoopstack@gmail.com>,"Wed, 21 Dec 2016 18:49:29 +0530",Re: Launching multiple spark jobs within a main spark job.,Sebastian Piu <sebastian.piu@gmail.com>,"Thanks Liang!
I get your point. It would mean that when launching spark jobs, mode needs
to be specified as client for all spark jobs.
However, my concern is to know if driver's memory(which is launching spark
jobs) will be used completely by the Future's(sparkcontext's) or these
spawned sparkcontexts will get different nodes / executors from resource
manager?


"
Ted Yu <yuzhihong@gmail.com>,"Wed, 21 Dec 2016 07:12:09 -0800",Re: Approach: Incremental data load from HBASE,Chetan Khatri <chetan.opensource@gmail.com>,"I haven't used Gobblin.
You can consider asking Gobblin mailing list of the first option.

The second option would work.



"
Mudit Kumar <mkumar128@sapient.com>,"Wed, 21 Dec 2016 15:32:55 +0000",unsubscribe,"""dev@spark.apache.org"" <dev@spark.apache.org>, user
	<user@spark.apache.org>","



"
Chetan Khatri <chetan.opensource@gmail.com>,"Wed, 21 Dec 2016 21:30:57 +0530",Re: Approach: Incremental data load from HBASE,Ted Yu <yuzhihong@gmail.com>,"Ok, Sure will ask.

But what would be generic best practice solution for Incremental load from
HBASE.


"
Ted Yu <yuzhihong@gmail.com>,"Wed, 21 Dec 2016 08:34:20 -0800",Re: Approach: Incremental data load from HBASE,Chetan Khatri <chetan.opensource@gmail.com>,"Incremental load traditionally means generating hfiles and
using org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles to load the
data into hbase.

For your use case, the producer needs to find rows where the flag is 0 or 1.
After such rows are obtained, it is up to you how the result of processing
is delivered to hbase.

Cheers


"
satyajit vegesna <satyajit.apasprk@gmail.com>,"Wed, 21 Dec 2016 10:59:19 -0800","Re: Null pointer exception with RDD while computing a method,
 creating dataframe.",Liang-Chi Hsieh <viirya@gmail.com>,"Hi,

val df = spark.read.parquet(....)
df.registerTempTable(""df"")
val zip = df.select(""zip_code"").distinct().as[String].rdd


def comp(zipcode:String):Unit={
val zipval = ""SELECT * FROM df WHERE
zip_code='$zipvalrepl'"".replace(""$zipvalrepl"",
zipcode)
val data = spark.sql(zipval)
data.write.parquet(......)
}

val sam = zip.map(x => comp(x)) //the whole idea is to run the comp method
parallely for multiple zipcodes on the cluster,
sam.count                                   but because i have to collect()
and apply map method , i would be ending calling comp for single zipcode
                                                and executing comp for each
zipcode sequentially.

Regards.


"
Sebastian Piu <sebastian.piu@gmail.com>,"Wed, 21 Dec 2016 12:13:11 +0000",Re: Launching multiple spark jobs within a main spark job.,"Naveen <hadoopstack@gmail.com>, David Hodeffi <David.Hodeffi@niceactimize.com>","Is there any reason you need a context on the application launching the
jobs?
You can use SparkLauncher in a normal app and just listen for state
transitions


"
David Hodeffi <David.Hodeffi@niceactimize.com>,"Wed, 21 Dec 2016 09:43:51 +0000",RE: Launching multiple spark jobs within a main spark job.,"Naveen <hadoopstack@gmail.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>, ""user@spark.apache.org"" <user@spark.apache.org>","I am not familiar of any problem with that.
Anyway, If you run spark applicaction you would have multiple jobs, which makes sense that it is not a problem.

Thanks David.

From: Naveen [mailto:hadoopstack@gmail.com]
Sent: Wednesday, December 21, 2016 9:18 AM
To: dev@spark.apache.org; user@spark.apache.org
Subject: Launching multiple spark jobs within a main spark job.

Hi Team,

Is it ok to spawn multiple spark jobs within a main spark job, my main spark job's driver which was launched on yarn cluster, will do some preprocessing and based on it, it needs to launch multilple spark jobs on yarn cluster. Not sure if this right pattern.

Please share your thoughts.
Sample code i ve is as below for better understanding..
---------------------

Object Mainsparkjob {

main(...){

val sc=new SparkContext(..)

Fetch from hive..using hivecontext
Fetch from hbase

//spawning multiple Futures..
Val future1=Future{
Val sparkjob= SparkLauncher(...).launch; spark.waitFor
}

Similarly, future2 to futureN.

future1.onComplete{...}
}

}// end of mainsparkjob
----------------------

Confidentiality: This communication and any attachments are intended for the above-named persons only and may be confidential and/or legally privileged. Any opinions expressed in this communication are not necessarily those of NICE Actimize. If this communication has come to you in error you must take no action based on it, nor must you copy or show it to anyone; please delete/destroy and inform the sender by e-mail immediately.  
Monitoring: NICE Actimize may monitor incoming and outgoing e-mails.
Viruses: Although we have taken steps toward ensuring that this e-mail and attachments are free from any virus, we advise that in keeping with good computing practice the recipient should ensure they are actually virus free.
"
Liang-Chi Hsieh <viirya@gmail.com>,"Wed, 21 Dec 2016 18:56:06 -0700 (MST)",Re: Launching multiple spark jobs within a main spark job.,dev@spark.apache.org,"
If you run the main driver and other Spark jobs in client mode, you can make
sure they (I meant all the drivers) are running at the same node. Of course
all drivers now consume the resources at the same node.

If you run the main driver in client mode, but run other Spark jobs in
cluster mode, the drivers of those Spark jobs will be launched at other
nodes in the cluster. It should work too. It is as same as you run a Spark
app in client mode and more others in cluster mode.

If you run your main driver in cluster mode, and run other Spark jobs in
cluster mode too, you may need  Spark properly installed in all nodes in the
cluster, because those Spark jobs will be launched at the node which the
main driver is running on.





-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
Liang-Chi Hsieh <viirya@gmail.com>,"Wed, 21 Dec 2016 19:04:18 -0700 (MST)","Re: Null pointer exception with RDD while computing a method,
 creating dataframe.",dev@spark.apache.org,"
Your sample codes first select distinct zipcodes, and then save the rows of
each distinct zipcode into a parquet file.

So I think you can simply partition your data by using
`DataFrameWriter.partitionBy` API, e.g.,

df.repartition(""zip_code"").write.partitionBy(""zip_code"").parquet(.....)




-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
Koert Kuipers <koert@tresata.com>,"Wed, 21 Dec 2016 21:24:14 -0500",Re: Aggregating over sorted data,Liang-Chi Hsieh <viirya@gmail.com>,"i think this works but it relies on groupBy and agg respecting the sorting.
the api provides no such guarantee, so this could break in future versions.
i would not rely on this i think...


Hi,

Can you try the combination of `repartition` + `sortWithinPartitions` on the
dataset?

E.g.,

    val df = Seq((2, ""b c a""), (1, ""c a b""), (3, ""a c b"")).toDF(""number"",
""letters"")
    val df2 =
      df.explode('letters) {
        case Row(letters: String) => letters.split("" "").map(Tuple1(_)).toSeq
      }

    df2
      .select('number, '_1 as 'letter)
      .repartition('number)
      .sortWithinPartitions('number, 'letter)
      .groupBy('number)
      .agg(collect_list('letter))
      .show()

    +------+--------------------+
    |number|collect_list(letter)|
    +------+--------------------+
    |     3|           [a, b, c]|
    |     1|           [a, b, c]|
    |     2|           [a, b, c]|
    +------+--------------------+

I think it should let you do aggregate on sorted data per key.




-----
Liang-Chi Hsieh | @viirya
Spark Technology Center
http://www.spark.tc/
--
developers-list.1001551.n3.nabble.com/Aggregating-over-
sorted-data-tp19999p20310.html
Nabble.com.

---------------------------------------------------------------------
"
ShaneTian <tianxinhui@ict.ac.cn>,"Wed, 21 Dec 2016 19:36:10 -0700 (MST)","Got an Accumulator error after adding some task metrics in Spark
 2.0.2",dev@spark.apache.org,"Hello all

I've tried to add some task metrics in
org.apache.spark.executor.ShuffleReadMetrics.scala in Spark 2.0.2, following
the format of other existing metrics, but when submitting applications, I
got these errors:

ERROR TaskSetManager: Failed to serialize task 0, not attempting to retry
it.
java.lang.UnsupportedOperationException: Accumulator must be registered
before send to executor
   at
org.apache.spark.util.AccumulatorV2.writeReplace(AccumulatorV2.scala:158)
   at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
   at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
   at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
   at java.lang.reflect.Method.invoke(Method.java:498)
   at
java.io.ObjectStreamClass.invokeWriteReplace(ObjectStreamClass.java:1118)
   at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1136)
   at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
   at
java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
   at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
   at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
   at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
   at
java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
   at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
   at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
   at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
   at
org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)
   at
org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
   at
org.apache.spark.scheduler.Task$.serializeWithDependencies(Task.scala:231)
   at
org.apache.spark.scheduler.TaskSetManager.resourceOffer(TaskSetManager.scala:458)
   ...
16/12/21 16:16:42 ERROR TaskSchedulerImpl: Resource offer failed, task set
TaskSet_0 was not serializable
Exception in thread ""main"" org.apache.spark.SparkException: Job aborted due
to stage failure: Failed to serialize task 0, not attempting to retry it.
Exception during serialization: java.lang.UnsupportedOperationException:
Accumulator must be registered before send to executor
   at
org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454)
   at
org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442)
   at
org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441)
   at
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
   at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
   at
org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441)
   at
org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
   at
org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
   at scala.Option.foreach(Option.scala:257)
   at
org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
   at
   at
org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622)
   at
org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611)
 ...
16/12/21 16:16:42 ERROR LiveListenerBus: SparkListenerBus has already
stopped! Dropping event    
SparkListenerBlockManagerAdded(1482308202401,BlockManagerId(7, 172.18.11.3,
42715),12409896960)
16/12/21 16:16:42 ERROR LiveListenerBus: SparkListenerBus has already
stopped! Dropping event    
SparkListenerBlockManagerAdded(1482308202445,BlockManagerId(5,
172.18.11.121, 41654),12409896960)

It seems like the Accumulator of task metrics has not been registered before
being used, but I also added the new metrics in the nameToAccums map in
TaskMetrics.scala:

private[spark] lazy val nameToAccums = LinkedHashMap(
  ...
  // add by txh
  shuffleRead.SHUFFLE_READ_TIME -> shuffleReadMetrics._shuffleReadTime,
  shuffleRead.SHUFFLE_MERGE_TIME -> shuffleReadMetrics._shuffleMergeTime,
  shuffleRead.SHUFFLE_MERGE_MEMORY ->
shuffleReadMetrics._shuffleMergeMemory,
  shuffleRead.SHUFFLE_USE_MEMORY -> shuffleReadMetrics._shuffleUseMemory,
  ...
)

What else should I add to make these new metrics be registered? Thanks very
much
ShuffleReadMetrics.scala
<http://apache-spark-developers-list.1001551.n3.nabble.com/file/n20330/ShuffleReadMetrics.scala>  



--

---------------------------------------------------------------------


"
Liang-Chi Hsieh <viirya@gmail.com>,"Wed, 21 Dec 2016 19:44:31 -0700 (MST)",Re: Aggregating over sorted data,dev@spark.apache.org,"
I agreed that to make sure this work, you might need to know the Spark
internal implementation for APIs such as `groupBy`.

But without any more changes to current Spark implementation, I think this
is the one possible way to achieve the required function to aggregate on
sorted data per key.





-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
Koert Kuipers <koert@tresata.com>,"Thu, 22 Dec 2016 00:13:28 -0500",Re: Aggregating over sorted data,Liang-Chi Hsieh <viirya@gmail.com>,"it can also be done with repartition + sortWithinPartitions + mapPartitions.
perhaps not as convenient but it does not rely on undocumented behavior.
i used this approach in spark-sorted. see here:
https://github.com/tresata/spark-sorted/blob/master/src/main/scala/com/tresata/spark/sorted/sql/GroupSortedDataset.scala


"
Liang-Chi Hsieh <viirya@gmail.com>,"Thu, 22 Dec 2016 03:20:47 -0700 (MST)",Re: Aggregating over sorted data,dev@spark.apache.org,"
You can't use existing aggregation functions with that. Besides, the
execution plan of `mapPartitions` doesn't support wholestage codegen.
Without that and some optimization around aggregation, that might be
possible performance degradation. Also when you have more than one keys in a
partition, you will need to take care of that in your function applied to
each partition.


Koert Kuipers wrote









-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Thu, 22 Dec 2016 03:45:15 -0700 (MST)",RE: Aggregating over sorted data,dev@spark.apache.org,"It seems that this aggregation is for dataset operations only. I would have hoped to be able to do dataframe aggregation. Something along the line of: sort_df(df).agg(my_agg_func)

In any case, note that this kind of sorting is less efficient than the sorting done in window functions for example. Specifically here what is happening is that first the data is shuffled and then the entire partition is sorted. It is possible to do it another way (although I have no idea how to do it in spark without writing a UDAF which is probably very inefficient). The other way would be to collect everything by key in each partition, sort within the key (which would be a lot faster since there are fewer elements) and then merge the results.

I was hoping to find something like: Efficient sortByKey to work with‚Ä¶

From: Koert Kuipers [via Apache Spark Developers List] [mailto:ml-node+s1001551n20332h81@n3.nabble.com]
Sent: Thursday, December 22, 2016 7:14 AM
To: Mendelson, Assaf
Subject: Re: Aggregating over sorted data

it can also be done with repartition + sortWithinPartitions + mapPartitions.
perhaps not as convenient but it does not rely on undocumented behavior.
i used this approach in spark-sorted. see here:
https://github.com/tresata/spark-sorted/blob/master/src/main/scala/com/tresata/spark/sorted/sql/GroupSortedDataset.scala


I agreed that to make sure this work, you might need to know the Spark
internal implementation for APIs such as `groupBy`.

But without any more changes to current Spark implementation, I think this
is the one possible way to achieve the required function to aggregate on
sorted data per key.





-----
Liang-Chi Hsieh | @viirya
Spark Technology Center
http://www.spark.tc/
--
3.nabble.com/Aggregating-over-sorted-data-tp19999p20331.html
om.

---------------------------------------------------------------------
=20332&i=1>


________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/Aggregating-over-sorted-data-tp19999p20332.html
To start a new topic under Apache Spark Developers List, email ml-node+s1001551n1h20@n3.nabble.com<mailto:ml-node+s1001551n1h20@n3.nabble.com>
spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=1&code=YXNzYWYubWVuZGVsc29uQHJzYS5jb218MXwtMTI4OTkxNTg1Mg==>.
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>




--
3.nabble.com/Aggregating-over-sorted-data-tp19999p20334.html
om."
trsell@gmail.com,"Thu, 22 Dec 2016 14:06:42 +0000",Re: Aggregating over sorted data,"""assaf.mendelson"" <assaf.mendelson@rsa.com>, dev@spark.apache.org","I would love this feature

:

ne
n
ow
re
Ä¶
[hidden
esata/spark/sorted/sql/GroupSortedDataset.scala
s
r-sorted-data-tp19999p20331.html
r-sorted-data-tp19999p20332.html
ervlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
er-sorted-data-tp19999p20334.html>
"
Koert Kuipers <koert@tresata.com>,"Thu, 22 Dec 2016 12:09:38 -0500",Re: Aggregating over sorted data,Liang-Chi Hsieh <viirya@gmail.com>,"yes it's less optimal because an abstraction is missing and with
mapPartitions it is done without optimizations. but aggregator is not the
right abstraction to begin with, is assumes a monoid which means no
ordering guarantees. you need a fold operation.


"
Liang-Chi Hsieh <viirya@gmail.com>,"Thu, 22 Dec 2016 21:23:02 -0700 (MST)",Re: stratified sampling scales poorly,dev@spark.apache.org,"
Hi,

I quoted the description of `sampleByKeyExact`:

""This method differs from [[sampleByKey]] in that we make additional passes
over the RDD to
create a sample size that's exactly equal to the sum of math.ceil(numItems *
samplingRate)
over all key values with a 99.99% confidence. When sampling without
replacement, we need one
additional pass over the RDD to guarantee sample size; when sampling with
replacement, we need
two additional passes.""

As you see, `sampleByKeyExact` needs additional passes over the RDD to make
sure returning correctly sample size.

If you don't need that, you can try `sampleByKey` which is also doing
stratified sampling without strict requirement of the correctness of  the
sample size.



Martin Le wrote
d
e,
‚Äù-> fraction, ‚Ä¶, ‚Äúkeyn‚Äù->





-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--
3.nabble.com/stratified-sampling-scales-poorly-tp20278p20337.html
om.

---------------------------------------------------------------------


"
Liang-Chi Hsieh <viirya@gmail.com>,"Thu, 22 Dec 2016 22:51:34 -0700 (MST)",Re: java.lang.AssertionError: assertion failed,dev@spark.apache.org,"
Hi,

called to release memory when there is another memory consumer tried to ask
more memory than current available.

I created a Jira and submit a PR for it. Please check out
https://issues.apache.org/jira/browse/SPARK-18986.



-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
Chetan Khatri <chetan.opensource@gmail.com>,"Fri, 23 Dec 2016 12:14:02 +0530",Best Practice for Spark Job Jar Generation,"user <user@spark.apache.org>, dev@spark.apache.org","Hello Spark Community,

For Spark Job Creation I use SBT Assembly to build Uber(""Super"") Jar and
then submit to spark-submit.

Example,

bin/spark-submit --class hbase.spark.chetan.com.SparkHbaseJob
/home/chetan/hbase-spark/SparkMSAPoc-assembly-1.0.jar

But other folks has debate with for Uber Less Jar, Guys can you please
explain me best practice industry standard for the same.

Thanks,

Chetan Khatri.
"
Chetan Khatri <chetan.opensource@gmail.com>,"Fri, 23 Dec 2016 16:26:47 +0530",Dependency Injection and Microservice development with Spark,"user <user@spark.apache.org>, dev@spark.apache.org","Hello Community,

Current approach I am using for Spark Job Development with Scala + SBT and
Uber Jar with yml properties file to pass configuration parameters. But If
i would like to use Dependency Injection and MicroService Development like
Spring Boot feature in Scala then what would be the standard approach.

Thanks

Chetan
"
Andy Dang <namd88@gmail.com>,"Fri, 23 Dec 2016 11:03:44 +0000",Negative number of active tasks,dev@spark.apache.org,"Hi all,

Today I hit a weird bug in Spark 2.0.2 (vanilla Spark) - the executor tab
shows negative number of active tasks.

I have about 25 jobs, each with 20k tasks so the numbers are not that crazy.

What could possibly the cause of this bug? This is the first time I've seen
it and the only special thing I'm doing is saving multiple datasets at the
same time to HDFS from different threads.

Thanks,
Andy

---------------------------------------------------------------------"
Jacek Laskowski <jacek@japila.pl>,"Fri, 23 Dec 2016 15:13:35 +0100",MapOutputTracker.getMapSizesByExecutorId and mutation on the driver?,dev <dev@spark.apache.org>,"Hi,

I've been reviewing how MapOutputTracker works and can't understand
the comment [1]:

// Synchronize on the returned array because, on the driver, it gets
mutated in place

How is this possible since ""the returned array"" is a local value? I'm
stuck and would appreciate help. Thanks!

(It also says ""Called from executors"" [2] so how could the driver be involved?!)

[1] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/MapOutputTracker.scala#L145

[2] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/MapOutputTracker.scala#L133

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Andy Dang <namd88@gmail.com>,"Fri, 23 Dec 2016 15:04:40 +0000",Re: Best Practice for Spark Job Jar Generation,Chetan Khatri <chetan.opensource@gmail.com>,"I used to use uber jar in Spark 1.x because of classpath issues (we
couldn't re-model our dependencies based on our code, and thus cluster's
run dependencies could be very different from running Spark directly in the
IDE. We had to use userClasspathFirst ""hack"" to work around this.

With Spark 2, it's easier to replace dependencies (say, Guava) than before.
We moved away from deploying superjar and just pass the libraries as part
of Spark jars (still can't use Guava v19 or later because Spark uses a
deprecated method that's not available, but that's not a big issue for us).

-------
Regards,
Andy


"
Chetan Khatri <chetan.opensource@gmail.com>,"Fri, 23 Dec 2016 23:30:18 +0530",Re: Best Practice for Spark Job Jar Generation,Andy Dang <namd88@gmail.com>,"Andy, Thanks for reply.

If we download all the dependencies at separate location  and link with
spark job jar on spark cluster, is it best way to execute spark job ?

Thanks.


"
Andy Dang <namd88@gmail.com>,"Fri, 23 Dec 2016 18:11:55 +0000",Re: Best Practice for Spark Job Jar Generation,Chetan Khatri <chetan.opensource@gmail.com>,"We remodel Spark dependencies and ours together and chuck them under the
/jars path. There are other ways to do it but we want the classpath to be
strictly as close to development as possible.

-------
Regards,
Andy


"
Chetan Khatri <chetan.opensource@gmail.com>,"Fri, 23 Dec 2016 23:53:21 +0530",Re: Best Practice for Spark Job Jar Generation,Andy Dang <namd88@gmail.com>,"Correct, so the approach you suggested and Uber Jar Approach. What i think
that Uber Jar approach is best practice because if you wish to do
environment migration then would be easy. and Performance wise also Uber
Jar Approach would be more optimised rather than Uber less approach.

Thanks.


"
Chetan Khatri <chetan.opensource@gmail.com>,"Fri, 23 Dec 2016 23:57:30 +0530",Re: Approach: Incremental data load from HBASE,Ted Yu <yuzhihong@gmail.com>,"Ted Correct, In my case i want Incremental Import from HBASE and
Incremental load to Hive. Both approach discussed earlier with Indexing
seems accurate to me. But like Sqoop support Incremental import and load
for RDBMS, Is there any tool which supports Incremental import from HBase ?




"
Chetan Khatri <chetan.opensource@gmail.com>,"Fri, 23 Dec 2016 23:58:53 +0530",Re: Negative number of active tasks,Andy Dang <namd88@gmail.com>,"Could you share Pseudo code for the same.

Cheers!

C Khatri.


"
Liang-Chi Hsieh <viirya@gmail.com>,"Sat, 24 Dec 2016 00:54:18 -0700 (MST)","Re: MapOutputTracker.getMapSizesByExecutorId and mutation on the
 driver?",dev@spark.apache.org,"
Hi,

I think the comment [1] is only correct for ""getStatistics"" as it is called
at driver side. It should be added in ""getMapSizesByExecutorId"" by mistake.



Jacek Laskowski wrote






-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
Naveen <hadoopstack@gmail.com>,"Sat, 24 Dec 2016 21:53:00 +0530",Re: Launching multiple spark jobs within a main spark job.,Liang-Chi Hsieh <viirya@gmail.com>,"Thanks Liang, Vadim and everyone for your inputs!!

With this clarity, I've tried client modes for both main and sub-spark
jobs. Every main spark job and its corresponding threaded spark jobs are
coming up on the YARN applications list and the jobs are getting executed
properly. I need to now test with cluster modes at both levels, and need to
setup spark-submit and few configurations properly on all data nodes in the
cluster. I will share the updates as and when I execute and analyze further.

Concern now which I am thinking is: how to throttle multiple jobs launching
based on the YARN cluster's availability. This exercise will be similar to
performing cluster's break-point analysis. But problem here is that we will
not know the file sizes until we read and get in memory and since Spark's
memory mechanics are more subtle and fragile, need to be 100% sure and
avoid OOM (out-of-memory) issues. Not sure if there is any process
available which can poll resource manager's information and tell if any
further jobs can be submitted to YARN.



"
Davies Liu <davies.liu@gmail.com>,"Sat, 24 Dec 2016 22:14:53 -0800",Re: Mesos Spark Fine Grained Execution - CPU count,dev@mesos.apache.org,"Using 0 for spark.mesos.mesosExecutor.cores is better than dynamic
allocation, but have to pay a little more overhead for launching a
task, which should be OK if the task is not trivial.

Since the direct result (up to 1M by default) will also go through
mesos, it's better to tune it lower, otherwise mesos could become the
bottleneck.

spark.task.maxDirectResultSize

e:
 / Berlin /
s
30 is
s
s
r
d)
om
s
uld
ob
ks
g
rs
 as
oks
ne
g.
 in
 a
and
ive
t
ks
s



-- 
 - Davies

---------------------------------------------------------------------


"
Niek <niek.bartholomeus@gmail.com>,"Sun, 25 Dec 2016 09:07:41 -0700 (MST)","Spark structured steaming from kafka - last message processed again
 after resume from checkpoint",dev@spark.apache.org,"Hi,

I described my issue in full detail on
http://stackoverflow.com/questions/41300223/spark-structured-steaming-from-kafka-last-message-processed-again-after-resume

Any idea what's going wrong?

Looking at the code base on
https://github.com/apache/spark/blob/3f62e1b5d9e75dc07bac3aa4db3e8d0615cc3cc3/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala#L290,
I don't understand why you are resuming with an already committed offset
(the one from currrentBatchId - 1) 

Thanks,

Niek.



--

---------------------------------------------------------------------


"
"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Sun, 25 Dec 2016 16:37:14 -0800","Re: Spark structured steaming from kafka - last message processed
 again after resume from checkpoint",Niek <niek.bartholomeus@gmail.com>,"Hi Niek,

That's expected. Just answered on stackoverflow.


"
"""Kazuaki Ishizaki"" <ISHIZAKI@jp.ibm.com>","Mon, 26 Dec 2016 10:12:29 +0900",Sharing data in columnar storage between two applications,"velvia@gmail.com, dev@spark.apache.org","Here is an interesting discussion to share data in columnar storage 
between two applications.
https://github.com/apache/spark/pull/15219#issuecomment-265835049

write. Each application can implement only one class to want to do (e.g. 
read or write). For example, FiloDB wants to provide a columnar storage 
that can be read from Spark. In that case, it is easy to implement only 
read APIs for Spark. These two classes can be prepared.
However, it may lead to incompatibility in ColumnarBatch. ColumnarBatch 
keeps a set of ColumnVector that can be read or written. The ColumnVector 
class should have read and write APIs. How can we put the new ColumnVector 
with only read APIs?  Here is an example to case incompatibility at 
https://gist.github.com/kiszk/00ab7d0c69f0e598e383cdc8e72bcc4d

Another possible idea is that both applications supports Apache Arrow 
APIs.
Other approaches could be.

What approach would be good for all of applications?

Regards,
Kazuaki Ishizaki

"
Mark Hamstra <mark@clearstorydata.com>,"Sun, 25 Dec 2016 20:24:29 -0500",Re: Sharing data in columnar storage between two applications,Kazuaki Ishizaki <ISHIZAKI@jp.ibm.com>,"NOt so much about between applications, rather multiple frameworks within
an application, but still related:
https://cs.stanford.edu/~matei/papers/2017/cidr_weld.pdf


"
Niek <niek.bartholomeus@gmail.com>,"Mon, 26 Dec 2016 03:37:35 -0700 (MST)","Re: Spark structured steaming from kafka - last message processed
 again after resume from checkpoint",dev@spark.apache.org,"Cool thanks!



--

---------------------------------------------------------------------


"
"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Mon, 26 Dec 2016 03:56:32 -0700 (MST)",Shuffle intermidiate results not being cached,dev@spark.apache.org,"Hi,

Sorry to be bothering everyone on the holidays but I have found what may be a bug.

I am doing a ""manual"" streaming (see http://stackoverflow.com/questions/41266956/apache-spark-streaming-performance for the specific code) where I essentially read an additional dataframe each time from file, union it with previous dataframes to create a ""window"" and then do double aggregation on the result.
Having looked at the documentation (https://spark.apache.org/docs/latest/programming-guide.html#which-storage-level-to-choose right above the headline) I expected spark to automatically cache the partial aggregation for each dataframe read and then continue with the aggregations from there. Instead it seems it reads each dataframe from file all over again.
Is this a bug? Am I doing something wrong?

Thanks.
                Assaf.




--"
Mark Hamstra <mark@clearstorydata.com>,"Mon, 26 Dec 2016 09:07:59 -0500",Re: Shuffle intermidiate results not being cached,"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Shuffle results are only reused if you are reusing the exact same RDD.  If
you are working with Dataframes that you have not explicitly cached, then
they are going to be producing new RDDs within their physical plan creation
and evaluation, so you won't get implicit shuffle reuse.  This is what
https://issues.apache.org/jira/browse/SPARK-11838 is about.


.com/
e,
d then do double
ns
idiate-results-not-being-cached-tp20358.html>
"
Mark Hamstra <mark@clearstorydata.com>,"Mon, 26 Dec 2016 09:12:06 -0500",Re: Sharing data in columnar storage between two applications,Evan Chan <velvia@gmail.com>,"Yes, this is part of Matei's current research, for which code is not yet
publicly available at all, much less in a form suitable for production use.


"
Liang-Chi Hsieh <viirya@gmail.com>,"Mon, 26 Dec 2016 08:41:40 -0700 (MST)",Re: Shuffle intermidiate results not being cached,dev@spark.apache.org,"
Hi,

Let me quote your example codes:

var totalTime: Long = 0
var allDF: org.apache.spark.sql.DataFrame = null
for {
  x <- dataframes
} {
  val timeLen = time {
    allDF = if (allDF == null) x else allDF.union(x)
    val grouped = allDF.groupBy(""cat1"",
""cat2"").agg(sum($""valToAdd"").alias(""v""))
    val grouped2 = grouped.groupBy(""cat1"").agg(sum($""v""), count($""cat2""))
    grouped2.show()
  }
  totalTime += timeLen
  println(s""Took $timeLen miliseconds"")
}
println(s""Total time was $totalTime miliseconds"")


Basically what you do is to union some dataframes for each iteration, and do
aggregation on this union data. I don't see any reused operations.

1st iteration: aggregation(x1 union x2)
2nd iteration: aggregation(x3 union (x1 union x2))
3rd iteration: aggregation(x4 union(x3 union (x1 union x2)))
...

Your first example just does two aggregation operations. But your second
example like above does this aggregation operations for each iteration. So
the time of second example grows as the iteration increases.
 




-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
Michael Gummelt <mgummelt@mesosphere.io>,"Mon, 26 Dec 2016 09:25:35 -0800",Re: Mesos Spark Fine Grained Execution - CPU count,Davies Liu <davies.liu@gmail.com>,"allocation

Maybe for CPU, but definitely not for memory.  Executors never shut down in
fine-grained mode, which means you only elastically grow and shrink CPU
usage, not memory.


:
am / Berlin
no
m
d
f
ks
or
)
 1
s
s
m
s



-- 
Michael Gummelt
Software Engineer
Mesosphere
"
Jacek Laskowski <jacek@japila.pl>,"Mon, 26 Dec 2016 21:39:09 +0100",Re: Mesos Spark Fine Grained Execution - CPU count,Michael Gummelt <mgummelt@mesosphere.io>,"Hi Michael,

That caught my attention...

Could you please elaborate on ""elastically grow and shrink CPU usage""
and how it really works under the covers? It seems that CPU usage is
just a ""label"" for an executor on Mesos. Where's this in the code?

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski


rote:
in
:
e:
t
dam / Berlin
c
t
nd
of
(
I
c
s.md)
d)
e
y
t
es
ks
p
ks
a
b

---------------------------------------------------------------------


"
Michael Gummelt <mgummelt@mesosphere.io>,"Mon, 26 Dec 2016 13:04:05 -0800",Re: Mesos Spark Fine Grained Execution - CPU count,Jacek Laskowski <jacek@japila.pl>,"In fine-grained mode (which is deprecated), Spark tasks (which are threads)
were implemented as Mesos tasks.  When a Mesos task starts and stops, its
underlying cgroup, and therefore the resources its consuming on the
cluster, grows or shrinks based on the resources allocated to the tasks,
which in Spark is just CPU.  This is what I mean by CPU usage ""elastically
growing"".

However, all Mesos tasks are run by an ""executor"", which has its own
resource allocation.  In Spark, the executor is the JVM, and all memory is
allocated to the executor, because JVMs can't relinquish memory.  If memory
were allocated to the tasks, then the cgroup's memory allocation would
shrink when the task terminated, but the JVM's memory consumption would
stay constant, and the JVM would OOM.

And, without dynamic allocation, executors never terminate during the
duration of a Spark job, because even if they're idle (no tasks), they
still may be hosting shuffle files.  That's why dynamic allocation depends
on an external shuffle service.  Since executors never terminate, and all
memory is allocated to the executors, Spark jobs even in fine-grained mode
only grow in memory allocation, they don't shrink.


n
he
re
erdam /
ve
e
p of
s
f
it
rs
ng
8
f
es
s
n



-- 
Michael Gummelt
Software Engineer
Mesosphere
"
Jacek Laskowski <jacek@japila.pl>,"Mon, 26 Dec 2016 23:11:51 +0100",Re: Mesos Spark Fine Grained Execution - CPU count,Michael Gummelt <mgummelt@mesosphere.io>,"Thanks a LOT, Michael!

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski


s)
er,
"".
s
ry
tay
ill
n
y
:
wn
U
c
terdam /
ve
ap
.
.
esos.md)
n.md)
rs
t
of
s
.
48
of
as
en

---------------------------------------------------------------------


"
"""Chawla,Sumit "" <sumitkchawla@gmail.com>","Mon, 26 Dec 2016 18:30:56 -0800",Re: Mesos Spark Fine Grained Execution - CPU count,Michael Gummelt <mgummelt@mesosphere.io>,"What is the expected effect of reducing the mesosExecutor.cores to zero?
What functionality of executor is impacted? Is the impact is just that it
just behaves like a regular process?

Regards
Sumit Chawla



U
:
t
dam /
c
t
nd
of
(
I
c
d
e
y
t
es
ks
p
a
b
"
Yu Wei <yu2003w@hotmail.com>,"Tue, 27 Dec 2016 02:50:12 +0000","Spark on mesos, it seemed spark dispatcher didn't abort when
 authorization failed",dev <dev@spark.apache.org>,"Hi Guys,


When running some cases about spark on mesos, it seemed that spark dispatcher didn't abort when authorization failed.

It seemed that spark dispatcher detected the error but did not handle it properly.

The detailed log is as below,

16/12/26 16:02:08 INFO Utils: Successfully started service on port 8081.
16/12/26 16:02:08 INFO MesosClusterUI: Bound MesosClusterUI to 0.0.0.0, and started at http://192.168.111.192:8081
I1226 16:02:08.861893 11966 sched.cpp:232] Version: 1.2.0
I1226 16:02:08.868672 11964 sched.cpp:336] New master detected at master@192.168.111.191:5050
I1226 16:02:08.870041 11964 sched.cpp:402] Authenticating with master master@192.168.111.191:5050
I1226 16:02:08.870066 11964 sched.cpp:409] Using default CRAM-MD5 authenticatee
I1226 16:02:08.870635 11959 authenticatee.cpp:97] Initializing client SASL
I1226 16:02:08.871201 11959 authenticatee.cpp:121] Creating new client SASL connection
I1226 16:02:08.971091 11964 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
I1226 16:02:08.971156 11964 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
I1226 16:02:08.972964 11962 authenticatee.cpp:259] Received SASL authentication step
I1226 16:02:08.974642 11957 authenticatee.cpp:299] Authentication success
I1226 16:02:08.975075 11964 sched.cpp:508] Successfully authenticated with master master@192.168.111.191:5050
I1226 16:02:08.977557 11960 sched.cpp:1177] Got error 'Not authorized to use role 'spark''
I1226 16:02:08.977583 11960 sched.cpp:2042] Asked to abort the driver
16/12/26 16:02:08 ERROR MesosClusterScheduler: Error received: Not authorized to use role 'spark'
I1226 16:02:08.978495 11960 sched.cpp:1223] Aborting framework
16/12/26 16:02:08 INFO MesosClusterScheduler: driver.run() returned with code DRIVER_ABORTED
16/12/26 16:02:08 INFO Utils: Successfully started service on port 7077.
16/12/26 16:02:08 INFO MesosRestServer: Started REST server for submitting applications on port 7077


It seems this is bug.


Thanks,

Jared, (??)
Software developer
Interested in open source software, big data, Linux
"
"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Tue, 27 Dec 2016 00:00:12 -0700 (MST)",RE: Shuffle intermidiate results not being cached,dev@spark.apache.org,"The reason I thought some operations would be reused is the fact that spark automatically caches shuffle data which means the partial aggregation for pervious dataframes would be saved. Unfortunatly, as Mark Hamstra explained this is not the case because this is considered a new RDD and therefore the previous data is lost.

I am still wondering if there is any way to do high performance streaming of SQL. Basically this is not far from what DStream would do assuming we convert a sliding window (e.g. 24 hours every 15 minutes) as we would be doing a foreachRDD which would do the joining behind the scenes.
The problem is that any attempt to do a streaming like this results in performance which is hundreds of times slower than batch.
Is there a correct way to do such an aggregation on streaming data (using dataframes rather than RDD operations).
Assaf.



From: Liang-Chi Hsieh [via Apache Spark Developers List] [mailto:ml-node+s1001551n20361h80@n3.nabble.com]
Sent: Monday, December 26, 2016 5:42 PM
To: Mendelson, Assaf
Subject: Re: Shuffle intermidiate results not being cached


Hi,

Let me quote your example codes:

var totalTime: Long = 0
var allDF: org.apache.spark.sql.DataFrame = null
for {
  x <- dataframes
} {
  val timeLen = time {
    allDF = if (allDF == null) x else allDF.union(x)
    val grouped = allDF.groupBy(""cat1"", ""cat2"").agg(sum($""valToAdd"").alias(""v""))
    val grouped2 = grouped.groupBy(""cat1"").agg(sum($""v""), count($""cat2""))
    grouped2.show()
  }
  totalTime += timeLen
  println(s""Took $timeLen miliseconds"")
}
println(s""Total time was $totalTime miliseconds"")


Basically what you do is to union some dataframes for each iteration, and do aggregation on this union data. I don't see any reused operations.

1st iteration: aggregation(x1 union x2)
2nd iteration: aggregation(x3 union (x1 union x2))
3rd iteration: aggregation(x4 union(x3 union (x1 union x2)))
...

Your first example just does two aggregation operations. But your second example like above does this aggregation operations for each iteration. So the time of second example grows as the iteration increases.

Liang-Chi Hsieh | @viirya
Spark Technology Center
http://www.spark.tc/

________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/Shuffle-intermidiate-results-not-being-cached-tp20358p20361.html
To start a new topic under Apache Spark Developers List, email ml-node+s1001551n1h20@n3.nabble.com<mailto:ml-node+s1001551n1h20@n3.nabble.com>
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>




--"
"""John Fang"" <xiaojian.fxj@alibaba-inc.com>","Tue, 27 Dec 2016 15:51:18 +0800",=?UTF-8?B?aG93IGNhbiBJIGdldCB0aGUgYXBwbGljYXRpb24gYmVsb25nIHRvIHRoZSBkcml2ZXLvvJ8=?=,"""spark-user"" <user@spark.apache.org>,
  ""spark-dev"" <dev@spark.apache.org>","I hope I can get the application by the driverId, but I don't find the rest api at spark„ÄÇThen how can i get the application, which belong to one driver„ÄÇ"
dragonly <liyilongko@gmail.com>,"Tue, 27 Dec 2016 07:32:59 -0700 (MST)","What is mainly different from a UDT and a spark internal type that
 ExpressionEncoder recognized?",dev@spark.apache.org,"I'm recently reading the source code of the SparkSQL project, and found some
interesting databricks blogs about the tungsten project. I've roughly read
through the encoder and unsafe representation part of the tungsten
project(haven't read the algorithm part such as cache friendly hashmap
algorithms).
Now there's a big puzzle in front of me about the codegen of SparkSQL and
how does the codegen utilize the tungsten encoding between JMV objects and
unsafe bits. 
So can anyone tell me that's the main difference in situations where I write
a UDT like ExamplePointUDT in SparkSQL or just create an ArrayType which can
be handled by the tungsten encoder? I'll really appreciate it if you can go
through some concrete code examples. thanks a lot!



--

---------------------------------------------------------------------


"
Liang-Chi Hsieh <viirya@gmail.com>,"Tue, 27 Dec 2016 18:58:56 -0700 (MST)",RE: Shuffle intermidiate results not being cached,dev@spark.apache.org,"
Hi,

Every iteration the data you run aggregation on it is different. As I showed
in previous reply:

1st iteration: aggregation(x1 union x2)
2nd iteration: aggregation(x3 union (x1 union x2))
3rd iteration: aggregation(x4 union(x3 union (x1 union x2)))

In 1st you run aggregation on the data of x1 and x2. In 2nd the data is x1,
x2 and x3. Even you work on the same RDD, you won't see reuse of the shuffle
data because the shuffle data is different.

In your second example, I think the way to reduce the computation is like:

var totalTime: Long = 0
var allDF: org.apache.spark.sql.DataFrame = null
for {
  x <- dataframes
} {
  val timeLen = time {
    allDF = if (allDF == null) x else allDF.union(x) // Union previous
aggregation summary with new dataframe in this window
    val grouped = allDF.groupBy(""cat1"",
""cat2"").agg(sum($""valToAdd"").alias(""v""))
    val grouped2 = grouped.groupBy(""cat1"").agg(sum($""v""), count($""cat2""))
    grouped2.show()
    allDF = grouped  // Replace the union of data with aggregated summary
  }
  totalTime += timeLen
  println(s""Took $timeLen miliseconds"")
}
println(s""Total time was $totalTime miliseconds"")

You don't need to recompute the aggregation of previous dataframes in each
iteration. You just need to get the summary and union it with new dataframe
to compute the newer aggregation summary in next iteration. It is more
similar to streaming case, I don't think you can/should recompute all the
data since the beginning of a stream.



assaf.mendelson wrote











-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
Jang tao <uuleon@live.cn>,"Wed, 28 Dec 2016 03:18:15 +0000",unsubscribe,"dev <dev@spark.apache.org>, ""user@spark.apache.org""
	<user@spark.apache.org>","
"
Michael Armbrust <michael@databricks.com>,"Tue, 27 Dec 2016 19:32:51 -0800","Re: What is mainly different from a UDT and a spark internal type
 that ExpressionEncoder recognized?",dragonly <liyilongko@gmail.com>,"An encoder uses reflection
<https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/ScalaReflection.scala>
to generate expressions that can extract data out of an object (by calling
methods on the object) and encode its contents directly into the tungsten
binary row format (and vice versa).  We codegenerate bytecode that
evaluates these expression in the same way that we code generate code for
normal expression evaluation in query processing.  However, this reflection
only works for simple ATDs
<https://en.wikipedia.org/wiki/Algebraic_data_type>.  Another key thing to
realize is that we do this reflection / code generation at runtime, so we
aren't constrained by binary compatibility across versions of spark.

UDTs let you write custom code that translates an object into into a
generic row, which we can then translate into Spark's internal format
(using a RowEncoder). Unlike expressions and tungsten binary encoding, the
Row type that you return here is a stable public API that hasn't changed
since Spark 1.3.

So to summarize, if encoders don't work for your specific types you can use
UDTs, but they probably won't be as efficient. I'd love to unify these code
paths more, but its actually a fair amount of work to come up with a good
stable public API that doesn't sacrifice performance.


"
Kyle Kelley <rgbkrk@gmail.com>,"Tue, 27 Dec 2016 19:37:11 -0800",Re: unsubscribe,Jang tao <uuleon@live.cn>,"You are now in position 238 for unsubscription. If you wish for your
subscription to occur immediately, please email
dev-unsubscribe@spark.apache.org

Best wishes.

---------------------------------------------------------------------


"
dragonly <liyilongko@gmail.com>,"Tue, 27 Dec 2016 22:05:29 -0700 (MST)","Re: What is mainly different from a UDT and a spark internal type
 that ExpressionEncoder recognized?",dev@spark.apache.org,"Thanks for your reply!

Here's my *understanding*:
basic types that ScalaReflection understands are encoded into tungsten
binary format, while UDTs are encoded into GenericInternalRow, which stores
the JVM objects in an Array[Any] under the hood, and thus lose those memory
footprint efficiency and cpu cache efficiency stuff provided by tungsten
encoding.

If the above is correct, then here are my *further questions*:
Are SparkPlan nodes (those ends with Exec) all codegenerated before actually
running the toRdd logic? I know there are some non-codegenable nodes which
implement trait CodegenFallback, but there's also a doGenCode method in the
trait, so the actual calling convention really puzzles me. And I've tried to
trace those calling flow for a few days but found them scattered every
where. I cannot make a big graph of the method calling order even with the
help of IntelliJ.

Let me rephrase this. How does the SparkSQL engine call the codegen APIs to
do the job of producing RDDs? What are those eval methods in Expressions for
given there's already a doGenCode next to it?



--

---------------------------------------------------------------------


"
"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Tue, 27 Dec 2016 23:28:21 -0700 (MST)",RE: Shuffle intermidiate results not being cached,dev@spark.apache.org,"I understand the actual dataframe is different, but the underlying partitions are not (hence the importance of mark's response). The code you suggested would not work as allDF and x would have different schema's (x is the original and allDF becomes the grouped).
I can do something like this:
  var totalTime: Long = 0
  var allDF: DataFrame = null
  for {
    x <- dataframes
  } {
    val timeLen = time {
      val grouped = x.groupBy(""cat1"", ""cat2"").agg(sum($""valToAdd"").alias(""v""))
      allDF = if (allDF == null) grouped else {
        allDF.union(grouped).groupBy(""cat1"", ""cat2"").agg(sum($""v"").alias(""v""))
      }
      val grouped2 = allDF.groupBy(""cat1"").agg(sum($""v""), count($""cat2""))
      grouped2.show()
    }
    totalTime += timeLen
    println(s""Took $timeLen miliseconds"")
  }
  println(s""Overall time was $totalTime miliseconds"")
}

and this indeed improves performance (I actually had a couple more tries) but:

1.       This still gives crappy performance (for 167 slices I get a throughput which is 10 times lower than batch after doing some tuning including caching and coalescing)

2.       This works because the aggregation here is sum and we don't forget. For more general aggregations we would have to join them together (can't do it for count distinct for example) and we will need to ""forget"" frames when moving out of the window (we can subtract a sum but not a max).

The best solution I found so far (performance wise) was to write a custom UDAF which does the window internally. This was still 8 times lower throughput than batch and required a lot of coding and is not a general solution.

I am looking for an approach to improve the performance even more (preferably to either be on par with batch or a relatively low factor which remains constant when the number of slices rise) and including the option to ""forget"" frames.

Assaf.




From: Liang-Chi Hsieh [via Apache Spark Developers List] [mailto:ml-node+s1001551n20371h90@n3.nabble.com]
Sent: Wednesday, December 28, 2016 3:59 AM
To: Mendelson, Assaf
Subject: RE: Shuffle intermidiate results not being cached


Hi,

Every iteration the data you run aggregation on it is different. As I showed in previous reply:

1st iteration: aggregation(x1 union x2)
2nd iteration: aggregation(x3 union (x1 union x2))
3rd iteration: aggregation(x4 union(x3 union (x1 union x2)))

In 1st you run aggregation on the data of x1 and x2. In 2nd the data is x1, x2 and x3. Even you work on the same RDD, you won't see reuse of the shuffle data because the shuffle data is different.

In your second example, I think the way to reduce the computation is like:

var totalTime: Long = 0
var allDF: org.apache.spark.sql.DataFrame = null
for {
  x <- dataframes
} {
  val timeLen = time {
    allDF = if (allDF == null) x else allDF.union(x) // Union previous aggregation summary with new dataframe in this window
    val grouped = allDF.groupBy(""cat1"", ""cat2"").agg(sum($""valToAdd"").alias(""v""))
    val grouped2 = grouped.groupBy(""cat1"").agg(sum($""v""), count($""cat2""))
    grouped2.show()
    allDF = grouped  // Replace the union of data with aggregated summary
  }
  totalTime += timeLen
  println(s""Took $timeLen miliseconds"")
}
println(s""Total time was $totalTime miliseconds"")

You don't need to recompute the aggregation of previous dataframes in each iteration. You just need to get the summary and union it with new dataframe to compute the newer aggregation summary in next iteration. It is more similar to streaming case, I don't think you can/should recompute all the data since the beginning of a stream.

assaf.mendelson wrote
The reason I thought some operations would be reused is the fact that spark automatically caches shuffle data which means the partial aggregation for pervious dataframes would be saved. Unfortunatly, as Mark Hamstra explained this is not the case because this is considered a new RDD and therefore the previous data is lost.

I am still wondering if there is any way to do high performance streaming of SQL. Basically this is not far from what DStream would do assuming we convert a sliding window (e.g. 24 hours every 15 minutes) as we would be doing a foreachRDD which would do the joining behind the scenes.
The problem is that any attempt to do a streaming like this results in performance which is hundreds of times slower than batch.
Is there a correct way to do such an aggregation on streaming data (using dataframes rather than RDD operations).
Assaf.



From: Liang-Chi Hsieh [via Apache Spark Developers List] [mailto:[hidden email]</user/SendEmail.jtp?type=node&node=20371&i=0>]
Sent: Monday, December 26, 2016 5:42 PM
To: Mendelson, Assaf
Subject: Re: Shuffle intermidiate results not being cached


Hi,

Let me quote your example codes:

var totalTime: Long = 0
var allDF: org.apache.spark.sql.DataFrame = null
for {
  x <- dataframes
} {
  val timeLen = time {
    allDF = if (allDF == null) x else allDF.union(x)
    val grouped = allDF.groupBy(""cat1"", ""cat2"").agg(sum($""valToAdd"").alias(""v""))
    val grouped2 = grouped.groupBy(""cat1"").agg(sum($""v""), count($""cat2""))
    grouped2.show()
  }
  totalTime += timeLen
  println(s""Took $timeLen miliseconds"")
}
println(s""Total time was $totalTime miliseconds"")


Basically what you do is to union some dataframes for each iteration, and do aggregation on this union data. I don't see any reused operations.

1st iteration: aggregation(x1 union x2)
2nd iteration: aggregation(x3 union (x1 union x2))
3rd iteration: aggregation(x4 union(x3 union (x1 union x2)))
...

Your first example just does two aggregation operations. But your second example like above does this aggregation operations for each iteration. So the time of second example grows as the iteration increases.

Liang-Chi Hsieh | @viirya
Spark Technology Center
http://www.spark.tc/

________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/Shuffle-intermidiate-results-not-being-cached-tp20358p20361.html
To start a new topic under Apache Spark Developers List, email [hidden email]</user/SendEmail.jtp?type=node&node=20371&i=1><mailto:[hidden email]</user/SendEmail.jtp?type=node&node=20371&i=2>>
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml><http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml%3e>
Liang-Chi Hsieh | @viirya
Spark Technology Center
http://www.spark.tc/

________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/Shuffle-intermidiate-results-not-being-cached-tp20358p20371.html
To start a new topic under Apache Spark Developers List, email ml-node+s1001551n1h20@n3.nabble.com<mailto:ml-node+s1001551n1h20@n3.nabble.com>
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>




--"
Jacek Laskowski <jacek@japila.pl>,"Wed, 28 Dec 2016 10:16:31 +0100","Why ShuffleManager.registerShuffle takes shuffleId since
 ShuffleDependency has it too?",dev <dev@spark.apache.org>,"Hi,

While reviewing ShuffleManager I've noticed that registerShuffle
method [1] takes shuffleId and ShuffleDependency which seems a code
duplication since ShuffleDependency has shuffleId.

Any reason for having shuffleId specified explicitly?

[1] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/shuffle/ShuffleManager.scala#L35

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Wed, 28 Dec 2016 10:21:58 +0100",Why is spark.shuffle.sort.bypassMergeThreshold 200?,dev <dev@spark.apache.org>,"Hi,

I'm wondering what's so special about 200 to have it the default value
of spark.shuffle.sort.bypassMergeThreshold?

Is this arbitrary number? Is there any theory behind it?

Is the number of partitions in Spark SQL, i.e. 200, somehow related to
spark.shuffle.sort.bypassMergeThreshold?

scala> spark.range(5).groupByKey(_ % 5).count.rdd.getNumPartitions
res3: Int = 200

I'd appreciate any guidance to get the gist of this seemingly magic
number. Thanks!

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Chetan Khatri <chetan.opensource@gmail.com>,"Wed, 28 Dec 2016 17:52:28 +0530",Apache Hive with Spark Configuration,"user <user@spark.apache.org>, dev@spark.apache.org","Hello Users / Developers,

I am using Hive 2.0.1 with MySql as a Metastore, can you tell me which
version is more compatible with Spark 2.0.2 ?

THanks
"
Chetan Khatri <chetan.opensource@gmail.com>,"Wed, 28 Dec 2016 21:00:55 +0530",Error: at sqlContext.createDataFrame with RDD and Schema,"user <user@spark.apache.org>, dev@spark.apache.org","Hello Spark Community,

I am reading HBase table from Spark and getting RDD but now i wants to
convert RDD of Spark Rows and want to convert to DF.

*Source Code:*

bin/spark-shell --packages
it.nerdammer.bigdata:spark-hbase-connector_2.10:1.0.3 --conf
spark.hbase.host=127.0.0.1

import it.nerdammer.spark.hbase._
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.sql.Row
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.types.StructField
import org.apache.spark.sql.types.StringType

val sparkConf = new SparkConf().setAppName(""HBase Spark POC"")

val sparkContext = new SparkContext(sparkConf)

val sqlContext = new org.apache.spark.sql.SQLContext(sparkContext)

val hBaseRDD = sc.hbaseTable[(Option[String], Option[Int], Option[Int],
Option[Int], Option[Int], Option[Int])](""university"").select(""maths"",
""english"",""science"",""history"",""computer"").inColumnFamily(""school"")

val rowRDD = hBaseRDD.map(i =>
Row(i._1.get,i._2.get,i._3.get,i._4.get,i._5.get,i._6.get))

val stdSchemaString= ""Rowid,maths,english,science,history,computer""

val stdSchema= StructType(stdSchemaString.split("","").map(fieldName =>
StructField(fieldName, StringType, true)))

val stdDf = sqlContext.createDataFrame(rowRDD,stdSchema);

// Getting Error

stdDf.registerTempTable(""student"")

sqlContext.sql(""select * from student"").show()

*Error*

scala> val stdDf = sqlContext.createDataFrame(rowRDD,stdSchema);
16/12/28 20:50:59 ERROR metastore.RetryingHMSHandler:
AlreadyExistsException(message:Database default already exists)
at
org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database(HiveMetaStore.java:891)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at
org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
at com.sun.proxy.$Proxy21.create_database(Unknown Source)
at
org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createDatabase(HiveMetaStoreClient.java:644)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at
org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)
at com.sun.proxy.$Proxy22.createDatabase(Unknown Source)
at org.apache.hadoop.hive.ql.metadata.Hive.createDatabase(Hive.java:306)
at
org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply$mcV$sp(HiveClientImpl.scala:309)
at
org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply(HiveClientImpl.scala:309)
at
org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply(HiveClientImpl.scala:309)
at
org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:280)
at
org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:227)
at
org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:226)
at
org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:269)
at
org.apache.spark.sql.hive.client.HiveClientImpl.createDatabase(HiveClientImpl.scala:308)
at
org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply$mcV$sp(HiveExternalCatalog.scala:99)
at
org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply(HiveExternalCatalog.scala:99)
at
org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply(HiveExternalCatalog.scala:99)
at
org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:72)
at
org.apache.spark.sql.hive.HiveExternalCatalog.createDatabase(HiveExternalCatalog.scala:98)
at
org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:147)
at
org.apache.spark.sql.catalyst.catalog.SessionCatalog.<init>(SessionCatalog.scala:89)
at
org.apache.spark.sql.hive.HiveSessionCatalog.<init>(HiveSessionCatalog.scala:51)
at
org.apache.spark.sql.hive.HiveSessionState.catalog$lzycompute(HiveSessionState.scala:49)
at
org.apache.spark.sql.hive.HiveSessionState.catalog(HiveSessionState.scala:48)
at
org.apache.spark.sql.hive.HiveSessionState$$anon$1.<init>(HiveSessionState.scala:63)
at
org.apache.spark.sql.hive.HiveSessionState.analyzer$lzycompute(HiveSessionState.scala:63)
at
org.apache.spark.sql.hive.HiveSessionState.analyzer(HiveSessionState.scala:62)
at
org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:49)
at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64)
at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:542)
at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:302)
at org.apache.spark.sql.SQLContext.createDataFrame(SQLContext.scala:337)
at
$line34.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:42)
at $line34.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:47)
at $line34.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:49)
at $line34.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:51)
at $line34.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:53)
at $line34.$read$$iw$$iw$$iw$$iw$$iw.<init>(<console>:55)
at $line34.$read$$iw$$iw$$iw$$iw.<init>(<console>:57)
at $line34.$read$$iw$$iw$$iw.<init>(<console>:59)
at $line34.$read$$iw$$iw.<init>(<console>:61)
at $line34.$read$$iw.<init>(<console>:63)
at $line34.$read.<init>(<console>:65)
at $line34.$read$.<init>(<console>:69)
at $line34.$read$.<clinit>(<console>)
at $line34.$eval$.$print$lzycompute(<console>:7)
at $line34.$eval$.$print(<console>:6)
at $line34.$eval.$print(<console>)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:786)
at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1047)
at
scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:638)
at
scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:637)
at
scala.reflect.internal.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:31)
at
scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:19)
at
scala.tools.nsc.interpreter.IMain$WrappedRequest.loadAndRunReq(IMain.scala:637)
at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:569)
at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:565)
at scala.tools.nsc.interpreter.ILoop.interpretStartingWith(ILoop.scala:807)
at scala.tools.nsc.interpreter.ILoop.command(ILoop.scala:681)
at scala.tools.nsc.interpreter.ILoop.processLine(ILoop.scala:395)
at scala.tools.nsc.interpreter.ILoop.loop(ILoop.scala:415)
at
scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply$mcZ$sp(ILoop.scala:923)
at
scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply(ILoop.scala:909)
at
scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply(ILoop.scala:909)
at
scala.reflect.internal.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:97)
at scala.tools.nsc.interpreter.ILoop.process(ILoop.scala:909)
at org.apache.spark.repl.Main$.doMain(Main.scala:68)
at org.apache.spark.repl.Main$.main(Main.scala:51)
at org.apache.spark.repl.Main.main(Main.scala)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at
org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736)
at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185)
at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210)
at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124)
at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

stdDf: org.apache.spark.sql.DataFrame = [Rowid: string, maths: string ... 4
more fields]

What would be resolution ?

Thanks,
Chetan
"
Chetan Khatri <chetan.opensource@gmail.com>,"Wed, 28 Dec 2016 21:32:12 +0530",Re: Error: at sqlContext.createDataFrame with RDD and Schema,"user <user@spark.apache.org>, dev@spark.apache.org","Resolved above error by creating SparkSession

val spark = SparkSession.builder().appName(""Hbase - Spark
POC"").getOrCreate()

Error after:

spark.sql(""SELECT * FROM student"").show()

But while doing show() action on Dataframe throws below error:

scala> sqlContext.sql(""select * from student"").show()
16/12/28 21:04:23 ERROR executor.Executor: Exception in task 0.0 in stage
2.0 (TID 2)
java.lang.RuntimeException: Error while encoding:
java.lang.RuntimeException: java.lang.Integer is not a valid external type
for schema of string
if (assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row
object).isNullAt) null else staticinvoke(class
org.apache.spark.unsafe.types.UTF8String, StringType, fromString,
validateexternaltype(getexternalrowfield(assertnotnull(input[0,
org.apache.spark.sql.Row, true], top level row object), 0, Rowid),
StringType), true) AS Rowid#35
+- if (assertnotnull(input[0, org.apache.spark.sql.Row, true], top level
row object).isNullAt) null else staticinvoke(class
org.apache.spark.unsafe.types.UTF8String, StringType, fromString,
validateexternaltype(getexternalrowfield(assertnotnull(input[0,
org.apache.spark.sql.Row, true], top level row object), 0, Rowid),
StringType), true)
   :- assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row
object).isNullAt
   :  :- assertnotnull(input[0, org.apache.spark.sql.Row, true], top level
row object)
   :  :  +- input[0, org.apache.spark.sql.Row, true]
   :  +- 0
   :- null
   +- staticinvoke(class org.apache.spark.unsafe.types.UTF8String,
StringType, fromString,
validateexternaltype(getexternalrowfield(assertnotnull(input[0,
org.apache.spark.sql.Row, true], top level row object), 0, Rowid),
StringType), true)
      +- validateexternaltype(getexternalrowfield(assertnotnull(input[0,
org.apache.spark.sql.Row, true], top level row object), 0, Rowid),
StringType)
         +- getexternalrowfield(assertnotnull(input[0,
org.apache.spark.sql.Row, true], top level row object), 0, Rowid)
            +- assertnotnull(input[0, org.apache.spark.sql.Row, true], top
level row object)
               +- input[0, org.apache.spark.sql.Row, true]

if (assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row
object).isNullAt) null else staticinvoke(class
org.apache.spark.unsafe.types.UTF8String, StringType, fromString,
validateexternaltype(getexternalrowfield(assertnotnull(input[0,
org.apache.spark.sql.Row, true], top level row object), 1, maths),
StringType), true) AS maths#36
+- if (assertnotnull(input[0, org.apache.spark.sql.Row, true], top level
row object).isNullAt) null else staticinvoke(class
org.apache.spark.unsafe.types.UTF8String, StringType, fromString,
validateexternaltype(getexternalrowfield(assertnotnull(input[0,
org.apache.spark.sql.Row, true], top level row object), 1, maths),
StringType), true)
   :- assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row
object).isNullAt
   :  :- assertnotnull(input[0, org.apache.spark.sql.Row, true], top level
row object)
   :  :  +- input[0, org.apache.spark.sql.Row, true]
   :  +- 1
   :- null
   +- staticinvoke(class org.apache.spark.unsafe.types.UTF8String,
StringType, fromString,
validateexternaltype(getexternalrowfield(assertnotnull(input[0,
org.apache.spark.sql.Row, true], top level row object), 1, maths),
StringType), true)
      +- validateexternaltype(getexternalrowfield(assertnotnull(input[0,
org.apache.spark.sql.Row, true], top level row object), 1, maths),
StringType)
         +- getexternalrowfield(assertnotnull(input[0,
org.apache.spark.sql.Row, true], top level row object), 1, maths)
            +- assertnotnull(input[0, org.apache.spark.sql.Row, true], top
level row object)
               +- input[0, org.apache.spark.sql.Row, true]

if (assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row
object).isNullAt) null else staticinvoke(class
org.apache.spark.unsafe.types.UTF8String, StringType, fromString,
validateexternaltype(getexternalrowfield(assertnotnull(input[0,
org.apache.spark.sql.Row, true], top level row object), 2, english),
StringType), true) AS english#37
+- if (assertnotnull(input[0, org.apache.spark.sql.Row, true], top level
row object).isNullAt) null else staticinvoke(class
org.apache.spark.unsafe.types.UTF8String, StringType, fromString,
validateexternaltype(getexternalrowfield(assertnotnull(input[0,
org.apache.spark.sql.Row, true], top level row object), 2, english),
StringType), true)

Kindly help, unable to check with error that what exactly is.

Thanks.,



"
Asher Krim <akrim@hubspot.com>,"Wed, 28 Dec 2016 21:52:25 +0200",ml word2vec finSynonyms return type,dev@spark.apache.org,"Hey all,

I would like to propose changing the return type of `findSynonyms` in ml's
Word2Vec
<https://github.com/apache/spark/blob/branch-2.1/mllib/src/main/scala/org/apache/spark/ml/feature/Word2Vec.scala#L233-L248>
:

def findSynonyms(word: String, num: Int): DataFrame = {
  val spark = SparkSession.builder().getOrCreate()
  spark.createDataFrame(wordVectors.findSynonyms(word, num)).toDF(""word"",
""similarity"")
}

I find it very strange that the results are parallelized before being
returned to the user. The results are already on the driver to begin with,
and I can imagine that for most usecases (and definitely for ours) the
synonyms are collected right back to the driver. This incurs both an added
cost of shipping data to and from the cluster, as well as a more cumbersome
interface than needed.

Can we change it to just the following?

def findSynonyms(word: String, num: Int): Array[(String, Double)] = {
  wordVectors.findSynonyms(word, num)
}

If the user wants the results parallelized, they can still do so on their
own.

(I had brought this up a while back in Jira. It was suggested that the
mailing list would be a better forum to discuss it, so here we are.)

Thanks,
-- 
Asher Krim
Senior Software Engineer
"
Liang-Chi Hsieh <viirya@gmail.com>,"Wed, 28 Dec 2016 19:19:22 -0700 (MST)",RE: Shuffle intermidiate results not being cached,dev@spark.apache.org,"
The shuffle data can be reused only if you use the same RDD. When you union
x1's RDD and x2's RDD in first iteration, and union x1's RDD and x2's RDD
and x3's RDD in 2nd iteration, how do you think they are the same RDD?

I just use the previous example code to show that you should not recompute
all data since the beginning of stream. Usually, a streaming job computes a
summary of collected data in a window. If you want to compute all the data,
you should use batch instead of streaming. In other words, if you run a
long-running streaming job, would you like to recompute all data every few
seconds after one year?

BTW, you don't need to compute:

      val grouped2 = allDF.groupBy(""cat1"").agg(sum($""v""), count($""cat2""))

for each iteration. This should be run after the loop if you want to compare
it with batch. And you don't need to run two aggregation in the loop for
allDF.

  var totalTime: Long = 0
  var allDF: DataFrame = null
  for {
    x <- dataframes
  } {
    val timeLen = time {
      allDF = if (allDF == null) x else {
        allDF.union(x).groupBy(""cat1"", ""cat2"").agg(sum($""v"").alias(""v""))
      }
    }
    val timeLen2 = time {
      val grouped2 = allDF.groupBy(""cat1"").agg(sum($""v""), count($""cat2""))
      grouped2.show()
    }
    totalTime += timeLen + timeLen2
    println(s""Took $timeLen miliseconds"")
  }
  println(s""Overall time was $totalTime miliseconds"")
}

Of course, this may not work for all aggregations. I just show that you do
redundant work in this version when comparing to your batch code. For other
aggregations, you may need other design to do similar job.



assaf.mendelson wrote











-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
Liang-Chi Hsieh <viirya@gmail.com>,"Wed, 28 Dec 2016 19:22:07 -0700 (MST)",RE: Shuffle intermidiate results not being cached,dev@spark.apache.org,"  var totalTime: Long = 0
  var allDF: DataFrame = null
  for {
    x <- dataframes
  } {
    val timeLen = time {
      allDF = if (allDF == null) x else {
        allDF.union(x).groupBy(""cat1"", ""cat2"").agg(sum($""v"").alias(""v""))
      }
    }
    println(s""Took $timeLen miliseconds"")
    totalTime += timeLen
  }
  val timeLen2 = time {
    val grouped2 = allDF.groupBy(""cat1"").agg(sum($""v""), count($""cat2""))
    grouped2.show()
  }
  totalTime += timeLen2
  println(s""Overall time was $totalTime miliseconds"")
} 



Liang-Chi Hsieh wrote











-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
Liang-Chi Hsieh <viirya@gmail.com>,"Wed, 28 Dec 2016 19:38:38 -0700 (MST)",Re: Error: at sqlContext.createDataFrame with RDD and Schema,dev@spark.apache.org,"
Your schema is all fields are string:


But looks like you have integer columns in the RDD?


Chetan Khatri wrote







-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
Miguel Morales <therevoltingx@gmail.com>,"Wed, 28 Dec 2016 18:41:12 -0800",Re: Dependency Injection and Microservice development with Spark,Lars Albertsson <lalle@mapflat.com>,"Hi

Not sure about Spring boot but trying to use DI libraries you'll run into serialization issues.    I've had luck using an old version of Scaldi.  Recently though I've been passing the class types as arguments with default values.  Then in the spark code it gets instantiated.  So you're basically passing and serializing a class name.

Sent from my iPhone

d
f i

---------------------------------------------------------------------


"
Liang-Chi Hsieh <viirya@gmail.com>,"Wed, 28 Dec 2016 20:05:12 -0700 (MST)",Re: Why is spark.shuffle.sort.bypassMergeThreshold 200?,dev@spark.apache.org,"
This https://github.com/apache/spark/pull/1799 seems the first PR to
introduce this number. But there is no explanation about the number.


Jacek Laskowski wrote






-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
Yin Huai <yhuai@databricks.com>,"Thu, 29 Dec 2016 08:02:59 -0800",[ANNOUNCE] Announcing Apache Spark 2.1.0,"""dev@spark.apache.org"" <dev@spark.apache.org>, user <user@spark.apache.org>","Hi all,

Apache Spark 2.1.0 is the second release of Spark 2.x line. This release
makes significant strides in the production readiness of Structured
Streaming, with added support for event time watermarks
<https://spark.apache.org/docs/2.1.0/structured-streaming-programming-guide.html#handling-late-data-and-watermarking>
and Kafka 0.10 support
<https://spark.apache.org/docs/2.1.0/structured-streaming-kafka-integration.html>.
In addition, this release focuses more on usability, stability, and polish,
resolving over 1200 tickets.

We'd like to thank our contributors and users for their contributions and
early feedback to this release. This release would not have been possible
without you.

To download Spark 2.1.0, head over to the download page:
http://spark.apache.org/downloads.html

To view the release notes:
https://spark.apache.org/releases/spark-release-2-1-0.html

(note: If you see any issues with the release notes, webpage or published
artifacts, please contact me directly off-list)
"
=?UTF-8?B?5Yav5L+K5bOw?= <junfeng.feng@gmail.com>,"Thu, 29 Dec 2016 16:26:54 +0000",Re: [ANNOUNCE] Announcing Apache Spark 2.1.0,"Yin Huai <yhuai@databricks.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>, 
	user <user@spark.apache.org>","Congratulations! Will try this out.


Hi all,

Apache Spark 2.1.0 is the second release of Spark 2.x line. This release
makes significant strides in the production readiness of Structured
Streaming, with added support for event time watermarks
<https://spark.apache.org/docs/2.1.0/structured-streaming-programming-guide.html#handling-late-data-and-watermarking>
and Kafka 0.10 support
<https://spark.apache.org/docs/2.1.0/structured-streaming-kafka-integration.html>.
In addition, this release focuses more on usability, stability, and polish,
resolving over 1200 tickets.

We'd like to thank our contributors and users for their contributions and
early feedback to this release. This release would not have been possible
without you.

To download Spark 2.1.0, head over to the download page:
http://spark.apache.org/downloads.html

To view the release notes:
https://spark.apache.org/releases/spark-release-2-1-0.html

(note: If you see any issues with the release notes, webpage or published
artifacts, please contact me directly off-list)
"
Koert Kuipers <koert@tresata.com>,"Thu, 29 Dec 2016 11:49:08 -0500",shapeless in spark 2.1.0,"""dev@spark.apache.org"" <dev@spark.apache.org>","i just noticed that spark 2.1.0 bring in a new transitive dependency on
shapeless 2.0.0

shapeless is a popular library for scala users, and shapeless 2.0.0 is old
(2014) and not compatible with more current versions.

so this means a spark user that uses shapeless in his own development
cannot upgrade safely from 2.0.0 to 2.1.0, i think.

wish i had noticed this sooner
"
=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@databricks.com>,"Thu, 29 Dec 2016 18:10:46 +0100",Re: shapeless in spark 2.1.0,Koert Kuipers <koert@tresata.com>,"Which dependency pulls in shapeless?


d



-- 

Herman van H√∂vell

Software Engineer

Databricks Inc.

hvanhovell@databricks.com

+31 6 420 590 27

databricks.com

[image: http://databricks.com] <http://databricks.com/>
"
"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Thu, 29 Dec 2016 10:21:23 -0700 (MST)",repeated unioning of dataframes take worse than O(N^2) time,dev@spark.apache.org,"Hi,

I have been playing around with doing union between a large number of dataframes and saw that the performance of the actual union (not the action) is worse than O(N^2). Since a union basically defines a lineage (i.e. current + union with of other as a child) this should be almost instantaneous, however in practice this can be very costly.

I was wondering why this is and if there is a way to fix this.

A sample test:
def testUnion(n: Int): Long = {
  val dataframes = for {
    x <- 0 until n
  } yield spark.range(1000)

  val t0 = System.currentTimeMillis()
  val allDF = dataframes.reduceLeft(_.union(_))
  val t1 = System.currentTimeMillis()
  val totalTime = t1 - t0
  println(s""$totalTime miliseconds"")
  totalTime
}

scala> testUnion(100)
193 miliseconds
res5: Long = 193

scala> testUnion(200)
759 miliseconds
res1: Long = 759

scala> testUnion(500)
4438 miliseconds
res2: Long = 4438

scala> testUnion(1000)
18441 miliseconds
res6: Long = 18441

scala> testUnion(2000)
88498 miliseconds
res7: Long = 88498

scala> testUnion(5000)
822305 miliseconds
res8: Long = 822305






--"
Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Thu, 29 Dec 2016 18:38:12 +0100",Re: repeated unioning of dataframes take worse than O(N^2) time,dev@spark.apache.org,"Iterative union like this creates a deeply nested recursive structure in
a similar manner to described here http://stackoverflow.com/q/34461804

You can try something like this http://stackoverflow.com/a/37612978 but
there is of course on overhead of conversion between Dataset and RDD.



-- 
Maciej Szymkiewicz

"
Ryan Williams <ryan.blake.williams@gmail.com>,"Thu, 29 Dec 2016 17:47:08 +0000",Re: shapeless in spark 2.1.0,"=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@databricks.com>, 
	Koert Kuipers <koert@tresata.com>","`mvn dependency:tree -Dverbose -Dincludes=:shapeless_2.11` shows:

[INFO] \- org.apache.spark:spark-mllib_2.11:jar:2.1.0:provided
[INFO]    \- org.scalanlp:breeze_2.11:jar:0.12:provided
[INFO]       \- com.chuusai:shapeless_2.11:jar:2.0.0:provided


d
"
Sean Owen <sowen@cloudera.com>,"Thu, 29 Dec 2016 18:05:39 +0000",Re: shapeless in spark 2.1.0,"Koert Kuipers <koert@tresata.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","It is breeze, but, what's the option? It can't be excluded. I think this
falls in the category of things an app would need to shade in this
situation.


"
Sean Owen <sowen@cloudera.com>,"Thu, 29 Dec 2016 18:06:25 +0000",Re: repeated unioning of dataframes take worse than O(N^2) time,"""assaf.mendelson"" <assaf.mendelson@rsa.com>, dev@spark.apache.org","Don't do that. Union them all at once with SparkContext.union


"
Ryan Williams <ryan.blake.williams@gmail.com>,"Thu, 29 Dec 2016 18:13:15 +0000",Re: shapeless in spark 2.1.0,"Sean Owen <sowen@cloudera.com>, Koert Kuipers <koert@tresata.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","Other option would presumably be for someone to make a release of breeze
with old-shapeless shaded... unless shapeless classes are exposed in
breeze's public API, in which case you'd have to copy the relevant
shapeless classes into breeze and then publish that?


"
Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Thu, 29 Dec 2016 19:17:58 +0100",Re: shapeless in spark 2.1.0,dev@spark.apache.org,"Breeze 0.13 (RC-1 right now) bumps shapeless to 2.2.0 and 2.2.5 for
Scala 2.10 and 2.11 respectively:

https://github.com/scalanlp/breeze/pull/509


-- 
Maciej Szymkiewicz

"
Jacek Laskowski <jacek@japila.pl>,"Thu, 29 Dec 2016 19:58:55 +0100",Re: [ANNOUNCE] Announcing Apache Spark 2.1.0,Yin Huai <yhuai@databricks.com>,"Hi Yan,

I've been surprised the first time when I noticed rxin stepped back and a
new release manager stepped in. Congrats on your first ANNOUNCE!

I can only expect even more great stuff coming in to Spark from the dev
team after Reynold spared some time üòâ

Can't wait to read the changes...

Jacek


de.html#handling-late-data-and-watermarking>
on.html>.
h,
"
Yin Huai <yhuai@databricks.com>,"Thu, 29 Dec 2016 11:17:08 -0800",Re: [ANNOUNCE] Announcing Apache Spark 2.1.0,Jacek Laskowski <jacek@japila.pl>,"Hello Jacek,

Actually, Reynold is still the release manager and I am just sending this
message for him :) Sorry. I should have made it clear in my original email.

Thanks,

Yin


ide.html#handling-late-data-and-watermarking>
ion.html>.
sh,
d
e
d
"
"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Thu, 29 Dec 2016 12:19:32 -0700 (MST)",RE: repeated unioning of dataframes take worse than O(N^2) time,dev@spark.apache.org,"Hi,

I understand that doing a union creates a nested structures, however why isn‚Äôt it O(N)? If I look at the code it seems this should be a tree merge of two plans, that should occur at O(N) not O(N^2).
Even when running the plan that should be O(N*LOG(N)) instead of O(N^2) or worse.
Assaf.

From: Maciej Szymkiewicz [via Apache Spark Developers List] [mailto:ml-node+s1001551n20395h34@n3.nabble.com]
Sent: Thursday, December 29, 2016 7:39 PM
To: Mendelson, Assaf
Subject: Re: repeated unioning of dataframes take worse than O(N^2) time


Iterative union like this creates a deeply nested recursive structure in a similar manner to described here http://stackoverflow.com/q/34461804

You can try something like this http://stackoverflow.com/a/37612978 but there is of course on overhead of conversion between Dataset and RDD.


Hi,

I have been playing around with doing union between a large number of dataframes and saw that the performance of the actual union (not the action) is worse than O(N^2). Since a union basically defines a lineage (i.e. current + union with of other as a child) this should be almost instantaneous, however in practice this can be very costly.

I was wondering why this is and if there is a way to fix this.

A sample test:
def testUnion(n: Int): Long = {
  val dataframes = for {
    x <- 0 until n
  } yield spark.range(1000)

  val t0 = System.currentTimeMillis()
  val allDF = dataframes.reduceLeft(_.union(_))
  val t1 = System.currentTimeMillis()
  val totalTime = t1 - t0
  println(s""$totalTime miliseconds"")
  totalTime
}

scala> testUnion(100)
193 miliseconds
res5: Long = 193

scala> testUnion(200)
759 miliseconds
res1: Long = 759

scala> testUnion(500)
4438 miliseconds
res2: Long = 4438

scala> testUnion(1000)
18441 miliseconds
res6: Long = 18441

scala> testUnion(2000)
88498 miliseconds
res7: Long = 88498

scala> testUnion(5000)
822305 miliseconds
res8: Long = 822305



________________________________
View this message in context: repeated unioning of dataframes take worse than O(N^2) time<http://apache-spark-developers-list.1001551.n3.nabble.com/repeated-unioning-of-dataframes-take-worse-than-O-N-2-time-tp20394.html>
he-spark-developers-list.1001551.n3.nabble.com/> at Nabble.com.



--

Maciej Szymkiewicz

________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/repeated-unioning-of-dataframes-take-worse-than-O-N-2-time-tp20394p20395.html
To start a new topic under Apache Spark Developers List, email ml-node+s1001551n1h20@n3.nabble.com<mailto:ml-node+s1001551n1h20@n3.nabble.com>
spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=1&code=YXNzYWYubWVuZGVsc29uQHJzYS5jb218MXwtMTI4OTkxNTg1Mg==>.
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>




--
3.nabble.com/repeated-unioning-of-dataframes-take-worse-than-O-N-2-time-tp20394p20403.html
om."
Koert Kuipers <koert@tresata.com>,"Thu, 29 Dec 2016 16:14:19 -0500",Re: shapeless in spark 2.1.0,Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"we also use spray for webservices that execute on spark, and spray depends
on even older (and incompatible) shapeless 1.x
to get rid of the old shapeless i would have to upgrade from spray to
akka-http, which means going to java 8

this might also affect spark-job-server, which it seems uses spray.


"
Fei Hu <hufei68@gmail.com>,"Thu, 29 Dec 2016 23:06:39 -0500",RDD Location,"user@spark.apache.org, ""user@hadoop.apache.org"" <user@hadoop.apache.org>, dev@spark.apache.org","Dear all,

Is there any way to change the host location for a certain partition of RDD?

""protected def getPreferredLocations(split: Partition)"" can be used to
initialize the location, but how to change it after the initialization?


Thanks,
Fei
"
Ilya Matiach <ilmat@microsoft.com>,"Fri, 30 Dec 2016 04:36:55 +0000",mllib metrics vs ml evaluators and how to improve apis for users,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi ML/MLLib developers,
1.    I'm trying to add a weights column to ml spark evaluators (RegressionEvaluator, BinaryClassificationEvaluator, MutliclassClassificationEvaluator) that use mllib metrics and I have a few questions (JIRA
2.    SPARK-18693<https://issues.apache.org/jira/browse/SPARK-18693>).  I didn't see any similar question on the forums or stackoverflow.
Moving forward, will we keep mllib metrics (RegressionMetrics, MulticlassMetrics, BinaryClassificationMetrics) as something separate to the evaluators, or will we remove them when mllib is removed in spark 3.0?
The mllib metrics seem very useful because they are able to compute/expose many metrics on one dataset, whereas with the evaluators it is not performant to re-evaluate the entire dataset for a single different metric.
For example, if I calculate the RMSE and then MSE using the ML RegressionEvaluator, I will be redoing most of the work twice, so the ML api doesn't make sense to use in this scenario.
Also, the ml evaluators expose a lot fewer metrics than the mllib metrics classes, so it seems like the ml evaluators are not at parity with the mllib metrics classes.
I can see how the ml evaluators are useful in CrossValidator, but for exploring all metrics from a scored dataset it doesn't really make sense to use them.
ean that the mllib metrics classes should be moved to ml?
That would solve my issue if that is what is planned in the future.  However, that doesn't make sense to me, because it may cause some confusion for ml users to see metrics and evaluators classes.

Instead, it seems like the ml evaluators need to be changed at the api layer to:

  1.  Allow the user to either retrieve a single value
  2.  Allow the user to retrieve all metrics or a set of metrics
ng like:

override def evaluate(dataset: Dataset[_]): Double
override def evaluate(dataset: Dataset[_], metrics:Array[String]): Dataset[_]

But for some metrics like confusion matrix you couldn't really fit the data into the result of the second api in addition to the single-value metrics.
The format of the mllib metrics classes was much more convenient, as you could retrieve them directly.
Following this line of thought, maybe the APIs could be:

override def evaluate(dataset: Dataset[_]): Double
def evaluateMetrics(dataset: Dataset[_]): RegressionEvaluation (or classification/multiclass etc)

where the evaluation class returned will have very similar fields to the corresponding mllib RegressionMetrics class that can be called by the user.

Any thoughts/ideas about spark ml evaluators/mllib metrics apis, coding suggestions for the api proposed, or a general roadmap would be really appreciated.

Thank you, Ilya
"
Sun Rui <sunrise_win@163.com>,"Fri, 30 Dec 2016 15:38:00 +0800",Re: RDD Location,Fei Hu <hufei68@gmail.com>,"Maybe you can create your own subclass of RDD and override the getPreferredLocations() to implement the logic of dynamic changing of the locations.
of RDD?
initialize the location, but how to change it after the initialization?



---------------------------------------------------------------------


"
Liang-Chi Hsieh <viirya@gmail.com>,"Fri, 30 Dec 2016 00:59:50 -0700 (MST)",Re: repeated unioning of dataframes take worse than O(N^2) time,dev@spark.apache.org,"
Actually, as you use Dataset's union API, unlike RDD's union API, it will
break the nested structure. So that should not be the issue.

The additional time introduced when the number of dataframes grows, is spent
on analysis stage. I can think that as the Union has a long children list,
the analyzer needs more time to traverse the tree.

When the dataset of Union(Range1, Range2) is created, the Analyzer needs to
go through 2 Range(s). When the next union happens, i.e., Union(Range1,
Range2, Range3), the Analyzer needs to go through 3 Range(s), except for the
first 2 Range(s). The two Range plans are overlapped. But the Analyzer still
goes through them.

If there is an Union with 5 Range logical plans, the Analyzer goes through:

2 + 3 + 4 + 5 = 14 Range(s) under the Union

When you increase the Range plans to 10. It becomes:

2 + 3 + 4 + 5 + ... + 10 = 54 Range(s)

So if an Union of 100 Range plans, there are 5049 Range(s) needed to go
through. For 200 Range plans, it becomes 20099.

You can see it is not linear relation.





-----
Liang-Chi Hsieh | @viirya 
Spark Technology Center 
http://www.spark.tc/ 
--

---------------------------------------------------------------------


"
Brian Cajes <brian.cajes@gmail.com>,"Fri, 30 Dec 2016 18:27:13 -0500",[ML] [GraphFrames] : Bayesian Network framework,spark-dev <dev@spark.apache.org>,"Hi, I'm interested in using (or contributing to an implementation) of a
Bayesian Network framework within Spark.  Similar to
https://github.com/jmschrei/pomegranate/blob/master/examples/bayesnet_monty_hall_train.ipynb
.  I've found a related library for spark:
https://github.com/HewlettPackard/sandpiper , but it's not quite what I'm
looking for.  It would be nice if this framework integrated with ML or
GraphFrames.  Anyone know of any other Bayesian Network frameworks using
Spark?  If not, would this sort of framework be a worthwhile addition to
ml, graphframes or spark-packages?
"
Fei Hu <hufei68@gmail.com>,"Fri, 30 Dec 2016 20:48:23 -0500",Re: RDD Location,Sun Rui <sunrise_win@163.com>,"That is a good idea.

I tried add the following code to get getPreferredLocations() function:

val results: Array[Array[DataChunkPartition]] = context.runJob(
      partitionsRDD, (context: TaskContext, partIter:
Iterator[DataChunkPartition]) => partIter.toArray, dd, allowLocal = true)

But it seems to be suspended when executing this function. But if I move
the code to other places, like the main() function, it runs well.

What is the reason for it?

Thanks,
Fei


"
Fei Hu <hufei68@gmail.com>,"Fri, 30 Dec 2016 20:55:10 -0500",context.runJob() was suspended in getPreferredLocations() function,"user@spark.apache.org, ""user@hadoop.apache.org"" <user@hadoop.apache.org>, dev@spark.apache.org","Dear all,

I tried to customize my own RDD. In the getPreferredLocations() function, I
used the following code to query anonter RDD, which was used as an input to
initialize this customized RDD:

                   * val results: Array[Array[DataChunkPartition]] =
context.runJob(partitionsRDD, (context: TaskContext, partIter:
Iterator[DataChunkPartition]) => partIter.toArray, partitions, allowLocal =
true)*

The problem is that when executing the above code, the task seemed to be
suspended. I mean the job just stopped at this code, but no errors and no
outputs.

What is the reason for it?

Thanks,
Fei
"
Sun Rui <sunrise_win@163.com>,"Sat, 31 Dec 2016 11:41:59 +0800",Re: RDD Location,Fei Hu <hufei68@gmail.com>,"You can‚Äôt call runJob inside getPreferredLocations().
You can take a look at the source  code of HadoopRDD to help you implement getPreferredLocations() appropriately.
function:
Iterator[DataChunkPartition]) => partIter.toArray, dd, allowLocal = true)
move the code to other places, like the main() function, it runs well.
getPreferredLocations() to implement the logic of dynamic changing of the locations.
of RDD?
to initialize the location, but how to change it after the initialization?

"
Fei Hu <hufei68@gmail.com>,"Sat, 31 Dec 2016 01:12:58 -0500",Re: RDD Location,Sun Rui <sunrise_win@163.com>,"It will be very appreciated if you can give more details about why runJob
function could not be called in getPreferredLocations()

In the NewHadoopRDD class and HadoopRDD class, they get the location
information from the inputSplit. But there may be an issue in NewHadoopRDD,
because it generates all of the inputSplits on the master node, which means
I can only use a single node to generate and filter the inputSplits even if
the number of inputSplits is huge. Will it be a performance bottleneck?

Thanks,
Fei






t getPreferredLocations()
rue)
e
f
"
Felix Cheung <felixcheung_m@hotmail.com>,"Sat, 31 Dec 2016 06:29:41 +0000",Re: ml word2vec finSynonyms return type,"Asher Krim <akrim@hubspot.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","Could you link to the JIRA here?

What you suggest makes sense to me. Though we might want to maintain compatibility and add a new method instead of changing the return type of the existing one.


_____________________________
From: Asher Krim <akrim@hubspot.com<mailto:akrim@hubspot.com>>
Sent: Wednesday, December 28, 2016 11:52 AM
Subject: ml word2vec finSynonyms return type
To: <dev@spark.apache.org<mailto:dev@spark.apache.org>>
Cc: <manojkumarsivaraj334@gmail.com<mailto:manojkumarsivaraj334@gmail.com>>, Joseph Bradley <joseph@databricks.com<mailto:joseph@databricks.com>>


Hey all,

I would like to propose changing the return type of `findSynonyms` in ml's Word2Vec<https://github.com/apache/spark/blob/branch-2.1/mllib/src/main/scala/org/apache/spark/ml/feature/Word2Vec.scala#L233-L248>:

def findSynonyms(word: String, num: Int): DataFrame = {
  val spark = SparkSession.builder().getOrCreate()
  spark.createDataFrame(wordVectors.findSynonyms(word, num)).toDF(""word"", ""similarity"")
}


I find it very strange that the results are parallelized before being returned to the user. The results are already on the driver to begin with, and I can imagine that for most usecases (and definitely for ours) the synonyms are collected right back to the driver. This incurs both an added cost of shipping data to and from the cluster, as well as a more cumbersome interface than needed.

Can we change it to just the following?

def findSynonyms(word: String, num: Int): Array[(String, Double)] = {
  wordVectors.findSynonyms(word, num)
}

If the user wants the results parallelized, they can still do so on their own.

(I had brought this up a while back in Jira. It was suggested that the mailing list would be a better forum to discuss it, so here we are.)

Thanks,
--
Asher Krim
Senior Software Engineer
[http://cdn2.hubspot.net/hub/137828/file-223457316-png/HubSpot_User_Group_Images/HUG_lrg_HS.png?t=1477096082917]


"
Felix Cheung <felixcheung_m@hotmail.com>,"Sat, 31 Dec 2016 06:39:33 +0000",Re: [ML] [GraphFrames] : Bayesian Network framework,"Brian Cajes <brian.cajes@gmail.com>, spark-dev <dev@spark.apache.org>","GraphFrames has a Belief Propagation example
Have you checked it out?

graphframes.github.io/api/scala/index.html#org.graphframes.examples.BeliefPropagation$<http://graphframes.github.io/api/scala/index.html#org.graphframes.examples.BeliefPropagation$>


________________________________
From: Brian Cajes <brian.cajes@gmail.com>
Sent: Friday, December 30, 2016 3:27:13 PM
To: spark-dev
Subject: [ML] [GraphFrames] : Bayesian Network framework

Hi, I'm interested in using (or contributing to an implementation) of a Bayesian Network framework within Spark.  Similar to https://github.com/jmschrei/pomegranate/blob/master/examples/bayesnet_monty_hall_train.ipynb .  I've found a related library for spark: https://github.com/HewlettPackard/sandpiper , but it's not quite what I'm looking for.  It would be nice if this framework integrated with ML or GraphFrames.  Anyone know of any other Bayesian Network frameworks using Spark?  If not, would this sort of framework be a worthwhile addition to ml, graphframes or spark-packages?
"
