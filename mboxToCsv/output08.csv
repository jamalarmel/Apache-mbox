Henry Saputra <henry.saputra@gmail.com>,"Tue, 31 Dec 2013 16:46:05 -0800",Re: Incubator PMC/Board report for Jan 2014 ([ppmc]),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>, 
	""private@spark.incubator.apache.org"" <private@spark.incubator.apache.org>","Any volunteer to write up the report for Spark?

- Henry


"
Patrick Wendell <pwendell@gmail.com>,"Tue, 31 Dec 2013 17:05:27 -0800",Disallowing null mergeCombiners,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey All,

There is a small API change that we are considering for the external
sort patch. Previously we allowed mergeCombiner to be null when map
side aggregation was not enabled. This is because it wasn't necessary
in that case since mappers didn't ship pre-aggregated values to
reducers.

Because the external sort capability also relies on the mergeCombiner
function to merge partially-aggregated on-disk segments, we now need
it all the time, even if map side aggregation is enabled. This is a
fairly esoteric thing that I'm not sure anyone other than Shark ever
used, but I want to check in case anyone had feelings about this.

The relevant code is here:

https://github.com/apache/incubator-spark/pull/303/files#diff-f70e97c099b5eac05c75288cb215e080R72

- Patrick

"
Reynold Xin <rxin@databricks.com>,"Tue, 31 Dec 2013 17:09:36 -0800",Re: Disallowing null mergeCombiners,dev@spark.incubator.apache.org,"I added the option that doesn't require the caller to specify the
mergeCombiner closure a while ago when I wanted to disable mapSideCombine.
In virtually all use cases I know of, it is fine & easy to specify a
mergeCombiner, so I'm all for this given it simplifies the codebase.



"
Matei Zaharia <matei.zaharia@gmail.com>,"Wed, 1 Jan 2014 01:08:36 -0500",Re: Incubator PMC/Board report for Jan 2014 ([ppmc]),private@spark.incubator.apache.org,"Yup, Iíll write it up soon.

The main thing missing for us is unfortunately still the JIRA import ó letís see if we can get some help with that, since I think we have at least one version that is importable. If you know anyone who can do this please point them to https://issues.apache.org/jira/browse/INFRA-6419.

Matei


Incubator PMC.
quarterly
PST. The report
Incubator PMC
meeting, to allow
PMC, and
latest you
the project
towards
be aware of
this page is
on the
the
Incubator PMC.


"
Henry Saputra <henry.saputra@gmail.com>,"Tue, 31 Dec 2013 23:51:41 -0800",Re: Incubator PMC/Board report for Jan 2014 ([ppmc]),"""private@spark.incubator.apache.org"" <private@spark.incubator.apache.org>","Thanks Matei, will try to solicit help for our JIRA import.

Oh and happy new year to Spark dev community :)

- Henry


Äî
e at least
cript:;>>
:;>>
t
n
"
Marvin <no-reply@apache.org>,"Wed,  1 Jan 2014 14:15:02 +0000 (UTC)",Incubator PMC/Board report for Jan 2014 ([ppmc]),dev@spark.incubator.apache.org,"

Dear podling,

This email was sent by an automated system on behalf of the Apache Incubator PMC.
It is an initial reminder to give you plenty of time to prepare your quarterly
board report.

The board meeting is scheduled for Wed, 15 January 2014, 10:30:30:00 PST. The report 
for your podling will form a part of the Incubator PMC report. The Incubator PMC 
requires your report to be submitted 2 weeks before the board meeting, to allow 
sufficient time for review and submission (Wed, Jan 1st).

Please submit your report with sufficient time to allow the incubator PMC, and 
subsequently board members to review and digest. Again, the very latest you 
should submit your report is 2 weeks prior to the board meeting.

Thanks,

The Apache Incubator PMC

Submitting your Report
----------------------

Your report should contain the following:

 * Your project name
 * A brief description of your project, which assumes no knowledge of the project
   or necessarily of its field
 * A list of the three most important issues to address in the move towards 
   graduation.
 * Any issues that the Incubator PMC or ASF Board might wish/need to be aware of
 * How has the community developed since the last report
 * How has the project developed since the last report.
 
This should be appended to the Incubator Wiki page at:

  http://wiki.apache.org/incubator/January2014

Note: This manually populated. You may need to wait a little before this page is
      created from a template.

Mentors
-------
Mentors should review reports for their project(s) and sign them off on the 
Incubator wiki page. Signing off reports shows that you are following the 
project - projects that are not signed may raise alarms for the Incubator PMC.

Incubator PMC


"
Matei Zaharia <matei.zaharia@gmail.com>,"Wed, 1 Jan 2014 12:39:49 -0500",Re: Incubator PMC/Board report for Jan 2014 ([ppmc]),dev@spark.incubator.apache.org,"Alright, Iíve posted an update at https://wiki.apache.org/incubator/January2014.

If we canít get the JIRA imported soon, my suggestion would be just to skip importing the old issues and start the ones here at a higher issue number. People will still be able to find the old ones through Google, and we can configure the old JIRA to send emails to our mailing list here if thatís a concern. To me the most important thing is to finalize our infrastructure so that we avoid confusing developers and users, and the sooner we do this the better, as it will get more and more disruptive over time.

Matei


ó
at least
please
<henry.saputra@gmail.com<javascript:;>>
<no-reply@apache.org<javascript:;>>
your
10:30:30:00
meeting,
incubator
latest
of
be
off on
following


"
Henry Saputra <henry.saputra@gmail.com>,"Wed, 1 Jan 2014 13:04:21 -0800",Re: Incubator PMC/Board report for Jan 2014 ([ppmc]),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","+1 for the report, thanks Matei. Will sign off if by EOD if no more comments.

As for JIRA import, yeah the worst case we could just skip the import
and do manual ""import"" for some important issues from amplab JIRA.
That what we did in MetaModel podling. "
Debasish Das <debasish.das83@gmail.com>,"Thu, 2 Jan 2014 15:16:33 -0800",Spark Matrix Factorization,"user@spark.incubator.apache.org, dev@spark.incubator.apache.org","Hi,

I am not noticing any DSGD implementation of ALS in Spark.

There are two ALS implementations.

org.apache.spark.examples.SparkALS does not run on large matrices and seems
more like a demo code.

org.apache.spark.mllib.recommendation.ALS looks feels more robust version
and I am experimenting with it.

References here are Jellyfish, Twitter's implementation of Jellyfish called
Scalafish, Google paper called Sparkler and similar idea put forward by IBM
paper by Gemulla et al. (large-scale matrix factorization with distributed
stochastic gradient descent)

https://github.com/azymnis/scalafish

Are there any plans of adding DSGD in Spark or there are any existing JIRA ?

Thanks.
Deb
"
Ameet Talwalkar <ameet@eecs.berkeley.edu>,"Thu, 2 Jan 2014 16:06:57 -0800",Re: Spark Matrix Factorization,user@spark.incubator.apache.org,"Hi Deb,

Thanks for your email.  We currently do not have a DSGD implementation in
MLlib. Also, just to clarify, DSGD is not a variant of ALS, but rather a
different algorithm for solving the same the same bi-convex objective
function.

It would be a good thing to do add, but to the best of my knowledge, no one
is actively working on this right now.

Also, as you mentioned, the ALS implementation in mllib is more
robust/scalable than the one in spark.examples.

-Ameet



"
Ted Yu <yuzhihong@gmail.com>,"Thu, 2 Jan 2014 21:06:59 -0800",compiling against hadoop 2.2,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi,
I used the following command to compile against hadoop 2.2:
mvn clean package -DskipTests -Pnew-yarn

But I got a lot of compilation errors.

Did I use the wrong command ?

Cheers
"
"""Liu, Raymond"" <raymond.liu@intel.com>","Fri, 3 Jan 2014 06:08:57 +0000",RE: compiling against hadoop 2.2,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I think you also need to set yarn.version

Say something like 

mvn -Pyarn -Dhadoop.version=2.2.0 -Dyarn.version=2.2.0 -DskipTests clean package


hadoop.version is default to 2.2.0 while yarn.version not when you chose the new-yarn profile. We probably need to fix it later for easy usage.



Best Regards,
Raymond Liu

I used the following command to compile against hadoop 2.2:
mvn clean package -DskipTests -Pnew-yarn

But I got a lot of compilation errors.

Did I use the wrong command ?

Cheers

"
"""Liu, Raymond"" <raymond.liu@intel.com>","Fri, 3 Jan 2014 06:10:09 +0000",RE: compiling against hadoop 2.2,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Sorry , mvn -Pnew-yarn -Dhadoop.version=2.2.0 -Dyarn.version=2.2.0 -DskipTests clean package

The one in previous mail not yet available.


Best Regards,
Raymond Liu



Say something like 

mvn -Pyarn -Dhadoop.version=2.2.0 -Dyarn.version=2.2.0 -DskipTests clean package


hadoop.version is default to 2.2.0 while yarn.version not when you chose the new-yarn profile. We probably need to fix it later for easy usage.



Best Regards,
Raymond Liu

I used the following command to compile against hadoop 2.2:
mvn clean package -DskipTests -Pnew-yarn

But I got a lot of compilation errors.

Did I use the wrong command ?

Cheers

"
Ted Yu <yuzhihong@gmail.com>,"Thu, 2 Jan 2014 22:20:56 -0800",Re: compiling against hadoop 2.2,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Specification of yarn.version can be inserted following this line (#762 in
pom.xml), right ?
         <hadoop.version>2.2.0</hadoop.version>



"
"""Liu, Raymond"" <raymond.liu@intel.com>","Fri, 3 Jan 2014 06:28:52 +0000",RE: compiling against hadoop 2.2,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Yep, you are right. While we will merge in new code pretty soon ( maybe today? I hope so) on this part. Might shift a few lines

Best Regards,
Raymond Liu

pom.xml), right ?
         <hadoop.version>2.2.0</hadoop.version>


:

ge.

"
"""Liu, Raymond"" <raymond.liu@intel.com>","Fri, 3 Jan 2014 06:34:09 +0000",RE: compiling against hadoop 2.2,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","And I am not sure where it is value able to providing different setting for hadoop/hdfs and yarn version. When build with SBT, they will always be the same. Maybe in mvn we should do so too. 

Best Regards,
Raymond Liu


ay? I hope so) on this part. Might shift a few lines

Best Regards,
Raymond Liu

pom.xml), right ?
         <hadoop.version>2.2.0</hadoop.version>


:

ge.

"
Andrew Ash <andrew@andrewash.com>,"Fri, 3 Jan 2014 01:39:33 -0500","Terminology: ""worker"" vs ""slave""",dev@spark.incubator.apache.org,"The terms worker and slave seem to be used interchangeably.  Are they the
same?

Worker is used more frequently in the codebase:

aash@aash-mbp ~/git/spark$ git grep -i worker | wc -l
     981
aash@aash-mbp ~/git/spark$ git grep -i slave | wc -l
     348
aash@aash-mbp ~/git/spark$

Does it make sense to unify on one or the other?
"
Reynold Xin <rxin@databricks.com>,"Thu, 2 Jan 2014 22:42:45 -0800","Re: Terminology: ""worker"" vs ""slave""",dev@spark.incubator.apache.org,"It is historic.

I think we are converging towards

worker: the ""slave"" daemon in the standalone cluster manager

executor: the jvm process that is launched by the worker that executes tasks




"
Patrick Wendell <pwendell@gmail.com>,"Thu, 2 Jan 2014 22:45:08 -0800","Re: Terminology: ""worker"" vs ""slave""","""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Ya we've been trying to standardize on the terminology here (see glossary):

http://spark.incubator.apache.org/docs/latest/cluster-overview.html

I think ""slave"" actually isn't mentioned here at all - but references
to slave in the codebase are synonymous with ""worker"".

- Patrick


"
Andrew Ash <andrew@andrewash.com>,"Fri, 3 Jan 2014 01:46:37 -0500","Re: Terminology: ""worker"" vs ""slave""",dev@spark.incubator.apache.org,"That matches my mental model -- glad to hear we're converging on one though
for consistency.

Thanks guys!



"
Kai Backman <kai@airstonelabs.com>,"Thu, 2 Jan 2014 23:03:52 -0800",Making core Spark trun on non-IP network stack,dev@spark.incubator.apache.org,"dev,

I would be interesting in understanding how to make core Spark run on a non
IP network stack like MPI. The main dependencies seem to be
in org.apache.spark.network but I also see some other dependencies
sprinkled in auxiliary functions.

Pointers to code, mailing list discussions or people to talk to appreciated.

Take care,

  Kai

-- 
Kai Backman, CEO
http://airstonelabs.com
"
Ted Yu <yuzhihong@gmail.com>,"Fri, 3 Jan 2014 10:20:34 -0800",Re: compiling against hadoop 2.2,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I think yarn.version should be specified in pom.xml along with
hadoop.version

Cheers



"
Debasish Das <debasish.das83@gmail.com>,"Fri, 3 Jan 2014 10:59:00 -0800",Re: Spark Matrix Factorization,user@spark.incubator.apache.org,"Hi Ameet,

Matrix factorization is a non-convex problem and ALS solves it using 2
convex problems, DSGD solves the problem by finding a local minima.

I am experimenting with Spark Parallel ALS but I intend to port Scalafish
https://github.com/azymnis/scalafish to Spark as well.

For bigger matrices jury is not out that which algorithms provides a better
local optima with an iteration bound. It is also highly dependent on
datasets I believe.

Thanks.
Deb




"
Ameet Talwalkar <ameet@eecs.berkeley.edu>,"Fri, 3 Jan 2014 11:03:56 -0800",Re: Spark Matrix Factorization,user@spark.incubator.apache.org,"ALS and SGD solve the same non-convex objective function, and thus both
yield local minima.  The following reference provides a nice overview (in
particular see equation 2 of this paper):

http://www2.research.att.com/~volinsky/papers/ieeecomputer.pdf


"
Matei Zaharia <matei.zaharia@gmail.com>,"Fri, 3 Jan 2014 14:15:59 -0500",Re: Making core Spark trun on non-IP network stack,dev@spark.incubator.apache.org,"I think this would be a significant undertaking ó my suggestion would be to make the network emulate IP (I believe that option exists for many networks) and then only optimize individual packages where it makes sense. Lots of the communication is just ìcontrol planeî (small messages) and only the block manager does more expensive transfers.

Matei


a non
appreciated.


"
Tathagata Das <tathagata.das1565@gmail.com>,"Fri, 3 Jan 2014 11:15:31 -0800",Re: Making core Spark trun on non-IP network stack,dev@spark.incubator.apache.org,"There are two main uses of network in Spark (ignoring Spark Streaming)
through
1. spark.network stuff for bulk data transfer
2. Akka <http://akka.io/> actor library for control plane messaging

Besides porting the spark.network, you will also have to port Akka to run
on your stack. You will find most of the control layer class like
DAGScheduler, BlockManager, etc uses actor to communicate as well as to
process events in a single-threaded fashion. For example, the
BlockManagerMaster (driver) and the BlockManager (worker) communicate
(control messages only) using the BlockManagerMasterActor and
BlockManagerSlaveActor, respectively.

A good place to start would be to first read up on Akka from online
docs<http://akka.io/docs/> and
look at the code to see how we use it.

TD



"
Patrick Wendell <pwendell@gmail.com>,"Fri, 3 Jan 2014 11:27:23 -0800",Changes that affect packaging and running Spark,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey All,

Today we merged a pull request which changes the organization of the
scripts use to run Spark. The change is fairly minor but it will
affect anyone who packages spark or uses spark's binary scripts.
Spark's scripts are now divided into a /bin and /sbin directories.
This will be the layout in Spark 0.9.

/bin contains user-facing scripts:
/bin/spark-shell
/bin/pyspark
/bin/spark-class
/bin/run-example

/sbin contains administrative scripts for launching the standalone
cluster manager:
/bin/start-master.sh
/bin/start-all.sh
...etc

This allows administrators to set permissions differently for user and
admin scripts. It is also consistent with the way many other projects
package binaries. This change was contributed by @xiajunluan,
@shane-huang, and @ScrapCodes.

https://github.com/apache/incubator-spark/pull/317

Cheers,
- Patrick

"
Patrick Wendell <pwendell@gmail.com>,"Fri, 3 Jan 2014 11:29:44 -0800",Re: Changes that affect packaging and running Spark,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","--> Small correction


"
Roman Shaposhnik <roman@shaposhnik.org>,"Fri, 3 Jan 2014 13:39:15 -0800",Re: Incubator PMC/Board report for Jan 2014 ([ppmc]),private@spark.incubator.apache.org,"Great report!

Thanks,
Roman.

te:
nts.
rote:
nuary2014.
 skip importing the old issues and start the ones here at a higher issue number. People will still be able to find the old ones through Google, and we can configure the old JIRA to send emails to our mailing list here if thatís a concern. To me the most important thing is to finalize our infrastructure so that we avoid confusing developers and users, and the sooner we do this the better, as it will get more and more disruptive over time.
e:
ó
t least
ase
vascript:;>>
pt:;>>
g,
r
est
be
 on
g

"
Ted Yu <yuzhihong@gmail.com>,"Fri, 3 Jan 2014 14:33:20 -0800",classes for YARN 2.2,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi,
There're two copies of classes w.r.t. YARN:

-rw-r--r--  1 tyu  staff  9710 Jan  3 10:14
/Users/tyu/spark//yarn/alpha/src/main/scala/org/apache/spark/deploy/yarn/WorkerLauncher.scala
-rw-r--r--  1 tyu  staff  8189 Jan  3 10:14
/Users/tyu/spark//yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/WorkerLauncher.scala

The class under yarn/stable is for YARN 2.2, right ?

Cheers
"
Patrick Wendell <pwendell@gmail.com>,"Fri, 3 Jan 2014 14:34:47 -0800",Re: classes for YARN 2.2,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Yes


"
Patrick Wendell <pwendell@gmail.com>,"Fri, 3 Jan 2014 14:39:05 -0800",Re: classes for YARN 2.2,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Just to give a bit more color - these exist because the yarn API
changed and we'd like to support both for a while. Rather than
introduce shims we just factored out the common code and maintain
isolated builds for either one.

- Patrick


"
Ted Yu <yuzhihong@gmail.com>,"Fri, 3 Jan 2014 15:14:27 -0800",Re: classes for YARN 2.2,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","In yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala
, at line 86:

    // Workaround until hadoop moves to something which has
    // https://issues.apache.org/jira/browse/HADOOP-8406 - fixed in
(2.0.2-alpha but no 0.23 line)
    //
org.apache.hadoop.io.compress.CompressionCodecFactory.getCodecClasses(conf)

Looks like the above comment can be removed since hadoop 2.2 has HADOOP-8406

Cheers



"
Kai Backman <kai@airstonelabs.com>,"Fri, 3 Jan 2014 21:04:25 -0800",Re: Making core Spark trun on non-IP network stack,dev@spark.incubator.apache.org,"Thank you for your replies, this is a good start.

Take care,

  Kai






-- 
Kai Backman, CEO
http://airstonelabs.com
"
Patrick Wendell <pwendell@gmail.com>,"Sat, 4 Jan 2014 00:02:18 -0800",Build Changes for SBT Users,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey All,

Due to an ASF requirement, we recently merged a patch which removes
the sbt jar from the build. This is necessary because we aren't
allowed to distributed binary artifacts with our source packages.

This means that instead of building Spark with ""sbt/sbt XXX"", you'll
need to have sbt yourself and just run ""sbt XXX"" from within the Spark
directory. This is similar to the maven build, where we expect users
already have maven installed.

You can download sbt at http://www.scala-sbt.org/. It's okay to just
download the most recent version of sbt, since sbt knows how to fetch
other versions of itself and will always use the one we specify in our
build file to compile spark.

- Patrick

"
Holden Karau <holden@pigscanfly.ca>,"Sat, 4 Jan 2014 01:48:09 -0800",Re: Build Changes for SBT Users,dev@spark.incubator.apache.org,"Could we ship a shell script which downloads the sbt jar if not present
(like for example https://github.com/holdenk/slashem/blob/master/sbt )?






-- 
Cell : 425-233-8271
"
Andrew Ash <andrew@andrewash.com>,"Sat, 4 Jan 2014 12:34:38 -0500",Re: Build Changes for SBT Users,dev@spark.incubator.apache.org,"+1 on bundling a script similar to that one



"
Patrick Wendell <pwendell@gmail.com>,"Sat, 4 Jan 2014 10:56:44 -0800",Re: Build Changes for SBT Users,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","We thought about this but elected not to do this for a few reasons.

1. Some people build from machines that do not have internet access
for security reasons and retrieve dependency from internal nexus
repositories. So having a build dependency that relies on internet
downloads is not desirable.

2. It's a hard to ensure stability of a particular URL in perpetuity.
This is why maven central and other mirror networks exist. Keep in
mind that we can't change the release code ever once we release it,
and if something changed about the particular URL it could break the
build.

- Patrick


"
Holden Karau <holden@pigscanfly.ca>,"Sat, 4 Jan 2014 13:20:55 -0800",Re: Build Changes for SBT Users,dev@spark.incubator.apache.org,"That makes sense, I think we could structure a script in such a way that it
would overcome these problems though and probably provide a fair a mount of
benefit for people who just want to get started quickly.

The easiest would be to have it use the system sbt if present and then fall
back to downloading the sbt jar. As far as stability of the URL goes we
could solve this by either having it point at a domain we control, or just
with an clear error message indicating it failed to download sbt and the
user needs to install sbt.

If a restructured script in that manner would be useful I could whip up a
pull request :)






-- 
Cell : 425-233-8271
"
Jey Kottalam <jey@cs.berkeley.edu>,"Sat, 4 Jan 2014 13:24:10 -0800",Re: Build Changes for SBT Users,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I'm in full agreement with Holden. We should provide a smooth out of the
box experience while also not getting in the way of those who provide their
own sbt installs.


"
Patrick Wendell <pwendell@gmail.com>,"Sat, 4 Jan 2014 16:13:31 -0800",Re: Build Changes for SBT Users,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey Holden,

That sounds reasonable to me. Where would we get a url we can control
though? Right now the project has web space is at incubator.apache...
but later this will change to a full apache domain. Is there somewhere
in maven central these jars are hosted... that would be the nicest
because things like repo1.maven.org basically never changes.

- Patrick


"
Reynold Xin <rxin@databricks.com>,"Sat, 4 Jan 2014 16:28:37 -0800",Re: Build Changes for SBT Users,dev@spark.incubator.apache.org,"Doesn't Apache do redirection from incubation. to the normal website also?
 By the time that happens, we can also update the URL in the script?



"
Henry Saputra <henry.saputra@gmail.com>,"Sat, 4 Jan 2014 17:39:13 -0800","Re: Terminology: ""worker"" vs ""slave""","""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","+1 for worker =)


"
Xuefeng Wu <benewu@gmail.com>,"Sun, 5 Jan 2014 09:57:21 +0800",Re: Build Changes for SBT Users,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Sound reasonable.  But I think few installed sbt even it is easy to install.  I think can provide this tricky script in online document, user could download this script to install sbt independence. Sound like a yet another brew install sbt?
:)

Yours, Xuefeng Wu Œ‚—©∑Â æ¥…œ

e:

"
Patrick Wendell <pwendell@gmail.com>,"Sat, 4 Jan 2014 19:06:02 -0800",Re: Build Changes for SBT Users,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Reynold the issue is releases are immutable and we expect them to be
downloaded for several years after the release date.

ll.  I think can provide this tricky script in online document, user could download this script to install sbt independence. Sound like a yet another brew install sbt?
:
ote:
t
?
k
r

"
Tathagata Das <tathagata.das1565@gmail.com>,"Sat, 4 Jan 2014 19:13:25 -0800",Re: Build Changes for SBT Users,dev@spark.incubator.apache.org,"Patrick, that is right. All we are trying to ensure is to make a
""best-effort"" attempt to make it smooth for a new user. The script will try
its best to automatically install / download sbt for the user. The fallback
will be that the user will have to install sbt on their own. If the URL
happens to change and our script fails to automatically download, then we
are *no worse* than not providing the script at all.

TD



)?
l
s
t
ch
"
Patrick Wendell <pwendell@gmail.com>,"Sat, 4 Jan 2014 21:07:15 -0800",Re: Build Changes for SBT Users,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I agree TD - I was just saying that Reynold's proposal that we could
update the release post-hoc is unfortunately not possible.

ry
ck
e:
r
t)?
m>
s
ll
rs
st
tch

"
Holden Karau <holden@pigscanfly.ca>,"Sat, 4 Jan 2014 21:11:04 -0800",Re: Build Changes for SBT Users,dev@spark.incubator.apache.org,"So I've put together a pull request which depends on the typesafe URLs
(they have been stable for a long time)
https://github.com/apache/incubator-spark/pull/331 and uses the system sbt
if it is present. How do people feel about this?



we
et
y.
e
a
.
in



-- 
Cell : 425-233-8271
"
q q <monoid123@outlook.com>,"Mon, 6 Jan 2014 03:12:04 +0800",how to set up environment to develop Spark on macbook pro retina?,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hello,
I have a macbook pro retina, wondering how people with similar model set up development environment:
1. if I choose to install a linux VM and set up development env inside the VM, the display is terrible (font is very blurred).
2. If I choose to set up develop environment on Mac OS directly, looks it's not exactly same with linux, for example, some project won't be built successfully if it's not tweaked. 
Have the same question to develop Shark too.
So what's your experience? 
Thanks!
 		 	   		  "
Mark Hamstra <mark@clearstorydata.com>,"Sun, 5 Jan 2014 11:19:49 -0800",Re: how to set up environment to develop Spark on macbook pro retina?,dev@spark.incubator.apache.org,"I don't understand: What are you finding different in developing on OSX vs.
Linux that requires tweaking?



"
q q <monoid123@outlook.com>,"Mon, 6 Jan 2014 07:18:31 +0800","RE: how to set up environment to develop Spark on macbook pro
 retina?","""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","one example of tweaking is when building the project, there're some difference of gcc between OSX (Darwin) and Linux(Ubuntu), so we need check the OS and give different parameters to gcc.
also do you mean Spark and Shark can fully(function wise) run on OSX like Linux?
Thanks. 
> Date: Sun, 5 Jan 2014 11:19:49 -0800
> Subject: Re: how to set up environment to develop Spark on macbook pro retina?
> From: mark@clearstorydata.com
> To: dev@spark.incubator.apache.org
> 
> I don't understand: What are you finding different in developing on OSX vs.
> Linux that requires tweaking?
> 
> 
> On Sun, Jan 5, 2014 at 11:12 AM, q q <monoid123@outlook.com> wrote:
> 
> > Hello,
> > I have a macbook pro retina, wondering how people with similar model set
> > up development environment:
> > 1. if I choose to install a linux VM and set up development env inside the
> > VM, the display is terrible (font is very blurred).
> > 2. If I choose to set up develop environment on Mac OS directly, looks
> > it's not exactly same with linux, for example, some project won't be built
> > successfully if it's not tweaked.
> > Have the same question to develop Shark too.
> > So what's your experience?
> > Thanks!
> >
 		 	   		  "
Nan Zhu <zhunanmcgill@gmail.com>,"Sun, 5 Jan 2014 18:20:47 -0500","Re: how to set up environment to develop Spark on macbook pro
 retina?",dev@spark.incubator.apache.org,"why you need gcc to compile spark? 

-- 
Nan Zhu





"
Mark Hamstra <mark@clearstorydata.com>,"Sun, 5 Jan 2014 15:47:15 -0800",Re: how to set up environment to develop Spark on macbook pro retina?,dev@spark.incubator.apache.org,"You don't need gcc to build Spark, and there is very little difference
between developing and running Spark and Shark on OSX vs. Linux.  In fact,
I dare say that most of the Spark committers develop primarily on MBPs and
run it at different times on their local OSX machine and on physical or
virtual Linux machines.



"
Reynold Xin <rxin@databricks.com>,"Sun, 5 Jan 2014 16:24:38 -0800",Re: Build Changes for SBT Users,dev@spark.incubator.apache.org,"Why is it not possible? You always update the script; just can't update
scripts for released versions.





we
et
y.
e
a
.
in
"
q q <monoid123@outlook.com>,"Mon, 6 Jan 2014 08:25:16 +0800","RE: how to set up environment to develop Spark on macbook pro
 retina?","""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Thanks, yes, I made a mistake, building spark doesn't need gcc.
Another related question is when developing code in either Eclipse or IntelliJ, is there a good way to build it directly in IDE or general practice is use IDE to view and edit code, then use command line (sbt) to build and package, then use log to debug?  


> Date: Sun, 5 Jan 2014 15:47:15 -0800
> Subject: Re: how to set up environment to develop Spark on macbook pro retina?
> From: mark@clearstorydata.com
> To: dev@spark.incubator.apache.org
> 
> You don't need gcc to build Spark, and there is very little difference
> between developing and running Spark and Shark on OSX vs. Linux.  In fact,
> I dare say that most of the Spark committers develop primarily on MBPs and
> run it at different times on their local OSX machine and on physical or
> virtual Linux machines.
> 
> 
> On Sun, Jan 5, 2014 at 3:18 PM, q q <monoid123@outlook.com> wrote:
> 
> > one example of tweaking is when building the project, there're some
> > difference of gcc between OSX (Darwin) and Linux(Ubuntu), so we need check
> > the OS and give different parameters to gcc.
> > also do you mean Spark and Shark can fully(function wise) run on OSX like
> > Linux?
> > Thanks.
> > > Date: Sun, 5 Jan 2014 11:19:49 -0800
> > > Subject: Re: how to set up environment to develop Spark on macbook pro
> > retina?
> > > From: mark@clearstorydata.com
> > > To: dev@spark.incubator.apache.org
> > >
> > > I don't understand: What are you finding different in developing on OSX
> > vs.
> > > Linux that requires tweaking?
> > >
> > >
> > > On Sun, Jan 5, 2014 at 11:12 AM, q q <monoid123@outlook.com> wrote:
> > >
> > > > Hello,
> > > > I have a macbook pro retina, wondering how people with similar model
> > set
> > > > up development environment:
> > > > 1. if I choose to install a linux VM and set up development env inside
> > the
> > > > VM, the display is terrible (font is very blurred).
> > > > 2. If I choose to set up develop environment on Mac OS directly, looks
> > > > it's not exactly same with linux, for example, some project won't be
> > built
> > > > successfully if it's not tweaked.
> > > > Have the same question to develop Shark too.
> > > > So what's your experience?
> > > > Thanks!
> > > >
> >
> >
 		 	   		  "
Mark Hamstra <mark@clearstorydata.com>,"Sun, 5 Jan 2014 17:39:33 -0800",Re: how to set up environment to develop Spark on macbook pro retina?,dev@spark.incubator.apache.org,"Some do work with Spark as much as possible within IntelliJ or Eclipse (or
Sublime), but I'm not one of them.  I work closer to your second option,
with some Emacs and deployment tooling also thrown into the mix.  In short,
you need to find someone else to answer questions about how to most fully
use an IDE to develop Spark and/or Spark applications.



"
Patrick Wendell <pwendell@gmail.com>,"Sun, 5 Jan 2014 18:17:43 -0800",Re: Build Changes for SBT Users,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Ya I was referring to already released version. Of course we can
update for subsequent releases...

e:
l
L
 we
yet
.
s
ty.
,
he
ca
s.
e
 in

"
Tom Graves <tgraves_cs@yahoo.com>,"Mon, 6 Jan 2014 09:32:19 -0800 (PST)",sbt to run a single test,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey all,

I haven't used sbt for spark much, mostly maven. I have been using it now recently and want to run just a single test. †Is there a way to do it?
I tried sbt test-only it didn't seem to work but perhaps my syntax was wrong. †SPARK_HADOOP_VERSION=0.23.10 †SPARK_YARN=true sbt/sbt test-only org.apache.spark.BroadcastSuite

Thanks,
Tom"
Jey Kottalam <jey@cs.berkeley.edu>,"Mon, 6 Jan 2014 09:42:50 -0800",Re: sbt to run a single test,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>, Tom Graves <tgraves_cs@yahoo.com>","sbt/sbt ""test-only org.apache.spark.BroadcastSuite""

I.e. As single argument. Hope that helps.



"
Jey Kottalam <jey@cs.berkeley.edu>,"Mon, 6 Jan 2014 09:46:50 -0800",Re: sbt to run a single test,,"Also, you can amortize JVM and SBT initialization time across multiple
runs by invoking SBT with no arguments then issuing your commands at the
prompt.


"
Tom Graves <tgraves_cs@yahoo.com>,"Mon, 6 Jan 2014 10:02:26 -0800 (PST)",Re: sbt to run a single test,"""jey@cs.berkeley.edu"" <jey@cs.berkeley.edu>","M and SBT initialization†time across multiple runs†by invoking SBT withspark.BroadcastSuite""
>
>I.e. As single argument. Hope that helps.†ll,
>>
>>I haven't used sbt for spark much, mostly maven. I have been using it now recently and want to run just a single test. †Is there a way to do it?
>>I tried sbt test-only it didn't seem to work but perhaps my syntax was wrong. †SPARK_HADOOP_VERSION=0.23.10 †SPARK_YARN=true sbt/sbt test-only org.apache.spark.BroadcastSuite
>>
>>Thanks,
>>Tom"
Michael Kun Yang <kunyang@stanford.edu>,"Mon, 6 Jan 2014 10:57:34 -0800",multinomial logistic regression,dev@spark.incubator.apache.org,"Hi Spark-ers,

I implemented a SGD version of multinomial logistic regression based on
mllib's optimization package. If this classifier is in the future plan of
mllib, I will be happy to contribute my code.

Cheers
"
Evan Sparks <evan.sparks@gmail.com>,"Mon, 6 Jan 2014 11:13:57 -0800",Re: multinomial logistic regression,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi Michael,

 I've got an optimized version of that method that I've been meaning to clean up and commit for a while. In particular, rather than shipping a (potentially very big) model with each map task, I ship it once before each iteration with a broadcast variable. Perhaps we can compare versions and incorporate some of my optimizations into your code?

Thanks,
Evan

:

"
Michael Kun Yang <kunyang@stanford.edu>,"Mon, 6 Jan 2014 11:33:06 -0800",Re: multinomial logistic regression,dev@spark.incubator.apache.org,"I actually have two versions:
one is based on gradient descent like the logistic regression on mllib.
the other is based on Newtown iteration, it is not as fast as SGD, but we
can get all the statistics from it like deviance, p-values and fisher info.

we can get confusion matrix in both versions

the gradient descent version is just a modification of logistic regression
with my own implementation. I did not use LabeledPoints class.



"
Alex Cozzi <alexcozzi@gmail.com>,"Mon, 6 Jan 2014 14:33:12 -0800",excluding hadoop dependencies in spark's assembly files,dev@spark.incubator.apache.org,"I am trying to exclude the hadoop jar dependencies from sparkís assembly files, the reason being that in order to work on our cluster it is necessary to use our now version of those files instead of the published ones. I tried define the hadoop dependencies as ìprovidedî, but surpassingly this causes compilation errors in the build. Just to be clear, I modified the sbt build file 
as follows:

  def yarnEnabledSettings = Seq(
    libraryDependencies ++= Seq(
      // Exclude rule required for all ?
      ""org.apache.hadoop"" % ""hadoop-client"" % hadoopVersion  % ""provided"" excludeAll(excludeJackson, excludeNetty, excludeAsm, excludeCglib),
      ""org.apache.hadoop"" % ""hadoop-yarn-api"" % hadoopVersion  % ""provided"" excludeAll(excludeJackson, excludeNetty, excludeAsm, excludeCglib),
      ""org.apache.hadoop"" % ""hadoop-yarn-common"" % hadoopVersion  % ""provided"" excludeAll(excludeJackson, excludeNetty, excludeAsm, excludeCglib),
      ""org.apache.hadoop"" % ""hadoop-yarn-client"" % hadoopVersion  % ""provided"" excludeAll(excludeJackson, excludeNetty, excludeAsm, excludeCglib)
    )
  )

and compile as 

 SPARK_HADOOP_VERSION=2.2.0 SPARK_YARN=true SPARK_IS_NEW_HADOOP=true sbt  assembly


but the assembly still includes the hadoop libraries, contrary to what the assembly docs say. I managed to exclude them instead by using the non-recommended way:
def extraAssemblySettings() = Seq(
    test in assembly := {},
    mergeStrategy in assembly := {
      case m if m.toLowerCase.endsWith(""manifest.mf"") => MergeStrategy.discard
      case m if m.toLowerCase.matches(""meta-inf.*\\.sf$"") => MergeStrategy.discard
      case ""log4j.properties"" => MergeStrategy.discard
      case m if m.toLowerCase.startsWith(""meta-inf/services/"") => MergeStrategy.filterDistinctLines
      case ""reference.conf"" => MergeStrategy.concat
      case _ => MergeStrategy.first
    },
    excludedJars in assembly <<= (fullClasspath in assembly) map { cp => 
     cp filter {_.data.getName.contains(""hadoop"")}
    }
)


But I would like to hear whether there is interest in excluding the hadoop jar by default in the build
Alex Cozzi
alexcozzi@gmail.com
"
Roman Shaposhnik <roman@apache.org>,"Mon, 6 Jan 2014 16:55:35 -0800",Re: excluding hadoop dependencies in spark's assembly files,dev@spark.incubator.apache.org,"Alex,

I don't know if it helps or not but sometimes back I made maven assembly to be
able to package Spark in Bigtop. That assembly exclude all hadoop
dependencies. So, you can simply build it using maven, instead of sbt.

Regards,
  Cos


"
Konstantin Boudnik <cos@apache.org>,"Mon, 6 Jan 2014 17:01:32 -0800",Re: excluding hadoop dependencies in spark's assembly files,dev@spark.incubator.apache.org,"Well,

somehow I managed to send an email as Roman Shaposhnik :) I guess he got himself a
stacker ;)

Cos


"
Hossein <falaki@gmail.com>,"Mon, 6 Jan 2014 17:34:22 -0800",Re: multinomial logistic regression,dev@spark.incubator.apache.org,"Hi Michael,

This sounds great. Would you please send these as a pull request.
Especially if you can make your Newtown method implementation generic such
that it can later be used by other algorithms, it would be very helpful.
For example, you could add it as another optimization method under
mllib/optimization.

Was there a particular reason you chose not use LabeledPoint?

We have some instructions for contributions here: <
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark>

Thanks,

--Hossein



"
Michael Kun Yang <kunyang@stanford.edu>,"Mon, 6 Jan 2014 18:15:09 -0800",Re: multinomial logistic regression,dev@spark.incubator.apache.org,"Hi Hossein,

I can still use LabeledPoint with little modification. Currently I convert
the category into {0, 1} sequence, but I can do the conversion in the body
of methods or functions.

In order to make the code run faster, I try not to use DoubleMatrix
abstraction to avoid memory allocation; another reason is that jblas has no
data structure to handle symmetric matrix addition efficiently.

My code is not very pretty because I handle matrix operations manually (by
indexing).

If you think it is ok, I will make a pull request.



"
Reynold Xin <rxin@databricks.com>,"Mon, 6 Jan 2014 18:21:25 -0800",Re: multinomial logistic regression,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Thanks. Why don't you submit a pr and then we can work on it?


"
Michael Kun Yang <kunyang@stanford.edu>,"Mon, 6 Jan 2014 18:26:21 -0800",Re: multinomial logistic regression,dev@spark.incubator.apache.org,"Thanks, will do.



"
Michael Kun Yang <kunyang@stanford.edu>,"Mon, 6 Jan 2014 21:14:27 -0800",Re: multinomial logistic regression,dev@spark.incubator.apache.org,"I just sent the pr for multinomial logistic regression.



"
Michael Kun Yang <kunyang@stanford.edu>,"Mon, 6 Jan 2014 21:18:26 -0800",Re: multinomial logistic regression,dev@spark.incubator.apache.org,"I will follow up the newtown one later



"
Alex Cozzi <alexcozzi@gmail.com>,"Mon, 6 Jan 2014 21:44:33 -0800",small changes to improve interoperability with eclipse,dev@spark.incubator.apache.org,"I tried to use ""sbt eclipseî on my mac to generate the eclipse files and noticed that, at least on my machine. I did 3 small changes to the spark build fail to fix the build, i.e. not using relative paths and including sources for ease of debugging. If you would like to include them:

diff --git a/project/SparkBuild.scala b/project/SparkBuild.scala
index 051e510..5427c84 100644
--- a/project/SparkBuild.scala
+++ b/project/SparkBuild.scala
@@ -21,6 +21,8 @@ import Keys._
 import sbtassembly.Plugin._
 import AssemblyKeys._
 import scala.util.Properties
+import com.typesafe.sbteclipse.plugin.EclipsePlugin.EclipseKeys
+
 // For Sonatype publishing
 //import com.jsuereth.pgp.sbtplugin.PgpKeys._
 
@@ -111,6 +113,10 @@ object SparkBuild extends Build {
     transitiveClassifiers in Scope.GlobalScope := Seq(""sources""),
     testListeners <<= target.map(t => Seq(new eu.henkelmann.sbt.JUnitXmlTestsListener(t.getAbsolutePath))),
 
+    // Eclipse settings
+    EclipseKeys.relativizeLibs := false,
+    EclipseKeys.withSource := true,
+
     // Fork new JVMs for tests and set Java options for those
     fork := true,
     javaOptions in Test += ""-Dspark.home="" + sparkHome,

"
"""Xia, Junluan"" <junluan.xia@intel.com>","Tue, 7 Jan 2014 13:59:16 +0000",About Spark job web ui persist(JIRA-969),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi all
         Spark job web ui will not be available when job is over, but it is convenient for developer to debug with persisting job web ui. I just come up with draft for this issue.

1.       We could simply save the web page with html/xml format(stages/executors/storages/environment) to certain location when job finished

2.       But it is not easy for user to review the job info with #1, we could build extra job history service for developers

3.       But where will we build this history service? In Driver node or Master node?

Any suggestions about this improvement?

regards,
Andrew

"
purav aggarwal <puravaggarwal123@gmail.com>,"Tue, 7 Jan 2014 23:59:42 +0530",Re: Large DataStructure to Broadcast,dev@spark.incubator.apache.org,"Thanks.
Broadcasting such huge entities does not seem like a feasible solution.
Serialization-Deserialization and network seem to have a huge overhead for
large files.

Before I consider moving into an external lookup service (as Christopher
rightly suggested) I was wondering if I could make each slave load the
large file in memory and do lookup operations in parallel.

*I am struck at how to make each slave load the files just once and perform
the lookup service.*

I tried using a hack where I check if the object is not initialised, I
shall initialise it. The problem is now for multiple threads running on a
single slave, I need a global object (specific to the JVM on that slave) to
hold on the other threads using ""synchronized"" while one of them is loading
the large file for me.
Any suggestions what can that unique object specific to that particular JVM
be. Is SparkContext an option ?




"
Tom Graves <tgraves_cs@yahoo.com>,"Tue, 7 Jan 2014 14:51:04 -0800 (PST)",Re: About Spark job web ui persist(JIRA-969),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I don't think you want to save the html/xml files. I would rather see the info saved into a history file in like a json format that could then be re-read and the web ui display the info, hopefully without much change to the UI parts. †For instance perhaps the history server could read the file and populate the appropriate Spark data structures that the web ui already uses. †

I would suggest making it so the history server is an optional server and could be run on any node. That way if the load on a particular node becomes to much it could be moved, but you also could run it on the same node as the Master. †All it really needs to know is where to get the history files from and have access to that location.

Hadoop actually has a history server for MapReduce which works very similar to what I mention  sure that the history files can only be read by users who have the appropriate permissions. †The history server itself could run as †a superuser who has permission to server up the files based on the acls.†



ble when job is over, but it is convenient for developer to debug with persisting job web ui. I just come up with draft for this issue.

1.† † †  We could simply save the web page with html/xml format(stages/executors/storages/environment) to certain location when job finished

2.† † †  But it is not easy for user to review the job info with #1, we could build extra job history service for developers

3.† † †  But where will we build this history service? In Driver node or Master node?

Any suggestions about this improvement?

regards,
Andrew"
Sandy Ryza <sandy.ryza@cloudera.com>,"Tue, 7 Jan 2014 15:18:49 -0800",Re: About Spark job web ui persist(JIRA-969),"dev@spark.incubator.apache.org, Tom Graves <tgraves_cs@yahoo.com>","As a sidenote, it would be nice to make sure that whatever done here will
work with the YARN Application History Server (YARN-321), a generic history
server that functions similarly to MapReduce's JobHistoryServer.  It will
eventually have the ability to store application-specific data.

-Sandy



"
Patrick Wendell <pwendell@gmail.com>,"Tue, 7 Jan 2014 16:13:37 -0800",Re: About Spark job web ui persist(JIRA-969),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey Sandy,

Do you know what the status is for YARN-321 and what version of YARN
it's targeted for? Also, is there any kind of documentation or API for
this? Does it control the presentation of the data itself (e.g. it
actually has its own UI)?

@Tom - having an optional history server sounds like a good idea.

idea would be to add JSON as an intermediate format inside of the
current WebUI, and then any JSON page could be persisted and rendered
it could dump a series of named paths each with a JSON file. Then the
history server could load those paths and pass them through the second
rendering stage (JSON => XML) to create each page.

It would be good if SPARK-969 had a good design doc before anyone
starts working on it.

- Patrick


"
Christopher Nguyen <ctn@adatao.com>,"Tue, 7 Jan 2014 17:16:45 -0800",Re: Large DataStructure to Broadcast,dev@spark.incubator.apache.org,"Purav, look up the Singleton pattern which is what you seem to be
describing.

The strategy you describe does not sound like a good idea, however. It
couples the ""lookup"" service rather strongly (and serially) to its data
processing clients. This is usually, though not always, less robust and
efficient.

Sent while mobile. Pls excuse typos etc.

"
Sandy Ryza <sandy.ryza@cloudera.com>,"Tue, 7 Jan 2014 22:43:12 -0800",Re: About Spark job web ui persist(JIRA-969),dev@spark.incubator.apache.org,"Hey,

YARN-321 is targeted for the Hadoop 2.4.  The minimum feature set doesn't
include application-specific data, so that probably won't be part of 2.4
unless other things delay the release for a while.  There are no APIs for
it yet and pluggable UIs have been discussed but not agreed upon.  I think
requirements from Spark could be useful in helping shape what gets done
there.

-Sandy



"
Michael Allman <msa@allman.ms>,"Wed, 8 Jan 2014 11:00:37 -0800 (PST)",spark code formatter?,dev@spark.incubator.apache.org,"Hi,

I've read the spark code style guide for contributors here:

https://cwiki.apache.org/confluence/display/SPARK/Spark+Code+Style+Guide

For scala code, do you have a scalariform configuration that you use to 
format your code to these specs?

Cheers,

Michael

"
Matei Zaharia <matei.zaharia@gmail.com>,"Wed, 8 Jan 2014 20:02:09 -0800",Re: spark code formatter?,dev@spark.incubator.apache.org,"Not that I know of. This would be very useful to add, especially if we can make SBT automatically check the code style (or we can somehow plug this into Jenkins).

Matei


https://cwiki.apache.org/confluence/display/SPARK/Spark+Code+Style+Guide
to format your code to these specs?


"
Reynold Xin <rxin@databricks.com>,"Wed, 8 Jan 2014 21:50:18 -0800",Re: spark code formatter?,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","We have a Scala style configuration file in Shark:
https://github.com/amplab/shark/blob/master/scalastyle-config.xml

However, the scalastyle project is still pretty primitive and doesn't cover
most of the use cases. It is still great to include it to cover basic
checks such as 100-char wide lines.



"
DB Tsai <dbtsai@alpinenow.com>,"Wed, 8 Jan 2014 22:09:41 -0800",Re: spark code formatter?,dev@spark.incubator.apache.org,"We use sbt-scalariform in our company, and it can automatically format
the coding style when runs `sbt compile`.

https://github.com/sbt/sbt-scalariform

We ask our developers to run `sbt compile` before commit, and it's
really nice to see everyone has the same spacing and indentation.

Sincerely,

DB Tsai
Machine Learning Engineer
Alpine Data Labs
--------------------------------------
Web: http://alpinenow.com/



"
DB Tsai <dbtsai@alpinenow.com>,"Wed, 8 Jan 2014 22:28:15 -0800",Re: spark code formatter?,dev <dev@spark.incubator.apache.org>,"A pull request for scalariform.
https://github.com/apache/incubator-spark/pull/365

Sincerely,

DB Tsai
Machine Learning Engineer
Alpine Data Labs
--------------------------------------
Web: http://alpinenow.com/



"
Reynold Xin <rxin@databricks.com>,"Wed, 8 Jan 2014 23:51:39 -0800",Re: spark code formatter?,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Thanks for doing that, DB. Not sure about others, but I'm actually strongly
against blanket automatic code formatters, given that they can be
disruptive. Often humans would intentionally choose to style things in a
certain way for more clear semantics and better readability. Code
formatters don't capture these nuances. It is pretty dangerous to just auto
format everything.

Maybe it'd be ok if we restrict the code formatters to a very limited set
of things, such as indenting function parameters, etc.



"
Patrick Wendell <pwendell@gmail.com>,"Thu, 9 Jan 2014 00:32:13 -0800",Re: spark code formatter?,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I'm also very wary of using a code formatter for the reasons already
mentioned by Reynold.

Does scaliform have a mode where it just provides style checks rather
than reformat the code? This is something we really need for, e.g.,
reviewing the many submissions to the project.

- Patrick


"
Tom Graves <tgraves_cs@yahoo.com>,"Thu, 9 Jan 2014 06:13:18 -0800 (PST)",Re: About Spark job web ui persist(JIRA-969),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Note that it looks like we are planning on adding support for application specific frameworks to YARN sooner rather then later. There is an initial design up here:†https://issues.apache.org/jira/browse/YARN-1530. Note this has not been reviewed yet so changes are likely but gives an idea of the general direction. †If anyone has comments on how that might work with SPARK I encourage you to post to the jira.

As Sandy mentioned it would be very nice if the solution could be compatible with that. †

Tom

The minimum feature set doesn't
include application-specific data, so that probably won't be part of 2.4
unless other things delay the release for a while.† There are no APIs for
it yet and pluggable UIs have been discussed but not agreed upon.† I think
requirements from Spark could be u
> Hey Sandy,
>
> Do you know what the status is for YARN-321 and what version of YARN
> it's targeted for? Also, is there any kind of documentation or API for
> this? Does it control the presentation of the data itself (e.g. it
> actually has its own UI)?
>
> @Tom - having an optionmat to use for storing the data and how the
> persisted format relates ton intermediate format inside of the
> current WebUI, and then any JSON page could be persisted and rendered
> by the history server using the sameeach with a JSON file. Then the
> history server could load those paths and pass them through the second
> rendering stage (JSON => XML) to create each page.
>
> It would be good if SPARK-969 had a good design doc b As a sidenote, it would be nice to make sure that whatever done here will
> > work with the YARN Application History Server (YARN-321), a generic
> history
> > server that functions similarly to MapReduce's JobHistoryServer.† It will
> > eventually have the ability to store application-k you want to save the html/xml files. I would rather see
> the
> >> info saved into a history file in like a json format that could then be
> >read the
> file
> >> and populate the appropriate Spark data structures that the web ui
> already
> >> uses.
> >>
> >> I would suggest making it so the history server is an optional server
> and
> >> could be run on any node. That way if the load on a particular node
> becomes
> 
> >> the Master.† All it really needs to know is where to get the history
> files
> >> from and have access to that location.
> >>
> >> Hadoop actually has a history server for MapReduce which works very
> >> s server itself
> >> could run as† a superuser who has permission to server up the files
> based
> >> on the acls.
> >>
> >>
> >>
> >> b web ui will not be available when job is over, but it
> >> is convenient for developer to debug with persisting job web ui. I just
> >> come up with draft for this issue.
> >>
> >> 1.† † †  We could simply save the web page with html/xml
> >> format(stages/executors/storages/environment) to certain location when
> job
> >> finished
> >>
> >> 2.† † †  But it is not easy for user to review the job info with #1, we
† † †  But where will we build this history service? In Driver node or
> >> Master node?
> >>
> >> Any suggestions about this improvement?
> >>
> >> regards,
> >> Andrew
> >>
>"
DB Tsai <dbtsai@alpinenow.com>,"Thu, 9 Jan 2014 12:23:52 -0800",Re: spark code formatter?,dev@spark.incubator.apache.org,"Initially, we also had the same concern, so we started from limited
set of rules. Gradually, we found that it increases the productivity
and readability of our codebase.

PS, Scalariform is compatible with the Scala Style Guide in the sense
that, given the right preference settings, source code that is
initially compiliant with the Style Guide will not become uncompliant
after formatting. In a number of cases, running the formatter will
make uncompliant source more compliant.

I added the configuration option in the latest PR to limit the set of
rules. The options are https://github.com/mdr/scalariform

When developers wants to choose their own style for whatever reasons,
they can source directives to turn it off by `// format: OFF`.

Just quickly run the formatter, and I found that Spark is in general
in good shape; most of the changes are extra space after semicolon.

-  def run[K: Manifest, V <: Vertex : Manifest, M <: Message[K] :
Manifest, C: Manifest](

+  def run[K: Manifest, V <: Vertex: Manifest, M <: Message[K]:
Manifest, C: Manifest](

-  def addFile(file: File) : String = {
+ def addFile(file: File): String = {

Sincerely,

DB Tsai
Machine Learning Engineer
Alpine Data Labs
--------------------------------------
Web: http://alpinenow.com/



"
Alex Cozzi <alexcozzi@gmail.com>,"Thu, 9 Jan 2014 13:32:07 -0800","yarn, fat-jars and lib_managed",dev@spark.incubator.apache.org,"I am just starting out playing with spark on our hadoop 2.2 cluster and I have a question.

The current way to submit jobs to the cluster is to create fat-jars with sbt assembly. This approach works but I think is less than optimal in many large hadoop installation:

the way we interact with the cluster is to log into a CLI machine, which is the only authorized to submit jobs. Now, I can not use the CLI machine as a dev environment since for security reason the CLI and hadoop cluster is fire-walled and can not reach out to the internet, so sbt and manven resolution does not work.

So the procedure now is:
- hack code
- sbt assembly
- rsync my spark directory to the CLI machine
- run my job.

the issue is that every time i need to shuttle large binary files (all the fat-jars) back and forth, they are about 120Mb now, which is slow, particularly when I am working remotely from home.

I was wondering whether a better solution would be to create normal thin-jars of my code, which is very small, less than a Mb, and have no problem to copy every time to the cluster, but to take advantage of the sbt-create directory lib_managed to handle dependencies. We already have this directory that sbt handles with all the needed dependencies for the job to run. Wouldnít be possible to have the Spark Yarn Client take care of adding all the jars in lib_managed to class path and distribute them to the workers automatically (and they could also be cached across invocations of spark, after all those jars are versioned and immutable, with the possible exception of -SNAPSHOT releases). I think that this would greatly simplify the development procedure and remove the need of messing with ADD_JAR and SPARK_CLASSPATH.

What do you think?

Alex 
"
"""Liu, Raymond"" <raymond.liu@intel.com>","Fri, 10 Jan 2014 02:10:48 +0000","RE: yarn, fat-jars and lib_managed","""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I think you could put the spark jar and other jar your app depends on while not changes a lot on HDFS, and use --files or --addjars ( depends on the mode you run YarnClient/YarnStandalone ) to refer to them.
And then just need to redeploy your thin app jar on each invoke.

Best Regards,
Raymond Liu


ave a question.

The current way to submit jobs to the cluster is to create fat-jars with sbt assembly. This approach works but I think is less than optimal in many large hadoop installation:

the way we interact with the cluster is to log into a CLI machine, which is the only authorized to submit jobs. Now, I can not use the CLI machine as a dev environment since for security reason the CLI and hadoop cluster is fire-walled and can not reach out to the internet, so sbt and manven resolution does not work.

So the procedure now is:
- hack code
- sbt assembly
- rsync my spark directory to the CLI machine
- run my job.

the issue is that every time i need to shuttle large binary files (all the fat-jars) back and forth, they are about 120Mb now, which is slow, particularly when I am working remotely from home.

I was wondering whether a better solution would be to create normal thin-jars of my code, which is very small, less than a Mb, and have no problem to copy every time to the cluster, but to take advantage of the sbt-create directory lib_managed to handle dependencies. We already have this directory that sbt handles with all the needed dependencies for the job to run. Wouldn't be possible to have the Spark Yarn Client take care of adding all the jars in lib_managed to class path and distribute them to the workers automatically (and they could also be cached across invocations of spark, after all those jars are versioned and immutable, with the possible exception of -SNAPSHOT releases). I think that this would greatly simplify the development procedure and remove the need of messing with ADD_JAR and SPARK_CLASSPATH.

What do you think?

Alex 

"
Michael Kun Yang <kunyang@stanford.edu>,"Thu, 9 Jan 2014 21:23:47 -0800",Re: multinomial logistic regression,"dev@spark.incubator.apache.org, falaki@gmail.com","I just sent the pr, fixed a typo in the comment. Add some comments and unit
test. Please let me know if you receive the patch.



"
"""Shao, Saisai"" <saisai.shao@intel.com>","Fri, 10 Jan 2014 05:29:34 +0000",Contribute SimRank algorightm to mllib,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi All,

We would like to contribute SimRank algorithm to mllib. SimRank algorithm used to calculate similarity rank between two objects based on graph structure, details can be seen in (http://ilpubs.stanford.edu:8090/508/1/2001-41.pdf), here we implemented a matrix multiplication method based on basic algorithm, the description of matrix multiplication method can be seen in (http://www.cse.unsw.edu.au/~zhangw/files/wwwj.pdf) chapter 4.1.

The implementation is abstracted and generalized from our customer's real case, we made some tradeoffs to improve the speed and reduce the shuffle size. we just wondered if this algorithm be suitable to put into mllib? What else should we take care about?

Any suggestion would be really appreciated.

Thanks
Jerry
"
Evan Chan <ev@ooyala.com>,"Fri, 10 Jan 2014 12:30:48 -0800",Re: About Spark job web ui persist(JIRA-969),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>, Tom Graves <tgraves_cs@yahoo.com>","+1 for JSON.  The job server could also save context history and state and
make it queryable via REST APIs regardless of YARN / Mesos / standalone.






-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

<http://www.ooyala.com/>
<http://www.facebook.com/o"
Reynold Xin <rxin@databricks.com>,"Fri, 10 Jan 2014 22:40:01 -0800",Re: Contribute SimRank algorightm to mllib,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi Jerry,

Why don't you submit a pull request and then we can discuss there? If
SimRank is not common enough, we might take the matrix multiplication
method in and merge that. At the very least, even if SimRank doesn't get
merged into Spark, we can include a contrib package or a Wiki page that
links to examples of various algorithms community members have implemented.





"
Alex Cozzi <alexcozzi@gmail.com>,"Fri, 10 Jan 2014 23:30:20 -0800","Re: yarn, fat-jars and lib_managed",dev@spark.incubator.apache.org,"well, yes, you can, but it would be much more convenient if spark were to
automatically take care of all the jars under lib_managed, rather than
having to list 20-30 jar in --files and --addJars



"
"""Shao, Saisai"" <saisai.shao@intel.com>","Sat, 11 Jan 2014 08:57:19 +0000",RE: Contribute SimRank algorightm to mllib,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi Reynold,

Thanks for your reply. We will submit a PR after all the code and test under current master branch is done.

Thanks
Jerry

Hi Jerry,

Why don't you submit a pull request and then we can discuss there? If SimRank is not common enough, we might take the matrix multiplication method in and merge that. At the very least, even if SimRank doesn't get merged into Spark, we can include a contrib package or a Wiki page that links to examples of various algorithms community members have implemented.





mllib?

"
Pillis Work <pillis.work@gmail.com>,"Sun, 12 Jan 2014 19:21:56 -0800",Re: About Spark job web ui persist(JIRA-969),dev@spark.incubator.apache.org," IMHO from a pure Spark standpoint, I don't know if having a dedicated
history service makes sense as of now - considering that cluster managers
have their own history servers. Just showing UI of history runs might be
too thin a requirement for a full service. Spark should store history
information that can later be exposed in required ways.

Since each SparkContext is the logical entry and exit point for doing
something useful in Spark, during its stop(), it should serialize that
run's statistics into a JSON file - like ""sc_run_[name]_[start-time].json"".
When SparkUI.stop() is called, it in turn asks its UI objects (which should
implement a trait) to provide either a flat or hierarchical Map of String
key/value pairs. This map (flat, hierarchical) is then serialized to a
configured path (default being ""var/history"").

With regards to Mesos or YARN, their applications during shutdown can have
API to import this Spark history into their history servers - by making API
calls etc.

This way Spark's history information is persisted independent of cluster
framework, and cluster frameworks can import the history when/as needed.
Hope this helps.
Regards,
pillis



"
Tom Graves <tgraves_cs@yahoo.com>,"Mon, 13 Jan 2014 06:15:50 -0800 (PST)",Re: About Spark job web ui persist(JIRA-969),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","So the downside to just saving stuff at the end is that if the app crashes or exits badly you don't have anything. † Hadoop has taken the approach of saving events along the way. †But Hadoop also uses that history file to start where it left off at if something bad happens and it gets restarted. †I don't think the latter really applies to spark though.

Does mesoark standpoint, I don't know if having a dedicated
history service makes sense as of now - considering that cluster managers
have their own history servers. Just showing UI of history runs might be
too thin a requirement for a full service. Spark should store history
information that can later be exposed in required ways.

Since each SparkContext is the logical entry and exit point for doing
something useful in Spark, during its stop(), it should serialize that
run's statistics into a JSON file - like ""sc_run_[name]_[start-time].json"".
When SparkUI.stop() is called, it in turn asks its UI objects (which should
implement a trait) to provide either a flat or hierarchical Map of String
key/value pairs. This map (flat, hierarchical) is then serialized to a
configured path (default being ""var/history"").

With regards to Mesos or YARN, their applications during shutdown can have
API to import this Spark history into their history servers - by making API
calls etc.

This way Spark's history information is persisted independent of cluster
framework, and cluster frameworks can import the history when/as needed.
Hope this helps.
Regards,
pillis
 application
> specific frameworks to YARN sooner rather then later. There is an initial
> design up here: https://issues.apache.org/jira/browse/YARN-1530. Note
> this has not been reviewed yet so changes are likely but gives an idea of
> the general direction.† If anyone has comments on how that might work with
> SPARK I encourage you to post to the jira.
>
> As Sandy mentioned it would be very nice if the solution could be
> ,
>
> YARN-321 is targeted for the Hadoop 2.4.† The minimum feature set doesn't
> include application-specific data, so that probably won't be part of 2.4
> unless other things delay the release for a while.† There are no APIs for
> it yet and pluggable UIs have been discussed but not agreed upon.† I think
> requirements from Spark could be useful in help
> > Hey Sandy,
> >
> > Do you know what the status is for YARN-321 and what version of YARN
> > it's targeted for? Also, is there any kind of documentation or API for
> > this? Does it control the presentation of the data itself (e.g. it
> > actually has its own UI)?
> >
> > @Tom - question is what format to use for storing the data and how the
> > persiould be to add JSON as an intermediate format inside of the
> > current WebUI, and then any JSON page could be persisted and rendered
> > by the h dump a series of named paths each with a JSON file. Then the
> > history server could load those paths and pass them through the second
> > rendering stage (JSON => XML) to create each page.
> >
> > It would be good if SPARK-969 had a good design doc before anyone
> > starts working on d be nice to make sure that whatever done here
> will
> > > work with the YARN Application History Server (YARN-321), a generic
> > history
> 
> will
> > > eventually have the ability to store application-specificdon't think you want to save the html/xml files. I would rather see
> > the
> > >> info saved into a history file in like a json format that could then
> be
> > >> re-read and the web ui display the info, hopefully without much change
> > to
> > >> the UI parts.† For instance perhaps the history server could read the
> > file
> > >> and populate the appropriate Spark data structures that the web ui
> > already
> > >> uses.
> > >>
> > >> I would suggest making it so the history server is an optional server
> > and
> > >> could be run on any node. That way if the load on a particular node
> > becomes
> > >> to much it could be moved, but you also could run it on the same node
> as
> > >> the Master.† All it really needs to know is where to get the history
> > files
> > >has a history server for MapReduce which works very
> > >> similar to wha.† You want to make sure that the history files can only be
> > read by
> > >> users who have the appropriate permissions.† The history server itself
> > >> could run as† a superuser who has permission to server up the files
> > based
> > >> on the acls.
> > >>
> > >>
> > >>
 † † Spark job web ui will not be available when job is over, but
> it
> > >> is convenient for developer to debug with persisting job web ui. I
> just
> > >> come up with draft for this issue.
> > >>
> > >> 1.† † †  We could simply save the web page with html/xml
> > >> format(stages/executors/storages/environment) to certain location when
> > job
> > >> finished
> > >>
> > >> 2.† † †  But it is not easy for user to review the job info with #1,
> we
> > >> could build extra job history service for developers
> > >>
> > >> 3.† † †  But where will we build this history service? In Driver node
> or
> > >> Master node?
> > >>
> > >> Any suggestions about this improvement?
> > >>
> > >> regards,
> > >> Andrew
> > >>
> >
>"
Pillis Work <pillis.work@gmail.com>,"Mon, 13 Jan 2014 10:43:15 -0800",Re: About Spark job web ui persist(JIRA-969),dev@spark.incubator.apache.org,"The listeners in SparkUI which update the counters can trigger saves along
the way.
The save can be on a 500ms delay after the last update, to batch changes.
This solution would not require save on stop().




"
Patrick Wendell <pwendell@gmail.com>,"Mon, 13 Jan 2014 13:32:50 -0800",Re: About Spark job web ui persist(JIRA-969),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Pillis - I agree we need to decouple the representation from a
particular history server. But why not provide as simple history
server people can (optionally) run if they aren't using Yarn or Mesos?
For people running the standalone cluster scheduler this seems
important. Giving them only a JSON dump isn't super consumable for
most users.

- Patrick


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Mon, 13 Jan 2014 14:34:24 -0800",Re: About Spark job web ui persist(JIRA-969),dev@spark.incubator.apache.org,"tools to parse the JSON dump. I think something like a python wrapper
script that given a (job-id, stage-id) can parse the JSON and print
out the stage / task timings, with histograms etc. From my own
perspective, It'll be super useful for generating data that can be
used in benchmarks and experiment results etc.

Thanks
Shivaram


"
Pillis Work <pillis.work@gmail.com>,"Mon, 13 Jan 2014 14:43:52 -0800",Re: About Spark job web ui persist(JIRA-969),dev@spark.incubator.apache.org,"My primary concern was having too many watches for history.
But I do not have an argument against stand-alone mode.

Also questions about automatic lifecycle management of history server -
when to start / stop etc. in a cluster environment would be difficult when
one is not the cluster manager.

I agree that JSON files are not easily readable.
Maybe a stand-alone history server which makes it implicit that it is only
for that host would be fine.






"
Nan Zhu <zhunanmcgill@gmail.com>,"Mon, 13 Jan 2014 21:13:27 -0500","Is there any plan to develop an application level fair
 scheduler?",dev@spark.incubator.apache.org,"Hi, All  

Is there any plan to develop an application level fair scheduler?

I think it will have more value than a fair scheduler within the application (actually I didn‚Äôt understand why we want to fairly share the resource among jobs within the application, in usual, users submit different applications, not jobs)‚Ä¶

Best,  

--  
Nan Zhu

"
"""Xia, Junluan"" <junluan.xia@intel.com>","Tue, 14 Jan 2014 05:56:06 +0000","RE: Is there any plan to develop an application level fair
 scheduler?","""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Are you sure that you must deploy spark in standalone mode?(it currently only support FIFO)

If you could setup Spark on Yarn or Mesos, then it has supported Fair scheduler in application level.

-----Original Message-----
sday, January 14, 2014 10:13 AM
To: dev@spark.incubator.apache.org
Subject: Is there any plan to develop an application level fair scheduler?

Hi, All  

Is there any plan to develop an application level fair scheduler?

I think it will have more value than a fair scheduler within the application (actually I didn‚Äôt understand why we want to fairly share the resource among jobs within the application, in usual, users submit different applications, not jobs)‚Ä¶

Best,  

--  
Nan Zhu

"
Nan Zhu <zhunanmcgill@gmail.com>,"Tue, 14 Jan 2014 08:43:20 -0500","Re: Is there any plan to develop an application level fair
 scheduler?",dev@spark.incubator.apache.org,"Hi, Junluan,   

Thank you for the reply  

but for the long-term plan, Spark will depend on Yarn and Mesos for application level scheduling in the coming versions?

Best,  

--  
Nan Zhu



tly only support FIFO)
 scheduler in application level.
he.org)
er?
ation (actually I didn‚Äôt understand why we want to fairly share the resource among jobs within the application, in usual, users submit different applications, not jobs)‚Ä¶


"
"""Xia, Junluan"" <junluan.xia@intel.com>","Wed, 15 Jan 2014 02:32:07 +0000","RE: Is there any plan to develop an application level fair
 scheduler?","""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Yes, Spark depends on Yarn or Mesos for application level scheduling.

-----Original Message-----
From: Nan Zhu [mailto:zhunanmcgill@gmail.com] 
Sent: Tuesday, January 14, 2014 9:43 PM
To: dev@spark.incubator.apache.org
Subject: Re: Is there any plan to develop an application level fair scheduler?

Hi, Junluan,   

Thank you for the reply  

but for the long-term plan, Spark will depend on Yarn and Mesos for application level scheduling in the coming versions?

Best,  

--  
Nan Zhu


On Tuesday, January 14, 2014 at 12:56 AM, Xia, Junluan wrote:

> Are you sure that you must deploy spark in standalone mode?(it currently only support FIFO)
>  
> If you could setup Spark on Yarn or Mesos, then it has supported Fair scheduler in application level.
>  
> -----Original Message-----
> From: Nan Zhu [mailto:zhunanmcgill@gmail.com]  
> Sent: Tuesday, January 14, 2014 10:13 AM
> To: dev@spark.incubator.apache.org (mailto:dev@spark.incubator.apache.org)
> Subject: Is there any plan to develop an application level fair scheduler?
>  
> Hi, All  
>  
> Is there any plan to develop an application level fair scheduler?
>  
> I think it will have more value than a fair scheduler within the application (actually I didn‚Äôt understand why we want to fairly share the resource among jobs within the application, in usual, users submit different applications, not jobs)‚Ä¶
>  
> Best,  
>  
> --  
> Nan Zhu
>  
>  


"
Matei Zaharia <matei.zaharia@gmail.com>,"Tue, 14 Jan 2014 20:10:08 -0800",Re: Is there any plan to develop an application level fair scheduler?,dev@spark.incubator.apache.org,"This is true for now, we didnít want to replicate those systems. But it may change if we see demand for fair scheduling in our standalone cluster manager.

Matei


scheduler?
application level scheduling in the coming versions?
currently only support FIFO)
scheduler in application level.
(mailto:dev@spark.incubator.apache.org)
scheduler?
application (actually I didnít understand why we want to fairly share the resource among jobs within the application, in usual, users submit different applications, not jobs)Ö


"
Eduardo Costa Alfaia <e.costaalfaia@studenti.unibs.it>,"Wed, 15 Jan 2014 15:01:18 +0100",JavaNetworkWordCount Researches,dev@spark.incubator.apache.org,"Hi Guys,

I did some changes in JavaNetworkWordCount for my researches in streaming process and I have added to the code the following lines in red:

ssc1.checkpoint(""hdfs://computer22:54310/user/root/INPUT"");
 JavaDStream<String> lines1 = ssc1.socketTextStream(""localhost"", Integer.parseInt(""12345""));
 JavaDStream<String> lines2 = ssc1.socketTextStream(""localhost"", Integer.parseInt(""12345""));
 JavaDStream<String> union2 = lines1.union(lines2);
     JavaDStream<String> words = union2.flatMap(new FlatMapFunction<String, String>() {
     @Override
       public Iterable<String> call(String x) {
          return Lists.newArrayList(SPACE.split(x));
        }
     });
     JavaPairDStream<String, Integer> wordCounts = words.map(
new PairFunction<String, String, Integer>() {
          @Override
          public Tuple2<String, Integer> call(String s) {
           return new Tuple2<String, Integer>(s, 1);
         }
        }).reduceByKey(new Function2<Integer, Integer, Integer>() {
          @Override
         public Integer call(Integer i1, Integer i2) {
           return i1 + i2;
          }
        });
  
      JavaPairDStream<String, Integer> counts = wordCounts.reduceByKeyAndWindow(
        new Function2<Integer, Integer, Integer>() {
          public Integer call(Integer i1, Integer i2) { return i1 + i2; }
        },
        new Function2<Integer, Integer, Integer>() {
          public Integer call(Integer i1, Integer i2) { return i1 - i2; }
        },
        new Duration(60 * 5 * 1000),
     new Duration(1 * 1000)
         
	 counts.print();
   ssc1.start();
 
   }
 }


- We did a code in C that send words to workers.

- Result From Master terminal:

Time: 1389794084000 ms
-------------------------------------------
(,14294)
(impertinences,2)
(protracted.,3)
(burlesque.,3)
(Dorothea,,85)
(grant,,5)
(temples,,2)
(discord,17)
(conscience,48)
(singed,,2)
...

-------------------------------------------
Time: 1389794085000 ms
-------------------------------------------
(,38580)
(impertinences,5)
(protracted.,7)
(burlesque.,7)
(Dorothea,,259)
(grant,,12)
(temples,,7)
(discord,47)
(conscience,130)
(singed,,5)
...

My question is, where does it happening the union()? between in the nodes or in the master?  I am using three machines( 1 Master + 2 Nodes).
How could I get a total count of the words and show in the terminal?

Thanks all  




-- 
---
INFORMATIVA SUL TRATTAMENTO DEI DATI PERSONALI

I dati utilizzati per l'invio del presente messaggio sono trattati 
dall'Universit‡ degli Studi di Brescia esclusivamente per finalit‡ 
istituzionali. Informazioni pi˘ dettagliate anche in ordine ai diritti 
dell'interessato sono riposte nell'informativa generale e nelle notizie 
pubblicate sul sito web dell'Ateneo nella sezione ""Privacy"".

Il contenuto di questo messaggio Ë rivolto unicamente alle persona cui 
Ë indirizzato e puÚ contenere informazioni la cui riservatezza Ë 
tutelata legalmente. Ne sono vietati la riproduzione, la diffusione e l'uso 
in mancanza di autorizzazione del destinatario. Qualora il messaggio 
fosse pervenuto per errore, preghiamo di eliminarlo.
"
Scott walent <scottwalent@gmail.com>,"Wed, 15 Jan 2014 12:02:46 -0800",Join us for AMP Camp 4 on February 11 and enter to win a free Strata pass,dev@spark.incubator.apache.org,"Hey,


We are excited that AMP Camp 4 is less than a month away!


AMP Camp 4 will be a compact one-day version of AMP Camp, consisting of two
tutorials at 2014 Strata Conference <http://strataconf.com/strata2014> in
Santa Clara. Those tutorials will be run by PhD students and faculty from
the UC Berkeley AMPLab and contain
talks<http://strataconf.com/strata2014/public/schedule/detail/32374>and
hands-on
exercises <http://strataconf.com/strata2014/public/schedule/detail/32386>about
the newest components of the Berkeley
Data Analytics Stack (BDAS) <https://amplab.cs.berkeley.edu/software> that
they are actively developing.

The first part<http://strataconf.com/strata2014/public/schedule/detail/32374>of
the two-part series will kick off with talks on Spark, Spark Streaming
and Shark. We will then highlight three new components of the Berkeley Data
Analytics Stack (BDAS). We‚Äôll start with BlinkDB, an approximate query
engine for running interactive SQL queries on large volumes of data. Second
up will be MLBase, a platform for implementing and consuming machine
learning algorithms at scale. Then Tachyon, a fault tolerant distributed
in-memory file system.

Later in the day we will host hands-on
training<http://strataconf.com/strata2014/public/schedule/detail/32386>covering
basic Spark, Shark and these new BDAS components. Each member of
the training will be provided with access to their own EC2 cluster
pre-loaded with datasets that they can use to run exercises utilizing these
new technologies.

To sign up for AMP Camp 4,
register<https://en.oreilly.com/strata2014/public/register>with the
Strata Conference and be sure to select the tutorials day. The
folks at O‚ÄôReilly have been gracious enough to offer a 20% discount to the
friends of the AMPLab.

Use the code AMP20 when registering to get 20% off your ticket price.

Lastly, we have a free 3-day conference pass to give out! If you can take a
minute to fill out this short
form<https://docs.google.com/forms/d/1y6rmzU_i_PbuV17mHauwqQyIvdBX-8g8opKSHyrvs0A/viewform>(your
emails will not be used for marketing purposes), you will be entered
in a drawing to win a 3-day Strata Santa Clara pass. The winner will be
able to join us for AMP Camp 4 and the following 2 days of Strata.

You can access the web form at: ampcamp.berkeley.edu/2014StrataPass

We are excited to share what we have been working on with you. Hope to see
you there!

All the best,
Scott
"
Chen Jin <karen.cj@gmail.com>,"Wed, 15 Jan 2014 14:04:49 -0800",The performance of group operation on SSD,dev@spark.incubator.apache.org,"Dear Spark developers:

We are benchmarking spark operations such as filter, group, join on
ssd instance i2.2xlarge on EC2. Most operations are similar or
slightly better than ephemeral disks on EC2, however, the performance
of group operation on SDD  are much worse than regular disks, at least
2x to 3x worse. Could any of you shed some lights on this behavior?

Thanks a lot,

-chen

"
Tathagata Das <tathagata.das1565@gmail.com>,"Wed, 15 Jan 2014 16:48:43 -0800",Re: JavaNetworkWordCount Researches,dev@spark.incubator.apache.org,"All the computation with the data (that is, union, flatmap, map,
reduceByKey, reduceByKeyAndWindow) are executed on the workers in a
distributed manner. The data is received by the worker nodes and kept in
memory, then the computation is executed on the workers to the in-memory
data.

After the count is computed for every batch of data, the first 10 elements
of the generated counts are brought to master for being printed on the
screen. This is done by the counts.print() which pulls those 10 word-count
pairs and prints them.

the first reduceByKey. The reduceByKeyAndWindow takes care of doing the
reduceByKey per batch and then doing the reduce across a window.

TD



so
"
Patrick Wendell <pwendell@gmail.com>,"Wed, 15 Jan 2014 17:48:41 -0800",[VOTE] Release Apache Spark 0.9.0-incubating (rc1),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Please vote on releasing the following candidate as Apache Spark
(incubating) version 0.9.0.

A draft of the release notes along with the changes file is attached
to this e-mail.

The tag to be voted on is v0.9.0-incubating (commit 7348893):
https://git-wip-us.apache.org/repos/asf?p=incubator-spark.git;a=commit;h=7348893f0edd96dacce2f00970db1976266f7008

The release files, including signatures, digests, etc can be found at:
http://people.apache.org/~pwendell/spark-0.9.0-incubating-rc1/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1001/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-0.9.0-incubating-rc1-docs/

Please vote on releasing this package as Apache Spark 0.9.0-incubating!

The vote is open until Sunday, January 19, at 02:00 UTC
and passes if a majority of at least 3 +1 PPMC votes are cast.

[ ] +1 Release this package as Apache Spark 0.9.0-incubating
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.incubator.apache.org/
"
Alex Cozzi <alexcozzi@gmail.com>,"Wed, 15 Jan 2014 23:41:41 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc1),dev@spark.incubator.apache.org,"Just testing out the rc1. I create a dependent project (using maven) and I copied the HdfsTest.scala test, but I added a single line to save the file back to disk:

package org.apache.spark.examples

import org.apache.spark._

object HdfsTest {
  def main(args: Array[String]) {
    val sc = new SparkContext(args(0), ""HdfsTest"",
      System.getenv(""SPARK_HOME""), SparkContext.jarOfClass(this.getClass))
    val file = sc.textFile(args(1))
    val mapped = file.map(s => s.length).cache()
    for (iter <- 1 to 10) {
      val start = System.currentTimeMillis()
      for (x <- mapped) { x + 2 }
      //  println(""Processing: "" + x)
      val end = System.currentTimeMillis()
      println(""Iteration "" + iter + "" took "" + (end-start) + "" ms"")
      mapped.saveAsTextFile(""out"")
    }
    System.exit(0)
  }
}

and this my pom file:
<project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
	xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd"">
	<modelVersion>4.0.0</modelVersion>
	<groupId>my.examples</groupId>
	<artifactId>spark-samples</artifactId>
	<version>0.0.1-SNAPSHOT</version>
	<inceptionYear>2014</inceptionYear>

	<properties>
		<maven.compiler.source>1.6</maven.compiler.source>
		<maven.compiler.target>1.6</maven.compiler.target>
		<encoding>UTF-8</encoding>
		<scala.tools.version>2.10</scala.tools.version>
		<scala.version>2.10.0</scala.version>
	</properties>

	<repositories>
		<repository>
		<id>spark staging</id>
		<url>https://repository.apache.org/content/repositories/orgapachespark-1001</url>
		</repository>
	</repositories>

	<dependencies>
		<dependency>
			<groupId>org.scala-lang</groupId>
			<artifactId>scala-library</artifactId>
			<version>${scala.version}</version>
		</dependency>

		<dependency>
			<groupId>org.apache.spark</groupId>
			<artifactId>spark-core_${scala.tools.version}</artifactId>
			<version>0.9.0-incubating</version>
		</dependency>

		<!-- Test -->
		<dependency>
			<groupId>junit</groupId>
			<artifactId>junit</artifactId>
			<version>4.11</version>
			<scope>test</scope>
		</dependency>
		<dependency>
			<groupId>org.specs2</groupId>
			<artifactId>specs2_${scala.tools.version}</artifactId>
			<version>1.13</version>
			<scope>test</scope>
		</dependency>
		<dependency>
			<groupId>org.scalatest</groupId>
			<artifactId>scalatest_${scala.tools.version}</artifactId>
			<version>2.0.M6-SNAP8</version>
			<scope>test</scope>
		</dependency>
	</dependencies>

	<build>
		<sourceDirectory>src/main/scala</sourceDirectory>
		<testSourceDirectory>src/test/scala</testSourceDirectory>
		<plugins>
			<plugin>
				<!-- see http://davidb.github.com/scala-maven-plugin -->
				<groupId>net.alchim31.maven</groupId>
				<artifactId>scala-maven-plugin</artifactId>
				<version>3.1.6</version>
				<configuration>
					<scalaCompatVersion>2.10</scalaCompatVersion>
					<jvmArgs>
						<jvmArg>-Xms128m</jvmArg>
						<jvmArg>-Xmx2048m</jvmArg>
					</jvmArgs>
				</configuration>
				<executions>
					<execution>
						<goals>
							<goal>compile</goal>
							<goal>testCompile</goal>
						</goals>
						<configuration>
							<args>
								<arg>-make:transitive</arg>
								<arg>-dependencyfile</arg>
								<arg>${project.build.directory}/.scala_dependencies</arg>
							</args>
						</configuration>
					</execution>
				</executions>
			</plugin>
			<plugin>
				<groupId>org.apache.maven.plugins</groupId>
				<artifactId>maven-surefire-plugin</artifactId>
				<version>2.13</version>
				<configuration>
					<useFile>false</useFile>
					<disableXmlReport>true</disableXmlReport>
					<!-- If you have classpath issue like NoDefClassError,... -->
					<!-- 					<includes>
						<include>**/*Test.*</include>
						<include>**/*Suite.*</include>
					</includes>
				</configuration>
			</plugin>
			<plugin>
				<groupId>org.codehaus.mojo</groupId>
				<artifactId>exec-maven-plugin</artifactId>
				<version>1.2.1</version>
				<executions>
					<execution>
						<goals>
							<goal>exec</goal>
						</goals>
					</execution>
				</executions>
				<configuration>
					<mainClass>org.apache.spark.examples.HdfsTest</mainClass>
					<arguments>
						<argument>local</argument>
						<argument>pom.xml</argument>
					</arguments>
				</configuration>
			</plugin>
		</plugins>
	</build>
</project>


now, when I run it either in eclipse or using ""mvn exec:java""  I get the following error:
[INFO] 
[INFO] --- exec-maven-plugin:1.2.1:java (default-cli) @ spark-samples ---
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/Users/acozzi/.m2/repository/org/slf4j/slf4j-log4j12/1.6.1/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/Users/acozzi/.m2/repository/org/slf4j/slf4j-simple/1.6.1/slf4j-simple-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
14/01/15 23:37:57 INFO slf4j.Slf4jLogger: Slf4jLogger started
14/01/15 23:37:57 INFO Remoting: Starting remoting
14/01/15 23:37:57 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://spark@10.0.1.10:53682]
14/01/15 23:37:57 INFO Remoting: Remoting now listens on addresses: [akka.tcp://spark@10.0.1.10:53682]
14/01/15 23:37:57 INFO spark.SparkEnv: Registering BlockManagerMaster
14/01/15 23:37:57 INFO storage.DiskBlockManager: Created local directory at /var/folders/mm/4qxz27w91p96v2zp5f9ncmqm38ychm/T/spark-local-20140115233757-7a41
14/01/15 23:37:57 INFO storage.MemoryStore: MemoryStore started with capacity 1218.8 MB.
14/01/15 23:37:57 INFO network.ConnectionManager: Bound socket to port 53683 with id = ConnectionManagerId(10.0.1.10,53683)
14/01/15 23:37:57 INFO storage.BlockManagerMaster: Trying to register BlockManager
14/01/15 23:37:57 INFO storage.BlockManagerMasterActor$BlockManagerInfo: Registering block manager 10.0.1.10:53683 with 1218.8 MB RAM
14/01/15 23:37:57 INFO storage.BlockManagerMaster: Registered BlockManager
14/01/15 23:37:57 INFO spark.HttpServer: Starting HTTP Server
14/01/15 23:37:57 INFO server.Server: jetty-7.6.8.v20121106
14/01/15 23:37:57 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:53684
14/01/15 23:37:57 INFO broadcast.HttpBroadcast: Broadcast server started at http://10.0.1.10:53684
14/01/15 23:37:57 INFO spark.SparkEnv: Registering MapOutputTracker
14/01/15 23:37:57 INFO spark.HttpFileServer: HTTP File server directory is /var/folders/mm/4qxz27w91p96v2zp5f9ncmqm38ychm/T/spark-e9304513-3714-430f-aa14-1a430a915d98
14/01/15 23:37:57 INFO spark.HttpServer: Starting HTTP Server
14/01/15 23:37:57 INFO server.Server: jetty-7.6.8.v20121106
14/01/15 23:37:57 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:53685
14/01/15 23:37:57 INFO server.Server: jetty-7.6.8.v20121106
14/01/15 23:37:57 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/storage/rdd,null}
14/01/15 23:37:57 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/storage,null}
14/01/15 23:37:57 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/stages/stage,null}
14/01/15 23:37:57 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/stages/pool,null}
14/01/15 23:37:57 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/stages,null}
14/01/15 23:37:57 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/environment,null}
14/01/15 23:37:57 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/executors,null}
14/01/15 23:37:57 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/metrics/json,null}
14/01/15 23:37:57 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/static,null}
14/01/15 23:37:57 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/,null}
14/01/15 23:37:57 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040
14/01/15 23:37:57 INFO ui.SparkUI: Started Spark Web UI at http://10.0.1.10:4040
2014-01-15 23:37:57.929 java[34819:1020b] Unable to load realm mapping info from SCDynamicStore
14/01/15 23:37:58 INFO storage.MemoryStore: ensureFreeSpace(35456) called with curMem=0, maxMem=1278030643
14/01/15 23:37:58 INFO storage.MemoryStore: Block broadcast_0 stored as values to memory (estimated size 34.6 KB, free 1218.8 MB)
14/01/15 23:37:58 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
14/01/15 23:37:58 WARN snappy.LoadSnappy: Snappy native library not loaded
14/01/15 23:37:58 INFO mapred.FileInputFormat: Total input paths to process : 1
14/01/15 23:37:58 INFO spark.SparkContext: Starting job: foreach at HdfsTest.scala:30
14/01/15 23:37:58 INFO scheduler.DAGScheduler: Got job 0 (foreach at HdfsTest.scala:30) with 1 output partitions (allowLocal=false)
14/01/15 23:37:58 INFO scheduler.DAGScheduler: Final stage: Stage 0 (foreach at HdfsTest.scala:30)
14/01/15 23:37:58 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/01/15 23:37:58 INFO scheduler.DAGScheduler: Missing parents: List()
14/01/15 23:37:58 INFO scheduler.DAGScheduler: Submitting Stage 0 (MappedRDD[2] at map at HdfsTest.scala:27), which has no missing parents
14/01/15 23:37:58 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from Stage 0 (MappedRDD[2] at map at HdfsTest.scala:27)
14/01/15 23:37:58 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
14/01/15 23:37:58 INFO scheduler.TaskSetManager: Starting task 0.0:0 as TID 0 on executor localhost: localhost (PROCESS_LOCAL)
14/01/15 23:37:58 INFO scheduler.TaskSetManager: Serialized task 0.0:0 as 1778 bytes in 5 ms
14/01/15 23:37:58 INFO executor.Executor: Running task ID 0
14/01/15 23:37:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/01/15 23:37:58 INFO spark.CacheManager: Partition rdd_2_0 not found, computing it
14/01/15 23:37:58 INFO rdd.HadoopRDD: Input split: file:/Users/acozzi/Documents/workspace/spark-samples/pom.xml:0+4092
14/01/15 23:37:58 INFO storage.MemoryStore: ensureFreeSpace(2853) called with curMem=35456, maxMem=1278030643
14/01/15 23:37:58 INFO storage.MemoryStore: Block rdd_2_0 stored as values to memory (estimated size 2.8 KB, free 1218.8 MB)
14/01/15 23:37:58 INFO storage.BlockManagerMasterActor$BlockManagerInfo: Added rdd_2_0 in memory on 10.0.1.10:53683 (size: 2.8 KB, free: 1218.8 MB)
14/01/15 23:37:58 INFO storage.BlockManagerMaster: Updated info of block rdd_2_0
14/01/15 23:37:58 INFO executor.Executor: Serialized size of result for 0 is 525
14/01/15 23:37:58 INFO executor.Executor: Sending result for 0 directly to driver
14/01/15 23:37:58 INFO executor.Executor: Finished task ID 0
14/01/15 23:37:58 INFO scheduler.TaskSetManager: Finished TID 0 in 61 ms on localhost (progress: 0/1)
14/01/15 23:37:58 INFO scheduler.DAGScheduler: Completed ResultTask(0, 0)
14/01/15 23:37:58 INFO scheduler.TaskSchedulerImpl: Remove TaskSet 0.0 from pool 
14/01/15 23:37:58 INFO scheduler.DAGScheduler: Stage 0 (foreach at HdfsTest.scala:30) finished in 0.071 s
14/01/15 23:37:58 INFO spark.SparkContext: Job finished: foreach at HdfsTest.scala:30, took 0.151199 s
Iteration 1 took 189 ms
[WARNING] 
java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.mojo.exec.ExecJavaMojo$1.run(ExecJavaMojo.java:297)
	at java.lang.Thread.run(Thread.java:695)
Caused by: java.lang.IncompatibleClassChangeError: Implementing class
	at java.lang.ClassLoader.defineClass1(Native Method)
	at java.lang.ClassLoader.defineClassCond(ClassLoader.java:637)
	at java.lang.ClassLoader.defineClass(ClassLoader.java:621)
	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:141)
	at java.net.URLClassLoader.defineClass(URLClassLoader.java:283)
	at java.net.URLClassLoader.access$000(URLClassLoader.java:58)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:197)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:171)
	at org.apache.hadoop.mapred.SparkHadoopMapRedUtil$class.firstAvailableClass(SparkHadoopMapRedUtil.scala:48)
	at org.apache.hadoop.mapred.SparkHadoopMapRedUtil$class.newJobContext(SparkHadoopMapRedUtil.scala:23)
	at org.apache.hadoop.mapred.SparkHadoopWriter.newJobContext(SparkHadoopWriter.scala:40)
	at org.apache.hadoop.mapred.SparkHadoopWriter.getJobContext(SparkHadoopWriter.scala:149)
	at org.apache.hadoop.mapred.SparkHadoopWriter.preSetup(SparkHadoopWriter.scala:64)
	at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:713)
	at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:686)
	at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:572)
	at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:894)
	at org.apache.spark.examples.HdfsTest$$anonfun$main$1.apply$mcVI$sp(HdfsTest.scala:34)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:142)
	at org.apache.spark.examples.HdfsTest$.main(HdfsTest.scala:28)
	at org.apache.spark.examples.HdfsTest.main(HdfsTest.scala)
	... 6 more
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 3.224s
[INFO] Finished at: Wed Jan 15 23:37:58 PST 2014
[INFO] Final Memory: 12M/81M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.codehaus.mojo:exec-maven-plugin:1.2.1:java (default-cli) on project spark-samples: An exception occured while executing the Java class. null: InvocationTargetException: Implementing class -> [Help 1]


Alex Cozzi
alexcozzi@gmail.com

https://git-wip-us.apache.org/repos/asf?p=incubator-spark.git;a=commit;h=7348893f0edd96dacce2f00970db1976266f7008
https://repository.apache.org/content/repositories/orgapachespark-1001/
0.9.0-incubating!

"
Alex Cozzi <alexcozzi@gmail.com>,"Wed, 15 Jan 2014 23:43:24 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc1),dev@spark.incubator.apache.org,"Oh, I forgot: I am using the ìyarnî maven profile to target yarn 2.2

Alex Cozzi
alexcozzi@gmail.com

and I copied the HdfsTest.scala test, but I added a single line to save the file back to disk:
SparkContext.jarOfClass(this.getClass))
xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
http://maven.apache.org/maven-v4_0_0.xsd"">
<url>https://repository.apache.org/content/repositories/orgapachespark-1001</url>
<artifactId>spark-core_${scala.tools.version}</artifactId>
<artifactId>specs2_${scala.tools.version}</artifactId>
<artifactId>scalatest_${scala.tools.version}</artifactId>
<testSourceDirectory>src/test/scala</testSourceDirectory>
http://davidb.github.com/scala-maven-plugin -->
<artifactId>scala-maven-plugin</artifactId>
<scalaCompatVersion>2.10</scalaCompatVersion>
<jvmArg>-Xms128m</jvmArg>
<jvmArg>-Xmx2048m</jvmArg>
<goal>compile</goal>
<goal>testCompile</goal>
<arg>-make:transitive</arg>
<arg>-dependencyfile</arg>
<arg>${project.build.directory}/.scala_dependencies</arg>
<groupId>org.apache.maven.plugins</groupId>
<artifactId>maven-surefire-plugin</artifactId>
<disableXmlReport>true</disableXmlReport>
like NoDefClassError,... -->
<include>**/*Test.*</include>
<include>**/*Suite.*</include>
<artifactId>exec-maven-plugin</artifactId>
<goal>exec</goal>
<mainClass>org.apache.spark.examples.HdfsTest</mainClass>
<argument>local</argument>
<argument>pom.xml</argument>
the following error:
---
[jar:file:/Users/acozzi/.m2/repository/org/slf4j/slf4j-log4j12/1.6.1/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
[jar:file:/Users/acozzi/.m2/repository/org/slf4j/slf4j-simple/1.6.1/slf4j-simple-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
explanation.
addresses :[akka.tcp://spark@10.0.1.10:53682]
[akka.tcp://spark@10.0.1.10:53682]
directory at /var/folders/mm/4qxz27w91p96v2zp5f9ncmqm38ychm/T/spark-local-20140115233757-7a41
capacity 1218.8 MB.
53683 with id = ConnectionManagerId(10.0.1.10,53683)
BlockManager
storage.BlockManagerMasterActor$BlockManagerInfo: Registering block manager 10.0.1.10:53683 with 1218.8 MB RAM
BlockManager
SocketConnector@0.0.0.0:53684
started at http://10.0.1.10:53684
directory is /var/folders/mm/4qxz27w91p96v2zp5f9ncmqm38ychm/T/spark-e9304513-3714-430f-aa14-1a430a915d98
SocketConnector@0.0.0.0:53685
o.e.j.s.h.ContextHandler{/storage/rdd,null}
o.e.j.s.h.ContextHandler{/storage,null}
o.e.j.s.h.ContextHandler{/stages/stage,null}
o.e.j.s.h.ContextHandler{/stages/pool,null}
o.e.j.s.h.ContextHandler{/stages,null}
o.e.j.s.h.ContextHandler{/environment,null}
o.e.j.s.h.ContextHandler{/executors,null}
o.e.j.s.h.ContextHandler{/metrics/json,null}
o.e.j.s.h.ContextHandler{/static,null}
o.e.j.s.h.ContextHandler{/,null}
SelectChannelConnector@0.0.0.0:4040
http://10.0.1.10:4040
info from SCDynamicStore
called with curMem=0, maxMem=1278030643
as values to memory (estimated size 34.6 KB, free 1218.8 MB)
native-hadoop library for your platform... using builtin-java classes where applicable
loaded
process : 1
HdfsTest.scala:30
HdfsTest.scala:30) with 1 output partitions (allowLocal=false)
(foreach at HdfsTest.scala:30)
List()
(MappedRDD[2] at map at HdfsTest.scala:27), which has no missing parents
tasks from Stage 0 (MappedRDD[2] at map at HdfsTest.scala:27)
0.0 with 1 tasks
as TID 0 on executor localhost: localhost (PROCESS_LOCAL)
as 1778 bytes in 5 ms
locally
found, computing it
file:/Users/acozzi/Documents/workspace/spark-samples/pom.xml:0+4092
called with curMem=35456, maxMem=1278030643
values to memory (estimated size 2.8 KB, free 1218.8 MB)
storage.BlockManagerMasterActor$BlockManagerInfo: Added rdd_2_0 in memory on 10.0.1.10:53683 (size: 2.8 KB, free: 1218.8 MB)
block rdd_2_0
for 0 is 525
directly to driver
ms on localhost (progress: 0/1)
0)
from pool 
HdfsTest.scala:30) finished in 0.071 s
HdfsTest.scala:30, took 0.151199 s
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
org.codehaus.mojo.exec.ExecJavaMojo$1.run(ExecJavaMojo.java:297)
java.security.SecureClassLoader.defineClass(SecureClassLoader.java:141)
org.apache.hadoop.mapred.SparkHadoopMapRedUtil$class.firstAvailableClass(SparkHadoopMapRedUtil.scala:48)
org.apache.hadoop.mapred.SparkHadoopMapRedUtil$class.newJobContext(SparkHadoopMapRedUtil.scala:23)
org.apache.hadoop.mapred.SparkHadoopWriter.newJobContext(SparkHadoopWriter.scala:40)
org.apache.hadoop.mapred.SparkHadoopWriter.getJobContext(SparkHadoopWriter.scala:149)
org.apache.hadoop.mapred.SparkHadoopWriter.preSetup(SparkHadoopWriter.scala:64)
org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:713)
org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:686)
org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:572)
org.apache.spark.examples.HdfsTest$$anonfun$main$1.apply$mcVI$sp(HdfsTest.scala:34)
scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:142)
------------------------------------------------------------------------
------------------------------------------------------------------------
------------------------------------------------------------------------
org.codehaus.mojo:exec-maven-plugin:1.2.1:java (default-cli) on project spark-samples: An exception occured while executing the Java class. null: InvocationTargetException: Implementing class -> [Help 1]
https://git-wip-us.apache.org/repos/asf?p=incubator-spark.git;a=commit;h=7348893f0edd96dacce2f00970db1976266f7008
at:
https://repository.apache.org/content/repositories/orgapachespark-1001/
0.9.0-incubating!

"
Pillis Work <pillis.work@gmail.com>,"Thu, 16 Jan 2014 01:37:10 -0800",Re: About Spark job web ui persist(JIRA-969),dev@spark.incubator.apache.org,"Hello,
I wanted to write down at a high level the changes I was thinking of.
Please feel free to critique and suggest changes.

SparkContext:
SparkContext start will not be starting UI anymore. Rather it will launch a
SparkContextObserver (has SparkListener trait) which will generate a
SparkContextData instance. SparkContextObserver keeps SparkContextData
uptodate. SparkContextData will have all the historical information anyone
needs. Stopping a SparkContext stops the SparkContextObserver.

SparkContextData:
Has all historical information of a SparkContext run. Periodically persists
itself to disk as JSON. Can hydrate itself from the same JSON.
SparkContextDatas are created without any UI usage. SparkContextData can
evolve independently of what UI needs - like having non-UI data needed for
third party integration.

SparkUI:
No longer needs SparkContext. Will need an array of SparkContextDatas
(either by polling folder or other means). UI pages at render time will
access appropriate SparkContextData and produce HTML. SparkUI can be
started and stopped independently of SparkContexts. Multiple SparkContexts
can be shown in UI.

I have purposefully not gone into much detail. Please let me know if any
piece needs to be elaborated.
Regards,
Pillis





"
Eduardo Costa Alfaia <e.costaalfaia@studenti.unibs.it>,"Thu, 16 Jan 2014 11:03:13 +0100",Re: JavaNetworkWordCount Researches,dev@spark.incubator.apache.org,"Hi Tathagata,
Thank you very much by the explain.
Another curiosity is that I did some tests with this code yesterday where I used three machines like worker and I can see that one these machines have had the RAM memory increased, about 90% in use,  in compare the others this  hasnít changed drastically and in this same machine I can see that the parts of file, in this case I am using the book Don Quixote in txt,  are save in hard disk specifically in /tmp/spark-local<numbers> increasing the used space. Sorry by the severals questions I am a newer in Stream processing and I looking for understand better how to work Spark DStream.

Best Regards

:

s
t
d
g
s
uso


-- 
---
INFORMATIVA SUL TRATTAMENTO DEI DATI PERSONALI

I dati utilizzati per l'invio del presente messaggio sono trattati 
dall'Universit‡ degli Studi di Brescia esclusivamente per finalit‡ 
istituzionali. Informazioni pi˘ dettagliate anche in ordine ai diritti 
dell'interessato sono riposte nell'informativa generale e nelle notizie 
pubblicate sul sito web dell'Ateneo nella sezione ""Privacy"".

Il contenuto di questo messaggio Ë rivolto unicamente alle persona cui 
Ë indirizzato e puÚ contenere informazioni la cui riservatezza Ë 
tutelata legalmente. Ne sono vietati la riproduzione, la diffusione e l'uso 
in mancanza di autorizzazione del destinatario. Qualora il messaggio 
fosse pervenuto per errore, preghiamo di eliminarlo.

"
Eduardo Costa Alfaia <e.costaalfaia@studenti.unibs.it>,"Thu, 16 Jan 2014 11:37:35 +0100",Changing print(),dev@spark.incubator.apache.org,"Hi guys,

How could I change the def print() to print more lines in JavaNetworkWordCount.java?

Thanks all
-- 
---
INFORMATIVA SUL TRATTAMENTO DEI DATI PERSONALI

I dati utilizzati per l'invio del presente messaggio sono trattati 
dall'Universit‡ degli Studi di Brescia esclusivamente per finalit‡ 
istituzionali. Informazioni pi˘ dettagliate anche in ordine ai diritti 
dell'interessato sono riposte nell'informativa generale e nelle notizie 
pubblicate sul sito web dell'Ateneo nella sezione ""Privacy"".

Il contenuto di questo messaggio Ë rivolto unicamente alle persona cui 
Ë indirizzato e puÚ contenere informazioni la cui riservatezza Ë 
tutelata legalmente. Ne sono vietati la riproduzione, la diffusione e l'uso 
in mancanza di autorizzazione del destinatario. Qualora il messaggio 
fosse pervenuto per errore, preghiamo di eliminarlo.

"
"""Xia, Junluan"" <junluan.xia@intel.com>","Thu, 16 Jan 2014 16:15:47 +0000",RE: About Spark job web ui persist(JIRA-969),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi Pillis

It sound goods
1. For SparkContextData, I think we could persist in HDFS not in local disk(one SparkUI service may show more than one sparkcontext)
2. we also could consider SparkContextData as one metrics input(MetricsSource), for long running spark job, SparkContextData will shown in ganglia/jmx .....
3. if we persist SparkContextData periodically, we need to rewrite the UI logic as spark ui now just show one timestamp information.

I wanted to write down at a high level the changes I was thinking of.
Please feel free to critique and suggest changes.

SparkContext:
SparkContext start will not be starting UI anymore. Rather it will launch a SparkContextObserver (has SparkListener trait) which will generate a SparkContextData instance. SparkContextObserver keeps SparkContextData uptodate. SparkContextData will have all the historical information anyone needs. Stopping a SparkContext stops the SparkContextObserver.

SparkContextData:
Has all historical information of a SparkContext run. Periodically persists itself to disk as JSON. Can hydrate itself from the same JSON.
SparkContextDatas are created without any UI usage. SparkContextData can evolve independently of what UI needs - like having non-UI data needed for third party integration.

SparkUI:
No longer needs SparkContext. Will need an array of SparkContextDatas (either by polling folder or other means). UI pages at render time will access appropriate SparkContextData and produce HTML. SparkUI can be started and stopped independently of SparkContexts. Multiple SparkContexts can be shown in UI.

I have purposefully not gone into much detail. Please let me know if any piece needs to be elaborated.
Regards,
Pillis




:

s.
h
as needed.
.
h
r

"
Pillis Work <pillis.work@gmail.com>,"Thu, 16 Jan 2014 09:28:15 -0800",Re: About Spark job web ui persist(JIRA-969),dev@spark.incubator.apache.org,"Hi Junluan,
1. Yes, we could persist to HDFS or any FS. I think at a minimum we should
persist it to local disk - keeps the core simple.
We can think of HDFS interactions as level-2 functionality that can be
implemented once we have a good local implementation. The
persistence/hydration layer of a SparkContextData can be made pluggable as
a next step.
Also, as mentioned in previous mail, SparkUI will now show multiple
SparkContexts using data from SparkContextDatas.

2. Yes, we could

3. Yes, SparkUI will need a rewrite to deal with SparkContextDatas (either
live, or hydrated from historical JSONs).
Regards





"
Patrick Wendell <pwendell@gmail.com>,"Thu, 16 Jan 2014 09:39:04 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc1),dev@spark.incubator.apache.org,"Hey Alex,

Thanks for testing out this rc. Would you mind forking this into a
different thread so we can discuss there?

Also, does your application build and run correctly with spark 0.8.1? That
would determine whether the problem is specifically with this rc...

Patrick

---
sent from my phone

d
d>
d>
e
--
j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
-simple-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
y
57-7a41
:
d
-aa14-1a430a915d98
re
d
:
)
k
s
0)
:39)
mpl.java:25)
7)
SparkHadoopMapRedUtil.scala:48)
adoopMapRedUtil.scala:23)
r.scala:40)
r.scala:149)
la:64)
s.scala:713)
cala:686)
cala:572)
.scala:34)
2)
:
t;h=7348893f0edd96dacce2f00970db1976266f7008
/
!
"
Patrick Wendell <pwendell@gmail.com>,"Thu, 16 Jan 2014 10:43:45 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc1),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I also ran your example locally and it worked with 0.8.1 and
0.9.0-rc1. So it's possible somehow you are pulling in an older
version if Spark or an incompatible version of Hadoop.

- Patrick

:
ent
t
nd
e file
-1001</url>
g>
d>
t>
he
lf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
f4j-simple-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
ry
33757-7a41
o:
ed
y
30f-aa14-1a430a915d98
s
where
ts
0
s
,
ed
o:
 MB)
ck
r
y
ms
ava:39)
orImpl.java:25)
)
ss(SparkHadoopMapRedUtil.scala:48)
rkHadoopMapRedUtil.scala:23)
iter.scala:40)
iter.scala:149)
scala:64)
ions.scala:713)
s.scala:686)
s.scala:572)
est.scala:34)
--
--
--
t
ll:
e:
ommit;h=7348893f0edd96dacce2f00970db1976266f7008
:
1/
g!

"
Kay Ousterhout <keo@eecs.berkeley.edu>,"Thu, 16 Jan 2014 11:35:41 -0800",GC tuning for Spark,dev@spark.incubator.apache.org,"Hi all,

I'm finding that Java GC can be a major performance bottleneck when running
Spark at high (>50% or so) memory utilization.  What GC tuning have people
tried for Spark and how effective has it been?

Thanks!

Kay
"
Tathagata Das <tathagata.das1565@gmail.com>,"Thu, 16 Jan 2014 11:47:26 -0800",Re: JavaNetworkWordCount Researches,dev@spark.incubator.apache.org,"Hi Eduardo,

If the streaming data is sent to Worker X, then the data is stored in the
memory of Worker X and another worker Y. if replication is disabled through
the StorageLevel in the input stream, then only worker X. That is why you
could be seeing one the machines have a high memory usage. The data is
essentially stored as RDDs within the memory of the Spark's worker
processes and by default, the used data is thrown out using Spark's default
LRU method. If you want to reduce the memory consumption within Spark's
storage memory, then there are two options.
1. You can set the Java property ""spark.cleaner.ttl"" appropriately (see Spark
configuration<http://spark.incubator.apache.org/docs/latest/configuration.html>).
This forces all data persisted in memory to be cleaned up. But this needs
to be set conservative so that you dont accidentally clear up data that
havent been process yet.
2. In the latest code of Spark Streaming (master branch of the Spark
repo<https://github.com/apache/incubator-spark/>),
if you set the property spark.streaming.unpersist, then the Spark Streaming
will automatically throw away data that is not needed any more.

Hopefully this clarifies the doubts. Feel free to ask more questions.

TD



rs
t
ng
n
y
 }
 }
ti
e
ui
so
"
Tathagata Das <tathagata.das1565@gmail.com>,"Thu, 16 Jan 2014 11:54:52 -0800",Re: GC tuning for Spark,dev@spark.incubator.apache.org,"There are a bunch of tricks noted in the Tuning
Guide<http://spark.incubator.apache.org/docs/latest/tuning.html#memory-tuning>.
You may have seen them already but I thought its still worth mentioning for
the records.

Besides those, if you are concerned about consistent latency (that is, low
variability in the job processing times), then using
concurrent-mark-and-sweep GC is recommended. Instead of big stop-the-world
GC pauses, there are many smaller pauses. This reduction in variability
comes at the cost of processing throughput though, so thats a tradeoff.

TD



"
Mark Hamstra <mark@clearstorydata.com>,"Thu, 16 Jan 2014 12:30:04 -0800",Re: GC tuning for Spark,dev@spark.incubator.apache.org,"And, of course, there are the bigger-hammer-than-GC-tuning approaches using
some combination of unchecked, off-heap and Tachyon.



"
Binh Nguyen <ngbinh@gmail.com>,"Thu, 16 Jan 2014 12:34:02 -0800",Re: GC tuning for Spark,dev@spark.incubator.apache.org,"I think incorporating https://github.com/amplab/tachyon/wiki is a better
solution. I remembered Matei has said that it was in his plan but not sure
about the ETA for it to happen.






-- 

Binh Nguyen
"
Patrick Wendell <pwendell@gmail.com>,"Thu, 16 Jan 2014 13:05:25 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc1),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I'll kick this vote off with a +1.

e:
te:
rent
at
2
and
he file
k-1001</url>
y>
g>
rg>
e
Id>
l>
nt>
the
slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
lf4j-simple-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
ory
233757-7a41
t
fo:
ted
ry
430f-aa14-1a430a915d98
g
as
 where
:
)
nts
.0
as
0
d,
led
fo:
8 MB)
ock
or
ly
 ms
,
0
java:39)
sorImpl.java:25)
1)
ass(SparkHadoopMapRedUtil.scala:48)
arkHadoopMapRedUtil.scala:23)
riter.scala:40)
riter.scala:149)
.scala:64)
tions.scala:713)
ns.scala:686)
ns.scala:572)
Test.scala:34)
---
---
---
ct
ull:
te:
commit;h=7348893f0edd96dacce2f00970db1976266f7008
t:
01/
ng!

"
Alex Cozzi <alexcozzi@gmail.com>,"Thu, 16 Jan 2014 13:13:10 -0800",testing 0.9.0-incubating and maven,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi Patrick,
thank you for testing. I think I found out what is wrong: I am trying to build my own examples that also depend on another library which in turns depends on hadoop 2.2.
what was happening is that my library brings in hadoop 2.2, while spark depends on hadoop 1.04 and then I think I get conflict versions of the classes. 

A couple of things are not clear to me:

1: do the published artifacts support YARN and hadoop 2.2 or will I need to make my own build? 
2: if they do, how do I activate the profiles in my maven config? I tried mvn -Pyarn compile but it does not work (maven says ì[WARNING] The requested profile ""yarn"" could not be activated because it does not exist.î)


essentially I would like to specify the spark dependencies as:

<dependencies>
		<dependency>
			<groupId>org.scala-lang</groupId>
			<artifactId>scala-library</artifactId>
			<version>${scala.version}</version>
		</dependency>

		<dependency>
			<groupId>org.apache.spark</groupId>
			<artifactId>spark-core_${scala.tools.version}</artifactId>
			<version>0.9.0-incubating</version>
		</dependency>

and tell maven to use the ìyarnî profile for this dependency, but I do not seem to be able to make it work. 
Anybody has any suggestion?

Alex"
Patrick Wendell <pwendell@gmail.com>,"Thu, 16 Jan 2014 13:54:39 -0800",Re: testing 0.9.0-incubating and maven,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey Alex,

Maven profiles only affect the Spark build itself. They do not
transitively affect your own build.

Checkout the docs for how to deploy applications on yarn:
http://spark.incubator.apache.org/docs/latest/running-on-yarn.html

When compiling your application, just should explicitly add the hadoop
version you depend on to your own build (e.g. a hadoop-client
dependency). Take a look at the example here where we show adding
hadoop-client:

http://spark.incubator.apache.org/docs/latest/quick-start.html

When deploying Spark applications on YARN, you actually want to mark
spark as a provided dependency in your application's maven and bundle
your application as an assembly jar, then submit it with a Spark YARN
bundle to a YARN cluster. The instructions are the same as they were
in 0.8.1.

For the spark jar you want to submit to YARN, you can download the
precompiled Spark one.

It might make sense to try this pipeline with 0.8.1 and get it working
there. It sounds here more like you are dealing with getting the build
set-up rather than a particular issue with the 0.9.0 RC.

- Patrick

build my own examples that also depend on another library which in turns depends on hadoop 2.2.
epends on hadoop 1.04 and then I think I get conflict versions of the classes.
to make my own build?
 mvn -Pyarn compile but it does not work (maven says ì[WARNING] The requested profile ""yarn"" could not be activated because it does not exist.î)
tifactId>
o not seem to be able to make it work.

"
Alex Cozzi <alexcozzi@gmail.com>,"Thu, 16 Jan 2014 14:12:36 -0800",Re: testing 0.9.0-incubating and maven,dev@spark.incubator.apache.org,"Thanks for the help. I am doing progress, but I found I need to do a bit of fiddling with excluding dependencies from spark in order to have mine take effect. As soon as I have a working pom I will post here as an example. 

Alex Cozzi
alexcozzi@gmail.com
------------------------------------------------------
eBay is hiring! Check out our job openings
http://ebay.referrals.selectminds.com/?et=OlVHMHJl


to build my own examples that also depend on another library which in turns depends on hadoop 2.2.
spark depends on hadoop 1.04 and then I think I get conflict versions of the classes.
need to make my own build?
tried mvn -Pyarn compile but it does not work (maven says ì[WARNING] The requested profile ""yarn"" could not be activated because it does not exist.î)
<artifactId>spark-core_${scala.tools.version}</artifactId>
I do not seem to be able to make it work.


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Thu, 16 Jan 2014 14:14:40 -0800",SparkR developer release,"dev@spark.incubator.apache.org, user@spark.incubator.apache.org","I'm happy to announce the developer preview of SparkR, an R frontend
for Spark. SparkR presents Spark's API in R and allows you to write
code in R and run the computation on a Spark cluster. You can try out
SparkR today by installing it from our github repo at
https://github.com/amplab-extras/SparkR-pkg .

Right now SparkR is available as a standalone package that can be
installed to run on an existing Spark installation. Note that SparkR
requires Spark >= 0.9 and the default build uses the recent 0.9
release candidate. In the future we will consider merging this with
Apache Spark.

More details about SparkR and examples of SparkR code can be found at
http://amplab-extras.github.io/SparkR-pkg. I would like to thank
Zongheng Yang, Matei Zaharia and Matt Massie for their contributions
and help in developing SparkR.

Comments and pull requests are welcome on github.

Thanks
Shivaram

"
Chester Chen <chesterxgchen@yahoo.com>,"Thu, 16 Jan 2014 14:38:39 -0800",Re: SparkR developer release,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","This is something I am looking for, definitely will take a look
Chester

Sent from my iPhone



"
"""=?utf-8?B?YW5keS5wZXRyZWxsYUBnbWFpbC5jb20=?="" <andy.petrella@gmail.com>","Thu, 16 Jan 2014 23:47:02 +0100",=?utf-8?B?UmUgOiBTcGFya1IgZGV2ZWxvcGVyIHJlbGVhc2U=?=,"shivaram@eecs.berkeley.edu,""=?utf-8?B?ZGV2QHNwYXJrLmluY3ViYXRvci5hcGFjaGUub3Jn?="" <dev@spark.incubator.apache.org>,""=?utf-8?B?dXNlckBzcGFyay5pbmN1YmF0b3IuYXBhY2hlLm9yZw==?="" <user@spark.incubator.apache.org>","Cool that's awesome and something I'll surely investigate in the coming weeks.
Great job!

Envoy√© depuis mon HTC

----- Reply message -----
De : ""Shivaram Venkataraman"" <shivaram@eecs.berkeley.edu>
Pour¬†: <dev@spark.incubator.apache.org>, <user@spark.incubator.apache.org>
Cc : ""Zongheng Yang"" <zongheng.y@gmail.com>, ""Matthew L Massie"" <massie@berkeley.edu>
Objet : SparkR developer release
Date : jeu., janv. 16, 2014 23:14


I'm happy to announce the developer preview of SparkR, an R frontend
for Spark. SparkR presents Spark's API in R and allows you to write
code in R and run the computation on a Spark cluster. You can try out
SparkR today by installing it from our github repo at
https://github.com/amplab-extras/SparkR-pkg .

Right now SparkR is available as a standalone package that can be
installed to run on an existing Spark installation. Note that SparkR
requires Spark >= 0.9 and the default build uses the recent 0.9
release candidate. In the future we will consider merging this with
Apache Spark.

More details about SparkR and examples of SparkR code can be found at
http://amplab-extras.github.io/SparkR-pkg. I would like to thank
Zongheng Yang, Matei Zaharia and Matt Massie for their contributions
and help in developing SparkR.

Comments and pull requests are welcome on github.

Thanks
Shivaram
"
Matei Zaharia <matei.zaharia@gmail.com>,"Thu, 16 Jan 2014 15:23:41 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc1),dev@spark.incubator.apache.org,"+1 for me as well.

I built and tested this on Mac OS X, and looked through the new docs.

Matei


https://git-wip-us.apache.org/repos/asf?p=incubator-spark.git;a=commit;h=7348893f0edd96dacce2f00970db1976266f7008
https://repository.apache.org/content/repo"
Reynold Xin <rxin@databricks.com>,"Thu, 16 Jan 2014 15:33:31 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc1),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>",1
Raja Pasupuleti <rajag.pasupuleti@gmail.com>,"Thu, 16 Jan 2014 19:12:42 -0500",Re: SparkR developer release,"dev@spark.incubator.apache.org, shivaram@eecs.berkeley.edu","Nice!



"
Ali Ghodsi <alig@cs.berkeley.edu>,"Thu, 16 Jan 2014 16:33:47 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc1),dev@spark.incubator.apache.org,"+1

Builds fine on Maverick, runs tests, spark-shell, sbt assembly, maven
build, etc.

--Ali



"
Pillis Work <pillis.work@gmail.com>,"Thu, 16 Jan 2014 17:07:12 -0800",Re: About Spark job web ui persist(JIRA-969),dev@spark.incubator.apache.org,"Hello,
If changes are acceptable, I would like to request assignment of JIRA to me
for implementation.
Regards
pillis



"
Henry Saputra <henry.saputra@gmail.com>,"Thu, 16 Jan 2014 20:17:43 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc1),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","NOTICE and LICENSE look good.

No more executable binaries/jars packaged with source - good.

Hashes look good.
Signatures look good.
Untar and run sbt compile, assembly, test.
Run some examples in local and ran ok.

+1

- Henry



"
"""Xia, Junluan"" <junluan.xia@intel.com>","Fri, 17 Jan 2014 05:51:50 +0000",RE: About Spark job web ui persist(JIRA-969),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi pillis

Do you mind to submit more detail design document? May contain
1. what data structures will be exposed to external UI/metrics/third party
2. what new UI looks like if you want to persist SparkContextData periodically
3. .........

Any other suggestions?

If changes are acceptable, I would like to request assignment of JIRA to me for implementation.
Regards
pillis



e:
parkContextObserver.
s.
a.
a.
s

"
Sean McNamara <Sean.McNamara@Webtrends.com>,"Fri, 17 Jan 2014 05:51:47 +0000",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc1),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","+1

Compiled and ran fine on Mavericks and Precise.

Sean



t;h=


"
"""Kk.gmail"" <khanderao.kand@gmail.com>","Fri, 17 Jan 2014 03:36:51 -0800",Re: SparkR developer release,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Great news . A lot of data scientists are looking for this .




"
Jay <hjayin@gmail.com>,"Fri, 17 Jan 2014 21:21:56 +0800",Re: The performance of group operation on SSD,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","OS memory cache??

Sent from my iPad.

n.cj@gmail.com> –¥µ¿£∫

"
Tom Graves <tgraves_cs@yahoo.com>,"Fri, 17 Jan 2014 07:20:55 -0800 (PST)",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc1),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","+1. †

verified sigs/hashes, NOTICE, LICENSE
Built from source on redhat linux using maven.
Ran some tests on secure Hadoop YARN 0.23 and 2.esday, January 15, 2014 7:49 PM, Patrick Wendell <pwendell@gmail.com> wrote:
 
Please vote on releasing the followi"
Tom Graves <tgraves_cs@yahoo.com>,"Fri, 17 Jan 2014 07:53:37 -0800 (PST)",spark compatibility between releases?,Dev <dev@spark.incubator.apache.org>,"Hey everyone,

What is the story as far as Spark compatibility between releases? †Are we saying recompile needed until we get to 1.X type releases? †

It might be a good idea for us to document our compatibility strategy going forward.†

I had someone say they built against master and tried to run on 0.8 and were getting errors. (invalid type code: AC - when doing†matrix qr)

Thanks,
Tom"
Andrew Ash <andrew@andrewash.com>,"Fri, 17 Jan 2014 08:01:10 -0800",Re: The performance of group operation on SSD,dev@spark.incubator.apache.org,"Are there different amounts of RAM on the SSD machines vs the Spinny disk
machines?

Sent from my mobile phone

6:04ÔºåChen Jin <karen.cj@gmail.com> ÂÜôÈÅìÔºö
"
Matei Zaharia <matei.zaharia@gmail.com>,"Fri, 17 Jan 2014 11:20:36 -0800",Re: spark compatibility between releases?,Tom Graves <tgraves_cs@yahoo.com>,"Hi Tom,

So far youíve had to recompile with each release. Between master and 0.8 in particular, one problem is that the Scala version changed, which will always require a recompile in Scala programs.

In the future, we want to make minor releases not require a recompile.

Matei


we saying recompile needed until we get to 1.X type releases?  
going forward. 
and were getting errors. (invalid type code: AC - when doing matrix qr)


"
Matei Zaharia <matei.zaharia@gmail.com>,"Fri, 17 Jan 2014 11:22:25 -0800",Re: The performance of group operation on SSD,dev@spark.incubator.apache.org,"ext3, which is the default on ephemeral disks on EC2, scales very poorly to multicore workloads. We recommend reformatting those as XFS (which is very fast to format) or ext4 (which unfortunately takes a few hours to finalize). Maybe there are also other FS options that affect SSDs.

Matei


disk
6:04ÔºåChen Jin <karen.cj@gmail.com> ÂÜôÈÅìÔºö
performance
least


"
Matei Zaharia <matei.zaharia@gmail.com>,"Fri, 17 Jan 2014 11:23:31 -0800",Re: The performance of group operation on SSD,dev@spark.incubator.apache.org,"Also I should say with this that 1.0 will stay on Scala 2.10, and more generally I think we want to keep having releases for Scala 2.10 at least for this year. It should be easier to cross-build future releases for both 2.10 and 2.11 than it was with the 2.9 -> 2.10 jump.

Matei


ext3, which is the default on ephemeral disks on EC2, scales very poorly to multicore workloads. We recommend reformatting those as XFS (which is very fast to format) or ext4 (which unfortunately takes a few hours to finalize). Maybe there are also other FS options that affect SSDs.
disk
6:04ÔºåChen Jin <karen.cj@gmail.com> ÂÜôÈÅìÔºö
performance
least


"
Evan Chan <ev@ooyala.com>,"Fri, 17 Jan 2014 12:17:44 -0800",Re: Is there any plan to develop an application level fair scheduler?,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","What is the reason that standalone mode doesn't support the fair scheduler?
Does that mean that Mesos coarse mode also doesn't support the fair
scheduler?


te:

he


-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

<http://www.ooyala.com/>
<http://www.facebook.com/ooyala><http://www.linkedin.com/company/ooyala><http://www.twitter.com/ooyala>
"
Evan Chan <ev@ooyala.com>,"Fri, 17 Jan 2014 12:19:53 -0800",Re: spark code formatter?,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","BTW, we also run Scalariform, but we don't turn it on automatically.  We
find that for the most part it is good, but there are a few places where it
reformats things and doesn't look good, and requires cleanup.  I think
Scalariform requires some more rules to make it more generally useful.

-Evan







-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

<http://www.ooyala.com/>
<http://www.facebook.com/ooyala><http://www.linkedin.com/company/ooyala><http://www.twitter.com/ooyala>
"
Reynold Xin <rxin@databricks.com>,"Fri, 17 Jan 2014 12:21:21 -0800",Re: Is there any plan to develop an application level fair scheduler?,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","It does.

There are two scheduling levels here.

The first level is what the cluster manager does. The standalone cluster
manager for Spark only supports FIFO at the moment at the level of
applications.

Regarding Spark itself. Within a single Spark application, both FIFO and
fair scheduling are supported, regardless of what your cluster manager is




r?
it
er
:
r
 the
"
"""Kk.gmail"" <khanderao.kand@gmail.com>","Fri, 17 Jan 2014 12:22:57 -0800",Re: The performance of group operation on SSD,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","This would be good to support backward and forward versions.




e:
erally I think we want to keep having releases for Scala 2.10 at least for this year. It should be easier to cross-build future releases for both 2.10 and 2.11 than it was with the 2.9 -> 2.10 jump.
te:
t3, which is the default on ephemeral disks on EC2, scales very poorly to multicore workloads. We recommend reformatting those as XFS (which is very fast to format) or ext4 (which unfortunately takes a few hours to finalize). Maybe there are also other FS options that affect SSDs.
k
6:04ÔºåChen Jin <karen.cj@gmail.com> ÂÜôÈÅìÔºö


"
Christopher Nguyen <ctn@adatao.com>,"Fri, 17 Jan 2014 13:27:22 -0800",Re: The performance of group operation on SSD,dev@spark.incubator.apache.org,"Chen, I would also look at actual I/O patterns of the operations. SSDs
writes are sensitive to significantly variable performance depending on the
exact scenario, and can easily underperform HDD given the ""right""
conditions. Generically quoted IOPS numbers are not reliable across a
variety of commonly occurring workloads.

http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/i2-instances.html
http://www.diffen.com/difference/HDD_vs_SSD
http://www.storagesearch.com/problem-write-iops.html
http://www.dhtusa.com/media/SSDBench.pdf



--
Christopher T. Nguyen
Co-founder & CEO, Adatao <http://adatao.com>
linkedin.com/in/ctnguyen



ote:

h
ext3,
y
à6:04ÔºåChen Jin <karen.cj@gmail.com> ÂÜôÈÅìÔºö
e
st
"
Matei Zaharia <matei.zaharia@gmail.com>,"Fri, 17 Jan 2014 13:35:20 -0800",Re: Is there any plan to develop an application level fair scheduler?,dev@spark.incubator.apache.org,"Yup, this page describes the two levels of scheduling (within and across applications): http://spark.incubator.apache.org/docs/latest/job-scheduling.html



cluster
and
is
scheduler?
<matei.zaharia@gmail.com
But it
cluster
scheduling.
Fair
share the
<http://www.facebook.com/ooyala><http://www.linkedin.com/company/ooyala><


"
Michael Armbrust <michael@databricks.com>,"Fri, 17 Jan 2014 13:39:31 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc1),dev@spark.incubator.apache.org,"+1

Ran all catalyst tests against the RC using the staging repo.  Everything
looks good.



"
Andy Konwinski <andykonwinski@gmail.com>,"Fri, 17 Jan 2014 13:45:45 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc1),Michael Armbrust <michael@databricks.com>,"+1

Successfully built on mavericks. Great stuff.

"
Mark Hamstra <mark@clearstorydata.com>,"Fri, 17 Jan 2014 13:45:45 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc1),dev@spark.incubator.apache.org,"+1 LGTM



"
Hossein <falaki@gmail.com>,"Fri, 17 Jan 2014 19:02:47 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc1),dev@spark.incubator.apache.org,"+1

I compiled and tested the jar files in another project.

--Hossein

--Hossein



"
Mridul Muralidharan <mridul@gmail.com>,"Sun, 19 Jan 2014 00:40:47 +0530",Config properties broken in master,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi,

  Unless I am mistaken, the change to using typesafe ConfigFactory has
broken some of the system properties we use in spark.

For example: if we have both
-Dspark.speculation=true -Dspark.speculation.multiplier=0.95
set, then the spark.speculation property is dropped.

The rules of parseProperty actually document this clearly [1]


I am not sure what the right fix here would be (other than replacing
use of config that is).

Any thoughts ?
I would vote -1 for 0.9 to be released before this is fixed.


Regards,
Mridul


[1] http://typesafehub.github.io/config/latest/api/com/typesafe/config/ConfigFactory.html#parseProperties%28java.util.Properties,%20com.typesafe.config.ConfigParseOptions%29

"
Mridul Muralidharan <mridul@gmail.com>,"Sun, 19 Jan 2014 00:44:55 +0530",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc1),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I would vote -1 for this release until we resolve config property
issue [1] : if there is a known resolution for this (which I could not
find unfortunately, apologies if it exists !), then will change my
vote.

Thanks,
Mridul


[1] http://apache-spark-developers-list.1001551.n3.nabble.com/Config-properties-broken-in-master-td208.html


"
Nan Zhu <zhunanmcgill@gmail.com>,"Sat, 18 Jan 2014 14:21:08 -0500",Re: Config properties broken in master,dev@spark.incubator.apache.org,"change spark.speculation to spark.speculation.switch?

maybe we can restrict that all properties in Spark should be ""three levels""



"
Mridul Muralidharan <mridul@gmail.com>,"Sun, 19 Jan 2014 01:00:27 +0530",Re: Config properties broken in master,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi,

  Speculation was an example, there are others in spark which are
affected by this ...
Some of them have been around for a while, so will break existing code/scripts.

Regards,
Mridul


"
Matei Zaharia <matei.zaharia@gmail.com>,"Sat, 18 Jan 2014 11:36:08 -0800",Re: Config properties broken in master,dev@spark.incubator.apache.org,"This is definitely an important issue to fix. Instead of renaming properties, one solution would be to replace Typesafe Config with just reading Java system properties, and disable config files for this release. I kind of like that over renaming.

Matei


code/scripts.
levels""
has
http://typesafehub.github.io/config/latest/api/com/typesafe/config/ConfigFactory.html#parseProperties%28java.util.Properties,%20com.typesafe.config.ConfigParseOptions%29


"
Reynold Xin <rxin@databricks.com>,"Sat, 18 Jan 2014 11:40:23 -0800",Re: Config properties broken in master,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I also just went over the config options to see how pervasive this is. In
addition to speculation, there is one more ""conflict"" of this kind:

spark.locality.wait
spark.locality.wait.node
spark.locality.wait.process
spark.locality.wait.rack


spark.speculation
spark.speculation.interval
spark.speculation.multiplier
spark.speculation.quantile



"
Mark Hamstra <mark@clearstorydata.com>,"Sat, 18 Jan 2014 12:01:51 -0800",Re: Config properties broken in master,dev@spark.incubator.apache.org,"Really?  Disabling config files seems to me to be a bigger/more onerous
change for users than spark.speculation=true|false =>
spark.speculation.enabled=true|false and spark.locality.wait =>
spark.locality.wait.default.



"
Matei Zaharia <matei.zaharia@gmail.com>,"Sat, 18 Jan 2014 12:11:25 -0800",Re: Config properties broken in master,dev@spark.incubator.apache.org,"We can add config files in a later release. They were never officially released, and were only in master for about a month.

limited anyway. Users will want to have a separate config file with each app, which they have to ship with its classpath, and thereís no great way of merging them in the current setup. Iím not actually sure itís a feature we want to support, compared to say just a SparkConf.fromFile method that reads a Java Properties file.

Matei


onerous
just
release.
<mridul@gmail.com
has
replacing
http://typesafehub.github.io/config/latest/api/com/typesafe/config/ConfigFactory.html#parseProperties%28java.util.Properties,%20com.typesafe.config.ConfigParseOptions%29


"
Patrick Wendell <pwendell@gmail.com>,"Sat, 18 Jan 2014 12:14:04 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc1),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Mridul, thanks a *lot* for pointing this out. This is indeed an issue
and something which warrants cutting a new RC.

- Patrick


"
Mridul Muralidharan <mridul@gmail.com>,"Sun, 19 Jan 2014 01:44:28 +0530",Re: Config properties broken in master,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","IMO we should shoot for more stable interfaces and not break them just
to workaround bugs - unless the benefit of breaking compatibility is
offset by the added functionality.
Since I was not around for a while, I am not sure how much config file
feature was requested ...

Regards,
Mridul


"
Matei Zaharia <matei.zaharia@gmail.com>,"Sat, 18 Jan 2014 12:16:55 -0800",Re: Config properties broken in master,dev@spark.incubator.apache.org,"Yeah, this is exactly my reasoning as well.

Matei


onerous
just
release.
""three
<mridul@gmail.com
has
replacing
http://typesafehub.github.io/config/latest/api/com/typesafe/config/ConfigFactory.html#parseProperties%28java.util.Properties,%20com.typesafe.config.ConfigParseOptions%29


"
Mark Hamstra <mark@clearstorydata.com>,"Sat, 18 Jan 2014 12:17:25 -0800",Re: Config properties broken in master,dev@spark.incubator.apache.org,"That later release should be at least 0.10.0, then, since use of config
files won't be backward compatible with 0.9.0.


ote:

d
ch
ng
nt to
va
m
s
g
Factory.html#parseProperties%28java.util.Properties,%20com.typesafe.config.ConfigParseOptions%29
"
Patrick Wendell <pwendell@gmail.com>,"Sat, 18 Jan 2014 12:27:33 -0800",Re: Config properties broken in master,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey Mark - ya if we did add this I think it would be in the next major release.

ote:
ed
ich
ing
ant to
ava
s
om
t
as
ng
gFactory.html#parseProperties%28java.util.Properties,%20com.typesafe.config.ConfigParseOptions%29

"
Mark Hamstra <mark@clearstorydata.com>,"Sat, 18 Jan 2014 12:42:35 -0800",Re: Config properties broken in master,dev@spark.incubator.apache.org,"Yeah, I can get on board with that -- gives us another chance to
re-think/re-work config files to address the limitations Matei mentioned
before the interface is fixed for 1.0.


:

m
 want
ee
Factory.html#parseProperties%28java.util.Properties,%20com.typesafe.config.ConfigParseOptions%29
"
Mark Hamstra <mark@clearstorydata.com>,"Sat, 18 Jan 2014 12:46:28 -0800",Re: Config properties broken in master,dev@spark.incubator.apache.org,"Hah!  Stupid English language -- by ""fixed"" I mean established/stabilized,
not repaired.


te:

te:
g
y
e
a
m
y
5
gFactory.html#parseProperties%28java.util.Properties,%20com.typesafe.config.ConfigParseOptions%29
"
Sandy Ryza <sandy.ryza@cloudera.com>,"Sat, 18 Jan 2014 12:51:36 -0800",Should Spark on YARN example include --addJars?,dev@spark.incubator.apache.org,"Hey All,

I ran into an issue when trying to run SparkPi as described in the Spark on
YARN doc.

14/01/18 10:52:09 ERROR spark.SparkContext: Error adding jar
(java.io.FileNotFoundException:
spark-examples-assembly-0.9.0-incubating-SNAPSHOT.jar (No such file or
directory)), was the --addJars option used?

Is addJars not needed here?

Here's the doc:

SPARK_JAR=./assembly/target/scala-2.9.3/spark-assembly-0.8.1-incubating-hadoop2.0.5-alpha.jar
\
    ./spark-class org.apache.spark.deploy.yarn.Client \
      --jar examples/target/scala-2.9.3/spark-examples-assembly-0.8.1-incubating.jar
\
      --class org.apache.spark.examples.SparkPi \
      --args yarn-standalone \
      --num-workers 3 \
      --master-memory 4g \
      --worker-memory 2g \
      --worker-cores 1


thanks,
Sandy
"
Patrick Wendell <pwendell@gmail.com>,"Sat, 18 Jan 2014 22:59:49 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc1),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","This vote is cancelled in favor of rc2 which I'll post shortly.


"
Patrick Wendell <pwendell@gmail.com>,"Sat, 18 Jan 2014 23:05:46 -0800",[VOTE] Release Apache Spark 0.9.0-incubating (rc2),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Please vote on releasing the following candidate as Apache Spark
(incubating) version 0.9.0.

A draft of the release notes along with the changes file is attached
to this e-mail.

The tag to be voted on is v0.9.0-incubating (commit 00c847a):
https://git-wip-us.apache.org/repos/asf?p=incubator-spark.git;a=commit;h=00c847af1d4be2fe5fad887a57857eead1e517dc

The release files, including signatures, digests, etc can be found at:
http://people.apache.org/~pwendell/spark-0.9.0-incubating-rc2/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1003/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-0.9.0-incubating-rc2-docs/

Please vote on releasing this package as Apache Spark 0.9.0-incubating!

The vote is open until Wednesday, January 22, at 07:05 UTC
and passes if a majority of at least 3 +1 PPMC votes are cast.

[ ] +1 Release this package as Apache Spark 0.9.0-incubating
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.incubator.apache.org/
Spark Change Log

Release 0.9.0-incubating

  03019d1 Sat Jan 18 16:29:43 2014 -0800
  Merge pull request #459 from srowen/UpdaterL2Regularization
  [Correct L2 regularized weight update with canonical form]

  76147a2 Sat Jan 18 16:24:16 2014 -0800
  Merge pull request #437 from mridulm/master
  [Minor api usability changes]

  4ac8cab Sat Jan 18 16:22:46 2014 -0800
  Merge pull request #426 from mateiz/py-ml-tests
  [Re-enable Python MLlib tests (require Python 2.7 and NumPy 1.7+)]

  34e911c Sat Jan 18 16:17:34 2014 -0800
  Merge pull request #462 from mateiz/conf-file-fix
  [Remove Typesafe Config usage and conf files to fix nested property names]

  ff7201c Sat Jan 18 12:50:02 2014 -0800
  Merge pull request #461 from pwendell/master
  [Use renamed shuffle spill config in CoGroupedRDD.scala]

  7b0d5a5 Thu Jan 16 23:18:48 2014 -0800
  Merge pull request #451 from Qiuzhuang/master
  [Fixed Window spark shell launch script error.]

  4ccedb3 Wed Jan 15 14:26:48 2014 -0800
  Merge pull request #444 from mateiz/py-version
  [Clarify that Python 2.7 is only needed for MLlib]

  e3fa36f Wed Jan 15 13:56:04 2014 -0800
  Merge pull request #442 from pwendell/standalone
  [Workers should use working directory as spark home if it's not specified]

  29c76d9 Wed Jan 15 13:55:48 2014 -0800
  Merge pull request #443 from tdas/filestream-fix
  [Made some classes private[stremaing] and deprecated a method in JavaStreamingContext.]

  aca40aa Wed Jan 15 11:15:47 2014 -0800
  Merge pull request #441 from pwendell/graphx-build
  [GraphX shouldn't list Spark as provided.]

  e12c374 Wed Jan 15 10:01:43 2014 -0800
  Merge pull request #433 from markhamstra/debFix
  [Updated Debian packaging]

  2f015c2 Tue Jan 14 23:17:28 2014 -0800
  Merge pull request #436 from ankurdave/VertexId-case
  [Rename VertexID -> VertexId in GraphX]

  2859cab Tue Jan 14 23:08:19 2014 -0800
  Merge pull request #435 from tdas/filestream-fix
  [Fixed the flaky tests by making SparkConf not serializable]

  fbfbb33 Tue Jan 14 23:06:29 2014 -0800
  Merge pull request #434 from rxin/graphxmaven
  [Fixed SVDPlusPlusSuite in Maven build.]

  2c6c07f Tue Jan 14 21:53:05 2014 -0800
  Merge pull request #424 from jegonzal/GraphXProgrammingGuide
  [Additional edits for clarity in the graphx programming guide.]

  6fa4e02 Tue Jan 14 21:51:25 2014 -0800
  Merge pull request #431 from ankurdave/graphx-caching-doc
  [Describe caching and uncaching in GraphX programming guide]

  2f930d5 Tue Jan 14 15:00:11 2014 -0800
  Merge pull request #428 from pwendell/writeable-objects
  [Don't clone records for text files]

  329c9df Tue Jan 14 14:53:36 2014 -0800
  Merge pull request #429 from ankurdave/graphx-examples-pom.xml
  [Add GraphX dependency to examples/pom.xml]

  a14933d Tue Jan 14 14:52:42 2014 -0800
  Merge pull request #427 from pwendell/deprecate-aggregator
  [Deprecate rather than remove old combineValuesByKey function]

  119b6c5 Tue Jan 14 13:29:08 2014 -0800
  Merge pull request #425 from rxin/scaladoc
  [API doc update & make Broadcast public]

  bf3b150 Tue Jan 14 09:45:22 2014 -0800
  Merge pull request #423 from jegonzal/GraphXProgrammingGuide
  [Improving the graphx-programming-guide]

  1b4adc2 Tue Jan 14 01:19:24 2014 -0800
  Merge pull request #420 from pwendell/header-files
  [Add missing header files]

  b60840e Tue Jan 14 00:48:34 2014 -0800
  Merge pull request #418 from pwendell/0.9-versions
  [Version changes for release 0.9.0.]

  980250b Tue Jan 14 00:05:37 2014 -0800
  Merge pull request #416 from tdas/filestream-fix
  [Removed unnecessary DStream operations and updated docs]

  055be5c Mon Jan 13 23:26:44 2014 -0800
  Merge pull request #415 from pwendell/shuffle-compress
  [Enable compression by default for spills]

  fdaabdc Mon Jan 13 23:08:26 2014 -0800
  Merge pull request #380 from mateiz/py-bayes
  [Add Naive Bayes to Python MLlib, and some API fixes]

  4a805af Mon Jan 13 22:58:38 2014 -0800
  Merge pull request #367 from ankurdave/graphx
  [GraphX: Unifying Graphs and Tables]

  945fe7a Mon Jan 13 22:56:12 2014 -0800
  Merge pull request #408 from pwendell/external-serializers
  [Improvements to external sorting]

  68641bc Mon Jan 13 22:54:13 2014 -0800
  Merge pull request #413 from rxin/scaladoc
  [Adjusted visibility of various components and documentation for 0.9.0 release.]

  0ca0d4d Mon Jan 13 22:32:21 2014 -0800
  Merge pull request #401 from andrewor14/master
  [External sorting - Add number of bytes spilled to Web UI]

  08b9fec Mon Jan 13 22:29:03 2014 -0800
  Merge pull request #409 from tdas/unpersist
  [Automatically unpersisting RDDs that have been cleaned up from DStreams]

  b07bc02 Mon Jan 13 20:45:22 2014 -0800
  Merge pull request #412 from harveyfeng/master
  [Add default value for HadoopRDD's `cloneRecords` constructor arg]

  a2fee38 Mon Jan 13 19:45:26 2014 -0800
  Merge pull request #411 from tdas/filestream-fix
  [Improved logic of finding new files in FileInputDStream]

  01c0d72 Mon Jan 13 16:24:30 2014 -0800
  Merge pull request #410 from rxin/scaladoc1
  [Updated JavaStreamingContext to make scaladoc compile.]

  8038da2 Mon Jan 13 14:59:30 2014 -0800
  Merge pull request #2 from jegonzal/GraphXCCIssue
  [Improving documentation and identifying potential bug in CC calculation.]

  b93f9d4 Mon Jan 13 12:18:05 2014 -0800
  Merge pull request #400 from tdas/dstream-move
  [Moved DStream and PairDSream to org.apache.spark.streaming.dstream]

  e6ed13f Sun Jan 12 22:35:14 2014 -0800
  Merge pull request #397 from pwendell/host-port
  [Remove now un-needed hostPort option]

  0b96d85 Sun Jan 12 21:31:43 2014 -0800
  Merge pull request #399 from pwendell/consolidate-off
  [Disable shuffle file consolidation by default]

  0ab505a Sun Jan 12 21:31:04 2014 -0800
  Merge pull request #395 from hsaputra/remove_simpleredundantreturn_scala
  [Remove simple redundant return statements for Scala methods/functions]

  405bfe8 Sun Jan 12 20:04:21 2014 -0800
  Merge pull request #394 from tdas/error-handling
  [Better error handling in Spark Streaming and more API cleanup]

  28a6b0c Sun Jan 12 19:49:36 2014 -0800
  Merge pull request #398 from pwendell/streaming-api
  [Rename DStream.foreach to DStream.foreachRDD]

  074f502 Sun Jan 12 17:01:13 2014 -0800
  Merge pull request #396 from pwendell/executor-env
  [Setting load defaults to true in executor]

  82e2b92 Sun Jan 12 16:55:11 2014 -0800
  Merge pull request #392 from rxin/listenerbus
  [Stop SparkListenerBus daemon thread when DAGScheduler is stopped.]

  288a878 Sat Jan 11 21:53:19 2014 -0800
  Merge pull request #389 from rxin/clone-writables
  [Minor update for clone writables and more documentation.]

  dbc11df Sat Jan 11 18:07:13 2014 -0800
  Merge pull request #388 from pwendell/master
  [Fix UI bug introduced in #244.]

  409866b Sat Jan 11 17:12:06 2014 -0800
  Merge pull request #393 from pwendell/revert-381
  [Revert PR 381]

  6510f04 Sat Jan 11 12:48:26 2014 -0800
  Merge pull request #387 from jerryshao/conf-fix
  [Fix configure didn't work small problem in ALS]

  ee6e7f9 Sat Jan 11 12:07:55 2014 -0800
  Merge pull request #359 from ScrapCodes/clone-writables
  [We clone hadoop key and values by default and reuse objects if asked to.]

  4216178 Sat Jan 11 09:46:48 2014 -0800
  Merge pull request #373 from jerryshao/kafka-upgrade
  [Upgrade Kafka dependecy to 0.8.0 release version]

  92ad18b Fri Jan 10 23:25:15 2014 -0800
  Merge pull request #376 from prabeesh/master
  [Change clientId to random clientId]

  0b5ce7a Fri Jan 10 23:23:21 2014 -0800
  Merge pull request #386 from pwendell/typo-fix
  [Small typo fix]

  1d7bef0 Fri Jan 10 18:53:03 2014 -0800
  Merge pull request #381 from mateiz/default-ttl
  [Fix default TTL for metadata cleaner]

  44d6a8e Fri Jan 10 17:51:50 2014 -0800
  Merge pull request #382 from RongGu/master
  [Fix a type error in comment lines]

  88faa30 Fri Jan 10 17:14:22 2014 -0800
  Merge pull request #385 from shivaram/add-i2-instances
  [Add i2 instance types to Spark EC2.]

  f265531 Fri Jan 10 16:25:44 2014 -0800
  Merge pull request #383 from tdas/driver-test
  [API for automatic driver recovery for streaming programs and other bug fixes]

  d37408f Fri Jan 10 16:25:01 2014 -0800
  Merge pull request #377 from andrewor14/master
  [External Sorting for Aggregator and CoGroupedRDDs (Revisited)]

  0eaf01c Fri Jan 10 15:32:19 2014 -0800
  Merge pull request #369 from pillis/master
  [SPARK-961 Add a Vector.random() method]

  7cef843 Fri Jan 10 15:34:15 2014 -0600
  Merge pull request #371 from tgravescs/yarn_client_addjar_misc_fixes
  [Yarn client addjar and misc fixes]

  7b58f11 Fri Jan 10 12:47:46 2014 -0800
  Merge pull request #384 from pwendell/debug-logs
  [Make DEBUG-level logs consummable.]

  23d2995 Fri Jan 10 10:20:02 2014 -0800
  Merge pull request #1 from jegonzal/graphx
  [ProgrammingGuide]

  0ebc973 Thu Jan 9 23:58:49 2014 -0800
  Merge pull request #375 from mateiz/option-fix
  [Fix bug added when we changed AppDescription.maxCores to an Option]

  dd03cea Thu Jan 9 23:38:03 2014 -0800
  Merge pull request #378 from pwendell/consolidate_on
  [Enable shuffle consolidation by default.]

  997c830 Thu Jan 9 22:22:20 2014 -0800
  Merge pull request #363 from pwendell/streaming-logs
  [Set default logging to WARN for Spark streaming examples.]

  300eaa9 Thu Jan 9 20:29:51 2014 -0800
  Merge pull request #353 from pwendell/ipython-simplify
  [Simplify and fix pyspark script.]

  4b074fa Thu Jan 9 19:03:55 2014 -0800
  Merge pull request #374 from mateiz/completeness
  [Add some missing Java API methods]

  a9d5333 Thu Jan 9 18:46:46 2014 -0800
  Merge pull request #294 from RongGu/master
  [Bug fixes for updating the RDD block's memory and disk usage information]

  d86a85e Thu Jan 9 18:37:52 2014 -0800
  Merge pull request #293 from pwendell/standalone-driver
  [SPARK-998: Support Launching Driver Inside of Standalone Mode]

  26cdb5f Thu Jan 9 17:16:34 2014 -0800
  Merge pull request #372 from pwendell/log4j-fix-1
  [Send logs to stderr by default (instead of stdout).]

  12f414e Thu Jan 9 15:31:30 2014 -0800
  Merge pull request #362 from mateiz/conf-getters
  [Use typed getters for configuration settings]

  365cac9 Thu Jan 9 00:56:16 2014 -0800
  Merge pull request #361 from rxin/clean
  [Minor style cleanup. Mostly on indenting & line width changes.]

  73c724e Thu Jan 9 00:32:19 2014 -0800
  Merge pull request #368 from pwendell/sbt-fix
  [Don't delegate to users `sbt`.]

  dceedb4 Wed Jan 8 23:19:28 2014 -0800
  Merge pull request #364 from pwendell/fix
  [Fixing config option ""retained_stages"" => ""retainedStages"".]

  04d83fc Wed Jan 8 11:55:37 2014 -0800
  Merge pull request #360 from witgo/master
  [fix make-distribution.sh show version: command not found]

  56ebfea Wed Jan 8 11:50:06 2014 -0800
  Merge pull request #357 from hsaputra/set_boolean_paramname
  [Set boolean param name for call to SparkHadoopMapReduceUtil.newTaskAttemptID]

  bdeaeaf Wed Jan 8 11:48:39 2014 -0800
  Merge pull request #358 from pwendell/add-cdh
  [Add CDH Repository to Maven Build]

  5cae05f Wed Jan 8 11:47:28 2014 -0800
  Merge pull request #356 from hsaputra/remove_deprecated_cleanup_method
  [Remove calls to deprecated mapred's OutputCommitter.cleanupJob]

  6eef78d Wed Jan 8 08:49:20 2014 -0600
  Merge pull request #345 from colorant/yarn
  [support distributing extra files to worker for yarn client mode]

  bb6a39a Tue Jan 7 22:32:18 2014 -0800
  Merge pull request #322 from falaki/MLLibDocumentationImprovement
  [SPARK-1009 Updated MLlib docs to show how to use it in Python]

  cb1b927 Tue Jan 7 22:26:28 2014 -0800
  Merge pull request #355 from ScrapCodes/patch-1
  [Update README.md]

  c0f0155 Tue Jan 7 22:21:52 2014 -0800
  Merge pull request #313 from tdas/project-refactor
  [Refactored the streaming project to separate external libraries like Twitter, Kafka, Flume, etc.]

  f5f12dc Tue Jan 7 21:56:35 2014 -0800
  Merge pull request #336 from liancheng/akka-remote-lookup
  [Get rid of `Either[ActorRef, ActorSelection]']

  11891e6 Wed Jan 8 00:32:18 2014 -0500
  Merge pull request #327 from lucarosellini/master
  [Added ‚Äò-i‚Äô command line option to Spark REPL]

  7d0aac9 Wed Jan 8 00:30:45 2014 -0500
  Merge pull request #354 from hsaputra/addasfheadertosbt
  [Add ASF header to the new sbt script.]

  d75dc42 Wed Jan 8 00:30:03 2014 -0500
  Merge pull request #350 from mateiz/standalone-limit
  [Add way to limit default # of cores used by apps in standalone mode]

  61674bc Tue Jan 7 18:32:13 2014 -0800
  Merge pull request #352 from markhamstra/oldArch
  [Don't leave os.arch unset after BlockManagerSuite]

  b2e690f Tue Jan 7 16:57:08 2014 -0800
  Merge pull request #328 from falaki/MatrixFactorizationModel-fix
  [SPARK-1012: DAGScheduler Exception Fix]

  6ccf8ce Tue Jan 7 15:49:14 2014 -0800
  Merge pull request #351 from pwendell/maven-fix
  [Add log4j exclusion rule to maven.]

  7d5fa17 Tue Jan 7 11:31:34 2014 -0800
  Merge pull request #337 from yinxusen/mllib-16-bugfix
  [Mllib 16 bugfix]

  71fc113 Tue Jan 7 11:30:35 2014 -0800
  Merge pull request #349 from CodingCat/support-worker_dir
  [add the comments about SPARK_WORKER_DIR]

  15d9534 Tue Jan 7 08:10:02 2014 -0800
  Merge pull request #318 from srowen/master
  [Suggested small changes to Java code for slightly more standard style, encapsulation and in some cases performance]

  468af0f Tue Jan 7 08:09:01 2014 -0800
  Merge pull request #348 from prabeesh/master
  [spark -> org.apache.spark]

  c3cf047 Tue Jan 7 00:54:25 2014 -0800
  Merge pull request #339 from ScrapCodes/conf-improvements
  [Conf improvements]

  a862caf Tue Jan 7 00:18:20 2014 -0800
  Merge pull request #331 from holdenk/master
  [Add a script to download sbt if not present on the system]

  b97ef21 Mon Jan 6 20:12:57 2014 -0800
  Merge pull request #346 from sproblvem/patch-1
  [Update stop-slaves.sh]

  7210257 Mon Jan 6 18:25:44 2014 -0800
  Merge pull request #128 from adamnovak/master
  [Fix failing ""sbt/sbt publish-local"" by adding a no-argument PrimitiveKeyOpenHashMap constructor]

  e4d6057 Mon Jan 6 14:56:54 2014 -0800
  Merge pull request #343 from pwendell/build-fix
  [Fix test breaking downstream builds]

  93bf962 Mon Jan 6 11:42:41 2014 -0800
  Merge pull request #340 from ScrapCodes/sbt-fixes
  [Made java options to be applied during tests so that they become self explanatory.]

  60edeb3 Mon Jan 6 11:40:32 2014 -0800
  Merge pull request #338 from ScrapCodes/ning-upgrade
  [SPARK-1005 Ning upgrade]

  c708e81 Mon Jan 6 11:35:48 2014 -0800
  Merge pull request #341 from ash211/patch-5
  [Clarify spark.cores.max in docs]

  33fcb91 Mon Jan 6 11:19:23 2014 -0800
  Merge pull request #342 from tgravescs/fix_maven_protobuf
  [Change protobuf version for yarn alpha back to 2.4.1]

  357083c Mon Jan 6 10:29:04 2014 -0800
  Merge pull request #330 from tgravescs/fix_addjars_null_handling
  [Fix handling of empty SPARK_EXAMPLES_JAR]

  a2e7e04 Sun Jan 5 22:37:36 2014 -0800
  Merge pull request #333 from pwendell/logging-silence
  [Quiet ERROR-level Akka Logs]

  5b0986a Sun Jan 5 19:25:09 2014 -0800
  Merge pull request #334 from pwendell/examples-fix
  [Removing SPARK_EXAMPLES_JAR in the code]

  f4b924f Sun Jan 5 17:11:47 2014 -0800
  Merge pull request #335 from rxin/ser
  [Fall back to zero-arg constructor for Serializer initialization if there is no constructor that accepts SparkConf.]

  d43ad3e Sat Jan 4 16:29:30 2014 -0800
  Merge pull request #292 from soulmachine/naive-bayes
  [standard Naive Bayes classifier]

  86404da Sat Jan 4 14:55:54 2014 -0800
  Merge pull request #127 from jegonzal/MapByPartition
  [Adding mapEdges and mapTriplets by Partition]

  e68cdb1 Sat Jan 4 13:46:02 2014 -0800
  Merge pull request #124 from jianpingjwang/master
  [refactor and bug fix]

  280ddf6 Sat Jan 4 12:54:41 2014 -0800
  Merge pull request #121 from ankurdave/more-simplify
  [Simplify GraphImpl internals further]

  10fe23b Fri Jan 3 23:50:14 2014 -0800
  Merge pull request #329 from pwendell/remove-binaries
  [SPARK-1002: Remove Binaries from Spark Source]

  c4d6145 Fri Jan 3 16:30:53 2014 -0800
  Merge pull request #325 from witgo/master
  [Modify spark on yarn to create SparkConf process]

  4ae101f Fri Jan 3 11:24:35 2014 -0800
  Merge pull request #317 from ScrapCodes/spark-915-segregate-scripts
  [Spark-915 segregate scripts]

  87248bd Fri Jan 3 00:45:31 2014 -0800
  Merge pull request #1 from apache/master
  [Merge latest Spark changes]

  30b9db0 Thu Jan 2 23:15:55 2014 -0800
  Merge pull request #285 from colorant/yarn-refactor
  [Yarn refactor]

  498a5f0 Thu Jan 2 19:06:40 2014 -0800
  Merge pull request #323 from tgravescs/sparkconf_yarn_fix
  [fix spark on yarn after the sparkConf changes]

  0475ca8 Thu Jan 2 15:17:08 2014 -0800
  Merge pull request #320 from kayousterhout/erroneous_failed_msg
  [Remove erroneous FAILED state for killed tasks.]

  588a169 Thu Jan 2 13:20:54 2014 -0800
  Merge pull request #297 from tdas/window-improvement
  [Improvements to DStream window ops and refactoring of Spark's CheckpointSuite]

  5e67cdc Thu Jan 2 12:56:28 2014 -0800
  Merge pull request #319 from kayousterhout/remove_error_method
  [Removed redundant TaskSetManager.error() function.]

  ca67909 Thu Jan 2 15:54:54 2014 -0500
  Merge pull request #311 from tmyklebu/master
  [SPARK-991: Report information gleaned from a Python stacktrace in the UI]

  3713f81 Wed Jan 1 21:29:12 2014 -0800
  Merge pull request #309 from mateiz/conf2
  [SPARK-544. Migrate configuration to a SparkConf class]

  c1d928a Wed Jan 1 17:03:48 2014 -0800
  Merge pull request #312 from pwendell/log4j-fix-2
  [SPARK-1008: Logging improvments]

  dc9cb83 Wed Jan 1 13:28:34 2014 -0800
  Merge pull request #126 from jegonzal/FixingPersist
  [Fixing Persist Behavior]

  9a0ff72 Tue Dec 31 21:50:24 2013 -0800
  Merge pull request #314 from witgo/master
  [restore core/pom.xml file modification]

  8b8e70e Tue Dec 31 17:48:24 2013 -0800
  Merge pull request #73 from falaki/ApproximateDistinctCount
  [Approximate distinct count]

  63b411d Tue Dec 31 14:31:28 2013 -0800
  Merge pull request #238 from ngbinh/upgradeNetty
  [upgrade Netty from 4.0.0.Beta2 to 4.0.13.Final]

  32d6ae9 Tue Dec 31 13:51:07 2013 -0800
  Merge pull request #120 from ankurdave/subgraph-reuses-view
  [Reuse VTableReplicated in GraphImpl.subgraph]

  55b7e2f Tue Dec 31 10:12:51 2013 -0800
  Merge pull request #289 from tdas/filestream-fix
  [Bug fixes for file input stream and checkpointing]

  2b71ab9 Mon Dec 30 11:01:30 2013 -0800
  Merge pull request from aarondav: Utilize DiskBlockManager pathway for temp file writing
  [This gives us a couple advantages:]

  50e3b8e Mon Dec 30 07:44:26 2013 -0800
  Merge pull request #308 from kayousterhout/stage_naming
  [Changed naming of StageCompleted event to be consistent]

  72a17b6 Sat Dec 28 21:25:40 2013 -1000
  Revert ""Merge pull request #310 from jyunfan/master""
  [This reverts commit 79b20e4dbe3dcd8559ec8316784d3334bb55868b, reversing]

  79b20e4 Sat Dec 28 21:13:36 2013 -1000
  Merge pull request #310 from jyunfan/master
  [Fix typo in the Accumulators section]

  7375047 Sat Dec 28 13:25:06 2013 -0800
  Merge pull request #304 from kayousterhout/remove_unused
  [Removed unused failed and causeOfFailure variables (in TaskSetManager)]

  ad3dfd1 Fri Dec 27 22:10:14 2013 -0500
  Merge pull request #307 from kayousterhout/other_failure
  [Removed unused OtherFailure TaskEndReason.]

  b579b83 Fri Dec 27 22:09:04 2013 -0500
  Merge pull request #306 from kayousterhout/remove_pending
  [Remove unused hasPendingTasks methods]

  19672dc Fri Dec 27 13:37:10 2013 -0800
  Merge pull request #305 from kayousterhout/line_spacing
  [Fixed >100char lines in DAGScheduler.scala]

  7be1e57 Thu Dec 26 23:41:40 2013 -1000
  Merge pull request #298 from aarondav/minor
  [Minor: Decrease margin of left side of Log page]

  7d811ba Thu Dec 26 23:39:58 2013 -1000
  Merge pull request #302 from pwendell/SPARK-1007
  [SPARK-1007: spark-class2.cmd should change SCALA_VERSION to be 2.10]

  5e69fc5 Thu Dec 26 19:10:39 2013 -0500
  Merge pull request #295 from markhamstra/JobProgressListenerNPE
  [Avoid a lump of coal (NPE) in JobProgressListener's stocking.]

  da20270 Thu Dec 26 12:11:52 2013 -0800
  Merge pull request #1 from aarondav/driver
  [Refactor DriverClient to be more Actor-based]

  e240bad Thu Dec 26 12:30:48 2013 -0500
  Merge pull request #296 from witgo/master
  [Renamed ClusterScheduler to TaskSchedulerImpl for yarn and new-yarn package]

  c344ed0 Thu Dec 26 01:31:06 2013 -0500
  Merge pull request #283 from tmyklebu/master
  [Python bindings for mllib]

  56094bc Wed Dec 25 13:14:33 2013 -0500
  Merge pull request #290 from ash211/patch-3
  [Typo: avaiable -> available]

  4842a07 Wed Dec 25 01:52:15 2013 -0800
  Merge pull request #287 from azuryyu/master
  [Fixed job name in the java streaming example.]

  85a344b Tue Dec 24 16:35:06 2013 -0800
  Merge pull request #127 from kayousterhout/consolidate_schedulers
  [Deduplicate Local and Cluster schedulers.]

  c2dd6bc Tue Dec 24 14:36:47 2013 -0800
  Merge pull request #279 from aarondav/shuffle-cleanup0
  [Clean up shuffle files once their metadata is gone]

  3bf7c70 Tue Dec 24 16:37:13 2013 -0500
  Merge pull request #275 from ueshin/wip/changeclasspathorder
  [Change the order of CLASSPATH.]

  d63856c Mon Dec 23 22:07:26 2013 -0800
  Merge pull request #286 from rxin/build
  [Show full stack trace and time taken in unit tests.]

  23a9ae6 Tue Dec 24 00:08:48 2013 -0500
  Merge pull request #277 from tdas/scheduler-update
  [Refactored the streaming scheduler and added StreamingListener interface]

  11107c9 Mon Dec 23 10:38:20 2013 -0800
  Merge pull request #244 from leftnoteasy/master
  [Added SPARK-968 implementation for review]

  44e4205 Sun Dec 22 11:44:18 2013 -0800
  Merge pull request #116 from jianpingjwang/master
  [remove unused variables and fix a bug]

  4797c22 Fri Dec 20 13:30:39 2013 -0800
  Merge pull request #118 from ankurdave/VertexPartitionSuite
  [Test VertexPartition and fix bugs]

  0bc57c5 Fri Dec 20 11:56:54 2013 -0800
  Merge pull request #280 from aarondav/minor
  [Minor cleanup for standalone scheduler]

  ac70b8f Fri Dec 20 10:56:10 2013 -0800
  Merge pull request #117 from ankurdave/more-tests
  [More tests]

  45310d4 Thu Dec 19 22:08:20 2013 -0800
  Merge pull request #115 from ankurdave/test-reorg
  [Reorganize unit tests; improve GraphSuite test coverage]

  9228ec8 Thu Dec 19 21:37:15 2013 -0800
  Merge pull request #1 from aarondav/127
  [Merge master into 127]

  eca68d4 Thu Dec 19 18:12:22 2013 -0800
  Merge pull request #272 from tmyklebu/master
  [Track and report task result serialisation time.]

  7990c56 Thu Dec 19 13:35:09 2013 -0800
  Merge pull request #276 from shivaram/collectPartition
  [Add collectPartition to JavaRDD interface.]

  440e531 Thu Dec 19 10:38:56 2013 -0800
  Merge pull request #278 from MLnick/java-python-tostring
  [Add toString to Java RDD, and __repr__ to Python RDD]

  d8d3f3e Thu Dec 19 00:06:43 2013 -0800
  Merge pull request #183 from aarondav/spark-959
  [[SPARK-959] Explicitly depend on org.eclipse.jetty.orbit jar]

  bfba532 Wed Dec 18 22:22:21 2013 -0800
  Merge pull request #247 from aarondav/minor
  [Increase spark.akka.askTimeout default to 30 seconds]

  da301b5 Wed Dec 18 20:03:29 2013 -0800
  Merge pull request #112 from amatsukawa/scc
  [Strongly connected component algorithm]

  c64a53a Wed Dec 18 16:56:26 2013 -0800
  Merge pull request #267 from JoshRosen/cygwin
  [Fix Cygwin support in several scripts.]

  a645ef6 Wed Dec 18 16:07:52 2013 -0800
  Merge pull request #48 from amatsukawa/add_project_to_graph
  [Add mask operation on graph and filter graph primitive]

  d7ebff0 Wed Dec 18 15:38:48 2013 -0800
  Merge pull request #1 from ankurdave/add_project_to_graph
  [Merge current master and reimplement Graph.mask using innerJoin]

  5ea1872 Wed Dec 18 15:27:24 2013 -0800
  Merge pull request #274 from azuryy/master
  [Fixed the example link in the Scala programing guid.]

  3fd2e09 Wed Dec 18 12:52:36 2013 -0800
  Merge pull request #104 from jianpingjwang/master
  [SVD++ demo]

  f4effb3 Tue Dec 17 22:26:21 2013 -0800
  Merge pull request #273 from rxin/top
  [Fixed a performance problem in RDD.top and BoundedPriorityQueue]

  1b5eacb Tue Dec 17 13:49:17 2013 -0800
  Merge pull request #102 from ankurdave/clustered-edge-index
  [Add clustered index on edges by source vertex]

  7a8169b Mon Dec 16 22:42:21 2013 -0800
  Merge pull request #268 from pwendell/shaded-protobuf
  [Add support for 2.2. to master (via shaded jars)]

  0476c84 Mon Dec 16 17:19:25 2013 -0800
  Merge pull request #100 from ankurdave/mrTriplets-active-set
  [Support activeSet option in mapReduceTriplets]

  964a3b6 Mon Dec 16 15:23:51 2013 -0800
  Merge pull request #270 from ewencp/really-force-ssh-pseudo-tty-master
  [Force pseudo-tty allocation in spark-ec2 script.]

  5192ef3 Mon Dec 16 15:08:08 2013 -0800
  Merge pull request #94 from ankurdave/load-edges-columnar
  [Load edges in columnar format]

  883e034 Mon Dec 16 14:16:02 2013 -0800
  Merge pull request #245 from gregakespret/task-maxfailures-fix
  [Fix for spark.task.maxFailures not enforced correctly.]

  a51f340 Sun Dec 15 22:02:30 2013 -0800
  Merge pull request #265 from markhamstra/scala.binary.version
  [DRY out the POMs with scala.binary.version]

  ded10ce Sun Dec 15 17:25:33 2013 -0800
  Merge pull request #103 from amplab/optimizations
  [Optimizations cherry-picked from SIGMOD branches]

  d2ced6d Sun Dec 15 14:11:34 2013 -0800
  Merge pull request #256 from MLnick/master
  [Fix 'IPYTHON=1 ./pyspark' throwing ValueError]

  c55e698 Sun Dec 15 12:49:02 2013 -0800
  Merge pull request #257 from tgravescs/sparkYarnFixName
  [Fix the --name option for Spark on Yarn]

  ab85f88 Sun Dec 15 12:48:32 2013 -0800
  Merge pull request #264 from shivaram/spark-class-fix
  [Use CoarseGrainedExecutorBackend in spark-class]

  8a56c1f Sat Dec 14 16:29:24 2013 -0800
  Merge pull request #84 from amatsukawa/graphlab_enhancements
  [GraphLab bug fix & set start vertex]

  7db9165 Sat Dec 14 14:16:34 2013 -0800
  Merge pull request #251 from pwendell/master
  [Fix list rendering in YARN markdown docs.]

  2fd781d Sat Dec 14 12:59:37 2013 -0800
  Merge pull request #249 from ngbinh/partitionInJavaSortByKey
  [Expose numPartitions parameter in JavaPairRDD.sortByKey()]

  9bf192b Sat Dec 14 12:52:18 2013 -0800
  Merge pull request #91 from amplab/standalone-pagerank
  [Standalone PageRank]

  840af5e Sat Dec 14 12:51:51 2013 -0800
  Merge pull request #99 from ankurdave/only-dynamic-pregel
  [Remove static Pregel; take maxIterations in dynamic Pregel]

  97ac060 Sat Dec 14 00:22:45 2013 -0800
  Merge pull request #259 from pwendell/scala-2.10
  [Migration to Scala 2.10]

  7ac944f Fri Dec 13 23:22:08 2013 -0800
  Merge pull request #262 from pwendell/mvn-fix
  [Fix maven build issues in 2.10 branch]

  6defb06 Fri Dec 13 21:18:57 2013 -0800
  Merge pull request #261 from ScrapCodes/scala-2.10
  [Added a comment about ActorRef and ActorSelection difference.]

  76566b1 Fri Dec 13 10:11:02 2013 -0800
  Merge pull request #260 from ScrapCodes/scala-2.10
  [Review comments on the PR for scala 2.10 migration.]

  0aeb182 Thu Dec 12 21:14:42 2013 -0800
  Merge pull request #255 from ScrapCodes/scala-2.10
  [Disabled yarn 2.2 in sbt and mvn build and added a message in the sbt build.]

  2e89398 Wed Dec 11 23:10:53 2013 -0800
  Merge pull request #254 from ScrapCodes/scala-2.10
  [Scala 2.10 migration]

  ce6ca4e Wed Dec 11 22:30:54 2013 -0800
  Merge pull request #97 from dcrankshaw/fix-rddtop
  [Added BoundedPriorityQueue kryo registrator. Fixes top issue.]

  d2efe13 Tue Dec 10 13:01:26 2013 -0800
  Merge pull request #250 from pwendell/master
  [README incorrectly suggests build sources spark-env.sh]

  6169fe1 Mon Dec 9 16:51:36 2013 -0800
  Merge pull request #246 from pwendell/master
  [Add missing license headers]

  d992ec6 Sun Dec 8 20:49:20 2013 -0800
  Merge pull request #195 from dhardy92/fix_DebScriptPackage
  [[Deb] fix package of Spark classes adding org.apache prefix in scripts embeded in .deb]

  1f4a4bc Sat Dec 7 22:34:34 2013 -0800
  Merge pull request #242 from pwendell/master
  [Update broken links and add HDP 2.0 version string]

  6494d62 Sat Dec 7 11:56:16 2013 -0800
  Merge pull request #240 from pwendell/master
  [SPARK-917 Improve API links in nav bar]

  f466f79 Sat Dec 7 11:51:52 2013 -0800
  Merge pull request #239 from aarondav/nit
  [Correct spellling error in configuration.md]

  3abfbfb Sat Dec 7 11:24:19 2013 -0800
  Merge pull request #92 from ankurdave/rdd-names
  [Set RDD names for easy debugging]

  31e8a14 Fri Dec 6 21:49:55 2013 -0800
  Merge pull request #90 from amplab/pregel-replicate-changed
  [Replicate only changed vertices]

  10c3c0c Fri Dec 6 20:29:45 2013 -0800
  Merge pull request #237 from pwendell/formatting-fix
  [Formatting fix]

  1b38f5f Fri Dec 6 20:16:15 2013 -0800
  Merge pull request #236 from pwendell/shuffle-docs
  [Adding disclaimer for shuffle file consolidation]

  e5d5728 Fri Dec 6 20:14:56 2013 -0800
  Merge pull request #235 from pwendell/master
  [Minor doc fixes and updating README]

  241336a Fri Dec 6 17:29:03 2013 -0800
  Merge pull request #234 from alig/master
  [Updated documentation about the YARN v2.2 build process]

  e039234 Fri Dec 6 11:49:59 2013 -0800
  Merge pull request #190 from markhamstra/Stages4Jobs
  [stageId <--> jobId mapping in DAGScheduler]

  bfa6860 Fri Dec 6 11:04:03 2013 -0800
  Merge pull request #233 from hsaputra/changecontexttobackend
  [Change the name of input argument in ClusterScheduler#initialize from context to backend.]

  3fb302c Fri Dec 6 11:03:32 2013 -0800
  Merge pull request #205 from kayousterhout/logging
  [Added logging of scheduler delays to UI]

  87676a6 Fri Dec 6 11:01:42 2013 -0800
  Merge pull request #220 from rxin/zippart
  [Memoize preferred locations in ZippedPartitionsBaseRDD]

  0780498 Thu Dec 5 23:29:42 2013 -0800
  Merge pull request #232 from markhamstra/FiniteWait
  [jobWaiter.synchronized before jobWaiter.wait]

  1c8500e Thu Dec 5 16:25:44 2013 -0800
  Merge pull request #88 from amplab/varenc
  [Fixed a bug that variable encoding doesn't work for ints that use all 64 bits.]

  e0bcaa0 Thu Dec 5 12:37:02 2013 -0800
  Merge pull request #86 from ankurdave/vid-varenc
  [Finish work on #85]

  5d46025 Thu Dec 5 12:31:24 2013 -0800
  Merge pull request #228 from pwendell/master
  [Document missing configs and set shuffle consolidation to false.]

  3e96b9a Thu Dec 5 12:07:36 2013 -0800
  Merge pull request #85 from ankurdave/vid-varenc
  [Always write Vids using variable encoding]

  72b6961 Wed Dec 4 23:33:04 2013 -0800
  Merge pull request #199 from harveyfeng/yarn-2.2
  [Hadoop 2.2 migration]

  e0347ba Wed Dec 4 17:38:06 2013 -0800
  Merge pull request #83 from ankurdave/fix-tests
  [Fix compile errors in GraphSuite and SerializerSuite]

  182f9ba Wed Dec 4 15:52:07 2013 -0800
  Merge pull request #227 from pwendell/master
  [Fix small bug in web UI and minor clean-up.]

  cbd3b75 Wed Dec 4 15:35:26 2013 -0800
  Merge pull request #81 from amplab/clean1
  [Codebase refactoring]

  b9e7609 Wed Dec 4 14:42:09 2013 -0800
  Merge pull request #225 from ash211/patch-3
  [Add missing space after ""Serialized"" in StorageLevel]

  055462c Wed Dec 4 14:02:11 201"
"
Patrick Wendell <pwendell@gmail.com>,Sat"," 18 Jan 2014 23:11:35 -0800""",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc2),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I'll kick of the voting with a +1.


"
Reynold Xin <rxin@databricks.com>,"Sun, 19 Jan 2014 00:55:45 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc2),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>",1
Sandy Ryza <sandy.ryza@cloudera.com>,"Sun, 19 Jan 2014 01:24:12 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc2),dev@spark.incubator.apache.org,"Has anybody tested against YARN 2.2?  I tried it out against a
pseudo-distributed cluster and ran into an issue I just filed as
SPARK-1031<https://spark-project.atlassian.net/browse/SPARK-1031>
.

thanks,
Sandy



"
Mridul Muralidharan <mridul@gmail.com>,"Sun, 19 Jan 2014 18:26:55 +0530",Re: Config properties broken in master,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Chanced upon spill related config which exhibit same pattern ...

- Mridul


"
Patrick Wendell <pwendell@gmail.com>,"Sun, 19 Jan 2014 09:05:04 -0800",Re: Config properties broken in master,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey Mridul this was patched and we cut a new release candidate. There
were several different config options which had a.b and a.b.c... they
should all work in the new RC.


"
Mridul Muralidharan <mridul@gmail.com>,"Mon, 20 Jan 2014 00:03:03 +0530",Re: Config properties broken in master,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Oh great, just saw the PR from Matei ... for some odd reason, the dev
mails are coming to be horribly delayed.


Thanks,
Mridul


"
Taka Shinagawa <taka.epsilon@gmail.com>,"Sun, 19 Jan 2014 12:37:00 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc2),dev@spark.incubator.apache.org,"I've found a problem with the cartesian method on Pyspark and filed
as SPARK-1034
https://spark-project.atlassian.net/browse/SPARK-1034


It's also nice if SPARK-978 can be fixed, too.
https://spark-project.atlassian.net/browse/SPARK-978

Thanks,
Taka



"
Patrick Wendell <pwendell@gmail.com>,"Sun, 19 Jan 2014 14:15:03 -0800",[VOTE] Release Apache Spark 0.9.0-incubating (rc3),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Please vote on releasing the following candidate as Apache Spark
(incubating) version 0.9.0.

A draft of the release notes along with the changes file is attached
to this e-mail.

The tag to be voted on is v0.9.0-incubating (commit a7760eff):
https://git-wip-us.apache.org/repos/asf?p=incubator-spark.git;a=commit;h=a7760eff4ea6a474cab68896a88550f63bae8b0d

The release files, including signatures, digests, etc can be found at:
http://people.apache.org/~pwendell/spark-0.9.0-incubating-rc3/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1004/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-0.9.0-incubating-rc3-docs/

Please vote on releasing this package as Apache Spark 0.9.0-incubating!

The vote is open until Wednesday, January 22, at 22:15 UTC and passes
if a majority of at least 3 +1 PPMC votes are cast.

[ ] +1 Release this package as Apache Spark 0.9.0-incubating
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.incubator.apache.org/
Spark Change Log

Release 0.9.0-incubating

  94ae25d Sun Jan 19 11:33:51 2014 -0800
  Merge pull request #470 from tgravescs/fix_spark_examples_yarn
  [Only log error on missing jar to allow spark examples to jar.]

  0f077b5 Sun Jan 19 10:30:29 2014 -0800
  Merge pull request #458 from tdas/docs-update
  [Updated java API docs for streaming, along with very minor changes in the code examples.]

  03019d1 Sat Jan 18 16:29:43 2014 -0800
  Merge pull request #459 from srowen/UpdaterL2Regularization
  [Correct L2 regularized weight update with canonical form]

  76147a2 Sat Jan 18 16:24:16 2014 -0800
  Merge pull request #437 from mridulm/master
  [Minor api usability changes]

  4ac8cab Sat Jan 18 16:22:46 2014 -0800
  Merge pull request #426 from mateiz/py-ml-tests
  [Re-enable Python MLlib tests (require Python 2.7 and NumPy 1.7+)]

  34e911c Sat Jan 18 16:17:34 2014 -0800
  Merge pull request #462 from mateiz/conf-file-fix
  [Remove Typesafe Config usage and conf files to fix nested property names]

  ff7201c Sat Jan 18 12:50:02 2014 -0800
  Merge pull request #461 from pwendell/master
  [Use renamed shuffle spill config in CoGroupedRDD.scala]

  7b0d5a5 Thu Jan 16 23:18:48 2014 -0800
  Merge pull request #451 from Qiuzhuang/master
  [Fixed Window spark shell launch script error.]

  4ccedb3 Wed Jan 15 14:26:48 2014 -0800
  Merge pull request #444 from mateiz/py-version
  [Clarify that Python 2.7 is only needed for MLlib]

  e3fa36f Wed Jan 15 13:56:04 2014 -0800
  Merge pull request #442 from pwendell/standalone
  [Workers should use working directory as spark home if it's not specified]

  29c76d9 Wed Jan 15 13:55:48 2014 -0800
  Merge pull request #443 from tdas/filestream-fix
  [Made some classes private[stremaing] and deprecated a method in JavaStreamingContext.]

  aca40aa Wed Jan 15 11:15:47 2014 -0800
  Merge pull request #441 from pwendell/graphx-build
  [GraphX shouldn't list Spark as provided.]

  e12c374 Wed Jan 15 10:01:43 2014 -0800
  Merge pull request #433 from markhamstra/debFix
  [Updated Debian packaging]

  2f015c2 Tue Jan 14 23:17:28 2014 -0800
  Merge pull request #436 from ankurdave/VertexId-case
  [Rename VertexID -> VertexId in GraphX]

  2859cab Tue Jan 14 23:08:19 2014 -0800
  Merge pull request #435 from tdas/filestream-fix
  [Fixed the flaky tests by making SparkConf not serializable]

  fbfbb33 Tue Jan 14 23:06:29 2014 -0800
  Merge pull request #434 from rxin/graphxmaven
  [Fixed SVDPlusPlusSuite in Maven build.]

  2c6c07f Tue Jan 14 21:53:05 2014 -0800
  Merge pull request #424 from jegonzal/GraphXProgrammingGuide
  [Additional edits for clarity in the graphx programming guide.]

  6fa4e02 Tue Jan 14 21:51:25 2014 -0800
  Merge pull request #431 from ankurdave/graphx-caching-doc
  [Describe caching and uncaching in GraphX programming guide]

  2f930d5 Tue Jan 14 15:00:11 2014 -0800
  Merge pull request #428 from pwendell/writeable-objects
  [Don't clone records for text files]

  329c9df Tue Jan 14 14:53:36 2014 -0800
  Merge pull request #429 from ankurdave/graphx-examples-pom.xml
  [Add GraphX dependency to examples/pom.xml]

  a14933d Tue Jan 14 14:52:42 2014 -0800
  Merge pull request #427 from pwendell/deprecate-aggregator
  [Deprecate rather than remove old combineValuesByKey function]

  119b6c5 Tue Jan 14 13:29:08 2014 -0800
  Merge pull request #425 from rxin/scaladoc
  [API doc update & make Broadcast public]

  bf3b150 Tue Jan 14 09:45:22 2014 -0800
  Merge pull request #423 from jegonzal/GraphXProgrammingGuide
  [Improving the graphx-programming-guide]

  1b4adc2 Tue Jan 14 01:19:24 2014 -0800
  Merge pull request #420 from pwendell/header-files
  [Add missing header files]

  b60840e Tue Jan 14 00:48:34 2014 -0800
  Merge pull request #418 from pwendell/0.9-versions
  [Version changes for release 0.9.0.]

  980250b Tue Jan 14 00:05:37 2014 -0800
  Merge pull request #416 from tdas/filestream-fix
  [Removed unnecessary DStream operations and updated docs]

  055be5c Mon Jan 13 23:26:44 2014 -0800
  Merge pull request #415 from pwendell/shuffle-compress
  [Enable compression by default for spills]

  fdaabdc Mon Jan 13 23:08:26 2014 -0800
  Merge pull request #380 from mateiz/py-bayes
  [Add Naive Bayes to Python MLlib, and some API fixes]

  4a805af Mon Jan 13 22:58:38 2014 -0800
  Merge pull request #367 from ankurdave/graphx
  [GraphX: Unifying Graphs and Tables]

  945fe7a Mon Jan 13 22:56:12 2014 -0800
  Merge pull request #408 from pwendell/external-serializers
  [Improvements to external sorting]

  68641bc Mon Jan 13 22:54:13 2014 -0800
  Merge pull request #413 from rxin/scaladoc
  [Adjusted visibility of various components and documentation for 0.9.0 release.]

  0ca0d4d Mon Jan 13 22:32:21 2014 -0800
  Merge pull request #401 from andrewor14/master
  [External sorting - Add number of bytes spilled to Web UI]

  08b9fec Mon Jan 13 22:29:03 2014 -0800
  Merge pull request #409 from tdas/unpersist
  [Automatically unpersisting RDDs that have been cleaned up from DStreams]

  b07bc02 Mon Jan 13 20:45:22 2014 -0800
  Merge pull request #412 from harveyfeng/master
  [Add default value for HadoopRDD's `cloneRecords` constructor arg]

  a2fee38 Mon Jan 13 19:45:26 2014 -0800
  Merge pull request #411 from tdas/filestream-fix
  [Improved logic of finding new files in FileInputDStream]

  01c0d72 Mon Jan 13 16:24:30 2014 -0800
  Merge pull request #410 from rxin/scaladoc1
  [Updated JavaStreamingContext to make scaladoc compile.]

  8038da2 Mon Jan 13 14:59:30 2014 -0800
  Merge pull request #2 from jegonzal/GraphXCCIssue
  [Improving documentation and identifying potential bug in CC calculation.]

  b93f9d4 Mon Jan 13 12:18:05 2014 -0800
  Merge pull request #400 from tdas/dstream-move
  [Moved DStream and PairDSream to org.apache.spark.streaming.dstream]

  e6ed13f Sun Jan 12 22:35:14 2014 -0800
  Merge pull request #397 from pwendell/host-port
  [Remove now un-needed hostPort option]

  0b96d85 Sun Jan 12 21:31:43 2014 -0800
  Merge pull request #399 from pwendell/consolidate-off
  [Disable shuffle file consolidation by default]

  0ab505a Sun Jan 12 21:31:04 2014 -0800
  Merge pull request #395 from hsaputra/remove_simpleredundantreturn_scala
  [Remove simple redundant return statements for Scala methods/functions]

  405bfe8 Sun Jan 12 20:04:21 2014 -0800
  Merge pull request #394 from tdas/error-handling
  [Better error handling in Spark Streaming and more API cleanup]

  28a6b0c Sun Jan 12 19:49:36 2014 -0800
  Merge pull request #398 from pwendell/streaming-api
  [Rename DStream.foreach to DStream.foreachRDD]

  074f502 Sun Jan 12 17:01:13 2014 -0800
  Merge pull request #396 from pwendell/executor-env
  [Setting load defaults to true in executor]

  82e2b92 Sun Jan 12 16:55:11 2014 -0800
  Merge pull request #392 from rxin/listenerbus
  [Stop SparkListenerBus daemon thread when DAGScheduler is stopped.]

  288a878 Sat Jan 11 21:53:19 2014 -0800
  Merge pull request #389 from rxin/clone-writables
  [Minor update for clone writables and more documentation.]

  dbc11df Sat Jan 11 18:07:13 2014 -0800
  Merge pull request #388 from pwendell/master
  [Fix UI bug introduced in #244.]

  409866b Sat Jan 11 17:12:06 2014 -0800
  Merge pull request #393 from pwendell/revert-381
  [Revert PR 381]

  6510f04 Sat Jan 11 12:48:26 2014 -0800
  Merge pull request #387 from jerryshao/conf-fix
  [Fix configure didn't work small problem in ALS]

  ee6e7f9 Sat Jan 11 12:07:55 2014 -0800
  Merge pull request #359 from ScrapCodes/clone-writables
  [We clone hadoop key and values by default and reuse objects if asked to.]

  4216178 Sat Jan 11 09:46:48 2014 -0800
  Merge pull request #373 from jerryshao/kafka-upgrade
  [Upgrade Kafka dependecy to 0.8.0 release version]

  92ad18b Fri Jan 10 23:25:15 2014 -0800
  Merge pull request #376 from prabeesh/master
  [Change clientId to random clientId]

  0b5ce7a Fri Jan 10 23:23:21 2014 -0800
  Merge pull request #386 from pwendell/typo-fix
  [Small typo fix]

  1d7bef0 Fri Jan 10 18:53:03 2014 -0800
  Merge pull request #381 from mateiz/default-ttl
  [Fix default TTL for metadata cleaner]

  44d6a8e Fri Jan 10 17:51:50 2014 -0800
  Merge pull request #382 from RongGu/master
  [Fix a type error in comment lines]

  88faa30 Fri Jan 10 17:14:22 2014 -0800
  Merge pull request #385 from shivaram/add-i2-instances
  [Add i2 instance types to Spark EC2.]

  f265531 Fri Jan 10 16:25:44 2014 -0800
  Merge pull request #383 from tdas/driver-test
  [API for automatic driver recovery for streaming programs and other bug fixes]

  d37408f Fri Jan 10 16:25:01 2014 -0800
  Merge pull request #377 from andrewor14/master
  [External Sorting for Aggregator and CoGroupedRDDs (Revisited)]

  0eaf01c Fri Jan 10 15:32:19 2014 -0800
  Merge pull request #369 from pillis/master
  [SPARK-961 Add a Vector.random() method]

  7cef843 Fri Jan 10 15:34:15 2014 -0600
  Merge pull request #371 from tgravescs/yarn_client_addjar_misc_fixes
  [Yarn client addjar and misc fixes]

  7b58f11 Fri Jan 10 12:47:46 2014 -0800
  Merge pull request #384 from pwendell/debug-logs
  [Make DEBUG-level logs consummable.]

  23d2995 Fri Jan 10 10:20:02 2014 -0800
  Merge pull request #1 from jegonzal/graphx
  [ProgrammingGuide]

  0ebc973 Thu Jan 9 23:58:49 2014 -0800
  Merge pull request #375 from mateiz/option-fix
  [Fix bug added when we changed AppDescription.maxCores to an Option]

  dd03cea Thu Jan 9 23:38:03 2014 -0800
  Merge pull request #378 from pwendell/consolidate_on
  [Enable shuffle consolidation by default.]

  997c830 Thu Jan 9 22:22:20 2014 -0800
  Merge pull request #363 from pwendell/streaming-logs
  [Set default logging to WARN for Spark streaming examples.]

  300eaa9 Thu Jan 9 20:29:51 2014 -0800
  Merge pull request #353 from pwendell/ipython-simplify
  [Simplify and fix pyspark script.]

  4b074fa Thu Jan 9 19:03:55 2014 -0800
  Merge pull request #374 from mateiz/completeness
  [Add some missing Java API methods]

  a9d5333 Thu Jan 9 18:46:46 2014 -0800
  Merge pull request #294 from RongGu/master
  [Bug fixes for updating the RDD block's memory and disk usage information]

  d86a85e Thu Jan 9 18:37:52 2014 -0800
  Merge pull request #293 from pwendell/standalone-driver
  [SPARK-998: Support Launching Driver Inside of Standalone Mode]

  26cdb5f Thu Jan 9 17:16:34 2014 -0800
  Merge pull request #372 from pwendell/log4j-fix-1
  [Send logs to stderr by default (instead of stdout).]

  12f414e Thu Jan 9 15:31:30 2014 -0800
  Merge pull request #362 from mateiz/conf-getters
  [Use typed getters for configuration settings]

  365cac9 Thu Jan 9 00:56:16 2014 -0800
  Merge pull request #361 from rxin/clean
  [Minor style cleanup. Mostly on indenting & line width changes.]

  73c724e Thu Jan 9 00:32:19 2014 -0800
  Merge pull request #368 from pwendell/sbt-fix
  [Don't delegate to users `sbt`.]

  dceedb4 Wed Jan 8 23:19:28 2014 -0800
  Merge pull request #364 from pwendell/fix
  [Fixing config option ""retained_stages"" => ""retainedStages"".]

  04d83fc Wed Jan 8 11:55:37 2014 -0800
  Merge pull request #360 from witgo/master
  [fix make-distribution.sh show version: command not found]

  56ebfea Wed Jan 8 11:50:06 2014 -0800
  Merge pull request #357 from hsaputra/set_boolean_paramname
  [Set boolean param name for call to SparkHadoopMapReduceUtil.newTaskAttemptID]

  bdeaeaf Wed Jan 8 11:48:39 2014 -0800
  Merge pull request #358 from pwendell/add-cdh
  [Add CDH Repository to Maven Build]

  5cae05f Wed Jan 8 11:47:28 2014 -0800
  Merge pull request #356 from hsaputra/remove_deprecated_cleanup_method
  [Remove calls to deprecated mapred's OutputCommitter.cleanupJob]

  6eef78d Wed Jan 8 08:49:20 2014 -0600
  Merge pull request #345 from colorant/yarn
  [support distributing extra files to worker for yarn client mode]

  bb6a39a Tue Jan 7 22:32:18 2014 -0800
  Merge pull request #322 from falaki/MLLibDocumentationImprovement
  [SPARK-1009 Updated MLlib docs to show how to use it in Python]

  cb1b927 Tue Jan 7 22:26:28 2014 -0800
  Merge pull request #355 from ScrapCodes/patch-1
  [Update README.md]

  c0f0155 Tue Jan 7 22:21:52 2014 -0800
  Merge pull request #313 from tdas/project-refactor
  [Refactored the streaming project to separate external libraries like Twitter, Kafka, Flume, etc.]

  f5f12dc Tue Jan 7 21:56:35 2014 -0800
  Merge pull request #336 from liancheng/akka-remote-lookup
  [Get rid of `Either[ActorRef, ActorSelection]']

  11891e6 Wed Jan 8 00:32:18 2014 -0500
  Merge pull request #327 from lucarosellini/master
  [Added ‚Äò-i‚Äô command line option to Spark REPL]

  7d0aac9 Wed Jan 8 00:30:45 2014 -0500
  Merge pull request #354 from hsaputra/addasfheadertosbt
  [Add ASF header to the new sbt script.]

  d75dc42 Wed Jan 8 00:30:03 2014 -0500
  Merge pull request #350 from mateiz/standalone-limit
  [Add way to limit default # of cores used by apps in standalone mode]

  61674bc Tue Jan 7 18:32:13 2014 -0800
  Merge pull request #352 from markhamstra/oldArch
  [Don't leave os.arch unset after BlockManagerSuite]

  b2e690f Tue Jan 7 16:57:08 2014 -0800
  Merge pull request #328 from falaki/MatrixFactorizationModel-fix
  [SPARK-1012: DAGScheduler Exception Fix]

  6ccf8ce Tue Jan 7 15:49:14 2014 -0800
  Merge pull request #351 from pwendell/maven-fix
  [Add log4j exclusion rule to maven.]

  7d5fa17 Tue Jan 7 11:31:34 2014 -0800
  Merge pull request #337 from yinxusen/mllib-16-bugfix
  [Mllib 16 bugfix]

  71fc113 Tue Jan 7 11:30:35 2014 -0800
  Merge pull request #349 from CodingCat/support-worker_dir
  [add the comments about SPARK_WORKER_DIR]

  15d9534 Tue Jan 7 08:10:02 2014 -0800
  Merge pull request #318 from srowen/master
  [Suggested small changes to Java code for slightly more standard style, encapsulation and in some cases performance]

  468af0f Tue Jan 7 08:09:01 2014 -0800
  Merge pull request #348 from prabeesh/master
  [spark -> org.apache.spark]

  c3cf047 Tue Jan 7 00:54:25 2014 -0800
  Merge pull request #339 from ScrapCodes/conf-improvements
  [Conf improvements]

  a862caf Tue Jan 7 00:18:20 2014 -0800
  Merge pull request #331 from holdenk/master
  [Add a script to download sbt if not present on the system]

  b97ef21 Mon Jan 6 20:12:57 2014 -0800
  Merge pull request #346 from sproblvem/patch-1
  [Update stop-slaves.sh]

  7210257 Mon Jan 6 18:25:44 2014 -0800
  Merge pull request #128 from adamnovak/master
  [Fix failing ""sbt/sbt publish-local"" by adding a no-argument PrimitiveKeyOpenHashMap constructor]

  e4d6057 Mon Jan 6 14:56:54 2014 -0800
  Merge pull request #343 from pwendell/build-fix
  [Fix test breaking downstream builds]

  93bf962 Mon Jan 6 11:42:41 2014 -0800
  Merge pull request #340 from ScrapCodes/sbt-fixes
  [Made java options to be applied during tests so that they become self explanatory.]

  60edeb3 Mon Jan 6 11:40:32 2014 -0800
  Merge pull request #338 from ScrapCodes/ning-upgrade
  [SPARK-1005 Ning upgrade]

  c708e81 Mon Jan 6 11:35:48 2014 -0800
  Merge pull request #341 from ash211/patch-5
  [Clarify spark.cores.max in docs]

  33fcb91 Mon Jan 6 11:19:23 2014 -0800
  Merge pull request #342 from tgravescs/fix_maven_protobuf
  [Change protobuf version for yarn alpha back to 2.4.1]

  357083c Mon Jan 6 10:29:04 2014 -0800
  Merge pull request #330 from tgravescs/fix_addjars_null_handling
  [Fix handling of empty SPARK_EXAMPLES_JAR]

  a2e7e04 Sun Jan 5 22:37:36 2014 -0800
  Merge pull request #333 from pwendell/logging-silence
  [Quiet ERROR-level Akka Logs]

  5b0986a Sun Jan 5 19:25:09 2014 -0800
  Merge pull request #334 from pwendell/examples-fix
  [Removing SPARK_EXAMPLES_JAR in the code]

  f4b924f Sun Jan 5 17:11:47 2014 -0800
  Merge pull request #335 from rxin/ser
  [Fall back to zero-arg constructor for Serializer initialization if there is no constructor that accepts SparkConf.]

  d43ad3e Sat Jan 4 16:29:30 2014 -0800
  Merge pull request #292 from soulmachine/naive-bayes
  [standard Naive Bayes classifier]

  86404da Sat Jan 4 14:55:54 2014 -0800
  Merge pull request #127 from jegonzal/MapByPartition
  [Adding mapEdges and mapTriplets by Partition]

  e68cdb1 Sat Jan 4 13:46:02 2014 -0800
  Merge pull request #124 from jianpingjwang/master
  [refactor and bug fix]

  280ddf6 Sat Jan 4 12:54:41 2014 -0800
  Merge pull request #121 from ankurdave/more-simplify
  [Simplify GraphImpl internals further]

  10fe23b Fri Jan 3 23:50:14 2014 -0800
  Merge pull request #329 from pwendell/remove-binaries
  [SPARK-1002: Remove Binaries from Spark Source]

  c4d6145 Fri Jan 3 16:30:53 2014 -0800
  Merge pull request #325 from witgo/master
  [Modify spark on yarn to create SparkConf process]

  4ae101f Fri Jan 3 11:24:35 2014 -0800
  Merge pull request #317 from ScrapCodes/spark-915-segregate-scripts
  [Spark-915 segregate scripts]

  87248bd Fri Jan 3 00:45:31 2014 -0800
  Merge pull request #1 from apache/master
  [Merge latest Spark changes]

  30b9db0 Thu Jan 2 23:15:55 2014 -0800
  Merge pull request #285 from colorant/yarn-refactor
  [Yarn refactor]

  498a5f0 Thu Jan 2 19:06:40 2014 -0800
  Merge pull request #323 from tgravescs/sparkconf_yarn_fix
  [fix spark on yarn after the sparkConf changes]

  0475ca8 Thu Jan 2 15:17:08 2014 -0800
  Merge pull request #320 from kayousterhout/erroneous_failed_msg
  [Remove erroneous FAILED state for killed tasks.]

  588a169 Thu Jan 2 13:20:54 2014 -0800
  Merge pull request #297 from tdas/window-improvement
  [Improvements to DStream window ops and refactoring of Spark's CheckpointSuite]

  5e67cdc Thu Jan 2 12:56:28 2014 -0800
  Merge pull request #319 from kayousterhout/remove_error_method
  [Removed redundant TaskSetManager.error() function.]

  ca67909 Thu Jan 2 15:54:54 2014 -0500
  Merge pull request #311 from tmyklebu/master
  [SPARK-991: Report information gleaned from a Python stacktrace in the UI]

  3713f81 Wed Jan 1 21:29:12 2014 -0800
  Merge pull request #309 from mateiz/conf2
  [SPARK-544. Migrate configuration to a SparkConf class]

  c1d928a Wed Jan 1 17:03:48 2014 -0800
  Merge pull request #312 from pwendell/log4j-fix-2
  [SPARK-1008: Logging improvments]

  dc9cb83 Wed Jan 1 13:28:34 2014 -0800
  Merge pull request #126 from jegonzal/FixingPersist
  [Fixing Persist Behavior]

  9a0ff72 Tue Dec 31 21:50:24 2013 -0800
  Merge pull request #314 from witgo/master
  [restore core/pom.xml file modification]

  8b8e70e Tue Dec 31 17:48:24 2013 -0800
  Merge pull request #73 from falaki/ApproximateDistinctCount
  [Approximate distinct count]

  63b411d Tue Dec 31 14:31:28 2013 -0800
  Merge pull request #238 from ngbinh/upgradeNetty
  [upgrade Netty from 4.0.0.Beta2 to 4.0.13.Final]

  32d6ae9 Tue Dec 31 13:51:07 2013 -0800
  Merge pull request #120 from ankurdave/subgraph-reuses-view
  [Reuse VTableReplicated in GraphImpl.subgraph]

  55b7e2f Tue Dec 31 10:12:51 2013 -0800
  Merge pull request #289 from tdas/filestream-fix
  [Bug fixes for file input stream and checkpointing]

  2b71ab9 Mon Dec 30 11:01:30 2013 -0800
  Merge pull request from aarondav: Utilize DiskBlockManager pathway for temp file writing
  [This gives us a couple advantages:]

  50e3b8e Mon Dec 30 07:44:26 2013 -0800
  Merge pull request #308 from kayousterhout/stage_naming
  [Changed naming of StageCompleted event to be consistent]

  72a17b6 Sat Dec 28 21:25:40 2013 -1000
  Revert ""Merge pull request #310 from jyunfan/master""
  [This reverts commit 79b20e4dbe3dcd8559ec8316784d3334bb55868b, reversing]

  79b20e4 Sat Dec 28 21:13:36 2013 -1000
  Merge pull request #310 from jyunfan/master
  [Fix typo in the Accumulators section]

  7375047 Sat Dec 28 13:25:06 2013 -0800
  Merge pull request #304 from kayousterhout/remove_unused
  [Removed unused failed and causeOfFailure variables (in TaskSetManager)]

  ad3dfd1 Fri Dec 27 22:10:14 2013 -0500
  Merge pull request #307 from kayousterhout/other_failure
  [Removed unused OtherFailure TaskEndReason.]

  b579b83 Fri Dec 27 22:09:04 2013 -0500
  Merge pull request #306 from kayousterhout/remove_pending
  [Remove unused hasPendingTasks methods]

  19672dc Fri Dec 27 13:37:10 2013 -0800
  Merge pull request #305 from kayousterhout/line_spacing
  [Fixed >100char lines in DAGScheduler.scala]

  7be1e57 Thu Dec 26 23:41:40 2013 -1000
  Merge pull request #298 from aarondav/minor
  [Minor: Decrease margin of left side of Log page]

  7d811ba Thu Dec 26 23:39:58 2013 -1000
  Merge pull request #302 from pwendell/SPARK-1007
  [SPARK-1007: spark-class2.cmd should change SCALA_VERSION to be 2.10]

  5e69fc5 Thu Dec 26 19:10:39 2013 -0500
  Merge pull request #295 from markhamstra/JobProgressListenerNPE
  [Avoid a lump of coal (NPE) in JobProgressListener's stocking.]

  da20270 Thu Dec 26 12:11:52 2013 -0800
  Merge pull request #1 from aarondav/driver
  [Refactor DriverClient to be more Actor-based]

  e240bad Thu Dec 26 12:30:48 2013 -0500
  Merge pull request #296 from witgo/master
  [Renamed ClusterScheduler to TaskSchedulerImpl for yarn and new-yarn package]

  c344ed0 Thu Dec 26 01:31:06 2013 -0500
  Merge pull request #283 from tmyklebu/master
  [Python bindings for mllib]

  56094bc Wed Dec 25 13:14:33 2013 -0500
  Merge pull request #290 from ash211/patch-3
  [Typo: avaiable -> available]

  4842a07 Wed Dec 25 01:52:15 2013 -0800
  Merge pull request #287 from azuryyu/master
  [Fixed job name in the java streaming example.]

  85a344b Tue Dec 24 16:35:06 2013 -0800
  Merge pull request #127 from kayousterhout/consolidate_schedulers
  [Deduplicate Local and Cluster schedulers.]

  c2dd6bc Tue Dec 24 14:36:47 2013 -0800
  Merge pull request #279 from aarondav/shuffle-cleanup0
  [Clean up shuffle files once their metadata is gone]

  3bf7c70 Tue Dec 24 16:37:13 2013 -0500
  Merge pull request #275 from ueshin/wip/changeclasspathorder
  [Change the order of CLASSPATH.]

  d63856c Mon Dec 23 22:07:26 2013 -0800
  Merge pull request #286 from rxin/build
  [Show full stack trace and time taken in unit tests.]

  23a9ae6 Tue Dec 24 00:08:48 2013 -0500
  Merge pull request #277 from tdas/scheduler-update
  [Refactored the streaming scheduler and added StreamingListener interface]

  11107c9 Mon Dec 23 10:38:20 2013 -0800
  Merge pull request #244 from leftnoteasy/master
  [Added SPARK-968 implementation for review]

  44e4205 Sun Dec 22 11:44:18 2013 -0800
  Merge pull request #116 from jianpingjwang/master
  [remove unused variables and fix a bug]

  4797c22 Fri Dec 20 13:30:39 2013 -0800
  Merge pull request #118 from ankurdave/VertexPartitionSuite
  [Test VertexPartition and fix bugs]

  0bc57c5 Fri Dec 20 11:56:54 2013 -0800
  Merge pull request #280 from aarondav/minor
  [Minor cleanup for standalone scheduler]

  ac70b8f Fri Dec 20 10:56:10 2013 -0800
  Merge pull request #117 from ankurdave/more-tests
  [More tests]

  45310d4 Thu Dec 19 22:08:20 2013 -0800
  Merge pull request #115 from ankurdave/test-reorg
  [Reorganize unit tests; improve GraphSuite test coverage]

  9228ec8 Thu Dec 19 21:37:15 2013 -0800
  Merge pull request #1 from aarondav/127
  [Merge master into 127]

  eca68d4 Thu Dec 19 18:12:22 2013 -0800
  Merge pull request #272 from tmyklebu/master
  [Track and report task result serialisation time.]

  7990c56 Thu Dec 19 13:35:09 2013 -0800
  Merge pull request #276 from shivaram/collectPartition
  [Add collectPartition to JavaRDD interface.]

  440e531 Thu Dec 19 10:38:56 2013 -0800
  Merge pull request #278 from MLnick/java-python-tostring
  [Add toString to Java RDD, and __repr__ to Python RDD]

  d8d3f3e Thu Dec 19 00:06:43 2013 -0800
  Merge pull request #183 from aarondav/spark-959
  [[SPARK-959] Explicitly depend on org.eclipse.jetty.orbit jar]

  bfba532 Wed Dec 18 22:22:21 2013 -0800
  Merge pull request #247 from aarondav/minor
  [Increase spark.akka.askTimeout default to 30 seconds]

  da301b5 Wed Dec 18 20:03:29 2013 -0800
  Merge pull request #112 from amatsukawa/scc
  [Strongly connected component algorithm]

  c64a53a Wed Dec 18 16:56:26 2013 -0800
  Merge pull request #267 from JoshRosen/cygwin
  [Fix Cygwin support in several scripts.]

  a645ef6 Wed Dec 18 16:07:52 2013 -0800
  Merge pull request #48 from amatsukawa/add_project_to_graph
  [Add mask operation on graph and filter graph primitive]

  d7ebff0 Wed Dec 18 15:38:48 2013 -0800
  Merge pull request #1 from ankurdave/add_project_to_graph
  [Merge current master and reimplement Graph.mask using innerJoin]

  5ea1872 Wed Dec 18 15:27:24 2013 -0800
  Merge pull request #274 from azuryy/master
  [Fixed the example link in the Scala programing guid.]

  3fd2e09 Wed Dec 18 12:52:36 2013 -0800
  Merge pull request #104 from jianpingjwang/master
  [SVD++ demo]

  f4effb3 Tue Dec 17 22:26:21 2013 -0800
  Merge pull request #273 from rxin/top
  [Fixed a performance problem in RDD.top and BoundedPriorityQueue]

  1b5eacb Tue Dec 17 13:49:17 2013 -0800
  Merge pull request #102 from ankurdave/clustered-edge-index
  [Add clustered index on edges by source vertex]

  7a8169b Mon Dec 16 22:42:21 2013 -0800
  Merge pull request #268 from pwendell/shaded-protobuf
  [Add support for 2.2. to master (via shaded jars)]

  0476c84 Mon Dec 16 17:19:25 2013 -0800
  Merge pull request #100 from ankurdave/mrTriplets-active-set
  [Support activeSet option in mapReduceTriplets]

  964a3b6 Mon Dec 16 15:23:51 2013 -0800
  Merge pull request #270 from ewencp/really-force-ssh-pseudo-tty-master
  [Force pseudo-tty allocation in spark-ec2 script.]

  5192ef3 Mon Dec 16 15:08:08 2013 -0800
  Merge pull request #94 from ankurdave/load-edges-columnar
  [Load edges in columnar format]

  883e034 Mon Dec 16 14:16:02 2013 -0800
  Merge pull request #245 from gregakespret/task-maxfailures-fix
  [Fix for spark.task.maxFailures not enforced correctly.]

  a51f340 Sun Dec 15 22:02:30 2013 -0800
  Merge pull request #265 from markhamstra/scala.binary.version
  [DRY out the POMs with scala.binary.version]

  ded10ce Sun Dec 15 17:25:33 2013 -0800
  Merge pull request #103 from amplab/optimizations
  [Optimizations cherry-picked from SIGMOD branches]

  d2ced6d Sun Dec 15 14:11:34 2013 -0800
  Merge pull request #256 from MLnick/master
  [Fix 'IPYTHON=1 ./pyspark' throwing ValueError]

  c55e698 Sun Dec 15 12:49:02 2013 -0800
  Merge pull request #257 from tgravescs/sparkYarnFixName
  [Fix the --name option for Spark on Yarn]

  ab85f88 Sun Dec 15 12:48:32 2013 -0800
  Merge pull request #264 from shivaram/spark-class-fix
  [Use CoarseGrainedExecutorBackend in spark-class]

  8a56c1f Sat Dec 14 16:29:24 2013 -0800
  Merge pull request #84 from amatsukawa/graphlab_enhancements
  [GraphLab bug fix & set start vertex]

  7db9165 Sat Dec 14 14:16:34 2013 -0800
  Merge pull request #251 from pwendell/master
  [Fix list rendering in YARN markdown docs.]

  2fd781d Sat Dec 14 12:59:37 2013 -0800
  Merge pull request #249 from ngbinh/partitionInJavaSortByKey
  [Expose numPartitions parameter in JavaPairRDD.sortByKey()]

  9bf192b Sat Dec 14 12:52:18 2013 -0800
  Merge pull request #91 from amplab/standalone-pagerank
  [Standalone PageRank]

  840af5e Sat Dec 14 12:51:51 2013 -0800
  Merge pull request #99 from ankurdave/only-dynamic-pregel
  [Remove static Pregel; take maxIterations in dynamic Pregel]

  97ac060 Sat Dec 14 00:22:45 2013 -0800
  Merge pull request #259 from pwendell/scala-2.10
  [Migration to Scala 2.10]

  7ac944f Fri Dec 13 23:22:08 2013 -0800
  Merge pull request #262 from pwendell/mvn-fix
  [Fix maven build issues in 2.10 branch]

  6defb06 Fri Dec 13 21:18:57 2013 -0800
  Merge pull request #261 from ScrapCodes/scala-2.10
  [Added a comment about ActorRef and ActorSelection difference.]

  76566b1 Fri Dec 13 10:11:02 2013 -0800
  Merge pull request #260 from ScrapCodes/scala-2.10
  [Review comments on the PR for scala 2.10 migration.]

  0aeb182 Thu Dec 12 21:14:42 2013 -0800
  Merge pull request #255 from ScrapCodes/scala-2.10
  [Disabled yarn 2.2 in sbt and mvn build and added a message in the sbt build.]

  2e89398 Wed Dec 11 23:10:53 2013 -0800
  Merge pull request #254 from ScrapCodes/scala-2.10
  [Scala 2.10 migration]

  ce6ca4e Wed Dec 11 22:30:54 2013 -0800
  Merge pull request #97 from dcrankshaw/fix-rddtop
  [Added BoundedPriorityQueue kryo registrator. Fixes top issue.]

  d2efe13 Tue Dec 10 13:01:26 2013 -0800
  Merge pull request #250 from pwendell/master
  [README incorrectly suggests build sources spark-env.sh]

  6169fe1 Mon Dec 9 16:51:36 2013 -0800
  Merge pull request #246 from pwendell/master
  [Add missing license headers]

  d992ec6 Sun Dec 8 20:49:20 2013 -0800
  Merge pull request #195 from dhardy92/fix_DebScriptPackage
  [[Deb] fix package of Spark classes adding org.apache prefix in scripts embeded in .deb]

  1f4a4bc Sat Dec 7 22:34:34 2013 -0800
  Merge pull request #242 from pwendell/master
  [Update broken links and add HDP 2.0 version string]

  6494d62 Sat Dec 7 11:56:16 2013 -0800
  Merge pull request #240 from pwendell/master
  [SPARK-917 Improve API links in nav bar]

  f466f79 Sat Dec 7 11:51:52 2013 -0800
  Merge pull request #239 from aarondav/nit
  [Correct spellling error in configuration.md]

  3abfbfb Sat Dec 7 11:24:19 2013 -0800
  Merge pull request #92 from ankurdave/rdd-names
  [Set RDD names for easy debugging]

  31e8a14 Fri Dec 6 21:49:55 2013 -0800
  Merge pull request #90 from amplab/pregel-replicate-changed
  [Replicate only changed vertices]

  10c3c0c Fri Dec 6 20:29:45 2013 -0800
  Merge pull request #237 from pwendell/formatting-fix
  [Formatting fix]

  1b38f5f Fri Dec 6 20:16:15 2013 -0800
  Merge pull request #236 from pwendell/shuffle-docs
  [Adding disclaimer for shuffle file consolidation]

  e5d5728 Fri Dec 6 20:14:56 2013 -0800
  Merge pull request #235 from pwendell/master
  [Minor doc fixes and updating README]

  241336a Fri Dec 6 17:29:03 2013 -0800
  Merge pull request #234 from alig/master
  [Updated documentation about the YARN v2.2 build process]

  e039234 Fri Dec 6 11:49:59 2013 -0800
  Merge pull request #190 from markhamstra/Stages4Jobs
  [stageId <--> jobId mapping in DAGScheduler]

  bfa6860 Fri Dec 6 11:04:03 2013 -0800
  Merge pull request #233 from hsaputra/changecontexttobackend
  [Change the name of input argument in ClusterScheduler#initialize from context to backend.]

  3fb302c Fri Dec 6 11:03:32 2013 -0800
  Merge pull request #205 from kayousterhout/logging
  [Added logging of scheduler delays to UI]

  87676a6 Fri Dec 6 11:01:42 2013 -0800
  Merge pull request #220 from rxin/zippart
  [Memoize preferred locations in ZippedPartitionsBaseRDD]

  0780498 Thu Dec 5 23:29:42 2013 -0800
  Merge pull request #232 from markhamstra/FiniteWait
  [jobWaiter.synchronized before jobWaiter.wait]

  1c8500e Thu Dec 5 16:25:44 2013 -0800
  Merge pull request #88 from amplab/varenc
  [Fixed a bug that variable encoding doesn't work for ints that use all 64 bits.]

  e0bcaa0 Thu Dec 5 12:37:02 2013 -0800
  Merge pull request #86 from ankurdave/vid-varenc
  [Finish work on #85]

  5d46025 Thu Dec 5 12:31:24 2013 -0800
  Merge pull request #228 from pwendell/master
  [Document missing configs and set shuffle consolidation to false.]

  3e96b9a Thu Dec 5 12:07:36 2013 -0800
  Merge pull request #85 from ankurdave/vid-varenc
  [Always write Vids using variable encoding]

  72b6961 Wed Dec 4 23:33:04 2013 -0800
  Merge pull request #199 from harveyfeng/yarn-2.2
  [Hadoop 2.2 migration]

  e0347ba Wed Dec 4 17:38:06 2013 -0800
  Merge pull request #83 from ankurdave/fix-tests
  [Fix compile errors in GraphSuite and SerializerSuite]

  182f9ba Wed Dec 4 15:52:07 2013 -0800
  Merge pull request #227"
"
Patrick Wendell <pwendell@gmail.com>,Sun"," 19 Jan 2014 14:16:08 -0800""",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc2),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","This vote is cancelled in favor of rc3 - which fixes the YARN issue
Sandy ran into.

@taka - thanks for reporting that bug. It's not enough to block this
and it will be in 0.9.1


"
Tathagata Das <tathagata.das1565@gmail.com>,"Sun, 19 Jan 2014 19:09:24 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc3),dev@spark.incubator.apache.org,"Starting off.
 +1



"
Matei Zaharia <matei.zaharia@gmail.com>,"Sun, 19 Jan 2014 19:33:15 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc3),dev@spark.incubator.apache.org,"+1

Re-tested on Mac.

Matei


https://git-wip-us.apache.org/repos/asf?p=incubator-spark.git;a=commit;h=a7760eff4ea6a474cab68896a88550f63bae8b0d
at:
https://repository.apache.org/content/repositories/orgapachespark-1004/
0.9.0-incubating!


"
Patrick Wendell <pwendell@gmail.com>,"Sun, 19 Jan 2014 20:05:33 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc3),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I'll add my +1 as well


"
Patrick Wendell <pwendell@gmail.com>,"Sun, 19 Jan 2014 20:41:30 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc3),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Attempting to attach the release notes again (I think it may have been
blocked previously due to not having an extension).

Spark 0.9.0 is a major release that adds significant new features. It updates Spark to Scala 2.10, simplifies high availability, and updates numerous components of the project. This release includes a first version of GraphX, a powerful new framework for graph processing that comes with a library of standard algorithms. In addition, Spark Streaming is now out of alpha, and includes significant optimizations and simplified high availability deployment.

### Scala 2.10 Support

Spark now runs on Scala 2.10, letting users benefit from the language and library improvements in this version.

### Configuration System

The new [SparkConf] class is now the preferred way to configure advanced settings on your SparkContext, though the previous Java system property still works. SparkConf is especially useful in tests to make sure properties don‚Äôt stay set across tests.

### Spark Streaming Improvements

Spark Streaming is no longer alpha, and comes with simplified high availability and several optimizations.

* When running on a Spark standalone cluster with the [standalone cluster high availability mode], you can submit a Spark Streaming driver application to the cluster and have it automatically recovered if either the driver or the cluster master crashes.
* Windowed operators have been sped up by 30-50%.
* Spark Streaming‚Äôs input source plugins (e.g. for Twitter, Kafka and Flume) are now separate projects, making it easier to pull in only the dependencies you need.
* A new StreamingListener interface has been added for monitoring statistics about the streaming computation.
* A few aspects of the API have been improved:
* `DStream` and `PairDStream` classes have been moved from `org.apache.spark.streaming` to `org.apache.spark.streaming.dstream` to keep it consistent with `org.apache.spark.rdd.RDD`.
* `DStream.foreach` -> `DStream.foreachRDD` to make it explicit that it works for every RDD, not every element
* `StreamingContext.awaitTermination()` allows you wait for context shutdown and catch any exception that occurs in the streaming computation.
*`StreamingContext.stop()` now allows stopping of StreamingContext without stopping the underlying SparkContext.

### GraphX Alpha

GraphX is a new API for graph processing that uses recent advances in graph-parallel computation. It lets you build a graph within a Spark program using the standard Spark operators, then process it with new graph operators that are optimized for distributed computation. It includes basic transformations, a Pregel API for iterative computation, and a standard library of graph loaders and analytics algorithms. By offering these features within the Spark engine, GraphX can significantly speed up processing tasks compared to workflows that use different engines.

GraphX features in this release include:

* Building graphs from arbitrary Spark RDDs
* Basic operations to transform graphs or extract subgraphs
* An optimized Pregel API that takes advantage of graph partitioning and indexing
* Standard algorithms including PageRank, connected components, strongly connected components, SVD++, and triangle counting
* Interactive use from the Spark shell

GraphX is still marked as alpha in this first release, but we recommend for new users to use it instead of the more limited Bagel API.

### MLlib Improvements

* Spark‚Äôs machine learning library (MLlib) is now available in Python, where it operates on NumPy data (currently requires Python 2.7 and NumPy 1.7)
* A new algorithm has been added for Naive Bayes classification
* Alternating Least Squares can now be used to predict ratings for multiple items in parallel
* MLlib‚Äôs documentation was expanded to include more examples in Scala, Java and Python

### Python Changes

* Python users can now use MLlib (requires Python 2.7 and NumPy 1.7)
* PySpark now shows the call sites of running jobs in the Spark application UI (http://<driver>:4040), making it easy to see which part of your code is running
* IPython operation has been improved

### Packaging

* Spark‚Äôs scripts have been organized into ‚Äúbin‚Äù and ‚Äúsbin‚Äù directories to make it easier to separate admin scripts from user ones and install Spark on standard Linux paths.
* Log configuration has been improved so that Spark finds a default log4j.properties file if you don‚Äôt specify one.

### Core Engine

* Spark‚Äôs standalone mode now supports submitting a driver program to run on the cluster instead of on the external machine submitting it. You can access this functionality through the [org.apache.spark.deploy.Client] class.
* Large reduce operations now automatically spill data to disk if it does not fit in memory.
* Users of standalone mode can now limit how many cores an application will use by default if the application writer didn‚Äôt configure its size. Previously, such applications took all available cores on the cluster.
* `spark-shell` now supports the `-i` option to run a script on startup.
* New `histogram` and `countDistinctApprox` operators have been added for working with numerical data.
* Methods for reading Hadoop `Writable` objects, such as `SparkContext.sequenceFile` and `hadoopRDD`, now clone the records read by default instead of reusing the same Writable instance, making the resulting RDDs easier to cache; if you only want to use each object once, you can pass an extra flag to disable this cloning
* YARN mode now supports distributing extra files with the application, and several bugs have been fixed.

### Compatibility

This release is compatible with the previous APIs in stable components, but several language versions and script locations have changed.

* Scala programs now need to use Scala 2.10 instead of 2.9.
* Scripts such as `spark-shell` and `pyspark` have been moved into the `bin` folder, while administrative scripts to start and stop standalone clusters have been moved into `sbin`.
* Spark Streaming‚Äôs API has been changed to move external input sources into separate modules, `DStream` and `PairDStream` has been moved to package `org.apache.spark.streaming.dstream` and `DStream.foreach` has been renamed to `foreachRDD`. We expect the current API to be stable now that Spark Streaming is out of alpha.
* While the old method of configuring Spark through Java system properties still works, we recommend that users update to the new SparkConf, which is easier to inspect and use.

We expect all of the current APIs and script locations in Spark 0.9 to remain stable when we release Spark 1.0. We wanted to make these updates early to give users a chance to switch to the new API.


"
Roman Shaposhnik <rvs@apache.org>,"Sun, 19 Jan 2014 21:29:03 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc1),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Great! Was just about to pile on top of Mridul's feedback.

Thanks,
Roman.


"
Henry Saputra <henry.saputra@gmail.com>,"Sun, 19 Jan 2014 21:57:58 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc3),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi Patrick, quick question, where are you planning to add the release notes?
I dont think it is part of the source, is it?

- Henry


"
Patrick Wendell <pwendell@gmail.com>,"Sun, 19 Jan 2014 22:01:24 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc3),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Eventually the notes get posted on the apache website. I attached them
to this e-mail so that people can get a sense of what is in the
release before they vote on it.


"
Pillis W <pillis.work@gmail.com>,"Sun, 19 Jan 2014 22:06:51 -0800",Re: About Spark job web ui persist(JIRA-969),dev@spark.incubator.apache.org,"Hi Junluan,
I was thinking that the SparkContextData structure will mirror what the UI
needs, as it is a good use case.
It could look like this approximately
- SparkContextData
  - StagesData
    - SchedulingMode
    - Stages[]
  - StorageData
  - EnvironmentData
    - Map <category:String, Map<key:String, value:String>>
  - ExecutorsData
    - Executors[]

Metrics can show the data structure in JMX as is.

I was thinking the UI will look exactly the same as it does right now,
except that there will be a Combo-box on the title bar to pick the
same.
state - running, stopped, etc.

Hope that helps.
Regards,
Pillis





"
Henry Saputra <henry.saputra@gmail.com>,"Sun, 19 Jan 2014 22:07:30 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc3),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Ah yes, makes sense, thanks!

- Henry


"
Eduardo Costa Alfaia <e.costaalfaia@studenti.unibs.it>,"Mon, 20 Jan 2014 11:02:27 +0100",Print in JavaNetworkWordCount,user@spark.incubator.apache.org,"Hi guys,

Somebody help me, Where do I get change the print() function to print more than 10 lines in screen? Is there a manner to print the count total of all words in a batch?

Best Regards 
-- 
---
INFORMATIVA SUL TRATTAMENTO DEI DATI PERSONALI

I dati utilizzati per l'invio del presente messaggio sono trattati 
dall'Universit‡ degli Studi di Brescia esclusivamente per finalit‡ 
istituzionali. Informazioni pi˘ dettagliate anche in ordine ai diritti 
dell'interessato sono riposte nell'informativa generale e nelle notizie 
pubblicate sul sito web dell'Ateneo nella sezione ""Privacy"".

Il contenuto di questo messaggio Ë rivolto unicamente alle persona cui 
Ë indirizzato e puÚ contenere informazioni la cui riservatezza Ë 
tutelata legalmente. Ne sono vietati la riproduzione, la diffusione e l'uso 
in mancanza di autorizzazione del destinatario. Qualora il messaggio 
fosse pervenuto per errore, preghiamo di eliminarlo.

"
Tathagata Das <tathagata.das1565@gmail.com>,"Mon, 20 Jan 2014 10:11:30 -0800",Re: Print in JavaNetworkWordCount,user@spark.incubator.apache.org,"Hi Eduardo,

You can do arbitrary stuff with the data in a DStream using the operation
foreachRDD.

yourDStream.foreachRDD(rdd => {

   // Get and print first n elements
   val firstN = rdd.take(n)
   println(""First N elements = "" + firstN)

  // Count the number of elements in each batch
  println(""RDD has "" + rdd.count() + "" elements"")

})


Alternatively, just for printing the counts, you can also do

yourDStream.count.print()

Hope this helps!

TD



2014/1/20 Eduardo Costa Alfaia <e.costaalfaia@studenti.unibs.it>

e
l
so
"
Ewen Cheslack-Postava <ewencp@fastmail.fm>,"Sun, 19 Jan 2014 23:35:05 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc3),dev@spark.incubator.apache.org,"I can't get the tests to run on a Mac, 10.7.5, java -version output:

java version ""1.6.0_65""
Java(TM) SE Runtime Environment (build 1.6.0_65-b14-462-11M4609)
Java HotSpot(TM) 64-Bit Server VM (build 20.65-b04-462, mixed mode)

For reference, Spark 0.8.* build and test find on the same 
configuration. 0.9.0-rc3 fails *after* PrimitiveVectorSuite, I'm not 
sure what it's running at that time since all the tests in 
PrimitiveVectorSuite seem to have finished:

[info] PrimitiveVectorSuite:
[info] - primitive value (4 milliseconds)
[info] - non-primitive value (5 milliseconds)
[info] - ideal growth (4 milliseconds)
[info] - ideal size (5 milliseconds)
[info] - resizing (6 milliseconds)
[ERROR] [01/19/2014 23:16:27.508] 
[spark-akka.actor.default-dispatcher-4] [ActorSystem(spark)] exception 
while executing timer task
org.apache.spark.SparkException: Error sending message to 
BlockManagerMaster [message = HeartBeat(BlockManagerId(<driver>, 
localhost, 51634, 0))]
     at 
org.apache.spark.storage.BlockManagerMaster.askDriverWithReply(BlockManagerMaster.scala:176)
     at 
org.apache.spark.storage.BlockManagerMaster.sendHeartBeat(BlockManagerMaster.scala:52)
     at 
org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$heartBeat(BlockManager.scala:97)
     at 
org.apache.spark.storage.BlockManager$$anonfun$initialize$1.apply$mcV$sp(BlockManager.scala:135)
     at akka.actor.Scheduler$$anon$9.run(Scheduler.scala:80)
     at 
akka.actor.LightArrayRevolverScheduler$$anon$3$$anon$2.run(Scheduler.scala:241)
     at 
akka.actor.LightArrayRevolverScheduler$TaskHolder.run(Scheduler.scala:464)
     at 
akka.actor.LightArrayRevolverScheduler$$anonfun$close$1.apply(Scheduler.scala:281)
     at 
akka.actor.LightArrayRevolverScheduler$$anonfun$close$1.apply(Scheduler.scala:280)
     at scala.collection.Iterator$class.foreach(Iterator.scala:727)
     at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
     at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
     at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
     at akka.actor.LightArrayRevolverScheduler.close(Scheduler.scala:279)
     at akka.actor.ActorSystemImpl.stopScheduler(ActorSystem.scala:630)
     at 
akka.actor.ActorSystemImpl$$anonfun$_start$1.apply$mcV$sp(ActorSystem.scala:582)
     at 
akka.actor.ActorSystemImpl$$anonfun$_start$1.apply(ActorSystem.scala:582)
     at 
akka.actor.ActorSystemImpl$$anonfun$_start$1.apply(ActorSystem.scala:582)
     at akka.actor.ActorSystemImpl$$anon$3.run(ActorSystem.scala:596)
     at 
akka.actor.ActorSystemImpl$TerminationCallbacks$$anonfun$run$1.runNext$1(ActorSystem.scala:750)
     at 
akka.actor.ActorSystemImpl$TerminationCallbacks$$anonfun$run$1.apply$mcV$sp(ActorSystem.scala:753)
     at 
akka.actor.ActorSystemImpl$TerminationCallbacks$$anonfun$run$1.apply(ActorSystem.scala:746)
     at 
akka.actor.ActorSystemImpl$TerminationCallbacks$$anonfun$run$1.apply(ActorSystem.scala:746)
     at akka.util.ReentrantGuard.withGuard(LockUtil.scala:15)
     at 
akka.actor.ActorSystemImpl$TerminationCallbacks.run(ActorSystem.scala:746)
     at 
akka.actor.ActorSystemImpl$$anonfun$terminationCallbacks$1.apply(ActorSystem.scala:593)
     at 
akka.actor.ActorSystemImpl$$anonfun$terminationCallbacks$1.apply(ActorSystem.scala:593)
     at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
     at 
akka.dispatch.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:67)
     at 
akka.dispatch.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:82)
     at 
akka.dispatch.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:59)
     at 
akka.dispatch.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:59)
     at 
scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
     at akka.dispatch.BatchingExecutor$Batch.run(BatchingExecutor.scala:58)
     at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:42)
     at 
akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
     at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
     at 
scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
     at 
scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
     at 
scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: akka.pattern.AskTimeoutException: 
Recipient[Actor[akka://spark/user/BlockManagerMaster#927284646]] had 
already been terminated.
     at akka.pattern.AskableActorRef$.ask$extension(AskSupport.scala:134)
     at 
org.apache.spark.storage.BlockManagerMaster.askDriverWithReply(BlockManagerMaster.scala:161)
     ... 39 more

[ERROR] [01/19/2014 23:20:03.131] 
[sparkWorker2-akka.actor.default-dispatcher-19] 
[ActorSystem(sparkWorker2)] Uncaught fatal error from thread 
[sparkWorker2-akka.actor.default-dispatcher-19] shutting down 
ActorSystem [sparkWorker2]
java.lang.OutOfMemoryError: PermGen space

[ERROR] [01/19/2014 23:20:05.226] 
[sparkWorker2-akka.actor.default-dispatcher-2] 
[ActorSystem(sparkWorker2)] Uncaught fatal error from thread 
[sparkWorker2-akka.actor.default-dispatcher-2] shutting down ActorSystem 
[sparkWorker2]
java.lang.OutOfMemoryError: PermGen space

[... snip ... followed by a bunch more like these ]

It looks like it's just a heap size issue. But since it's leaving a 
process running that has -Xmx3g explicitly specified on the command line 
I'm surprised it works for everyone else but consistently fails for me. 
Almost certainly not a blocker, but some memory setting in the tests 
might need adjustment.

-Ewen
"
Reynold Xin <rxin@databricks.com>,"Mon, 20 Jan 2014 10:15:38 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc3),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","That's a perm gen issue - you need to adjust the perm gem size. In sbt it
should've been set automatically, but I think for Maven, you need to set
the maven opts, which is documented in the build instructions.



"
Nan Zhu <zhunanmcgill@gmail.com>,"Mon, 20 Jan 2014 13:23:41 -0500",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc3),dev@spark.incubator.apache.org,"running the test cases, always block after passing ‚ÄúInteract with Files‚Äù in ReplSuite.scala

I checked the code, it is test(""local-cluster mode"") {

What happened here?  

--  
Nan Zhu



it should've been set automatically, but I think for Maven, you need to set the maven opts, which is documented in the build instructions.  
ation. 0.9.0-rc3 fails *after* PrimitiveVectorSuite, I'm not sure what it's running at that time since all the tests in PrimitiveVectorSuite seem to have finished:
-dispatcher-4] [ActorSystem(spark)] exception while executing timer task
rMaster [message = HeartBeat(BlockManagerId(<driver>, localhost, 51634, 0))]
(BlockManagerMaster.scala:176)
kManagerMaster.scala:52)
park.storage.BlockManager.org)$apache$spark$storage$BlockManager$$heartBeat(BlockManager.scala:97)
e$1.apply$mcV$sp(BlockManager.scala:135)
$2.run(Scheduler.scala:241)
ler.scala:464)
1.apply(Scheduler.scala:281)
1.apply(Scheduler.scala:280)

la:72)
79)
)
mcV$sp(ActorSystem.scala:582)
ctorSystem.scala:582)
ctorSystem.scala:582)
:596)
$run$1.runNext$1(ActorSystem.scala:750)
$run$1.apply$mcV$sp(ActorSystem.scala:753)
$run$1.apply(ActorSystem.scala:746)
$run$1.apply(ActorSystem.scala:746)
tem.scala:746)
$1.apply(ActorSystem.scala:593)
$1.apply(ActorSystem.scala:593)
processBatch$1(BatchingExecutor.scala:67)
apply$mcV$sp(BatchingExecutor.scala:82)
apply(BatchingExecutor.scala:59)
apply(BatchingExecutor.scala:59)
.scala:72)
ala:58)
ask.exec(AbstractDispatcher.scala:386)
.java:260)
orkJoinPool.java:1339)
ool.java:1979)
nWorkerThread.java:107)
://spark/user/BlockManagerMaster#927284646]] had already been terminated.
la:134)
(BlockManagerMaster.scala:161)
default-dispatcher-19] [ActorSystem(sparkWorker2)] Uncaught fatal error from thread [sparkWorker2-akka.actor.default-dispatcher-19] shutting down ActorSystem [sparkWorker2]
default-dispatcher-2] [ActorSystem(sparkWorker2)] Uncaught fatal error from thread [sparkWorker2-akka.actor.default-dispatcher-2] shutting down ActorSystem [sparkWorker2]
rocess running that has -Xmx3g explicitly specified on the command line I'm surprised it works for everyone else but consistently fails for me. Almost certainly not a blocker, but some memory setting in the tests might need adjustment.
hem
se notes?
een

"
Ewen Cheslack-Postava <me@ewencp.org>,"Mon, 20 Jan 2014 16:05:23 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc3),dev@spark.incubator.apache.org,"I'm using sbt and also tried doubling all the values in the sbt/sbt 
script. Still fails during the tests, and leaves a sbt.ForkMain process 
running (and eating up 100% CPU...).

I can break this out into a separate issue/thread, I just figured it 
might be relevant since I'd imagine this is a fairly common setup 
(Macbook Air).

-Ewen
"
Eduardo Costa Alfaia <e.costaalfaia@studenti.unibs.it>,"Tue, 21 Jan 2014 10:30:28 +0100",Re: Print in JavaNetworkWordCount,dev@spark.incubator.apache.org,"Thanks again Tathagata for your help

Best Regards
e:

re
ll
uso


-- 
---
INFORMATIVA SUL TRATTAMENTO DEI DATI PERSONALI

I dati utilizzati per l'invio del presente messaggio sono trattati 
dall'Universit‡ degli Studi di Brescia esclusivamente per finalit‡ 
istituzionali. Informazioni pi˘ dettagliate anche in ordine ai diritti 
dell'interessato sono riposte nell'informativa generale e nelle notizie 
pubblicate sul sito web dell'Ateneo nella sezione ""Privacy"".

Il contenuto di questo messaggio Ë rivolto unicamente alle persona cui 
Ë indirizzato e puÚ contenere informazioni la cui riservatezza Ë 
tutelata legalmente. Ne sono vietati la riproduzione, la diffusione e l'uso 
in mancanza di autorizzazione del destinatario. Qualora il messaggio 
fosse pervenuto per errore, preghiamo di eliminarlo.

"
Patrick Wendell <pwendell@gmail.com>,"Tue, 21 Jan 2014 03:14:51 -0800",[VOTE] Release Apache Spark 0.9.0-incubating (rc4),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Please vote on releasing the following candidate as Apache Spark
(incubating) version 0.9.0.

A draft of the release notes along with the changes file is attached
to this e-mail.

The tag to be voted on is v0.9.0-incubating (commit 2da4b0f):
https://git-wip-us.apache.org/repos/asf?p=incubator-spark.git;a=tag;h=2da4b0f131dc8dda34ad44b073515fd3811a0660

The release files, including signatures, digests, etc can be found at:
http://people.apache.org/~pwendell/spark-0.9.0-incubating-rc4

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1005/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-0.9.0-incubating-rc4-docs/

Please vote on releasing this package as Apache Spark 0.9.0-incubating!

The vote is open until Friday, January 24, at 11:15 UTC and passes if
a majority of at least 3 +1 PPMC votes are cast.

[ ] +1 Release this package as Apache Spark 0.9.0-incubating
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.incubator.apache.org/
Spark 0.9.0 is a major release that adds significant new features. It updates Spark to Scala 2.10, simplifies high availability, and updates numerous components of the project. This release includes a first version of GraphX, a powerful new framework for graph processing that comes with a library of standard algorithms. In addition, Spark Streaming is now out of alpha, and includes significant optimizations and simplified high availability deployment.

### Scala 2.10 Support

Spark now runs on Scala 2.10, letting users benefit from the language and library improvements in this version.

### Configuration System

The new [SparkConf] class is now the preferred way to configure advanced settings on your SparkContext, though the previous Java system property still works. SparkConf is especially useful in tests to make sure properties don‚Äôt stay set across tests.

### Spark Streaming Improvements

Spark Streaming is no longer alpha, and comes with simplified high availability and several optimizations.

* When running on a Spark standalone cluster with the [standalone cluster high availability mode], you can submit a Spark Streaming driver application to the cluster and have it automatically recovered if either the driver or the cluster master crashes.
* Windowed operators have been sped up by 30-50%.
* Spark Streaming‚Äôs input source plugins (e.g. for Twitter, Kafka and Flume) are now separate projects, making it easier to pull in only the dependencies you need.
* A new StreamingListener interface has been added for monitoring statistics about the streaming computation.
* A few aspects of the API have been improved:
* `DStream` and `PairDStream` classes have been moved from `org.apache.spark.streaming` to `org.apache.spark.streaming.dstream` to keep it consistent with `org.apache.spark.rdd.RDD`.
* `DStream.foreach` -> `DStream.foreachRDD` to make it explicit that it works for every RDD, not every element
* `StreamingContext.awaitTermination()` allows you wait for context shutdown and catch any exception that occurs in the streaming computation.
*`StreamingContext.stop()` now allows stopping of StreamingContext without stopping the underlying SparkContext.

### GraphX Alpha

GraphX is a new API for graph processing that uses recent advances in graph-parallel computation. It lets you build a graph within a Spark program using the standard Spark operators, then process it with new graph operators that are optimized for distributed computation. It includes basic transformations, a Pregel API for iterative computation, and a standard library of graph loaders and analytics algorithms. By offering these features within the Spark engine, GraphX can significantly speed up processing tasks compared to workflows that use different engines.

GraphX features in this release include:

* Building graphs from arbitrary Spark RDDs
* Basic operations to transform graphs or extract subgraphs
* An optimized Pregel API that takes advantage of graph partitioning and indexing
* Standard algorithms including PageRank, connected components, strongly connected components, SVD++, and triangle counting
* Interactive use from the Spark shell

GraphX is still marked as alpha in this first release, but we recommend for new users to use it instead of the more limited Bagel API.

### MLlib Improvements

* Spark‚Äôs machine learning library (MLlib) is now available in Python, where it operates on NumPy data (currently requires Python 2.7 and NumPy 1.7)
* A new algorithm has been added for Naive Bayes classification
* Alternating Least Squares can now be used to predict ratings for multiple items in parallel
* MLlib‚Äôs documentation was expanded to include more examples in Scala, Java and Python

### Python Changes

* Python users can now use MLlib (requires Python 2.7 and NumPy 1.7)
* PySpark now shows the call sites of running jobs in the Spark application UI (http://<driver>:4040), making it easy to see which part of your code is running
* IPython operation has been improved

### Packaging

* Spark‚Äôs scripts have been organized into ‚Äúbin‚Äù and ‚Äúsbin‚Äù directories to make it easier to separate admin scripts from user ones and install Spark on standard Linux paths.
* Log configuration has been improved so that Spark finds a default log4j.properties file if you don‚Äôt specify one.

### Core Engine

* Spark‚Äôs standalone mode now supports submitting a driver program to run on the cluster instead of on the external machine submitting it. You can access this functionality through the [org.apache.spark.deploy.Client] class.
* Large reduce operations now automatically spill data to disk if it does not fit in memory.
* Users of standalone mode can now limit how many cores an application will use by default if the application writer didn‚Äôt configure its size. Previously, such applications took all available cores on the cluster.
* `spark-shell` now supports the `-i` option to run a script on startup.
* New `histogram` and `countDistinctApprox` operators have been added for working with numerical data.
* Methods for reading Hadoop `Writable` objects, such as `SparkContext.sequenceFile` and `hadoopRDD`, now clone the records read by default instead of reusing the same Writable instance, making the resulting RDDs easier to cache; if you only want to use each object once, you can pass an extra flag to disable this cloning
* YARN mode now supports distributing extra files with the application, and several bugs have been fixed.

### Compatibility

This release is compatible with the previous APIs in stable components, but several language versions and script locations have changed.

* Scala programs now need to use Scala 2.10 instead of 2.9.
* Scripts such as `spark-shell` and `pyspark` have been moved into the `bin` folder, while administrative scripts to start and stop standalone clusters have been moved into `sbin`.
* Spark Streaming‚Äôs API has been changed to move external input sources into separate modules, `DStream` and `PairDStream` has been moved to package `org.apache.spark.streaming.dstream` and `DStream.foreach` has been renamed to `foreachRDD`. We expect the current API to be stable now that Spark Streaming is out of alpha.
* While the old method of configuring Spark through Java system properties still works, we recommend that users update to the new SparkConf, which is easier to inspect and use.

We expect all of the current APIs and script locations in Spark 0.9 to remain stable when we release Spark 1.0. We wanted to make these updates early to give users a chance to switch to the new API.


Spark Change Log

Release 0.9.0-incubating

  b6fd3cd Tue Jan 21 00:12:01 2014 -0800
  Merge pull request #480 from pwendell/0.9-fixes
  [Handful of 0.9 fixes]

  e5f8917 Mon Jan 20 23:35:07 2014 -0800
  Merge pull request #484 from tdas/run-example-fix
  [Made run-example respect SPARK_JAVA_OPTS and SPARK_MEM.]

  410ba06 Mon Jan 20 22:26:14 2014 -0800
  Merge pull request #482 from tdas/streaming-example-fix
  [Added StreamingContext.awaitTermination to streaming examples]

  f137947 Mon Jan 20 22:24:07 2014 -0800
  Merge pull request #483 from pwendell/gitignore
  [Restricting /lib to top level directory in .gitignore]

  94ae25d Sun Jan 19 11:33:51 2014 -0800
  Merge pull request #470 from tgravescs/fix_spark_examples_yarn
  [Only log error on missing jar to allow spark examples to jar.]

  0f077b5 Sun Jan 19 10:30:29 2014 -0800
  Merge pull request #458 from tdas/docs-update
  [Updated java API docs for streaming, along with very minor changes in the code examples.]

  03019d1 Sat Jan 18 16:29:43 2014 -0800
  Merge pull request #459 from srowen/UpdaterL2Regularization
  [Correct L2 regularized weight update with canonical form]

  76147a2 Sat Jan 18 16:24:16 2014 -0800
  Merge pull request #437 from mridulm/master
  [Minor api usability changes]

  4ac8cab Sat Jan 18 16:22:46 2014 -0800
  Merge pull request #426 from mateiz/py-ml-tests
  [Re-enable Python MLlib tests (require Python 2.7 and NumPy 1.7+)]

  34e911c Sat Jan 18 16:17:34 2014 -0800
  Merge pull request #462 from mateiz/conf-file-fix
  [Remove Typesafe Config usage and conf files to fix nested property names]

  ff7201c Sat Jan 18 12:50:02 2014 -0800
  Merge pull request #461 from pwendell/master
  [Use renamed shuffle spill config in CoGroupedRDD.scala]

  7b0d5a5 Thu Jan 16 23:18:48 2014 -0800
  Merge pull request #451 from Qiuzhuang/master
  [Fixed Window spark shell launch script error.]

  4ccedb3 Wed Jan 15 14:26:48 2014 -0800
  Merge pull request #444 from mateiz/py-version
  [Clarify that Python 2.7 is only needed for MLlib]

  e3fa36f Wed Jan 15 13:56:04 2014 -0800
  Merge pull request #442 from pwendell/standalone
  [Workers should use working directory as spark home if it's not specified]

  29c76d9 Wed Jan 15 13:55:48 2014 -0800
  Merge pull request #443 from tdas/filestream-fix
  [Made some classes private[stremaing] and deprecated a method in JavaStreamingContext.]

  aca40aa Wed Jan 15 11:15:47 2014 -0800
  Merge pull request #441 from pwendell/graphx-build
  [GraphX shouldn't list Spark as provided.]

  e12c374 Wed Jan 15 10:01:43 2014 -0800
  Merge pull request #433 from markhamstra/debFix
  [Updated Debian packaging]

  2f015c2 Tue Jan 14 23:17:28 2014 -0800
  Merge pull request #436 from ankurdave/VertexId-case
  [Rename VertexID -> VertexId in GraphX]

  2859cab Tue Jan 14 23:08:19 2014 -0800
  Merge pull request #435 from tdas/filestream-fix
  [Fixed the flaky tests by making SparkConf not serializable]

  fbfbb33 Tue Jan 14 23:06:29 2014 -0800
  Merge pull request #434 from rxin/graphxmaven
  [Fixed SVDPlusPlusSuite in Maven build.]

  2c6c07f Tue Jan 14 21:53:05 2014 -0800
  Merge pull request #424 from jegonzal/GraphXProgrammingGuide
  [Additional edits for clarity in the graphx programming guide.]

  6fa4e02 Tue Jan 14 21:51:25 2014 -0800
  Merge pull request #431 from ankurdave/graphx-caching-doc
  [Describe caching and uncaching in GraphX programming guide]

  2f930d5 Tue Jan 14 15:00:11 2014 -0800
  Merge pull request #428 from pwendell/writeable-objects
  [Don't clone records for text files]

  329c9df Tue Jan 14 14:53:36 2014 -0800
  Merge pull request #429 from ankurdave/graphx-examples-pom.xml
  [Add GraphX dependency to examples/pom.xml]

  a14933d Tue Jan 14 14:52:42 2014 -0800
  Merge pull request #427 from pwendell/deprecate-aggregator
  [Deprecate rather than remove old combineValuesByKey function]

  119b6c5 Tue Jan 14 13:29:08 2014 -0800
  Merge pull request #425 from rxin/scaladoc
  [API doc update & make Broadcast public]

  bf3b150 Tue Jan 14 09:45:22 2014 -0800
  Merge pull request #423 from jegonzal/GraphXProgrammingGuide
  [Improving the graphx-programming-guide]

  1b4adc2 Tue Jan 14 01:19:24 2014 -0800
  Merge pull request #420 from pwendell/header-files
  [Add missing header files]

  b60840e Tue Jan 14 00:48:34 2014 -0800
  Merge pull request #418 from pwendell/0.9-versions
  [Version changes for release 0.9.0.]

  980250b Tue Jan 14 00:05:37 2014 -0800
  Merge pull request #416 from tdas/filestream-fix
  [Removed unnecessary DStream operations and updated docs]

  055be5c Mon Jan 13 23:26:44 2014 -0800
  Merge pull request #415 from pwendell/shuffle-compress
  [Enable compression by default for spills]

  fdaabdc Mon Jan 13 23:08:26 2014 -0800
  Merge pull request #380 from mateiz/py-bayes
  [Add Naive Bayes to Python MLlib, and some API fixes]

  4a805af Mon Jan 13 22:58:38 2014 -0800
  Merge pull request #367 from ankurdave/graphx
  [GraphX: Unifying Graphs and Tables]

  945fe7a Mon Jan 13 22:56:12 2014 -0800
  Merge pull request #408 from pwendell/external-serializers
  [Improvements to external sorting]

  68641bc Mon Jan 13 22:54:13 2014 -0800
  Merge pull request #413 from rxin/scaladoc
  [Adjusted visibility of various components and documentation for 0.9.0 release.]

  0ca0d4d Mon Jan 13 22:32:21 2014 -0800
  Merge pull request #401 from andrewor14/master
  [External sorting - Add number of bytes spilled to Web UI]

  08b9fec Mon Jan 13 22:29:03 2014 -0800
  Merge pull request #409 from tdas/unpersist
  [Automatically unpersisting RDDs that have been cleaned up from DStreams]

  b07bc02 Mon Jan 13 20:45:22 2014 -0800
  Merge pull request #412 from harveyfeng/master
  [Add default value for HadoopRDD's `cloneRecords` constructor arg]

  a2fee38 Mon Jan 13 19:45:26 2014 -0800
  Merge pull request #411 from tdas/filestream-fix
  [Improved logic of finding new files in FileInputDStream]

  01c0d72 Mon Jan 13 16:24:30 2014 -0800
  Merge pull request #410 from rxin/scaladoc1
  [Updated JavaStreamingContext to make scaladoc compile.]

  8038da2 Mon Jan 13 14:59:30 2014 -0800
  Merge pull request #2 from jegonzal/GraphXCCIssue
  [Improving documentation and identifying potential bug in CC calculation.]

  b93f9d4 Mon Jan 13 12:18:05 2014 -0800
  Merge pull request #400 from tdas/dstream-move
  [Moved DStream and PairDSream to org.apache.spark.streaming.dstream]

  e6ed13f Sun Jan 12 22:35:14 2014 -0800
  Merge pull request #397 from pwendell/host-port
  [Remove now un-needed hostPort option]

  0b96d85 Sun Jan 12 21:31:43 2014 -0800
  Merge pull request #399 from pwendell/consolidate-off
  [Disable shuffle file consolidation by default]

  0ab505a Sun Jan 12 21:31:04 2014 -0800
  Merge pull request #395 from hsaputra/remove_simpleredundantreturn_scala
  [Remove simple redundant return statements for Scala methods/functions]

  405bfe8 Sun Jan 12 20:04:21 2014 -0800
  Merge pull request #394 from tdas/error-handling
  [Better error handling in Spark Streaming and more API cleanup]

  28a6b0c Sun Jan 12 19:49:36 2014 -0800
  Merge pull request #398 from pwendell/streaming-api
  [Rename DStream.foreach to DStream.foreachRDD]

  074f502 Sun Jan 12 17:01:13 2014 -0800
  Merge pull request #396 from pwendell/executor-env
  [Setting load defaults to true in executor]

  82e2b92 Sun Jan 12 16:55:11 2014 -0800
  Merge pull request #392 from rxin/listenerbus
  [Stop SparkListenerBus daemon thread when DAGScheduler is stopped.]

  288a878 Sat Jan 11 21:53:19 2014 -0800
  Merge pull request #389 from rxin/clone-writables
  [Minor update for clone writables and more documentation.]

  dbc11df Sat Jan 11 18:07:13 2014 -0800
  Merge pull request #388 from pwendell/master
  [Fix UI bug introduced in #244.]

  409866b Sat Jan 11 17:12:06 2014 -0800
  Merge pull request #393 from pwendell/revert-381
  [Revert PR 381]

  6510f04 Sat Jan 11 12:48:26 2014 -0800
  Merge pull request #387 from jerryshao/conf-fix
  [Fix configure didn't work small problem in ALS]

  ee6e7f9 Sat Jan 11 12:07:55 2014 -0800
  Merge pull request #359 from ScrapCodes/clone-writables
  [We clone hadoop key and values by default and reuse objects if asked to.]

  4216178 Sat Jan 11 09:46:48 2014 -0800
  Merge pull request #373 from jerryshao/kafka-upgrade
  [Upgrade Kafka dependecy to 0.8.0 release version]

  92ad18b Fri Jan 10 23:25:15 2014 -0800
  Merge pull request #376 from prabeesh/master
  [Change clientId to random clientId]

  0b5ce7a Fri Jan 10 23:23:21 2014 -0800
  Merge pull request #386 from pwendell/typo-fix
  [Small typo fix]

  1d7bef0 Fri Jan 10 18:53:03 2014 -0800
  Merge pull request #381 from mateiz/default-ttl
  [Fix default TTL for metadata cleaner]

  44d6a8e Fri Jan 10 17:51:50 2014 -0800
  Merge pull request #382 from RongGu/master
  [Fix a type error in comment lines]

  88faa30 Fri Jan 10 17:14:22 2014 -0800
  Merge pull request #385 from shivaram/add-i2-instances
  [Add i2 instance types to Spark EC2.]

  f265531 Fri Jan 10 16:25:44 2014 -0800
  Merge pull request #383 from tdas/driver-test
  [API for automatic driver recovery for streaming programs and other bug fixes]

  d37408f Fri Jan 10 16:25:01 2014 -0800
  Merge pull request #377 from andrewor14/master
  [External Sorting for Aggregator and CoGroupedRDDs (Revisited)]

  0eaf01c Fri Jan 10 15:32:19 2014 -0800
  Merge pull request #369 from pillis/master
  [SPARK-961 Add a Vector.random() method]

  7cef843 Fri Jan 10 15:34:15 2014 -0600
  Merge pull request #371 from tgravescs/yarn_client_addjar_misc_fixes
  [Yarn client addjar and misc fixes]

  7b58f11 Fri Jan 10 12:47:46 2014 -0800
  Merge pull request #384 from pwendell/debug-logs
  [Make DEBUG-level logs consummable.]

  23d2995 Fri Jan 10 10:20:02 2014 -0800
  Merge pull request #1 from jegonzal/graphx
  [ProgrammingGuide]

  0ebc973 Thu Jan 9 23:58:49 2014 -0800
  Merge pull request #375 from mateiz/option-fix
  [Fix bug added when we changed AppDescription.maxCores to an Option]

  dd03cea Thu Jan 9 23:38:03 2014 -0800
  Merge pull request #378 from pwendell/consolidate_on
  [Enable shuffle consolidation by default.]

  997c830 Thu Jan 9 22:22:20 2014 -0800
  Merge pull request #363 from pwendell/streaming-logs
  [Set default logging to WARN for Spark streaming examples.]

  300eaa9 Thu Jan 9 20:29:51 2014 -0800
  Merge pull request #353 from pwendell/ipython-simplify
  [Simplify and fix pyspark script.]

  4b074fa Thu Jan 9 19:03:55 2014 -0800
  Merge pull request #374 from mateiz/completeness
  [Add some missing Java API methods]

  a9d5333 Thu Jan 9 18:46:46 2014 -0800
  Merge pull request #294 from RongGu/master
  [Bug fixes for updating the RDD block's memory and disk usage information]

  d86a85e Thu Jan 9 18:37:52 2014 -0800
  Merge pull request #293 from pwendell/standalone-driver
  [SPARK-998: Support Launching Driver Inside of Standalone Mode]

  26cdb5f Thu Jan 9 17:16:34 2014 -0800
  Merge pull request #372 from pwendell/log4j-fix-1
  [Send logs to stderr by default (instead of stdout).]

  12f414e Thu Jan 9 15:31:30 2014 -0800
  Merge pull request #362 from mateiz/conf-getters
  [Use typed getters for configuration settings]

  365cac9 Thu Jan 9 00:56:16 2014 -0800
  Merge pull request #361 from rxin/clean
  [Minor style cleanup. Mostly on indenting & line width changes.]

  73c724e Thu Jan 9 00:32:19 2014 -0800
  Merge pull request #368 from pwendell/sbt-fix
  [Don't delegate to users `sbt`.]

  dceedb4 Wed Jan 8 23:19:28 2014 -0800
  Merge pull request #364 from pwendell/fix
  [Fixing config option ""retained_stages"" => ""retainedStages"".]

  04d83fc Wed Jan 8 11:55:37 2014 -0800
  Merge pull request #360 from witgo/master
  [fix make-distribution.sh show version: command not found]

  56ebfea Wed Jan 8 11:50:06 2014 -0800
  Merge pull request #357 from hsaputra/set_boolean_paramname
  [Set boolean param name for call to SparkHadoopMapReduceUtil.newTaskAttemptID]

  bdeaeaf Wed Jan 8 11:48:39 2014 -0800
  Merge pull request #358 from pwendell/add-cdh
  [Add CDH Repository to Maven Build]

  5cae05f Wed Jan 8 11:47:28 2014 -0800
  Merge pull request #356 from hsaputra/remove_deprecated_cleanup_method
  [Remove calls to deprecated mapred's OutputCommitter.cleanupJob]

  6eef78d Wed Jan 8 08:49:20 2014 -0600
  Merge pull request #345 from colorant/yarn
  [support distributing extra files to worker for yarn client mode]

  bb6a39a Tue Jan 7 22:32:18 2014 -0800
  Merge pull request #322 from falaki/MLLibDocumentationImprovement
  [SPARK-1009 Updated MLlib docs to show how to use it in Python]

  cb1b927 Tue Jan 7 22:26:28 2014 -0800
  Merge pull request #355 from ScrapCodes/patch-1
  [Update README.md]

  c0f0155 Tue Jan 7 22:21:52 2014 -0800
  Merge pull request #313 from tdas/project-refactor
  [Refactored the streaming project to separate external libraries like Twitter, Kafka, Flume, etc.]

  f5f12dc Tue Jan 7 21:56:35 2014 -0800
  Merge pull request #336 from liancheng/akka-remote-lookup
  [Get rid of `Either[ActorRef, ActorSelection]']

  11891e6 Wed Jan 8 00:32:18 2014 -0500
  Merge pull request #327 from lucarosellini/master
  [Added ‚Äò-i‚Äô command line option to Spark REPL]

  7d0aac9 Wed Jan 8 00:30:45 2014 -0500
  Merge pull request #354 from hsaputra/addasfheadertosbt
  [Add ASF header to the new sbt script.]

  d75dc42 Wed Jan 8 00:30:03 2014 -0500
  Merge pull request #350 from mateiz/standalone-limit
  [Add way to limit default # of cores used by apps in standalone mode]

  61674bc Tue Jan 7 18:32:13 2014 -0800
  Merge pull request #352 from markhamstra/oldArch
  [Don't leave os.arch unset after BlockManagerSuite]

  b2e690f Tue Jan 7 16:57:08 2014 -0800
  Merge pull request #328 from falaki/MatrixFactorizationModel-fix
  [SPARK-1012: DAGScheduler Exception Fix]

  6ccf8ce Tue Jan 7 15:49:14 2014 -0800
  Merge pull request #351 from pwendell/maven-fix
  [Add log4j exclusion rule to maven.]

  7d5fa17 Tue Jan 7 11:31:34 2014 -0800
  Merge pull request #337 from yinxusen/mllib-16-bugfix
  [Mllib 16 bugfix]

  71fc113 Tue Jan 7 11:30:35 2014 -0800
  Merge pull request #349 from CodingCat/support-worker_dir
  [add the comments about SPARK_WORKER_DIR]

  15d9534 Tue Jan 7 08:10:02 2014 -0800
  Merge pull request #318 from srowen/master
  [Suggested small changes to Java code for slightly more standard style, encapsulation and in some cases performance]

  468af0f Tue Jan 7 08:09:01 2014 -0800
  Merge pull request #348 from prabeesh/master
  [spark -> org.apache.spark]

  c3cf047 Tue Jan 7 00:54:25 2014 -0800
  Merge pull request #339 from ScrapCodes/conf-improvements
  [Conf improvements]

  a862caf Tue Jan 7 00:18:20 2014 -0800
  Merge pull request #331 from holdenk/master
  [Add a script to download sbt if not present on the system]

  b97ef21 Mon Jan 6 20:12:57 2014 -0800
  Merge pull request #346 from sproblvem/patch-1
  [Update stop-slaves.sh]

  7210257 Mon Jan 6 18:25:44 2014 -0800
  Merge pull request #128 from adamnovak/master
  [Fix failing ""sbt/sbt publish-local"" by adding a no-argument PrimitiveKeyOpenHashMap constructor]

  e4d6057 Mon Jan 6 14:56:54 2014 -0800
  Merge pull request #343 from pwendell/build-fix
  [Fix test breaking downstream builds]

  93bf962 Mon Jan 6 11:42:41 2014 -0800
  Merge pull request #340 from ScrapCodes/sbt-fixes
  [Made java options to be applied during tests so that they become self explanatory.]

  60edeb3 Mon Jan 6 11:40:32 2014 -0800
  Merge pull request #338 from ScrapCodes/ning-upgrade
  [SPARK-1005 Ning upgrade]

  c708e81 Mon Jan 6 11:35:48 2014 -0800
  Merge pull request #341 from ash211/patch-5
  [Clarify spark.cores.max in docs]

  33fcb91 Mon Jan 6 11:19:23 2014 -0800
  Merge pull request #342 from tgravescs/fix_maven_protobuf
  [Change protobuf version for yarn alpha back to 2.4.1]

  357083c Mon Jan 6 10:29:04 2014 -0800
  Merge pull request #330 from tgravescs/fix_addjars_null_handling
  [Fix handling of empty SPARK_EXAMPLES_JAR]

  a2e7e04 Sun Jan 5 22:37:36 2014 -0800
  Merge pull request #333 from pwendell/logging-silence
  [Quiet ERROR-level Akka Logs]

  5b0986a Sun Jan 5 19:25:09 2014 -0800
  Merge pull request #334 from pwendell/examples-fix
  [Removing SPARK_EXAMPLES_JAR in the code]

  f4b924f Sun Jan 5 17:11:47 2014 -0800
  Merge pull request #335 from rxin/ser
  [Fall back to zero-arg constructor for Serializer initialization if there is no constructor that accepts SparkConf.]

  d43ad3e Sat Jan 4 16:29:30 2014 -0800
  Merge pull request #292 from soulmachine/naive-bayes
  [standard Naive Bayes classifier]

  86404da Sat Jan 4 14:55:54 2014 -0800
  Merge pull request #127 from jegonzal/MapByPartition
  [Adding mapEdges and mapTriplets by Partition]

  e68cdb1 Sat Jan 4 13:46:02 2014 -0800
  Merge pull request #124 from jianpingjwang/master
  [refactor and bug fix]

  280ddf6 Sat Jan 4 12:54:41 2014 -0800
  Merge pull request #121 from ankurdave/more-simplify
  [Simplify GraphImpl internals further]

  10fe23b Fri Jan 3 23:50:14 2014 -0800
  Merge pull request #329 from pwendell/remove-binaries
  [SPARK-1002: Remove Binaries from Spark Source]

  c4d6145 Fri Jan 3 16:30:53 2014 -0800
  Merge pull request #325 from witgo/master
  [Modify spark on yarn to create SparkConf process]

  4ae101f Fri Jan 3 11:24:35 2014 -0800
  Merge pull request #317 from ScrapCodes/spark-915-segregate-scripts
  [Spark-915 segregate scripts]

  87248bd Fri Jan 3 00:45:31 2014 -0800
  Merge pull request #1 from apache/master
  [Merge latest Spark changes]

  30b9db0 Thu Jan 2 23:15:55 2014 -0800
  Merge pull request #285 from colorant/yarn-refactor
  [Yarn refactor]

  498a5f0 Thu Jan 2 19:06:40 2014 -0800
  Merge pull request #323 from tgravescs/sparkconf_yarn_fix
  [fix spark on yarn after the sparkConf changes]

  0475ca8 Thu Jan 2 15:17:08 2014 -0800
  Merge pull request #320 from kayousterhout/erroneous_failed_msg
  [Remove erroneous FAILED state for killed tasks.]

  588a169 Thu Jan 2 13:20:54 2014 -0800
  Merge pull request #297 from tdas/window-improvement
  [Improvements to DStream window ops and refactoring of Spark's CheckpointSuite]

  5e67cdc Thu Jan 2 12:56:28 2014 -0800
  Merge pull request #319 from kayousterhout/remove_error_method
  [Removed redundant TaskSetManager.error() function.]

  ca67909 Thu Jan 2 15:54:54 2014 -0500
  Merge pull request #311 from tmyklebu/master
  [SPARK-991: Report information gleaned from a Python stacktrace in the UI]

  3713f81 Wed Jan 1 21:29:12 2014 -0800
  Merge pull request #309 from mateiz/conf2
  [SPARK-544. Migrate configuration to a SparkConf class]

  c1d928a Wed Jan 1 17:03:48 2014 -0800
  Merge pull request #312 from pwendell/log4j-fix-2
  [SPARK-1008: Logging improvments]

  dc9cb83 Wed Jan 1 13:28:34 2014 -0800
  Merge pull request #126 from jegonzal/FixingPersist
  [Fixing Persist Behavior]

  9a0ff72 Tue Dec 31 21:50:24 2013 -0800
  Merge pull request #314 from witgo/master
  [restore core/pom.xml file modification]

  8b8e70e Tue Dec 31 17:48:24 2013 -0800
  Merge pull request #73 from falaki/ApproximateDistinctCount
  [Approximate distinct count]

  63b411d Tue Dec 31 14:31:28 2013 -0800
  Merge pull request #238 from ngbinh/upgradeNetty
  [upgrade Netty from 4.0.0.Beta2 to 4.0.13.Final]

  32d6ae9 Tue Dec 31 13:51:07 2013 -0800
  Merge pull request #120 from ankurdave/subgraph-reuses-view
  [Reuse VTableReplicated in GraphImpl.subgraph]

  55b7e2f Tue Dec 31 10:12:51 2013 -0800
  Merge pull request #289 from tdas/filestream-fix
  [Bug fixes for file input stream and checkpointing]

  2b71ab9 Mon Dec 30 11:01:30 2013 -0800
  Merge pull request from aarondav: Utilize DiskBlockManager pathway for temp file writing
  [This gives us a couple advantages:]

  50e3b8e Mon Dec 30 07:44:26 2013 -0800
  Merge pull request #308 from kayousterhout/stage_naming
  [Changed naming of StageCompleted event to be consistent]

  72a17b6 Sat Dec 28 21:25:40 2013 -1000
  Revert ""Merge pull request #310 from jyunfan/master""
  [This reverts commit 79b20e4dbe3dcd8559ec8316784d3334bb55868b, reversing]

  79b20e4 Sat Dec 28 21:13:36 2013 -1000
  Merge pull request #310 from jyunfan/master
  [Fix typo in the Accumulators section]

  7375047 Sat Dec 28 13:25:06 2013 -0800
  Merge pull request #304 from kayousterhout/remove_unused
  [Removed unused failed and causeOfFailure variables (in TaskSetManager)]

  ad3dfd1 Fri Dec 27 22:10:14 2013 -0500
  Merge pull request #307 from kayousterhout/other_failure
  [Removed unused OtherFailure TaskEndReason.]

  b579b83 Fri Dec 27 22:09:04 2013 -0500
  Merge pull request #306 from kayousterhout/remove_pending
  [Remove unused hasPendingTasks methods]

  19672dc Fri Dec 27 13:37:10 2013 -0800
  Merge pull request #305 from kayousterhout/line_spacing
  [Fixed >100char lines in DAGScheduler.scala]

  7be1e57 Thu Dec 26 23:41:40 2013 -1000
  Merge pull request #298 from aarondav/minor
  [Minor: Decrease margin of left side of Log page]

  7d811ba Thu Dec 26 23:39:58 2013 -1000
  Merge pull request #302 from pwendell/SPARK-1007
  [SPARK-1007: spark-class2.cmd should change SCALA_VERSION to be 2.10]

  5e69fc5 Thu Dec 26 19:10:39 2013 -0500
  Merge pull request #295 from markhamstra/JobProgressListenerNPE
  [Avoid a lump of coal (NPE) in JobProgressListener's stocking.]

  da20270 Thu Dec 26 12:11:52 2013 -0800
  Merge pull request #1 from aarondav/driver
  [Refactor DriverClient to be more Actor-based]

  e240bad Thu Dec 26 12:30:48 2013 -0500
  Merge pull request #296 from witgo/master
  [Renamed ClusterScheduler to TaskSchedulerImpl for yarn and new-yarn package]

  c344ed0 Thu Dec 26 01:31:06 2013 -0500
  Merge pull request #283 from tmyklebu/master
  [Python bindings for mllib]

  56094bc Wed Dec 25 13:14:33 2013 -0500
  Merge pull request #290 from ash211/patch-3
  [Typo: avaiable -> available]

  4842a07 Wed Dec 25 01:52:15 2013 -0800
  Merge pull request #287 from azuryyu/master
  [Fixed job name in the java streaming example.]

  85a344b Tue Dec 24 16:35:06 2013 -0800
  Merge pull request #127 from kayousterhout/consolidate_schedulers
  [Deduplicate Local and Cluster schedulers.]

  c2dd6bc Tue Dec 24 14:36:47 2013 -0800
  Merge pull request #279 from aarondav/shuffle-cleanup0
  [Clean up shuffle files once their metadata is gone]

  3bf7c70 Tue Dec 24 16:37:13 2013 -0500
  Merge pull request #275 from ueshin/wip/changeclasspathorder
  [Change the order of CLASSPATH.]

  d63856c Mon Dec 23 22:07:26 2013 -0800
  Merge pull request #286 from rxin/build
  [Show full stack trace and time taken in unit tests.]

  23a9ae6 Tue Dec 24 00:08:48 2013 -0500
  Merge pull request #277 from tdas/scheduler-update
  [Refactored the streaming scheduler and added StreamingListener interface]

  11107c9 Mon Dec 23 10:38:20 2013 -0800
  Merge pull request #244 from leftnoteasy/master
  [Added SPARK-968 implementation for review]

  44e4205 Sun Dec 22 11:44:18 2013 -0800
  Merge pull request #116 from jianpingjwang/master
  [remove unused variables and fix a bug]

  4797c22 Fri Dec 20 13:30:39 2013 -0800
  Merge pull request #118 from ankurdave/VertexPartitionSuite
  [Test VertexPartition and fix bugs]

  0bc57c5 Fri Dec 20 11:56:54 2013 -0800
  Merge pull request #280 from aarondav/minor
  [Minor cleanup for standalone scheduler]

  ac70b8f Fri Dec 20 10:56:10 2013 -0800
  Merge pull request #117 from ankurdave/more-tests
  [More tests]

  45310d4 Thu Dec 19 22:08:20 2013 -0800
  Merge pull request #115 from ankurdave/test-reorg
  [Reorganize unit tests; improve GraphSuite test coverage]

  9228ec8 Thu Dec 19 21:37:15 2013 -0800
  Merge pull request #1 from aarondav/127
  [Merge master into 127]

  eca68d4 Thu Dec 19 18:12:22 2013 -0800
  Merge pull request #272 from tmyklebu/master
  [Track and report task result serialisation time.]

  7990c56 Thu Dec 19 13:35:09 2013 -0800
  Merge pull request #276 from shivaram/collectPartition
  [Add collectPartition to JavaRDD interface.]

  440e531 Thu Dec 19 10:38:56 2013 -0800
  Merge pull request #278 from MLnick/java-python-tostring
  [Add toString to Java RDD, and __repr__ to Python RDD]

  d8d3f3e Thu Dec 19 00:06:43 2013 -0800
  Merge pull request #183 from aarondav/spark-959
  [[SPARK-959] Explicitly depend on org.eclipse.jetty.orbit jar]

  bfba532 Wed Dec 18 22:22:21 2013 -0800
  Merge pull request #247 from aarondav/minor
  [Increase spark.akka.askTimeout default to 30 seconds]

  da301b5 Wed Dec 18 20:03:29 2013 -0800
  Merge pull request #112 from amatsukawa/scc
  [Strongly connected component algorithm]

  c64a53a Wed Dec 18 16:56:26 2013 -0800
  Merge pull request #267 from JoshRosen/cygwin
  [Fix Cygwin support in several scripts.]

  a645ef6 Wed Dec 18 16:07:52 2013 -0800
  Merge pull request #48 from amatsukawa/add_project_to_graph
  [Add mask operation on graph and filter graph primitive]

  d7ebff0 Wed Dec 18 15:38:48 2013 -0800
  Merge pull request #1 from ankurdave/add_project_to_graph
  [Merge current master and reimplement Graph.mask usin"
"
Patrick Wendell <pwendell@gmail.com>,Tue"," 21 Jan 2014 03:15:06 -0800""",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc3),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","This vote is cancelled in favor of rc4.



"
Patrick Wendell <pwendell@gmail.com>,"Tue, 21 Jan 2014 03:16:14 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc4),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","(Correction to the hash above).

The tag to be voted on is v0.9.0-incubating (commit 0771df67):
https://git-wip-us.apache.org/repos/asf?p=incubator-spark.git;a=commit;h=0771df675363c69622404cb514bd751bc90526af


"
Patrick Wendell <pwendell@gmail.com>,"Tue, 21 Jan 2014 03:21:34 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc4),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hm - it seems like the CHANGES.txt files are for some reason missing
from the packaged artifacts. Please disregard this RC... I'll post a
new one in the morning (too late now I'm afraid).

-Patrick


"
Patrick Wendell <pwendell@gmail.com>,"Tue, 21 Jan 2014 08:54:41 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc4),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Okay please defer to the new thread for RC4. Turns out the artifacts
were fine I'm just going to re-post this to remove clutter in this
thread.


"
Patrick Wendell <pwendell@gmail.com>,"Tue, 21 Jan 2014 09:01:38 -0800",[VOTE] Release Apache Spark 0.9.0-incubating (rc4),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Please vote on releasing the following candidate as Apache Spark
(incubating) version 0.9.0.

A draft of the release notes along with the changes file is attached
to this e-mail.

The tag to be voted on is v0.9.0-incubating (commit 0771df67):
https://git-wip-us.apache.org/repos/asf?p=incubator-spark.git;a=commit;h=0771df675363c69622404cb514bd751bc90526af

The release files, including signatures, digests, etc can be found at:
http://people.apache.org/~pwendell/spark-0.9.0-incubating-rc4

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1005/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-0.9.0-incubating-rc4-docs/

Please vote on releasing this package as Apache Spark 0.9.0-incubating!

The vote is open until Friday, January 24, at 11:15 UTC and passes if
a majority of at least 3 +1 PPMC votes are cast.

[ ] +1 Release this package as Apache Spark 0.9.0-incubating
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.incubator.apache.org/
Spark 0.9.0 is a major release that adds significant new features. It updates Spark to Scala 2.10, simplifies high availability, and updates numerous components of the project. This release includes a first version of GraphX, a powerful new framework for graph processing that comes with a library of standard algorithms. In addition, Spark Streaming is now out of alpha, and includes significant optimizations and simplified high availability deployment.

### Scala 2.10 Support

Spark now runs on Scala 2.10, letting users benefit from the language and library improvements in this version.

### Configuration System

The new [SparkConf] class is now the preferred way to configure advanced settings on your SparkContext, though the previous Java system property still works. SparkConf is especially useful in tests to make sure properties don‚Äôt stay set across tests.

### Spark Streaming Improvements

Spark Streaming is no longer alpha, and comes with simplified high availability and several optimizations.

* When running on a Spark standalone cluster with the [standalone cluster high availability mode], you can submit a Spark Streaming driver application to the cluster and have it automatically recovered if either the driver or the cluster master crashes.
* Windowed operators have been sped up by 30-50%.
* Spark Streaming‚Äôs input source plugins (e.g. for Twitter, Kafka and Flume) are now separate projects, making it easier to pull in only the dependencies you need.
* A new StreamingListener interface has been added for monitoring statistics about the streaming computation.
* A few aspects of the API have been improved:
* `DStream` and `PairDStream` classes have been moved from `org.apache.spark.streaming` to `org.apache.spark.streaming.dstream` to keep it consistent with `org.apache.spark.rdd.RDD`.
* `DStream.foreach` -> `DStream.foreachRDD` to make it explicit that it works for every RDD, not every element
* `StreamingContext.awaitTermination()` allows you wait for context shutdown and catch any exception that occurs in the streaming computation.
*`StreamingContext.stop()` now allows stopping of StreamingContext without stopping the underlying SparkContext.

### GraphX Alpha

GraphX is a new API for graph processing that uses recent advances in graph-parallel computation. It lets you build a graph within a Spark program using the standard Spark operators, then process it with new graph operators that are optimized for distributed computation. It includes basic transformations, a Pregel API for iterative computation, and a standard library of graph loaders and analytics algorithms. By offering these features within the Spark engine, GraphX can significantly speed up processing tasks compared to workflows that use different engines.

GraphX features in this release include:

* Building graphs from arbitrary Spark RDDs
* Basic operations to transform graphs or extract subgraphs
* An optimized Pregel API that takes advantage of graph partitioning and indexing
* Standard algorithms including PageRank, connected components, strongly connected components, SVD++, and triangle counting
* Interactive use from the Spark shell

GraphX is still marked as alpha in this first release, but we recommend for new users to use it instead of the more limited Bagel API.

### MLlib Improvements

* Spark‚Äôs machine learning library (MLlib) is now available in Python, where it operates on NumPy data (currently requires Python 2.7 and NumPy 1.7)
* A new algorithm has been added for Naive Bayes classification
* Alternating Least Squares can now be used to predict ratings for multiple items in parallel
* MLlib‚Äôs documentation was expanded to include more examples in Scala, Java and Python

### Python Changes

* Python users can now use MLlib (requires Python 2.7 and NumPy 1.7)
* PySpark now shows the call sites of running jobs in the Spark application UI (http://<driver>:4040), making it easy to see which part of your code is running
* IPython operation has been improved

### Packaging

* Spark‚Äôs scripts have been organized into ‚Äúbin‚Äù and ‚Äúsbin‚Äù directories to make it easier to separate admin scripts from user ones and install Spark on standard Linux paths.
* Log configuration has been improved so that Spark finds a default log4j.properties file if you don‚Äôt specify one.

### Core Engine

* Spark‚Äôs standalone mode now supports submitting a driver program to run on the cluster instead of on the external machine submitting it. You can access this functionality through the [org.apache.spark.deploy.Client] class.
* Large reduce operations now automatically spill data to disk if it does not fit in memory.
* Users of standalone mode can now limit how many cores an application will use by default if the application writer didn‚Äôt configure its size. Previously, such applications took all available cores on the cluster.
* `spark-shell` now supports the `-i` option to run a script on startup.
* New `histogram` and `countDistinctApprox` operators have been added for working with numerical data.
* Methods for reading Hadoop `Writable` objects, such as `SparkContext.sequenceFile` and `hadoopRDD`, now clone the records read by default instead of reusing the same Writable instance, making the resulting RDDs easier to cache; if you only want to use each object once, you can pass an extra flag to disable this cloning
* YARN mode now supports distributing extra files with the application, and several bugs have been fixed.

### Compatibility

This release is compatible with the previous APIs in stable components, but several language versions and script locations have changed.

* Scala programs now need to use Scala 2.10 instead of 2.9.
* Scripts such as `spark-shell` and `pyspark` have been moved into the `bin` folder, while administrative scripts to start and stop standalone clusters have been moved into `sbin`.
* Spark Streaming‚Äôs API has been changed to move external input sources into separate modules, `DStream` and `PairDStream` has been moved to package `org.apache.spark.streaming.dstream` and `DStream.foreach` has been renamed to `foreachRDD`. We expect the current API to be stable now that Spark Streaming is out of alpha.
* While the old method of configuring Spark through Java system properties still works, we recommend that users update to the new SparkConf, which is easier to inspect and use.

We expect all of the current APIs and script locations in Spark 0.9 to remain stable when we release Spark 1.0. We wanted to make these updates early to give users a chance to switch to the new API.


Spark Change Log

Release 0.9.0-incubating

  b6fd3cd Tue Jan 21 00:12:01 2014 -0800
  Merge pull request #480 from pwendell/0.9-fixes
  [Handful of 0.9 fixes]

  e5f8917 Mon Jan 20 23:35:07 2014 -0800
  Merge pull request #484 from tdas/run-example-fix
  [Made run-example respect SPARK_JAVA_OPTS and SPARK_MEM.]

  410ba06 Mon Jan 20 22:26:14 2014 -0800
  Merge pull request #482 from tdas/streaming-example-fix
  [Added StreamingContext.awaitTermination to streaming examples]

  f137947 Mon Jan 20 22:24:07 2014 -0800
  Merge pull request #483 from pwendell/gitignore
  [Restricting /lib to top level directory in .gitignore]

  94ae25d Sun Jan 19 11:33:51 2014 -0800
  Merge pull request #470 from tgravescs/fix_spark_examples_yarn
  [Only log error on missing jar to allow spark examples to jar.]

  0f077b5 Sun Jan 19 10:30:29 2014 -0800
  Merge pull request #458 from tdas/docs-update
  [Updated java API docs for streaming, along with very minor changes in the code examples.]

  03019d1 Sat Jan 18 16:29:43 2014 -0800
  Merge pull request #459 from srowen/UpdaterL2Regularization
  [Correct L2 regularized weight update with canonical form]

  76147a2 Sat Jan 18 16:24:16 2014 -0800
  Merge pull request #437 from mridulm/master
  [Minor api usability changes]

  4ac8cab Sat Jan 18 16:22:46 2014 -0800
  Merge pull request #426 from mateiz/py-ml-tests
  [Re-enable Python MLlib tests (require Python 2.7 and NumPy 1.7+)]

  34e911c Sat Jan 18 16:17:34 2014 -0800
  Merge pull request #462 from mateiz/conf-file-fix
  [Remove Typesafe Config usage and conf files to fix nested property names]

  ff7201c Sat Jan 18 12:50:02 2014 -0800
  Merge pull request #461 from pwendell/master
  [Use renamed shuffle spill config in CoGroupedRDD.scala]

  7b0d5a5 Thu Jan 16 23:18:48 2014 -0800
  Merge pull request #451 from Qiuzhuang/master
  [Fixed Window spark shell launch script error.]

  4ccedb3 Wed Jan 15 14:26:48 2014 -0800
  Merge pull request #444 from mateiz/py-version
  [Clarify that Python 2.7 is only needed for MLlib]

  e3fa36f Wed Jan 15 13:56:04 2014 -0800
  Merge pull request #442 from pwendell/standalone
  [Workers should use working directory as spark home if it's not specified]

  29c76d9 Wed Jan 15 13:55:48 2014 -0800
  Merge pull request #443 from tdas/filestream-fix
  [Made some classes private[stremaing] and deprecated a method in JavaStreamingContext.]

  aca40aa Wed Jan 15 11:15:47 2014 -0800
  Merge pull request #441 from pwendell/graphx-build
  [GraphX shouldn't list Spark as provided.]

  e12c374 Wed Jan 15 10:01:43 2014 -0800
  Merge pull request #433 from markhamstra/debFix
  [Updated Debian packaging]

  2f015c2 Tue Jan 14 23:17:28 2014 -0800
  Merge pull request #436 from ankurdave/VertexId-case
  [Rename VertexID -> VertexId in GraphX]

  2859cab Tue Jan 14 23:08:19 2014 -0800
  Merge pull request #435 from tdas/filestream-fix
  [Fixed the flaky tests by making SparkConf not serializable]

  fbfbb33 Tue Jan 14 23:06:29 2014 -0800
  Merge pull request #434 from rxin/graphxmaven
  [Fixed SVDPlusPlusSuite in Maven build.]

  2c6c07f Tue Jan 14 21:53:05 2014 -0800
  Merge pull request #424 from jegonzal/GraphXProgrammingGuide
  [Additional edits for clarity in the graphx programming guide.]

  6fa4e02 Tue Jan 14 21:51:25 2014 -0800
  Merge pull request #431 from ankurdave/graphx-caching-doc
  [Describe caching and uncaching in GraphX programming guide]

  2f930d5 Tue Jan 14 15:00:11 2014 -0800
  Merge pull request #428 from pwendell/writeable-objects
  [Don't clone records for text files]

  329c9df Tue Jan 14 14:53:36 2014 -0800
  Merge pull request #429 from ankurdave/graphx-examples-pom.xml
  [Add GraphX dependency to examples/pom.xml]

  a14933d Tue Jan 14 14:52:42 2014 -0800
  Merge pull request #427 from pwendell/deprecate-aggregator
  [Deprecate rather than remove old combineValuesByKey function]

  119b6c5 Tue Jan 14 13:29:08 2014 -0800
  Merge pull request #425 from rxin/scaladoc
  [API doc update & make Broadcast public]

  bf3b150 Tue Jan 14 09:45:22 2014 -0800
  Merge pull request #423 from jegonzal/GraphXProgrammingGuide
  [Improving the graphx-programming-guide]

  1b4adc2 Tue Jan 14 01:19:24 2014 -0800
  Merge pull request #420 from pwendell/header-files
  [Add missing header files]

  b60840e Tue Jan 14 00:48:34 2014 -0800
  Merge pull request #418 from pwendell/0.9-versions
  [Version changes for release 0.9.0.]

  980250b Tue Jan 14 00:05:37 2014 -0800
  Merge pull request #416 from tdas/filestream-fix
  [Removed unnecessary DStream operations and updated docs]

  055be5c Mon Jan 13 23:26:44 2014 -0800
  Merge pull request #415 from pwendell/shuffle-compress
  [Enable compression by default for spills]

  fdaabdc Mon Jan 13 23:08:26 2014 -0800
  Merge pull request #380 from mateiz/py-bayes
  [Add Naive Bayes to Python MLlib, and some API fixes]

  4a805af Mon Jan 13 22:58:38 2014 -0800
  Merge pull request #367 from ankurdave/graphx
  [GraphX: Unifying Graphs and Tables]

  945fe7a Mon Jan 13 22:56:12 2014 -0800
  Merge pull request #408 from pwendell/external-serializers
  [Improvements to external sorting]

  68641bc Mon Jan 13 22:54:13 2014 -0800
  Merge pull request #413 from rxin/scaladoc
  [Adjusted visibility of various components and documentation for 0.9.0 release.]

  0ca0d4d Mon Jan 13 22:32:21 2014 -0800
  Merge pull request #401 from andrewor14/master
  [External sorting - Add number of bytes spilled to Web UI]

  08b9fec Mon Jan 13 22:29:03 2014 -0800
  Merge pull request #409 from tdas/unpersist
  [Automatically unpersisting RDDs that have been cleaned up from DStreams]

  b07bc02 Mon Jan 13 20:45:22 2014 -0800
  Merge pull request #412 from harveyfeng/master
  [Add default value for HadoopRDD's `cloneRecords` constructor arg]

  a2fee38 Mon Jan 13 19:45:26 2014 -0800
  Merge pull request #411 from tdas/filestream-fix
  [Improved logic of finding new files in FileInputDStream]

  01c0d72 Mon Jan 13 16:24:30 2014 -0800
  Merge pull request #410 from rxin/scaladoc1
  [Updated JavaStreamingContext to make scaladoc compile.]

  8038da2 Mon Jan 13 14:59:30 2014 -0800
  Merge pull request #2 from jegonzal/GraphXCCIssue
  [Improving documentation and identifying potential bug in CC calculation.]

  b93f9d4 Mon Jan 13 12:18:05 2014 -0800
  Merge pull request #400 from tdas/dstream-move
  [Moved DStream and PairDSream to org.apache.spark.streaming.dstream]

  e6ed13f Sun Jan 12 22:35:14 2014 -0800
  Merge pull request #397 from pwendell/host-port
  [Remove now un-needed hostPort option]

  0b96d85 Sun Jan 12 21:31:43 2014 -0800
  Merge pull request #399 from pwendell/consolidate-off
  [Disable shuffle file consolidation by default]

  0ab505a Sun Jan 12 21:31:04 2014 -0800
  Merge pull request #395 from hsaputra/remove_simpleredundantreturn_scala
  [Remove simple redundant return statements for Scala methods/functions]

  405bfe8 Sun Jan 12 20:04:21 2014 -0800
  Merge pull request #394 from tdas/error-handling
  [Better error handling in Spark Streaming and more API cleanup]

  28a6b0c Sun Jan 12 19:49:36 2014 -0800
  Merge pull request #398 from pwendell/streaming-api
  [Rename DStream.foreach to DStream.foreachRDD]

  074f502 Sun Jan 12 17:01:13 2014 -0800
  Merge pull request #396 from pwendell/executor-env
  [Setting load defaults to true in executor]

  82e2b92 Sun Jan 12 16:55:11 2014 -0800
  Merge pull request #392 from rxin/listenerbus
  [Stop SparkListenerBus daemon thread when DAGScheduler is stopped.]

  288a878 Sat Jan 11 21:53:19 2014 -0800
  Merge pull request #389 from rxin/clone-writables
  [Minor update for clone writables and more documentation.]

  dbc11df Sat Jan 11 18:07:13 2014 -0800
  Merge pull request #388 from pwendell/master
  [Fix UI bug introduced in #244.]

  409866b Sat Jan 11 17:12:06 2014 -0800
  Merge pull request #393 from pwendell/revert-381
  [Revert PR 381]

  6510f04 Sat Jan 11 12:48:26 2014 -0800
  Merge pull request #387 from jerryshao/conf-fix
  [Fix configure didn't work small problem in ALS]

  ee6e7f9 Sat Jan 11 12:07:55 2014 -0800
  Merge pull request #359 from ScrapCodes/clone-writables
  [We clone hadoop key and values by default and reuse objects if asked to.]

  4216178 Sat Jan 11 09:46:48 2014 -0800
  Merge pull request #373 from jerryshao/kafka-upgrade
  [Upgrade Kafka dependecy to 0.8.0 release version]

  92ad18b Fri Jan 10 23:25:15 2014 -0800
  Merge pull request #376 from prabeesh/master
  [Change clientId to random clientId]

  0b5ce7a Fri Jan 10 23:23:21 2014 -0800
  Merge pull request #386 from pwendell/typo-fix
  [Small typo fix]

  1d7bef0 Fri Jan 10 18:53:03 2014 -0800
  Merge pull request #381 from mateiz/default-ttl
  [Fix default TTL for metadata cleaner]

  44d6a8e Fri Jan 10 17:51:50 2014 -0800
  Merge pull request #382 from RongGu/master
  [Fix a type error in comment lines]

  88faa30 Fri Jan 10 17:14:22 2014 -0800
  Merge pull request #385 from shivaram/add-i2-instances
  [Add i2 instance types to Spark EC2.]

  f265531 Fri Jan 10 16:25:44 2014 -0800
  Merge pull request #383 from tdas/driver-test
  [API for automatic driver recovery for streaming programs and other bug fixes]

  d37408f Fri Jan 10 16:25:01 2014 -0800
  Merge pull request #377 from andrewor14/master
  [External Sorting for Aggregator and CoGroupedRDDs (Revisited)]

  0eaf01c Fri Jan 10 15:32:19 2014 -0800
  Merge pull request #369 from pillis/master
  [SPARK-961 Add a Vector.random() method]

  7cef843 Fri Jan 10 15:34:15 2014 -0600
  Merge pull request #371 from tgravescs/yarn_client_addjar_misc_fixes
  [Yarn client addjar and misc fixes]

  7b58f11 Fri Jan 10 12:47:46 2014 -0800
  Merge pull request #384 from pwendell/debug-logs
  [Make DEBUG-level logs consummable.]

  23d2995 Fri Jan 10 10:20:02 2014 -0800
  Merge pull request #1 from jegonzal/graphx
  [ProgrammingGuide]

  0ebc973 Thu Jan 9 23:58:49 2014 -0800
  Merge pull request #375 from mateiz/option-fix
  [Fix bug added when we changed AppDescription.maxCores to an Option]

  dd03cea Thu Jan 9 23:38:03 2014 -0800
  Merge pull request #378 from pwendell/consolidate_on
  [Enable shuffle consolidation by default.]

  997c830 Thu Jan 9 22:22:20 2014 -0800
  Merge pull request #363 from pwendell/streaming-logs
  [Set default logging to WARN for Spark streaming examples.]

  300eaa9 Thu Jan 9 20:29:51 2014 -0800
  Merge pull request #353 from pwendell/ipython-simplify
  [Simplify and fix pyspark script.]

  4b074fa Thu Jan 9 19:03:55 2014 -0800
  Merge pull request #374 from mateiz/completeness
  [Add some missing Java API methods]

  a9d5333 Thu Jan 9 18:46:46 2014 -0800
  Merge pull request #294 from RongGu/master
  [Bug fixes for updating the RDD block's memory and disk usage information]

  d86a85e Thu Jan 9 18:37:52 2014 -0800
  Merge pull request #293 from pwendell/standalone-driver
  [SPARK-998: Support Launching Driver Inside of Standalone Mode]

  26cdb5f Thu Jan 9 17:16:34 2014 -0800
  Merge pull request #372 from pwendell/log4j-fix-1
  [Send logs to stderr by default (instead of stdout).]

  12f414e Thu Jan 9 15:31:30 2014 -0800
  Merge pull request #362 from mateiz/conf-getters
  [Use typed getters for configuration settings]

  365cac9 Thu Jan 9 00:56:16 2014 -0800
  Merge pull request #361 from rxin/clean
  [Minor style cleanup. Mostly on indenting & line width changes.]

  73c724e Thu Jan 9 00:32:19 2014 -0800
  Merge pull request #368 from pwendell/sbt-fix
  [Don't delegate to users `sbt`.]

  dceedb4 Wed Jan 8 23:19:28 2014 -0800
  Merge pull request #364 from pwendell/fix
  [Fixing config option ""retained_stages"" => ""retainedStages"".]

  04d83fc Wed Jan 8 11:55:37 2014 -0800
  Merge pull request #360 from witgo/master
  [fix make-distribution.sh show version: command not found]

  56ebfea Wed Jan 8 11:50:06 2014 -0800
  Merge pull request #357 from hsaputra/set_boolean_paramname
  [Set boolean param name for call to SparkHadoopMapReduceUtil.newTaskAttemptID]

  bdeaeaf Wed Jan 8 11:48:39 2014 -0800
  Merge pull request #358 from pwendell/add-cdh
  [Add CDH Repository to Maven Build]

  5cae05f Wed Jan 8 11:47:28 2014 -0800
  Merge pull request #356 from hsaputra/remove_deprecated_cleanup_method
  [Remove calls to deprecated mapred's OutputCommitter.cleanupJob]

  6eef78d Wed Jan 8 08:49:20 2014 -0600
  Merge pull request #345 from colorant/yarn
  [support distributing extra files to worker for yarn client mode]

  bb6a39a Tue Jan 7 22:32:18 2014 -0800
  Merge pull request #322 from falaki/MLLibDocumentationImprovement
  [SPARK-1009 Updated MLlib docs to show how to use it in Python]

  cb1b927 Tue Jan 7 22:26:28 2014 -0800
  Merge pull request #355 from ScrapCodes/patch-1
  [Update README.md]

  c0f0155 Tue Jan 7 22:21:52 2014 -0800
  Merge pull request #313 from tdas/project-refactor
  [Refactored the streaming project to separate external libraries like Twitter, Kafka, Flume, etc.]

  f5f12dc Tue Jan 7 21:56:35 2014 -0800
  Merge pull request #336 from liancheng/akka-remote-lookup
  [Get rid of `Either[ActorRef, ActorSelection]']

  11891e6 Wed Jan 8 00:32:18 2014 -0500
  Merge pull request #327 from lucarosellini/master
  [Added ‚Äò-i‚Äô command line option to Spark REPL]

  7d0aac9 Wed Jan 8 00:30:45 2014 -0500
  Merge pull request #354 from hsaputra/addasfheadertosbt
  [Add ASF header to the new sbt script.]

  d75dc42 Wed Jan 8 00:30:03 2014 -0500
  Merge pull request #350 from mateiz/standalone-limit
  [Add way to limit default # of cores used by apps in standalone mode]

  61674bc Tue Jan 7 18:32:13 2014 -0800
  Merge pull request #352 from markhamstra/oldArch
  [Don't leave os.arch unset after BlockManagerSuite]

  b2e690f Tue Jan 7 16:57:08 2014 -0800
  Merge pull request #328 from falaki/MatrixFactorizationModel-fix
  [SPARK-1012: DAGScheduler Exception Fix]

  6ccf8ce Tue Jan 7 15:49:14 2014 -0800
  Merge pull request #351 from pwendell/maven-fix
  [Add log4j exclusion rule to maven.]

  7d5fa17 Tue Jan 7 11:31:34 2014 -0800
  Merge pull request #337 from yinxusen/mllib-16-bugfix
  [Mllib 16 bugfix]

  71fc113 Tue Jan 7 11:30:35 2014 -0800
  Merge pull request #349 from CodingCat/support-worker_dir
  [add the comments about SPARK_WORKER_DIR]

  15d9534 Tue Jan 7 08:10:02 2014 -0800
  Merge pull request #318 from srowen/master
  [Suggested small changes to Java code for slightly more standard style, encapsulation and in some cases performance]

  468af0f Tue Jan 7 08:09:01 2014 -0800
  Merge pull request #348 from prabeesh/master
  [spark -> org.apache.spark]

  c3cf047 Tue Jan 7 00:54:25 2014 -0800
  Merge pull request #339 from ScrapCodes/conf-improvements
  [Conf improvements]

  a862caf Tue Jan 7 00:18:20 2014 -0800
  Merge pull request #331 from holdenk/master
  [Add a script to download sbt if not present on the system]

  b97ef21 Mon Jan 6 20:12:57 2014 -0800
  Merge pull request #346 from sproblvem/patch-1
  [Update stop-slaves.sh]

  7210257 Mon Jan 6 18:25:44 2014 -0800
  Merge pull request #128 from adamnovak/master
  [Fix failing ""sbt/sbt publish-local"" by adding a no-argument PrimitiveKeyOpenHashMap constructor]

  e4d6057 Mon Jan 6 14:56:54 2014 -0800
  Merge pull request #343 from pwendell/build-fix
  [Fix test breaking downstream builds]

  93bf962 Mon Jan 6 11:42:41 2014 -0800
  Merge pull request #340 from ScrapCodes/sbt-fixes
  [Made java options to be applied during tests so that they become self explanatory.]

  60edeb3 Mon Jan 6 11:40:32 2014 -0800
  Merge pull request #338 from ScrapCodes/ning-upgrade
  [SPARK-1005 Ning upgrade]

  c708e81 Mon Jan 6 11:35:48 2014 -0800
  Merge pull request #341 from ash211/patch-5
  [Clarify spark.cores.max in docs]

  33fcb91 Mon Jan 6 11:19:23 2014 -0800
  Merge pull request #342 from tgravescs/fix_maven_protobuf
  [Change protobuf version for yarn alpha back to 2.4.1]

  357083c Mon Jan 6 10:29:04 2014 -0800
  Merge pull request #330 from tgravescs/fix_addjars_null_handling
  [Fix handling of empty SPARK_EXAMPLES_JAR]

  a2e7e04 Sun Jan 5 22:37:36 2014 -0800
  Merge pull request #333 from pwendell/logging-silence
  [Quiet ERROR-level Akka Logs]

  5b0986a Sun Jan 5 19:25:09 2014 -0800
  Merge pull request #334 from pwendell/examples-fix
  [Removing SPARK_EXAMPLES_JAR in the code]

  f4b924f Sun Jan 5 17:11:47 2014 -0800
  Merge pull request #335 from rxin/ser
  [Fall back to zero-arg constructor for Serializer initialization if there is no constructor that accepts SparkConf.]

  d43ad3e Sat Jan 4 16:29:30 2014 -0800
  Merge pull request #292 from soulmachine/naive-bayes
  [standard Naive Bayes classifier]

  86404da Sat Jan 4 14:55:54 2014 -0800
  Merge pull request #127 from jegonzal/MapByPartition
  [Adding mapEdges and mapTriplets by Partition]

  e68cdb1 Sat Jan 4 13:46:02 2014 -0800
  Merge pull request #124 from jianpingjwang/master
  [refactor and bug fix]

  280ddf6 Sat Jan 4 12:54:41 2014 -0800
  Merge pull request #121 from ankurdave/more-simplify
  [Simplify GraphImpl internals further]

  10fe23b Fri Jan 3 23:50:14 2014 -0800
  Merge pull request #329 from pwendell/remove-binaries
  [SPARK-1002: Remove Binaries from Spark Source]

  c4d6145 Fri Jan 3 16:30:53 2014 -0800
  Merge pull request #325 from witgo/master
  [Modify spark on yarn to create SparkConf process]

  4ae101f Fri Jan 3 11:24:35 2014 -0800
  Merge pull request #317 from ScrapCodes/spark-915-segregate-scripts
  [Spark-915 segregate scripts]

  87248bd Fri Jan 3 00:45:31 2014 -0800
  Merge pull request #1 from apache/master
  [Merge latest Spark changes]

  30b9db0 Thu Jan 2 23:15:55 2014 -0800
  Merge pull request #285 from colorant/yarn-refactor
  [Yarn refactor]

  498a5f0 Thu Jan 2 19:06:40 2014 -0800
  Merge pull request #323 from tgravescs/sparkconf_yarn_fix
  [fix spark on yarn after the sparkConf changes]

  0475ca8 Thu Jan 2 15:17:08 2014 -0800
  Merge pull request #320 from kayousterhout/erroneous_failed_msg
  [Remove erroneous FAILED state for killed tasks.]

  588a169 Thu Jan 2 13:20:54 2014 -0800
  Merge pull request #297 from tdas/window-improvement
  [Improvements to DStream window ops and refactoring of Spark's CheckpointSuite]

  5e67cdc Thu Jan 2 12:56:28 2014 -0800
  Merge pull request #319 from kayousterhout/remove_error_method
  [Removed redundant TaskSetManager.error() function.]

  ca67909 Thu Jan 2 15:54:54 2014 -0500
  Merge pull request #311 from tmyklebu/master
  [SPARK-991: Report information gleaned from a Python stacktrace in the UI]

  3713f81 Wed Jan 1 21:29:12 2014 -0800
  Merge pull request #309 from mateiz/conf2
  [SPARK-544. Migrate configuration to a SparkConf class]

  c1d928a Wed Jan 1 17:03:48 2014 -0800
  Merge pull request #312 from pwendell/log4j-fix-2
  [SPARK-1008: Logging improvments]

  dc9cb83 Wed Jan 1 13:28:34 2014 -0800
  Merge pull request #126 from jegonzal/FixingPersist
  [Fixing Persist Behavior]

  9a0ff72 Tue Dec 31 21:50:24 2013 -0800
  Merge pull request #314 from witgo/master
  [restore core/pom.xml file modification]

  8b8e70e Tue Dec 31 17:48:24 2013 -0800
  Merge pull request #73 from falaki/ApproximateDistinctCount
  [Approximate distinct count]

  63b411d Tue Dec 31 14:31:28 2013 -0800
  Merge pull request #238 from ngbinh/upgradeNetty
  [upgrade Netty from 4.0.0.Beta2 to 4.0.13.Final]

  32d6ae9 Tue Dec 31 13:51:07 2013 -0800
  Merge pull request #120 from ankurdave/subgraph-reuses-view
  [Reuse VTableReplicated in GraphImpl.subgraph]

  55b7e2f Tue Dec 31 10:12:51 2013 -0800
  Merge pull request #289 from tdas/filestream-fix
  [Bug fixes for file input stream and checkpointing]

  2b71ab9 Mon Dec 30 11:01:30 2013 -0800
  Merge pull request from aarondav: Utilize DiskBlockManager pathway for temp file writing
  [This gives us a couple advantages:]

  50e3b8e Mon Dec 30 07:44:26 2013 -0800
  Merge pull request #308 from kayousterhout/stage_naming
  [Changed naming of StageCompleted event to be consistent]

  72a17b6 Sat Dec 28 21:25:40 2013 -1000
  Revert ""Merge pull request #310 from jyunfan/master""
  [This reverts commit 79b20e4dbe3dcd8559ec8316784d3334bb55868b, reversing]

  79b20e4 Sat Dec 28 21:13:36 2013 -1000
  Merge pull request #310 from jyunfan/master
  [Fix typo in the Accumulators section]

  7375047 Sat Dec 28 13:25:06 2013 -0800
  Merge pull request #304 from kayousterhout/remove_unused
  [Removed unused failed and causeOfFailure variables (in TaskSetManager)]

  ad3dfd1 Fri Dec 27 22:10:14 2013 -0500
  Merge pull request #307 from kayousterhout/other_failure
  [Removed unused OtherFailure TaskEndReason.]

  b579b83 Fri Dec 27 22:09:04 2013 -0500
  Merge pull request #306 from kayousterhout/remove_pending
  [Remove unused hasPendingTasks methods]

  19672dc Fri Dec 27 13:37:10 2013 -0800
  Merge pull request #305 from kayousterhout/line_spacing
  [Fixed >100char lines in DAGScheduler.scala]

  7be1e57 Thu Dec 26 23:41:40 2013 -1000
  Merge pull request #298 from aarondav/minor
  [Minor: Decrease margin of left side of Log page]

  7d811ba Thu Dec 26 23:39:58 2013 -1000
  Merge pull request #302 from pwendell/SPARK-1007
  [SPARK-1007: spark-class2.cmd should change SCALA_VERSION to be 2.10]

  5e69fc5 Thu Dec 26 19:10:39 2013 -0500
  Merge pull request #295 from markhamstra/JobProgressListenerNPE
  [Avoid a lump of coal (NPE) in JobProgressListener's stocking.]

  da20270 Thu Dec 26 12:11:52 2013 -0800
  Merge pull request #1 from aarondav/driver
  [Refactor DriverClient to be more Actor-based]

  e240bad Thu Dec 26 12:30:48 2013 -0500
  Merge pull request #296 from witgo/master
  [Renamed ClusterScheduler to TaskSchedulerImpl for yarn and new-yarn package]

  c344ed0 Thu Dec 26 01:31:06 2013 -0500
  Merge pull request #283 from tmyklebu/master
  [Python bindings for mllib]

  56094bc Wed Dec 25 13:14:33 2013 -0500
  Merge pull request #290 from ash211/patch-3
  [Typo: avaiable -> available]

  4842a07 Wed Dec 25 01:52:15 2013 -0800
  Merge pull request #287 from azuryyu/master
  [Fixed job name in the java streaming example.]

  85a344b Tue Dec 24 16:35:06 2013 -0800
  Merge pull request #127 from kayousterhout/consolidate_schedulers
  [Deduplicate Local and Cluster schedulers.]

  c2dd6bc Tue Dec 24 14:36:47 2013 -0800
  Merge pull request #279 from aarondav/shuffle-cleanup0
  [Clean up shuffle files once their metadata is gone]

  3bf7c70 Tue Dec 24 16:37:13 2013 -0500
  Merge pull request #275 from ueshin/wip/changeclasspathorder
  [Change the order of CLASSPATH.]

  d63856c Mon Dec 23 22:07:26 2013 -0800
  Merge pull request #286 from rxin/build
  [Show full stack trace and time taken in unit tests.]

  23a9ae6 Tue Dec 24 00:08:48 2013 -0500
  Merge pull request #277 from tdas/scheduler-update
  [Refactored the streaming scheduler and added StreamingListener interface]

  11107c9 Mon Dec 23 10:38:20 2013 -0800
  Merge pull request #244 from leftnoteasy/master
  [Added SPARK-968 implementation for review]

  44e4205 Sun Dec 22 11:44:18 2013 -0800
  Merge pull request #116 from jianpingjwang/master
  [remove unused variables and fix a bug]

  4797c22 Fri Dec 20 13:30:39 2013 -0800
  Merge pull request #118 from ankurdave/VertexPartitionSuite
  [Test VertexPartition and fix bugs]

  0bc57c5 Fri Dec 20 11:56:54 2013 -0800
  Merge pull request #280 from aarondav/minor
  [Minor cleanup for standalone scheduler]

  ac70b8f Fri Dec 20 10:56:10 2013 -0800
  Merge pull request #117 from ankurdave/more-tests
  [More tests]

  45310d4 Thu Dec 19 22:08:20 2013 -0800
  Merge pull request #115 from ankurdave/test-reorg
  [Reorganize unit tests; improve GraphSuite test coverage]

  9228ec8 Thu Dec 19 21:37:15 2013 -0800
  Merge pull request #1 from aarondav/127
  [Merge master into 127]

  eca68d4 Thu Dec 19 18:12:22 2013 -0800
  Merge pull request #272 from tmyklebu/master
  [Track and report task result serialisation time.]

  7990c56 Thu Dec 19 13:35:09 2013 -0800
  Merge pull request #276 from shivaram/collectPartition
  [Add collectPartition to JavaRDD interface.]

  440e531 Thu Dec 19 10:38:56 2013 -0800
  Merge pull request #278 from MLnick/java-python-tostring
  [Add toString to Java RDD, and __repr__ to Python RDD]

  d8d3f3e Thu Dec 19 00:06:43 2013 -0800
  Merge pull request #183 from aarondav/spark-959
  [[SPARK-959] Explicitly depend on org.eclipse.jetty.orbit jar]

  bfba532 Wed Dec 18 22:22:21 2013 -0800
  Merge pull request #247 from aarondav/minor
  [Increase spark.akka.askTimeout default to 30 seconds]

  da301b5 Wed Dec 18 20:03:29 2013 -0800
  Merge pull request #112 from amatsukawa/scc
  [Strongly connected component algorithm]

  c64a53a Wed Dec 18 16:56:26 2013 -0800
  Merge pull request #267 from JoshRosen/cygwin
  [Fix Cygwin support in several scripts.]

  a645ef6 Wed Dec 18 16:07:52 2013 -0800
  Merge pull request #48 from amatsukawa/add_project_to_graph
  [Add mask operation on graph and filter graph primitive]

  d7ebff0 Wed Dec 18 15:38:48 2013 -0800
  Merge pull request #1 from ankurdave/add_project_to_graph
  [Merge current master and reimplement Graph.mask "
"
Kay Ousterhout <keo@eecs.berkeley.edu>,Tue"," 21 Jan 2014 10:02:19 -0800""",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc4),dev@spark.incubator.apache.org,"For those using gmail, note that it condenses Patrick's ""new thread"" into
the same thread as the original email (see
http://apache-spark-developers-list.1001551.n3.nabble.com/ for the two
separate threads).



"
Andy Konwinski <andykonwinski@gmail.com>,"Tue, 21 Jan 2014 13:04:39 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc3),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I think we might need to change the subject to [RESULT][VOTE] or something
so that the apache scripts can tell when a vote thread ends, even if it's
simply being replaced by another vote thread for rc{n+1}.



"
Patrick Wendell <pwendell@gmail.com>,"Tue, 21 Jan 2014 13:52:21 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc3),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","This isn't a new RC or a result - I just tried to make an new vote
thread and the mail clients get confused and merge it in with the old
thread :(


"
Patrick Wendell <pwendell@gmail.com>,"Tue, 21 Jan 2014 13:53:11 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc3),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Ah oops Andy I see - thought you were discussing something different.


"
Patrick Wendell <pwendell@gmail.com>,"Tue, 21 Jan 2014 14:45:13 -0800",[VOTE] Release Apache Spark 0.9.0-incubating (rc4) [new thread],"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Please vote on releasing the following candidate as Apache Spark
(incubating) version 0.9.0.

A draft of the release notes along with the changes file is attached
to this e-mail.

The tag to be voted on is v0.9.0-incubating (commit 0771df67):
https://git-wip-us.apache.org/repos/asf?p=incubator-spark.git;a=commit;h=0771df675363c69622404cb514bd751bc90526af

The release files, including signatures, digests, etc can be found at:
http://people.apache.org/~pwendell/spark-0.9.0-incubating-rc4

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1005/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-0.9.0-incubating-rc4-docs/

Please vote on releasing this package as Apache Spark 0.9.0-incubating!

The vote is open until Friday, January 24, at 11:15 UTC and passes if
a majority of at least 3 +1 PPMC votes are cast.

[ ] +1 Release this package as Apache Spark 0.9.0-incubating
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.incubator.apache.org/
Spark 0.9.0 is a major release that adds significant new features. It updates Spark to Scala 2.10, simplifies high availability, and updates numerous components of the project. This release includes a first version of GraphX, a powerful new framework for graph processing that comes with a library of standard algorithms. In addition, Spark Streaming is now out of alpha, and includes significant optimizations and simplified high availability deployment.

### Scala 2.10 Support

Spark now runs on Scala 2.10, letting users benefit from the language and library improvements in this version.

### Configuration System

The new [SparkConf] class is now the preferred way to configure advanced settings on your SparkContext, though the previous Java system property still works. SparkConf is especially useful in tests to make sure properties don‚Äôt stay set across tests.

### Spark Streaming Improvements

Spark Streaming is no longer alpha, and comes with simplified high availability and several optimizations.

* When running on a Spark standalone cluster with the [standalone cluster high availability mode], you can submit a Spark Streaming driver application to the cluster and have it automatically recovered if either the driver or the cluster master crashes.
* Windowed operators have been sped up by 30-50%.
* Spark Streaming‚Äôs input source plugins (e.g. for Twitter, Kafka and Flume) are now separate projects, making it easier to pull in only the dependencies you need.
* A new StreamingListener interface has been added for monitoring statistics about the streaming computation.
* A few aspects of the API have been improved:
* `DStream` and `PairDStream` classes have been moved from `org.apache.spark.streaming` to `org.apache.spark.streaming.dstream` to keep it consistent with `org.apache.spark.rdd.RDD`.
* `DStream.foreach` -> `DStream.foreachRDD` to make it explicit that it works for every RDD, not every element
* `StreamingContext.awaitTermination()` allows you wait for context shutdown and catch any exception that occurs in the streaming computation.
*`StreamingContext.stop()` now allows stopping of StreamingContext without stopping the underlying SparkContext.

### GraphX Alpha

GraphX is a new API for graph processing that uses recent advances in graph-parallel computation. It lets you build a graph within a Spark program using the standard Spark operators, then process it with new graph operators that are optimized for distributed computation. It includes basic transformations, a Pregel API for iterative computation, and a standard library of graph loaders and analytics algorithms. By offering these features within the Spark engine, GraphX can significantly speed up processing tasks compared to workflows that use different engines.

GraphX features in this release include:

* Building graphs from arbitrary Spark RDDs
* Basic operations to transform graphs or extract subgraphs
* An optimized Pregel API that takes advantage of graph partitioning and indexing
* Standard algorithms including PageRank, connected components, strongly connected components, SVD++, and triangle counting
* Interactive use from the Spark shell

GraphX is still marked as alpha in this first release, but we recommend for new users to use it instead of the more limited Bagel API.

### MLlib Improvements

* Spark‚Äôs machine learning library (MLlib) is now available in Python, where it operates on NumPy data (currently requires Python 2.7 and NumPy 1.7)
* A new algorithm has been added for Naive Bayes classification
* Alternating Least Squares can now be used to predict ratings for multiple items in parallel
* MLlib‚Äôs documentation was expanded to include more examples in Scala, Java and Python

### Python Changes

* Python users can now use MLlib (requires Python 2.7 and NumPy 1.7)
* PySpark now shows the call sites of running jobs in the Spark application UI (http://<driver>:4040), making it easy to see which part of your code is running
* IPython operation has been improved

### Packaging

* Spark‚Äôs scripts have been organized into ‚Äúbin‚Äù and ‚Äúsbin‚Äù directories to make it easier to separate admin scripts from user ones and install Spark on standard Linux paths.
* Log configuration has been improved so that Spark finds a default log4j.properties file if you don‚Äôt specify one.

### Core Engine

* Spark‚Äôs standalone mode now supports submitting a driver program to run on the cluster instead of on the external machine submitting it. You can access this functionality through the [org.apache.spark.deploy.Client] class.
* Large reduce operations now automatically spill data to disk if it does not fit in memory.
* Users of standalone mode can now limit how many cores an application will use by default if the application writer didn‚Äôt configure its size. Previously, such applications took all available cores on the cluster.
* `spark-shell` now supports the `-i` option to run a script on startup.
* New `histogram` and `countDistinctApprox` operators have been added for working with numerical data.
* Methods for reading Hadoop `Writable` objects, such as `SparkContext.sequenceFile` and `hadoopRDD`, now clone the records read by default instead of reusing the same Writable instance, making the resulting RDDs easier to cache; if you only want to use each object once, you can pass an extra flag to disable this cloning
* YARN mode now supports distributing extra files with the application, and several bugs have been fixed.

### Compatibility

This release is compatible with the previous APIs in stable components, but several language versions and script locations have changed.

* Scala programs now need to use Scala 2.10 instead of 2.9.
* Scripts such as `spark-shell` and `pyspark` have been moved into the `bin` folder, while administrative scripts to start and stop standalone clusters have been moved into `sbin`.
* Spark Streaming‚Äôs API has been changed to move external input sources into separate modules, `DStream` and `PairDStream` has been moved to package `org.apache.spark.streaming.dstream` and `DStream.foreach` has been renamed to `foreachRDD`. We expect the current API to be stable now that Spark Streaming is out of alpha.
* While the old method of configuring Spark through Java system properties still works, we recommend that users update to the new SparkConf, which is easier to inspect and use.

We expect all of the current APIs and script locations in Spark 0.9 to remain stable when we release Spark 1.0. We wanted to make these updates early to give users a chance to switch to the new API.


Spark Change Log

Release 0.9.0-incubating

  b6fd3cd Tue Jan 21 00:12:01 2014 -0800
  Merge pull request #480 from pwendell/0.9-fixes
  [Handful of 0.9 fixes]

  e5f8917 Mon Jan 20 23:35:07 2014 -0800
  Merge pull request #484 from tdas/run-example-fix
  [Made run-example respect SPARK_JAVA_OPTS and SPARK_MEM.]

  410ba06 Mon Jan 20 22:26:14 2014 -0800
  Merge pull request #482 from tdas/streaming-example-fix
  [Added StreamingContext.awaitTermination to streaming examples]

  f137947 Mon Jan 20 22:24:07 2014 -0800
  Merge pull request #483 from pwendell/gitignore
  [Restricting /lib to top level directory in .gitignore]

  94ae25d Sun Jan 19 11:33:51 2014 -0800
  Merge pull request #470 from tgravescs/fix_spark_examples_yarn
  [Only log error on missing jar to allow spark examples to jar.]

  0f077b5 Sun Jan 19 10:30:29 2014 -0800
  Merge pull request #458 from tdas/docs-update
  [Updated java API docs for streaming, along with very minor changes in the code examples.]

  03019d1 Sat Jan 18 16:29:43 2014 -0800
  Merge pull request #459 from srowen/UpdaterL2Regularization
  [Correct L2 regularized weight update with canonical form]

  76147a2 Sat Jan 18 16:24:16 2014 -0800
  Merge pull request #437 from mridulm/master
  [Minor api usability changes]

  4ac8cab Sat Jan 18 16:22:46 2014 -0800
  Merge pull request #426 from mateiz/py-ml-tests
  [Re-enable Python MLlib tests (require Python 2.7 and NumPy 1.7+)]

  34e911c Sat Jan 18 16:17:34 2014 -0800
  Merge pull request #462 from mateiz/conf-file-fix
  [Remove Typesafe Config usage and conf files to fix nested property names]

  ff7201c Sat Jan 18 12:50:02 2014 -0800
  Merge pull request #461 from pwendell/master
  [Use renamed shuffle spill config in CoGroupedRDD.scala]

  7b0d5a5 Thu Jan 16 23:18:48 2014 -0800
  Merge pull request #451 from Qiuzhuang/master
  [Fixed Window spark shell launch script error.]

  4ccedb3 Wed Jan 15 14:26:48 2014 -0800
  Merge pull request #444 from mateiz/py-version
  [Clarify that Python 2.7 is only needed for MLlib]

  e3fa36f Wed Jan 15 13:56:04 2014 -0800
  Merge pull request #442 from pwendell/standalone
  [Workers should use working directory as spark home if it's not specified]

  29c76d9 Wed Jan 15 13:55:48 2014 -0800
  Merge pull request #443 from tdas/filestream-fix
  [Made some classes private[stremaing] and deprecated a method in JavaStreamingContext.]

  aca40aa Wed Jan 15 11:15:47 2014 -0800
  Merge pull request #441 from pwendell/graphx-build
  [GraphX shouldn't list Spark as provided.]

  e12c374 Wed Jan 15 10:01:43 2014 -0800
  Merge pull request #433 from markhamstra/debFix
  [Updated Debian packaging]

  2f015c2 Tue Jan 14 23:17:28 2014 -0800
  Merge pull request #436 from ankurdave/VertexId-case
  [Rename VertexID -> VertexId in GraphX]

  2859cab Tue Jan 14 23:08:19 2014 -0800
  Merge pull request #435 from tdas/filestream-fix
  [Fixed the flaky tests by making SparkConf not serializable]

  fbfbb33 Tue Jan 14 23:06:29 2014 -0800
  Merge pull request #434 from rxin/graphxmaven
  [Fixed SVDPlusPlusSuite in Maven build.]

  2c6c07f Tue Jan 14 21:53:05 2014 -0800
  Merge pull request #424 from jegonzal/GraphXProgrammingGuide
  [Additional edits for clarity in the graphx programming guide.]

  6fa4e02 Tue Jan 14 21:51:25 2014 -0800
  Merge pull request #431 from ankurdave/graphx-caching-doc
  [Describe caching and uncaching in GraphX programming guide]

  2f930d5 Tue Jan 14 15:00:11 2014 -0800
  Merge pull request #428 from pwendell/writeable-objects
  [Don't clone records for text files]

  329c9df Tue Jan 14 14:53:36 2014 -0800
  Merge pull request #429 from ankurdave/graphx-examples-pom.xml
  [Add GraphX dependency to examples/pom.xml]

  a14933d Tue Jan 14 14:52:42 2014 -0800
  Merge pull request #427 from pwendell/deprecate-aggregator
  [Deprecate rather than remove old combineValuesByKey function]

  119b6c5 Tue Jan 14 13:29:08 2014 -0800
  Merge pull request #425 from rxin/scaladoc
  [API doc update & make Broadcast public]

  bf3b150 Tue Jan 14 09:45:22 2014 -0800
  Merge pull request #423 from jegonzal/GraphXProgrammingGuide
  [Improving the graphx-programming-guide]

  1b4adc2 Tue Jan 14 01:19:24 2014 -0800
  Merge pull request #420 from pwendell/header-files
  [Add missing header files]

  b60840e Tue Jan 14 00:48:34 2014 -0800
  Merge pull request #418 from pwendell/0.9-versions
  [Version changes for release 0.9.0.]

  980250b Tue Jan 14 00:05:37 2014 -0800
  Merge pull request #416 from tdas/filestream-fix
  [Removed unnecessary DStream operations and updated docs]

  055be5c Mon Jan 13 23:26:44 2014 -0800
  Merge pull request #415 from pwendell/shuffle-compress
  [Enable compression by default for spills]

  fdaabdc Mon Jan 13 23:08:26 2014 -0800
  Merge pull request #380 from mateiz/py-bayes
  [Add Naive Bayes to Python MLlib, and some API fixes]

  4a805af Mon Jan 13 22:58:38 2014 -0800
  Merge pull request #367 from ankurdave/graphx
  [GraphX: Unifying Graphs and Tables]

  945fe7a Mon Jan 13 22:56:12 2014 -0800
  Merge pull request #408 from pwendell/external-serializers
  [Improvements to external sorting]

  68641bc Mon Jan 13 22:54:13 2014 -0800
  Merge pull request #413 from rxin/scaladoc
  [Adjusted visibility of various components and documentation for 0.9.0 release.]

  0ca0d4d Mon Jan 13 22:32:21 2014 -0800
  Merge pull request #401 from andrewor14/master
  [External sorting - Add number of bytes spilled to Web UI]

  08b9fec Mon Jan 13 22:29:03 2014 -0800
  Merge pull request #409 from tdas/unpersist
  [Automatically unpersisting RDDs that have been cleaned up from DStreams]

  b07bc02 Mon Jan 13 20:45:22 2014 -0800
  Merge pull request #412 from harveyfeng/master
  [Add default value for HadoopRDD's `cloneRecords` constructor arg]

  a2fee38 Mon Jan 13 19:45:26 2014 -0800
  Merge pull request #411 from tdas/filestream-fix
  [Improved logic of finding new files in FileInputDStream]

  01c0d72 Mon Jan 13 16:24:30 2014 -0800
  Merge pull request #410 from rxin/scaladoc1
  [Updated JavaStreamingContext to make scaladoc compile.]

  8038da2 Mon Jan 13 14:59:30 2014 -0800
  Merge pull request #2 from jegonzal/GraphXCCIssue
  [Improving documentation and identifying potential bug in CC calculation.]

  b93f9d4 Mon Jan 13 12:18:05 2014 -0800
  Merge pull request #400 from tdas/dstream-move
  [Moved DStream and PairDSream to org.apache.spark.streaming.dstream]

  e6ed13f Sun Jan 12 22:35:14 2014 -0800
  Merge pull request #397 from pwendell/host-port
  [Remove now un-needed hostPort option]

  0b96d85 Sun Jan 12 21:31:43 2014 -0800
  Merge pull request #399 from pwendell/consolidate-off
  [Disable shuffle file consolidation by default]

  0ab505a Sun Jan 12 21:31:04 2014 -0800
  Merge pull request #395 from hsaputra/remove_simpleredundantreturn_scala
  [Remove simple redundant return statements for Scala methods/functions]

  405bfe8 Sun Jan 12 20:04:21 2014 -0800
  Merge pull request #394 from tdas/error-handling
  [Better error handling in Spark Streaming and more API cleanup]

  28a6b0c Sun Jan 12 19:49:36 2014 -0800
  Merge pull request #398 from pwendell/streaming-api
  [Rename DStream.foreach to DStream.foreachRDD]

  074f502 Sun Jan 12 17:01:13 2014 -0800
  Merge pull request #396 from pwendell/executor-env
  [Setting load defaults to true in executor]

  82e2b92 Sun Jan 12 16:55:11 2014 -0800
  Merge pull request #392 from rxin/listenerbus
  [Stop SparkListenerBus daemon thread when DAGScheduler is stopped.]

  288a878 Sat Jan 11 21:53:19 2014 -0800
  Merge pull request #389 from rxin/clone-writables
  [Minor update for clone writables and more documentation.]

  dbc11df Sat Jan 11 18:07:13 2014 -0800
  Merge pull request #388 from pwendell/master
  [Fix UI bug introduced in #244.]

  409866b Sat Jan 11 17:12:06 2014 -0800
  Merge pull request #393 from pwendell/revert-381
  [Revert PR 381]

  6510f04 Sat Jan 11 12:48:26 2014 -0800
  Merge pull request #387 from jerryshao/conf-fix
  [Fix configure didn't work small problem in ALS]

  ee6e7f9 Sat Jan 11 12:07:55 2014 -0800
  Merge pull request #359 from ScrapCodes/clone-writables
  [We clone hadoop key and values by default and reuse objects if asked to.]

  4216178 Sat Jan 11 09:46:48 2014 -0800
  Merge pull request #373 from jerryshao/kafka-upgrade
  [Upgrade Kafka dependecy to 0.8.0 release version]

  92ad18b Fri Jan 10 23:25:15 2014 -0800
  Merge pull request #376 from prabeesh/master
  [Change clientId to random clientId]

  0b5ce7a Fri Jan 10 23:23:21 2014 -0800
  Merge pull request #386 from pwendell/typo-fix
  [Small typo fix]

  1d7bef0 Fri Jan 10 18:53:03 2014 -0800
  Merge pull request #381 from mateiz/default-ttl
  [Fix default TTL for metadata cleaner]

  44d6a8e Fri Jan 10 17:51:50 2014 -0800
  Merge pull request #382 from RongGu/master
  [Fix a type error in comment lines]

  88faa30 Fri Jan 10 17:14:22 2014 -0800
  Merge pull request #385 from shivaram/add-i2-instances
  [Add i2 instance types to Spark EC2.]

  f265531 Fri Jan 10 16:25:44 2014 -0800
  Merge pull request #383 from tdas/driver-test
  [API for automatic driver recovery for streaming programs and other bug fixes]

  d37408f Fri Jan 10 16:25:01 2014 -0800
  Merge pull request #377 from andrewor14/master
  [External Sorting for Aggregator and CoGroupedRDDs (Revisited)]

  0eaf01c Fri Jan 10 15:32:19 2014 -0800
  Merge pull request #369 from pillis/master
  [SPARK-961 Add a Vector.random() method]

  7cef843 Fri Jan 10 15:34:15 2014 -0600
  Merge pull request #371 from tgravescs/yarn_client_addjar_misc_fixes
  [Yarn client addjar and misc fixes]

  7b58f11 Fri Jan 10 12:47:46 2014 -0800
  Merge pull request #384 from pwendell/debug-logs
  [Make DEBUG-level logs consummable.]

  23d2995 Fri Jan 10 10:20:02 2014 -0800
  Merge pull request #1 from jegonzal/graphx
  [ProgrammingGuide]

  0ebc973 Thu Jan 9 23:58:49 2014 -0800
  Merge pull request #375 from mateiz/option-fix
  [Fix bug added when we changed AppDescription.maxCores to an Option]

  dd03cea Thu Jan 9 23:38:03 2014 -0800
  Merge pull request #378 from pwendell/consolidate_on
  [Enable shuffle consolidation by default.]

  997c830 Thu Jan 9 22:22:20 2014 -0800
  Merge pull request #363 from pwendell/streaming-logs
  [Set default logging to WARN for Spark streaming examples.]

  300eaa9 Thu Jan 9 20:29:51 2014 -0800
  Merge pull request #353 from pwendell/ipython-simplify
  [Simplify and fix pyspark script.]

  4b074fa Thu Jan 9 19:03:55 2014 -0800
  Merge pull request #374 from mateiz/completeness
  [Add some missing Java API methods]

  a9d5333 Thu Jan 9 18:46:46 2014 -0800
  Merge pull request #294 from RongGu/master
  [Bug fixes for updating the RDD block's memory and disk usage information]

  d86a85e Thu Jan 9 18:37:52 2014 -0800
  Merge pull request #293 from pwendell/standalone-driver
  [SPARK-998: Support Launching Driver Inside of Standalone Mode]

  26cdb5f Thu Jan 9 17:16:34 2014 -0800
  Merge pull request #372 from pwendell/log4j-fix-1
  [Send logs to stderr by default (instead of stdout).]

  12f414e Thu Jan 9 15:31:30 2014 -0800
  Merge pull request #362 from mateiz/conf-getters
  [Use typed getters for configuration settings]

  365cac9 Thu Jan 9 00:56:16 2014 -0800
  Merge pull request #361 from rxin/clean
  [Minor style cleanup. Mostly on indenting & line width changes.]

  73c724e Thu Jan 9 00:32:19 2014 -0800
  Merge pull request #368 from pwendell/sbt-fix
  [Don't delegate to users `sbt`.]

  dceedb4 Wed Jan 8 23:19:28 2014 -0800
  Merge pull request #364 from pwendell/fix
  [Fixing config option ""retained_stages"" => ""retainedStages"".]

  04d83fc Wed Jan 8 11:55:37 2014 -0800
  Merge pull request #360 from witgo/master
  [fix make-distribution.sh show version: command not found]

  56ebfea Wed Jan 8 11:50:06 2014 -0800
  Merge pull request #357 from hsaputra/set_boolean_paramname
  [Set boolean param name for call to SparkHadoopMapReduceUtil.newTaskAttemptID]

  bdeaeaf Wed Jan 8 11:48:39 2014 -0800
  Merge pull request #358 from pwendell/add-cdh
  [Add CDH Repository to Maven Build]

  5cae05f Wed Jan 8 11:47:28 2014 -0800
  Merge pull request #356 from hsaputra/remove_deprecated_cleanup_method
  [Remove calls to deprecated mapred's OutputCommitter.cleanupJob]

  6eef78d Wed Jan 8 08:49:20 2014 -0600
  Merge pull request #345 from colorant/yarn
  [support distributing extra files to worker for yarn client mode]

  bb6a39a Tue Jan 7 22:32:18 2014 -0800
  Merge pull request #322 from falaki/MLLibDocumentationImprovement
  [SPARK-1009 Updated MLlib docs to show how to use it in Python]

  cb1b927 Tue Jan 7 22:26:28 2014 -0800
  Merge pull request #355 from ScrapCodes/patch-1
  [Update README.md]

  c0f0155 Tue Jan 7 22:21:52 2014 -0800
  Merge pull request #313 from tdas/project-refactor
  [Refactored the streaming project to separate external libraries like Twitter, Kafka, Flume, etc.]

  f5f12dc Tue Jan 7 21:56:35 2014 -0800
  Merge pull request #336 from liancheng/akka-remote-lookup
  [Get rid of `Either[ActorRef, ActorSelection]']

  11891e6 Wed Jan 8 00:32:18 2014 -0500
  Merge pull request #327 from lucarosellini/master
  [Added ‚Äò-i‚Äô command line option to Spark REPL]

  7d0aac9 Wed Jan 8 00:30:45 2014 -0500
  Merge pull request #354 from hsaputra/addasfheadertosbt
  [Add ASF header to the new sbt script.]

  d75dc42 Wed Jan 8 00:30:03 2014 -0500
  Merge pull request #350 from mateiz/standalone-limit
  [Add way to limit default # of cores used by apps in standalone mode]

  61674bc Tue Jan 7 18:32:13 2014 -0800
  Merge pull request #352 from markhamstra/oldArch
  [Don't leave os.arch unset after BlockManagerSuite]

  b2e690f Tue Jan 7 16:57:08 2014 -0800
  Merge pull request #328 from falaki/MatrixFactorizationModel-fix
  [SPARK-1012: DAGScheduler Exception Fix]

  6ccf8ce Tue Jan 7 15:49:14 2014 -0800
  Merge pull request #351 from pwendell/maven-fix
  [Add log4j exclusion rule to maven.]

  7d5fa17 Tue Jan 7 11:31:34 2014 -0800
  Merge pull request #337 from yinxusen/mllib-16-bugfix
  [Mllib 16 bugfix]

  71fc113 Tue Jan 7 11:30:35 2014 -0800
  Merge pull request #349 from CodingCat/support-worker_dir
  [add the comments about SPARK_WORKER_DIR]

  15d9534 Tue Jan 7 08:10:02 2014 -0800
  Merge pull request #318 from srowen/master
  [Suggested small changes to Java code for slightly more standard style, encapsulation and in some cases performance]

  468af0f Tue Jan 7 08:09:01 2014 -0800
  Merge pull request #348 from prabeesh/master
  [spark -> org.apache.spark]

  c3cf047 Tue Jan 7 00:54:25 2014 -0800
  Merge pull request #339 from ScrapCodes/conf-improvements
  [Conf improvements]

  a862caf Tue Jan 7 00:18:20 2014 -0800
  Merge pull request #331 from holdenk/master
  [Add a script to download sbt if not present on the system]

  b97ef21 Mon Jan 6 20:12:57 2014 -0800
  Merge pull request #346 from sproblvem/patch-1
  [Update stop-slaves.sh]

  7210257 Mon Jan 6 18:25:44 2014 -0800
  Merge pull request #128 from adamnovak/master
  [Fix failing ""sbt/sbt publish-local"" by adding a no-argument PrimitiveKeyOpenHashMap constructor]

  e4d6057 Mon Jan 6 14:56:54 2014 -0800
  Merge pull request #343 from pwendell/build-fix
  [Fix test breaking downstream builds]

  93bf962 Mon Jan 6 11:42:41 2014 -0800
  Merge pull request #340 from ScrapCodes/sbt-fixes
  [Made java options to be applied during tests so that they become self explanatory.]

  60edeb3 Mon Jan 6 11:40:32 2014 -0800
  Merge pull request #338 from ScrapCodes/ning-upgrade
  [SPARK-1005 Ning upgrade]

  c708e81 Mon Jan 6 11:35:48 2014 -0800
  Merge pull request #341 from ash211/patch-5
  [Clarify spark.cores.max in docs]

  33fcb91 Mon Jan 6 11:19:23 2014 -0800
  Merge pull request #342 from tgravescs/fix_maven_protobuf
  [Change protobuf version for yarn alpha back to 2.4.1]

  357083c Mon Jan 6 10:29:04 2014 -0800
  Merge pull request #330 from tgravescs/fix_addjars_null_handling
  [Fix handling of empty SPARK_EXAMPLES_JAR]

  a2e7e04 Sun Jan 5 22:37:36 2014 -0800
  Merge pull request #333 from pwendell/logging-silence
  [Quiet ERROR-level Akka Logs]

  5b0986a Sun Jan 5 19:25:09 2014 -0800
  Merge pull request #334 from pwendell/examples-fix
  [Removing SPARK_EXAMPLES_JAR in the code]

  f4b924f Sun Jan 5 17:11:47 2014 -0800
  Merge pull request #335 from rxin/ser
  [Fall back to zero-arg constructor for Serializer initialization if there is no constructor that accepts SparkConf.]

  d43ad3e Sat Jan 4 16:29:30 2014 -0800
  Merge pull request #292 from soulmachine/naive-bayes
  [standard Naive Bayes classifier]

  86404da Sat Jan 4 14:55:54 2014 -0800
  Merge pull request #127 from jegonzal/MapByPartition
  [Adding mapEdges and mapTriplets by Partition]

  e68cdb1 Sat Jan 4 13:46:02 2014 -0800
  Merge pull request #124 from jianpingjwang/master
  [refactor and bug fix]

  280ddf6 Sat Jan 4 12:54:41 2014 -0800
  Merge pull request #121 from ankurdave/more-simplify
  [Simplify GraphImpl internals further]

  10fe23b Fri Jan 3 23:50:14 2014 -0800
  Merge pull request #329 from pwendell/remove-binaries
  [SPARK-1002: Remove Binaries from Spark Source]

  c4d6145 Fri Jan 3 16:30:53 2014 -0800
  Merge pull request #325 from witgo/master
  [Modify spark on yarn to create SparkConf process]

  4ae101f Fri Jan 3 11:24:35 2014 -0800
  Merge pull request #317 from ScrapCodes/spark-915-segregate-scripts
  [Spark-915 segregate scripts]

  87248bd Fri Jan 3 00:45:31 2014 -0800
  Merge pull request #1 from apache/master
  [Merge latest Spark changes]

  30b9db0 Thu Jan 2 23:15:55 2014 -0800
  Merge pull request #285 from colorant/yarn-refactor
  [Yarn refactor]

  498a5f0 Thu Jan 2 19:06:40 2014 -0800
  Merge pull request #323 from tgravescs/sparkconf_yarn_fix
  [fix spark on yarn after the sparkConf changes]

  0475ca8 Thu Jan 2 15:17:08 2014 -0800
  Merge pull request #320 from kayousterhout/erroneous_failed_msg
  [Remove erroneous FAILED state for killed tasks.]

  588a169 Thu Jan 2 13:20:54 2014 -0800
  Merge pull request #297 from tdas/window-improvement
  [Improvements to DStream window ops and refactoring of Spark's CheckpointSuite]

  5e67cdc Thu Jan 2 12:56:28 2014 -0800
  Merge pull request #319 from kayousterhout/remove_error_method
  [Removed redundant TaskSetManager.error() function.]

  ca67909 Thu Jan 2 15:54:54 2014 -0500
  Merge pull request #311 from tmyklebu/master
  [SPARK-991: Report information gleaned from a Python stacktrace in the UI]

  3713f81 Wed Jan 1 21:29:12 2014 -0800
  Merge pull request #309 from mateiz/conf2
  [SPARK-544. Migrate configuration to a SparkConf class]

  c1d928a Wed Jan 1 17:03:48 2014 -0800
  Merge pull request #312 from pwendell/log4j-fix-2
  [SPARK-1008: Logging improvments]

  dc9cb83 Wed Jan 1 13:28:34 2014 -0800
  Merge pull request #126 from jegonzal/FixingPersist
  [Fixing Persist Behavior]

  9a0ff72 Tue Dec 31 21:50:24 2013 -0800
  Merge pull request #314 from witgo/master
  [restore core/pom.xml file modification]

  8b8e70e Tue Dec 31 17:48:24 2013 -0800
  Merge pull request #73 from falaki/ApproximateDistinctCount
  [Approximate distinct count]

  63b411d Tue Dec 31 14:31:28 2013 -0800
  Merge pull request #238 from ngbinh/upgradeNetty
  [upgrade Netty from 4.0.0.Beta2 to 4.0.13.Final]

  32d6ae9 Tue Dec 31 13:51:07 2013 -0800
  Merge pull request #120 from ankurdave/subgraph-reuses-view
  [Reuse VTableReplicated in GraphImpl.subgraph]

  55b7e2f Tue Dec 31 10:12:51 2013 -0800
  Merge pull request #289 from tdas/filestream-fix
  [Bug fixes for file input stream and checkpointing]

  2b71ab9 Mon Dec 30 11:01:30 2013 -0800
  Merge pull request from aarondav: Utilize DiskBlockManager pathway for temp file writing
  [This gives us a couple advantages:]

  50e3b8e Mon Dec 30 07:44:26 2013 -0800
  Merge pull request #308 from kayousterhout/stage_naming
  [Changed naming of StageCompleted event to be consistent]

  72a17b6 Sat Dec 28 21:25:40 2013 -1000
  Revert ""Merge pull request #310 from jyunfan/master""
  [This reverts commit 79b20e4dbe3dcd8559ec8316784d3334bb55868b, reversing]

  79b20e4 Sat Dec 28 21:13:36 2013 -1000
  Merge pull request #310 from jyunfan/master
  [Fix typo in the Accumulators section]

  7375047 Sat Dec 28 13:25:06 2013 -0800
  Merge pull request #304 from kayousterhout/remove_unused
  [Removed unused failed and causeOfFailure variables (in TaskSetManager)]

  ad3dfd1 Fri Dec 27 22:10:14 2013 -0500
  Merge pull request #307 from kayousterhout/other_failure
  [Removed unused OtherFailure TaskEndReason.]

  b579b83 Fri Dec 27 22:09:04 2013 -0500
  Merge pull request #306 from kayousterhout/remove_pending
  [Remove unused hasPendingTasks methods]

  19672dc Fri Dec 27 13:37:10 2013 -0800
  Merge pull request #305 from kayousterhout/line_spacing
  [Fixed >100char lines in DAGScheduler.scala]

  7be1e57 Thu Dec 26 23:41:40 2013 -1000
  Merge pull request #298 from aarondav/minor
  [Minor: Decrease margin of left side of Log page]

  7d811ba Thu Dec 26 23:39:58 2013 -1000
  Merge pull request #302 from pwendell/SPARK-1007
  [SPARK-1007: spark-class2.cmd should change SCALA_VERSION to be 2.10]

  5e69fc5 Thu Dec 26 19:10:39 2013 -0500
  Merge pull request #295 from markhamstra/JobProgressListenerNPE
  [Avoid a lump of coal (NPE) in JobProgressListener's stocking.]

  da20270 Thu Dec 26 12:11:52 2013 -0800
  Merge pull request #1 from aarondav/driver
  [Refactor DriverClient to be more Actor-based]

  e240bad Thu Dec 26 12:30:48 2013 -0500
  Merge pull request #296 from witgo/master
  [Renamed ClusterScheduler to TaskSchedulerImpl for yarn and new-yarn package]

  c344ed0 Thu Dec 26 01:31:06 2013 -0500
  Merge pull request #283 from tmyklebu/master
  [Python bindings for mllib]

  56094bc Wed Dec 25 13:14:33 2013 -0500
  Merge pull request #290 from ash211/patch-3
  [Typo: avaiable -> available]

  4842a07 Wed Dec 25 01:52:15 2013 -0800
  Merge pull request #287 from azuryyu/master
  [Fixed job name in the java streaming example.]

  85a344b Tue Dec 24 16:35:06 2013 -0800
  Merge pull request #127 from kayousterhout/consolidate_schedulers
  [Deduplicate Local and Cluster schedulers.]

  c2dd6bc Tue Dec 24 14:36:47 2013 -0800
  Merge pull request #279 from aarondav/shuffle-cleanup0
  [Clean up shuffle files once their metadata is gone]

  3bf7c70 Tue Dec 24 16:37:13 2013 -0500
  Merge pull request #275 from ueshin/wip/changeclasspathorder
  [Change the order of CLASSPATH.]

  d63856c Mon Dec 23 22:07:26 2013 -0800
  Merge pull request #286 from rxin/build
  [Show full stack trace and time taken in unit tests.]

  23a9ae6 Tue Dec 24 00:08:48 2013 -0500
  Merge pull request #277 from tdas/scheduler-update
  [Refactored the streaming scheduler and added StreamingListener interface]

  11107c9 Mon Dec 23 10:38:20 2013 -0800
  Merge pull request #244 from leftnoteasy/master
  [Added SPARK-968 implementation for review]

  44e4205 Sun Dec 22 11:44:18 2013 -0800
  Merge pull request #116 from jianpingjwang/master
  [remove unused variables and fix a bug]

  4797c22 Fri Dec 20 13:30:39 2013 -0800
  Merge pull request #118 from ankurdave/VertexPartitionSuite
  [Test VertexPartition and fix bugs]

  0bc57c5 Fri Dec 20 11:56:54 2013 -0800
  Merge pull request #280 from aarondav/minor
  [Minor cleanup for standalone scheduler]

  ac70b8f Fri Dec 20 10:56:10 2013 -0800
  Merge pull request #117 from ankurdave/more-tests
  [More tests]

  45310d4 Thu Dec 19 22:08:20 2013 -0800
  Merge pull request #115 from ankurdave/test-reorg
  [Reorganize unit tests; improve GraphSuite test coverage]

  9228ec8 Thu Dec 19 21:37:15 2013 -0800
  Merge pull request #1 from aarondav/127
  [Merge master into 127]

  eca68d4 Thu Dec 19 18:12:22 2013 -0800
  Merge pull request #272 from tmyklebu/master
  [Track and report task result serialisation time.]

  7990c56 Thu Dec 19 13:35:09 2013 -0800
  Merge pull request #276 from shivaram/collectPartition
  [Add collectPartition to JavaRDD interface.]

  440e531 Thu Dec 19 10:38:56 2013 -0800
  Merge pull request #278 from MLnick/java-python-tostring
  [Add toString to Java RDD, and __repr__ to Python RDD]

  d8d3f3e Thu Dec 19 00:06:43 2013 -0800
  Merge pull request #183 from aarondav/spark-959
  [[SPARK-959] Explicitly depend on org.eclipse.jetty.orbit jar]

  bfba532 Wed Dec 18 22:22:21 2013 -0800
  Merge pull request #247 from aarondav/minor
  [Increase spark.akka.askTimeout default to 30 seconds]

  da301b5 Wed Dec 18 20:03:29 2013 -0800
  Merge pull request #112 from amatsukawa/scc
  [Strongly connected component algorithm]

  c64a53a Wed Dec 18 16:56:26 2013 -0800
  Merge pull request #267 from JoshRosen/cygwin
  [Fix Cygwin support in several scripts.]

  a645ef6 Wed Dec 18 16:07:52 2013 -0800
  Merge pull request #48 from amatsukawa/add_project_to_graph
  [Add mask operation on graph and filter graph primitive]

  d7ebff0 Wed Dec 18 15:38:48 2013 -0800
  Merge pull request #1 from ankurdave/add_project_to_graph
  [Merge current master and reimplement Graph.mask "
"
Patrick Wendell <pwendell@gmail.com>,Tue"," 21 Jan 2014 14:45:28 -0800""",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc4),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I just explicitly created a new thread, so please defer to that
thread. This still pertains to rc4. Thanks


"
Patrick Wendell <pwendell@gmail.com>,"Tue, 21 Jan 2014 14:45:42 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc4) [new thread],"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I'll kick it off with a +1.


"
Pat McDonough <pat.mcdonough@databricks.com>,"Tue, 21 Jan 2014 16:16:01 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc4) [new thread],dev@spark.incubator.apache.org,1
Matei Zaharia <matei.zaharia@gmail.com>,"Tue, 21 Jan 2014 16:47:21 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc4) [new thread],dev@spark.incubator.apache.org,"+1 looked at changes since last RC and tested on mac.

Matei


https://git-wip-us.apache.org/repos/asf?p=incubator-spark.git;a=commit;h=0771df675363c69622404cb514bd751bc90526af
at:
https://repository.apache.org/content/repositories/orgapachespark-1005/
0."
Henry Saputra <henry.saputra@gmail.com>,"Wed, 22 Jan 2014 01:44:21 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc4) [new thread],"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Notice and license files ok
No exec files ok
Sig and hashes looks good
Compiled and tests

+1


"
Henry Saputra <henry.saputra@gmail.com>,"Wed, 22 Jan 2014 01:46:08 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc4) [new thread],"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Would love to hear from Mridul to verify the fixes for problems he saw are
in.


"
Chen Jin <karen.cj@gmail.com>,"Wed, 22 Jan 2014 03:31:59 -0800",Re: The performance of group operation on SSD,dev@spark.incubator.apache.org,"EC2 doesn't mount SSDs automatically, so I actually reformat those as
ex4, mount them and point hdfs to them. I haven't tweaked any FS
options and not sure what cause the problem. I will follow
Christopher's suggestion and see whether I can find any cause.

-chen

rote:
t3, which is the default on ephemeral disks on EC2, scales very poorly to multicore workloads. We recommend reformatting those as XFS (which is very fast to format) or ext4 (which unfortunately takes a few hours to finalize). Maybe there are also other FS options that affect SSDs.
k
à6:04ÔºåChen Jin <karen.cj@gmail.com> ÂÜôÈÅìÔºö

"
Guillaume Pitel <guillaume.pitel@exensa.com>,"Wed, 22 Jan 2014 12:51:59 +0100",Re: The performance of group operation on SSD,dev@spark.incubator.apache.org,"Hi,

I think intermediate results are stored in spark.local.dir (/tmp by default), 
not in hdfs.

So if you see big latency in a shuffle-heavy operation, it's probably due to the 
device where /tmp reside. You can run iotop on your slaves to check where the 
operations are occuring. Check your wait% in top, to see if you have I/O problems

You can activate the consolidateFiles option if you use ext4, it helps when 
dealing with big intermediate results

Guillaume


-- 
eXenSa

	
*Guillaume PITEL, Pr√©sident*
+33(0)6 25 48 86 80

eXenSa S.A.S. <http://www.exensa.com/>
41, rue P√©rier - 92120 Montrouge - FRANCE
Tel +33(0)1 84 16 36 77 / Fax +33(0)9 72 28 37 05

"
Tom Graves <tgraves_cs@yahoo.com>,"Wed, 22 Jan 2014 08:18:19 -0800 (PST)",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc4) [new thread],"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","It looks like the latest round of changes took out spark.conf. †Are thereuld love to hear from Mridul to verify the fixes for problems he saw are
e Spark
> (incubating) version 0.9.0.
>
> A draft of the release notes along with the changes file is attached
> to this e-mail.
>
> The tag to be voted on is v0.9.0-incubating (commit 0771df67):
>
> https://git-wip-us.apache.org/repos/asf?p=incubator-spark.git;a=commit;h=0771df675363c69622404cb514bd751bc90526af
>
> The release files, including signatures, digests, etc can be found at:
> http://people.apache.org/~pwendell/spark-0.9.0-incubating-rc4
>
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/pwendell.asc
>
> The staging repository for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1005/
>
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~pwendell/spark-0.9.0-incubating-rc4-docs/
>
> Please vote on releasing this package as Apache Spark 0.9.0-incubating!
>
> The vote is open until Friday, January 24, at 11:15 UTC and passes if
> a majority of at least 3 +1 PPMC votes are cast.
>
> [ ] +1 Release this package as Apache Spark 0.9.0-incubating
> [ ] -1 Do not release this package because ...
>
> To learn more about Apache Spark, please see
> http://spark.incubator.apache.org/
>"
peizhongshuai <799203320@qq.com>,"Wed, 22 Jan 2014 08:45:16 -0800 (PST)",a question about replicate() in BlockManager.scala,dev@spark.incubator.apache.org,"  var cachedPeers: Seq[BlockManagerId] = null
  private def replicate(blockId: String, data: ByteBuffer, level:
StorageLevel) {
    val tLevel = StorageLevel(level.useDisk, level.useMemory,
level.deserialized, 1)
    if (cachedPeers == null) {
      cachedPeers = master.getPeers(blockManagerId, level.replication - 1)
    }
    for (peer: BlockManagerId <- cachedPeers) {
      val start = System.nanoTime
      data.rewind()
      logDebug(""Try to replicate BlockId "" + blockId + "" once; The size of
the data is ""
        + data.limit() + "" Bytes. To node: "" + peer)
      if (!BlockManagerWorker.syncPutBlock(PutBlock(blockId, data, tLevel),
        new ConnectionManagerId(peer.host, peer.port))) {
        logError(""Failed to call syncPutBlock to "" + peer)
      }
      logDebug(""Replicated BlockId "" + blockId + "" once used "" +
        (System.nanoTime - start) / 1e6 + "" s; The size of the data is "" +
        data.limit() + "" bytes."")
    }
	
	when the method called firstly, ""cachedPeers == null"" is true.
	Then ""master.getPeers(blockManagerId, level.replication - 1)"" execute and
we get peers that will be used to replicate blocks.
	But when the method called again, ""cachedPeers == null"" is false.
    ""master.getPeers(blockManagerId, level.replication - 1)"" will not
execute, and the peers that will be used to replicate blocks is same to the
peers we get firstly.
	Is it right?



--

"
Tom Graves <tgraves_cs@yahoo.com>,"Wed, 22 Jan 2014 09:07:55 -0800 (PST)",spork?,Dev <dev@spark.incubator.apache.org>,"Hey everyone,

At one point I heard of pig on Spark (Spork). †The only thing I could find on it was†https://github.com/dvryaboy/pig†and it doesn't look like there has been any work on it in quite a while. †

Does anyone know the current status of that or has tried it?

Thanks,
Tom"
Patrick Wendell <pwendell@gmail.com>,"Wed, 22 Jan 2014 09:14:58 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc4) [new thread],"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>, Tom Graves <tgraves_cs@yahoo.com>","Hey Tom,

Matei had to remove this because it turns out that there was a fairly
serious bug in the Typesafe config library we use for parsing conf
files [1]. There wasn't an immediate solution to this so he just
removed the capability for this release and we can revisit it in the
next release.

http://apache-spark-developers-list.1001551.n3.nabble.com/Config-properties-broken-in-master-td208.html

- Patrick


"
Patrick Wendell <pwendell@gmail.com>,"Wed, 22 Jan 2014 09:36:41 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc4) [new thread],"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>, Tom Graves <tgraves_cs@yahoo.com>","Btw - to be clear this was an incompatibility between Spark's config
names and constraints on names imposed by typesafe. So didn't mean to
imply there was something broken in their config library.


"
Khanderao kand <khanderao.kand@gmail.com>,"Wed, 22 Jan 2014 10:28:03 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc4) [new thread],"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","+1

However please fix a minor issue in documentation.

Please correct spark version in the ""A Note About Hadoop Versions""  from
Spark (0.8.1) to Spark (0.9.0)

http://people.apache.org/~pwendell/spark-0.9.0-incubating-rc4-docs/

A minor suggestion:
Since"
Matei Zaharia <matei.zaharia@gmail.com>,"Wed, 22 Jan 2014 10:45:41 -0800",Re: spork?,"dev@spark.incubator.apache.org,
 Tom Graves <tgraves_cs@yahoo.com>","Yup, that was it, though I believe people at Twitter picked it up again recently. Iíd suggest asking Dmitriy if you know him. Iíve seen interest in this from several other groups, and if thereís enough of it, maybe we can start another open source repo to track it. The work in that repo you pointed to was done over one week, and already had most of Pigís operators working. (I helped out with this prototype over Twitterís hack week.) That work also calls the Scala API directly, because it was done before we had a Java API; it should be easier with the Java one.

Matei


find on it was https://github.com/dvryaboy/pig and it doesn't look like there has been any work on it in quite a while.  


"
Paul Brown <prb@mult.ifario.us>,"Wed, 22 Jan 2014 11:12:59 -0800",Re: spork?,"dev@spark.incubator.apache.org, Tom Graves <tgraves_cs@yahoo.com>","Just an IMHO.  We've moved a number of Pig jobs to Spark jobs, and the
benefits of a genuine conversion (type safety, terseness, debugging, lack
of gratuitous shells and shims around UDFs, ...) are sufficient that a
rewrite is probably preferable from an adapter unless you have an enormous
Pig investment that you want to continue investing in.

-- Paul


‚Äî
prb@mult.ifario.us | Multifarious, Inc. | http://mult.ifario.us/



d
"
Matei Zaharia <matei.zaharia@gmail.com>,"Wed, 22 Jan 2014 11:43:28 -0800",Re: spork?,dev@spark.incubator.apache.org,"Yeah, this is true for new jobs, but some organizations have large libraries of Pig scripts and UDFs. I think what would be ideal is a Pig-to-Scala or Pig-to-Python binding layer similar to Sharkís sql2rdd. That way you could run your existing Pig scripts, but also call them from Scala or Python to transition over to these programming languages.

Matei


lack
enormous
find
there


"
Sourav Chandra <sourav.chandra@livestream.com>,"Thu, 23 Jan 2014 14:31:34 +0530",DStream foreachRdd not working in standalone cluster mode,"user@spark.incubator.apache.org, dev@spark.incubator.apache.org","Hi,

I am using spark streaming along with kafka dstream. and running the
application against standalone cluster

Spark version => https://github.com/apache/incubator-spark/tree/branch-0.9

It seems  after transformation, when i o foreachRDD, its not working.

code snippet is below :
---------------------------------------------------------------
val ssc = new StreamingContext(...)
val stream = KafkaUtils.createStream(...)
val processedStream = stream.flatMap(...)
val newStream = processedStream.map(x => (x, 1L)).reduceByKeyAndWindow(_ +
_, _ - _, Seconds(1), Seconds(1), 2)
newStream.foreachRDD(rdd => {
      rdd.foreach({
        case (value, count) => {
          println(""##########################################"")
          println(""value --> "" + value + "" with count --> "" + count)
          println(""##########################################"")
        }
      })
    })

---------------------------------------------------------------

If I run the application locally (local instead of spark://), it is working

Can you suggest what is going on here?

-- 

Sourav Chandra

Senior Software Engineer

∑ ∑ ∑ ∑ ∑ ∑ ∑ ∑ ∑ ∑ ∑ ∑ ∑ ∑ ∑ ∑ ∑ ∑ ∑ ∑ ∑ ∑ ∑ ∑ ∑ ∑ ∑ ∑ ∑ ∑ ∑ ∑ ∑

sourav.chandra@livestream.com

o: +91 80 4121 8723

m: +91 988 699 3746

skype: sourav.chandra

Livestream

""Ajmera Summit"", First Floor, #3/D, 68 Ward, 3rd Cross, 7th C Main, 3rd
Block, Koramangala Industrial Area,

Bangalore 560034

www.livestream.com
"
Tom Graves <tgraves_cs@yahoo.com>,"Thu, 23 Jan 2014 07:44:22 -0800 (PST)",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc4) [new thread],"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","+1. †

verified sigs/hashes, NOTICE, LICENSE
Built from source on redhat linux using maven.
Ran some basic tests on secure Hadoop YARN 0.23 and 2.2 clusters in both standalone and client modes.

Tom



On Wednesday, January 22, 2014 12:28 PM, Khanderao ka"
Henry Saputra <henry.saputra@gmail.com>,"Thu, 23 Jan 2014 08:46:00 -0800",Re: Config properties broken in master,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi Mridul,

Would you mind help to vet the latest 0.9 RC 4 candidate to make sure
the fixes are included since you raised the issues in prev RC?

Thanks,

Henry


"
Mridul Muralidharan <mridul@gmail.com>,"Thu, 23 Jan 2014 23:36:08 +0530",Re: Config properties broken in master,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Yeah, it is in my TODO to test latest RC - unfortunately, the test
setup is slightly busy running other things.

Regards,
Mridul


"
Henry Saputra <henry.saputra@gmail.com>,"Thu, 23 Jan 2014 10:40:53 -0800",Re: Config properties broken in master,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Cool thanks, appreciate it =)

- Henry


"
Linky <linkasim@gmail.com>,"Thu, 23 Jan 2014 15:30:31 -0500",debugger,dev@spark.incubator.apache.org,"Hi,

Is there a debugger available for spark jobs. Something like what is
reported here : https://github.com/mesos/spark/wiki/Spark-Debugger

I need to understand how the RDDs have evolved and the delta changes in
them. Is there an existing API that helps me do this.

Thanks.
"
Josh Rosen <rosenville@gmail.com>,"Thu, 23 Jan 2014 12:35:03 -0800",Re: debugger,"""Spark Dev (Apache Incubator)"" <dev@spark.incubator.apache.org>","There's a pair of open pull requests that implement replay debugging for
newer versions of Spark:

https://github.com/apache/incubator-spark/pull/224
https://github.com/apache/incubator-spark/pull/284



"
Linky <linkasim@gmail.com>,"Thu, 23 Jan 2014 15:53:51 -0500",Re: debugger,dev@spark.incubator.apache.org,"Thanks.



"
Matei Zaharia <matei.zaharia@gmail.com>,"Thu, 23 Jan 2014 14:45:23 -0800",[DISCUSS] Graduating as a TLP,dev@spark.incubator.apache.org,"Hi folks,

Weíve been working on the transition to Apache for a while, and our last shepherdís report says the following:

--------------------
Spark

Alan Cabrera (acabrera):

  Seems like a nice active project.  IMO, there's no need to wait import
  to JIRA to graduate. Seems like they can graduate now.
--------------------

What do you think about graduating to a top-level project? As far as I can tell, weíve completed all the requirements for graduating:

- Made 2 releases (working on a third now)
- Added new committers and PPMC members (4 of them)
- Did IP clearance
- Moved infrastructure over to Apache, except for the JIRA above, which INFRA is working on and which shouldnít block us.

If everything is okay, Iíll call a VOTE on graduating in 48 hours. The one final thing missing is that weíll need to nominate an initial VP for the project.

Matei
"
Christopher Nguyen <ctn@adatao.com>,"Thu, 23 Jan 2014 14:58:48 -0800",Re: [DISCUSS] Graduating as a TLP,dev@spark.incubator.apache.org,"Cool, Matei!

Sent while mobile. Pls excuse typos etc.

t
n
one
e
"
yao <yaoshengzhe@gmail.com>,"Thu, 23 Jan 2014 14:59:26 -0800",Re: [DISCUSS] Graduating as a TLP,dev@spark.incubator.apache.org,"+1


te:

t
n
one
e
"
Konstantin Boudnik <cos@apache.org>,"Thu, 23 Jan 2014 15:17:58 -0800",Re: [DISCUSS] Graduating as a TLP,dev@spark.incubator.apache.org,"Great news!


"
Reynold Xin <rxin@databricks.com>,"Thu, 23 Jan 2014 15:18:51 -0800",Re: [DISCUSS] Graduating as a TLP,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","+1


te:

t
n
one
e
"
Jey Kottalam <jey@cs.berkeley.edu>,"Thu, 23 Jan 2014 15:21:14 -0800",Re: [DISCUSS] Graduating as a TLP,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","+1

The sooner we stabilize the URLs and identifiers, the better!

st shepherdís report says the following:
an tell, weíve completed all the requirements for graduating:
INFRA is working on and which shouldnít block us.
 one final thing missing is that we"
Tathagata Das <tathagata.das1565@gmail.com>,"Thu, 23 Jan 2014 15:21:29 -0800",Re: [DISCUSS] Graduating as a TLP,dev@spark.incubator.apache.org,"+1



ast
t
e
the
"
Sean McNamara <Sean.McNamara@Webtrends.com>,"Thu, 23 Jan 2014 23:23:17 +0000",Re: [DISCUSS] Graduating as a TLP,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","+1


he


"
Patrick Wendell <pwendell@gmail.com>,"Thu, 23 Jan 2014 15:34:40 -0800",Re: [DISCUSS] Graduating as a TLP,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","+1

I
The
r

"
Mark Hamstra <mark@clearstorydata.com>,"Thu, 23 Jan 2014 15:43:30 -0800",Re: [DISCUSS] Graduating as a TLP,dev@spark.incubator.apache.org,"+1

What are the requirements and responsibilities of the VP?  (Or a link to
where they are laid out.)


:

ur
s
.
for
"
Chris Mattmann <mattmann@apache.org>,"Thu, 23 Jan 2014 16:11:49 -0800",Re: [DISCUSS] Graduating as a TLP,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","+1 from me.

I'll throw Matei's name into the hat for VP. He's done a great job
and has stood out to me with his report filing and tenacity and
would make an excellent chair.

Being a chair entails:

1. being the eyes and ears of the board on the project."
Reynold Xin <rxin@databricks.com>,"Thu, 23 Jan 2014 16:13:53 -0800",Re: [DISCUSS] Graduating as a TLP,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","+1 supporting Matei as the VP.


:

g
st
or
"
Tom Graves <tgraves_cs@yahoo.com>,"Thu, 23 Jan 2014 16:16:05 -0800 (PST)",Re: [DISCUSS] Graduating as a TLP,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","ote:

> +1 from me.
>
> I'll throw Matei's name into the hat for VP. He's done a great job
> and has stood out to me with his report filing and tenacity and
> would make an excellent chair.
>
> Being a chair entails:
>
> 1. being the eyes and ears of the board on the project.
> 2. filing a monthly (first 3 months, then quarterly) board report
> similar to the incubator report.
>
> Not too bad.
>
> +1 for graduation from me binding when the VOTE comes. We need our
> mentors and IPMC members to chime in and we should be in time for
> February 2014 board meetin
> >† Seems like a nice active project.† IMO, there's no need to wait import
> >† to JIRA to graduate. Seems like they can graduate now.
> >--------------------
> >
> >What do you think about graduating to a top-level project? As far as I
> >can tell, weπve completed all the requirements for graduating:
> >
> >- Made 2 releases (working on a third now)
> >- Added new committers and PPMC members (4 of them)
> >- Did IP clearance
> >- Moved infrastructure over to Apache, except for the JIRA above, which
> >INFRA is working on and which shouldnπt block us.
> >
> >If everything is okay, Iπll call a VOTE on graduating in 48 hours. The
> >one final thing missing is that weπll need to nominate an initial VP for
> >the project.
> >
> >Matei
>
>
>"
Henry Saputra <henry.saputra@gmail.com>,"Thu, 23 Jan 2014 16:21:38 -0800",Re: [DISCUSS] Graduating as a TLP,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","+1 for graduation and propose Matei as VP candidate.


- Henry

ote:
ur last shepherd‚Äôs report says the following:
n tell, we‚Äôve completed all the requirements for graduating:
NFRA is working on and which shouldn‚Äôt block us.
. The one final thing mi"
Khanderao kand <khanderao.kand@gmail.com>,"Thu, 23 Jan 2014 16:21:58 -0800",Re: [DISCUSS] Graduating as a TLP,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I'll throw Matei's name into the hat for VP. He's done a great job
and has stood out to me with his report filing and tenacity and
would make an excellent chair.

+1


:

g
st
or
"
Tathagata Das <tathagata.das1565@gmail.com>,"Thu, 23 Jan 2014 16:22:31 -0800",Re: [DISCUSS] Graduating as a TLP,dev@spark.incubator.apache.org,"+1 for Matei as VP candidate.


te:

ast
t
e
r
"
Debasish Das <debasish.das83@gmail.com>,"Thu, 23 Jan 2014 16:23:48 -0800",Re: [DISCUSS] Graduating as a TLP,dev@spark.incubator.apache.org,"+1 for tlp and Matei as VP

last
rt
h
he
 for
"
Matt Massie <massie@berkeley.edu>,"Thu, 23 Jan 2014 16:23:41 -0800",Re: [DISCUSS] Graduating as a TLP,dev@spark.incubator.apache.org,"+1 on moving to a TLP with Matei as the VP!

--
Matt Massie
UC, Berkeley AMPLab
Twitter: @matt_massie <https://twitter.com/matt_massie>,
@amplab<https://twitter.com/amplab>
https://amplab.cs.berkeley.edu/


rote:

last
rt
h
he
 for
"
Andy Konwinski <andykonwinski@gmail.com>,"Thu, 23 Jan 2014 16:32:10 -0800",Re: [DISCUSS] Graduating as a TLP,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","+2 (1 for graduating + 1 for matei as VP)!


:

g
st
or
"
Mark Hamstra <mark@clearstorydata.com>,"Thu, 23 Jan 2014 16:40:50 -0800",Re: [DISCUSS] Graduating as a TLP,dev@spark.incubator.apache.org,"Is there any other choice that makes any kind of sense?  Matei for VP: +1.


ote:

last
rt
h
he
 for
ps
an
"
Taka Shinagawa <taka.epsilon@gmail.com>,"Thu, 23 Jan 2014 17:05:50 -0800",Re: [DISCUSS] Graduating as a TLP,dev@spark.incubator.apache.org,"+1 for graduation. +1 for Matei as VP (The father of Spark should be the VP)


e:

.
r
 I
 The
VP
d
"
Hossein <falaki@gmail.com>,"Thu, 23 Jan 2014 17:25:58 -0800",Re: [DISCUSS] Graduating as a TLP,dev@spark.incubator.apache.org,"+1 to both.

--Hossein


e:

.
 our
 I
rs. The
al VP
d
"
prabeesh k <prabsmails@gmail.com>,"Fri, 24 Jan 2014 09:52:43 +0530",Re: [DISCUSS] Graduating as a TLP,dev@spark.incubator.apache.org,"+1



m
g
our
s.
l VP
"
Josh Rosen <rosenville@gmail.com>,"Thu, 23 Jan 2014 20:26:26 -0800",Re: [DISCUSS] Graduating as a TLP,"""Spark Dev (Apache Incubator)"" <dev@spark.incubator.apache.org>","+1 to both!



:
g
b
r
d
t
r
,
urs.
ial
e
"
Mosharaf Chowdhury <mosharafkabir@gmail.com>,"Thu, 23 Jan 2014 20:33:07 -0800",Re: [DISCUSS] Graduating as a TLP,dev@spark.incubator.apache.org,"+1 & +1


m
rt
or
and
g:
t,
t
"
Mridul Muralidharan <mridul@gmail.com>,"Fri, 24 Jan 2014 10:12:44 +0530",Re: [DISCUSS] Graduating as a TLP,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Great news !

+1

Regards,
Mridul

ote:
t shepherdís report says the following:
n tell, weíve completed all the requirements for graduating:
NFRA is working on and which shouldnít block us.
one final thing missing is that weíll need to nominate an initial VP for the project.

"
"""Nick Pentreath"" <nick.pentreath@gmail.com>","Thu, 23 Jan 2014 20:46:53 -0800 (PST)",Re: [DISCUSS] Graduating as a TLP,dev@spark.incubator.apache.org,"+1 fantastic news¬†‚Äî
Sent from Mailbox for iPhone


our last shepherd‚Äôs report says the following:
import
 can tell, we‚Äôve completed all the requirements for graduating:
INFRA is working on and which shouldn‚Äôt block us.
hours. The one final thing "
"""Xia, Junluan"" <junluan.xia@intel.com>","Fri, 24 Jan 2014 04:48:56 +0000",RE: [DISCUSS] Graduating as a TLP,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Great new! Cool!

-----Original Message-----
From: Nick Pentreath [mailto:nick.pentreath@gmail.com] 
Sent: Friday, January 24, 2014 12:47 PM
To: dev@spark.incubator.apache.org
Subject: Re: [DISCUSS] Graduating as a TLP

+1 fantastic news¬†‚Äî
Sent from Mailbox for iPhone

On Fri, Jan 24, 2014 at 6:43 AM, Mridul Muralidharan <mridul@gmail.com>
wrote:

> Great news !
> +1
> Regards,
> Mridul
> On Fri, Jan 24, 2014 at 4:15 AM, Matei Zaharia <matei.zaharia@gmail.com> wrote:
>> Hi folks,
>>
>> We‚Äôve been working on the transition to Apache for a while, and our last shepherd‚Äôs report says the following:
>>
>> --------------------
>> Spark
>>
>> Alan Cabrera (acabrera):
>>
>>   Seems like a nice active project.  IMO, there's no need to wait import
>>   to JIRA to graduate. Seems like they can graduate now.
>> --------------------
>>
>> What do you think about graduating to a top-level project? As far as I can tell, we‚Äôve completed all the requirements for graduating:
>>
>> - Made 2 releases (working on a third now)
>> - Added new committers and PPMC members (4 of them)
>> - Did IP clearance
>> - Moved infrastructure over to Apache, except for the JIRA above, which INFRA is working on and which shouldn‚Äôt block us.
>>
>> If everything is okay, I‚Äôll call a VOTE on graduating in 48 hours. The one final thing missing is that we‚Äôll need to nominate an initial VP for the project.
>>
>> Matei
"
Haoyuan Li <haoyuan.li@gmail.com>,"Thu, 23 Jan 2014 22:25:18 -0800",Re: [DISCUSS] Graduating as a TLP,dev@spark.incubator.apache.org,"+1 to both.


m

r
t
nd
d
,
s
ing:
8
.
"
Patrick Wendell <pwendell@gmail.com>,"Thu, 23 Jan 2014 23:05:36 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc4) [new thread],"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Today there was a bug reported almost simultaneously by Tom Graves and
Matt Massie that was enough to warrant a new RC. I'll post RC5
momentarily and I'm also going to post a result thread for this one
(which apparently helps automated scripts that consume the list) to
say it was cancelled.


"
Patrick Wendell <pwendell@gmail.com>,"Thu, 23 Jan 2014 23:06:07 -0800",[RESULT] [VOTE] Release Apache Spark 0.9.0-incubating (rc4),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","The RC4 vote was cancelled in favor of RC5.


---------- Forwarded message ----------
From: Patrick Wendell <pwendell@gmail.com>
Date: Thu, Jan 23, 2014 at 11:05 PM
Subject: Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc4) [new thread]
To: ""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>


Today there was a bug reported almost simultaneously by Tom Graves and
Matt Massie that was enough to warrant a new RC. I'll post RC5
momentarily and I'm also going to post a result thread for this one
(which apparently helps automated scripts that consume the list) to
say it was cancelled.


"
Patrick Wendell <pwendell@gmail.com>,"Thu, 23 Jan 2014 23:33:04 -0800",[VOTE] Release Apache Spark 0.9.0-incubating (rc5),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Please vote on releasing the following candidate as Apache Spark
(incubating) version 0.9.0.

A draft of the release notes along with the changes file is attached
to this e-mail.

The tag to be voted on is v0.9.0-incubating (commit 95d28ff3):
https://git-wip-us.apache.org/repos/asf?p=incubator-spark.git;a=commit;h=95d28ff3d0d20d9c583e184f9e2c5ae842d8a4d9

The release files, including signatures, digests, etc can be found at:
http://people.apache.org/~pwendell/spark-0.9.0-incubating-rc5

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1006/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-0.9.0-incubating-rc5-docs/

Please vote on releasing this package as Apache Spark 0.9.0-incubating!

The vote is open until Monday, January 27, at 07:30 UTC and passes ifa
majority of at least 3 +1 PPMC votes are cast.

[ ] +1 Release this package as Apache Spark 0.9.0-incubating
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.incubator.apache.org/
Spark 0.9.0 is a major release that adds significant new features. It updates Spark to Scala 2.10, simplifies high availability, and updates numerous components of the project. This release includes a first version of GraphX, a powerful new framework for graph processing that comes with a library of standard algorithms. In addition, Spark Streaming is now out of alpha, and includes significant optimizations and simplified high availability deployment.

### Scala 2.10 Support

Spark now runs on Scala 2.10, letting users benefit from the language and library improvements in this version.

### Configuration System

The new [SparkConf] class is now the preferred way to configure advanced settings on your SparkContext, though the previous Java system property still works. SparkConf is especially useful in tests to make sure properties don‚Äôt stay set across tests.

### Spark Streaming Improvements

Spark Streaming is no longer alpha, and comes with simplified high availability and several optimizations.

* When running on a Spark standalone cluster with the [standalone cluster high availability mode], you can submit a Spark Streaming driver application to the cluster and have it automatically recovered if either the driver or the cluster master crashes.
* Windowed operators have been sped up by 30-50%.
* Spark Streaming‚Äôs input source plugins (e.g. for Twitter, Kafka and Flume) are now separate projects, making it easier to pull in only the dependencies you need.
* A new StreamingListener interface has been added for monitoring statistics about the streaming computation.
* A few aspects of the API have been improved:
* `DStream` and `PairDStream` classes have been moved from `org.apache.spark.streaming` to `org.apache.spark.streaming.dstream` to keep it consistent with `org.apache.spark.rdd.RDD`.
* `DStream.foreach` -> `DStream.foreachRDD` to make it explicit that it works for every RDD, not every element
* `StreamingContext.awaitTermination()` allows you wait for context shutdown and catch any exception that occurs in the streaming computation.
*`StreamingContext.stop()` now allows stopping of StreamingContext without stopping the underlying SparkContext.

### GraphX Alpha

GraphX is a new API for graph processing that uses recent advances in graph-parallel computation. It lets you build a graph within a Spark program using the standard Spark operators, then process it with new graph operators that are optimized for distributed computation. It includes basic transformations, a Pregel API for iterative computation, and a standard library of graph loaders and analytics algorithms. By offering these features within the Spark engine, GraphX can significantly speed up processing tasks compared to workflows that use different engines.

GraphX features in this release include:

* Building graphs from arbitrary Spark RDDs
* Basic operations to transform graphs or extract subgraphs
* An optimized Pregel API that takes advantage of graph partitioning and indexing
* Standard algorithms including PageRank, connected components, strongly connected components, SVD++, and triangle counting
* Interactive use from the Spark shell

GraphX is still marked as alpha in this first release, but we recommend for new users to use it instead of the more limited Bagel API.

### MLlib Improvements

* Spark‚Äôs machine learning library (MLlib) is now available in Python, where it operates on NumPy data (currently requires Python 2.7 and NumPy 1.7)
* A new algorithm has been added for Naive Bayes classification
* Alternating Least Squares can now be used to predict ratings for multiple items in parallel
* MLlib‚Äôs documentation was expanded to include more examples in Scala, Java and Python

### Python Changes

* Python users can now use MLlib (requires Python 2.7 and NumPy 1.7)
* PySpark now shows the call sites of running jobs in the Spark application UI (http://<driver>:4040), making it easy to see which part of your code is running
* IPython operation has been improved

### Packaging

* Spark‚Äôs scripts have been organized into ‚Äúbin‚Äù and ‚Äúsbin‚Äù directories to make it easier to separate admin scripts from user ones and install Spark on standard Linux paths.
* Log configuration has been improved so that Spark finds a default log4j.properties file if you don‚Äôt specify one.

### Core Engine

* Spark‚Äôs standalone mode now supports submitting a driver program to run on the cluster instead of on the external machine submitting it. You can access this functionality through the [org.apache.spark.deploy.Client] class.
* Large reduce operations now automatically spill data to disk if it does not fit in memory.
* Users of standalone mode can now limit how many cores an application will use by default if the application writer didn‚Äôt configure its size. Previously, such applications took all available cores on the cluster.
* `spark-shell` now supports the `-i` option to run a script on startup.
* New `histogram` and `countDistinctApprox` operators have been added for working with numerical data.
* Methods for reading Hadoop `Writable` objects, such as `SparkContext.sequenceFile` and `hadoopRDD`, now clone the records read by default instead of reusing the same Writable instance, making the resulting RDDs easier to cache; if you only want to use each object once, you can pass an extra flag to disable this cloning
* YARN mode now supports distributing extra files with the application, and several bugs have been fixed.

### Compatibility

This release is compatible with the previous APIs in stable components, but several language versions and script locations have changed.

* Scala programs now need to use Scala 2.10 instead of 2.9.
* Scripts such as `spark-shell` and `pyspark` have been moved into the `bin` folder, while administrative scripts to start and stop standalone clusters have been moved into `sbin`.
* Spark Streaming‚Äôs API has been changed to move external input sources into separate modules, `DStream` and `PairDStream` has been moved to package `org.apache.spark.streaming.dstream` and `DStream.foreach` has been renamed to `foreachRDD`. We expect the current API to be stable now that Spark Streaming is out of alpha.
* While the old method of configuring Spark through Java system properties still works, we recommend that users update to the new SparkConf, which is easier to inspect and use.

We expect all of the current APIs and script locations in Spark 0.9 to remain stable when we release Spark 1.0. We wanted to make these updates early to give users a chance to switch to the new API.


Spark Change Log

Release 0.9.0-incubating

  d0a105d Thu Jan 23 20:53:31 2014 -0800
  Merge pull request #505 from JoshRosen/SPARK-1026
  [Deprecate mapPartitionsWithSplit in PySpark (SPARK-1026)]

  e66d4c2 Thu Jan 23 19:47:16 2014 -0800
  Merge pull request #503 from pwendell/master
  [Fix bug on read-side of external sort when using Snappy.]

  e8d3f2b Thu Jan 23 19:20:22 2014 -0800
  Merge pull request #502 from pwendell/clone-1
  [Remove Hadoop object cloning and warn users making Hadoop RDD's.]

  7a62353 Thu Jan 23 19:09:25 2014 -0800
  Merge pull request #501 from JoshRosen/cartesian-rdd-fixes
  [Fix two bugs in PySpark cartesian(): SPARK-978 and SPARK-1034]

  51960b8 Wed Jan 22 19:37:50 2014 -0800
  Merge pull request #496 from pwendell/master
  [Fix bug in worker clean-up in UI]

  828f7b4 Wed Jan 22 15:45:18 2014 -0800
  Merge pull request #495 from srowen/GraphXCommonsMathDependency
  [Fix graphx Commons Math dependency]

  dc5857a Wed Jan 22 14:33:25 2014 -0800
  Merge pull request #492 from skicavs/master
  [fixed job name and usage information for the JavaSparkPi example]

  dd533c9 Wed Jan 22 14:15:58 2014 -0800
  Merge pull request #478 from sryza/sandy-spark-1033
  [SPARK-1033. Ask for cores in Yarn container requests]

  b6fd3cd Tue Jan 21 00:12:01 2014 -0800
  Merge pull request #480 from pwendell/0.9-fixes
  [Handful of 0.9 fixes]

  e5f8917 Mon Jan 20 23:35:07 2014 -0800
  Merge pull request #484 from tdas/run-example-fix
  [Made run-example respect SPARK_JAVA_OPTS and SPARK_MEM.]

  410ba06 Mon Jan 20 22:26:14 2014 -0800
  Merge pull request #482 from tdas/streaming-example-fix
  [Added StreamingContext.awaitTermination to streaming examples]

  f137947 Mon Jan 20 22:24:07 2014 -0800
  Merge pull request #483 from pwendell/gitignore
  [Restricting /lib to top level directory in .gitignore]

  94ae25d Sun Jan 19 11:33:51 2014 -0800
  Merge pull request #470 from tgravescs/fix_spark_examples_yarn
  [Only log error on missing jar to allow spark examples to jar.]

  0f077b5 Sun Jan 19 10:30:29 2014 -0800
  Merge pull request #458 from tdas/docs-update
  [Updated java API docs for streaming, along with very minor changes in the code examples.]

  03019d1 Sat Jan 18 16:29:43 2014 -0800
  Merge pull request #459 from srowen/UpdaterL2Regularization
  [Correct L2 regularized weight update with canonical form]

  76147a2 Sat Jan 18 16:24:16 2014 -0800
  Merge pull request #437 from mridulm/master
  [Minor api usability changes]

  4ac8cab Sat Jan 18 16:22:46 2014 -0800
  Merge pull request #426 from mateiz/py-ml-tests
  [Re-enable Python MLlib tests (require Python 2.7 and NumPy 1.7+)]

  34e911c Sat Jan 18 16:17:34 2014 -0800
  Merge pull request #462 from mateiz/conf-file-fix
  [Remove Typesafe Config usage and conf files to fix nested property names]

  ff7201c Sat Jan 18 12:50:02 2014 -0800
  Merge pull request #461 from pwendell/master
  [Use renamed shuffle spill config in CoGroupedRDD.scala]

  7b0d5a5 Thu Jan 16 23:18:48 2014 -0800
  Merge pull request #451 from Qiuzhuang/master
  [Fixed Window spark shell launch script error.]

  4ccedb3 Wed Jan 15 14:26:48 2014 -0800
  Merge pull request #444 from mateiz/py-version
  [Clarify that Python 2.7 is only needed for MLlib]

  e3fa36f Wed Jan 15 13:56:04 2014 -0800
  Merge pull request #442 from pwendell/standalone
  [Workers should use working directory as spark home if it's not specified]

  29c76d9 Wed Jan 15 13:55:48 2014 -0800
  Merge pull request #443 from tdas/filestream-fix
  [Made some classes private[stremaing] and deprecated a method in JavaStreamingContext.]

  aca40aa Wed Jan 15 11:15:47 2014 -0800
  Merge pull request #441 from pwendell/graphx-build
  [GraphX shouldn't list Spark as provided.]

  e12c374 Wed Jan 15 10:01:43 2014 -0800
  Merge pull request #433 from markhamstra/debFix
  [Updated Debian packaging]

  2f015c2 Tue Jan 14 23:17:28 2014 -0800
  Merge pull request #436 from ankurdave/VertexId-case
  [Rename VertexID -> VertexId in GraphX]

  2859cab Tue Jan 14 23:08:19 2014 -0800
  Merge pull request #435 from tdas/filestream-fix
  [Fixed the flaky tests by making SparkConf not serializable]

  fbfbb33 Tue Jan 14 23:06:29 2014 -0800
  Merge pull request #434 from rxin/graphxmaven
  [Fixed SVDPlusPlusSuite in Maven build.]

  2c6c07f Tue Jan 14 21:53:05 2014 -0800
  Merge pull request #424 from jegonzal/GraphXProgrammingGuide
  [Additional edits for clarity in the graphx programming guide.]

  6fa4e02 Tue Jan 14 21:51:25 2014 -0800
  Merge pull request #431 from ankurdave/graphx-caching-doc
  [Describe caching and uncaching in GraphX programming guide]

  2f930d5 Tue Jan 14 15:00:11 2014 -0800
  Merge pull request #428 from pwendell/writeable-objects
  [Don't clone records for text files]

  329c9df Tue Jan 14 14:53:36 2014 -0800
  Merge pull request #429 from ankurdave/graphx-examples-pom.xml
  [Add GraphX dependency to examples/pom.xml]

  a14933d Tue Jan 14 14:52:42 2014 -0800
  Merge pull request #427 from pwendell/deprecate-aggregator
  [Deprecate rather than remove old combineValuesByKey function]

  119b6c5 Tue Jan 14 13:29:08 2014 -0800
  Merge pull request #425 from rxin/scaladoc
  [API doc update & make Broadcast public]

  bf3b150 Tue Jan 14 09:45:22 2014 -0800
  Merge pull request #423 from jegonzal/GraphXProgrammingGuide
  [Improving the graphx-programming-guide]

  1b4adc2 Tue Jan 14 01:19:24 2014 -0800
  Merge pull request #420 from pwendell/header-files
  [Add missing header files]

  b60840e Tue Jan 14 00:48:34 2014 -0800
  Merge pull request #418 from pwendell/0.9-versions
  [Version changes for release 0.9.0.]

  980250b Tue Jan 14 00:05:37 2014 -0800
  Merge pull request #416 from tdas/filestream-fix
  [Removed unnecessary DStream operations and updated docs]

  055be5c Mon Jan 13 23:26:44 2014 -0800
  Merge pull request #415 from pwendell/shuffle-compress
  [Enable compression by default for spills]

  fdaabdc Mon Jan 13 23:08:26 2014 -0800
  Merge pull request #380 from mateiz/py-bayes
  [Add Naive Bayes to Python MLlib, and some API fixes]

  4a805af Mon Jan 13 22:58:38 2014 -0800
  Merge pull request #367 from ankurdave/graphx
  [GraphX: Unifying Graphs and Tables]

  945fe7a Mon Jan 13 22:56:12 2014 -0800
  Merge pull request #408 from pwendell/external-serializers
  [Improvements to external sorting]

  68641bc Mon Jan 13 22:54:13 2014 -0800
  Merge pull request #413 from rxin/scaladoc
  [Adjusted visibility of various components and documentation for 0.9.0 release.]

  0ca0d4d Mon Jan 13 22:32:21 2014 -0800
  Merge pull request #401 from andrewor14/master
  [External sorting - Add number of bytes spilled to Web UI]

  08b9fec Mon Jan 13 22:29:03 2014 -0800
  Merge pull request #409 from tdas/unpersist
  [Automatically unpersisting RDDs that have been cleaned up from DStreams]

  b07bc02 Mon Jan 13 20:45:22 2014 -0800
  Merge pull request #412 from harveyfeng/master
  [Add default value for HadoopRDD's `cloneRecords` constructor arg]

  a2fee38 Mon Jan 13 19:45:26 2014 -0800
  Merge pull request #411 from tdas/filestream-fix
  [Improved logic of finding new files in FileInputDStream]

  01c0d72 Mon Jan 13 16:24:30 2014 -0800
  Merge pull request #410 from rxin/scaladoc1
  [Updated JavaStreamingContext to make scaladoc compile.]

  8038da2 Mon Jan 13 14:59:30 2014 -0800
  Merge pull request #2 from jegonzal/GraphXCCIssue
  [Improving documentation and identifying potential bug in CC calculation.]

  b93f9d4 Mon Jan 13 12:18:05 2014 -0800
  Merge pull request #400 from tdas/dstream-move
  [Moved DStream and PairDSream to org.apache.spark.streaming.dstream]

  e6ed13f Sun Jan 12 22:35:14 2014 -0800
  Merge pull request #397 from pwendell/host-port
  [Remove now un-needed hostPort option]

  0b96d85 Sun Jan 12 21:31:43 2014 -0800
  Merge pull request #399 from pwendell/consolidate-off
  [Disable shuffle file consolidation by default]

  0ab505a Sun Jan 12 21:31:04 2014 -0800
  Merge pull request #395 from hsaputra/remove_simpleredundantreturn_scala
  [Remove simple redundant return statements for Scala methods/functions]

  405bfe8 Sun Jan 12 20:04:21 2014 -0800
  Merge pull request #394 from tdas/error-handling
  [Better error handling in Spark Streaming and more API cleanup]

  28a6b0c Sun Jan 12 19:49:36 2014 -0800
  Merge pull request #398 from pwendell/streaming-api
  [Rename DStream.foreach to DStream.foreachRDD]

  074f502 Sun Jan 12 17:01:13 2014 -0800
  Merge pull request #396 from pwendell/executor-env
  [Setting load defaults to true in executor]

  82e2b92 Sun Jan 12 16:55:11 2014 -0800
  Merge pull request #392 from rxin/listenerbus
  [Stop SparkListenerBus daemon thread when DAGScheduler is stopped.]

  288a878 Sat Jan 11 21:53:19 2014 -0800
  Merge pull request #389 from rxin/clone-writables
  [Minor update for clone writables and more documentation.]

  dbc11df Sat Jan 11 18:07:13 2014 -0800
  Merge pull request #388 from pwendell/master
  [Fix UI bug introduced in #244.]

  409866b Sat Jan 11 17:12:06 2014 -0800
  Merge pull request #393 from pwendell/revert-381
  [Revert PR 381]

  6510f04 Sat Jan 11 12:48:26 2014 -0800
  Merge pull request #387 from jerryshao/conf-fix
  [Fix configure didn't work small problem in ALS]

  ee6e7f9 Sat Jan 11 12:07:55 2014 -0800
  Merge pull request #359 from ScrapCodes/clone-writables
  [We clone hadoop key and values by default and reuse objects if asked to.]

  4216178 Sat Jan 11 09:46:48 2014 -0800
  Merge pull request #373 from jerryshao/kafka-upgrade
  [Upgrade Kafka dependecy to 0.8.0 release version]

  92ad18b Fri Jan 10 23:25:15 2014 -0800
  Merge pull request #376 from prabeesh/master
  [Change clientId to random clientId]

  0b5ce7a Fri Jan 10 23:23:21 2014 -0800
  Merge pull request #386 from pwendell/typo-fix
  [Small typo fix]

  1d7bef0 Fri Jan 10 18:53:03 2014 -0800
  Merge pull request #381 from mateiz/default-ttl
  [Fix default TTL for metadata cleaner]

  44d6a8e Fri Jan 10 17:51:50 2014 -0800
  Merge pull request #382 from RongGu/master
  [Fix a type error in comment lines]

  88faa30 Fri Jan 10 17:14:22 2014 -0800
  Merge pull request #385 from shivaram/add-i2-instances
  [Add i2 instance types to Spark EC2.]

  f265531 Fri Jan 10 16:25:44 2014 -0800
  Merge pull request #383 from tdas/driver-test
  [API for automatic driver recovery for streaming programs and other bug fixes]

  d37408f Fri Jan 10 16:25:01 2014 -0800
  Merge pull request #377 from andrewor14/master
  [External Sorting for Aggregator and CoGroupedRDDs (Revisited)]

  0eaf01c Fri Jan 10 15:32:19 2014 -0800
  Merge pull request #369 from pillis/master
  [SPARK-961 Add a Vector.random() method]

  7cef843 Fri Jan 10 15:34:15 2014 -0600
  Merge pull request #371 from tgravescs/yarn_client_addjar_misc_fixes
  [Yarn client addjar and misc fixes]

  7b58f11 Fri Jan 10 12:47:46 2014 -0800
  Merge pull request #384 from pwendell/debug-logs
  [Make DEBUG-level logs consummable.]

  23d2995 Fri Jan 10 10:20:02 2014 -0800
  Merge pull request #1 from jegonzal/graphx
  [ProgrammingGuide]

  0ebc973 Thu Jan 9 23:58:49 2014 -0800
  Merge pull request #375 from mateiz/option-fix
  [Fix bug added when we changed AppDescription.maxCores to an Option]

  dd03cea Thu Jan 9 23:38:03 2014 -0800
  Merge pull request #378 from pwendell/consolidate_on
  [Enable shuffle consolidation by default.]

  997c830 Thu Jan 9 22:22:20 2014 -0800
  Merge pull request #363 from pwendell/streaming-logs
  [Set default logging to WARN for Spark streaming examples.]

  300eaa9 Thu Jan 9 20:29:51 2014 -0800
  Merge pull request #353 from pwendell/ipython-simplify
  [Simplify and fix pyspark script.]

  4b074fa Thu Jan 9 19:03:55 2014 -0800
  Merge pull request #374 from mateiz/completeness
  [Add some missing Java API methods]

  a9d5333 Thu Jan 9 18:46:46 2014 -0800
  Merge pull request #294 from RongGu/master
  [Bug fixes for updating the RDD block's memory and disk usage information]

  d86a85e Thu Jan 9 18:37:52 2014 -0800
  Merge pull request #293 from pwendell/standalone-driver
  [SPARK-998: Support Launching Driver Inside of Standalone Mode]

  26cdb5f Thu Jan 9 17:16:34 2014 -0800
  Merge pull request #372 from pwendell/log4j-fix-1
  [Send logs to stderr by default (instead of stdout).]

  12f414e Thu Jan 9 15:31:30 2014 -0800
  Merge pull request #362 from mateiz/conf-getters
  [Use typed getters for configuration settings]

  365cac9 Thu Jan 9 00:56:16 2014 -0800
  Merge pull request #361 from rxin/clean
  [Minor style cleanup. Mostly on indenting & line width changes.]

  73c724e Thu Jan 9 00:32:19 2014 -0800
  Merge pull request #368 from pwendell/sbt-fix
  [Don't delegate to users `sbt`.]

  dceedb4 Wed Jan 8 23:19:28 2014 -0800
  Merge pull request #364 from pwendell/fix
  [Fixing config option ""retained_stages"" => ""retainedStages"".]

  04d83fc Wed Jan 8 11:55:37 2014 -0800
  Merge pull request #360 from witgo/master
  [fix make-distribution.sh show version: command not found]

  56ebfea Wed Jan 8 11:50:06 2014 -0800
  Merge pull request #357 from hsaputra/set_boolean_paramname
  [Set boolean param name for call to SparkHadoopMapReduceUtil.newTaskAttemptID]

  bdeaeaf Wed Jan 8 11:48:39 2014 -0800
  Merge pull request #358 from pwendell/add-cdh
  [Add CDH Repository to Maven Build]

  5cae05f Wed Jan 8 11:47:28 2014 -0800
  Merge pull request #356 from hsaputra/remove_deprecated_cleanup_method
  [Remove calls to deprecated mapred's OutputCommitter.cleanupJob]

  6eef78d Wed Jan 8 08:49:20 2014 -0600
  Merge pull request #345 from colorant/yarn
  [support distributing extra files to worker for yarn client mode]

  bb6a39a Tue Jan 7 22:32:18 2014 -0800
  Merge pull request #322 from falaki/MLLibDocumentationImprovement
  [SPARK-1009 Updated MLlib docs to show how to use it in Python]

  cb1b927 Tue Jan 7 22:26:28 2014 -0800
  Merge pull request #355 from ScrapCodes/patch-1
  [Update README.md]

  c0f0155 Tue Jan 7 22:21:52 2014 -0800
  Merge pull request #313 from tdas/project-refactor
  [Refactored the streaming project to separate external libraries like Twitter, Kafka, Flume, etc.]

  f5f12dc Tue Jan 7 21:56:35 2014 -0800
  Merge pull request #336 from liancheng/akka-remote-lookup
  [Get rid of `Either[ActorRef, ActorSelection]']

  11891e6 Wed Jan 8 00:32:18 2014 -0500
  Merge pull request #327 from lucarosellini/master
  [Added ‚Äò-i‚Äô command line option to Spark REPL]

  7d0aac9 Wed Jan 8 00:30:45 2014 -0500
  Merge pull request #354 from hsaputra/addasfheadertosbt
  [Add ASF header to the new sbt script.]

  d75dc42 Wed Jan 8 00:30:03 2014 -0500
  Merge pull request #350 from mateiz/standalone-limit
  [Add way to limit default # of cores used by apps in standalone mode]

  61674bc Tue Jan 7 18:32:13 2014 -0800
  Merge pull request #352 from markhamstra/oldArch
  [Don't leave os.arch unset after BlockManagerSuite]

  b2e690f Tue Jan 7 16:57:08 2014 -0800
  Merge pull request #328 from falaki/MatrixFactorizationModel-fix
  [SPARK-1012: DAGScheduler Exception Fix]

  6ccf8ce Tue Jan 7 15:49:14 2014 -0800
  Merge pull request #351 from pwendell/maven-fix
  [Add log4j exclusion rule to maven.]

  7d5fa17 Tue Jan 7 11:31:34 2014 -0800
  Merge pull request #337 from yinxusen/mllib-16-bugfix
  [Mllib 16 bugfix]

  71fc113 Tue Jan 7 11:30:35 2014 -0800
  Merge pull request #349 from CodingCat/support-worker_dir
  [add the comments about SPARK_WORKER_DIR]

  15d9534 Tue Jan 7 08:10:02 2014 -0800
  Merge pull request #318 from srowen/master
  [Suggested small changes to Java code for slightly more standard style, encapsulation and in some cases performance]

  468af0f Tue Jan 7 08:09:01 2014 -0800
  Merge pull request #348 from prabeesh/master
  [spark -> org.apache.spark]

  c3cf047 Tue Jan 7 00:54:25 2014 -0800
  Merge pull request #339 from ScrapCodes/conf-improvements
  [Conf improvements]

  a862caf Tue Jan 7 00:18:20 2014 -0800
  Merge pull request #331 from holdenk/master
  [Add a script to download sbt if not present on the system]

  b97ef21 Mon Jan 6 20:12:57 2014 -0800
  Merge pull request #346 from sproblvem/patch-1
  [Update stop-slaves.sh]

  7210257 Mon Jan 6 18:25:44 2014 -0800
  Merge pull request #128 from adamnovak/master
  [Fix failing ""sbt/sbt publish-local"" by adding a no-argument PrimitiveKeyOpenHashMap constructor]

  e4d6057 Mon Jan 6 14:56:54 2014 -0800
  Merge pull request #343 from pwendell/build-fix
  [Fix test breaking downstream builds]

  93bf962 Mon Jan 6 11:42:41 2014 -0800
  Merge pull request #340 from ScrapCodes/sbt-fixes
  [Made java options to be applied during tests so that they become self explanatory.]

  60edeb3 Mon Jan 6 11:40:32 2014 -0800
  Merge pull request #338 from ScrapCodes/ning-upgrade
  [SPARK-1005 Ning upgrade]

  c708e81 Mon Jan 6 11:35:48 2014 -0800
  Merge pull request #341 from ash211/patch-5
  [Clarify spark.cores.max in docs]

  33fcb91 Mon Jan 6 11:19:23 2014 -0800
  Merge pull request #342 from tgravescs/fix_maven_protobuf
  [Change protobuf version for yarn alpha back to 2.4.1]

  357083c Mon Jan 6 10:29:04 2014 -0800
  Merge pull request #330 from tgravescs/fix_addjars_null_handling
  [Fix handling of empty SPARK_EXAMPLES_JAR]

  a2e7e04 Sun Jan 5 22:37:36 2014 -0800
  Merge pull request #333 from pwendell/logging-silence
  [Quiet ERROR-level Akka Logs]

  5b0986a Sun Jan 5 19:25:09 2014 -0800
  Merge pull request #334 from pwendell/examples-fix
  [Removing SPARK_EXAMPLES_JAR in the code]

  f4b924f Sun Jan 5 17:11:47 2014 -0800
  Merge pull request #335 from rxin/ser
  [Fall back to zero-arg constructor for Serializer initialization if there is no constructor that accepts SparkConf.]

  d43ad3e Sat Jan 4 16:29:30 2014 -0800
  Merge pull request #292 from soulmachine/naive-bayes
  [standard Naive Bayes classifier]

  86404da Sat Jan 4 14:55:54 2014 -0800
  Merge pull request #127 from jegonzal/MapByPartition
  [Adding mapEdges and mapTriplets by Partition]

  e68cdb1 Sat Jan 4 13:46:02 2014 -0800
  Merge pull request #124 from jianpingjwang/master
  [refactor and bug fix]

  280ddf6 Sat Jan 4 12:54:41 2014 -0800
  Merge pull request #121 from ankurdave/more-simplify
  [Simplify GraphImpl internals further]

  10fe23b Fri Jan 3 23:50:14 2014 -0800
  Merge pull request #329 from pwendell/remove-binaries
  [SPARK-1002: Remove Binaries from Spark Source]

  c4d6145 Fri Jan 3 16:30:53 2014 -0800
  Merge pull request #325 from witgo/master
  [Modify spark on yarn to create SparkConf process]

  4ae101f Fri Jan 3 11:24:35 2014 -0800
  Merge pull request #317 from ScrapCodes/spark-915-segregate-scripts
  [Spark-915 segregate scripts]

  87248bd Fri Jan 3 00:45:31 2014 -0800
  Merge pull request #1 from apache/master
  [Merge latest Spark changes]

  30b9db0 Thu Jan 2 23:15:55 2014 -0800
  Merge pull request #285 from colorant/yarn-refactor
  [Yarn refactor]

  498a5f0 Thu Jan 2 19:06:40 2014 -0800
  Merge pull request #323 from tgravescs/sparkconf_yarn_fix
  [fix spark on yarn after the sparkConf changes]

  0475ca8 Thu Jan 2 15:17:08 2014 -0800
  Merge pull request #320 from kayousterhout/erroneous_failed_msg
  [Remove erroneous FAILED state for killed tasks.]

  588a169 Thu Jan 2 13:20:54 2014 -0800
  Merge pull request #297 from tdas/window-improvement
  [Improvements to DStream window ops and refactoring of Spark's CheckpointSuite]

  5e67cdc Thu Jan 2 12:56:28 2014 -0800
  Merge pull request #319 from kayousterhout/remove_error_method
  [Removed redundant TaskSetManager.error() function.]

  ca67909 Thu Jan 2 15:54:54 2014 -0500
  Merge pull request #311 from tmyklebu/master
  [SPARK-991: Report information gleaned from a Python stacktrace in the UI]

  3713f81 Wed Jan 1 21:29:12 2014 -0800
  Merge pull request #309 from mateiz/conf2
  [SPARK-544. Migrate configuration to a SparkConf class]

  c1d928a Wed Jan 1 17:03:48 2014 -0800
  Merge pull request #312 from pwendell/log4j-fix-2
  [SPARK-1008: Logging improvments]

  dc9cb83 Wed Jan 1 13:28:34 2014 -0800
  Merge pull request #126 from jegonzal/FixingPersist
  [Fixing Persist Behavior]

  9a0ff72 Tue Dec 31 21:50:24 2013 -0800
  Merge pull request #314 from witgo/master
  [restore core/pom.xml file modification]

  8b8e70e Tue Dec 31 17:48:24 2013 -0800
  Merge pull request #73 from falaki/ApproximateDistinctCount
  [Approximate distinct count]

  63b411d Tue Dec 31 14:31:28 2013 -0800
  Merge pull request #238 from ngbinh/upgradeNetty
  [upgrade Netty from 4.0.0.Beta2 to 4.0.13.Final]

  32d6ae9 Tue Dec 31 13:51:07 2013 -0800
  Merge pull request #120 from ankurdave/subgraph-reuses-view
  [Reuse VTableReplicated in GraphImpl.subgraph]

  55b7e2f Tue Dec 31 10:12:51 2013 -0800
  Merge pull request #289 from tdas/filestream-fix
  [Bug fixes for file input stream and checkpointing]

  2b71ab9 Mon Dec 30 11:01:30 2013 -0800
  Merge pull request from aarondav: Utilize DiskBlockManager pathway for temp file writing
  [This gives us a couple advantages:]

  50e3b8e Mon Dec 30 07:44:26 2013 -0800
  Merge pull request #308 from kayousterhout/stage_naming
  [Changed naming of StageCompleted event to be consistent]

  72a17b6 Sat Dec 28 21:25:40 2013 -1000
  Revert ""Merge pull request #310 from jyunfan/master""
  [This reverts commit 79b20e4dbe3dcd8559ec8316784d3334bb55868b, reversing]

  79b20e4 Sat Dec 28 21:13:36 2013 -1000
  Merge pull request #310 from jyunfan/master
  [Fix typo in the Accumulators section]

  7375047 Sat Dec 28 13:25:06 2013 -0800
  Merge pull request #304 from kayousterhout/remove_unused
  [Removed unused failed and causeOfFailure variables (in TaskSetManager)]

  ad3dfd1 Fri Dec 27 22:10:14 2013 -0500
  Merge pull request #307 from kayousterhout/other_failure
  [Removed unused OtherFailure TaskEndReason.]

  b579b83 Fri Dec 27 22:09:04 2013 -0500
  Merge pull request #306 from kayousterhout/remove_pending
  [Remove unused hasPendingTasks methods]

  19672dc Fri Dec 27 13:37:10 2013 -0800
  Merge pull request #305 from kayousterhout/line_spacing
  [Fixed >100char lines in DAGScheduler.scala]

  7be1e57 Thu Dec 26 23:41:40 2013 -1000
  Merge pull request #298 from aarondav/minor
  [Minor: Decrease margin of left side of Log page]

  7d811ba Thu Dec 26 23:39:58 2013 -1000
  Merge pull request #302 from pwendell/SPARK-1007
  [SPARK-1007: spark-class2.cmd should change SCALA_VERSION to be 2.10]

  5e69fc5 Thu Dec 26 19:10:39 2013 -0500
  Merge pull request #295 from markhamstra/JobProgressListenerNPE
  [Avoid a lump of coal (NPE) in JobProgressListener's stocking.]

  da20270 Thu Dec 26 12:11:52 2013 -0800
  Merge pull request #1 from aarondav/driver
  [Refactor DriverClient to be more Actor-based]

  e240bad Thu Dec 26 12:30:48 2013 -0500
  Merge pull request #296 from witgo/master
  [Renamed ClusterScheduler to TaskSchedulerImpl for yarn and new-yarn package]

  c344ed0 Thu Dec 26 01:31:06 2013 -0500
  Merge pull request #283 from tmyklebu/master
  [Python bindings for mllib]

  56094bc Wed Dec 25 13:14:33 2013 -0500
  Merge pull request #290 from ash211/patch-3
  [Typo: avaiable -> available]

  4842a07 Wed Dec 25 01:52:15 2013 -0800
  Merge pull request #287 from azuryyu/master
  [Fixed job name in the java streaming example.]

  85a344b Tue Dec 24 16:35:06 2013 -0800
  Merge pull request #127 from kayousterhout/consolidate_schedulers
  [Deduplicate Local and Cluster schedulers.]

  c2dd6bc Tue Dec 24 14:36:47 2013 -0800
  Merge pull request #279 from aarondav/shuffle-cleanup0
  [Clean up shuffle files once their metadata is gone]

  3bf7c70 Tue Dec 24 16:37:13 2013 -0500
  Merge pull request #275 from ueshin/wip/changeclasspathorder
  [Change the order of CLASSPATH.]

  d63856c Mon Dec 23 22:07:26 2013 -0800
  Merge pull request #286 from rxin/build
  [Show full stack trace and time taken in unit tests.]

  23a9ae6 Tue Dec 24 00:08:48 2013 -0500
  Merge pull request #277 from tdas/scheduler-update
  [Refactored the streaming scheduler and added StreamingListener interface]

  11107c9 Mon Dec 23 10:38:20 2013 -0800
  Merge pull request #244 from leftnoteasy/master
  [Added SPARK-968 implementation for review]

  44e4205 Sun Dec 22 11:44:18 2013 -0800
  Merge pull request #116 from jianpingjwang/master
  [remove unused variables and fix a bug]

  4797c22 Fri Dec 20 13:30:39 2013 -0800
  Merge pull request #118 from ankurdave/VertexPartitionSuite
  [Test VertexPartition and fix bugs]

  0bc57c5 Fri Dec 20 11:56:54 2013 -0800
  Merge pull request #280 from aarondav/minor
  [Minor cleanup for standalone scheduler]

  ac70b8f Fri Dec 20 10:56:10 2013 -0800
  Merge pull request #117 from ankurdave/more-tests
  [More tests]

  45310d4 Thu Dec 19 22:08:20 2013 -0800
  Merge pull request #115 from ankurdave/test-reorg
  [Reorganize unit tests; improve GraphSuite test coverage]

  9228ec8 Thu Dec 19 21:37:15 2013 -0800
  Merge pull request #1 from aarondav/127
  [Merge master into 127]

  eca68d4 Thu Dec 19 18:12:22 2013 -0800
  Merge pull request #272 from tmyklebu/master
  [Track and report task"
"
Austen McRae <austen.mcrae@gmail.com>,Thu"," 23 Jan 2014 23:32:47 -0800""",Re: [DISCUSS] Graduating as a TLP,dev@spark.incubator.apache.org,"+1 to both (awesome job all!)



me
le,
to
 48
n
om
"
Evan Chan <ev@ooyala.com>,"Fri, 24 Jan 2014 11:35:26 -0800",Re: [DISCUSS] Graduating as a TLP,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","+1 to both!

te:
:
y
ime
ile,
 to
?
A
n 48
an
e
rom



-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

"
Kapil Malik <kmalik@adobe.com>,"Fri, 24 Jan 2014 22:35:44 +0000",Running spark driver inside a servlet,"""user@spark.incubator.apache.org"" <user@spark.incubator.apache.org>","Hi all,

Is it possible to create a Spark Context (i.e. the driver program) from a servlet deployed on some application server ?
I am able to run spark Java driver successfully via maven / standalone (after specifying the classpath), but when I bundle spark libraries in a JAR, along with my servlet (using maven shade plugin), it gives me security exception. Any suggestions?

Thanks and regards,

Kapil Malik

"
Andrew Ash <andrew@andrewash.com>,"Fri, 24 Jan 2014 14:37:32 -0800",Re: Running spark driver inside a servlet,user@spark.incubator.apache.org,"Can you paste the exception you're seeing?

Sent from my mobile phone

"
Kapil Malik <kmalik@adobe.com>,"Fri, 24 Jan 2014 23:23:52 +0000",RE: Running spark driver inside a servlet,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>,
	""user@spark.incubator.apache.org"" <user@spark.incubator.apache.org>","Hi Andrew,



Here's the exception I get while trying to build an OSGi bundle using maven SCR plugin -



[ERROR] Failed to execute goal org.apache.felix:maven-scr-plugin:1.9.0:scr (generate-scr-scrdescriptor) on project repo-spark: Execution generate-scr-scrdescriptor of goal org.apache.felix:maven-scr-plugin:1.9.0:scr failed: Invalid signature file digest for Manifest main attributes -> [Help 1]

org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.felix:maven-scr-plugin:1.9.0:scr (generate-scr-scrdescriptor) on project repo-spark: Execution generate-scr-scrdescriptor of goal org.apache.felix:maven-scr-plugin:1.9.0:scr failed: Invalid signature file digest for Manifest main attributes

  at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:225)

...

Caused by: org.apache.maven.plugin.PluginExecutionException: Execution generate-scr-scrdescriptor of goal org.apache.felix:maven-scr-plugin:1.9.0:scr failed: Invalid signature file digest for Manifest main attributes

  at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:110)

  at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:209)

  ... 19 more

Caused by: java.lang.SecurityException: Invalid signature file digest for Manifest main attributes

  at sun.security.util.SignatureFileVerifier.processImpl(SignatureFileVerifier.java:240)

...





Also, from eclipse, if I build a simple main program. Then, I can create an executable JAR in 3 ways -

a.       Extract required libraries into generated JAR ( individual classes inside my JAR)

On running main program on this JAR ‚Äì

Exception in thread ""main"" com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'akka.remote.log-received-messages'

        at com.typesafe.config.impl.SimpleConfig.findKey(SimpleConfig.java:126)

        at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:146)



b.      Package required libraries into generated JAR (all JARs inside my JAR)

On running main program on this JAR ‚Äì

Exception in thread ""main"" java.lang.reflect.InvocationTargetException

        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)

        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

        at java.lang.reflect.Method.invoke(Method.java:616)

        at org.eclipse.jdt.internal.jarinjarloader.JarRsrcLoader.main(JarRsrcLoader.java:58)

Caused by: java.lang.Exception: Could not find resource path for Web UI: org/apache/spark/ui/static

        at org.apache.spark.ui.JettyUtils$.createStaticHandler(JettyUtils.scala:89)

        at org.apache.spark.ui.SparkUI.<init>(SparkUI.scala:40)

        at org.apache.spark.SparkContext.<init>(SparkContext.scala:122)

        at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:67)



c.       Copy required libraries into a sub-folder next to generated JAR

This works well ! But problem is that it‚Äôs not portable to a java server.



Regards,



Kapil Malik | kmalik@adobe.com



-----Original Message-----
From: Andrew Ash [mailto:andrew@andrewash.com]
Sent: 25 January 2014 04:08
To: user@spark.incubator.apache.org
Cc: dev@spark.incubator.apache.org
Subject: Re: Running spark driver inside a servlet



Can you paste the exception you're seeing?



Sent from my mobile phone

On Jan 24, 2014 2:36 PM, ""Kapil Malik"" <kmalik@adobe.com<mailto:kmalik@adobe.com>> wrote:



>  Hi all,

>

>

>

> Is it possible to create a Spark Context (i.e. the driver program)

> from a servlet deployed on some application server ?

>

> I am able to run spark Java driver successfully via maven / standalone

> (after specifying the classpath), but when I bundle spark libraries in

> a JAR, along with my servlet (using maven shade plugin), it gives me

> security exception. Any suggestions?

>

>

>

> Thanks and regards,

>

>

>

> Kapil Malik

>

>

>
"
Chen Jin <karen.cj@gmail.com>,"Fri, 24 Jan 2014 17:55:51 -0800",JavaRDD.collect(),dev@spark.incubator.apache.org,"Hi All,

I have some metadata saved as a single partition on HDFS (a few
hundred bytes) and when I want to get the content of the data:

JavaRDD blob = sc.textFile();
List<String> lines = blob.collect();

However, collect takes probably more than 3 seconds at least but
first() only take 0.1 second,

Could you advise on what's the best practice to read small files using spark.

-chen


en SCR plugin -
r (generate-scr-scrdescriptor) on project repo-spark: Execution generate-scr-scrdescriptor of goal org.apache.felix:maven-scr-plugin:1.9.0:scr failed: Invalid signature file digest for Manifest main attributes -> [Help 1]
 goal org.apache.felix:maven-scr-plugin:1.9.0:scr (generate-scr-scrdescriptor) on project repo-spark: Execution generate-scr-scrdescriptor of goal org.apache.felix:maven-scr-plugin:1.9.0:scr failed: Invalid signature file digest for Manifest main attributes
r.java:225)
nerate-scr-scrdescriptor of goal org.apache.felix:maven-scr-plugin:1.9.0:scr failed: Invalid signature file digest for Manifest main attributes
tBuildPluginManager.java:110)
r.java:209)
 Manifest main attributes
ifier.java:240)
an executable JAR in 3 ways -
es inside my JAR)
o configuration setting found for key 'akka.remote.log-received-messages'
a:126)
46)
 JAR)
orImpl.java:57)
odAccessorImpl.java:43)
RsrcLoader.java:58)
org/apache/spark/ui/static
.scala:89)
text.scala:67)
r.

"
Tathagata Das <tathagata.das1565@gmail.com>,"Fri, 24 Jan 2014 18:25:58 -0800",Re: JavaRDD.collect(),dev@spark.incubator.apache.org,"RDD.first() doesnt have to scan the whole partition. It gets only the first
item and returns it.
RDD.collect() has to scan the whole partition, collect all of it and send
all of it back (serialization + deserialization costs, etc.)

TD



on
e
e
a:225)
e
dPluginManager.java:110)
a:209)
.java:240)
e
'
:57)
mpl.java:43)
java:58)
:
67)
R
ver.
"
Chen Jin <karen.cj@gmail.com>,"Fri, 24 Jan 2014 18:34:44 -0800",Re: JavaRDD.collect(),dev@spark.incubator.apache.org,"Hi Tathagata,

Thanks for the detailed explanation, I thought so too.However,
currently I only have one text partition which contains two lines.
Each line is like tens of characters in total. Why is there such a big
difference between first() and collect()? Instead 2x, I have around
30x difference.


st
 on
le
le
va:225)
le
ldPluginManager.java:110)
va:209)
r.java:240)
te
:
s'
a:57)
Impl.java:43)
.java:58)
I:
)
:67)
AR
rver.
e
n

"
Reynold Xin <rxin@databricks.com>,"Fri, 24 Jan 2014 18:52:27 -0800",Re: JavaRDD.collect(),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","The reason is likely because first() is entirely executed on the driver
node in the same process, while collect() needs to connect with worker
nodes.

Usually the first time you run an action, most of the JVM code are not
optimized, and the classloader also needs to load a lot of things on the
fly. Having to connect with other processes via RPC can slow the first
execution down in collect.

That said, if you run this a few times (in the same driver program) and it
is still much slower, you should look into other factors such as network
congestion, cpu/memory load on workers, etc.


BTW - this is a dev list for Spark development itself (not Spark
application development). Questions like this probably go better in the
user list in the future.



nd
g
a:225)
on
dPluginManager.java:110)
a:209)
t
.java:240)
de
on
d)
:57)
mpl.java:43)
java:58)
9)
67)
e
"
Patrick Wendell <pwendell@gmail.com>,"Sat, 25 Jan 2014 11:38:26 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc5),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I'll kick of the voting with a +1.


"
Hossein <falaki@gmail.com>,"Sat, 25 Jan 2014 12:07:04 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc5),dev@spark.incubator.apache.org,"+1

Compiled and tested on Mavericks.

--Hossein



"
Reynold Xin <rxin@databricks.com>,"Sat, 25 Jan 2014 14:27:57 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc5),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>",1
Andy Konwinski <andykonwinski@gmail.com>,"Sat, 25 Jan 2014 14:37:42 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc5),Reynold Xin <rxin@databricks.com>,1
Mark Hamstra <mark@clearstorydata.com>,"Sat, 25 Jan 2014 15:04:39 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc5),dev@spark.incubator.apache.org,1
Sean McNamara <Sean.McNamara@Webtrends.com>,"Sun, 26 Jan 2014 01:00:08 +0000",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc5),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","+1


t;h=


"
"""Xia, Junluan"" <junluan.xia@intel.com>","Sun, 26 Jan 2014 05:49:51 +0000","Any suggestion about JIRA 1006 ""MLlib ALS gets stack overflow with
 too many iterations""?","""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi all

The description about this Bug submitted by Matei is as following


The tipping point seems to be around 50. We should fix this by checkpointing the RDDs every 10-20 iterations to break the lineage chain, but checkpointing currently requires HDFS installed, which not all users will have.

We might also be able to fix DAGScheduler to not be recursive.


regards,
Andrew

"
Reynold Xin <rxin@databricks.com>,"Sat, 25 Jan 2014 21:57:00 -0800","Re: Any suggestion about JIRA 1006 ""MLlib ALS gets stack overflow
 with too many iterations""?","""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","It seems to me fixing DAGScheduler to make it not recursive is the better
solution here, given the cost of checkpointing.


"
Aaron Davidson <ilikerps@gmail.com>,"Sat, 25 Jan 2014 22:01:34 -0800","Re: Any suggestion about JIRA 1006 ""MLlib ALS gets stack overflow
 with too many iterations""?",dev@spark.incubator.apache.org,"I'm an idiot, but which part of the DAGScheduler is recursive here? Seems
like processEvent shouldn't have inherently recursive properties.



"
Reynold Xin <rxin@databricks.com>,"Sat, 25 Jan 2014 22:03:51 -0800","Re: Any suggestion about JIRA 1006 ""MLlib ALS gets stack overflow
 with too many iterations""?","""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I'm not entirely sure, but two candidates are


submitStage







"
"""Shao, Saisai"" <saisai.shao@intel.com>","Sun, 26 Jan 2014 06:52:32 +0000","RE: Any suggestion about JIRA 1006 ""MLlib ALS gets stack overflow
 with too many iterations""?","""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","In my test I found this phenomenon might be caused by RDD's long dependency chain, this dependency chain is serialized into task and sent to each executor, while deserializing this task will cause stack overflow.

Especially in iterative job, like:
var rdd = ..

for (i <- 0 to 100)
 rdd = rdd.map(x=>x)

rdd = rdd.cache

Here rdd's dependency will be chained, at some point stack overflow will occur.

You can check (https://groups.google.com/forum/?fromgroups#!searchin/spark-users/dependency/spark-users/-Cyfe3G6VwY/PFFnslzWn6AJ) and (https://groups.google.com/forum/?fromgroups#!searchin/spark-users/dependency/spark-users/NkxcmmS-DbM/c9qvuShbHEUJ) for details. Current workaround method is to cut the dependency chain by checkpointing RDD, maybe a better way is to clean the dependency chain after materialize stage is executed.

Thanks
Jerry

I'm not entirely sure, but two candidates are


submitStage






:


"
Ewen Cheslack-Postava <me@ewencp.org>,"Sat, 25 Jan 2014 23:06:47 -0800","Re: Any suggestion about JIRA 1006 ""MLlib ALS gets stack overflow
 with too many iterations""?",dev@spark.incubator.apache.org,"The three obvious ones in DAGScheduler.scala are in:

getParentStages
getMissingParentStages

They all follow the same pattern though (def visit(), followed by 
visit(root)), so they should be easy to replace with a Scala stack in 
place of the call stack.

"
Matei Zaharia <matei.zaharia@gmail.com>,"Sat, 25 Jan 2014 23:58:44 -0800","Re: Any suggestion about JIRA 1006 ""MLlib ALS gets stack overflow with too many iterations""?","dev@spark.incubator.apache.org,
 me@ewencp.org","I looked into this after I opened that JIRA and itís actually a bit harder to fix. While changing these visit() calls to use a stack manually instead of being recursive helps avoid a StackOverflowError there, you still get a StackOverflowError when you send the task to a worker node because Java serialization uses recursion. The only real fix therefore with the current codebase is to increase your JVM stack size. Longer-term, Iíd like us to automatically call checkpoint() to break lineage graphs when they exceed a certain size, which would avoid the problems in both DAGScheduler and Java serialization. We could also manually add this to ALS now without having a solution for other programs. That would be a great change to make to fix this JIRA.

Matei


visit(root)), so they should be easy to replace with a Scala stack in place of the call stack.
dependency chain, this dependency chain is serialized into task and sent to each executor, while deserializing this task will cause stack overflow.
will occur.
(https://groups.google.com/forum/?fromgroups#!searchin/spark-users/dependency/spark-users/-Cyfe3G6VwY/PFFnslzWn6AJ) and (https://groups.google.com/forum/?fromgroups#!searchin/spark-users/dependency/spark-users/NkxcmmS-DbM/c9qvuShbHEUJ) for details. Current workaround method is to cut the dependency chain by checkpointing RDD, maybe a better way is to clean the dependency chain after materialize stage is executed.
overflow with too many iterations""?
Seems
better
checkpointing the RDDs every 10-20 iterations to break the lineage chain, but checkpointing currently requires HDFS installed, which not all users will have.

"
"""Nick Pentreath"" <nick.pentreath@gmail.com>","Sun, 26 Jan 2014 00:21:05 -0800 (PST)","Re: Any suggestion about JIRA 1006 ""MLlib ALS gets stack overflow
 with too many iterations""?",dev@spark.incubator.apache.org,"Agree that it should be fixed if possible. But why run ALS for 50 iterations? It tends to pretty much converge (to within 0.001 or so RMSE) after 5-10 and even 20 is probably overkill.‚Äî
Sent from Mailbox for iPhone


bit harder to fix. While changing these visit() calls to use a stack manually instead of being recursive helps avoid a StackOverflowError there, you still get a StackOverflowError when you send the task to a worker node because Java serialization uses recursion. The only real fix therefore with the current codebase is to increase your JVM stack size. Longer-term, I‚Äôd like us to automatically call checkpoint() to break lineage graphs when they exceed a certain size, which would avoid the problems in both DAGScheduler and Java serialization. We could also manually add this to ALS now without having a solution for other programs. That would be a great change to make to fix this JIRA.
visit(root)), so they should be easy to replace with a Scala stack in place of the call stack.
dependency chain, this dependency chain is serialized into task and sent to each executor, while deserializing this task will cause stack overflow.
will occur.
searchin/spark-users/dependency/spark-users/-Cyfe3G6VwY/PFFnslzWn6AJ) and (https://groups.google.com/forum/?fromgroups#!searchin/spark-users/dependency/spark-users/NkxcmmS-DbM/c9qvuShbHEUJ) for details. Current workaround method is to cut the dependency chain by checkpointing RDD, maybe a better way is to clean the dependency chain after materialize stage is executed.
overflow with too many iterations""?
Seems
better
checkpointing the RDDs every 10-20 iterations to break the lineage chain, but checkpointing currently requires HDFS installed, which not all users will have."
"""Nick Pentreath"" <nick.pentreath@gmail.com>","Sun, 26 Jan 2014 00:23:49 -0800 (PST)","Re: Any suggestion about JIRA 1006 ""MLlib ALS gets stack overflow
 with too many iterations""?",dev@spark.incubator.apache.org,"If you want to spend the time running 50 iterations, you're better off re-running 5x10 iterations with different random start to get a better local minimum...‚Äî
Sent from Mailbox for iPhone


bit harder to fix. While changing these visit() calls to use a stack manually instead of being recursive helps avoid a StackOverflowError there, you still get a StackOverflowError when you send the task to a worker node because Java serialization uses recursion. The only real fix therefore with the current codebase is to increase your JVM stack size. Longer-term, I‚Äôd like us to automatically call checkpoint() to break lineage graphs when they exceed a certain size, which would avoid the problems in both DAGScheduler and Java serialization. We could also manually add this to ALS now without having a solution for other programs. That would be a great change to make to fix this JIRA.
visit(root)), so they should be easy to replace with a Scala stack in place of the call stack.
dependency chain, this dependency chain is serialized into task and sent to each executor, while deserializing this task will cause stack overflow.
will occur.
searchin/spark-users/dependency/spark-users/-Cyfe3G6VwY/PFFnslzWn6AJ) and (https://groups.google.com/forum/?fromgroups#!searchin/spark-users/dependency/spark-users/NkxcmmS-DbM/c9qvuShbHEUJ) for details. Current workaround method is to cut the dependency chain by checkpointing RDD, maybe a better way is to clean the dependency chain after materialize stage is executed.
overflow with too many iterations""?
Seems
better
checkpointing the RDDs every 10-20 iterations to break the lineage chain, but checkpointing currently requires HDFS installed, which not all users will have."
"""Xia, Junluan"" <junluan.xia@intel.com>","Sun, 26 Jan 2014 08:42:07 +0000","RE: Any suggestion about JIRA 1006 ""MLlib ALS gets stack overflow
 with too many iterations""?","""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Yes, I agree with Matei, but I think that it is meaningful to change visit() function to non-recursive and avoid Stackoverflow in driver.

-----Original Message-----
From: Nick Pentreath [mailto:nick.pentreath@gmail.com] 
Sent: Sunday, January 26, 2014 4:24 PM
To: dev@spark.incubator.apache.org
Subject: Re: Any suggestion about JIRA 1006 ""MLlib ALS gets stack overflow with too many iterations""?

If you want to spend the time running 50 iterations, you're better off re-running 5x10 iterations with different random start to get a better local minimum...‚Äî Sent from Mailbox for iPhone

On Sun, Jan 26, 2014 at 9:59 AM, Matei Zaharia <matei.zaharia@gmail.com>
wrote:

> I looked into this after I opened that JIRA and it‚Äôs actually a bit harder to fix. While changing these visit() calls to use a stack manually instead of being recursive helps avoid a StackOverflowError there, you still get a StackOverflowError when you send the task to a worker node because Java serialization uses recursion. The only real fix therefore with the current codebase is to increase your JVM stack size. Longer-term, I‚Äôd like us to automatically call checkpoint() to break lineage graphs when they exceed a certain size, which would avoid the problems in both DAGScheduler and Java serialization. We could also manually add this to ALS now without having a solution for other programs. That would be a great change to make to fix this JIRA.
> Matei
> On Jan 25, 2014, at 11:06 PM, Ewen Cheslack-Postava <me@ewencp.org> wrote:
>> The three obvious ones in DAGScheduler.scala are in:
>> 
>> getParentStages
>> getMissingParentStages
>> stageDependsOn
>> 
>> They all follow the same pattern though (def visit(), followed by visit(root)), so they should be easy to replace with a Scala stack in place of the call stack.
>> 
>>> 	Shao, Saisai	January 25, 2014 at 10:52 PM
>>> In my test I found this phenomenon might be caused by RDD's long dependency chain, this dependency chain is serialized into task and sent to each executor, while deserializing this task will cause stack overflow.
>>> 
>>> Especially in iterative job, like:
>>> var rdd = ..
>>> 
>>> for (i <- 0 to 100)
>>> rdd = rdd.map(x=>x)
>>> 
>>> rdd = rdd.cache
>>> 
>>> Here rdd's dependency will be chained, at some point stack overflow will occur.
>>> 
>>> You can check (https://groups.google.com/forum/?fromgroups#!searchin/spark-users/dependency/spark-users/-Cyfe3G6VwY/PFFnslzWn6AJ) and (https://groups.google.com/forum/?fromgroups#!searchin/spark-users/dependency/spark-users/NkxcmmS-DbM/c9qvuShbHEUJ) for details. Current workaround method is to cut the dependency chain by checkpointing RDD, maybe a better way is to clean the dependency chain after materialize stage is executed.
>>> 
>>> Thanks
>>> Jerry
>>> 
>>> -----Original Message-----
>>> From: Reynold Xin [mailto:rxin@databricks.com]
>>> Sent: Sunday, January 26, 2014 2:04 PM
>>> To: dev@spark.incubator.apache.org
>>> Subject: Re: Any suggestion about JIRA 1006 ""MLlib ALS gets stack overflow with too many iterations""?
>>> 
>>> I'm not entirely sure, but two candidates are
>>> 
>>> the visit function in stageDependsOn
>>> 
>>> submitStage
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 	Reynold Xin	January 25, 2014 at 10:03 PM
>>> I'm not entirely sure, but two candidates are
>>> 
>>> the visit function in stageDependsOn
>>> 
>>> submitStage
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 	Aaron Davidson	January 25, 2014 at 10:01 PM
>>> I'm an idiot, but which part of the DAGScheduler is recursive here? 
>>> Seems like processEvent shouldn't have inherently recursive properties.
>>> 
>>> 
>>> 
>>> 	Reynold Xin	January 25, 2014 at 9:57 PM
>>> It seems to me fixing DAGScheduler to make it not recursive is the 
>>> better solution here, given the cost of checkpointing.
>>> 
>>> 
>>> 	Xia, Junluan	January 25, 2014 at 9:49 PM
>>> Hi all
>>> 
>>> The description about this Bug submitted by Matei is as following
>>> 
>>> 
>>> The tipping point seems to be around 50. We should fix this by checkpointing the RDDs every 10-20 iterations to break the lineage chain, but checkpointing currently requires HDFS installed, which not all users will have.
>>> 
>>> We might also be able to fix DAGScheduler to not be recursive.
>>> 
>>> 
>>> regards,
>>> Andrew
>>> 
>>> 
"
Sean Owen <sowen@cloudera.com>,"Sun, 26 Jan 2014 08:43:24 +0000","Re: Any suggestion about JIRA 1006 ""MLlib ALS gets stack overflow
 with too many iterations""?",dev@spark.incubator.apache.org,"I think ""it depends"" a fair bit here.

That's a good default absolute convergence cutoff, although it's not crazy
to want to run to further convergence since +/- 0.001 can make a difference
in top-N recommendations that is noticeable, and it can seem weird that
it's 'converged' while answers are non-trivially changing.

And this depends on how fast it converges, and that can be influenced by
the scale of the data (vs rank) and lambda.

50 is generally a lot although it's not going to get the same error as 5 x
10-iteration runs. I am sure it will be necessary for Spark to succeed on
50 iterations of something, which is not news.

I was hoping to propose for ALS was a convergence criterion, since indeed I
think most cases will converge much faster. Then the iterations param can
be ""max iterations"" instead, and at least it's harder to make the thing try
to do 50+ iterations.

(We are also looking to rebuild some other related functions on top, like
running N models at once -- for the cost of more computation should get you
to a better solution in fewer iterations over all the models. Sort of
helps.)


So far this message has not been relevant to the original issue. $0.02:
This problem is, I think, one that is more likely to come up at scale.
Those are environments where people are probably running on a cluster,
which has HDFS. If the checkpoint answer needs HDFS, it may be just fine to
solve the problem with checkpointing only where checkpointing is available.


--
Sean Owen | Director, Data Science | London


rote:

 bit
th
‚Äôd
LS
ce
to
ncy/spark-users/-Cyfe3G6VwY/PFFnslzWn6AJ)
ncy/spark-users/NkxcmmS-DbM/c9qvuShbHEUJ)
"
Archit Thakur <archit279thakur@gmail.com>,"Mon, 27 Jan 2014 01:52:51 +0530",GroupByKey implementation.,"user@spark.incubator.apache.org, dev@spark.incubator.apache.org","Hi,

Below is the implementation for GroupByKey. (v, 0.8.0)


def groupByKey(partitioner: Partitioner): RDD[(K, Seq[V])] = {
    def createCombiner(v: V) = ArrayBuffer(v)
    def mergeValue(buf: ArrayBuffer[V], v: V) = buf += v
    val bufs = combineByKey[ArrayBuffer[V]](
      createCombiner _, mergeValue _, null, partitioner,
mapSideCombine=false)
    bufs.asInstanceOf[RDD[(K, Seq[V])]]
  }

and CombineValuesByKey (Aggregator.scala):

def combineValuesByKey(iter: Iterator[_ <: Product2[K, V]]) : Iterator[(K,
C)] = {
    val combiners = new JHashMap[K, C]
    for (kv <- iter) {
      val oldC = combiners.get(kv._1)
      if (oldC == null) {
        combiners.put(kv._1, createCombiner(kv._2))
      } else {
        combiners.put(kv._1, mergeValue(oldC, kv._2))
      }
    }
    combiners.iterator
  }

My doubt is why null is being passed for mergeCombiners closure.

If two different partitions have same key, wouldn't there be the
requirement to merge them afterwards?

Thanks,
Archit.
"
Mark Hamstra <mark@clearstorydata.com>,"Sun, 26 Jan 2014 13:42:00 -0800",Re: GroupByKey implementation.,dev@spark.incubator.apache.org,"groupByKey does merge the values associated with the same key in different
partitions:

scala> val rdd = sc.parallelize(List(1, 1, 1, 1),
4).mapPartitionsWithIndex((idx, itr) => List((""foo"", idx ->
math.random),(""bar"", idx -> math.random)).toIterator)

scala> rdd.collect.foreach(println)

(foo,(0,0.7387266457142971))
(bar,(0,0.06390701080780203))
(foo,(1,0.3601832111876926))
(bar,(1,0.5247725435958681))
(foo,(2,0.7486323021599729))
(bar,(2,0.9185837845634715))
(foo,(3,0.17591718413623136))
(bar,(3,0.12096331089133605))

scala> rdd.groupByKey.collect.foreach(println)

(foo,ArrayBuffer((0,0.8432285514154537), (1,0.3005967566708283),
(2,0.6150820518108783), (3,0.4779052219014124)))
(bar,ArrayBuffer((0,0.8190206253566251), (1,0.3465707665527258),
(2,0.5187789456090471), (3,0.9612998198743644)))



"
Archit Thakur <archit279thakur@gmail.com>,"Mon, 27 Jan 2014 03:15:04 +0530",Re: GroupByKey implementation.,dev@spark.incubator.apache.org,"Which spark version are you on?



"
Matei Zaharia <matei.zaharia@gmail.com>,"Sun, 26 Jan 2014 13:49:38 -0800",[VOTE] Graduation of Apache Spark,dev@spark.incubator.apache.org,"Hi guys,

Discussion has proceeded positively, so I'm calling for a community VOTE for the graduation of Apache Spark (incubating) into a top level project. If this VOTE is successful, then I'll call an Incubator PMC VOTE in 72 hours, and if that is successful, weíll submit the project graduation resolution below into the board agenda for the next Apache board meeting.

So far, I've heard the following VOTEs (implied) during the DISCUSS thread. If you see your name there is no need to VOTE again and Iíll carry through the VOTE as below. If you want to change your VOTE, or I got it wrong, let me know and we'll change it.

+1

Matei Zaharia
Reynold Xin
Tathagata Das
Sean McNamara
Patrick Wendell
Mark Hamstra
Chris Mattmann *
Tom Graves
Henry Saputra *
Andy Konwinski
Josh Rosen
Mosharaf Chowdhury
Mridul Muralidharan
Nick Pentreath
Andrew Xia
Haoyuan Li

* - indicates IPMC

Anyone else interested, please VOTE to graduate Apache Spark from the Incubator. I'll try and close the VOTE on Wednesday and then start the Incubator PMC VOTE on general@incubator.apache.org.

[ ] +1 Graduate Apache Spark (incubating) from the Incubator per the resolution below.
[ ] +0 Don't care.
[ ] -1 Don't graduate Apache Spark (incubating) from the Incubator because..

Thanks guys! The resolution is included below.

---

WHEREAS, the Board of Directors deems it to be in the best
interests of the Foundation and consistent with the
Foundation's purpose to establish a Project Management
Committee charged with the creation and maintenance of
open-source software, for distribution at no charge to the
public, related to fast and flexible large-scale data analysis
on clusters.

NOW, THEREFORE, BE IT RESOLVED, that a Project Management
Committee (PMC), to be known as the ""Apache Spark Project"", be
and hereby is established pursuant to Bylaws of the Foundation;
and be it further

RESOLVED, that the Apache Spark Project be and hereby is
responsible for the creation and maintenance of software
related to efficient cluster management, resource isolation
and sharing across distributed applications; and be it further
RESOLVED, that the office of ""Vice President, Apache Spark"" be
and hereby is created, the person holding such office to serve
at the direction of the Board of Directors as the chair of the
Apache Spark Project, and to have primary responsibility for
management of the projects within the scope of responsibility
of the Apache Spark Project; and be it further
RESOLVED, that the persons listed immediately below be and
hereby are appointed to serve as the initial members of the
Apache Spark Project:

* Mosharaf Chowdhury <mosharaf@apache.org>
* Jason Dai <jasondai@apache.org>
* Tathagata Das <tdas@eecs.berkeley.edu>
* Ankur Dave <ankurdave@gmail.com>
* Aaron Davidson <aarondavidson@berkeley.edu>
* Thomas Dudziak <tomdz@apache.org>
* Robert Evans <bobby@apache.org>
* Thomas Graves <tgraves@apache.org>
* Andy Konwinski <andrew@apache.org>
* Stephen Haberman <stephenh@apache.org>
* Mark Hamstra <markhamstra@apache.org>
* Shane Huang <shane_huang@apache.org>
* Ryan LeCompte <ryanlecompte@apache.org>
* Haoyuan Li <haoyuan@apache.org>
* Sean McNamara <mcnamara@apache.org>
* Mridul Muralidharam <mridulm@yahoo-inc.com>
* Kay Ousterhout <keo@eecs.berkeley.edu>
* Nick Pentreath <mlnick@apache.org>
* Imran Rashid <imran@quantifind.com>
* Charles Reiss <woggle@apache.org>
* Josh Rosen <joshrosen@apache.org>
* Prashant Sharma <prashant@apache.org>
* Ram Sriharsha <harshars@yahoo-inc.com>
* Shivaram Venkataraman <shivaram@apache.org>
* Patrick Wendell <pwendell@apache.org>
* Andrew Xia <xiajunluan@gmail.com>
* Reynold Xin <rxin@apache.org>
* Matei Zaharia <matei@apache.org>

NOW, THEREFORE, BE IT FURTHER RESOLVED, that Matei Zaharia be
appointed to the office of Vice President, Apache Spark, to
serve in accordance with and subject to the direction of the
Board of Directors and the Bylaws of the Foundation until
death, resignation, retirement, removal or disqualification, or
until a successor is appointed; and be it further

RESOLVED, that the Apache Spark Project be and hereby is
tasked with the migration and rationalization of the Apache
Incubator Spark podling; and be it further

RESOLVED, that all responsibilities pertaining to the Apache
Incubator Spark podling encumbered upon the Apache Incubator
Project are hereafter discharged.

---



"
Sandy Ryza <sandy.ryza@cloudera.com>,"Sun, 26 Jan 2014 14:05:30 -0800",Re: [VOTE] Graduation of Apache Spark,dev@spark.incubator.apache.org,"+1!


te:

rry
"
Sebastian Schelter <ssc@apache.org>,"Sun, 26 Jan 2014 23:08:07 +0100",Re: [VOTE] Graduation of Apache Spark,dev@spark.incubator.apache.org,"+1 (I'm also on the IPMC)



"
Kostas Sakellis <kostas@cloudera.com>,"Sun, 26 Jan 2014 14:10:37 -0800",Re: [VOTE] Graduation of Apache Spark,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>, ""ssc@apache.org"" <ssc@apache.org>","+1!


rry
"
Christopher Nguyen <ctn@adatao.com>,"Sun, 26 Jan 2014 14:10:47 -0800",Re: [VOTE] Graduation of Apache Spark,dev@spark.incubator.apache.org,"+1

Sent while mobile. Pls excuse typos etc.

rry
"
Aaron Davidson <ilikerps@gmail.com>,"Sun, 26 Jan 2014 14:24:11 -0800",Re: [VOTE] Graduation of Apache Spark,dev@spark.incubator.apache.org,1
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Sun, 26 Jan 2014 14:33:28 -0800",Re: [VOTE] Graduation of Apache Spark,dev@spark.incubator.apache.org,1
Mark Hamstra <mark@clearstorydata.com>,"Sun, 26 Jan 2014 14:42:47 -0800",Re: GroupByKey implementation.,dev@spark.incubator.apache.org,"That was run on 0.8.0-incubating ...which raises a question that has been
recurring to me of late: Why are people continuing to use 0.8.0 months
after 0.8.1 has been out and when 0.9.0 is in release candidates?  It
doesn't make a relevant difference in this case, but in general, chasing
bugs in code that is two generations out-of-date doesn't make for very
efficient development.  Spark is still pre-1.0 and is rapidly-developing
software.  As such, you should expect that the pain of staying up-to-date
is less than the pain of falling months behind -- but there is no avoiding
versioning/release practices with 1.0, it will make more sense to stick
with a major.minor release for a while and only pick up the
major.minor.patchlevel increments, but we're not there yet.



"
Kay Ousterhout <keo@eecs.berkeley.edu>,"Sun, 26 Jan 2014 14:52:59 -0800",Re: [VOTE] Graduation of Apache Spark,dev@spark.incubator.apache.org,1
Evan Sparks <evan.sparks@gmail.com>,"Sun, 26 Jan 2014 15:05:22 -0800",Re: [VOTE] Graduation of Apache Spark,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>",1
Xuefeng Wu <benewu@gmail.com>,"Mon, 27 Jan 2014 07:06:12 +0800",Re: [VOTE] Graduation of Apache Spark,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","+1


Yours, Xuefeng Wu Œ‚—©∑Â æ¥…œ

or the graduation of Apache Spark (incubating) into a top level project. If this VOTE is successful, then I'll call an Incubator PMC VOTE in 72 hours, and if that is successful, we°Øll submit the project graduation reso"
Konstantin Boudnik <cos@apache.org>,"Sun, 26 Jan 2014 15:31:06 -0800",Re: [VOTE] Graduation of Apache Spark,dev@spark.incubator.apache.org,"+1

for the graduation of Apache Spark (incubating) into a top level project. If this VOTE is successful, then I'll call an Incubator PMC VOTE in 72 hours, and if that is successful, we‚Äôll submit the project graduation resolution below into the board ag"
Reynold Xin <rxin@databricks.com>,"Sun, 26 Jan 2014 15:37:39 -0800",Re: GroupByKey implementation.,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","While I echo Mark's sentiment, versioning has nothing to do with this
problem. It has been the case even in Spark 0.8.0.

Note that mapSideCombine is turned off for groupByKey, so there is no need
to merge any combiners.



"
Tathagata Das <tathagata.das1565@gmail.com>,"Sun, 26 Jan 2014 16:14:10 -0800",Re: [VOTE] Graduation of Apache Spark,dev@spark.incubator.apache.org,"+1



E
rry
"
Chris Mattmann <mattmann@apache.org>,"Sun, 26 Jan 2014 17:09:18 -0800",[DISCUSS] [VOTE] Graduation of Apache Spark,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi Matei,

off of the PMC constituency. While I don't personally take any offense
to that, I think you will want to keep at least a few of your mentors
around that are ASF members. This has the dual benefit of:

1. Making sure that Apache Spark has a shot at getting members elected
at the next members meeting (which would be tough unless you had some
mentors who knew what was going on in the project -- not impossible but
tough)

2. Helping and hanging around with any Apache way stuff

Up to you but I thought I'd raise the point.

Cheers,
Chris







"
Matei Zaharia <matei.zaharia@gmail.com>,"Sun, 26 Jan 2014 17:42:51 -0800",Re: [DISCUSS] [VOTE] Graduation of Apache Spark,dev@spark.incubator.apache.org,"Hey Chris, this is a good point ó I didnít realize that the mentors would have other roles later in interacting with the ASF, I thought they were just focused on the incubation process. In that case I think Iíd propose we add you, for example, once the project is up. For simplicity I donít think it makes sense to cancel the current vote and add another person, but we can have a new vote once we have a PMC. Help in this area is definitely appreciated.

Matei


but
<dev@spark.incubator.apache.org>
VOTE
project.
72
graduation
meeting.
I
the


"
Rahul Chugh <rchugh@playdom.com>,"Sun, 26 Jan 2014 17:52:31 -0800",Re: [VOTE] Graduation of Apache Spark,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","+1

te:
e:


tion

ll carry

"
Chris Mattmann <mattmann@apache.org>,"Sun, 26 Jan 2014 17:57:49 -0800",Re: [DISCUSS] [VOTE] Graduation of Apache Spark,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Thanks, Matei.

Cheers,
Chris


d



"
Prashant Sharma <scrapcodes@gmail.com>,"Mon, 27 Jan 2014 10:59:24 +0530",Re: [VOTE] Graduation of Apache Spark,dev@spark.incubator.apache.org,"+1



t.
g.



-- 
Prashant
"
Stephen Haberman <stephen.haberman@gmail.com>,"Mon, 27 Jan 2014 00:25:15 -0600",Re: [VOTE] Graduation of Apache Spark,dev@spark.incubator.apache.org,"
+1

- Stephen


"
prabeesh k <prabsmails@gmail.com>,"Mon, 27 Jan 2014 11:55:27 +0530",Re: [VOTE] Graduation of Apache Spark,dev@spark.incubator.apache.org,"+1


te:

2
on
l
it
he
e
e
"
"""Shao, Saisai"" <saisai.shao@intel.com>","Mon, 27 Jan 2014 06:45:40 +0000",RE: [VOTE] Graduation of Apache Spark,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","+1!

Thanks
Jerry



te:


"
Taka Shinagawa <taka.epsilon@gmail.com>,"Sun, 26 Jan 2014 22:49:07 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc5),dev@spark.incubator.apache.org,"If I build Spark for Hadoop 1.0.4 (either ""SPARK_HADOOP_VERSION=1.0.4
sbt/sbt assembly""  or ""sbt/sbt assembly"") or use the binary distribution,
'sbt/sbt test' runs successfully.

However, if I build Spark targeting any other Hadoop versions (e.g.
""SPARK_HADOOP_VERSION=1.2.1 sbt/sbt assembly"", ""SPARK_HADOOP_VERSION=2.2.0
sbt/sbt assembly""), I'm getting the following errors with 'sbt/sbt test':

1) type mismatch errors with JavaPairDStream.scala
2) following test failures
[error] Failed tests:
[error] org.apache.spark.ShuffleNettySuite
[error] org.apache.spark.ShuffleSuite
[error] org.apache.spark.FileServerSuite
[error] org.apache.spark.DistributedSuite

I don't have Hadoop 1.0.4 installed on my test systems (but the test
succeeds, and failed with the installed Hadoop versions). I'm seeing these
sbt test errors with the previous 0.9.0 RCs and 0.8.1, too.

I'm wondering if anyone else has seen this problem or I'm missing something
to run the test correctly.

Thanks,
Taka





"
Reynold Xin <rxin@databricks.com>,"Sun, 26 Jan 2014 22:52:06 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc5),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","It is possible that you have generated the assembly jar using one version
of Hadoop, and then another assembly jar with another version. Those tests
that failed are all using a local cluster that sets up multiple processes,
which would require launching Spark worker processes using the assembly
jar. If that's indeed the problem, removing the extra assembly jars should
fix them.



"
Patrick Wendell <pwendell@gmail.com>,"Sun, 26 Jan 2014 22:58:40 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc5),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey Taka,

If you build a second version you need to clean the existing assembly jar.

The reference implementation of the tests are the ones on the U.C.
Berkeley Jenkins. These are passing for Branch 0.9 for both Hadoop 1
and Hadoop 2 versions, so I'm inclined to think it's an issue with
your test env or setup.

https://amplab.cs.berkeley.edu/jenkins/view/Spark/

- Patrick


"
Taka Shinagawa <taka.epsilon@gmail.com>,"Sun, 26 Jan 2014 23:48:06 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc5),dev@spark.incubator.apache.org,"I'm always running sbt clean before building Spark. And I'm seeing the
errors when I build Spark first time after downloading and extracting
spark-0.9.0-incubating.tgz<http://people.apache.org/~pwendell/spark-0.9.0-incubating-rc5/spark-0.9.0-incubating.tgz>

Just in case, I deleted the test.jar files (in work/app-201401XXXXXX-0000
directories) which didn't get deleted by sbt clean. But I still see the
errors.

This is not a blocker for the release at all. I've tested the RC5 and don't
find any other issue right now.

I'm curious if anyone else sees this.





"
Patrick Wendell <pwendell@gmail.com>,"Sun, 26 Jan 2014 23:51:43 -0800",[RESULT] [VOTE] Release Apache Spark 0.9.0-incubating (rc5),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Voting is now closed. This vote passes with 5 binding +1 votes and no
0 or -1 votes. This vote will now go to the IPMC list for a second
72-hour vote. Spark developers are encouraged to comment on the IPMC
vote as well.

The totals are:

+1
Patrick Wendell*
Hossein Falaki
Reynold Xin*
Andy Konwinski*
Mark Hamstra*
Sean McNamara*

0: (none)
-1: (none)


"
Taka Shinagawa <taka.epsilon@gmail.com>,"Sun, 26 Jan 2014 23:52:04 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc5),dev@spark.incubator.apache.org,"downloading and extracting
spark-0.9.0-incubating.tgz<http://people.apache.org/~pwendell/spark-0.9.0-incubating-rc5/spark-0.9.0-incubating.tgz>
A little clarification... I see the errors during sbt test after building
Spark, not during build.



"
Sebastian Schelter <ssc@apache.org>,"Mon, 27 Jan 2014 08:52:50 +0100",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc5),dev@spark.incubator.apache.org,"Hi Taka,

please write a new mail for your problem and don't reply to an existing 
(unrelated) thread:

https://people.apache.org/~hossman/#threadhijack




"
Archit Thakur <archit279thakur@gmail.com>,"Mon, 27 Jan 2014 13:28:28 +0530",Re: GroupByKey implementation.,dev@spark.incubator.apache.org,"Thanks Mark, Reynold for the quick response.



"
Archit Thakur <archit279thakur@gmail.com>,"Mon, 27 Jan 2014 20:14:16 +0530",Problems while moving from 0.8.0 to 0.8.1,"dev@spark.incubator.apache.org, user@spark.incubator.apache.org","Hi,

Implementation of aggregation logic has been changed with 0.8.1
(Aggregator.scala)

release.

Aggregator.scala
def combineValuesByKey(iter: Iterator[_ <: Product2[K, V]]) : Iterator[(K,
C)] = {
    var kv: Product2[K, V] = null
    val update = (hadValue: Boolean, oldValue: C) => {
      if (hadValue) mergeValue(oldValue, kv._2) else createCombiner(kv._2)
    }
    while (iter.hasNext) {
      kv = iter.next()
      combiners.changeValue(kv._1, update)
    }
    combiners.iterator
  }

computes,
val curKey = data(2 * pos)
which is coming as null and eventually giving NPE.

def changeValue(key: K, updateFunc: (Boolean, V) => V): V = {
    val k = key.asInstanceOf[AnyRef]
    if (k.eq(null)) {
      if (!haveNullValue) {
        incrementSize()
      }
      nullValue = updateFunc(haveNullValue, nullValue)
      haveNullValue = true
      return nullValue
    }
    var pos = rehash(k.hashCode) & mask
    var i = 1
    while (true) {
      val curKey = data(2 * pos)
      if (k.eq(curKey) || k.equals(curKey)) {
        val newValue = updateFunc(true, data(2 * pos + 1).asInstanceOf[V])
        data(2 * pos + 1) = newValue.asInstanceOf[AnyRef]
        return newValue
      } else if (curKey.eq(null)) {
        val newValue = updateFunc(false, null.asInstanceOf[V])
        data(2 * pos) = k
        data(2 * pos + 1) = newValue.asInstanceOf[AnyRef]
        incrementSize()
        return newValue
      } else {
        val delta = i
        pos = (pos + delta) & mask
        i += 1
      }
    }
    null.asInstanceOf[V] // Never reached but needed to keep compiler happy
  }


Other info:
1. My code works fine with 0.8.0.
2. I used groupByKey transformation.
3. I replaces the Aggregator.scala with the older version(0.8.0), compiled
it, Restarted Master and Worker, It ran successfully.

Thanks and Regards,
Archit Thakur.
"
Nick Pentreath <nick.pentreath@gmail.com>,"Mon, 27 Jan 2014 16:51:05 +0200",Fwd: Represent your project at ApacheCon,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Is Spark active in submitting anything for this?


---------- Forwarded message ----------
From: Rich Bowen <rbowen@redhat.com>
Date: Mon, Jan 27, 2014 at 4:20 PM
Subject: Represent your project at ApacheCon
To: committers@apache.org


Folks,

5 days from the end of the CFP, we have only 50 talks submitted. We need
three times that just to fill the space, and preferably a lot more so that
we have some variety to choose from to put together a schedule.

I know that we usually have over half the content submitted in the last 48
hours, so I'm not panicking yet, but it's worrying. More worrying, however
is that 2/3 of those submissions are from the Usual Suspects (ie, httpd and
Tomcat), and YOUR project isn't represented.

We would love to have a whole day of Lucene, and of OpenOffice, and of
Cordova, and of Felix and Celix and Helix and Nelix. Or a half day.

We need your talk submissions. We need you to come tell the world why your
project matters, why you spend your time working on it, and what exciting
new thing you hacked into it during the snow storms. (Or heat wave, as the
case may be.)

Please help us get the word out to your developer and user communities that
we're looking for quality talks about their favorite Apache project, about
related technologies, about ways that it's being used, and plans for its
future. Help us make this ApacheCon amazing.

--rcb

-- 
Rich Bowen - rbowen@redhat.com
OpenStack Community Liaison
http://openstack.redhat.com/
"
Mayur Rustagi <mayur.rustagi@gmail.com>,"Mon, 27 Jan 2014 20:46:31 +0530",Re: Represent your project at ApacheCon,dev@spark.incubator.apache.org,"We have a couple of customer stories around Spark would that be something
that would be interesting here?
Also what is the process of submitting talks?
Regards
Mayur

Mayur Rustagi
Ph: +919632149971
h <https://twitter.com/mayur_rustagi>ttp://www.sigmoidanalytics.com
https://twitter.com/mayur_rustagi




"
Heiko Braun <ike.braun@googlemail.com>,"Mon, 27 Jan 2014 16:58:38 +0100",Moving to Typesafe Config?,dev@spark.incubator.apache.org,"
Is there any interest in moving to a more structured approach for configuring spark components? I.e. moving to the typesafe config [1]. Since spark already leverages akka, this seems to be a reasonable choice IMO.

[1] https://github.com/typesafehub/config

Regards, Heiko


"
Mark Hamstra <mark@clearstorydata.com>,"Mon, 27 Jan 2014 08:05:41 -0800",Re: Moving to Typesafe Config?,dev@spark.incubator.apache.org,"Been done and undone, and will probably be redone for 1.0.  See
https://mail.google.com/mail/ca/u/0/#search/config/143a6c39e3995882



"
Heiko Braun <ike.braun@googlemail.com>,"Mon, 27 Jan 2014 17:13:38 +0100",Re: Moving to Typesafe Config?,dev@spark.incubator.apache.org,"Thanks Mark.


Since
IMO.


"
Mark Hamstra <mark@clearstorydata.com>,"Mon, 27 Jan 2014 08:34:57 -0800",Re: Moving to Typesafe Config?,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","And it would be more helpful if I gave you a usable link http://apache-spark-developers-list.1001551.n3.nabble.com/Config-properties-broken-in-master-td208.html

Sent from my iPhone


ote:
nce
"
Heiko Braun <ike.braun@googlemail.com>,"Mon, 27 Jan 2014 17:59:25 +0100",Re: Moving to Typesafe Config?,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Thanks. I found the discussion myself ;)

/heiko

rk-developers-list.1001551.n3.nabble.com/Config-properties-broken-in-master-td208.html
:
rote:
ince


"
dachuan <hdc1112@gmail.com>,"Mon, 27 Jan 2014 12:22:35 -0500",Fwd: real world streaming code,dev@spark.incubator.apache.org,"This email, which includes my questions about spark streaming, is forwarded
from user@mailing-list. Sorry about this, because I haven't got any reply
yet.

thanks,
dachuan.

---------- Forwarded message ----------
From: dachuan <hdc1112@gmail.com>
Date: Fri, Jan 24, 2014 at 10:28 PM
Subject: real world streaming code
To: user@spark.incubator.apache.org


Hello, community,

I have three questions about spark streaming.

1,
I noticed that one streaming example (StatefulNetworkWordCount) has one
interesting phenomenon:
since this workload only prints the first 10 rows of the final RDD, this
means if the data influx rate is fast enough (much faster than hand typing
in keyboard), then the final RDD would have more than one partition, assume
it's 2 partitions, but the second partition won't be computed at all
because the first partition suffice to serve the first 10 rows. However,
these two workloads must make checkpoint to that RDD. This would lead to a
very time consuming checkpoint process because the checkpoint to the second
partition can only start before it is computed. So, is this workload only
designed for demonstration purpose, for example, only designed for one
partition RDD?

(I have attached a figure to illustrate what I've said, please tell me if
mailing list doesn't welcome attachment.
A short description about the experiment
Hardware specs: 4 cores
Software specs: spark local cluster, 5 executors (workers), each one has
one core, each executor has 1G memory
Data influx speed: 3MB/s
Data source: one ServerSocket in local file
Streaming App's name: StatefulNetworkWordCount
Job generation frequency: one job per second
Checkpoint time: once per 10s
JobManager.numThreads = 2)



(And another workload might have the same problem:
PageViewStream's slidingPageCounts)

2,
Does anybody have a Top-K wordcount streaming source code?

3,
Can anybody share your real world streaming example? for example, including
source code, and cluster configuration details?

thanks,
dachuan.

-- 
Dachuan Huang
Cellphone: 614-390-7234
2015 Neil Avenue
Ohio State University
Columbus, Ohio
U.S.A.
43210



-- 
Dachuan Huang
Cellphone: 614-390-7234
2015 Neil Avenue
Ohio State University
Columbus, Ohio
U.S.A.
43210
"
Reynold Xin <rxin@databricks.com>,"Mon, 27 Jan 2014 09:24:16 -0800",Re: Problems while moving from 0.8.0 to 0.8.1,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Do you mind pasting the whole stack trace for the NPE?




"
Mayur Rustagi <mayur.rustagi@gmail.com>,"Mon, 27 Jan 2014 23:00:33 +0530",Re: real world streaming code,dev <dev@spark.incubator.apache.org>,"I can help you setup streaming with algebird for Uniques.. I suppose you
can extend that to top K using algebird functions.
First why dont you setup spark streaming on your machine using this guide:
http://docs.sigmoidanalytics.com/index.php/Running_A_Simple_Streaming_Job_in_Local_Machine
Then lemme rummage around for my algebird codebase.
Regards
Mayur


Mayur Rustagi
Ph: +919632149971
h <https://twitter.com/mayur_rustagi>ttp://www.sigmoidanalytics.com
https://twitter.com/mayur_rustagi




"
Mayur Rustagi <mayur.rustagi@gmail.com>,"Mon, 27 Jan 2014 23:07:11 +0530",Re: real world streaming code,dev <dev@spark.incubator.apache.org>,"Now add these jars to the lib folder of the streaming project as well as in
the sparkstreamingcontext object jar list:
https://www.dropbox.com/sh/00sy9mv8qsefwc1/vsEXF0aHsJ
These are algebird jars.

This also contains the algebird scala for streaming uniques:
https://www.dropbox.com/s/ydyn7kd75hhnnpo/Algebird.scala


Mayur Rustagi
Ph: +919632149971
h <https://twitter.com/mayur_rustagi>ttp://www.sigmoidanalytics.com
https://twitter.com/mayur_rustagi




"
Patrick Wendell <pwendell@gmail.com>,"Mon, 27 Jan 2014 10:05:12 -0800",Re: Moving to Typesafe Config?,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey Heiko,

Spark 0.9 introduced a common config class for Spark applications. It
also (initially) supported loading config files in the nested typesafe
format, but this was removed last minute due to a bug. In 1.0 we'll
probably add support for config files, though it may not support
typesafe's tree-style config files because that conflicts with the
naming style of several spark options (we have options where x.y and
x.y.z are both named keys, and the typesafe parser doesn't allow
that).

- Patrick


"
=?GB2312?B?t+u/obfl?= <junfeng.feng@gmail.com>,"Mon, 27 Jan 2014 13:43:37 -0500",Re: [VOTE] Graduation of Apache Spark,dev@spark.incubator.apache.org,"+1

rry
"
Archit Thakur <archit279thakur@gmail.com>,"Tue, 28 Jan 2014 01:04:59 +0530",Re: Problems while moving from 0.8.0 to 0.8.1,dev@spark.incubator.apache.org,"ERROR executor.Executor: Exception in task ID 20
java.lang.NullPointerException
        at
com.xyz.spark.common.collection.MyCustomKeyType.equals(MyCustomKeyType.java:200)
        at
        at
org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:42)
        at
org.apache.spark.rdd.PairRDDFunctions$$anonfun$combineByKey$3.apply(PairRDDFunctions.scala:103)
        at
org.apache.spark.rdd.PairRDDFunctions$$anonfun$combineByKey$3.apply(PairRDDFunctions.scala:102)
        at org.apache.spark.rdd.RDD$$anonfun$3.apply(RDD.scala:465)
        at org.apache.spark.rdd.RDD$$anonfun$3.apply(RDD.scala:465)
        at
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:34)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:237)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:226)
        at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:29)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:237)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:226)
        at
org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:107)
        at org.apache.spark.scheduler.Task.run(Task.scala:53)
        at
org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:215)
        at
org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:50)
        at
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:182)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown
Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown
Source)
        at java.lang.Thread.run(Unknown Source)




"
Aaron Davidson <ilikerps@gmail.com>,"Mon, 27 Jan 2014 11:55:25 -0800",Re: Problems while moving from 0.8.0 to 0.8.1,dev@spark.incubator.apache.org,"Looks like your MyCustomKeyType.equals() method doesn't correctly handle a
null argument. In general, the contract of equals is to return false if
called with a null argument, which this code currently relies on.

This could still be patched for the sake of backwards compatibility with
similarly incomplete equals() methods which previously worked.



"
Archit Thakur <archit279thakur@gmail.com>,"Tue, 28 Jan 2014 01:42:47 +0530",Re: Problems while moving from 0.8.0 to 0.8.1,dev@spark.incubator.apache.org,"
Ok. But what if it is tried to call with null object rather than passing
null.
Code would be internally doing a.equals(b) somewhere where b was null. What
if a happens to be null. Wont it give NPE there itself. Or Does it have a
null check there. I mean why would null be created for MyCustomKeyType,
when it should have some data.
How could MyCustomKeyType be null when I never filled null anywhere.


Agreed.

Thanks and Regards,
Archit Thakur.



"
Aaron Davidson <ilikerps@gmail.com>,"Mon, 27 Jan 2014 12:26:41 -0800",Re: Problems while moving from 0.8.0 to 0.8.1,dev@spark.incubator.apache.org,"The code would not call null.equals(b); only a.equals(null) is allowable
(we check for k.eq(null) at the top).

a.equals(null) returning false is part of the Object.equals()
contract<http://docs.oracle.com/javase/7/docs/api/java/lang/Object.html#equals(java.lang.Object)>,
so it is not invalid to use it, it just may be unintuitive if you expect
never to have null references of MyCustomKeyType hanging around.



"
dachuan <hdc1112@gmail.com>,"Mon, 27 Jan 2014 15:34:25 -0500",Re: real world streaming code,dev@spark.incubator.apache.org,"thanks, Mayur. I will read this workload's code first.






-- 
Dachuan Huang
Cellphone: 614-390-7234
2015 Neil Avenue
Ohio State University
Columbus, Ohio
U.S.A.
43210
"
ipodfans <yaoshengzhe@gmail.com>,"Mon, 27 Jan 2014 19:58:51 -0800 (PST)",Re: Moving to Typesafe Config?,dev@spark.incubator.apache.org,"Hi Patrick,

That sounds great, I just fired a JIRA ticket for this
(https://spark-project.atlassian.net/browse/SPARK-1048). We are setting up
spark in one of our data center (~100 machines) and would like to have this
feature ASAP. Please let me know if there are things we can help.

Thanks
-Shengzhe  



--

"
q q <monoid123@outlook.com>,"Tue, 28 Jan 2014 13:24:50 +0800",Question regarding prepareCommand in WorkerRunnableUtil,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi there,
When reading Spark on YARN code, I have one question regarding prepareCommand in WorkerRunnableUtil:
This function is used to build command to run in executor, why doesn't it follow the same way as ExectorRunner(Standalone mode) where the ""run"" script  is used to run the command?  Is there any special consideration to choose a different way? 
The reason I ask this  question is we have already put some stuff in the run script such as classpath, if we can use the same way, we can just use the same script.
Thanks.

 		 	   		  "
Konstantin Boudnik <cos@apache.org>,"Mon, 27 Jan 2014 21:29:50 -0800",Re: Fwd: Represent your project at ApacheCon,dev@spark.incubator.apache.org,"I have a solid story of our commercial support for Spark & Shark stack for
real-world customer. Did that talk at Spark summit a couple of months. Would
be happy to make it more OSS oriented and present. Would it be of any
interest?

Cos

nd
at

"
"""Mattmann, Chris A (398J)"" <chris.a.mattmann@jpl.nasa.gov>","Tue, 28 Jan 2014 05:35:58 +0000",Re: Represent your project at ApacheCon,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Cos go for it -- if you have the time and energy I personally at least
would welcome your submission to ApacheCon!

Cheers,
Chris






"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 27 Jan 2014 21:40:48 -0800",Re: Represent your project at ApacheCon,dev@spark.incubator.apache.org,"That certainly sounds good. Iím not sure if people at Berkeley or Databricks will go there, but it may make sense. (Basically just not sure how much the audience there is interested in big data.. but perhaps just the ability to hack parallel stuff in Python and Scala will be interesting.)

Matei


for
Would
need
that
last 48
however
httpd and
of
your
exciting
as the
communities that
about
its


"
Chris Mattmann <mattmann@apache.org>,"Mon, 27 Jan 2014 21:43:09 -0800",Re: Represent your project at ApacheCon,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi Matei,

There have been several tracks on Big Data technologies at the past
few ApacheCons and they're definitely worth attending.

Cheers,
Chris





"
Konstantin Boudnik <cos@apache.org>,"Mon, 27 Jan 2014 22:54:33 -0800",Re: Represent your project at ApacheCon,dev@spark.incubator.apache.org,"Thanks for the feedback, Chris. I will submit the proposal tomorrow.

Time is the priceless commodity for all of us, but I think Spark/Shark have
great potentials to be the next generation platform for _all_ parallel and
highly scalable analytics. hence, I am totally ok with spending energy to help
to advance the platform as much as I can. And it is not a tongue-in-a-cheek
statement :)

Cos

g>
or
ed
ts
"
Qiuzhuang Lian <qiuzhuang.lian@gmail.com>,"Tue, 28 Jan 2014 17:15:42 +0800","Re: Any suggestion about JIRA 1006 ""MLlib ALS gets stack overflow
 with too many iterations""?",dev@spark.incubator.apache.org,"I see this error thrown from Executor.scala:

task = ser.deserialize[Task[Any]](taskBytes,
Thread.currentThread.getContextClassLoader)

Any suggestions to break down the task to smaller chunk to avoid this?

Thanks,
Qiuzhuang





"
Cheng Lian <rhythm.mail@gmail.com>,"Tue, 28 Jan 2014 18:49:18 +0800","Re: Any suggestion about JIRA 1006 ""MLlib ALS gets stack overflow
 with too many iterations""?",dev@spark.incubator.apache.org,"We experienced similar problem when implementing LDA on Spark.  Now we call
RDD.checkpoint every 10 iterations to cut the lineage DAG.  Notice that
checkpointing hurts performance since it submits a job to write HDFS.



"
Stephen Haberman <stephen.haberman@gmail.com>,"Tue, 28 Jan 2014 11:01:57 -0600",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc5),dev@spark.incubator.apache.org,"Hi Patrick,


I was going to import this rc5 release into our internal Maven repo to
try it out, but noticed that the version doesn't have ""rc5"" in it.

This means that, if there is an rc6, I'll have to re-import over the same
artifacts, which is generally not a good thing given Maven assumes artifacts
never change.

Is this restriction required by the blessing process, or would it be
possible to sneak rc5 into the pre-final version number?

For now, I'll just build a local version, at the same commit, but with
the as 0.9.0-incubating-rc5.

Apologies if this was discussed before and I just missed it.

- Stephen


"
Eduardo Costa Alfaia <e.costaalfaia@unibs.it>,"Tue, 28 Jan 2014 15:21:01 +0100",Re: Print in JavaNetworkWordCount,dev@spark.incubator.apache.org,"Hi Tathagata,

This code that you have sent me is it a scala code?

yourDStream.foreachRDD(rdd => {

   // Get and print first n elements
   val firstN = rdd.take(n)
   println(""First N elements = "" + firstN)

  // Count the number of elements in each batch
  println(""RDD has "" + rdd.count() + "" elements"")

})

Thanks



Il giorno 20 gennaio 2014 19:11, Tathagata Das
<tathagata.das1565@gmail.com>ha scritto:

i
i

-- 
---
INFORMATIVA SUL TRATTAMENTO DEI DATI PERSONALI

I dati utilizzati per l'invio del presente messaggio sono trattati 
dall'Universit‡ degli Studi di Brescia esclusivamente per finalit‡ 
istituzionali. Informazioni pi˘ dettagliate anche in ordine ai diritti 
dell'interessato sono riposte nell'informativa generale e nelle notizie 
pubblicate sul sito web dell'Ateneo nella sezione ""Privacy"".

Il contenuto di questo messaggio Ë rivolto unicamente alle persona cui 
Ë indirizzato e puÚ contenere informazioni la cui riservatezza Ë 
tutelata legalmente. Ne sono vietati la riproduzione, la diffusione e l'uso 
in mancanza di autorizzazione del destinatario. Qualora il messaggio 
fosse pervenuto per errore, preghiamo di eliminarlo.
"
Patrick Wendell <pwendell@gmail.com>,"Tue, 28 Jan 2014 12:45:52 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc5),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey Stephen,

Yes this runs afoul of good practice in Maven where a given version
shouldn't be re-used. As far as I understand though, it is required by
the way the Apache release process works.

The artifacts and repository content that get voted on need to exactly
match the final release. So we can't hold a vote on a version of the
code where everything says ""-rcx"", then we go back and change the
source code and do a second push to maven with code that doesn't have
an ""-rcx"" suffix. This would effectively change the code that is being
released.

I was thinking as a work around that maybe we could publish a second
set of staging artifacts that are versioned with -rcX for people to
test against. I think as long as we make it clear that these are not
the ""official artifacts"" being voted on it might be okay. I'm not
totally sure if this is allowed though.

- Patrick


"
Tathagata Das <tathagata.das1565@gmail.com>,"Tue, 28 Jan 2014 13:36:53 -0800",Re: Print in JavaNetworkWordCount,dev@spark.incubator.apache.org,"Yes, it was my intention to write scala code. But I may have failed to
write a correct one that compiles. Apologies.

Also, something to keep in mind. This is the dev mailing for Spark
developers. Questions related to using Spark should be sent to
user@spark.incubator.apache.org

TD



2014/1/28 Eduardo Costa Alfaia <e.costaalfaia@unibs.it>

on
f
‡
tti
ie
cui
Ë
so
"
Patrick Wendell <pwendell@gmail.com>,"Tue, 28 Jan 2014 13:42:33 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc5),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I'll add my own +1.


"
Stephen Haberman <stephen.haberman@gmail.com>,"Tue, 28 Jan 2014 16:20:15 -0600",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc5),dev@spark.incubator.apache.org,"

Okay, that makes sense. I was assuming that was the restriction.


I think that would be really helpful. For scala, the rc/milestone
releases are even pushed to Maven central, although I understand if
that's not worth doing for Spark.

- Stephen


"
Eduardo Costa Alfaia <e.costaalfaia@unibs.it>,"Tue, 28 Jan 2014 23:56:47 +0100",Re: Print in JavaNetworkWordCount,dev@spark.incubator.apache.org,"Hi Tathagata, doesn't worry I am looking for a manner in the source code of
JavaNetworkWordcount print me in console the sum of the total of words in a
file, not one word by line.

Thanks


Il giorno 28 gennaio 2014 22:36, Tathagata Das
<tathagata.das1565@gmail.com>ha scritto:

nt
‡
a
Ë
 e
o
i
i

-- 
---
INFORMATIVA SUL TRATTAMENTO DEI DATI PERSONALI

I dati utilizzati per l'invio del presente messaggio sono trattati 
dall'Universit‡ degli Studi di Brescia esclusivamente per finalit‡ 
istituzionali. Informazioni pi˘ dettagliate anche in ordine ai diritti 
dell'interessato sono riposte nell'informativa generale e nelle notizie 
pubblicate sul sito web dell'Ateneo nella sezione ""Privacy"".

Il contenuto di questo messaggio Ë rivolto unicamente alle persona cui 
Ë indirizzato e puÚ contenere informazioni la cui riservatezza Ë 
tutelata legalmente. Ne sono vietati la riproduzione, la diffusione e l'uso 
in mancanza di autorizzazione del destinatario. Qualora il messaggio 
fosse pervenuto per errore, preghiamo di eliminarlo.
"
Tathagata Das <tathagata.das1565@gmail.com>,"Tue, 28 Jan 2014 15:09:27 -0800",Re: Print in JavaNetworkWordCount,dev@spark.incubator.apache.org,"Something like maybe. From this example -
https://github.com/tdas/incubator-spark/blob/recoverable-example-fix/examples/src/main/java/org/apache/spark/streaming/examples/JavaRecoverableWordCount.java


      wordCounts.foreachRDD(new Function2<JavaPairRDD<String,
Integer>, Time, Void>() {

        @Override public Void call(JavaPairRDD<String, Integer>
pairRDD, Time time) throws Exception {

          String counts = ""Counts at time "" + time + "" ["" +
Joiner.on("", "").join(pairRDD.collect())  + ""]"";

          System.out.println(counts);

          System.out.println(""Appending to "" + outputFile.getAbsolutePath());

          Files.append(counts + ""\n"", outputFile, Charset.defaultCharset());

          return null;

        }
      });

Note that Joiner is a class from Google Commons library. Not integral
to what you want to do.


TD


2014/1/28 Eduardo Costa Alfaia <e.costaalfaia@unibs.it>

of
 a
al
i
it‡
ona
a Ë
‡
tti
ie
cui
Ë
so
"
Evan Chan <ev@ooyala.com>,"Tue, 28 Jan 2014 17:02:08 -0800","Re: Any suggestion about JIRA 1006 ""MLlib ALS gets stack overflow
 with too many iterations""?","""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","By the way, is there any plan to make a pluggable backend for
checkpointing?   We might be interested in writing a, for example,
Cassandra backend.




-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

"
Matei Zaharia <matei.zaharia@gmail.com>,"Tue, 28 Jan 2014 23:10:10 -0800","Re: Any suggestion about JIRA 1006 ""MLlib ALS gets stack overflow with too many iterations""?",dev@spark.incubator.apache.org,"That would be great to add. Right now it would be easy to change it to use another Hadoop FileSystem implementation at the very least (I think you can just pass the URL for that), but for Cassandra youíd have to use a different InputFormat or some direct Cassandra access API.

Matei


checkpointing the RDDs every 10-20 iterations to break the lineage chain, but checkpointing currently requires HDFS installed, which not all users will have.


"
Jason Dai <jason.dai@gmail.com>,"Thu, 30 Jan 2014 09:09:06 +0800",Re: [VOTE] Graduation of Apache Spark,dev@spark.incubator.apache.org,"+1



E
t.
uation
g.
ôll
"
=?UTF-8?Q?Stevo_Slavi=C4=87?= <sslavic@gmail.com>,"Thu, 30 Jan 2014 08:15:14 +0100",Re: [VOTE] Graduation of Apache Spark,dev@spark.incubator.apache.org,"+1



2
aduation
Äôll
it
e
"
Heiko Braun <ike.braun@googlemail.com>,"Thu, 30 Jan 2014 08:28:44 +0100",Re: [VOTE] Graduation of Apache Spark,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","

+1



uation
ôll


"
Xia Zhu <xia.zhu@gmail.com>,"Wed, 29 Jan 2014 23:56:45 -0800",Re: [VOTE] Graduation of Apache Spark,dev@spark.incubator.apache.org,"+1


te:

:
72
raduation
Äôll
e
he
"
Manoj Awasthi <awasthi.manoj@gmail.com>,"Thu, 30 Jan 2014 16:17:53 +0530",Re: [VOTE] Graduation of Apache Spark,dev@spark.incubator.apache.org,"+1

----
*Proud to be a donor of the AAP. Join the movement today.
**http://donate.aamaadmiparty.org
<http://messenger.aamaadmiparty.org/l/cbfetvjEQCcn892763mMmlNiaw/9KS50hXPn6DRjLRWMOJzkw/M4r8D763LgDtF7f16PFYqQeA>*
the dreamers of the day are dangerous m"
Eduardo Costa Alfaia <e.costaalfaia@unibs.it>,"Thu, 30 Jan 2014 17:12:06 +0100",Source code JavaNetworkWordcount,user@spark.incubator.apache.org,"Hi Guys,

I'm not very good like java programmer, so anybody could me help with this
code piece from JavaNetworkWordcount:

JavaPairDStream<String, Integer> wordCounts = words.map(
        new PairFunction<String, String, Integer>() {
     @Override
          public Tuple2<String, Integer> call(String s) throws Exception {
            return new Tuple2<String, Integer>(s, 1);
          }
        }).reduceByKey(new Function2<Integer, Integer, Integer>() {
          @Override
          public Integer call(Integer i1, Integer i2) throws Exception {
            return i1 + i2;
          }
        });

      JavaPairDStream<String, Integer> counts =
wordCounts.reduceByKeyAndWindow(
        new Function2<Integer, Integer, Integer>() {
          public Integer call(Integer i1, Integer i2) { return i1 + i2; }
        },
        new Function2<Integer, Integer, Integer>() {
          public Integer call(Integer i1, Integer i2) { return i1 - i2; }
        },
        new Duration(60 * 5 * 1000),
        new Duration(1 * 1000)
      );

I would like to think a manner of counting and after summing  and getting a
total from words counted in a single file, for example a book in txt
extension Don Quixote. The counts function give me the resulted from each
word has found and not a total of words from the file.
Tathagata has sent me a piece from scala code, Thanks Tathagata by your
attention with my posts I am very thankfully,

  yourDStream.foreachRDD(rdd => {

   // Get and print first n elements
   val firstN = rdd.take(n)
   println(""First N elements = "" + firstN)

  // Count the number of elements in each batch
  println(""RDD has "" + rdd.count() + "" elements"")

})

yourDStream.count.print()

Could anybody help me?


Thanks Guys

-- 
INFORMATIVA SUL TRATTAMENTO DEI DATI PERSONALI

I dati utilizzati per l'invio del presente messaggio sono trattati 
dall'Universit‡ degli Studi di Brescia esclusivamente per finalit‡ 
istituzionali. Informazioni pi˘ dettagliate anche in ordine ai diritti 
dell'interessato sono riposte nell'informativa generale e nelle notizie 
pubblicate sul sito web dell'Ateneo nella sezione ""Privacy"".

Il contenuto di questo messaggio Ë rivolto unicamente alle persona cui 
Ë indirizzato e puÚ contenere informazioni la cui riservatezza Ë 
tutelata legalmente. Ne sono vietati la riproduzione, la diffusione e l'uso 
in mancanza di autorizzazione del destinatario. Qualora il messaggio 
fosse pervenuto per errore, preghiamo di eliminarlo.
"
Evan Chan <ev@ooyala.com>,"Thu, 30 Jan 2014 11:08:12 -0800",ApacheCon,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I might have missed it earlier, but is anybody planning to present at
ApacheCon?  I think it's in Denver this year, April 7-9.

Thinking of submitting a talk about how we use Spark and Cassandra.

-Evan


-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

"
Ted Yu <yuzhihong@gmail.com>,"Thu, 30 Jan 2014 11:11:02 -0800",Re: ApacheCon,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","There is one proposal on Spark:
http://events.linuxfoundation.org/cfp/cfp-list?&&&&page=1#overlay=cfp/proposals/1461



"
Ameet Kini <ameetkini@gmail.com>,"Thu, 30 Jan 2014 14:12:25 -0500",rough date for spark summit 2014,dev@spark.incubator.apache.org,"I know this is still a few months off and folks are rushing towards 0.9
release, but do the devs have a rough date for Spark Summit 2014? Looks
like it'll be in summer, but is it Jun / July / Aug / Sep ? Even
""late-summer"" would help.

Summer being a popular vacation time, a few months advance notice would be
greatly appreciated (read: I missed last summit due to a pre-scheduled
vacation and would hate to miss this one :)

Thanks,
Ameet
"
Tathagata Das <tathagata.das1565@gmail.com>,"Thu, 30 Jan 2014 11:15:06 -0800",Re: Source code JavaNetworkWordcount,dev@spark.incubator.apache.org,"Let me first ask for a few clarifications.

1. If you just want to count the words in a single text file like Don
Quixote (that is, not for a stream of data), you should use only Spark.
Then the program to count the frequency of words in a text file would look
like this in Java. If you are not super-comfortable with Java, then I
strongly recommend using the Scala API or pyspark. For scala, it may be a
little trickier to learn if you have absolutely no idea. But it is worth
it. The frequency count would look like this.

val sc = new SparkContext(...)
val linesInFile = sc.textFile(""path_to_file"")
val words = linesInFile.flatMap(line => line.split("" ""))
val frequencies = words.map(word => (word, 1L)).reduceByKey(_ + _)
println(""Word frequencies = "" + frequences.collect())      // collect is
costly if the file is large


2. Let me assume that you want to do read a stream of text over the network
and then print the count of total number of words into a file. Note that it
is ""total number of words"" and not ""frequency of each word"". The Java
version would be something like this.

DStream<Integer> totalCounts = words.count();

totalCounts.foreachRDD(new Function2<JavaRDD<Long>, Time, Void>() {
   @Override public Void call(JavaRDD<Long> pairRDD, Time time) throws
Exception {
           Long totalCount = totalCounts.first();

           // print to screen
           System.out.println(totalCount);

          // append count to file
          ...
          return null;
    }
})

This is count how many words have been received in each batch. The Scala
version would be much simpler to read.

words.count().foreachRDD(rdd => {
    val totalCount = rdd.first()

    // print to screen
    println(totalCount)

    // append count to file
    ...
})

Hope this helps! I apologize if the code doesnt compile, I didnt test for
syntax and stuff.

TD




s
{
 a
so
"
Henry Saputra <henry.saputra@gmail.com>,"Thu, 30 Jan 2014 11:20:01 -0800",Re: ApacheCon,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I believe Cos was planning to submit one about Spark and Shark in real
prod. Similar to what he did for Spark summit.

But more talks are better =)

- Henry


"
bkrouse <brian@houseofkrouse.com>,"Thu, 30 Jan 2014 11:47:30 -0800 (PST)",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc5),dev@spark.incubator.apache.org,"I just tried the EC2 scripts as a part of this rc5, and it *looks* like it
did not setup this version properly.  Is that in scope for this rc?

Brian



--

"
Reynold Xin <rxin@databricks.com>,"Thu, 30 Jan 2014 12:10:12 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc5),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Thanks. That does go out of the scope of the Spark release. The EC2 script
starts instances and use some scripts to setup this version. For that to
work, we need to have a release first.



"
Konstantin Boudnik <cos@apache.org>,"Thu, 30 Jan 2014 13:33:51 -0800",Re: ApacheCon,dev@spark.incubator.apache.org,"Yes, I did a couple of days ago. And I will tweak it to be more technical than
@spark-summit, cause I hope the audience will more development oriented. 

I agree that more the merrier though!
  Cos


"
Patrick Wendell <pwendell@gmail.com>,"Thu, 30 Jan 2014 13:47:08 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc5),dev@spark.incubator.apache.org,"They won't work until after the release is finalized.

---
sent from my phone

"
Pillis W <pillis.work@gmail.com>,"Fri, 31 Jan 2014 07:41:57 -0800",Re: About Spark job web ui persist(JIRA-969),dev@spark.incubator.apache.org,"I have attached a proposal design document to the SPARK-969 JIRA for
discussion.
If not assigned to anyone, I would be interested in implementing it.
Best regards,
pillis





"
