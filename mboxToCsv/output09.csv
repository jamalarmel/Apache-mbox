Matei Zaharia <matei.zaharia@gmail.com>,"Fri, 31 Jan 2014 17:59:21 -0800",[RESULT][VOTE] Graduation of Apache Spark,dev@spark.incubator.apache.org,"Thanks everyone for voting! With 37 +1s (three from the IPMC and 23 from the PPMC) and no -1s, the resolution passes. I’ve added a tally of the votes below:

+1
Matei Zaharia
Reynold Xin
Tathagata Das
Sean McNamara
Patrick Wendell
Mark Hamstra
Chris Mattmann *
Tom Graves
Henry Saputra *
Andy Konwinski
Josh Rosen
Mosharaf Chowdhury
Mridul Muralidharan
Nick Pentreath
Andrew Xia
Haoyuan Li
Sandy Ryza
Sebastian Schelter *
Kostas Sakellis
Christopher Nguyen
Aaron Davidson
Shivaram Venkataraman
Kay Ousterhout
Evan Sparks
Xuefeng Wu
Konstantin Boudnik
Rahul Chugh
Prashant Sharma
Stephen Haberman
Prabeesh K.
Saisai Shao
Junfeng Feng
Jason Dai
Stevo Slavic
Heiko Braun
Xia Zhu
Manoj Awasthi

+0
(none)

-1
(none)

* indicates IPMC member

The next step will be to send the vote to general@incubator.apache.org — I’ll do that momentarily.


Matei




VOTE for the graduation of Apache Spark (incubating) into a top level project. If this VOTE is successful, then I'll call an Incubator PMC VOTE in 72 hours, and if that is successful, we’ll submit the project graduation resolution below into the board agenda for the next Apache board meeting.
thread. If you see your name there is no need to VOTE again and I’ll carry through the VOTE as below. If you want to change your VOTE, or I got it wrong, let me know and we'll change it.
Incubator. I'll try and close the VOTE on Wednesday and then start the Incubator PMC VOTE on general@incubator.apache.org.
resolution below.
because..


"
Suresh Marru <smarru@apache.org>,"Fri, 31 Jan 2014 22:37:06 -0500",Re: [RESULT][VOTE] Graduation of Apache Spark,dev@spark.incubator.apache.org,"+ 1. 

Congratulations to the community for a quick graduation. Sorry I missed the action coming of an extended break. If you would like, please count my + 1 (IPMC) on the PPMC thread, if not I will vote on the general thread.

Suresh


from the PPMC) and"
"""C. Ross Jam"" <crossjam@crossjam.net>","Sat, 1 Feb 2014 10:47:39 -0500",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc5),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Curious lurker here. Did this vote close successfully? Should I wait for an
official 0.9 release?

Cheers!


"
Kapil Malik <kmalik@adobe.com>,"Sat, 1 Feb 2014 15:57:20 +0000",RE: [VOTE] Release Apache Spark 0.9.0-incubating (rc5),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","+1 for Q !
Have been monitoring this thread from past 3 weeks in anticipation :)
Any tentative dates for official 0.9 release ?

Kapil Malik | kmalik@adobe.com | 33430 / 8800836581

 official 0.9 release?

Cheers!



"
Kapil Malik <kmalik@adobe.com>,"Sat, 1 Feb 2014 15:59:01 +0000",RE: [VOTE] Release Apache Spark 0.9.0-incubating (rc5),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Sent too early ... 1 week* (maybe I refreshed too fast)

Have been monitoring this thread from past 3 weeks in anticipation :) Any tentative dates for official 0.9 release ?

Kapil Malik | kmalik@adobe.com | 33430 / 8800836581

 official 0.9 release?

Cheers!



"
=?UTF-8?Q?Stevo_Slavi=C4=87?= <sslavic@gmail.com>,"Sat, 1 Feb 2014 17:02:34 +0100",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc5),dev@spark.incubator.apache.org,"Apache Spark 0.9.0 artifacts are on Maven central repo (see
http://central.maven.org/maven2/org/apache/spark/spark-core_2.10/0.9.0-incubating/)

Kind regards,
Stevo Slavic



"
Kapil Malik <kmalik@adobe.com>,"Sat, 1 Feb 2014 16:19:38 +0000",RE: [VOTE] Release Apache Spark 0.9.0-incubating (rc5),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi Stevo,
Thanks for the link. Indeed, different versions are available on maven repository which I can clone/sync for development purposes. But I'm more confident about official release version when deploying to a cluster which is used by multiple people.
Hence curious about date for 0.9 official release.

Thanks and regards,

Kapil

http://central.maven.org/maven2/org/apache/spark/spark-core_2.10/0.9.0-incubating/)

Kind regards,
Stevo Slavic




"
Jey Kottalam <jey@cs.berkeley.edu>,"Sat, 1 Feb 2014 12:51:28 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc5),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi Kapil,

It looks to me like the artifacts in Maven are the official 0.9.0
release, though the website has not yet been updated. The IPMC
approved RC5 as of yesterday:

https://mail-archives.apache.org/mod_mbox/incubator-general/201401.mbox/<CABPQxstJm+pO7_22bDYBqXK90ZSy3PnxPpFT87-9Xdff98u6QA@mail.gmail.com>

-Jey

pository which I can clone/sync for development purposes. But I'm more confident about official release version when deploying to a cluster which is used by multiple people.
cubating/)

"
Henry Saputra <henry.saputra@gmail.com>,"Sat, 1 Feb 2014 15:17:11 -0800",Fwd: [RESULT] [VOTE] Release Apache Spark 0.9.0-incubating (rc5),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","FYI


---------- Forwarded message ----------
From: *Patrick Wendell* <pwendell@gmail.com>
Date: Thursday, January 30, 2014
Subject: [RESULT] [VOTE] Release Apache Spark 0.9.0-incubating (rc5)
To: general@incubator.apache.org


Voting is now closed. This vote passes with 3 IPCM +1 votes and no 0
or -1 votes. Thank you to everyone who voted. Totals:

+1:
Matei Zaharia
Andy Konwinski
Henry Saputra*
Patrick Wendell
Patrick Hunt*

0:

-1:

* = binding

Thanks to all those who voted.

- Patrick

;>>
t:;>>
pt:;>>
avascript:;>>
https://git-wip-us.apache.org/repos/asf?p=incubator-spark.git;a=commit;h=95d28ff3d0d20d9c583e184f9e2c5ae842d8a4d9
/
!
cript:;>
ascript:;>
pt:;>
ript:;>

---------------------------------------------------------------------
:;>
pt:;>
"
Matei Zaharia <matei.zaharia@gmail.com>,"Sat, 1 Feb 2014 18:39:05 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc5),"dev@spark.incubator.apache.org,
 ""jey@cs.berkeley.edu Kottalam"" <jey@cs.berkeley.edu>","Yup, weâ€™re still working on putting it on the website, but this is the final release. You can download the RC5 artifacts from http://apache-spark-developers-list.1001551.n3.nabble.com/VOTE-Release-Apache-Spark-0-9-0-incubating-rc5-td318.html.

Matei


https://mail-archives.apache.org/mod_mbox/incubator-general/201401.mbox/<CABPQxstJm+pO7_22bDYBqXK90ZSy3PnxPpFT87-9Xdff98u6QA@mail.gmail.com>
maven repository which I can clone/sync for development purposes. But I'm more confident about official release version when deploying to a cluster which is used by multiple people.
http://central.maven.org/maven2/org/apache/spark/spark-core_2.10/0.9.0-incubating/)
:)


"
Kapil Malik <kmalik@adobe.com>,"Sun, 2 Feb 2014 04:08:06 +0000",RE: [VOTE] Release Apache Spark 0.9.0-incubating (rc5),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>,
	""jey@cs.berkeley.edu Kottalam"" <jey@cs.berkeley.edu>","Awesome ! Thanks everyone :)

-----Original Message-----
From: Matei Zaharia [mailto:matei.zaharia@gmail.com] 
Sent: 02 February 2014 08:09
To: dev@spark.incubator.apache.org; jey@cs.berkeley.edu Kottalam
Subject: Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc5)

Yup, weâ€™re still working on putting it on the website, but this is the final release. You can download the RC5 artifacts from http://apache-spark-developers-list.1001551.n3.nabble.com/VOTE-Release-Apache-Spark-0-9-0-incubating-rc5-td318.html.

Matei

On Feb 1, 2014, at 12:51 PM, Jey Kottalam <jey@cs.berkeley.edu> wrote:

> Hi Kapil,
> 
> It looks to me like the artifacts in Maven are the official 0.9.0 
> release, though the website has not yet been updated. The IPMC 
> approved RC5 as of yesterday:
> 
> https://mail-archives.apache.org/mod_mbox/incubator-general/201401.mbo
> x/<CABPQxstJm+pO7_22bDYBqXK90ZSy3PnxPpFT87-9Xdff98u6QA@mail.gmail.com>
> 
> -Jey
> 
> On Sat, Feb 1, 2014 at 8:19 AM, Kapil Malik <kmalik@adobe.com> wrote:
>> Hi Stevo,
>> Thanks for the link. Indeed, different versions are available on maven repository which I can clone/sync for development purposes. But I'm more confident about official release version when deploying to a cluster which is used by multiple people.
>> Hence curious about date for 0.9 official release.
>> 
>> Thanks and regards,
>> 
>> Kapil
>> 
>> -----Original Mecom]
>> Sent: 01 February 2014 21:33
>> To: dev@spark.incubator.apache.org
>> Subject: Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc5)
>> 
>> Apache Spark 0.9.0 artifacts are on Maven central repo (see
>> http://central.maven.org/maven2/org/apache/spark/spark-core_2.10/0.9.
>> 0-incubating/)
>> 
>> Kind regards,
>> Stevo Slavic
>> 
>> 
>> On Sat, Feb 1, 2014 at 4:59 PM, Kapil Malik <kmalik@adobe.com> wrote:
>> 
>>> Sent too early ... 1 week* (maybe I refreshed too fast)
>>> 
>>> -----Original Message-----
>>> From: Kapil Malik
>>> Sent: 01 February 2014 21:27
>>> To: dev@spark.incubator.apache.org
>>> Subject: RE: [VOTE] Release Apache Spark 0.9.0-incubating (rc5)
>>> 
>>> +1 for Q !
>>> Have been monitoring this thread from past 3 weeks in anticipation 
>>> :) Any tentative dates for official 0.9 release ?
>>> 
>>> Kapil Malik | kmalik@adobe.com | 33430 / 8800836581
>>> 
>>> -----Original Message-----
>>> From: C. Ross Jam [mailto:crossjam@crossjam.net]
>>> Sent: 01 February 2014 21:18
>>> To: dev@spark.incubator.apache.org
>>> Subject: Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc5)
>>> 
>>> Curious lurker here. Did this vote close successfully? Should I wait 
>>> for an official 0.9 release?
>>> 
>>> Cheers!
>>> 
>>> On Friday, January 24, 2014, Patrick Wendell <pwendell@gmail.com> wrote:
>>> 
>>>> Please vote on releasing the following candidate as Apache Spark
>>>> (incubating) version 0.9.0.
>>>> 
>>>> 
>>> 

"
Patrick Wendell <pwendell@gmail.com>,"Sun, 2 Feb 2014 12:21:32 -0800",Re: [VOTE] Release Apache Spark 0.9.0-incubating (rc5),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","It takes a day or two to package the release pass votes and is cut to
maven. Coming soon!

s the final release. You can download the RC5 artifacts from http://apache-spark-developers-list.1001551.n3.nabble.com/VOTE-Release-Apache-Spark-0-9-0-incubating-rc5-td318.html.
repository which I can clone/sync for development purposes. But I'm more confident about official release version when deploying to a cluster which is used by multiple people.
e:

"
Patrick Wendell <pwendell@gmail.com>,"Mon, 3 Feb 2014 00:49:25 -0800",Spark 0.9.0 Released,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi Everyone,

We've just posted Spark 0.9.0, a major release with several new
features and improvements. 0.9.0 is Spark's largest release ever, with
contributions from 83 developers. This release expands Spark's
standard libraries, introducing a new graph computation package
(GraphX) and adding several new features to the machine learning and
stream-processing packages. It also makes major improvements to the
core engine, including external aggregations, a simplified H/A mode
for long lived applications, and hardened YARN support.

The full release notes are at:
http://spark.incubator.apache.org/releases/spark-release-0-9-0.html

You can download the release at:
http://spark.incubator.apache.org/downloads

Thanks to the following people who contributed to this release:
Andrew Ash, Pierre Borckmans, Russell Cardullo, Evan Chan, Vadim
Chekan, Lian Cheng, Ewen Cheslack-Postava, Mosharaf Chowdhury, Dan
Crankshaw, Haider Haidi, Frank Dai, Tathagata Das, Ankur Dave, Henry
Davidge, Aaron Davidson, Kyle Ellrott, Hossein Falaki, Harvey Feng,
Ali Ghodsi, Joseph E. Gonzalez, Thomas Graves, Rong Gu, Stephen
Haberman, Walker Hamilton, Mark Hamstra, Damien Hardy, Nathan Howell,
Grace Huang, Shane Huang, Prabeesh K, Holden Karau, KarthikTunga,
Grega Kespret, Marek Kolodziej, Jey Kottalam, Du Li, Haoyuan Li,
LiGuoqiang, Raymond Liu, George Loentiev, Akihiro Matsukawa, David
McCauley, Mike, Fabrizio (Misto) Milo, Mridul Muralidharan, Tor
Myklebust, Sundeep Narravula, Binh Nguyen, Adam Novak, Andrew Or, Kay
Ousterhout, Sean Owen, Nick Pentreath, Pillis, Imran Rashid, Ahir
Reddy, Luca Rosellini, Josh Rosen, Henry Saputra, Andre Schumacher,
Jerry Shao, Prashant Sharma, Shiyun, Wangda Tan, Matthew Taylor,
Jyun-Fan Tsai, Takuya Ueshin, Shivaram Venkataraman, Jianping J Wang,
Martin Weindel, Patrick Wendell, Neal Wiggins, Andrew Xia, Reynold
Xin, Dong Yan, Haitao Yao, Xusen Yin, Fengdong Yu, Matei Zaharia, Wu
Zeming, and Nan Zhu

- Patrick

"
Sourav Chandra <sourav.chandra@livestream.com>,"Mon, 3 Feb 2014 15:05:00 +0530",Re: Spark 0.9.0 Released,dev@spark.incubator.apache.org,"Hi Patrick,

Congrats all for the release.

The download link is broken. Maybe its under transition mode

Thanks,
Sourav






-- 

Sourav Chandra

Senior Software Engineer

· · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · ·

sourav.chandra@livestream.com

o: +91 80 4121 8723

m: +91 988 699 3746

skype: sourav.chandra

Livestream

""Ajmera Summit"", First Floor, #3/D, 68 Ward, 3rd Cross, 7th C Main, 3rd
Block, Koramangala Industrial Area,

Bangalore 560034

www.livestream.com
"
Kapil Malik <kmalik@adobe.com>,"Mon, 3 Feb 2014 10:29:03 +0000",RE: Spark 0.9.0 Released,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi Sourav,



I think the problem is with link structure. Please try correcting the version number in the directory path as well -

For example, when you visit http://spark.incubator.apache.org/downloads.html ,

then for a mirror link like - http://<mirror>/incubator/spark/spark-0.8.1-incubating/spark-0.9.0-incubating.tgz<http://%3cmirror%3e/incubator/spark/spark-0.8.1-incubating/spark-0.9.0-incubating.tgz>  which doesn't work,

try http://<mirror>/incubator/spark/spark-0.9.0-incubating/spark-0.9.0-incubating.tgz<http://%3cmirror%3e/incubator/spark/spark-0.9.0-incubating/spark-0.9.0-incubating.tgz>



Regards,



Kapil Malik | kmalik@adobe.com<mailto:kmalik@adobe.com> | 33430 / 8800836581




Hi Patrick,



Congrats all for the release.



The download link is broken. Maybe its under transition mode



Thanks,

Sourav





















































--



Sourav Chandra



Senior Software Engineer



* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *



sourav.chandra@livestream.com<mailto:sourav.chandra@livestream.com>



o: +91 80 4121 8723



m: +91 988 699 3746



skype: sourav.chandra



Livestream



""Ajmera Summit"", First Floor, #3/D, 68 Ward, 3rd Cross, 7th C Main, 3rd Block, Koramangala Industrial Area,



Bangalore 560034



www.livestream.com<http://www.livestream.com>
"
Sourav Chandra <sourav.chandra@livestream.com>,"Mon, 3 Feb 2014 16:03:40 +0530",Re: Spark 0.9.0 Released,dev@spark.incubator.apache.org,"Hi Kapil,

I tried that and at that time spark-0.9xxx folder was only not created in
the mirrored links. if I do this
http://apache.mirrors.tds.net/incubator/spark/

Now I can see the new folder is created.

Anyway thanks for reminding me to check the link once again and find its
created now :)

Happy Sparking :)

Thanks,
Sourav



z<http://%3cmirror%3e/incubator/spark/spark-0.8.1-incubating/spark-0.9.0-incubating.tgz>
z<http://
g.tgz>



-- 

Sourav Chandra

Senior Software Engineer

· · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · ·

sourav.chandra@livestream.com

o: +91 80 4121 8723

m: +91 988 699 3746

skype: sourav.chandra

Livestream

""Ajmera Summit"", First Floor, #3/D, 68 Ward, 3rd Cross, 7th C Main, 3rd
Block, Koramangala Industrial Area,

Bangalore 560034

www.livestream.com
"
Kapil Malik <kmalik@adobe.com>,"Mon, 3 Feb 2014 10:42:24 +0000",RE: Spark 0.9.0 Released,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Oh I see. Perils of being too fast :)
You too !

Regards,

Kapil Malik


I tried that and at that time spark-0.9xxx folder was only not created in the mirrored links. if I do this http://apache.mirrors.tds.net/incubator/spark/

Now I can see the new folder is created.

Anyway thanks for reminding me to check the link once again and find its created now :)

Happy Sparking :)

Thanks,
Sourav






-- 

Sourav Chandra

Senior Software Engineer

* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *

sourav.chandra@livestream.com

o: +91 80 4121 8723

m: +91 988 699 3746

skype: sourav.chandra

Livestream

""Ajmera Summit"", First Floor, #3/D, 68 Ward, 3rd Cross, 7th C Main, 3rd Block, Koramangala Industrial Area,

Bangalore 560034

www.livestream.com

"
Patrick Wendell <pwendell@gmail.com>,"Mon, 3 Feb 2014 10:59:45 -0800",Re: Spark 0.9.0 Released,dev@spark.incubator.apache.org,"The link delay was because of propagation time to the mirror network -
should be okay now!


"
Henry Saputra <henry.saputra@gmail.com>,"Mon, 3 Feb 2014 11:24:41 -0800",Not closing the merged PRs anymore from Spark github mirror?,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Seems like some merged PRs by Reynold and Patrick did not close the PR
automatically anymore?

- Henry

"
Reynold Xin <rxin@databricks.com>,"Mon, 3 Feb 2014 11:28:01 -0800",Re: Not closing the merged PRs anymore from Spark github mirror?,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","It was a transient thing. There's a script that we are using to
automatically fetch diffs from a PR and apply the diff against the git
repo. Patrick changed the way it works last week, and a regression there
was PRs are no longer closed automatically.

I believe he has fixed it. Patrick will also write an email about the
details of that script soon.






"
Henry Saputra <henry.saputra@gmail.com>,"Mon, 3 Feb 2014 11:31:38 -0800",Re: Not closing the merged PRs anymore from Spark github mirror?,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Ah thanks for the info, Reynold!

- Henry


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Mon, 3 Feb 2014 13:01:54 -0800",Re: Spark 0.9.0 Released,dev@spark.incubator.apache.org,"Hey Patrick

FYI the link is still broken for the Apache mirrors in the website.
For example I get pointed to
http://www.eng.lsu.edu/mirrors/apache/incubator/spark/spark-0.8.1-incubating/spark-0.9.0-incubating.tgz
instead of
http://www.eng.lsu.edu/mirrors/apache/incubator/spark/spark-0.9.0-incubating/spark-0.9.0-incubating.tgz

Thanks
Shivaram


"
Patrick Wendell <pwendell@gmail.com>,"Mon, 3 Feb 2014 13:25:30 -0800",Re: Spark 0.9.0 Released,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>, 
	Shivaram Venkataraman <shivaram@eecs.berkeley.edu>","Ah thanks @shivaram. Check now?


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Mon, 3 Feb 2014 13:39:27 -0800",Re: Spark 0.9.0 Released,Patrick Wendell <pwendell@gmail.com>,"Works fine now.

Shivaram


"
Eduardo Costa Alfaia <e.costaalfaia@unibs.it>,"Tue, 4 Feb 2014 00:19:57 +0100",Re: Source code JavaNetworkWordcount,dev@spark.incubator.apache.org,"Hi Tathagata,

You were right when you have said for me to use scala against java, scala
is very easy. I have implemented that code you have given (in bold), but I
have implemented also an union function(in red) because I am testing with 2
stream sources, my idea is putting 3 or more stream sources and doing the
union.

object NetworkWordCount {
 37   def main(args: Array[String]) {
 38     if (args.length < 1) {
 39       System.err.println(""Usage: NetworkWordCount <master> <hostname>
<port>\n"" +
 40         ""In local mode, <master> should be 'local[n]' with n > 1"")
 41       System.exit(1)
 42     }
 43
 44     StreamingExamples.setStreamingLogLevels()
 45
 46     // Create the context with a 1 second batch size
 47     val ssc = new StreamingContext(args(0), ""NetworkWordCount"",
Seconds(1),
 48       System.getenv(""SPARK_HOME""),
StreamingContext.jarOfClass(this.getClass))
 49         ssc.checkpoint(""hdfs://computer22:54310/user/root/INPUT"")
 50     // Create a socket text stream on target ip:port and count the
 51     // words in the input stream of \n delimited text (eg. generated by
'nc')
 52     *val lines1 = ssc.socketTextStream(""localhost"", ""12345"".toInt,
StorageLevel.MEMORY_ONLY_SER)*
* 53     val lines2 = ssc.socketTextStream(""localhost"", ""12345"".toInt,
StorageLevel.MEMORY_ONLY_SER)*
* 54     val union2 = lines1.union(lines2)*
 55         //val words = lines.flatMap(_.split("" ""))
 56         *val words = union2.flatMap(_.split("" ""))*
 57     val wordCounts = words.map(x => (x, 1)).reduceByKey(_ + _)
 58
 59        * words.count().foreachRDD(rdd => {*
* 60     val totalCount = rdd.first()*
* 61 *
* 62     // print to screen*
* 63     println(totalCount)*
* 64 *
* 65     // append count to file*
* 66   //  ...*
* 67 })*
         //wordCounts.print()
 70     ssc.start()
 71     ssc.awaitTermination()
 72   }
 73 }

What do you think? is My code right?

I have obtained the follow result:

root@computer8:/opt/unibs_test/incubator-spark-tdas# bin/run-example
org.apache.spark.streaming.examples.NetworkWordCount
spark://192.168.0.13:7077SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in
[jar:file:/opt/unibs_test/incubator-spark-tdas/examples/target/scala-2.10/spark-examples-assembly-0.9.0-incubating-SNAPSHOT.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in
[jar:file:/opt/unibs_test/incubator-spark-tdas/assembly/target/scala-2.10/spark-assembly-0.9.0-incubating-SNAPSHOT-hadoop1.0.4.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an
explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
14/02/04 00:02:07 INFO StreamingExamples: Using Spark's default log4j
profile: org/apache/spark/log4j-defaults.properties
14/02/04 00:02:07 INFO StreamingExamples: Setting log level to [WARN] for
streaming example. To override add a custom log4j.properties to the
classpath.
0
0
0
0
0
0
0
0
0
0
0
0
90715
1375825
882490
941226
811032
734399
804453
718688
1058695
854417
813263
798885
785455
952804
780140
697533


Thanks Tathagata.

Att


2014-01-30 Eduardo Costa Alfaia <e.costaalfaia@unibs.it>:

ct
n.
ok
a
is
ote
r
n
{
 }
 }
ch
i
i


-- 
MSc Eduardo Costa Alfaia
PhD Student
Università degli Studi di Brescia

-- 
INFORMATIVA SUL TRATTAMENTO DEI DATI PERSONALI

I dati utilizzati per l'invio del presente messaggio sono trattati 
dall'Università degli Studi di Brescia esclusivamente per finalità 
istituzionali. Informazioni più dettagliate anche in ordine ai diritti 
dell'interessato sono riposte nell'informativa generale e nelle notizie 
pubblicate sul sito web dell'Ateneo nella sezione ""Privacy"".

Il contenuto di questo messaggio è rivolto unicamente alle persona cui 
è indirizzato e può contenere informazioni la cui riservatezza è 
tutelata legalmente. Ne sono vietati la riproduzione, la diffusione e l'uso 
in mancanza di autorizzazione del destinatario. Qualora il messaggio 
fosse pervenuto per errore, preghiamo di eliminarlo.
"
Sourav Chandra <sourav.chandra@livestream.com>,"Wed, 5 Feb 2014 12:02:41 +0530",Message processing rate of spark,"user@spark.incubator.apache.org, dev@spark.incubator.apache.org","HI,

We are currently evaluating spark streaming for our analytics application.

It reads from Kafka, processes and then persists into cassandra

As part of poc project, we need to see the message processing rate of spark
i.e. end to end time taken.

I looked into metrics but did not find any way how to capture this info.

Is there any way to see these metrics?

I am using spark 0.9.0

Thanks,
-- 

Sourav Chandra

Senior Software Engineer

· · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · ·

sourav.chandra@livestream.com

o: +91 80 4121 8723

m: +91 988 699 3746

skype: sourav.chandra

Livestream

""Ajmera Summit"", First Floor, #3/D, 68 Ward, 3rd Cross, 7th C Main, 3rd
Block, Koramangala Industrial Area,

Bangalore 560034

www.livestream.com
"
Tathagata Das <tathagata.das1565@gmail.com>,"Wed, 5 Feb 2014 01:22:51 -0800",Re: Source code JavaNetworkWordcount,dev@spark.incubator.apache.org,"Seems good to me. BTW, its find to MEMORY_ONLY (i.e. without replication)
for testing, but you should turn on replication if you want
fault-tolerance.

TD


t

I
 2
by
/spark-examples-assembly-0.9.0-incubating-SNAPSHOT.jar!/org/slf4j/impl/StaticLoggerBinder.class]
/spark-assembly-0.9.0-incubating-SNAPSHOT-hadoop1.0.4.jar!/org/slf4j/impl/StaticLoggerBinder.class]
.
e
th
t is
he
la
n
ur
à
tti
ie
cui
è
so
"
Eduardo Costa Alfaia <e.costaalfaia@unibs.it>,"Wed, 5 Feb 2014 12:15:43 -0400",Re: Source code JavaNetworkWordcount,dev@spark.incubator.apache.org,"Hi Tathagata
I am playing with NetworkWordCount.scala, I did some changes like this(in red):

 // Create the context with a 1 second batch size
 67     val ssc = new StreamingContext(args(0), ""NetworkWordCount"", Seconds(1),
 68       System.getenv(""SPARK_HOME""), StreamingContext.jarOfClass(this.getClass))
 69         ssc.checkpoint(""hdfs://computer8:54310/user/root/INPUT"")
 70     // Create a socket text stream on target ip:port and count the
 71     // words in the input stream of \n delimited text (eg. generated by 'nc')
 72     val lines1 = ssc.socketTextStream(""localhost"", ""12345"".toInt, StorageLevel.MEMORY_ONLY)
 73     val lines2 = ssc.socketTextStream(""localhost"", ""12345"".toInt, StorageLevel.MEMORY_ONLY)
 74     val lines3 = ssc.socketTextStream(""localhost"", ""12345"".toInt, StorageLevel.MEMORY_ONLY)
 75     val union2 = lines1.union(lines2)
 76     val union3 = union2.union(lines3)
 77 
 78         //val words = lines.flatMap(_.split("" ""))
 79         val words = union3.flatMap(_.split("" ""))
 80 //    val wordCounts = words.map(x => (x, 1)).reduceByKey(_ + _)
 81         val wordCounts = words.reduceByKeyAndWindow(_ + _, Seconds(30), Seconds(10))

However I have gotten the error bellow:

[error] /opt/unibs_test/incubator-spark-tdas/examples/src/main/scala/org/apache/spark/streaming/examples/NetworkWordCount.scala:81: value reduceByKeyAndWindow is not a member of org.apache.spark.streaming.dstream.DStream[String]
[error] 	val wordCounts = words.reduceByKeyAndWindow(_ + _, Seconds(30), Seconds(10))
[error] 	                       ^
[error] one error found
[error] (examples/compile:compile) Compilation failed
[error] Total time: 15 s, completed 05-Feb-2014 17:10:38


The class is import within the code:

import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.streaming.StreamingContext._
import org.apache.spark.storage.StorageLevel


Thanks


.it
a
 I
h 2
e
by
0/spark-examples-assembly-0.9.0-incubating-SNAPSHOT.jar!/org/slf4j/impl/StaticLoggerBinder.class]
0/spark-assembly-0.9.0-incubating-SNAPSHOT-hadoop1.0.4.jar!/org/slf4j/impl/StaticLoggerBinder.class]
r
.
e
th
t is
he
la
ur
à
tti
ie
cui
è
uso


-- 
INFORMATIVA SUL TRATTAMENTO DEI DATI PERSONALI

I dati utilizzati per l'invio del presente messaggio sono trattati 
dall'Università degli Studi di Brescia esclusivamente per finalità 
istituzionali. Informazioni più dettagliate anche in ordine ai diritti 
dell'interessato sono riposte nell'informativa generale e nelle notizie 
pubblicate sul sito web dell'Ateneo nella sezione ""Privacy"".

Il contenuto di questo messaggio è rivolto unicamente alle persona cui 
è indirizzato e può contenere informazioni la cui riservatezza è 
tutelata legalmente. Ne sono vietati la riproduzione, la diffusione e l'uso 
in mancanza di autorizzazione del destinatario. Qualora il messaggio 
fosse pervenuto per errore, preghiamo di eliminarlo.
"
Tathagata Das <tathagata.das1565@gmail.com>,"Wed, 5 Feb 2014 10:07:56 -0800",Re: Source code JavaNetworkWordcount,dev@spark.incubator.apache.org,"The reduceByKeyAndWindow and other ***ByKey****   operations work only on
DStreams of key-value pairs. ""Words"" is a DStream[String], so its not
key-value pairs. ""words.map(x => (x, 1))"" is DStream[(String, Int)] that
has key-value pairs, so you can call reduceByKeyAndWindow.

TD


t

park/streaming/examples/NetworkWordCount.scala:81:
n)
e>
d
,
t,
/spark-examples-assembly-0.9.0-incubating-SNAPSHOT.jar!/org/slf4j/impl/StaticLoggerBinder.class]
/spark-assembly-0.9.0-incubating-SNAPSHOT-hadoop1.0.4.jar!/org/slf4j/impl/StaticLoggerBinder.class]
w
 C
e
,
,
,
r
n
d
I
_)
ect
e.
s
t
th
on
t
m
à
a
è
 e
o
ti
e
ui
so
"
Eduardo Costa Alfaia <e.costaalfaia@unibs.it>,"Wed, 5 Feb 2014 18:33:01 -0400",Re: Source code JavaNetworkWordcount,dev@spark.incubator.apache.org,"So I could use reduceByKeyAndWindow like this
val wordCounts = words.map(x => (x, 1)).reduceByKeyAndWindow(_ + _, Seconds(30), Seconds(10))
?


t
.it
n
spark/streaming/examples/NetworkWordCount.scala:81:
n)
e>
d
,
t,
0/spark-examples-assembly-0.9.0-incubating-SNAPSHOT.jar!/org/slf4j/impl/StaticLoggerBinder.class]
0/spark-assembly-0.9.0-incubating-SNAPSHOT-hadoop1.0.4.jar!/org/slf4j/impl/StaticLoggerBinder.class]
w
 C
e
r
n
d
I
_)
ect
e.
t
th
n
t
m
à
a
è
 e
o
ti
e
ui
uso


-- 
INFORMATIVA SUL TRATTAMENTO DEI DATI PERSONALI

I dati utilizzati per l'invio del presente messaggio sono trattati 
dall'Università degli Studi di Brescia esclusivamente per finalità 
istituzionali. Informazioni più dettagliate anche in ordine ai diritti 
dell'interessato sono riposte nell'informativa generale e nelle notizie 
pubblicate sul sito web dell'Ateneo nella sezione ""Privacy"".

Il contenuto di questo messaggio è rivolto unicamente alle persona cui 
è indirizzato e può contenere informazioni la cui riservatezza è 
tutelata legalmente. Ne sono vietati la riproduzione, la diffusione e l'uso 
in mancanza di autorizzazione del destinatario. Qualora il messaggio 
fosse pervenuto per errore, preghiamo di eliminarlo.

"
Henry Saputra <henry.saputra@gmail.com>,"Wed, 5 Feb 2014 14:49:28 -0800",Discussion on strategy or roadmap should happen on dev@ list,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi Guys,

Just friendly reminder, some of you guys may work closely or
collaborate outside the dev@ list and sometimes it is easier.
But, as part of Apache Software Foundation project, any decision or
outcome that could or will be implemented in the Apache Spark need to
happen in the dev@ list as we are open and collaborative as community.

If offline discussions happen please forward the history or potential
solution to the dev@ list before any action taken.

Most of us work remote so email is the official channel of discussion
about stuff related to development in Spark.

Github pull request is not the appropriate vehicle for technical
discussions. It is used primarily for review of proposed patch which
means initial problem most of the times had been identified and
discussed.

Thanks for understanding.

- Henry

"
Matei Zaharia <matei.zaharia@gmail.com>,"Wed, 5 Feb 2014 14:56:40 -0800",Re: Discussion on strategy or roadmap should happen on dev@ list,dev@spark.incubator.apache.org,"Hey Henry, this makes sense. I’d like to add that one other vehicle for discussion has been JIRA at https://spark-project.atlassian.net/browse/SPARK. Right now the dev list is not subscribed to JIRA, but we’d be happy to subscribe it anytime if that helps. We were hoping to do this only when JIRA has been moved to the ASF, since infra can set up the forwarding automatically. But most major discussions (e.g. https://spark-project.atlassian.net/browse/SPARK-964, https://spark-project.atlassian.net/browse/SPARK-969) happen there. I think this is the model we want to have in the future — most other projects I’ve participated in also used JIRA for their discussion, and mirrored to either the “dev” list or an “issues” list.

Matei




"
Patrick Wendell <pwendell@gmail.com>,"Wed, 5 Feb 2014 16:20:10 -0800",Proposal for Spark Release Strategy,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi Everyone,

In an effort to coordinate development amongst the growing list of
Spark contributors, I've taken some time to write up a proposal to
formalize various pieces of the development process. The next release
of Spark will likely be Spark 1.0.0, so this message is intended in
part to coordinate the release plan for 1.0.0 and future releases.
I'll post this on the wiki after discussing it on this thread as
tentative project guidelines.

== Spark Release Structure ==
Starting with Spark 1.0.0, the Spark project will follow the semantic
versioning guidelines (http://semver.org/) with a few deviations.
These small differences account for Spark's nature as a multi-module
project.

Each Spark release will be versioned:
[MAJOR].[MINOR].[MAINTENANCE]

All releases with the same major version number will have API
compatibility, defined as [1]. Major version numbers will remain
stable over long periods of time. For instance, 1.X.Y may last 1 year
or more.

Minor releases will typically contain new features and improvements.
change we'd like to make is to announce fixed release dates and merge
windows for each release, to facilitate coordination. Each minor
release will have a merge window where new patches can be merged, a QA
window when only fixes can be merged, then a final period where voting
occurs on release candidates. These windows will be announced
immediately after the previous minor release to give people plenty of
time, and over time, we might make the whole release process more
regular (similar to Ubuntu). At the bottom of this document is an
example window for the 1.0.0 release.

Maintenance releases will occur more frequently and depend on specific
patches introduced (e.g. bug fixes) and their urgency. In general
these releases are designed to patch bugs. However, higher level
libraries may introduce small features, such as a new algorithm,
provided they are entirely additive and isolated from existing code
paths. Spark core may not introduce any features.

When new components are added to Spark, they may initially be marked
as ""alpha"". Alpha components do not have to abide by the above
guidelines, however, to the maximum extent possible, they should try
guidelines. At present, GraphX is the only alpha component of Spark.

[1] API compatibility:

An API is any public class or interface exposed in Spark that is not
marked as semi-private or experimental. Release A is API compatible
with release B if code compiled against release A *compiles cleanly*
against B. This does not guarantee that a compiled application that is
linked against version A will link cleanly against version B without
re-compiling. Link-level compatibility is something we'll try to
guarantee that as well, and we might make it a requirement in the
future, but challenges with things like Scala versions have made this
difficult to guarantee in the past.

== Merging Pull Requests ==
To merge pull requests, committers are encouraged to use this tool [2]
to collapse the request into one commit rather than manually
performing git merges. It will also format the commit message nicely
in a way that can be easily parsed later when writing credits.
Currently it is maintained in a public utility repository, but we'll
merge it into mainline Spark soon.

[2] https://github.com/pwendell/spark-utils/blob/master/apache_pr_merge.py

== Tentative Release Window for 1.0.0 ==
Feb 1st - April 1st: General development
April 1st: Code freeze for new features
April 15th: RC1

== Deviations ==
For now, the proposal is to consider these tentative guidelines. We
can vote to formalize these as project rules at a later time after
these guidelines will be subject to a lazy majority vote.

- Patrick

"
Tathagata Das <tathagata.das1565@gmail.com>,"Wed, 5 Feb 2014 16:19:46 -0800",Re: Source code JavaNetworkWordcount,dev@spark.incubator.apache.org,"Yes. You should be able to.

Lets try to have future conversations through the
user@spark.incubator.apache.org mailing list :)


t

on
hat
d
)
park/streaming/examples/NetworkWordCount.scala:81:
g
)
e
nt,
Int,
)
/spark-examples-assembly-0.9.0-incubating-SNAPSHOT.jar!/org/slf4j/impl/StaticLoggerBinder.class]
/spark-assembly-0.9.0-incubating-SNAPSHOT-hadoop1.0.4.jar!/org/slf4j/impl/StaticLoggerBinder.class]
j
]
e
"",
"",
"",
n
ay
+ _)
e
"".
{
ws
y
i
ità
ona
a è
à
itti
 cui
è
e
ti
e
ui
so
"
Mark Hamstra <mark@clearstorydata.com>,"Wed, 5 Feb 2014 16:39:00 -0800",Re: Proposal for Spark Release Strategy,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Looks good.


How are Alpha components and higher level libraries which may add small
features within a maintenance release going to be marked with that status?
 Somehow/somewhere within the code itself, as just as some kind of external
reference?

I would strongly encourage that developers submitting pull requests include
within the description of that PR whether you intend the contribution to be
mergeable at the maintenance level, minor level, or major level.  That will
help those of us doing code reviews and merges decide where the code should
go and how closely to scrutinize the PR for changes that are not compatible
with the intended release level.



"
Patrick Wendell <pwendell@gmail.com>,"Wed, 5 Feb 2014 16:55:57 -0800",Re: Proposal for Spark Release Strategy,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","
I think we'd mark alpha features as such in the java/scaladoc. This is
what scala does with experimental features. Higher level libraries are
anything that isn't Spark core. Maybe we can formalize this more
somehow.

We might be able to annotate the new features as experimental if they
end up in a patch release. This could make it more clear.


I'd say the default is the minor level. If contributors know it should
be added in a maintenance release, it's great if they say so. However
I'd say this is also responsibility with the committers, since
individual contributors may not know. It will probably be a while
before major level patches are being merged :P

"
Mark Hamstra <mark@clearstorydata.com>,"Wed, 5 Feb 2014 17:01:10 -0800",Re: Proposal for Spark Release Strategy,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Yup, the intended merge level is just a hint, the responsibility still lies
with the committers.  It can be a helpful hint, though.



"
Heiko Braun <ike.braun@googlemail.com>,"Thu, 6 Feb 2014 06:44:15 +0100",Re: Proposal for Spark Release Strategy,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I would even take it further, when it comes to PR's:

- any pr needs to reference a jira
- the pr should be rebased before submitting, to avoid merge commits
- as patrick said: require squashed commits

/heiko





"
Heiko Braun <ike.braun@googlemail.com>,"Thu, 6 Feb 2014 06:49:33 +0100",Re: Proposal for Spark Release Strategy,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","+1 on time boxed releases and compatibility guidelines



"
Andrew Ash <andrew@andrewash.com>,"Wed, 5 Feb 2014 21:52:12 -0800",Re: Proposal for Spark Release Strategy,dev@spark.incubator.apache.org,"Agree on timeboxed releases as well.

Is there a vision for where we want to be as a project before declaring the
first 1.0 release?  While we're in the 0.x days per semver we can break
backcompat at will (though we try to avoid it where possible), and that
luxury goes away with 1.x  I just don't want to release a 1.0 simply
because it seems to follow after 0.9 rather than making an intentional
decision that we're at the point where we can stand by the current APIs and
binary compatibility for the next year or so of the major release.

Until that decision is made as a group I'd rather we do an immediate
version bump to 0.10.0-SNAPSHOT and then if discussion warrants it later,
replace that with 1.0.0-SNAPSHOT.  It's very easy to go from 0.10 to 1.0
but not the other way around.

https://github.com/apache/incubator-spark/pull/542

Cheers!
Andrew



"
Andy Konwinski <andykonwinski@gmail.com>,"Wed, 5 Feb 2014 22:20:20 -0800",Re: Proposal for Spark Release Strategy,andrew@andrewash.com,"+1 for 0.10.0 now with the option to switch to 1.0.0 after further
discussion.

"
Mridul Muralidharan <mridul@gmail.com>,"Thu, 6 Feb 2014 12:07:33 +0530",Re: Proposal for Spark Release Strategy,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Before we move to 1.0, we need to address two things :

a) backward compatibility not just at api level, but also at binary
level (not forcing recompile).

b) minimize external dependencies - some of them would go away/not be
actively maintained.


Regards,
Mridul



"
Patrick Wendell <pwendell@gmail.com>,"Wed, 5 Feb 2014 22:38:52 -0800",Re: Proposal for Spark Release Strategy,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","If people feel that merging the intermediate SNAPSHOT number is
significant, let's just defer merging that until this discussion
concludes.

That said - the decision to settle on 1.0 for the next release is not
just because it happens to come after 0.9. It's a conscientious
decision based on the development of the project to this point. A
major focus of the 0.9 release was tying off loose ends in terms of
backwards compatibility (e.g. spark configuration). There was some
discussion back then of maybe cutting a 1.0 release but the decision
was deferred until after 0.9.

@mridul - pleas see the original post for discussion about binary compatibility.


"
Heiko Braun <ike.braun@googlemail.com>,"Thu, 6 Feb 2014 08:30:18 +0100",Re: Proposal for Spark Release Strategy,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","If we could minimize the external dependencies, it would certainly be beneficial long term. 



"
Heiko Braun <ike.braun@googlemail.com>,"Thu, 6 Feb 2014 09:41:54 +0100",SPARK-964 & related issues,dev@spark.incubator.apache.org,"

Hi everybody, 
two questions, somehow related to the discussion about JDK 8 support [1].

- Has support for Spores [2] been considered/discussed already?
- Scala 2.11 is due in the next few month [3]. Would this be a reasonable target version for spark 1.0?

As far as I know, both JDK 8 support and Spores should become part of scala 2.11.

Regards, Heiko

[1] https://spark-project.atlassian.net/browse/SPARK-964
[2] http://docs.scala-lang.org/sips/pending/spores.html
[3] https://issues.scala-lang.org/browse/SI/component/10600?selectedTab=com.atlassian.jira.plugin.system.project%3Acomponent-roadmap-panel
"
Mridul Muralidharan <mridul@gmail.com>,"Thu, 6 Feb 2014 14:19:19 +0530",Re: Proposal for Spark Release Strategy,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","The reason I explicitly mentioned about binary compatibility was
because it was sort of hand waved in the proposal as good to have.
My understanding is that scala does make it painful to ensure binary
compatibility - but stability of interfaces is vital to ensure
dependable platforms.
Recompilation might be a viable option for developers - not for users.

Regards,
Mridul



"
Prashant Sharma <scrapcodes@gmail.com>,"Thu, 6 Feb 2014 14:32:42 +0530",Re: SPARK-964 & related issues,dev@spark.incubator.apache.org,"Hi,

Well in the context of only JDK8 support, there is actually no need to
migrate to Scala 2.11 or JDK 8 itself. The trick is to use Interfaces
instead of AbstractClasses for accepting functions.







-- 
Prashant
"
Andrew Ash <andrew@andrewash.com>,"Thu, 6 Feb 2014 02:20:56 -0800",[0.9.0] Possible deadlock in shutdown hook?,dev@spark.incubator.apache.org,"Hi Spark devs,

Occasionally when hitting Ctrl-C in the scala spark shell on 0.9.0 one of
my workers goes dead in the spark master UI.  I'm using the standalone
cluster and didn't ever see this while using 0.8.0 so I think it may be a
regression.

When I prod on the hung CoarseGrainedExecutorBackend JVM with jstack and
jmap -heap, it doesn't respond unless I add the -F force flag.  The heap
isn't full, but there are some interesting bits in the jstack.  Poking
around a little, I think there may be some kind of deadlock in the shutdown
hooks.

Below are the threads I think are most interesting:

Thread 14308: (state = BLOCKED)
 - java.lang.Shutdown.exit(int) @bci=96, line=212 (Interpreted frame)
 - java.lang.Runtime.exit(int) @bci=14, line=109 (Interpreted frame)
 - java.lang.System.exit(int) @bci=4, line=962 (Interpreted frame)
 -
org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$receive$1.applyOrElse(java.lang.Object,
scala.Function1) @bci=352, line=81 (Interpreted frame)
 - akka.actor.ActorCell.receiveMessage(java.lang.Object) @bci=25, line=498
(Interpreted frame)
 - akka.actor.ActorCell.invoke(akka.dispatch.Envelope) @bci=39, line=456
(Interpreted frame)
 - akka.dispatch.Mailbox.processMailbox(int, long) @bci=24, line=237
(Interpreted frame)
 - akka.dispatch.Mailbox.run() @bci=20, line=219 (Interpreted frame)
 - akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec()
@bci=4, line=386 (Interpreted frame)
 - scala.concurrent.forkjoin.ForkJoinTask.doExec() @bci=10, line=260
(Compiled frame)
 -
scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(scala.concurrent.forkjoin.ForkJoinTask)
@bci=10, line=1339 (Compiled frame)
 -
scala.concurrent.forkjoin.ForkJoinPool.runWorker(scala.concurrent.forkjoin.ForkJoinPool$WorkQueue)
@bci=11, line=1979 (Compiled frame)
 - scala.concurrent.forkjoin.ForkJoinWorkerThread.run() @bci=14, line=107
(Interpreted frame)

Thread 3865: (state = BLOCKED)
 - java.lang.Object.wait(long) @bci=0 (Interpreted frame)
 - java.lang.Thread.join(long) @bci=38, line=1280 (Interpreted frame)
 - java.lang.Thread.join() @bci=2, line=1354 (Interpreted frame)
 - java.lang.ApplicationShutdownHooks.runHooks() @bci=87, line=106
(Interpreted frame)
 - java.lang.ApplicationShutdownHooks$1.run() @bci=0, line=46 (Interpreted
frame)
 - java.lang.Shutdown.runHooks() @bci=39, line=123 (Interpreted frame)
 - java.lang.Shutdown.sequence() @bci=26, line=167 (Interpreted frame)
 - java.lang.Shutdown.exit(int) @bci=96, line=212 (Interpreted frame)
 - java.lang.Terminator$1.handle(sun.misc.Signal) @bci=8, line=52
(Interpreted frame)
 - sun.misc.Signal$1.run() @bci=8, line=212 (Interpreted frame)
 - java.lang.Thread.run() @bci=11, line=744 (Interpreted frame)


Thread 3987: (state = BLOCKED)
 - java.io.UnixFileSystem.list(java.io.File) @bci=0 (Interpreted frame)
 - java.io.File.list() @bci=29, line=1116 (Interpreted frame)
 - java.io.File.listFiles() @bci=1, line=1201 (Compiled frame)
 - org.apache.spark.util.Utils$.listFilesSafely(java.io.File) @bci=1,
line=466 (Interpreted frame)
 - org.apache.spark.util.Utils$.deleteRecursively(java.io.File) @bci=9,
line=478 (Compiled frame)
 -
org.apache.spark.util.Utils$$anonfun$deleteRecursively$1.apply(java.io.File)
@bci=4, line=479 (Compiled frame)
 -
org.apache.spark.util.Utils$$anonfun$deleteRecursively$1.apply(java.lang.Object)
@bci=5, line=478 (Compiled frame)
 -
scala.collection.IndexedSeqOptimized$class.foreach(scala.collection.IndexedSeqOptimized,
scala.Function1) @bci=22, line=33 (Compiled frame)
 - scala.collection.mutable.WrappedArray.foreach(scala.Function1) @bci=2,
line=34 (Compiled frame)
 - org.apache.spark.util.Utils$.deleteRecursively(java.io.File) @bci=19,
line=478 (Interpreted frame)
 -
org.apache.spark.storage.DiskBlockManager$$anon$1$$anonfun$run$2.apply(java.io.File)
@bci=14, line=141 (Interpreted frame)
 -
org.apache.spark.storage.DiskBlockManager$$anon$1$$anonfun$run$2.apply(java.lang.Object)
@bci=5, line=139 (Interpreted frame)
 -
scala.collection.IndexedSeqOptimized$class.foreach(scala.collection.IndexedSeqOptimized,
scala.Function1) @bci=22, line=33 (Compiled frame)
 - scala.collection.mutable.ArrayOps$ofRef.foreach(scala.Function1) @bci=2,
line=108 (Interpreted frame)
 - org.apache.spark.storage.DiskBlockManager$$anon$1.run() @bci=39,
line=139 (Interpreted frame)


I think what happened here is that thread 14308 received the akka
""shutdown"" message and called System.exit().  This started thread 3865,
which is the JVM shutting itself down.  Part of that process is running the
shutdown hooks, so it started thread 3987.  That thread is the shutdown
hook from addShutdownHook() in DiskBlockManager.scala, which looks like
this:

  private def addShutdownHook() {
    localDirs.foreach(localDir => Utils.registerShutdownDeleteDir(localDir))
    Runtime.getRuntime.addShutdownHook(new Thread(""delete Spark local
dirs"") {
      override def run() {
        logDebug(""Shutdown hook called"")
        localDirs.foreach { localDir =>
          try {
            if (!Utils.hasRootAsShutdownDeleteDir(localDir))
Utils.deleteRecursively(localDir)
          } catch {
            case t: Throwable =>
              logError(""Exception while deleting local spark dir: "" +
localDir, t)
          }
        }

        if (shuffleSender != null) {
          shuffleSender.stop()
        }
      }
    })
  }

It goes through and deletes the directories recursively.  I was thinking
there might be some issues with concurrently-running shutdown hooks
deleting things out from underneath each other (shutdown hook javadocs say
they're all started in parallel if multiple hooks are added) causing the
File.list() in that last thread to take quite some time.

While I was looking through the stacktrace the JVM finally exited (after
15-20min at least) so I won't be able to debug more until this bug strikes
again.

Any ideas on what might be going on here?

Thanks!
Andrew
"
Mridul Muralidharan <mridul@gmail.com>,"Thu, 6 Feb 2014 16:00:19 +0530",Re: [0.9.0] Possible deadlock in shutdown hook?,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","shutdown hooks should not take 15 mins are you mentioned !
(either due to spark or something else ?)

It might just be that there was a lot of stuff to remove ?

Regards,
Mridul



"
Andrew Ash <andrew@andrewash.com>,"Thu, 6 Feb 2014 02:49:57 -0800",Re: [0.9.0] Possible deadlock in shutdown hook?,dev@spark.incubator.apache.org,"Got a repro locally on my MBP (the other was on a CentOS machine).

Build spark, run a master and a worker with the sbin/start-all.sh script,
then run this in a shell:

import org.apache.spark.storage.StorageLevel._
val s = sc.parallelize(1 to 1000000000).persist(MEMORY_AND_DISK_SER);
s.count

After about a minute, this line appears in the shell logging output:

14/02/06 02:44:44 WARN BlockManagerMasterActor: Removing BlockManager
BlockManagerId(0, aash-mbp.dyn.yojoe.local, 57895, 0) with no recent heart
beats: 57510ms exceeds 45000ms

Ctrl-C the shell.  In jps there is now a worker, a master, and a
CoarseGrainedExecutorBackend.

Run jstack on the CGEBackend JVM, and I got the attached stacktraces.  I
waited around for 15min then kill -9'd the JVM and restarted the process.

I wonder if what's happening here is that the threads that are spewing data
to disk (as that parallelize and persist would do) can write to disk faster
than the cleanup threads can delete from disk.

What do you think of that theory?


Andrew




"
Andrew Ash <andrew@andrewash.com>,"Thu, 6 Feb 2014 03:27:05 -0800",Re: [0.9.0] Possible deadlock in shutdown hook?,dev@spark.incubator.apache.org,"Per the book Java Concurrency in Practice the already-running threads
continue running while the shutdown hooks run.  So I think the race between
the writing thread and the deleting thread could be a very real possibility
:/

http://stackoverflow.com/a/3332925/120915



"
Pascal Voitot Dev <pascal.voitot.dev@gmail.com>,"Thu, 6 Feb 2014 17:40:38 +0100",Any work on improving Serializer for Macro-based picklers?,dev@spark.incubator.apache.org,"Hello guys,

I'm investigating a bit how I could use scala-pickling as a serializer in
Spark and after reading some code, I discovered current serializer
implementation prevents from using macro-based tooling with it.

I wanted to know whether someone was already working on improving current
serializer design to this end?

As Spark 0.9 is 2.10.0 compatible, I believe you must already have a few
ideas with macros in it... ;)

Thanks in advance!

Best regards
Pascal (@mandubian)
"
Henry Saputra <henry.saputra@gmail.com>,"Thu, 6 Feb 2014 09:55:58 -0800",Re: Discussion on strategy or roadmap should happen on dev@ list,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","setup for Spark it will happen automatically.

- Henry

te:
le for discussion has been JIRA at https://spark-project.atlassian.net/browse/SPARK. Right now the dev list is not subscribed to JIRA, but weâ€™d be happy to subscribe it anytime if that helps. We were hoping to do this only when JIRA has been moved to the ASF, since infra can set up the forwarding automatically. But most major discussions (e.g. https://spark-project.atlassian.net/browse/SPARK-964, https://spark-project.atlassian.net/browse/SPARK-969) happen there. I think this is the model we want to have in the future â€” most other projects Iâ€™ve participated in also used JIRA for their discussion, and mirrored to either the â€œdevâ€ list or an â€œissuesâ€ list.
:

"
Sandy Ryza <sandy.ryza@cloudera.com>,"Thu, 6 Feb 2014 10:19:50 -0800",Re: Proposal for Spark Release Strategy,dev@spark.incubator.apache.org,"Thanks for all this Patrick.

I like Heiko's proposal that requires every pull request to reference a
JIRA.  This is how things are done in Hadoop and it makes it much easier
to, for example, find out whether an issue you came across when googling
for an error is in a release.

I agree with Mridul about binary compatibility.  It can be a dealbreaker
for organizations that are considering an upgrade. The two ways I'm aware
of that cause binary compatibility are scala version upgrades and messing
around with inheritance.  Are these not avoidable at least for minor
releases?

-Sandy





"
Reynold Xin <rxin@databricks.com>,"Thu, 6 Feb 2014 10:34:26 -0800",Re: [0.9.0] Possible deadlock in shutdown hook?,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Is it safe if we interrupt the running thread during shutdown?





"
Patrick Wendell <pwendell@gmail.com>,"Thu, 6 Feb 2014 10:42:23 -0800",Re: Proposal for Spark Release Strategy,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","
I think this is a good idea and something on which there is wide
consensus. I separately was going to suggest this in a later e-mail
necessary is because it's becoming hard to track which features ended
up in which releases.


This is clearly a goal but I'm hesitant to codify it until we
understand all of the reasons why it might not work. I've heard in
general with Scala there are many non-obvious things that can break
binary compatibility and we need to understand what they are. I'd
propose we add the migration tool [1] here to our build and use it for
a few months and see what happens (hat tip to Michael Armbrust).

It's easy to formalize this as a requirement later, it's impossible to
go the other direction. For Scala major versions it's possible we can
cross-build between 2.10 and 2.11 to retain link-level compatibility.
It's just entirely uncharted territory and AFAIK no one who's
suggesting this is speaking from experience maintaining this guarantee
for a Scala project.

That would be the strongest convincing reason for me - if someone has
actually done this in the past in a Scala project and speaks from
experience. Most of use are speaking from the perspective of Java
projects where we understand well the trade-off's and costs of
maintaining this guarantee.

[1] https://github.com/typesafehub/migration-manager

- Patrick

"
Henry Saputra <henry.saputra@gmail.com>,"Thu, 6 Feb 2014 10:43:06 -0800",Re: Proposal for Spark Release Strategy,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Thanks Patick to initiate the discussion about next road map for Apache Spark.

I am +1 for 0.10.0 for next version.

It will give us as community some time to digest the process and the
vision and make adjustment accordingly.

Release a 1.0.0 is a huge milestone and if we do need to break API
somehow or modify internal behavior dramatically we could take
advantage to release 1.0.0 as good step to go to.


- Henry




"
Matei Zaharia <matei.zaharia@gmail.com>,"Thu, 6 Feb 2014 10:56:24 -0800",Re: Proposal for Spark Release Strategy,dev@spark.incubator.apache.org,"I think it’s important to do 1.0 next. The project has been around for 4 years, and I’d be comfortable maintaining the current codebase for a long time in an API and binary compatible way through 1.x releases. Over the past 4 years we haven’t actually had major changes to the user-facing API — the only ones were changing the package to org.apache.spark, and upgrading the Scala version. I’d be okay leaving 1.x to always use Scala 2.10 for example, or later cross-building it for Scala 2.11. Updating to 1.0 says two things: it tells users that they can be confident that version will be maintained for a long time, which we absolutely want to do, and it lets outsiders see that the project is now fairly mature (for many people, pre-1.0 might still cause them not to try it). I think both are good for the community.

Regarding binary compatibility, I agree that it’s what we should strive for, but it just seems premature to codify now. Let’s see how it works between, say, 1.0 and 1.1, and then we can codify it.

Matei


Apache Spark.
declaring the
break
that
intentional
APIs and
later,
1.0
<pwendell@gmail.com>:
release
semantic
multi-module
year
improvements.
merge
QA
voting
of
specific
marked
try
Spark.
not
cleanly*
is
without
this
[2]
nicely
we'll
https://github.com/pwendell/spark-utils/blob/master/apache_pr_merge.py
to


"
Mridul Muralidharan <mridul@gmail.com>,"Fri, 7 Feb 2014 00:24:55 +0530",Re: [0.9.0] Possible deadlock in shutdown hook?,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Looks like a pathological corner case here - where the the delete
thread is not getting run while the OS is busy prioritizing the thread
writing data (probably with heavy gc too).
Ideally, the delete thread would list files, remove them and then fail
when it tries to remove the non empty directory (since other thread
might be creating more in parallel).


Regards,
Mridul



"
Sandy Ryza <sandy.ryza@cloudera.com>,"Thu, 6 Feb 2014 11:03:17 -0800",Re: Proposal for Spark Release Strategy,dev@spark.incubator.apache.org,"Not codifying binary compatibility as a hard rule sounds fine to me.  Would
it make sense to put something in that . I.e. avoid making needless changes
to class hierarchies.

Whether Spark considers itself stable or not, users are beginning to treat
it so.  A responsible project will acknowledge this and provide the
stability needed by its user base.  I think some projects have made the
mistake of waiting too long to release a 1.0.0.  It allows them to put off
making the hard decisions, but users and downstream projects suffer.

If Spark needs to go through dramatic changes, there's always the option of
a 2.0.0 that allows for this.

-Sandy




"
Sandy Ryza <sandy.ryza@cloudera.com>,"Thu, 6 Feb 2014 11:05:09 -0800",Re: Proposal for Spark Release Strategy,dev@spark.incubator.apache.org,"Bleh, hit send to early again.  My second paragraph was to argue for 1.0.0
instead of 0.10.0, not to hammer on the binary compatibility point.



"
Sandy Ryza <sandy.ryza@cloudera.com>,"Thu, 6 Feb 2014 11:04:01 -0800",Re: Proposal for Spark Release Strategy,dev@spark.incubator.apache.org,"*Would it make sense to put in something that strongly discourages binary
incompatible changes when possible?



"
Evan Chan <ev@ooyala.com>,"Thu, 6 Feb 2014 11:54:57 -0800",Re: Proposal for Spark Release Strategy,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","+1 for 0.10.0.

It would give more time to study things (such as the new SparkConf)
and let the community decide if any breaking API changes are needed.

Also, a +1 for minor revisions not breaking code compatibility,
including Scala versions.   (I guess "
Evan Chan <ev@ooyala.com>,"Thu, 6 Feb 2014 11:56:41 -0800",Re: Proposal for Spark Release Strategy,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","The other reason for waiting are things like stability.

It would be great to have as a goal for 1.0.0 that under most heavy
use scenarios, workers and executors don't just die, which is not true
today.
Also, there should be minimal ""silent failures"" which are difficult to debug.




-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

"
Matei Zaharia <matei.zaharia@gmail.com>,"Thu, 6 Feb 2014 12:02:13 -0800",Re: Proposal for Spark Release Strategy,dev@spark.incubator.apache.org,"
binary

Yes, I like this idea. Let’s just say we’ll strive for this as much as possible and think about codifying it after some experience doing this.

Matei



"
Matei Zaharia <matei.zaharia@gmail.com>,"Thu, 6 Feb 2014 12:07:24 -0800",Re: Proposal for Spark Release Strategy,dev@spark.incubator.apache.org,"

debug.

I think this is orthogonal to the version number. 1.x versions can have bugs — it’s almost unavoidable in the distributed system space. The version number is more about the level of compatibility and support people can expect, which I think is something we want to solidify. Calling it 1.x will also make it more likely that we have long-term maintenance releases, because with the current project, people expect that they have to keep jumping to the latest version. Just as an example, when we did a survey a while back, out of ~100 respondents, all were either on the very latest release or on master (!). I’ve had multiple people ask me about longer-term supported versions (e.g. if I download 1.x now, will it still have maintenance releases a year from now, or will it be left in the dust).

Matei


"
Reynold Xin <rxin@databricks.com>,"Thu, 6 Feb 2014 12:15:41 -0800",Re: Proposal for Spark Release Strategy,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","+1 for 1.0


The point of 1.0 is for us to self-enforce API compatibility in the context
of longer term support. If we continue down the 0.xx road, we will always
have excuse for breaking APIs. That said, a major focus of 0.9 and some of
the work that are"
Imran Rashid <imran@quantifind.com>,"Thu, 6 Feb 2014 14:39:07 -0600",Re: Proposal for Spark Release Strategy,dev@spark.incubator.apache.org,"I don't really agree with this logic.  I think we haven't broken API so far
because we just keep adding stuff on to it, and we haven't bothered to
clean the api up, specifically to *avoid* breaking things.  Here's a
handful of api breaking things that we might want to consider:

* should we look at all the various configuration properties, and maybe
some of them should get renamed for consistency / clarity?
* do all of the functions on RDD need to be in core?  or do some of them
that are simple additions built on top of the primitives really belong in a
""utils"" package or something?  Eg., maybe we should get rid of all the
variants of the mapPartitions / mapWith / etc.  just have map, and
mapPartitionsWithIndex  (too many choices in the api can also be confusing
to the user)
* are the right things getting tracked in SparkListener?  Do we need to add
or remove anything?

This is probably not the right list of questions, that's just an idea of
the kind of thing we should be thinking about.

Its also fine with me if 1.0 is next, I just think that we ought to be
asking these kinds of questions up and down the entire api before we
release 1.0.  And given that we haven't even started that discussion, it
seems possible that there could be new features we'd like to release in
0.10 before that discussion is finished.




"
Sandy Ryza <sandy.ryza@cloudera.com>,"Thu, 6 Feb 2014 12:49:10 -0800",Re: Proposal for Spark Release Strategy,dev@spark.incubator.apache.org,"If the APIs are usable, stability and continuity are much more important
than perfection.  With many already relying on the current APIs, I think
trying to clean them up will just cause pain for users and integrators.
 Hadoop made this mistake when they decided the original MapReduce APIs
were ugly and introduced a new set of APIs to do the same thing.  Even
though this happened in a pre-1.0 release, three years down the road, both
the old and new APIs are still supported, causing endless confusion for
users.  If individual functions or configuration properties have unclear
names, they can be deprecated and replaced, but redoing the APIs or
breaking compatibility at this point is simply not worth it.



"
Patrick Wendell <pwendell@gmail.com>,"Thu, 6 Feb 2014 13:12:25 -0800",Re: Proposal for Spark Release Strategy,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Just to echo others - The relevant question is whether we want to
advertise stable API's for users that we will support for a long time
horizon. And doing this is critical to being taken seriously as a
mature project.

The question is not whether or not there are things we want to improve
about Spark (further reduce dependencies, runtime stability, etc) - of
course everyone wants to improve those things!

In the next few months ahead of 1.0 the plan would be to invest effort
in finishing off loose ends in the API and of course, no 1.0 release
candidate will pass muster if these aren't addressed. I only see a few
fairly small blockers though wrt API issues:

- We should mark things that may evolve and change as semi-private
developer API's (e.g. the Spark Listener).
- We need to standardize the Java API in a way that supports Java 8 lamdbas.

Other than that - I don't see many blockers in terms of API changes we
might want to make. A lot of those were dealt with in 0.9 specifically
to prepare for this.

The broader question API ""clean-up"" brings up a debate about the trade
off of compatibility with older pre-1.0 versions of Spark. This is not
the primary issue under discussion and can be debated separably.

The primary issue at hand is whether to have 1.0 in ~3 months vs
pushing it to ~6 months from now or more.

- Patrick


"
Mark Hamstra <mark@clearstorydata.com>,"Thu, 6 Feb 2014 14:34:33 -0800",Re: Proposal for Spark Release Strategy,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Imran:



And moving master to 1.0.0-SNAPSHOT doesn't preclude that.  If anything, it
turns that ""ought to"" into ""must"" -- which is another way of saying what
Reynold said: ""The point of 1.0 is for us to self-enforce API compatibility
in the context of longer term support. If we continue down the 0.xx road,
we will always have excuse for breaking APIs.""

1.0.0-SNAPSHOT doesn't mean that the API is final right now.  It means that
what is released next will be final over what is intended to be the lengthy
scope of a major release.  That means that adding new features and
functionality (at least to core spark) should be a very low priority for
this development cycle, and establishing the 1.0 API from what is already
in 0.9.0 should be our first priority.  It wouldn't trouble me at all if
not-strictly-necessary new features were left to hang out on the pull
request queue for quite awhile until we are ready to add them in 1.1.0, if
we were to do pretty much nothing else during this cycle except to get the
1.0 API to where most of us agree that it is in good shape.

If we're not adding new features and extending the 0.9.0 API, then there
really is no need for a 0.10.0 minor-release, whose main purpose would be
to collect the API additions from 0.9.0.  Bug-fixes go in 0.9.1-SNAPSHOT;
bug-fixes and finalized 1.0 API go in 1.0.0-SNAPSHOT; almost all new
features are put on hold and wait for 1.1.0-SNAPSHOT.

... it seems possible that there could be new features we'd like to release


We certainly can add new features to 1.0.0, but they will have to go
through a rigorous review to be certain that they are things that we really
want to commit to keeping going forward.  But after 1.0, that is true for
any new feature proposal unless we create specifically experimental
branches.  So what moving to 1.0.0-SNAPSHOT really means is that we are
saying that we have gone beyond the development phase where more-or-less
experimental features can be added to Spark releases only to be withdrawn
later -- that time is done after 1.0.0-SNAPSHOT.  Now to be fair,
tentative/experimental features have not been added willy-nilly to Spark
over recent releases, and withdrawal/replacement has been about as limited
in scope as could be fairly expected, so this shouldn't be a radically new
and different development paradigm.  There are, though, some experiments
that were added in the past and should probably now be withdrawn (or at
least deprecated in 1.0.0, withdrawn in 1.1.0.)  I'll put my own
contribution of mapWith, filterWith, et. al on the chopping block as an
effort that, at least in its present form, doesn't provide enough extra
over mapPartitionsWithIndex, and whose syntax is awkward enough that I
don't believe these methods have ever been widely used, so that their
inclusion in the 1.0 API is probably not warranted.

There are other elements of Spark that also should be culled and/or
refactored before 1.0.  Imran has listed a few. I'll also suggest that
there are at least parts of alternative Broadcast variable implementations
that should probably be left behind.  In any event, Imran is absolutely
correct that we need to have a discussion about these issues.  Moving to
1.0.0-SNAPSHOT forces us to begin that discussion.

So, I'm +1 for 1.0.0-incubating-SNAPSHOT (and looking forward to losing the
""incubating""!)





"
Matei Zaharia <matei.zaharia@gmail.com>,"Thu, 6 Feb 2014 14:39:32 -0800",Re: Proposal for Spark Release Strategy,dev@spark.incubator.apache.org,"I think these are good questions to bring up, Imran. Here are my thoughts on them (I’ve thought about some of these in the past):


so far
maybe

I know that some names are suboptimal, but I absolutely detest breaking APIs, config names, etc. I’ve seen it happen way too often in other projects (even things we depend on that are officially post-1.0, like Akka or Protobuf or Hadoop), and it’s very painful. I think that we as fairly cutting-edge users are okay with libraries occasionally changing, but many others will consider it a show-stopper. Given this, I think that any cosmetic change now, even though it might improve clarity slightly, is not worth the tradeoff in terms of creating an update barrier for existing users.

them
in a
confusing

Again, for the reason above, I’d keep them where they are and consider adding other stuff later. Also personally I want to optimize the API for usability, not for Spark developers. If it’s easier for a user to call RDD.mapPartitions instead of AdvancedUtils.mapPartitions(rdd, func), and the only cost is a longer RDD.scala class, I’d go for the former. If you think there are some API methods that should just go away, that would be good to discuss — we can deprecate them for example.

to add

This is an API that will probably be experimental or semi-private at first.

Anyway, as I said, these are good questions — I’d be happy to see suggestions on any of these fronts. I just wanted to point out the importance of compatibility. I think it’s been awesome that most of our users have been able to keep up with the latest version of Spark, getting all the new fixes and simultaneously increasing the amount of contributions we get on master and decreasing the backporting burden on old branches. We might take it for granted, but I’ve seen similar projects that didn't manage to do this. In particular, compatibility in Hadoop has been a mess, with some major users diverging from Apache early (e.g. Facebook) and never being able to contribute back, and with big API cleanups (e.g. mapred -> mapreduce) being proposed after the project already had a lot of momentum and never making it through. The experience of seeing those has made me very conservative. The longer we can keep a unified community, the better it will be for all users of the project.

Matei

of
it
in
for 4
long
the
API --
upgrading
for
says
will be
lets
for
strive
works
Apache
declaring
break
that
simply
intentional
APIs
immediate
to 1.0
<ike.braun@googlemail.com
<pwendell@gmail.com>:
of
to
release
in
releases.
semantic
multi-module
year
improvements.
merge
a QA
voting
plenty of
specific
code
marked
try
Spark.
not
compatible
cleanly*
that is
without
this
tool [2]
nicely
we'll
https://github.com/pwendell/spark-utils/blob/master/apache_pr_merge.py
We
after
to


"
Mark Hamstra <mark@clearstorydata.com>,"Thu, 6 Feb 2014 14:39:41 -0800",Re: Proposal for Spark Release Strategy,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I'm not sure that that is the conclusion that I would draw from the Hadoop
example.  I would certainly agree that maintaining and supporting both an
old and a new API is a cause of endless confusion for users.  If we are
going to change or drop things from the API to reach 1.0, then we shouldn't
be maintaining and support the prior way of doing things beyond a 1.0.0 ->
1.1.0 deprecation cycle.



"
Patrick Wendell <pwendell@gmail.com>,"Thu, 6 Feb 2014 16:05:53 -0800",Proposal for JIRA and Pull Request Policy,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","As a break out from the other thread. I'd like to propose two
guidelines for pull requests. These guidelines are to make things
easier to track for developers and users, and to help organize the
large number of PR's that we are receiving. Thoughts?

1. Pull requests will require associated JIRA's. We will ask people to
create a JIRA if there is none yet.

2. Pull request names should ideally convey:
(a) The JIRA name
(b) A title summarizing the patch
(c) [optional prefix] The library it is related to
(d) [optional prefix] WIP or RFC if it is not finished.

Example names:
SPARK-123: Add some feature to Spark
[STREAMING] SPARK-123: Add some feature to Spark streaming
[MLLIB] [WIP] SPARK-123: Some potentially useful feature for MLLib

- Patrick

"
Mridul Muralidharan <mridul@gmail.com>,"Fri, 7 Feb 2014 05:40:56 +0530",Re: Proposal for JIRA and Pull Request Policy,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","+1

Would be great if the JIRA tag was 'clickable' to go to the actual JIRA :-)

Regards,
Mridul



"
Sandy Ryza <sandy.ryza@cloudera.com>,"Thu, 6 Feb 2014 16:11:58 -0800",Re: Proposal for JIRA and Pull Request Policy,dev@spark.incubator.apache.org,1
Andrew Ash <andrew@andrewash.com>,"Thu, 6 Feb 2014 16:27:18 -0800",Re: [0.9.0] Possible deadlock in shutdown hook?,dev@spark.incubator.apache.org,"I think the solution where we stop the writing threads and then let the
deleting threads completely clean up is the best option since the final
state doesn't have half-deleted temp dirs scattered across the cluster.

How feasible do you think it'd be to interrupt the other threads?



"
Nan Zhu <zhunanmcgill@gmail.com>,"Thu, 6 Feb 2014 19:46:57 -0500",Is there any way to make a quick test on some pre-commit code?,dev@spark.incubator.apache.org,"Hi, all 

Is it always necessary to run sbt assembly when you want to test some code, 

Sometimes you just repeatedly change one or two lines for some failed test case, it is really time-consuming to sbt assembly every time

any faster way?

Best, 

-- 
Nan Zhu

"
Reynold Xin <rxin@databricks.com>,"Thu, 6 Feb 2014 16:50:57 -0800",Re: Is there any way to make a quick test on some pre-commit code?,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","You can do

sbt/sbt assemble-deps


and then just run

sbt/sbt package

each time.


You can even do

sbt/sbt ~package

for automatic incremental compilation.




"
Nan Zhu <zhunanmcgill@gmail.com>,"Thu, 6 Feb 2014 19:57:23 -0500","Re: Is there any way to make a quick test on some pre-commit
 code?",dev@spark.incubator.apache.org,"Thank you very much, Reynold 

-- 
Nan Zhu






"
Matei Zaharia <matei.zaharia@gmail.com>,"Thu, 6 Feb 2014 18:27:56 -0800",Re: Discussion on strategy or roadmap should happen on dev@ list,dev@spark.incubator.apache.org,"Henry (or anyone else), do you have any preference on sending these directly to “dev"" versus creating another list for “issues”? I guess we can try “dev” for a while and let people decide if it gets too spammy. We’ll just have to advertise it in advance.

Matei


for discussion has been JIRA at https://spark-project.atlassian.net/browse/SPARK. Right now the dev list is not subscribed to JIRA, but we’d be happy to subscribe it anytime if that helps. We were hoping to do this only when JIRA has been moved to the ASF, since infra can set up the forwarding automatically. But most major discussions (e.g. https://spark-project.atlassian.net/browse/SPARK-964, https://spark-project.atlassian.net/browse/SPARK-969) happen there. I think this is the model we want to have in the future — most other projects I’ve participated in also used JIRA for their discussion, and mirrored to either the “dev” list or an “issues” list.
to
community.
potential
discussion


"
Reynold Xin <rxin@databricks.com>,"Thu, 6 Feb 2014 18:29:37 -0800",Re: Discussion on strategy or roadmap should happen on dev@ list,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","We can try it on dev, but I personally find the JIRA notifications pretty
spammy ... It will clutter the dev list, and make it harder to search for
useful information here.



"
Andrew Ash <andrew@andrewash.com>,"Thu, 6 Feb 2014 18:30:31 -0800",Re: Discussion on strategy or roadmap should happen on dev@ list,dev@spark.incubator.apache.org,"I'd prefer a ""spark-jira-activity"" list so I can filter appropriately.  A
separate request, but a ""spark-commits"" list that has all commits emailed
to it has been helpful at work to keep a pulse on activity.


e:

sâ€? I guess we can
o spammy. Weâ€™ll
:
hicle for
ime if
e
r projects
rored to
.
.
"
Matei Zaharia <matei.zaharia@gmail.com>,"Thu, 6 Feb 2014 19:56:00 -0800",Re: Discussion on strategy or roadmap should happen on dev@ list,dev@spark.incubator.apache.org,"You can already get commits on commits@spark.incubator.apache.org actually.

Regarding JIRA issues, I think I’ll just try putting them on dev for now, and we can move to a separate list later. If you’d like to filter them, the address is jira@spark-project.atlassian.net.

Matei


 A
emailed
guess we can
spammy. We’ll
<matei.zaharia@gmail.com>
vehicle for
list
anytime if
to the
major
https://spark-project.atlassian.net/browse/SPARK-964,
projects
mirrored to
or
to
community.
potential
discussion
which


"
Nan Zhu <zhunanmcgill@gmail.com>,"Thu, 6 Feb 2014 23:01:22 -0500","Re: Discussion on strategy or roadmap should happen on dev@
 list",dev@spark.incubator.apache.org,"Hi, Matei,  

Does it mean that I will receive the notification on every issue, instead of just the ones Iâ€™m watching on?  

Best,  

--  
Nan Zhu



ilto:commits@spark.incubator.apache.org) actually.
v for now, and we can move to a separate list later. If youâ€™d like to filter them, the address is jira@spark-project.atlassian.net (mailto:jira@spark-project.atlassian.net).
tely. A
 emailed

œissuesâ€? I guess we can
ets too spammy. Weâ€™ll
RA
gmail.com (mailto:matei.zaharia@gmail.com)>
ther vehicle for
 list
t anytime if
 to the
t major
964,
 I
t other projects
nd mirrored to
 list.
ail.com (mailto:henry.saputra@gmail.com)>
r.
ision or
 need to
 community.
otential
scussion
cal
h which
nd

"
Matei Zaharia <matei.zaharia@gmail.com>,"Thu, 6 Feb 2014 19:58:45 -0800",Notice: JIRA messages will be forwarded to this list,dev@spark.incubator.apache.org,"As part of ASF policy, we need to archive all development discussions on a mailing list, so I’m going to do this on the dev list for now. You can filter these messages by the sender, which will be jira@spark-project.atlassian.net. If the list becomes too spammy as a result, we can create a separate “issues” list later, but I just want to have something in place to archive them for now.

Matei
"
Matei Zaharia <matei.zaharia@gmail.com>,"Thu, 6 Feb 2014 20:03:21 -0800",Re: Discussion on strategy or roadmap should happen on dev@ list,dev@spark.incubator.apache.org,"Yes, notifications on every issue will be sent to dev@spark.incubator.apache.org. You can filter them out though by matching on (sender = jira@spark-project.atlassian.net AND to = dev@spark.incubator.apache.org). Should be fairly straightforward with Gmail.

Matei


instead of just the ones I’m watching on?  
(mailto:commits@spark.incubator.apache.org) actually.
for now, and we can move to a separate list later. If you’d like to filter them, the address is jira@spark-project.atlassian.net (mailto:jira@spark-project.atlassian.net).
appropriately. A
emailed
I guess we can
spammy. We’ll
<matei.zaharia@gmail.com (mailto:matei.zaharia@gmail.com)>
vehicle for
list
anytime if
to the
major
https://spark-project.atlassian.net/browse/SPARK-964,
I
other projects
mirrored to
<henry.saputra@gmail.com (mailto:henry.saputra@gmail.com)>
or
need to
community.
potential
discussion
which


"
Matei Zaharia <matei@mit.edu>,"Thu, 6 Feb 2014 20:07:06 -0800",Test,dev@spark.incubator.apache.org,"Just sending a test email from another email address to check for bounces..

"
Matei Zaharia <matei.zaharia@gmail.com>,"Thu, 6 Feb 2014 20:17:50 -0800",Re: Notice: JIRA messages will be forwarded to this list,dev@spark.incubator.apache.org,"BTW, I’ve set this up, but I think the emails are currently getting filtered, perhaps due to being HTML. I’ll look into who can fix that. Our old mailing list for issues at is working https://groups.google.com/forum/#!forum/spark-issues just fine...

Matei


on a mailing list, so I’m going to do this on the dev list for now. You can filter these messages by the sender, which will be jira@spark-project.atlassian.net. If the list becomes too spammy as a result, we can create a separate “issues” list later, but I just want to have something in place to archive them for now.


"
Henry Saputra <henry.saputra@gmail.com>,"Thu, 6 Feb 2014 20:34:06 -0800",Discussion on strategy or roadmap should happen on dev@ list,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I'd say let's try with dev@ list and see how ""spammy"" it is.

If it is get too noisy we could always create issues@ list.

- Henry

<matei.zaharia@gmail.com<javascript:_e(%7B%7D,'cvml','matei.zaharia@gmail.com');>>

sâ€? I guess we can
o spammy. Weâ€™ll
:
hicle for
ime if
e
r projects
rored to
.
.
"
Mridul Muralidharan <mridul@gmail.com>,"Fri, 7 Feb 2014 12:06:22 +0530",Re: [0.9.0] Possible deadlock in shutdown hook?,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Ideally, interrupting the thread writing to disk should be sufficient
- though since we are in middle of shutdown when this is happening, it
is best case effort anyway.
Identifying which threads to interrupt will be interesting since most
of them are driven by threadpool's and we cant list all threads and
interrupt all of them !


Regards,
Mridul



"
Mridul Muralidharan <mridul@gmail.com>,"Fri, 7 Feb 2014 12:07:58 +0530",Re: Is there any way to make a quick test on some pre-commit code?,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","This is neat, thanks Reynold !

Regards,
Mridul


"
Andrew Ash <andrew@andrewash.com>,"Thu, 6 Feb 2014 22:39:35 -0800",Re: [0.9.0] Possible deadlock in shutdown hook?,dev@spark.incubator.apache.org,"There is probably just one threadpool that has task threads -- is it
possible to enumerate and interrupt just those?  We may need to keep string
a reference to that threadpool through to the shutdown thread to make that
happen.



"
Patrick Wendell <pwendell@gmail.com>,"Thu, 6 Feb 2014 22:46:01 -0800",Re: Is there any way to make a quick test on some pre-commit code?,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","We should document this on the wiki!


"
Henry Saputra <henry.saputra@gmail.com>,"Thu, 6 Feb 2014 22:59:38 -0800",Re: Is there any way to make a quick test on some pre-commit code?,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>",1
Tathagata Das <tathagata.das1565@gmail.com>,"Thu, 6 Feb 2014 23:05:19 -0800",Re: [0.9.0] Possible deadlock in shutdown hook?,dev@spark.incubator.apache.org,"Its highly likely that the executor with the threadpool that runs the tasks
are the only set of threads that writes to disk. The tasks are designed to
be interrupted when the corresponding job is cancelled. So a reasonably
simple way could be to actually cancel the currently active jobs, which
would send the signal to the worker to stop the tasks. Currently, the
DAGScheduler<https://github.com/apache/incubator-spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L610>does
not seem to actually cancel the jobs, only mark them as failed. So it
may be a simple addition.

There may be some complications with the external spilling of shuffle data
to disk not stopping immediately when the task is marked for killing. Gotta
try it out.

TD


"
Andrew Ash <andrew@andrewash.com>,"Thu, 6 Feb 2014 23:30:15 -0800",Re: [0.9.0] Possible deadlock in shutdown hook?,dev@spark.incubator.apache.org,"That's genius.  Of course when a worker is told to shutdown it should
interrupt its worker threads -- I think that would address this issue.

Are you thinking to put

running.map(_.jobId).foreach { handleJobCancellation }

at the top of the StopDAGScheduler block?



"
Matei Zaharia <matei.zaharia@gmail.com>,"Thu, 6 Feb 2014 23:38:57 -0800",Re: [0.9.0] Possible deadlock in shutdown hook?,dev@spark.incubator.apache.org,"I don’t think we necessarily want to do this through the DAGScheduler because the worker might also shut down due to some unusual termination condition, like the driver node crashing. Can’t we do it at the top of the shutdown hook instead? If all the threads are in the same thread pool it might be possible to interrupt or stop the whole pool.

Matei


tasks
designed to
reasonably
which
https://github.com/apache/incubator-spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L610
data
Gotta
make
<mridul@gmail.com
sufficient
it
most
let
thread
machine).
sbin/start-all.sh
<andrew@andrewash.com
(Interpreted
(Interpreted
org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$receive$1.applyOrElse(java.lang.Object,

(Interpreted
scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(scala.concurrent.forkjoin.ForkJoinTask)
scala.concurrent.forkjoin.ForkJoinPool.runWorker(scala.concurrent.forkjoin.ForkJoinPool$WorkQueue)
(Interpreted

(Interpreted
(Interpreted
(Interpreted
frame)
frame)
frame)
frame)
org.apache.spark.util.Utils$$anonfun$deleteRecursively$1.apply(java.io.File)
org.apache.spark.util.Utils$$anonfun$deleteRecursively$1.apply(java.lang.Object)
scala.collection.IndexedSeqOptimized$class.foreach(scala.collection.IndexedSeqOptimized,
org.apache.spark.storage.DiskBlockManager$$anon$1$$anonfun$run$2.apply(java.io.File)
org.apache.spark.storage.DiskBlockManager$$anon$1$$anonfun$run$2.apply(java.lang.Object)
scala.collection.IndexedSeqOptimized$class.foreach(scala.collection.IndexedSeqOptimized,
is
was


"
Andy Konwinski <andykonwinski@gmail.com>,"Thu, 6 Feb 2014 23:40:03 -0800",Re: rough date for spark summit 2014,ameetkini@gmail.com,"We just announced this and tickets are available now. June 30 thru July 2
in downtown SF. Details at http://spark-summit.org

I know this is still a few months off and folks are rushing towards 0.9
release, but do the devs have a rough date for Spark Summit 2014? Looks
like it'll be in summer, but is it Jun / July / Aug / Sep ? Even
""late-summer"" would help.

Summer being a popular vacation time, a few months advance notice would be
greatly appreciated (read: I missed last summit due to a pre-scheduled
vacation and would hate to miss this one :)

Thanks,
Ameet

--
You received this message because you are subscribed to the Google Groups
""Unofficial Apache Spark Dev Mailing List Mirror"" group.
email to apache-spark-dev-mirror+unsubscribe@googlegroups.com.
For more options, visit https://groups.google.com/groups/opt_out.
"
Paul Brown <prb@mult.ifario.us>,"Thu, 6 Feb 2014 23:41:02 -0800",0.9.0 forces log4j usage,dev@spark.incubator.apache.org,"We have a few applications that embed Spark, and in 0.8.0 and 0.8.1, we
were able to use slf4j, but 0.9.0 broke that and unintentionally forces
direct use of log4j as the logging backend.

The issue is here in the org.apache.spark.Logging trait:

https://github.com/apache/incubator-spark/blame/master/core/src/main/scala/org/apache/spark/Logging.scala#L107

log4j-over-slf4j *always* returns an empty enumeration for appenders to the
ROOT logger:

https://github.com/qos-ch/slf4j/blob/master/log4j-over-slf4j/src/main/java/org/apache/log4j/Category.java?source=c#L81

And this causes an infinite loop and an eventual stack overflow.

I'm happy to submit a Jira and a patch, but it would be significant enough
reversal of recent changes that it's probably worth discussing before I
sink a half hour into it.  My suggestion would be that initialization (or
not) should be left to the user with reasonable default behavior supplied
by the spark commandline tooling and not forced on applications that
incorporate Spark.

Thoughts/opinions?

-- Paul
â€”
prb@mult.ifario.us | Multifarious, Inc. | http://mult.ifario.us/
"
Tathagata Das <tathagata.das1565@gmail.com>,"Thu, 6 Feb 2014 23:45:53 -0800",Re: [0.9.0] Possible deadlock in shutdown hook?,dev@spark.incubator.apache.org,"That definitely sound more reliable. Worth trying out if there is a
reliable way of reproducing the deadlock-like scenario.

TD



"
Andrew Ash <andrew@andrewash.com>,"Thu, 6 Feb 2014 23:49:40 -0800",Re: [0.9.0] Possible deadlock in shutdown hook?,dev@spark.incubator.apache.org,"I think we can enumerate all current threads with the ThreadMXBean, filter
to those threads with the name of executor pool in them, and interrupt them.

http://docs.oracle.com/javase/6/docs/api/java/lang/management/ManagementFactory.html#getThreadMXBean%28%29

The executor threads are currently named according to the pattern ""Executor
task launch worker-X""



"
Kostas Sakellis <kostas@cloudera.com>,"Thu, 6 Feb 2014 23:54:25 -0800",Re: Proposal for JIRA and Pull Request Policy,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>",1
Patrick Wendell <pwendell@gmail.com>,"Fri, 7 Feb 2014 00:03:04 -0800",Re: 0.9.0 forces log4j usage,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey Paul,

Thanks for digging this up. I worked on this feature and the intent
was to give users good default behavior if they didn't include any
logging configuration on the classpath.

The problem with assuming that CL tooling is going to fix the job is
that many people link against spark as a library and run their
application using their own scripts. In this case the first thing
people see when they run an application that links against Spark was a
big ugly logging warning.

I'm not super familiar with log4j-over-slf4j, but this behavior of
returning null for the appenders seems a little weird. What is the use
case for using this and not just directly use slf4j-log4j12 like Spark
itself does?

Did you have a more general fix for this in mind? Or was your plan to
just revert the existing behavior... We might be able to add a
configuration option to disable this logging default stuff. Or we
could just rip it out - but I'd like to avoid that if possible.

- Patrick


"
Patrick Wendell <pwendell@gmail.com>,"Fri, 7 Feb 2014 00:05:40 -0800",Re: 0.9.0 forces log4j usage,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","A config option e.g. could just be to add:

spark.logging.loadDefaultLogger (default true)
If set to true, Spark will try to initialize a log4j logger if none is
detected. Otherwise Spark will not modify logging behavior.

Then users could just set this to false if they have a logging set-up
that conflicts with this.

Maybe there is a nicer fix...


"
Tathagata Das <tathagata.das1565@gmail.com>,"Fri, 7 Feb 2014 00:13:57 -0800",Re: [0.9.0] Possible deadlock in shutdown hook?,dev@spark.incubator.apache.org,"Or we can try adding a shutdown hook in the
Executor<https://github.com/apache/incubator-spark/blob/master/core/src/main/scala/org/apache/spark/executor/Executor.scala?source=c#L127>to
call threadPool.shutdownNow(). May have to catch the
InterruptedException and handle it gracefully out
here<https://github.com/apache/incubator-spark/blob/master/core/src/main/scala/org/apache/spark/executor/Executor.scala?source=c#L255>
.

TD



"
Andrew Ash <andrew@andrewash.com>,"Fri, 7 Feb 2014 00:21:09 -0800",Re: [0.9.0] Possible deadlock in shutdown hook?,dev@spark.incubator.apache.org,"An additional shutdown hook to stop the threadpool is much more elegant
than the name matching and thread interrupting I was thinking about.  That
Javadoc looks like it's a best-effort shutdown and won't hard kill threads,
but that's at least a step forward from current behavior.

http://docs.oracle.com/javase/7/docs/api/java/util/concurrent/ThreadPoolExecutor.html#shutdownNow()



"
Tathagata Das <tathagata.das1565@gmail.com>,"Fri, 7 Feb 2014 00:31:08 -0800",Re: [0.9.0] Possible deadlock in shutdown hook?,dev@spark.incubator.apache.org,"I think if the threads in the threadpool catch and ignore
InterruptedException then those thread cant be stopped. So there is not
guarantee, but it will probably most of the time. Unless some user code
catches Interrupted exception.
We can probably first try to
kill<https://github.com/apache/incubator-spark/blob/master/core/src/main/scala/org/apache/spark/executor/Executor.scala?source=c#L255>
the
currently running
tasks<https://github.com/apache/incubator-spark/blob/master/core/src/main/scala/org/apache/spark/executor/Executor.scala?source=c#L130>,
which will make an attempt to ""gracefully"" shut them down. That probably
cannot be overriden by user code. Then use the threadpool.shutdownNow.
Double whammy!

TD



"
Mridul Muralidharan <mridul@gmail.com>,"Fri, 7 Feb 2014 14:21:00 +0530",Re: [0.9.0] Possible deadlock in shutdown hook?,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","This looks like the most reasonable approach to resolve this !

Regards,
Mridul



"
Andrew Ash <andrew@andrewash.com>,"Fri, 7 Feb 2014 00:53:49 -0800",Re: [0.9.0] Possible deadlock in shutdown hook?,dev@spark.incubator.apache.org,"Agreed.  Also I'm happy to test any patches since I have a consistent repro
now (see one of my first responses in this thread)



"
Paul Brown <prb@mult.ifario.us>,"Fri, 7 Feb 2014 00:54:30 -0800",Re: 0.9.0 forces log4j usage,dev@spark.incubator.apache.org,"Hi, Patrick --

Spark is shipped) or you can route log4j through slf4j and then on to a
different backend (e.g., logback).  We're doing the latter and manipulating
the dependencies in the build because that's the way the enclosing
application is set up.

The issue with the current situation is that there's no way for an end user
to choose to *not* use the log4j backend.  (My short-term solution was to
use the Maven shade plugin to swap in a version of the Logging trait with
the body of that method commented out.)  In addition to the situation with
log4j-over-slf4j and the empty enumeration of ROOT appenders, you might
also run afoul of someone who intentionally configured log4j with an empty
set of appenders at the time that Spark is initializing.

I'd be happy with any implementation that lets me choose my logging
backend: override default behavior via system property, plug-in
architecture, etc.  I do think it's reasonable to expect someone digesting
a substantial JDK-based system like Spark to understand how to initialize
logging â€” surely they're using logging of some kind elsewhere in their
application â€” but if you want the default behavior there as a courtesy, it
might be worth putting an INFO (versus a the glaring log4j WARN) message on
the output that says something like ""Initialized default logging via Log4J;
pass -Dspark.logging.loadDefaultLogger=false to disable this behavior."" so
that it's both convenient and explicit.

Cheers.
-- Paul






â€”
prb@mult.ifario.us | Multifarious, Inc. | http://mult.ifario.us/


:

e
s
a/org/apache/spark/Logging.scala#L107
o
a/org/apache/log4j/Category.java?source=c#L81
I
"
Koert Kuipers <koert@tresata.com>,"Fri, 7 Feb 2014 09:36:05 -0500",Re: 0.9.0 forces log4j usage,dev@spark.incubator.apache.org,"Totally agree with Paul: a library should not pick the slf4j backend. It
defeats the purpose of slf4j. That big ugly warning is there to alert
people that its their responsibility to pick the back end...

"
jfarrell <git@git.apache.org>,"Fri,  7 Feb 2014 15:46:51 +0000 (UTC)",[GitHub] incubator-spark pull request: ,dev@spark.incubator.apache.org,"Github user jfarrell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/557#issuecomment-34453405
  
    testing updates to webhook for issue comments


"
Henry Saputra <henry.saputra@gmail.com>,"Fri, 7 Feb 2014 08:41:33 -0800",Fwd: Github pull request hooks,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","FYI good news :)

---------- Forwarded message ----------
From: *Jake Farrell* <jfarrell@apache.org>
Date: Friday, February 7, 2014
Subject: Github pull request hooks
To: ""general@incubator.apache.org"" <general@incubator.apache.org>


I just wanted to follow up on my previous comments about the Github
webhooks not posting comments, Daniel Gruno and I have been debugging the
Github webhooks we had in place to send pull request notifications and we
have fixed it so comments for issues and pull requests will now go to the
dev@ lists. If anyone replies to this mail it will stay on the dev@ list.

-Jake


NOTE: This still remains a project/PMC responsibility to ensure
communication is recorded as this is coming from a 3rd party we do not
control. Communications should be encouraged to occur on our mailing lists.
We (infra) are working to make things easier and allow for better
integrations and are always open to anyone wanting to help contribute to
these efforts.
"
mateiz <git@git.apache.org>,"Fri,  7 Feb 2014 17:06:57 +0000 (UTC)",[GitHub] incubator-spark pull request: ,dev@spark.incubator.apache.org,"Github user mateiz commented on the pull request:

    https://github.com/apache/incubator-spark/pull/543#issuecomment-34472787
  
    Jenkins, this is ok to test


"
mateiz <git@git.apache.org>,"Fri,  7 Feb 2014 17:07:03 +0000 (UTC)",[GitHub] incubator-spark pull request: ,dev@spark.incubator.apache.org,"Github user mateiz commented on the pull request:

    https://github.com/apache/incubator-spark/pull/551#issuecomment-34472816
  
    Jenkins, this is ok to test


"
AmplabJenkins <git@git.apache.org>,"Fri,  7 Feb 2014 17:08:02 +0000 (UTC)",[GitHub] incubator-spark pull request: ,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/551#issuecomment-34473018
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Fri,  7 Feb 2014 17:08:02 +0000 (UTC)",[GitHub] incubator-spark pull request: ,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/551#issuecomment-34473020
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Fri,  7 Feb 2014 17:08:03 +0000 (UTC)",[GitHub] incubator-spark pull request: ,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/543#issuecomment-34473027
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Fri,  7 Feb 2014 17:08:03 +0000 (UTC)",[GitHub] incubator-spark pull request: ,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/543#issuecomment-34473028
  
    Merged build started.


"
Matei Zaharia <matei.zaharia@gmail.com>,"Fri, 7 Feb 2014 09:09:52 -0800",Re: Notice: JIRA messages will be forwarded to this list,dev@spark.incubator.apache.org,"FYI, it looks like JIRA notifications are still not quite being forwarded, but GitHub ones now are, including comments. (Thanks to Jake Ferrel for setting this up!). If you need to filter out the GitHub ones, they all come from git@git.apache.org.

This list is about to get a lot busier… When we create a TLP, we will likely create a separate notifications list, but sorry if you find yourself flooded with messages in the meantime.

Matei


filtered, perhaps due to being HTML. I’ll look into who can fix that. Our old mailing list for issues at is working https://groups.google.com/forum/#!forum/spark-issues just fine...
on a mailing list, so I’m going to do this on the dev list for now. You can filter these messages by the sender, which will be jira@spark-project.atlassian.net. If the list becomes too spammy as a result, we can create a separate “issues” list later, but I just want to have something in place to archive them for now.


"
AmplabJenkins <git@git.apache.org>,"Fri,  7 Feb 2014 17:34:42 +0000 (UTC)",[GitHub] incubator-spark pull request: ,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/551#issuecomment-34477732
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Fri,  7 Feb 2014 17:34:43 +0000 (UTC)",[GitHub] incubator-spark pull request: ,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/551#issuecomment-34477734
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12617/


"
AmplabJenkins <git@git.apache.org>,"Fri,  7 Feb 2014 17:35:50 +0000 (UTC)",[GitHub] incubator-spark pull request: ,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/543#issuecomment-34478004
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12618/


"
AmplabJenkins <git@git.apache.org>,"Fri,  7 Feb 2014 17:35:50 +0000 (UTC)",[GitHub] incubator-spark pull request: ,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/543#issuecomment-34478001
  
    Merged build finished.


"
Patrick Wendell <pwendell@gmail.com>,"Fri, 7 Feb 2014 10:08:43 -0800",Re: 0.9.0 forces log4j usage,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey Guys,

Thanks for explainning. Ya this is a problem - we didn't really know
that people are using other slf4j backends, slf4j is in there for
historical reasons but I think we may assume in a few places that
log4j is being used and we should minimize those.

We should patch this and get a fix into 0.9.1. So some solutions I see are:

(a) Add SparkConf option to disable this. I'm fine with this one.

(b) Ask slf4j which backend is active and only try to enforce this
default if we know slf4j is using log4j. Do either of you know if this
is possible? Not sure if slf4j exposes this.

(c) Just remove this default stuff. We'd rather not do this. The goal
of this thing is to provide good usability for people who have linked
against Spark and haven't done anything to configure logging. For
beginners we try to minimize the assumptions about what else they know
about, and I've found log4j configuration is a huge mental barrier for
people who are getting started.

Paul if you submit a patch doing (a) we can merge it in. If you have
any idea if (b) is possible I prefer that one, but it may not be
possible or might be brittle.

- Patrick


"
Patrick Wendell <pwendell@gmail.com>,"Fri, 7 Feb 2014 10:11:15 -0800",Re: 0.9.0 forces log4j usage,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","This also seems relevant - but not my area of expertise (whether this
is a valid way to check this).

http://stackoverflow.com/questions/10505418/how-to-find-which-library-slf4j-has-bound-itself-to


"
AmplabJenkins <git@git.apache.org>,"Fri,  7 Feb 2014 18:23:15 +0000 (UTC)",[GitHub] incubator-spark pull request: ,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-34483415
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Fri,  7 Feb 2014 18:23:15 +0000 (UTC)",[GitHub] incubator-spark pull request: ,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-34483416
  
    Merged build started.


"
Will Benton <willb@redhat.com>,"Fri, 7 Feb 2014 13:28:52 -0500 (EST)",Re: Proposal for Spark Release Strategy,dev@spark.incubator.apache.org,"Semantic versioning is great, and I think the proposed extensions for adopting it in Spark make a lot of sense.  However, by focusing strictly on public APIs, semantic versioning only solves part of the problem (albeit certainly the most interesting part).  I'd like to raise another issue that the semantic versioning guidelines explicitly exclude: the relative stability of dependencies and dependency versions.  This is less of a concern for end-users than it is for downstream packagers, but I believe that the relative stability of a dependency stack *should* be part of what is implied by a major version number.

Here are some suggestions for how to incorporate dependency stack versioning into semantic versioning in order to make life easier for downstreams; please consider all of these to be prefaced with ""If at all possible,"":

1.  Switching a dependency to an incompatible version should be reserved for major releases.  In general, downstream operating system distributions support only one version of each library, although in rare cases alternate versions are available for backwards compatibility.  If a bug fix or feature addition in a patch or minor release depends on adopting a version of some library that is incompatible with the one used by the prior patch or minor release, then downstreams may not be able to incorporate the fix or functionality until every package impacted by the dependency can be updated to work with the new version.

2.  New dependencies should only be introduced with new features (and thus with new minor versions).  This suggestion is probably uncontroversial, since features are more likely than bugfixes to require additional external libraries.

3.  The scope of new dependencies should be proportional to the benefit that they provide.  Of course, we want to avoid reinventing the wheel, but if the alternative is pulling in a framework for WheelFactory generation, a WheelContainer library, and a dozen transitive dependencies, maybe it's worth considering reinventing at least the simplest and least general wheels.

4.  If new functionality requires additional dependencies, it should be developed to work with the most recent stable version of those libraries that is generally available.  Again, since downstreams typically support only one version per library at a time, this will make their job easier.  (This will benefit everyone, though, since the most recent version of some dependency is more likely to see active maintenance efforts.)

5.  Dependencies can be removed at any time.

I hope these can be a starting point for further discussion and adoption of practices that demarcate the scope of dependency changes in a given version stream.



best,
wb


	by minotaur.apache.org (Postfix) with SMTP id E4A28102EE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  7 Feb 2014 18:31:39 +0000 (UTC)
Received: (qmail 50215 invoked by uid 500); 7 Feb 2014 18:31:38 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 50145 invoked by uid 500); 7 Feb 2014 18:31:38 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 50137 invoked by uid 99); 7 Feb 2014 18:31:38 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 07 Feb 2014 18:31:38 +0000
X-ASF-Spam-Status: No, hits=-2000.5 required=5.0
	tests=ALL_TRUSTED,RP_MATCHES_RCVD
X-Spam-Check-By: apache.org
Received: from [140.211.11.3] (HELO mail.apache.org) (140.211.11.3)
    by apache.org (qpsmtpd/0.29) with SMTP; Fri, 07 Feb 2014 18:31:36 +0000
Received: (qmail 50070 invoked by uid 99); 7 Feb 2014 18:31:14 -0000
Received: from tyr.zones.apache.org (HELO tyr.zones.apache.org) (140.211.11.114)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 07 Feb 2014 18:31:14 +0000
Received: by tyr.zones.apache.org (Postfix, from userid 65534)
	id 52EFF91F5D4; Fri,  7 Feb 2014 18:31:14 +0000 (UTC)
From: kayousterhout <git@git.apache.org>
To: dev@spark.incubator.apache.org
Reply-To: dev@spark.incubator.apache.org
Subject: [GitHub] incubator-spark pull request: 
Content-Type: text/plain
Message-Id: <20140207183114.52EFF91F5D4@tyr.zones.apache.org>
Date: Fri,  7 Feb 2014 18:31:14 +0000 (UTC)
X-Virus-Checked: Checked by ClamAV on apache.org

Github user kayousterhout commented on the pull request:

    https://github.com/apache/incubator-spark/pull/517#issuecomment-34484190
  
    I don't know of any precommit scripts (I think there's been talk of adding a general style checker script but AFAIK it hasn't been done yet); I just add highlighting in my editor so it's obvious when I'm writing lines that are longer than 100 characters.


"
mridulm <git@git.apache.org>,"Fri,  7 Feb 2014 18:33:53 +0000 (UTC)",[GitHub] incubator-spark pull request: ,dev@spark.incubator.apache.org,"Github user mridulm commented on the pull request:

    https://github.com/apache/incubator-spark/pull/517#issuecomment-34484468
  
    I am hoping that the PR Prashant Sharma submitted would also include
    ability to check these things once committed !
    Thanks Kay
    
    
    
    > I don't know of any precommit scripts (I think there's been talk of adding
    > a general style checker script but AFAIK it hasn't been done yet); I just
    > add highlighting in my editor so it's obvious when I'm writing lines that
    > are longer than 100 characters.
    >
    > --
    > Reply to this email directly or view it on GitHub<https://github.com/apache/incubator-spark/pull/517#issuecomment-34484190>
    > .
    >


"
aarondav <git@git.apache.org>,"Fri,  7 Feb 2014 18:39:19 +0000 (UTC)",[GitHub] incubator-spark pull request: ,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/517#issuecomment-34486235
  
    PR #557 does indeed add an automatic style checker which includes checking line lengths.


"
AmplabJenkins <git@git.apache.org>,"Fri,  7 Feb 2014 18:41:47 +0000 (UTC)",[GitHub] incubator-spark pull request: ,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-34486470
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Fri,  7 Feb 2014 18:41:47 +0000 (UTC)",[GitHub] incubator-spark pull request: ,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-34486471
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12619/


"
Koert Kuipers <koert@tresata.com>,"Fri, 7 Feb 2014 14:09:08 -0500",Re: 0.9.0 forces log4j usage,dev@spark.incubator.apache.org,"the issue is that slf4j uses static binding. you can put only one slf4j
backend on the classpath, and that's what it uses. more than one is not
allowed.

so you either keep the slf4j-log4j12 dependency for spark, and then you
took away people's choice of slf4j backend which is considered bad form for
a library, or you do not include it and then people will always get the big
fat ugly warning and slf4j logging will not flow to log4j.

including log4j itself is not necessary a problem i think?



"
Koert Kuipers <koert@tresata.com>,"Fri, 7 Feb 2014 14:14:02 -0500",Re: 0.9.0 forces log4j usage,dev@spark.incubator.apache.org,"well ""static binding"" is probably the wrong terminology but you get the
idea. multiple backends are not allowed and cause an even uglier warning...

see also here:
https://github.com/twitter/scalding/pull/636
and here:
https://groups.google.com/forum/#!topic/cascading-user/vYvnnN_15ls
all me being annoying and complaining about slf4j-log4j12 dependencies
(which did get removed).



"
AmplabJenkins <git@git.apache.org>,"Fri,  7 Feb 2014 19:28:09 +0000 (UTC)",[GitHub] incubator-spark pull request: ,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/517#issuecomment-34490968
  
    Build started.


"
AmplabJenkins <git@git.apache.org>,"Fri,  7 Feb 2014 19:28:09 +0000 (UTC)",[GitHub] incubator-spark pull request: ,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/517#issuecomment-34490967
  
     Build triggered.


"
Patrick Wendell <pwendell@gmail.com>,"Fri, 7 Feb 2014 11:31:15 -0800",Re: 0.9.0 forces log4j usage,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Koert - my suggestion was this. We let users use any slf4j backend
they want. If we detect that they are using the log4j backend and
*also* they didn't configure any log4j appenders, we set up some nice
defaults for them. If they are using another backend, Spark doesn't
try to modify the configuration at all.


"
Henry Saputra <henry.saputra@gmail.com>,"Fri, 7 Feb 2014 11:35:10 -0800",[TODO] Document the release process for Apache Spark,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi Patrick,

As part of the unofficial checklist for graduation, we need to have a
documented steps to make a release.

As the first and so far the only RE for Apache Spark, I would like to
ask for your help to document the steps to release. This will help
other member to do the release and take turns to make sure all future
PMCs and committers know how to do Apache Spark release.

Most of the steps are probably similar to other projects but it is
always useful for each podling to have its own documentation to
release artifacts.

Really appreciate your help.


Thanks,

- Henry

"
pwendell <git@git.apache.org>,"Fri,  7 Feb 2014 19:38:52 +0000 (UTC)",[GitHub] incubator-spark pull request: ,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/517#issuecomment-34492639
  
    @mridulm - we are adding checking for line lengths and some basic checks... also many IDE's can be configured to enforce style guidelines.


"
schmit <git@git.apache.org>,"Fri,  7 Feb 2014 19:42:59 +0000 (UTC)",[GitHub] incubator-spark pull request: ,dev@spark.incubator.apache.org,"Github user schmit commented on the pull request:

    https://github.com/apache/incubator-spark/pull/550#issuecomment-34493669
  
    @srowen I have updated and abstracted the evaluation, it should now also work for the SVM classifier and for other classifiers as well when they get implemented.


"
AmplabJenkins <git@git.apache.org>,"Fri,  7 Feb 2014 19:43:02 +0000 (UTC)",[GitHub] incubator-spark pull request: ,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/550#issuecomment-34493700
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Fri,  7 Feb 2014 19:43:02 +0000 (UTC)",[GitHub] incubator-spark pull request: ,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/550#issuecomment-34493701
  
    Merged build started.


"
Patrick Wendell <pwendell@gmail.com>,"Fri, 7 Feb 2014 11:46:20 -0800",Re: [TODO] Document the release process for Apache Spark,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey Henry,

Let me document this on the wiki. I've already keep pretty thorough
docs on this I just need to migrate them to the wiki. I've created a
JIRA here:

https://spark-project.atlassian.net/browse/SPARK-1066

- Patrick


"
Nan Zhu <zhunanmcgill@gmail.com>,"Fri, 7 Feb 2014 14:50:29 -0500","Is it possible to grant the edit permission to the issue
 report?",dev@spark.incubator.apache.org,"Hi, all 

Sometimes, you report an issue, you try to fix by yourself, then you found that the involved scope of the code base was not understood correctly, or your description of the issue may not be so complete/accurate

but currently, it seems that the reporter has to comment under the original post, 

I think it would be nice if the reporter can revise their original issue description in the above cases

Is it possible to do it?

Best, 

-- 
Nan Zhu

"
Henry Saputra <henry.saputra@gmail.com>,"Fri, 7 Feb 2014 11:51:35 -0800",Re: [TODO] Document the release process for Apache Spark,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Cool, Thanks Patrick! Really appreciate it =)

- Henry


"
Henry Saputra <henry.saputra@gmail.com>,"Fri, 7 Feb 2014 11:53:05 -0800",Re: Is it possible to grant the edit permission to the issue report?,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi Nan Zhu,

Are you talking about JIRA or Github pull request?

- Henry


"
Nan Zhu <zhunanmcgill@gmail.com>,"Fri, 7 Feb 2014 14:57:30 -0500","Re: Is it possible to grant the edit permission to the issue
 report?",dev@spark.incubator.apache.org,"Hi, Henry  

Itâ€™s JIRA, like https://spark-project.atlassian.net/browse/SPARK-1060?jql=project%20%3D%20SPARK

Best,  

--  
Nan Zhu




found that the involved scope of the code base was not understood correctly, or your description of the issue may not be so complete/accurate
iginal post,
sue description in the above cases


"
Andrew Ash <andrew@andrewash.com>,"Fri, 7 Feb 2014 11:55:02 -0800",Re: Is it possible to grant the edit permission to the issue report?,dev@spark.incubator.apache.org,"JIRA I'd guess -- I observed the same thing. Reporters should be able to
edit their own bug!

Sent from my mobile phone

"
AmplabJenkins <git@git.apache.org>,"Fri,  7 Feb 2014 19:55:18 +0000 (UTC)",[GitHub] incubator-spark pull request: ,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/517#issuecomment-34495304
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12620/


"
AmplabJenkins <git@git.apache.org>,"Fri,  7 Feb 2014 19:55:18 +0000 (UTC)",[GitHub] incubator-spark pull request: ,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/517#issuecomment-34495302
  
    Build finished.


"
kayousterhout <git@git.apache.org>,"Fri,  7 Feb 2014 20:09:07 +0000 (UTC)",[GitHub] incubator-spark pull request: ,dev@spark.incubator.apache.org,"Github user kayousterhout commented on the pull request:

    https://github.com/apache/incubator-spark/pull/548#issuecomment-34496801
  
    Jenkins, test this please


"
AmplabJenkins <git@git.apache.org>,"Fri,  7 Feb 2014 20:10:15 +0000 (UTC)",[GitHub] incubator-spark pull request: ,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/550#issuecomment-34496900
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12621/


"
AmplabJenkins <git@git.apache.org>,"Fri,  7 Feb 2014 20:10:14 +0000 (UTC)",[GitHub] incubator-spark pull request: ,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/550#issuecomment-34496898
  
    Merged build finished.


"
Sean Owen <sowen@cloudera.com>,"Fri, 7 Feb 2014 15:25:16 -0500",Re: 0.9.0 forces log4j usage,dev@spark.incubator.apache.org,"As a slf4j user, FWIW, I think this approach is fine. Just note that
you will have to handle log4j classes via reflection if they are not
going to always be on the user classpath.

Is it sufficient to bundle log4j.properties? no programmatic config
mess then. I don't know log4j enough to know if that accomplishes the
goal.

While we're on the topic, I see that additional slf4j shims are in
place to route java.util.logging calls through slf4j and therefore on
to log4j or whatever for centralized config. The same ought to be done
for commons-logging, no? and we also should really exclude log4j and
include log4j-to-slf4j if we really want people to be able to use
something besides log4j.

I can easily whip up a PR if anyone thinks that's a good idea.



"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Fri, 7 Feb 2014 12:31:46 -0800",Re: Notice: JIRA messages will be forwarded to this list,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","All the emails I get from github seem to have the same subject line
""[GitHub] incubator-spark pull request:"" and get grouped under the
same thread in gmail -- Is there a way to put the pull request title
in the email subject ?

Thanks
Shivaram

te:
, but GitHub ones now are, including comments. (Thanks to Jake Ferrel for setting this up!). If you need to filter out the GitHub ones, they all come from git@git.apache.org.
ikely create a separate notifications list, but sorry if you find yourself flooded with messages in the meantime.
:
ered, perhaps due to being HTML. I'll look into who can fix that. Our old mailing list for issues at is working https://groups.google.com/forum/#!forum/spark-issues just fine...
e:
n a mailing list, so I'm going to do this on the dev list for now. You can filter these messages by the sender, which will be jira@spark-project.atlassian.net. If the list becomes too spammy as a result, we can create a separate ""issues"" list later, but I just want to have something in place to archive them for now.

"
Humbedooh <git@git.apache.org>,"Fri,  7 Feb 2014 20:34:14 +0000 (UTC)",[GitHub] incubator-spark pull request: ,dev@spark.incubator.apache.org,"Github user Humbedooh commented on the pull request:

    https://github.com/apache/incubator-spark/pull/557#issuecomment-34499399
  
    Testing whether the title of the issue appears in the email subject or not, please ignore this comment and any more I may or may not have to send to get this fixed :)


"
Aaron Davidson <ilikerps@gmail.com>,"Fri, 7 Feb 2014 12:37:15 -0800",Re: Notice: JIRA messages will be forwarded to this list,"dev@spark.incubator.apache.org, 
	Shivaram Venkataraman <shivaram@eecs.berkeley.edu>","Also, it may be intentional but the only PR-level comments seem to get
forwarded. Comments on code are not sent.



"
Humbedooh <git@git.apache.org>,"Fri,  7 Feb 2014 20:43:08 +0000 (UTC)",[GitHub] incubator-spark pull request: ,dev@spark.incubator.apache.org,"Github user Humbedooh commented on the pull request:

    https://github.com/apache/incubator-spark/pull/557#issuecomment-34500486
  
    Looks like I'll have to add some more fake comments to track this down, apologies!


"
Humbedooh <git@git.apache.org>,"Fri,  7 Feb 2014 20:55:10 +0000 (UTC)","[GitHub] incubator-spark pull request: [WIP] SPARK-1058, Fix Style Errors a...",dev@spark.incubator.apache.org,"Github user Humbedooh commented on the pull request:

    https://github.com/apache/incubator-spark/pull/557#issuecomment-34501886
  


"
YongFeiWang <git@git.apache.org>,"Fri,  7 Feb 2014 21:00:45 +0000 (UTC)",[GitHub] incubator-spark pull request: Feibranches,dev@spark.incubator.apache.org,"Github user YongFeiWang closed the pull request at:

    https://github.com/apache/incubator-spark/pull/3


"
AmplabJenkins <git@git.apache.org>,"Fri,  7 Feb 2014 21:43:03 +0000 (UTC)",[GitHub] incubator-spark pull request: ROC AUC and Average precision metric...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/550#issuecomment-34506820
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Fri,  7 Feb 2014 21:43:03 +0000 (UTC)",[GitHub] incubator-spark pull request: ROC AUC and Average precision metric...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/550#issuecomment-34506818
  
     Merged build triggered.


"
Koert Kuipers <koert@tresata.com>,"Fri, 7 Feb 2014 17:02:36 -0500",Re: 0.9.0 forces log4j usage,dev@spark.incubator.apache.org,"got it. that sounds reasonable



"
CodingCat <git@git.apache.org>,"Fri,  7 Feb 2014 22:08:33 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-979] a LRU scheduler for load...,dev@spark.incubator.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/incubator-spark/pull/548#issuecomment-34511462
  
    what happened here? Jenkins dead?


"
AmplabJenkins <git@git.apache.org>,"Fri,  7 Feb 2014 22:08:50 +0000 (UTC)",[GitHub] incubator-spark pull request: ROC AUC and Average precision metric...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/550#issuecomment-34511509
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Fri,  7 Feb 2014 22:08:50 +0000 (UTC)",[GitHub] incubator-spark pull request: ROC AUC and Average precision metric...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/550#issuecomment-34511511
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12622/


"
Patrick Wendell <pwendell@gmail.com>,"Fri, 7 Feb 2014 14:23:34 -0800",Re: Proposal for Spark Release Strategy,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Will,

Thanks for these thoughts - this is something we should try to be
attentive to in the way we think about versioning.

(2)-(5) are pretty consistent with the guidelines we already follow. I
think the biggest proposed difference is to be conscious of (1), which
at least I had not given much thought to in the past. Specifically, if
we make major version upgrades of dependencies within a major release
of Spark, it can cause issues for downstream packagers. I can't easily
recall how often we do this or whether this will be hard for us to
guarantee (maybe others can...). It's something to keep in mind though
- thanks for bringing it up.

- Patrick

pting it in Spark make a lot of sense.  However, by focusing strictly on public APIs, semantic versioning only solves part of the problem (albeit certainly the most interesting part).  I'd like to raise another issue that the semantic versioning guidelines explicitly exclude: the relative stability of dependencies and dependency versions.  This is less of a concern for end-users than it is for downstream packagers, but I believe that the relative stability of a dependency stack *should* be part of what is implied by a major version number.
ing into semantic versioning in order to make life easier for downstreams; please consider all of these to be prefaced with ""If at all possible,"":
for major releases.  In general, downstream operating system distributions support only one version of each library, although in rare cases alternate versions are available for backwards compatibility.  If a bug fix or feature addition in a patch or minor release depends on adopting a version of some library that is incompatible with the one used by the prior patch or minor release, then downstreams may not be able to incorporate the fix or functionality until every package impacted by the dependency can be updated to work with the new version.
s with new minor versions).  This suggestion is probably uncontroversial, since features are more likely than bugfixes to require additional external libraries.
hat they provide.  Of course, we want to avoid reinventing the wheel, but if the alternative is pulling in a framework for WheelFactory generation, a WheelContainer library, and a dozen transitive dependencies, maybe it's worth considering reinventing at least the simplest and least general wheels.
eveloped to work with the most recent stable version of those libraries that is generally available.  Again, since downstreams typically support only one version per library at a time, this will make their job easier.  (This will benefit everyone, though, since the most recent version of some dependency is more likely to see active maintenance efforts.)
of practices that demarcate the scope of dependency changes in a given version stream.
py

"
tdas <git@git.apache.org>,"Fri,  7 Feb 2014 22:33:21 +0000 (UTC)",[GitHub] incubator-spark pull request: Refactored NetworkReceiver in Spark ...,dev@spark.incubator.apache.org,"GitHub user tdas opened a pull request:

    https://github.com/apache/incubator-spark/pull/558

    Refactored NetworkReceiver in Spark Streaming to allow for a 

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark graceful-shutdown

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/558.patch

----
commit eab351d05c0baef1d4b549e1581310087158d78d
Author: Tathagata Das <tathagata.das1565@gmail.com>
Date:   2014-01-23T06:17:15Z

    Update Spark Streaming Programming Guide.

commit 036a7d46187ea3f2a0fb8349ef78f10d6c0b43a9
Author: Tathagata Das <tathagata.das1565@gmail.com>
Date:   2014-01-23T06:17:42Z

    Merge remote-tracking branch 'apache/master' into docs-update

commit f06b964a51bb3b21cde2ff8bdea7d9785f6ce3a9
Author: Tathagata Das <tathagata.das1565@gmail.com>
Date:   2014-01-23T06:49:12Z

    Fixed missing endhighlight tag in the MLlib guide.

commit 6c29524639463f11eec721e4d17a9d7159f2944b
Author: Tathagata Das <tathagata.das1565@gmail.com>
Date:   2014-01-24T02:49:39Z

    Added example and figure for window operations, and links to Kafka and Flume API docs.

commit e3dcb46ab83d7071f611d9b5008ba6bc16c9f951
Author: Tathagata Das <tathagata.das1565@gmail.com>
Date:   2014-01-27T02:41:12Z

    Fixed docs on StreamingContext.getOrCreate.

commit d5b6196b532b5746e019b959a79ea0cc013a8fc3
Author: Tathagata Das <tathagata.das1565@gmail.com>
Date:   2014-01-27T04:15:58Z

    Added spark.streaming.unpersist config and info on StreamingListener interface.

commit 89a81ff25726bf6d26163e0dd938290a79582c0f
Author: Tathagata Das <tathagata.das1565@gmail.com>
Date:   2014-01-27T21:08:34Z

    Updated docs based on Patricks PR comments.

commit f338a60ae8069e0a382d2cb170227e5757cc0b7a
Author: Tathagata Das <tathagata.das1565@gmail.com>
Date:   2014-01-28T06:42:42Z

    More updates based on Patrick and Harvey's comments.

commit 34a5a6008dac2e107624c7ff0db0824ee5bae45f
Author: Tathagata Das <tathagata.das1565@gmail.com>
Date:   2014-01-29T02:02:28Z

    Updated github url to use SPARK_GITHUB_URL variable.

commit 18ff10556570b39d672beeb0a32075215cfcc944
Author: Tathagata Das <tathagata.das1565@gmail.com>
Date:   2014-01-29T05:49:30Z

    Fixed a lot of broken links.

commit 24165ff00a892809e10b5c39d4d86023260213bd
Author: Tathagata Das <tathagata.das1565@gmail.com>
Date:   2014-02-06T09:48:28Z

    Added graceful shutdown to Spark Streaming.

commit 20f10c0bac64e2a95a1cf7b44a7f440be0897d9c
Author: Tathagata Das <tathagata.das1565@gmail.com>
Date:   2014-02-07T22:07:15Z

    Refactored NetworkReceiver to clearly define the public API and separate all the code to run a receiver into NetworkReceiverHandler.

commit 177b12f8656aeeacf8125b86bb5c8baddf9b6650
Author: Tathagata Das <tathagata.das1565@gmail.com>
Date:   2014-02-07T22:16:13Z

    Merge remote-tracking branch 'apache/master' into graceful-shutdown
    
    Conflicts:
    	streaming/src/main/scala/org/apache/spark/streaming/receivers/ActorReceiver.scala

commit 1b1cbfcddfa020fea3e6e0ff2a1fa7c1c82f6b8f
Author: Tathagata Das <tathagata.das1565@gmail.com>
Date:   2014-02-07T22:19:28Z

    Revert accidental change in slaves file.

commit 281fead3a36836dc75083fe269062bd4b7014fd4
Author: Tathagata Das <tathagata.das1565@gmail.com>
Date:   2014-02-07T22:29:56Z

    Minor changes before PR.

----


"
tdas <git@git.apache.org>,"Fri,  7 Feb 2014 22:33:33 +0000 (UTC)",[GitHub] incubator-spark pull request: Refactored NetworkReceiver in Spark ...,dev@spark.incubator.apache.org,"Github user tdas closed the pull request at:

    https://github.com/apache/incubator-spark/pull/558


"
tdas <git@git.apache.org>,"Fri,  7 Feb 2014 22:53:49 +0000 (UTC)",[GitHub] incubator-spark pull request: Improved NetworkReceiver in Spark St...,dev@spark.incubator.apache.org,"GitHub user tdas opened a pull request:

    https://github.com/apache/incubator-spark/pull/559

    Improved NetworkReceiver in Spark Streaming to allow for a more graceful StreamingContext shutdown

    Current version of StreamingContext.stop() directly killed all the receivers without waiting for data received to be persisted and processed. This leads to a large window of data loss. This PR primarily addresses that. It also fixes the NetworkReceiver API to be more stable while allowing future optimizations. This has been a semi-public API (though not explicitly mentioned anywhere) and is not heavily used. However, for future stability, this API change is important.
    
    The detailed changes are as follows
    - Redefined NetworkReceiver public API for future stability. If any one has written their own NetworkReceiver using the [custom receiver guidelines](http://spark.incubator.apache.org/docs/latest/streaming-custom-receivers.html), then he/she can migrate very easily. Instead of creating the `blockGenerator` object and using `blockGenerator += data`  to push received data to Spark, one can now calls `store(data)`. The core interface of `onStart()` and `onStop()` remains the same.
    - Changed all the receivers to use the new API.
    - Added an additional flag to `StreamingContext.stop()` to specify whether to shutdown gracefully. Without setting the flag, the system will shutdown immediately (current default behavior). With the flag set, the system may take time to shutdown to ensure data received by the system is processed completely.
    - Clearly defined the behavior of `StreamingContext.start()` and `stop()` - calling stop() twice or calling stop() before start() will give no exception. However, calling start() twice or start() after stop() throws an exception as they are a strong indicator of incorrect control flow. 
    
    
    


You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark graceful-shutdown

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/559.patch

----
commit eab351d05c0baef1d4b549e1581310087158d78d
Author: Tathagata Das <tathagata.das1565@gmail.com>
Date:   2014-01-23T06:17:15Z

    Update Spark Streaming Programming Guide.

commit 036a7d46187ea3f2a0fb8349ef78f10d6c0b43a9
Author: Tathagata Das <tathagata.das1565@gmail.com>
Date:   2014-01-23T06:17:42Z

    Merge remote-tracking branch 'apache/master' into docs-update

commit f06b964a51bb3b21cde2ff8bdea7d9785f6ce3a9
Author: Tathagata Das <tathagata.das1565@gmail.com>
Date:   2014-01-23T06:49:12Z

    Fixed missing endhighlight tag in the MLlib guide.

commit 6c29524639463f11eec721e4d17a9d7159f2944b
Author: Tathagata Das <tathagata.das1565@gmail.com>
Date:   2014-01-24T02:49:39Z

    Added example and figure for window operations, and links to Kafka and Flume API docs.

commit e3dcb46ab83d7071f611d9b5008ba6bc16c9f951
Author: Tathagata Das <tathagata.das1565@gmail.com>
Date:   2014-01-27T02:41:12Z

    Fixed docs on StreamingContext.getOrCreate.

commit d5b6196b532b5746e019b959a79ea0cc013a8fc3
Author: Tathagata Das <tathagata.das1565@gmail.com>
Date:   2014-01-27T04:15:58Z

    Added spark.streaming.unpersist config and info on StreamingListener interface.

commit 89a81ff25726bf6d26163e0dd938290a79582c0f
Author: Tathagata Das <tathagata.das1565@gmail.com>
Date:   2014-01-27T21:08:34Z

    Updated docs based on Patricks PR comments.

commit f338a60ae8069e0a382d2cb170227e5757cc0b7a
Author: Tathagata Das <tathagata.das1565@gmail.com>
Date:   2014-01-28T06:42:42Z

    More updates based on Patrick and Harvey's comments.

commit 34a5a6008dac2e107624c7ff0db0824ee5bae45f
Author: Tathagata Das <tathagata.das1565@gmail.com>
Date:   2014-01-29T02:02:28Z

    Updated github url to use SPARK_GITHUB_URL variable.

commit 18ff10556570b39d672beeb0a32075215cfcc944
Author: Tathagata Das <tathagata.das1565@gmail.com>
Date:   2014-01-29T05:49:30Z

    Fixed a lot of broken links.

commit 24165ff00a892809e10b5c39d4d86023260213bd
Author: Tathagata Das <tathagata.das1565@gmail.com>
Date:   2014-02-06T09:48:28Z

    Added graceful shutdown to Spark Streaming.

commit 20f10c0bac64e2a95a1cf7b44a7f440be0897d9c
Author: Tathagata Das <tathagata.das1565@gmail.com>
Date:   2014-02-07T22:07:15Z

    Refactored NetworkReceiver to clearly define the public API and separate all the code to run a receiver into NetworkReceiverHandler.

commit 177b12f8656aeeacf8125b86bb5c8baddf9b6650
Author: Tathagata Das <tathagata.das1565@gmail.com>
Date:   2014-02-07T22:16:13Z

    Merge remote-tracking branch 'apache/master' into graceful-shutdown
    
    Conflicts:
    	streaming/src/main/scala/org/apache/spark/streaming/receivers/ActorReceiver.scala

commit 1b1cbfcddfa020fea3e6e0ff2a1fa7c1c82f6b8f
Author: Tathagata Das <tathagata.das1565@gmail.com>
Date:   2014-02-07T22:19:28Z

    Revert accidental change in slaves file.

commit 281fead3a36836dc75083fe269062bd4b7014fd4
Author: Tathagata Das <tathagata.das1565@gmail.com>
Date:   2014-02-07T22:29:56Z

    Minor changes before PR.

----


"
AmplabJenkins <git@git.apache.org>,"Fri,  7 Feb 2014 22:58:00 +0000 (UTC)",[GitHub] incubator-spark pull request: Improved NetworkReceiver in Spark St...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/559#issuecomment-34516697
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Fri,  7 Feb 2014 22:57:59 +0000 (UTC)",[GitHub] incubator-spark pull request: Improved NetworkReceiver in Spark St...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/559#issuecomment-34516699
  
    Merged build started.


"
Patrick Wendell <pwendell@gmail.com>,"Fri, 7 Feb 2014 15:01:16 -0800",Re: 0.9.0 forces log4j usage,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Paul,

Looking back at your problem. I think it's the one here:
http://www.slf4j.org/codes.html#log4jDelegationLoop

So let me just be clear what you are doing so I understand. You have
some other application that directly calls log4j. So you have to
include log4j-over-slf4j to route those logs through slf4j to logback.

At the same time you embed Spark in this application. In the past it
was fine, but now that Spark programmatic ally initializes log4j, it
screws up your application because log4j-over-slf4j doesn't work with
applications that do this explicilty as discussed here:
http://www.slf4j.org/legacy.html

Correct?

- Patrick


"
AmplabJenkins <git@git.apache.org>,"Fri,  7 Feb 2014 23:23:56 +0000 (UTC)",[GitHub] incubator-spark pull request: Improved NetworkReceiver in Spark St...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/559#issuecomment-34518331
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Fri,  7 Feb 2014 23:23:56 +0000 (UTC)",[GitHub] incubator-spark pull request: Improved NetworkReceiver in Spark St...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/559#issuecomment-34518332
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12623/


"
Dean Wampler <deanwampler@gmail.com>,"Fri, 7 Feb 2014 16:55:25 -0800",Re: [GitHub] incubator-spark pull request:,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","This SPAM is not doing anyone any good. How about another mailing list for people who want to see this?

Sent from my rotary phone. 


8
g


he/incubator-spark/pull/517#issuecomment-34484190>

"
shivaram <git@git.apache.org>,"Sat,  8 Feb 2014 01:06:02 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-979] a LRU scheduler for load...,dev@spark.incubator.apache.org,"Github user shivaram commented on the pull request:

    https://github.com/apache/incubator-spark/pull/548#issuecomment-34523513
  
    Jenkins, test this please


"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 01:08:06 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-979] a LRU scheduler for load...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/548#issuecomment-34523595
  
     Build triggered.


"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 01:08:07 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-979] a LRU scheduler for load...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/548#issuecomment-34523596
  
    Build started.


"
Paul Brown <prb@mult.ifario.us>,"Fri, 7 Feb 2014 17:14:48 -0800",Re: 0.9.0 forces log4j usage,dev@spark.incubator.apache.org,"Hi, Patrick --

That's close but not quite it.

The issue that occurs is not the delegation loop mentioned in slf4j
documentation.  The stack overflow is entirely within the code in the Spark
trait:

at org.apache.spark.Logging$class.initializeLogging(Logging.scala:112)
at org.apache.spark.Logging$class.initializeIfNecessary(Logging.scala:97)
at org.apache.spark.Logging$class.log(Logging.scala:36)
at org.apache.spark.SparkEnv$.log(SparkEnv.scala:94)


And then that repeats.

As for our situation, we exclude the slf4j-log4j12 dependency when we
import the Spark library (because we don't want to use log4j) and have
log4j-over-slf4j already in place to ensure that all of the logging in the
overall application runs through slf4j and then out through logback.  (We
also, as another poster already mentioned, also force jcl and jul through
slf4j.)

The zen of slf4j for libraries is that the library uses the slf4j API and
then the enclosing application can route logging as it sees fit.  Spark
master CLI would log via slf4j and include the slf4j-log4j12 backend; same
for Spark worker CLI.  Spark as a library (versus as a container) would not
include any backend to the slf4j API and leave this up to the application.
 (FWIW, this would also avoid your log4j warning message.)

But as I was saying before, I'd be happy with a situation where I can avoid
log4j being enabled or configured, and I think you'll find an existing
choice of logging framework to be a common scenario for those embedding
Spark in other systems.

Best.
-- Paul

â€”
prb@mult.ifario.us | Multifarious, Inc. | http://mult.ifario.us/



es
4j-has-bound-itself-to
r
t
 I
.
is
f
r
y
er
:
n
r
n
ne
N)
ng
if
ng
he
t
t
2
.
a/org/apache/spark/Logging.scala#L107
r
a/org/apache/log4j/Category.java?source=c#L81
"
Reynold Xin <rxin@databricks.com>,"Fri, 7 Feb 2014 17:19:18 -0800",Re: [GitHub] incubator-spark pull request:,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I concur wholeheartedly ...



"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 01:33:21 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-979] a LRU scheduler for load...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/548#issuecomment-34524466
  
    Build finished.


"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 01:33:22 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-979] a LRU scheduler for load...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/548#issuecomment-34524467
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12624/


"
Patrick Wendell <pwendell@gmail.com>,"Fri, 7 Feb 2014 17:38:57 -0800",Re: 0.9.0 forces log4j usage,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey Paul,

So if your goal is ultimately to output to logback. Then why don't you
just use slf4j and logback-classic.jar as described here [1]. Why
involve log4j-over-slf4j at all?

Let's say we refactored the spark build so it didn't advertise
slf4j-log4j12 as a dependency. Would you still be using
log4j-over-slf4j... or is this just a ""fix"" to deal with the fact that
Spark is somewhat log4j dependent at this point.

[1] http://www.slf4j.org/manual.html

- Patrick


"
Andrew Ash <andrew@andrewash.com>,"Fri, 7 Feb 2014 17:43:50 -0800",Re: [GitHub] incubator-spark pull request:,dev@spark.incubator.apache.org,"+1 on moving this stuff to a separate mailing list.  It's Apache policy
that discussion is archived, but it's not policy that it must be
interleaved with other dev discussion.  Let's move it to a
spark-github-discuss list (or a different name) and people "
Matei Zaharia <matei.zaharia@gmail.com>,"Fri, 7 Feb 2014 17:55:04 -0800",Re: [GitHub] incubator-spark pull request:,dev@spark.incubator.apache.org,"Sorry for the spam, guys. I’m going to work on getting a separate mailing list, but we needed to have an initial version of this quickly. Note that you can filter all these emails as they come from git@git.apache.org.

Matei


policy
see
list
https://github.com/apache/incubator-spark/pull/517#issuecomment-34484468
include
of
I
lines
https://github.com/apache/incubator-spark/pull/517#issuecomment-34484190


"
Paul Brown <prb@mult.ifario.us>,"Fri, 7 Feb 2014 17:57:26 -0800",Re: 0.9.0 forces log4j usage,dev@spark.incubator.apache.org,"Hi, Patrick --

I forget which other component is responsible, but we're using the
log4j-over-slf4j as part of an overall requirement to centralize logging,
i.e., *someone* else is logging over log4j and we're pulling that in.
 (There's also some jul logging from Jersey, etc.)

Goals:

- Fully control/capture all possible logging.  (God forbid we have to grab
System.out/err, but we'd do it if needed.)
- Use the backend we like best at the moment.  (Happens to be logback.)

Possible cases:

- If Spark used Log4j at all, we would pull in that logging via
log4j-over-slf4j.
- If Spark used only slf4j and referenced no backend, we would use it as-is
although we'd still have the log4j-over-slf4j because of other libraries.
- If Spark used only slf4j and referenced the slf4j-log4j12 backend, we
would exclude that one dependency (via our POM).

Best.
-- Paul


â€”
prb@mult.ifario.us | Multifarious, Inc. | http://mult.ifario.us/



7)
We
gh
nd
ce
et
s
e
ad
r
4j-has-bound-itself-to
ly
w
e
ou
s,
j
s
his
r.
d
ix
n
s
o
t:
a/org/apache/spark/Logging.scala#L107
a/org/apache/log4j/Category.java?source=c#L81
e
t
"
Chris Mattmann <mattmann@apache.org>,"Fri, 07 Feb 2014 18:02:04 -0800",Re: [GitHub] incubator-spark pull request:,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Guys this Github discussion seems like dev discussion in which case it
must be
on dev list and not moved - the whole point of this is that development,
including
conversations related to it, which are the lifeblood of the project should
occur
on the ASF mailing lists.

Refactoring the lists is one thing for the more automated messages, but the
comments below look like Kay commenting on some relevant stuff in which
case
I would argue against (paraphrased) ""moving it to some ASF list that those
who
care can subscribe to"". ""Those who care"" in this case should be people who
care about Kay's comments (which aren't automated commit messages from
some bot;
they are relevant dev comments) in which case ""those who care"" should be
the
PMC.

My suggestion is if there is a notifications list set up, it can be like
for
automated stuff - but *NOT* for dev discussion -- that needs to happen on
the
dev lists. If it's on another list, then I would expect periodically
(frequently;
with enough diligence to VOTE on and discuss and contribute to) to see that
flushed or summarized on the dev list.

Cheers,
Chris







"
chrismattmann <git@git.apache.org>,"Sat,  8 Feb 2014 02:06:08 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-979] a LRU scheduler for load...,dev@spark.incubator.apache.org,"Github user chrismattmann commented on the pull request:

    https://github.com/apache/incubator-spark/pull/548#issuecomment-34525458
  
    wait a sec - you guys told Jenkins to test this please and it frickin' did? Bad ass.


"
pwendell <git@git.apache.org>,"Sat,  8 Feb 2014 02:07:17 +0000 (UTC)",[GitHub] incubator-spark pull request: [WIP] SPARK-1067: Default log4j init...,dev@spark.incubator.apache.org,"GitHub user pwendell opened a pull request:

    https://github.com/apache/incubator-spark/pull/560

    [WIP] SPARK-1067: Default log4j initialization causes errors for those not using log4j

    To fix this - we add a check when initializing log4j.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark logging

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/560.patch

----
commit 6f871c7e614c3f2566bd1c5e1d4086b0e3b43fc7
Author: Patrick Wendell <pwendell@gmail.com>
Date:   2014-02-07T23:22:29Z

    Logging fix

----


"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 02:07:59 +0000 (UTC)",[GitHub] incubator-spark pull request: [WIP] SPARK-1067: Default log4j init...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/560#issuecomment-34525518
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 02:07:59 +0000 (UTC)",[GitHub] incubator-spark pull request: [WIP] SPARK-1067: Default log4j init...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/560#issuecomment-34525519
  
    Merged build started.


"
Patrick Wendell <pwendell@gmail.com>,"Fri, 7 Feb 2014 18:10:10 -0800",Re: 0.9.0 forces log4j usage,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Ah okay sounds good. This is what I meant earlier by ""You have
some other application that directly calls log4j.""... i.e. you have
for historical reasons installed the log4j-over-slf4j.

Would you mind trying out this fix and seeing if it works? This is
designed to be a hotfix for 0.9, not a general solution where we rip
out log4j from our published dependencies:

https://github.com/apache/incubator-spark/pull/560/files

- Patrick


"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 02:13:01 +0000 (UTC)",[GitHub] incubator-spark pull request: [WIP] SPARK-1067: Default log4j init...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/560#issuecomment-34525659
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 02:13:01 +0000 (UTC)",[GitHub] incubator-spark pull request: [WIP] SPARK-1067: Default log4j init...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/560#issuecomment-34525657
  
     Merged build triggered.


"
Henry Saputra <henry.saputra@gmail.com>,"Fri, 7 Feb 2014 18:23:55 -0800",Re: [GitHub] incubator-spark pull request:,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I am with Chris on this one.

These github notifications are similar to JIRA updates that in most
ASF projects are sent to dev@ list, and these are valid messages that
contributors in the project should concern about.

Especially the PPMCs (which willl be PMCs hopefully soon) need to know
about them and become audit trail/ archive of development discussions
for ASF.

We already have user@ list which targeted for people interested to ask
for questions using Spark and should be the proper list for people
interested on using Spark.

As Matei have said, you can filter these github notifications email easily.

Thanks,


- Henry



"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 02:35:12 +0000 (UTC)",[GitHub] incubator-spark pull request: [WIP] SPARK-1067: Default log4j init...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/560#issuecomment-34526331
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 02:35:15 +0000 (UTC)",[GitHub] incubator-spark pull request: [WIP] SPARK-1067: Default log4j init...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/560#issuecomment-34526332
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12625/


"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 02:40:20 +0000 (UTC)",[GitHub] incubator-spark pull request: [WIP] SPARK-1067: Default log4j init...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/560#issuecomment-34526461
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12626/


"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 02:40:20 +0000 (UTC)",[GitHub] incubator-spark pull request: [WIP] SPARK-1067: Default log4j init...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/560#issuecomment-34526460
  
    Merged build finished.


"
CodingCat <git@git.apache.org>,"Sat,  8 Feb 2014 04:29:37 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-979] a LRU scheduler for load...,dev@spark.incubator.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/incubator-spark/pull/548#issuecomment-34532680
  
    Why this will affect the correctness of test cases in streaming....and this error does not happen at all time...


"
shivaram <git@git.apache.org>,"Sat,  8 Feb 2014 04:33:25 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-979] a LRU scheduler for load...,dev@spark.incubator.apache.org,"Github user shivaram commented on the pull request:

    https://github.com/apache/incubator-spark/pull/548#issuecomment-34532851
  
    Jenkins, retest this please


"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 04:38:07 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-979] a LRU scheduler for load...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/548#issuecomment-34533112
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 04:38:07 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-979] a LRU scheduler for load...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/548#issuecomment-34533113
  
    Merged build started.


"
Nan Zhu <zhunanmcgill@gmail.com>,"Fri, 7 Feb 2014 23:58:25 -0500",Re: [GitHub] incubator-spark pull request:,dev@spark.incubator.apache.org,"If we reply these emails, will the reply be posted on pull request discussion board automatically? 

if yes, that would be very nice 

-- 
Nan Zhu





"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 05:05:12 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-979] a LRU scheduler for load...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/548#issuecomment-34534125
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12627/


"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 05:05:11 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-979] a LRU scheduler for load...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/548#issuecomment-34534124
  
    Merged build finished.


"
CodingCat <git@git.apache.org>,"Sat,  8 Feb 2014 05:07:25 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-979] a LRU scheduler for load...,dev@spark.incubator.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/incubator-spark/pull/548#issuecomment-34534153
  
    passed, what I have changed after the previous failure is 
    
    1. make LRU scheduling as optional, i.e. the default case is the ""round-robin""
    
    2. rebase with the master branch
    



"
CodingCat <git@git.apache.org>,"Sat,  8 Feb 2014 05:15:51 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-979] a LRU scheduler for load...,dev@spark.incubator.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/incubator-spark/pull/548#issuecomment-34534298
  
    I am still confused by the previous failure, how can this change interacts with the streaming recovery mechanism?
    
    actually, even without the above two changes, I cannot reproduce the failed case in my mbp


"
Reynold Xin <rxin@databricks.com>,"Fri, 7 Feb 2014 22:21:41 -0800","Re: [GitHub] incubator-spark pull request: Improved NetworkReceiver
 in Spark St...","""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","test



"
Reynold Xin <rxin@databricks.com>,"Fri, 7 Feb 2014 22:21:58 -0800",Re: [GitHub] incubator-spark pull request:,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I don't think it does.



"
Xuefeng Wu <benewu@gmail.com>,"Sat, 8 Feb 2014 14:34:56 +0800",Re: [GitHub] incubator-spark pull request:,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","github have this feature, but these mails are from git@git.apache.org.  I
think some github information are filtered.

https://github.com/blog/811-reply-to-comments-from-email



w
k
t
ld
y
:
:
re
8
o
en
0



-- 

~Yours, Xuefeng Wu/ÎâÑ©·å  ¾´ÉÏ
"
Qiuzhuang <git@git.apache.org>,"Sat,  8 Feb 2014 08:10:48 +0000 (UTC)",[GitHub] incubator-spark pull request: Kill drivers in postStop() for Worke...,dev@spark.incubator.apache.org,"GitHub user Qiuzhuang opened a pull request:

    https://github.com/apache/incubator-spark/pull/561

    Kill drivers in postStop() for Worker.

     JIRA SPARK-1068:https://spark-project.atlassian.net/browse/SPARK-1068

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark master

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/561.patch

----
commit 9c19ce63637eee9369edd235979288d3d9fc9105
Author: Qiuzhuang Lian <qiuzhuang.lian@gmail.com>
Date:   2014-02-08T08:07:39Z

    Kill drivers in postStop() for Worker.
     JIRA SPARK-1068:https://spark-project.atlassian.net/browse/SPARK-1068

----


"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 08:13:04 +0000 (UTC)",[GitHub] incubator-spark pull request: Kill drivers in postStop() for Worke...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/561#issuecomment-34538380
  
    Can one of the admins verify this patch?


"
jyotiska <git@git.apache.org>,"Sat,  8 Feb 2014 10:13:53 +0000 (UTC)",[GitHub] incubator-spark pull request: Added example Python code for sort,dev@spark.incubator.apache.org,"GitHub user jyotiska opened a pull request:

    https://github.com/apache/incubator-spark/pull/562

    Added example Python code for sort

    I added an example Python code for sort. Right now, PySpark has limited examples for new people willing to use the project. This example code sorts integers stored in a file. I was able to sort 5 million, 10 million and 25 million integers with this code.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark master

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/562.patch

----
commit 945e39a5d68daa7e5bab0d96cbd35d7c4b04eafb
Author: jyotiska <jyotiska123@gmail.com>
Date:   2014-02-08T07:29:09Z

    Added example python code for sort

commit 6f98f1e313f4472a7c2207d36c4f0fbcebc95a8c
Author: jyotiska <jyotiska123@gmail.com>
Date:   2014-02-08T07:42:37Z

    Updated python example code sort.py

----


"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 10:18:03 +0000 (UTC)",[GitHub] incubator-spark pull request: Added example Python code for sort,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/562#issuecomment-34540329
  
    Can one of the admins verify this patch?


"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 12:13:01 +0000 (UTC)","[GitHub] incubator-spark pull request: [WIP] SPARK-1058, Fix Style Errors a...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/557#issuecomment-34542412
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 12:13:01 +0000 (UTC)","[GitHub] incubator-spark pull request: [WIP] SPARK-1058, Fix Style Errors a...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/557#issuecomment-34542411
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 12:40:56 +0000 (UTC)","[GitHub] incubator-spark pull request: [WIP] SPARK-1058, Fix Style Errors a...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/557#issuecomment-34543052
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 12:40:56 +0000 (UTC)","[GitHub] incubator-spark pull request: [WIP] SPARK-1058, Fix Style Errors a...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/557#issuecomment-34543053
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12628/


"
"""=?UTF-8?B?5qyn6Ziz5pmLKOasp+mYs+aZiyk=?="" <jin.oyj@alibaba-inc.com>","Sat, 08 Feb 2014 22:13:49 +0800",=?UTF-8?B?562U5aSN77yaW0dpdEh1Yl0gaW5jdWJhdG9yLXNwYXJrIHB1bGwgcmVxdWVzdDo=?=,"""dev"" <dev@spark.incubator.apache.org>","I think may be like yarn , any JIRA creation will be forward to dev@ list , dev list also include discussion of new features and bugs , and all Jenkins build message for Pre commit . And any update of a specific JIRA, like assigne or comment will be forward to issues@ list. yarn also have a commit@ list if any svn ci happens . Maybe we can use some of it , it's just a advice ^_^

------------------------------------------------------------------
å‘ä»¶äººï¼šXuefeng Wu <benewu@gmail.com>
å‘é€æ—¶é—´ï¼š2014å¹´2æœˆ8æ—¥(æ˜ŸæœŸå…­) 14:34
æ”¶ä»¶äººï¼šdev@spark.incubator.apache.org <dev@spark.incubator.apache.org>
ä¸»ã€€é¢˜ï¼šRe: [GitHub] incubator-spark pull request:

github have this feature, but these mails are from git@git.apache.org.  I
think some github information are filtered.

https://github.com/blog/811-reply-to-comments-from-email


Oly be posted on pull request
> > discussion board automatically?
> >
> > if yes, that would be very nice
> >
> > --
> > Nan Zhu
> >
> >
>  > > I am with Chris on this one.
> > >
> > > These github notifications are similar to JIRA updates that in most
> > > ASF projects are sent to dev@ list, and these are valid messages that
> > > contributors in the project should concern about.
> > >
> > > Especially the PPMCs (which willl be PMCs hopefully soon) need to know
> > > about them and become audit trail/ archive of development discussions
> > > for ASF.
> > >
> > > We already have user@ list which targeted for people interested to ask
> > > for questions using Spark and should be the proper list for people
> > > interested on using Spark.
> > >
> > > As Matei have said, you can filter these github notifications email
> > easily.
> > >
> > > Thanks,
> > >
> > >
> > > - Guys this Github discussion seems like dev discussion in which case
> it
is that
> > development,
> > > > including
> > > > conversations related to it, which are the lifeblood of the project
> > should
> > > > occur
 one thing for the more automated messages,
> > but the
> > > > comments below look like Kay commenting on some relevant stuff in
> which
> > > > case
> > > > I would argue against (paraphrased) ""moving it to some ASF list that
> > those
> > > > who
> > > > care can subscribe to"". ""Those who care"" in this case should be
> people
> > who
> > > > care about Kay's comments (which aren't automated commit messages
> from
> > > > some bot;
> ere is a notifications list set up, it can be
> > like
> > > > for
> > > > automated stuff - but *NOT* for dev discussion -- that needs to
> happen
d expect periodically
> > > > (frequently;
> > > > with enough diligence to VOTE on and discuss and contribute to) to
> see
> > that
> > > > flushed or summarized on the dev list.
> > > >
> > > > Cheers,
> > > > Chris
 > > > From: Andrew Ash <andrew@andrewash.com (mailto:andrew@andrewash.com
> )>
> > > > Reply-To: ""dev@spark.incubator.apache.org (mailto:
> > dev@spark.incubator.apache.org)"" <dev@spark.incubator.apache.org(mailto:
> > dev@spark.incubator.apache.org)>
> > > > Date: Friday, February 7, 2014 5:43 PM
.apache.org)"" <dev@spark.incubator.apache.org(mailto:
> > dev@spark.incubator.apache.org)>
> > > > Subject: Re: [GitHub] incubator-spark pull request:
pache
> > policy
> > > > > that discussion is archived, but it's not policy that it must be
> > > > > interleaved with other dev discussion. Let's move it to a
> > > > > spark-github-discuss list (or a different name) and people who care
> > to see
> > > > > it can subscribe.
> > > > >
> > > > > 2014 at 4:55 PM, Dean Wampler <
> > deanwampler@gmail.com (mailto:deanwampl not doing anyone any good. How about another
> > mailing list
> > > > > > for
> > > > > > > people who want to see this?
> > > > > > >
> > > > > ridulm commented on the pull request:
> > > > > >
> > https://github.com/apache/incubator-spark/pull/517#issuecomment-34484468
> > > > > > > >
> > > > > > > > I am hoping that the PR Prashant Sharma submitted would also
> >  !
> > > > > > > > Thanks Kay
> > > > > > > >
> > > > > > > >
> > > >  > > > >
> > > > > > > > > I don't know of any precommit scripts (I think there's been
> > talk of
> > > > > > > adding
> > > > > > > > > a general style checker script but AFAIK it hasn't been
> done
> > yet);
> > > > > > > >
> > > > > > >
> > > > > > >
> > > > > >
> > > > > > I
> > > > > > > just
> > > > > > > > > add highlighting in my editor so it's obvious when I'm
> > writing
> > > > > > > >
> > > > > > >
> > > > > > >
> > > > > >
> > > > > > lines
> > > > > > > that
> > > > > > > > > are longer than 100 characters.
> > > > > > > > >
> > > > > > > > > --
> > > > > > > > > Reply to this email directly or view it on GitHub<
> > > > > > > > 90
> > > > > > >
> > > > > > > > > .
> >
> >
>



-- 

~Yours, Xuefeng Wu/å´é›ªå³°  æ•¬ä¸Š
"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 15:33:15 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-34546755
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 15:33:15 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-34546756
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 15:33:26 +0000 (UTC)",[GitHub] incubator-spark pull request: [PySpark] Adding support for Sequenc...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/263#issuecomment-34546762
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 15:33:26 +0000 (UTC)",[GitHub] incubator-spark pull request: [PySpark] Adding support for Sequenc...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/263#issuecomment-34546763
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 15:34:42 +0000 (UTC)",[GitHub] incubator-spark pull request: [PySpark] Adding support for Sequenc...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/263#issuecomment-34546803
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12630/


"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 15:34:43 +0000 (UTC)",[GitHub] incubator-spark pull request: [PySpark] Adding support for Sequenc...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/263#issuecomment-34546802
  
    Merged build finished.


"
MLnick <git@git.apache.org>,"Sat,  8 Feb 2014 15:36:30 +0000 (UTC)",[GitHub] incubator-spark pull request: [PySpark] Adding support for Sequenc...,dev@spark.incubator.apache.org,"Github user MLnick commented on the pull request:

    https://github.com/apache/incubator-spark/pull/263#issuecomment-34546862
  
    @laserson sorry this has gone quiet as I have been totally slammed with work stuff! 
    
    From my side I just need to clean up some of @JoshRosen comments. The main thing is the default `toString` which will require bringing back the `PairMUTF8Deserializer` since `msgpack` won't be used in that case (or perhaps just having `UTF8Deserializer` handle (string, string) also.
    
    The other major missing piece is then the key and value wrapper approach, which would be useful to get your thoughts on.
    
    Perhaps you could test the PR out on Avro/Parquet data, and see if it works, or if not where it falls down that we may need to improve?


"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 16:00:50 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-34547493
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12629/


"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 16:00:50 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-34547492
  
    Merged build finished.


"
martinjaggi <git@git.apache.org>,"Sat,  8 Feb 2014 19:36:04 +0000 (UTC)",[GitHub] incubator-spark pull request: new MLlib documentation for optimiza...,dev@spark.incubator.apache.org,"GitHub user martinjaggi opened a pull request:

    https://github.com/apache/incubator-spark/pull/563

    new MLlib documentation for optimization, regression and classification

    new documentation with tex formulas, hopefully improving usability and reproducibility of the offered MLlib methods.
    also did some minor changes in the code for consistency. scala tests pass.
    
    for easier merging, we could maybe rebase these changes (only > feb 7 is relevant) after 
    https://github.com/apache/incubator-spark/pull/552
    is merged?
    
    jira:
    https://spark-project.atlassian.net/browse/MLLIB-19

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark polishing-opt-MLlib

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/563.patch

----
commit d73948db0d9bc36296054e79fec5b1a657b4eab4
Author: Martin Jaggi <m.jaggi@gmail.com>
Date:   2014-02-06T15:57:23Z

    minor update on how to compile the documentation

commit d1c5212b93c67436543c2d8ddbbf610fdf0a26eb
Author: Martin Jaggi <m.jaggi@gmail.com>
Date:   2014-02-06T15:59:43Z

    enable mathjax formula in the .md documentation files
    
    code by @shivaram

commit bbafafd2b497a5acaa03a140bb9de1fbb7d67ffa
Author: Martin Jaggi <m.jaggi@gmail.com>
Date:   2014-02-06T16:31:29Z

    split MLlib documentation by techniques
    
    and linked from the main mllib-guide.md site

commit dcd2142c164b2f602bf472bb152ad55bae82d31a
Author: Martin Jaggi <m.jaggi@gmail.com>
Date:   2014-02-06T17:04:26Z

    enabling inline latex formulas with $.$
    
    same mathjax configuration as used in math.stackexchange.com
    
    sample usage in the linear algebra (SVD) documentation

commit 0364bfabbfc347f917216057a20c39b631842481
Author: Martin Jaggi <m.jaggi@gmail.com>
Date:   2014-02-07T02:19:38Z

    minor polishing, as suggested by @pwendell

commit 93d74988c33a9e4ef0d15e39c8b8fc9e6c36bb28
Author: Martin Jaggi <m.jaggi@gmail.com>
Date:   2014-02-07T16:33:24Z

    renaming LeastSquaresGradient
    
    not to confuse with squared regularizer or a squared gradient. added
    some more comments as what the loss functions are good for

commit e4cbe99bbcf7f53ebb8f1a0d2e0b869a4922bca4
Author: Martin Jaggi <m.jaggi@gmail.com>
Date:   2014-02-07T16:34:45Z

    use d for the number of features
    
    try to be consistent, that n is the number of data examples in the RDD,
    and each of them has d entries (also in documentation)

commit 79768fd3429df5c6d56f05ac93bdd8cf4355d946
Author: Martin Jaggi <m.jaggi@gmail.com>
Date:   2014-02-07T17:13:17Z

    correct scaling for MSE loss
    
    to be consistent with the documentation

commit 1e228062b01ac806c4bd032eb0975a8b92431fd9
Author: Martin Jaggi <m.jaggi@gmail.com>
Date:   2014-02-07T17:15:44Z

    new classification and regression documentation
    
    with complete mathematical formulations. trying to be general for
    adding future ML methods as well. table of all subgradients used for
    reference.
    this change also required a small addition to the mathjax
    configuration, to allow equation numbers.

commit 89e472f4121debb175b625ab0c138e24c4e60de8
Author: Martin Jaggi <m.jaggi@gmail.com>
Date:   2014-02-07T17:16:51Z

    new optimization documentation
    
    explaining GD and SGD and the distributed versions that MLlib
    implements.

commit a33be78a47bad1745a03a6e0ee1a4ea1a7893805
Author: Martin Jaggi <m.jaggi@gmail.com>
Date:   2014-02-07T17:38:57Z

    better comments in SGD code for regression

commit 73f5e71e3d9a253ff378907fca202b8d6aae1268
Author: Martin Jaggi <m.jaggi@gmail.com>
Date:   2014-02-07T22:41:42Z

    lambda R() in documentation

commit eec58c9c860def9b3b7604c990ec1697812bcbbf
Author: Martin Jaggi <m.jaggi@gmail.com>
Date:   2014-02-08T17:31:05Z

    telling what updater actually does
    
    also use proper scaling for the L2 regularization (using 1/2 as in the
    documentation)

commit 2c1cf8d35145081a61865f55f4e48fcfbafddbbe
Author: Martin Jaggi <m.jaggi@gmail.com>
Date:   2014-02-08T17:56:01Z

    remove broken url

commit ecbac73a7450fc90ef1509d9a410c9b627617130
Author: Martin Jaggi <m.jaggi@gmail.com>
Date:   2014-02-08T17:57:12Z

    better description of GradientDescent

----


"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 19:38:00 +0000 (UTC)",[GitHub] incubator-spark pull request: new MLlib documentation for optimiza...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/563#issuecomment-34553550
  
    Can one of the admins verify this patch?


"
pwendell <git@git.apache.org>,"Sat,  8 Feb 2014 19:39:44 +0000 (UTC)",[GitHub] incubator-spark pull request: tex formulas in the documentation,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/552#issuecomment-34553596
  
    Ah sorry forgot to merge. I just did so it should show up here within an hour.


"
pwendell <git@git.apache.org>,"Sat,  8 Feb 2014 19:40:14 +0000 (UTC)",[GitHub] incubator-spark pull request: new MLlib documentation for optimiza...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/563#issuecomment-34553608
  
    Jenkins add to whitelist.


"
pwendell <git@git.apache.org>,"Sat,  8 Feb 2014 19:40:20 +0000 (UTC)",[GitHub] incubator-spark pull request: new MLlib documentation for optimiza...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/563#issuecomment-34553612
  
    Jenkins, test this please.


"
martinjaggi <git@git.apache.org>,"Sat,  8 Feb 2014 19:40:48 +0000 (UTC)",[GitHub] incubator-spark pull request: tex formulas in the documentation,dev@spark.incubator.apache.org,"Github user martinjaggi commented on the pull request:

    https://github.com/apache/incubator-spark/pull/552#issuecomment-34553631
  
    ok thanks!


"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 19:43:00 +0000 (UTC)",[GitHub] incubator-spark pull request: new MLlib documentation for optimiza...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/563#issuecomment-34553685
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 19:43:00 +0000 (UTC)",[GitHub] incubator-spark pull request: new MLlib documentation for optimiza...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/563#issuecomment-34553684
  
     Merged build triggered.


"
pwendell <git@git.apache.org>,"Sat,  8 Feb 2014 19:49:07 +0000 (UTC)","[GitHub] incubator-spark pull request: [WIP] SPARK-1058, Fix Style Errors a...",dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/557#issuecomment-34553870
  
    @ScrapCodes Words cannot express my elation at having this patch. I noticed there are still style errors. Did you want me to merge this as-is and then you will add future pull requests (to avoid conflicts)?


"
martinjaggi <git@git.apache.org>,"Sat,  8 Feb 2014 19:57:23 +0000 (UTC)",[GitHub] incubator-spark pull request: tex formulas in the documentation,dev@spark.incubator.apache.org,"Github user martinjaggi closed the pull request at:

    https://github.com/apache/incubator-spark/pull/552


"
pwendell <git@git.apache.org>,"Sat,  8 Feb 2014 19:57:24 +0000 (UTC)","[GitHub] incubator-spark pull request: [WIP] SPARK-1058, Fix Style Errors a...",dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/557#issuecomment-34554122
  
    Hey @ScrapCodes I noticed the size of indent is inconsistent. The rule is to always use 2 spaces. If you are breaking initialization of a code block (e.g. a function signature) then it's okay to use 4 spaces to distinguish it from the body. I think scala is silent on this exception but it's the convention we usually use.
    
    If you could go through and address those I'm happy to merge an intermediate clean-up to avoid conflicts.


"
martinjaggi <git@git.apache.org>,"Sat,  8 Feb 2014 20:01:30 +0000 (UTC)",[GitHub] incubator-spark pull request: tex formulas in the documentation,dev@spark.incubator.apache.org,"GitHub user martinjaggi reopened a pull request:

    https://github.com/apache/incubator-spark/pull/552

    tex formulas in the documentation

    using mathjax.
    and spliting the MLlib documentation by techniques
    
    see jira
    https://spark-project.atlassian.net/browse/MLLIB-19
    and
    https://github.com/shivaram/spark/compare/mathjax

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark master

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/552.patch

----
commit d73948db0d9bc36296054e79fec5b1a657b4eab4
Author: Martin Jaggi <m.jaggi@gmail.com>
Date:   2014-02-06T15:57:23Z

    minor update on how to compile the documentation

commit d1c5212b93c67436543c2d8ddbbf610fdf0a26eb
Author: Martin Jaggi <m.jaggi@gmail.com>
Date:   2014-02-06T15:59:43Z

    enable mathjax formula in the .md documentation files
    
    code by @shivaram

commit bbafafd2b497a5acaa03a140bb9de1fbb7d67ffa
Author: Martin Jaggi <m.jaggi@gmail.com>
Date:   2014-02-06T16:31:29Z

    split MLlib documentation by techniques
    
    and linked from the main mllib-guide.md site

commit dcd2142c164b2f602bf472bb152ad55bae82d31a
Author: Martin Jaggi <m.jaggi@gmail.com>
Date:   2014-02-06T17:04:26Z

    enabling inline latex formulas with $.$
    
    same mathjax configuration as used in math.stackexchange.com
    
    sample usage in the linear algebra (SVD) documentation

commit 0364bfabbfc347f917216057a20c39b631842481
Author: Martin Jaggi <m.jaggi@gmail.com>
Date:   2014-02-07T02:19:38Z

    minor polishing, as suggested by @pwendell

----


"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 20:03:05 +0000 (UTC)",[GitHub] incubator-spark pull request: tex formulas in the documentation,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/552#issuecomment-34554284
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 20:03:06 +0000 (UTC)",[GitHub] incubator-spark pull request: tex formulas in the documentation,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/552#issuecomment-34554285
  
    Merged build started.


"
pwendell <git@git.apache.org>,"Sat,  8 Feb 2014 20:03:51 +0000 (UTC)",[GitHub] incubator-spark pull request: Kill drivers in postStop() for Worke...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/561#issuecomment-34554306
  
    Good catch. Jenkins, test this please.


"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 20:08:02 +0000 (UTC)",[GitHub] incubator-spark pull request: Kill drivers in postStop() for Worke...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/561#issuecomment-34554453
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 20:08:02 +0000 (UTC)",[GitHub] incubator-spark pull request: Kill drivers in postStop() for Worke...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/561#issuecomment-34554452
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 20:10:21 +0000 (UTC)",[GitHub] incubator-spark pull request: new MLlib documentation for optimiza...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/563#issuecomment-34554533
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 20:10:21 +0000 (UTC)",[GitHub] incubator-spark pull request: new MLlib documentation for optimiza...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/563#issuecomment-34554535
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12631/


"
pwendell <git@git.apache.org>,"Sat,  8 Feb 2014 20:13:01 +0000 (UTC)",[GitHub] incubator-spark pull request: ROC AUC and Average precision metric...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/550#issuecomment-34554613
  
    @schmit Mind adding a JIRA for this?


"
pwendell <git@git.apache.org>,"Sat,  8 Feb 2014 20:23:55 +0000 (UTC)",[GitHub] incubator-spark pull request: Make sbt download an atomic operatio...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/454#issuecomment-34554940
  
    Seems reasonable to me, I'll merge this.


"
jey <git@git.apache.org>,"Sat,  8 Feb 2014 20:26:19 +0000 (UTC)",[GitHub] incubator-spark pull request: Make sbt download an atomic operatio...,dev@spark.incubator.apache.org,"Github user jey closed the pull request at:

    https://github.com/apache/incubator-spark/pull/454


"
martinjaggi <git@git.apache.org>,"Sat,  8 Feb 2014 20:26:20 +0000 (UTC)",[GitHub] incubator-spark pull request: tex formulas in the documentation,dev@spark.incubator.apache.org,"Github user martinjaggi closed the pull request at:

    https://github.com/apache/incubator-spark/pull/552


"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 20:30:28 +0000 (UTC)",[GitHub] incubator-spark pull request: tex formulas in the documentation,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/552#issuecomment-34555135
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12632/


"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 20:30:28 +0000 (UTC)",[GitHub] incubator-spark pull request: tex formulas in the documentation,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/552#issuecomment-34555134
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 20:33:01 +0000 (UTC)",[GitHub] incubator-spark pull request: new MLlib documentation for optimiza...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/563#issuecomment-34555221
  
     Build triggered.


"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 20:33:01 +0000 (UTC)",[GitHub] incubator-spark pull request: new MLlib documentation for optimiza...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/563#issuecomment-34555222
  
    Build started.


"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 20:34:58 +0000 (UTC)",[GitHub] incubator-spark pull request: Kill drivers in postStop() for Worke...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/561#issuecomment-34555275
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 20:34:58 +0000 (UTC)",[GitHub] incubator-spark pull request: Kill drivers in postStop() for Worke...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/561#issuecomment-34555277
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12633/


"
Patrick Wendell <pwendell@gmail.com>,"Sat, 8 Feb 2014 12:56:16 -0800",[SUMMARY] Proposal for Spark Release Strategy,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey All,

Thanks for everyone who participated in this thread. I've distilled
feedback based on the discussion and wanted to summarize the
conclusions:

- People seem universally +1 on semantic versioning in general.

- People seem universally +1 on having a public merge windows for releases.

- People seem universally +1 on a policy of having associated JIRA's
with features.

- Everyone believes link-level compatiblity should be the goal. Some
people think we should outright promise it now. Others thing we should
either not promise it or promise it later.
--> Compromise: let's do one minor release 1.0->1.1 to convince
ourselves this is possible (some issues with Scala traits will make
this tricky). Then we can codify it in writing. I've created
SPARK-1069 [1] to clearly establish that this is the goal for 1.X
family of releases.

- Some people think we should add particular features before having 1.0.
--> Version 1.X indicates API stability rather than a feature set;
this was clarified.
--> That said, people still have several months to work on features if
they really want to get them in for this release.

I'm going to integrate this feedback and post a tentative version of
the release guidelines to the wiki.

With all this said, I would like to move the master version to
1.0.0-SNAPSHOT as the main concerns with this have been addressed and
clarified. This merely represents a tentative consensus and the
release is still subject to a formal vote amongst PMC members.

[1] https://spark-project.atlassian.net/browse/SPARK-1069

- Patrick

"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 21:00:11 +0000 (UTC)",[GitHub] incubator-spark pull request: new MLlib documentation for optimiza...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/563#issuecomment-34555965
  
    Build finished.


"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 21:00:11 +0000 (UTC)",[GitHub] incubator-spark pull request: new MLlib documentation for optimiza...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/563#issuecomment-34555966
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12634/


"
pwendell <git@git.apache.org>,"Sat,  8 Feb 2014 21:00:33 +0000 (UTC)",[GitHub] incubator-spark pull request: Kill drivers in postStop() for Worke...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/561#issuecomment-34555972
  
    Thanks for this fix, I merged it into master and 0.9.


"
rezazadeh <git@git.apache.org>,"Sat,  8 Feb 2014 21:01:45 +0000 (UTC)",[GitHub] incubator-spark pull request: Principal Component Analysis,dev@spark.incubator.apache.org,"GitHub user rezazadeh opened a pull request:

    https://github.com/apache/incubator-spark/pull/564

    Principal Component Analysis

    # Principal Component Analysis
    
    Computes the top k principal component coefficients for the m-by-n data matrix X. Rows of X correspond to observations and columns correspond to variables. The coefficient matrix is n-by-k. Each column of coeff contains coefficients for one principal component, and the columns are in descending
    order of component variance. This function centers the data and uses the singular value decomposition (SVD) algorithm.
    
    # Testing
    Tests included:
     * All principal components
    
    # Documentation
    
    # Example Usage 
    import org.apache.spark.SparkContext
    import org.apache.spark.mllib.linalg.PCA
    import org.apache.spark.mllib.linalg.SparseMatrix
    import org.apache.spark.mllib.linalg.MatrixEntry
    
    // Load and parse the data file
    val data = sc.textFile(""mllib/data/als/test.data"").map { line =>
      val parts = line.split(',')
      MatrixEntry(parts(0).toInt, parts(1).toInt, parts(2).toDouble)
    }
    val m = 4
    val n = 4
    val k = 1
    
    // recover top principal component
    val coeffs = PCA.computePCA(SparseMatrix(data, m, n), k)
    
    {% endhighlight %}


You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark pca

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/564.patch

----
commit 0642afb2ec1ca6896ffd1a4d3b12eca3f4db52b3
Author: Reza Zadeh <rizlar@gmail.com>
Date:   2014-02-02T05:53:33Z

    Initial files

commit 371f40ae288d45986c364adcfe4b584a9b00aa3d
Author: Reza Zadeh <rizlar@gmail.com>
Date:   2014-02-08T01:50:59Z

    new interfaces

commit 173148288dffe6cfa1d6671fa8dd9c57499fd0e8
Author: Reza Zadeh <rizlar@gmail.com>
Date:   2014-02-08T04:04:46Z

    add option to compute U

commit fb022fcc857bc3bbbb793882587480671b3e0b23
Author: Reza Zadeh <rizlar@gmail.com>
Date:   2014-02-08T08:48:24Z

    new tests, SVD interface

commit f756aff7b322504f09236f3ad4e05d4b75e8cc42
Author: Reza Zadeh <rizlar@gmail.com>
Date:   2014-02-08T08:49:47Z

    fix tests

commit 2d831f8f734ddf207707b721aa9718ebd7e65ca9
Author: Reza Zadeh <rizlar@gmail.com>
Date:   2014-02-08T09:04:48Z

    Documentation, yo

commit 31a5ecf977e6e4e6cd4d038aaa9f3d1ad1b3de49
Author: Reza Zadeh <rizlar@gmail.com>
Date:   2014-02-08T09:15:23Z

    added mllib guide docs

commit 57fe6d4ed9e214a504dbb2c5c66205045d5846b5
Author: Reza Zadeh <rizlar@gmail.com>
Date:   2014-02-08T09:18:07Z

    SparkPCA example

commit 07657476d3be2bd177090aaa37f6a4357329a188
Author: Reza Zadeh <rizlar@gmail.com>
Date:   2014-02-08T09:22:15Z

    fix typo

commit b45c1e88cb36ce2e5c78f493b05455f87ecfc662
Author: Reza Zadeh <rizlar@gmail.com>
Date:   2014-02-08T09:23:15Z

    fix example

----


"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 21:03:02 +0000 (UTC)",[GitHub] incubator-spark pull request: Principal Component Analysis,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/564#issuecomment-34556062
  
    Build started.


"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 21:03:02 +0000 (UTC)",[GitHub] incubator-spark pull request: Principal Component Analysis,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/564#issuecomment-34556061
  
     Build triggered.


"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 21:08:00 +0000 (UTC)",[GitHub] incubator-spark pull request: Principal Component Analysis,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/564#issuecomment-34556206
  
    Build started.


"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 21:07:59 +0000 (UTC)",[GitHub] incubator-spark pull request: Principal Component Analysis,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/564#issuecomment-34556205
  
     Build triggered.


"
Qiuzhuang <git@git.apache.org>,"Sat,  8 Feb 2014 21:26:00 +0000 (UTC)",[GitHub] incubator-spark pull request: Kill drivers in postStop() for Worke...,dev@spark.incubator.apache.org,"Github user Qiuzhuang closed the pull request at:

    https://github.com/apache/incubator-spark/pull/561


"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 21:30:17 +0000 (UTC)",[GitHub] incubator-spark pull request: Principal Component Analysis,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/564#issuecomment-34556909
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12635/


"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 21:30:18 +0000 (UTC)",[GitHub] incubator-spark pull request: Principal Component Analysis,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/564#issuecomment-34556908
  
    Build finished.


"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 21:34:28 +0000 (UTC)",[GitHub] incubator-spark pull request: Principal Component Analysis,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/564#issuecomment-34557013
  
    Build finished.


"
AmplabJenkins <git@git.apache.org>,"Sat,  8 Feb 2014 21:34:28 +0000 (UTC)",[GitHub] incubator-spark pull request: Principal Component Analysis,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/564#issuecomment-34557014
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12636/


"
Andy Konwinski <andykonwinski@gmail.com>,"Sat, 8 Feb 2014 13:35:44 -0800",Re: [SUMMARY] Proposal for Spark Release Strategy,dev@spark.incubator.apache.org,"Thanks for the summary Patrick. I'm glad that we discussed the options
before pulling the trigger on a version number update (my -1 had only been
about committing a major version update without thorough discussion).
IMO that's been addressed and given the discussion, I'm changing to a +1
for 1.0.0

"
Henry Saputra <henry.saputra@gmail.com>,"Sat, 8 Feb 2014 14:57:21 -0800",Re: [SUMMARY] Proposal for Spark Release Strategy,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Patrick, do you know if there is a way to check if a Github PR's
subject/ title contains JIRA number and will raise warning by the
Jenkins?

- Henry


"
pwendell <git@git.apache.org>,"Sat,  8 Feb 2014 23:09:32 +0000 (UTC)",[GitHub] incubator-spark pull request: new MLlib documentation for optimiza...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/563#issuecomment-34559476
  
    @martinjaggi can you rebase this now?


"
martinjaggi <git@git.apache.org>,"Sat,  8 Feb 2014 23:11:54 +0000 (UTC)",[GitHub] incubator-spark pull request: new MLlib documentation for optimiza...,dev@spark.incubator.apache.org,"Github user martinjaggi commented on the pull request:

    https://github.com/apache/incubator-spark/pull/563#issuecomment-34559530
  
    can i do this on the github website or only in command line?
    
    
    
    > @martinjaggi <https://github.com/martinjaggi> can you rebase this now?
    >
    > --
    > Reply to this email directly or view it on GitHub<https://github.com/apache/incubator-spark/pull/563#issuecomment-34559476>
    > .
    >


"
Mark Hamstra <mark@clearstorydata.com>,"Sat, 8 Feb 2014 15:20:30 -0800",Re: [SUMMARY] Proposal for Spark Release Strategy,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I know that it can be done -- which is different from saying that I know how to set it up.



te:
s.

"
martinjaggi <git@git.apache.org>,"Sat,  8 Feb 2014 23:25:01 +0000 (UTC)",[GitHub] incubator-spark pull request: new MLlib documentation for optimiza...,dev@spark.incubator.apache.org,"Github user martinjaggi commented on the pull request:

    https://github.com/apache/incubator-spark/pull/563#issuecomment-34559872
  
    i'm scared of ""the wrath of the git gods"" ;)
    https://help.github.com/articles/interactive-rebase
    
    (the rebase succeeded locally on my machine, but nothing has happened on
    github yet)
    
    
    
    > can i do this on the github website or only in command line?
    >
    >
    >
    >> @martinjaggi <https://github.com/martinjaggi> can you rebase this now?
    >>
    >> --
    >> Reply to this email directly or view it on GitHub<https://github.com/apache/incubator-spark/pull/563#issuecomment-34559476>
    >> .
    >>
    >
    >


"
rxin <git@git.apache.org>,"Sat,  8 Feb 2014 23:25:47 +0000 (UTC)",[GitHub] incubator-spark pull request: new MLlib documentation for optimiza...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/563#issuecomment-34559894
  
    To play it safe, you can always create a new branch and do the rebase there so it doesn't change your current branch.


"
rxin <git@git.apache.org>,"Sat,  8 Feb 2014 23:26:00 +0000 (UTC)",[GitHub] incubator-spark pull request: new MLlib documentation for optimiza...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/563#issuecomment-34559900
  
    (And then submit a new PR and close this one)


"
Patrick Wendell <pwendell@gmail.com>,"Sat, 8 Feb 2014 15:39:02 -0800",Re: [SUMMARY] Proposal for Spark Release Strategy,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>",":P - I'm pretty sure this can be done but it will require some work -
we already use the github API in our merge script and we could hook
something like that up with the jenkins tests. Henry maybe you could
create a JIRA for this for Spark 1.0?

- Patrick


"
pwendell <git@git.apache.org>,"Sat,  8 Feb 2014 23:55:57 +0000 (UTC)",[GitHub] incubator-spark pull request: Principal Component Analysis,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/564#issuecomment-34560617
  
    @rezazadeh Mind adding a JIRA for this?


"
pwendell <git@git.apache.org>,"Sun,  9 Feb 2014 00:01:05 +0000 (UTC)",[GitHub] incubator-spark pull request: Version number to 1.0.0-SNAPSHOT,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/542#issuecomment-34560733
  
    Per dev list discussion I am going to merge this. Thanks mark!


"
martinjaggi <git@git.apache.org>,"Sun,  9 Feb 2014 00:13:35 +0000 (UTC)",[GitHub] incubator-spark pull request: new MLlib documentation for optimiza...,dev@spark.incubator.apache.org,"Github user martinjaggi closed the pull request at:

    https://github.com/apache/incubator-spark/pull/563


"
markhamstra <git@git.apache.org>,"Sun,  9 Feb 2014 00:24:23 +0000 (UTC)",[GitHub] incubator-spark pull request: Version number to 1.0.0-SNAPSHOT,dev@spark.incubator.apache.org,"Github user markhamstra closed the pull request at:

    https://github.com/apache/incubator-spark/pull/542


"
martinjaggi <git@git.apache.org>,"Sun,  9 Feb 2014 00:54:49 +0000 (UTC)",[GitHub] incubator-spark pull request: new MLlib documentation for optimiza...,dev@spark.incubator.apache.org,"GitHub user martinjaggi reopened a pull request:

    https://github.com/apache/incubator-spark/pull/563

    new MLlib documentation for optimization, regression and classification

    new documentation with tex formulas, hopefully improving usability and reproducibility of the offered MLlib methods.
    also did some minor changes in the code for consistency. scala tests pass.
    
    for easier merging, we could maybe rebase these changes (only > feb 7 is relevant) after 
    https://github.com/apache/incubator-spark/pull/552
    is merged?
    
    jira:
    https://spark-project.atlassian.net/browse/MLLIB-19

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark polishing-opt-MLlib

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/563.patch

----
commit d73948db0d9bc36296054e79fec5b1a657b4eab4
Author: Martin Jaggi <m.jaggi@gmail.com>
Date:   2014-02-06T15:57:23Z

    minor update on how to compile the documentation

commit d1c5212b93c67436543c2d8ddbbf610fdf0a26eb
Author: Martin Jaggi <m.jaggi@gmail.com>
Date:   2014-02-06T15:59:43Z

    enable mathjax formula in the .md documentation files
    
    code by @shivaram

commit bbafafd2b497a5acaa03a140bb9de1fbb7d67ffa
Author: Martin Jaggi <m.jaggi@gmail.com>
Date:   2014-02-06T16:31:29Z

    split MLlib documentation by techniques
    
    and linked from the main mllib-guide.md site

commit dcd2142c164b2f602bf472bb152ad55bae82d31a
Author: Martin Jaggi <m.jaggi@gmail.com>
Date:   2014-02-06T17:04:26Z

    enabling inline latex formulas with $.$
    
    same mathjax configuration as used in math.stackexchange.com
    
    sample usage in the linear algebra (SVD) documentation

commit 0364bfabbfc347f917216057a20c39b631842481
Author: Martin Jaggi <m.jaggi@gmail.com>
Date:   2014-02-07T02:19:38Z

    minor polishing, as suggested by @pwendell

commit 93d74988c33a9e4ef0d15e39c8b8fc9e6c36bb28
Author: Martin Jaggi <m.jaggi@gmail.com>
Date:   2014-02-07T16:33:24Z

    renaming LeastSquaresGradient
    
    not to confuse with squared regularizer or a squared gradient. added
    some more comments as what the loss functions are good for

commit e4cbe99bbcf7f53ebb8f1a0d2e0b869a4922bca4
Author: Martin Jaggi <m.jaggi@gmail.com>
Date:   2014-02-07T16:34:45Z

    use d for the number of features
    
    try to be consistent, that n is the number of data examples in the RDD,
    and each of them has d entries (also in documentation)

commit 79768fd3429df5c6d56f05ac93bdd8cf4355d946
Author: Martin Jaggi <m.jaggi@gmail.com>
Date:   2014-02-07T17:13:17Z

    correct scaling for MSE loss
    
    to be consistent with the documentation

commit 1e228062b01ac806c4bd032eb0975a8b92431fd9
Author: Martin Jaggi <m.jaggi@gmail.com>
Date:   2014-02-07T17:15:44Z

    new classification and regression documentation
    
    with complete mathematical formulations. trying to be general for
    adding future ML methods as well. table of all subgradients used for
    reference.
    this change also required a small addition to the mathjax
    configuration, to allow equation numbers.

commit 89e472f4121debb175b625ab0c138e24c4e60de8
Author: Martin Jaggi <m.jaggi@gmail.com>
Date:   2014-02-07T17:16:51Z

    new optimization documentation
    
    explaining GD and SGD and the distributed versions that MLlib
    implements.

commit a33be78a47bad1745a03a6e0ee1a4ea1a7893805
Author: Martin Jaggi <m.jaggi@gmail.com>
Date:   2014-02-07T17:38:57Z

    better comments in SGD code for regression

commit 73f5e71e3d9a253ff378907fca202b8d6aae1268
Author: Martin Jaggi <m.jaggi@gmail.com>
Date:   2014-02-07T22:41:42Z

    lambda R() in documentation

commit eec58c9c860def9b3b7604c990ec1697812bcbbf
Author: Martin Jaggi <m.jaggi@gmail.com>
Date:   2014-02-08T17:31:05Z

    telling what updater actually does
    
    also use proper scaling for the L2 regularization (using 1/2 as in the
    documentation)

commit 2c1cf8d35145081a61865f55f4e48fcfbafddbbe
Author: Martin Jaggi <m.jaggi@gmail.com>
Date:   2014-02-08T17:56:01Z

    remove broken url

commit ecbac73a7450fc90ef1509d9a410c9b627617130
Author: Martin Jaggi <m.jaggi@gmail.com>
Date:   2014-02-08T17:57:12Z

    better description of GradientDescent

commit eae3dce25a4b68bf32ece1ca7783f9b2ffd56dff
Author: Martin Jaggi <m.jaggi@gmail.com>
Date:   2014-02-08T20:30:35Z

    line wrap at 100 chars

----


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 00:57:59 +0000 (UTC)",[GitHub] incubator-spark pull request: new MLlib documentation for optimiza...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/563#issuecomment-34561878
  
     Build triggered.


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 00:58:00 +0000 (UTC)",[GitHub] incubator-spark pull request: new MLlib documentation for optimiza...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/563#issuecomment-34561880
  
    Build started.


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 01:25:22 +0000 (UTC)",[GitHub] incubator-spark pull request: new MLlib documentation for optimiza...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/563#issuecomment-34562386
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12637/


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 01:25:22 +0000 (UTC)",[GitHub] incubator-spark pull request: new MLlib documentation for optimiza...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/563#issuecomment-34562385
  
    Build finished.


"
Henry Saputra <henry.saputra@gmail.com>,"Sat, 8 Feb 2014 17:45:02 -0800",Re: [SUMMARY] Proposal for Spark Release Strategy,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>",":)

Sure thing. I will create JIRA ticket for this.

Thx guys,

Henry


"
mateiz <git@git.apache.org>,"Sun,  9 Feb 2014 01:45:13 +0000 (UTC)",[GitHub] incubator-spark pull request: Principal Component Analysis,dev@spark.incubator.apache.org,"Github user mateiz commented on the pull request:

    https://github.com/apache/incubator-spark/pull/564#issuecomment-34562749
  
    Made a few comments on the style.


"
rezazadeh <git@git.apache.org>,"Sun,  9 Feb 2014 02:13:58 +0000 (UTC)",[GitHub] incubator-spark pull request: Principal Component Analysis,dev@spark.incubator.apache.org,"Github user rezazadeh commented on the pull request:

    https://github.com/apache/incubator-spark/pull/564#issuecomment-34563228
  
    @pwendell Not sure why you want this, but here you go:
    https://spark-project.atlassian.net/browse/MLLIB-21


"
pwendell <git@git.apache.org>,"Sun,  9 Feb 2014 02:17:03 +0000 (UTC)",[GitHub] incubator-spark pull request: Principal Component Analysis,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/564#issuecomment-34563275
  
    @rezazadeh We need to track all features with JIRA's it's an Apache requirement.


"
rezazadeh <git@git.apache.org>,"Sun,  9 Feb 2014 02:42:10 +0000 (UTC)",[GitHub] incubator-spark pull request: Principal Component Analysis,dev@spark.incubator.apache.org,"Github user rezazadeh commented on the pull request:

    https://github.com/apache/incubator-spark/pull/564#issuecomment-34563699
  
    @mateiz All those style changes made.


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 02:43:00 +0000 (UTC)",[GitHub] incubator-spark pull request: Principal Component Analysis,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/564#issuecomment-34563717
  
    Build started.


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 02:43:00 +0000 (UTC)",[GitHub] incubator-spark pull request: Principal Component Analysis,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/564#issuecomment-34563716
  
     Build triggered.


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 03:10:01 +0000 (UTC)",[GitHub] incubator-spark pull request: Principal Component Analysis,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/564#issuecomment-34564094
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12638/


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 03:10:01 +0000 (UTC)",[GitHub] incubator-spark pull request: Principal Component Analysis,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/564#issuecomment-34564093
  
    Build finished.


"
rxin <git@git.apache.org>,"Sun,  9 Feb 2014 04:33:06 +0000 (UTC)",[GitHub] incubator-spark pull request: new MLlib documentation for optimiza...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/563#issuecomment-34565306
  
    Maybe you can create a patch, and apply that patch directly on master?
    
    The changes look pretty easy to apply to me, so it shouldn't be too hard. 


"
Patrick Wendell <pwendell@gmail.com>,"Sat, 8 Feb 2014 21:42:50 -0800",Re: [TODO] Document the release process for Apache Spark,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I ported the release docs to the wiki today. Thanks for reminding me
about this Henry:

https://cwiki.apache.org/confluence/display/SPARK/Preparing+Spark+Releases

- Patrick


"
pwendell <git@git.apache.org>,"Sun,  9 Feb 2014 06:35:44 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1066: Add developer scripts to...,dev@spark.incubator.apache.org,"GitHub user pwendell opened a pull request:

    https://github.com/apache/incubator-spark/pull/565

    SPARK-1066: Add developer scripts to repository.

    These are some developer scripts I've been maintaining in a separate public repo. This patch adds them to the Spark repository so they can evolve here and are clearly accessible to all committers.
    
    I may do some small additional clean-up in this PR, but wanted to put them here in case others want to review.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark dev-scripts

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/565.patch

----
commit 5d5d331d01f6fd59c2eb830f652955119b012173
Author: Patrick Wendell <pwendell@gmail.com>
Date:   2014-02-09T06:11:47Z

    SPARK-1066: Add developer scripts to repository.

----


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 06:38:00 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1066: Add developer scripts to...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/565#issuecomment-34566956
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 06:38:00 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1066: Add developer scripts to...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/565#issuecomment-34566955
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 07:05:07 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1066: Add developer scripts to...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/565#issuecomment-34567294
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12639/


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 07:05:07 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1066: Add developer scripts to...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/565#issuecomment-34567293
  
    Merged build finished.


"
rxin <git@git.apache.org>,"Sun,  9 Feb 2014 07:13:20 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1066: Add developer scripts to...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/565#issuecomment-34567423
  
    Ok I merged it. Thanks!


"
pwendell <git@git.apache.org>,"Sun,  9 Feb 2014 07:24:57 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1066: Add developer scripts to...,dev@spark.incubator.apache.org,"Github user pwendell closed the pull request at:

    https://github.com/apache/incubator-spark/pull/565


"
Henry Saputra <henry.saputra@gmail.com>,"Sat, 8 Feb 2014 23:28:32 -0800",Re: [TODO] Document the release process for Apache Spark,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Cool! Thanks Patrick.

Looks good to me. Just one small recommendation about ""Get Access to
Apache Nexus for Publishing Artifacts"", as I remember you need to file
INFRA ticket for your Apache id [1] to get it?

If it is then probably good idea to add it to the wiki.

- Henry


[1] https://issues.apache.org/jira



"
rxin <git@git.apache.org>,"Sun,  9 Feb 2014 07:35:27 +0000 (UTC)",[GitHub] incubator-spark pull request: [WIP] SPARK-1067: Default log4j init...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/560#issuecomment-34567772
  
    Ok merged in master & branch-0.9.


"
rxin <git@git.apache.org>,"Sun,  9 Feb 2014 07:36:41 +0000 (UTC)",[GitHub] incubator-spark pull request: Added example Python code for sort,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/562#issuecomment-34567786
  
    Thanks. Merged this in master & branch-0.9.


"
rxin <git@git.apache.org>,"Sun,  9 Feb 2014 07:39:14 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1060] startJettyServer should...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/556#issuecomment-34567822
  
    Thanks. I merged this in master.


"
rxin <git@git.apache.org>,"Sun,  9 Feb 2014 07:40:34 +0000 (UTC)",[GitHub] incubator-spark pull request: [WIP] SPARK-1067: Default log4j init...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/560#issuecomment-34567839
  
    Oops I didn't realize the WIP in title. Feel free to revert if necessary.


"
qqsun8819 <git@git.apache.org>,"Sun,  9 Feb 2014 07:42:57 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1038] Add more fields in Json...,dev@spark.incubator.apache.org,"Github user qqsun8819 commented on the pull request:

    https://github.com/apache/incubator-spark/pull/551#issuecomment-34567873
  
    I update the diff , using hard-coded json string for json data verification. @pwendell  @rxin  and @aarondav  Please reivew it again. Thanks very much!


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 07:43:03 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1038] Add more fields in Json...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/551#issuecomment-34567876
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 07:43:03 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1038] Add more fields in Json...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/551#issuecomment-34567877
  
    Merged build started.


"
Henry Saputra <henry.saputra@gmail.com>,"Sat, 8 Feb 2014 23:44:05 -0800",Re: [SUMMARY] Proposal for Spark Release Strategy,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Ok, JIRA ticket filed [1] for this one.

- Henry

[1] https://spark-project.atlassian.net/browse/SPARK-1070


"
rxin <git@git.apache.org>,"Sun,  9 Feb 2014 07:51:17 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1038] Add more fields in Json...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/551#issuecomment-34567984
  
    Thanks. I left some comments to improve readability of the code.


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 08:08:58 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1038] Add more fields in Json...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/551#issuecomment-34568218
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 08:08:58 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1038] Add more fields in Json...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/551#issuecomment-34568219
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12640/


"
pwendell <git@git.apache.org>,"Sun,  9 Feb 2014 08:27:09 +0000 (UTC)",[GitHub] incubator-spark pull request: [WIP] SPARK-1067: Default log4j init...,dev@spark.incubator.apache.org,"Github user pwendell closed the pull request at:

    https://github.com/apache/incubator-spark/pull/560


"
CodingCat <git@git.apache.org>,"Sun,  9 Feb 2014 08:27:09 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1060] startJettyServer should...,dev@spark.incubator.apache.org,"Github user CodingCat closed the pull request at:

    https://github.com/apache/incubator-spark/pull/556


"
jyotiska <git@git.apache.org>,"Sun,  9 Feb 2014 08:27:09 +0000 (UTC)",[GitHub] incubator-spark pull request: Added example Python code for sort,dev@spark.incubator.apache.org,"Github user jyotiska closed the pull request at:

    https://github.com/apache/incubator-spark/pull/562


"
MLnick <git@git.apache.org>,"Sun,  9 Feb 2014 08:43:42 +0000 (UTC)",[GitHub] incubator-spark pull request: Support negative implicit input in A...,dev@spark.incubator.apache.org,"Github user MLnick commented on the pull request:

    https://github.com/apache/incubator-spark/pull/500#issuecomment-34568750
  
    This looks like a nice simple extension that gives some very useful functionality for practical use cases. Indeed it doesn't have any impact on the existing algorithm if users are using only positive implicit feedback, so I would be :+1: on this.


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 08:48:02 +0000 (UTC)","[GitHub] incubator-spark pull request: [WIP] SPARK-1058, Fix Style Errors a...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/557#issuecomment-34568814
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 08:48:02 +0000 (UTC)","[GitHub] incubator-spark pull request: [WIP] SPARK-1058, Fix Style Errors a...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/557#issuecomment-34568813
  
     Merged build triggered.


"
ScrapCodes <git@git.apache.org>,"Sun,  9 Feb 2014 08:57:29 +0000 (UTC)","[GitHub] incubator-spark pull request: SPARK-1058, Fix Style Errors and Add...",dev@spark.incubator.apache.org,"Github user ScrapCodes commented on the pull request:

    https://github.com/apache/incubator-spark/pull/557#issuecomment-34568958
  
    Hey Patrick, 
    
    There will be another PR with more clean up coming. This should be close to consistent with all your comments. 
    
    Thanks 


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 08:58:02 +0000 (UTC)","[GitHub] incubator-spark pull request: SPARK-1058, Fix Style Errors and Add...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/557#issuecomment-34568970
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 08:58:02 +0000 (UTC)","[GitHub] incubator-spark pull request: SPARK-1058, Fix Style Errors and Add...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/557#issuecomment-34568971
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 09:15:36 +0000 (UTC)","[GitHub] incubator-spark pull request: SPARK-1058, Fix Style Errors and Add...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/557#issuecomment-34569245
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12641/


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 09:15:36 +0000 (UTC)","[GitHub] incubator-spark pull request: SPARK-1058, Fix Style Errors and Add...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/557#issuecomment-34569244
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 09:26:55 +0000 (UTC)","[GitHub] incubator-spark pull request: SPARK-1058, Fix Style Errors and Add...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/557#issuecomment-34569439
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 09:26:55 +0000 (UTC)","[GitHub] incubator-spark pull request: SPARK-1058, Fix Style Errors and Add...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/557#issuecomment-34569440
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12642/


"
qqsun8819 <git@git.apache.org>,"Sun,  9 Feb 2014 09:53:23 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1038] Add more fields in Json...,dev@spark.incubator.apache.org,"Github user qqsun8819 commented on the pull request:

    https://github.com/apache/incubator-spark/pull/551#issuecomment-34569840
  
    @rxin Thank you very much for your review. And 2 test case failed becuase I hard  code a Date string . I 'll find out why because all case are passed in my own ubuntu system. I 'll update the diff ASAP after I finish the correction. 


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 11:23:00 +0000 (UTC)",[GitHub] incubator-spark pull request: new MLlib documentation for optimiza...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/563#issuecomment-34571348
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 11:23:00 +0000 (UTC)",[GitHub] incubator-spark pull request: new MLlib documentation for optimiza...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/563#issuecomment-34571347
  
     Merged build triggered.


"
martinjaggi <git@git.apache.org>,"Sun,  9 Feb 2014 11:38:38 +0000 (UTC)",[GitHub] incubator-spark pull request: new MLlib documentation for optimiza...,dev@spark.incubator.apache.org,"Github user martinjaggi closed the pull request at:

    https://github.com/apache/incubator-spark/pull/563


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 11:50:11 +0000 (UTC)",[GitHub] incubator-spark pull request: new MLlib documentation for optimiza...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/563#issuecomment-34571893
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 11:50:11 +0000 (UTC)",[GitHub] incubator-spark pull request: new MLlib documentation for optimiza...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/563#issuecomment-34571894
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12643/


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 12:27:59 +0000 (UTC)","[GitHub] incubator-spark pull request: SPARK-1058, Fix Style Errors and Add...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/557#issuecomment-34572575
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 12:27:59 +0000 (UTC)","[GitHub] incubator-spark pull request: SPARK-1058, Fix Style Errors and Add...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/557#issuecomment-34572576
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 12:55:27 +0000 (UTC)","[GitHub] incubator-spark pull request: SPARK-1058, Fix Style Errors and Add...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/557#issuecomment-34573058
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 12:55:27 +0000 (UTC)","[GitHub] incubator-spark pull request: SPARK-1058, Fix Style Errors and Add...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/557#issuecomment-34573059
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12644/


"
ScrapCodes <git@git.apache.org>,"Sun,  9 Feb 2014 13:04:41 +0000 (UTC)","[GitHub] incubator-spark pull request: SPARK-1058, Fix Style Errors and Add...",dev@spark.incubator.apache.org,"Github user ScrapCodes commented on the pull request:

    https://github.com/apache/incubator-spark/pull/557#issuecomment-34573230
  
    https://github.com/Alefas/intellij-scalastyle Seems to be a cool plugin for intellij, but does not support all the features.


"
martinjaggi <git@git.apache.org>,"Sun,  9 Feb 2014 13:25:55 +0000 (UTC)",[GitHub] incubator-spark pull request: new MLlib documentation for optimiza...,dev@spark.incubator.apache.org,"GitHub user martinjaggi opened a pull request:

    https://github.com/apache/incubator-spark/pull/566

    new MLlib documentation for optimization, regression and classification

    new documentation with tex formulas, hopefully improving usability and reproducibility of the offered MLlib methods.
    also did some minor changes in the code for consistency. scala tests pass.
    
    this is the rebased branch, i deleted the old PR
    
    jira:
    https://spark-project.atlassian.net/browse/MLLIB-19

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark copy-MLlib-d

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/566.patch

----
commit 1d7ba79c27458687833bafeb7668c5ceba8489ca
Author: Martin Jaggi <m.jaggi@gmail.com>
Date:   2014-02-07T16:33:24Z

    renaming LeastSquaresGradient
    
    not to confuse with squared regularizer or a squared gradient. added
    some more comments as what the loss functions are good for

commit e6ef3e8b870af40996a2e8f1bc3765262cf6c714
Author: Martin Jaggi <m.jaggi@gmail.com>
Date:   2014-02-07T16:34:45Z

    use d for the number of features
    
    try to be consistent, that n is the number of data examples in the RDD,
    and each of them has d entries (also in documentation)

commit da439c161d1763dfce9833efe4f8b845b00e5bef
Author: Martin Jaggi <m.jaggi@gmail.com>
Date:   2014-02-07T17:13:17Z

    correct scaling for MSE loss
    
    to be consistent with the documentation

commit d28ac774f4582370748aca379d2931520ff68fa4
Author: Martin Jaggi <m.jaggi@gmail.com>
Date:   2014-02-07T17:15:44Z

    new classification and regression documentation
    
    with complete mathematical formulations. trying to be general for
    adding future ML methods as well. table of all subgradients used for
    reference.
    this change also required a small addition to the mathjax
    configuration, to allow equation numbers.

commit 580b846c19bb98d4000d46a3682d3855b0a488e7
Author: Martin Jaggi <m.jaggi@gmail.com>
Date:   2014-02-07T17:16:51Z

    new optimization documentation
    
    explaining GD and SGD and the distributed versions that MLlib
    implements.

commit 6c7a7dc07afbf471fef7bf14a5751c80c6bb39aa
Author: Martin Jaggi <m.jaggi@gmail.com>
Date:   2014-02-07T17:38:57Z

    better comments in SGD code for regression

commit 25acda728ae6ec3f3eea5462073f70b29526b3e2
Author: Martin Jaggi <m.jaggi@gmail.com>
Date:   2014-02-07T22:41:42Z

    lambda R() in documentation

commit d82f9e80605c6dabf90135416f8921834621fdfe
Author: Martin Jaggi <m.jaggi@gmail.com>
Date:   2014-02-08T17:31:05Z

    telling what updater actually does
    
    also use proper scaling for the L2 regularization (using 1/2 as in the
    documentation)

commit fa355498331180623664a425783b96a12a8bcee9
Author: Martin Jaggi <m.jaggi@gmail.com>
Date:   2014-02-08T17:56:01Z

    remove broken url

commit 564cb0cee0ae2fb41baa264ed21f28c9336bc1ba
Author: Martin Jaggi <m.jaggi@gmail.com>
Date:   2014-02-08T17:57:12Z

    better description of GradientDescent

commit 29e5d65e6098c3f25bcbeada99bcf9cbb151eea7
Author: Martin Jaggi <m.jaggi@gmail.com>
Date:   2014-02-08T20:30:35Z

    line wrap at 100 chars

----


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 13:28:00 +0000 (UTC)",[GitHub] incubator-spark pull request: new MLlib documentation for optimiza...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/566#issuecomment-34573706
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 13:28:00 +0000 (UTC)",[GitHub] incubator-spark pull request: new MLlib documentation for optimiza...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/566#issuecomment-34573707
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 13:53:01 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1038] Add more fields in Json...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/551#issuecomment-34574307
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 13:53:01 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1038] Add more fields in Json...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/551#issuecomment-34574306
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 13:55:04 +0000 (UTC)",[GitHub] incubator-spark pull request: new MLlib documentation for optimiza...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/566#issuecomment-34574358
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12645/


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 13:55:04 +0000 (UTC)",[GitHub] incubator-spark pull request: new MLlib documentation for optimiza...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/566#issuecomment-34574357
  
    Merged build finished.


"
Nan Zhu <zhunanmcgill@gmail.com>,"Sun, 9 Feb 2014 09:21:14 -0500","How to write test cases for the functionalities which involves
 actor communication",dev@spark.incubator.apache.org,"Hi, all 

I have a question when trying to write some test cases for the PR

The key functionality in my PR involves actor communication between master and worker, like the worker does something and returns the result to the master via a message, I want to test if the master can do the right thing according to the number of workers existing in the cluster and the return result from the worker, 

Is there any way to test this via some test cases?

Thank you

Best, 

-- 
Nan Zhu

"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 14:20:36 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1038] Add more fields in Json...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/551#issuecomment-34575030
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 14:20:36 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1038] Add more fields in Json...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/551#issuecomment-34575031
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12646/


"
ScrapCodes <git@git.apache.org>,"Sun,  9 Feb 2014 14:52:46 +0000 (UTC)","[GitHub] incubator-spark pull request: SPARK-1058, Fix Style Errors and Add...",dev@spark.incubator.apache.org,"GitHub user ScrapCodes opened a pull request:

    https://github.com/apache/incubator-spark/pull/567

    SPARK-1058, Fix Style Errors and Add Scala Style to Spark Build. Pt 2

    Continuation of PR #557
    
    With this all scala style errors are fixed across the code base !!
    
    The reason for creating a separate PR was to not interrupt an already reviewed and ready to merge PR. Hope this gets reviewed soon and merged too.
    


You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark style2

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/567.patch

----
commit f0a9b8a61c870f67cb397dd34fcd7f04f73d6586
Author: Patrick Wendell <pwendell@gmail.com>
Date:   2014-01-24T19:22:53Z

    Adding scalastyle snapshot

commit 98d502f151c54d45780fec6aa4e96dea29fdfab3
Author: Prashant Sharma <scrapcodes@gmail.com>
Date:   2014-02-09T12:09:07Z

    scala style fixes

commit 3872fc91748259fec5f206f265441b6e2fccad9a
Author: Prashant Sharma <scrapcodes@gmail.com>
Date:   2014-02-09T14:31:15Z

    scala style fixes 2

----


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 14:52:59 +0000 (UTC)","[GitHub] incubator-spark pull request: SPARK-1058, Fix Style Errors and Add...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/567#issuecomment-34575822
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 14:52:58 +0000 (UTC)","[GitHub] incubator-spark pull request: SPARK-1058, Fix Style Errors and Add...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/567#issuecomment-34575821
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 15:20:45 +0000 (UTC)","[GitHub] incubator-spark pull request: SPARK-1058, Fix Style Errors and Add...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/567#issuecomment-34576491
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 15:20:45 +0000 (UTC)","[GitHub] incubator-spark pull request: SPARK-1058, Fix Style Errors and Add...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/567#issuecomment-34576492
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12647/


"
bijaybisht <git@git.apache.org>,"Sun,  9 Feb 2014 15:55:17 +0000 (UTC)",[GitHub] incubator-spark pull request: fix for https://spark-project.atlass...,dev@spark.incubator.apache.org,"GitHub user bijaybisht opened a pull request:

    https://github.com/apache/incubator-spark/pull/568

    fix for https://spark-project.atlassian.net/browse/SPARK-1052

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark SPARK-1052

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/568.patch

----
commit fdb1d94a8c7ef6a4afae0c4d1baad11fa002e88e
Author: Bijay Bisht <bijay.bisht@gmail.com>
Date:   2014-02-09T02:06:39Z

    fix for https://spark-project.atlassian.net/browse/SPARK-1052

----


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 15:57:58 +0000 (UTC)",[GitHub] incubator-spark pull request: fix for https://spark-project.atlass...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/568#issuecomment-34577386
  
    Can one of the admins verify this patch?


"
pwendell <git@git.apache.org>,"Sun,  9 Feb 2014 18:09:40 +0000 (UTC)","[GitHub] incubator-spark pull request: SPARK-1058, Fix Style Errors and Add...",dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/557#issuecomment-34580972
  
    Prashant - thanks a ton! I've merged this into master.


"
Patrick Wendell <pwendell@gmail.com>,"Sun, 9 Feb 2014 10:13:26 -0800",Re: [TODO] Document the release process for Apache Spark,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Done, thanks. Feel free to edit it directly as well :)


"
Patrick Wendell <pwendell@gmail.com>,"Sun, 9 Feb 2014 10:23:54 -0800","Re: How to write test cases for the functionalities which involves
 actor communication","""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","It's possible to mock out actors... we have a few examples in the code

https://github.com/apache/incubator-spark/blob/master/core/src/test/scala/org/apache/spark/deploy/worker/WorkerWatcherSuite.scala

r and worker, like the worker does something and returns the result to the master via a message, I want to test if the master can do the right thing according to the number of workers existing in the cluster and the return result from the worker,

"
ScrapCodes <git@git.apache.org>,"Sun,  9 Feb 2014 18:26:26 +0000 (UTC)","[GitHub] incubator-spark pull request: SPARK-1058, Fix Style Errors and Add...",dev@spark.incubator.apache.org,"Github user ScrapCodes closed the pull request at:

    https://github.com/apache/incubator-spark/pull/557


"
pwendell <git@git.apache.org>,"Sun,  9 Feb 2014 18:32:15 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1039] Set the upper bound for...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/472#issuecomment-34581663
  


"
CodingCat <git@git.apache.org>,"Sun,  9 Feb 2014 18:34:43 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1039] Set the upper bound for...,dev@spark.incubator.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/incubator-spark/pull/472#issuecomment-34581720
  
    @pwendell sure, the more eyes the better! 


"
pwendell <git@git.apache.org>,"Sun,  9 Feb 2014 18:37:20 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1038] Add more fields in Json...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/551#issuecomment-34581811
  
    @rxin look good to you?


"
pwendell <git@git.apache.org>,"Sun,  9 Feb 2014 19:03:31 +0000 (UTC)","[GitHub] incubator-spark pull request: SPARK-1058, Fix Style Errors and Add...",dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/567#issuecomment-34582688
  
    @ScrapCodes Looks good to me! If you can re-base this on your existing patch I will merge it. Also can you add an Apache header to the xml file?


"
rezazadeh <git@git.apache.org>,"Sun,  9 Feb 2014 19:10:46 +0000 (UTC)",[GitHub] incubator-spark pull request: Principal Component Analysis,dev@spark.incubator.apache.org,"Github user rezazadeh commented on the pull request:

    https://github.com/apache/incubator-spark/pull/564#issuecomment-34582915
  


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 19:13:00 +0000 (UTC)",[GitHub] incubator-spark pull request: Principal Component Analysis,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/564#issuecomment-34582981
  
     Build triggered.


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 19:13:05 +0000 (UTC)",[GitHub] incubator-spark pull request: Principal Component Analysis,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/564#issuecomment-34582982
  
    Build started.


"
sscdotopen <git@git.apache.org>,"Sun,  9 Feb 2014 19:16:33 +0000 (UTC)",[GitHub] incubator-spark pull request: Principal Component Analysis,dev@spark.incubator.apache.org,"Github user sscdotopen commented on the pull request:

    https://github.com/apache/incubator-spark/pull/564#issuecomment-34583075
  
    Centering a sparse matrix like it is done in textbook PCA is a serious scalability bottleneck as it densifies the input matrix, not sure if you can apply the algorithm like this.


"
Will Benton <willb@redhat.com>,"Sun, 9 Feb 2014 14:28:08 -0500 (EST)",proposal:  replace lift-json with spray-json,dev@spark.incubator.apache.org,"lift-json is a nice library, but Lift is a pretty heavyweight dependency to track just for its JSON support.  (lift-json is relatively self-contained as a dependency from an end-user's perspective, but downstream distributors need to build all of Lift in order to package the JSON support.)  I understand that this has come up before (cf. SPARK-883) and that the uncertain future of JSON support in the Scala standard library is the motivator for relying on an external library.

I'm proposing replacing lift-json in Spark with something more lightweight.  I've evaluated apparent project liveness and dependency scope for most of the current Scala JSON libraries and believe the best candidate is spray-json (https://github.com/spray/spray-json), the JSON library used by the Spray HTTP toolkit. spray-json is Apache-licensed, actively developed, and builds and works independently of Spray with only one external dependency.

It looks to me like a pretty straightforward change (although JsonProtocol.scala would be a little more verbose since it couldn't use the Lift JSON DSL), and I'd like to do it.  I'm writing now to ask for some community feedback before making the change (and submitting a JIRA and PR).  If no one has any serious objections (to the effort in general or to to the choice of spark-json in particular), I'll go ahead and do it, but if anyone has concerns, I'd be happy to discuss and address them before getting started.


thanks,
wb

"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 19:40:21 +0000 (UTC)",[GitHub] incubator-spark pull request: Principal Component Analysis,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/564#issuecomment-34583712
  
    Build finished.


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 19:40:21 +0000 (UTC)",[GitHub] incubator-spark pull request: Principal Component Analysis,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/564#issuecomment-34583713
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12648/


"
=?ISO-8859-1?Q?Luis_=C1ngel_Vicente_S=E1nchez?= <langel.groups@gmail.com>,"Sun, 9 Feb 2014 19:50:08 +0000",Re: proposal: replace lift-json with spray-json,dev@spark.incubator.apache.org,"spray-json future is not clear as spray is going to become akka-http and
the spray team is still deciding the future of spray-json... it may stay or
it may be combined with play-json to create a better library.

spray-json only relies on parboiled, a library mantained by the spray-team
itself. Right now it's performance is much worse than lift-json (
http://engineering.ooyala.com/blog/comparing-scala-json-libraries) but that
would change when they finished parboiled2.

And alternative could be argonaut.io but you would bring scalaz as a
dependency.
lift-json is a nice library, but Lift is a pretty heavyweight dependency to
track just for its JSON support.  (lift-json is relatively self-contained
as a dependency from an end-user's perspective, but downstream distributors
need to build all of Lift in order to package the JSON support.)  I
understand that this has come up before (cf. SPARK-883) and that the
uncertain future of JSON support in the Scala standard library is the
motivator for relying on an external library.

I'm proposing replacing lift-json in Spark with something more lightweight.
 I've evaluated apparent project liveness and dependency scope for most of
the current Scala JSON libraries and believe the best candidate is
spray-json (https://github.com/spray/spray-json), the JSON library used by
the Spray HTTP toolkit. spray-json is Apache-licensed, actively developed,
and builds and works independently of Spray with only one external
dependency.

It looks to me like a pretty straightforward change (although
JsonProtocol.scala would be a little more verbose since it couldn't use the
Lift JSON DSL), and I'd like to do it.  I'm writing now to ask for some
community feedback before making the change (and submitting a JIRA and PR).
 If no one has any serious objections (to the effort in general or to to
the choice of spark-json in particular), I'll go ahead and do it, but if
anyone has concerns, I'd be happy to discuss and address them before
getting started.


thanks,
wb
"
andy petrella <andy.petrella@gmail.com>,"Sun, 9 Feb 2014 21:09:05 +0100",Re: proposal: replace lift-json with spray-json,dev@spark.incubator.apache.org,"Pickling ?
At least, Heather did some benchmarks as well and the computability plus
its automation thanks to macro are two encouraging features.
Also, it is low level enough to allow some de/serialisation optimization.

The ooyala blog doesn't include it already, but I know his author knows
about Pickling and I guess t best the time ran over :-)

Andy
 Le 9 fÃ©vr. 2014 20:50, ""Luis Ãngel Vicente SÃ¡nchez"" <
langel.groups@gmail.com> a Ã©crit :

or
m
to
rs
t.
f
y
,
he
).
"
Will Benton <willb@redhat.com>,"Sun, 9 Feb 2014 15:12:24 -0500 (EST)",Re: proposal: replace lift-json with spray-json,dev@spark.incubator.apache.org,"
(It looks like json4s and jackson-module-scala are excellent candidates in any case.)


best,
wb

"
Will Benton <willb@redhat.com>,"Sun, 9 Feb 2014 15:13:07 -0500 (EST)",Re: proposal: replace lift-json with spray-json,dev@spark.incubator.apache.org,"Thanks; I'll take a look!


ght.
 of
 by
ed,
 the
PR).
o
f

"
pwendell <git@git.apache.org>,"Sun,  9 Feb 2014 20:21:25 +0000 (UTC)",[GitHub] incubator-spark pull request: spark on yarn - yarn-client mode doe...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/526#issuecomment-34585015
  
    @rxin Hm seems like this didn't link up correctly either, wonder if this is because the ""Closes XX"" gets broken in the way that github renders the pull request title.
    



"
rezazadeh <git@git.apache.org>,"Sun,  9 Feb 2014 20:22:05 +0000 (UTC)",[GitHub] incubator-spark pull request: Principal Component Analysis,dev@spark.incubator.apache.org,"Github user rezazadeh commented on the pull request:

    https://github.com/apache/incubator-spark/pull/564#issuecomment-34585044
  
    
    Mentioned before, sparse vector primitives are tracked by https://spark-project.atlassian.net/browse/MLLIB-18


"
pwendell <git@git.apache.org>,"Sun,  9 Feb 2014 20:22:52 +0000 (UTC)",[GitHub] incubator-spark pull request: spark on yarn - yarn-client mode doe...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/526#issuecomment-34585080
  
    @rxin - ya I just checked. Kay's pull request had the same issue. I'll change the script around to fix this.


"
=?ISO-8859-1?Q?Luis_=C1ngel_Vicente_S=E1nchez?= <langel.groups@gmail.com>,"Sun, 9 Feb 2014 20:26:30 +0000",Re: proposal: replace lift-json with spray-json,dev@spark.incubator.apache.org,"I could take a look on how lift-json is used in spark and compare it to
argonaut.io. That would be a nice exercise as I still haven't played with
json in spark.

"
pwendell <git@git.apache.org>,"Sun,  9 Feb 2014 20:53:23 +0000 (UTC)",[GitHub] incubator-spark pull request: Fixes bug where merges won't close a...,dev@spark.incubator.apache.org,"GitHub user pwendell opened a pull request:

    https://github.com/apache/incubator-spark/pull/569

    Fixes bug where merges won't close associated pull request.

    Previously we added ""Closes #XX"" in the title. Github will sometimes
    linbreak the title in a way that causes this to not work. This patch
    instead adds the line in the body.
    
    This also makes the commit format more concise for merge commits.
    We might consider just dropping those in the future.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark merge-fixes

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/569.patch

----
commit 732eba113bbbda177d39d8f88d24e2b0acb440a5
Author: Patrick Wendell <pwendell@gmail.com>
Date:   2014-02-09T20:51:14Z

    Fixes bug where merges won't close associated pull request.
    
    Previously we added ""Closes #XX"" in the title. Github will sometimes
    linbreak the title in a way that causes this to not work. This patch
    instead adds the line in the body.
    
    This also makes the commit format more concise for merge commits.
    We might consider just dropping those in the future.

----


"
pwendell <git@git.apache.org>,"Sun,  9 Feb 2014 20:53:28 +0000 (UTC)",[GitHub] incubator-spark pull request: Fixes bug where merges won't close a...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/569#issuecomment-34586150
  
    /cc @rxin


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 20:57:59 +0000 (UTC)",[GitHub] incubator-spark pull request: Fixes bug where merges won't close a...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/569#issuecomment-34586304
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 20:57:59 +0000 (UTC)",[GitHub] incubator-spark pull request: Fixes bug where merges won't close a...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/569#issuecomment-34586303
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 21:25:28 +0000 (UTC)",[GitHub] incubator-spark pull request: Fixes bug where merges won't close a...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/569#issuecomment-34587275
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12649/


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 21:25:28 +0000 (UTC)",[GitHub] incubator-spark pull request: Fixes bug where merges won't close a...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/569#issuecomment-34587274
  
    Merged build finished.


"
rxin <git@git.apache.org>,"Sun,  9 Feb 2014 21:54:32 +0000 (UTC)",[GitHub] incubator-spark pull request: Fixes bug where merges won't close a...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/569#issuecomment-34588246
  
    Ok I merged this PR using the script in this PR. Let's see ...


"
rxin <git@git.apache.org>,"Sun,  9 Feb 2014 21:57:27 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1038] Add more fields in Json...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/551#issuecomment-34588354
  
    LGTM. Merging this now. Thanks!



"
rxin <git@git.apache.org>,"Sun,  9 Feb 2014 22:00:22 +0000 (UTC)",[GitHub] incubator-spark pull request: new MLlib documentation for optimiza...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/566#issuecomment-34588467
  
    Thanks for rebasing this. There's a couple small conflicts, likely from the style cleanup pull request. Do you mind updating this so it can be merged cleanly? 



"
qqsun8819 <git@git.apache.org>,"Sun,  9 Feb 2014 22:27:37 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1038] Add more fields in Json...,dev@spark.incubator.apache.org,"Github user qqsun8819 closed the pull request at:

    https://github.com/apache/incubator-spark/pull/551


"
pwendell <git@git.apache.org>,"Sun,  9 Feb 2014 22:27:37 +0000 (UTC)",[GitHub] incubator-spark pull request: Fixes bug where merges won't close a...,dev@spark.incubator.apache.org,"Github user pwendell closed the pull request at:

    https://github.com/apache/incubator-spark/pull/569


"
martinjaggi <git@git.apache.org>,"Sun,  9 Feb 2014 22:27:43 +0000 (UTC)",[GitHub] incubator-spark pull request: new MLlib documentation for optimiza...,dev@spark.incubator.apache.org,"Github user martinjaggi commented on the pull request:

    https://github.com/apache/incubator-spark/pull/566#issuecomment-34589384
  
    yes, that conflict came from the style change. i have rebased the patch again now


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 22:28:03 +0000 (UTC)",[GitHub] incubator-spark pull request: new MLlib documentation for optimiza...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/566#issuecomment-34589395
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 22:28:03 +0000 (UTC)",[GitHub] incubator-spark pull request: new MLlib documentation for optimiza...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/566#issuecomment-34589394
  
     Merged build triggered.


"
Matei Zaharia <matei.zaharia@gmail.com>,"Sun, 9 Feb 2014 14:29:20 -0800",Re: proposal:  replace lift-json with spray-json,dev@spark.incubator.apache.org,"Will, why are you saying that downstream distributes need to build all of Lift to package lift-json? Spark just downloads it from Maven Central, where it’s a JAR with no external dependencies. We don’t have any dependency on the rest of lift.

Matei


dependency to track just for its JSON support.  (lift-json is relatively self-contained as a dependency from an end-user's perspective, but downstream distributors need to build all of Lift in order to package the JSON support.)  I understand that this has come up before (cf. SPARK-883) and that the uncertain future of JSON support in the Scala standard library is the motivator for relying on an external library.
lightweight.  I've evaluated apparent project liveness and dependency scope for most of the current Scala JSON libraries and believe the best candidate is spray-json (https://github.com/spray/spray-json), the JSON library used by the Spray HTTP toolkit. spray-json is Apache-licensed, actively developed, and builds and works independently of Spray with only one external dependency.
JsonProtocol.scala would be a little more verbose since it couldn't use the Lift JSON DSL), and I'd like to do it.  I'm writing now to ask for some community feedback before making the change (and submitting a JIRA and PR).  If no one has any serious objections (to the effort in general or to to the choice of spark-json in particular), I'll go ahead and do it, but if anyone has concerns, I'd be happy to discuss and address them before getting started.


"
rxin <git@git.apache.org>,"Sun,  9 Feb 2014 22:45:39 +0000 (UTC)","[GitHub] incubator-spark pull request: SPARK-1058, Fix Style Errors and Add...",dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/567#issuecomment-34589963
  
    Thanks, @ScrapCodes, for doing this! I just went through all your changes. I made some comments to improve the readability of some line wraps. Often you want to take into account the semantic meaning of a line break in addition to just satisfying the rules. 


"
pwendell <git@git.apache.org>,"Sun,  9 Feb 2014 22:52:20 +0000 (UTC)",[GitHub] incubator-spark pull request: spark on yarn - yarn-client mode doe...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/526#issuecomment-34590172
  
    @tgravescs mind closing this


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 22:53:01 +0000 (UTC)",[GitHub] incubator-spark pull request: new MLlib documentation for optimiza...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/566#issuecomment-34590189
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 22:53:01 +0000 (UTC)",[GitHub] incubator-spark pull request: new MLlib documentation for optimiza...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/566#issuecomment-34590188
  
     Merged build triggered.


"
srowen <git@git.apache.org>,"Sun,  9 Feb 2014 22:53:26 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1071: Tidy logging strategy an...,dev@spark.incubator.apache.org,"GitHub user srowen opened a pull request:

    https://github.com/apache/incubator-spark/pull/570

    SPARK-1071: Tidy logging strategy and use of log4j

    Prompted by a recent thread on the mailing list, I tried and failed to see if Spark can be made independent of log4j. There are a few cases where control of the underlying logging is pretty useful, and to do that, you have to bind to a specific logger. 
    
    Instead I propose some tidying that leaves Spark's use of log4j, but gets rid of warnings and should still enable downstream users to switch. The idea is to pipe everything (except log4j) through SLF4J, and have Spark use SLF4J directly when logging, and where Spark needs to output info (REPL and tests), bind from SLF4J to log4j.
    
    This leaves the same behavior in Spark. It means that downstream users who want to use something except log4j should:
    
    - Exclude dependencies on log4j, slf4j-log4j12 from Spark
    - Include dependency on log4j-over-slf4j
    - Include dependency on another logger X, and another slf4j-X
    - Recreate any log config that Spark does, that is needed, in the other logger's config
    
    That sounds about right.
    
    Here are the key changes: 
    
    - Include the jcl-over-slf4j shim everywhere by depending on it in core.
    - Exclude dependencies on commons-logging from third-party libraries.
    - Include the jul-to-slf4j shim everywhere by depending on it in core.
    - Exclude slf4j-* dependencies from third-party libraries to prevent collision or warnings
    - Added missing slf4j-log4j12 binding to GraphX, Bagel module tests
    
    And minor/incidental changes:
    
    - Update to SLF4J 1.7.5, which happily matches Hadoop 2â€™s version and is a recommended update over 1.7.2
    - (Remove a duplicate HBase dependency declaration in SparkBuild.scala)
    - (Remove a duplicate mockito dependency declaration that was causing warnings and bugging me)

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark SPARK-1071

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/570.patch

----
commit 4f27b339cb12a61e70d61d99594cca00340f31c8
Author: Sean Owen <sowen@cloudera.com>
Date:   2014-02-09T22:51:17Z

    SPARK-1071: Tidy logging strategy and use of log4j

----


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 22:55:44 +0000 (UTC)",[GitHub] incubator-spark pull request: new MLlib documentation for optimiza...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/566#issuecomment-34590263
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 22:55:44 +0000 (UTC)",[GitHub] incubator-spark pull request: new MLlib documentation for optimiza...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/566#issuecomment-34590264
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12650/


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 22:57:59 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1071: Tidy logging strategy an...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/570#issuecomment-34590322
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 22:57:59 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1071: Tidy logging strategy an...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/570#issuecomment-34590323
  
    Merged build started.


"
Pascal Voitot Dev <pascal.voitot.dev@gmail.com>,"Mon, 10 Feb 2014 00:00:53 +0100",Re: proposal: replace lift-json with spray-json,dev <dev@spark.incubator.apache.org>,"Hi,
I'm one of the play-json developer and I'm advocating for making it an
completely independent library outside of Play (it's already independent of
Play but it's still delivered  as a module of Play) and integrating useful
tooling I've been developing last year (JsZipper + Json interpolators).
With Play2.3 new Generic Validation API, it will be even more relevant to
externalize Play/Json. So let's see what happens ;)

Anyway, if some people want to speak about integrating Play/json with
Spark, don't hesitate to ask, I'm currently diving in Spark code and begin
to know it a bit more precisely ;)

Regards
Pascal




or
m
to
rs
t.
f
y
,
he
).
"
Pascal Voitot Dev <pascal.voitot.dev@gmail.com>,"Mon, 10 Feb 2014 00:06:48 +0100",Re: proposal: replace lift-json with spray-json,dev@spark.incubator.apache.org,"Hey andy ;)

pickling would be cool for raw serialization/deserialization (not custom
validations naturally).
Anyway, as said in a previous mail on this mailing list, pickling can't be
integrated now until we modify serializers to carry more type information
so that pickling macros can be compiled...

regards
Pascal

e:

d
y
y
ed
o
f
"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 23:21:00 +0000 (UTC)",[GitHub] incubator-spark pull request: new MLlib documentation for optimiza...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/566#issuecomment-34590973
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 23:21:01 +0000 (UTC)",[GitHub] incubator-spark pull request: new MLlib documentation for optimiza...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/566#issuecomment-34590974
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12651/


"
mengxr <git@git.apache.org>,"Sun,  9 Feb 2014 23:23:31 +0000 (UTC)",[GitHub] incubator-spark pull request: Support negative implicit input in A...,dev@spark.incubator.apache.org,"Github user mengxr commented on the pull request:

    https://github.com/apache/incubator-spark/pull/500#issuecomment-34591039
  
    LGTM and thanks for fixing some existing errors!


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 23:25:36 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1071: Tidy logging strategy an...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/570#issuecomment-34591107
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12652/


"
AmplabJenkins <git@git.apache.org>,"Sun,  9 Feb 2014 23:25:36 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1071: Tidy logging strategy an...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/570#issuecomment-34591106
  
    Merged build finished.


"
rxin <git@git.apache.org>,"Sun,  9 Feb 2014 23:28:03 +0000 (UTC)",[GitHub] incubator-spark pull request: new MLlib documentation for optimiza...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/566#issuecomment-34591185
  
    Thanks. I've merged this.


"
Mark Hamstra <mark@clearstorydata.com>,"Sun, 9 Feb 2014 15:35:49 -0800",Re: proposal: replace lift-json with spray-json,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","The JSON handling in the Rapture I/O <http://rapture.io/jsonSupport> library
is also pretty interesting, but I have no idea what its performance now is
or is likely to be, and code maturity is an issue with this project.



e
s
n.
t
st
se
me
d
"
martinjaggi <git@git.apache.org>,"Mon, 10 Feb 2014 00:24:27 +0000 (UTC)",[GitHub] incubator-spark pull request: new MLlib documentation for optimiza...,dev@spark.incubator.apache.org,"Github user martinjaggi closed the pull request at:

    https://github.com/apache/incubator-spark/pull/566


"
Will Benton <willb@redhat.com>,"Sun, 9 Feb 2014 19:50:25 -0500 (EST)",Re: proposal:  replace lift-json with spray-json,dev@spark.incubator.apache.org,"Matei, sorry if I was unclear:  I'm referring to downstream operating system distributions (like Fedora or Debian) that  have policies requiring that all packages are built from source (using only tools already packaged in the distribution).  So end-users (and distributions with different policies) don't have to build Lift to get the lift-json artifact, but it is a concern for many open-source communities.

best,
wb


ght.
of
 by
ed,
l
it,

"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 02:03:08 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1022] Add real unit test for ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/510#issuecomment-34595768
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 02:03:09 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1022] Add real unit test for ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/510#issuecomment-34595769
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 02:31:02 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1022] Add real unit test for ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/510#issuecomment-34596635
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 02:31:02 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1022] Add real unit test for ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/510#issuecomment-34596636
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12653/


"
Nan Zhu <zhunanmcgill@gmail.com>,"Sun, 9 Feb 2014 22:11:01 -0500","Re: How to write test cases for the functionalities which
 involves actor communication",dev@spark.incubator.apache.org,"Good, thank you very much Patrick 

Best, 

-- 
Nan Zhu






"
Nan Zhu <zhunanmcgill@gmail.com>,"Sun, 9 Feb 2014 22:11:01 -0500","Re: How to write test cases for the functionalities which
 involves actor communication",dev@spark.incubator.apache.org,"Good, thank you very much Patrick 

Best, 

-- 
Nan Zhu






"
hsaputra <git@git.apache.org>,"Mon, 10 Feb 2014 04:03:50 +0000 (UTC)",[GitHub] incubator-spark pull request: Improved NetworkReceiver in Spark St...,dev@spark.incubator.apache.org,"Github user hsaputra commented on the pull request:

    https://github.com/apache/incubator-spark/pull/559#issuecomment-34599448
  
    @tdas, per discussion we have in the dev list, could you file JIRA ticket for this PR? Thanks


"
rxin <git@git.apache.org>,"Mon, 10 Feb 2014 05:04:25 +0000 (UTC)",[GitHub] incubator-spark pull request: Support negative implicit input in A...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/500#issuecomment-34601206
  
    Thanks, @mengxr and @MLnick for reviewing this.
    
    @srowen this is no longer mergeable due to the style update pr. Do you mind updating the PR so it can be merged? I believe the conflicts should be trivial to resolve.


"
CodingCat <git@git.apache.org>,"Mon, 10 Feb 2014 05:21:33 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1039] Set the upper bound for...,dev@spark.incubator.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/incubator-spark/pull/472#issuecomment-34601727
  
    added some test cases for the feature, waiting for more feedbacks from the users..


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 05:43:00 +0000 (UTC)","[GitHub] incubator-spark pull request: SPARK-1058, Fix Style Errors and Add...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/567#issuecomment-34602376
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 05:43:00 +0000 (UTC)","[GitHub] incubator-spark pull request: SPARK-1058, Fix Style Errors and Add...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/567#issuecomment-34602377
  
    Merged build started.


"
aarondav <git@git.apache.org>,"Mon, 10 Feb 2014 05:51:01 +0000 (UTC)","[GitHub] incubator-spark pull request: SPARK-1058, Fix Style Errors and Add...",dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/567#issuecomment-34602635
  
    Added a couple comments on top of @rxin's. Thanks so much for doing this @ScrapCodes!


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 05:53:01 +0000 (UTC)","[GitHub] incubator-spark pull request: SPARK-1058, Fix Style Errors and Add...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/567#issuecomment-34602711
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 05:53:01 +0000 (UTC)","[GitHub] incubator-spark pull request: SPARK-1058, Fix Style Errors and Add...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/567#issuecomment-34602710
  
     Merged build triggered.


"
ScrapCodes <git@git.apache.org>,"Mon, 10 Feb 2014 05:53:43 +0000 (UTC)","[GitHub] incubator-spark pull request: SPARK-1058, Fix Style Errors and Add...",dev@spark.incubator.apache.org,"Github user ScrapCodes commented on the pull request:

    https://github.com/apache/incubator-spark/pull/567#issuecomment-34602737
  
    Hey thanks @aarondav, they should have been fixed by now :)


"
aarondav <git@git.apache.org>,"Mon, 10 Feb 2014 06:00:29 +0000 (UTC)","[GitHub] incubator-spark pull request: SPARK-1058, Fix Style Errors and Add...",dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/567#issuecomment-34602966
  
    This patch LGTM then. We should get it in soon (once @rxin is happy) to avoid merge conflicts.


"
rxin <git@git.apache.org>,"Mon, 10 Feb 2014 06:02:06 +0000 (UTC)","[GitHub] incubator-spark pull request: SPARK-1058, Fix Style Errors and Add...",dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/567#issuecomment-34603004
  
    Jenkins, retest this please.


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 06:03:00 +0000 (UTC)","[GitHub] incubator-spark pull request: SPARK-1058, Fix Style Errors and Add...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/567#issuecomment-34603040
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 06:03:00 +0000 (UTC)","[GitHub] incubator-spark pull request: SPARK-1058, Fix Style Errors and Add...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/567#issuecomment-34603041
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 06:10:37 +0000 (UTC)","[GitHub] incubator-spark pull request: SPARK-1058, Fix Style Errors and Add...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/567#issuecomment-34603284
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 06:10:37 +0000 (UTC)","[GitHub] incubator-spark pull request: SPARK-1058, Fix Style Errors and Add...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/567#issuecomment-34603285
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12654/


"
rxin <git@git.apache.org>,"Mon, 10 Feb 2014 06:17:46 +0000 (UTC)","[GitHub] incubator-spark pull request: SPARK-1058, Fix Style Errors and Add...",dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/567#issuecomment-34603519
  
    Thanks! I'm merging this now.


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 06:20:48 +0000 (UTC)","[GitHub] incubator-spark pull request: SPARK-1058, Fix Style Errors and Add...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/567#issuecomment-34603635
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12655/


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 06:20:48 +0000 (UTC)","[GitHub] incubator-spark pull request: SPARK-1058, Fix Style Errors and Add...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/567#issuecomment-34603633
  
    Merged build finished.


"
ScrapCodes <git@git.apache.org>,"Mon, 10 Feb 2014 06:29:45 +0000 (UTC)","[GitHub] incubator-spark pull request: SPARK-1058, Fix Style Errors and Add...",dev@spark.incubator.apache.org,"Github user ScrapCodes closed the pull request at:

    https://github.com/apache/incubator-spark/pull/567


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 06:33:56 +0000 (UTC)","[GitHub] incubator-spark pull request: SPARK-1058, Fix Style Errors and Add...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/567#issuecomment-34604097
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12656/


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 06:33:56 +0000 (UTC)","[GitHub] incubator-spark pull request: SPARK-1058, Fix Style Errors and Add...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/567#issuecomment-34604095
  
    Merged build finished.


"
pwendell <git@git.apache.org>,"Mon, 10 Feb 2014 07:04:26 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1071: Tidy logging strategy an...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/570#issuecomment-34605097
  
    @prb the changes you are referring to are ripping out direct use of log4j there... right?


"
holdenk <git@git.apache.org>,"Mon, 10 Feb 2014 07:32:15 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1072 Use binary search when ne...,dev@spark.incubator.apache.org,"GitHub user holdenk opened a pull request:

    https://github.com/apache/incubator-spark/pull/571

    SPARK-1072 Use binary search when needed in RangePartioner

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark switchtobinarysearch

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/571.patch

----
commit a21e097bcca07983e3b672590a3e3b7458cbcee7
Author: Holden Karau <holden@pigscanfly.ca>
Date:   2014-01-15T06:43:06Z

    Use binary search if we have more than 1000 elements inside of RangePartitioner

----


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 07:32:58 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1072 Use binary search when ne...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/571#issuecomment-34606224
  
    Can one of the admins verify this patch?


"
Paul Brown <prb@mult.ifario.us>,"Sun, 9 Feb 2014 23:35:18 -0800",Re: 0.9.0 forces log4j usage,dev@spark.incubator.apache.org,"Hi, Patrick --

I gave that a go locally, and it works as desired.

Best.
-- Paul

â€”
prb@mult.ifario.us | Multifarious, Inc. | http://mult.ifario.us/



g,
s.
e
2)
e
ve
in
I
;
n
ng
e
t
t
th
nd
d
m
u
er
d
d
ys
4j-has-bound-itself-to
es
is
s.
se
f
is
nd
er
m
e
4j
e
't
o
e
is
d.
e
f
a/org/apache/spark/Logging.scala#L107
a/org/apache/log4j/Category.java?source=c#L81
d
"
pwendell <git@git.apache.org>,"Mon, 10 Feb 2014 07:36:14 +0000 (UTC)",[GitHub] incubator-spark pull request: [WIP] SPARK-1067: Default log4j init...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/560#issuecomment-34606359
  
    FYI I reverted in both branches this because it caused a maven build error. Also it was a WIP and @rxin merged it by accident.


"
holdenk <git@git.apache.org>,"Mon, 10 Feb 2014 07:42:24 +0000 (UTC)",[GitHub] incubator-spark pull request: MLI-2: Add k-fold cross validation t...,dev@spark.incubator.apache.org,"GitHub user holdenk opened a pull request:

    https://github.com/apache/incubator-spark/pull/572

    MLI-2: Add k-fold cross validation to MLLib

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark addkfoldcrossvalidation

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/572.patch

----
commit a5a8492fee4265b1a4225a4a89ce942350c76e4f
Author: Holden Karau <holden@pigscanfly.ca>
Date:   2014-02-05T23:16:54Z

    Add k-fold cross validation to MLLib

----


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 07:42:58 +0000 (UTC)",[GitHub] incubator-spark pull request: MLI-2: Add k-fold cross validation t...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/572#issuecomment-34606646
  
    Can one of the admins verify this patch?


"
rxin <git@git.apache.org>,"Mon, 10 Feb 2014 07:43:31 +0000 (UTC)",[GitHub] incubator-spark pull request: MLI-2: Add k-fold cross validation t...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/572#issuecomment-34606676
  
    Jenkins, add to whitelist. 


"
Patrick Wendell <pwendell@gmail.com>,"Sun, 9 Feb 2014 23:44:22 -0800",Re: 0.9.0 forces log4j usage,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Thanks Paul - it isn't mean to be a ""full solution"" but just a fix for
the 0.9 branch - for the full solution there is another PR by Sean
Owen.


"
prb <git@git.apache.org>,"Mon, 10 Feb 2014 07:44:47 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1071: Tidy logging strategy an...,dev@spark.incubator.apache.org,"Github user prb commented on the pull request:

    https://github.com/apache/incubator-spark/pull/570#issuecomment-34606733
  
    @pwendell I think @srowen's changes are actually cleaning up some different loose ends around jcl versus log4j, and what he's talking about being incumbent on the integrator (e.g., me) is performing the exclusion of log4j/sl4j-log4j.  Otherwise, I think his changes are preserving the status quo.
    
    So your PR #560 resolves the stack overflow issue and this PR further tidies up logging wrt slf4j.


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 07:47:59 +0000 (UTC)",[GitHub] incubator-spark pull request: MLI-2: Add k-fold cross validation t...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/572#issuecomment-34606891
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 07:48:00 +0000 (UTC)",[GitHub] incubator-spark pull request: MLI-2: Add k-fold cross validation t...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/572#issuecomment-34606890
  
     Merged build triggered.


"
pwendell <git@git.apache.org>,"Mon, 10 Feb 2014 07:48:28 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1071: Tidy logging strategy an...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/570#issuecomment-34606918
  
    FYI #560 had a build issue so I had to revert it. But we'll fix that up and get it merged soon into 0.9 as a fix.


"
pwendell <git@git.apache.org>,"Mon, 10 Feb 2014 07:48:55 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1071: Tidy logging strategy an...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/570#issuecomment-34606938
  
    @prb  - understood that this maintains the status quo otherwise, I hadn't looked closely at it.


"
pwendell <git@git.apache.org>,"Mon, 10 Feb 2014 07:51:20 +0000 (UTC)",[GitHub] incubator-spark pull request: Default log4j initialization causes ...,dev@spark.incubator.apache.org,"GitHub user pwendell opened a pull request:

    https://github.com/apache/incubator-spark/pull/573

    Default log4j initialization causes errors for those not using log4j

    This is a re-do of #560 which got merged too soon. There are some maven build issues we need to fix before merging this.
    
    Note currently the diff here is weird because github hasn't caught up with apache git.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark logging

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/573.patch

----
commit d6a9bdc097458ee961072e67627ade8a0a9e3c58
Author: Patrick Wendell <pwendell@gmail.com>
Date:   2014-02-10T07:35:06Z

    Revert ""Merge pull request #560 from pwendell/logging. Closes #560.""
    
    This reverts commit b6d40b782327188a25ded5b22790552121e5271f.

commit 66594e88e5be50fca073a7ef38fa62db4082b3c8
Author: Patrick Wendell <pwendell@gmail.com>
Date:   2014-02-07T23:22:29Z

    Logging fix

----


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 07:52:58 +0000 (UTC)",[GitHub] incubator-spark pull request: Default log4j initialization causes ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/573#issuecomment-34607128
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 07:52:58 +0000 (UTC)",[GitHub] incubator-spark pull request: Default log4j initialization causes ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/573#issuecomment-34607127
  
     Merged build triggered.


"
pwendell <git@git.apache.org>,"Mon, 10 Feb 2014 07:55:24 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1072 Use binary search when ne...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/571#issuecomment-34607216
  
    Jenkins, add to whitelist. Jenkins, test this please.


"
Andrew Ash <andrew@andrewash.com>,"Sun, 9 Feb 2014 23:55:34 -0800",Github merge script,dev@spark.incubator.apache.org,"The current script for merging a GitHub PR squashes the commits and sticks
a ""Merge pull request #123 from abc/def"" at the top of the commit message.
 However this obscures the original commit message when doing a short
gitlog (first line only) so the recent history is much less meaningful than
before.

Compare recent history A:

* 919bd7f Prashant Sharma 86 minutes ago  (origin/master, origin/HEAD)Merge
pull request #567 from ScrapCodes/style2.
* 2182aa3 Martin Jaggi 8 hours ago Merge pull request #566 from
martinjaggi/copy-MLlib-d.
* afc8f3c qqsun8819 10 hours ago Merge pull request #551 from
qqsun8819/json-protocol.
* 94ccf86 Patrick Wendell 10 hours ago Merge pull request #569 from
pwendell/merge-fixes.
* b69f8b2 Patrick Wendell 14 hours ago Merge pull request #557 from
ScrapCodes/style. Closes #557.
* b6dba10 CodingCat 24 hours ago Merge pull request #556 from
CodingCat/JettyUtil. Closes #556.
| * de22abc jyotiska 24 hours ago  (origin/branch-0.9)Merge pull request
#562 from jyotiska/master. Closes #562.
* | 2ef37c9 jyotiska 24 hours ago Merge pull request #562 from
jyotiska/master. Closes #562.
| * 2e3d1c3 Patrick Wendell 24 hours ago Merge pull request #560 from
pwendell/logging. Closes #560.
* | b6d40b7 Patrick Wendell 24 hours ago Merge pull request #560 from
pwendell/logging. Closes #560.
* | f892da8 Patrick Wendell 25 hours ago Merge pull request #565 from
pwendell/dev-scripts. Closes #565.
* | c2341c9 Mark Hamstra 32 hours ago Merge pull request #542 from
markhamstra/versionBump. Closes #542.
| * 22e0a3b Qiuzhuang Lian 35 hours ago Merge pull request #561 from
Qiuzhuang/master. Closes #561.
* | f0ce736 Qiuzhuang Lian 35 hours ago Merge pull request #561 from
Qiuzhuang/master. Closes #561.
* | 7805080 Jey Kottalam 35 hours ago Merge pull request #454 from
jey/atomic-sbt-download. Closes #454.
* | fabf174 Martin Jaggi 2 days ago Merge pull request #552 from
martinjaggi/master. Closes #552.
* | 3a9d82c Andrew Ash 3 days ago Merge pull request #506 from
ash211/intersection. Closes #506.
| * ce179f6 Andrew Or 3 days ago Merge pull request #533 from
andrewor14/master. Closes #533.


To B:

If you go back some time in history, you get a much more branched history,
like this:

| * | | | | | | | | 0984647 Patrick Wendell 4 weeks ago Enable compression
by default for spills
|/ / / / / / / / /
| * | | | | | | | 4e497db Tathagata Das 4 weeks ago Removed
StreamingContext.registerInputStream and registerOutputStream - they were
useless as InputDStream has been made to register itself. Also made DS
* | | | | | | | |   fdaabdc Patrick Wendell 4 weeks ago Merge pull request
#380 from mateiz/py-bayes
|\ \ \ \ \ \ \ \ \
| | | | | * | | | | c2852cf Frank Dai 4 weeks ago Indent two spaces
* | | | | | | | | |   4a805af Patrick Wendell 4 weeks ago Merge pull
request #367 from ankurdave/graphx
|\ \ \ \ \ \ \ \ \ \
| * | | | | | | | | | 80e73ed Joseph E. Gonzalez 4 weeks ago Adding minimal
additional functionality to EdgeRDD
* | | | | | | | | | |   945fe7a Patrick Wendell 4 weeks ago Merge pull
request #408 from pwendell/external-serializers
|\ \ \ \ \ \ \ \ \ \ \
| | * | | | | | | | | | 4bafc4f Joseph E. Gonzalez 4 weeks ago adding
documentation about EdgeRDD
* | | | | | | | | | | |   68641bc Patrick Wendell 4 weeks ago Merge pull
request #413 from rxin/scaladoc
|\ \ \ \ \ \ \ \ \ \ \ \
| | | | | | | | * | | | | 12386b3 Frank Dai 4 weeks ago Since getLong() and
getInt() have side effect, get back parentheses, and remove an empty line
| | | | | | | | * | | | | 0d94d74 Frank Dai 4 weeks ago Code clean up for
mllib
* | | | | | | | | | | | |   0ca0d4d Patrick Wendell 4 weeks ago Merge pull
request #401 from andrewor14/master
|\ \ \ \ \ \ \ \ \ \ \ \ \
| | | | * | | | | | | | | | af645be Ankur Dave 4 weeks ago Fix all code
examples in guide
| | | | * | | | | | | | | | 2cd9358 Ankur Dave 4 weeks ago Finish
6f6f8c928ce493357d4d32e46971c5e401682ea8
* | | | | | | | | | | | | |   08b9fec Patrick Wendell 4 weeks ago Merge
pull request #409 from tdas/unpersist

Ignoring the merge commits here, the commit messages are much better here
than in the current setup because they're what the original author wrote.
 Not a pretty generic ""merged pull request #123 from ash211/patch5"" or
similar.

Looking at one of those squashed commits, we can see the commit message:

$ git show afc8f3c
commit afc8f3cb9a7afe3249500a7d135b4a54bb3e58c4
Author: qqsun8819 <jin.oyj@alibaba-inc.com>
Date:   Sun Feb 9 13:57:29 2014 -0800

    Merge pull request #551 from qqsun8819/json-protocol.

    [SPARK-1038] Add more fields in JsonProtocol and add tests that verify
the JSON itself

    This is a PR for SPARK-1038. Two major changes:
    1 add some fields to JsonProtocol which is new and important to
standalone-related data structures
    2 Use Diff in liftweb.json to verity the stringified Json output for
detecting someone mod type T to Option[T]

    Author: qqsun8819 <jin.oyj@alibaba-inc.com>

    Closes #551 and squashes the following commits:

    fdf0b4e [qqsun8819] [SPARK-1038] 1. Change code style for more readable
according to rxin review 2. change submitdate hard-coded string to a date
object toString for more complexiblity
    095a26f [qqsun8819] [SPARK-1038] mod according to  review of pwendel,
use hard-coded json string for json data validation. Each test use its own
json string
    0524e41 [qqsun8819] Merge remote-tracking branch 'upstream/master' into
json-protocol
    d203d5c [qqsun8819] [SPARK-1038] Add more fields in JsonProtocol and
add tests that verify the JSON itself


I'd like to propose modifying the git merge/squash script to place that
first line (""Merge pull request #551 from qqsun8819/json-protocol"") farther
down in the squashed commit message, to right above the ""Closes #551 and
squashes the following commits:"" line.

That way the author's original one-line commit message title remains intact.

Thoughts?

Thanks!
Andrew


P.S. These graphs are made with this hlog alias:

hlog = log --date-order --all --graph --format=\""%C(green)%h%Creset
%C(yellow)%an%Creset %C(blue bold)%ar%Creset %C(red bold)%d%Creset%s\""
"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 07:58:00 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1072 Use binary search when ne...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/571#issuecomment-34607326
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 07:57:59 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1072 Use binary search when ne...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/571#issuecomment-34607324
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 07:58:51 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1072 Use binary search when ne...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/571#issuecomment-34607363
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12659/


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 07:58:52 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1072 Use binary search when ne...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/571#issuecomment-34607362
  
    Merged build finished.


"
rxin <git@git.apache.org>,"Mon, 10 Feb 2014 08:01:33 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1072 Use binary search when ne...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/571#issuecomment-34607475
  
    Our Jenkins style checker actually worked!!!


"
Patrick Wendell <pwendell@gmail.com>,"Mon, 10 Feb 2014 00:03:35 -0800",Re: Github merge script,Andrew Ash <andrew@andrewash.com>,"Hey Andrew,

The intent was to be consistent with the way the merge messages look
before. But I agree it obfuscates the commit messages from the user
and hides them further down.

I think your proposal is good, but it might be better to use the title
of their pull request message rather than the first line of the most
recent commit in their branch (not sure what you meant by ""commit
message"").

Maybe you could submit a pull request for this? The script we use to
merge things is in dev/merge_spark_pr.py.

Another nice thing is if people are formatting their titles with
jira's then it will all look nice and pretty... which is kind of the
goal.

- Patrick


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 08:16:28 +0000 (UTC)",[GitHub] incubator-spark pull request: MLI-2: Add k-fold cross validation t...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/572#issuecomment-34608209
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 08:16:28 +0000 (UTC)",[GitHub] incubator-spark pull request: MLI-2: Add k-fold cross validation t...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/572#issuecomment-34608210
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12657/


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 08:21:01 +0000 (UTC)",[GitHub] incubator-spark pull request: Default log4j initialization causes ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/573#issuecomment-34608468
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 08:21:01 +0000 (UTC)",[GitHub] incubator-spark pull request: Default log4j initialization causes ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/573#issuecomment-34608470
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12658/


"
ash211 <git@git.apache.org>,"Mon, 10 Feb 2014 08:22:08 +0000 (UTC)",[GitHub] incubator-spark pull request: Keep GitHub pull request title as co...,dev@spark.incubator.apache.org,"GitHub user ash211 opened a pull request:

    https://github.com/apache/incubator-spark/pull/574

    Keep GitHub pull request title as commit summary

    The first line of a git commit message is the line that's used with many git
    tools as the most concise textual description of that message.  The most
    common use that I see is in the short log, which is a one line per commit
    log of recent commits.
    
    This commit moves the line
    
      Merge pull request #%s from %s.
    
    Lower into the message to reserve the first line of the resulting commit for
    the much more important pull request title.
    
    http://tbaggery.com/2008/04/19/a-note-about-git-commit-messages.html

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark gh-pr-merge-title

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/574.patch

----
commit d2986db96d847ee4f888bbb3e247f73b99f7ea7f
Author: Andrew Ash <andrew@andrewash.com>
Date:   2014-02-10T08:16:01Z

    Keep GitHub pull request title as commit summary
    
    The first line of a git commit message is the line that's used with many git
    tools as the most concise textual description of that message.  The most
    common use that I see is in the short log, which is a one line per commit
    log of recent commits.
    
    This commit moves the line
    
      Merge pull request #%s from %s.
    
    Lower into the message to reserve the first line of the resulting commit for
    the much more important pull request title.
    
    http://tbaggery.com/2008/04/19/a-note-about-git-commit-messages.html

----


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 08:22:58 +0000 (UTC)",[GitHub] incubator-spark pull request: Keep GitHub pull request title as co...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/574#issuecomment-34608573
  
    Merged build started.


"
Pascal Voitot Dev <pascal.voitot.dev@gmail.com>,"Mon, 10 Feb 2014 09:22:41 +0100",Re: proposal: replace lift-json with spray-json,dev <dev@spark.incubator.apache.org>,"Hi again,

If spark just need Json with serialization/deserialization of basic
structures and some potential simple validations for webUI, let's remind
that play/json (without any other dependency than Jackson) is about 200-300
lines of code... the only dependency is jackson which is the best json
parser that I know. The rest of code is about typesafety & composition...
If Spark need some json veryveryvery performant JSON (de)serialization, we
will have to look on things like pickling and potentially some streaming
parsers (I think this is a domain under work right now...)

Pascal



"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 08:22:58 +0000 (UTC)",[GitHub] incubator-spark pull request: Keep GitHub pull request title as co...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/574#issuecomment-34608572
  
     Merged build triggered.


"
Andrew Ash <andrew@andrewash.com>,"Mon, 10 Feb 2014 00:23:52 -0800",Re: Github merge script,Patrick Wendell <pwendell@gmail.com>,"I didn't realize that it was the PR title I was looking at above and not
the git commit message summary, so the script is already doing as you
suggest.

A commit message is the text that describes what the change was in the
commit.  There's a standard format (linked in the below PR) that people
tend to follow now.

https://github.com/apache/incubator-spark/pull/574



"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 08:48:15 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-34610073
  
    Build started.


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 08:48:14 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-34610072
  
     Build triggered.


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 08:49:09 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-34610111
  
    Build finished.


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 08:49:09 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-34610112
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12661/


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 08:51:00 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1073 Keep GitHub pull request ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/574#issuecomment-34610236
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 08:51:00 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1073 Keep GitHub pull request ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/574#issuecomment-34610237
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12660/


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 08:58:03 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1072 Use binary search when ne...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/571#issuecomment-34610665
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 08:58:03 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1072 Use binary search when ne...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/571#issuecomment-34610666
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 09:25:36 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1072 Use binary search when ne...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/571#issuecomment-34612377
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 09:25:37 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1072 Use binary search when ne...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/571#issuecomment-34612379
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12662/


"
Aaron Davidson <ilikerps@gmail.com>,"Mon, 10 Feb 2014 01:34:55 -0800",Proposal: Clarifying minor points of Scala style,dev@spark.incubator.apache.org,"There are a few bits of the Scala style that are underspecified by
both the Scala
style guide <http://docs.scala-lang.org/style/> and our own supplemental
notes<https://cwiki.apache.org/confluence/display/SPARK/Spark+Code+Style+Guide>.
Often, this leads to inconsistent formatting within the codebase, so I'd
like to propose some general guidelines which we can add to the wiki and
use in the future:

1) Line-wrapped method return type is indented with two spaces:
def longMethodName(... long param list ...)
  : Long = {
  2
}

*Justification: *I think this is the most commonly used style in Spark
today. It's also similar to the ""extends"" style used in classes, with the
same justification: it is visually distinguished from the 4-indented
parameter list.

2) URLs and code examples in comments should not be line-wrapped.
Here<https://github.com/apache/incubator-spark/pull/557/files#diff-c338f10f3567d4c1d7fec4bf9e2677e1L29>is
an example of the latter.

*Justification*: Line-wrapping can cause confusion when trying to
copy-paste a URL or command. Can additionally cause IDE issues or,
avoidably, Javadoc issues.

Any thoughts on these, or additional style issues not explicitly covered in
either the Scala style guide or Spark wiki?
"
srowen <git@git.apache.org>,"Mon, 10 Feb 2014 11:54:36 +0000 (UTC)",[GitHub] incubator-spark pull request: Default log4j initialization causes ...,dev@spark.incubator.apache.org,"Github user srowen commented on the pull request:

    https://github.com/apache/incubator-spark/pull/573#issuecomment-34623406
  
    A few thoughts on this issue -- I don't know if these lead to a solution --
    
    Is it necessary to guard this block of code with a check to see if log4j is being used at all? the log4j-over-slf4j shim has a no-op implementation of PropertyConfigurator, so callers that swap out log4j for the shim will see this code do nothing. 
    
    Or, it would be nice to just set the config file with the system property `-Dlog4j.configuration=...` It's not great to make people specify this on the command line. It could be set programmatically at this point in the code, although I don't know if it's ""too late"" then.
    
    Or, what about packaging the file in the default location/name of `/log4j.properties` in the artifacts? It doesn't have effect unless you use log4j. We would be clobbering other copies of this file from other third-party libs, but that's either not a problem, or, something Maven can help us with by merging the files. I know the mojo for that if necessary.


"
srowen <git@git.apache.org>,"Mon, 10 Feb 2014 12:42:37 +0000 (UTC)",[GitHub] incubator-spark pull request: MLLIB-22. Support negative implicit ...,dev@spark.incubator.apache.org,"Github user srowen commented on the pull request:

    https://github.com/apache/incubator-spark/pull/500#issuecomment-34627005
  
    @rxin Done, and I opened a JIRA for this too, retroactively, given that's the current practice.


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 12:43:10 +0000 (UTC)",[GitHub] incubator-spark pull request: MLLIB-22. Support negative implicit ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/500#issuecomment-34627027
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 12:43:10 +0000 (UTC)",[GitHub] incubator-spark pull request: MLLIB-22. Support negative implicit ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/500#issuecomment-34627028
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 12:48:04 +0000 (UTC)",[GitHub] incubator-spark pull request: [WIP] [javaAPI] SPARK-964 Investigat...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-34627337
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 12:48:04 +0000 (UTC)",[GitHub] incubator-spark pull request: [WIP] [javaAPI] SPARK-964 Investigat...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-34627336
  
     Merged build triggered.


"
ScrapCodes <git@git.apache.org>,"Mon, 10 Feb 2014 12:52:05 +0000 (UTC)",[GitHub] incubator-spark pull request: [WIP] [javaAPI] SPARK-964 Investigat...,dev@spark.incubator.apache.org,"Github user ScrapCodes commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-34627582
  
    Tests added in the PR will run only if sbt sees java 1.8 is set on the system. 


"
ScrapCodes <git@git.apache.org>,"Mon, 10 Feb 2014 13:03:38 +0000 (UTC)",[GitHub] incubator-spark pull request: [WIP] [javaAPI] SPARK-964 Investigat...,dev@spark.incubator.apache.org,"Github user ScrapCodes commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-34628377
  
    Also the failing/ignored tests in this PR pass if we use 
    ```scala
         ""org.ow2.asm""              % ""asm""              % ""5.0_BETA"",
    ```
    
    Since it is in beta, I am not sure ! 
    @JoshRosen What do you think ?


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 13:08:04 +0000 (UTC)",[GitHub] incubator-spark pull request: [WIP] [javaAPI] SPARK-964 Investigat...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-34628726
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 13:08:04 +0000 (UTC)",[GitHub] incubator-spark pull request: [WIP] [javaAPI] SPARK-964 Investigat...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-34628724
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 13:10:57 +0000 (UTC)",[GitHub] incubator-spark pull request: MLLIB-22. Support negative implicit ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/500#issuecomment-34628966
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 13:10:57 +0000 (UTC)",[GitHub] incubator-spark pull request: MLLIB-22. Support negative implicit ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/500#issuecomment-34628967
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12663/


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 13:16:40 +0000 (UTC)",[GitHub] incubator-spark pull request: [WIP] [javaAPI] SPARK-964 Investigat...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-34629435
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 13:16:40 +0000 (UTC)",[GitHub] incubator-spark pull request: [WIP] [javaAPI] SPARK-964 Investigat...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-34629437
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12664/


"
ScrapCodes <git@git.apache.org>,"Mon, 10 Feb 2014 13:25:15 +0000 (UTC)",[GitHub] incubator-spark pull request: [WIP] [javaAPI] SPARK-964 Investigat...,dev@spark.incubator.apache.org,"Github user ScrapCodes commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-34630047
  
    There is another point, if asm 5.0 is not used, the test fail irrespective of lambdas are used or not.


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 13:33:04 +0000 (UTC)",[GitHub] incubator-spark pull request: [WIP] [javaAPI] SPARK-964 Investigat...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-34631237
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 13:33:04 +0000 (UTC)",[GitHub] incubator-spark pull request: [WIP] [javaAPI] SPARK-964 Investigat...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-34631235
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 13:36:22 +0000 (UTC)",[GitHub] incubator-spark pull request: [WIP] [javaAPI] SPARK-964 Investigat...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-34631798
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 13:36:22 +0000 (UTC)",[GitHub] incubator-spark pull request: [WIP] [javaAPI] SPARK-964 Investigat...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-34631799
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12665/


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 13:56:15 +0000 (UTC)",[GitHub] incubator-spark pull request: [WIP] [javaAPI] SPARK-964 Investigat...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-34633358
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 13:56:15 +0000 (UTC)",[GitHub] incubator-spark pull request: [WIP] [javaAPI] SPARK-964 Investigat...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-34633360
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12666/


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 14:01:34 +0000 (UTC)",[GitHub] incubator-spark pull request: [WIP] [javaAPI] SPARK-964 Investigat...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-34633818
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 14:01:34 +0000 (UTC)",[GitHub] incubator-spark pull request: [WIP] [javaAPI] SPARK-964 Investigat...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-34633819
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12667/


"
markhamstra <git@git.apache.org>,"Mon, 10 Feb 2014 17:22:45 +0000 (UTC)","[GitHub] incubator-spark pull request: Upgraded sbt (0.12.4 to 0.13.1), sbt...",dev@spark.incubator.apache.org,"Github user markhamstra commented on the pull request:

    https://github.com/apache/incubator-spark/pull/266#issuecomment-34657219
  
    Looks like a couple of recent commits have pulled in piecemeal all elements of this PR, so this one can now be closed. 


"
ash211 <git@git.apache.org>,"Mon, 10 Feb 2014 17:36:28 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark Job Server Preview,dev@spark.incubator.apache.org,"Github user ash211 commented on the pull request:

    https://github.com/apache/incubator-spark/pull/222#issuecomment-34658693
  
    Is this PR still in active development?  I for one would find this feature very valuable.


"
markhamstra <git@git.apache.org>,"Mon, 10 Feb 2014 17:39:22 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark Job Server Preview,dev@spark.incubator.apache.org,"Github user markhamstra commented on the pull request:

    https://github.com/apache/incubator-spark/pull/222#issuecomment-34659012
  
    If it's not, I think it should be.  Evan?
    
    Some kind of canonical example job server is something I think should be in 1.0.


"
velvia <git@git.apache.org>,"Mon, 10 Feb 2014 17:43:57 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark Job Server Preview,dev@spark.incubator.apache.org,"Github user velvia commented on the pull request:

    https://github.com/apache/incubator-spark/pull/222#issuecomment-34659521
  
    Hey guys,
    
    Sorry, should update this PR.   The current plan is to move the job server
    into a spark-contrib project, but the location is unknown.
    The API will remain pretty much the same, so you could try out this PR and
    it should continue to work in the new location.
    
    Perhaps this discussion should be moved into a JIRA ticket?
    
    
    
    > If it's not, I think it should be. Evan?
    >
    > Some kind of canonical example job server is something I think should be
    > in 1.0.
    >
    > --
    > Reply to this email directly or view it on GitHub<https://github.com/apache/incubator-spark/pull/222#issuecomment-34659012>
    > .
    >
    
    
    
    -- 
    The fruit of silence is prayer;
    the fruit of prayer is faith;
    the fruit of faith is love;
    the fruit of love is service;
    the fruit of service is peace.  -- Mother Teresa


"
mengxr <git@git.apache.org>,"Mon, 10 Feb 2014 17:46:19 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"GitHub user mengxr opened a pull request:

    https://github.com/apache/incubator-spark/pull/575

    [Proposal] Adding sparse data support and update KMeans

    This is a proposal for sparse data support in mllib (https://spark-project.atlassian.net/browse/MLLIB-18). 
    
    The idea of the proposal is that we define simple data models and factory methods for user to provide sparse input. Then instead of writing a linear algebra library for mllib, we take leverage on an existing linear algebra package in implementing algorithms. So we can change the underlying implementation without breaking the interfaces in the future. We need the following:
    
    * data models for sparse vectors. We need data models for dense vector, sequential access sparse vector (backed by two parallel arrays), and random access sparse vector (backed by a primitive-typed hash map, not in this pull request.). Those are defined in the Vec class in this PR.
    * a linear algebra package. We are considering either breeze or mahout-math. Both have pros and cons, and we can discuss more in the JIRA. This PR uses mahout-math. Mahout vectors do not implement serializable, so we need a serializable wrapper class (MahoutVectorWrapper) to use in spark. As a result, we added not only mahout-math but also mathout-core into dependencies because we need VectorWritable defined mahout-core for the wrapper class. But we can certainly remove most transitive dependencies of mahout-core.
    * lightweight converters. The conversion between our data models and Mahout vectors shouldn't involve data copying. However, Mahout vectors hide their members. In this PR, Java reflection is used to get the private fields out. This doesn't seem to be a good solution, but I didn't figure out a better one.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark sparse

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/575.patch

----
commit 37c423e746e26d9c3db23580df34a959ddf8fe44
Author: Xiangrui Meng <meng@databricks.com>
Date:   2014-02-10T08:33:42Z

    add mahout-math and mahout-core to mllib

commit 6d3fda1b07683824719330243cb07275d5537f76
Author: Xiangrui Meng <meng@databricks.com>
Date:   2014-02-10T08:44:13Z

    add MahoutVectorWrapper

commit ff2c072b712479f2f664a4f3828b85f46c46128e
Author: Xiangrui Meng <meng@databricks.com>
Date:   2014-02-10T08:47:35Z

    add implicit conversions

commit ee53e096bb101571a559df1d78c8091bb3ba4b0a
Author: Xiangrui Meng <meng@databricks.com>
Date:   2014-02-10T09:38:12Z

    add Vec and MahoutVectorHelper

commit 92be705a9c317308554d23105e6fc747764e6568
Author: Xiangrui Meng <meng@databricks.com>
Date:   2014-02-10T10:04:01Z

    update LocalKMeans

commit d56239954fc41482a86993896ca9229c6dbb0756
Author: Xiangrui Meng <meng@databricks.com>
Date:   2014-02-10T10:15:02Z

    use mahout in KMeans

commit 38f0cc6058c88628827c1513845de04eed5da69e
Author: Xiangrui Meng <meng@databricks.com>
Date:   2014-02-10T10:41:58Z

    add Vec interface to KMeansModel
    add a sparse test to KMeansSuite

commit 686fb79872421ff5cbc0e083051fae78c79186aa
Author: Xiangrui Meng <meng@databricks.com>
Date:   2014-02-10T10:49:20Z

    add headers and docs

commit 273af590eb8c9b720cc5dffa1b1e03447c93362e
Author: Xiangrui Meng <meng@databricks.com>
Date:   2014-02-10T10:55:04Z

    add VecSuite

commit b7e06c86f65830d8704977960698e13bc4d06070
Author: Xiangrui Meng <meng@databricks.com>
Date:   2014-02-10T11:00:40Z

    remove default constructor from MahoutVectorWrapper

----


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 17:48:00 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-34659925
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 17:48:00 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-34659923
  
     Merged build triggered.


"
ash211 <git@git.apache.org>,"Mon, 10 Feb 2014 17:51:50 +0000 (UTC)",[GitHub] incubator-spark pull request: Patch for SPARK-942,dev@spark.incubator.apache.org,"Github user ash211 commented on the pull request:

    https://github.com/apache/incubator-spark/pull/180#issuecomment-34660329
  
    I often use Spark for ETL and the ability to stream directly onto (HDFS) disk would be valuable for me.  No need to buffer in memory if we don't have to.
    
    Admins, is there anything else you want to see from this PR before merging?


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 18:16:14 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-34662959
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 18:16:13 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-34662960
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12668/


"
pwendell <git@git.apache.org>,"Mon, 10 Feb 2014 18:35:40 +0000 (UTC)",[GitHub] incubator-spark pull request: Default log4j initialization causes ...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/573#issuecomment-34665107
  
    @srowen There is a guard in the form of checking that slf4j is bound to log4j... is there any other guard you can think of? I think this means this should only be enabled if the user has statically bound the log4j connector to slf4j.


"
laserson <git@git.apache.org>,"Mon, 10 Feb 2014 18:36:20 +0000 (UTC)",[GitHub] incubator-spark pull request: Added parquetFileAsJSON to read Parq...,dev@spark.incubator.apache.org,"GitHub user laserson opened a pull request:

    https://github.com/apache/incubator-spark/pull/576

    Added parquetFileAsJSON to read Parquet data into JSON strings

    This function makes it incredibly easy to read Parquet data especially with PySpark.  Is there any interest in this?  It requires pulling in some Parquet dependencies, and adding some Parquet jars to SPARK_CLASSPATH.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark pyspark-parquet

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/576.patch

----
commit 2e1969e33da97253eb3dccf51e54afb469ed9fd5
Author: Uri Laserson <laserson@cloudera.com>
Date:   2014-02-10T01:28:08Z

    Added parquetFileAsJSON to read Parquet data into JSON strings

----


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 18:37:59 +0000 (UTC)",[GitHub] incubator-spark pull request: Added parquetFileAsJSON to read Parq...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/576#issuecomment-34665329
  
    Can one of the admins verify this patch?


"
srowen <git@git.apache.org>,"Mon, 10 Feb 2014 18:48:25 +0000 (UTC)",[GitHub] incubator-spark pull request: Default log4j initialization causes ...,dev@spark.incubator.apache.org,"Github user srowen commented on the pull request:

    https://github.com/apache/incubator-spark/pull/573#issuecomment-34666426
  


"
mengxr <git@git.apache.org>,"Mon, 10 Feb 2014 19:04:26 +0000 (UTC)",[GitHub] incubator-spark pull request: MLI-2: Add k-fold cross validation t...,dev@spark.incubator.apache.org,"Github user mengxr commented on the pull request:

    https://github.com/apache/incubator-spark/pull/572#issuecomment-34668194
  
    @holdenk , the PartitionwiseSampledRDD was designed with this use case in mind. Both the folded RDD and its complement can be represented by PartitionwiseSampledRDD with BernoulliSamplers. Do you mind modifying your code to use it? Also, cross-validation is a machine learning specific operation. spark.rdd.RDD may not be a good place for it. 


"
holdenk <git@git.apache.org>,"Mon, 10 Feb 2014 19:29:01 +0000 (UTC)",[GitHub] incubator-spark pull request: MLI-2: Add k-fold cross validation t...,dev@spark.incubator.apache.org,"Github user holdenk commented on the pull request:

    https://github.com/apache/incubator-spark/pull/572#issuecomment-34671873
  
    Sure, I'll take a look at that tonight. From the earlier pull request that
    was abandoned someone had asked that its PartionedRDD (which only did it
    for k=2) be in the core rather than mllib.
    
    
    
    > @holdenk <https://github.com/holdenk> , the PartitionwiseSampledRDD was
    > designed with this use case in mind. Both the folded RDD and its complement
    > can be represented by PartitionwiseSampledRDD with BernoulliSamplers. Do
    > you mind modifying your code to use it? Also, cross-validation is a machine
    > learning specific operation. spark.rdd.RDD may not be a good place for it.
    >
    > --
    > Reply to this email directly or view it on GitHub<https://github.com/apache/incubator-spark/pull/572#issuecomment-34668194>
    > .
    >
    
    
    
    -- 
    Cell : 425-233-8271


"
Reynold Xin <rxin@databricks.com>,"Mon, 10 Feb 2014 11:33:27 -0800",Re: Proposal: Clarifying minor points of Scala style,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","+1 on both



"
sscdotopen <git@git.apache.org>,"Mon, 10 Feb 2014 19:51:33 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user sscdotopen commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-34674441
  
    I think making making the heavyweight mahout-core a dependency just for access to the sparse vectors is no good idea. A better way would be to just depend on mahout-math and port the serialization code from VectorWritable.


"
hsaputra <git@git.apache.org>,"Mon, 10 Feb 2014 19:59:03 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1075 Fix doc in the Spark Stre...,dev@spark.incubator.apache.org,"GitHub user hsaputra opened a pull request:

    https://github.com/apache/incubator-spark/pull/577

    SPARK-1075 Fix doc in the Spark Streaming custom receiver closing bracket in the class constructor

    The closing parentheses in the constructor in the first code block example is reversed:
    diff --git a/docs/streaming-custom-receivers.md b/docs/streaming-custom-receivers.md
    index 4e27d65..3fb540c 100644
    â€” a/docs/streaming-custom-receivers.md
    +++ b/docs/streaming-custom-receivers.md
    @@ -14,7 +14,7 @@ This starts with implementing NetworkReceiver(api/streaming/index.html#org.apa
    The following is a simple socket text-stream receiver.
    {% highlight scala %}
    class SocketTextStreamReceiver(host: String, port: Int(
    + class SocketTextStreamReceiver(host: String, port: Int)
    extends NetworkReceiverString
    {
    protected lazy val blocksGenerator: BlockGenerator =

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark fix_simple_streaming_doc

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/577.patch

----
commit 6508341372193428616a5cc70eaec87616a8d78f
Author: Henry Saputra <henry@platfora.com>
Date:   2014-02-10T19:57:22Z

    SPARK-1075 Fix doc in the Spark Streaming custom receiver.

----


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 20:02:59 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1075 Fix doc in the Spark Stre...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/577#issuecomment-34675686
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 20:03:00 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1075 Fix doc in the Spark Stre...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/577#issuecomment-34675689
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 20:30:53 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1075 Fix doc in the Spark Stre...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/577#issuecomment-34678627
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12669/


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 20:30:53 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1075 Fix doc in the Spark Stre...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/577#issuecomment-34678626
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 20:58:02 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1072 Use binary search when ne...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/571#issuecomment-34681468
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 20:58:02 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1072 Use binary search when ne...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/571#issuecomment-34681467
  
     Merged build triggered.


"
debasish83 <git@git.apache.org>,"Mon, 10 Feb 2014 21:08:43 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user debasish83 commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-34682539
  
    I agree...depending on mahout-math is much better than bringing in the mahout-core...mahout-math code I think will compile fine with Apache Hadoop, CDH and HDP...mahout-core master right now does not compile with CDH 4.5...I had to get binary jars from CDH...


"
sryza <git@git.apache.org>,"Mon, 10 Feb 2014 21:17:42 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1051. Executors should doAs su...,dev@spark.incubator.apache.org,"Github user sryza commented on the pull request:

    https://github.com/apache/incubator-spark/pull/538#issuecomment-34683445
  
    Updated the patch to work with yarn-standalone mode as well.  Does a doAs in the application master when running the user class.


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 21:18:06 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1051. Executors should doAs su...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/538#issuecomment-34683487
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 21:18:06 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1051. Executors should doAs su...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/538#issuecomment-34683488
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 21:25:36 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1072 Use binary search when ne...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/571#issuecomment-34684202
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 21:25:36 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1072 Use binary search when ne...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/571#issuecomment-34684204
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12670/


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Mon, 10 Feb 2014 13:42:30 -0800",Re: Proposal: Clarifying minor points of Scala style,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","For the 1st case wouldn't it be better to just wrap the parameters to
the next line as we do in other cases ? For example

def longMethodName(
    param1,
    param2, ...) : Long = {
}

Are there a lot functions which use the old format ? Can we just stick
to the above for new functions ?

Thanks
Shivaram


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 21:46:28 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1051. Executors should doAs su...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/538#issuecomment-34686449
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 21:46:29 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1051. Executors should doAs su...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/538#issuecomment-34686450
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12671/


"
Aaron Davidson <ilikerps@gmail.com>,"Mon, 10 Feb 2014 14:13:24 -0800",Re: Proposal: Clarifying minor points of Scala style,"dev@spark.incubator.apache.org, 
	Shivaram Venkataraman <shivaram@eecs.berkeley.edu>","Shivaram, is your recommendation to wrap the parameter list even if it
fits, but just the return type doesn't? Personally, I think the cost of
moving from a single-line parameter list to an n-ine list is pretty high,
as it takes up a lot more space. I am even in favor of allowing a parameter
list to overflow into a second line (but not a third) instead of spreading
them out, if it's a private helper method (where the parameters are
probably not as important as the implementation, unlike a public API).



"
srowen <git@git.apache.org>,"Mon, 10 Feb 2014 22:33:33 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user srowen commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-34692729
  
    The mahout-math implementation of vectors is encumbered with a few bad design choices, Hadoop stuff that's not needed here, dependence on that old fork of colt code, and a few lingering bugs. From experience I would strongly recommend not using this code. You're having to use reflection (!!) and wrappers to get it working -- this can't be OK for new code.
    
    MLLib is already using JBlas. That doesn't have a sparse representation, but, this is making things worse since it's bringing in and using a second dense vector representation.
    
    I have used Commons Math successfully, but strangely they're deprecating the sparse representation, although it's been perfectly fine for me. I'd recommend it, still.
    
    In the past I have used Commons Math in order to have one unified API for sparse/dense, and then translated to JBlas in key cases for speed. (Using JBlas everywhere might not be a great idea, actually.) I'd recommend that road if we're bothering to overhaul this.
    
    Failing that, if MLLib is going to use JBlas everywhere it can be used, it should stick to JBlas for all dense vectors and matrices. Something else is needed for sparse. I still recommend Commons Math, or some derivative of it. There's always the possibility of writing a JBlas-like sparse API, which would be tidy and consistent, but annoying to reinvent the wheel again.
    
    (mahout does not compile against Hadoop 2 unless you change the profile -- this is why you need CDH4.5-mr1 artifacts, or need to use the Hadoop 2 profile)



"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Mon, 10 Feb 2014 14:36:56 -0800",Re: Proposal: Clarifying minor points of Scala style,Aaron Davidson <ilikerps@gmail.com>,"Yeah that was my proposal - Essentially we can just have two styles:
The entire function + parameterList + return type fits in one line or
when it doesn't we wrap parameters into lines.
I agree that it makes the code a more verbose, but it'll make code
style more consistent.

Shivaram


"
Michael Armbrust <michael@databricks.com>,"Mon, 10 Feb 2014 14:56:50 -0800",Re: Proposal: Clarifying minor points of Scala style,"dev@spark.incubator.apache.org, shivaram@eecs.berkeley.edu","+1 to Shivaram's proposal.  I think we should try to avoid functions with
many args as much as possible so having a high vertical cost here isn't the
worst thing.  I also like the visual consistency.

FWIW, (based on a cursory inspection) in the scala com"
pwendell <git@git.apache.org>,"Mon, 10 Feb 2014 22:57:34 +0000 (UTC)",[GitHub] incubator-spark pull request: Default log4j initialization causes ...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/573#issuecomment-34698102
  
    The issue is a user reported problems when they were writing from slf4j to Logback, they had log4j-over-slf4j on the classpath, and this initialization code kicked in when they never intended to output to log4j. I'm proposing this as a defensive check to fix this problem in the 0.9 branch with a minimal modification. If the user has log4j-over-slf4j enabled and someone manually creates a properties configuration it's been known to cause problems. Checkout the section here ""When does this not work"":
    
    http://www.slf4j.org/legacy.html#losFail


"
srowen <git@git.apache.org>,"Mon, 10 Feb 2014 23:13:21 +0000 (UTC)",[GitHub] incubator-spark pull request: Default log4j initialization causes ...,dev@spark.incubator.apache.org,"Github user srowen commented on the pull request:

    https://github.com/apache/incubator-spark/pull/573#issuecomment-34701207
  
    I think I misunderstood the nature of the infinite loop and thought it had to do with querying for the appenders. If not, yeah, removing the guard does not affect the problem. I would hope simply doing it non-programmatically like by packaging the props file in the standard place might work? You're most familiar with this part, and these ideas may not lead to anything.


"
mengxr <git@git.apache.org>,"Mon, 10 Feb 2014 23:29:22 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding assignRanks and assignUniqueI...,dev@spark.incubator.apache.org,"GitHub user mengxr opened a pull request:

    https://github.com/apache/incubator-spark/pull/578

    Adding assignRanks and assignUniqueIds to RDD

    Assign ranks to an ordered or unordered data set is a common operation. This could be done by first counting records in each partition and then assign ranks in parallel.
    
    The purpose of assigning ranks to an unordered set is usually to get a unique id for each item, e.g., to map feature names to feature indices. In such cases, the assignment could be done without counting records, saving one spark job.
    
    https://spark-project.atlassian.net/browse/SPARK-1076

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark rank

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/578.patch

----
commit 21b434b77f1a7ffd75ba2d1ad4ab2296f1914971
Author: Xiangrui Meng <meng@databricks.com>
Date:   2014-02-10T23:18:41Z

    add assignRanks and assignUniqueIds to RDD

commit 630868c88f14ea955991acfd3d68caa8be6dedec
Author: Xiangrui Meng <meng@databricks.com>
Date:   2014-02-10T23:20:21Z

    newline

----


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 23:32:59 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding assignRanks and assignUniqueI...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/578#issuecomment-34705056
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Mon, 10 Feb 2014 23:32:59 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding assignRanks and assignUniqueI...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/578#issuecomment-34705055
  
     Merged build triggered.


"
mengxr <git@git.apache.org>,"Mon, 10 Feb 2014 23:44:48 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user mengxr commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-34707127
  
    @sscdotopen @debasish83 , I'm okay with copying VectorWritable and remove mahout-core from dependencies.
    
    @srowen Just as you mentioned, the sparse vector in commons-math is deprecated and will be removed in 4.0, and it is annoying to implement another sparse linear algebra package and take care of it. I've been struggling between mahout-math and breeze-math for a while. I don't like the design of mahout-math either. I would like to go with breeze-math if it becomes more stable, which utilizes netlib-java for calling native BLAS/LAPACK/ARPACK routines, but now it has performance issues (see the benchmark I posted to the JIRA) and it has been merged into breeze. I don't think there is a clear choice at this time. This is also why we want to separate what we use in the implementation from what interfaces we provide to users.


"
AmplabJenkins <git@git.apache.org>,"Tue, 11 Feb 2014 00:00:53 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding assignRanks and assignUniqueI...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/578#issuecomment-34710309
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Tue, 11 Feb 2014 00:00:53 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding assignRanks and assignUniqueI...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/578#issuecomment-34710311
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12672/


"
srowen <git@git.apache.org>,"Tue, 11 Feb 2014 00:06:24 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user srowen commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-34711528
  
    I see the other discussion -- https://github.com/mesos/spark/pull/736 ? I didn't see the benchmark but maybe missed it.
    
    I think there was an impression there that `mahout-math` is being actively improved, and I don't think there have or appear to be any substantial changes or work on older open issues/bugs: https://github.com/apache/mahout/commits/trunk/math 
    
    No obvious right answers indeed. Breeze sounds like the best fit but I have no familiarity with how much work it would be to speed up whatever needs to be sped up.
    
    Commons Math 3.x will be the mainline release for I presume ~6 more months, with 3.3 coming soon and still including this code. If the idea is that this might be a temporary step, I would suggest it as the least-bad temporary solution for sparse vectors. It is at least somewhat actively supported. And pretty well known in Java land.
    
    Either way, I think it's probably bad to use DenseVector *and* JBlas both.
    
    Agree with trying to shield the user and even implementation details from the choice as much as possible. It's hard to keep abstractions up and get good performance in some cases. 
    
    Take that strong sentiment for what it's worth, having worked with Mahout, JBlas, Commons Math extensively.


"
debasish83 <git@git.apache.org>,"Mon, 10 Feb 2014 23:59:53 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user debasish83 commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-34710100
  
    @mengxr as long as the interface is clean and we can bring in netlib-java, start with mahout-math does not seem like a bad idea...netlib-java uses jni while it seems using jna/bridj might be even faster....it is an evolving area...any idea why netlib-java is showing so much worse runtime compared to mahout-math ? does not seem right if they are using the underlying fortran blas libraries ! is it a bug ? are they aware of it ?


"
mengxr <git@git.apache.org>,"Tue, 11 Feb 2014 00:12:24 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user mengxr commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-34712992
  
    @debasish83 Are you speaking of the benchmark I posted to the JIRA? BLAS/LAPACK cannot be used for dense vector + sparse vector. Those are designed for dense-only operations. So netlib-java is not involved in the benchmark. The performance issue I showed in the benchmark was fixed yesterday but we need to wait for the next release in order to switch to it.


"
mengxr <git@git.apache.org>,"Tue, 11 Feb 2014 00:22:21 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user mengxr commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-34714242
  
    @srowen Thanks for the information! I believe native BLAS/LAPACK libraries performs much better than Java implementation for level 2 and level 3 operations, but for level 1 operations, the overhead of calling native routines would slow down native BLAS/LAPACK. See 
    
    https://code.google.com/p/java-matrix-benchmark/wiki/RuntimeCorei7v2600_2013_10
    
    for a recent benchmark including native netlib-java and jblas. 


"
Evan Chan <ev@ooyala.com>,"Mon, 10 Feb 2014 16:31:55 -0800",Re: proposal: replace lift-json with spray-json,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","By the way, I did a benchmark on JSON parsing performance recently.
 Based on that, spray-json was about 10x slower than the Jackson-based
parsers.  I recommend json4s-jackson, because jackson is almost
certainly already a dependency of Sparks (many other Java libraries
use it), so the dependencies are very lightweight.   I didn't
benchmark Argonaut or play-json, partly because play-json pulled in
all the Play dependencies, tho as someone else commented in this
thread, they plan to split it out.




-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

"
srowen <git@git.apache.org>,"Tue, 11 Feb 2014 00:37:18 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user srowen commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-34715135
  
    Wow nice writeup. (Is Breeze benchmarked too somewhere? don't see it there). Totally agree. That's why I would use JBlas at least for the complex operations. Although to keep it simple, using it for all dense operations isn't bad. And, the code already does that. (matei said it won't use native code for simple operations anyway?)
    
    I'm interested here in the sparse operations question, which can't be JBlas. Should the other library used just be used for sparse operations, or also for most dense operations except the ones where JBlas is better? Either makes sense to me. Previously I've gone with Commons Math, with a hint of JBlas optionally for expensive operations. 
    
    So here I suppose all I am suggesting is: need to be careful about mixing use of JBlas and another dense library -- should probably be all JBlas, or 90% the other library that's also being used for sparse. And then also voting for CM for sparse for at least the short term for other reasons.


"
berngp <git@git.apache.org>,"Tue, 11 Feb 2014 00:39:26 +0000 (UTC)",[GitHub] incubator-spark pull request: Hadoop jar name,dev@spark.incubator.apache.org,"Github user berngp commented on the pull request:

    https://github.com/apache/incubator-spark/pull/522#issuecomment-34715269
  
    Is there anything we can do to facilitate the merge into master. I am also looking forward for this change.


"
Evan Chan <ev@ooyala.com>,"Mon, 10 Feb 2014 16:40:50 -0800",Re: Proposal: Clarifying minor points of Scala style,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","+1 to the proposal.




-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

"
velvia <git@git.apache.org>,"Tue, 11 Feb 2014 00:43:14 +0000 (UTC)",[GitHub] incubator-spark pull request: Added parquetFileAsJSON to read Parq...,dev@spark.incubator.apache.org,"Github user velvia commented on the pull request:

    https://github.com/apache/incubator-spark/pull/576#issuecomment-34715532
  
    My concern with this is that Parquet is typically used for high performance OLAP queries, and changing it to JSON makes it much slower.  Out of curiosity, I have used Parquet with Thrift, and I know it supports Avro, is JSON a separate path?
    
    I'm probably going to publish a blog post on Parquet, Thrift, and Spark soon.


"
bijaybisht <git@git.apache.org>,"Tue, 11 Feb 2014 00:47:17 +0000 (UTC)",[GitHub] incubator-spark pull request: Hadoop jar name,dev@spark.incubator.apache.org,"Github user bijaybisht closed the pull request at:

    https://github.com/apache/incubator-spark/pull/522


"
prb <git@git.apache.org>,"Tue, 11 Feb 2014 00:50:44 +0000 (UTC)",[GitHub] incubator-spark pull request: Default log4j initialization causes ...,dev@spark.incubator.apache.org,"Github user prb commented on the pull request:

    https://github.com/apache/incubator-spark/pull/573#issuecomment-34715973
  
    The infinite loop was caused by the fact that the list of appenders is *always* empty when the slf4j mock implementation of log4j is in place.


"
bijaybisht <git@git.apache.org>,"Tue, 11 Feb 2014 00:59:50 +0000 (UTC)",[GitHub] incubator-spark pull request: Hadoop jar name,dev@spark.incubator.apache.org,"GitHub user bijaybisht reopened a pull request:

    https://github.com/apache/incubator-spark/pull/522

    Hadoop jar name

    This pull request is a copy of 
    #121 - Fix for hadoop client jar name, which got changed from 1.*. The other one was from master, which is wrong way of generating the pull requests. 

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark hadoop_jar_name

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/522.patch

----
commit 31c4a5aff1a8b62a1bb66146e0b74e36ede54024
Author: Bijay Bisht <bijay.bisht@gmai.com>
Date:   2013-10-28T16:14:56Z

    Fix for hadoop client jar name, which got changed from 1.0.1

----


"
AmplabJenkins <git@git.apache.org>,"Tue, 11 Feb 2014 01:03:08 +0000 (UTC)",[GitHub] incubator-spark pull request: Hadoop jar name,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/522#issuecomment-34716681
  
    Can one of the admins verify this patch?


"
AmplabJenkins <git@git.apache.org>,"Tue, 11 Feb 2014 01:18:07 +0000 (UTC)",[GitHub] incubator-spark pull request: Fix typos in Spark Streaming program...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/536#issuecomment-34717534
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Tue, 11 Feb 2014 01:18:07 +0000 (UTC)",[GitHub] incubator-spark pull request: Fix typos in Spark Streaming program...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/536#issuecomment-34717536
  
    Merged build started.


"
laserson <git@git.apache.org>,"Tue, 11 Feb 2014 01:33:49 +0000 (UTC)",[GitHub] incubator-spark pull request: Added parquetFileAsJSON to read Parq...,dev@spark.incubator.apache.org,"Github user laserson commented on the pull request:

    https://github.com/apache/incubator-spark/pull/576#issuecomment-34718389
  
    No, this actually constructs Avro `GenericRecord` objects in memory.  The problem is that if you want access to the Parquet data through PySpark, there is no obvious/general way to convert from the Java in-memory representation (which can be Thrift or Avro) to some Python-friendly object.  In principle, you could serialize as Thrift or Avro and have the Python workers read this byte stream.  However, since PySpark currently serializes it data through text, you might as well use a text representation of the Thrift/Avro records, which is JSON.
    
    You're right that this function is not to be used for fast OLAP-style processing, but rather to give PySpark users an easy way to access Parquet data.


"
AmplabJenkins <git@git.apache.org>,"Tue, 11 Feb 2014 01:46:14 +0000 (UTC)",[GitHub] incubator-spark pull request: Fix typos in Spark Streaming program...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/536#issuecomment-34719030
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Tue, 11 Feb 2014 01:46:14 +0000 (UTC)",[GitHub] incubator-spark pull request: Fix typos in Spark Streaming program...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/536#issuecomment-34719031
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12673/


"
Will Benton <willb@redhat.com>,"Mon, 10 Feb 2014 21:07:59 -0500 (EST)",Re: proposal: replace lift-json with spray-json,dev@spark.incubator.apache.org,"Evan, yes!  Luis linked your blog post earlier and it was really helpful.  The other advantage of json4s-jackson is that the interface is mostly compatible with lift-json.  I made the (few and trivial) changes necessary to get everything in Spark switched over earlier today and will write it up in a PR for further discussion later tonight or tomorrow.


best,
wb

	by minotaur.apache.org (Postfix) with SMTP id 0842810384
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 11 Feb 2014 02:19:29 +0000 (UTC)
Received: (qmail 45798 invoked by uid 500); 11 Feb 2014 02:19:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 45753 invoked by uid 500); 11 Feb 2014 02:19:27 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 45745 invoked by uid 99); 11 Feb 2014 02:19:27 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 11 Feb 2014 02:19:27 +0000
X-ASF-Spam-Status: No, hits=-2000.6 required=5.0
	tests=ALL_TRUSTED,RP_MATCHES_RCVD
X-Spam-Check-By: apache.org
Received: from [140.211.11.3] (HELO mail.apache.org) (140.211.11.3)
    by apache.org (qpsmtpd/0.29) with SMTP; Tue, 11 Feb 2014 02:19:26 +0000
Received: (qmail 44059 invoked by uid 99); 11 Feb 2014 02:19:05 -0000
Received: from tyr.zones.apache.org (HELO tyr.zones.apache.org) (140.211.11.114)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 11 Feb 2014 02:19:05 +0000
Received: by tyr.zones.apache.org (Postfix, from userid 65534)
	id 477A4923298; Tue, 11 Feb 2014 02:19:04 +0000 (UTC)
From: schmit <git@git.apache.org>
To: dev@spark.incubator.apache.org
Reply-To: dev@spark.incubator.apache.org
References: <git-pr-550-incubator-spark@git.apache.org>
In-Reply-To: <git-pr-550-incubator-spark@git.apache.org>
Subject: [GitHub] incubator-spark pull request: ROC AUC and Average precision metric...
Content-Type: text/plain
Message-Id: <20140211021904.477A4923298@tyr.zones.apache.org>
Date: Tue, 11 Feb 2014 02:19:04 +0000 (UTC)
X-Virus-Checked: Checked by ClamAV on apache.org

Github user schmit commented on the pull request:

    https://github.com/apache/incubator-spark/pull/550#issuecomment-34720671
  
    https://spark-project.atlassian.net/browse/MLLIB-23


"
haoyuan <git@git.apache.org>,"Tue, 11 Feb 2014 03:02:08 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user haoyuan commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-34722448
  
    Jenkins, test this please.


"
AmplabJenkins <git@git.apache.org>,"Tue, 11 Feb 2014 03:03:16 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-34722512
  
     Build triggered.


"
AmplabJenkins <git@git.apache.org>,"Tue, 11 Feb 2014 03:03:16 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-34722513
  
    Build started.


"
AmplabJenkins <git@git.apache.org>,"Tue, 11 Feb 2014 03:04:02 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-34722555
  
    Build finished.


"
AmplabJenkins <git@git.apache.org>,"Tue, 11 Feb 2014 03:04:02 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-34722556
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12674/


"
CodingCat <git@git.apache.org>,"Tue, 11 Feb 2014 03:04:28 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-979] a LRU scheduler for load...,dev@spark.incubator.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/incubator-spark/pull/548#issuecomment-34722573
  
    added a test case


"
Chris Mattmann <mattmann@apache.org>,"Mon, 10 Feb 2014 20:27:39 -0800",[VOTE] Graduation of Apache Spark from the Incubator,"""general@incubator.apache.org"" <general@incubator.apache.org>","Hi Everyone,

This is a new VOTE to decide if Apache Spark should graduate
from the Incubator. Please VOTE on the resolution pasted below
the ballot. I'll leave this VOTE open for at least 72 hours.

Thanks!

[ ] +1 Graduate Apache Spark from the Incubator.
[ ] +0 Don't care.
[ ] -1 Don't graduate Apache Spark from the Incubator because..

Here is my +1 binding for graduation.

Cheers,
Chris

---- snip

WHEREAS, the Board of Directors deems it to be in the best
interests of the Foundation and consistent with the
Foundation's purpose to establish a Project Management
Committee charged with the creation and maintenance of
open-source software, for distribution at no charge to the
public, related to fast and flexible large-scale data analysis
on clusters.

NOW, THEREFORE, BE IT RESOLVED, that a Project Management
Committee (PMC), to be known as the ""Apache Spark Project"", be
and hereby is established pursuant to Bylaws of the Foundation;
and be it further

RESOLVED, that the Apache Spark Project be and hereby is
responsible for the creation and maintenance of software
related to fast and flexible large-scale data analysis
on clusters; and be it further RESOLVED, that the office
of ""Vice President, Apache Spark"" be and hereby is created,
the person holding such office to serve at the direction of
the Board of Directors as the chair of the Apache Spark
Project, and to have primary responsibility for management
of the projects within the scope of responsibility
of the Apache Spark Project; and be it further
RESOLVED, that the persons listed immediately below be and
hereby are appointed to serve as the initial members of the
Apache Spark Project:

* Mosharaf Chowdhury <mosharaf@apache.org>
* Jason Dai <jasondai@apache.org>
* Tathagata Das <tdas@apache.org>
* Ankur Dave <ankurdave@apache.org>
* Aaron Davidson <adav@apache.org>
* Thomas Dudziak <tomdz@apache.org>
* Robert Evans <bobby@apache.org>
* Thomas Graves <tgraves@apache.org>
* Andy Konwinski <andrew@apache.org>
* Stephen Haberman <stephenh@apache.org>
* Mark Hamstra <markhamstra@apache.org>
* Shane Huang <shane_huang@apache.org>
* Ryan LeCompte <ryanlecompte@apache.org>
* Haoyuan Li <haoyuan@apache.org>
* Sean McNamara <mcnamara@apache.org>
* Mridul Muralidharam <mridulm80@apache.org>
* Kay Ousterhout <kayousterhout@apache.org>
* Nick Pentreath <mlnick@apache.org>
* Imran Rashid <irashid@apache.org>
* Charles Reiss <woggle@apache.org>
* Josh Rosen <joshrosen@apache.org>
* Prashant Sharma <prashant@apache.org>
* Ram Sriharsha <harsha@apache.org>
* Shivaram Venkataraman <shivaram@apache.org>
* Patrick Wendell <pwendell@apache.org>
* Andrew Xia <xiajunluan@apache.org>
* Reynold Xin <rxin@apache.org>
* Matei Zaharia <matei@apache.org>

NOW, THEREFORE, BE IT FURTHER RESOLVED, that Matei Zaharia be
appointed to the office of Vice President, Apache Spark, to
serve in accordance with and subject to the direction of the
Board of Directors and the Bylaws of the Foundation until
death, resignation, retirement, removal or disqualification, or
until a successor is appointed; and be it further

RESOLVED, that the Apache Spark Project be and hereby is
tasked with the migration and rationalization of the Apache
Incubator Spark podling; and be it further

RESOLVED, that all responsibilities pertaining to the Apache
Incubator Spark podling encumbered upon the Apache Incubator
Project are hereafter discharged.

----




"
Andrew Or <andrewor14@gmail.com>,"Mon, 10 Feb 2014 20:38:45 -0800",Re: [VOTE] Graduation of Apache Spark from the Incubator,dev@spark.incubator.apache.org,"+1!


2014-02-10 20:27 GMT-08:00 Chris Mattmann <mattmann@apache.org>:

"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 10 Feb 2014 20:45:31 -0800",Re: [VOTE] Graduation of Apache Spark from the Incubator,dev@spark.incubator.apache.org,1
Mosharaf Chowdhury <mosharafkabir@gmail.com>,"Mon, 10 Feb 2014 20:50:56 -0800",Re: [VOTE] Graduation of Apache Spark from the Incubator,dev@spark.incubator.apache.org,"+1

--
Mosharaf Chowdhury
http://www.mosharaf.com/



"
prabeesh k <prabsmails@gmail.com>,"Tue, 11 Feb 2014 10:23:06 +0530",Re: [VOTE] Graduation of Apache Spark from the Incubator,dev@spark.incubator.apache.org,1
Henry Saputra <henry.saputra@gmail.com>,"Mon, 10 Feb 2014 20:56:00 -0800",Re: [VOTE] Graduation of Apache Spark from the Incubator,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","+1 (binding)


- Henry


"
Reynold Xin <rxin@databricks.com>,"Mon, 10 Feb 2014 20:58:39 -0800",Re: [VOTE] Graduation of Apache Spark from the Incubator,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","+1 (binding)



"
Aaron Davidson <ilikerps@gmail.com>,"Mon, 10 Feb 2014 21:00:18 -0800",Re: Proposal: Clarifying minor points of Scala style,dev@spark.incubator.apache.org,"Alright, makes sense -- consistency is more important than special casing
for possible readability benefits. That is one of the main points behind a
style guide after all. I switch my vote for (1) to Shivaram's proposal as
well.



"
Aaron Davidson <ilikerps@gmail.com>,"Mon, 10 Feb 2014 21:00:45 -0800",Re: [VOTE] Graduation of Apache Spark from the Incubator,dev@spark.incubator.apache.org,1
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Mon, 10 Feb 2014 21:20:23 -0800",Re: [VOTE] Graduation of Apache Spark from the Incubator,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>",1
Evan Sparks <evan.sparks@gmail.com>,"Mon, 10 Feb 2014 21:28:54 -0800",Re: [VOTE] Graduation of Apache Spark from the Incubator,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","+1

rote:
ote:
e:


"
Ameet Talwalkar <ameet@eecs.berkeley.edu>,"Mon, 10 Feb 2014 21:30:02 -0800",Re: [VOTE] Graduation of Apache Spark from the Incubator,dev@spark.incubator.apache.org,1
Azuryy Yu <azuryyyu@gmail.com>,"Tue, 11 Feb 2014 13:34:34 +0800",Re: [VOTE] Graduation of Apache Spark from the Incubator,dev@spark.incubator.apache.org,1
Kay Ousterhout <keo@eecs.berkeley.edu>,"Mon, 10 Feb 2014 21:36:17 -0800",Re: [VOTE] Graduation of Apache Spark from the Incubator,dev@spark.incubator.apache.org,1
Bharath Mundlapudi <mundlapudi@gmail.com>,"Mon, 10 Feb 2014 21:38:48 -0800",Re: [VOTE] Graduation of Apache Spark from the Incubator,dev@spark.incubator.apache.org,1
Nathan Kronenfeld <nkronenfeld@oculusinfo.com>,"Tue, 11 Feb 2014 00:44:02 -0500",Re: [VOTE] Graduation of Apache Spark from the Incubator,dev@spark.incubator.apache.org,"Who is allowed to vote on stuff like this?





-- 
Nathan Kronenfeld
Senior Visualization Developer
Oculus Info Inc
2 Berkeley Street, Suite 600,
Phone:  +1-416-203-3003 x 238
Email:  nkronenfeld@oculusinfo.com
"
Mark Hamstra <mark@clearstorydata.com>,"Mon, 10 Feb 2014 21:57:04 -0800",Re: [VOTE] Graduation of Apache Spark from the Incubator,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>",1
Sandy Ryza <sandy.ryza@cloudera.com>,"Mon, 10 Feb 2014 22:02:12 -0800",Re: [VOTE] Graduation of Apache Spark from the Incubator,dev@spark.incubator.apache.org,1
AmplabJenkins <git@git.apache.org>,"Tue, 11 Feb 2014 06:03:07 +0000 (UTC)",[GitHub] incubator-spark pull request: [WIP] [javaAPI] SPARK-964 Investigat...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-34728598
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Tue, 11 Feb 2014 06:03:07 +0000 (UTC)",[GitHub] incubator-spark pull request: [WIP] [javaAPI] SPARK-964 Investigat...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-34728599
  
    Merged build started.


"
"""Mattmann, Chris A (3980)"" <chris.a.mattmann@jpl.nasa.gov>","Tue, 11 Feb 2014 06:20:14 +0000",Re: [VOTE] Graduation of Apache Spark from the Incubator,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi Nathan, anybody is welcome to to VOTE. Thank you.
I welcome and will tally all VOTEs provided.

Cheers,
Chris






"
Andy Konwinski <andykonwinski@gmail.com>,"Mon, 10 Feb 2014 22:20:29 -0800",Re: [VOTE] Graduation of Apache Spark from the Incubator,dev@spark.incubator.apache.org,1
Patrick Wendell <pwendell@gmail.com>,"Mon, 10 Feb 2014 22:21:06 -0800",Re: [VOTE] Graduation of Apache Spark from the Incubator,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>, general@incubator.apache.org","+1

To clarify to others, this is an IPCM vote so only the IPCM votes are binding :)


"
Reynold Xin <rxin@databricks.com>,"Mon, 10 Feb 2014 22:21:36 -0800",Re: [VOTE] Graduation of Apache Spark from the Incubator,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Actually I made a mistake by saying binding.

Just +1 here.



"
CrazyJvm <git@git.apache.org>,"Tue, 11 Feb 2014 06:27:38 +0000 (UTC)","[GitHub] incubator-spark pull request: ""in the source DStream"" rather than ...",dev@spark.incubator.apache.org,"GitHub user CrazyJvm opened a pull request:

    https://github.com/apache/incubator-spark/pull/579

    ""in the source DStream"" rather than ""int the source DStream""

    ""flatMap is a one-to-many DStream operation that creates a new DStream by generating multiple new records from each record int the source DStream."" 

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark patch-1

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/579.patch

----
commit 4abcae35da38e747ae1ea7beea9d4d7a32a7e4ee
Author: Chen Chao <crazyjvm@gmail.com>
Date:   2014-02-11T06:25:42Z

    in the source DStream

----


"
AmplabJenkins <git@git.apache.org>,"Tue, 11 Feb 2014 06:27:58 +0000 (UTC)","[GitHub] incubator-spark pull request: ""in the source DStream"" rather than ...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/579#issuecomment-34729420
  
    Can one of the admins verify this patch?


"
rxin <git@git.apache.org>,"Tue, 11 Feb 2014 06:28:38 +0000 (UTC)","[GitHub] incubator-spark pull request: ""in the source DStream"" rather than ...",dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/579#issuecomment-34729446
  
    Thanks. Merged.


"
AmplabJenkins <git@git.apache.org>,"Tue, 11 Feb 2014 06:30:58 +0000 (UTC)",[GitHub] incubator-spark pull request: [javaAPI] SPARK-964 Investigate the ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-34729544
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Tue, 11 Feb 2014 06:30:58 +0000 (UTC)",[GitHub] incubator-spark pull request: [javaAPI] SPARK-964 Investigate the ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-34729545
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12675/


"
CrazyJvm <git@git.apache.org>,"Tue, 11 Feb 2014 07:01:25 +0000 (UTC)","[GitHub] incubator-spark pull request: ""in the source DStream"" rather than ...",dev@spark.incubator.apache.org,"Github user CrazyJvm closed the pull request at:

    https://github.com/apache/incubator-spark/pull/579


"
AmplabJenkins <git@git.apache.org>,"Tue, 11 Feb 2014 07:08:06 +0000 (UTC)",[GitHub] incubator-spark pull request: [javaAPI] SPARK-964 Investigate the ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-34730943
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Tue, 11 Feb 2014 07:08:06 +0000 (UTC)",[GitHub] incubator-spark pull request: [javaAPI] SPARK-964 Investigate the ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-34730944
  
    Merged build started.


"
Zongheng Yang <zongheng.y@gmail.com>,"Mon, 10 Feb 2014 23:12:13 -0800",Re: [VOTE] Graduation of Apache Spark from the Incubator,dev@spark.incubator.apache.org,1
Matt Massie <massie@berkeley.edu>,"Mon, 10 Feb 2014 23:17:06 -0800",Re: [VOTE] Graduation of Apache Spark from the Incubator,dev@spark.incubator.apache.org,"+1

--
Matt Massie
UC, Berkeley AMPLab
Twitter: @matt_massie <https://twitter.com/matt_massie>,
@amplab<https://twitter.com/amplab>
https://amplab.cs.berkeley.edu/



"
semihsalihoglu <git@git.apache.org>,"Tue, 11 Feb 2014 07:27:33 +0000 (UTC)",[GitHub] incubator-spark pull request: Graph primitives2,dev@spark.incubator.apache.org,"GitHub user semihsalihoglu opened a pull request:

    https://github.com/apache/incubator-spark/pull/580

    Graph primitives2

    Hi guys,
    
    I'm following Joey and Ankur's suggestions to add collectEdges and pickRandomVertex. I'm also adding the tests for collectEdges and refactoring one method getCycleGraph in GraphOpsSuite.scala.
    
    Thank you,
    
    semih 

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark GraphPrimitives2

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/580.patch

----
commit 41265a66afed2d412d2e26b1e4eccb6bf5048f94
Author: Semih Salihoglu <semihsalihoglu@gmail.com>
Date:   2014-02-11T07:13:29Z

    - Adding collectEdges and pickRandomVertex.
    - Adding tests for collectEdges.
    - Recycling a getCycle utility test file.

commit a69a1523d3983f71c8224ef4b57320a180585e15
Author: Semih Salihoglu <semihsalihoglu@gmail.com>
Date:   2014-02-11T07:25:07Z

    - Adding collectEdges and pickRandomVertices.
    - Adding tests for collectEdges.
    - Refactoring a getCycle utility function for GraphOpsSuite.scala.

----


"
AmplabJenkins <git@git.apache.org>,"Tue, 11 Feb 2014 07:27:58 +0000 (UTC)",[GitHub] incubator-spark pull request: Graph primitives2,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/580#issuecomment-34731728
  
    Can one of the admins verify this patch?


"
AmplabJenkins <git@git.apache.org>,"Tue, 11 Feb 2014 07:36:29 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-34732076
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Tue, 11 Feb 2014 07:36:29 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-34732078
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12676/


"
Pascal Voitot Dev <pascal.voitot.dev@gmail.com>,"Tue, 11 Feb 2014 08:49:45 +0100",Re: proposal: replace lift-json with spray-json,dev <dev@spark.incubator.apache.org>,"Evan,

Excuse me but that's WRONG that play-json pulls all play deps!

PLAY/JSON has NO HEAVY DEP ON PLAY!

I personally worked to make it an independent module in play!
So play/json has just one big dep which is Jackson!

I agree that jackson is the right way to go as a beginning.
But for scala developers, a higher thin layer like play/json is useful to
bring typesafety...

Pascal


"
Nick Pentreath <nick.pentreath@gmail.com>,"Tue, 11 Feb 2014 10:02:57 +0200",Re: [VOTE] Graduation of Apache Spark from the Incubator,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>",1
Sebastian Schelter <ssc@apache.org>,"Tue, 11 Feb 2014 09:20:24 +0100",Re: [VOTE] Graduation of Apache Spark from the Incubator,dev@spark.incubator.apache.org,"+1 (binding)



"
NirmalReddy <git@git.apache.org>,"Tue, 11 Feb 2014 09:06:04 +0000 (UTC)",[GitHub] incubator-spark pull request: Java-api completeness,dev@spark.incubator.apache.org,"Github user NirmalReddy commented on the pull request:

    https://github.com/apache/incubator-spark/pull/475#issuecomment-34736734
  
    @tdas Can you please verify this patch. Thanks !!


"
jyotiska <git@git.apache.org>,"Tue, 11 Feb 2014 14:19:22 +0000 (UTC)",[GitHub] incubator-spark pull request: Added extra description on ValueErro...,dev@spark.incubator.apache.org,"GitHub user jyotiska opened a pull request:

    https://github.com/apache/incubator-spark/pull/581

    Added extra description on ValueError when one Spark context already exists

    I added extra description on the ValueError message, when more than one Spark context already exists. Added the current master name and the app name for easier understanding.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark master

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/581.patch

----
commit 945e39a5d68daa7e5bab0d96cbd35d7c4b04eafb
Author: jyotiska <jyotiska123@gmail.com>
Date:   2014-02-08T07:29:09Z

    Added example python code for sort

commit 6f98f1e313f4472a7c2207d36c4f0fbcebc95a8c
Author: jyotiska <jyotiska123@gmail.com>
Date:   2014-02-08T07:42:37Z

    Updated python example code sort.py

commit 8ad8faf6c8e02ae1cd68565d98524edf165f54df
Author: jyotiska <jyotiska123@gmail.com>
Date:   2014-02-09T05:30:41Z

    Added comments in code on collect() method

commit 92e23fea707ed6de551dc8d5ffa9b4f987683628
Author: jyotiska <jyotiska123@gmail.com>
Date:   2014-02-11T13:40:15Z

    Added extra description on ValueError when one Spark context already running

commit d90bea59f15738a00e03c9761ea27157ad2ef04d
Author: jyotiska <jyotiska123@gmail.com>
Date:   2014-02-11T13:55:12Z

    Merge remote-tracking branch 'upstream/master'

----


"
AmplabJenkins <git@git.apache.org>,"Tue, 11 Feb 2014 14:22:59 +0000 (UTC)",[GitHub] incubator-spark pull request: Added extra description on ValueErro...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/581#issuecomment-34758328
  
    Can one of the admins verify this patch?


"
jyotiska <git@git.apache.org>,"Tue, 11 Feb 2014 14:35:08 +0000 (UTC)",[GitHub] incubator-spark pull request: Added extra description on ValueErro...,dev@spark.incubator.apache.org,"Github user jyotiska closed the pull request at:

    https://github.com/apache/incubator-spark/pull/581


"
jyotiska <git@git.apache.org>,"Tue, 11 Feb 2014 14:35:12 +0000 (UTC)",[GitHub] incubator-spark pull request: Added extra description on ValueErro...,dev@spark.incubator.apache.org,"GitHub user jyotiska reopened a pull request:

    https://github.com/apache/incubator-spark/pull/581

    Added extra description on ValueError when one Spark context already exists

    I added extra description on the ValueError message, when more than one Spark context already exists. Added the current master name and the app name for easier understanding.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark master

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/581.patch

----
commit 945e39a5d68daa7e5bab0d96cbd35d7c4b04eafb
Author: jyotiska <jyotiska123@gmail.com>
Date:   2014-02-08T07:29:09Z

    Added example python code for sort

commit 6f98f1e313f4472a7c2207d36c4f0fbcebc95a8c
Author: jyotiska <jyotiska123@gmail.com>
Date:   2014-02-08T07:42:37Z

    Updated python example code sort.py

commit 8ad8faf6c8e02ae1cd68565d98524edf165f54df
Author: jyotiska <jyotiska123@gmail.com>
Date:   2014-02-09T05:30:41Z

    Added comments in code on collect() method

commit 92e23fea707ed6de551dc8d5ffa9b4f987683628
Author: jyotiska <jyotiska123@gmail.com>
Date:   2014-02-11T13:40:15Z

    Added extra description on ValueError when one Spark context already running

commit d90bea59f15738a00e03c9761ea27157ad2ef04d
Author: jyotiska <jyotiska123@gmail.com>
Date:   2014-02-11T13:55:12Z

    Merge remote-tracking branch 'upstream/master'

----


"
Mridul Muralidharan <mridul@gmail.com>,"Tue, 11 Feb 2014 20:25:13 +0530",Re: [VOTE] Graduation of Apache Spark from the Incubator,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","+1 !

- Mridul


"
Haoyuan Li <haoyuan.li@gmail.com>,"Tue, 11 Feb 2014 07:00:01 -0800",Re: [VOTE] Graduation of Apache Spark from the Incubator,dev@spark.incubator.apache.org,"+1





-- 
Haoyuan Li
Algorithms, Machines, People Lab, EECS, UC Berkeley
http://www.cs.berkeley.edu/~haoyuan/
"
tgravescs <git@git.apache.org>,"Tue, 11 Feb 2014 15:17:32 +0000 (UTC)","[GitHub] incubator-spark pull request: Add Security to Spark - Akka, Http, ...",dev@spark.incubator.apache.org,"Github user tgravescs commented on the pull request:

    https://github.com/apache/incubator-spark/pull/332#issuecomment-34763698
  
    Sorry for my delay, I was on vacation last week. Thanks for the reviews, I'll go through them and respond.  
    
    It sounds like you want 2 types of documents, more user docs which I'll look at adding more to the .md files and then a dev design doc. where do you want that to live? or were you just thinking more comments in the code?
    
    The SaslClient and SaslServer classes are java as the original was borrowed from Hadoop and then modified. We can convert to scala. 
    
    Also note that I concentrated on spark on yarn for this.  It still works for standalone deploy but probably isn't ideal as far as how the shared secret is actually created/configured.



"
"""=?gb18030?B?0ac=?="" <witgo@qq.com>","Tue, 11 Feb 2014 23:47:40 +0800",Re:[VOTE] Graduation of Apache Spark from the Incubator,"""=?gb18030?B?ZGV2?="" <dev@spark.incubator.apache.org>",1
Giri Iyengar <giri.iyengar@velos.io>,"Tue, 11 Feb 2014 10:50:03 -0500",Re: [VOTE] Graduation of Apache Spark from the Incubator,dev@spark.incubator.apache.org,"+1

-- 
GIRI IYENGAR, CTO
VELOS.IO
Simple. Powerful. Predictions.

440 NINTH AVE, 11TH FLOOR NEW YORK CITY, NY 10001
O: 917.525.2466x104   M: 914.924.7935
E: *giri.iyengar@v <giri.iyengar@sociocast.com>elos.io
<http://elos.io>* W: *www.velos.
<http://www."
Xuefeng Wu <benewu@gmail.com>,"Wed, 12 Feb 2014 00:02:13 +0800",Re: [VOTE] Graduation of Apache Spark from the Incubator,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","+1


Yours, Xuefeng Wu ÎâÑ©·å ¾´ÉÏ


"
Andrew Psaltis <psaltis.andrew@gmail.com>,"Tue, 11 Feb 2014 09:02:53 -0700",Re: [VOTE] Graduation of Apache Spark from the Incubator,dev@spark.incubator.apache.org,1
tgravescs <git@git.apache.org>,"Tue, 11 Feb 2014 16:11:50 +0000 (UTC)","[GitHub] incubator-spark pull request: Add Security to Spark - Akka, Http, ...",dev@spark.incubator.apache.org,"Github user tgravescs commented on the pull request:

    https://github.com/apache/incubator-spark/pull/332#issuecomment-34769796
  
    A small explanation here.
    
    This pull request is just for authentication via a shared secret.  It does not handle encryption or qop after authentication. The idea was to add the basics and then enhance as needed.  Note that for the http/jetty and sasl we are using DIGEST authentication mechanism so the password is not sent in plain text.  Akka remoting only gives you one option with the shared cookie and the documentation didn't specify so I'm not sure about that. I'll try to take a look at the code.
    
    Also note for the sasl stuff you can set the level of protecting via the Sasl.QOP setting.  We are simply using auth, but you could support auth-int and auth-conf. Those would require you to also wrap the messages being sent.    Akka also supports ssl and you can also configure jetty to use https.  I haven't looked at any of those in detail at this point though.


"
Sean McNamara <Sean.McNamara@Webtrends.com>,"Tue, 11 Feb 2014 16:31:49 +0000",Re: [VOTE] Graduation of Apache Spark from the Incubator,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>,
	""general@incubator.apache.org"" <general@incubator.apache.org>","+1 !!!




"
Tom Graves <tgraves_cs@yahoo.com>,"Tue, 11 Feb 2014 08:32:54 -0800 (PST)",Re: [VOTE] Graduation of Apache Spark from the Incubator,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","
> --
> GIRI IYENGAR, CTO
> VELOS.IO
> Simple. Powerful. Predictions.
>
> 440 NINTH AVE, 11TH FLOOR NEW YORK CITY, NY 10001
> O: 917.525.2466x104   M: 914.924.7935
> E: *giri.iyengar@v <giri.iyengar@sociocast.com>elos.io
> <http://elos.io>* W: *www.velos.
> <http://www.sociocast.com/>io*
>"
Tom Graves <tgraves_cs@yahoo.com>,"Tue, 11 Feb 2014 08:40:30 -0800 (PST)",Re: Proposal for JIRA and Pull Request Policy,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","+1

This is a bit aside, but are we also getting jenkins machine in the apache domain?

Tom



 
+1


"
Jyotiska NK <jyotiska123@gmail.com>,"Tue, 11 Feb 2014 08:59:10 -0800 (PST)",Re: [VOTE] Graduation of Apache Spark from the Incubator,dev@spark.incubator.apache.org,"+1 



--

"
Craig L Russell <craig.russell@oracle.com>,"Tue, 11 Feb 2014 09:33:34 -0800",Re: [VOTE] Graduation of Apache Spark from the Incubator,general@incubator.apache.org,"+1

Craig



Craig L Russell
Architect, Oracle
http://db.apache.org/jdo
408 276-5638 mailto:Craig.Russell@oracle.com
P.S. A good JDO? O, Gasp!


"
Suresh Marru <smarru@apache.org>,"Tue, 11 Feb 2014 12:43:22 -0500",Re: [VOTE] Graduation of Apache Spark from the Incubator,dev@spark.incubator.apache.org,"+ 1 (binding).

Suresh




"
jyotiska <git@git.apache.org>,"Tue, 11 Feb 2014 17:45:09 +0000 (UTC)",[GitHub] incubator-spark pull request: Added extra description on ValueErro...,dev@spark.incubator.apache.org,"Github user jyotiska commented on the pull request:

    https://github.com/apache/incubator-spark/pull/581#issuecomment-34782466
  
    This tries to solve [SPARK-972](https://spark-project.atlassian.net/browse/SPARK-972)


"
Marvin Humphrey <marvin@rectangular.com>,"Tue, 11 Feb 2014 09:45:38 -0800",Re: [VOTE] Graduation of Apache Spark from the Incubator,"""general@incubator.apache.org"" <general@incubator.apache.org>","

+1 (binding)

Marvin Humphrey

"
Mark Hamstra <mark@clearstorydata.com>,"Tue, 11 Feb 2014 09:58:09 -0800",Re: Proposal for JIRA and Pull Request Policy,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>, Tom Graves <tgraves_cs@yahoo.com>","Whether that is a good idea or not depends largely, I think, on who will
have control of that putative Apache Jenkins.  The
AMPLab-now-mostly-Databricks guys (i.e. Andy and others) who did the work
of setting up, configuring and maintaining the current Jenkins have done a
great job and have been very responsive when Jenkins has needed attention.
 I would require convincing that the Apache infra team could match that
performance.



"
=?GB2312?B?t+u/obfl?= <junfeng.feng@gmail.com>,"Tue, 11 Feb 2014 13:00:33 -0500",Re: [VOTE] Graduation of Apache Spark from the Incubator,dev@spark.incubator.apache.org,1
Nat Padmanabhan <reachnatp@gmail.com>,"Tue, 11 Feb 2014 10:15:46 -0800",Re: [VOTE] Graduation of Apache Spark from the Incubator,dev@spark.incubator.apache.org,1
Henry Saputra <henry.saputra@gmail.com>,"Tue, 11 Feb 2014 10:44:48 -0800",Proposal for JIRA and Pull Request Policy,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","The ASF infra support all Hadoop and it's community projects CI requests,
as well as other ASF projects.

great.

And also ASF infra is accepting help if you guys think you could spare time
and knowledge.

Henry

<mark@clearstorydata.com<javascript:_e(%7B%7D,'cvml','mark@clearstorydata.com');>>

"
Henry Saputra <henry.saputra@gmail.com>,"Tue, 11 Feb 2014 10:46:36 -0800",Re: Proposal for JIRA and Pull Request Policy,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","+1 to the proposal


"
AmplabJenkins <git@git.apache.org>,"Tue, 11 Feb 2014 19:08:02 +0000 (UTC)",[GitHub] incubator-spark pull request: MLI-2: Add k-fold cross validation t...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/572#issuecomment-34792615
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Tue, 11 Feb 2014 19:08:02 +0000 (UTC)",[GitHub] incubator-spark pull request: MLI-2: Add k-fold cross validation t...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/572#issuecomment-34792614
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Tue, 11 Feb 2014 19:09:05 +0000 (UTC)",[GitHub] incubator-spark pull request: MLI-2: Add k-fold cross validation t...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/572#issuecomment-34792737
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Tue, 11 Feb 2014 19:09:06 +0000 (UTC)",[GitHub] incubator-spark pull request: MLI-2: Add k-fold cross validation t...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/572#issuecomment-34792740
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12677/


"
Jake Farrell <jfarrell@apache.org>,"Tue, 11 Feb 2014 13:14:10 -0500",Re: [VOTE] Graduation of Apache Spark from the Incubator,"""general@incubator.apache.org"" <general@incubator.apache.org>","+1 (binding)

-Jake



"
Josh Rosen <rosenville@gmail.com>,"Tue, 11 Feb 2014 11:19:54 -0800",Re: [VOTE] Graduation of Apache Spark from the Incubator,"""Spark Dev (Apache Incubator)"" <dev@spark.incubator.apache.org>",1
JoshRosen <git@git.apache.org>,"Tue, 11 Feb 2014 19:29:31 +0000 (UTC)",[GitHub] incubator-spark pull request: Added extra description on ValueErro...,dev@spark.incubator.apache.org,"Github user JoshRosen commented on the pull request:

    https://github.com/apache/incubator-spark/pull/581#issuecomment-34795439
  
    In [SPARK-972](https://spark-project.atlassian.net/browse/SPARK-972), my intent was to log the _call site_ of the original SparkContext construction to help figure out when/where it was created.  Logging the address that the original SparkContext connected to is an improvement over what we have now, but it doesn't help to debug cases where some complex user-written initialization code resulted in the creation of multiple SparkContexts connected to the same master.  [SPARK-991](https://spark-project.atlassian.net/browse/SPARK-991), which appears to have been resolved in #311, lays the groundwork for recording call sites.


"
Roman Shaposhnik <rvs@apache.org>,"Tue, 11 Feb 2014 11:29:44 -0800",Re: [VOTE] Graduation of Apache Spark from the Incubator,"""general@incubator.apache.org"" <general@incubator.apache.org>","
+1 (binding)

Thanks,
Roman.

"
AmplabJenkins <git@git.apache.org>,"Tue, 11 Feb 2014 19:33:02 +0000 (UTC)",[GitHub] incubator-spark pull request: MLI-2: Add k-fold cross validation t...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/572#issuecomment-34795846
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Tue, 11 Feb 2014 19:33:02 +0000 (UTC)",[GitHub] incubator-spark pull request: MLI-2: Add k-fold cross validation t...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/572#issuecomment-34795845
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Tue, 11 Feb 2014 19:43:04 +0000 (UTC)",[GitHub] incubator-spark pull request: MLI-2: Add k-fold cross validation t...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/572#issuecomment-34797009
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Tue, 11 Feb 2014 19:43:04 +0000 (UTC)",[GitHub] incubator-spark pull request: MLI-2: Add k-fold cross validation t...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/572#issuecomment-34797008
  
     Merged build triggered.


"
jyotiska <git@git.apache.org>,"Tue, 11 Feb 2014 19:42:45 +0000 (UTC)",[GitHub] incubator-spark pull request: Added extra description on ValueErro...,dev@spark.incubator.apache.org,"Github user jyotiska commented on the pull request:

    https://github.com/apache/incubator-spark/pull/581#issuecomment-34796972
  
    I understand. Maybe we can use a global variable to store the call site (line number?) and print it back along with the address. What do you think?


"
JoshRosen <git@git.apache.org>,"Tue, 11 Feb 2014 19:47:17 +0000 (UTC)",[GitHub] incubator-spark pull request: Added extra description on ValueErro...,dev@spark.incubator.apache.org,"Github user JoshRosen commented on the pull request:

    https://github.com/apache/incubator-spark/pull/581#issuecomment-34797494
  
    I'd store the constructor's call site (filename + line number) as an instance variable inside SparkContext, since we already have the `SparkContext._active_spark_context` global to retrieve the running context.


"
jyotiska <git@git.apache.org>,"Tue, 11 Feb 2014 19:52:40 +0000 (UTC)",[GitHub] incubator-spark pull request: Added extra description on ValueErro...,dev@spark.incubator.apache.org,"Github user jyotiska commented on the pull request:

    https://github.com/apache/incubator-spark/pull/581#issuecomment-34798126
  
    Got it. I will work on it and report it back to you. I guess traceback module will be useful here.


"
AmplabJenkins <git@git.apache.org>,"Tue, 11 Feb 2014 19:59:14 +0000 (UTC)",[GitHub] incubator-spark pull request: MLI-2: Add k-fold cross validation t...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/572#issuecomment-34798894
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Tue, 11 Feb 2014 19:59:15 +0000 (UTC)",[GitHub] incubator-spark pull request: MLI-2: Add k-fold cross validation t...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/572#issuecomment-34798895
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12678/


"
JoshRosen <git@git.apache.org>,"Tue, 11 Feb 2014 20:02:09 +0000 (UTC)",[GitHub] incubator-spark pull request: Added extra description on ValueErro...,dev@spark.incubator.apache.org,"Github user JoshRosen commented on the pull request:

    https://github.com/apache/incubator-spark/pull/581#issuecomment-34799232
  
    You may be able to re-use the [`_extract_concise_traceback`](https://github.com/apache/incubator-spark/pull/311/files#diff-d6fe2792e44f6babc94aabfefc8b9bceR43) method in `rdd.py`.


"
willb <git@git.apache.org>,"Tue, 11 Feb 2014 20:05:40 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1078: Replace lift-json with j...,dev@spark.incubator.apache.org,"GitHub user willb opened a pull request:

    https://github.com/apache/incubator-spark/pull/582

    SPARK-1078:  Replace lift-json with json4s-jackson.

    The aim of the Json4s project is to provide a common API for
    Scala JSON libraries.  It is Apache-licensed, easier for
    downstream distributions to package, and mostly API-compatible
    with lift-json.  Furthermore, the Jackson-backed implementation
    parses faster than lift-json on all but the smallest inputs.


You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark json4s

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/582.patch

----
commit 2c9f7f77849d83ab0b250c5fe80d8c599c2a8855
Author: William Benton <self@willbenton.com>
Date:   2014-02-11T17:11:56Z

    Replace lift-json with json4s-jackson.
    
    The aim of the Json4s project is to provide a common API for
    Scala JSON libraries.  It is Apache-licensed, easier for
    downstream distributions to package, and mostly API-compatible
    with lift-json.  Furthermore, the Jackson-backed implementation
    parses faster than lift-json on all but the smallest inputs.
    
    Squashed commit of the following:
    
    commit d07b75538af5c8c4f698834aa4203ff8def69872
    commit c54addda88cd9369aaeb22c5485630bf76450452
    commit 2c1387f2f8e4798be9c8d4535cf2853ca3982a91

----


"
AmplabJenkins <git@git.apache.org>,"Tue, 11 Feb 2014 20:07:59 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1078: Replace lift-json with j...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/582#issuecomment-34799861
  
    Can one of the admins verify this patch?


"
AmplabJenkins <git@git.apache.org>,"Tue, 11 Feb 2014 20:10:13 +0000 (UTC)",[GitHub] incubator-spark pull request: MLI-2: Add k-fold cross validation t...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/572#issuecomment-34800114
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12679/


"
AmplabJenkins <git@git.apache.org>,"Tue, 11 Feb 2014 20:10:13 +0000 (UTC)",[GitHub] incubator-spark pull request: MLI-2: Add k-fold cross validation t...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/572#issuecomment-34800110
  
    Merged build finished.


"
rxin <git@git.apache.org>,"Tue, 11 Feb 2014 20:21:12 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1078: Replace lift-json with j...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/582#issuecomment-34801246
  
    Jenkins, add to whitelist.


"
AmplabJenkins <git@git.apache.org>,"Tue, 11 Feb 2014 20:23:00 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1078: Replace lift-json with j...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/582#issuecomment-34801415
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Tue, 11 Feb 2014 20:23:00 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1078: Replace lift-json with j...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/582#issuecomment-34801416
  
    Merged build started.


"
jyotiska <git@git.apache.org>,"Tue, 11 Feb 2014 20:49:25 +0000 (UTC)",[GitHub] incubator-spark pull request: Added extra description on ValueErro...,dev@spark.incubator.apache.org,"Github user jyotiska commented on the pull request:

    https://github.com/apache/incubator-spark/pull/581#issuecomment-34804481
  
    I have used the _extract_concise_traceback to print the callsite of existing SparkContext. I guess this is what you had in mind.


"
AmplabJenkins <git@git.apache.org>,"Tue, 11 Feb 2014 20:51:13 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1078: Replace lift-json with j...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/582#issuecomment-34804701
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Tue, 11 Feb 2014 20:51:13 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1078: Replace lift-json with j...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/582#issuecomment-34804702
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12680/


"
rxin <git@git.apache.org>,"Tue, 11 Feb 2014 22:46:20 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1075 Fix doc in the Spark Stre...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/577#issuecomment-34817976
  
    Thanks. I've merged this.


"
rxin <git@git.apache.org>,"Tue, 11 Feb 2014 22:48:58 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1072 Use binary search when ne...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/571#issuecomment-34818238
  
    Thanks. I've merged this.


"
Henry Saputra <henry.saputra@gmail.com>,"Tue, 11 Feb 2014 14:58:03 -0800","Could someone with karma to add my userid hsaputra so I could assign
 issue in https://spark-project.atlassian.net?","""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi Guys,

With ASF JIRA still in transfer mode, could someone with permission to
add my userid ""hsaputra"" in https://spark-project.atlassian.net so I
could assign issues and resolve them myself?

CC @pwendell or @rxin


Thanks,

- Henry

"
Reynold Xin <rxin@databricks.com>,"Tue, 11 Feb 2014 15:05:30 -0800","Re: Could someone with karma to add my userid hsaputra so I could
 assign issue in https://spark-project.atlassian.net?","""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I added you to the dev list on jira for spark.



"
Henry Saputra <henry.saputra@gmail.com>,"Tue, 11 Feb 2014 15:10:26 -0800","Re: Could someone with karma to add my userid hsaputra so I could
 assign issue in https://spark-project.atlassian.net?","""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Thanks Reynold! =)

- Henry


"
hsaputra <git@git.apache.org>,"Tue, 11 Feb 2014 23:26:57 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1075 Fix doc in the Spark Stre...,dev@spark.incubator.apache.org,"Github user hsaputra closed the pull request at:

    https://github.com/apache/incubator-spark/pull/577


"
holdenk <git@git.apache.org>,"Tue, 11 Feb 2014 23:26:57 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1072 Use binary search when ne...,dev@spark.incubator.apache.org,"Github user holdenk closed the pull request at:

    https://github.com/apache/incubator-spark/pull/571


"
Dave Lester <dave@ischool.berkeley.edu>,"Tue, 11 Feb 2014 13:53:05 -0800",Re: [VOTE] Graduation of Apache Spark from the Incubator,"""general@incubator.apache.org"" <general@incubator.apache.org>","+1 (non-binding)



"
liancheng <git@git.apache.org>,"Wed, 12 Feb 2014 00:22:00 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1072 Use binary search when ne...,dev@spark.incubator.apache.org,"Github user liancheng commented on the pull request:

    https://github.com/apache/incubator-spark/pull/571#issuecomment-34825349
  
    @aarondav I wonder is `@specialized` can be used here to avoid the type dispatching in `CollectionUtils`?


"
aarondav <git@git.apache.org>,"Wed, 12 Feb 2014 01:35:07 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1078: Replace lift-json with j...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/582#issuecomment-34829822
  
    Does json4s-jackson include any dependencies that lift-json doesn't? Does the versioning match?
    
    It seems that this version of json4s-jackson pulls in jackson 2.3.0, whereas currently our dependencies are pulling in 2.2.2. Are these versions binary compatible, or could we see issues crop up?


"
colorant <git@git.apache.org>,"Wed, 12 Feb 2014 01:45:47 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1053. Don't require SPARK_YARN...,dev@spark.incubator.apache.org,"Github user colorant commented on the pull request:

    https://github.com/apache/incubator-spark/pull/553#issuecomment-34830448
  
    @sryza I think this depends on what you mean to do with the yarn-client mode, for spark-shell, since it is already packaged in the assembly jar. you don't need to set the SPARK_YARN_APP_JAR env. And the yarn doc just happen to take spark-shell as an example. While for those user apps which are not packaged in the spark assembly jar, you will need it , for either yarn-standalone and yarn-client mode. ( while, there are also other ways to add them, say with --files etc, but this one is a shortcut and actually treat default local file URL format differently ).


"
sryza <git@git.apache.org>,"Wed, 12 Feb 2014 01:53:05 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1053. Don't require SPARK_YARN...,dev@spark.incubator.apache.org,"Github user sryza commented on the pull request:

    https://github.com/apache/incubator-spark/pull/553#issuecomment-34830852
  
    It's true that you may need it when running in yarn-client mode.  And also true that you will not when running spark-shell.  Because it depends, I think not requiring it is easiest.  We could do something to try to figure out whether spark-shell is being run, and printing an error in that case, but that sounds like overkill to me.  What do you think? The doc covers both cases and sets it in the example that doesn't use spark-shell. 


"
colorant <git@git.apache.org>,"Wed, 12 Feb 2014 02:00:12 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1053. Don't require SPARK_YARN...,dev@spark.incubator.apache.org,"Github user colorant commented on the pull request:

    https://github.com/apache/incubator-spark/pull/553#issuecomment-34831254
  
    I agree that we do not need a hard check on it :) This patch looks good to me, I just wonder we might need a few more explanation to clarify why this one is not need in certain case like spark-shell :)


"
Nan Zhu <zhunanmcgill@gmail.com>,"Tue, 11 Feb 2014 21:28:35 -0500",Does yarn-stable still accept pull request?,dev@spark.incubator.apache.org,"Hi, all

Iâ€™m a new user of spark-yarn  

I would like to create a pull request for an issue found in my usage, where should I modify the code, stable or alpha (the problem exists in both)?

Best,  

--  
Nan Zhu


"
"""Liu, Raymond"" <raymond.liu@intel.com>","Wed, 12 Feb 2014 02:28:11 +0000",RE: Does yarn-stable still accept pull request?,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Should be fixed in both alpha and stable code base, since we aim to support both version

Best Regards,
Raymond Liu

-----Original Message-----
From: Nan Zhu [mailto2014 10:29 AM
To: dev@spark.incubator.apache.org
Subject: Does yarn-stable still accept pull request?

Hi, all

Iâ€™m a new user of spark-yarn  

I would like to create a pull request for an issue found in my usage, where should I modify the code, stable or alpha (the problem exists in both)?

Best,  

--  
Nan Zhu


"
willb <git@git.apache.org>,"Wed, 12 Feb 2014 03:03:42 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1078: Replace lift-json with j...,dev@spark.incubator.apache.org,"Github user willb commented on the pull request:

    https://github.com/apache/incubator-spark/pull/582#issuecomment-34834423
  
    The current version of json4s-jackson depends on paranamer 2.6, while the current version of lift-json depends on 2.4.1.  That's the only difference that winds up getting pulled in by a Spark build, although json4s has an optional extension (not currently exercised by Spark AFAICT) to use joda-time 2.3 and joda-convert 1.5.  Lift's dependencies on these joda libraries (also not exercised by Spark AFAICT) are on versions 2.1 and 1.2, respectively.
    
    FasterXML claims that subsequent Jackson minor releases in a major series are backwards-compatible; looking at the version history, it doesn't appear that anything critical has been removed from 2.2.2 to 2.3.


"
Aaron Davidson <ilikerps@gmail.com>,"Tue, 11 Feb 2014 19:42:03 -0800",Re: proposal: replace lift-json with spray-json,dev@spark.incubator.apache.org,"I am not sure I fully understand this reasoning. I imagine that lift-json
is only one of hundreds of packages that would have to be built if you
wanted to build all of Spark's transitive dependencies from source.

Additionally, to make sure I understand the impact -- this is only intended
to simplify the process of packaging Spark on a new OS distribution that
disallows pulling in binaries?



"
jyotiska <git@git.apache.org>,"Wed, 12 Feb 2014 03:50:52 +0000 (UTC)",[GitHub] incubator-spark pull request: Added extra description on ValueErro...,dev@spark.incubator.apache.org,"Github user jyotiska commented on the pull request:

    https://github.com/apache/incubator-spark/pull/581#issuecomment-34836413
  
    I think for future improvements, it would be a good idea to move the traceback code to a separate file and use that to return callsite. Please start a JIRA ticket and assign it to me. I will submit a PR once I am done.


"
aarondav <git@git.apache.org>,"Wed, 12 Feb 2014 03:51:16 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1078: Replace lift-json with j...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/582#issuecomment-34836430
  
    Thanks for looking into it! The situation sounds fine for the next minor release, and I don't think this patch needs to be included in the next maintenance release anyway (following your very own [suggestion](http://mail-archives.apache.org/mod_mbox/spark-dev/201402.mbox/browser) on the dev list).
    
    While this patch looks good to me, I am not sure I fully understand the need for it. I posted my question on the [dev list thread](http://mail-archives.apache.org/mod_mbox/spark-dev/201402.mbox/%3C945190638.685798.1391974088596.JavaMail.zimbra%40redhat.com%3E). Besides the dependency change, you also mention performance improvements. [This benchmark](http://engineering.ooyala.com/blog/comparing-scala-json-libraries) does show Jackson outperforming lift on a particular workload, but do you have another source showing how the relative performance changes with input size?


"
Mark Hamstra <mark@clearstorydata.com>,"Tue, 11 Feb 2014 19:56:58 -0800","Re: [GitHub] incubator-spark pull request: SPARK-1078: Replace
 lift-json with j...","""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","

I don't understand what you mean by this.  According to my current
understanding, the next release of Spark other than maintenance releases on
0.9.x is intended to be a major release, 1.0.0, and there are no plans for
an intervening minor release, which would be 0.10.0.  Thus ""the next minor
release"" would be 1.1.0, and I fail to see why we would wait for that
instead of putting the dependency change (assuming that it is something
that we do, indeed, want) in 1.0.0.




"
Patrick Wendell <pwendell@gmail.com>,"Tue, 11 Feb 2014 20:08:53 -0800","Re: [GitHub] incubator-spark pull request: SPARK-1078: Replace
 lift-json with j...","""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I think Aaron just meant 1.0.0 by ""the next minor release"".


"
Aaron Davidson <ilikerps@gmail.com>,"Tue, 11 Feb 2014 20:09:01 -0800","Re: [GitHub] incubator-spark pull request: SPARK-1078: Replace
 lift-json with j...",dev@spark.incubator.apache.org,"My apologies, my intention was that it is fine for the next minor
*or*major release, regardless of what comes next. I only wanted to
distinguish
that from the next maintenance release, since my understanding is that we
wish to avoid changing dependencies during maintenance releases.



"
colorant <git@git.apache.org>,"Wed, 12 Feb 2014 04:36:43 +0000 (UTC)",[GitHub] incubator-spark pull request: Minor fix for ZooKeeperPersistenceEn...,dev@spark.incubator.apache.org,"GitHub user colorant opened a pull request:

    https://github.com/apache/incubator-spark/pull/583

    Minor fix for ZooKeeperPersistenceEngine to use configured working dir

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark zookeeper

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/583.patch

----
commit b4861ee75dbd5cb55ba40805cd58fc3a0cdc5159
Author: Raymond Liu <raymond.liu@intel.com>
Date:   2014-02-12T03:35:36Z

    Minor fix for ZooKeeperPersistenceEngine to use configured working dir

----


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 04:37:59 +0000 (UTC)",[GitHub] incubator-spark pull request: Minor fix for ZooKeeperPersistenceEn...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/583#issuecomment-34838194
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 04:37:59 +0000 (UTC)",[GitHub] incubator-spark pull request: Minor fix for ZooKeeperPersistenceEn...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/583#issuecomment-34838192
  
     Merged build triggered.


"
aarondav <git@git.apache.org>,"Wed, 12 Feb 2014 04:38:15 +0000 (UTC)",[GitHub] incubator-spark pull request: Minor fix for ZooKeeperPersistenceEn...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/583#issuecomment-34838201
  
    Good catch! LGTM.


"
colorant <git@git.apache.org>,"Wed, 12 Feb 2014 04:42:57 +0000 (UTC)",[GitHub] incubator-spark pull request: Minor fix for ZooKeeperPersistenceEn...,dev@spark.incubator.apache.org,"Github user colorant commented on the pull request:

    https://github.com/apache/incubator-spark/pull/583#issuecomment-34838369
  
    oops, catch me ;) updated


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 04:42:59 +0000 (UTC)",[GitHub] incubator-spark pull request: Minor fix for ZooKeeperPersistenceEn...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/583#issuecomment-34838371
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 04:42:59 +0000 (UTC)",[GitHub] incubator-spark pull request: Minor fix for ZooKeeperPersistenceEn...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/583#issuecomment-34838372
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 04:48:00 +0000 (UTC)",[GitHub] incubator-spark pull request: Minor fix for ZooKeeperPersistenceEn...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/583#issuecomment-34838606
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 04:48:00 +0000 (UTC)",[GitHub] incubator-spark pull request: Minor fix for ZooKeeperPersistenceEn...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/583#issuecomment-34838607
  
    Merged build started.


"
aarondav <git@git.apache.org>,"Wed, 12 Feb 2014 04:48:18 +0000 (UTC)",[GitHub] incubator-spark pull request: Minor fix for ZooKeeperPersistenceEn...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/583#issuecomment-34838620
  
    Alright, now looks-really-good-to-me :)
    
    This should be included in 0.9.1.


"
pwendell <git@git.apache.org>,"Wed, 12 Feb 2014 04:57:24 +0000 (UTC)",[GitHub] incubator-spark pull request: Hadoop jar name,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/522#issuecomment-34838973
  
    Jenkins, test this please. This was reviewed previously as #121 by matei. I think it's good to go.


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 04:58:08 +0000 (UTC)",[GitHub] incubator-spark pull request: Hadoop jar name,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/522#issuecomment-34839012
  
     Build triggered.


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 04:58:09 +0000 (UTC)",[GitHub] incubator-spark pull request: Hadoop jar name,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/522#issuecomment-34839013
  
    Build started.


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 04:58:57 +0000 (UTC)",[GitHub] incubator-spark pull request: Hadoop jar name,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/522#issuecomment-34839049
  
    Build finished.


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 04:58:57 +0000 (UTC)",[GitHub] incubator-spark pull request: Hadoop jar name,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/522#issuecomment-34839050
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12684/


"
pwendell <git@git.apache.org>,"Wed, 12 Feb 2014 05:03:49 +0000 (UTC)",[GitHub] incubator-spark pull request: Hadoop jar name,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/522#issuecomment-34839244
  
    @bijaybisht this is out of date with master - mind bringing it up to date?


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 05:05:57 +0000 (UTC)",[GitHub] incubator-spark pull request: Minor fix for ZooKeeperPersistenceEn...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/583#issuecomment-34839331
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 05:05:57 +0000 (UTC)",[GitHub] incubator-spark pull request: Minor fix for ZooKeeperPersistenceEn...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/583#issuecomment-34839333
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12681/


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 05:11:17 +0000 (UTC)",[GitHub] incubator-spark pull request: Minor fix for ZooKeeperPersistenceEn...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/583#issuecomment-34839550
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12682/


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 05:11:17 +0000 (UTC)",[GitHub] incubator-spark pull request: Minor fix for ZooKeeperPersistenceEn...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/583#issuecomment-34839549
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 05:16:41 +0000 (UTC)",[GitHub] incubator-spark pull request: Minor fix for ZooKeeperPersistenceEn...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/583#issuecomment-34839768
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12683/


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 05:16:41 +0000 (UTC)",[GitHub] incubator-spark pull request: Minor fix for ZooKeeperPersistenceEn...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/583#issuecomment-34839767
  
    Merged build finished.


"
bijaybisht <git@git.apache.org>,"Wed, 12 Feb 2014 05:39:54 +0000 (UTC)",[GitHub] incubator-spark pull request: Hadoop jar name,dev@spark.incubator.apache.org,"Github user bijaybisht closed the pull request at:

    https://github.com/apache/incubator-spark/pull/522


"
bijaybisht <git@git.apache.org>,"Wed, 12 Feb 2014 05:43:42 +0000 (UTC)",[GitHub] incubator-spark pull request: Hadoop jar name,dev@spark.incubator.apache.org,"GitHub user bijaybisht reopened a pull request:

    https://github.com/apache/incubator-spark/pull/522

    Hadoop jar name

    This pull request is a copy of 
    #121 - Fix for hadoop client jar name, which got changed from 1.*. The other one was from master, which is wrong way of generating the pull requests. 

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark hadoop_jar_name

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/522.patch

----
commit 0ff38c22205f14770ecca1e66378e7c207ca2d1d
Author: Erik Selin <erik.selin@jadedpixel.com>
Date:   2014-01-29T20:44:54Z

    Merge pull request #494 from tyro89/worker_registration_issue
    
    Issue with failed worker registrations
    
    I've been going through the spark source after having some odd issues with workers dying and not coming back. After some digging (I'm very new to scala and spark) I believe I've found a worker registration issue. It looks to me like a failed registration follows the same code path as a successful registration which end up with workers believing they are connected (since they received a `RegisteredWorker` event) even tho they are not registered on the Master.
    
    This is a quick fix that I hope addresses this issue (assuming I didn't completely miss-read the code and I'm about to look like a silly person :P)
    
    I'm opening this pr now to start a chat with you guys while I do some more testing on my side :)
    
    Author: Erik Selin <erik.selin@jadedpixel.com>
    
    == Merge branch commits ==
    
    commit 973012f8a2dcf1ac1e68a69a2086a1b9a50f401b
    Author: Erik Selin <erik.selin@jadedpixel.com>
    Date:   Tue Jan 28 23:36:12 2014 -0500
    
        break logwarning into two lines to respect line character limit.
    
    commit e3754dc5b94730f37e9806974340e6dd93400f85
    Author: Erik Selin <erik.selin@jadedpixel.com>
    Date:   Tue Jan 28 21:16:21 2014 -0500
    
        add log warning when worker registration fails due to attempt to re-register on same address.
    
    commit 14baca241fa7823e1213cfc12a3ff2a9b865b1ed
    Author: Erik Selin <erik.selin@jadedpixel.com>
    Date:   Wed Jan 22 21:23:26 2014 -0500
    
        address code style comment
    
    commit 71c0d7e6f59cd378d4e24994c21140ab893954ee
    Author: Erik Selin <erik.selin@jadedpixel.com>
    Date:   Wed Jan 22 16:01:42 2014 -0500
    
        Make a failed registration not persist, not send a `RegisteredWordker` event and not run `schedule` but rather send a `RegisterWorkerFailed` message to the worker attempting to register.

commit ac712e48af3068672e629cec7766caae3cd77c37
Author: Reynold Xin <rxin@apache.org>
Date:   2014-01-30T17:33:18Z

    Merge pull request #524 from rxin/doc
    
    Added spark.shuffle.file.buffer.kb to configuration doc.
    
    Author: Reynold Xin <rxin@apache.org>
    
    == Merge branch commits ==
    
    commit 0eea1d761ff772ff89be234e1e28035d54e5a7de
    Author: Reynold Xin <rxin@apache.org>
    Date:   Wed Jan 29 14:40:48 2014 -0800
    
        Added spark.shuffle.file.buffer.kb to configuration doc.

commit a8cf3ec157fc9a512421b319cfffc5e4f07cf1f3
Author: Ankur Dave <ankurdave@gmail.com>
Date:   2014-02-01T00:52:02Z

    Merge pull request #527 from ankurdave/graphx-assembly-pom
    
    Add GraphX to assembly/pom.xml
    
    Author: Ankur Dave <ankurdave@gmail.com>
    
    == Merge branch commits ==
    
    commit bb0b33ef9eb1b3d4a4fc283d9abb2ece4abcac23
    Author: Ankur Dave <ankurdave@gmail.com>
    Date:   Fri Jan 31 15:24:52 2014 -0800
    
        Add GraphX to assembly/pom.xml

commit 0386f42e383dc01b8df33c4a70b024e7902b5fdd
Author: Henry Saputra <hsaputra@apache.org>
Date:   2014-02-03T05:51:17Z

    Merge pull request #529 from hsaputra/cleanup_right_arrowop_scala
    
    Change the â‡’ character (maybe from scalariform) to => in Scala code for style consistency
    
    Looks like there are some â‡’ Unicode character (maybe from scalariform) in Scala code.
    This PR is to change it to => to get some consistency on the Scala code.
    
    If we want to use â‡’ as default we could use sbt plugin scalariform to make sure all Scala code has â‡’ instead of =>
    
    And remove unused imports found in TwitterInputDStream.scala while I was there =)
    
    Author: Henry Saputra <hsaputra@apache.org>
    
    == Merge branch commits ==
    
    commit 29c1771d346dff901b0b778f764e6b4409900234
    Author: Henry Saputra <hsaputra@apache.org>
    Date:   Sat Feb 1 22:05:16 2014 -0800
    
        Change the â‡’ character (maybe from scalariform) to => in Scala code for style consistency.

commit 1625d8c44693420de026138f3abecce2d12f895c
Author: Aaron Davidson <aaron@databricks.com>
Date:   2014-02-03T19:25:39Z

    Merge pull request #530 from aarondav/cleanup. Closes #530.
    
    Remove explicit conversion to PairRDDFunctions in cogroup()
    
    As SparkContext._ is already imported, using the implicit conversion appears to make the code much cleaner. Perhaps there was some sinister reason for doing the conversion explicitly, however.
    
    Author: Aaron Davidson <aaron@databricks.com>
    
    == Merge branch commits ==
    
    commit aa4a63f1bfd5b5178fe67364dd7ce4d84c357996
    Author: Aaron Davidson <aaron@databricks.com>
    Date:   Sun Feb 2 23:48:04 2014 -0800
    
        Remove explicit conversion to PairRDDFunctions in cogroup()
    
        As SparkContext._ is already imported, using the implicit conversion
        appears to make the code much cleaner. Perhaps there was some sinister
        reason for doing the converion explicitly, however.

commit 23af00f9e0e5108f62cdb9629e3eb4e54bbaa321
Author: Xiangrui Meng <meng@databricks.com>
Date:   2014-02-03T21:02:09Z

    Merge pull request #528 from mengxr/sample. Closes #528.
    
     Refactor RDD sampling and add randomSplit to RDD (update)
    
    Replace SampledRDD by PartitionwiseSampledRDD, which accepts a RandomSampler instance as input. The current sample with/without replacement can be easily integrated via BernoulliSampler and PoissonSampler. The benefits are:
    
    1) RDD.randomSplit is implemented in the same way, related to https://github.com/apache/incubator-spark/pull/513
    2) Stratified sampling and importance sampling can be implemented in the same manner as well.
    
    Unit tests are included for samplers and RDD.randomSplit.
    
    This should performance better than my previous request where the BernoulliSampler creates many Iterator instances:
    https://github.com/apache/incubator-spark/pull/513
    
    Author: Xiangrui Meng <meng@databricks.com>
    
    == Merge branch commits ==
    
    commit e8ce957e5f0a600f2dec057924f4a2ca6adba373
    Author: Xiangrui Meng <meng@databricks.com>
    Date:   Mon Feb 3 12:21:08 2014 -0800
    
        more docs to PartitionwiseSampledRDD
    
    commit fbb4586d0478ff638b24bce95f75ff06f713d43b
    Author: Xiangrui Meng <meng@databricks.com>
    Date:   Mon Feb 3 00:44:23 2014 -0800
    
        move XORShiftRandom to util.random and use it in BernoulliSampler
    
    commit 987456b0ee8612fd4f73cb8c40967112dc3c4c2d
    Author: Xiangrui Meng <meng@databricks.com>
    Date:   Sat Feb 1 11:06:59 2014 -0800
    
        relax assertions in SortingSuite because the RangePartitioner has large variance in this case
    
    commit 3690aae416b2dc9b2f9ba32efa465ba7948477f4
    Author: Xiangrui Meng <meng@databricks.com>
    Date:   Sat Feb 1 09:56:28 2014 -0800
    
        test split ratio of RDD.randomSplit
    
    commit 8a410bc933a60c4d63852606f8bbc812e416d6ae
    Author: Xiangrui Meng <meng@databricks.com>
    Date:   Sat Feb 1 09:25:22 2014 -0800
    
        add a test to ensure seed distribution and minor style update
    
    commit ce7e866f674c30ab48a9ceb09da846d5362ab4b6
    Author: Xiangrui Meng <meng@databricks.com>
    Date:   Fri Jan 31 18:06:22 2014 -0800
    
        minor style change
    
    commit 750912b4d77596ed807d361347bd2b7e3b9b7a74
    Author: Xiangrui Meng <meng@databricks.com>
    Date:   Fri Jan 31 18:04:54 2014 -0800
    
        fix some long lines
    
    commit c446a25c38d81db02821f7f194b0ce5ab4ed7ff5
    Author: Xiangrui Meng <meng@databricks.com>
    Date:   Fri Jan 31 17:59:59 2014 -0800
    
        add complement to BernoulliSampler and minor style changes
    
    commit dbe2bc2bd888a7bdccb127ee6595840274499403
    Author: Xiangrui Meng <meng@databricks.com>
    Date:   Fri Jan 31 17:45:08 2014 -0800
    
        switch to partition-wise sampling for better performance
    
    commit a1fca5232308feb369339eac67864c787455bb23
    Merge: ac712e4 cf6128f
    Author: Xiangrui Meng <meng@databricks.com>
    Date:   Fri Jan 31 16:33:09 2014 -0800
    
        Merge branch 'sample' of github.com:mengxr/incubator-spark into sample
    
    commit cf6128fb672e8c589615adbd3eaa3cbdb72bd461
    Author: Xiangrui Meng <meng@databricks.com>
    Date:   Sun Jan 26 14:40:07 2014 -0800
    
        set SampledRDD deprecated in 1.0
    
    commit f430f847c3df91a3894687c513f23f823f77c255
    Author: Xiangrui Meng <meng@databricks.com>
    Date:   Sun Jan 26 14:38:59 2014 -0800
    
        update code style
    
    commit a8b5e2021a9204e318c80a44d00c5c495f1befb6
    Author: Xiangrui Meng <meng@databricks.com>
    Date:   Sun Jan 26 12:56:27 2014 -0800
    
        move package random to util.random
    
    commit ab0fa2c4965033737a9e3a9bf0a59cbb0df6a6f5
    Author: Xiangrui Meng <meng@databricks.com>
    Date:   Sun Jan 26 12:50:35 2014 -0800
    
        add Apache headers and update code style
    
    commit 985609fe1a55655ad11966e05a93c18c138a403d
    Author: Xiangrui Meng <meng@databricks.com>
    Date:   Sun Jan 26 11:49:25 2014 -0800
    
        add new lines
    
    commit b21bddf29850a2c006a868869b8f91960a029322
    Author: Xiangrui Meng <meng@databricks.com>
    Date:   Sun Jan 26 11:46:35 2014 -0800
    
        move samplers to random.IndependentRandomSampler and add tests
    
    commit c02dacb4a941618e434cefc129c002915db08be6
    Author: Xiangrui Meng <meng@databricks.com>
    Date:   Sat Jan 25 15:20:24 2014 -0800
    
        add RandomSampler
    
    commit 8ff7ba3c5cf1fc338c29ae8b5fa06c222640e89c
    Author: Xiangrui Meng <meng@databricks.com>
    Date:   Fri Jan 24 13:23:22 2014 -0800
    
        init impl of IndependentlySampledRDD

commit 0c05cd374dac309b5444980f10f8dcb820c752c2
Author: Stevo SlaviÄ‡ <sslavic@gmail.com>
Date:   2014-02-04T17:45:46Z

    Merge pull request #535 from sslavic/patch-2. Closes #535.
    
    Fixed typo in scaladoc
    
    Author: Stevo SlaviÄ‡ <sslavic@gmail.com>
    
    == Merge branch commits ==
    
    commit 0a77f789e281930f4168543cc0d3b3ffbf5b3764
    Author: Stevo SlaviÄ‡ <sslavic@gmail.com>
    Date:   Tue Feb 4 15:30:27 2014 +0100
    
        Fixed typo in scaladoc

commit 92092879c3b8001a456fefc2efc0df16585515a8
Author: Stevo SlaviÄ‡ <sslavic@gmail.com>
Date:   2014-02-04T17:47:11Z

    Merge pull request #534 from sslavic/patch-1. Closes #534.
    
    Fixed wrong path to compute-classpath.cmd
    
    compute-classpath.cmd is in bin, not in sbin directory
    
    Author: Stevo SlaviÄ‡ <sslavic@gmail.com>
    
    == Merge branch commits ==
    
    commit 23deca32b69e9429b33ad31d35b7e1bfc9459f59
    Author: Stevo SlaviÄ‡ <sslavic@gmail.com>
    Date:   Tue Feb 4 15:01:47 2014 +0100
    
        Fixed wrong path to compute-classpath.cmd
    
        compute-classpath.cmd is in bin, not in sbin directory

commit f7fd80d9a71069cba94294e6b77c0eaeb90e73d7
Author: Stevo SlaviÄ‡ <sslavic@gmail.com>
Date:   2014-02-05T18:29:45Z

    Merge pull request #540 from sslavic/patch-3. Closes #540.
    
    Fix line end character stripping for Windows
    
    LogQuery Spark example would produce unwanted result when run on Windows platform because of different, platform specific trailing line end characters (not only \n but \r too).
    
    This fix makes use of Scala's standard library string functions to properly strip all trailing line end characters, letting Scala handle the platform specific stuff.
    
    Author: Stevo SlaviÄ‡ <sslavic@gmail.com>
    
    == Merge branch commits ==
    
    commit 1e43ba0ea773cc005cf0aef78b6c1755f8e88b27
    Author: Stevo SlaviÄ‡ <sslavic@gmail.com>
    Date:   Wed Feb 5 14:48:29 2014 +0100
    
        Fix line end character stripping for Windows
    
        LogQuery Spark example would produce unwanted result when run on Windows platform because of different, platform specific trailing line end characters (not only \n but \r too).
    
        This fix makes use of Scala's standard library string functions to properly strip all trailing line end characters, letting Scala handle the platform specific stuff.

commit cc14ba974c8e98c08548a2ccf64c2765f313f649
Author: Kay Ousterhout <kayousterhout@gmail.com>
Date:   2014-02-05T20:44:24Z

    Merge pull request #544 from kayousterhout/fix_test_warnings. Closes #544.
    
    Fixed warnings in test compilation.
    
    This commit fixes two problems: a redundant import, and a
    deprecated function.
    
    Author: Kay Ousterhout <kayousterhout@gmail.com>
    
    == Merge branch commits ==
    
    commit da9d2e13ee4102bc58888df0559c65cb26232a82
    Author: Kay Ousterhout <kayousterhout@gmail.com>
    Date:   Wed Feb 5 11:41:51 2014 -0800
    
        Fixed warnings in test compilation.
    
        This commit fixes two problems: a redundant import, and a
        deprecated function.

commit 18c4ee71e27189f5f3f4eb6bfc6ad8860aa254c6
Author: CodingCat <zhunansjtu@gmail.com>
Date:   2014-02-06T06:08:47Z

    Merge pull request #549 from CodingCat/deadcode_master. Closes #549.
    
    remove actorToWorker in master.scala, which is actually not used
    
    actorToWorker is actually not used in the code....just remove it
    
    Author: CodingCat <zhunansjtu@gmail.com>
    
    == Merge branch commits ==
    
    commit 52656c2d4bbf9abcd8bef65d454badb9cb14a32c
    Author: CodingCat <zhunansjtu@gmail.com>
    Date:   Thu Feb 6 00:28:26 2014 -0500
    
        remove actorToWorker in master.scala, which is actually not used

commit 38020961d101e792393855fd00d8e42f40713754
Author: Thomas Graves <tgraves@apache.org>
Date:   2014-02-06T07:37:07Z

    Merge pull request #526 from tgravescs/yarn_client_stop_am_fix. Closes #526.
    
    spark on yarn - yarn-client mode doesn't always exit immediately
    
    https://spark-project.atlassian.net/browse/SPARK-1049
    
    If you run in the yarn-client mode but you don't get all the workers you requested right away and then you exit your application, the application master stays around until it gets the number of workers you initially requested. This is a waste of resources.  The AM should exit immediately upon the client going away.
    
    This fix simply checks to see if the driver closed while its waiting for the initial # of workers.
    
    Author: Thomas Graves <tgraves@apache.org>
    
    == Merge branch commits ==
    
    commit 03f40a62584b6bdd094ba91670cd4aa6afe7cd81
    Author: Thomas Graves <tgraves@apache.org>
    Date:   Fri Jan 31 11:23:10 2014 -0600
    
        spark on yarn - yarn-client mode doesn't always exit immediately

commit 79c95527a77af32bd83a968c1a56feb22e441b7d
Author: Kay Ousterhout <kayousterhout@gmail.com>
Date:   2014-02-06T07:38:12Z

    Merge pull request #545 from kayousterhout/fix_progress. Closes #545.
    
    Fix off-by-one error with task progress info log.
    
    Author: Kay Ousterhout <kayousterhout@gmail.com>
    
    == Merge branch commits ==
    
    commit 29798fc685c4e7e3eb3bf91c75df7fa8ec94a235
    Author: Kay Ousterhout <kayousterhout@gmail.com>
    Date:   Wed Feb 5 13:40:01 2014 -0800
    
        Fix off-by-one error with task progress info log.

commit 084839ba357e03bb56517620123682b50a91cb0b
Author: Prashant Sharma <prashant.s@imaginea.com>
Date:   2014-02-06T22:58:35Z

    Merge pull request #498 from ScrapCodes/python-api. Closes #498.
    
    Python api additions
    
    Author: Prashant Sharma <prashant.s@imaginea.com>
    
    == Merge branch commits ==
    
    commit 8b51591f1a7a79a62c13ee66ff8d83040f7eccd8
    Author: Prashant Sharma <prashant.s@imaginea.com>
    Date:   Fri Jan 24 11:50:29 2014 +0530
    
        Josh's and Patricks review comments.
    
    commit d37f9677838e43bef6c18ef61fbf08055ba6d1ca
    Author: Prashant Sharma <prashant.s@imaginea.com>
    Date:   Thu Jan 23 17:27:17 2014 +0530
    
        fixed doc tests
    
    commit 27cb54bf5c99b1ea38a73858c291d0a1c43d8b7c
    Author: Prashant Sharma <prashant.s@imaginea.com>
    Date:   Thu Jan 23 16:48:43 2014 +0530
    
        Added keys and values methods for PairFunctions in python
    
    commit 4ce76b396fbaefef2386d7a36d611572bdef9b5d
    Author: Prashant Sharma <prashant.s@imaginea.com>
    Date:   Thu Jan 23 13:51:26 2014 +0530
    
        Added foreachPartition
    
    commit 05f05341a187cba829ac0e6c2bdf30be49948c89
    Author: Prashant Sharma <prashant.s@imaginea.com>
    Date:   Thu Jan 23 13:02:59 2014 +0530
    
        Added coalesce fucntion to python API
    
    commit 6568d2c2fa14845dc56322c0f39ba2e13b3b26dd
    Author: Prashant Sharma <prashant.s@imaginea.com>
    Date:   Thu Jan 23 12:52:44 2014 +0530
    
        added repartition function to python API.

commit 446403b63763157831ddbf6209044efc3cc7bf7c
Author: Sandy Ryza <sandy@cloudera.com>
Date:   2014-02-06T23:41:16Z

    Merge pull request #554 from sryza/sandy-spark-1056. Closes #554.
    
    SPARK-1056. Fix header comment in Executor to not imply that it's only u...
    
    ...sed for Mesos and Standalone.
    
    Author: Sandy Ryza <sandy@cloudera.com>
    
    == Merge branch commits ==
    
    commit 1f2443d902a26365a5c23e4af9077e1539ed2eab
    Author: Sandy Ryza <sandy@cloudera.com>
    Date:   Thu Feb 6 15:03:50 2014 -0800
    
        SPARK-1056. Fix header comment in Executor to not imply that it's only used for Mesos and Standalone

commit 18ad59e2c6b7bd009e8ba5ebf8fcf99630863029
Author: Kay Ousterhout <kayousterhout@gmail.com>
Date:   2014-02-07T00:10:48Z

    Merge pull request #321 from kayousterhout/ui_kill_fix. Closes #321.
    
    Inform DAG scheduler about all started/finished tasks.
    
    Previously, the DAG scheduler was not always informed
    when tasks started and finished. The simplest example here
    is for speculated tasks: the DAGScheduler was only told about
    the first attempt of a task, meaning that SparkListeners were
    also not told about multiple task attempts, so users can't see
    what's going on with speculation in the UI.  The DAGScheduler
    also wasn't always told about finished tasks, so in the UI, some
    tasks will never be shown as finished (this occurs, for example,
    if a task set gets killed).
    
    The other problem is that the fairness accounting was wrong
    -- the number of running tasks in a pool was decreased when a
    task set was considered done, even if all of its tasks hadn't
    yet finished.
    
    Author: Kay Ousterhout <kayousterhout@gmail.com>
    
    == Merge branch commits ==
    
    commit c8d547d0f7a17f5a193bef05f5872b9f475675c5
    Author: Kay Ousterhout <kayousterhout@gmail.com>
    Date:   Wed Jan 15 16:47:33 2014 -0800
    
        Addressed Reynold's review comments.
    
        Always use a TaskEndReason (remove the option), and explicitly
        signal when we don't know the reason. Also, always tell
        DAGScheduler (and associated listeners) about started tasks, even
        when they're speculated.
    
    commit 3fee1e2e3c06b975ff7f95d595448f38cce97a04
    Author: Kay Ousterhout <kayousterhout@gmail.com>
    Date:   Wed Jan 8 22:58:13 2014 -0800
    
        Fixed broken test and improved logging
    
    commit ff12fcaa2567c5d02b75a1d5db35687225bcd46f
    Author: Kay Ousterhout <kayousterhout@gmail.com>
    Date:   Sun Dec 29 21:08:20 2013 -0800
    
        Inform DAG scheduler about all finished tasks.
    
        Previously, the DAG scheduler was not always informed
        when tasks finished. For example, when a task set was
        aborted, the DAG scheduler was never told when the tasks
        in that task set finished. The DAG scheduler was also
        never told about the completion of speculated tasks.
        This led to confusion with SparkListeners because information
        about the completion of those tasks was never passed on to
        the listeners (so in the UI, for example, some tasks will never
        be shown as finished).
    
        The other problem is that the fairness accounting was wrong
        -- the number of running tasks in a pool was decreased when a
        task set was considered done, even if all of its tasks hadn't
        yet finished.

commit 0b448df6ac520a7977b1eb51e8c55e33f3fd2da8
Author: Kay Ousterhout <kayousterhout@gmail.com>
Date:   2014-02-07T00:15:24Z

    Merge pull request #450 from kayousterhout/fetch_failures. Closes #450.
    
    
    Previously, the ResubmitFailedStages event was called every
    200 milliseconds, leading to a lot of unnecessary event processing
    and clogged DAGScheduler logs.
    
    Author: Kay Ousterhout <kayousterhout@gmail.com>
    
    == Merge branch commits ==
    
    commit e603784b3a562980e6f1863845097effe2129d3b
    Author: Kay Ousterhout <kayousterhout@gmail.com>
    Date:   Wed Feb 5 11:34:41 2014 -0800
    
        Re-add check for empty set of failed stages
    
    commit d258f0ef50caff4bbb19fb95a6b82186db1935bf
    Author: Kay Ousterhout <kayousterhout@gmail.com>
    Date:   Wed Jan 15 23:35:41 2014 -0800
    
    
        Previously, the ResubmitFailedStages event was called every
        200 milliseconds, leading to a lot of unnecessary event processing
        and clogged DAGScheduler logs.

commit 1896c6e7c9f5c29284a045128b4aca0d5a6e7220
Author: Andrew Or <andrewor14@gmail.com>
Date:   2014-02-07T06:05:53Z

    Merge pull request #533 from andrewor14/master. Closes #533.
    
    External spilling - generalize batching logic
    
    The existing implementation consists of a hack for Kryo specifically and only works for LZF compression. Introducing an intermediate batch-level stream takes care of pre-fetching and other arbitrary behavior of higher level streams in a more general way.
    
    Author: Andrew Or <andrewor14@gmail.com>
    
    == Merge branch commits ==
    
    commit 3ddeb7ef89a0af2b685fb5d071aa0f71c975cc82
    Author: Andrew Or <andrewor14@gmail.com>
    Date:   Wed Feb 5 12:09:32 2014 -0800
    
        Also privatize fields
    
    commit 090544a87a0767effd0c835a53952f72fc8d24f0
    Author: Andrew Or <andrewor14@gmail.com>
    Date:   Wed Feb 5 10:58:23 2014 -0800
    
        Privatize methods
    
    commit 13920c918efe22e66a1760b14beceb17a61fd8cc
    Author: Andrew Or <andrewor14@gmail.com>
    Date:   Tue Feb 4 16:34:15 2014 -0800
    
        Update docs
    
    commit bd5a1d7350467ed3dc19c2de9b2c9f531f0e6aa3
    Author: Andrew Or <andrewor14@gmail.com>
    Date:   Tue Feb 4 13:44:24 2014 -0800
    
        Typo: phyiscal -> physical
    
    commit 287ef44e593ad72f7434b759be3170d9ee2723d2
    Author: Andrew Or <andrewor14@gmail.com>
    Date:   Tue Feb 4 13:38:32 2014 -0800
    
        Avoid reading the entire batch into memory; also simplify streaming logic
    
        Additionally, address formatting comments.
    
    commit 3df700509955f7074821e9aab1e74cb53c58b5a5
    Merge: a531d2e 164489d
    Author: Andrew Or <andrewor14@gmail.com>
    Date:   Mon Feb 3 18:27:49 2014 -0800
    
        Merge branch 'master' of github.com:andrewor14/incubator-spark
    
    commit a531d2e347acdcecf2d0ab72cd4f965ab5e145d8
    Author: Andrew Or <andrewor14@gmail.com>
    Date:   Mon Feb 3 18:18:04 2014 -0800
    
        Relax assumptions on compressors and serializers when batching
    
        This commit introduces an intermediate layer of an input stream on the batch level.
        This guards against interference from higher level streams (i.e. compression and
        deserialization streams), especially pre-fetching, without specifically targeting
        particular libraries (Kryo) and forcing shuffle spill compression to use LZF.
    
    commit 164489d6f176bdecfa9dabec2dfce5504d1ee8af
    Author: Andrew Or <andrewor14@gmail.com>
    Date:   Mon Feb 3 18:18:04 2014 -0800
    
        Relax assumptions on compressors and serializers when batching
    
        This commit introduces an intermediate layer of an input stream on the batch level.
        This guards against interference from higher level streams (i.e. compression and
        deserialization streams), especially pre-fetching, without specifically targeting
        particular libraries (Kryo) and forcing shuffle spill compression to use LZF.

commit 3a9d82cc9e85accb5c1577cf4718aa44c8d5038c
Author: Andrew Ash <andrew@andrewash.com>
Date:   2014-02-07T06:38:36Z

    Merge pull request #506 from ash211/intersection. Closes #506.
    
    SPARK-1062 Add rdd.intersection(otherRdd) method
    
    Author: Andrew Ash <andrew@andrewash.com>
    
    == Merge branch commits ==
    
    commit 5d9982b171b9572649e9828f37ef0b43f0242912
    Author: Andrew Ash <andrew@andrewash.com>
    Date:   Thu Feb 6 18:11:45 2014 -0800
    
        Minor fixes
    
        - style: (v,null) => (v, null)
        - mention the shuffle in Javadoc
    
    commit b86d02f14e810902719cef893cf6bfa18ff9acb0
    Author: Andrew Ash <andrew@andrewash.com>
    Date:   Sun Feb 2 13:17:40 2014 -0800
    
        Overload .intersection() for numPartitions and custom Partitioner
    
    commit bcaa34911fcc6bb5bc5e4f9fe46d1df73cb71c09
    Author: Andrew Ash <andrew@andrewash.com>
    Date:   Sun Feb 2 13:05:40 2014 -0800
    
        Better naming of parameters in intersection's filter
    
    commit b10a6af2d793ec6e9a06c798007fac3f6b860d89
    Author: Andrew Ash <andrew@andrewash.com>
    Date:   Sat Jan 25 23:06:26 2014 -0800
    
        Follow spark code format conventions of tab => 2 spaces
    
    commit 965256e4304cca514bb36a1a36087711dec535ec
    Author: Andrew Ash <andrew@andrewash.com>
    Date:   Fri Jan 24 00:28:01 2014 -0800
    
        Add rdd.intersection(otherRdd) method

commit fabf1749995103841e6a3975892572f376ee48d0
Author: Martin Jaggi <m.jaggi@gmail.com>
Date:   2014-02-08T19:39:13Z

    Merge pull request #552 from martinjaggi/master. Closes #552.
    
    tex formulas in the documentation
    
    using mathjax.
    and spliting the MLlib documentation by techniques
    
    see jira
    https://spark-project.atlassian.net/browse/MLLIB-19
    and
    https://github.com/shivaram/spark/compare/mathjax
    
    Author: Martin Jaggi <m.jaggi@gmail.com>
    
    == Merge branch commits ==
    
    commit 0364bfabbfc347f917216057a20c39b631842481
    Author: Martin Jaggi <m.jaggi@gmail.com>
    Date:   Fri Feb 7 03:19:38 2014 +0100
    
        minor polishing, as suggested by @pwendell
    
    commit dcd2142c164b2f602bf472bb152ad55bae82d31a
    Author: Martin Jaggi <m.jaggi@gmail.com>
    Date:   Thu Feb 6 18:04:26 2014 +0100
    
        enabling inline latex formulas with $.$
    
        same mathjax configuration as used in math.stackexchange.com
    
        sample usage in the linear algebra (SVD) documentation
    
    commit bbafafd2b497a5acaa03a140bb9de1fbb7d67ffa
    Author: Martin Jaggi <m.jaggi@gmail.com>
    Date:   Thu Feb 6 17:31:29 2014 +0100
    
        split MLlib documentation by techniques
    
        and linked from the main mllib-guide.md site
    
    commit d1c5212b93c67436543c2d8ddbbf610fdf0a26eb
    Author: Martin Jaggi <m.jaggi@gmail.com>
    Date:   Thu Feb 6 16:59:43 2014 +0100
    
        enable mathjax formula in the .md documentation files
    
        code by @shivaram
    
    commit d73948db0d9bc36296054e79fec5b1a657b4eab4
    Author: Martin Jaggi <m.jaggi@gmail.com>
    Date:   Thu Feb 6 16:57:23 2014 +0100
    
        minor update on how to compile the documentation

commit 78050805bc691a00788f6e51f23dd785ca25b227
Author: Jey Kottalam <jey@cs.berkeley.edu>
Date:   2014-02-08T20:24:08Z

    Merge pull request #454 from jey/atomic-sbt-download. Closes #454.
    
    Make sbt download an atomic operation
    
    Modifies the `sbt/sbt` script to gracefully recover when a previous invocation died in the middle of downloading the SBT jar.
    
    Author: Jey Kottalam <jey@cs.berkeley.edu>
    
    == Merge branch commits ==
    
    commit 6c600eb434a2f3e7d70b67831aeebde9b5c0f43b
    Author: Jey Kottalam <jey@cs.berkeley.edu>
    Date:   Fri Jan 17 10:43:54 2014 -0800
    
        Make sbt download an atomic operation

commit f0ce736fadbcb7642b6148ad740f4508cd7dcd4d
Author: Qiuzhuang Lian <qiuzhuang.lian@gmail.com>
Date:   2014-02-08T20:59:48Z

    Merge pull request #561 from Qiuzhuang/master. Closes #561.
    
    Kill drivers in postStop() for Worker.
    
     JIRA SPARK-1068:https://spark-project.atlassian.net/browse/SPARK-1068
    
    Author: Qiuzhuang Lian <Qiuzhuang.Lian@gmail.com>
    
    == Merge branch commits ==
    
    commit 9c19ce63637eee9369edd235979288d3d9fc9105
    Author: Qiuzhuang Lian <Qiuzhuang.Lian@gmail.com>
    Date:   Sat Feb 8 16:07:39 2014 +0800
    
        Kill drivers in postStop() for Worker.
         JIRA SPARK-1068:https://spark-project.atlassian.net/browse/SPARK-1068

commit c2341c92bb206938fd9b18e2a714e5c6de55b06d
Author: Mark Hamstra <markhamstra@gmail.com>
Date:   2014-02-09T00:00:43Z

    Merge pull request #542 from markhamstra/versionBump. Closes #542.
    
    Version number to 1.0.0-SNAPSHOT
    
    Since 0.9.0-incubating is done and out the door, we shouldn't be building 0.9.0-incubating-SNAPSHOT anymore.
    
    @pwendell
    
    Author: Mark Hamstra <markhamstra@gmail.com>
    
    == Merge branch commits ==
    
    commit 1b00a8a7c1a7f251b4bb3774b84b9e64758eaa71
    Author: Mark Hamstra <markhamstra@gmail.com>
    Date:   Wed Feb 5 09:30:32 2014 -0800
    
        Version number to 1.0.0-SNAPSHOT

commit f892da8716d614467fddcc3a1b2b589979414219
Author: Patrick Wendell <pwendell@gmail.com>
Date:   2014-02-09T07:13:34Z

    Merge pull request #565 from pwendell/dev-scripts. Closes #565.
    
    SPARK-1066: Add developer scripts to repository.
    
    These are some developer scripts I've been maintaining in a separate public repo. This patch adds them to the Spark repository so they can evolve here and are clearly accessible to all committers.
    
    I may do some small additional clean-up in this PR, but wanted to put them here in case others want to review. There are a few types of scripts here:
    
    1. A tool to merge pull requests.
    2. A script for packaging releases.
    3. A script for auditing release candidates.
    
    Author: Patrick Wendell <pwendell@gmail.com>
    
    == Merge branch commits ==
    
    commit 5d5d331d01f6fd59c2eb830f652955119b012173
    Author: Patrick Wendell <pwendell@gmail.com>
    Date:   Sat Feb 8 22:11:47 2014 -0800
    
        SPARK-1066: Add developer scripts to repository.

commit b6d40b782327188a25ded5b22790552121e5271f
Author: Patrick Wendell <pwendell@gmail.com>
Date:   2014-02-09T07:35:31Z

    Merge pull request #560 from pwendell/logging. Closes #560.
    
    [WIP] SPARK-1067: Default log4j initialization causes errors for those not using log4j
    
    To fix this - we add a check when initializing log4j.
    
    Author: Patrick Wendell <pwendell@gmail.com>
    
    == Merge branch commits ==
    
    commit ffdce513877f64b6eed6d36138c3e0003d392889
    Author: Patrick Wendell <pwendell@gmail.com>
    Date:   Fri Feb 7 15:22:29 2014 -0800
    
        Logging fix

commit 2ef37c93664d74de6d7f6144834883a4a4ef79b7
Author: jyotiska <jyotiska123@gmail.com>
Date:   2014-02-09T07:36:48Z

    Merge pull request #562 from jyotiska/master. Closes #562.
    
    Added example Python code for sort
    
    I added an example Python code for sort. Right now, PySpark has limited examples for new people willing to use the project. This example code sorts integers stored in a file. I was able to sort 5 million, 10 million and 25 million integers with this code.
    
    Author: jyotiska <jyotiska123@gmail.com>
    
    == Merge branch commits ==
    
    commit 8ad8faf6c8e02ae1cd68565d98524edf165f54df
    Author: jyotiska <jyotiska123@gmail.com>
    Date:   Sun Feb 9 11:00:41 2014 +0530
    
        Added comments in code on collect() method
    
    commit 6f98f1e313f4472a7c2207d36c4f0fbcebc95a8c
    Author: jyotiska <jyotiska123@gmail.com>
    Date:   Sat Feb 8 13:12:37 2014 +0530
    
        Upda"
"
AmplabJenkins <git@git.apache.org>,Wed"," 12 Feb 2014 05:48:09 +0000 (UTC)""",[GitHub] incubator-spark pull request: Hadoop jar name,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/522#issuecomment-34841092
  
    Can one of the admins verify this patch?


"
bijaybisht <git@git.apache.org>,"Wed, 12 Feb 2014 05:48:25 +0000 (UTC)",[GitHub] incubator-spark pull request: Hadoop jar name,dev@spark.incubator.apache.org,"Github user bijaybisht closed the pull request at:

    https://github.com/apache/incubator-spark/pull/522


"
bijaybisht <git@git.apache.org>,"Wed, 12 Feb 2014 05:52:33 +0000 (UTC)",[GitHub] incubator-spark pull request: Ported hadoopClient jar for < 1.0.1 ...,dev@spark.incubator.apache.org,"GitHub user bijaybisht opened a pull request:

    https://github.com/apache/incubator-spark/pull/584

    Ported hadoopClient jar for < 1.0.1 fix

    #522 got messed after i rewrote the branch hadoop_jar_name. So created a new one. 

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark hadoop_jar_name_on_0.9.0

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/584.patch

----
commit 1b6fb3c9de0f70ce3187a58817a244325bbbdac4
Author: Bijay Bisht <bijay.bisht@gmail.com>
Date:   2014-02-05T17:34:55Z

    Ported hadoopClient jar for < 1.0.1 fix

----


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 05:52:59 +0000 (UTC)",[GitHub] incubator-spark pull request: Ported hadoopClient jar for < 1.0.1 ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/584#issuecomment-34841291
  
    Can one of the admins verify this patch?


"
bijaybisht <git@git.apache.org>,"Wed, 12 Feb 2014 05:56:28 +0000 (UTC)",[GitHub] incubator-spark pull request: Hadoop jar name,dev@spark.incubator.apache.org,"Github user bijaybisht commented on the pull request:

    https://github.com/apache/incubator-spark/pull/522#issuecomment-34841425
  
    Pls merge #584 which is merge able now.


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 06:18:03 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding assignRanks and assignUniqueI...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/578#issuecomment-34842336
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 06:18:03 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding assignRanks and assignUniqueI...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/578#issuecomment-34842337
  
    Merged build started.


"
mengxr <git@git.apache.org>,"Wed, 12 Feb 2014 06:19:44 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding assignRanks and assignUniqueI...,dev@spark.incubator.apache.org,"Github user mengxr commented on the pull request:

    https://github.com/apache/incubator-spark/pull/578#issuecomment-34842407
  
    @rxin Thanks! Please see the updated code.


"
Mark Hamstra <mark@clearstorydata.com>,"Tue, 11 Feb 2014 22:26:44 -0800","Re: [GitHub] incubator-spark pull request: SPARK-1078: Replace
 lift-json with j...","""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Sounds good, now that we are all clear on what we mean.  Didn't mean to be
a dick, just was a little confused on what you meant.



"
aarondav <git@git.apache.org>,"Wed, 12 Feb 2014 06:32:30 +0000 (UTC)",[GitHub] incubator-spark pull request: Minor fix for ZooKeeperPersistenceEn...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/583#issuecomment-34842990
  
    Created issue [SPARK-1080](https://spark-project.atlassian.net/browse/SPARK-1080) to associate with this PR.


"
pwendell <git@git.apache.org>,"Wed, 12 Feb 2014 06:41:03 +0000 (UTC)",[GitHub] incubator-spark pull request: Ported hadoopClient jar for < 1.0.1 ...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/584#issuecomment-34843361
  
    Jenkins, test this please.


"
pwendell <git@git.apache.org>,"Wed, 12 Feb 2014 06:41:14 +0000 (UTC)",[GitHub] incubator-spark pull request: Ported hadoopClient jar for < 1.0.1 ...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/584#issuecomment-34843366
  
    LGTM pending tests.


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 06:42:59 +0000 (UTC)",[GitHub] incubator-spark pull request: Ported hadoopClient jar for < 1.0.1 ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/584#issuecomment-34843445
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 06:42:59 +0000 (UTC)",[GitHub] incubator-spark pull request: Ported hadoopClient jar for < 1.0.1 ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/584#issuecomment-34843444
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 06:43:51 +0000 (UTC)",[GitHub] incubator-spark pull request: Ported hadoopClient jar for < 1.0.1 ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/584#issuecomment-34843472
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 06:43:51 +0000 (UTC)",[GitHub] incubator-spark pull request: Ported hadoopClient jar for < 1.0.1 ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/584#issuecomment-34843473
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12686/


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 06:45:54 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding assignRanks and assignUniqueI...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/578#issuecomment-34843553
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12685/


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 06:45:54 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding assignRanks and assignUniqueI...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/578#issuecomment-34843552
  
    Merged build finished.


"
aarondav <git@git.apache.org>,"Wed, 12 Feb 2014 06:50:46 +0000 (UTC)",[GitHub] incubator-spark pull request: Minor fix for ZooKeeperPersistenceEn...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/583#issuecomment-34843731
  
    Merged into master and cherry-picked into branch-0.9 and branch-0.8.


"
aarondav <git@git.apache.org>,"Wed, 12 Feb 2014 06:59:49 +0000 (UTC)",[GitHub] incubator-spark pull request: Ported hadoopClient jar for < 1.0.1 ...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/584#issuecomment-34844075
  
    Failure was due to scalastyle failing to run (not even a failure due to a style violation). Weird.


"
aarondav <git@git.apache.org>,"Wed, 12 Feb 2014 07:03:12 +0000 (UTC)",[GitHub] incubator-spark pull request: Ported hadoopClient jar for < 1.0.1 ...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/584#issuecomment-34844226
  
    Jenkins, retest this please (can I do this?)


"
rxin <git@git.apache.org>,"Wed, 12 Feb 2014 07:04:07 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding assignRanks and assignUniqueI...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/578#issuecomment-34844284
  
    BTW - the new dev rules require us to create a JIRA for this, and update the PR description to include a link to the JIRA.


"
andyk <git@git.apache.org>,"Wed, 12 Feb 2014 07:16:16 +0000 (UTC)",[GitHub] incubator-spark pull request: Ported hadoopClient jar for < 1.0.1 ...,dev@spark.incubator.apache.org,"Github user andyk commented on the pull request:

    https://github.com/apache/incubator-spark/pull/584#issuecomment-34844857
  
    Jenkins, retest this please (can I do this?)


"
rxin <git@git.apache.org>,"Wed, 12 Feb 2014 07:17:03 +0000 (UTC)",[GitHub] incubator-spark pull request: Ported hadoopClient jar for < 1.0.1 ...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/584#issuecomment-34844882
  
    Jenkins, retest this please.


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 07:17:58 +0000 (UTC)",[GitHub] incubator-spark pull request: Ported hadoopClient jar for < 1.0.1 ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/584#issuecomment-34844919
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 07:17:58 +0000 (UTC)",[GitHub] incubator-spark pull request: Ported hadoopClient jar for < 1.0.1 ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/584#issuecomment-34844917
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 07:18:09 +0000 (UTC)",[GitHub] incubator-spark pull request: Ported hadoopClient jar for < 1.0.1 ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/584#issuecomment-34844926
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 07:18:13 +0000 (UTC)",[GitHub] incubator-spark pull request: Ported hadoopClient jar for < 1.0.1 ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/584#issuecomment-34844927
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12687/


"
rxin <git@git.apache.org>,"Wed, 12 Feb 2014 07:18:14 +0000 (UTC)",[GitHub] incubator-spark pull request: Ported hadoopClient jar for < 1.0.1 ...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/584#issuecomment-34844932
  
    It's because branch-0.9 doesn't have the scalastyle stuff ...


"
rxin <git@git.apache.org>,"Wed, 12 Feb 2014 07:18:40 +0000 (UTC)",[GitHub] incubator-spark pull request: Ported hadoopClient jar for < 1.0.1 ...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/584#issuecomment-34844952
  
    BTW Jenkins desire assertiveness. By asking him whether you could do this, he wouldn't do it ...


"
mengxr <git@git.apache.org>,"Wed, 12 Feb 2014 07:29:47 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding assignRanks and assignUniqueI...,dev@spark.incubator.apache.org,"Github user mengxr commented on the pull request:

    https://github.com/apache/incubator-spark/pull/578#issuecomment-34845415
  
    The link is at the bottom of the PR description.


"
colorant <git@git.apache.org>,"Wed, 12 Feb 2014 07:31:29 +0000 (UTC)",[GitHub] incubator-spark pull request: Minor fix for ZooKeeperPersistenceEn...,dev@spark.incubator.apache.org,"Github user colorant closed the pull request at:

    https://github.com/apache/incubator-spark/pull/583


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 07:33:10 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding zipWithIndex and zipWithUniqu...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/578#issuecomment-34845563
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 07:33:10 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding zipWithIndex and zipWithUniqu...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/578#issuecomment-34845562
  
     Merged build triggered.


"
aarondav <git@git.apache.org>,"Wed, 12 Feb 2014 07:34:01 +0000 (UTC)",[GitHub] incubator-spark pull request: Ported hadoopClient jar for < 1.0.1 ...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/584#issuecomment-34845592
  
    Jenkins, retest this please.


"
rxin <git@git.apache.org>,"Wed, 12 Feb 2014 07:46:36 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1076: zipWithIndex and zipWith...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/578#issuecomment-34846154
  
    lgtm. will merge once jenkins is happy


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 08:01:11 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1076: zipWithIndex and zipWith...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/578#issuecomment-34846846
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 08:01:11 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1076: zipWithIndex and zipWith...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/578#issuecomment-34846847
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12688/


"
rxin <git@git.apache.org>,"Wed, 12 Feb 2014 08:10:02 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1076: zipWithIndex and zipWith...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/578#issuecomment-34847266
  
    Jenkins, retest this please.


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 08:13:01 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1076: zipWithIndex and zipWith...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/578#issuecomment-34847422
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 08:13:01 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1076: zipWithIndex and zipWith...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/578#issuecomment-34847421
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 08:41:12 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1076: zipWithIndex and zipWith...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/578#issuecomment-34848917
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12689/


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 08:41:12 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1076: zipWithIndex and zipWith...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/578#issuecomment-34848915
  
    Merged build finished.


"
rxin <git@git.apache.org>,"Wed, 12 Feb 2014 08:42:38 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1076: zipWithIndex and zipWith...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/578#issuecomment-34848995
  
    Thanks. Merging this. 
    
    We might need to add Java / Python APIs too ... but that can be done in a later PR.


"
rxin <git@git.apache.org>,"Wed, 12 Feb 2014 08:51:45 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1021 sortByKey() shouldn't lau...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/379#issuecomment-34849506
  
    @ash211 mind closing this for now? We should revisit this when we find a solution.


"
rxin <git@git.apache.org>,"Wed, 12 Feb 2014 08:52:26 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1073 Keep GitHub pull request ...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/574#issuecomment-34849557
  
    @pwendell comment? Would be great to have this merged soon.


"
ScrapCodes <git@git.apache.org>,"Wed, 12 Feb 2014 08:57:09 +0000 (UTC)",[GitHub] incubator-spark pull request: Support MiMa for reporting binary co...,dev@spark.incubator.apache.org,"GitHub user ScrapCodes opened a pull request:

    https://github.com/apache/incubator-spark/pull/585

    Support MiMa for reporting binary compatibility accross versions.

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark mima-support

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/585.patch

----
commit 260e6d20a95558d53377ddc8cd1c9e3891ef83e2
Author: Prashant Sharma <prashant.s@imaginea.com>
Date:   2014-02-12T08:57:22Z

    Support MiMa for reporting binary compatibility accross versions.

----


"
rxin <git@git.apache.org>,"Wed, 12 Feb 2014 08:57:53 +0000 (UTC)",[GitHub] incubator-spark pull request: Support MiMa for reporting binary co...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-34849894
  
    Do you have an example of the report?


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 08:58:03 +0000 (UTC)",[GitHub] incubator-spark pull request: Support MiMa for reporting binary co...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-34849905
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 08:58:04 +0000 (UTC)",[GitHub] incubator-spark pull request: Support MiMa for reporting binary co...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-34849906
  
    Merged build started.


"
jyotiska <git@git.apache.org>,"Wed, 12 Feb 2014 09:02:31 +0000 (UTC)",[GitHub] incubator-spark pull request: Added extra description on ValueErro...,dev@spark.incubator.apache.org,"Github user jyotiska commented on the pull request:

    https://github.com/apache/incubator-spark/pull/581#issuecomment-34850215
  
    @JoshRosen 


"
ScrapCodes <git@git.apache.org>,"Wed, 12 Feb 2014 09:03:01 +0000 (UTC)",[GitHub] incubator-spark pull request: Support MiMa for reporting binary co...,dev@spark.incubator.apache.org,"Github user ScrapCodes commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-34850248
  
    Well in this commit itself, I have configured it for 0.9.0-incubating (which is not correct, as we don't intend version compatibility across 1.0.0 and 0.9 ). It generates a whole lot of errors. Should It be posted here ?
    
    It can be run like this.
    `sbt mima-report-binary-issues`
    
    A sample(first few lines) of errors...
    
    ```
    [info] spark-mllib: found 6 potential binary incompatibilities
    [error]  * object org.apache.spark.mllib.recommendation.MFDataGenerator does not have a correspondent in new version
    [error]    filter with: ProblemFilters.exclude[MissingClassProblem](""org.apache.spark.mllib.recommendation.MFDataGenerator$"")
    [error]  * class org.apache.spark.mllib.recommendation.MFDataGenerator does not have a correspondent in new version
    [error]    filter with: ProblemFilters.exclude[MissingClassProblem](""org.apache.spark.mllib.recommendation.MFDataGenerator"")
    [error]  * class org.apache.spark.mllib.optimization.SquaredGradient does not have a correspondent in new version
    [error]    filter with: ProblemFilters.exclude[MissingClassProblem](""org.apache.spark.mllib.optimization.SquaredGradient"")
    [error]  * method gradient()org.apache.spark.mllib.optimization.SquaredGradient in class org.apache.spark.mllib.regression.LinearRegressionWithSGD has now a different result type; was: org.apache.spark.mllib.optimization.SquaredGradient, is now: org.apache.spark.mllib.optimization.LeastSquaresGradient
    [error]    filter with: ProblemFilters.exclude[IncompatibleResultTypeProblem](""org.apache.spark.mllib.regression.LinearRegressionWithSGD.gradient"")
    [error]  * method gradient()org.apache.spark.mllib.optimization.SquaredGradient in class org.apache.spark.mllib.regression.RidgeRegressionWithSGD has now a different result type; was: org.apache.spark.mllib.optimization.SquaredGradient, is now: org.apache.spark.mllib.optimization.LeastSquaresGradient
    [error]    filter with: ProblemFilters.exclude[IncompatibleResultTypeProblem](""org.apache.spark.mllib.regression.RidgeRegressionWithSGD.gradient"")
    [error]  * method gradient()org.apache.spark.mllib.optimization.SquaredGradient in class org.apache.spark.mllib.regression.LassoWithSGD has now a different result type; was: org.apache.spark.mllib.optimization.SquaredGradient, is now: org.apache.spark.mllib.optimization.LeastSquaresGradient
    [error]    filter with: ProblemFilters.exclude[IncompatibleResultTypeProblem](""org.apache.spark.mllib.regression.LassoWithSGD.gradient"")
    [info] spark-streaming: found 0 potential binary incompatibilities
    ...(elided)
    ```


"
rxin <git@git.apache.org>,"Wed, 12 Feb 2014 09:08:15 +0000 (UTC)",[GitHub] incubator-spark pull request: Support MiMa for reporting binary co...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-34850552
  
    It's probably good to have the result sent to the dev list at some point.


"
ScrapCodes <git@git.apache.org>,"Wed, 12 Feb 2014 09:10:33 +0000 (UTC)",[GitHub] incubator-spark pull request: Support MiMa for reporting binary co...,dev@spark.incubator.apache.org,"Github user ScrapCodes commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-34850725
  
    hm.. Should it be run on 0.9.1-SNAPSHOT and 0.9.0 or 1.0.0-SNAPSHOT and 0.9.0 (which is whole lot of errors.) ?


"
ash211 <git@git.apache.org>,"Wed, 12 Feb 2014 09:11:34 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1021 sortByKey() shouldn't lau...,dev@spark.incubator.apache.org,"Github user ash211 commented on the pull request:

    https://github.com/apache/incubator-spark/pull/379#issuecomment-34850790
  
    Yep will close this PR since we don't have a future direction but leave the Jira open since it's still affecting me.


"
ash211 <git@git.apache.org>,"Wed, 12 Feb 2014 09:11:35 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1021 sortByKey() shouldn't lau...,dev@spark.incubator.apache.org,"Github user ash211 closed the pull request at:

    https://github.com/apache/incubator-spark/pull/379


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 09:26:20 +0000 (UTC)",[GitHub] incubator-spark pull request: Support MiMa for reporting binary co...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-34851732
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 09:26:20 +0000 (UTC)",[GitHub] incubator-spark pull request: Support MiMa for reporting binary co...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-34851733
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12690/


"
mengxr <git@git.apache.org>,"Wed, 12 Feb 2014 09:28:44 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1076: zipWithIndex and zipWith...,dev@spark.incubator.apache.org,"Github user mengxr closed the pull request at:

    https://github.com/apache/incubator-spark/pull/578


"
ScrapCodes <git@git.apache.org>,"Wed, 12 Feb 2014 09:44:23 +0000 (UTC)",[GitHub] incubator-spark pull request: Support MiMa for reporting binary co...,dev@spark.incubator.apache.org,"Github user ScrapCodes commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-34852986
  
    A gist of report generated for branch-0.9  and v0.9.0 with associated patch. 
    https://gist.github.com/ScrapCodes/c4bdf6b73caf149319ba


"
srowen <git@git.apache.org>,"Wed, 12 Feb 2014 13:29:31 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1084. Fix most build warnings,dev@spark.incubator.apache.org,"GitHub user srowen opened a pull request:

    https://github.com/apache/incubator-spark/pull/586

    SPARK-1084. Fix most build warnings

    https://spark-project.atlassian.net/browse/SPARK-1084 
    
    I hope another boring tidy-up JIRA might be welcome. I'd like to fix most of the warnings that appear during build, so that developers don't become accustomed to them. The accompanying pull request contains a number of commits to quash most warnings observed through the `mvn` and `sbt` builds, although not all of them.
    
    
    ### FIXED!
    
    `[WARNING] Parameter tasks is deprecated, use target instead`
    
    Just a matter of updating <tasks> -> <target> in inline Ant scripts.
    
    
    `WARNING: -p has been deprecated and will be reused for a different (but still very cool) purpose in ScalaTest 2.0. Please change all uses of -p to -R.`
    
    Goes away with updating scalatest plugin -> 1.0-RC2
    
    
    ```
    [WARNING] Note: /Users/srowen/Documents/incubator-spark/core/src/test/scala/org/apache/spark/JavaAPISuite.java uses unchecked or unsafe operations.
    [WARNING] Note: Recompile with -Xlint:unchecked for details.
    ```
    
    Mostly `@SuppressWarnings(""unchecked"")` but needed a few more things to reveal the warning source: <fork>true</fork> (also needd for <maxmem>) and version 3.1 of the plugin. In a few cases some declaration changes were appropriate to avoid warnings.
    
    
    ```
    /Users/srowen/Documents/incubator-spark/core/src/main/scala/org/apache/spark/util/IndestructibleActorSystem.scala:25: warning: Could not find any member to link for ""akka.actor.ActorSystem"".
    /**
    ^
    ```
    
    Getting several scaladoc errors like this and I'm not clear why it can't find the type -- outside its module? Remove the links as they're evidently not linking anyway?
    
    
    ```
    /Users/srowen/Documents/incubator-spark/repl/src/main/scala/org/apache/spark/repl/SparkIMain.scala:86: warning: Variable eval undefined in comment for class SparkIMain in class SparkIMain
    ```
    
    $ has to be escaped as \$ in scaladoc, apparently
    
    
    ```
    [WARNING] 'dependencyManagement.dependencies.dependency.exclusions.exclusion.artifactId' for org.apache.hadoop:hadoop-yarn-client:jar with value '*' does not match a valid id pattern. @ org.apache.spark:spark-parent:1.0.0-incubating-SNAPSHOT, /Users/srowen/Documents/incubator-spark/pom.xml, line 494, column 25
    ```
    
    This one might need review.
    
    This is valid Maven syntax, but, Maven still warns on it. I wanted to see if we can do without it. 
    
    These are trying to exclude:
    - `org.codehaus.jackson`
    - `org.sonatype.sisu.inject`
    - `org.xerial.snappy`
    
    `org.sonatype.sisu.inject` doesn't actually seem to be a dependency anyway. `org.xerial.snappy` is used by dependencies but the version seems to match anyway (1.0.5).
    
    `org.codehaus.jackson` was intended to exclude 1.8.8, since Spark streaming wants 1.9.11 directly. But the exclusion is in the wrong place if so, since Spark depends straight on Avro, which is what brings in 1.8.8, still. (hadoop-client 1.0.4 includes Jackson 1.0.1, so that needs an exclusion, but the other Hadoop modules don't.)
    
    HBase depends on 1.8.8 but figured it was intentional to leave that as it would not collide with Spark streaming. (?)
    
    (I understand this varies by Hadoop version but confirmed this is all the same for 1.0.4, 0.23.7, 2.2.0.)
    
    
    
    ### NOT FIXED
    
    ```
    [warn] /Users/srowen/Documents/incubator-spark/streaming/src/test/scala/org/apache/spark/streaming/InputStreamsSuite.scala:305: method connect in class IOManager is deprecated: use the new implementation in package akka.io instead
    [warn]   override def preStart = IOManager(context.system).connect(new InetSocketAddress(port))
    ```
    
    Not confident enough to fix this.
    
    
    `[WARNING] there were 6 feature warning(s); re-run with -feature for details`
    
    Don't know enough Scala to address these, yet.
    
    
    `[WARNING] We have a duplicate org/yaml/snakeyaml/scanner/ScannerImpl$Chomping.class in /Users/srowen/.m2/repository/org/yaml/snakeyaml/1.6/snakeyaml-1.6.jar`
    
    Probably addressable by being more careful about how binaries are packed though this appear to be ignorable; two identical copies of the class are colliding.
    
    
    `[WARNING] Zinc server is not available at port 3030 - reverting to normal incremental compile`
    and
    `[WARNING] JAR will be empty - no content was marked for inclusion!`
    
    Apparently harmless warnings, but I don't know how to disable them.


You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark FixWarnings

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/586.patch

----
commit 7f6879c601c929c884b4e1fcaf155b1740dc8881
Author: Sean Owen <sowen@cloudera.com>
Date:   2014-02-11T14:37:03Z

    Replace deprecated Ant <tasks> with <target>

commit c4f37ebd4c28fd46f1d4fff2613cfea8ac9fb2aa
Author: Sean Owen <sowen@cloudera.com>
Date:   2014-02-11T14:37:58Z

    Fix unescaped $, bad tag in scaladoc

commit c5befe1c47923a43fc11d8c7fa10f223eec4257c
Author: Sean Owen <sowen@cloudera.com>
Date:   2014-02-11T14:38:23Z

    Remove dead scaladoc links

commit c88a983ea686de4381f1f367a9f49faedaf7fcc1
Author: Sean Owen <sowen@cloudera.com>
Date:   2014-02-11T14:38:57Z

    Suppress and/or fix unchecked javac warnings

commit 5d86e49b7c8d6473ecb90248c022d2b59791254b
Author: Sean Owen <sowen@cloudera.com>
Date:   2014-02-11T14:39:48Z

    Fix scaladoc invocation warning, and enable javac warnings properly, with plugin config updates

commit 4093ed0e947c83f258106a07e0377e70c03535d8
Author: Sean Owen <sowen@cloudera.com>
Date:   2014-02-11T18:55:57Z

    Fix (?) Jackson exclusion, and remove apparently unneeded exclusions, all of which were causing build warnings

----


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 13:33:00 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1084. Fix most build warnings,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/586#issuecomment-34868554
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 13:33:10 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1084. Fix most build warnings,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/586#issuecomment-34868552
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 13:34:03 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1084. Fix most build warnings,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/586#issuecomment-34868638
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 13:34:03 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1084. Fix most build warnings,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/586#issuecomment-34868641
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12691/


"
khayyatzy <git@git.apache.org>,"Wed, 12 Feb 2014 13:43:26 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding RDD unique self cross product,dev@spark.incubator.apache.org,"GitHub user khayyatzy opened a pull request:

    https://github.com/apache/incubator-spark/pull/587

    Adding RDD unique self cross product

    Hi,
    
    I am using Spark in some data analysis project and I frequently requires the unique self cross product for a single RDD. Since I am using Spark's Java API, I added the new function ""selfCartesian"" JavaRDDLike.scala. I also modify RDD.scala where it calls function ""CartesianRDD2"". ""CartesianRDD2"" Has similar implementation to ""CartesianRDD"", where it only returns elements (a, b) if a.index <= b.index. I have been using this Spark's modification for couple of months and the function always return correct results
    
    I hope this new small feature would be favorable for other Spark users.
    
    Regards,
    Zuhair Khayyat

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark master

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/587.patch

----
commit 82a80ef264ad15fa706eb566691470308b30f63a
Author: Zuhair Khayyat <zuhair.khayyat@gmail.com>
Date:   2014-02-12T12:46:05Z

    Adding unique self cross product of in a single RDD

commit fb8ad2eee1c4ce175f3cf4227492bbc9f3502db3
Author: Zuhair Khayyat <zuhair.khayyat@gmail.com>
Date:   2014-02-12T12:54:11Z

    changing ClassManifest to ClassTag in unique self product classes

commit ea02451bb11274f55be3706ea86e21e43e54fd35
Author: Zuhair Khayyat <zuhair.khayyat@gmail.com>
Date:   2014-02-12T12:59:16Z

    adding import scala.reflect.ClassTag to CartesianRDD2.scala

commit 8f81706f374773aeea0d608b6baa9d2164c8f364
Author: Zuhair Khayyat <zuhair.khayyat@gmail.com>
Date:   2014-02-12T13:29:27Z

    removing unwanted text from CartesianRDD2.scala

----


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 13:48:02 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1084. Fix most build warnings,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/586#issuecomment-34869799
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 13:48:02 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1084. Fix most build warnings,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/586#issuecomment-34869797
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 13:48:01 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding RDD unique self cross product,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/587#issuecomment-34869793
  
    Can one of the admins verify this patch?


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 14:15:48 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1084. Fix most build warnings,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/586#issuecomment-34872114
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 14:15:48 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1084. Fix most build warnings,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/586#issuecomment-34872115
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12692/


"
Will Benton <willb@redhat.com>,"Wed, 12 Feb 2014 11:12:13 -0500 (EST)",Re: proposal: replace lift-json with spray-json,dev@spark.incubator.apache.org,"
There are other actual and potential advantages, though; here are a few:

1.  Based on some simple timing runs I did, json4s-jackson is faster all around when running warm (i.e. on subsequent timing runs in the same VM or timing runs with enough iterations to last for more than a few seconds), slightly slower when running cold on very small parsing tasks, and significantly (~10x) faster on large parsing tasks whether cold or warm.  The knee in the cold lift-json performance curve is somewhere between 2kb and 50kb of JSON source text.  json4s-jackson is nominally faster cold with a 12kb file, 40% faster with a 50kb file, 2.6x faster with a 500kb file and 10x faster with files ranging from 4-20mb.  Given how Spark uses JSON at the moment, the improved large-file parsing performance seems unlikely to be a huge practical advantage for json4s-jackson, but it's worth noting.
2.  The release schedule of json4s isn't coupled to the release schedule of a larger project.
3.  json4s is intended to provide a uniform interface to Scala JSON libraries, and it provides multiple backends, which offers potential flexibility in the future.  (To be fair, this interface is heavily based on the one provided by Lift, so it would be only slightly more work to go from lift-json to json4s, as my patch does, as it would be to switch between json4s backends.)

Again, this change is primarily motivated by a desire to make life easier for downstream packagers, but there is no obvious downside (beyond the downsides inherent in changing library dependencies) and several minor advantages.


best,
wb

"
tgravescs <git@git.apache.org>,"Wed, 12 Feb 2014 16:12:36 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1053. Don't require SPARK_YARN...,dev@spark.incubator.apache.org,"Github user tgravescs commented on the pull request:

    https://github.com/apache/incubator-spark/pull/553#issuecomment-34884178
  
    So what does the user see in the yarn-client batch mode if SPARK_YARN_APP_JAR isn't specified?


"
mrt <git@git.apache.org>,"Wed, 12 Feb 2014 17:31:44 +0000 (UTC)","[GitHub] incubator-spark pull request: Upgraded sbt (0.12.4 to 0.13.1), sbt...",dev@spark.incubator.apache.org,"Github user mrt commented on the pull request:

    https://github.com/apache/incubator-spark/pull/266#issuecomment-34893474
  
    Please close this PR. Thanks for integrating them.


"
Paul Brown <prb@mult.ifario.us>,"Wed, 12 Feb 2014 09:47:11 -0800",Re: proposal: replace lift-json with spray-json,dev@spark.incubator.apache.org,"And, with my FasterXML hat on, if you ask, you'll find the Jackson folks
will turn around issues quickly.  FWIW, there is a full-suite Jackson 2.3.2
release rolling right up if you wait a couple of days to pull that in.

-- Paul

â€”
prb@mult.ifario.us | Multifarious, Inc. | http://mult.ifario.us/



on
e
t
ra
r
b
th
nd
on
om
"
markhamstra <git@git.apache.org>,"Wed, 12 Feb 2014 17:55:18 +0000 (UTC)","[GitHub] incubator-spark pull request: Upgraded sbt (0.12.4 to 0.13.1), sbt...",dev@spark.incubator.apache.org,"Github user markhamstra commented on the pull request:

    https://github.com/apache/incubator-spark/pull/266#issuecomment-34895896
  
    It's your PR, so there should be a big Close button when you view it on github (but not when I do.)


"
CodingCat <git@git.apache.org>,"Wed, 12 Feb 2014 18:25:58 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1041] remove dead code in sta...,dev@spark.incubator.apache.org,"GitHub user CodingCat opened a pull request:

    https://github.com/apache/incubator-spark/pull/588

    [SPARK-1041] remove dead code in start script, remind user to set that in spark-env.sh

    the lines in start-master.sh and start-slave.sh no longer work
    
    in ec2, the host name has changed, e.g.
    
    ubuntu@ip-172-31-36-93:~$ hostname
    ip-172-31-36-93
    
    also, the URL to fetch public DNS name also changed, e.g.
    
    ubuntu@ip-172-31-36-93:~$ wget -q -O - http://instance-data.ec2.internal/latest/meta-data/public-hostname
    ubuntu@ip-172-31-36-93:~$  (returns nothing)
    
    since we have spark-ec2 project, we don't need to have such ec2-specific lines here, instead, user only need to set in spark-env.sh
    


You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark deadcode_in_sbin

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/588.patch

----
commit e4236e0c717e1ce7b6f91a83ce4c0cb7a4ae8046
Author: CodingCat <zhunansjtu@gmail.com>
Date:   2014-02-12T18:24:34Z

    remove dead code in start script, remind user set that in spark-env.sh

----


"
CodingCat <git@git.apache.org>,"Wed, 12 Feb 2014 18:26:47 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1041] ec2-related lines in st...,dev@spark.incubator.apache.org,"Github user CodingCat closed the pull request at:

    https://github.com/apache/incubator-spark/pull/391


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 18:27:59 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1041] remove dead code in sta...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/588#issuecomment-34899334
  
    Can one of the admins verify this patch?


"
Aaron Davidson <ilikerps@gmail.com>,"Wed, 12 Feb 2014 10:38:04 -0800",Re: proposal: replace lift-json with spray-json,dev@spark.incubator.apache.org,"Will, thanks for the clarifications. I think Spark's main use-case is
""warm, small inputs"" right now, but the change seems reasonable to me
nevertheless.

Paul, do you know if there are any issues relevant to Spark that we need
from 2.3.2? We would also have to wait for json4s to release a new version
that depends on 2.3.2, or else pull it in ourselves.



"
mengxr <git@git.apache.org>,"Wed, 12 Feb 2014 18:39:29 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1076: Convert Int to Long to a...,dev@spark.incubator.apache.org,"GitHub user mengxr opened a pull request:

    https://github.com/apache/incubator-spark/pull/589

    SPARK-1076: Convert Int to Long to avoid overflow

    Patch for PR #578.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark index

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/589.patch

----
commit 98c435eebd74dcfe57212ff5d240b8a00a5d0bdf
Author: Xiangrui Meng <meng@databricks.com>
Date:   2014-02-12T18:34:02Z

    cast Int to Long to avoid Int overflow

----


"
aarondav <git@git.apache.org>,"Wed, 12 Feb 2014 18:42:31 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1076: Convert Int to Long to a...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/589#issuecomment-34900982
  
    LGTM


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 18:43:04 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1076: Convert Int to Long to a...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/589#issuecomment-34901047
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 18:43:04 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1076: Convert Int to Long to a...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/589#issuecomment-34901044
  
     Merged build triggered.


"
mengxr <git@git.apache.org>,"Wed, 12 Feb 2014 18:44:44 +0000 (UTC)",[GitHub] incubator-spark pull request: MLI-2: Add k-fold cross validation t...,dev@spark.incubator.apache.org,"Github user mengxr commented on the pull request:

    https://github.com/apache/incubator-spark/pull/572#issuecomment-34901227
  


"
rxin <git@git.apache.org>,"Wed, 12 Feb 2014 18:47:15 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1076: Convert Int to Long to a...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/589#issuecomment-34901501
  
    Thanks. Merged.


"
holdenk <git@git.apache.org>,"Wed, 12 Feb 2014 18:50:47 +0000 (UTC)",[GitHub] incubator-spark pull request: MLI-2: Add k-fold cross validation t...,dev@spark.incubator.apache.org,"Github user holdenk commented on the pull request:

    https://github.com/apache/incubator-spark/pull/572#issuecomment-34901891
  
    Sounds reasonable.
    
    
    
    > @holdenk <https://github.com/holdenk> How about splitting this PR into
    > BernoulliSampler, and the other contains the crossValidate function which
    > we can discuss more.
    >
    > --
    > Reply to this email directly or view it on GitHub<https://github.com/apache/incubator-spark/pull/572#issuecomment-34901227>
    > .
    >
    
    
    
    -- 
    Cell : 425-233-8271


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 18:53:00 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1076: Convert Int to Long to a...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/589#issuecomment-34902192
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 18:53:00 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1076: Convert Int to Long to a...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/589#issuecomment-34902193
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 19:11:09 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1076: Convert Int to Long to a...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/589#issuecomment-34904374
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 19:11:09 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1076: Convert Int to Long to a...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/589#issuecomment-34904375
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12693/


"
aarondav <git@git.apache.org>,"Wed, 12 Feb 2014 19:18:16 +0000 (UTC)",[GitHub] incubator-spark pull request: fix for https://spark-project.atlass...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/568#issuecomment-34905299
  
    Looks good to me, will merge as soon as comments have been addressed.


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 19:21:19 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1076: Convert Int to Long to a...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/589#issuecomment-34905625
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 19:21:19 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1076: Convert Int to Long to a...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/589#issuecomment-34905626
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12694/


"
aarondav <git@git.apache.org>,"Wed, 12 Feb 2014 19:23:06 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1076: Convert Int to Long to a...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/589#issuecomment-34905810
  
    Uh-oh, @rxin did your merge include the second commit?


"
mengxr <git@git.apache.org>,"Wed, 12 Feb 2014 19:25:23 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1076: Convert Int to Long to a...,dev@spark.incubator.apache.org,"Github user mengxr commented on the pull request:

    https://github.com/apache/incubator-spark/pull/589#issuecomment-34906057
  
    I will make another PR for the second commit. Next time we should leave the PR open for a day or half before merge.


"
Paul Brown <prb@mult.ifario.us>,"Wed, 12 Feb 2014 11:29:58 -0800",Re: proposal: replace lift-json with spray-json,dev@spark.incubator.apache.org,"Hi, Aaron --

I can't speak to issues relevant to Spark, but it looks like json4s is
currently using the Jackson Scala module 2.1.3 and Scala 2.9.2.  There have
been quite a few significant changes to the Scala module and underpinnings
between the 2.1.x and 2.3.x series, but I can't speak to how that interacts
with json4s.  Many of those changes are convenience for direct usage of the
Jackson Scala module in binding case classes transparently, but you
wouldn't need or benefit from those through the json4s API.  (FWIW, we use
Jackson Scala 2.3.2 in our Spark jobs to bind lines of JSON from text files
to case classes.)

I'll reach out to json4s and see if I can get them to update to the 2.3.x
Jackson series and Scala 2.10, but I think it makes sense to for Spark to
just use the released version and then update when a json4s release is
available.

Best.
-- Paul

â€”
prb@mult.ifario.us | Multifarious, Inc. | http://mult.ifario.us/


:

n
s
VM
n
d
le
y
o
en
e
r
"
sryza <git@git.apache.org>,"Wed, 12 Feb 2014 19:30:59 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1053. Don't require SPARK_YARN...,dev@spark.incubator.apache.org,"Github user sryza commented on the pull request:

    https://github.com/apache/incubator-spark/pull/553#issuecomment-34906646
  
    @tgravescs I should have tried this - it looks like it actually works fine when SPARK_YARN_APP_JAR isn't specified.  The client must be serving the jar in the typical Spark way.  Are there any situations I'm not thinking of where this wouldn't hold?
    
    Either way, I'll need to update the doc.


"
Aaron Davidson <ilikerps@gmail.com>,"Wed, 12 Feb 2014 11:35:07 -0800",Re: proposal: replace lift-json with spray-json,dev@spark.incubator.apache.org,"The version of json4s we're using (3.2.6 in the 2.10 branch) does seem to
depend on Jackson 2.3.0 and Scala 2.10.0:
http://mvnrepository.com/artifact/org.json4s/json4s-jackson_2.10/3.2.6



"
Pascal Voitot Dev <pascal.voitot.dev@gmail.com>,"Wed, 12 Feb 2014 20:36:10 +0100",Re: proposal: replace lift-json with spray-json,dev <dev@spark.incubator.apache.org>,"I have one question : isn't it possible to abstract a bit and not depend on
a given json implementation as this is still a moving target?

Regards
Pascal
Le 12 févr. 2014 20:30, ""Paul Brown"" <prb@mult.ifario.us> a écrit :

ve
s
ts
he
e
es
d
.
:
f
e.
he
y
on
ft
to
e
e
e
r
e
ON
l
"
aarondav <git@git.apache.org>,"Wed, 12 Feb 2014 19:40:40 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1076: Convert Int to Long to a...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/589#issuecomment-34908063
  
    Timely merges may not have been the problem -- the title of this PR only mentions the change you had already made :)
    
    Please signal if your PR is not ready to be reviewed in full.


"
Aaron Davidson <ilikerps@gmail.com>,"Wed, 12 Feb 2014 11:44:37 -0800",Re: proposal: replace lift-json with spray-json,dev@spark.incubator.apache.org,"As far as I understand, json4s attempts to do this by providing a single
interface for users where the underlying JSON parser is pluggable.
Currently, there are lift-json (called ""native"") and jackson plugins, and
we are using the latter in the current PR.



on
.x
to
on
re
d
it
 a
d
r
b
ON
y
to
e
d
"
mengxr <git@git.apache.org>,"Wed, 12 Feb 2014 19:46:34 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1076: Convert Int to Long to a...,dev@spark.incubator.apache.org,"Github user mengxr closed the pull request at:

    https://github.com/apache/incubator-spark/pull/589


"
Paul Brown <prb@mult.ifario.us>,"Wed, 12 Feb 2014 11:47:09 -0800",Re: proposal: replace lift-json with spray-json,dev@spark.incubator.apache.org,"Yup; you're right:

https://github.com/json4s/json4s/blob/3.2.6_2.10/project/Dependencies.scala

The older deps are only in use in examples/benchmarking.  All good.


â€”
prb@mult.ifario.us | Multifarious, Inc. | http://mult.ifario.us/


:

.x
to
on
re
d
it
 a
d
r
b
ON
y
to
e
d
"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 20:18:06 +0000 (UTC)",[GitHub] incubator-spark pull request: MLI-2: Start adding k-fold cross val...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/572#issuecomment-34912556
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 20:18:06 +0000 (UTC)",[GitHub] incubator-spark pull request: MLI-2: Start adding k-fold cross val...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/572#issuecomment-34912554
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 20:46:37 +0000 (UTC)",[GitHub] incubator-spark pull request: MLI-2: Start adding k-fold cross val...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/572#issuecomment-34915737
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 20:46:37 +0000 (UTC)",[GitHub] incubator-spark pull request: MLI-2: Start adding k-fold cross val...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/572#issuecomment-34915739
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12695/


"
rxin <git@git.apache.org>,"Wed, 12 Feb 2014 20:52:00 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding RDD unique self cross product,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/587#issuecomment-34916300
  
    Thanks for submitting this. Just curious, what is the advantage of this over rdd.cartesian(rdd), i.e. just use cartesian to join itself?


"
aarondav <git@git.apache.org>,"Wed, 12 Feb 2014 21:18:23 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1084. Fix most build warnings,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/586#issuecomment-34918907
  
    Thanks for this! Did a quick pass and left some comments.


"
Aaron Davidson <ilikerps@gmail.com>,"Wed, 12 Feb 2014 13:29:09 -0800",Re: Proposal: Clarifying minor points of Scala style,dev@spark.incubator.apache.org,"Regarding styling: as we all know, constructor parameters in Scala are
automatically upgraded to ""private val"" fields if they're referenced
outside of the constructor. For instance:
class Foo(a: Int, b: Int) {
  def getB = b
}

In the above case, 'b' is actually a ""private val"" field of Foo, whereas
'a' never left the constructor's stack.

Is this usage kosher, or should we prefer that such fields are marked
""private val"" explicitly if they're intended to be used outside of the
constructor? This behavior is often harmless, but it has some evil
implications with regards to serialization and shadowing during
inheritance. It's especially concerning when a field starts out as a
constructor parameter and during a later patch becomes a field, and now
we're serializing it.



"
aarondav <git@git.apache.org>,"Wed, 12 Feb 2014 21:54:35 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1078: Replace lift-json with j...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/582#issuecomment-34922638
  
    This looks good to me, only significant change is correcting the thrown exception. I am somewhat underwhelmed by json4s's documentation. For instance, this file includes absolutely no docs, despite being of paramount importance to the library: https://github.com/json4s/json4s/blob/master/core/src/main/scala/org/json4s/JsonMethods.scala
    
    Will also give more time for other potential reviewers and any further discussion on the dev list..


"
rxin <git@git.apache.org>,"Wed, 12 Feb 2014 21:57:43 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1085: Fix Jenkins pull request...,dev@spark.incubator.apache.org,"GitHub user rxin opened a pull request:

    https://github.com/apache/incubator-spark/pull/590

    SPARK-1085: Fix Jenkins pull request builder for branch-0.9 (scalastyle command not found)

    Added a dummy scalastyle task to sbt.
    
    https://spark-project.atlassian.net/browse/SPARK-1085

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark scalastyle

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/590.patch

----
commit d0889bdeb71fbad586fb24fec526bb0df944708a
Author: Reynold Xin <rxin@apache.org>
Date:   2014-02-12T21:56:26Z

    SPARK-1085: Fix Jenkins pull request builder for branch-0.9 (scalastyle command not found)

----


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 21:58:03 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1085: Fix Jenkins pull request...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/590#issuecomment-34923008
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 21:58:04 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1085: Fix Jenkins pull request...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/590#issuecomment-34923009
  
    Merged build started.


"
rxin <git@git.apache.org>,"Wed, 12 Feb 2014 21:58:06 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1085: Fix Jenkins pull request...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/590#issuecomment-34923019
  
    @aarondav @marmbrus 
    
    i'm no sbt expert


"
marmbrus <git@git.apache.org>,"Wed, 12 Feb 2014 21:59:55 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1085: Fix Jenkins pull request...,dev@spark.incubator.apache.org,"Github user marmbrus commented on the pull request:

    https://github.com/apache/incubator-spark/pull/590#issuecomment-34923209
  
    LGTM if jenkins likes it :)


"
rxin <git@git.apache.org>,"Wed, 12 Feb 2014 22:00:27 +0000 (UTC)",[GitHub] incubator-spark pull request: Ported hadoopClient jar for < 1.0.1 ...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/584#issuecomment-34923271
  
    We can test this once #590 is in.


"
Mark Hamstra <mark@clearstorydata.com>,"Wed, 12 Feb 2014 14:02:18 -0800",Re: Proposal: Clarifying minor points of Scala style,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","It's actually a little more complicated than that, mostly due to the
difference between private and private[this].  Allow me to demonstrate:

package dummy

class Foo1(a: Int, b: Int) {
  private val c = a + b
}

class Foo2(a: Int, b: Int) {
  private[this] val c = a + b
}

class Foo3(a: Int, b: Int) {
  def getB = b
}

class Foo4(a: Int, private val b: Int) {
  def getB = b
}

class Foo5(a: Int, private[this] val b: Int) {
  def getB = b
}


scalac Foo.scala

Now let's look at what we've got using `javap -c -private` on the resulting
classes:

public class dummy.Foo1 {

  private final int c;


  private int c();

    Code:

       0: aload_0

       1: getfield      #13                 // Field c:I

       4: ireturn


  public dummy.Foo1(int, int);

    Code:

       0: aload_0

       1: invokespecial #20                 // Method
java/lang/Object.""<init>"":()V

       4: aload_0

       5: iload_1

       6: iload_2

       7: iadd

       8: putfield      #13                 // Field c:I

      11: return

}

...compare that to Foo2 and note how `c` isn't a plain field in Foo1, but
has an accessor method `private int c()`:


public class dummy.Foo2 {

  private final int c;


  public dummy.Foo2(int, int);

    Code:

       0: aload_0

       1: invokespecial #15                 // Method
java/lang/Object.""<init>"":()V

       4: aload_0

       5: iload_1

       6: iload_2

       7: iadd

       8: putfield      #17                 // Field c:I

      11: return

}


Okay?  So you really need private[this] if you want to generate plain Java
fields.

Now let's look at what happens when you close over an unannotated
constructor parameter:


public class dummy.Foo3 {

  private final int b;


  public int getB();

    Code:

       0: aload_0

       1: getfield      #14                 // Field b:I

       4: ireturn


  public dummy.Foo3(int, int);

    Code:

       0: aload_0

       1: iload_2

       2: putfield      #14                 // Field b:I

       5: aload_0

       6: invokespecial #21                 // Method
java/lang/Object.""<init>"":()V

       9: return

}


...which is not the same as what you get if you annotate `b` with `private
val` -- notice `private int b()`:


public class dummy.Foo4 {

  private final int b;


  private int b();

    Code:

       0: aload_0

       1: getfield      #13                 // Field b:I

       4: ireturn


  public int getB();

    Code:

       0: aload_0

       1: invokespecial #18                 // Method b:()I

       4: ireturn


  public dummy.Foo4(int, int);

    Code:

       0: aload_0

       1: iload_2

       2: putfield      #13                 // Field b:I

       5: aload_0

       6: invokespecial #23                 // Method
java/lang/Object.""<init>"":()V

       9: return

}


...however, compare Foo3 to what you get when `b` is `private[this] val`:


public class dummy.Foo5 {

  private final int b;


  public int getB();

    Code:

       0: aload_0

       1: getfield      #14                 // Field b:I

       4: ireturn


  public dummy.Foo5(int, int);

    Code:

       0: aload_0

       1: iload_2

       2: putfield      #14                 // Field b:I

       5: aload_0

       6: invokespecial #21                 // Method
java/lang/Object.""<init>"":()V

       9: return

}





"
Aaron Davidson <ilikerps@gmail.com>,"Wed, 12 Feb 2014 14:14:50 -0800",Re: Proposal: Clarifying minor points of Scala style,dev@spark.incubator.apache.org,"Thanks for the clarification, Mark. So ""private val"" generates the Scala
getter whereas ""private[this] val"" does not, and the field-ified
constructor parameter mimics ""private[this] val"".

However, the distinction between those two seems less important than the
distinction between a constructor parameter and a field. In particular, the
existence of the Scala private getter won't affect serialization and makes
the shadowing explicit. Other than some weird uses of reflection, I'm not
sure how the difference could cause an issue.



"
mengxr <git@git.apache.org>,"Wed, 12 Feb 2014 22:17:02 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1076: [Fix #578] add @transien...,dev@spark.incubator.apache.org,"GitHub user mengxr opened a pull request:

    https://github.com/apache/incubator-spark/pull/591

    SPARK-1076: [Fix #578] add @transient to some vals

    I'll try to be more careful next time.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark transient-new

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/591.patch

----
commit 2b4f044be8d198ce70c69a7494cc7adfa95f2e4f
Author: Xiangrui Meng <meng@databricks.com>
Date:   2014-02-12T18:50:49Z

    add @transient to prev in ZippedWithIndexRDD
    add @transient to seed in PartitionwiseSampledRDD

----


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 22:17:58 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1076: [Fix #578] add @transien...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/591#issuecomment-34925086
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 22:17:58 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1076: [Fix #578] add @transien...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/591#issuecomment-34925087
  
    Merged build started.


"
markhamstra <git@git.apache.org>,"Wed, 12 Feb 2014 22:23:56 +0000 (UTC)",[GitHub] incubator-spark pull request: streaming iterable,dev@spark.incubator.apache.org,"Github user markhamstra commented on the pull request:

    https://github.com/apache/incubator-spark/pull/421#issuecomment-34925691
  
    @rxin Any thoughts on how to do something like: 
    ```scala
    val futureAction = rdd.toIterator.nextAsync
    futureAction.cancel()
    ```


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 22:25:25 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1085: Fix Jenkins pull request...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/590#issuecomment-34925842
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 22:25:25 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1085: Fix Jenkins pull request...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/590#issuecomment-34925843
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12696/


"
aarondav <git@git.apache.org>,"Wed, 12 Feb 2014 22:27:53 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1085: Fix Jenkins pull request...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/590#issuecomment-34926093
  
    Thanks, merged!


"
Mark Hamstra <mark@clearstorydata.com>,"Wed, 12 Feb 2014 14:34:41 -0800",Re: Proposal: Clarifying minor points of Scala style,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","There is at least potential for performance difference with the extra level
of indirection of `private val` compared to `private[this] val`; so from
that perspective, `private[this]` or closing over an unannotated
the other hand, having a field secretly spring into existence as soon an
hand, annotating constructor parameters with `private` or `private[this]`
is not the expected Scala way (i.e. unannotated parameters) of accessing
those parameters privately within their respective classes.

In other words, I don't have a good answer, just more things to consider.




"
Aaron Davidson <ilikerps@gmail.com>,"Wed, 12 Feb 2014 14:44:38 -0800",Re: Proposal: Clarifying minor points of Scala style,dev@spark.incubator.apache.org,"I believe the performance implications are negligible due to the JIT. If
that getter is not inlined at runtime, I will eat a shoe.

I think the main question is still whether we want to avoid fields secretly
springing into existence or we want the significantly more concise syntax
of unannotated parameters. If we do want the former, then ""private"" versus
""private[this]"" is the next question.



"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 22:45:55 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1076: [Fix #578] add @transien...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/591#issuecomment-34927811
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12697/


"
AmplabJenkins <git@git.apache.org>,"Wed, 12 Feb 2014 22:45:55 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1076: [Fix #578] add @transien...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/591#issuecomment-34927810
  
    Merged build finished.


"
Mark Hamstra <mark@clearstorydata.com>,"Wed, 12 Feb 2014 14:50:56 -0800",Re: Proposal: Clarifying minor points of Scala style,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Usually inlined, not always.  From the infamous Coda Hale rant:

4. Always use private[this]. Doing so avoids turning simple field access into an


"
willb <git@git.apache.org>,"Wed, 12 Feb 2014 23:30:01 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1078: Replace lift-json with j...,dev@spark.incubator.apache.org,"Github user willb commented on the pull request:

    https://github.com/apache/incubator-spark/pull/582#issuecomment-34931526
  
    Thanks for the review, Aaron!  After others have weighed in, I'll amend my branch to catch the right exception (and correct my email address).


"
Henry Saputra <henry.saputra@gmail.com>,"Wed, 12 Feb 2014 15:39:15 -0800",Re: [TODO] Document the release process for Apache Spark,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Sorry, missed this email reply =)

Thanks much, looks good to me, will update the wiki as needed to help
add more links to ASF resources.

- Henry


"
rxin <git@git.apache.org>,"Thu, 13 Feb 2014 00:26:24 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1076: [Fix #578] add @transien...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/591#issuecomment-34935532
  
    Ok I'm merging this one. Thanks.


"
mengxr <git@git.apache.org>,"Thu, 13 Feb 2014 00:28:23 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1076: [Fix #578] add @transien...,dev@spark.incubator.apache.org,"Github user mengxr closed the pull request at:

    https://github.com/apache/incubator-spark/pull/591


"
srowen <git@git.apache.org>,"Thu, 13 Feb 2014 00:29:22 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1084. Fix most build warnings,dev@spark.incubator.apache.org,"Github user srowen commented on the pull request:

    https://github.com/apache/incubator-spark/pull/586#issuecomment-34935722
  
    I have a new commit ready to go that addresses the comments, but before I
    pull the trigger, see replies inline with some questions about how you'd
    like to proceed.
    
    
    
    > Thanks for this! Did a quick pass and left some comments.
    >
    > â€”
    > Reply to this email directly or view it on GitHub<https://github.com/apache/incubator-spark/pull/586#issuecomment-34918907>
    > .
    >


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 01:48:00 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1084. Fix most build warnings,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/586#issuecomment-34940302
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 01:48:00 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1084. Fix most build warnings,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/586#issuecomment-34940301
  
     Merged build triggered.


"
srowen <git@git.apache.org>,"Thu, 13 Feb 2014 01:48:07 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1084. Fix most build warnings,dev@spark.incubator.apache.org,"Github user srowen commented on the pull request:

    https://github.com/apache/incubator-spark/pull/586#issuecomment-34940308
  
    Done, I believe. Have another glance at it.


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 02:16:07 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1084. Fix most build warnings,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/586#issuecomment-34941773
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12698/


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 02:16:07 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1084. Fix most build warnings,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/586#issuecomment-34941772
  
    Merged build finished.


"
pwendell <git@git.apache.org>,"Thu, 13 Feb 2014 03:49:39 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1085: Fix Jenkins pull request...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/590#issuecomment-34945973
  
    btw - I think the long term solution here is to have a script inside of the spark repo that jenkins calls to run tests (./dev/run-tests) that way as we evolve the testing framework we can have test requirements be versioned with spark. Also that means people can easily run exactly the same tests as jenkins.


"
pwendell <git@git.apache.org>,"Thu, 13 Feb 2014 03:59:04 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1073 Keep GitHub pull request ...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/574#issuecomment-34946324
  
    Hey @ash211 this is an improvement! Let's just remove the ""Merge pull request..."" as it is now redundant anyways with other information. Instead this content can be merged into the line that starts ""Closes #XXX"" by just adding the remote name. So you'd have something like this:
    
        Closes #574 from ash211/gh-pr-merge-title and squashes the following commits:
    
    BTW - that ""Closes #XXX"" thing is necessary to make github correctly close the PR>


"
rxin <git@git.apache.org>,"Thu, 13 Feb 2014 04:25:14 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1085: Fix Jenkins pull request...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/590#issuecomment-34947355
  
    That's a good idea. I will submit a PR.


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 04:48:09 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1073 Keep GitHub pull request ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/574#issuecomment-34948232
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 04:48:09 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1073 Keep GitHub pull request ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/574#issuecomment-34948233
  
    Merged build started.


"
shivaram <git@git.apache.org>,"Thu, 13 Feb 2014 04:49:24 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user shivaram commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-34948280
  
    Sorry I missed this thread, but I'd like to understand a bit more about the scope of what we require in terms of library support before taking a decision.  
    
    1. I guess everybody agrees that we want our external interfaces to be simple / general and not impose any requirements of mahout-math / JBlas / Commons Math etc.  So the first question I guess is that we need to come up with an external representation for Sparse data (matrix & vector). Is the proposal that we use something like [CSR](https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_row_.28CSR_or_CRS.29) where we compress every row at a time ? 
     
    2. Dense Linear algebra: For dense matrices, the reason we chose JBlas was that it provided a low overhead interface for calling into BLAS-2/BLAS-3 functions through JNI. This was in line with the kind of operations we were using for things like ALS and Regression etc. I'd suggest we stick to this as JBlas has a reasonable API and few external dependencies. Are there any features that JBlas is missing that we want to use ?
    
    3. Sparse Linear algebra: I'm still not sure what kind of operations we want for Sparse data. The most basic things I can think of are operations like dot products, indexing, traversing which shouldn't require calling into a native library. If the set of operations is small, I'd prefer an in-house implementation rather than depending on either a slow (Breeze) or to be deprecated (Commons Math) library.  Again I think this depends on the features we need, so it'll be good to sketch out one or two algorithms for Sparse data and pull in heavier libraries if we need them


"
rxin <git@git.apache.org>,"Thu, 13 Feb 2014 04:52:35 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1088: Create a script for runn...,dev@spark.incubator.apache.org,"GitHub user rxin opened a pull request:

    https://github.com/apache/incubator-spark/pull/592

    SPARK-1088: Create a script for running tests so we can have version specific testing on Jenkins.

    @pwendell

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark test

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/592.patch

----
commit be02359be7f728f4500011896a40bf139fa61419
Author: Reynold Xin <rxin@apache.org>
Date:   2014-02-13T04:49:11Z

    SPARK-1088: Create a script for running tests so we can have version specific testing on Jenkins.

----


"
rxin <git@git.apache.org>,"Thu, 13 Feb 2014 04:52:46 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1088: Create a script for runn...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/592#issuecomment-34948410
  
    I will submit another one for branch-0.9 if this looks good.


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 04:52:59 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1088: Create a script for runn...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/592#issuecomment-34948425
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 04:52:59 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1088: Create a script for runn...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/592#issuecomment-34948424
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 05:16:13 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1073 Keep GitHub pull request ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/574#issuecomment-34949294
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 05:16:12 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1073 Keep GitHub pull request ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/574#issuecomment-34949295
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12699/


"
rxin <git@git.apache.org>,"Thu, 13 Feb 2014 05:18:38 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1088: Create a script for runn...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/592#issuecomment-34949375
  
    As suggested in #590.


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 05:21:21 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1088: Create a script for runn...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/592#issuecomment-34949471
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 05:21:21 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1088: Create a script for runn...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/592#issuecomment-34949472
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12700/


"
ash211 <git@git.apache.org>,"Thu, 13 Feb 2014 05:22:15 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1073 Keep GitHub pull request ...,dev@spark.incubator.apache.org,"Github user ash211 commented on the pull request:

    https://github.com/apache/incubator-spark/pull/574#issuecomment-34949509
  
    Those errors look like unrelated pyspark issues.  Those weren't caused by
    this change were they?
    
    https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12699/consoleFull
    
    
    
    > Refer to this link for build results:
    > https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12699/
    >
    > â€”
    > Reply to this email directly or view it on GitHub<https://github.com/apache/incubator-spark/pull/574#issuecomment-34949295>
    > .
    >


"
aarondav <git@git.apache.org>,"Thu, 13 Feb 2014 06:28:43 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1088: Create a script for runn...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/592#issuecomment-34951992
  
    What is jenkins currently running? It is a script similar to this, that simply wasn't previously in our repo? I'm wondering, for instance, how scalastyle was added to Jenkins.


"
rxin <git@git.apache.org>,"Thu, 13 Feb 2014 06:29:50 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1088: Create a script for runn...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/592#issuecomment-34952038
  
    Yea. It just ran a script similar to this, except it is not a ""script"", just a sequence of commands defined in jenkins.


"
aarondav <git@git.apache.org>,"Thu, 13 Feb 2014 06:33:07 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1088: Create a script for runn...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/592#issuecomment-34952134
  
    Gotcha, well, this is great, we can tell people that they can run it manually before submitting a PR to catch style violations and such. Easier than telling them to run scalastyle, then sbt/sbt test, and to make sure assembly works on top of all that, for instance.


"
aarondav <git@git.apache.org>,"Thu, 13 Feb 2014 06:36:44 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1088: Create a script for runn...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/592#issuecomment-34952267
  
    Thanks, merged into master.


"
pwendell <git@git.apache.org>,"Thu, 13 Feb 2014 06:39:59 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1073 Keep GitHub pull request ...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/574#issuecomment-34952393
  
    Jenkins, retest this please.


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 06:43:03 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1073 Keep GitHub pull request ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/574#issuecomment-34952511
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 06:43:03 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1073 Keep GitHub pull request ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/574#issuecomment-34952512
  
    Merged build started.


"
pwendell <git@git.apache.org>,"Thu, 13 Feb 2014 06:46:58 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1073 Keep GitHub pull request ...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/574#issuecomment-34952676
  
    Andrew - looks great. Thanks for contributing this. I think the test error was unrelated... pending successful tests LGTM.


"
pwendell <git@git.apache.org>,"Thu, 13 Feb 2014 06:54:59 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1088: Create a script for runn...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/592#issuecomment-34952990
  
    @aarondav @rxin we should modify Jenkins now so if this script is present, it calls it :)


"
rxin <git@git.apache.org>,"Thu, 13 Feb 2014 06:55:21 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1088: Create a script for runn...,dev@spark.incubator.apache.org,"GitHub user rxin opened a pull request:

    https://github.com/apache/incubator-spark/pull/593

    SPARK-1088: Create a script for running tests so we can have version specific testing on Jenkins (branch-0.9)

    This is for branch-0.9.
    
    #592 is for master branch (1.0).

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark test-0.9

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/593.patch

----
commit 85a3aa0d773b9df72837e420249a4dca6b1a50ab
Author: Reynold Xin <rxin@apache.org>
Date:   2014-02-13T06:54:10Z

    SPARK-1088: Create a script for running tests so we can have version specific testing on Jenkins.

----


"
rxin <git@git.apache.org>,"Thu, 13 Feb 2014 06:56:07 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1085: Fix Jenkins pull request...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/590#issuecomment-34953037
  
    Closing this one since it has been merged.


"
rxin <git@git.apache.org>,"Thu, 13 Feb 2014 06:56:09 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1085: Fix Jenkins pull request...,dev@spark.incubator.apache.org,"Github user rxin closed the pull request at:

    https://github.com/apache/incubator-spark/pull/590


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 06:57:58 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1088: Create a script for runn...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/593#issuecomment-34953102
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 06:57:58 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1088: Create a script for runn...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/593#issuecomment-34953101
  
     Merged build triggered.


"
pwendell <git@git.apache.org>,"Thu, 13 Feb 2014 07:09:07 +0000 (UTC)",[GitHub] incubator-spark pull request: Ported hadoopClient jar for < 1.0.1 ...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/584#issuecomment-34953539
  
    Jenkins, test this please.


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 07:11:04 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1073 Keep GitHub pull request ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/574#issuecomment-34953629
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 07:11:04 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1073 Keep GitHub pull request ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/574#issuecomment-34953630
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12701/


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 07:13:00 +0000 (UTC)",[GitHub] incubator-spark pull request: Ported hadoopClient jar for < 1.0.1 ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/584#issuecomment-34953722
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 07:13:00 +0000 (UTC)",[GitHub] incubator-spark pull request: Ported hadoopClient jar for < 1.0.1 ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/584#issuecomment-34953723
  
    Merged build started.


"
pwendell <git@git.apache.org>,"Thu, 13 Feb 2014 07:17:27 +0000 (UTC)",[GitHub] incubator-spark pull request: fix for https://spark-project.atlass...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/568#issuecomment-34953913
  
    @bijaybisht Could you explain somewhere what the bug is that this is fixing? The jira references ""spark.driver.host"" but in theory those should have worked under the old codepath.


"
ScrapCodes <git@git.apache.org>,"Thu, 13 Feb 2014 07:23:38 +0000 (UTC)",[GitHub] incubator-spark pull request: fix for https://spark-project.atlass...,dev@spark.incubator.apache.org,"Github user ScrapCodes commented on the pull request:

    https://github.com/apache/incubator-spark/pull/568#issuecomment-34954162
  
    Not if they have set it via SparkConf. (I guess)


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 07:25:32 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1088: Create a script for runn...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/593#issuecomment-34954239
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12702/


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 07:25:32 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1088: Create a script for runn...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/593#issuecomment-34954238
  
    Merged build finished.


"
pwendell <git@git.apache.org>,"Thu, 13 Feb 2014 07:28:23 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1088: Create a script for runn...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/593#issuecomment-34954387
  
    LGTM


"
pwendell <git@git.apache.org>,"Thu, 13 Feb 2014 07:28:39 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1073 Keep GitHub pull request ...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/574#issuecomment-34954401
  
    Merged (guess how!). Thanks Andrew


"
ash211 <git@git.apache.org>,"Thu, 13 Feb 2014 07:31:36 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1073 Keep GitHub pull request ...,dev@spark.incubator.apache.org,"Github user ash211 closed the pull request at:

    https://github.com/apache/incubator-spark/pull/574


"
rxin <git@git.apache.org>,"Thu, 13 Feb 2014 07:31:36 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1088: Create a script for runn...,dev@spark.incubator.apache.org,"Github user rxin closed the pull request at:

    https://github.com/apache/incubator-spark/pull/592


"
srowen <git@git.apache.org>,"Thu, 13 Feb 2014 07:37:02 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user srowen commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-34954740
  
    My $0.02 to the discussion:
    
    1. Within whatever operations mllib provides, serialization can be considered an implementation detail. But external serialization will come up, and I favor supporting something terribly simple. Text-based ""row,col,val"" format strikes me as most standard (which is not quite CSR but almost) since this can be parsed by, say, R or Octave.
    
    2. Agree. Its primary purpose is a hook into BLAS from Java, but its API is ""good enough"" for purposes here I think in that it supports all the primitive ops I think one would want, and the more complex standard ones like solving a system.
    
    3. I think one should assume sparse is incompatible with native code, yes. I think the set of operations that are needed is pretty straightforward and provided by anything one picks off the shelf. 
    
    
    A quick way to achieve this is to repurpose the Commons Math class and chop it up. At least, no need to write from scratch and rewrite bugs.
    
    There's an idea in this thread to make a faÃ§ade to insulate everything from this choice. This also amounts to writing half of a matrix library, since you will end up with a lot of engineering to maintain abstractions and performance.
    
    Here are my personal current top favorite ideas:
    
    1. Use Commons Math everywhere and slip in JBLAS where needed. Consistent API, no rewriting, and still get the speed where needed
    2. Repurpose Commons Math sparse implementation to create a new sparse counterpart to JBLAS API. Consistent API, a bit of rewriting needed.
    3. The faÃ§ade idea, implemented on top of Commons Math sparse and JBLAS for now.
    
    ... and then long-term I would love to see that this question gets solved really well by the likes of Breeze or something and then this project uses that.


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 07:40:19 +0000 (UTC)",[GitHub] incubator-spark pull request: Ported hadoopClient jar for < 1.0.1 ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/584#issuecomment-34954877
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 07:40:20 +0000 (UTC)",[GitHub] incubator-spark pull request: Ported hadoopClient jar for < 1.0.1 ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/584#issuecomment-34954878
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12703/


"
rxin <git@git.apache.org>,"Thu, 13 Feb 2014 07:42:22 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1088: Create a script for runn...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/593#issuecomment-34954956
  
    Ok I merged this using the new script!



"
pwendell <git@git.apache.org>,"Thu, 13 Feb 2014 07:42:38 +0000 (UTC)",[GitHub] incubator-spark pull request: Ported hadoopClient jar for < 1.0.1 ...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/584#issuecomment-34954968
  
    Thanks I merged this into master and 0.9.


"
pwendell <git@git.apache.org>,"Thu, 13 Feb 2014 07:44:02 +0000 (UTC)",[GitHub] incubator-spark pull request: fix for https://spark-project.atlass...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/568#issuecomment-34955031
  
    @ScrapCodes ah I see - this is changing the system properties iterator to a spark conf iterator. Makes sense.


"
pwendell <git@git.apache.org>,"Thu, 13 Feb 2014 07:44:07 +0000 (UTC)",[GitHub] incubator-spark pull request: fix for https://spark-project.atlass...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/568#issuecomment-34955035
  
    LGTM


"
pwendell <git@git.apache.org>,"Thu, 13 Feb 2014 07:44:28 +0000 (UTC)",[GitHub] incubator-spark pull request: fix for https://spark-project.atlass...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/568#issuecomment-34955056
  
    @aarondav pick this into master as well when you merge it.


"
bijaybisht <git@git.apache.org>,"Thu, 13 Feb 2014 07:47:05 +0000 (UTC)",[GitHub] incubator-spark pull request: fix for https://spark-project.atlass...,dev@spark.incubator.apache.org,"Github user bijaybisht commented on the pull request:

    https://github.com/apache/incubator-spark/pull/568#issuecomment-34955181
  
    Yes it is getting set via SparkConf now. 


"
berngp <git@git.apache.org>,"Thu, 13 Feb 2014 07:51:39 +0000 (UTC)",[GitHub] incubator-spark pull request: Ported hadoopClient jar for < 1.0.1 ...,dev@spark.incubator.apache.org,"Github user berngp commented on the pull request:

    https://github.com/apache/incubator-spark/pull/584#issuecomment-34955400
  
    Thanks @pwendell 


"
Evan Chan <ev@ooyala.com>,"Thu, 13 Feb 2014 00:27:29 -0800",Fast Serialization,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Any interest in adding Fast Serialization (or possibly replacing the
default of Java Serialization)?
https://code.google.com/p/fast-serialization/

-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

"
bijaybisht <git@git.apache.org>,"Thu, 13 Feb 2014 08:35:55 +0000 (UTC)",[GitHub] incubator-spark pull request: Ported hadoopClient jar for < 1.0.1 ...,dev@spark.incubator.apache.org,"Github user bijaybisht closed the pull request at:

    https://github.com/apache/incubator-spark/pull/584


"
Zuhair Khayyat <zuhair.khayyat@gmail.com>,"Thu, 13 Feb 2014 11:51:44 +0300","Re: [GitHub] incubator-spark pull request: Adding RDD unique self
 cross product",dev@spark.incubator.apache.org,"I am using rdd.selfCartesian for optimization purposes. I am using Spark
for large data analytic project on relational data. My application
sometimes require to compare the table with itself looking for
inconsistency within the data regardless of the order of compared tuples.

the results of rdd.cartesian(rdd). For example, a table with 100 rows,
the rdd.cartesian(rdd)
will generate 10000 tuples to compare while the rdd.selfCartesian will only
generate 5050 tuples.

Another advantage is that rdd.selfCartesian helps me to get rid of the
duplicate errors when searching for tuple inconsistencies. In
my application, if an error can be found for tuples with the order (tx,ty),
the same error can also be found if they are in the opposite order (ty,tx).
If I used rdd.cartesian(rdd) I will have to look for duplicate errors in
the resulted RDDPair and remove them.

Regards,
Zuhair Khayyat



"
khayyatzy <git@git.apache.org>,"Thu, 13 Feb 2014 08:53:57 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding RDD unique self cross product,dev@spark.incubator.apache.org,"Github user khayyatzy commented on the pull request:

    https://github.com/apache/incubator-spark/pull/587#issuecomment-34959013
  
    I am using rdd.selfCartesian for optimization purposes. I am using Spark for large data analytic project on relational data. My application sometimes require to compare the table with itself looking for inconsistency within the data regardless of the order of compared tuples. 
    
    
    Another advantage is that rdd.selfCartesian helps me to get rid of the duplicate errors when searching for tuple inconsistencies. In my application, if an error can be found for tuples with the order (tx,ty), the same error can also be found if they are in the opposite order (ty,tx). If I used rdd.cartesian(rdd) I will have to look for duplicate errors in the resulted RDDPair and remove them.
    
    Regards,
    Zuhair Khayyat


"
Qiuzhuang <git@git.apache.org>,"Thu, 13 Feb 2014 09:16:35 +0000 (UTC)",[GitHub] incubator-spark pull request: Set ContextWaiter's var stopped to b...,dev@spark.incubator.apache.org,"GitHub user Qiuzhuang opened a pull request:

    https://github.com/apache/incubator-spark/pull/594

    Set ContextWaiter's var stopped to be true if wait() has been triggered

    [STREAMING-58]: https://spark-project.atlassian.net/browse/STREAMING-58

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark master

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/594.patch

----
commit 9c19ce63637eee9369edd235979288d3d9fc9105
Author: Qiuzhuang Lian <qiuzhuang.lian@gmail.com>
Date:   2014-02-08T08:07:39Z

    Kill drivers in postStop() for Worker.
     JIRA SPARK-1068:https://spark-project.atlassian.net/browse/SPARK-1068

commit 18ce572b2b8f10cbd16f73098baea652f9f39d81
Author: Qiuzhuang Lian <qiuzhuang.lian@gmail.com>
Date:   2014-02-09T09:51:53Z

    Merge branch 'master' of https://github.com/apache/incubator-spark

commit 5e96680e0be7f76891aed71dc828a8cfb95ee470
Author: Qiuzhuang Lian <qiuzhuang.lian@gmail.com>
Date:   2014-02-10T02:12:30Z

    Merge branch 'master' of https://github.com/apache/incubator-spark

commit 398723a8acbff0e61301bfd2d75c11a76ffe8244
Author: Qiuzhuang Lian <qiuzhuang.lian@gmail.com>
Date:   2014-02-11T03:00:15Z

    Merge branch 'master' of https://github.com/apache/incubator-spark

commit 687e44b1195c128557c8f8cb754c6c5e613bab90
Author: Qiuzhuang Lian <qiuzhuang.lian@gmail.com>
Date:   2014-02-12T03:07:46Z

    Merge branch 'master' of https://github.com/apache/incubator-spark

commit f4e08eba27b27bb02ab072010548a1116071123e
Author: Qiuzhuang Lian <qiuzhuang.lian@gmail.com>
Date:   2014-02-13T02:57:54Z

    Merge branch 'master' of https://github.com/apache/incubator-spark

commit 5ff8896b9673d2e10f8150abfefe1fb26f7aa81b
Author: Qiuzhuang Lian <qiuzhuang.lian@gmail.com>
Date:   2014-02-13T08:14:01Z

    Set ContextWaiter's var stopped to be true if wait() has been triggered
    [STREAMING-58]: https://spark-project.atlassian.net/browse/STREAMING-58

commit af66d8019ebbdc4003ce601f5e3ecd53ac44b005
Author: Qiuzhuang Lian <qiuzhuang.lian@gmail.com>
Date:   2014-02-13T09:10:59Z

    Merge branch 'master' of https://github.com/apache/incubator-spark

----


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 09:17:57 +0000 (UTC)",[GitHub] incubator-spark pull request: Set ContextWaiter's var stopped to b...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/594#issuecomment-34960587
  
    Can one of the admins verify this patch?


"
mridulm <git@git.apache.org>,"Thu, 13 Feb 2014 09:22:25 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding RDD unique self cross product,dev@spark.incubator.apache.org,"Github user mridulm commented on the pull request:

    https://github.com/apache/incubator-spark/pull/587#issuecomment-34960884
  
    For the specific functionality you are looking for, isnt it not just 
    rdd.cartesian(rdd).filter { case (v1, v2) => v1 <= v2 } ?
    Or did I miss something here ..


"
jyotiska <git@git.apache.org>,"Thu, 13 Feb 2014 09:31:30 +0000 (UTC)",[GitHub] incubator-spark pull request: Added extra description on ValueErro...,dev@spark.incubator.apache.org,"Github user jyotiska commented on the pull request:

    https://github.com/apache/incubator-spark/pull/581#issuecomment-34961462
  
    Can one of the admins test and merge this?


"
chrisavl <git@git.apache.org>,"Thu, 13 Feb 2014 09:59:26 +0000 (UTC)",[GitHub] incubator-spark pull request: Add c3 instance types to Spark EC2,dev@spark.incubator.apache.org,"GitHub user chrisavl opened a pull request:

    https://github.com/apache/incubator-spark/pull/595

    Add c3 instance types to Spark EC2


You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark branch-0.9

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/595.patch

----
commit c8af5f978fc24c2292c13847af4ed020c78ef3e9
Author: Christian Lundgren <christian.lundgren@gameanalytics.com>
Date:   2014-02-13T09:57:19Z

    Add c3 instance types to Spark EC2

----


"
khayyatzy <git@git.apache.org>,"Thu, 13 Feb 2014 10:01:10 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding RDD unique self cross product,dev@spark.incubator.apache.org,"Github user khayyatzy commented on the pull request:

    https://github.com/apache/incubator-spark/pull/587#issuecomment-34963700
  
    Yes it is the same as you just described. But rdd.selfCartesian avoids the extra work required.


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 10:02:57 +0000 (UTC)",[GitHub] incubator-spark pull request: Add c3 instance types to Spark EC2,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/595#issuecomment-34963826
  
    Can one of the admins verify this patch?


"
David Nalley <david@gnsa.us>,"Thu, 13 Feb 2014 02:34:36 -0500",Re: [VOTE] Graduation of Apache Spark from the Incubator,general@incubator.apache.org,"+1 (binding)

--David


"
giyengar <git@git.apache.org>,"Thu, 13 Feb 2014 15:24:13 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user giyengar commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-34988403
  
    @shivaram, @srowen:
    
    I am a new user of Spark and MLLib in particular. I love the clean interface that MLLib currently has. It is a joy to code machine learning algorithms in it. 
    
    To address shivaram's (3), I'd say about 80% of the needs of Regression and Classification tasks can be solved using basic linear algebra operations that can potentially be built in-house though it seems rather wasteful to build yet another sparse linear algebra library. I have used Commons Math and was a bit disappointed to see that sparse vector support is being deprecated. 
    
    I like Sean's top 3 ideas with (2) and (1) reversed. It might make sense to take Commons Math sparse implementations and repurpose it as MLLib-sparse. 
    
    Best Regards, 


"
Henry Saputra <henry.saputra@gmail.com>,"Thu, 13 Feb 2014 08:28:50 -0800","Adding my wiki user id (hsaputra) as contributors in Apache Spark
 confluence wiki space","""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","HI Andy,

Could you or someone with space admin role in the Spark wiki [1]
kindly help to add my userid ""hsaputra"" as collaborators to edit/ add
new content in the Spark wiki space?

I believe Andy's userid  was granted the space admin to the wiki.

Thank you,

- Henry

[1]  https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage

"
velvia <git@git.apache.org>,"Thu, 13 Feb 2014 17:36:18 +0000 (UTC)",[GitHub] incubator-spark pull request: Enable SparkContext.addJars() to loa...,dev@spark.incubator.apache.org,"Github user velvia commented on the pull request:

    https://github.com/apache/incubator-spark/pull/299#issuecomment-35003997
  
    <ping> anybody want to have a look again?
    
    I'm also open to exploring a slightly different idea, which is to load the SparkContext with a custom, URLLoader-derived classloader.  This would in theory cause all of SparkContext's classes to be loaded with this classloader.


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 17:38:18 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-35004193
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 17:38:17 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-35004192
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 17:39:19 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-35004298
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12704/


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 17:39:19 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-35004297
  
    Merged build finished.


"
pwendell <git@git.apache.org>,"Thu, 13 Feb 2014 17:41:24 +0000 (UTC)",[GitHub] incubator-spark pull request: fix for https://spark-project.atlass...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/568#issuecomment-35004505
  
    Jenkins, test this please.


"
pwendell <git@git.apache.org>,"Thu, 13 Feb 2014 17:41:38 +0000 (UTC)",[GitHub] incubator-spark pull request: Set ContextWaiter's var stopped to b...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/594#issuecomment-35004540
  
    /cc @tdas


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 17:43:04 +0000 (UTC)",[GitHub] incubator-spark pull request: fix for https://spark-project.atlass...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/568#issuecomment-35004662
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 17:43:04 +0000 (UTC)",[GitHub] incubator-spark pull request: fix for https://spark-project.atlass...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/568#issuecomment-35004661
  
     Merged build triggered.


"
pwendell <git@git.apache.org>,"Thu, 13 Feb 2014 17:44:40 +0000 (UTC)",[GitHub] incubator-spark pull request: Graph primitives2,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/580#issuecomment-35004818
  
    Jenkins, test this please. cc/ @ankurdave


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 17:48:01 +0000 (UTC)",[GitHub] incubator-spark pull request: Graph primitives2,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/580#issuecomment-35005143
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 17:48:01 +0000 (UTC)",[GitHub] incubator-spark pull request: Graph primitives2,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/580#issuecomment-35005144
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 17:49:06 +0000 (UTC)",[GitHub] incubator-spark pull request: Graph primitives2,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/580#issuecomment-35005269
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 17:49:06 +0000 (UTC)",[GitHub] incubator-spark pull request: Graph primitives2,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/580#issuecomment-35005270
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12706/


"
Mayur Rustagi <mayur.rustagi@gmail.com>,"Thu, 13 Feb 2014 09:54:36 -0800","Re: Adding my wiki user id (hsaputra) as contributors in Apache Spark
 confluence wiki space",dev <dev@spark.incubator.apache.org>,"I can help out here as well. I am trying to develop docs around setting up
Spark, Streaming and Shark, currently doing it on my wiki (
docs.sigmoidanalytics.com). Would love to contribute.
Regards
Mayur

Mayur Rustagi
Ph: +919632149971
h <https://twitter.com/mayur_rustagi>ttp://www.sigmoidanalytics.com
https://twitter.com/mayur_rustagi




"
semihsalihoglu <git@git.apache.org>,"Thu, 13 Feb 2014 17:56:51 +0000 (UTC)",[GitHub] incubator-spark pull request: Graph primitives2,dev@spark.incubator.apache.org,"Github user semihsalihoglu commented on the pull request:

    https://github.com/apache/incubator-spark/pull/580#issuecomment-35006091
  
    Hi guys,
    
    Apparently something failed. I looked at the console, there's one error
    early on about the file ending with a new line but don't have any idea
    about the actual Run-time error. Can someone help?
    
    GitHub pull request #580 of commit
    a69a1523d3983f71c8224ef4b57320a180585e15 automatically merged.
    [EnvInject] - Loading node environment variables.
    Building remotely on CentOS 6.4 Build Worker 2
    <https://amplab.cs.berkeley.edu/jenkins/computer/CentOS%206.4%20Build%20Worker%202>
    in workspace /root/workspace/SparkPullRequestBuilder
    Fetching changes from the remote Git repository
    Fetching upstream changes from https://github.com/apache/incubator-spark.git
    Checking out Revision bd27353f0b0c9fcc19730885152f68a16e75908b
    (origin/pr/580/merge)
    First time build. Skipping changelog.
    [SparkPullRequestBuilder] $ /bin/bash /tmp/hudson7359991664654040889.sh
    Running Scala style checks
    Launching sbt from sbt/sbt-launch-0.13.1.jar
    [info] Loading project definition from
    /root/workspace/SparkPullRequestBuilder/project/project
    [info] Loading project definition from
    /root/workspace/SparkPullRequestBuilder/project
    [info] Compiling 1 Scala source to
    /root/workspace/SparkPullRequestBuilder/project/target/scala-2.10/sbt-0.13/classes...
    [warn] there were 1 deprecation warning(s); re-run with -deprecation for details
    [warn] one warning found
    [info] Set current project to root (in build
    file:/root/workspace/SparkPullRequestBuilder/)
    [success] Total time: 10 s, completed Feb 13, 2014 9:48:57 AM
    Processed 0 file(s)
    Found 0 errors
    Found 0 warnings
    Found 0 infos
    Finished in 6 ms
    [success] created: sbt.SettingKey$$anon$4@2adf56
    Processed 0 file(s)
    Found 0 errors
    Found 0 warnings
    Found 0 infos
    Finished in 7 ms
    [success] created: sbt.SettingKey$$anon$4@2adf56
    Processed 2 file(s)
    Found 0 errors
    Found 0 warnings
    Found 0 infos
    Finished in 1 ms
    [success] created: sbt.SettingKey$$anon$4@2adf56
    Processed 2 file(s)
    Found 0 errors
    Found 0 warnings
    Found 0 infos
    Finished in 1 ms
    [success] created: sbt.SettingKey$$anon$4@2adf56
    Processed 2 file(s)
    Found 0 errors
    Found 0 warnings
    Found 0 infos
    Finished in 1 ms
    [success] created: sbt.SettingKey$$anon$4@2adf56
    Processed 2 file(s)
    Found 0 errors
    Found 0 warnings
    Found 0 infos
    Finished in 0 ms
    Processed 1 file(s)
    [success] created: sbt.SettingKey$$anon$4@2adf56
    Found 0 errors
    Found 0 warnings
    Found 0 infos
    Finished in 0 ms
    [success] created: sbt.SettingKey$$anon$4@2adf56
    Processed 2 file(s)
    Found 0 errors
    Found 0 warnings
    Found 0 infos
    Finished in 0 ms
    [success] created: sbt.SettingKey$$anon$4@2adf56
    Processed 1 file(s)
    Found 0 errors
    Found 0 warnings
    Found 0 infos
    Finished in 0 ms
    [success] created: sbt.SettingKey$$anon$4@2adf56
    Processed 31 file(s)
    Found 0 errors
    Found 0 warnings
    Found 0 infos
    Finished in 1 ms
    [success] created: sbt.SettingKey$$anon$4@2adf56
    Processed 45 file(s)
    Found 0 errors
    Found 0 warnings
    Found 0 infos
    Finished in 0 ms
    [success] created: sbt.SettingKey$$anon$4@2adf56
    Processed 13 file(s)
    Found 0 errors
    Found 0 warnings
    Found 0 infos
    Finished in 1 ms
    [success] created: sbt.SettingKey$$anon$4@2adf56
    error file=/root/workspace/SparkPullRequestBuilder/graphx/src/main/scala/org/apache/spark/graphx/GraphOps.scala
    message=Line contains a tab line=266 column=13
    error file=/root/workspace/SparkPullRequestBuilder/graphx/src/main/scala/org/apache/spark/graphx/GraphOps.scala
    message=File must end with newline character
    Processed 31 file(s)
    Found 2 errors
    Found 0 warnings
    Found 0 infos
    Finished in 1 ms
    [success] created: sbt.SettingKey$$anon$4@2adf56
    Processed 52 file(s)
    Found 0 errors
    Found 0 warnings
    Found 0 infos
    Finished in 1 ms
    [success] created: sbt.SettingKey$$anon$4@2adf56
    Processed 303 file(s)
    Found 0 errors
    Found 0 warnings
    Found 0 infos
    Finished in 1 ms
    [success] created: sbt.SettingKey$$anon$4@2adf56
    java.lang.RuntimeException: exists error
    	at scala.sys.package$.error(package.scala:27)
    	at scala.Predef$.error(Predef.scala:142)
    	at org.scalastyle.sbt.Tasks$.onHasErrors$1(Plugin.scala:99)
    	at org.scalastyle.sbt.Tasks$.doScalastyle(Plugin.scala:106)
    	at org.scalastyle.sbt.ScalastylePlugin$$anonfun$4$$anonfun$apply$1.apply(Plugin.scala:63)
    	at org.scalastyle.sbt.ScalastylePlugin$$anonfun$4$$anonfun$apply$1.apply(Plugin.scala:63)
    	at scala.Function6$$anonfun$tupled$1.apply(Function6.scala:35)
    	at scala.Function6$$anonfun$tupled$1.apply(Function6.scala:34)
    	at scala.Function1$$anonfun$compose$1.apply(Function1.scala:47)
    	at sbt.$tilde$greater$$anonfun$$u2219$1.apply(TypeFunctions.scala:42)
    	at sbt.std.Transform$$anon$4.work(System.scala:64)
    	at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:237)
    	at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:237)
    	at sbt.ErrorHandling$.wideConvert(ErrorHandling.scala:18)
    	at sbt.Execute.work(Execute.scala:244)
    	at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:237)
    	at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:237)
    	at sbt.ConcurrentRestrictions$$anon$4$$anonfun$1.apply(ConcurrentRestrictions.scala:160)
    	at sbt.CompletionService$$anon$2.call(CompletionService.scala:30)
    	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
    	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
    	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
    	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
    	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
    	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    	at java.lang.Thread.run(Thread.java:724)
    [error] (graphx/*:scalastyle) exists error
    [error] Total time: 4 s, completed Feb 13, 2014 9:49:01 AM
    Build step 'Execute shell' marked build as failure
    Recording test results
    Finished: FAILURE
    
    
    
    
    > Refer to this link for build results:
    > https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12706/
    >
    > --
    > Reply to this email directly or view it on GitHub<https://github.com/apache/incubator-spark/pull/580#issuecomment-35005270>
    > .
    >


"
shivaram <git@git.apache.org>,"Thu, 13 Feb 2014 18:04:52 +0000 (UTC)",[GitHub] incubator-spark pull request: Graph primitives2,dev@spark.incubator.apache.org,"Github user shivaram commented on the pull request:

    https://github.com/apache/incubator-spark/pull/580#issuecomment-35006955
  
    The errors are from scalastyle - the style checking plugin we are using. The relevant lines seem to be 
    ```
    error file=/root/workspace/SparkPullRequestBuilder/graphx/src/main/scala/org/apache/spark/graphx/GraphOps.scala
    message=Line contains a tab line=266 column=13
    error file=/root/workspace/SparkPullRequestBuilder/graphx/src/main/scala/org/apache/spark/graphx/GraphOps.scala
    message=File must end with newline character
    ```


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 18:10:05 +0000 (UTC)",[GitHub] incubator-spark pull request: fix for https://spark-project.atlass...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/568#issuecomment-35007475
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12705/


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 18:10:05 +0000 (UTC)",[GitHub] incubator-spark pull request: fix for https://spark-project.atlass...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/568#issuecomment-35007472
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 18:13:25 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-35007814
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 18:13:25 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-35007813
  
    Merged build started.


"
shivaram <git@git.apache.org>,"Thu, 13 Feb 2014 18:13:13 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user shivaram commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35007797
  
    @srowen  Thanks for the summary.
    
    
    About the top 3 ideas, I'd also prefer (2) first and then (1). I am not sure what the faÃ§ade is buying us as these are internal library calls that we can change going forward.


"
pwendell <git@git.apache.org>,"Thu, 13 Feb 2014 18:13:08 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-35007789
  
    Jenkins, test this please.


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 18:13:49 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-35007851
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12707/


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 18:13:50 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-35007850
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 18:18:19 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-35008285
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 18:18:19 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-35008284
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 18:18:34 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-35008314
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 18:18:34 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-35008316
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12708/


"
Patrick Wendell <pwendell@gmail.com>,"Thu, 13 Feb 2014 10:35:18 -0800","Re: Adding my wiki user id (hsaputra) as contributors in Apache Spark
 confluence wiki space","""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey Henry,

Ya unfortunately I have no idea how to do this!


"
shivaram <git@git.apache.org>,"Thu, 13 Feb 2014 18:42:11 +0000 (UTC)",[GitHub] incubator-spark pull request: Add c3 instance types to Spark EC2,dev@spark.incubator.apache.org,"Github user shivaram commented on the pull request:

    https://github.com/apache/incubator-spark/pull/595#issuecomment-35010761
  
    Jenkins, test this please


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 18:42:58 +0000 (UTC)",[GitHub] incubator-spark pull request: Add c3 instance types to Spark EC2,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/595#issuecomment-35010866
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 18:42:58 +0000 (UTC)",[GitHub] incubator-spark pull request: Add c3 instance types to Spark EC2,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/595#issuecomment-35010869
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 19:09:39 +0000 (UTC)",[GitHub] incubator-spark pull request: Add c3 instance types to Spark EC2,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/595#issuecomment-35013734
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 19:09:39 +0000 (UTC)",[GitHub] incubator-spark pull request: Add c3 instance types to Spark EC2,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/595#issuecomment-35013735
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12709/


"
Andy Konwinski <andykonwinski@gmail.com>,"Thu, 13 Feb 2014 11:12:38 -0800","Re: Adding my wiki user id (hsaputra) as contributors in Apache Spark
 confluence wiki space","""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Henry,

I just gave you all of the permissions I know how to give. Let me know if
that does the trick.

Andy



"
Henry Saputra <henry.saputra@gmail.com>,"Thu, 13 Feb 2014 11:26:04 -0800","Re: Adding my wiki user id (hsaputra) as contributors in Apache Spark
 confluence wiki space","""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Cool, whatever you just did allow me to have edit access to the wiki =)

Thanks a lot Andy!

- Henry


"
MLnick <git@git.apache.org>,"Thu, 13 Feb 2014 19:28:21 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user MLnick commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35015689
  
    I guess we should discuss on the JIRA ideally, but most of the discussion seems to be here. So I'll comment here.
    
    I've chimed in before in the original PR - at the time I advocated for Breeze or mahout-math (with the new Scala DSL potentially).
    
    I still advocate for Breeze on the basis that it's a Scala project in the data / ML community, and I believe that it's a great base project that the Scala community should support. Yes it may need some bug fixes and performance enhancements but with netlib-java at its core it should be able to be performant. If there's going to be effort put into Spark sparse matrix code why not have all involved put effort into improving Breeze and making it Scala's numpy? As evidenced above, there may well be some easy win performance enhancements that can be achieved and @dlwh will I'm sure help in making a release when Spark would want to merge.
    
    I again raise the point that maintaining a lot (any?) of matrix algebra code should not be a goal for Spark MLlib. While mahout-math is a highly commendable project I think the weight of maintenance of such a library on the project and committers is clear.
    
    If Breeze is not an option because of implicit-related slowness etc, and mahout-math is not an option for whatever reason, then how about MTJ (https://github.com/fommil/matrix-toolkits-java)? It's updated a lot in recent times, is based on netlib-java with native support, and outperforms JBLAS. Its pure Java performance is decent and could be wrapped in a lightweight DSL (like Dmitriy's mahout-math effort). The API is decent. It has sparse matrix and vector support.
    
    For distributed ML all that is really needed 90% of the time is dense vectors, dense matrices and sparse vectors (with dot product, element-wise stuff, solvers for things like ALS and possibly the odd matrix multiply). It is true that the sparse features missing are fairly lightweight so ultimately it doesn't much matter what is chosen so long as its consistent, reasonably performant and APIs are clean and simple as possible.


"
Andy Konwinski <andykonwinski@gmail.com>,"Thu, 13 Feb 2014 11:34:59 -0800","Re: Adding my wiki user id (hsaputra) as contributors in Apache Spark
 confluence wiki space","""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Great. I'm glad it worked. You're welcome.



"
shivaram <git@git.apache.org>,"Thu, 13 Feb 2014 19:42:12 +0000 (UTC)",[GitHub] incubator-spark pull request: Add c3 instance types to Spark EC2,dev@spark.incubator.apache.org,"Github user shivaram commented on the pull request:

    https://github.com/apache/incubator-spark/pull/595#issuecomment-35017225
  
    LGTM - I wasn't able to figure out if c3 instances were pvm or hvm -- The EC2 wizard seem to suggest they are both, so I think this should be fine.


"
mengxr <git@git.apache.org>,"Thu, 13 Feb 2014 19:47:48 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user mengxr commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35017848
  
    @shivaram @srowen @giyengar Thanks for keeping the discussion running!
    
    @shivaram The requirement is to add sparse data support in all existing MLlib algorithms. The first decision we want to make is what interfaces we want to provide for sparse data, and the second decision to make is how to implement sparse algorithms internally, which package to use or shall we implement and maintain our own.
    
    1. We need sequential access sparse vector for gradient-based algorithm and random access sparse vector for feature transformation and tree-based algorithms. The input to clustering/classification algorithms should be labeled sparse/dense vectors, which is easy for users to provide. We can assemble local sparse matrix blocks (CSR or CSC) if it improves the performance and later make the interface available for advanced users, but this is out of the scope of this discussion. For collaborative filtering, I believe the most convenient format for users is (i, j, x) (COO). 
    
    2 & 3. Yes, we should stick to native BLAS/LAPACK for level 2 & 3 operations. But if we stick to JBLAS, we have  to wrap JBLAS's dense vector/matrix in order to interact with sparse vectors and maintain the code. However, if breeze manages to have very good performance and provides unified interface for both dense and sparse linear algebra. I would certainly choose breeze with netlib-java from JBLAS. The only reason I didn't use breeze in this PR is the slow dense + generic operation, which might be fixed already. If @dlwh plans to make a release in the near future, I'm happy to do a benchmark with existing JBLAS implementation.


"
mengxr <git@git.apache.org>,"Thu, 13 Feb 2014 19:51:14 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user mengxr commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35018212
  
    @MLnick MTJ is not an option because of its license.


"
tsdeng <git@git.apache.org>,"Thu, 13 Feb 2014 19:54:17 +0000 (UTC)",[GitHub] incubator-spark pull request: add onExecutorsStopped to event list...,dev@spark.incubator.apache.org,"GitHub user tsdeng opened a pull request:

    https://github.com/apache/incubator-spark/pull/596

    add onExecutorsStopped to event listeners for getting final stats

    add onExecutorStopped event handler.
    So when master command a shutdown, the executors can send the final message reporting its statuses by replying ExecutorFinalState message(currently only hdfsRead is reported, more can be added later).
    And the master will wait for all response to comeback within the timeout window.


You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark on_executors_stopped

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/596.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #596
    
----
commit 1e6c79e60c969d6af6ca226a0c58e00efeab30bb
Author: Tianshuo Deng <tdeng@twitter.com>
Date:   2014-01-28T23:51:52Z

    add onExecutorsStopped to event listeners for getting final stats from executors when command a shutting down

----


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 19:57:58 +0000 (UTC)",[GitHub] incubator-spark pull request: add onExecutorsStopped to event list...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/596#issuecomment-35018960
  
    Can one of the admins verify this patch?


"
dlwh <git@git.apache.org>,"Thu, 13 Feb 2014 20:10:42 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user dlwh commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35020275
  
    I can cut a release this weekend. We wrap @fommil's netlib-java (
    https://github.com/fommil/netlib-java), whose performance tracks with C
    pretty well. jblas is barely maintained, at this point, so I don't know
    that you'll want to go with them, in any event.
    
    Can you point to any benchmarks you guys are using?
    
    -- David
    
    
    
    > @MLnick <https://github.com/MLnick> MTJ is not an option because of its
    > license.
    >
    > â€”
    > Reply to this email directly or view it on GitHub<https://github.com/apache/incubator-spark/pull/575#issuecomment-35018212>
    > .
    >


"
fommil <git@git.apache.org>,"Thu, 13 Feb 2014 20:14:26 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user fommil commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35020661
  
    @dlwh tracks? I thought I conclusively showed that there was zero impact to go native ;-) BTW, MTJ has sparse matrix support... and I also maintain it. What problem do you have with the license?


"
semihsalihoglu <git@git.apache.org>,"Thu, 13 Feb 2014 20:17:16 +0000 (UTC)",[GitHub] incubator-spark pull request: Graph primitives2,dev@spark.incubator.apache.org,"Github user semihsalihoglu commented on the pull request:

    https://github.com/apache/incubator-spark/pull/580#issuecomment-35020911
  
    I see. OK, I downloaded scalastyle and fixed the style problems. Can we try
    again?
    
    Thank you,
    
    semih
    
    
    
    > The errors are from scalastyle - the style checking plugin we are using.
    > The relevant lines seem to be
    >
    > error file=/root/workspace/SparkPullRequestBuilder/graphx/src/main/scala/org/apache/spark/graphx/GraphOps.scala
    > message=Line contains a tab line=266 column=13
    > error file=/root/workspace/SparkPullRequestBuilder/graphx/src/main/scala/org/apache/spark/graphx/GraphOps.scala
    > message=File must end with newline character
    >
    > --
    > Reply to this email directly or view it on GitHub<https://github.com/apache/incubator-spark/pull/580#issuecomment-35006955>
    > .
    >


"
dlwh <git@git.apache.org>,"Thu, 13 Feb 2014 20:19:32 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user dlwh commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35021161
  
    @fommil :-) Sorry to undersell.
    
    Breeze also has CSCMatrix support, but that's not entirely finished.
    
    
    
    
    
    > @dlwh <https://github.com/dlwh> tracks? I thought I conclusively showed
    > that there was zero impact to go native ;-) BTW, MTJ has sparse matrix
    > support... and I also maintain it. What problem do you have with the
    > license?
    >
    > â€”
    > Reply to this email directly or view it on GitHub<https://github.com/apache/incubator-spark/pull/575#issuecomment-35020661>
    > .
    >


"
Mistobaan <git@git.apache.org>,"Thu, 13 Feb 2014 20:36:08 +0000 (UTC)",[GitHub] incubator-spark pull request: Graph primitives2,dev@spark.incubator.apache.org,"Github user Mistobaan commented on the pull request:

    https://github.com/apache/incubator-spark/pull/580#issuecomment-35022812
  
    I see a bunch of commented tests in the patch ... 


"
pwendell <git@git.apache.org>,"Thu, 13 Feb 2014 20:43:21 +0000 (UTC)",[GitHub] incubator-spark pull request: add onExecutorsStopped to event list...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/596#issuecomment-35023537
  
    Hey @tsdeng - this request is against branch 0.8, but this is a feature and not a bug fix, so this should be against master.


"
pwendell <git@git.apache.org>,"Thu, 13 Feb 2014 20:43:52 +0000 (UTC)",[GitHub] incubator-spark pull request: Add c3 instance types to Spark EC2,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/595#issuecomment-35023583
  
    LGTM as well... I'll merge this thanks!


"
pwendell <git@git.apache.org>,"Thu, 13 Feb 2014 20:44:38 +0000 (UTC)",[GitHub] incubator-spark pull request: Add c3 instance types to Spark EC2,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/595#issuecomment-35023676
  
    Merged, thanks


"
pwendell <git@git.apache.org>,"Thu, 13 Feb 2014 20:47:48 +0000 (UTC)",[GitHub] incubator-spark pull request: Add c3 instance types to Spark EC2,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/595#issuecomment-35024028
  
    I put this in master in addition to branch 0.9


"
semihsalihoglu <git@git.apache.org>,"Thu, 13 Feb 2014 21:00:09 +0000 (UTC)",[GitHub] incubator-spark pull request: Graph primitives2,dev@spark.incubator.apache.org,"Github user semihsalihoglu commented on the pull request:

    https://github.com/apache/incubator-spark/pull/580#issuecomment-35025243
  
    GraphOpsSuite.scala? Are you looking at the version of the last commits? The first commit had some stuff, which I deleted. I don't see them anymore.  


"
tsdeng <git@git.apache.org>,"Thu, 13 Feb 2014 21:03:34 +0000 (UTC)",[GitHub] incubator-spark pull request: add onExecutorsStopped to event list...,dev@spark.incubator.apache.org,"Github user tsdeng closed the pull request at:

    https://github.com/apache/incubator-spark/pull/596


"
JoshRosen <git@git.apache.org>,"Thu, 13 Feb 2014 21:04:44 +0000 (UTC)",[GitHub] incubator-spark pull request: Added extra description on ValueErro...,dev@spark.incubator.apache.org,"Github user JoshRosen commented on the pull request:

    https://github.com/apache/incubator-spark/pull/581#issuecomment-35025658
  
    Jenkins, test this please.


"
tsdeng <git@git.apache.org>,"Thu, 13 Feb 2014 21:04:39 +0000 (UTC)",[GitHub] incubator-spark pull request: add event listener when executors ar...,dev@spark.incubator.apache.org,"GitHub user tsdeng opened a pull request:

    https://github.com/apache/incubator-spark/pull/597

    add event listener when executors are stopped, for sending back final stats

    add onExecutorStopped event handler.
    So when master command a shutdown, the executors can send the final message reporting its statuses by replying ExecutorFinalState message(currently only hdfsRead is reported, more can be added later).
    And the master will wait for all response to comeback within the timeout window.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark on_executors_stopped_master

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/597.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #597
    
----
commit 7a45209600017bf9e8b89ff79d23bc245358429f
Author: Tianshuo Deng <tdeng@twitter.com>
Date:   2014-02-13T21:02:54Z

    add event listener when executors are stopped, for sending back final executor state

----


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 21:08:01 +0000 (UTC)",[GitHub] incubator-spark pull request: add event listener when executors ar...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/597#issuecomment-35025992
  
    Can one of the admins verify this patch?


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 21:08:02 +0000 (UTC)",[GitHub] incubator-spark pull request: Added extra description on ValueErro...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/581#issuecomment-35026001
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 21:08:02 +0000 (UTC)",[GitHub] incubator-spark pull request: Added extra description on ValueErro...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/581#issuecomment-35026000
  
     Merged build triggered.


"
semihsalihoglu <git@git.apache.org>,"Thu, 13 Feb 2014 21:08:09 +0000 (UTC)",[GitHub] incubator-spark pull request: Graph primitives2,dev@spark.incubator.apache.org,"Github user semihsalihoglu commented on the pull request:

    https://github.com/apache/incubator-spark/pull/580#issuecomment-35026015
  
    Just to clarify, here's what happened exactly. I had forgotten to delete those comments before committing the first time by mistake. Then I fixed them and pushed a second commit. This second commit had two style errors. For which I had a third commit. If you guys prefer, I can start from scratch. Let me know.


"
chrisavl <git@git.apache.org>,"Thu, 13 Feb 2014 21:30:14 +0000 (UTC)",[GitHub] incubator-spark pull request: Add c3 instance types to Spark EC2,dev@spark.incubator.apache.org,"Github user chrisavl closed the pull request at:

    https://github.com/apache/incubator-spark/pull/595


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 21:34:30 +0000 (UTC)",[GitHub] incubator-spark pull request: Added extra description on ValueErro...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/581#issuecomment-35028747
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 21:34:30 +0000 (UTC)",[GitHub] incubator-spark pull request: Added extra description on ValueErro...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/581#issuecomment-35028748
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12710/


"
rxin <git@git.apache.org>,"Thu, 13 Feb 2014 21:42:32 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1088: Create a script for runn...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/593#issuecomment-35029540
  
    Closing since it's been merged.


"
rxin <git@git.apache.org>,"Thu, 13 Feb 2014 21:42:32 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1088: Create a script for runn...,dev@spark.incubator.apache.org,"Github user rxin closed the pull request at:

    https://github.com/apache/incubator-spark/pull/593


"
mengxr <git@git.apache.org>,"Thu, 13 Feb 2014 21:54:38 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user mengxr commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35030774
  
    @fommil MTJ use LGPL. See http://www.apache.org/legal/resolved.html


"
shivaram <git@git.apache.org>,"Thu, 13 Feb 2014 22:21:56 +0000 (UTC)",[GitHub] incubator-spark pull request: Update spark_ec2 to use 0.9.0 by def...,dev@spark.incubator.apache.org,"GitHub user shivaram opened a pull request:

    https://github.com/apache/incubator-spark/pull/598

    Update spark_ec2 to use 0.9.0 by default

    Backports change from branch-0.9

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark master

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/598.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #598
    
----
commit f6d3ed0d969d2d2a05ba5131aba45648257c57d0
Author: Shivaram Venkataraman <shivaram@eecs.berkeley.edu>
Date:   2014-02-13T22:20:12Z

    Update spark_ec2 to use 0.9.0 by default
    Backports change from branch-0.9

----


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 22:22:58 +0000 (UTC)",[GitHub] incubator-spark pull request: Update spark_ec2 to use 0.9.0 by def...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/598#issuecomment-35033671
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 22:22:59 +0000 (UTC)",[GitHub] incubator-spark pull request: Update spark_ec2 to use 0.9.0 by def...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/598#issuecomment-35033672
  
    Merged build started.


"
aarondav <git@git.apache.org>,"Thu, 13 Feb 2014 22:26:25 +0000 (UTC)",[GitHub] incubator-spark pull request: Update spark_ec2 to use 0.9.0 by def...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/598#issuecomment-35033971
  
    Thanks! Merged.


"
pwendell <git@git.apache.org>,"Thu, 13 Feb 2014 22:26:51 +0000 (UTC)",[GitHub] incubator-spark pull request: Added parquetFileAsJSON to read Parq...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/576#issuecomment-35034019
  
    Hey there - this is interesting but I don't think it's something that we should put inside of SparkContext. It's nice code as an example thought for other people working with Parquet files though... but Spark inter-operates with many formats and we don't want to put them as dependencies into the spark build or have format-specific utility functions at this point.


"
pwendell <git@git.apache.org>,"Thu, 13 Feb 2014 22:28:01 +0000 (UTC)",[GitHub] incubator-spark pull request: Update spark_ec2 to use 0.9.0 by def...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/598#issuecomment-35034124
  
    @shivaram  sorry for being lame and not doing this earlier... appreciate you sending a PR.


"
laserson <git@git.apache.org>,"Thu, 13 Feb 2014 22:44:14 +0000 (UTC)",[GitHub] incubator-spark pull request: Added parquetFileAsJSON to read Parq...,dev@spark.incubator.apache.org,"Github user laserson closed the pull request at:

    https://github.com/apache/incubator-spark/pull/576


"
laserson <git@git.apache.org>,"Thu, 13 Feb 2014 22:44:14 +0000 (UTC)",[GitHub] incubator-spark pull request: Added parquetFileAsJSON to read Parq...,dev@spark.incubator.apache.org,"Github user laserson commented on the pull request:

    https://github.com/apache/incubator-spark/pull/576#issuecomment-35035595
  
    Yes, I have since thought about it more and agree that this would actually be a bad idea.  No need to add additional dependencies on other specific file formats.  I'm closing this PR.


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 22:51:14 +0000 (UTC)",[GitHub] incubator-spark pull request: Update spark_ec2 to use 0.9.0 by def...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/598#issuecomment-35036331
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12711/


"
AmplabJenkins <git@git.apache.org>,"Thu, 13 Feb 2014 22:51:14 +0000 (UTC)",[GitHub] incubator-spark pull request: Update spark_ec2 to use 0.9.0 by def...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/598#issuecomment-35036329
  
    Merged build finished.


"
fommil <git@git.apache.org>,"Thu, 13 Feb 2014 22:52:25 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user fommil commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35036443
  
    @mengxr and someone decided to roll their own instead of talking to me, why? I've had previous discussions with Apache about the MTJ license and I said we could probably work around it by contacting the original author. The only reason we didn't do anything was because commons-math didn't want to use the f2j java binaries because it has class file manipulation.


"
mengxr <git@git.apache.org>,"Thu, 13 Feb 2014 23:21:03 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user mengxr commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35038739
  
    @fommil I don't quite understand what ""roll their own"" means exactly here. I didn't propose to re-implement one or half linear algebra library in the PR. For the license issue, it would be great if the original author of MTJ agrees to change the license to Apache. With the LGPL license, there is not much we can do. 


"
alig <git@git.apache.org>,"Thu, 13 Feb 2014 23:22:46 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user alig commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-35038876
  
    @RongGu @haoyuan Please also add a new file called ```tachyon.md``` in docs similar to this:
    https://github.com/apache/incubator-spark/blob/master/docs/ec2-scripts.md
    
    That file will just describe that Spark has a block manager inside the Executors that let you chose memory, disk, or Tachyon. The latter is for storing RDDs off-heap outside the Executor JVM on top of the memory management system Tachyon. This has the advantage that: a) executor crash won't lose the data cached b) executors can have smaller memory footprint, allowing you to run more executors on the same machine as the bulk of the memory will be inside Tachyon c) There won't be GC overheads with data stored in the cache.
    
    You can link to the Tachyon homepage for the installation, but please describe in this document how to configure the block manager to use Tachyon. 
    
    Many thanks!


"
velvia <git@git.apache.org>,"Thu, 13 Feb 2014 23:25:06 +0000 (UTC)",[GitHub] incubator-spark pull request: Added parquetFileAsJSON to read Parq...,dev@spark.incubator.apache.org,"Github user velvia commented on the pull request:

    https://github.com/apache/incubator-spark/pull/576#issuecomment-35039082
  
    Uri,
    
    What you can do is, in Scala, have an implicit conversion to your own
    class, effectively extending SparkContext yourself.  We do this for our own
    private inputs.   For example:
    
    implicit class MySparkContext(sc: SparkContext) {
      def parquetJsonFile(path: String): RDD[JsValue] = ....
    }
    
    
    
    
    > Yes, I have since thought about it more and agree that this would actually
    > be a bad idea. No need to add additional dependencies on other specific
    > file formats. I'm closing this PR.
    >
    > --
    > Reply to this email directly or view it on GitHub<https://github.com/apache/incubator-spark/pull/576#issuecomment-35035595>
    > .
    >
    
    
    
    -- 
    The fruit of silence is prayer;
    the fruit of prayer is faith;
    the fruit of faith is love;
    the fruit of love is service;
    the fruit of service is peace.  -- Mother Teresa


"
laserson <git@git.apache.org>,"Thu, 13 Feb 2014 23:40:13 +0000 (UTC)",[GitHub] incubator-spark pull request: Added parquetFileAsJSON to read Parq...,dev@spark.incubator.apache.org,"Github user laserson commented on the pull request:

    https://github.com/apache/incubator-spark/pull/576#issuecomment-35040314
  
    Yes, that's a much better suggestion.  Thanks!


"
shivaram <git@git.apache.org>,"Thu, 13 Feb 2014 23:42:44 +0000 (UTC)",[GitHub] incubator-spark pull request: Update spark_ec2 to use 0.9.0 by def...,dev@spark.incubator.apache.org,"Github user shivaram closed the pull request at:

    https://github.com/apache/incubator-spark/pull/598


"
Reynold Xin <rxin@databricks.com>,"Thu, 13 Feb 2014 17:48:29 -0800",Re: Fast Serialization,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","The perf difference between that and Kryo is pretty small according to
their own benchmark. However, if they can provide better compatibility than
Kryo, we should definitely give it a shot!

Would you like to do some testing?



"
CodingCat <git@git.apache.org>,"Fri, 14 Feb 2014 02:04:46 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1090] improvement on spark_sh...,dev@spark.incubator.apache.org,"GitHub user CodingCat opened a pull request:

    https://github.com/apache/incubator-spark/pull/599

    [SPARK-1090] improvement on spark_shell (help information, configure memory)

    spark-shell should print help information about parameters and should allow user to configure exe memory
    there is no document about hot to set --cores/-c in spark-shell
    
    and also
    
    users should be able to set executor memory through command line options
    
    In this PR I also check the format of the options passed by the user

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark spark_shell_improve

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/599.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #599
    
----
commit 2039fd77cd1e70ca7261c5da5fcf5340f28069e0
Author: CodingCat <zhunansjtu@gmail.com>
Date:   2014-02-14T02:03:34Z

    improvement on spark_shell (help information, configure memory)

----


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 02:07:59 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1090] improvement on spark_sh...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/599#issuecomment-35049082
  
    Can one of the admins verify this patch?


"
punya <git@git.apache.org>,"Fri, 14 Feb 2014 03:08:30 +0000 (UTC)",[GitHub] incubator-spark pull request: Add subtractByKey to the JavaPairRDD...,dev@spark.incubator.apache.org,"GitHub user punya opened a pull request:

    https://github.com/apache/incubator-spark/pull/600

    Add subtractByKey to the JavaPairRDD wrapper

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark subtractByKey-java

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/600.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #600
    
----
commit 5df45fb01bddd784cc28ddc7b23369ea52c9edb7
Author: Punya Biswal <pbiswal@palantir.com>
Date:   2014-02-14T03:00:38Z

    Add subtractByKey to the JavaPairRDD wrapper

----


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 03:12:58 +0000 (UTC)",[GitHub] incubator-spark pull request: Add subtractByKey to the JavaPairRDD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/600#issuecomment-35051869
  
    Can one of the admins verify this patch?


"
jyotiska <git@git.apache.org>,"Fri, 14 Feb 2014 04:29:14 +0000 (UTC)",[GitHub] incubator-spark pull request: Added extra description on ValueErro...,dev@spark.incubator.apache.org,"Github user jyotiska commented on the pull request:

    https://github.com/apache/incubator-spark/pull/581#issuecomment-35054777
  
    I think this is good to go. Can I ask Jenkins to test this?


"
jyotiska <git@git.apache.org>,"Fri, 14 Feb 2014 04:29:26 +0000 (UTC)",[GitHub] incubator-spark pull request: Added extra description on ValueErro...,dev@spark.incubator.apache.org,"Github user jyotiska commented on the pull request:

    https://github.com/apache/incubator-spark/pull/581#issuecomment-35054785
  
    Jenkins, test this please.


"
"""Nick Pentreath"" <nick.pentreath@gmail.com>","Thu, 13 Feb 2014 20:35:18 -0800 (PST)","Re: [GitHub] incubator-spark pull request: [Proposal] Adding
 sparse data suppor...",dev@spark.incubator.apache.org,"@fommil @mengxr I think it's always worth a shot at a license change. Scikit learn devs have been successful before in getting such things over the line.


Assuming we can make that happen, what do folks think about MTJ vs Breeze vs JBLAS + commons-math s"
MLnick <git@git.apache.org>,"Fri, 14 Feb 2014 04:54:47 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user MLnick commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35055610
  
    Originally sent this to dev list not github - the autopsy emails are a bit confusing on mobile :)
    
    
    
    
    @fommil @mengxr I think it's always worth a shot at a license change. Scikit learn devs have been successful before in getting such things over the line.
    
    
    
    
    Assuming we can make that happen, what do folks think about MTJ vs Breeze vs JBLAS + commons-math since these seem like the viable alternatives?
    â€”
    Sent from Mailbox for iPhone
    
    
    > @fommil I don't quite understand what ""roll their own"" means exactly here. I didn't propose to re-implement one or half linear algebra library in the PR. For the license issue, it would be great if the original author of MTJ agrees to change the license to Apache. With the LGPL license, there is not much we can do. 
    > ---
    > Reply to this email directly or view it on GitHub:
    > https://github.com/apache/incubator-spark/pull/575#issuecomment-35038739


"
JoshRosen <git@git.apache.org>,"Fri, 14 Feb 2014 05:17:35 +0000 (UTC)",[GitHub] incubator-spark pull request: Added extra description on ValueErro...,dev@spark.incubator.apache.org,"Github user JoshRosen commented on the pull request:

    https://github.com/apache/incubator-spark/pull/581#issuecomment-35056776
  
    Jenkins, test this please.


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 05:18:01 +0000 (UTC)",[GitHub] incubator-spark pull request: Added extra description on ValueErro...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/581#issuecomment-35056796
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 05:18:01 +0000 (UTC)",[GitHub] incubator-spark pull request: Added extra description on ValueErro...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/581#issuecomment-35056797
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 05:45:51 +0000 (UTC)",[GitHub] incubator-spark pull request: Added extra description on ValueErro...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/581#issuecomment-35058209
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12712/


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 05:45:51 +0000 (UTC)",[GitHub] incubator-spark pull request: Added extra description on ValueErro...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/581#issuecomment-35058208
  
    Merged build finished.


"
pwendell <git@git.apache.org>,"Fri, 14 Feb 2014 05:47:13 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-35058261
  
    Hey @RongGu I did a pretty thorough review - thanks for adding this!
    
    In the code I put a bunch of lower level comments. Here is some high level feedback:
    
    1. Tests - it would be great if this had more unit tests to verify pieces of functionality. For instance, creating the temporary directories has a bunch of logic that could be tested. It also seems like it would be easy to mock the tachyon client and test interactions with Tachyon. We are trying to move towards better test coverage overall and in this case there are a lot of easily testable components.
    2. Documentation - there isn't much documentation here. At a minimum the new config options should be documented (spark.tachyonstore.dir and spark.tachyonmaster.address) in configuration.md. I would document tachyon as an ""experimental"" storage level on this page (scala-programming-guide.html). I'd also add a note on this page that says there is currently experimental support for an off-process cache (cluster-overview.html). If this is documented then people will use it!
    3. The Tachyon storage level should also be added to Java and Python like the other storage levels.
    4. Style - I caught a bunch of style errors, but it would be good to fix things like extra spaces in and comments that don't put a space after the `//`.
    5. The capitalization of tachyon/Tachyon is inconsistent - per @alig let's just keep it uppercase always: ""Tachyon""
    6. Update the maven build to include Tachyon as well.
    7. The amount of recursive dependencies for tachyon is troubling. It would be good if Tachyon provided a ""tachyon-client"" artifact which only depended on things needed to talk to Tachyon. That would probably mean restructuring the build of tachyon. It might also make sense to mark Tachyon as ""provided"" in the Spark build and ask people to locally include Tachyon when running Spark applications. That way the Spark assembly won't include Tachyon so it's dependencies won't get in the way. I haven't come to a full conclusion on this one yet...


"
pwendell <git@git.apache.org>,"Fri, 14 Feb 2014 05:52:34 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-35058467
  
    Hey @alig re your comment about the documentation - it might make more sense to integrate Tachyon in the existing pages that cover storage and provisioning (I mentioned some above) rather than have it's own page. I just feel it would be a bit more integrated that way.


"
pwendell <git@git.apache.org>,"Fri, 14 Feb 2014 06:01:28 +0000 (UTC)",[GitHub] incubator-spark pull request: Add subtractByKey to the JavaPairRDD...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/600#issuecomment-35058750
  
    Jenkins, test this please.


"
jyotiska <git@git.apache.org>,"Fri, 14 Feb 2014 06:02:04 +0000 (UTC)",[GitHub] incubator-spark pull request: Added extra description on ValueErro...,dev@spark.incubator.apache.org,"Github user jyotiska commented on the pull request:

    https://github.com/apache/incubator-spark/pull/581#issuecomment-35058781
  
    @JoshRosen, I think when the traceback function doesn't return the callsite info, it simply returns ""I'm lost!"". Handled that case in this commit. Can you ask Jenkins to test this?


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 06:02:59 +0000 (UTC)",[GitHub] incubator-spark pull request: Add subtractByKey to the JavaPairRDD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/600#issuecomment-35058813
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 06:02:59 +0000 (UTC)",[GitHub] incubator-spark pull request: Add subtractByKey to the JavaPairRDD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/600#issuecomment-35058812
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 06:04:25 +0000 (UTC)",[GitHub] incubator-spark pull request: Add subtractByKey to the JavaPairRDD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/600#issuecomment-35058869
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 06:04:26 +0000 (UTC)",[GitHub] incubator-spark pull request: Add subtractByKey to the JavaPairRDD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/600#issuecomment-35058870
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12713/


"
ScrapCodes <git@git.apache.org>,"Fri, 14 Feb 2014 06:20:16 +0000 (UTC)",[GitHub] incubator-spark pull request: Clean up and added a few java api me...,dev@spark.incubator.apache.org,"Github user ScrapCodes commented on the pull request:

    https://github.com/apache/incubator-spark/pull/402#issuecomment-35059415
  
    Honestly @rxin, I have started to feel these ""With"" functions are less useful for java. Someone can easily write them if he needs on top of any map/filter/flatMap api functions.


"
ScrapCodes <git@git.apache.org>,"Fri, 14 Feb 2014 06:21:04 +0000 (UTC)",[GitHub] incubator-spark pull request: [Application-UI] Reporting waiting s...,dev@spark.incubator.apache.org,"Github user ScrapCodes commented on the pull request:

    https://github.com/apache/incubator-spark/pull/123#issuecomment-35059439
  
    ping !


"
rxin <git@git.apache.org>,"Fri, 14 Feb 2014 06:21:45 +0000 (UTC)",[GitHub] incubator-spark pull request: Clean up and added a few java api me...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/402#issuecomment-35059461
  
    I agree with you. I actually think we should consider deprecating them for Scala as well. The same sentiment is also echoed by @markhamstra who initially wrote this. 
    
    @markhamstra should we just deprecate the *With functions?



"
rxin <git@git.apache.org>,"Fri, 14 Feb 2014 06:22:11 +0000 (UTC)",[GitHub] incubator-spark pull request: [Application-UI] Reporting waiting s...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/123#issuecomment-35059482
  
    ping @kayousterhout 


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 06:28:15 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1063 Add .sortBy(f) method on ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/508#issuecomment-35059685
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 06:28:15 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1063 Add .sortBy(f) method on ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/508#issuecomment-35059686
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 06:29:27 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1063 Add .sortBy(f) method on ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/508#issuecomment-35059728
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 06:29:27 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1063 Add .sortBy(f) method on ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/508#issuecomment-35059729
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12714/


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 06:43:16 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1063 Add .sortBy(f) method on ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/508#issuecomment-35060221
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 06:43:17 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1063 Add .sortBy(f) method on ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/508#issuecomment-35060222
  
    Merged build started.


"
markhamstra <git@git.apache.org>,"Fri, 14 Feb 2014 07:03:37 +0000 (UTC)",[GitHub] incubator-spark pull request: Clean up and added a few java api me...,dev@spark.incubator.apache.org,"Github user markhamstra commented on the pull request:

    https://github.com/apache/incubator-spark/pull/402#issuecomment-35061086
  
    It's what I'd be inclined to do.
    
    I think there is a need for something with a higher-level of abstraction than mapPartitions/mapPartitionsWithIndex that avoids the boilerplate and potential pitfalls when the need is to create some per-partition resource and then clean those up when they are no longer needed; but the *With functions neither meet that entire need nor do they do what they do in a way that is significantly easier to code than using mapPartitions/mapPartitionsWithIndex directly.
    
    Deprecate these and perhaps add a better attempt in 1.1+.0.


"
rxin <git@git.apache.org>,"Fri, 14 Feb 2014 07:04:53 +0000 (UTC)",[GitHub] incubator-spark pull request: Clean up and added a few java api me...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/402#issuecomment-35061130
  
    Great. @ScrapCodes can you mark the scala *With's as deprecated in 1.0.0, and remove the java apis from this pr?


"
ScrapCodes <git@git.apache.org>,"Fri, 14 Feb 2014 07:05:52 +0000 (UTC)",[GitHub] incubator-spark pull request: Clean up and added a few java api me...,dev@spark.incubator.apache.org,"Github user ScrapCodes commented on the pull request:

    https://github.com/apache/incubator-spark/pull/402#issuecomment-35061165
  
    yes ofcourse !
    
    
    
    > Great. @ScrapCodes <https://github.com/ScrapCodes> can you mark the scala
    > *With's as deprecated in 1.0.0, and remove the java apis from this pr?
    >
    > --
    > Reply to this email directly or view it on GitHub<https://github.com/apache/incubator-spark/pull/402#issuecomment-35061130>
    > .
    >
    
    
    
    -- 
    Prashant


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 07:11:00 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1063 Add .sortBy(f) method on ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/508#issuecomment-35061379
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12715/


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 07:11:00 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1063 Add .sortBy(f) method on ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/508#issuecomment-35061377
  
    Merged build finished.


"
alig <git@git.apache.org>,"Fri, 14 Feb 2014 07:21:33 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user alig commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-35061768
  
    Agree. Let's put it like this:
    - Must have: what you suggested that covers the config integrated into existing docs etc
    - Nice to have: in addition to above have a separate page that explains a bit more about the Tachyon usage in spark (e.g. tachyon:// vs StorageLevel).
    
    > 
    > Hey @alig re your comment about the documentation - it might make more sense to integrate Tachyon in the existing pages that cover storage and provisioning (I mentioned some above) rather than have it's own page. I just feel it would be a bit more integrated that way.
    > 
    > â€”
    > Reply to this email directly or view it on GitHub.


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 07:43:22 +0000 (UTC)",[GitHub] incubator-spark pull request: Clean up and added a few java api me...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/402#issuecomment-35062669
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 07:43:22 +0000 (UTC)",[GitHub] incubator-spark pull request: Clean up and added a few java api me...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/402#issuecomment-35062670
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 08:11:16 +0000 (UTC)",[GitHub] incubator-spark pull request: Deprecated and added a few java api ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/402#issuecomment-35063988
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12716/


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 08:11:16 +0000 (UTC)",[GitHub] incubator-spark pull request: Deprecated and added a few java api ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/402#issuecomment-35063987
  
    Merged build finished.


"
ScrapCodes <git@git.apache.org>,"Fri, 14 Feb 2014 08:12:28 +0000 (UTC)",[GitHub] incubator-spark pull request: Deprecated and added a few java api ...,dev@spark.incubator.apache.org,"Github user ScrapCodes commented on the pull request:

    https://github.com/apache/incubator-spark/pull/402#issuecomment-35064042
  
    Failures in python is I guess not related, perhaps we have sporadic failures in python ?


"
ash211 <git@git.apache.org>,"Fri, 14 Feb 2014 08:19:26 +0000 (UTC)",[GitHub] incubator-spark pull request: Typo: Standlone -> Standalone,dev@spark.incubator.apache.org,"GitHub user ash211 opened a pull request:

    https://github.com/apache/incubator-spark/pull/601

    Typo: Standlone -> Standalone

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark typo

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/601.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #601
    
----
commit 873bd2fda0e13545b5b5063e833aca0c79459c05
Author: Andrew Ash <andrew@andrewash.com>
Date:   2014-02-14T08:18:46Z

    Typo: Standlone -> Standalone

----


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 08:22:58 +0000 (UTC)",[GitHub] incubator-spark pull request: Typo: Standlone -> Standalone,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/601#issuecomment-35064602
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 08:22:58 +0000 (UTC)",[GitHub] incubator-spark pull request: Typo: Standlone -> Standalone,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/601#issuecomment-35064601
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 08:50:47 +0000 (UTC)",[GitHub] incubator-spark pull request: Typo: Standlone -> Standalone,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/601#issuecomment-35066139
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 08:50:47 +0000 (UTC)",[GitHub] incubator-spark pull request: Typo: Standlone -> Standalone,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/601#issuecomment-35066141
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12717/


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 09:22:59 +0000 (UTC)",[GitHub] incubator-spark pull request: Typo: Standlone -> Standalone,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/601#issuecomment-35068158
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 09:22:59 +0000 (UTC)",[GitHub] incubator-spark pull request: Typo: Standlone -> Standalone,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/601#issuecomment-35068157
  
     Merged build triggered.


"
fommil <git@git.apache.org>,"Fri, 14 Feb 2014 09:37:37 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user fommil commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35069073
  
    @MLnick I have it on good authority (from the author of JBLAS) that he consider `netlib-java` to exceed his original goals for JBLAS. I am utterly confused why commons-math would accept JBLAS but reject netlib-java as a backend, given that JBLAS doesn't even have a java backend and they were concerned that the netlib-java java backend could not be compiled without classfile manipulation.
    
    I'll hold fire on the license request until you decide based on technical merit. I don't anticipate any problems: in fact I believe he would be delighted that MTJ is continuing.


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 09:50:51 +0000 (UTC)",[GitHub] incubator-spark pull request: Typo: Standlone -> Standalone,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/601#issuecomment-35069917
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 09:50:51 +0000 (UTC)",[GitHub] incubator-spark pull request: Typo: Standlone -> Standalone,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/601#issuecomment-35069919
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12718/


"
MLnick <git@git.apache.org>,"Fri, 14 Feb 2014 09:54:46 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user MLnick commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35070192
  
    I for one would prefer to use netlib-java, whether it is via MTJ or Breeze.
    I've used both and find the MTJ API pretty good and with good sparse
    support (though this was a few years ago admittedly). Breeze has a nicer
    more ""Matlab"" DSL but as I mentioned there's no reason not to wrap MTJ in a
    lighter-weight DSL that is not heavy on implicits, if for performance
    reasons it is desirable.
    
    
    
    > @MLnick <https://github.com/MLnick> I have it on good authority (from the
    > author of JBLAS) that he consider netlib-java to exceed his original
    > goals for JBLAS. I am utterly confused why commons-math would accept JBLAS
    > but reject netlib-java as a backend, given that JBLAS doesn't even have a
    > java backend and they were concerned that the netlib-java java backend
    > could not be compiled without classfile manipulation.
    >
    > I'll hold fire on the license request until you decide based on technical
    > merit. I don't anticipate any problems: in fact I believe he would be
    > delighted that MTJ is continuing.
    >
    > --
    > Reply to this email directly or view it on GitHub<https://github.com/apache/incubator-spark/pull/575#issuecomment-35069073>
    > .
    >


"
mikiobraun <git@git.apache.org>,"Fri, 14 Feb 2014 10:55:24 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user mikiobraun commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35073967
  
    @fommil @MLnick Hello, I'm the author of jblas, and I'd like to clarify what fommil said about netlib-java exceeding my original goals for JBLAS, because I don't quite agree with that statement.
    
    netlib-java provides BLAS/LAPACK for Java. If native libraries are found, it uses those, otherwise, it will fall back to a (very slow) implementation of Fortran which is basically a Java version of the netlib blas and lapack implementations (no optimization whatsoever). netlib-java has full coverage of the BLAS and LAPACK routines while jblas only includes what I needed. netlib-java also has the fallback mode (which I honestly think you wouldn't want to use because it's quite slow).
    
    jblas however, is not just a wrapper around BLAS/LAPACK but a matrix library on top of that, so it's netlib-java wrong to say that it exceeds the goals of jblas, because netlib-java has different goals.
    
    In my view, the biggest feature of jblas is the way I've packaged the ATLAS libraries for Windows, Mac, and Linux, 32bit and 64bit into the jar so you can really just put in a maven dependency and get native performance. I don't think other libraries have gone through that pain, mainly to recompile ATLAS on all those platforms, and so on. So I guess jblas is the simplest way to get native performance on Java.
    
    It has no support for sparse matrices, however, and I had relatively little time to work on it lately, so I'm not mad if you choose to go another way ;)


"
Andrew Ash <andrew@andrewash.com>,"Fri, 14 Feb 2014 03:38:29 -0800",RDD.top() stacktrace,dev@spark.incubator.apache.org,"Spark 0.9.0


Hi Spark devs,

I'm pretty sure this stacktrace is a bug in the way Spark is using the type
system but I don't quite know what it is.  Something to do with type bounds
judging from my Googling.

Can someone with more Scala-foo than me please take a look?  In the
meantime I'll be avoiding top() for a bit.

Thanks!
Andrew



This stracktrace came about when I called
val myRDD: RDD[(String,Int)] = ...
myRDD.reduceByKey(_+_).top(100)

But my toy example doesn't trigger the repro:
sc.parallelize(Seq( (""A"",10), (""B"",5), (""A"",4), (""C"", 15)
)).reduceByKey(_+_).top(2)



scala.collection.immutable.$colon$colon cannot be cast to
org.apache.spark.util.BoundedPriorityQueue
java.lang.ClassCastException: scala.collection.immutable.$colon$colon
cannot be cast to org.apache.spark.util.BoundedPriorityQueue
        at org.apache.spark.rdd.RDD$$anonfun$top$2.apply(RDD.scala:873)
        at org.apache.spark.rdd.RDD$$anonfun$6.apply(RDD.scala:671)
        at org.apache.spark.rdd.RDD$$anonfun$6.apply(RDD.scala:668)
        at
org.apache.spark.scheduler.JobWaiter.taskSucceeded(JobWaiter.scala:56)
        at
org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:859)
        at
org.apache.spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:616)
        at
org.apache.spark.scheduler.DAGScheduler$$anonfun$start$1$$anon$2$$anonfun$receive$1.applyOrElse(DAGScheduler.scala:207)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
        at akka.actor.ActorCell.invoke(ActorCell.scala:456)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
        at akka.dispatch.Mailbox.run(Mailbox.scala:219)
        at
akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
        at
scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
        at
scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
        at
scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
        at
scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
"
Xuefeng Wu <benewu@gmail.com>,"Fri, 14 Feb 2014 19:58:24 +0800",Re: RDD.top() stacktrace,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi Andrew,

Sorry, I can not reproduce the issue by:

scala> import org.apache.spark.rdd.RDD

scala> val myRDD: RDD[(String,Int)] = sc.parallelize(Seq( (""A"",10),
(""B"",5), (""A"",4), (""C"", 15)))

scala> myRDD.reduceByKey(_+_).top(2)

Any different compare with your example ?




pe
ds
.scala:859)
16)
$receive$1.applyOrElse(DAGScheduler.scala:207)
Dispatcher.scala:386)
a:1339)
ava:107)



-- 

~Yours, Xuefeng Wu/ÎâÑ©·å  ¾´ÉÏ
"
Andrew Ash <andrew@andrewash.com>,"Fri, 14 Feb 2014 04:02:14 -0800",Re: RDD.top() stacktrace,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I'm sorry I do not yet have a consistent repro that doesn't use my internal
dataset.  That example is the repro I attempted to create but it does not
trigger the issue.  Hopefully I'll be able to get a consistent repro to
share.



.scala:859)
16)
$receive$1.applyOrElse(DAGScheduler.scala:207)
Dispatcher.scala:386)
a:1339)
)
ava:107)
"
punya <git@git.apache.org>,"Fri, 14 Feb 2014 16:12:07 +0000 (UTC)",[GitHub] incubator-spark pull request: Add subtractByKey to the JavaPairRDD...,dev@spark.incubator.apache.org,"Github user punya commented on the pull request:

    https://github.com/apache/incubator-spark/pull/600#issuecomment-35097483
  
    @pwendell, the build hasn't retriggered even though I updated my patch. Could you please ask Jenkins to test this pull request.


"
"""Deyhim, Parviz"" <parvizd@amazon.com>","Fri, 14 Feb 2014 16:53:53 +0000",Spark 0.8.1 on Amazon Elastic MapReduce,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>,
        ""user@spark.incubator.apache.org"" <user@spark.incubator.apache.org>","Spark community,

Wanted to let you know that the version of Spark and Shark on Amazon Elastic MapReduce has been updated to 0.8.1. This new version provides a much better experience in terms of stability and performance but also supports the following features:

  *   Integration with Amazon Cloudwatch
  *   Integration of Spark Streaming with Amazon Kinesis.
  *   Automatic log shipping to S3

For a complete detail of the features Spark on EMR provides, please see the following article: http://aws.amazon.com/articles/4926593393724923

And yes I'm working hard to push another update to support 0.9.0 :)

What would be great is to hear from the community on what other features you like to see on Spark on EMR. For example, how useful is autoscaling for Spark? Any other features you like to see?

Thanks,

Parviz Deyhim
Solutions Architect
Amazon Web Services<http://aws.amazon.com/>
E: parvizd@amazon.com
M:  408.315.2305

[Description: Description: Description: C:\Users\aiden\AppData\Local\Microsoft\Windows\Temporary Internet Files\Content.Word\aws.gif]<http://aws.amazon.com/>
"
Nick Pentreath <nick.pentreath@gmail.com>,"Fri, 14 Feb 2014 19:28:20 +0200",Re: Spark 0.8.1 on Amazon Elastic MapReduce,"""user@spark.incubator.apache.org"" <user@spark.incubator.apache.org>","Thanks Parviz, this looks great and good to see it getting updated. Look
forward to 0.9.0!

A perhaps stupid question - where does the KinesisWordCount example live?
Is that an Amazon example, since I don't see it under the streaming
examples included in the Spark project. If it's a third party example is it
possible to get the code?

Thanks
Nick



"
dlwh <git@git.apache.org>,"Fri, 14 Feb 2014 17:30:02 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user dlwh commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35105330
  
    Just to follow up on Breeze performance: in the latest snapshot, we are
    consistently faster than JBlas and Mahout in @mengxr's benchmarks.
    
    
    
    > I for one would prefer to use netlib-java, whether it is via MTJ or Breeze.
    > I've used both and find the MTJ API pretty good and with good sparse
    > support (though this was a few years ago admittedly). Breeze has a nicer
    > more ""Matlab"" DSL but as I mentioned there's no reason not to wrap MTJ in a
    > lighter-weight DSL that is not heavy on implicits, if for performance
    > reasons it is desirable.
    >
    >
    >
    > > @MLnick <https://github.com/MLnick> I have it on good authority (from
    > the
    >
    > > author of JBLAS) that he consider netlib-java to exceed his original
    > > goals for JBLAS. I am utterly confused why commons-math would accept
    > JBLAS
    > > but reject netlib-java as a backend, given that JBLAS doesn't even have a
    > > java backend and they were concerned that the netlib-java java backend
    > > could not be compiled without classfile manipulation.
    > >
    > > I'll hold fire on the license request until you decide based on technical
    > > merit. I don't anticipate any problems: in fact I believe he would be
    > > delighted that MTJ is continuing.
    > >
    > > --
    > > Reply to this email directly or view it on GitHub<
    > https://github.com/apache/incubator-spark/pull/575#issuecomment-35069073>
    >
    > > .
    > >
    >
    > â€”
    > Reply to this email directly or view it on GitHub<https://github.com/apache/incubator-spark/pull/575#issuecomment-35070192>
    > .
    >


"
aarondav <git@git.apache.org>,"Fri, 14 Feb 2014 18:01:12 +0000 (UTC)",[GitHub] incubator-spark pull request: Typo: Standlone -> Standalone,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/601#issuecomment-35108234
  
    Thanks! Merged.


"
jyotiska <git@git.apache.org>,"Fri, 14 Feb 2014 18:41:50 +0000 (UTC)",[GitHub] incubator-spark pull request: Added extra description on ValueErro...,dev@spark.incubator.apache.org,"Github user jyotiska commented on the pull request:

    https://github.com/apache/incubator-spark/pull/581#issuecomment-35111896
  
    Jenkins, test this please.


"
Tathagata Das <tathagata.das1565@gmail.com>,"Fri, 14 Feb 2014 10:42:43 -0800",Re: Spark 0.8.1 on Amazon Elastic MapReduce,dev@spark.incubator.apache.org,"That's interesting! I am curious to find out as well.

TD



"
punya <git@git.apache.org>,"Fri, 14 Feb 2014 19:02:35 +0000 (UTC)",[GitHub] incubator-spark pull request: Add subtractByKey to the JavaPairRDD...,dev@spark.incubator.apache.org,"Github user punya commented on the pull request:

    https://github.com/apache/incubator-spark/pull/600#issuecomment-35113788
  
    Jenkins, test this please.


"
JoshRosen <git@git.apache.org>,"Fri, 14 Feb 2014 19:06:54 +0000 (UTC)",[GitHub] incubator-spark pull request: Add subtractByKey to the JavaPairRDD...,dev@spark.incubator.apache.org,"Github user JoshRosen commented on the pull request:

    https://github.com/apache/incubator-spark/pull/600#issuecomment-35114189
  
    Jenkins, this is ok to test.


"
JoshRosen <git@git.apache.org>,"Fri, 14 Feb 2014 19:07:03 +0000 (UTC)",[GitHub] incubator-spark pull request: Added extra description on ValueErro...,dev@spark.incubator.apache.org,"Github user JoshRosen commented on the pull request:

    https://github.com/apache/incubator-spark/pull/581#issuecomment-35114200
  
    Jenkins, this is ok to test.


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 19:07:59 +0000 (UTC)",[GitHub] incubator-spark pull request: Add subtractByKey to the JavaPairRDD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/600#issuecomment-35114288
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 19:07:59 +0000 (UTC)",[GitHub] incubator-spark pull request: Add subtractByKey to the JavaPairRDD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/600#issuecomment-35114287
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 19:08:02 +0000 (UTC)",[GitHub] incubator-spark pull request: Added extra description on ValueErro...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/581#issuecomment-35114289
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 19:08:03 +0000 (UTC)",[GitHub] incubator-spark pull request: Added extra description on ValueErro...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/581#issuecomment-35114290
  
    Merged build started.


"
pwendell <git@git.apache.org>,"Fri, 14 Feb 2014 19:24:32 +0000 (UTC)",[GitHub] incubator-spark pull request: Deprecated and added a few java api ...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/402#issuecomment-35115847
  
    @ScrapCodes as discussed earlier if you could add either tests or explicit typing for the accessor methods that would be great.


"
ash211 <git@git.apache.org>,"Fri, 14 Feb 2014 19:29:10 +0000 (UTC)",[GitHub] incubator-spark pull request: Typo: Standlone -> Standalone,dev@spark.incubator.apache.org,"Github user ash211 closed the pull request at:

    https://github.com/apache/incubator-spark/pull/601


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 19:35:51 +0000 (UTC)",[GitHub] incubator-spark pull request: Add subtractByKey to the JavaPairRDD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/600#issuecomment-35116857
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 19:35:52 +0000 (UTC)",[GitHub] incubator-spark pull request: Add subtractByKey to the JavaPairRDD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/600#issuecomment-35116858
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12719/


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 19:36:09 +0000 (UTC)",[GitHub] incubator-spark pull request: Added extra description on ValueErro...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/581#issuecomment-35116879
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 19:36:10 +0000 (UTC)",[GitHub] incubator-spark pull request: Added extra description on ValueErro...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/581#issuecomment-35116880
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12720/


"
jyotiska <git@git.apache.org>,"Fri, 14 Feb 2014 19:43:41 +0000 (UTC)",[GitHub] incubator-spark pull request: Added extra description on ValueErro...,dev@spark.incubator.apache.org,"Github user jyotiska commented on the pull request:

    https://github.com/apache/incubator-spark/pull/581#issuecomment-35117581
  
    @JoshRosen, I think the problem is with file names with spaces in them. In that case, the line split is not working and returning the wrong value. Lets try moving the traceback method from rdd and putting in a separate file. Also, I will put a separate function for returning callsite info as dict.


"
RongGu <git@git.apache.org>,"Fri, 14 Feb 2014 19:43:42 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user RongGu commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-35117584
  
    Hey @pwendell @alig , thank you for reviewing my code and making so many comments. I will process them in these two days :)


"
JoshRosen <git@git.apache.org>,"Fri, 14 Feb 2014 19:45:35 +0000 (UTC)",[GitHub] incubator-spark pull request: Added extra description on ValueErro...,dev@spark.incubator.apache.org,"Github user JoshRosen commented on the pull request:

    https://github.com/apache/incubator-spark/pull/581#issuecomment-35117763
  
    I may have given some bad suggestions earlier with using `_extract_concise_traceback`; I think you may just want to use the regular traceback module and just use the attribute that drops the frame that calls the traceback (so you get the frame of the call into `SparkContext`__init__`).


"
RongGu <git@git.apache.org>,"Fri, 14 Feb 2014 20:15:08 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user RongGu commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#discussion_r9762266
  
    Actually, the compiler would report [error] (Note that variables need to be initialized to be defined) here. So, I initialize it with null.


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 20:18:02 +0000 (UTC)",[GitHub] incubator-spark pull request: Added extra description on ValueErro...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/581#issuecomment-35120692
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 20:18:02 +0000 (UTC)",[GitHub] incubator-spark pull request: Added extra description on ValueErro...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/581#issuecomment-35120691
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 20:18:18 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-35120716
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 20:18:18 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-35120715
  
     Merged build triggered.


"
jyotiska <git@git.apache.org>,"Fri, 14 Feb 2014 20:18:29 +0000 (UTC)",[GitHub] incubator-spark pull request: Added extra description on ValueErro...,dev@spark.incubator.apache.org,"Github user jyotiska commented on the pull request:

    https://github.com/apache/incubator-spark/pull/581#issuecomment-35120729
  
    Wait, what happened to foreachPartition, keys and repartition methods? Did I just accidentally deleted them or have they been discontinued?


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 20:46:08 +0000 (UTC)",[GitHub] incubator-spark pull request: Added extra description on ValueErro...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/581#issuecomment-35123080
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 20:46:08 +0000 (UTC)",[GitHub] incubator-spark pull request: Added extra description on ValueErro...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/581#issuecomment-35123081
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12721/


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 20:47:25 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-35123189
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 20:47:25 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-35123190
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12722/


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 20:58:05 +0000 (UTC)",[GitHub] incubator-spark pull request: Added extra description on ValueErro...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/581#issuecomment-35124161
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 20:58:05 +0000 (UTC)",[GitHub] incubator-spark pull request: Added extra description on ValueErro...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/581#issuecomment-35124160
  
     Merged build triggered.


"
hsaputra <git@git.apache.org>,"Fri, 14 Feb 2014 21:03:27 +0000 (UTC)","[GitHub] incubator-spark pull request: Add Security to Spark - Akka, Http, ...",dev@spark.incubator.apache.org,"Github user hsaputra commented on the pull request:

    https://github.com/apache/incubator-spark/pull/332#issuecomment-35124599
  
    Hi Tom, do you think it is useful to describe to security implementation and potential override/ customization under separate .md file under docs directory for future references?


"
tgravescs <git@git.apache.org>,"Fri, 14 Feb 2014 21:08:30 +0000 (UTC)","[GitHub] incubator-spark pull request: Add Security to Spark - Akka, Http, ...",dev@spark.incubator.apache.org,"Github user tgravescs commented on the pull request:

    https://github.com/apache/incubator-spark/pull/332#discussion_r9764008
  
    nope, just forgot to remove the comment. removing it.



"
tgravescs <git@git.apache.org>,"Fri, 14 Feb 2014 21:15:14 +0000 (UTC)","[GitHub] incubator-spark pull request: Add Security to Spark - Akka, Http, ...",dev@spark.incubator.apache.org,"Github user tgravescs commented on the pull request:

    https://github.com/apache/incubator-spark/pull/332#discussion_r9764216
  
    Unfortunately this was needed for yarn-client mode in batch mode.  The spark-shell stuff is taken care of by the changes I made to repl, but the yarn-client batch mode needs. this.


"
tgravescs <git@git.apache.org>,"Fri, 14 Feb 2014 21:15:44 +0000 (UTC)","[GitHub] incubator-spark pull request: Add Security to Spark - Akka, Http, ...",dev@spark.incubator.apache.org,"Github user tgravescs commented on the pull request:

    https://github.com/apache/incubator-spark/pull/332#discussion_r9764238
  
    Yep, YARN handles distributed the UGI for you.


"
fommil <git@git.apache.org>,"Fri, 14 Feb 2014 21:25:40 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user fommil commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35126301
  
    @mikiobraun apologies for misquoting you: I have obviously inferred too much from your commendation of `netlib-java`'s recent updates, and when we discussed a potential merger of the projects (you suggested replacing the JBLAS native layer with `netlib-java`, but cited lack of time). I also know you originally wrote JBLAS because you thought MTJ was unmaintained. We'll call it even, since you claimed at the ICML that netlib-java was not maintained :-P
    
    To correct a minor point: `netlib-java` will use **system optimised** compliant natives if they are available, falling back to fortran (which is still faster than any Java implementation I've seen), and falling back to a pure JVM implementation (which is on par with most of the other implementations).
    
    MTJ, like Breeze, is a linear algebra library on top of `netlib-java`. In this regard, it is perhaps best to be comparing JBLAS to MTJ or Breeze... but with MTJ/Breeze both using `netlib-java` so having comparable performance.
    
    Regarding pre-built ATLAS binaries: my benchmarks have shown ATLAS to be **significantly** slower than Mac's bundled `vecLib`. Also, ATLAS is already available on most Linux distributions with a single command line call... whereas `netlib-java` can even use commercial vendor packages if needed (such as Intel, AMD or GPU implementations).
    
    Having said all this, the binary compatibility on Linux (of both `netlib-java` and JBLAS) is pretty flakey, as the various distributions tend to introduce arbitrary binary incompatibilities. I tested on Debian,  Ubuntu and RHEL, but I've heard reports that it breaks in SuSe.
    
    From a purely selfish point of view, I recommend Breeze with `netlib-java` if the end API is intended to be Scala, to avoid creating endless layers upon layers and everybody filing bugs with two of my projects :-D That said, I think MTJ's sparse support is pretty good... have you seen the recent ARPACK changes I made? (They'd be pretty trivial to re-implement in Breeze since it is provided by `netlib-java`)
    
    @mengxr I am interested in your benchmarks. Have you seen Peter Abeles' suite? https://code.google.com/p/java-matrix-benchmark/



"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 21:25:52 +0000 (UTC)",[GitHub] incubator-spark pull request: Added extra description on ValueErro...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/581#issuecomment-35126312
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12723/


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 21:25:52 +0000 (UTC)",[GitHub] incubator-spark pull request: Added extra description on ValueErro...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/581#issuecomment-35126310
  
    Merged build finished.


"
jyotiska <git@git.apache.org>,"Fri, 14 Feb 2014 21:29:00 +0000 (UTC)",[GitHub] incubator-spark pull request: Added extra description on ValueErro...,dev@spark.incubator.apache.org,"Github user jyotiska commented on the pull request:

    https://github.com/apache/incubator-spark/pull/581#issuecomment-35126548
  
    @JoshRosen, can you review this and merge? This closes SPARK-1087 along with SPARK-972.


"
fommil <git@git.apache.org>,"Fri, 14 Feb 2014 21:29:16 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user fommil commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35126575
  
    @MLnick btw, I recently added some new ARPACK and linked-sparse matrix structures to MTJ that should give you all some ideas. BTW, I also recently created a ""fast create"" data structure (`CompressedColumns`) for building adjacency graphs: a fast compressed column structure: https://github.com/fommil/shelmet/blob/master/src/main/scala/org/shelmet/heap/parser/hack.scala#L153 the  `LinkedMatrix` structure in the same file was a disappointment and a Scala equivalent of the new  `LinkedSparseMatrix` of https://github.com/fommil/matrix-toolkits-java/


"
tgravescs <git@git.apache.org>,"Fri, 14 Feb 2014 21:32:40 +0000 (UTC)","[GitHub] incubator-spark pull request: Add Security to Spark - Akka, Http, ...",dev@spark.incubator.apache.org,"Github user tgravescs commented on the pull request:

    https://github.com/apache/incubator-spark/pull/332#issuecomment-35126849
  
    @hsaputra, not sure I follow what you mean.  what do you mean by potential override/customerization?  
    
    My plan is to put the user docs in a .md file - describing options and exactly what we are protecting against (ie we aren't doing encryption after auth, etc).
    Then I was going to be more detailed dev description in code comments so that hopefully it stays in sync with the code. 


"
hsaputra <git@git.apache.org>,"Fri, 14 Feb 2014 21:46:04 +0000 (UTC)","[GitHub] incubator-spark pull request: Add Security to Spark - Akka, Http, ...",dev@spark.incubator.apache.org,"Github user hsaputra commented on the pull request:

    https://github.com/apache/incubator-spark/pull/332#issuecomment-35127945
  
    @tgravescs, I meant something like if developer user wants to override the behavior of SparkSaslServer to use different digest than MD5, or maybe somehow override behavior of SecurityManager (it is a private class now I suppose).
    Will such overriding of default flow be allowed or expected? 
    
    Thx.


"
mikiobraun <git@git.apache.org>,"Fri, 14 Feb 2014 21:56:41 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user mikiobraun commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35128801
  
    @fommil apology accepted ;) and yeah, sorry about what I said at ICML, I was misinformed. I think we can call it even! ;) 


"
Evan Chan <ev@ooyala.com>,"Fri, 14 Feb 2014 14:21:29 -0800",Re: proposal: replace lift-json with spray-json,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Pascal,

Ah, I stand corrected, thanks.




-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

"
rezazadeh <git@git.apache.org>,"Fri, 14 Feb 2014 22:36:07 +0000 (UTC)",[GitHub] incubator-spark pull request: Principal Component Analysis,dev@spark.incubator.apache.org,"Github user rezazadeh commented on the pull request:

    https://github.com/apache/incubator-spark/pull/564#discussion_r9767019
  
    The example matrix that is already in the data directory has a matrix that is 1-indexed, I want users to be able to use that. All code is zero-indexed.


"
rezazadeh <git@git.apache.org>,"Fri, 14 Feb 2014 22:36:16 +0000 (UTC)",[GitHub] incubator-spark pull request: Principal Component Analysis,dev@spark.incubator.apache.org,"Github user rezazadeh commented on the pull request:

    https://github.com/apache/incubator-spark/pull/564#discussion_r9767024
  
    fixed


"
rezazadeh <git@git.apache.org>,"Fri, 14 Feb 2014 22:36:21 +0000 (UTC)",[GitHub] incubator-spark pull request: Principal Component Analysis,dev@spark.incubator.apache.org,"Github user rezazadeh commented on the pull request:

    https://github.com/apache/incubator-spark/pull/564#discussion_r9767026
  
    fixed


"
rezazadeh <git@git.apache.org>,"Fri, 14 Feb 2014 22:36:24 +0000 (UTC)",[GitHub] incubator-spark pull request: Principal Component Analysis,dev@spark.incubator.apache.org,"Github user rezazadeh commented on the pull request:

    https://github.com/apache/incubator-spark/pull/564#discussion_r9767028
  
    fixed


"
rezazadeh <git@git.apache.org>,"Fri, 14 Feb 2014 22:36:34 +0000 (UTC)",[GitHub] incubator-spark pull request: Principal Component Analysis,dev@spark.incubator.apache.org,"Github user rezazadeh commented on the pull request:

    https://github.com/apache/incubator-spark/pull/564#discussion_r9767031
  
    Done


"
rezazadeh <git@git.apache.org>,"Fri, 14 Feb 2014 22:38:01 +0000 (UTC)",[GitHub] incubator-spark pull request: Principal Component Analysis,dev@spark.incubator.apache.org,"Github user rezazadeh commented on the pull request:

    https://github.com/apache/incubator-spark/pull/564#discussion_r9767064
  
    It is the standard way to compute the coefficient matrix matrix, even though the principal components aren't affected by it, it's good style in case we want to output the variances later on.


"
mengxr <git@git.apache.org>,"Fri, 14 Feb 2014 22:38:25 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user mengxr commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35132098
  
    @fommil Yes, I mentioned the benchmark suite from Peter to @srowen in my previous comment, but it is designed for dense linear algebra. I put some of the code I used to test sparse linear algebra operations to https://github.com/mengxr/linalg-test. It only contains a small subset of the operations needed to support sparse data in existing algorithms in MLlib. I will try to use the upcoming release of breeze and add more tests later.


"
aarondav <git@git.apache.org>,"Fri, 14 Feb 2014 22:39:30 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1090] improvement on spark_sh...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/599#discussion_r9767096
  
    Right now the only way to specify the spark-shell memory is to use the now-undocumented SPARK_MEM environment variable. I have raised this issue on the JIRA ticket that deprecated SPARK_MEM (https://spark-project.atlassian.net/browse/SPARK-929), and the resolution there may affect how we want to proceed here.
    
    (As an intermediate solution, you can always just add an option which sets SPARK_MEM, and we can change that later to something like SPARK_DRIVER_MEM when we add it.)


"
rezazadeh <git@git.apache.org>,"Fri, 14 Feb 2014 22:39:50 +0000 (UTC)",[GitHub] incubator-spark pull request: Principal Component Analysis,dev@spark.incubator.apache.org,"Github user rezazadeh commented on the pull request:

    https://github.com/apache/incubator-spark/pull/564#discussion_r9767102
  
    done


"
rezazadeh <git@git.apache.org>,"Fri, 14 Feb 2014 22:41:21 +0000 (UTC)",[GitHub] incubator-spark pull request: Principal Component Analysis,dev@spark.incubator.apache.org,"Github user rezazadeh commented on the pull request:

    https://github.com/apache/incubator-spark/pull/564#discussion_r9767137
  
    done


"
rezazadeh <git@git.apache.org>,"Fri, 14 Feb 2014 22:46:23 +0000 (UTC)",[GitHub] incubator-spark pull request: Principal Component Analysis,dev@spark.incubator.apache.org,"Github user rezazadeh commented on the pull request:

    https://github.com/apache/incubator-spark/pull/564#issuecomment-35132663
  
    I made it explicit that the function operates on dense matrices. I will add sparse PCA in a different PR since that will be an entirely different algorithm.
    
    @mengxr All changes made.
    @mateiz The input/output of the function is now a DenseMatrix class, clearly distinguished from SparseMatrix.


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 22:48:04 +0000 (UTC)",[GitHub] incubator-spark pull request: Principal Component Analysis,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/564#issuecomment-35132767
  
    Build started.


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 22:48:04 +0000 (UTC)",[GitHub] incubator-spark pull request: Principal Component Analysis,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/564#issuecomment-35132766
  
     Build triggered.


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 22:49:08 +0000 (UTC)",[GitHub] incubator-spark pull request: Principal Component Analysis,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/564#issuecomment-35132848
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12724/


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 22:49:09 +0000 (UTC)",[GitHub] incubator-spark pull request: Principal Component Analysis,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/564#issuecomment-35132845
  
    Build finished.


"
Pascal Voitot Dev <pascal.voitot.dev@gmail.com>,"Fri, 14 Feb 2014 23:48:56 +0100",Re: proposal: replace lift-json with spray-json,dev <dev@spark.incubator.apache.org>,"No worry!
I didn't want to be too harsh too :D

upper-case was just to make it clear!

Pascal


"
CodingCat <git@git.apache.org>,"Fri, 14 Feb 2014 22:49:47 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1090] improvement on spark_sh...,dev@spark.incubator.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/incubator-spark/pull/599#discussion_r9767385
  
    Hi, @aarondav, just a bit confused. from the code
    
    private[spark] val executorMemory = conf.getOption(""spark.executor.memory"")
        .orElse(Option(System.getenv(""SPARK_MEM"")))
        .map(Utils.memoryStringToMb)
        .getOrElse(512)
    
    SPARK_MEM is to set the memory used by the executor, which has been done in this PR, 
    
    what you are proposing is to control the memory used by the driver. I think it is hard to achieve since users may run spark-shell in a machine out of the control of Spark.


"
aarondav <git@git.apache.org>,"Fri, 14 Feb 2014 22:59:32 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1090] improvement on spark_sh...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/599#discussion_r9767634
  
    This is what I am referring to: https://github.com/CodingCat/incubator-spark/blob/spark_shell_improve/bin/spark-class#L93


"
pwendell <git@git.apache.org>,"Fri, 14 Feb 2014 23:12:19 +0000 (UTC)","[GitHub] incubator-spark pull request: Add Security to Spark - Akka, Http, ...",dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/332#issuecomment-35134709
  
    @hsaputra @tgravescs I think it's fine if, for now, this hard codes the authentication mechanism (based on the description it seems the proposal is to get something working for now). However it would be good to note with in-code documentation where things might be able to use other mechanisms.


"
CodingCat <git@git.apache.org>,"Fri, 14 Feb 2014 23:15:45 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1090] improvement on spark_sh...,dev@spark.incubator.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/incubator-spark/pull/599#discussion_r9768064
  
    OK, I got it


"
CodingCat <git@git.apache.org>,"Fri, 14 Feb 2014 23:19:25 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1090] improvement on spark_sh...,dev@spark.incubator.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/incubator-spark/pull/599#issuecomment-35135807
  
    added a new parameter to set the memory used by spark-shell driver


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 23:28:05 +0000 (UTC)",[GitHub] incubator-spark pull request: Principal Component Analysis,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/564#issuecomment-35137139
  
    Merged build started.


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 23:28:05 +0000 (UTC)",[GitHub] incubator-spark pull request: Principal Component Analysis,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/564#issuecomment-35137137
  
     Merged build triggered.


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 23:28:45 +0000 (UTC)",[GitHub] incubator-spark pull request: Principal Component Analysis,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/564#issuecomment-35137256
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 23:28:45 +0000 (UTC)",[GitHub] incubator-spark pull request: Principal Component Analysis,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/564#issuecomment-35137257
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12725/


"
CodingCat <git@git.apache.org>,"Fri, 14 Feb 2014 23:33:58 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1092] remove SPARK_MEM usage ...,dev@spark.incubator.apache.org,"GitHub user CodingCat opened a pull request:

    https://github.com/apache/incubator-spark/pull/602

    [SPARK-1092] remove SPARK_MEM usage in sparkcontext.scala

    https://spark-project.atlassian.net/browse/SPARK-1092?jql=project%20%3D%20SPARK
    
    Currently, users will usually set SPARK_MEM to control the memory usage of driver programs, (in spark-class)
    91 JAVA_OPTS=""$OUR_JAVA_OPTS""
    92 JAVA_OPTS=""$JAVA_OPTS -Djava.library.path=$SPARK_LIBRARY_PATH""
    93 JAVA_OPTS=""$JAVA_OPTS -Xms$SPARK_MEM -Xmx$SPARK_MEM""
    if they didn't set spark.executor.memory, the value in this environment variable will also affect the memory usage of executors, because the following lines in SparkContext
    privatespark val executorMemory = conf.getOption(""spark.executor.memory"")
    .orElse(Option(System.getenv(""SPARK_MEM"")))
    .map(Utils.memoryStringToMb)
    .getOrElse(512)
    also
    since SPARK_MEM has been (proposed to) deprecated in SPARK-929 (https://spark-project.atlassian.net/browse/SPARK-929) and the corresponding PR (https://github.com/apache/incubator-spark/pull/104)
    we should remove this line

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark clean_spark_mem

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/602.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #602
    
----
commit 31ea1cfe4231c7ff14f2721fa2be99baa43c29d0
Author: CodingCat <zhunansjtu@gmail.com>
Date:   2014-02-14T23:31:03Z

    remove SPARK_MEM usage in sparkcontext.scala

----


"
CodingCat <git@git.apache.org>,"Fri, 14 Feb 2014 23:36:37 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1090] improvement on spark_sh...,dev@spark.incubator.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/incubator-spark/pull/599#issuecomment-35138442
  
    @aarondav  I totally agree that we should deprecate SPARK_MEM, since that PR is still in progress (seems dead), I think we should avoid reading this variable in SparkContext to set executor memory. Otherwise the user may set this variable to limit memory usage of driver but affect the executor memory unintentionally  
    
    I created a PR to remove that line https://github.com/apache/incubator-spark/pull/602


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 23:37:58 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1092] remove SPARK_MEM usage ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/602#issuecomment-35138646
  
    Can one of the admins verify this patch?


"
pwendell <git@git.apache.org>,"Fri, 14 Feb 2014 23:38:33 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1092] remove SPARK_MEM usage ...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/602#issuecomment-35138743
  
    We can't accept this patch because it will regress behavior for existing users. See this JIRA for a proposal relating to SPARK_MEM: https://spark-project.atlassian.net/browse/SPARK-929


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 23:43:06 +0000 (UTC)",[GitHub] incubator-spark pull request: Principal Component Analysis,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/564#issuecomment-35139392
  
     Merged build triggered.


"
aarondav <git@git.apache.org>,"Fri, 14 Feb 2014 23:43:07 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1090] improvement on spark_sh...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/599#issuecomment-35139397
  
    That is ideal, but as Patrick mentioned in #602, we cannot do this because users currently depend on the behavior of SPARK_MEM. Thus we must deprecate it and encourage users not to use it, but still support it for existing users.
    
    This is why we have suggested creating new environment variables for setting the shell memory (and example memory) to avoid changing the executor memory without changing past behavior.


"
AmplabJenkins <git@git.apache.org>,"Fri, 14 Feb 2014 23:43:07 +0000 (UTC)",[GitHub] incubator-spark pull request: Principal Component Analysis,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/564#issuecomment-35139393
  
    Merged build started.


"
pwendell <git@git.apache.org>,"Fri, 14 Feb 2014 23:43:28 +0000 (UTC)",[GitHub] incubator-spark pull request: Support MiMa for reporting binary co...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35139456
  
    Hey @ScrapCodes - this is a great start! To give some background, we actually want to use this and reject pull requests that cause Mima errors. That means we need to configure it so normal development won't return errors by having appropriate excludes. I just ran it and many of the errors seem like false positives - for instance they are in the ""deploy"" package. The Mima code isn't super well documented, but I believe the offer excludes for these things - so it would be great if you could check these out and try to put in the appropriate excludes to distinguish false positives from actual issues.
    



"
CodingCat <git@git.apache.org>,"Fri, 14 Feb 2014 23:45:42 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1092] remove SPARK_MEM usage ...,dev@spark.incubator.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/incubator-spark/pull/602#issuecomment-35139787
  
    @pwendell @aarondav thanks for pointing this out
    
    agree, closed


"
CodingCat <git@git.apache.org>,"Fri, 14 Feb 2014 23:45:44 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1092] remove SPARK_MEM usage ...,dev@spark.incubator.apache.org,"Github user CodingCat closed the pull request at:

    https://github.com/apache/incubator-spark/pull/602


"
CodingCat <git@git.apache.org>,"Fri, 14 Feb 2014 23:47:42 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1092] remove SPARK_MEM usage ...,dev@spark.incubator.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/incubator-spark/pull/602#issuecomment-35139930
  
    @pwendell @aarondav can any of you help to close the related JIRA?
    
    https://spark-project.atlassian.net/browse/SPARK-1092
    
    Thank you


"
AmplabJenkins <git@git.apache.org>,"Sat, 15 Feb 2014 00:10:49 +0000 (UTC)",[GitHub] incubator-spark pull request: Principal Component Analysis,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/564#issuecomment-35140991
  
    Merged build finished.


"
AmplabJenkins <git@git.apache.org>,"Sat, 15 Feb 2014 00:10:49 +0000 (UTC)",[GitHub] incubator-spark pull request: Principal Component Analysis,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/564#issuecomment-35140993
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12726/


"
CodingCat <git@git.apache.org>,"Sat, 15 Feb 2014 00:12:03 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1092] remove SPARK_MEM usage ...,dev@spark.incubator.apache.org,"GitHub user CodingCat reopened a pull request:

    https://github.com/apache/incubator-spark/pull/602

    [SPARK-1092] remove SPARK_MEM usage in sparkcontext.scala

    https://spark-project.atlassian.net/browse/SPARK-1092?jql=project%20%3D%20SPARK
    
    Currently, users will usually set SPARK_MEM to control the memory usage of driver programs, (in spark-class)
    91 JAVA_OPTS=""$OUR_JAVA_OPTS""
    92 JAVA_OPTS=""$JAVA_OPTS -Djava.library.path=$SPARK_LIBRARY_PATH""
    93 JAVA_OPTS=""$JAVA_OPTS -Xms$SPARK_MEM -Xmx$SPARK_MEM""
    if they didn't set spark.executor.memory, the value in this environment variable will also affect the memory usage of executors, because the following lines in SparkContext
    privatespark val executorMemory = conf.getOption(""spark.executor.memory"")
    .orElse(Option(System.getenv(""SPARK_MEM"")))
    .map(Utils.memoryStringToMb)
    .getOrElse(512)
    also
    since SPARK_MEM has been (proposed to) deprecated in SPARK-929 (https://spark-project.atlassian.net/browse/SPARK-929) and the corresponding PR (https://github.com/apache/incubator-spark/pull/104)
    we should remove this line

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark clean_spark_mem

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/602.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #602
    
----
commit 31ea1cfe4231c7ff14f2721fa2be99baa43c29d0
Author: CodingCat <zhunansjtu@gmail.com>
Date:   2014-02-14T23:31:03Z

    remove SPARK_MEM usage in sparkcontext.scala

commit c9b4872d366a5d41400eacd8d2feef0020d0c109
Author: CodingCat <zhunansjtu@gmail.com>
Date:   2014-02-15T00:14:25Z

    print warning information if user use SPARK_MEM to regulate executor memory usage

----


"
AmplabJenkins <git@git.apache.org>,"Sat, 15 Feb 2014 00:12:58 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1092] remove SPARK_MEM usage ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/602#issuecomment-35141094
  
    Can one of the admins verify this patch?


"
CodingCat <git@git.apache.org>,"Sat, 15 Feb 2014 00:17:08 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1092] print warning informati...,dev@spark.incubator.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/incubator-spark/pull/602#issuecomment-35141292
  
    Hi, @pwendell @aarondav, I reopened it and changed my commit to printing warning information
    
    I just think that executor and driver share the same env variable is not so good 


"
koertkuipers <git@git.apache.org>,"Sat, 15 Feb 2014 00:32:01 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user koertkuipers commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35141897
  
    we use mahout-math in a scalding for similar purposes. see here:
    https://github.com/tresata/ganitha
    
    the main motivation for us was the sparse vectors, and some handy routines in mahout for vectorization of input documents. 
    
    however if mahout-math is no longer actively maintained then that's worrisome. i wasnt aware of that.
    
    i ended up on this pull request because i was just considering using mahout vectors in spark for a project... guess i will just follow this thread now and sit tight :)


"
CodingCat <git@git.apache.org>,"Sat, 15 Feb 2014 14:28:35 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1092] print warning informati...,dev@spark.incubator.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/incubator-spark/pull/602#issuecomment-35157378
  
    Hi, @mridulm , I think it will be used in local, mesos, and standalone mode
    
    1. local
    
    case LOCAL_CLUSTER_REGEX(numSlaves, coresPerSlave, memoryPerSlave) =>
            // Check to make sure memory requested <= memoryPerSlave. Otherwise Spark will just hang.
            val memoryPerSlaveInt = memoryPerSlave.toInt
            if (sc.executorMemory > memoryPerSlaveInt) {
              throw new SparkException(
                ""Asked to launch cluster with %d MB RAM / worker but requested %d MB/worker"".format(
                  memoryPerSlaveInt, sc.executorMemory))
            }
    
            val scheduler = new TaskSchedulerImpl(sc)
            val localCluster = new LocalSparkCluster(
              numSlaves.toInt, coresPerSlave.toInt, memoryPerSlaveInt)
            val masterUrls = localCluster.start()
            val backend = new SparkDeploySchedulerBackend(scheduler, sc, masterUrls, appName)
            scheduler.initialize(backend)
            backend.shutdownCallback = (backend: SparkDeploySchedulerBackend) => {
              localCluster.stop()
            }
            scheduler
    
    2. standalone (SparkDeployClusterBackend.scala)
    
    override def start() {
        super.start()
    
        // The endpoint for executors to talk to us
        val driverUrl = ""akka.tcp://spark@%s:%s/user/%s"".format(
          conf.get(""spark.driver.host""),  conf.get(""spark.driver.port""),
          CoarseGrainedSchedulerBackend.ACTOR_NAME)
        val args = Seq(driverUrl, ""{{EXECUTOR_ID}}"", ""{{HOSTNAME}}"", ""{{CORES}}"", ""{{WORKER_URL}}"")
        val command = Command(
          ""org.apache.spark.executor.CoarseGrainedExecutorBackend"", args, sc.executorEnvs)
        val sparkHome = sc.getSparkHome()
        val appDesc = new ApplicationDescription(appName, maxCores, sc.executorMemory, command,
          sparkHome, ""http://"" + sc.ui.appUIAddress)
    
        client = new AppClient(sc.env.actorSystem, masters, appDesc, this, conf)
        client.start()
      }
    
    3. CoarseMesosSchedulerBackend.scala
    
    resourceOffers(d: SchedulerDriver, offers: JList[Offer])


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
nicklan <git@git.apache.org>,"Sat, 15 Feb 2014 18:19:08 +0000 (UTC)",[GitHub] incubator-spark pull request: Tachyon scripts,dev@spark.incubator.apache.org,"Github user nicklan commented on the pull request:

    https://github.com/apache/incubator-spark/pull/603#discussion_r9773703
  
    Yep, that's why the grep is there, so it will only print if it actually kills something.  The tachyon stop-master and stop-worker scripts look specifically for the tachyon processes, so they are no-ops otherwise. 


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
alig <git@git.apache.org>,"Sat, 15 Feb 2014 21:45:04 +0000 (UTC)",[GitHub] incubator-spark pull request: Tachyon scripts,dev@spark.incubator.apache.org,"Github user alig commented on the pull request:

    https://github.com/apache/incubator-spark/pull/603#discussion_r9774589
  
    Should probably also add --with-tachyon as part of the optional parameters around line 30 in this file.


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
mateiz <git@git.apache.org>,"Sat, 15 Feb 2014 21:47:52 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user mateiz commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35168965
  
    Guys, I haven't looked at the full discussion on SPARK-964 yet, but is there no way to avoid the ""of"" wrapper? The wrapper seems pretty awkward, and doesn't make the API *that* much more concise.
    


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
Matei Zaharia <matei.zaharia@gmail.com>,"Sat, 15 Feb 2014 13:52:51 -0800",Re: proposal: replace lift-json with spray-json,dev@spark.incubator.apache.org,"Guys, just to be clear, the JSON use in Spark is primarily to serve the web UI, not to do any heavy parsing. Given that, I wouldn’t update the library for performance, just for other reasons. If you bring in another library that uses Jackson, one problem is to make sure that it also works with other libraries we depend on that use Jackson, like Hadoop. You’d need to test it on multiple Hadoop versions (1.0.4 and up since that’s where we start) to make sure it doesn’t create a problem.

Matei


useful to
Jackson-based
basic
about
json
operating
different
but
build
for
submitting
in


"
mateiz <git@git.apache.org>,"Sat, 15 Feb 2014 21:56:37 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user mateiz commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#discussion_r9774626
  
    You need to add this dependency into the Maven build as well.


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
mateiz <git@git.apache.org>,"Sat, 15 Feb 2014 23:56:11 +0000 (UTC)",[GitHub] incubator-spark pull request: Tachyon scripts,dev@spark.incubator.apache.org,"Github user mateiz commented on the pull request:

    https://github.com/apache/incubator-spark/pull/603#issuecomment-35171986
  
    Hey Nick, regarding the Tachyon JSPs and web resources, couldn't those be included in a Maven artifact published by Tachyon? JSPs should be compilable once and then included as classes in the JAR, while PNGs can be files in it loaded using Class.getResource. Hadoop's web UI uses JSPs and does this for example.


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
mateiz <git@git.apache.org>,"Sat, 15 Feb 2014 23:57:54 +0000 (UTC)",[GitHub] incubator-spark pull request: Tachyon scripts,dev@spark.incubator.apache.org,"Github user mateiz commented on the pull request:

    https://github.com/apache/incubator-spark/pull/603#issuecomment-35172028
  


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
mateiz <git@git.apache.org>,"Sun, 16 Feb 2014 00:08:22 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user mateiz commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35172231
  
    BTW according to http://www.oracle.com/technetwork/articles/java/lambda-1984522.html for example, lambdas should work with method overloading, in which case Java will select the method that best matches the return type of your lambda. Look at their Callable vs Runnable example. So I'd really recommend trying this without the wrappers.


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
mateiz <git@git.apache.org>,"Sun, 16 Feb 2014 00:10:05 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user mateiz commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35172268
  
    See also http://docs.oracle.com/javase/tutorial/java/javaOO/lambdaexpressions.html#target-types-and-method-arguments for example.


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
"""Mattmann, Chris A (3980)"" <chris.a.mattmann@jpl.nasa.gov>","Sun, 16 Feb 2014 07:03:03 +0000",[RESULT] [VOTE] Graduation of Apache Spark from the Incubator,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>,
        ""general@incubator.apache.org"" <general@incubator.apache.org>","Hi Everyone,

This VOTE has passed with the following tallies:

+1
Chris Mattmann*
Andrew Or
Matei Zaharia
Mosharaf Chowdhury
Prabeesh K
Henry Saputra*
Reynold Xin
Aaron Davidson
Prashant Sharma
Shivaram Venkataraman
Evan Sparks
Ameet Talwalkar
Nan Zhu
Azuryy Yu
Kay Outserhout
Bharath Mundlapudi
Mark Hamstra
Sandy Ryza
Andy Konwinski
Patrick Wendell
Zongheng Yang
Matt Massie
Nick Pentreath
Sebastian Schelter*
Alex Karasulu*
Ted Dunning*
Daniel Kulp*
Michael Joyce
Debo Dutta
Mridul Muralidharan
Haoyuan Li
Xuefeng Wu
Andrew Psaltis
Alan Cabrera*
Sean McNamara
Tom Graves
Jyotiska NK
Craig L Russell*
Suresh Marru*
Marvin Humphrey*
Junfeng Feng
Jake Farrell*
Nat Padmanabhan
Josh Rosen
Roman Shaposhnik*
Dave Lester
Doug Cutting*
David Nalley*


I will paste the resolution into the board agenda and we'll await the
result of the board meeting. Thanks for VOTE'ing and for being patient
while we got everything squared away.

Cheers,
Chris




"
mateiz <git@git.apache.org>,"Sun, 16 Feb 2014 07:36:38 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user mateiz commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-35179416
  
    Hey @RongGu, I looked at this and I do see a major problem with the design. The current design only seems to pass an AppID to the executor in standalone mode, so it won't work on Mesos and YARN, and furthermore the AppID passed to SparkEnv on the driver (<driver> + app name) is not guaranteed to be unique because the app name is set by the user. Why not just generate a random name for a temp folder in Tachyon inside SparkContext, and use that throughout the application? That way the driver and worker files can be in the same directory (perhaps under driver/ and executor-<execID>/), it will work with any deploy mode, and it will be guaranteed to be unique.
    
    Also, you can pass the name of this folder to the workers by just setting a property in the SparkConf. There's no need to add a new appID command-line argument and then pass it around throughout the code. I'd prefer using the SparkConf for this.


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
mateiz <git@git.apache.org>,"Sun, 16 Feb 2014 07:38:57 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user mateiz commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-35179446
  
    Also a second note, related to cleanup -- if you make a single top-level directory, then cleanup can just be done in the driver program's shutdown hook, or even better in SparkContext.stop (actually you should do it in both because the user is not guaranteed to call SparkContext.stop; but if they do call that you can remove the shutdown hook).
    
    Finally about the lazy init thing, I personally don't like using a lazy variable if we're confused about the exact semantics and how it interacts with synchronized. Just manage stuff by hand if it gets confusing, the same way you'd do it in Java.


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
ScrapCodes <git@git.apache.org>,"Sun, 16 Feb 2014 09:32:28 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user ScrapCodes commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35180854
  
    Hey Matei,
    
    I have already tried that option, trouble is not with types but with
    closure cleaner which does getResourceAsStream on a lambda expression and
    fails(which has to be fixed incase we chose this path.).
    Another problem is with API, for lambda expression we would need Interfaces
    with apply and at the moment we have a lot of them as abstract classes with
    ""call"" method. Would it be okay to deprecate it ? Or take the other way
    round as to have separate java8 API.
    
    
    
    > See also
    > http://docs.oracle.com/javase/tutorial/java/javaOO/lambdaexpressions.html#target-types-and-method-argumentsfor example.
    >
    > --
    > Reply to this email directly or view it on GitHub<https://github.com/apache/incubator-spark/pull/539#issuecomment-35172268>
    > .
    >
    
    
    
    -- 
    Prashant


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
ScrapCodes <git@git.apache.org>,"Sun, 16 Feb 2014 09:43:57 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user ScrapCodes commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35181038
  
    >Another problem is with API, for lambda expression we would need Interfaces
    with apply and at the moment we have a lot of them as abstract classes with
    ""call"" method. Would it be okay to deprecate it ? Or take the other way
    round as to have separate java8 API.
    This should not bother, I guess I was wrong. 


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
RongGu <git@git.apache.org>,"Sun, 16 Feb 2014 11:56:25 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user RongGu commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#discussion_r9776702
  
    Good suggestion, I followed it.


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
AmplabJenkins <git@git.apache.org>,"Sun, 16 Feb 2014 11:58:18 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-35195000
  
    Merged build started.


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
AmplabJenkins <git@git.apache.org>,"Sun, 16 Feb 2014 11:58:18 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-35194999
  
     Merged build triggered.


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
RongGu <git@git.apache.org>,"Sun, 16 Feb 2014 12:02:55 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user RongGu commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#discussion_r9776720
  
    Thanks for reminding. I've done this now.


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
AmplabJenkins <git@git.apache.org>,"Sun, 16 Feb 2014 12:03:19 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-35195090
  
     Merged build triggered.


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
AmplabJenkins <git@git.apache.org>,"Sun, 16 Feb 2014 12:03:19 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-35195091
  
    Merged build started.


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
AmplabJenkins <git@git.apache.org>,"Sun, 16 Feb 2014 12:26:42 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-35195492
  
    Merged build finished.


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
AmplabJenkins <git@git.apache.org>,"Sun, 16 Feb 2014 12:26:42 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-35195493
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12728/


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
AmplabJenkins <git@git.apache.org>,"Sun, 16 Feb 2014 12:32:29 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-35195598
  
    Merged build finished.


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
AmplabJenkins <git@git.apache.org>,"Sun, 16 Feb 2014 12:32:29 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-35195599
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12729/


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
RongGu <git@git.apache.org>,"Sun, 16 Feb 2014 12:34:34 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user RongGu commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-35195633
  
    Hey @mateiz, thanks for reviewing the code. I read your comment about the directory's name on tachyon. Yes, your concern about the driver's directory name is right. We indeed need a unique top-level directory name here. The reason why I made each executor's directory name to be the appID is to make it better to be shared across SparkContexts in our future plan. I will follow the directory organization of your solution  at our current stage.



If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
RongGu <git@git.apache.org>,"Sun, 16 Feb 2014 13:20:43 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user RongGu commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#discussion_r9776920
  
    Hi,  Patrick. The same as diskStore and memoryStore, the tachyonStore is not an Iterable class here. So, I am afraid we can not use foreach or the map( in the comment below) functions here. I think it's kind of wired to make it iterable, because each executor only has one tachyonStore itself.


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
Punya Biswal <pbiswal@palantir.com>,"Sun, 16 Feb 2014 13:33:08 +0000",Review request for PR#600 (subtractByKey Java API),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Dear Spark devs,

Can someone review my pull request
(https://github.com/apache/incubator-spark/pull/600), which adds
subtractByKey to JavaPairRDD? Thanks!

Punya
"
RongGu <git@git.apache.org>,"Sun, 16 Feb 2014 13:39:22 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user RongGu commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#discussion_r9776978
  
    I agree with your advice. Make it  ""def tachyonStore = synchronized {"" now.


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
RongGu <git@git.apache.org>,"Sun, 16 Feb 2014 13:43:44 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user RongGu commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#discussion_r9776990
  
    Good advice!


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
AmplabJenkins <git@git.apache.org>,"Sun, 16 Feb 2014 13:53:19 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-35197171
  
     Merged build triggered.


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
AmplabJenkins <git@git.apache.org>,"Sun, 16 Feb 2014 13:53:19 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-35197172
  
    Merged build started.


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
RongGu <git@git.apache.org>,"Sun, 16 Feb 2014 13:53:39 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user RongGu commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#discussion_r9777027
  
    Yes, Tachyon also has a concept of inodes. It is necessary in Tachyon.


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
RongGu <git@git.apache.org>,"Sun, 16 Feb 2014 13:57:11 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user RongGu commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#discussion_r9777038
  
    In fact, this comment is copied and modified from the DiskBlockManager to make the comment style consistent. We do not have a mapBlockToFileSegment, Tachyon would handle file segment itself. So, I remove this part of comment now. Thanks for pointing it out.


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
RongGu <git@git.apache.org>,"Sun, 16 Feb 2014 14:02:32 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user RongGu commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#discussion_r9777048
  
    Yes, I have take your advice.


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
RongGu <git@git.apache.org>,"Sun, 16 Feb 2014 14:02:32 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user RongGu commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#discussion_r9777049
  
    Yes, I have taken your advice.


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
RongGu <git@git.apache.org>,"Sun, 16 Feb 2014 14:03:31 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user RongGu commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#discussion_r9777062
  
    okay.


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
RongGu <git@git.apache.org>,"Sun, 16 Feb 2014 14:05:41 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user RongGu commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#discussion_r9777074
  
    This parameter name is mocked from the DiskBlockManager's def getFile(filename: String): File  function. They are similar.


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
RongGu <git@git.apache.org>,"Sun, 16 Feb 2014 14:14:38 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user RongGu commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#discussion_r9777098
  
    In fact they are the same. I followed the style of the DiskStore here. 


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
AmplabJenkins <git@git.apache.org>,"Sun, 16 Feb 2014 14:21:33 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-35197810
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12730/


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
AmplabJenkins <git@git.apache.org>,"Sun, 16 Feb 2014 14:21:33 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-35197809
  
    Merged build finished.


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
AmplabJenkins <git@git.apache.org>,"Sun, 16 Feb 2014 14:28:19 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-35197961
  
     Merged build triggered.


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
AmplabJenkins <git@git.apache.org>,"Sun, 16 Feb 2014 14:28:19 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-35197962
  
    Merged build started.


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
RongGu <git@git.apache.org>,"Sun, 16 Feb 2014 14:43:39 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user RongGu commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-35198329
  
    Hey @pwendell, I have processed the lower level comments put forward by you and updated the code in this PR.  Next, I will modify the namespace of tachyon as suggested by @mateiz.
    
    My respondence to your  high level feedback is as below:
        Tests - 
    Currently, I have put a small UTs about TacyonStore in the BlockManagerSuite.scala. I also set a switch parameter for disabling or enabling the test on Tahcyon. I would add more UTs into it. BTW, I have try some Spark applications on my  my computer, this PR works well on Tachyon:)
        Documentation - 
    Thank you and @ alig for having already given a lot of advice about the Documentation. I will follow your advice and  @haoyuan will also help me with that.
       Code Style - 
    Sorry about that. I have fixed a lot of them. When the PR comes to its code freeze stage, I will check the whole code's style again.
       Update the maven build to include Tachyon as well.-
    I have done this now. Thanks for reminding.
       The amount of recursive dependencies for tachyon is troubling.-
    Yes, that's right. Actually, this is also a work in Tachyon @haoyuan is talking with me these days offline, we are planning to separate Tachyon into different component projects as Tachyon-Client, Tachyon-Server, Tachyon-Common (like core, mllib, etc. in Spark). When this work is done, Spark will just need to include Tachyon-Client into the build file, then it will avoid a lot of jar dependencies. I am planning to do this when this PR's work is finished.


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
AmplabJenkins <git@git.apache.org>,"Sun, 16 Feb 2014 14:56:34 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-35198631
  
    Merged build finished.


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
AmplabJenkins <git@git.apache.org>,"Sun, 16 Feb 2014 14:56:36 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-35198632
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12731/


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
markhamstra <git@git.apache.org>,"Sun, 16 Feb 2014 18:05:26 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user markhamstra commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#discussion_r9777964
  
    Doesn't need to be `Iterable` itself, but only implicitly `Iterable` (as is `Option`).  In fact, what I believe Patrick was suggesting is that `tachyonStore` be made into an `Option` so that the common Scala idiom of `anOption.foreach(doSomethingIfSome(_))` can be used.  Even without changing the type of `tachyonStore`, a form of this idiom can be used anywhere `!= null` is used (but that's not a stylistic quirk that we've adopted consistently): `Option(tachyonStore).foreach(_.doSomething)`


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
aarondav <git@git.apache.org>,"Sun, 16 Feb 2014 18:05:44 +0000 (UTC)",[GitHub] incubator-spark pull request: Support MiMa for reporting binary co...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#discussion_r9777966
  
    Is this a three space indentation?


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
aarondav <git@git.apache.org>,"Sun, 16 Feb 2014 18:07:18 +0000 (UTC)",[GitHub] incubator-spark pull request: Support MiMa for reporting binary co...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#discussion_r9777977
  
    not sure this comment is useful, could be removed


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
aarondav <git@git.apache.org>,"Sun, 16 Feb 2014 18:48:56 +0000 (UTC)",[GitHub] incubator-spark pull request: Add subtractByKey to the JavaPairRDD...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/600#discussion_r9778178
  
    ""cm"" is back from when these were ClassManifests. Everywhere else it's still called that, though, so maybe this is a minor cleanup for another PR.


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
mateiz <git@git.apache.org>,"Sun, 16 Feb 2014 18:54:19 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user mateiz commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35206774
  
    What exactly is the problem with the closure cleaner? In the Java API (JavaRDD and such), we can always wrap the closure into a class in Scala. Why can't we take a closure from them and call Function.of(that) for example?


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
aarondav <git@git.apache.org>,"Sun, 16 Feb 2014 19:07:06 +0000 (UTC)",[GitHub] incubator-spark pull request: Add subtractByKey to the JavaPairRDD...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/600#issuecomment-35207151
  
    CC @pwendell and @JoshRosen 
    
    This looks good to me in that it should work in the style elsewhere in this file, but I have a question for someone more knowledgeable with this area of the code than I. This line of code (used analogously throughout this file):
    
    `implicit val cmw: ClassTag[W] = implicitly[ClassTag[AnyRef]].asInstanceOf[ClassTag[W]]`
    
    doesn't actually get a ClassTag[W], it just gets ClassTag.AnyRef and dresses it up like one. If any of the Scala APIs really needed W's ClassTag, they would fail. But AnyRef can make a poor approximation of W for creating stuff like Arrays, which can certainly hold W's, but also anything else. Are we just lucky that this works right now, or is there a deeper reason for why this is a good idea?


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
aarondav <git@git.apache.org>,"Sun, 16 Feb 2014 19:11:40 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1092] print warning informati...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/602#discussion_r9778280
  
    typo: using => use


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
aarondav <git@git.apache.org>,"Sun, 16 Feb 2014 19:15:49 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1092] print warning informati...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/602#issuecomment-35207406
  
    Looks good to me, just fix the little typo and I'll merge it in. (I would do it myself but I'm not certain how to amend the commit while keeping you as the original author.)
    
    @mridulm You make a good point about YARN mode -- this warning may not be applicable there. I don't think we need to move the warning in this PR, though, since SPARK_MEM will soon be generally deprecated and shouldn't be used directly by anyone.


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
CodingCat <git@git.apache.org>,"Sun, 16 Feb 2014 19:19:39 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1092] print warning informati...,dev@spark.incubator.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/incubator-spark/pull/602#issuecomment-35207540
  
    @aarondav just fixed


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
aarondav <git@git.apache.org>,"Sun, 16 Feb 2014 19:40:02 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1092] print warning informati...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/602#issuecomment-35208183
  
    Jenkins doesn't like me at the moment, @rxin is trying to get that fixed. In the meantime, can someone with permissions say ""Jenkins, test this please."" ?


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
AmplabJenkins <git@git.apache.org>,"Sun, 16 Feb 2014 19:43:01 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1092] print warning informati...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/602#issuecomment-35208277
  
     Merged build triggered.


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
AmplabJenkins <git@git.apache.org>,"Sun, 16 Feb 2014 19:43:01 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1092] print warning informati...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/602#issuecomment-35208278
  
    Merged build started.


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
aarondav <git@git.apache.org>,"Sun, 16 Feb 2014 20:02:56 +0000 (UTC)",[GitHub] incubator-spark pull request: Add subtractByKey to the JavaPairRDD...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/600#issuecomment-35208921
  
    To answer my own question, the reason we do this is probably because we have no choice when integrating with Java. Additionally, use of ClassTag as in our API rarely affects correctness, just potentially safety and performance (e.g., creating a primitive-backed map versus an object one).
    
    This unfortunately adds a burden we must keep in mind as we develop the Spark Scala APIs:
    
    - ClassTags should be used sparingly and never relied upon for correctness.
    - The Java API may be unavoidably slower than the Scala API where ClassTags are used for perf.
    
    This also means it is very hard, in general, to verify the correctness of a Java API like the one introduced in this patch, since we're tricking the compiler and introducing unusual runtime behavior.
    
    In this particular case, I can verify that the API will work because subtractByKey never actually uses W's ClassTag (I can even remove it from the signature without issue), but that's not guaranteed to hold in the future.


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
AmplabJenkins <git@git.apache.org>,"Sun, 16 Feb 2014 20:11:14 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1092] print warning informati...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/602#issuecomment-35209157
  
    Merged build finished.


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
AmplabJenkins <git@git.apache.org>,"Sun, 16 Feb 2014 20:11:14 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1092] print warning informati...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/602#issuecomment-35209158
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12732/


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
aarondav <git@git.apache.org>,"Sun, 16 Feb 2014 20:26:19 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1092] print warning informati...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/602#issuecomment-35211178
  
    Neato, Jenkins actually listened to me. Merged into master, thanks!


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
kellrott <git@git.apache.org>,"Sun, 16 Feb 2014 20:28:24 +0000 (UTC)",[GitHub] incubator-spark pull request: Patch for SPARK-942,dev@spark.incubator.apache.org,"Github user kellrott commented on the pull request:

    https://github.com/apache/incubator-spark/pull/180#issuecomment-35211241
  
    Are there any other remaining issues that are preventing this pull request from being reviewed/merged?


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
martinjaggi <git@git.apache.org>,"Sun, 16 Feb 2014 20:58:00 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user martinjaggi commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35212055
  
    Really looking forward to having sparse vectors in MLlib soon, this is super important! And thanks for your efforts so far!
    
    Just a quick comment about the benchmarks and requirements:
    The biggest impact of sparse vectors will likely be in the classification&regression methods, where the theoretical speedup is linear with the sparsity of the vectors. 
    This is since the (sparse) vectors are all that is communicated in each round (e.g. in SGD). It's not only that the original data was sparse (as in the current k-means benchmark). To send such things over spark, super **fast serialization** is essential. It shouldn't be that hard to implement, since as @mengxr already mentioned, all we need here is sequential access sparse vectors (backed by two parallel arrays). But I see that it can be quite an architecture question.
    
    When comparing different implementations, I think it would therefore be convenient to see how they impact SGD, for example in logistic regression on some realistic data with 1% sparsity or so.
    
    Sanjay Krishnan had some good results with using `BidMat` as an implementation for exactly this, maybe we could ask him.


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
CodingCat <git@git.apache.org>,"Sun, 16 Feb 2014 21:27:31 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1092] print warning informati...,dev@spark.incubator.apache.org,"Github user CodingCat closed the pull request at:

    https://github.com/apache/incubator-spark/pull/602


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
fommil <git@git.apache.org>,"Sun, 16 Feb 2014 21:58:12 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user fommil commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35216981
  
    @martinjaggi I believe you would be making a massive mistake by agreeing on a single serialisation scheme for sparse vectors, unless that format is independent of the storage structure, e.g. MatrixMarket, or if you're happy to pass implementation detail of the sparse format along with the structure. BTW, solving the generic problem of building a sparse matrix from a data stream is far from solved... some structures are just intrinsically slow to construct and others require up-front hints to avoid waste during their construction.
    
    Most of the battle when doing sparse matrix calculations is choosing, or writing, an appropriate data storage format that best suits the problem. Having a toolkit of generic sparse storage formats is good, but orders of magnitude improvement can be obtained with a custom made format. What works for you might be really bad for somebody else.
    
    There is no silver bullet sparse matrix format.
    
    MTJ has compressed row, compressed column and compressed diagonal formats (with some custom sparse structures for special matrices) and a special doubly linked structure for unstructured sparse matrices.
    
    Insertion is usually slow for structures that are optimised for calculations. I recently created a compressed column structure holding a hand-crafted primitive linked list... optimised for insertion speed, but potentially duplicating entries, and definitely not optimal for row traversal (although column traversal would be reasonable). If insertion is to be performed concurrently, then that again requires more thought.
    
    I don't know what you mean by sequential access because that depends on how you're doing the sequential ordering: by row, column, diagonal, arrowhead? To optimise calculations, you want the JIT to be able to treat everything as close as possible to aligned arrays to help the CPU caches (which is by far the biggest bottleneck in linear algebra). Exotic data structures can look great on paper, but the JIT just gets confused and all of a sudden you're into RAM access.
    
    Btw, 1% sparsity is actually quite dense when you're talking about really large matrices. I think whatever benchmarks you're coming up with, they are probably going to be highly biased to the kind of problems that you're solving. Creating a suite of benchmarks would be quite the undertaking!
    
    Basically, I'm saying that Spark should remain flexible for future sparse formats and shouldn't pick one at this early stage because it works well for logistic regression.



If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
martinjaggi <git@git.apache.org>,"Sun, 16 Feb 2014 22:11:37 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user martinjaggi commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35217718
  
    Hope you don't get me wrong, I was not at all proposing to fix a single scheme, neither for serialization, or for the choice of sparse library. I was just suggesting that the existing MLlib classification/regression code would be a nice benchmark to see how the several candidate implementations perform in reality (these only need vectors, no matrices). No matter what we will choose, serialization time will also play an important role in the end.


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
fommil <git@git.apache.org>,"Sun, 16 Feb 2014 22:20:47 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user fommil commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35218098
  
    @martinjaggi I'm happy to advise on what the best sparse format would be for any particular problem that you're wanting to solve in spark. just let me know the matrix operations that you're performing (noting the sorts of structures you expect for each symbol) and at what points the formats have to be sent over the wire.
    
    I wouldn't get too caught up on sparse benchmarks. All they will show is which storage format works well for that problem. I could give you some incredibly efficient sparse formats that will epically fail that test, because they are designed for another problem. Column vs Row compression is a classic example: column compressed are great for multiplication from the right (or transpose mult) whereas row compression are great for multiplication from the left... but even that depends on the format of the matrix or vector on the right. And this might not be the most efficient format from a memory PoV... what if the matrices have a low band size?



If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
martinjaggi <git@git.apache.org>,"Sun, 16 Feb 2014 22:35:22 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user martinjaggi commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35218573
  
    @fommil No matrix operations are performed at all so far, only vector addition (of type dense += sparse). See the code in this PR by @mengxr . Vector operations are enough for clustering, classification and regression as currently in MLlib. I was referring to the k-Means benchmark posted in the JIRA.


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
dlwh <git@git.apache.org>,"Sun, 16 Feb 2014 22:46:51 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user dlwh commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35218872
  
    @martinjaggi For how it's usually implemented, that's right. But you can
    quite likely get better performance doing minibatches with dense vector/CSC
    multiply in lieu of a bunch of dot products.
    
    
    
    > @fommil <https://github.com/fommil> No matrix operations are performed at
    > all so far, only vector addition (of type dense += sparse). See the code in
    > this PR by @mengxr <https://github.com/mengxr> . Vector operations are
    > enough for clustering, classification and regression as currently in MLlib.
    > I was referring to the k-Means benchmark posted in the JIRA.
    >
    > â€”
    > Reply to this email directly or view it on GitHub<https://github.com/apache/incubator-spark/pull/575#issuecomment-35218573>
    > .
    >


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
mateiz <git@git.apache.org>,"Sun, 16 Feb 2014 22:47:06 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user mateiz commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35218879
  
    BTW, I've looked into this myself, and created a short project at https://github.com/mateiz/java8-test to show how an RDD-like API might work in Java. To make a long story short:
    
    * It looks like map() operations overloaded by just the return type of the lambda don't work (e.g. if you have `map(Function<T, U>)` as well as `map(PairFunction<T, K, V>)`).
    * The solution for this in Java 8's own [Stream](http://download.java.net/jdk8/docs/api/java/util/stream/Stream.html) API is to have separate functions for mapping to ""specialized"" types, like `mapToInt` and `mapToLong`. (Apparently at some point an overloaded `map` also worked, see [this](https://www.surveymonkey.com/sr.aspx?sm=9UyN8RRvMX8BnpTdd4rYgDlXU9uUVALNDjNn_2fY2e9_2fo_3d), but I'm not sure we can make it happen again.)
    
    Given this, I'd like us to simply add mapToPair and mapToDouble, and same for flatMap and such. If we do it, we can keep our API functions accepting lambda expressions throughout. I'm very much against requiring the user to write Function.of -- whatever that does, we should be able to do it after on the raw lambda expression in order to convert it to a Scala Function object.
    
    Personally I'm okay if this breaks the current Java API slightly, though it may also be possible to do it in a backward-compatible way (e.g. keep our current PairFunction, which is an abstract class so it can't be passed through as a lambda expression anyway, and add a new one that is an interface). But let's figure that out after we have a basic version working with the ""new"" API we want.


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
mateiz <git@git.apache.org>,"Sun, 16 Feb 2014 22:47:55 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user mateiz commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35218897
  
    BTW a usage example for my test project is https://github.com/mateiz/java8-test/blob/master/src/main/java/test/Main.java. This is what I'd like our code to look like.


If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.

"
mateiz <git@git.apache.org>,"Sun, 16 Feb 2014 22:57:31 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user mateiz commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#discussion_r9779496
  
    These factory methods can probably just be called `dense`, `sparse`, etc.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
martinjaggi <git@git.apache.org>,"Sun, 16 Feb 2014 23:17:57 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user martinjaggi commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35219684
  
    @dlwh Thanks! This is of course a nice idea. Perhaps surprisingly (and good for us) such tricks seem not even necessary in the current state of the art algorithms. It's usually faster to do the smaller but earlier updates after each dot-product, i.e. each worker/thread doing one dot product and then immediately updating its weight vector (typical in SGD for example).
    
    Taking a step back, I think the PR by @mengxr here is very nice and providing the right kind of interface for all stuff relying on vectors. (Just saying that we have to keep an eye on serialization speed, but that seems well possible with the current code structure, right?) 


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
dlwh <git@git.apache.org>,"Sun, 16 Feb 2014 23:39:49 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user dlwh commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35220185
  
    @martinjaggi I've often found that minibatching makes things converge much
    more quickly, since you get a nice variance reduction in the estimate of
    the gradient, and doesn't prevent any of the other tricks you described.
    That said, I mostly deal with structured prediction, not classification, so
    I'll defer to your experience.
    
    
    
    > @dlwh <https://github.com/dlwh> Thanks! This is of course a nice idea.
    > Perhaps surprisingly (and good for us) such tricks seem not even necessary
    > in the current state of the art algorithms. It's usually faster to do the
    > smaller but earlier updates after each dot-product, i.e. each worker/thread
    > doing one dot product and then immediately updating its weight vector
    > (typical in SGD for example).
    >
    > Taking a step back, I think the PR by @mengxr <https://github.com/mengxr>here is very nice and providing the right kind of interface for all stuff
    > relying on vectors. (Just saying that we have to keep an eye on
    > serialization speed, but that seems well possible with the current code
    > structure, right?)
    >
    > â€”
    > Reply to this email directly or view it on GitHub<https://github.com/apache/incubator-spark/pull/575#issuecomment-35219684>
    > .
    >


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
martinjaggi <git@git.apache.org>,"Sun, 16 Feb 2014 23:51:17 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user martinjaggi commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35220431
  
    @dlwh actually i think it's the same story in structured prediction (SGD or BCFW), immediate updates on the vector are usually faster for the local machine. but for structured stuff we can then open a separate PR maybe ;)


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Mon, 17 Feb 2014 00:26:28 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1098: Minor cleanup of ClassTa...,dev@spark.incubator.apache.org,"GitHub user aarondav opened a pull request:

    https://github.com/apache/incubator-spark/pull/604

    SPARK-1098: Minor cleanup of ClassTag usage in Java API

    Our usage of fake ClassTags in this manner is probably not healthy, but I'm not sure if there's a better solution available, so I just cleaned up and documented the current one.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark master

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/604.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #604
    
----
commit cf5ffd30f6e3bb749befc1945639e3fbfef7804b
Author: Aaron Davidson <aaron@databricks.com>
Date:   2014-02-16T20:25:10Z

    SPARK-1098: Minor cleanup of ClassTag usage in Java API

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Mon, 17 Feb 2014 00:27:49 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1098: Minor cleanup of ClassTa...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/604#discussion_r9780093
  
    Note: I removed the ClassTag for V, as it was not necessary. Also, I reordered the type parameters to put K in front.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 00:27:59 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1098: Minor cleanup of ClassTa...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/604#issuecomment-35221290
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 00:27:59 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1098: Minor cleanup of ClassTa...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/604#issuecomment-35221292
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Mon, 17 Feb 2014 00:29:13 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1098: Minor cleanup of ClassTa...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/604#discussion_r9780099
  
    ClassTags do not store generic information, so here we are still just finding Tuple2.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Mon, 17 Feb 2014 00:32:17 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1098: Minor cleanup of ClassTa...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/604#discussion_r9780111
  
    These changes may actually be problematic, as they are part of a publicly accessible API. Even removing the specialization of V is not really backwards-compatible for explicit callers from Java (though that's unlikely). Definitely willing revert either/both the ordering and the extraneous ClassTag.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Mon, 17 Feb 2014 00:36:56 +0000 (UTC)",[GitHub] incubator-spark pull request: Tachyon scripts,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/603#discussion_r9780128
  
    I don't know much about log4j, but could this accidentally override Spark's own logging properties if it is the first log4j.properties file found on the classpath?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Mon, 17 Feb 2014 00:40:08 +0000 (UTC)",[GitHub] incubator-spark pull request: Tachyon scripts,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/603#issuecomment-35221556
  
    Sorry if this is a stupid question, but why do we need Tachyon JSPs? Are we going to host Tachyon pages from our own UI?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Mon, 17 Feb 2014 00:43:13 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1090] improvement on spark_sh...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/599#discussion_r9780175
  
    It is perhaps unfortunate that SPARK_WORKER_MEMORY actually does exist, and controls the total amount of memory that a worker can lease across all executors on a machine. Since this is not the property we want, perhaps we'll be relegated to ""-em --executormem"" (kinda ugly, maybe there's something nicer-looking).


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
CodingCat <git@git.apache.org>,"Mon, 17 Feb 2014 00:45:34 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1090] improvement on spark_sh...,dev@spark.incubator.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/incubator-spark/pull/599#discussion_r9780199
  
    en....maybe --execmem?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
CodingCat <git@git.apache.org>,"Mon, 17 Feb 2014 00:45:43 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1090] improvement on spark_sh...,dev@spark.incubator.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/incubator-spark/pull/599#discussion_r9780202
  
    still ugly...


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Mon, 17 Feb 2014 00:46:13 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1090] improvement on spark_sh...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/599#discussion_r9780203
  
    A slightly confusing point is that the driver is in the same memory space as the shell, and we're really just controlling the memory of the shell process itself. I think the wording ""the memory used by the spark shell and driver"" may be slightly clearer in this sense.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Mon, 17 Feb 2014 00:47:32 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1090] improvement on spark_sh...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/599#discussion_r9780214
  
    Most of the rest of the code in this file uses echo, so I would avoid changing this unless you have a good reason.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Mon, 17 Feb 2014 00:51:34 +0000 (UTC)",[GitHub] incubator-spark pull request: Default log4j initialization causes ...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/573#issuecomment-35221813
  
    I'm not 100% caught up on the state of this issue. Is #570 a ""complete fix"" for this issue, or is this PR still the best fix we have in the pipeline? Is it in a mergeable state? Github still seems confused on the diff.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Mon, 17 Feb 2014 00:54:31 +0000 (UTC)",[GitHub] incubator-spark pull request: fix for https://spark-project.atlass...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/568#issuecomment-35221890
  
    Merged in master and branch-0.9. Thanks!


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 00:56:01 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1098: Minor cleanup of ClassTa...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/604#issuecomment-35221936
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 00:56:01 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1098: Minor cleanup of ClassTa...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/604#issuecomment-35221937
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12733/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
CodingCat <git@git.apache.org>,"Mon, 17 Feb 2014 00:57:14 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1090] improvement on spark_sh...,dev@spark.incubator.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/incubator-spark/pull/599#discussion_r9780299
  
    I thought that ""Created spark context"" is something signalling a significant step in starting spark-shell, we'd better write it to the log file to facilitate debugging 
    
    
    



---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
holdenk <git@git.apache.org>,"Mon, 17 Feb 2014 01:00:11 +0000 (UTC)",[GitHub] incubator-spark pull request: MLI-2: Start adding k-fold cross val...,dev@spark.incubator.apache.org,"Github user holdenk commented on the pull request:

    https://github.com/apache/incubator-spark/pull/572#discussion_r9780330
  
    Done :)


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
holdenk <git@git.apache.org>,"Mon, 17 Feb 2014 01:00:40 +0000 (UTC)",[GitHub] incubator-spark pull request: MLI-2: Start adding k-fold cross val...,dev@spark.incubator.apache.org,"Github user holdenk commented on the pull request:

    https://github.com/apache/incubator-spark/pull/572#discussion_r9780337
  
    oops, fixed


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
holdenk <git@git.apache.org>,"Mon, 17 Feb 2014 01:01:06 +0000 (UTC)",[GitHub] incubator-spark pull request: MLI-2: Start adding k-fold cross val...,dev@spark.incubator.apache.org,"Github user holdenk commented on the pull request:

    https://github.com/apache/incubator-spark/pull/572#discussion_r9780341
  
    Done


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
holdenk <git@git.apache.org>,"Mon, 17 Feb 2014 01:01:56 +0000 (UTC)",[GitHub] incubator-spark pull request: MLI-2: Start adding k-fold cross val...,dev@spark.incubator.apache.org,"Github user holdenk commented on the pull request:

    https://github.com/apache/incubator-spark/pull/572#discussion_r9780345
  
    Changed in both places.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Mon, 17 Feb 2014 01:02:10 +0000 (UTC)",[GitHub] incubator-spark pull request: Fix typos in Spark Streaming program...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/536#discussion_r9780346
  
    Not sure if this is the section you were talking about: http://kafka.apache.org/documentation.html#kafkahadoopconsumerapi


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Mon, 17 Feb 2014 01:05:10 +0000 (UTC)",[GitHub] incubator-spark pull request: Fix typos in Spark Streaming program...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/536#discussion_r9780371
  
    I don't think a semicolon is gramatically correct here. In fact, I think
    ""The `updateStateByKey` operation allows you to maintain some state data and continuously update it with new information.""
    sounds reasonable. ""Arbitrary stateful computation"" is probably one of the definitions of a Turing machine, after all.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
CodingCat <git@git.apache.org>,"Mon, 17 Feb 2014 01:09:22 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1090] improvement on spark_sh...,dev@spark.incubator.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/incubator-spark/pull/599#discussion_r9780400
  
    fixed the above two


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
bijaybisht <git@git.apache.org>,"Mon, 17 Feb 2014 01:27:03 +0000 (UTC)",[GitHub] incubator-spark pull request: fix for https://spark-project.atlass...,dev@spark.incubator.apache.org,"Github user bijaybisht closed the pull request at:

    https://github.com/apache/incubator-spark/pull/568


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
holdenk <git@git.apache.org>,"Mon, 17 Feb 2014 01:53:11 +0000 (UTC)",[GitHub] incubator-spark pull request: MLI-2: Start adding k-fold cross val...,dev@spark.incubator.apache.org,"Github user holdenk commented on the pull request:

    https://github.com/apache/incubator-spark/pull/572#discussion_r9780813
  
    SharedSparkContext isn't available inside of mllutils tests.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
mateiz <git@git.apache.org>,"Mon, 17 Feb 2014 01:57:47 +0000 (UTC)",[GitHub] incubator-spark pull request: Tachyon scripts,dev@spark.incubator.apache.org,"Github user mateiz commented on the pull request:

    https://github.com/apache/incubator-spark/pull/603#issuecomment-35223558
  
    @aarondav we don't, Tachyon actually compiles them at runtime it seems, but you can compile them when you publish the Tachyon JAR to avoid that.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
holdenk <git@git.apache.org>,"Mon, 17 Feb 2014 02:18:50 +0000 (UTC)",[GitHub] incubator-spark pull request: MLI-2: Start adding k-fold cross val...,dev@spark.incubator.apache.org,"Github user holdenk commented on the pull request:

    https://github.com/apache/incubator-spark/pull/572#discussion_r9781015
  
    Removed


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 02:23:06 +0000 (UTC)",[GitHub] incubator-spark pull request: MLI-2: Start adding k-fold cross val...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/572#issuecomment-35224270
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 02:23:06 +0000 (UTC)",[GitHub] incubator-spark pull request: MLI-2: Start adding k-fold cross val...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/572#issuecomment-35224269
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Mon, 17 Feb 2014 02:49:44 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1090] improvement on spark_sh...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/599#discussion_r9781246
  
    pattern should start with ^ and end with $ -- just tried with something like ""4gz"" and it passed


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Mon, 17 Feb 2014 02:49:48 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1090] improvement on spark_sh...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/599#discussion_r9781248
  
    change OPTIONS to SPARK_SHELL_OPTS!


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Mon, 17 Feb 2014 02:49:45 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1090] improvement on spark_sh...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/599#discussion_r9781247
  
    update for -em


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Mon, 17 Feb 2014 02:50:21 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1090] improvement on spark_sh...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/599#discussion_r9781252
  
    nit: maybe ""the maximum number of cores to be used by the spark shell""


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 02:51:06 +0000 (UTC)",[GitHub] incubator-spark pull request: MLI-2: Start adding k-fold cross val...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/572#issuecomment-35225143
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12734/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 02:51:06 +0000 (UTC)",[GitHub] incubator-spark pull request: MLI-2: Start adding k-fold cross val...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/572#issuecomment-35225140
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Mon, 17 Feb 2014 02:51:51 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1090] improvement on spark_sh...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/599#discussion_r9781258
  
    update or remove comment at the top of this file that talks about the options -- removal is fine since we have the list in code now


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Mon, 17 Feb 2014 02:59:03 +0000 (UTC)",[GitHub] incubator-spark pull request: Add subtractByKey to the JavaPairRDD...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/600#issuecomment-35225376
  
    This PR doesn't need to block on this discussion, since it doesn't actually rely on the fake ClassTag, so I've merged it into master.
    
    I've created [JIRA-1096](https://spark-project.atlassian.net/browse/SPARK-1098) to track any further discussion and PR #604 to add documentation for this issue. I will update #604 to include subtractByKey once this gets merged into github.
    
    Thanks, @punya!


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
CodingCat <git@git.apache.org>,"Mon, 17 Feb 2014 03:13:37 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1090] improvement on spark_sh...,dev@spark.incubator.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/incubator-spark/pull/599#issuecomment-35225862
  
    @aarondav thank you for the comments, another round of fix


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Mon, 17 Feb 2014 03:18:31 +0000 (UTC)",[GitHub] incubator-spark pull request: add event listener when executors ar...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/597#issuecomment-35226013
  
    What is the use-case you have in mind here? Just some sort of final status of all executors right before terminating a job/shell?
    
    If you're just interested in the HDFS stats, you might take a look at our [MetricsSystem](https://github.com/apache/incubator-spark/blob/master/core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala#L32), where we do register [hdfs bytes_read](https://github.com/apache/incubator-spark/blob/master/core/src/main/scala/org/apache/spark/executor/ExecutorSource.scala#L66) (sorry, this part of the code is a little hairy to follow). If you can attach to our metrics sink, you may be able to get the info you're looking for without modification to Spark.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
punya <git@git.apache.org>,"Mon, 17 Feb 2014 03:28:41 +0000 (UTC)",[GitHub] incubator-spark pull request: Add subtractByKey to the JavaPairRDD...,dev@spark.incubator.apache.org,"Github user punya closed the pull request at:

    https://github.com/apache/incubator-spark/pull/600


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
punya <git@git.apache.org>,"Mon, 17 Feb 2014 03:28:41 +0000 (UTC)",[GitHub] incubator-spark pull request: Add subtractByKey to the JavaPairRDD...,dev@spark.incubator.apache.org,"Github user punya commented on the pull request:

    https://github.com/apache/incubator-spark/pull/600#issuecomment-35226354
  
    Thanks @aarondav! I wondered about the dishonest `ClassTag`s too but convinced myself that in this case it was totally harmless because `subtractByKey` ignores the second component of the pair anyway.
    
    I'll work on a separate PR to rename the variables `cmw â†’ wTag` across `JavaRDD` and friends.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Mon, 17 Feb 2014 03:41:51 +0000 (UTC)",[GitHub] incubator-spark pull request: Add subtractByKey to the JavaPairRDD...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/600#issuecomment-35226744
  
    No need for your renaming PR @punya -- I've already taken care of it in #604 :) Thanks!


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Mon, 17 Feb 2014 03:47:55 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1098: Minor cleanup of ClassTa...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/604#discussion_r9781736
  
    Went ahead and made these private[spark]. It seems unlikely that anyone else would use these methods, and making them private means that the type parameter reordering and ClassTag removal is not a compatibility issue.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 03:48:03 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1098: Minor cleanup of ClassTa...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/604#issuecomment-35226956
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 03:48:03 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1098: Minor cleanup of ClassTa...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/604#issuecomment-35226957
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Mon, 17 Feb 2014 03:48:35 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1090] improvement on spark_sh...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/599#issuecomment-35226986
  
    Jenkins, test this please.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 03:53:02 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1090] improvement on spark_sh...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/599#issuecomment-35227135
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 03:53:02 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1090] improvement on spark_sh...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/599#issuecomment-35227134
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
jyotiska <git@git.apache.org>,"Mon, 17 Feb 2014 03:54:15 +0000 (UTC)",[GitHub] incubator-spark pull request: Added extra description on ValueErro...,dev@spark.incubator.apache.org,"Github user jyotiska commented on the pull request:

    https://github.com/apache/incubator-spark/pull/581#issuecomment-35227176
  
    @JoshRosen any updates on this?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
punya <git@git.apache.org>,"Mon, 17 Feb 2014 04:02:01 +0000 (UTC)",[GitHub] incubator-spark pull request: Move all Java code to src/main/java,dev@spark.incubator.apache.org,"GitHub user punya opened a pull request:

    https://github.com/apache/incubator-spark/pull/605

    Move all Java code to src/main/java

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark move-java-sources

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/605.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #605
    
----
commit 2154959d5bb624a7442ebf2b5bee752d9cc7417d
Author: Punya Biswal <pbiswal@palantir.com>
Date:   2014-02-17T03:59:56Z

    Move all Java code to src/main/java

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 04:03:01 +0000 (UTC)",[GitHub] incubator-spark pull request: Move all Java code to src/main/java,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/605#issuecomment-35227425
  
    Can one of the admins verify this patch?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 04:16:44 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1098: Minor cleanup of ClassTa...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/604#issuecomment-35227822
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12735/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 04:16:44 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1098: Minor cleanup of ClassTa...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/604#issuecomment-35227821
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 04:21:40 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1090] improvement on spark_sh...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/599#issuecomment-35227969
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12736/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 04:21:40 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1090] improvement on spark_sh...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/599#issuecomment-35227968
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Mon, 17 Feb 2014 04:26:13 +0000 (UTC)",[GitHub] incubator-spark pull request: Move all Java code to src/main/java,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/605#issuecomment-35228095
  
    Moving these to src/main/java is a good idea, but I wonder if most of these files would be better refactored into Scala. This should be trivial for all except JavaSparkContextVarargsWorkaround, which I would be happy to see stay go into src/main/java. The others are either very core Spark code (StorageLevels and SparkFiles) or linked closely with Scala code (Function, etc.), so it'd be pleasant if they could stay in the same directory as their peers.
    
    Such a refactor would maintain API compatibility but break binary compatibility. This should be perfectly fine for the 1.0 release, though.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
punya <git@git.apache.org>,"Mon, 17 Feb 2014 04:31:15 +0000 (UTC)",[GitHub] incubator-spark pull request: Move all Java code to src/main/java,dev@spark.incubator.apache.org,"Github user punya commented on the pull request:

    https://github.com/apache/incubator-spark/pull/605#issuecomment-35228237
  
    I'll give it a shot.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 05:13:24 +0000 (UTC)",[GitHub] incubator-spark pull request: Deprecated and added a few java api ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/402#issuecomment-35229331
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 05:13:24 +0000 (UTC)",[GitHub] incubator-spark pull request: Deprecated and added a few java api ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/402#issuecomment-35229332
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
nicklan <git@git.apache.org>,"Mon, 17 Feb 2014 05:13:38 +0000 (UTC)",[GitHub] incubator-spark pull request: Tachyon scripts,dev@spark.incubator.apache.org,"Github user nicklan commented on the pull request:

    https://github.com/apache/incubator-spark/pull/603#discussion_r9782432
  
    Not sure on this one, but spark shouldn't end up having this on the classpath anyway right, since it's buried in the sbin/tachyon folder


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
nicklan <git@git.apache.org>,"Mon, 17 Feb 2014 05:17:56 +0000 (UTC)",[GitHub] incubator-spark pull request: Tachyon scripts,dev@spark.incubator.apache.org,"Github user nicklan commented on the pull request:

    https://github.com/apache/incubator-spark/pull/603#issuecomment-35229452
  
    @mateiz I agree it would be good to have the tachyon scripts inside the jar, but afaik there isn't currently a JAR available with them anywhere, so I just copiedwe don't have that right now, so unless we want to build and distribute a tachyon jar them over.  Do we want to push HY to publish a JAR with scripts?
    
    I haven't ever worked with JSP stuff before, but I'll have a look at it.  I assume that we could have it in the JAR, but again, we'd have to publish it ourselves.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 05:18:23 +0000 (UTC)",[GitHub] incubator-spark pull request: Deprecated and added a few java api ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/402#issuecomment-35229464
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 05:18:23 +0000 (UTC)",[GitHub] incubator-spark pull request: Deprecated and added a few java api ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/402#issuecomment-35229465
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 05:23:29 +0000 (UTC)",[GitHub] incubator-spark pull request: Deprecated and added a few java api ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/402#issuecomment-35229620
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12737/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 05:23:29 +0000 (UTC)",[GitHub] incubator-spark pull request: Deprecated and added a few java api ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/402#issuecomment-35229619
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
punya <git@git.apache.org>,"Mon, 17 Feb 2014 05:39:36 +0000 (UTC)",[GitHub] incubator-spark pull request: Migrate Java code to Scala or move i...,dev@spark.incubator.apache.org,"Github user punya commented on the pull request:

    https://github.com/apache/incubator-spark/pull/605#issuecomment-35230127
  
    Jenkins, test this please.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
ScrapCodes <git@git.apache.org>,"Mon, 17 Feb 2014 05:42:27 +0000 (UTC)",[GitHub] incubator-spark pull request: Support MiMa for reporting binary co...,dev@spark.incubator.apache.org,"Github user ScrapCodes commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35230219
  
    Hey Patrick,
    


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 05:46:44 +0000 (UTC)",[GitHub] incubator-spark pull request: Deprecated and added a few java api ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/402#issuecomment-35230376
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 05:46:44 +0000 (UTC)",[GitHub] incubator-spark pull request: Deprecated and added a few java api ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/402#issuecomment-35230379
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12738/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 06:48:03 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35232312
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 06:48:03 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35232311
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 07:16:26 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35233367
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 07:16:26 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35233368
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12739/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
NirmalReddy <git@git.apache.org>,"Mon, 17 Feb 2014 07:30:41 +0000 (UTC)",[GitHub] incubator-spark pull request: added missing saveHadoopFile methods...,dev@spark.incubator.apache.org,"Github user NirmalReddy commented on the pull request:

    https://github.com/apache/incubator-spark/pull/403#issuecomment-35233963
  
    @pwendell Can you please verify this patch. Thanks !!


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
NirmalReddy <git@git.apache.org>,"Mon, 17 Feb 2014 07:31:16 +0000 (UTC)",[GitHub] incubator-spark pull request: Java-api completeness,dev@spark.incubator.apache.org,"Github user NirmalReddy commented on the pull request:

    https://github.com/apache/incubator-spark/pull/475#issuecomment-35234000
  
    @pwendell Can you please verify this patch. Thanks !!


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Mon, 17 Feb 2014 08:12:55 +0000 (UTC)",[GitHub] incubator-spark pull request: added missing saveHadoopFile methods...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/403#issuecomment-35236003
  
    Jenkins, test this please.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 08:13:22 +0000 (UTC)",[GitHub] incubator-spark pull request: added missing saveHadoopFile methods...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/403#issuecomment-35236033
  
     Build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 08:13:22 +0000 (UTC)",[GitHub] incubator-spark pull request: added missing saveHadoopFile methods...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/403#issuecomment-35236034
  
    Build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 08:13:52 +0000 (UTC)",[GitHub] incubator-spark pull request: added missing saveHadoopFile methods...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/403#issuecomment-35236062
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12740/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 08:13:52 +0000 (UTC)",[GitHub] incubator-spark pull request: added missing saveHadoopFile methods...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/403#issuecomment-35236060
  
    Build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Mon, 17 Feb 2014 08:14:18 +0000 (UTC)",[GitHub] incubator-spark pull request: Java-api completeness,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/475#issuecomment-35236088
  
    Maybe Jenkins was down last time. Jenkins, test this please.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 08:18:16 +0000 (UTC)",[GitHub] incubator-spark pull request: Java-api completeness,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/475#issuecomment-35236285
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 08:18:17 +0000 (UTC)",[GitHub] incubator-spark pull request: Java-api completeness,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/475#issuecomment-35236284
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
holdenk <git@git.apache.org>,"Mon, 17 Feb 2014 08:19:38 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark-615: make mapPartitionsWithInd...,dev@spark.incubator.apache.org,"GitHub user holdenk opened a pull request:

    https://github.com/apache/incubator-spark/pull/606

    Spark-615: make mapPartitionsWithIndex callable from java

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark spark-615-mapPartitionsWithIndex-callable-from-java

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/606.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #606
    
----
commit 9dff2d10c76c9817985ed79a249a02c8007d1f4e
Author: Holden Karau <holden@pigscanfly.ca>
Date:   2014-02-17T07:39:39Z

    Fix Java API for mapPartitionsWithIndex

commit 0e5ab061a9eadf4bf674d6ada54026203298018d
Author: Holden Karau <holden@pigscanfly.ca>
Date:   2014-02-17T08:13:37Z

    Check all the values

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 08:23:00 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark-615: make mapPartitionsWithInd...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/606#issuecomment-35236562
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 08:23:00 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark-615: make mapPartitionsWithInd...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/606#issuecomment-35236563
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 08:24:24 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark-615: make mapPartitionsWithInd...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/606#issuecomment-35236630
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 08:24:24 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark-615: make mapPartitionsWithInd...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/606#issuecomment-35236631
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12742/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 08:45:39 +0000 (UTC)",[GitHub] incubator-spark pull request: Java-api completeness,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/475#issuecomment-35237920
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 08:45:40 +0000 (UTC)",[GitHub] incubator-spark pull request: Java-api completeness,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/475#issuecomment-35237921
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12741/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Suraj Satishkumar Sheth <surajsat@adobe.com>,"Mon, 17 Feb 2014 09:09:55 +0000",Spark Streaming : Not working with  TextFileStream on HDFS,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi,
I am trying a Spark Streaming job with a Text File Stream on HDFS with Spark 0.9.0 from cloudera. 
I am saving the RDD (100 seconds is streaming frequency) to HDFS in a different directory. Every 100 seconds, it is creating a new directory in HDFS with _Success(stream-Random/_Success). But, it is not adding any data/output to it. I verified that I am adding new files to the correct HDFS directory. Another change I have noticed is that, Spark picks up the initial files present in the directory used for streaming. The behaviour seems like Batch processing. Although, at specified interval, it does create a new folder in HDFS with _Success.
So, the major issue is that it is not able to recognize new files created in HDFS.

Code used :
val ssc = new StreamingContext(ClusterConfig.sparkMaster, ""Hybrid"", Duration(100000), ClusterConfig.sparkHome, ClusterConfig.jars)
   
 val data = ssc.textFileStream(ClusterConfig.hdfsNN + ""correct/path/to/data"")
 data.foreachRDD(rdd => rdd.saveAsObjectFile(ClusterConfig.hdfsNN + ""/user<path/to/file>"" + Random.nextInt))
 ssc.start


It is creating these directories with only _Success : 
stream562343230
stream1228731977
stream318151149
stream603511115


This is the error stack I get :
14/02/17 14:08:20 INFO FileInputDStream: Finding new files took 549 ms
14/02/17 14:08:20 INFO FileInputDStream: New files at time 1392626300000 ms:

14/02/17 14:08:20 INFO JobScheduler: Added jobs for time 1392626300000 ms
14/02/17 14:08:20 INFO JobScheduler: Starting job streaming job 1392626300000 ms.0 from job set of time 1392626300000 ms
14/02/17 14:08:20 INFO SequenceFileRDDFunctions: Saving as sequence file of type (NullWritable,BytesWritable)
14/02/17 14:08:20 WARN Configuration: mapred.job.id is deprecated. Instead, use mapreduce.job.id
14/02/17 14:08:20 WARN Configuration: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
14/02/17 14:08:20 WARN Configuration: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
14/02/17 14:08:20 WARN Configuration: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
14/02/17 14:08:20 WARN Configuration: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
14/02/17 14:08:20 INFO SparkContext: Starting job: saveAsObjectFile at TestStreaming.scala:29
14/02/17 14:08:20 INFO SparkContext: Job finished: saveAsObjectFile at TestStreaming.scala:29, took 0.001934866 s
14/02/17 14:08:20 INFO JobScheduler: Finished job streaming job 1392626300000 ms.0 from job set of time 1392626300000 ms
14/02/17 14:08:20 INFO JobScheduler: Total delay: 0.741 s for time 1392626300000 ms (execution: 0.167 s)
14/02/17 14:08:20 INFO FileInputDStream: Cleared 0 old files that were older than 1392626200000 ms: 
14/02/17 14:10:00 INFO FileInputDStream: Finding new files took 6 ms
14/02/17 14:10:00 INFO FileInputDStream: New files at time 1392626400000 ms:

14/02/17 14:10:00 INFO JobScheduler: Added jobs for time 1392626400000 ms
14/02/17 14:10:00 INFO JobScheduler: Starting job streaming job 1392626400000 ms.0 from job set of time 1392626400000 ms
14/02/17 14:10:00 INFO SequenceFileRDDFunctions: Saving as sequence file of type (NullWritable,BytesWritable)
14/02/17 14:10:00 INFO SparkContext: Starting job: saveAsObjectFile at TestStreaming.scala:29
14/02/17 14:10:00 INFO SparkContext: Job finished: saveAsObjectFile at TestStreaming.scala:29, took 1.9016E-5 s
14/02/17 14:10:00 INFO JobScheduler: Finished job streaming job 1392626400000 ms.0 from job set of time 1392626400000 ms
14/02/17 14:10:00 INFO JobScheduler: Total delay: 0.085 s for time 1392626400000 ms (execution: 0.077 s)
14/02/17 14:10:00 INFO FileInputDStream: Cleared 0 old files that were older than 1392626300000 ms: 
14/02/17 14:11:40 INFO FileInputDStream: Finding new files took 5 ms
14/02/17 14:11:40 INFO FileInputDStream: New files at time 1392626500000 ms:

14/02/17 14:11:40 INFO JobScheduler: Added jobs for time 1392626500000 ms
14/02/17 14:11:40 INFO JobScheduler: Starting job streaming job 1392626500000 ms.0 from job set of time 1392626500000 ms
14/02/17 14:11:40 INFO SequenceFileRDDFunctions: Saving as sequence file of type (NullWritable,BytesWritable)
14/02/17 14:11:40 INFO SparkContext: Starting job: saveAsObjectFile at TestStreaming.scala:29
14/02/17 14:11:40 INFO SparkContext: Job finished: saveAsObjectFile at TestStreaming.scala:29, took 1.8111E-5 s
14/02/17 14:11:40 INFO JobScheduler: Finished job streaming job 1392626500000 ms.0 from job set of time 1392626500000 ms
14/02/17 14:11:40 INFO FileInputDStream: Cleared 1 old files that were older than 1392626400000 ms: 1392626300000 ms
14/02/17 14:11:40 INFO JobScheduler: Total delay: 0.110 s for time 1392626500000 ms (execution: 0.102 s)


Thanks and Regards,
Suraj Sheth

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 09:28:10 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35240586
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 09:28:10 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35240585
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 09:55:58 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35242603
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 09:55:58 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35242604
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12743/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
NirmalReddy <git@git.apache.org>,"Mon, 17 Feb 2014 10:27:05 +0000 (UTC)",[GitHub] incubator-spark pull request: added missing saveHadoopFile methods...,dev@spark.incubator.apache.org,"Github user NirmalReddy commented on the pull request:

    https://github.com/apache/incubator-spark/pull/403#issuecomment-35244753
  
    @aarondav Previously this branch was not in sync with master, now i rebased it with master. Please comment on it now. Thanks!! (The problem was scalasytle not found in sbt)


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 11:08:11 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35247536
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 11:08:12 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35247537
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
ScrapCodes <git@git.apache.org>,"Mon, 17 Feb 2014 11:10:52 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user ScrapCodes commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35247680
  
    I hope nothing is missed out, @mateiz take a look again ?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 11:36:16 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35249276
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 11:36:16 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35249277
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12744/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
NirmalReddy <git@git.apache.org>,"Mon, 17 Feb 2014 13:01:46 +0000 (UTC)",[GitHub] incubator-spark pull request: Added Java API for AsyncRDDActions w...,dev@spark.incubator.apache.org,"GitHub user NirmalReddy opened a pull request:

    https://github.com/apache/incubator-spark/pull/607

    Added Java API for AsyncRDDActions with tests

    Implemented countAsync() , foreachAsync() and foreachPartitionAsync() in java API . But couldn't find a way to convert FutureAction[Seq[T]] to FutureAction[List[T]] , so collectAsync() and takeAync() are left out. If some one could please suggest a way i shall complete them . 

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark java-api-asyncrddactions

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/607.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #607
    
----
commit 57eaf50c407b46a90ba8c672e6f29c2df733f2e2
Author: NirmalReddy <nirmal_reddy2000@yahoo.com>
Date:   2014-02-17T12:55:27Z

    Added Java API for AsyncRDDActions with tests

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 13:03:00 +0000 (UTC)",[GitHub] incubator-spark pull request: Added Java API for AsyncRDDActions w...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/607#issuecomment-35254787
  
    Can one of the admins verify this patch?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
dachuan <hdc1112@gmail.com>,"Mon, 17 Feb 2014 09:56:31 -0500",possible log info bug,dev@spark.incubator.apache.org,"Hi,

In spark-0.9.0-incubating, Master.scala, line 170

      logInfo(""Registering worker %s:%d with %d cores, %s RAM"".format(
        host, workerPort, cores, Utils.megabytesToString(memory)))

might need to be corrected to:

      logInfo(""Registering worker %s:%d with %d cores, %s RAM"".format(
        workerHost, workerPort, cores, Utils.megabytesToString(memory)))
"
punya <git@git.apache.org>,"Mon, 17 Feb 2014 16:54:47 +0000 (UTC)",[GitHub] incubator-spark pull request: Migrate Java code to Scala or move i...,dev@spark.incubator.apache.org,"Github user punya commented on the pull request:

    https://github.com/apache/incubator-spark/pull/605#issuecomment-35301961
  
    Jenkins, test this please.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
ash211 <git@git.apache.org>,"Mon, 17 Feb 2014 17:17:16 +0000 (UTC)",[GitHub] incubator-spark pull request: Worker registration logging fix,dev@spark.incubator.apache.org,"GitHub user ash211 opened a pull request:

    https://github.com/apache/incubator-spark/pull/608

    Worker registration logging fix

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark patch-7

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/608.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #608
    
----
commit bd85f2a56beb94dc313a9d7cfc9156a67879344a
Author: Andrew Ash <andrew@andrewash.com>
Date:   2014-02-17T17:17:00Z

    Worker registration logging fix

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 17:18:02 +0000 (UTC)",[GitHub] incubator-spark pull request: Worker registration logging fix,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/608#issuecomment-35304080
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 17:18:02 +0000 (UTC)",[GitHub] incubator-spark pull request: Worker registration logging fix,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/608#issuecomment-35304081
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Andrew Ash <andrew@andrewash.com>,"Mon, 17 Feb 2014 09:18:03 -0800",Re: possible log info bug,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi dachuan,

At first glance that does look like a bug.  I've opened a pull request with
the change here:

https://github.com/apache/incubator-spark/pull/608

Is that the fix you're proposing?

Thanks!
Andrew



"
aarondav <git@git.apache.org>,"Mon, 17 Feb 2014 17:27:14 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark-615: make mapPartitionsWithInd...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/606#discussion_r9798640
  
    Did you add this class? I don't see it in this PR.
    also, nit: the previous style was more correct, putting this parameter on the next line (and both indented 4 spaces in)


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
ash211 <git@git.apache.org>,"Mon, 17 Feb 2014 17:30:35 +0000 (UTC)",[GitHub] incubator-spark pull request: Migrate Java code to Scala or move i...,dev@spark.incubator.apache.org,"Github user ash211 commented on the pull request:

    https://github.com/apache/incubator-spark/pull/605#issuecomment-35305383
  
    Jenkins, test this please.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
ash211 <git@git.apache.org>,"Mon, 17 Feb 2014 17:30:46 +0000 (UTC)",[GitHub] incubator-spark pull request: Migrate Java code to Scala or move i...,dev@spark.incubator.apache.org,"Github user ash211 commented on the pull request:

    https://github.com/apache/incubator-spark/pull/605#issuecomment-35305400
  
    (not sure if that's going to work)


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Mon, 17 Feb 2014 17:37:18 +0000 (UTC)",[GitHub] incubator-spark pull request: Migrate Java code to Scala or move i...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/605#issuecomment-35305983
  
    Jenkins whitelist is separate from Jenkins admins, the latter of which is explicitly managed by the people who run the AMPLab Jenkins, alas.
    
    Jenkins, add to whitelist.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
dachuan <hdc1112@gmail.com>,"Mon, 17 Feb 2014 12:39:49 -0500",Re: possible log info bug,dev@spark.incubator.apache.org,"right, exactly. thanks!






-- 
Dachuan Huang
Cellphone: 614-390-7234
2015 Neil Avenue
Ohio State University
Columbus, Ohio
U.S.A.
43210
"
shivaram <git@git.apache.org>,"Mon, 17 Feb 2014 17:45:59 +0000 (UTC)",[GitHub] incubator-spark pull request: Migrate Java code to Scala or move i...,dev@spark.incubator.apache.org,"Github user shivaram commented on the pull request:

    https://github.com/apache/incubator-spark/pull/605#issuecomment-35306727
  
    Jenkins, test this please


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 17:46:14 +0000 (UTC)",[GitHub] incubator-spark pull request: Worker registration logging fix,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/608#issuecomment-35306750
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 17:46:14 +0000 (UTC)",[GitHub] incubator-spark pull request: Worker registration logging fix,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/608#issuecomment-35306751
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12745/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
shivaram <git@git.apache.org>,"Mon, 17 Feb 2014 17:47:26 +0000 (UTC)",[GitHub] incubator-spark pull request: Migrate Java code to Scala or move i...,dev@spark.incubator.apache.org,"Github user shivaram commented on the pull request:

    https://github.com/apache/incubator-spark/pull/605#issuecomment-35306867
  
    I am not sure, but I think the add to whitelist works from the next time a commit is made.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 17:48:01 +0000 (UTC)",[GitHub] incubator-spark pull request: Migrate Java code to Scala or move i...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/605#issuecomment-35306920
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 17:48:01 +0000 (UTC)",[GitHub] incubator-spark pull request: Migrate Java code to Scala or move i...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/605#issuecomment-35306921
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 17:48:26 +0000 (UTC)",[GitHub] incubator-spark pull request: Migrate Java code to Scala or move i...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/605#issuecomment-35306967
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12746/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 17:48:25 +0000 (UTC)",[GitHub] incubator-spark pull request: Migrate Java code to Scala or move i...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/605#issuecomment-35306966
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Mon, 17 Feb 2014 17:50:21 +0000 (UTC)",[GitHub] incubator-spark pull request: added missing saveHadoopFile methods...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/403#issuecomment-35307150
  
    Jenkins, add to whitelist and ok to test. (c'mon Jenkins, I believe in you)


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 17:53:24 +0000 (UTC)",[GitHub] incubator-spark pull request: added missing saveHadoopFile methods...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/403#issuecomment-35307412
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 17:53:24 +0000 (UTC)",[GitHub] incubator-spark pull request: added missing saveHadoopFile methods...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/403#issuecomment-35307413
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Mon, 17 Feb 2014 17:54:05 +0000 (UTC)",[GitHub] incubator-spark pull request: Worker registration logging fix,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/608#issuecomment-35307471
  
    Thanks!! Merged in master and branch-0.9. We should've listened to the ""suspicious shadowing"" warning in the first place, no good comes of those.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Mon, 17 Feb 2014 17:54:38 +0000 (UTC)",[GitHub] incubator-spark pull request: Added Java API for AsyncRDDActions w...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/607#issuecomment-35307506
  
    Jenkins, test this please.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 17:58:01 +0000 (UTC)",[GitHub] incubator-spark pull request: Added Java API for AsyncRDDActions w...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/607#issuecomment-35307799
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 17:58:02 +0000 (UTC)",[GitHub] incubator-spark pull request: Migrate Java code to Scala or move i...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/605#issuecomment-35307803
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 17:58:01 +0000 (UTC)",[GitHub] incubator-spark pull request: Added Java API for AsyncRDDActions w...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/607#issuecomment-35307800
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 17:58:02 +0000 (UTC)",[GitHub] incubator-spark pull request: Migrate Java code to Scala or move i...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/605#issuecomment-35307804
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Andrew Ash <andrew@andrewash.com>,"Mon, 17 Feb 2014 09:58:13 -0800",Re: possible log info bug,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi dachuan,

Aaron merged this in, so it should be fixed in the next release!

Thanks for the patch!
Andrew



"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 18:00:57 +0000 (UTC)",[GitHub] incubator-spark pull request: Added Java API for AsyncRDDActions w...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/607#issuecomment-35308067
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 18:00:57 +0000 (UTC)",[GitHub] incubator-spark pull request: Added Java API for AsyncRDDActions w...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/607#issuecomment-35308068
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12748/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 18:01:00 +0000 (UTC)",[GitHub] incubator-spark pull request: Migrate Java code to Scala or move i...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/605#issuecomment-35308071
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12749/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 18:01:00 +0000 (UTC)",[GitHub] incubator-spark pull request: Migrate Java code to Scala or move i...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/605#issuecomment-35308070
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
andrewor14 <git@git.apache.org>,"Mon, 17 Feb 2014 18:09:39 +0000 (UTC)",[GitHub] incubator-spark pull request: Fix typos in Spark Streaming program...,dev@spark.incubator.apache.org,"Github user andrewor14 commented on the pull request:

    https://github.com/apache/incubator-spark/pull/536#discussion_r9799844
  
    No it's supposed to link to the section called ""Where to Go from Here"" at the bottom, so the hash tags for (#where-to-go-from-here) should work. Maybe it's because there are words that begin with lowercase, but I'm not sure why it doesn't work.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 18:21:32 +0000 (UTC)",[GitHub] incubator-spark pull request: added missing saveHadoopFile methods...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/403#issuecomment-35309770
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 18:21:33 +0000 (UTC)",[GitHub] incubator-spark pull request: added missing saveHadoopFile methods...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/403#issuecomment-35309771
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12747/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
asfgit <git@git.apache.org>,"Mon, 17 Feb 2014 18:23:32 +0000 (UTC)",[GitHub] incubator-spark pull request: Worker registration logging fix,dev@spark.incubator.apache.org,"Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-spark/pull/608


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 18:28:06 +0000 (UTC)",[GitHub] incubator-spark pull request: Migrate Java code to Scala or move i...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/605#issuecomment-35310307
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 18:28:07 +0000 (UTC)",[GitHub] incubator-spark pull request: Migrate Java code to Scala or move i...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/605#issuecomment-35310306
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 18:28:54 +0000 (UTC)",[GitHub] incubator-spark pull request: Fix typos in Spark Streaming program...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/536#issuecomment-35310373
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 18:28:54 +0000 (UTC)",[GitHub] incubator-spark pull request: Fix typos in Spark Streaming program...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/536#issuecomment-35310374
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 18:38:11 +0000 (UTC)",[GitHub] incubator-spark pull request: Migrate Java code to Scala or move i...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/605#issuecomment-35311100
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12750/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 18:38:11 +0000 (UTC)",[GitHub] incubator-spark pull request: Migrate Java code to Scala or move i...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/605#issuecomment-35311099
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
mengxr <git@git.apache.org>,"Mon, 17 Feb 2014 18:51:42 +0000 (UTC)",[GitHub] incubator-spark pull request: MLI-2: Start adding k-fold cross val...,dev@spark.incubator.apache.org,"Github user mengxr commented on the pull request:

    https://github.com/apache/incubator-spark/pull/572#discussion_r9800999
  
    I'm not sure which style to use. @rxin ? I prefer the following:
    ~~~
    map { fold => (                                           // ""(("" seems to be unnecessary
      new PartitionwiseSampledRDD ...
          complement = false), seed),                         // indent 2+4 spaces 
      new PartitionwiseSampledRDD ...
          complement = true), seed)
    )}.toList
    ~~~


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 18:56:55 +0000 (UTC)",[GitHub] incubator-spark pull request: Fix typos in Spark Streaming program...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/536#issuecomment-35312592
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 18:56:55 +0000 (UTC)",[GitHub] incubator-spark pull request: Fix typos in Spark Streaming program...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/536#issuecomment-35312594
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12751/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Mon, 17 Feb 2014 18:57:12 +0000 (UTC)",[GitHub] incubator-spark pull request: MLI-2: Start adding k-fold cross val...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/572#discussion_r9801137
  
    (1 to folds) is preferred, your style is fine though we use 2 space wrapped indents instead of 4. Would this be possible, though?
    ```
    (1 to folds).map { fold => (
      new PartitionwiseSampledRDD(rdd, 
        new BernoulliSampler[T]((fold - 1) / foldsF, fold / foldsF, complement = false), seed),
       ...
    )}.toList
    ```
    
    anyway up to you but that way avoids breaking a line in a nested expression. 


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Mon, 17 Feb 2014 19:01:30 +0000 (UTC)",[GitHub] incubator-spark pull request: Fix typos in Spark Streaming program...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/536#issuecomment-35312964
  
    Thanks! Merged into master.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
asfgit <git@git.apache.org>,"Mon, 17 Feb 2014 19:25:27 +0000 (UTC)",[GitHub] incubator-spark pull request: Fix typos in Spark Streaming program...,dev@spark.incubator.apache.org,"Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-spark/pull/536


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
andy327 <git@git.apache.org>,"Mon, 17 Feb 2014 19:31:52 +0000 (UTC)",[GitHub] incubator-spark pull request: Add Shortest-path computations to gr...,dev@spark.incubator.apache.org,"GitHub user andy327 opened a pull request:

    https://github.com/apache/incubator-spark/pull/609

    Add Shortest-path computations to graphx.lib with unit tests.

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-spark feat-shortestpaths

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/609.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #609
    
----
commit 7496d6b7181ef9eecfdee5c84f154696302821df
Author: Andres Perez <andres@tresata.com>
Date:   2014-02-17T18:32:09Z

    Add Shortest-path computations to graphx.lib with unit tests.

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 19:33:03 +0000 (UTC)",[GitHub] incubator-spark pull request: Add Shortest-path computations to gr...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/609#issuecomment-35315474
  
    Can one of the admins verify this patch?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
rxin <git@git.apache.org>,"Mon, 17 Feb 2014 19:39:15 +0000 (UTC)",[GitHub] incubator-spark pull request: MLI-2: Start adding k-fold cross val...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/572#discussion_r9802309
  
    ```scala
    (1 to numFolds).map { fold =>
      val sampler = new BernoulliSampler[T]((fold-1)/foldsF,fold/foldsF, complement = false)
      val train = new PartitionwiseSampledRDD(rdd, sampler , seed)
      val test = new PartitionwiseSampledRDD(rdd, sampler , seed.complement)  // might need to create this
      (train, test)
    }
    ```
    
    Make sure you rename folds to numFolds.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
punya <git@git.apache.org>,"Mon, 17 Feb 2014 19:42:05 +0000 (UTC)",[GitHub] incubator-spark pull request: Migrate Java code to Scala or move i...,dev@spark.incubator.apache.org,"Github user punya commented on the pull request:

    https://github.com/apache/incubator-spark/pull/605#issuecomment-35316168
  
    Jenkins, test this please.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 19:43:00 +0000 (UTC)",[GitHub] incubator-spark pull request: Migrate Java code to Scala or move i...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/605#issuecomment-35316245
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 19:43:01 +0000 (UTC)",[GitHub] incubator-spark pull request: Migrate Java code to Scala or move i...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/605#issuecomment-35316247
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Mon, 17 Feb 2014 19:51:10 +0000 (UTC)",[GitHub] incubator-spark pull request: Tachyon scripts,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/603#issuecomment-35316947
  
    @mateiz @nicklan - I think it's fine to ask Tachyon to publish this stuff in their jars. Having Tachyon code inside of the Spark codebase is not good for a bunch of reasons.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 20:11:19 +0000 (UTC)",[GitHub] incubator-spark pull request: Migrate Java code to Scala or move i...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/605#issuecomment-35318595
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12752/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 17 Feb 2014 20:11:19 +0000 (UTC)",[GitHub] incubator-spark pull request: Migrate Java code to Scala or move i...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/605#issuecomment-35318592
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
mateiz <git@git.apache.org>,"Mon, 17 Feb 2014 20:57:55 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user mateiz commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35322176
  
    @ScrapCodes so in this version, do we still need the ""of"" methods or can we remove those?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
punya <git@git.apache.org>,"Mon, 17 Feb 2014 21:00:23 +0000 (UTC)",[GitHub] incubator-spark pull request: Migrate Java code to Scala or move i...,dev@spark.incubator.apache.org,"Github user punya commented on the pull request:

    https://github.com/apache/incubator-spark/pull/605#issuecomment-35322379
  
    @aarondav can I get a review?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
rxin <git@git.apache.org>,"Mon, 17 Feb 2014 22:00:13 +0000 (UTC)",[GitHub] incubator-spark pull request: MLI-2: Start adding k-fold cross val...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/572#discussion_r9805702
  
    Basically you want to make sure it is obvious that this returns a tuple (which can also be done through explicit type declaration but probably simpler this way)


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
rxin <git@git.apache.org>,"Mon, 17 Feb 2014 22:11:37 +0000 (UTC)",[GitHub] incubator-spark pull request: Added Java API for AsyncRDDActions w...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/607#issuecomment-35327669
  


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Mon, 17 Feb 2014 22:16:57 +0000 (UTC)",[GitHub] incubator-spark pull request: Tachyon scripts,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/603#issuecomment-35328014
  
    @nicklan @mateiz as an alternative, it might be simplest to just download a Tachyon package (e.g. don't use maven to deal with this). And have a nice graceful degradation if the download won't work. Similar to what we do with SBT.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Mon, 17 Feb 2014 23:13:27 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1090] improvement on spark_sh...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/599#issuecomment-35331962
  
    Thanks! Merged into master.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Stephen Haberman <stephen.haberman@gmail.com>,"Mon, 17 Feb 2014 17:17:49 -0600",oome from large map output status,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey,

I tracked an OOME on our 0.9 standalone master down to the master
making a large byte[] for the output statuses (serializeMapStatuses),
and it getting copied once/executor.

In our case, an RDD had 9,000 partitions, so, 81m shuffle combinations,
with the output status using 1 byte per compressed size, that's ~81m,
which, after gzipping, the byte[] was 49mb.

However, it's sent via an Akka message, so, a) 49mb is over the default
Akka frame size of 10mb (we'd already upped ours) and b) the byte[]
gets copied into a new byte[] for each slave/executor asking for it.
Plus a few more copies seem to have in the Netty/NIO stack.

AFAICT. As we basically ended with 70 of these 50mb byte[]s in RAM,
for a total of 3.5gb.

So, a few things:

1) Obviously we should not have an RDD with 9k partitions. I'll have the
job author fix that and then we should be fine.

2) That said, since this is sensitive to getting large easily (even if
in user error), perhaps a broadcast variable (or something) should be
used instead of sending the raw bytes through Akka itself?

IANAE, so perhaps optimizing our degenerate case is not worth it, but I
thought I would at least share what we ran in to.

Thanks,
Stephen

"
asfgit <git@git.apache.org>,"Mon, 17 Feb 2014 23:19:00 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1090] improvement on spark_sh...,dev@spark.incubator.apache.org,"Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-spark/pull/599


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
markhamstra <git@git.apache.org>,"Mon, 17 Feb 2014 23:44:51 +0000 (UTC)",[GitHub] incubator-spark pull request: Added Java API for AsyncRDDActions w...,dev@spark.incubator.apache.org,"Github user markhamstra commented on the pull request:

    https://github.com/apache/incubator-spark/pull/607#issuecomment-35333776
  
    Marking them as experimental for 1.0.0 might not be a bad idea.  I don't get the impression that the async actions are seeing a lot of use yet, so there may be some ""I wish this were different"" to be discovered as the async stuff is used more.  It is important and fundamentally well-done work that can lead to some significantly different usage of Spark, so I think it is a good idea to get some form of async Java API out there to start generate more grounds for discussion -- and that discussion is also a good idea.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Bryn Keller <xoltar@xoltar.org>,"Mon, 17 Feb 2014 15:57:04 -0800",Can't create issue in JIRA?,dev@spark.incubator.apache.org,"Hi Folks,

I need to open a JIRA ticket to report the problem I'm about to fix with my
first Spark pull request... Trouble is, once I created a user in the Spark
JIRA and logged in, my Issues menu doesn't have any option for creating an
issue. Can someone please assist?

Thanks,
Bryn
"
Andrew Ash <andrew@andrewash.com>,"Mon, 17 Feb 2014 16:01:24 -0800",Re: Can't create issue in JIRA?,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","The Create Issue button is in the top header bar in the center, right of
Agil and Capture.  Here's what my screen looks like:  Is that button not
there for you?

[image: Inline image 1]



"
Bryn Keller <xoltar@xoltar.org>,"Mon, 17 Feb 2014 16:06:25 -0800",Re: Can't create issue in JIRA?,dev@spark.incubator.apache.org,"Looks like your image didn't make it, but I don't have any buttons on the
menu bar to the right of Agile and Capture. Here's what I see (if the image
makes it...)

[image: Inline image 1]



"
Nan Zhu <zhunanmcgill@gmail.com>,"Mon, 17 Feb 2014 19:52:58 -0500",Re: Can't create issue in JIRA?,dev@spark.incubator.apache.org,"I think the mail list block the images? 

-- 
Nan Zhu




"
Bryn Keller <xoltar@xoltar.org>,"Mon, 17 Feb 2014 17:00:01 -0800",Re: Can't create issue in JIRA?,dev@spark.incubator.apache.org,"Quite possible. The problem I'm really asking about is the fact that I
can't create a JIRA issue. I have no ""Create an Issue"" button as Andrew was
describing.

Thanks,
Bryn



"
Nan Zhu <zhunanmcgill@gmail.com>,"Mon, 17 Feb 2014 20:07:09 -0500",Re: Can't create issue in JIRA?,dev@spark.incubator.apache.org,"after you login, can you access:https://spark-project.atlassian.net/secure/CreateIssue!default.jspa? 

-- 
Nan Zhu
Sent with Sparrow (http://www.sparrowmailapp.com/?sig)





"
Patrick Wendell <pwendell@gmail.com>,"Mon, 17 Feb 2014 17:18:27 -0800",Re: Can't create issue in JIRA?,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","You need to create an account. Anyone can sign up for an account on the JIRA.


"
Bryn Keller <xoltar@xoltar.org>,"Mon, 17 Feb 2014 17:27:16 -0800",Re: Can't create issue in JIRA?,dev@spark.incubator.apache.org,"When I visit that URL, even though my selected project in JIRA is Spark, I
get ""You have not selected a valid project to create an issue in.""



"
Bryn Keller <xoltar@xoltar.org>,"Mon, 17 Feb 2014 17:27:50 -0800",Re: Can't create issue in JIRA?,dev@spark.incubator.apache.org,"Hi Patrick, I did that. And I logged in. I still don't have a create
button. Perhaps someone has to add me to the Spark project?



"
Nan Zhu <zhunanmcgill@gmail.com>,"Mon, 17 Feb 2014 20:46:43 -0500",Re: Can't create issue in JIRA?,dev@spark.incubator.apache.org,"I just feel itâ€™s something related to your browserâ€¦.what are you using?  

Best,  

--  
Nan Zhu



, I
?
t I
s Andrew
m (mailto:zhunanmcgill@gmail.com)(mailto:
ons on
 (if
sh.com (mailto:andrew@andrewash.com)(mailto:
r,
tton
ar.org (mailto:xoltar@xoltar.org)(mailto:
t to
er in
for


"
Bryn Keller <xoltar@xoltar.org>,"Mon, 17 Feb 2014 18:02:05 -0800",Re: Can't create issue in JIRA?,dev@spark.incubator.apache.org,"I've tried Chrome, Firefox, and IE 11. All are the same.



"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Mon, 17 Feb 2014 18:15:52 -0800",Accessing Hadoop2 HDFS from Spark app,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>, Patrick Wendell <pwendell@gmail.com>","I ran into a weird bug today where trying to read a file from HDFS
built using Hadoop 2 gives an error saying ""No FileSystem for scheme:
hdfs"".  Specifically this only seems to happen when building an
assembly jar in the application and not when using sbt's run-main.

The project's setup[0] is pretty simple and is only a slight
modification of the project used by the release audit tool. The sbt
assembly instructions[1] are mostly copied from Spark's sbt build
files.

We run into this in SparkR as well, so it'll be great if anybody has
an idea on how to debug this.
To repoduce, you can do the following:

1. Launch a Spark EC2 cluster with 0.9.0 with --hadoop-major-version=2
2. Clone https://github.com/shivaram/spark-utils
3. Run release-audits/sbt_app_core/run-hdfs-test.sh

Thanks
Shivaram

[0] https://github.com/shivaram/spark-utils/blob/master/release-audits/sbt_app_core/src/main/scala/SparkHdfsApp.scala
[1] https://github.com/shivaram/spark-utils/blob/master/release-audits/sbt_app_core/build.sbt

"
Jey Kottalam <jey@cs.berkeley.edu>,"Mon, 17 Feb 2014 18:27:50 -0800",Re: Accessing Hadoop2 HDFS from Spark app,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>, 
	Shivaram Venkataraman <shivaram@eecs.berkeley.edu>","We ran into this issue with ADAM, and it came down to an issue of not
merging the ""META-INF/services"" files correctly. Here's the change we made
to our Maven build files to fix it, can probably do something similar under
SBT too:
https://github.com/bigdatagenomics/adam/commit/b0997760b23c4284efe32eeb968ef2744af8be82

-Jey



"
Bryn Keller <xoltar@xoltar.org>,"Mon, 17 Feb 2014 18:33:30 -0800",Re: Can't create issue in JIRA?,dev@spark.incubator.apache.org,"Hmm. Perhaps I'm supposed to use the JIRA at
https://issues.apache.org/jira/browse/SPARK ? Not this one
https://spark-project.atlassian.net/ that's linked from the Spark home page?



"
Nan Zhu <zhunanmcgill@gmail.com>,"Mon, 17 Feb 2014 21:40:25 -0500",Re: Can't create issue in JIRA?,dev@spark.incubator.apache.org,"apache JIRA is running? 

Best, 

-- 
Nan Zhu





"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Mon, 17 Feb 2014 18:47:28 -0800",Re: Accessing Hadoop2 HDFS from Spark app,Jeyasankar Kottalam <jey@cs.berkeley.edu>,"Thanks a lot Jey ! That fixes things. For reference I had to add the
following line to build.sbt

    case m if m.toLowerCase.matches(""meta-inf/services.*$"")  =>
MergeStrategy.concat

Should we also add this to Spark's assembly build ?

Thanks
Shivaram


"
Bryn Keller <xoltar@xoltar.org>,"Mon, 17 Feb 2014 19:18:04 -0800",Re: Can't create issue in JIRA?,dev@spark.incubator.apache.org,"I also tried from a different machine. Same result. I don't think it's
browser related.



"
Josh Rosen <rosenville@gmail.com>,"Mon, 17 Feb 2014 19:20:46 -0800",Re: Can't create issue in JIRA?,"""Spark Dev (Apache Incubator)"" <dev@spark.incubator.apache.org>","Which JIRA are you trying to create an issue in?  For now, you should be
creating issues in the https://spark-project.atlassian.net JIRA (in the
long run, we're going to move to the Apache JIRA, but we're still waiting
to import our issues there ).

What's your username on the https://spark-project.atlassian.net JIRA?



"
rxin <git@git.apache.org>,"Tue, 18 Feb 2014 03:23:22 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1098: Minor cleanup of ClassTa...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/604#issuecomment-35349097
  
    Thanks. I've merged this.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Bryn Keller <xoltar@xoltar.org>,"Mon, 17 Feb 2014 19:33:50 -0800",Re: Can't create issue in JIRA?,dev@spark.incubator.apache.org,"Hi Josh,

I only have a user on spark-project.atlassian.net so far - my user name
there is xoltar.

Thanks,
Bryn



"
Mridul Muralidharan <mridul@gmail.com>,"Tue, 18 Feb 2014 09:52:15 +0530",Re: oome from large map output status,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","There is nothing wrong with 9k partitions - I actually use much higher :-) [1]
I have not really seen this interesting issue you mentioned - should
investigate more, thanks for the note !


Regards,
Mridul

[1] I do use insanely high frame size anyway - and my workers/master
run with 8g; maybe why I did not see it yet ...


"
asfgit <git@git.apache.org>,"Tue, 18 Feb 2014 04:22:24 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1098: Minor cleanup of ClassTa...,dev@spark.incubator.apache.org,"Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-spark/pull/604


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
ScrapCodes <git@git.apache.org>,"Tue, 18 Feb 2014 04:44:10 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user ScrapCodes commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35352183
  
    Yes, we can remove those ""of"" methods, but we would still need to wrap the lambdas. Since closure cleaner won't work with out that. So as an alternative we can have private[spark] implicits defs listed in a single place which would do it seamlessly. Even in case we decide to keep those ""of"" methods there is no need for them to be exposed to user. 
    
    So will it be okay to do it implicitly ? 


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Tue, 18 Feb 2014 04:51:53 +0000 (UTC)",[GitHub] incubator-spark pull request: Enable SparkContext.addJars() to loa...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/299#discussion_r9812541
  
    To be consistent with the other naming this should be `camelCase` rather than `lower-with-hyphens`.
    
    Also, how about `spark.driver.addJars`. It seems to me more consistent with the rest of the code where `addJar` is understood as a mechanism that dynamically makes jar contents viewable by certain components.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Tue, 18 Feb 2014 04:56:50 +0000 (UTC)",[GitHub] incubator-spark pull request: Enable SparkContext.addJars() to loa...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/299#discussion_r9812596
  
    Now that I see this here, I wonder if maybe it just makes sense to note this in the configuration doc below rater than here. It's really only relevant to people mucking with this configuration... which will be a tiny subset of users. Then above maybe we could just change:
    
    ""is expected to exist as a local file on each worker node"" --> ""is expected to exists as a local file wherever jars are loaded (e.g. on workers)."" Or something like this, because currently that statement is not correct.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Tue, 18 Feb 2014 05:05:27 +0000 (UTC)",[GitHub] incubator-spark pull request: Enable SparkContext.addJars() to loa...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/299#discussion_r9812694
  
    I also think it's fine to ditch this and just include it in the config docs... it's up to you though... I just realized after seeing this there is a risk of confusing users who will never use this.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Tue, 18 Feb 2014 05:07:08 +0000 (UTC)",[GitHub] incubator-spark pull request: Enable SparkContext.addJars() to loa...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/299#discussion_r9812713
  
    Makes sense, I think it's fine to just change the local:// semantics a bit.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Tue, 18 Feb 2014 05:12:15 +0000 (UTC)",[GitHub] incubator-spark pull request: Enable SparkContext.addJars() to loa...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/299#discussion_r9812781
  
    Mind formatting this to meet the style guide:
    ```
    private[spark] val classLoader =
        if (conf.getBoolean(""spark.driver.add-dynamic-jars"", false)) {
          val loader = new SparkURLClassLoader(Array.empty[URL], this.getClass.getClassLoader)
          Thread.currentThread.setContextClassLoader(loader)
          Some(loader)
        } else {
          None
        }
    ```
    The only hard requirement is adding brackets and linebreaks around the `None`. The linebreak at the beginning I don't think we require... that one's up to you.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Tue, 18 Feb 2014 05:15:31 +0000 (UTC)",[GitHub] incubator-spark pull request: Enable SparkContext.addJars() to loa...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/299#discussion_r9812824
  
    I'm a bit confused on this one... if we set the context class loader to be the new one in SparkContext, and akka's default is to get the context class loader, than why do we need to explicitly pass our classloader to akka at all? Won't akka just pick up on your new classloader?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Tue, 18 Feb 2014 05:18:11 +0000 (UTC)",[GitHub] incubator-spark pull request: Enable SparkContext.addJars() to loa...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/299#discussion_r9812862
  
    It would be great if it were possible to not modify the way akka initializes... but there may be some reason I'm missing why we need to.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Tue, 18 Feb 2014 05:19:15 +0000 (UTC)",[GitHub] incubator-spark pull request: Enable SparkContext.addJars() to loa...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/299#issuecomment-35353411
  


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Patrick Wendell <pwendell@gmail.com>,"Mon, 17 Feb 2014 21:26:44 -0800",Re: Accessing Hadoop2 HDFS from Spark app,Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Ya I ran into this a few months ago. We actually patched the spark
build back then. It took me a long time to figure it out.

https://github.com/apache/incubator-spark/commit/0c1985b153a2dc2c891ae61c1ee67506926384ae


"
Patrick Wendell <pwendell@gmail.com>,"Mon, 17 Feb 2014 21:27:39 -0800",Re: Accessing Hadoop2 HDFS from Spark app,Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"BTW my fix in Spark was later generalized to be equivalent to what you
did, which is do this for the entire services directory rather than
just FileSystem.


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Mon, 17 Feb 2014 21:59:48 -0800",Re: Accessing Hadoop2 HDFS from Spark app,Patrick Wendell <pwendell@gmail.com>,"Thanks for the pointer -- I guess I should have checked Spark's build
script again while debugging. This might be useful to include in a
documentation page about how to write and run Spark apps. I think
there's are a bunch of such know-how just floating around right now.

Shivaram


"
NirmalReddy <git@git.apache.org>,"Tue, 18 Feb 2014 06:03:29 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark 1095 : Adding explicit return ...,dev@spark.incubator.apache.org,"GitHub user NirmalReddy opened a pull request:

    https://github.com/apache/incubator-spark/pull/610

    Spark 1095 : Adding explicit return types to all public methods 

    Its still wip and i am doing them in chunks.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/NirmalReddy/incubator-spark spark-1095

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/610.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #610
    
----
commit ecf4b70428838ad72e831a0272a6a31f98daa0b2
Author: NirmalReddy <nirmal_reddy2000@yahoo.com>
Date:   2014-02-17T15:57:50Z

    fixed explicit types in core package

commit a634617d38fe02c5e04f54026b343e1a3652e670
Author: NirmalReddy <nirmal_reddy2000@yahoo.com>
Date:   2014-02-17T16:13:20Z

    fixed explicit types in streaming package

commit 18bf830118ba952873ab490ca87a42e90a95a62e
Author: NirmalReddy <nirmal_reddy2000@yahoo.com>
Date:   2014-02-18T06:00:05Z

    fixed some more explicit types in core

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 06:08:02 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark 1095 : Adding explicit return ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/610#issuecomment-35355545
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 06:08:02 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark 1095 : Adding explicit return ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/610#issuecomment-35355546
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 06:08:43 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark 1095 : Adding explicit return ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/610#issuecomment-35355577
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 06:08:44 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark 1095 : Adding explicit return ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/610#issuecomment-35355578
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12753/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 06:33:03 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark 1095 : Adding explicit return ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/610#issuecomment-35356511
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 06:33:03 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark 1095 : Adding explicit return ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/610#issuecomment-35356512
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
prabinb <git@git.apache.org>,"Tue, 18 Feb 2014 06:46:00 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding Missing Python APIs,dev@spark.incubator.apache.org,"Github user prabinb commented on the pull request:

    https://github.com/apache/incubator-spark/pull/486#issuecomment-35357083
  
    @JoshRosen, addressed the review comments. Can you please verify the changes. Thanks!


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 07:01:58 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark 1095 : Adding explicit return ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/610#issuecomment-35357819
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 07:01:58 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark 1095 : Adding explicit return ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/610#issuecomment-35357820
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12754/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 07:18:00 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark 1095 : Adding explicit return ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/610#issuecomment-35358530
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 07:18:00 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark 1095 : Adding explicit return ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/610#issuecomment-35358529
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
colorant <git@git.apache.org>,"Tue, 18 Feb 2014 07:18:48 +0000 (UTC)","[GitHub] incubator-spark pull request: For SPARK-1082, Use Curator for ZK i...",dev@spark.incubator.apache.org,"GitHub user colorant opened a pull request:

    https://github.com/apache/incubator-spark/pull/611

    For SPARK-1082, Use Curator for ZK interaction in standalone cluster

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/colorant/incubator-spark curator

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/611.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #611
    
----
commit 05fc0da724bafba82f6fc92f2b04846ef3f2dbfe
Author: Raymond Liu <raymond.liu@intel.com>
Date:   2014-02-17T08:43:12Z

    Rewrite zookeeper client code with curator

commit ce3f7dc1bd98e611873b491468c2745d4035a20d
Author: Raymond Liu <raymond.liu@intel.com>
Date:   2014-02-18T06:53:53Z

    Ignore NodeExists exception

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 07:23:03 +0000 (UTC)","[GitHub] incubator-spark pull request: For SPARK-1082, Use Curator for ZK i...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/611#issuecomment-35358804
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 07:23:03 +0000 (UTC)","[GitHub] incubator-spark pull request: For SPARK-1082, Use Curator for ZK i...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/611#issuecomment-35358803
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 07:24:04 +0000 (UTC)","[GitHub] incubator-spark pull request: For SPARK-1082, Use Curator for ZK i...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/611#issuecomment-35358870
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 07:24:04 +0000 (UTC)","[GitHub] incubator-spark pull request: For SPARK-1082, Use Curator for ZK i...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/611#issuecomment-35358871
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12756/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 07:38:01 +0000 (UTC)","[GitHub] incubator-spark pull request: For SPARK-1082, Use Curator for ZK i...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/611#issuecomment-35359573
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 07:38:01 +0000 (UTC)","[GitHub] incubator-spark pull request: For SPARK-1082, Use Curator for ZK i...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/611#issuecomment-35359571
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 07:45:44 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark 1095 : Adding explicit return ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/610#issuecomment-35359957
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 07:45:44 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark 1095 : Adding explicit return ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/610#issuecomment-35359958
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12755/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 08:02:18 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark 1095 : Adding explicit return ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/610#issuecomment-35360834
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 08:02:18 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark 1095 : Adding explicit return ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/610#issuecomment-35360835
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 08:06:07 +0000 (UTC)","[GitHub] incubator-spark pull request: For SPARK-1082, Use Curator for ZK i...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/611#issuecomment-35361023
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12757/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 08:06:07 +0000 (UTC)","[GitHub] incubator-spark pull request: For SPARK-1082, Use Curator for ZK i...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/611#issuecomment-35361022
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Tue, 18 Feb 2014 08:10:49 +0000 (UTC)","[GitHub] incubator-spark pull request: For SPARK-1082, Use Curator for ZK i...",dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/611#discussion_r9815215
  
    explicit return type


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Tue, 18 Feb 2014 08:13:29 +0000 (UTC)","[GitHub] incubator-spark pull request: For SPARK-1082, Use Curator for ZK i...",dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/611#discussion_r9815286
  
    Let's not have a default value for this and throw an error if it's not defined. (I know that wasn't the original behavior, but I think this behavior was sorta an unintentional side-effect of the SparkConf changes.)


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Tue, 18 Feb 2014 08:13:45 +0000 (UTC)","[GitHub] incubator-spark pull request: For SPARK-1082, Use Curator for ZK i...",dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/611#discussion_r9815294
  
    extra newline?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Tue, 18 Feb 2014 08:14:35 +0000 (UTC)","[GitHub] incubator-spark pull request: For SPARK-1082, Use Curator for ZK i...",dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/611#discussion_r9815316
  
    This suggests we may be able to do away with the actor-ness of this class -- I haven't given it sufficient thought, but I suspect it's the case.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Tue, 18 Feb 2014 08:15:58 +0000 (UTC)","[GitHub] incubator-spark pull request: For SPARK-1082, Use Curator for ZK i...",dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/611#discussion_r9815357
  
    This statement doesn't really give us anything if we're not behind a lock. I think we should probably synchronize this method and notLeader and then these statements are sweet.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Tue, 18 Feb 2014 08:18:49 +0000 (UTC)","[GitHub] incubator-spark pull request: For SPARK-1082, Use Curator for ZK i...",dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/611#issuecomment-35361778
  
    Thanks for doing this! Took a very superficial pass, will attempt a deeper one later. Have you tried running FaultToleranceTest? If you're not on a Linux box, it could be difficult to run (since it relies on LXC), so I can run it instead if that's the case.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 08:29:54 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark 1095 : Adding explicit return ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/610#issuecomment-35362366
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 08:29:54 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark 1095 : Adding explicit return ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/610#issuecomment-35362367
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12758/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 09:53:01 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark 1095 : Adding explicit return ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/610#issuecomment-35368102
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 09:53:01 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark 1095 : Adding explicit return ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/610#issuecomment-35368104
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 10:20:55 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark 1095 : Adding explicit return ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/610#issuecomment-35370102
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12759/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 10:20:55 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark 1095 : Adding explicit return ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/610#issuecomment-35370100
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
guojc <git@git.apache.org>,"Tue, 18 Feb 2014 10:48:11 +0000 (UTC)",[GitHub] incubator-spark pull request: Fix ExternalMap on case of key's has...,dev@spark.incubator.apache.org,"GitHub user guojc opened a pull request:

    https://github.com/apache/incubator-spark/pull/612

    Fix ExternalMap on case of key's hashCode equal to Int.Maxvalue

    Currently StreamBuffer use hash value to compare between buffers. And it want emmit 
    buffer with lowest HashValue from PriorityQue first and empty buffer last. And it use Int.MaxValue
    for empty buffer. This can cause problem with non-empty buffer with HashValue equals to Int.MaxValue
    and cause exception. This problem will happen with high probability when key size approach Int.MaxValue.
    
    This fix change the compareTo method to comparing empty case first then hashValue and resolve the issue.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/guojc/incubator-spark master

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/612.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #612
    
----
commit e5982960184d1c40d56f5ac4dd0fc9b7979c42bf
Author: Jiacheng Guo <guojc03@gmail.com>
Date:   2014-02-18T10:28:34Z

    Currently StreamBuffer use hash value to compare between buffers. And it want emmit
    buffer with lowest HashValue from PriorityQue first and empty buffer last. And it use Int.MaxValue
    for empty buffer. This can cause problem with non-empty buffer with HashValue equals to Int.MaxValue
    and cause exception. This problem will happen with high probability when key size approach Int.MaxValue.
    
    This fix change the compareTo method to comparing empty case first then hashValue and resolve the issue.

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 10:52:59 +0000 (UTC)",[GitHub] incubator-spark pull request: Fix ExternalMap on case of key's has...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/612#issuecomment-35372535
  
    Can one of the admins verify this patch?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 11:02:49 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark 1095 : Adding explicit return ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/610#issuecomment-35373439
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 11:02:49 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark 1095 : Adding explicit return ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/610#issuecomment-35373442
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 11:30:22 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark 1095 : Adding explicit return ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/610#issuecomment-35375272
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12760/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 11:30:22 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark 1095 : Adding explicit return ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/610#issuecomment-35375271
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 11:33:05 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35375464
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 11:33:06 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35375462
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 11:36:34 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35375697
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 11:36:34 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35375698
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12761/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 11:48:06 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35376471
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 11:48:06 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35376470
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 11:53:07 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35376802
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 11:53:07 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35376803
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 12:16:19 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35378476
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12762/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 12:16:19 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35378475
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 12:22:43 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35379064
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 12:22:43 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35379065
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12763/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Nan Zhu <zhunanmcgill@gmail.com>,"Tue, 18 Feb 2014 09:03:07 -0500","IntelliJ inflates the CPU utilization when working on
 SparkIMain.scala",dev@spark.incubator.apache.org,"Hi, all  

Anyone else meets the same problem?

When I work on SparkIMain.scala, my IntelliJ inflates the CPU utilization a lot, the top shows:



PID  COMMAND      %CPU  TIME     #TH  #WQ  #PORT #MREG MEM    RPRVT  PURG   CMPR VPRVT  VSIZE  PGRP PPID STATE  
541  mdworker     1.3   00:00.07 5    1    56    75+   4540K+ 3636K+ 0B     0B   61M+   2423M+ 541  151  sleeping
540  top          5.2   00:01.14 1/1  0    23    37    1892K  1664K  0B     0B   44M    2403M  540  528  running
536  cookied      0.0   00:00.00 2    0    39    38    788K   496K   0B     0B   45M    2411M  536  151  sleeping
534  mdworker     0.0   00:00.18 4    0    55    87    8832K  7940K  0B     0B   52M    2422M  534  151  sleeping
528  bash         0.0   00:00.00 1    0    19    32    656K   524K   0B     0B   36M    2395M  528  527  sleeping
527  login        0.0   00:00.03 2    0    30    46    1008K  704K   0B     0B   53M    2411M  527  525  sleeping
525  Terminal     5.0   00:01.25 9    3    198+  161   16M+   8996K+ 12K    0B   48M+   2498M  525  151  sleeping
524  mdworker     0.0   00:00.17 4    0    54    97    9016K  7836K  0B     0B   54M    2425M  524  151  sleeping
522- Google Chrom 6.3   00:22.58 13   1    145   545   82M    76M    0B     0B   158M   1075M  365  365  sleeping
521  mdworker     0.0   00:00.26 4    0    58    123   10M    9368K  0B     0B   53M    2447M  521  151  sleeping
506- fsnotifier   0.0   00:00.06 3    0    32    48    820K   484K   0B     0B   41M    617M   499  499  sleeping
499  idea         122.5 10:17.05 53/1 2    601   746   783M+  781M+  72K    0B   1445M  4197M  499  151  runni



see idea, sometimes this value can be more than 300

I think itâ€™s a bug of IntelliJ?

Best,

--  
Nan Zhu


"
NirmalReddy <git@git.apache.org>,"Tue, 18 Feb 2014 15:59:03 +0000 (UTC)",[GitHub] incubator-spark pull request: Optimized imports,dev@spark.incubator.apache.org,"GitHub user NirmalReddy opened a pull request:

    https://github.com/apache/incubator-spark/pull/613

    Optimized imports

    Optimized imports and arranged according to scala style guide @
    https://cwiki.apache.org/confluence/display/SPARK/Spark+Code+Style+Guide#SparkCodeStyleGuide-Imports


You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/NirmalReddy/incubator-spark opt-imports

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/613.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #613
    
----
commit 776d6647b7337527b30fd099165cd8dbacb4d6bc
Author: NirmalReddy <nirmal_reddy2000@yahoo.com>
Date:   2014-02-18T15:42:58Z

    Optimized imports in core

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 16:03:18 +0000 (UTC)",[GitHub] incubator-spark pull request: Optimized imports,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/613#issuecomment-35399134
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 16:03:18 +0000 (UTC)",[GitHub] incubator-spark pull request: Optimized imports,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/613#issuecomment-35399136
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Heiko Braun <ike.braun@googlemail.com>,"Tue, 18 Feb 2014 17:24:45 +0100",Signal/Noise Ratio,dev@spark.incubator.apache.org,"

Wouldn't it be better to move the github messages to a dedicated email list? 

Regards, Heiko

"
Pascal Voitot Dev <pascal.voitot.dev@gmail.com>,"Tue, 18 Feb 2014 17:26:40 +0100",Re: Signal/Noise Ratio,dev <dev@spark.incubator.apache.org>,1
CodingCat <git@git.apache.org>,"Tue, 18 Feb 2014 16:29:40 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1089] fix the problem in 0.9 ...,dev@spark.incubator.apache.org,"GitHub user CodingCat opened a pull request:

    https://github.com/apache/incubator-spark/pull/614

    [SPARK-1089] fix the problem in 0.9 that  ADD_JARS value was not recognized

    https://spark-project.atlassian.net/browse/SPARK-1089
    
    load jar in process() and work around for scala issue
    
    The reason of this bug is two-folds
    
    1. in the current implementation of SparkILoop.scala, the settings.classpath is not set properly when the process() method is invoked
    
    2. the weird behaviour of Scala 2.10, (I personally thought it is a bug)
    
    if we simply set value of a PathSettings object (like settings.classpath), the isDefault is not set to true (this is a flag showing if the variable is modified), so it makes the PathResolver loads the default CLASSPATH environment variable value to calculated the path (see https://github.com/scala/scala/blob/2.10.x/src/compiler/scala/tools/util/PathResolver.scala#L215)
    
    what we have to do is to manually make this flag set, (https://github.com/CodingCat/incubator-spark/blob/e3991d97ddc33e77645e4559b13bf78b9e68239a/repl/src/main/scala/org/apache/spark/repl/SparkILoop.scala#L884)
    


You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/CodingCat/incubator-spark SPARK-1089

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/614.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #614
    
----
commit e3991d97ddc33e77645e4559b13bf78b9e68239a
Author: CodingCat <zhunansjtu@gmail.com>
Date:   2014-02-18T16:23:15Z

    load jar in process() and work around for scala issue

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 16:31:06 +0000 (UTC)",[GitHub] incubator-spark pull request: Optimized imports,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/613#issuecomment-35402590
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 16:31:06 +0000 (UTC)",[GitHub] incubator-spark pull request: Optimized imports,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/613#issuecomment-35402592
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12764/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 16:33:00 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1089] fix the problem in 0.9 ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/614#issuecomment-35402826
  
    Can one of the admins verify this patch?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Surendranauth Hiraman <suren.hiraman@velos.io>,"Tue, 18 Feb 2014 11:51:38 -0500",Re: Signal/Noise Ratio,dev@spark.incubator.apache.org,"github or specifically jenkins?

I find the actual github comments threads very useful.

-Suren






-- 

SUREN HIRAMAN, VP TECHNOLOGY
Velos
Simple. Powerful. Predictions.

440 NINTH AVENUE, 11TH FLOOR
NEW YORK, NY 10001
O: (917) 525-2466 ext. 105
F: 646.349.4063
E: suren.hiraman@v <suren.hiraman@sociocast.com>elos.io
W: www.velos.io
"
Heiko Braun <ike.braun@googlemail.com>,"Tue, 18 Feb 2014 17:53:42 +0100",Re: Signal/Noise Ratio,dev@spark.incubator.apache.org,"AFAIKT both end up in the PR messages that are send to the dev list.


<ike.braun@googlemail.com
email


"
aarondav <git@git.apache.org>,"Tue, 18 Feb 2014 17:57:28 +0000 (UTC)",[GitHub] incubator-spark pull request: Fix ExternalMap on case of key's has...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/612#discussion_r9833685
  
    This could be written pretty trivially using an Ordering, but I fear for the performance characteristics here, since this is a fast path. How does this look:
    ```
    if (pairs.isEmpty || other.pairs.isEmpty) {
       -pairs.length.compareTo(other.pairs.length)
    } else {
      -minKeyHash.compareTo(other.minKeyHash)
    }
    ```
    I think we may also need the `-` sign in front of the length for the same reason as we have it in the hash -- we want to invert the ordering since we deque the max not the min.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Tue, 18 Feb 2014 17:57:55 +0000 (UTC)",[GitHub] incubator-spark pull request: Fix ExternalMap on case of key's has...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/612#issuecomment-35413065
  
    Jenkins, this is ok to test.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Tue, 18 Feb 2014 17:59:39 +0000 (UTC)",[GitHub] incubator-spark pull request: Fix ExternalMap on case of key's has...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/612#issuecomment-35413258
  


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 18:03:15 +0000 (UTC)",[GitHub] incubator-spark pull request: Fix ExternalMap on case of key's has...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/612#issuecomment-35413691
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 18:03:15 +0000 (UTC)",[GitHub] incubator-spark pull request: Fix ExternalMap on case of key's has...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/612#issuecomment-35413690
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Tue, 18 Feb 2014 18:04:56 +0000 (UTC)",[GitHub] incubator-spark pull request: Optimized imports,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/613#discussion_r9833979
  
    Feel free to put them on the same line, we discussed that overly long imports are perfectly fine.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Gerard Maas <gerard.maas@gmail.com>,"Tue, 18 Feb 2014 19:04:55 +0100",Re: Signal/Noise Ratio,dev@spark.incubator.apache.org,"+1 please.



"
NirmalReddy <git@git.apache.org>,"Tue, 18 Feb 2014 18:06:32 +0000 (UTC)",[GitHub] incubator-spark pull request: Optimized imports,dev@spark.incubator.apache.org,"Github user NirmalReddy commented on the pull request:

    https://github.com/apache/incubator-spark/pull/613#discussion_r9834035
  
    Ohh sure will change it.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Tue, 18 Feb 2014 18:07:14 +0000 (UTC)",[GitHub] incubator-spark pull request: Optimized imports,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/613#discussion_r9834055
  
    These imports of Some are not actually required, just inserted by IDEs occasionally. This is actually the only remaining usage in our code base, so let's have away with it!


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Tue, 18 Feb 2014 18:07:39 +0000 (UTC)",[GitHub] incubator-spark pull request: Optimized imports,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/613#discussion_r9834070
  
    braces not required


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Tue, 18 Feb 2014 18:09:52 +0000 (UTC)",[GitHub] incubator-spark pull request: Optimized imports,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/613#discussion_r9834148
  
    extra whitespace here


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Tue, 18 Feb 2014 18:11:10 +0000 (UTC)",[GitHub] incubator-spark pull request: Optimized imports,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/613#discussion_r9834189
  
    nice! Deleted imports make me happy...


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
NirmalReddy <git@git.apache.org>,"Tue, 18 Feb 2014 18:12:40 +0000 (UTC)",[GitHub] incubator-spark pull request: Optimized imports,dev@spark.incubator.apache.org,"Github user NirmalReddy commented on the pull request:

    https://github.com/apache/incubator-spark/pull/613#discussion_r9834239
  
    I dint get it !! Where the extra whitespace ?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Tue, 18 Feb 2014 18:13:11 +0000 (UTC)",[GitHub] incubator-spark pull request: Optimized imports,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/613#issuecomment-35414789
  
    Woo-hoo! Thanks for doing this. This could generate a lot of merge conflicts, so I will merge after comments are addressed. Since everything compiles, it's unlikely to break anything...


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Tue, 18 Feb 2014 18:14:14 +0000 (UTC)",[GitHub] incubator-spark pull request: Optimized imports,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/613#discussion_r9834309
  
    Sorry, just meant there are 2 newlines between the last import and the following Javadoc. Not a change caused by you, but I thought I'd mention it anyway.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Tue, 18 Feb 2014 18:14:54 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#discussion_r9834337
  
    is it possible to exclude the entire `org.apache.spark.deploy` package?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
NirmalReddy <git@git.apache.org>,"Tue, 18 Feb 2014 18:17:09 +0000 (UTC)",[GitHub] incubator-spark pull request: Optimized imports,dev@spark.incubator.apache.org,"Github user NirmalReddy commented on the pull request:

    https://github.com/apache/incubator-spark/pull/613#discussion_r9834422
  
    Ya that is a clarification for me as i left of 2 newlines in many cases as i am not sure .. will complete all those in my  next commit !! Thanks !!


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Tue, 18 Feb 2014 18:21:01 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#discussion_r9834543
  
    I think you can do it with excludePackage:
    https://github.com/typesafehub/migration-manager/blob/master/core/src/main/scala/com/typesafe/tools/mima/core/Filters.scala


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Tue, 18 Feb 2014 18:22:35 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#discussion_r9834615
  


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Tue, 18 Feb 2014 18:22:55 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#discussion_r9834629
  
    Then you can add a comment that says ""Scheduler is not considered a public API""


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Tue, 18 Feb 2014 18:28:28 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#discussion_r9834854
  
    This and the one below were changed to private APIs. Also cogroupResult2ToJava, but that's not here for some reason.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 18:31:09 +0000 (UTC)",[GitHub] incubator-spark pull request: Fix ExternalMap on case of key's has...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/612#issuecomment-35416706
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 18:31:10 +0000 (UTC)",[GitHub] incubator-spark pull request: Fix ExternalMap on case of key's has...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/612#issuecomment-35416708
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12765/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Tue, 18 Feb 2014 18:33:54 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35416994
  
    @ScrapCodes hm... I noticed a bigger issue that things which are package private are not being filtered in MIMA and this is leading to a bunch of the false positives.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
mateiz <git@git.apache.org>,"Tue, 18 Feb 2014 19:01:26 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user mateiz commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35420086
  
    I just meant don't make the ""of"" methods public, since they're confusing for users to see.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Konstantin Boudnik <cos@apache.org>,"Tue, 18 Feb 2014 11:37:06 -0800","Re: IntelliJ inflates the CPU utilization when working on
	SparkIMain.scala",dev@spark.incubator.apache.org,"I haven't seen anything like this recently. But if anything, the bug is most
likely to be in the Scala plugin (or Scala itself [wink,wink]) than in the
core IDEA which has proven over the years to be incredibly stable.

If you continue seeing this, consider getting in touch with IntelliJ dev forum
that has a very short turn-around time.

Cos


"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 20:03:13 +0000 (UTC)",[GitHub] incubator-spark pull request: Optimized imports,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/613#issuecomment-35426876
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 20:03:13 +0000 (UTC)",[GitHub] incubator-spark pull request: Optimized imports,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/613#issuecomment-35426875
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Tue, 18 Feb 2014 20:09:13 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-929: Fully deprecate usage of ...,dev@spark.incubator.apache.org,"GitHub user aarondav opened a pull request:

    https://github.com/apache/incubator-spark/pull/615

    SPARK-929: Fully deprecate usage of SPARK_MEM

    This patch cements our deprecation of the SPARK_MEM environment variable by replacing it with three more specialized variables:
    SPARK_DAEMON_MEMORY, SPARK_EXECUTOR_MEM, and SPARK_DRIVER_MEM
    
    The creation of the latter two variables means that we can safely set driver/job memory without accidentally setting the executor memory.
    SPARK_EXECUTOR_MEM is not actually public -- it is only used by the Mesos scheduler (and set within SparkContext). The proper way of configuring executor memory is through the ""spark.executor.memory"" property.
    
    SPARK_DRIVER_MEM is a new public variable, which is needed because there is currently no public way of setting the memory of jobs launched by spark-class.
    
    Other memory considerations:
    - The repl's memory can be set through the ""--drivermem"" command-line option, which really just sets SPARK_DRIVER_MEM.
    - run-example doesn't use spark-class, so the only way to modify examples' memory is actually an unusual use of SPARK_JAVA_OPTS (which is normally overriden in all cases by spark-class).
    
    This patch also fixes a lurking bug where spark-shell misused spark-class (the first argument is supposed to be the main class name, not java options), as well as a bug in the Windows spark-class2.cmd. I have not yet tested this patch on either Windows or Mesos, however.


You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/aarondav/incubator-spark sparkmem

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/615.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #615
    
----
commit c6ff53251d49c6e4033d6b56268d2f3fb0166d88
Author: Aaron Davidson <aaron@databricks.com>
Date:   2014-02-17T23:09:51Z

    SPARK-929: Fully deprecate usage of SPARK_MEM
    
    This patch cements our deprecation of the SPARK_MEM environment variable
    by replacing it with case-specific variables:
    SPARK_DAEMON_MEMORY, SPARK_EXECUTOR_MEM, and SPARK_DRIVER_MEM
    
    The creation of the latter two variables means that we can safely
    set driver/job memory without accidentally setting the executor memory.
    SPARK_EXECUTOR_MEM is not actually public, though -- it is only used
    by the Mesos scheduler (and set within SparkContext). The proper way of
    configuring executor memory is through the ""spark.executor.memory""
    property.
    
    SPARK_DRIVER_MEM is a new public variable, which is needed because
    there is currently no public way of setting the memory of jobs
    launched by spark-class.
    
    Other memory considerations:
    - The repl's memory can be set through the ""--drivermem"" command-line option,
      which really just sets SPARK_DRIVER_MEM.
    - run-example doesn't use spark-class, so the only way to modify examples'
      memory is actually an unusual use of SPARK_JAVA_OPTS (which is normally
      overriden in all cases by spark-class).
    
    This patch also fixes a lurking bug where spark-shell misused spark-class
    (the first argument is supposed to be the main class name, not java
    options).

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Tue, 18 Feb 2014 20:11:58 +0000 (UTC)",[GitHub] incubator-spark pull request: Optimized imports,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/613#discussion_r9839626
  
    Nice catch on this one.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 20:13:02 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-929: Fully deprecate usage of ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/615#issuecomment-35427919
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 20:13:02 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-929: Fully deprecate usage of ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/615#issuecomment-35427920
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Aaron Davidson <ilikerps@gmail.com>,"Tue, 18 Feb 2014 12:20:45 -0800",Re: Signal/Noise Ratio,dev@spark.incubator.apache.org,"This is due, unfortunately, to Apache policies that all development-related
discussion should take place on the dev list. As we are attempting to
graduate from an incubating project to an Apache top level project, there
were some concerns raised about GitHub, and the fastest solution to avoid
conflict related to our graduation was to CC dev@ for all GitHub messages.
dealing with these messages.

In the meantime, one simple solution is to filter out all messages that
come from git@git.apache.org and are destined to
dev@spark.incubator.apache.org.



"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 20:30:59 +0000 (UTC)",[GitHub] incubator-spark pull request: Optimized imports,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/613#issuecomment-35429823
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12766/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 20:30:59 +0000 (UTC)",[GitHub] incubator-spark pull request: Optimized imports,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/613#issuecomment-35429821
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
NirmalReddy <git@git.apache.org>,"Tue, 18 Feb 2014 20:33:32 +0000 (UTC)",[GitHub] incubator-spark pull request: Optimized imports,dev@spark.incubator.apache.org,"Github user NirmalReddy commented on the pull request:

    https://github.com/apache/incubator-spark/pull/613#issuecomment-35430115
  
    @aarondav Now is it all ok to merge ??


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
rxin <git@git.apache.org>,"Tue, 18 Feb 2014 20:34:49 +0000 (UTC)",[GitHub] incubator-spark pull request: Optimized imports,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/613#discussion_r9840393
  
    We should probably rename this JDouble or just use java.lang.Double in the codebase. 


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Tue, 18 Feb 2014 20:46:28 +0000 (UTC)",[GitHub] incubator-spark pull request: Migrate Java code to Scala or move i...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/605#discussion_r9840846
  
    Hm, let's use JavaSparkContext's fakeClassTag method, so we link to the documentation on why we're doing this. You may have to rebase to get this, it was a recent change.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Matei Zaharia <matei.zaharia@gmail.com>,"Tue, 18 Feb 2014 12:46:41 -0800",Re: Signal/Noise Ratio,dev@spark.incubator.apache.org,"Yup, because these are development discussions, they need to be archived, even though some users may not view them this way (but rather subscribe using GitHub). Apache wants to make sure that all work on its projects is done “in the open” and is accessible in a central place. Just filter them out if you’d like (in Gmail, select a message and click “filter messages like this”).

Matei


development-related
there
avoid
messages.
way of
that
<mfernest@cloudera.com
<ike.braun@googlemail.com
email


"
rxin <git@git.apache.org>,"Tue, 18 Feb 2014 20:46:55 +0000 (UTC)",[GitHub] incubator-spark pull request: Optimized imports,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/613#issuecomment-35431514
  
    I made one comment on the ambiguity of Double. Other than that, LGTM.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Tue, 18 Feb 2014 20:51:04 +0000 (UTC)",[GitHub] incubator-spark pull request: Migrate Java code to Scala or move i...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/605#discussion_r9841053
  
    I'm like 85% sure we can use scala.Serializable (which extends java.io.Serializable) and should not require an import.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Tue, 18 Feb 2014 20:51:24 +0000 (UTC)",[GitHub] incubator-spark pull request: Migrate Java code to Scala or move i...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/605#discussion_r9841070
  
    same here
    `def returnType(): ClassTag[R] = JavaSparkContext.fakeClassTag`


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Tue, 18 Feb 2014 20:53:00 +0000 (UTC)",[GitHub] incubator-spark pull request: Migrate Java code to Scala or move i...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/605#issuecomment-35432171
  
    Any reason not to convert StorageLevels as well?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Tue, 18 Feb 2014 20:54:21 +0000 (UTC)",[GitHub] incubator-spark pull request: Optimized imports,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/613#issuecomment-35432325
  
    LGTM too after @rxin's change. Since Double is used throughout that file, I think renaming JDouble is cleanest.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
sryza <git@git.apache.org>,"Tue, 18 Feb 2014 20:58:59 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-929: Fully deprecate usage of ...,dev@spark.incubator.apache.org,"Github user sryza commented on the pull request:

    https://github.com/apache/incubator-spark/pull/615#issuecomment-35432894
  
    Why use MEMORY for the daemon, but MEM for the driver?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
mengxr <git@git.apache.org>,"Tue, 18 Feb 2014 21:00:59 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user mengxr commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35433131
  
    Thanks all for the suggestions! 
    
    @srowen @giyengar I updated the small benchmark suite to include commons-math3. It seems to me commons-math3 has couple design issues. First of all, its sparse implementation is based on a primitive-typed hash map, which is not efficient for most linear algebra operations. Secondly, it doesn't support in-place vector operations, e.g., BLAS's axpy. Both account to the performance drop in the benchmarks (see the results attached). Together with the fact that its sparse implementation is deprecated, I wouldn't recommend using commons-math3 as the underlying linear algebra package of mllib.
    
    For mahout-math, it is clear in the PR code that we need some hacks to avoiding copying data around and it doesn't support native BLAS/LAPACK. So we need to wrap mathout-math and jblas or netlib-java together to provide a full-speed linear algebra service, which breeze already provides.
    
    @dlwh Thanks for making a quick release of breeze-0.6.1! It is the best overall performer in the benchmarks. I'm okay to make the switch after we clear the license issues (see the license part).
    
    # Benchmarks
    
    I tested only three operations:
    
    1. dense SVD, which is used in PCA and also represents the performance of other matrix factorizations, etc.
    2. sparse matrix times dense matrix, which is used in gradient-based methods (training multiple models together), etc.
    3. dense vector plus sparse vector, which is used in KMeans, normalization, etc.
    
    This is certainly not a complete benchmark suite. Remember this PR is for finding a underlying linear algebra package for mllib's sparse data support. For simplicity, I didn't include test details. For the benchmark code, please go to https://github.com/mengxr/linalg-test
    
    ## Dense SVD
    
    breeze depends on netlib-all, which includes native libraries. jblas also packs native libraries, but it seems the performance is not as good as breeze/netlib-java. @fommil @mikiobraun Do you mind sharing which BLAS/LAPACK implementation you chose to make those native libraries and whether you enabled multi-threading? Thanks!
    
    ~~~
    jblas:    685.5577ms
    breeze:   135.0402ms
    mahout:  2626.1641ms
    commons: 2151.0861ms
    ~~~
    
    ## Sparse matrix times dense matrix
    
    The `barebone` implementation is operating directly on primitive arrays. 
    
    ~~~
    barebone:  20.1036ms
    breeze:    26.4364ms
    mahout:  2562.3702ms
    commons:   56.1518ms
    ~~~
    
    ## Dense vector plus sparse vector
    
    ~~~
    barebone:   0.033ms
    breeze:     0.037ms
    mahout:     0.075ms
    commons:   25.376ms
    ~~~
    
    # breeze and netlib-java license
    
    The following is the dependency graph of breeze-0.6.1. @fommil Could you confirm the license of netlib-all, netlib-core, and the native libraries? The jniloader is distributed under LGPL. Is it possible to change it to a commercial-friendly license such as Apache? 
    
    ~~~
    +-org.scalanlp:breeze_2.10:0.6.1                                   Apache 2.0
      +-com.github.fommil.netlib:all:1.1.2                             Same to netlib-java?
      | +-com.github.fommil.netlib:core:1.1.2                          Same to netlib-java?
      | +-com.github.fommil.netlib:netlib-native_ref-linux-x86_64:1.1  Same to netlib-java?
      | | +-com.github.fommil.netlib:native_ref-java:1.1               Same to netlib-java?
      | |   +-com.github.fommil:jniloader:1.1                          LGPL, is it okay to change it to Apache?
      | |   
      | +-net.sourceforge.f2j:arpack_combined_all:0.1                  University of Tennessee License
      | 
      +-com.github.rwl:jtransforms:2.4.0                               MPL/LGPL/GPL
      | +-junit:junit:4.8.2
      | 
      +-com.thoughtworks.paranamer:paranamer:2.2
      +-com.typesafe:scalalogging-slf4j_2.10:1.0.1
      | +-org.scala-lang:scala-reflect:2.10.0 (evicted by: 2.10.3)
      | +-org.scala-lang:scala-reflect:2.10.3
      | | +-org.scala-lang:scala-library:2.10.3
      | | 
      | +-org.slf4j:slf4j-api:1.7.2 (evicted by: 1.7.5)
      | +-org.slf4j:slf4j-api:1.7.5
      | 
      +-net.sf.opencsv:opencsv:2.3                                     Apache 2.0
      +-org.apache.commons:commons-math3:3.2
      +-org.scala-lang:scala-library:2.10.3
      +-org.scalanlp:breeze-macros_2.10:0.2
        +-org.scala-lang:scala-library:2.10.3
        +-org.scala-lang:scala-reflect:2.10.3
          +-org.scala-lang:scala-library:2.10.3
    ~~~


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Tue, 18 Feb 2014 21:02:15 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-929: Fully deprecate usage of ...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/615#issuecomment-35433301
  
    I'm amenable to switching to SPARK_DRIVER_MEMORY. I avoided it for two reasons:
    1) Following SPARK_MEM -> SPARK_DRIVER_MEM / SPARK_EXECUTOR_MEM
    2) It's shorter.
    
    However, since SPARK_MEM is supposed to never have existed, and being shorter isn't really a good argument, we can go with the version that's more consistent with the public variables.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 21:03:07 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-929: Fully deprecate usage of ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/615#issuecomment-35433387
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 21:03:07 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-929: Fully deprecate usage of ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/615#issuecomment-35433388
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12767/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 21:08:37 +0000 (UTC)",[GitHub] incubator-spark pull request: Optimized imports,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/613#issuecomment-35434152
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 21:08:37 +0000 (UTC)",[GitHub] incubator-spark pull request: Optimized imports,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/613#issuecomment-35434153
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
NirmalReddy <git@git.apache.org>,"Tue, 18 Feb 2014 21:09:05 +0000 (UTC)",[GitHub] incubator-spark pull request: Optimized imports,dev@spark.incubator.apache.org,"Github user NirmalReddy commented on the pull request:

    https://github.com/apache/incubator-spark/pull/613#issuecomment-35434195
  
    @aarondav @rxin Thanks !! All the comments addressed !!


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Pascal Voitot Dev <pascal.voitot.dev@gmail.com>,"Tue, 18 Feb 2014 22:11:24 +0100",Re: Signal/Noise Ratio,dev <dev@spark.incubator.apache.org>,"so -1 = 0 for me :D
I can understand and I'll use filter ;)

thks
Pascal



"
andy petrella <andy.petrella@gmail.com>,"Tue, 18 Feb 2014 22:25:40 +0100",Re: Signal/Noise Ratio,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Come on guyz, we could be a bit more creative right?

What about setting some streaming jobs to categorize the noisy ones (a
quick and dirty bayesian and whoooz), intercept them, add some Gmail's
spams characteristics that will make them skipping our mailbox
automatically.
A small batch job on the other for a bit of training and ...

... There ! Fixed it

Andy

PS: it was of course a joke




"
fommil <git@git.apache.org>,"Tue, 18 Feb 2014 21:36:08 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user fommil commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35437116
  
    @mengxr `netlib-java` uses **your** system optimised natives (if they are installed). So, back at you, what do you have installed?
    
    See the main page for benchmarks with Intel, AMD, Apple, ATLAS, OpenBLAS and Nvidia (CUDA) examples. It depends on your system. https://github.com/fommil/netlib-java/
    
    If you don't have BLAS installed, it'll default to the reference implementation, and if that fails to load it'll default to F2J.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
fommil <git@git.apache.org>,"Tue, 18 Feb 2014 21:43:12 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user fommil commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35437895
  
    @mengxr given that the JNILoader will likely be loading proprietary native implementations of BLAS/LAPACK, I consider it to be a moot point... but if the Apache foundation want to take an exclusive religious stance against Stallman, then I'll have to dual license to comply with your objectives.
    
    (I really don't understand why Apache have a problem with LGPL... it's a very sensible license that ensure changes are put back into the original project and **is** ""commercially friendly"")
    
    See the `netlib-java` website for further license information: in a nutshell, the bits you want are MIT since they derive from the original BLAS/LAPACK reference implementation.



---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
fommil <git@git.apache.org>,"Tue, 18 Feb 2014 21:45:58 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user fommil commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35438198
  
    @mengxr no MTJ in the benchmarks? ;-) Given that it has the most mature sparse library on the JVM, I am surprised that you omitted it.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
MLnick <git@git.apache.org>,"Tue, 18 Feb 2014 21:49:53 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user MLnick commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35438611
  
    @mengxr <https://github.com/mengxr> at the risk of adding to your
    workload... I think (license issues aside since I suppose both Breeze and
    MTJ are affected in the same way), that it would be great to get a sense of
    MTJ performance (which should in theory be as good as or better than
    Breeze).
    
    performance suffered due to heavy implicit use / object creation. It would
    be good to know if this is not the case anymore (ie if recent dev had fixed
    this). If not, MTJ would be a good alternative (again, wrapped in a
    lightweight DSL if we wish).
    
    
    
    > @mengxr <https://github.com/mengxr> no MTJ in the benchmarks? ;-) Given
    > that it has the most mature sparse library on the JVM, I am surprised that
    > you omitted it.
    >
    > --
    > Reply to this email directly or view it on GitHub<https://github.com/apache/incubator-spark/pull/575#issuecomment-35438198>
    > .
    >


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
mateiz <git@git.apache.org>,"Tue, 18 Feb 2014 22:01:45 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user mateiz commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35439917
  
    Hey Prashant, I've looked at this more, and one final thing I'd like to do is to see if we can reduce the number of methods and classes people have to deal with. In particular, in the current code, we have versions of each method for both Function and IFunction. Ideally, we'd like something like the following:
    * Users on Java 6/7 still write code the way they used to, e.g. `rdd.map(new Function() { ... })`).
    * The Function, PairFunction, etc classes are now interfaces with just a call() method. We wrap them into Scala function objects only later, using classes that are not visible in the public API (e.g. we have a `private[spark] class FunctionWrapper` that takes a `Function` and also extends `scala.Function1`).
    * This means that Java 8 users use the same methods as 6/7 ones, but can pass in a lambda.
    
    The only problem I see this is that the old API had overloaded methods based on the *type* of the function passed, e.g. `map` could take both a `Function` and a `PairFunction`. This is not going to work with lambdas, as mentioned above, so my suggestion here is to slightly *break* the old API, so that users who want to pass a `PairFunction` have to use `mapToPair`. It's a bit unfortunate that we have to do this, but the good thing is that it immediately creates a compile error and we can tell people how to recompile.
    
    If you find a way that still meets the 3 goals above and doesn't break the API, that's even better, but I'd go for this one right now.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 22:03:07 +0000 (UTC)",[GitHub] incubator-spark pull request: Optimized imports,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/613#issuecomment-35440054
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 18 Feb 2014 22:03:07 +0000 (UTC)",[GitHub] incubator-spark pull request: Optimized imports,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/613#issuecomment-35440056
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12768/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
tgravescs <git@git.apache.org>,"Tue, 18 Feb 2014 22:17:36 +0000 (UTC)","[GitHub] incubator-spark pull request: Add Security to Spark - Akka, Http, ...",dev@spark.incubator.apache.org,"Github user tgravescs commented on the pull request:

    https://github.com/apache/incubator-spark/pull/332#discussion_r9844868
  
    The realm isn't actually even used. This is just a remnant of copying it from Hadoop.  Its being passed in as the serverName. I think as along as the client and server match it doesn't matter. I'll update.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
mengxr <git@git.apache.org>,"Tue, 18 Feb 2014 22:17:36 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user mengxr commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35442089
  
    @fommil I have the native vecLib BLAS/LAPACK shipped with Mac OS X and OpenBLAS installed for testing. OpenBLAS is not on the search path. I deleted both and re-ran the benchmarks and got the same result for netlib-java, so I believe it used the native library from netlib-all. This is the message I got when running the benchmarks:
    
    ~~~
    Feb 18, 2014 12:12:45 PM com.github.fommil.jni.JniLoader liberalLoad
    INFO: successfully loaded /var/folders/n1/j__6_vpj1xs3w53qp52c6f2h0000gn/T/jniloader3087212581602171772netlib-native_system-osx-x86_64.jnilib
    ~~~
    
    @MLnick I talked to @etrain about the issues they've seen with breeze at the very early stage of mllib, but it seems hard to recall exactly what the issues are and how to reproduce them. @dlwh should be able to answer what has changed in breeze since then. Actually, I'm quite surprised to see that breeze maintains good performance with heavy implicit usage.
    
    @MLnick @fommil I'll add MTJ in the benchmarks when I have some free cycles. But please understand that this PR is for Spark/MLlib, where we cannot proceed without clearing the license problems. I don't quite understand the detailed reasoning, but I do know many companies would avoid LGPL.
    
    @fommil It would be great if you can add Apache license to jniloader dual. That would clear the license issue of breeze. I understand changing MTJ's license needs some communication with its original author and hence time, but it would be great if you have a good estimate on whether it is possible and how long it may take.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
MLnick <git@git.apache.org>,"Tue, 18 Feb 2014 22:27:33 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user MLnick commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35443064
  
    @mengxr <https://github.com/mengxr> ok, that is interesting. I have always
    advocated for Breeze, but was told 6 months ago that it was a non-starter
    due to these performance overheads. If that has changed I'm super happy.
    Given that all we need for MLlib (for now anyway) is sparse vectors, I am
    still in favour of Breeze. Since I believe breeze-math has great potential
    to become the de facto numpy of Scala (is already really), I believe we
    should rather spend resources making Breeze better where necessary, which
    benefits Spark as well as the rest of the (growing) Scala scientific
    computing community.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
tgravescs <git@git.apache.org>,"Tue, 18 Feb 2014 22:32:02 +0000 (UTC)","[GitHub] incubator-spark pull request: Add Security to Spark - Akka, Http, ...",dev@spark.incubator.apache.org,"Github user tgravescs commented on the pull request:

    https://github.com/apache/incubator-spark/pull/332#discussion_r9845370
  
    Can you clarify? What else do you think we should check.
    
    If its not a security negotiation message and we are expecting one the message ends up being ignored. As the comment states in the ConnectionManager code, we could enhance this to send a message back to the client for them to do something smart, like possibly retry if somehow the message got corrupt.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
tgravescs <git@git.apache.org>,"Tue, 18 Feb 2014 22:39:51 +0000 (UTC)","[GitHub] incubator-spark pull request: Add Security to Spark - Akka, Http, ...",dev@spark.incubator.apache.org,"Github user tgravescs commented on the pull request:

    https://github.com/apache/incubator-spark/pull/332#discussion_r9845698
  
    This is just to make sure if the connection is removed before sasl completes it gets cleaned up.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
fommil <git@git.apache.org>,"Tue, 18 Feb 2014 22:46:13 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user fommil commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35444819
  
    @mengxr that's using veclib.
    
    Re: implicits and breeze, I don't know why you think it's a problem. Implicits are a compile time feature and combined with value classes, there is zero runtime overhead. Or is there something else you are referring to?
    
    Re: LGPL, I don't know any OSS friendly companies that would avoid it (and I contract with all the big boys). The licensing is a pissing contest between Apache and FSF. LGPL gives the original developers much more power than the Apache suite.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Tue, 18 Feb 2014 22:46:37 +0000 (UTC)",[GitHub] incubator-spark pull request: Optimized imports,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/613#issuecomment-35444870
  
    Thanks! Merged into master. This PR was rebased up to the previous master HEAD, so there are no commits that could've conflicted and result in duplicate imports during the merge.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
tgravescs <git@git.apache.org>,"Tue, 18 Feb 2014 22:47:02 +0000 (UTC)","[GitHub] incubator-spark pull request: Add Security to Spark - Akka, Http, ...",dev@spark.incubator.apache.org,"Github user tgravescs commented on the pull request:

    https://github.com/apache/incubator-spark/pull/332#discussion_r9845970
  
    this was moved it til after the SparkEnv was created so that any classes loaded could use the SparkEnv for the securityManager. Specifically ExecutorClassLoader uses it.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
fommil <git@git.apache.org>,"Tue, 18 Feb 2014 22:47:38 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user fommil commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35444959
  
    @MLnick I agree, Breeze and Spire are a solid foundation for numerics in Scala.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
tgravescs <git@git.apache.org>,"Tue, 18 Feb 2014 22:48:58 +0000 (UTC)","[GitHub] incubator-spark pull request: Add Security to Spark - Akka, Http, ...",dev@spark.incubator.apache.org,"Github user tgravescs commented on the pull request:

    https://github.com/apache/incubator-spark/pull/332#discussion_r9846054
  
    its abbreviation of negotiation. I'll add some java docs above it and perhaps just put the entire word there.   Its just getting to be a long name.  I'll see if I can come up with better name.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
rxin <git@git.apache.org>,"Tue, 18 Feb 2014 22:57:18 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35445845
  
    @fommil maybe that changed in 2.10.x entirely given the addition of value classes, and maybe Breeze is very careful in its implicit usage, but often implicits in scala 2.9.x can be super confusing and result in creation of new objects that are not obvious and can be a performance killer. 


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
fommil <git@git.apache.org>,"Tue, 18 Feb 2014 23:05:02 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user fommil commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35446562
  
    @rxin I've not seen implicits to cause a problem, except in high frequency scenarios. I suspect Breeze might have suffered from auto boxing, brought on by implicit use, which Spire (and a new compiler plugin) is trying to address and where specialized failed 


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
fommil <git@git.apache.org>,"Tue, 18 Feb 2014 23:06:13 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user fommil commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35446662
  
    Actually, @non it's worth you casting an eye over this discussion as the primary author of spire


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
srowen <git@git.apache.org>,"Tue, 18 Feb 2014 23:16:18 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user srowen commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35447529
  
    For what it's worth, I like the idea of using breeze, even though I know little about it. Mostly, I like the idea of using something consistent most of all, and fast second, and for me that need not be commons math. 
    
    I am all too aware of how tidy abstractions in a library like this can add killer runtime overhead. I am doubly worried in Scala (not necessarily because of implicits). At the same time, looks like breeze does a fine job already at making things fast where they count.
    
    As an aside @fommil, while I don't have love for some of the ASF bike-shedding over license stuff, their assertion is that the LGPL is incompatible with AL2 (http://www.apache.org/legal/resolved.html#category-x), not just something ASF dislikes. (Your interpretations may vary; but hey that's Apache's and it's coherent.) AL2 indeed is supposed to empower the end user more than developer, and in some cases that's the right thing. In any event the AL2 is a faÃ®t acoompli for *Apache* Spark.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
asfgit <git@git.apache.org>,"Tue, 18 Feb 2014 23:25:25 +0000 (UTC)",[GitHub] incubator-spark pull request: Optimized imports,dev@spark.incubator.apache.org,"Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-spark/pull/613


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
martinjaggi <git@git.apache.org>,"Tue, 18 Feb 2014 23:30:09 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user martinjaggi commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35448685
  
    Thanks @mengxr for the benchmark efforts! Just not sure if you got my comment about part 2) in the benchmark, k-means: In my opinion this algorithm is not very unsuitable to judge the sparse vector overhead, since it's the only method in MLlib currently that does *not* communicate the vectors (only the dense centers). In contrast, all gradient based methods need to communicate the sparse vectors in each iteration (of a MR). For these, often serialization can take about the same time as taking the vector x vector product, which is all the computation; so just saying that both are important in practice, but currently we only benchmark one of the two, right?
    
    Maybe things like that might have something to do with what @etrain ran into with early sparse tests? Or do you guys think this is not an issue? I would be curious to see how the candidates perform on some of the gradient stuff, and like at which sparsity/load factor the sparse vectors will start beating the dense vectors.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
mengxr <git@git.apache.org>,"Tue, 18 Feb 2014 23:45:52 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user mengxr commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35449886
  
    @fommil @MLnick I included MTJ into the benchmarks (see the updated comment above). Basically it performs very similar to breeze.
    
    @martinjaggi Gradient based method needs dot product between sparse and dense vectors, or multiplication between sparse matrix and dense vectors if we consider creating a local sparse matrix first. If the input RDD to gradient based method is not cached, I would recommend cache it first or down-sample it if it is too large to cache. If serialization of the input data occurs for every iteration, the computation cost becomes negligible. If data is cached and we don't copy data around during the conversion from the data model we defined and the underlying vector implementation, the overhead is very small. I'm also working on a performance test suite for MLlib algorithms to make it easy for us to do the comparison.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
dlwh <git@git.apache.org>,"Tue, 18 Feb 2014 23:57:02 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user dlwh commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35450646
  
    @mengxr thanks for doing all this!  It's nice to see that the overhead in Breeze is largely negligible  as compared to MTJ (and maybe even slightly better sometimes?).
    
    I'm pretty aggressive in avoiding auto-boxing in Breeze, and there are relatively few implicit conversions. Auto-boxing definitely crops up here and there, but between codegen and specialization, I think most of the tight loops are entirely unboxed. Spire's macros would hopefully remove some of the byte code bloat from the codegen.. (and @VladUreche's miniboxing might make things even better, one day.) @fommil is right that implicits really don't cause a performance problem, especially not the way they're used in Breeze (which is to say, few implicit conversions, lots of implicit parameters).  Generic programming can be a problem, and implicit parameters make it tempting to do more of it. 
    
    I'm curious to know what you guys found in the past with Breeze. The only thing I ever heard from ya'll before now was someone trying to update a CSCMatrix constantly, which is just always going to be slow. (I'm sure there must be problems. Breeze is young enough that some code paths aren't as tested as they could be.)


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Punya Biswal <pbiswal@palantir.com>,"Wed, 19 Feb 2014 00:00:00 +0000","Request to review PR#605: migrate Java code to Scala or
 src/main/java","""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi all,

Could someone review my pull request
<https://github.com/apache/incubator-spark/pull/605> ? I implemented Aaron¹s
initial suggestion and all tests appear to pass.

Punya



"
Punya Biswal <pbiswal@palantir.com>,"Wed, 19 Feb 2014 00:04:43 +0000","Re: Request to review PR#605: migrate Java code to Scala or
 src/main/java","""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Apologies, I see that Aaron¹s already on it. Please disregard the noise.

Punya

From:  Punya Biswal <pbiswal@palantir.com>
Reply-To:  <dev@spark.incubator.apache.org>
Date:  Tuesday, February 18, 2014 at 7:00 PM
To:  ""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>
Subject:  Request to review PR#605: migrate Java code to Scala or
src/main/java

Hi all,

Could someone review my pull request
<https://urldefense.proofpoint.com/v1/url?u=https://github.com/apache/incuba
tor-spark/pull/605&k=fDZpZZQMmYwf27OU23GmAQ%3D%3D%0A&r=kTrYN051orSRhyA6mqYxb
jRIX%2BBCPm7thmzLC79vBeM%3D%0A&m=rIX85gxQBH4fNRKI5k9mr7fW%2BxLUlKjm5JsI2CfTW
cU%3D%0A&s=ea1d399fd9d276fd7401ee8cfc897a43c8b9a055e287b65efe3d5cfdfc205eb0? I implemented Aaron¹s initial suggestion and all tests appear to pass.

Punya



"
CodingCat <git@git.apache.org>,"Wed, 19 Feb 2014 00:20:03 +0000 (UTC)",[GitHub] incubator-spark pull request: fix site scala version error in doc,dev@spark.incubator.apache.org,"GitHub user CodingCat opened a pull request:

    https://github.com/apache/incubator-spark/pull/616

    fix site scala version error in doc

    fix site scala version error

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/CodingCat/incubator-spark doc_version

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/616.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #616
    
----
commit eafd99a1ba740fcb0b54ab6244db8104f46db0aa
Author: CodingCat <zhunansjtu@gmail.com>
Date:   2014-02-19T00:22:52Z

    fix site scala version error in doc

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 00:23:01 +0000 (UTC)",[GitHub] incubator-spark pull request: fix site scala version error in doc,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/616#issuecomment-35452414
  
    Can one of the admins verify this patch?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Wed, 19 Feb 2014 00:29:49 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1105] fix site scala version ...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/616#issuecomment-35452922
  
    Thanks! Merged.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Wed, 19 Feb 2014 00:37:16 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1105] fix site scala version ...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/616#issuecomment-35453423
  
    Also merged into branch-0.9, as we also use scala-2.10.3 there.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Wed, 19 Feb 2014 00:47:14 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark 1095 : Adding explicit return ...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/610#discussion_r9849773
  
    perhaps you meant to add a type here?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
punya <git@git.apache.org>,"Wed, 19 Feb 2014 00:50:02 +0000 (UTC)",[GitHub] incubator-spark pull request: Migrate Java code to Scala or move i...,dev@spark.incubator.apache.org,"Github user punya commented on the pull request:

    https://github.com/apache/incubator-spark/pull/605#issuecomment-35454254
  
    Unfortunately, a val in an object translates into a zero-argument static method rather than a static final field. There was a plan to add a @static annotation in Scala 2.10 which would have allowed people to create such fields, but that was reverted before release.
    
    As a result, we can either keep StorageLevels in Java for now, convert it to Scala now and break API compatibility for existing Java users, or wait until @static arrives in a future Scala version.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Wed, 19 Feb 2014 00:50:46 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark 1095 : Adding explicit return ...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/610#discussion_r9849854
  
    Unfortuantely, we decided not to go with the return type on a new line style (see dev list). The acceptable style is one of these two:
    
    ```
     def accumulable[T, R](initialValue: T)
                          (implicit param: AccumulableParam[T, R]): Accumulable[T, R] =
      new Accumulable(initialValue, param)
    ```
    or
    ```
     def accumulable[T, R]
        (initialValue: T)
        (implicit param: AccumulableParam[T, R]): Accumulable[T, R] =
      new Accumulable(initialValue, param)
    ```



---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
punya <git@git.apache.org>,"Wed, 19 Feb 2014 00:51:22 +0000 (UTC)",[GitHub] incubator-spark pull request: Migrate Java code to Scala or move i...,dev@spark.incubator.apache.org,"Github user punya commented on the pull request:

    https://github.com/apache/incubator-spark/pull/605#discussion_r9849872
  
    Done (here and elsewhere).


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
punya <git@git.apache.org>,"Wed, 19 Feb 2014 00:52:12 +0000 (UTC)",[GitHub] incubator-spark pull request: Migrate Java code to Scala or move i...,dev@spark.incubator.apache.org,"Github user punya commented on the pull request:

    https://github.com/apache/incubator-spark/pull/605#discussion_r9849897
  
    This is great -- when I first read the weird ClassTag pattern I was puzzled about its apparent dishonesty. Rebased and changed, here and elsewhere.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
CodingCat <git@git.apache.org>,"Wed, 19 Feb 2014 00:55:28 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1105] fix site scala version ...,dev@spark.incubator.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/incubator-spark/pull/616#issuecomment-35454588
  
    thanks


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Wed, 19 Feb 2014 00:57:49 +0000 (UTC)",[GitHub] incubator-spark pull request: Migrate Java code to Scala or move i...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/605#issuecomment-35454755
  
    Ah, got it. I actually didn't realize that StorageLevel.scala is the Scala version, and StorageLevels is the Java version (clear naming scheme we got going there). So from the beginning this was just intended as the Java API to the storage levels. Makes sense, thanks


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Wed, 19 Feb 2014 01:00:57 +0000 (UTC)",[GitHub] incubator-spark pull request: Migrate Java code to Scala or move i...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/605#discussion_r9850119
  
    The exact same confusion led me to centralize and document this :)
    
    Just as a stylistic thing, I think we can put these on one line (that's the ideal for single-line functions without braces). Also the [R] is inferred from the return type, so this is sufficient:
    `def returnType(): ClassTag[R] = JavaSparkContext.fakeClassTag`


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 01:04:21 +0000 (UTC)",[GitHub] incubator-spark pull request: Migrate Java code to Scala or move i...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/605#issuecomment-35455182
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 01:04:21 +0000 (UTC)",[GitHub] incubator-spark pull request: Migrate Java code to Scala or move i...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/605#issuecomment-35455183
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 01:05:53 +0000 (UTC)",[GitHub] incubator-spark pull request: Migrate Java code to Scala or move i...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/605#issuecomment-35455285
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12769/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 01:05:53 +0000 (UTC)",[GitHub] incubator-spark pull request: Migrate Java code to Scala or move i...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/605#issuecomment-35455283
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
mengxr <git@git.apache.org>,"Wed, 19 Feb 2014 01:26:32 +0000 (UTC)",[GitHub] incubator-spark pull request: check key name and identity file bef...,dev@spark.incubator.apache.org,"GitHub user mengxr opened a pull request:

    https://github.com/apache/incubator-spark/pull/617

    check key name and identity file before launch a cluster

    I launched an EC2 cluster without providing a key name and an identity file. The error showed up after two minutes. It would be good to check those options before launch, given the fact that EC2 billing rounds up to hours.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/mengxr/incubator-spark ec2

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/617.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #617
    
----
commit 2dfb3169996efe32a81c0ee7320ef72f1e2a68df
Author: Xiangrui Meng <meng@databricks.com>
Date:   2014-02-19T01:21:47Z

    check key name and identity file before launch a cluster

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
asfgit <git@git.apache.org>,"Wed, 19 Feb 2014 01:29:51 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1105] fix site scala version ...,dev@spark.incubator.apache.org,"Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-spark/pull/616


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Wed, 19 Feb 2014 01:45:47 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1105] fix site scala version ...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/616#issuecomment-35457550
  
    @aarondav this needs to be revered... it will break everywhere we tell people how to depend on Spark:
    
        scala-programming-guide.md:    artifactId = spark-core_{{site.SCALA_VERSION}}
    
    The issue is that in Scala 2.10+ artifacts are described as only 2.10. Probably we need to add a short scala version and a full length scala version and have two different variables in the docs:
    
    http://search.maven.org/#search%7Cga%7C1%7Ca%3A%22spark-core_2.10%22



---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Wed, 19 Feb 2014 01:48:43 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1105] fix site scala version ...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/616#issuecomment-35457734
  
    I reverted this in master and 0.9


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
CodingCat <git@git.apache.org>,"Wed, 19 Feb 2014 01:50:58 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1105] fix site scala version ...,dev@spark.incubator.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/incubator-spark/pull/616#issuecomment-35457861
  
    Sorry, @pwendell , I oversimplified the situation


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Wed, 19 Feb 2014 01:52:35 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1105] fix site scala version ...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/616#issuecomment-35457956
  
    I'm looking into it with @aarondav right now... just want to make sure this doesn't break anything.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 02:03:47 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1106: check key name and ident...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/617#issuecomment-35458583
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 02:03:47 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1106: check key name and ident...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/617#issuecomment-35458584
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 02:04:10 +0000 (UTC)",[GitHub] incubator-spark pull request: Migrate Java code to Scala or move i...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/605#issuecomment-35458603
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 02:13:51 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1106: check key name and ident...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/617#issuecomment-35459064
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12770/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 02:13:52 +0000 (UTC)",[GitHub] incubator-spark pull request: Migrate Java code to Scala or move i...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/605#issuecomment-35459066
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 02:13:51 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1106: check key name and ident...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/617#issuecomment-35459062
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
mengxr <git@git.apache.org>,"Wed, 19 Feb 2014 02:21:12 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1106: check key name and ident...,dev@spark.incubator.apache.org,"Github user mengxr commented on the pull request:

    https://github.com/apache/incubator-spark/pull/617#issuecomment-35459427
  
    retest this please


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Wed, 19 Feb 2014 02:21:25 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1105] fix site scala version ...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/616#issuecomment-35459440
  
    Hey so I looked at it more closely, we need to look on a case-by-case basis where this is used. Some of the cases should be changed to 2.10.3 and others should stay as 2.10. I think most of them should stay as-is but we should probably rename what is now SCALA_VERSION to SCALA_BINARY_VERSION.
    
    The main one I think we need to change is in quick-start.md but there may be others as well. If you look at the release audit script, I did something similar there where we used both SCALA_VERSION and SCALA_BINARY_VERSION.
    
    https://github.com/pwendell/spark-utils/blob/master/release-audits/release_auditor.py#L19


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Wed, 19 Feb 2014 02:24:18 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1106: check key name and ident...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/617#issuecomment-35459569
  
    Thanks this is a nice. LGTM.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
rxin <git@git.apache.org>,"Wed, 19 Feb 2014 02:24:51 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1106: check key name and ident...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/617#issuecomment-35459609
  
    Jenkins, retest this please.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
shivaram <git@git.apache.org>,"Wed, 19 Feb 2014 02:28:02 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1106: check key name and ident...,dev@spark.incubator.apache.org,"Github user shivaram commented on the pull request:

    https://github.com/apache/incubator-spark/pull/617#issuecomment-35459776
  
    BTW I don't think jenkins verifies this file, so you can merge this even if jenkins is flaky.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
CodingCat <git@git.apache.org>,"Wed, 19 Feb 2014 02:28:17 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1105] fix site scala version ...,dev@spark.incubator.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/incubator-spark/pull/616#issuecomment-35459785
  
    @pwendell , Yes, I just grep the string, it seems so
    
    I will fix this  


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
rxin <git@git.apache.org>,"Wed, 19 Feb 2014 02:29:44 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1106: check key name and ident...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/617#issuecomment-35459852
  
    Ok I merged this. Thanks guys.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
asfgit <git@git.apache.org>,"Wed, 19 Feb 2014 02:33:35 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1106: check key name and ident...,dev@spark.incubator.apache.org,"Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-spark/pull/617


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
asfgit <git@git.apache.org>,"Wed, 19 Feb 2014 02:33:39 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1106: check key name and ident...,dev@spark.incubator.apache.org,"Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-spark/pull/617


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
colorant <git@git.apache.org>,"Wed, 19 Feb 2014 02:44:37 +0000 (UTC)","[GitHub] incubator-spark pull request: For SPARK-1082, Use Curator for ZK i...",dev@spark.incubator.apache.org,"Github user colorant commented on the pull request:

    https://github.com/apache/incubator-spark/pull/611#discussion_r9852391
  
    Yes, I think we could if no other possible LeaderElectionAgent will need it. And it will need to change the program flow say create some method to replace prestart/poststop etc. So I just keep it untouched in this pull request. Or you think we should do away it right now?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
CodingCat <git@git.apache.org>,"Wed, 19 Feb 2014 02:45:47 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1105] fix site scala version ...,dev@spark.incubator.apache.org,"GitHub user CodingCat opened a pull request:

    https://github.com/apache/incubator-spark/pull/618

    [SPARK-1105] fix site scala version error in docs 

    https://spark-project.atlassian.net/browse/SPARK-1105
    
    fix site scala version error

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/CodingCat/incubator-spark doc_version

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/618.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #618
    
----
commit eafd99a1ba740fcb0b54ab6244db8104f46db0aa
Author: CodingCat <zhunansjtu@gmail.com>
Date:   2014-02-19T00:22:52Z

    fix site scala version error in doc

commit 46bf0c4a1e061da7e3d17efb8624b673cba5fdff
Author: CodingCat <zhunansjtu@gmail.com>
Date:   2014-02-19T02:46:08Z

    rename original SCALA_VERSION to SCALA_BINARY_VERSION and SCALA_VERSION is now the full version number

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
CodingCat <git@git.apache.org>,"Wed, 19 Feb 2014 02:47:51 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1105] fix site scala version ...,dev@spark.incubator.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/incubator-spark/pull/618#issuecomment-35460726
  
    @pwendell @aarondav I'm sorry, I will be more careful next time


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
colorant <git@git.apache.org>,"Wed, 19 Feb 2014 02:49:20 +0000 (UTC)","[GitHub] incubator-spark pull request: For SPARK-1082, Use Curator for ZK i...",dev@spark.incubator.apache.org,"Github user colorant commented on the pull request:

    https://github.com/apache/incubator-spark/pull/611#discussion_r9852483
  
    Actually, I thnk the LeaderLatch already synchronized on it's setLeadership method. And it will call isLeader/notLeader with sameThreadExecutor by execute method. it seems to me this will lead to an immediately call upon these two method. So I am not sure why curator's API doc mentioned that the hasLeaderShip could change during these two methods call.
    
    And, anyway, the is/notLeader() should be called in sequence, So I think by just return here and give the last one a real update chance is probably enough.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
CrazyJvm <git@git.apache.org>,"Wed, 19 Feb 2014 02:57:20 +0000 (UTC)",[GitHub] incubator-spark pull request: url error,dev@spark.incubator.apache.org,"GitHub user CrazyJvm opened a pull request:

    https://github.com/apache/incubator-spark/pull/619

    url error

    url of ""Collaborative Filtering for Implicit Feedback Datasets""  is invalid now. A new url is provided. http://research.yahoo.com/files/HuKorenVolinsky-ICDM08.pdf

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/CrazyJvm/incubator-spark master

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/619.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #619
    
----
commit 590d56e10f8b738d6b95f88564b5783d1098080b
Author: Chen Chao <crazyjvm@gmail.com>
Date:   2014-02-19T02:55:55Z

    url error
    
    url of ""Collaborative Filtering for Implicit Feedback Datasets"" is invalid now. A new url is provided. http://research.yahoo.com/files/HuKorenVolinsky-ICDM08.pdf]

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
rxin <git@git.apache.org>,"Wed, 19 Feb 2014 03:01:43 +0000 (UTC)",[GitHub] incubator-spark pull request: url error,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/619#issuecomment-35461371
  
    Do you mind creating a JIRA ticket and update the pull request title to include that?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 03:03:36 +0000 (UTC)",[GitHub] incubator-spark pull request: Migrate Java code to Scala or move i...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/605#issuecomment-35461454
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12771/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 03:03:36 +0000 (UTC)",[GitHub] incubator-spark pull request: Migrate Java code to Scala or move i...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/605#issuecomment-35461451
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 03:03:54 +0000 (UTC)",[GitHub] incubator-spark pull request: url error,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/619#issuecomment-35461470
  
    Can one of the admins verify this patch?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 03:03:55 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1105] fix site scala version ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/618#issuecomment-35461472
  
    Can one of the admins verify this patch?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
colorant <git@git.apache.org>,"Wed, 19 Feb 2014 03:04:58 +0000 (UTC)","[GitHub] incubator-spark pull request: For SPARK-1082, Use Curator for ZK i...",dev@spark.incubator.apache.org,"Github user colorant commented on the pull request:

    https://github.com/apache/incubator-spark/pull/611#issuecomment-35461508
  
    @aarondav , I don't run it with FaultToleranceTest, It seems to me it will need a docker installation? I did not play with docker before, I will try to set it up. And yet, I did manually verify it will zookeeper by kill / restart master / worker etc.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 03:08:56 +0000 (UTC)","[GitHub] incubator-spark pull request: For SPARK-1082, Use Curator for ZK i...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/611#issuecomment-35461699
  
     Build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 03:09:04 +0000 (UTC)","[GitHub] incubator-spark pull request: For SPARK-1082, Use Curator for ZK i...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/611#issuecomment-35461711
  
    Build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
CrazyJvm <git@git.apache.org>,"Wed, 19 Feb 2014 03:09:20 +0000 (UTC)","[GitHub] incubator-spark pull request: MLLIB-24: url of ""Collaborative Filt...",dev@spark.incubator.apache.org,"Github user CrazyJvm commented on the pull request:

    https://github.com/apache/incubator-spark/pull/619#issuecomment-35461718
  
    Done. MLLIB-24. https://spark-project.atlassian.net/browse/MLLIB-24


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
semihsalihoglu <git@git.apache.org>,"Wed, 19 Feb 2014 03:23:19 +0000 (UTC)",[GitHub] incubator-spark pull request: Graph primitives2,dev@spark.incubator.apache.org,"Github user semihsalihoglu commented on the pull request:

    https://github.com/apache/incubator-spark/pull/580#issuecomment-35462304
  
    I had written two comments to this but didn't get a reply yet:
    https://github.com/apache/incubator-spark/pull/580#issuecomment-34731728
    
    So just to copy and paste my reply:
    
    ""Just to clarify, here's what happened exactly. I had forgotten to delete
    those comments before committing the first time by mistake. Then I fixed
    them and pushed a second commit. This second commit had two style errors.
    For which I had a third commit. If you guys prefer, I can start from
    scratch. Let me know.""
    
    Thanks,
    
    semih
    
    
    
    > I see a bunch of commented tests in the patch ...
    >
    > --
    > Reply to this email directly or view it on GitHub<https://github.com/apache/incubator-spark/pull/580#issuecomment-35022812>
    > .
    >


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
rxin <git@git.apache.org>,"Wed, 19 Feb 2014 03:31:52 +0000 (UTC)","[GitHub] incubator-spark pull request: MLLIB-24: url of ""Collaborative Filt...",dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/619#discussion_r9853229
  
    Need to wrap the line so it fits within 100 chars.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 04:03:35 +0000 (UTC)","[GitHub] incubator-spark pull request: For SPARK-1082, Use Curator for ZK i...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/611#issuecomment-35464125
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12772/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 04:03:35 +0000 (UTC)","[GitHub] incubator-spark pull request: For SPARK-1082, Use Curator for ZK i...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/611#issuecomment-35464124
  
    Build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Andrew Ash <andrew@andrewash.com>,"Tue, 18 Feb 2014 20:07:51 -0800",Bug in spark.shuffle.spill setting? (0.9.0),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi dev list,

I'm running into an issue where I'm seeing different results from Spark
when I run with spark.shuffle.spill=false vs leaving it at the default
(true).

It's on internal data so I can't share my exact repro, but here's roughly
what I'm doing:

val rdd = sc.textFile(...)
  .map(l => ... (col1, col2))  // parse CSV into Tuple2[String,String]
  .distinct
  .join(
    sc.textFile(...)
       .map(l => ... (col1, col2))  // parse CSV into Tuple2[String,String]
       .distinct
  )
  .map{ case (k,(v1,v2)) => Seq(v1,k,v2).mkString(""|"") }

Then I output:
(rdd.count, rdd.distinct.count)

When I run with spark.shuffle.spill=false I get this:
(3192729,3192729)

And with spark.shuffle.spill=true I get this:
(3192931,3192726)

Has anyone else seen any bugs in join-heavy operations while using
spark.shuffle.spill=true?

My current theory is that I have a hashcode collision between rows (unusual
hashcode().

Would appreciate some additional eyes on this problem for sure.


Thanks!
Andrew
"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 04:09:42 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-929: Fully deprecate usage of ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/615#issuecomment-35464374
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 04:09:42 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-929: Fully deprecate usage of ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/615#issuecomment-35464375
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Wed, 19 Feb 2014 04:10:36 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-929: Fully deprecate usage of ...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/615#issuecomment-35464406
  
    Changed to be SPARK_DRIVER_MEMORY


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
mridulm <git@git.apache.org>,"Wed, 19 Feb 2014 04:15:11 +0000 (UTC)","[GitHub] incubator-spark pull request: Add Security to Spark - Akka, Http, ...",dev@spark.incubator.apache.org,"Github user mridulm commented on the pull request:

    https://github.com/apache/incubator-spark/pull/332#discussion_r9853850
  
    Oh, very benign change : i meant, the if else is not required :-)
    The line is equivalent to :
    newMessage.isSecurityNeg = header.securityNeg == 1


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
mridulm <git@git.apache.org>,"Wed, 19 Feb 2014 04:21:39 +0000 (UTC)","[GitHub] incubator-spark pull request: Add Security to Spark - Akka, Http, ...",dev@spark.incubator.apache.org,"Github user mridulm commented on the pull request:

    https://github.com/apache/incubator-spark/pull/332#discussion_r9853916
  
    Typically, this is set to the domain; though nowadays it is common to set it to a static application specific string (which is probably what hadoop is doing ?) since authorization is fairly limited in scope (allow or deny as a while : no domain specific acls and/or resource/access partitions).
    
    Would be nice if this could be overridden by users via some (hidden for now) env variable - simply to allow for future customization without code change : this is ofcourse not very important given the scope of the PR itself !


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
mridulm <git@git.apache.org>,"Wed, 19 Feb 2014 04:25:43 +0000 (UTC)","[GitHub] incubator-spark pull request: Add Security to Spark - Akka, Http, ...",dev@spark.incubator.apache.org,"Github user mridulm commented on the pull request:

    https://github.com/apache/incubator-spark/pull/332#discussion_r9853961
  
    good point


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Andrew Ash <andrew@andrewash.com>,"Tue, 18 Feb 2014 20:28:21 -0800",Re: Bug in spark.shuffle.spill setting? (0.9.0),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I confirmed also that the spill to disk _was_ occurring:

map of 634 MB to disk (1 time so far)
map of 581 MB to disk (1 time so far)



"
aarondav <git@git.apache.org>,"Wed, 19 Feb 2014 04:31:59 +0000 (UTC)","[GitHub] incubator-spark pull request: For SPARK-1082, Use Curator for ZK i...",dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/611#discussion_r9854034
  
    Looks like if you did not use a sameThreadExecutor, it could execute outside of the synchronization block, but I think you're right that we're technically safe here. Still, I think it would be clearer and no less performant to add an explicit synchronization to our functions, which also allows us to ignore the implementation details of Curator.
    
    This code path should be called so infrequently that we could probably throw in a Thread.sleep(1000) and no one would notice...


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
mridulm <git@git.apache.org>,"Wed, 19 Feb 2014 04:34:38 +0000 (UTC)","[GitHub] incubator-spark pull request: Add Security to Spark - Akka, Http, ...",dev@spark.incubator.apache.org,"Github user mridulm commented on the pull request:

    https://github.com/apache/incubator-spark/pull/332#discussion_r9854054
  
    is it possible for handleClientAuthNeg to be invoked right after sendSecurityMessage and before adding to connectionsAwaitingSasl ? (pre-emption, repsonse from other side before we get back here).
    If yes, we should add to connectionsAwaitingSasl before sendSecurityMessage.
    a conn.close will clean up connectionsAwaitingSasl anyway iirc, so it should not introduce any other side effect.
    
    any thoughts ?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
mridulm <git@git.apache.org>,"Wed, 19 Feb 2014 04:39:23 +0000 (UTC)","[GitHub] incubator-spark pull request: Add Security to Spark - Akka, Http, ...",dev@spark.incubator.apache.org,"Github user mridulm commented on the pull request:

    https://github.com/apache/incubator-spark/pull/332#discussion_r9854094
  
    While this makes sense (absence of connection in connectionsAwaitingSasl means server conn), how about matching it on type of Connection and throwing an error if client connection is not present in connectionsAwaitingSasl ?
    If I understood this right, it should not happen right now - but given how critical this codepath is for auth (incorrect in case we get security messages after we thought auth was done : which means something is wrong) would be better to guard it against future change which break this ?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Wed, 19 Feb 2014 04:39:59 +0000 (UTC)","[GitHub] incubator-spark pull request: For SPARK-1082, Use Curator for ZK i...",dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/611#discussion_r9854099
  
    It's as you said, this would be a significant change. Still, I think the correct way to implement this now would probably be to have the Master subscribe to an event stream that these LeaderElectionAgents push events to. Would you mind adding a TODO to LeaderElectionAgent to refactor it off of an Actor? No need to change this now.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Andrew Ash <andrew@andrewash.com>,"Tue, 18 Feb 2014 20:41:25 -0800",Re: [0.9.0] Possible deadlock in shutdown hook?,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","To keep this thread from getting lost, I've opened a ticket here:
https://spark-project.atlassian.net/browse/SPARK-1107



"
mridulm <git@git.apache.org>,"Wed, 19 Feb 2014 04:49:16 +0000 (UTC)","[GitHub] incubator-spark pull request: Add Security to Spark - Akka, Http, ...",dev@spark.incubator.apache.org,"Github user mridulm commented on the pull request:

    https://github.com/apache/incubator-spark/pull/332#issuecomment-35465952
  
    Not sure how much it helps, but currently we are throwing ""Exception"" in most places in the PR.
    Would help to specialize it for authentication failure ?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
ScrapCodes <git@git.apache.org>,"Wed, 19 Feb 2014 04:53:05 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user ScrapCodes commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35466111
  
    Yeah, you mean they were package private in previous release too ? In that case its sad. I guess bytecode does not preserve this information in javaish way. 


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Mridul Muralidharan <mridul@gmail.com>,"Wed, 19 Feb 2014 10:26:53 +0530",Re: Bug in spark.shuffle.spill setting? (0.9.0),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I had not resolved it in time for 0.9 - but IIRC there was a recent PR
which fixed bugs in spill [1] : are you able to reproduce this with
spark master ?

Regards,
Mridul

[1] https://github.com/apache/incubator-spark/pull/533


"
ScrapCodes <git@git.apache.org>,"Wed, 19 Feb 2014 05:00:50 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user ScrapCodes commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35466426
  
    Also we skip this for generating docs.
    ```scala
        scalacOptions in (Compile,doc) := Seq(""-skip-packages"", Seq(
          ""akka"",
          ""org.apache.spark.network"",
          ""org.apache.spark.deploy"",
          ""org.apache.spark.util.collection""
          ).mkString("":"")),
    ```
    
    It might be good to skip these for MiMa as well ? 



---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Andrew Or <andrewor14@gmail.com>,"Tue, 18 Feb 2014 21:02:03 -0800",Re: Bug in spark.shuffle.spill setting? (0.9.0),dev@spark.incubator.apache.org,"Looks like you have a large number of distinct keys. As you suspect, this
maybe due to hash collisions, which only up to 4 billion. It could be
related to this PR: https://github.com/apache/incubator-spark/pull/612.

The other thing is we had some issues with the behavior of arbitrary
serialization/compression engines, and this is solved in the PR that Mridul
referenced. What compression and serialization libraries are you using?


2014-02-18 20:56 GMT-08:00 Mridul Muralidharan <mridul@gmail.com>:

"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 05:03:43 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-929: Fully deprecate usage of ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/615#issuecomment-35466553
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12773/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 05:03:43 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-929: Fully deprecate usage of ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/615#issuecomment-35466551
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Andrew Ash <andrew@andrewash.com>,"Tue, 18 Feb 2014 21:08:45 -0800",Re: Bug in spark.shuffle.spill setting? (0.9.0),"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I'm using Kryo with these options:

-Dspark.shuffle.spill=false -Dspark.storage.memoryFraction=0.4
-Dspark.serializer=org.apache.spark.serializer.KryoSerializer
-Dspark.kryo.registrator=com.andrewash.CustomKryoRegistrator

The data is being read from a .lzo file and written back to another .lzo
file if that affects things.  Does that cover the compression and
serialization libraries question?

I can give master a shot with my repro but it may be some time now that I
have a workaround.  I'm trying to turn something around quickly and have my
own bugs to debug as well :)

Thanks!
Andrew



"
Andrew Or <andrewor14@gmail.com>,"Tue, 18 Feb 2014 21:29:32 -0800",Re: Bug in spark.shuffle.spill setting? (0.9.0),dev@spark.incubator.apache.org,"For compressing shuffle spills in 0.9, we added a hack such that it always
uses LZF, so actually your compression library shouldn't matter. We did
notice that Kryo was pre-fetching, however, such that batching reads led to
some items being lost. To fix this, we introduced a hack specifically for
Kryo that works around this. Although we tested it and the hack sufficed
back then, it is entirely possible that there are corner cases that we
missed. In any case, PR #533 (after 0.9 release) should have taken care of
the problem.

If you still run into the same problem on master, then it could be a corner
case that our current way of handling hash collisions missed. When you have
the time, do let us know what you find!

Andrew

2014-02-18 21:08 GMT-08:00 Andrew Ash <andrew@andrewash.com>:

"
aarondav <git@git.apache.org>,"Wed, 19 Feb 2014 06:01:42 +0000 (UTC)","[GitHub] incubator-spark pull request: For SPARK-1082, Use Curator for ZK i...",dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/611#discussion_r9855077
  
    Make sure to close latch and zk before restarting, since we construct new ones.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Wed, 19 Feb 2014 06:06:35 +0000 (UTC)","[GitHub] incubator-spark pull request: For SPARK-1082, Use Curator for ZK i...",dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/611#issuecomment-35469138
  
    Everything seems in order, just a few comments outstanding. I've run the FaultToleranceTest and all tests passed. I had to update it, though (for instance, the dockerfiles did not get updated with the bin/ change). I will submit a patch with the updated test stuff once this is merged.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 06:07:14 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark 1095 : Adding explicit return ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/610#issuecomment-35469184
  
     Build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 06:07:14 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark 1095 : Adding explicit return ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/610#issuecomment-35469185
  
    Build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
NirmalReddy <git@git.apache.org>,"Wed, 19 Feb 2014 06:08:31 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark 1095 : Adding explicit return ...,dev@spark.incubator.apache.org,"Github user NirmalReddy commented on the pull request:

    https://github.com/apache/incubator-spark/pull/610#issuecomment-35469237
  
    @aarondav I reflected the syntax you suggested in the code i changed but i can still find many places where the return types are on the new line, how will we go about that ?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
rxin <git@git.apache.org>,"Wed, 19 Feb 2014 06:14:00 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark 1095 : Adding explicit return ...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/610#issuecomment-35469499
  
    This discussion should probably have happened on the mailing list, but here you go.
    
    In general I think we should favor explicit type declaration in public APIs. However, I do think there are 3 cases we can avoid the public API definition because in these 3 cases the tpes are self-evident & repetitive.
    
    Case 1. toString
    
    Case 2. A method returning a string or a val defining a string
    ```scala
    def name = ""abcd"" // this is so obvious that it is a string
    
    val name = ""edfg"" // this too
    ```
    
    Case 3. The method or variable is invoking the constructor of a class and return that immediately. For example:
    ```scala
    val a = new SparkContext(...)
    
    implicit def rddToAsyncRDDActions[T: ClassTag](rdd: RDD[T]) = new AsyncRDDActions(rdd)
    ```



---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
NirmalReddy <git@git.apache.org>,"Wed, 19 Feb 2014 06:17:24 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark 1095 : Adding explicit return ...,dev@spark.incubator.apache.org,"Github user NirmalReddy commented on the pull request:

    https://github.com/apache/incubator-spark/pull/610#issuecomment-35469655
  
    Thanks @rxin !! This will be helpful !!


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Reynold Xin <rxin@databricks.com>,"Tue, 18 Feb 2014 22:22:54 -0800",coding style discussion: explicit return type in public APIs,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi guys,

Want to bring to the table this issue to see what other members of the
community think and then we can codify it in the Spark coding style guide.
The topic is about declaring return types explicitly in public APIs.

In general I think we should favor explicit type declaration in public
APIs. However, I do think there are 3 cases we can avoid the public API
definition because in these 3 cases the types are self-evident & repetitive.

Case 1. toString

Case 2. A method returning a string or a val defining a string

def name = ""abcd"" // this is so obvious that it is a string
val name = ""edfg"" // this too

Case 3. The method or variable is invoking the constructor of a class and
return that immediately. For example:

val a = new SparkContext(...)
implicit def rddToAsyncRDDActions[T: ClassTag](rdd: RDD[T]) = new
AsyncRDDActions(rdd)


Thoughts?
"
rxin <git@git.apache.org>,"Wed, 19 Feb 2014 06:23:25 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark 1095 : Adding explicit return ...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/610#issuecomment-35469927
  
    Yes, I agree for primitive types as well. 


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
colorant <git@git.apache.org>,"Wed, 19 Feb 2014 06:23:31 +0000 (UTC)","[GitHub] incubator-spark pull request: For SPARK-1082, Use Curator for ZK i...",dev@spark.incubator.apache.org,"Github user colorant commented on the pull request:

    https://github.com/apache/incubator-spark/pull/611#discussion_r9855368
  
    prerestart will call into poststop later. so it should be ok.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Reynold Xin <rxin@databricks.com>,"Tue, 18 Feb 2014 22:23:42 -0800",Re: coding style discussion: explicit return type in public APIs,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Case 2  should probably be expanded to cover most primitive types.



"
aarondav <git@git.apache.org>,"Wed, 19 Feb 2014 06:28:20 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark 1095 : Adding explicit return ...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/610#issuecomment-35470142
  
    Don't worry about changing the return types on new lines for code that you're not directly altering otherwise. We mostly decided this change of style for all future code.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Wed, 19 Feb 2014 06:29:43 +0000 (UTC)","[GitHub] incubator-spark pull request: For SPARK-1082, Use Curator for ZK i...",dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/611#discussion_r9855451
  
    Whoops, was thinking that preStart was called on restart as well, for some reason.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
mengxr <git@git.apache.org>,"Wed, 19 Feb 2014 06:31:30 +0000 (UTC)","[GitHub] incubator-spark pull request: MLLIB-24: url of ""Collaborative Filt...",dev@spark.incubator.apache.org,"Github user mengxr commented on the pull request:

    https://github.com/apache/incubator-spark/pull/619#issuecomment-35470291
  
    Do you mind using the DOI link of the paper: http://dx.doi.org/10.1109/ICDM.2008.22 ?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Christopher Nguyen <ctn@adatao.com>,"Tue, 18 Feb 2014 22:35:17 -0800",Re: coding style discussion: explicit return type in public APIs,dev@spark.incubator.apache.org,"Reynold, perhaps better than enumerating all the rules, it should be
generalized to a guideline that allows this relaxation when the return type
is immediately obvious from the next few tokens in the code. In exceptional
cases of doubt, provide the return type.

The cases you've listed should serve as examples of the guideline.
--
Christopher T. Nguyen
Co-founder & CEO, Adatao <http://adatao.com>
linkedin.com/in/ctnguyen




"
Mridul Muralidharan <mridul@gmail.com>,"Wed, 19 Feb 2014 12:07:40 +0530",Re: coding style discussion: explicit return type in public APIs,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Case 3 can be a potential issue.
Current implementation might be returning a concrete class which we
might want to change later - making it a type change.
The intention might be to return an RDD (for example), but the
inferred type might be a subclass of RDD - and future changes will
cause signature change.


Regards,
Mridul



"
Reynold Xin <rxin@databricks.com>,"Tue, 18 Feb 2014 22:40:33 -0800",Re: coding style discussion: explicit return type in public APIs,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","+1 Christopher's suggestion.

Mridul,

How would that happen? Case 3 requires the method to be invoking the
constructor directly. It was implicit in my email, but the return type
should be the same as the class itself.





"
pwendell <git@git.apache.org>,"Wed, 19 Feb 2014 06:46:35 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35471000
  
    It turns out our issues are shortfallings of the existing MIMA tool. The visibility of classes at the bytecode level is different than the visibility understood by the scala and java compilers - and MIMA currently only deals with the former. I added some issues there:
    
    https://github.com/typesafehub/migration-manager/issues/53
    https://github.com/typesafehub/migration-manager/issues/54
    
    @ScrapCodes I think we only want to ignore the deploy package for now.. 


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
colorant <git@git.apache.org>,"Wed, 19 Feb 2014 07:01:38 +0000 (UTC)","[GitHub] incubator-spark pull request: For SPARK-1082, Use Curator for ZK i...",dev@spark.incubator.apache.org,"Github user colorant commented on the pull request:

    https://github.com/apache/incubator-spark/pull/611#issuecomment-35471622
  
    code updated. Btw, I add the sleep 1000 there, while I am wondering, there are zookeeper session time out, connection time etc. Seems to me then there actually won't be case that leader gain and lost  in a short time? or it actually will happen?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 07:03:52 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark 1095 : Adding explicit return ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/610#issuecomment-35471720
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12774/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 07:03:52 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark 1095 : Adding explicit return ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/610#issuecomment-35471717
  
    Build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 07:04:25 +0000 (UTC)","[GitHub] incubator-spark pull request: For SPARK-1082, Use Curator for ZK i...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/611#issuecomment-35471746
  
    Build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 07:04:26 +0000 (UTC)","[GitHub] incubator-spark pull request: For SPARK-1082, Use Curator for ZK i...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/611#issuecomment-35471745
  
     Build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
colorant <git@git.apache.org>,"Wed, 19 Feb 2014 07:05:01 +0000 (UTC)","[GitHub] incubator-spark pull request: For SPARK-1082, Use Curator for ZK i...",dev@spark.incubator.apache.org,"Github user colorant commented on the pull request:

    https://github.com/apache/incubator-spark/pull/611#discussion_r9855914
  
    yes, seems preStart will be called by postRestart, and so it seems to me the flow will be prerestart -> postStop , postRestart -> preStart, So I think it should be ok


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 07:14:02 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark 1095 : Adding explicit return ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/610#issuecomment-35472216
  
     Build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 07:14:01 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark 1095 : Adding explicit return ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/610#issuecomment-35472217
  
    Build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Christopher Nguyen <ctn@adatao.com>,"Tue, 18 Feb 2014 23:14:07 -0800",Re: coding style discussion: explicit return type in public APIs,dev@spark.incubator.apache.org,"Mridul, IIUUC, what you've mentioned did come to mind, but I deemed it
orthogonal to the stylistic issue Reynold is talking about.

I believe you're referring to the case where there is a specific desired
return type by API design, but the implementation does not, in which case,
of course, one must define the return type. That's an API requirement and
not just a matter of readability.

We could add this as an NB in the proposed guideline.

--
Christopher T. Nguyen
Co-founder & CEO, Adatao <http://adatao.com>
linkedin.com/in/ctnguyen




"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 08:03:57 +0000 (UTC)","[GitHub] incubator-spark pull request: For SPARK-1082, Use Curator for ZK i...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/611#issuecomment-35474696
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12775/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 08:03:57 +0000 (UTC)","[GitHub] incubator-spark pull request: For SPARK-1082, Use Curator for ZK i...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/611#issuecomment-35474694
  
    Build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 08:04:01 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark 1095 : Adding explicit return ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/610#issuecomment-35474701
  
    Build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 08:04:02 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark 1095 : Adding explicit return ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/610#issuecomment-35474702
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12776/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Mridul Muralidharan <mridul@gmail.com>,"Wed, 19 Feb 2014 13:47:26 +0530",Re: coding style discussion: explicit return type in public APIs,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","You are right.
A degenerate case would be :

def createFoo = new FooImpl()

vs

def createFoo: Foo = new FooImpl()

Former will cause api instability. Reynold, maybe this is already
avoided - and I understood it wrong ?

Thanks,
Mridul




"
Reynold Xin <rxin@databricks.com>,"Wed, 19 Feb 2014 00:19:18 -0800",Re: coding style discussion: explicit return type in public APIs,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Yes, the case you brought up is not a matter of readability or style. If it
returns a different type, it should be declared (otherwise it is just
wrong).



"
CrazyJvm <git@git.apache.org>,"Wed, 19 Feb 2014 08:39:53 +0000 (UTC)","[GitHub] incubator-spark pull request: MLLIB-24: url of ""Collaborative Filt...",dev@spark.incubator.apache.org,"Github user CrazyJvm commented on the pull request:

    https://github.com/apache/incubator-spark/pull/619#issuecomment-35476826
  
    There seems no problem to use yahoo link. Or you are worried about the link might be invalid again?
    @mengxr  


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
NirmalReddy <git@git.apache.org>,"Wed, 19 Feb 2014 09:27:30 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark 1095 : Adding explicit return ...,dev@spark.incubator.apache.org,"Github user NirmalReddy commented on the pull request:

    https://github.com/apache/incubator-spark/pull/610#issuecomment-35480180
  
    @aarondav With this last commit i suppose i have completed the issue.(Spark-1095)


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Gino Mathews <gino.m@thinkpalm.com>,"Wed, 19 Feb 2014 09:45:37 +0000",Spark 0.9.0,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi,

I am trying to use Apache spark on a Standalone cluster.
After downloading the Spark I tried to build the package. However I am getting following error for the normal build using default Hadoop:

gino@gino008:~/Downloads/spark-0.9.0-incubating$ sbt assembly
Loading /usr/share/sbt/bin/sbt-launch-lib.bash
[info] Loading project definition from /home/gino/Downloads/spark-0.9.0-incubating/project/project
[info] Updating {file:/home/gino/Downloads/spark-0.9.0-incubating/project/project/}default-5f2b58...
[info] Resolving org.scala-lang#scala-library;2.9.2 ...
[error] Server access Error: Connection reset url=http://repo.typesafe.com/typesafe/ivy-releases/org.scala-lang/scala-library/2.9.2/jars/scala-library.jar
[error] Server access Error: Connection reset url=http://scalasbt.artifactoryonline.com/scalasbt/sbt-plugin-releases/org.scala-lang/scala-library/2.9.2/jars/scala-library.jar
[error] Server access Error: Connection reset url=http://repo1.maven.org/maven2/org/scala-lang/scala-library/2.9.2/scala-library-2.9.2.jar
[info] Resolving org.scala-sbt#control;0.12.4 ...

------------truncated---------

I am getting following error for the normal build using  Hadoop 2.2.0:



gino@gino008:~/Downloads/spark-0.9.0-incubating$ SPARK_HADOOP_VERSION=2.2.0 sbt assembly
Loading /usr/share/sbt/bin/sbt-launch-lib.bash
[info] Loading project definition from /home/gino/Downloads/spark-0.9.0-incubating/project/project
[info] Updating {file:/home/gino/Downloads/spark-0.9.0-incubating/project/project/}default-5f2b58...
[info] Resolving org.scala-lang#scala-compiler;2.9.2 ...
[error] Server access Error: Connection reset url=http://repo.typesafe.com/typesafe/ivy-releases/org.scala-lang/scala-compiler/2.9.2/jars/scala-compiler.jar
[error] Server access Error: Connection reset url=http://scalasbt.artifactoryonline.com/scalasbt/sbt-plugin-releases/org.scala-lang/scala-compiler/2.9.2/jars/scala-compiler.jar
[error] Server access Error: Connection reset url=http://repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.9.2/scala-compiler-2.9.2.jar
[info] Resolving org.sonatype.oss#oss-parent;7 ...
[error] Server access Error: Connection reset url=http://repo.typesafe.com/typesafe/ivy-releases/org.sonatype.oss/oss-parent/7/jars/oss-parent.jar
[error] Server access Error: Connection reset url=http://scalasbt.artifactoryonline.com/scalasbt/sbt-plugin-releases/org.sonatype.oss/oss-parent/7/jars/oss-parent.jar
[error] Server access Error: Connection reset url=http://repo1.maven.org/maven2/org/sonatype/oss/oss-parent/7/oss-parent-7.jar
[error] Server access Error: Connection reset url=http://repo.typesafe.com/typesafe/ivy-releases/jline/jline/1.0/jars/jline.jar
[error] Server access Error: Connection reset url=http://scalasbt.artifactoryonline.com/scalasbt/sbt-plugin-releases/jline/jline/1.0/jars/jline.jar
[error] Server access Error: Connection reset url=http://repo1.maven.org/maven2/jline/jline/1.0/jline-1.0.jar
[info] Resolving org.scala-sbt#api;0.12.4 ...

--------------truncated---------


Please guide how to download the maven repositories.

Thanks in Advance

Gino Mathews K




"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 12:10:22 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35491849
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 12:10:22 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35491848
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
ScrapCodes <git@git.apache.org>,"Wed, 19 Feb 2014 12:13:40 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user ScrapCodes commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/539#discussion_r9862681
  
    --- Diff: project/build.properties ---
    @@ -14,4 +14,4 @@
     # See the License for the specific language governing permissions and
     # limitations under the License.
     #
    -sbt.version=0.13.1
    +sbt.version=0.13.2-M1
    --- End diff --
    
    This is an accidental commit, but I think I personally use it as it makes the continuous compilation really fast !!!. 


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
ScrapCodes <git@git.apache.org>,"Wed, 19 Feb 2014 12:33:09 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user ScrapCodes commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/585#discussion_r9863061
  
    --- Diff: project/MimaBuild.scala ---
    @@ -0,0 +1,115 @@
    +import com.typesafe.tools.mima.plugin.MimaKeys.{binaryIssueFilters, previousArtifact}
    +import com.typesafe.tools.mima.plugin.MimaPlugin.mimaDefaultSettings
    +
    +object MimaBuild {
    +
    +  val ignoredABIProblems = {
    +    import com.typesafe.tools.mima.core._
    +    import com.typesafe.tools.mima.core.ProblemFilters._
    +    /**
    +     * A: Detections likely to become semi private at some point.
    +     */
    +    Seq(exclude[MissingClassProblem](""org.apache.spark.util.XORShiftRandom""),
    +      exclude[MissingClassProblem](""org.apache.spark.util.XORShiftRandom$""),
    +      exclude[MissingMethodProblem](""org.apache.spark.util.Utils.cloneWritables""),
    --- End diff --
    
    exclude for a class does not work I suppose. 


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
ScrapCodes <git@git.apache.org>,"Wed, 19 Feb 2014 12:43:28 +0000 (UTC)",[GitHub] incubator-spark pull request: Deprecated and added a few java api ...,dev@spark.incubator.apache.org,"Github user ScrapCodes commented on the pull request:

    https://github.com/apache/incubator-spark/pull/402#issuecomment-35494734
  
    @pwendell are you okay with the changes ?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 13:04:26 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35496676
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 13:04:26 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35496679
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12777/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 13:05:19 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35496741
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 13:05:19 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35496740
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 13:05:26 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35496755
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 13:05:26 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35496756
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 13:06:15 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35496818
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 13:06:15 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35496819
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12778/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 13:08:15 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35496946
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 13:08:15 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35496947
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
CodingCat <git@git.apache.org>,"Wed, 19 Feb 2014 13:13:38 +0000 (UTC)","[GitHub] incubator-spark pull request: MLLIB-24: url of ""Collaborative Filt...",dev@spark.incubator.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/incubator-spark/pull/619#issuecomment-35497321
  
    @mengxr DOI link may not be accessible to non-paid users, I think yahoo research is relatively stable enough


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 14:04:31 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35501415
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12780/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 14:04:31 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35501419
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12779/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 14:04:31 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35501414
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 14:04:31 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35501417
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
tgravescs <git@git.apache.org>,"Wed, 19 Feb 2014 15:22:59 +0000 (UTC)","[GitHub] incubator-spark pull request: Add Security to Spark - Akka, Http, ...",dev@spark.incubator.apache.org,"Github user tgravescs commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/332#discussion_r9868173
  
    --- Diff: core/src/main/java/org/apache/spark/SparkSaslServer.java ---
    @@ -0,0 +1,189 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * ""License""); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.spark;
    +
    +import org.apache.commons.net.util.Base64;
    +
    +import org.slf4j.Logger;
    +import org.slf4j.LoggerFactory;
    +
    +import java.util.Map;
    +import java.util.TreeMap;
    +
    +import javax.security.auth.callback.Callback;
    +import javax.security.auth.callback.CallbackHandler;
    +import javax.security.auth.callback.NameCallback;
    +import javax.security.auth.callback.PasswordCallback;
    +import javax.security.auth.callback.UnsupportedCallbackException;
    +import javax.security.sasl.AuthorizeCallback;
    +import javax.security.sasl.RealmCallback;
    +import javax.security.sasl.Sasl;
    +import javax.security.sasl.SaslException;
    +import javax.security.sasl.SaslServer;
    +import java.io.IOException;
    +
    +/**
    + * Encapsulates SASL server logic for Server
    + */
    +public class SparkSaslServer {
    +  /** Logger */
    +  private static Logger LOG = LoggerFactory.getLogger(""SparkSaslServer.class"");
    +
    +  /**
    +   * Actual SASL work done by this object from javax.security.sasl.
    +   * Initialized below in constructor.
    +   */
    +  private SaslServer saslServer;
    +
    +  public static final String SASL_DEFAULT_REALM = ""default"";
    --- End diff --
    
    Yeah that code was specifically copied from Hadoop 0.23.  I'll leave it for now and we can make it configurable in the next round of changes to add more configurability. 


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
tgravescs <git@git.apache.org>,"Wed, 19 Feb 2014 15:59:05 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1059. Now that we submit core ...,dev@spark.incubator.apache.org,"Github user tgravescs commented on the pull request:

    https://github.com/apache/incubator-spark/pull/555#issuecomment-35513822
  
    @sryza can this be closed then?  I think the important note you added to the running on yarn about the cores will suffice alone with my security PR.  


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 16:05:24 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35514615
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 16:05:24 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35514616
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Wed, 19 Feb 2014 16:18:15 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1105] fix site scala version ...,dev@spark.incubator.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/618#discussion_r9870730
  
    --- Diff: docs/index.md ---
    @@ -19,7 +19,7 @@ Spark uses [Simple Build Tool](http://www.scala-sbt.org), which is bundled with
     
         sbt/sbt assembly
     
    -For its Scala API, Spark {{site.SPARK_VERSION}} depends on Scala {{site.SCALA_VERSION}}. If you write applications in Scala, you will need to use this same version of Scala in your own program -- newer major versions may not work. You can get the right version of Scala from [scala-lang.org](http://www.scala-lang.org/download/).
    +For its Scala API, Spark {{site.SPARK_VERSION}} depends on Scala {{site.SCALA_BINARY_VERSION}}. If you write applications in Scala, you will need to use this same version of Scala in your own program -- newer major versions may not work. You can get the right version of Scala from [scala-lang.org](http://www.scala-lang.org/download/).
    --- End diff --
    
    To make this more clear, it might be good to say:
    
        If you write applications in Scala, you will need to use a compatible Scala version (e.g. {{site.SCALA_BINARY_VERSION}}.X) -- newer major versions may not work.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Wed, 19 Feb 2014 16:18:56 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1105] fix site scala version ...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/618#issuecomment-35516252
  
    LGTM pending a small fix -- @aarondav want to take a look?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Patrick Wendell <pwendell@gmail.com>,"Wed, 19 Feb 2014 08:37:56 -0800",Re: coding style discussion: explicit return type in public APIs,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","+1 overall.

Christopher - I agree that once the number of rules becomes large it's
more efficient to pursue a ""use your judgement"" approach. However,
since this is only 3 cases I'd prefer to wait to see if it grows.

The concern with this approach is tha"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 17:04:44 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35521749
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Wed, 19 Feb 2014 17:04:44 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35521750
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12781/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
sryza <git@git.apache.org>,"Wed, 19 Feb 2014 17:16:37 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1059. Now that we submit core ...,dev@spark.incubator.apache.org,"Github user sryza closed the pull request at:

    https://github.com/apache/incubator-spark/pull/555


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Wed, 19 Feb 2014 18:04:30 +0000 (UTC)","[GitHub] incubator-spark pull request: For SPARK-1082, Use Curator for ZK i...",dev@spark.incubator.apache.org,"Github user aarondav commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/611#discussion_r9875482
  
    --- Diff: core/src/main/scala/org/apache/spark/deploy/master/ZooKeeperLeaderElectionAgent.scala ---
    @@ -18,105 +18,73 @@
     package org.apache.spark.deploy.master
     
     import akka.actor.ActorRef
    -import org.apache.zookeeper._
    -import org.apache.zookeeper.Watcher.Event.EventType
     
     import org.apache.spark.{SparkConf, Logging}
     import org.apache.spark.deploy.master.MasterMessages._
    +import org.apache.curator.framework.CuratorFramework
    +import org.apache.curator.framework.recipes.leader.{LeaderLatchListener, LeaderLatch}
     
     private[spark] class ZooKeeperLeaderElectionAgent(val masterActor: ActorRef,
         masterUrl: String, conf: SparkConf)
    -  extends LeaderElectionAgent with SparkZooKeeperWatcher with Logging  {
    +  extends LeaderElectionAgent with LeaderLatchListener with Logging  {
     
       val WORKING_DIR = conf.get(""spark.deploy.zookeeper.dir"", ""/spark"") + ""/leader_election""
     
    -  private val watcher = new ZooKeeperWatcher()
    -  private val zk = new SparkZooKeeperSession(this, conf)
    +  private var zk: CuratorFramework = _
    +  private var leaderLatch: LeaderLatch = _
       private var status = LeadershipStatus.NOT_LEADER
    -  private var myLeaderFile: String = _
    -  private var leaderUrl: String = _
     
       override def preStart() {
    +
         logInfo(""Starting ZooKeeper LeaderElection agent"")
    -    zk.connect()
    -  }
    +    zk = SparkCuratorUtil.newClient(conf)
    +    leaderLatch = new LeaderLatch(zk, WORKING_DIR)
    +    leaderLatch.addListener(this)
     
    -  override def zkSessionCreated() {
    -    synchronized {
    -      zk.mkdirRecursive(WORKING_DIR)
    -      myLeaderFile =
    -        zk.create(WORKING_DIR + ""/master_"", masterUrl.getBytes, CreateMode.EPHEMERAL_SEQUENTIAL)
    -      self ! CheckLeader
    -    }
    +    leaderLatch.start()
       }
     
       override def preRestart(reason: scala.Throwable, message: scala.Option[scala.Any]) {
    -    logError(""LeaderElectionAgent failed, waiting "" + zk.ZK_TIMEOUT_MILLIS + ""..."", reason)
    -    Thread.sleep(zk.ZK_TIMEOUT_MILLIS)
    +    logError(""LeaderElectionAgent failed..."", reason)
         super.preRestart(reason, message)
       }
     
    -  override def zkDown() {
    -    logError(""ZooKeeper down! LeaderElectionAgent shutting down Master."")
    -    System.exit(1)
    -  }
    -
       override def postStop() {
    +    leaderLatch.close()
         zk.close()
       }
     
       override def receive = {
    -    case CheckLeader => checkLeader()
    +    case _ =>
       }
     
    -  private class ZooKeeperWatcher extends Watcher {
    -    def process(event: WatchedEvent) {
    -      if (event.getType == EventType.NodeDeleted) {
    -        logInfo(""Leader file disappeared, a master is down!"")
    -        self ! CheckLeader
    +  override def isLeader() {
    +    // In case that leadship gain and lost in a short time.
    +    Thread.sleep(1000)
    --- End diff --
    
    Ah, sorry if I was unclear, but I was just joking about putting a sleep(1000) in here. The real solution is to add a synchronized block to isLeader and notLeader -- I was just making a point that we're not concerned with the overhead of synchronization in this code path. (The synchronized block is not needed with the current implementation and use of Curator, but I think it makes the code clearer without a real downside.)


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Wed, 19 Feb 2014 18:11:03 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1105] fix site scala version ...,dev@spark.incubator.apache.org,"Github user aarondav commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/618#discussion_r9875739
  
    --- Diff: docs/scala-programming-guide.md ---
    @@ -17,12 +17,12 @@ This guide shows each of these features and walks through some samples. It assum
     
     # Linking with Spark
     
    -Spark {{site.SPARK_VERSION}} uses Scala {{site.SCALA_VERSION}}. If you write applications in Scala, you'll need to use this same version of Scala in your program -- newer major versions may not work.
    +Spark {{site.SPARK_VERSION}} uses Scala {{site.SCALA_BINARY_VERSION}}. If you write applications in Scala, you'll need to use this same version of Scala in your program -- newer major versions may not work.
    --- End diff --
    
    I suppose we should repeat Patrick's comment here.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
tgravescs <git@git.apache.org>,"Wed, 19 Feb 2014 18:13:46 +0000 (UTC)","[GitHub] incubator-spark pull request: Add Security to Spark - Akka, Http, ...",dev@spark.incubator.apache.org,"Github user tgravescs commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/332#discussion_r9875857
  
    --- Diff: core/src/main/scala/org/apache/spark/SecurityManager.scala ---
    @@ -0,0 +1,112 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * ""License""); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.spark
    +
    +import org.apache.hadoop.io.Text
    +import org.apache.hadoop.security.Credentials
    +import org.apache.hadoop.security.UserGroupInformation
    +
    +import org.apache.spark.deploy.SparkHadoopUtil
    +
    +/** 
    + * Spark class responsible for security.  
    + */
    +private[spark] class SecurityManager extends Logging {
    +
    +  private val viewAcls = System.getProperty(""spark.ui.view.acls"", """").split(',').map(_.trim()).toSet
    +  private val secretKey = generateSecretKey()
    + 
    +  /**
    +   * In Yarn mode it uses Hadoop UGI to pass the secret as that
    +   * will keep it protected.  For a standalone SPARK cluster
    +   * use a environment variable SPARK_SECRET to specify the secret.
    +   * This probably isn't ideal but only the user who starts the process
    +   * should have access to view the variable (at least on Linux).
    +   * Since we can't set the environment variable we set the 
    +   * java system property SPARK_SECRET so it will automatically
    +   * generate a secret is not specified.  This definitely is not
    +   * ideal since users can see it. We should switch to put it in 
    +   * a config.
    +   */
    +  private def generateSecretKey(): String = {
    +
    +    if (!isAuthenticationEnabled) return null
    +    // first check to see if secret already set, else generate it
    +    if (SparkHadoopUtil.get.isYarnMode) {
    +      val credentials = SparkHadoopUtil.get.getCurrentUserCredentials()
    +      if (credentials != null) { 
    +        val secretKey = credentials.getSecretKey(new Text(""akkaCookie""))
    +        if (secretKey != null) {
    +          logDebug(""in yarn mode, getting secret from credentials"")
    +          return new Text(secretKey).toString
    +        } else {
    +          logDebug(""getSecretKey: yarn mode, secret key from credentials is null"")
    +        }
    +      } else {
    +        logDebug(""getSecretKey: yarn mode, credentials are null"")
    +      }
    +    }
    +    val secret = System.getProperty(""SPARK_SECRET"", System.getenv(""SPARK_SECRET"")) 
    +    if (secret != null && !secret.isEmpty()) return secret 
    +    // generate one 
    +    val sCookie = akka.util.Crypt.generateSecureCookie
    +
    +    // if we generate we must be the first so lets set it so its used by everyone else
    +    if (SparkHadoopUtil.get.isYarnMode) {
    +      val creds = new Credentials()
    +      creds.addSecretKey(new Text(""akkaCookie""), sCookie.getBytes())
    +      SparkHadoopUtil.get.addCurrentUserCredentials(creds)
    +      logDebug(""adding secret to credentials yarn mode"")
    +    } else {
    +      System.setProperty(""SPARK_SECRET"", sCookie)
    +      logDebug(""adding secret to java property"")
    +    }
    +    return sCookie
    +  }
    +
    +
    +  // allow anyone in the acl list and the application owner 
    +  def checkUIViewPermissions(user: String): Boolean = {
    +    if (isUIAuthenticationEnabled() && (user != null)) {
    +      if ((!viewAcls.contains(user)) && (user != System.getProperty(""user.name""))) {
    --- End diff --
    
    Good idea to just prepopulate it.  I assume its safer just to add both user.name and SPARK_USER to acl list if they are set?  


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Wed, 19 Feb 2014 18:13:49 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1105] fix site scala version ...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/618#issuecomment-35529528
  
    LGTM too, apologies for not catching this issue earlier.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
tgravescs <git@git.apache.org>,"Wed, 19 Feb 2014 18:14:09 +0000 (UTC)","[GitHub] incubator-spark pull request: Add Security to Spark - Akka, Http, ...",dev@spark.incubator.apache.org,"Github user tgravescs commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/332#discussion_r9875875
  
    --- Diff: core/src/main/scala/org/apache/spark/SecurityManager.scala ---
    @@ -0,0 +1,112 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * ""License""); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.spark
    +
    +import org.apache.hadoop.io.Text
    +import org.apache.hadoop.security.Credentials
    +import org.apache.hadoop.security.UserGroupInformation
    +
    +import org.apache.spark.deploy.SparkHadoopUtil
    +
    +/** 
    + * Spark class responsible for security.  
    + */
    +private[spark] class SecurityManager extends Logging {
    +
    +  private val viewAcls = System.getProperty(""spark.ui.view.acls"", """").split(',').map(_.trim()).toSet
    +  private val secretKey = generateSecretKey()
    + 
    +  /**
    +   * In Yarn mode it uses Hadoop UGI to pass the secret as that
    +   * will keep it protected.  For a standalone SPARK cluster
    +   * use a environment variable SPARK_SECRET to specify the secret.
    +   * This probably isn't ideal but only the user who starts the process
    +   * should have access to view the variable (at least on Linux).
    +   * Since we can't set the environment variable we set the 
    +   * java system property SPARK_SECRET so it will automatically
    +   * generate a secret is not specified.  This definitely is not
    +   * ideal since users can see it. We should switch to put it in 
    +   * a config.
    +   */
    +  private def generateSecretKey(): String = {
    +
    +    if (!isAuthenticationEnabled) return null
    +    // first check to see if secret already set, else generate it
    +    if (SparkHadoopUtil.get.isYarnMode) {
    +      val credentials = SparkHadoopUtil.get.getCurrentUserCredentials()
    +      if (credentials != null) { 
    +        val secretKey = credentials.getSecretKey(new Text(""akkaCookie""))
    +        if (secretKey != null) {
    +          logDebug(""in yarn mode, getting secret from credentials"")
    +          return new Text(secretKey).toString
    +        } else {
    +          logDebug(""getSecretKey: yarn mode, secret key from credentials is null"")
    +        }
    +      } else {
    +        logDebug(""getSecretKey: yarn mode, credentials are null"")
    +      }
    +    }
    +    val secret = System.getProperty(""SPARK_SECRET"", System.getenv(""SPARK_SECRET"")) 
    +    if (secret != null && !secret.isEmpty()) return secret 
    +    // generate one 
    +    val sCookie = akka.util.Crypt.generateSecureCookie
    +
    +    // if we generate we must be the first so lets set it so its used by everyone else
    +    if (SparkHadoopUtil.get.isYarnMode) {
    +      val creds = new Credentials()
    +      creds.addSecretKey(new Text(""akkaCookie""), sCookie.getBytes())
    --- End diff --
    
    yep, I'll update.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
tgravescs <git@git.apache.org>,"Wed, 19 Feb 2014 18:14:38 +0000 (UTC)","[GitHub] incubator-spark pull request: Add Security to Spark - Akka, Http, ...",dev@spark.incubator.apache.org,"Github user tgravescs commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/332#discussion_r9875896
  
    --- Diff: core/src/main/scala/org/apache/spark/network/Connection.scala ---
    @@ -431,6 +466,7 @@ private[spark] class ReceivingConnection(channel_ : SocketChannel, selector_ : S
             val newMessage = Message.create(header).asInstanceOf[BufferMessage]
             newMessage.started = true
             newMessage.startTime = System.currentTimeMillis
    +        newMessage.isSecurityNeg = if (header.securityNeg == 1) true else false
    --- End diff --
    
    ah, ok.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
CodingCat <git@git.apache.org>,"Wed, 19 Feb 2014 18:23:53 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1105] fix site scala version ...,dev@spark.incubator.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/incubator-spark/pull/618#issuecomment-35530603
  
    done


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
CodingCat <git@git.apache.org>,"Wed, 19 Feb 2014 18:24:19 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1105] fix site scala version ...,dev@spark.incubator.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/incubator-spark/pull/618#issuecomment-35530650
  
    thank you very much for your comments @pwendell @aarondav 


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Aaron Davidson <ilikerps@gmail.com>,"Wed, 19 Feb 2014 10:32:03 -0800",Re: coding style discussion: explicit return type in public APIs,dev@spark.incubator.apache.org,"Longs can have semantic differences when it comes to overflow, so it's
often good to know what type of variable you're returning. Perhaps it is
sufficient to say that Int is the ""default"" numeric type, and that other
types should be specified explicitly.



"
Christopher Nguyen <ctn@adatao.com>,"Wed, 19 Feb 2014 10:34:26 -0800",Re: coding style discussion: explicit return type in public APIs,dev@spark.incubator.apache.org,"Patrick, I sympathize with your sensibility here, and at face value,
there's very little daylight between (a) a rule comprising small set of
enumerated items and (b) a guideline followed by the same set as examples.

My suggestion had a non-obvious tl;dr thesis behind it, so allow me to show
my cards :)

First, rules can be costly for the rule makers to create and maintain to
ensure necessity and sufficiency, and can unintentionally encourage
mischievous, often tedious arguments to work around those rules. In the
area of coding style, even at Google (at least when I was there), we had
guides rather than rules. It turns out that guidelines are also easier to
socialize and enforce than enumerated rules. <strawman_humor>A Google
search for ""coding style guide"" returns 15.7 million results, while that
for ""coding style rule"" has 6.6M, and most of *those* are articles about
""coding style guide"".</strawman_humor>

More importantly, I've found Spark's to be one of the best
socially-engineered communities I've participated in. It is quite helpful
and welcoming to newcomers while (not paradoxically) comprising one of the
highest median quality of participants, per my calibration of, e.g., the
various meetups I've gone to in the SF Bay Area. This community
friendliness and mutual regard are not accidental and have contributed in
part to Spark's success to date. It seems quite tolerant of ""newbies"" and
implicitly recognizes that there may be a lot of valuable expertise and
interesting use cases we can learn from the person behind that
idiotic-sounding question, who might go on to contribute valuable PRs. I've
yet to see the acronym RTFM used in anger here. Now, ""rules"" don't
automatically negate that, but they can be discouraging to navigate (""Have
I broken some rule?"") and misused as devices to shoot others (""You've just
broken our rule #178.S4.P2). I'd rather see those things kept to a minimum,
in locked cabinets.

For the above reasons, I would suggest, for Spark, guidelines over rules
whenever feasible & tolerable, certainly in the area of coding style.

Cheers,
--
Christopher T. Nguyen
Co-founder & CEO, Adatao <http://adatao.com>
linkedin.com/in/ctnguyen




"
Mridul Muralidharan <mridul@gmail.com>,"Thu, 20 Feb 2014 00:09:15 +0530",Re: coding style discussion: explicit return type in public APIs,dev@spark.incubator.apache.org,"Without bikeshedding this too much ... It is likely incorrect (not wrong) -
and rules like this potentially cause things to slip through.

Explicit return type strictly specifies what is being exposed (think in
face of impl change - createFoo changes in future from Foo to Foo1 or Foo2)
.. being conservative about how to specify exposed interfaces, imo,
outweighs potential gains in breveity of code.
Btw this is a degenerate contrieved example already stretching its use ...

Regards
Mridul

Regards
Mridul

"
Reynold Xin <rxin@databricks.com>,"Wed, 19 Feb 2014 10:42:13 -0800",Re: coding style discussion: explicit return type in public APIs,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Mridul,

Can you be more specific in the createFoo example?

def myFunc = createFoo

is disallowed in my guideline. It is invoking a function createFoo, not the
constructor of Foo.





"
Andrew Ash <andrew@andrewash.com>,"Wed, 19 Feb 2014 10:44:54 -0800",Re: coding style discussion: explicit return type in public APIs,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I found Haskell's convention of including type signatures as documentation
to be worthwhile.

http://www.haskell.org/haskellwiki/Type_signatures_as_good_style

I'd support a guideline to include type signatures where they're unclear
but would prefer to leave it quite vague.  In my experience, the lightest
process is the best process for contributions.  Strict rules here _will_
drive away contributors.



"
Mridul Muralidharan <mridul@gmail.com>,"Thu, 20 Feb 2014 00:28:41 +0530",Re: coding style discussion: explicit return type in public APIs,dev@spark.incubator.apache.org,"My initial mail had it listed, adding more details here since I assume I am
missing something or not being clear - please note, this is just
illustrative and my scala knowledge is bad :-) (I am trying to draw
parallels from mistakes in java world)

def createFoo = new Foo()

To

def createFoo = new Foo1()

To

def createFoo = new Foo2()

(appropriate inheritance applied - parent Foo).

I am thinking from api evolution and binary compatibility point of view

Regards,
Mridul

"
Mridul Muralidharan <mridul@gmail.com>,"Thu, 20 Feb 2014 00:32:46 +0530",Re: coding style discussion: explicit return type in public APIs,dev@spark.incubator.apache.org,"I agree, makes sense.
Please note I was referring only to exposed user api in my comments - not
other code !

Regards,
Mridul

"
mridulm <git@git.apache.org>,"Wed, 19 Feb 2014 19:09:09 +0000 (UTC)","[GitHub] incubator-spark pull request: Add Security to Spark - Akka, Http, ...",dev@spark.incubator.apache.org,"Github user mridulm commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/332#discussion_r9878293
  
    --- Diff: core/src/main/scala/org/apache/spark/SecurityManager.scala ---
    @@ -0,0 +1,112 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * ""License""); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.spark
    +
    +import org.apache.hadoop.io.Text
    +import org.apache.hadoop.security.Credentials
    +import org.apache.hadoop.security.UserGroupInformation
    +
    +import org.apache.spark.deploy.SparkHadoopUtil
    +
    +/** 
    + * Spark class responsible for security.  
    + */
    +private[spark] class SecurityManager extends Logging {
    +
    +  private val viewAcls = System.getProperty(""spark.ui.view.acls"", """").split(',').map(_.trim()).toSet
    +  private val secretKey = generateSecretKey()
    + 
    +  /**
    +   * In Yarn mode it uses Hadoop UGI to pass the secret as that
    +   * will keep it protected.  For a standalone SPARK cluster
    +   * use a environment variable SPARK_SECRET to specify the secret.
    +   * This probably isn't ideal but only the user who starts the process
    +   * should have access to view the variable (at least on Linux).
    +   * Since we can't set the environment variable we set the 
    +   * java system property SPARK_SECRET so it will automatically
    +   * generate a secret is not specified.  This definitely is not
    +   * ideal since users can see it. We should switch to put it in 
    +   * a config.
    +   */
    +  private def generateSecretKey(): String = {
    +
    +    if (!isAuthenticationEnabled) return null
    +    // first check to see if secret already set, else generate it
    +    if (SparkHadoopUtil.get.isYarnMode) {
    +      val credentials = SparkHadoopUtil.get.getCurrentUserCredentials()
    +      if (credentials != null) { 
    +        val secretKey = credentials.getSecretKey(new Text(""akkaCookie""))
    +        if (secretKey != null) {
    +          logDebug(""in yarn mode, getting secret from credentials"")
    +          return new Text(secretKey).toString
    +        } else {
    +          logDebug(""getSecretKey: yarn mode, secret key from credentials is null"")
    +        }
    +      } else {
    +        logDebug(""getSecretKey: yarn mode, credentials are null"")
    +      }
    +    }
    +    val secret = System.getProperty(""SPARK_SECRET"", System.getenv(""SPARK_SECRET"")) 
    +    if (secret != null && !secret.isEmpty()) return secret 
    +    // generate one 
    +    val sCookie = akka.util.Crypt.generateSecureCookie
    +
    +    // if we generate we must be the first so lets set it so its used by everyone else
    +    if (SparkHadoopUtil.get.isYarnMode) {
    +      val creds = new Credentials()
    +      creds.addSecretKey(new Text(""akkaCookie""), sCookie.getBytes())
    +      SparkHadoopUtil.get.addCurrentUserCredentials(creds)
    +      logDebug(""adding secret to credentials yarn mode"")
    +    } else {
    +      System.setProperty(""SPARK_SECRET"", sCookie)
    +      logDebug(""adding secret to java property"")
    +    }
    +    return sCookie
    +  }
    +
    +
    +  // allow anyone in the acl list and the application owner 
    +  def checkUIViewPermissions(user: String): Boolean = {
    +    if (isUIAuthenticationEnabled() && (user != null)) {
    +      if ((!viewAcls.contains(user)) && (user != System.getProperty(""user.name""))) {
    --- End diff --
    
    Agree, that sounds fine.
    
    Regards,
    Mridul
    
    > In core/src/main/scala/org/apache/spark/SecurityManager.scala:
    >
    > > +      creds.addSecretKey(new Text(""akkaCookie""), sCookie.getBytes())
    > > +      SparkHadoopUtil.get.addCurrentUserCredentials(creds)
    > > +      logDebug(""adding secret to credentials yarn mode"")
    > > +    } else {
    > > +      System.setProperty(""SPARK_SECRET"", sCookie)
    > > +      logDebug(""adding secret to java property"")
    > > +    }
    > > +    return sCookie
    > > +  }
    > > +
    > > +
    > > +  // allow anyone in the acl list and the application owner
    > > +  def checkUIViewPermissions(user: String): Boolean = {
    > > +    if (isUIAuthenticationEnabled() && (user != null)) {
    > > +      if ((!viewAcls.contains(user)) && (user != System.getProperty(""user.name""))) {
    >
    > Good idea to just prepopulate it. I assume its safer just to add both
    > user.name and SPARK_USER to acl list if they are set?
    >
    > --
    > Reply to this email directly or view it on GitHub<https://github.com/apache/incubator-spark/pull/332/files#r9875857>
    > .
    >


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
mengxr <git@git.apache.org>,"Wed, 19 Feb 2014 19:10:09 +0000 (UTC)","[GitHub] incubator-spark pull request: MLLIB-24: url of ""Collaborative Filt...",dev@spark.incubator.apache.org,"Github user mengxr commented on the pull request:

    https://github.com/apache/incubator-spark/pull/619#issuecomment-35535759
  
    DOI links are ""permanent"" so we don't need to worry about the link becoming invalid again. People will do a search and find the pdf easily if they don't have access to IEEE.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
fommil <git@git.apache.org>,"Wed, 19 Feb 2014 20:52:32 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user fommil commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35546981
  
    @mengxr consider this message to be proof that jniloader is distributed under the Apache license. I'll update the build files next time I need a code change. If you want it quicker, issue a PR (and add it as a dual license) ;-)


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
fommil <git@git.apache.org>,"Wed, 19 Feb 2014 20:55:14 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user fommil commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35547276
  
    @srowen ""The LGPL is ineligible primarily due to the restrictions it places on larger works, violating the third license criterion. Therefore, LGPL-licensed works must not be included in Apache products."" where third license criterion is ""The license must not place restrictions on the distribution of larger works, other than to require that the covered component still complies with the conditions of its license."" I do not see any violation here.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
fommil <git@git.apache.org>,"Wed, 19 Feb 2014 21:03:08 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user fommil commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35548061
  
    @srowen I've asked the question. I'm interested to see the response: https://issues.apache.org/jira/browse/LEGAL-192


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
mengxr <git@git.apache.org>,"Wed, 19 Feb 2014 22:26:59 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user mengxr commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35557645
  
    @fommil Thanks a lot! The license JIRA is also interesting to follow ~ :)


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Wed, 19 Feb 2014 23:57:18 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1105] fix site scala version ...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/618#issuecomment-35567163
  
    Thanks guys I put this in master and 0.9.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Thu, 20 Feb 2014 00:02:38 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/585#discussion_r9889925
  
    --- Diff: project/MimaBuild.scala ---
    @@ -0,0 +1,105 @@
    +import com.typesafe.tools.mima.plugin.MimaKeys.{binaryIssueFilters, previousArtifact}
    +import com.typesafe.tools.mima.plugin.MimaPlugin.mimaDefaultSettings
    +
    +object MimaBuild {
    +
    +  val ignoredABIProblems = {
    +    import com.typesafe.tools.mima.core._
    +    import com.typesafe.tools.mima.core.ProblemFilters._
    +    /**
    +     * A: Detections are semi private or likely to become semi private at some point.
    +     */
    +    Seq(exclude[MissingClassProblem](""org.apache.spark.util.XORShiftRandom""),
    +      exclude[MissingClassProblem](""org.apache.spark.util.XORShiftRandom$""),
    +      exclude[MissingMethodProblem](""org.apache.spark.util.Utils.cloneWritables""),
    +      // Scheduler is not considered a public API.
    +      excludePackage(""org.apache.spark.deploy""),
    +      // Was made private in 1.0
    --- End diff --
    
    Hey @ScrapCodes - now that we have a better understand of what is going on, let's do the following. For things where the class is package private let's exclude the entire class and then explain why. So here it could be:
    
        // The below classes are package private but not ignored correctly by MIMA
    
      


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Thu, 20 Feb 2014 00:06:24 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/585#discussion_r9890019
  
    --- Diff: project/MimaBuild.scala ---
    @@ -0,0 +1,115 @@
    +import com.typesafe.tools.mima.plugin.MimaKeys.{binaryIssueFilters, previousArtifact}
    +import com.typesafe.tools.mima.plugin.MimaPlugin.mimaDefaultSettings
    +
    +object MimaBuild {
    +
    +  val ignoredABIProblems = {
    +    import com.typesafe.tools.mima.core._
    +    import com.typesafe.tools.mima.core.ProblemFilters._
    +    /**
    +     * A: Detections likely to become semi private at some point.
    +     */
    +    Seq(exclude[MissingClassProblem](""org.apache.spark.util.XORShiftRandom""),
    +      exclude[MissingClassProblem](""org.apache.spark.util.XORShiftRandom$""),
    +      exclude[MissingMethodProblem](""org.apache.spark.util.Utils.cloneWritables""),
    +      exclude[MissingMethodProblem](""org.apache.spark.deploy.DeployMessages#LaunchExecutor.apply""),
    +      exclude[MissingMethodProblem](""org.apache.spark.deploy.DeployMessages#LaunchExecutor.apply""),
    +      exclude[IncompatibleResultTypeProblem](""org.apache.spark.deploy.ApplicationDescription.sparkHome""),
    +      exclude[IncompatibleMethTypeProblem](""org.apache.spark.deploy.ApplicationDescription.this""),
    +      exclude[MissingMethodProblem](""org.apache.spark.deploy.DeployMessages#LaunchExecutor.sparkHome""),
    +      exclude[MissingMethodProblem](""org.apache.spark.deploy.DeployMessages#LaunchExecutor.copy""),
    +      exclude[MissingMethodProblem](""org.apache.spark.deploy.DeployMessages#LaunchExecutor.copy$default$7""),
    +      exclude[MissingMethodProblem](""org.apache.spark.deploy.DeployMessages#LaunchExecutor.this""),
    +      exclude[MissingMethodProblem](""org.apache.spark.deploy.worker.Worker.sparkHome_=""),
    +      exclude[IncompatibleResultTypeProblem](""org.apache.spark.deploy.master.Master.registerWorker""),
    +      exclude[MissingMethodProblem](""org.apache.spark.deploy.master.Master.launchExecutor""),
    +      exclude[MissingMethodProblem](""org.apache.spark.deploy.master.Master.actorToWorker""),
    +      exclude[IncompatibleMethTypeProblem](""org.apache.spark.scheduler.TaskSchedulerImpl.handleFailedTask""),
    +      exclude[MissingMethodProblem](""org.apache.spark.scheduler.TaskSchedulerImpl.taskSetTaskIds""),
    +      exclude[IncompatibleMethTypeProblem](""org.apache.spark.scheduler.TaskSetManager.handleFailedTask""),
    +      exclude[MissingTypesProblem](""org.apache.spark.deploy.DeployMessages$LaunchExecutor$""),
    +      exclude[MissingMethodProblem](""org.apache.spark.scheduler.TaskSetManager.removeAllRunningTasks""),
    +      exclude[MissingMethodProblem](""org.apache.spark.scheduler.TaskSetManager.runningTasks_=""),
    +      exclude[MissingMethodProblem](""org.apache.spark.scheduler.DAGScheduler.lastFetchFailureTime""),
    +      exclude[MissingMethodProblem](""org.apache.spark.scheduler.DAGScheduler.lastFetchFailureTime_=""),
    +      exclude[MissingMethodProblem](""org.apache.spark.storage.BlockObjectWriter.bytesWritten"")) ++
    +    /**
    +     * B: Detections are mostly false +ve.
    +     */
    +    Seq(exclude[MissingMethodProblem](""org.apache.spark.SparkContext.newAPIHadoopFile$default$6""),
    +      exclude[MissingMethodProblem](""org.apache.spark.SparkContext.hadoopRDD""),
    +      exclude[MissingMethodProblem](""org.apache.spark.SparkContext.sequenceFile""),
    +      exclude[IncompatibleMethTypeProblem](""org.apache.spark.SparkContext.sequenceFile""),
    +      exclude[MissingMethodProblem](""org.apache.spark.SparkContext.sequenceFile""),
    +      exclude[MissingMethodProblem](""org.apache.spark.SparkContext.sequenceFile$default$4""),
    +      exclude[MissingMethodProblem](""org.apache.spark.SparkContext.newAPIHadoopFile$default$2""),
    +      exclude[MissingMethodProblem](""org.apache.spark.SparkContext.newAPIHadoopRDD$default$5""),
    +      exclude[IncompatibleMethTypeProblem](""org.apache.spark.SparkContext.hadoopFile""),
    +      exclude[MissingMethodProblem](""org.apache.spark.SparkContext.hadoopFile""),
    +      exclude[MissingMethodProblem](""org.apache.spark.SparkContext.hadoopFile""),
    +      exclude[MissingMethodProblem](""org.apache.spark.SparkContext.sequenceFile$default$3""),
    +      exclude[MissingMethodProblem](""org.apache.spark.SparkContext.hadoopFile$default$3""),
    +      exclude[MissingMethodProblem](""org.apache.spark.SparkContext.hadoopFile$default$6""),
    +      exclude[MissingMethodProblem](""org.apache.spark.SparkContext.sequenceFile$default$5""),
    +      exclude[MissingMethodProblem](""org.apache.spark.SparkContext.newAPIHadoopRDD""),
    +      exclude[MissingMethodProblem](""org.apache.spark.SparkContext.hadoopFile$default$2""),
    +      exclude[MissingMethodProblem](""org.apache.spark.SparkContext.hadoopRDD$default$6""),
    +      exclude[MissingMethodProblem](""org.apache.spark.SparkContext.newAPIHadoopFile""),
    +      exclude[IncompatibleMethTypeProblem](""org.apache.spark.SparkContext.newAPIHadoopFile""),
    +      exclude[MissingMethodProblem](""org.apache.spark.rdd.NewHadoopRDD.this""),
    +      exclude[MissingMethodProblem](""org.apache.spark.rdd.HadoopRDD.<init>$default$8""),
    +      exclude[MissingClassProblem](""org.apache.spark.rdd.ClassTags$""),
    +      exclude[MissingClassProblem](""org.apache.spark.rdd.ClassTags""),
    +      exclude[MissingMethodProblem](""org.apache.spark.rdd.HadoopRDD.this""),
    +      exclude[MissingMethodProblem](""org.apache.spark.rdd.HadoopRDD.this""),
    +      exclude[MissingMethodProblem](""org.apache.spark.api.java.JavaRDDLike.setGenerator""),
    +      exclude[MissingMethodProblem](""org.apache.spark.api.java.JavaRDDLike.mapPartitions""),
    +      exclude[MissingMethodProblem](""org.apache.spark.api.java.JavaRDDLike.mapPartitions""),
    +      exclude[MissingMethodProblem](""org.apache.spark.api.java.JavaRDDLike.mapPartitions""),
    +      exclude[MissingMethodProblem](""org.apache.spark.api.java.JavaRDDLike.foreachPartition""),
    +      exclude[MissingMethodProblem](""org.apache.spark.api.python.PythonRDD.writeToStream"")
    +    ) ++
    +    /**
    +     * Detections I am unsure about. Should be either moved to B (false +ve) or A.
    +     */
    +    Seq(exclude[MissingClassProblem](""org.apache.spark.mllib.recommendation.MFDataGenerator$""),
    +      exclude[MissingClassProblem](""org.apache.spark.mllib.recommendation.MFDataGenerator""),
    +      exclude[MissingClassProblem](""org.apache.spark.mllib.optimization.SquaredGradient""),
    +      exclude[IncompatibleResultTypeProblem](""org.apache.spark.mllib.regression.LinearRegressionWithSGD.gradient""),
    +      exclude[IncompatibleResultTypeProblem](""org.apache.spark.mllib.regression.RidgeRegressionWithSGD.gradient""),
    +      exclude[IncompatibleResultTypeProblem](""org.apache.spark.mllib.regression.LassoWithSGD.gradient""),
    +      exclude[MissingMethodProblem](""org.apache.spark.api.java.JavaPairRDD.cogroupResultToJava""),
    --- End diff --
    
    @aarondav this actually isn't private in master... should it be?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Thu, 20 Feb 2014 00:07:37 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/585#discussion_r9890048
  
    --- Diff: project/MimaBuild.scala ---
    @@ -0,0 +1,115 @@
    +import com.typesafe.tools.mima.plugin.MimaKeys.{binaryIssueFilters, previousArtifact}
    +import com.typesafe.tools.mima.plugin.MimaPlugin.mimaDefaultSettings
    +
    +object MimaBuild {
    +
    +  val ignoredABIProblems = {
    +    import com.typesafe.tools.mima.core._
    +    import com.typesafe.tools.mima.core.ProblemFilters._
    +    /**
    +     * A: Detections likely to become semi private at some point.
    +     */
    +    Seq(exclude[MissingClassProblem](""org.apache.spark.util.XORShiftRandom""),
    +      exclude[MissingClassProblem](""org.apache.spark.util.XORShiftRandom$""),
    +      exclude[MissingMethodProblem](""org.apache.spark.util.Utils.cloneWritables""),
    +      exclude[MissingMethodProblem](""org.apache.spark.deploy.DeployMessages#LaunchExecutor.apply""),
    +      exclude[MissingMethodProblem](""org.apache.spark.deploy.DeployMessages#LaunchExecutor.apply""),
    +      exclude[IncompatibleResultTypeProblem](""org.apache.spark.deploy.ApplicationDescription.sparkHome""),
    +      exclude[IncompatibleMethTypeProblem](""org.apache.spark.deploy.ApplicationDescription.this""),
    +      exclude[MissingMethodProblem](""org.apache.spark.deploy.DeployMessages#LaunchExecutor.sparkHome""),
    +      exclude[MissingMethodProblem](""org.apache.spark.deploy.DeployMessages#LaunchExecutor.copy""),
    +      exclude[MissingMethodProblem](""org.apache.spark.deploy.DeployMessages#LaunchExecutor.copy$default$7""),
    +      exclude[MissingMethodProblem](""org.apache.spark.deploy.DeployMessages#LaunchExecutor.this""),
    +      exclude[MissingMethodProblem](""org.apache.spark.deploy.worker.Worker.sparkHome_=""),
    +      exclude[IncompatibleResultTypeProblem](""org.apache.spark.deploy.master.Master.registerWorker""),
    +      exclude[MissingMethodProblem](""org.apache.spark.deploy.master.Master.launchExecutor""),
    +      exclude[MissingMethodProblem](""org.apache.spark.deploy.master.Master.actorToWorker""),
    +      exclude[IncompatibleMethTypeProblem](""org.apache.spark.scheduler.TaskSchedulerImpl.handleFailedTask""),
    +      exclude[MissingMethodProblem](""org.apache.spark.scheduler.TaskSchedulerImpl.taskSetTaskIds""),
    +      exclude[IncompatibleMethTypeProblem](""org.apache.spark.scheduler.TaskSetManager.handleFailedTask""),
    +      exclude[MissingTypesProblem](""org.apache.spark.deploy.DeployMessages$LaunchExecutor$""),
    +      exclude[MissingMethodProblem](""org.apache.spark.scheduler.TaskSetManager.removeAllRunningTasks""),
    +      exclude[MissingMethodProblem](""org.apache.spark.scheduler.TaskSetManager.runningTasks_=""),
    +      exclude[MissingMethodProblem](""org.apache.spark.scheduler.DAGScheduler.lastFetchFailureTime""),
    +      exclude[MissingMethodProblem](""org.apache.spark.scheduler.DAGScheduler.lastFetchFailureTime_=""),
    +      exclude[MissingMethodProblem](""org.apache.spark.storage.BlockObjectWriter.bytesWritten"")) ++
    +    /**
    +     * B: Detections are mostly false +ve.
    +     */
    +    Seq(exclude[MissingMethodProblem](""org.apache.spark.SparkContext.newAPIHadoopFile$default$6""),
    +      exclude[MissingMethodProblem](""org.apache.spark.SparkContext.hadoopRDD""),
    +      exclude[MissingMethodProblem](""org.apache.spark.SparkContext.sequenceFile""),
    +      exclude[IncompatibleMethTypeProblem](""org.apache.spark.SparkContext.sequenceFile""),
    +      exclude[MissingMethodProblem](""org.apache.spark.SparkContext.sequenceFile""),
    +      exclude[MissingMethodProblem](""org.apache.spark.SparkContext.sequenceFile$default$4""),
    +      exclude[MissingMethodProblem](""org.apache.spark.SparkContext.newAPIHadoopFile$default$2""),
    +      exclude[MissingMethodProblem](""org.apache.spark.SparkContext.newAPIHadoopRDD$default$5""),
    +      exclude[IncompatibleMethTypeProblem](""org.apache.spark.SparkContext.hadoopFile""),
    +      exclude[MissingMethodProblem](""org.apache.spark.SparkContext.hadoopFile""),
    +      exclude[MissingMethodProblem](""org.apache.spark.SparkContext.hadoopFile""),
    +      exclude[MissingMethodProblem](""org.apache.spark.SparkContext.sequenceFile$default$3""),
    +      exclude[MissingMethodProblem](""org.apache.spark.SparkContext.hadoopFile$default$3""),
    +      exclude[MissingMethodProblem](""org.apache.spark.SparkContext.hadoopFile$default$6""),
    +      exclude[MissingMethodProblem](""org.apache.spark.SparkContext.sequenceFile$default$5""),
    +      exclude[MissingMethodProblem](""org.apache.spark.SparkContext.newAPIHadoopRDD""),
    +      exclude[MissingMethodProblem](""org.apache.spark.SparkContext.hadoopFile$default$2""),
    +      exclude[MissingMethodProblem](""org.apache.spark.SparkContext.hadoopRDD$default$6""),
    +      exclude[MissingMethodProblem](""org.apache.spark.SparkContext.newAPIHadoopFile""),
    +      exclude[IncompatibleMethTypeProblem](""org.apache.spark.SparkContext.newAPIHadoopFile""),
    +      exclude[MissingMethodProblem](""org.apache.spark.rdd.NewHadoopRDD.this""),
    +      exclude[MissingMethodProblem](""org.apache.spark.rdd.HadoopRDD.<init>$default$8""),
    +      exclude[MissingClassProblem](""org.apache.spark.rdd.ClassTags$""),
    +      exclude[MissingClassProblem](""org.apache.spark.rdd.ClassTags""),
    +      exclude[MissingMethodProblem](""org.apache.spark.rdd.HadoopRDD.this""),
    +      exclude[MissingMethodProblem](""org.apache.spark.rdd.HadoopRDD.this""),
    +      exclude[MissingMethodProblem](""org.apache.spark.api.java.JavaRDDLike.setGenerator""),
    +      exclude[MissingMethodProblem](""org.apache.spark.api.java.JavaRDDLike.mapPartitions""),
    +      exclude[MissingMethodProblem](""org.apache.spark.api.java.JavaRDDLike.mapPartitions""),
    +      exclude[MissingMethodProblem](""org.apache.spark.api.java.JavaRDDLike.mapPartitions""),
    +      exclude[MissingMethodProblem](""org.apache.spark.api.java.JavaRDDLike.foreachPartition""),
    +      exclude[MissingMethodProblem](""org.apache.spark.api.python.PythonRDD.writeToStream"")
    +    ) ++
    +    /**
    +     * Detections I am unsure about. Should be either moved to B (false +ve) or A.
    +     */
    +    Seq(exclude[MissingClassProblem](""org.apache.spark.mllib.recommendation.MFDataGenerator$""),
    +      exclude[MissingClassProblem](""org.apache.spark.mllib.recommendation.MFDataGenerator""),
    +      exclude[MissingClassProblem](""org.apache.spark.mllib.optimization.SquaredGradient""),
    +      exclude[IncompatibleResultTypeProblem](""org.apache.spark.mllib.regression.LinearRegressionWithSGD.gradient""),
    +      exclude[IncompatibleResultTypeProblem](""org.apache.spark.mllib.regression.RidgeRegressionWithSGD.gradient""),
    +      exclude[IncompatibleResultTypeProblem](""org.apache.spark.mllib.regression.LassoWithSGD.gradient""),
    +      exclude[MissingMethodProblem](""org.apache.spark.api.java.JavaPairRDD.cogroupResultToJava""),
    --- End diff --
    
    hm, actually sorry they are private[spark]... weird.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
CodingCat <git@git.apache.org>,"Thu, 20 Feb 2014 00:13:23 +0000 (UTC)","[GitHub] incubator-spark pull request: MLLIB-24: url of ""Collaborative Filt...",dev@spark.incubator.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/incubator-spark/pull/619#issuecomment-35568283
  
    @mengxr good point, I agree


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Thu, 20 Feb 2014 00:15:21 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/585#discussion_r9890236
  
    --- Diff: project/MimaBuild.scala ---
    @@ -0,0 +1,105 @@
    +import com.typesafe.tools.mima.plugin.MimaKeys.{binaryIssueFilters, previousArtifact}
    +import com.typesafe.tools.mima.plugin.MimaPlugin.mimaDefaultSettings
    +
    +object MimaBuild {
    +
    +  val ignoredABIProblems = {
    +    import com.typesafe.tools.mima.core._
    +    import com.typesafe.tools.mima.core.ProblemFilters._
    +    /**
    +     * A: Detections are semi private or likely to become semi private at some point.
    +     */
    +    Seq(exclude[MissingClassProblem](""org.apache.spark.util.XORShiftRandom""),
    +      exclude[MissingClassProblem](""org.apache.spark.util.XORShiftRandom$""),
    +      exclude[MissingMethodProblem](""org.apache.spark.util.Utils.cloneWritables""),
    +      // Scheduler is not considered a public API.
    +      excludePackage(""org.apache.spark.deploy""),
    +      // Was made private in 1.0
    +      exclude[MissingMethodProblem](""org.apache.spark.api.java.JavaPairRDD.cogroupResultToJava""),
    +      exclude[MissingMethodProblem](""org.apache.spark.api.java.JavaPairRDD.groupByResultToJava""),
    +      exclude[IncompatibleMethTypeProblem](""org.apache.spark.scheduler.TaskSchedulerImpl.handleFailedTask""),
    +      exclude[MissingMethodProblem](""org.apache.spark.scheduler.TaskSchedulerImpl.taskSetTaskIds""),
    +      exclude[IncompatibleMethTypeProblem](""org.apache.spark.scheduler.TaskSetManager.handleFailedTask""),
    +      exclude[MissingMethodProblem](""org.apache.spark.scheduler.TaskSetManager.removeAllRunningTasks""),
    +      exclude[MissingMethodProblem](""org.apache.spark.scheduler.TaskSetManager.runningTasks_=""),
    +      exclude[MissingMethodProblem](""org.apache.spark.scheduler.DAGScheduler.lastFetchFailureTime""),
    +      exclude[MissingMethodProblem](""org.apache.spark.scheduler.DAGScheduler.lastFetchFailureTime_=""),
    +      exclude[MissingMethodProblem](""org.apache.spark.storage.BlockObjectWriter.bytesWritten"")) ++
    +    /**
    +     * B: Detections are mostly false +ve.
    +     */
    +    Seq(exclude[MissingMethodProblem](""org.apache.spark.SparkContext.newAPIHadoopFile$default$6""),
    --- End diff --
    
    @ScrapCodes I think you might be pulling in a locally published version of 0.9.0-incubating that was based on an older build, because none of these hadoop methods have been changed between master and 0.9.0. To test this I removed all of these and the mima check still went alright. Maybe try clearing your ivy cache and local?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Thu, 20 Feb 2014 00:21:05 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/585#discussion_r9890431
  
    --- Diff: project/MimaBuild.scala ---
    @@ -0,0 +1,105 @@
    +import com.typesafe.tools.mima.plugin.MimaKeys.{binaryIssueFilters, previousArtifact}
    +import com.typesafe.tools.mima.plugin.MimaPlugin.mimaDefaultSettings
    +
    +object MimaBuild {
    +
    +  val ignoredABIProblems = {
    +    import com.typesafe.tools.mima.core._
    +    import com.typesafe.tools.mima.core.ProblemFilters._
    +    /**
    +     * A: Detections are semi private or likely to become semi private at some point.
    +     */
    +    Seq(exclude[MissingClassProblem](""org.apache.spark.util.XORShiftRandom""),
    +      exclude[MissingClassProblem](""org.apache.spark.util.XORShiftRandom$""),
    +      exclude[MissingMethodProblem](""org.apache.spark.util.Utils.cloneWritables""),
    +      // Scheduler is not considered a public API.
    +      excludePackage(""org.apache.spark.deploy""),
    +      // Was made private in 1.0
    --- End diff --
    
    Ah darn, seems like this doesn't work.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
asfgit <git@git.apache.org>,"Thu, 20 Feb 2014 00:21:24 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1105] fix site scala version ...,dev@spark.incubator.apache.org,"Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-spark/pull/618


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
colorant <git@git.apache.org>,"Thu, 20 Feb 2014 00:46:57 +0000 (UTC)","[GitHub] incubator-spark pull request: For SPARK-1082, Use Curator for ZK i...",dev@spark.incubator.apache.org,"Github user colorant commented on the pull request:

    https://github.com/apache/incubator-spark/pull/611#issuecomment-35570472
  
    ah, so the sleep removed ;) and the synchronization block is already there, is it ok?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 20 Feb 2014 01:05:33 +0000 (UTC)","[GitHub] incubator-spark pull request: For SPARK-1082, Use Curator for ZK i...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/611#issuecomment-35572287
  
     Build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 20 Feb 2014 01:05:33 +0000 (UTC)","[GitHub] incubator-spark pull request: For SPARK-1082, Use Curator for ZK i...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/611#issuecomment-35572288
  
    Build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
hsaputra <git@git.apache.org>,"Thu, 20 Feb 2014 01:08:08 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-929: Fully deprecate usage of ...,dev@spark.incubator.apache.org,"Github user hsaputra commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/615#discussion_r9891770
  
    --- Diff: core/src/main/scala/org/apache/spark/SparkContext.scala ---
    @@ -165,19 +165,20 @@ class SparkContext(
         jars.foreach(addJar)
       }
     
    +  def warnSparkMem(value: String): String = {
    +    logWarning(""Using SPARK_MEM to set amount of memory to use per executor process is "" +
    +      ""deprecated, please use instead spark.executor.memory"")
    --- End diff --
    
    Small nit of the warning wording:
    
    ""deprecated, please use spark.executor.memory instead.""


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
sryza <git@git.apache.org>,"Thu, 20 Feb 2014 01:14:32 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-929: Fully deprecate usage of ...,dev@spark.incubator.apache.org,"Github user sryza commented on the pull request:

    https://github.com/apache/incubator-spark/pull/615#issuecomment-35572922
  
    Will this nomenclature make sense in the context of yarn-standalone mode, where spark-class is used, but the driver is run inside an application master on the cluster, not inside the client?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
CrazyJvm <git@git.apache.org>,"Thu, 20 Feb 2014 01:29:40 +0000 (UTC)","[GitHub] incubator-spark pull request: MLLIB-24: url of ""Collaborative Filt...",dev@spark.incubator.apache.org,"Github user CrazyJvm commented on the pull request:

    https://github.com/apache/incubator-spark/pull/619#issuecomment-35575435
  
    take ""permanent valid url"" into consideration, change url from yahoo to ieee. thx @mengxr .
    http://dx.doi.org/10.1109/ICDM.2008.22


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 20 Feb 2014 02:05:15 +0000 (UTC)","[GitHub] incubator-spark pull request: For SPARK-1082, Use Curator for ZK i...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/611#issuecomment-35579234
  
    Build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 20 Feb 2014 02:05:15 +0000 (UTC)","[GitHub] incubator-spark pull request: For SPARK-1082, Use Curator for ZK i...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/611#issuecomment-35579235
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12782/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Thu, 20 Feb 2014 02:22:15 +0000 (UTC)",[GitHub] incubator-spark pull request: Add a environment variable that allo...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/192#issuecomment-35580209
  
    See SPARK-1110... I took down some notes there relevant to this:
    https://spark-project.atlassian.net/browse/SPARK-1110


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Thu, 20 Feb 2014 02:43:58 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-929: Fully deprecate usage of ...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/615#issuecomment-35581271
  
    @sryza - I don't think this is relevant to the YARN codepath. AFAIK YARN doesn't use the ./spark-class script to launch the YARN application master (which embeds the driver program). I'm not totally sure how that JVM is actually launched though... couldn't figure it out on a quick glance at that code.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Thu, 20 Feb 2014 02:46:22 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-929: Fully deprecate usage of ...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/615#issuecomment-35581371
  
    It looks like there is a separate variable called `amMemory` that deals with this in YARN. The command for launching that JVM gets set-up in:
    
        common/src/main/scala/org/apache/spark/deploy/yarn/ClientBase.scala


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 20 Feb 2014 03:06:06 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-35582253
  
    Build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 20 Feb 2014 03:06:06 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-35582252
  
     Build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
ScrapCodes <git@git.apache.org>,"Thu, 20 Feb 2014 03:51:08 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user ScrapCodes commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35586860
  
    Hey Matei, 
    
    I feel this is better than before in overall. There is one thing I was not very sure about is putting a couple of implicits in JavaPairRDD. But this was already being done. There is no way I know our users from previous versions can avoid a recompile as such. 


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 20 Feb 2014 04:05:28 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-35587410
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12783/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 20 Feb 2014 04:05:28 +0000 (UTC)",[GitHub] incubator-spark pull request: Adding an option to persist Spark RD...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/468#issuecomment-35587409
  
    Build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
ScrapCodes <git@git.apache.org>,"Thu, 20 Feb 2014 04:16:13 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user ScrapCodes commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/585#discussion_r9895285
  
    --- Diff: project/MimaBuild.scala ---
    @@ -0,0 +1,105 @@
    +import com.typesafe.tools.mima.plugin.MimaKeys.{binaryIssueFilters, previousArtifact}
    +import com.typesafe.tools.mima.plugin.MimaPlugin.mimaDefaultSettings
    +
    +object MimaBuild {
    +
    +  val ignoredABIProblems = {
    +    import com.typesafe.tools.mima.core._
    +    import com.typesafe.tools.mima.core.ProblemFilters._
    +    /**
    +     * A: Detections are semi private or likely to become semi private at some point.
    +     */
    +    Seq(exclude[MissingClassProblem](""org.apache.spark.util.XORShiftRandom""),
    +      exclude[MissingClassProblem](""org.apache.spark.util.XORShiftRandom$""),
    +      exclude[MissingMethodProblem](""org.apache.spark.util.Utils.cloneWritables""),
    +      // Scheduler is not considered a public API.
    +      excludePackage(""org.apache.spark.deploy""),
    +      // Was made private in 1.0
    --- End diff --
    
    you are right.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
rxin <git@git.apache.org>,"Thu, 20 Feb 2014 06:06:38 +0000 (UTC)","[GitHub] incubator-spark pull request: MLLIB-24: url of ""Collaborative Filt...",dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/619#issuecomment-35592012
  
    Thanks. I've merged this.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
ScrapCodes <git@git.apache.org>,"Thu, 20 Feb 2014 06:43:24 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user ScrapCodes commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35593470
  
    Hey @pwendell, Not sure how, cleared ivy and m2 for spark but it is not possible to get rid of these. I am trying it with jenkins once, since you could remove them w/o errors.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
asfgit <git@git.apache.org>,"Thu, 20 Feb 2014 07:00:12 +0000 (UTC)","[GitHub] incubator-spark pull request: MLLIB-24: url of ""Collaborative Filt...",dev@spark.incubator.apache.org,"Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-spark/pull/619


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 20 Feb 2014 07:07:54 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35594451
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 20 Feb 2014 07:07:54 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35594452
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
MLnick <git@git.apache.org>,"Thu, 20 Feb 2014 07:41:01 +0000 (UTC)",[GitHub] incubator-spark pull request: MLLIB-22. Support negative implicit ...,dev@spark.incubator.apache.org,"Github user MLnick commented on the pull request:

    https://github.com/apache/incubator-spark/pull/500#issuecomment-35595951
  
    @rxin can we look at merging this in?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
rxin <git@git.apache.org>,"Thu, 20 Feb 2014 07:45:05 +0000 (UTC)",[GitHub] incubator-spark pull request: MLLIB-22. Support negative implicit ...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/500#issuecomment-35596136
  
    Merged. Thanks!



---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
asfgit <git@git.apache.org>,"Thu, 20 Feb 2014 07:58:46 +0000 (UTC)",[GitHub] incubator-spark pull request: MLLIB-22. Support negative implicit ...,dev@spark.incubator.apache.org,"Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-spark/pull/500


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 20 Feb 2014 08:05:30 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35597143
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12784/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 20 Feb 2014 08:05:30 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35597142
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
ScrapCodes <git@git.apache.org>,"Thu, 20 Feb 2014 08:07:22 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user ScrapCodes commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35597240
  
    Not sure what's wrong !


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 20 Feb 2014 08:07:51 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35597277
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 20 Feb 2014 08:07:51 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35597278
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
souravchandra <git@git.apache.org>,"Thu, 20 Feb 2014 08:28:30 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark Job Server Preview,dev@spark.incubator.apache.org,"Github user souravchandra commented on the pull request:

    https://github.com/apache/incubator-spark/pull/222#issuecomment-35598394
  
    Just came across this. Looks nice. Can it work with Spark Streaming?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
velvia <git@git.apache.org>,"Thu, 20 Feb 2014 08:30:59 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark Job Server Preview,dev@spark.incubator.apache.org,"Github user velvia commented on the pull request:

    https://github.com/apache/incubator-spark/pull/222#issuecomment-35598520
  
    Sourav, not yet.  Need to think about how to enable spark streaming support
    (I'm going to move this to its own repo, at which point pull requests for
    this kind of thing will definitely be welcome  :)
    
    
    
    > Just came across this. Looks nice. Can it work with Spark Streaming?
    >
    > --
    > Reply to this email directly or view it on GitHub<https://github.com/apache/incubator-spark/pull/222#issuecomment-35598394>
    > .
    >
    
    
    
    -- 
    The fruit of silence is prayer;
    the fruit of prayer is faith;
    the fruit of faith is love;
    the fruit of love is service;
    the fruit of service is peace.  -- Mother Teresa


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
souravchandra <git@git.apache.org>,"Thu, 20 Feb 2014 08:40:09 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark Job Server Preview,dev@spark.incubator.apache.org,"Github user souravchandra commented on the pull request:

    https://github.com/apache/incubator-spark/pull/222#issuecomment-35599036
  
    Thanks for prompt response :) Looking forward to it


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
mengxr <git@git.apache.org>,"Thu, 20 Feb 2014 08:50:59 +0000 (UTC)",[GitHub] incubator-spark pull request: Principal Component Analysis,dev@spark.incubator.apache.org,"Github user mengxr commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/564#discussion_r9899395
  
    --- Diff: mllib/src/main/scala/org/apache/spark/mllib/linalg/PCA.scala ---
    @@ -0,0 +1,119 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one or more
    + * contributor license agreements.  See the NOTICE file distributed with
    + * this work for additional information regarding copyright ownership.
    + * The ASF licenses this file to You under the Apache License, Version 2.0
    + * (the ""License""); you may not use this file except in compliance with
    + * the License.  You may obtain a copy of the License at
    + *
    + *    http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.spark.mllib.linalg
    +
    +import org.apache.spark.SparkContext
    +import org.apache.spark.SparkContext._
    +import org.apache.spark.rdd.RDD
    +
    +import org.apache.spark.mllib.util._
    +
    +
    +/**
    + * Class used to obtain principal components
    + */
    +class PCA {
    +  private var k: Int = 1
    +
    +  /**
    +   * Set the number of top-k principle components to return
    +   */
    +  def setK(k: Int): PCA = {
    +    this.k = k
    +    this
    +  }
    +
    +   /**
    +   * Compute PCA using the current set parameters
    +   */
    +  def compute(matrix: DenseMatrix): DenseMatrix = {
    +    computePCA(matrix, k)
    +  }
    +
    +  /**
    +  * Principal Component Analysis.
    +  * Computes the top k principal component coefficients for the m-by-n data matrix X.
    +  * Rows of X correspond to observations and columns correspond to variables. 
    +  * The coefficient matrix is n-by-k. Each column of coeff contains coefficients
    +  * for one principal component, and the columns are in descending 
    +  * order of component variance.
    +  * This function centers the data and uses the 
    +  * singular value decomposition (SVD) algorithm. 
    +  *
    +  * All input and output is expected in DenseMatrix format
    +  *
    +  * @param matrix dense matrix to perform pca on
    +  * @param k Recover k principal components
    +  * @return An nxk matrix of principal components
    +  */
    +  def computePCA(matrix: DenseMatrix, k: Int): DenseMatrix = {
    +    val m = matrix.m
    +    val n = matrix.n
    +
    +    if (m <= 0 || n <= 0) {
    +      throw new IllegalArgumentException(""Expecting a well-formed matrix"")
    +    }
    +
    +    // compute column sums and normalize matrix
    +    val rawData = matrix.rows.flatMap{
    +      x => Array.tabulate(x.data.size)(idx => MatrixEntry(x.i, idx, x.data(idx)))
    +    }
    +    val colSums = rawData.map(entry => (entry.j, entry.mval)).reduceByKey(_ + _)
    +    val data = rawData.map(entry => (entry.j, (entry.i, entry.mval))).join(colSums).map{
    +      case (col, ((row, mval), colsum)) =>
    +        MatrixEntry(row, col, (mval - colsum / m.toDouble) / Math.sqrt(n-1)) }
    --- End diff --
    
    The dense to sparse conversion is inefficient and can be avoided. It should be easy to compute the column sums and center each column if the input is basically a RDD[Array[Double]]. Would it be better if we update the SVD algorithm to accept a dense but tall and skinny matrix as input and use BLAS's DSYR or DSPR to compute A^T A?
    
    Also, though it is not hard to generate row indices (see RDD.zipWithIndex), row indices and number of rows are not necessary for the computation.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 20 Feb 2014 09:05:38 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35600627
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 20 Feb 2014 09:05:38 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35600628
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12785/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 20 Feb 2014 13:05:56 +0000 (UTC)",[GitHub] incubator-spark pull request: Fix ExternalMap on case of key's has...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/612#issuecomment-35619101
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 20 Feb 2014 13:05:56 +0000 (UTC)",[GitHub] incubator-spark pull request: Fix ExternalMap on case of key's has...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/612#issuecomment-35619100
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
srowen <git@git.apache.org>,"Thu, 20 Feb 2014 13:13:18 +0000 (UTC)",[GitHub] incubator-spark pull request: Default log4j initialization causes ...,dev@spark.incubator.apache.org,"Github user srowen commented on the pull request:

    https://github.com/apache/incubator-spark/pull/573#issuecomment-35619639
  
    @aarondav They are actually separate issues, although concerning the same specific aspect of the code. @pwendell has a good handle on this particular problem I think, which is a bit tricky. (See my comments here too?) But #570 is actually independent.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
srowen <git@git.apache.org>,"Thu, 20 Feb 2014 13:16:11 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1084. Fix most build warnings,dev@spark.incubator.apache.org,"Github user srowen commented on the pull request:

    https://github.com/apache/incubator-spark/pull/586#issuecomment-35619835
  
    Hello all - I think all outstanding questions and comments had been addressed in the last round of comments last week. Before it goes too stale, thought I'd ping the issue to see if it looks good to go to you guys. Totally up to you what you want to take or not take and I'm happy to rebase / tweak as you like.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 20 Feb 2014 13:34:35 +0000 (UTC)",[GitHub] incubator-spark pull request: Fix ExternalMap on case of key's has...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/612#issuecomment-35621469
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 20 Feb 2014 13:34:36 +0000 (UTC)",[GitHub] incubator-spark pull request: Fix ExternalMap on case of key's has...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/612#issuecomment-35621470
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12786/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Guillaume Pitel <guillaume.pitel@exensa.com>,"Thu, 20 Feb 2014 16:24:40 +0100",Problem with akka.frameSize,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi,

I've sent a few emails to the user mailing list, but since I believe this is 
bug, I think it's time to talk to the developpers.

So here is what happens: since we've migrated from 0.8.1 to 0.9, whatever the 
value of spark.akka.frameSize I set, the Executors lock when a collect tries to 
send more than 10MB of data to the driver.

Since 10MB is the spark default, I suspect it could be related to something in 
the configuration. We still use System.setProperty to set the frameSize.

As a workaround, setting the frameSize back to 10 seems to work.

Guillaume
-- 
eXenSa

	
*Guillaume PITEL, Président*
+33(0)6 25 48 86 80

eXenSa S.A.S. <http://www.exensa.com/>
41, rue Périer - 92120 Montrouge - FRANCE
Tel +33(0)1 84 16 36 77 / Fax +33(0)9 72 28 37 05

"
Andy Konwinski <andykonwinski@gmail.com>,"Thu, 20 Feb 2014 08:37:23 -0800","Fwd: ASF Board Meeting Summary - February 19, 2014",dev@spark.incubator.apache.org,"Congrats Spark community! I think this means we are officially now a TLP!
---------- Forwarded message ----------
From: ""Brett Porter"" <chairman@apache.org>
Date: Feb 19, 2014 11:26 PM
Subject: ASF Board Meeting Summary - February 19, 2014
To: <committers@apache.org>
Cc:

The February board meeting took place on the 19th.

The following directors were present:

  Shane Curcuru
  Bertrand Delacretaz
  Roy T. Fielding
  Jim Jagielski
  Chris Mattmann
  Brett Porter
  Greg Stein

Apologies were received from Sam Ruby.

The following officers were present:

  Ross Gardler
  Rich Bowen
  Craig L Russell

The following guests were present:

  Sean Kelly
  Daniel Gruno
  Phil Steitz
  Jake Farrell
  Marvin Humphrey
  David Nalley
  Noah Slater

The January minutes were approved.
Minutes will be posted to http://www.apache.org/foundation/records/minutes/

The following reports were not approved and are expected next month:

 Report from the Apache Lenya Project  [Richard Frovarp]

The following reports were not received and are expected next month:

  Report from the Apache Abdera Project  [Ant Elder]
  Report from the Apache Buildr Project  [Alex Boisvert]
  Report from the Apache Click Project  [Malcolm Edgar]
  Report from the Apache Community Development Project  [Luciano Resende]
  Report from the Apache Continuum Project  [Brent Atkinson]
  Report from the Apache Creadur Project  [Robert Burrell Donkin]
  Report from the Apache DirectMemory Project  [Raffaele P. Guidi]
  Report from the Apache Giraph Project  [Avery Ching]
  Report from the Apache Velocity Project  [Nathan Bubna]

All other reports to the board were approved.

The following resolutions were passed unanimously:

  A. Establish the Apache Open Climate Workbench Project (Michael Joyce, VP)
  B. Change the Apache Incubator Project Chair (Roman Shaposhnik, VP)
  C. Establish the Apache Spark Project (Matei Zaharia, VP)
  D. Establish the Apache Knox Project (Kevin Minder, VP)

The next board meeting will be on the 19th of March.
"
Guillaume Pitel <guillaume.pitel@exensa.com>,"Thu, 20 Feb 2014 17:39:34 +0100",Re: Problem with akka.frameSize,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Jira ticket created https://spark-project.atlassian.net/browse/SPARK-1112

Guillaume

-- 
eXenSa

	
*Guillaume PITEL, Président*
+33(0)6 25 48 86 80

eXenSa S.A.S. <http://www.exensa.com/>
41, rue Périer - 92120 Montrouge - FRANCE
Tel +33(0)1 84 16 36 77 / Fax +33(0)9 72 28 37 05

"
"""Mattmann, Chris A (3980)"" <chris.a.mattmann@jpl.nasa.gov>","Thu, 20 Feb 2014 16:40:20 +0000","Re: ASF Board Meeting Summary - February 19, 2014","""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Sure does! Congrats guys!





"
Christopher Nguyen <ctn@adatao.com>,"Thu, 20 Feb 2014 08:44:26 -0800","Re: ASF Board Meeting Summary - February 19, 2014",dev@spark.incubator.apache.org,"Very cool, Andy!

--
Christopher T. Nguyen
Co-founder & CEO, Adatao <http://adatao.com>
linkedin.com/in/ctnguyen




"
mag- <git@git.apache.org>,"Thu, 20 Feb 2014 16:51:00 +0000 (UTC)","[GitHub] incubator-spark pull request: Enhance ec2 scripts, add vpc support",dev@spark.incubator.apache.org,"GitHub user mag- opened a pull request:

    https://github.com/apache/incubator-spark/pull/620

    Enhance ec2 scripts, add vpc support

    I made a few changes to ec2 scripts, they now support launching clusters inside specified VPC and subnet.
    Also added support for placement groups, and a list of ""secure ips"" that are added to security groups so your cluster won't be open for a whole world.
    Ephemeral disks were not always properly attached ( sometimes I got only two on cc2.8xlarge when I should have 4 ) - fixed that
    


You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/mag-/incubator-spark add_vpc_support

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/620.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #620
    
----
commit 0ed50ec5e7a9e6bdd1a8b1c0c5a6a9aca2e98923
Author: Rafal ^^MAg^^ Kwasny <mag@entropy.be>
Date:   2014-02-19T16:24:58Z

    add support for VPC

commit 867c6f610c0fb03aae2ed99bb1a1cc1e7095f73c
Author: Rafal ^^MAg^^ Kwasny <mag@entropy.be>
Date:   2014-02-19T18:22:56Z

    add support for placement groups, and multipple secure ips

commit 523837d863925797e4de645f715a4b284f86797a
Author: Rafal ^^MAg^^ Kwasny <mag@entropy.be>
Date:   2014-02-20T16:45:08Z

    ensure all ephemeral disks are added to instance

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 20 Feb 2014 17:06:41 +0000 (UTC)","[GitHub] incubator-spark pull request: Enhance ec2 scripts, add vpc support",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/620#issuecomment-35644199
  
    Can one of the admins verify this patch?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
jyotiska <git@git.apache.org>,"Thu, 20 Feb 2014 18:41:24 +0000 (UTC)","[GitHub] incubator-spark pull request: doctest updated for mapValues, flatM...",dev@spark.incubator.apache.org,"GitHub user jyotiska opened a pull request:

    https://github.com/apache/incubator-spark/pull/621

    doctest updated for mapValues, flatMapValues in rdd.py

    Updated doctests for mapValues and flatMapValues in rdd.py

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/jyotiska/incubator-spark python_spark

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/621.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #621
    
----
commit 716f7cda5e567c7de6f00246bdb427e87b6bbf82
Author: jyotiska <jyotiska123@gmail.com>
Date:   2014-02-20T16:51:01Z

    doctest updated for mapValues, flatMapValues in rdd.py

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
tgravescs <git@git.apache.org>,"Thu, 20 Feb 2014 18:56:38 +0000 (UTC)","[GitHub] incubator-spark pull request: Add Security to Spark - Akka, Http, ...",dev@spark.incubator.apache.org,"Github user tgravescs commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/332#discussion_r9918847
  
    --- Diff: core/src/main/scala/org/apache/spark/network/ConnectionManager.scala ---
    @@ -533,10 +667,65 @@ private[spark] class ConnectionManager(port: Int, conf: SparkConf) extends Loggi
         }
       }
     
    +  private def checkSendAuthFirst(connManagerId: ConnectionManagerId, conn: SendingConnection) {
    +    // see if we need to do sasl before writing
    +    // this should only be the first negotiation as the Client!!!
    +    if (!conn.isSaslComplete()) {
    +      conn.synchronized {
    +        if (conn.sparkSaslClient == null) {
    +          conn.sparkSaslClient = new SparkSaslClient(securityManager)
    +          var firstResponse: Array[Byte] = null
    +          try {
    +            firstResponse = conn.sparkSaslClient.firstToken()
    +            var securityMsg = SecurityMessage.fromResponse(firstResponse, conn.connectionId.id)
    +            var message = securityMsg.toBufferMessage
    +            if (message == null) throw new Exception(""Error creating security message"")
    +            sendSecurityMessage(connManagerId, message)
    +            logDebug(""adding connectionsAwaitingSasl id: "" + conn.connectionId)
    +            connectionsAwaitingSasl += ((conn.connectionId, conn))
    +          } catch {
    --- End diff --
    
    Good catch, I'll move it up.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
punya <git@git.apache.org>,"Thu, 20 Feb 2014 18:59:33 +0000 (UTC)",[GitHub] incubator-spark pull request: Migrate Java code to Scala or move i...,dev@spark.incubator.apache.org,"Github user punya commented on the pull request:

    https://github.com/apache/incubator-spark/pull/605#issuecomment-35656179
  
    @aarondav is this ready to go?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 20 Feb 2014 19:06:58 +0000 (UTC)","[GitHub] incubator-spark pull request: doctest updated for mapValues, flatM...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/621#issuecomment-35656954
  
    Can one of the admins verify this patch?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Mridul Muralidharan <mridul@gmail.com>,"Fri, 21 Feb 2014 01:07:19 +0530","Re: Fwd: ASF Board Meeting Summary - February 19, 2014",dev@spark.incubator.apache.org,"Wonderful news ! Congrats all :-)

Regards,
Mridul

"
Giri Iyengar <giri.iyengar@velos.io>,"Thu, 20 Feb 2014 14:45:42 -0500","Re: ASF Board Meeting Summary - February 19, 2014",dev@spark.incubator.apache.org,"Awesome news. Congratulations, Spark!

-giri






-- 
GIRI IYENGAR, CTO
VELOS.IO
Simple. Powerful. Predictions.

440 NINTH AVE, 11TH FLOOR NEW YORK CITY, NY 10001
O: 917.525.2466x104   M: 914.924.7935
E: *giri.iyengar@v <giri.iyengar@sociocast.com>elos.io
<http://elos.io>* W: *www.velos.
<http://www.sociocast.com/>io*
"
tgravescs <git@git.apache.org>,"Thu, 20 Feb 2014 20:05:29 +0000 (UTC)","[GitHub] incubator-spark pull request: Add Security to Spark - Akka, Http, ...",dev@spark.incubator.apache.org,"Github user tgravescs commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/332#discussion_r9921913
  
    --- Diff: core/src/main/scala/org/apache/spark/network/ConnectionManager.scala ---
    @@ -483,10 +496,131 @@ private[spark] class ConnectionManager(port: Int, conf: SparkConf) extends Loggi
         /*handleMessage(connection, message)*/
       }
     
    -  private def handleMessage(connectionManagerId: ConnectionManagerId, message: Message) {
    +  private def handleClientAuthNeg(
    +      waitingConn: SendingConnection,
    +      securityMsg: SecurityMessage, 
    +      connectionId : ConnectionId) {
    +    if (waitingConn.isSaslComplete()) {
    +      logDebug(""Client sasl completed for id: ""  + waitingConn.connectionId)
    +      connectionsAwaitingSasl -= waitingConn.connectionId
    +      waitingConn.getAuthenticated().synchronized {
    +        waitingConn.getAuthenticated().notifyAll();
    +      }
    +      return
    +    } else {
    +      var replyToken : Array[Byte] = null
    +      try {
    +        replyToken = waitingConn.sparkSaslClient.saslResponse(securityMsg.getToken);
    +        if (waitingConn.isSaslComplete()) {
    +          logDebug(""Client sasl completed after evaluate for id: "" + waitingConn.connectionId)
    +          connectionsAwaitingSasl -= waitingConn.connectionId
    +          waitingConn.getAuthenticated().synchronized {
    +            waitingConn.getAuthenticated().notifyAll()
    +          }
    +          return
    +        }
    +        var securityMsgResp = SecurityMessage.fromResponse(replyToken, securityMsg.getConnectionId)
    +        var message = securityMsgResp.toBufferMessage
    +        if (message == null) throw new Exception(""Error creating security message"")
    +        sendSecurityMessage(waitingConn.getRemoteConnectionManagerId(), message)
    +      } catch  {
    +        case e: Exception => {
    +          logError(""Error doing sasl client: "" + e)
    +          waitingConn.close()
    +          throw new Exception(""error evaluating sasl response: "" + e)
    +        }
    +      }
    +    }
    +  }
    +
    +  private def handleServerAuthNeg(
    +      connection: Connection, 
    +      securityMsg: SecurityMessage,
    +      connectionId: ConnectionId) {
    +    if (!connection.isSaslComplete()) {
    +      logDebug(""saslContext not established"")
    +      var replyToken : Array[Byte] = null
    +      try {
    +        connection.synchronized {
    +          if (connection.sparkSaslServer == null) {
    +            logDebug(""Creating sasl Server"")
    +            connection.sparkSaslServer = new SparkSaslServer(securityManager)
    +          }
    +        }
    +        replyToken = connection.sparkSaslServer.response(securityMsg.getToken)
    +        if (connection.isSaslComplete()) {
    +          logDebug(""Server sasl completed: "" + connection.connectionId) 
    +        } else {
    +          logDebug(""Server sasl not completed: "" + connection.connectionId)
    +        }
    +        if (replyToken != null) {
    +          var securityMsgResp = SecurityMessage.fromResponse(replyToken, securityMsg.getConnectionId)
    +          var message = securityMsgResp.toBufferMessage
    +          if (message == null) throw new Exception(""Error creating security Message"")
    +          sendSecurityMessage(connection.getRemoteConnectionManagerId(), message)
    +        } 
    +      } catch {
    +        case e: Exception => {
    +          logError(""Error in server auth negotiation: "" + e)
    +          // It would probably be better to send an error message telling other side auth failed
    +          // but for now just close
    +          connection.close()
    +        }
    +      }
    +    } else {
    +      logDebug(""connection already established for this connection id: "" + connection.connectionId) 
    +    }
    +  }
    +
    +
    +  private def handleAuthentication(conn: Connection, bufferMessage: BufferMessage): Boolean = {
    +    if (bufferMessage.isSecurityNeg) {
    +      logDebug(""This is security neg message"")
    +
    +      // parse as SecurityMessage
    +      val securityMsg = SecurityMessage.fromBufferMessage(bufferMessage)
    +      val connectionId = new ConnectionId(securityMsg.getConnectionId)
    +
    +      connectionsAwaitingSasl.get(connectionId) match {
    +        case Some(waitingConn) => {
    +          // Client - this must be in response to us doing Send
    +          logDebug(""Client handleAuth for id: "" +  waitingConn.connectionId)
    +          handleClientAuthNeg(waitingConn, securityMsg, connectionId)
    +        }
    +        case None => {
    +          // Server - someone sent us something and we haven't authenticated yet
    +          logDebug(""Server handleAuth for id: "" + connectionId)
    +          handleServerAuthNeg(conn, securityMsg, connectionId)
    +        }
    +      }
    --- End diff --
    
    Not sure I follow. In both cases we are receiving messages on a ReceivingConnection so there is no way to differentiate on the connection it was sent on.  The type stored in connectionsAwaitingSasl is only SendingConnections.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Ethan Jewett <esjewett@gmail.com>,"Thu, 20 Feb 2014 14:15:32 -0600",Re: Signal/Noise Ratio,dev@spark.incubator.apache.org,"Is there anything stopping us from using a different list, segregated from
the dev list? The Github emails significantly reduce the signal-noise ratio
of this list, and while it is possible (but annoying) to filter them out in
our individual inboxes, it makes the archives of the list much less usable
in many ways.



"
Konstantin Boudnik <cos@apache.org>,"Thu, 20 Feb 2014 12:18:56 -0800","Re: Fwd: ASF Board Meeting Summary - February 19, 2014",dev@spark.incubator.apache.org,"Weeeee! ;)


"
Andy Konwinski <andykonwinski@gmail.com>,"Thu, 20 Feb 2014 12:36:08 -0800",Re: Signal/Noise Ratio,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","That is a very valid point about the list archives (which a mail filter
doesn't address and which impacts the community in a negative way).

As of today we are a Top Level Project so I think we have a little more
autonomy for this sort of dev vs separate list decision.



"
Nan Zhu <zhunanmcgill@gmail.com>,"Thu, 20 Feb 2014 15:47:00 -0500","Re: ASF Board Meeting Summary - February 19, 2014",dev@spark.incubator.apache.org,"Congratulations to all! 

-- 
Nan Zhu




"
"""Mattmann, Chris A (3980)"" <chris.a.mattmann@jpl.nasa.gov>","Thu, 20 Feb 2014 20:51:37 +0000",Re: Signal/Noise Ratio,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Guys,

Whether you are a TLP or not the big key here is making sure that
dev discussion does not happen elsewhere outside of the list. You
can create e.g., a github-dev@spark.a.o list, but you will need
to make sure that:

a) if dev discussion is happening there that it gets flowed up to
dev@spark.a.o. All development discussion must appear on the dev
list and must be traceable as a project discussion and decisions
appear on the list(s).

b) automated/etc. email is simply that, and there isn't a ton of
discussion going on on those github emails, and that it's mostly
going on on the dev@spark.a.o list.

If you can meet those 2 criteria/litmus test, I think it's fine.
The big concern is that if the discussion is not happening elsewhere,
then the decisions make for Apache Spark are based on information
that isn't co-located with the Apache Spark project. So that's the
thing that the PMC needs to keep in mind (note I said PMC now, yay!) :)

Cheers and just keep the above in mind and you'll be good.

Cheers,
Chris






"
Andrew Ash <andrew@andrewash.com>,"Thu, 20 Feb 2014 13:01:07 -0800",Re: Signal/Noise Ratio,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I'm fine with keeping the GitHub traffic if we can

a) take away the Jenkins build started / build finished / build succeeded /
build failed messages.  Those aren't ""dev discussion"" and are very noisy.
 I don't think they help anyone, and people who care about those for a
particular PR (because they're a reviewer or author on it) are already
subscribed through GitHub.
b) change the format of the emails that are sent out; I find them very
poorly formatted.  I'd prefer no deep tab for the message.

http://mail-archives.apache.org/mod_mbox/incubator-spark-dev/201402.mbox/%3C20140210192901.CE834922554@tyr.zones.apache.org%3E

FWIW I'm filtering all emails from git@git.apache.org straight to trash
right now because of the noise.



"
Ethan Jewett <esjewett@gmail.com>,"Thu, 20 Feb 2014 15:07:20 -0600",Re: Signal/Noise Ratio,dev@spark.incubator.apache.org,"That would be fine. I would just like the problem fixed. The list has gone
from being a consistently pretty interesting and content-heavy read to
being a trudge to go through and attempt to extract the relevant
information from every day.



"
andrewor14 <git@git.apache.org>,"Thu, 20 Feb 2014 21:32:04 +0000 (UTC)",[GitHub] incubator-spark pull request: Fix ExternalMap on case of key's has...,dev@spark.incubator.apache.org,"Github user andrewor14 commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/612#discussion_r9925460
  
           (3, Set[Int](30))))
       }
     
    +  test(""insert with collision on hashCode Int.MaxValue"") {
    +    val conf = new SparkConf(false)
    +    sc = new SparkContext(""local"", ""test"", conf)
    +
    --- End diff --
    
    Looks like this test currently does not induce spilling. I would set up the memory constraints as follows:
    
    val conf = new SparkConf()
    conf.set(""spark.shuffle.memoryFraction"", ""0.001"")
    sc = new SparkContext(""local-cluster[1,1,512]"", ""test"", conf)
    
    (Note that in this test it is crucial for SparkConf to take in no arguments. This is a workaround for the hacky way we currently pass in environment variables in the tests)


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
andrewor14 <git@git.apache.org>,"Thu, 20 Feb 2014 21:36:11 +0000 (UTC)",[GitHub] incubator-spark pull request: Fix ExternalMap on case of key's has...,dev@spark.incubator.apache.org,"Github user andrewor14 commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/612#discussion_r9925630
  
           (3, Set[Int](30))))
       }
     
    +  test(""insert with collision on hashCode Int.MaxValue"") {
    +    val conf = new SparkConf(false)
    +    sc = new SparkContext(""local"", ""test"", conf)
    +
    +      mergeValue, mergeCombiners)
    +
    +    map.insert(Int.MaxValue, 10)
    +    map.insert(2, 20)
    +    map.insert(3, 30)
    +    map.insert(Int.MaxValue, 100)
    +    map.insert(2, 200)
    +    map.insert(Int.MaxValue, 1000)
    +    val it = map.iterator
    +    assert(it.hasNext)
    +    val result = it.toSet[(Int, ArrayBuffer[Int])].map(kv => (kv._1, kv._2.toSet))
    +    assert(result == Set[(Int, Set[Int])](
    +      (Int.MaxValue, Set[Int](10, 100, 1000)),
    +      (2, Set[Int](20, 200)),
    +      (3, Set[Int](30))))
    --- End diff --
    
    Even after setting the memory parameters, we still need to insert a lot into the map to induce spilling. I have been able to trigger the exception that you found with the following:
    
    (1 until 100000).foreach { i => map.insert(i, i) }
    map.insert(Int.MaxValue, Int.MaxValue)
    
    val it = map.iterator
    while (it.hasNext) {
      it.next()
    }


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
andrewor14 <git@git.apache.org>,"Thu, 20 Feb 2014 21:39:08 +0000 (UTC)",[GitHub] incubator-spark pull request: Fix ExternalMap on case of key's has...,dev@spark.incubator.apache.org,"Github user andrewor14 commented on the pull request:

    https://github.com/apache/incubator-spark/pull/612#issuecomment-35672119
  
    Thanks again for finding this bug. A number of users have reported it before and we had not been able to provide a good answer, but this patch pretty much explains it.
    
    I have left some comments regarding the corresponding test. In particular, the one you have now does not induce spilling, and so does not trigger the exception that you ran into. I left an example of how to make this happen.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
andrewor14 <git@git.apache.org>,"Thu, 20 Feb 2014 21:40:21 +0000 (UTC)",[GitHub] incubator-spark pull request: Fix ExternalMap on case of key's has...,dev@spark.incubator.apache.org,"Github user andrewor14 commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/612#discussion_r9925806
  
           (3, Set[Int](30))))
       }
     
    +  test(""insert with collision on hashCode Int.MaxValue"") {
    --- End diff --
    
    I would rename this ""spilling with..."" instead of ""insert with..."". More details explained below.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
tgravescs <git@git.apache.org>,"Thu, 20 Feb 2014 21:52:10 +0000 (UTC)","[GitHub] incubator-spark pull request: Add Security to Spark - Akka, Http, ...",dev@spark.incubator.apache.org,"Github user tgravescs commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/332#discussion_r9926321
  
    --- Diff: core/src/main/scala/org/apache/spark/SecurityManager.scala ---
    @@ -0,0 +1,112 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * ""License""); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.spark
    +
    +import org.apache.hadoop.io.Text
    +import org.apache.hadoop.security.Credentials
    +import org.apache.hadoop.security.UserGroupInformation
    +
    +import org.apache.spark.deploy.SparkHadoopUtil
    +
    +/** 
    + * Spark class responsible for security.  
    + */
    +private[spark] class SecurityManager extends Logging {
    +
    +  private val viewAcls = System.getProperty(""spark.ui.view.acls"", """").split(',').map(_.trim()).toSet
    +  private val secretKey = generateSecretKey()
    + 
    +  /**
    +   * In Yarn mode it uses Hadoop UGI to pass the secret as that
    +   * will keep it protected.  For a standalone SPARK cluster
    +   * use a environment variable SPARK_SECRET to specify the secret.
    +   * This probably isn't ideal but only the user who starts the process
    +   * should have access to view the variable (at least on Linux).
    +   * Since we can't set the environment variable we set the 
    +   * java system property SPARK_SECRET so it will automatically
    +   * generate a secret is not specified.  This definitely is not
    +   * ideal since users can see it. We should switch to put it in 
    +   * a config.
    +   */
    +  private def generateSecretKey(): String = {
    +
    +    if (!isAuthenticationEnabled) return null
    +    // first check to see if secret already set, else generate it
    +    if (SparkHadoopUtil.get.isYarnMode) {
    +      val credentials = SparkHadoopUtil.get.getCurrentUserCredentials()
    +      if (credentials != null) { 
    +        val secretKey = credentials.getSecretKey(new Text(""akkaCookie""))
    +        if (secretKey != null) {
    +          logDebug(""in yarn mode, getting secret from credentials"")
    +          return new Text(secretKey).toString
    +        } else {
    +          logDebug(""getSecretKey: yarn mode, secret key from credentials is null"")
    +        }
    +      } else {
    +        logDebug(""getSecretKey: yarn mode, credentials are null"")
    +      }
    +    }
    +    val secret = System.getProperty(""SPARK_SECRET"", System.getenv(""SPARK_SECRET"")) 
    +    if (secret != null && !secret.isEmpty()) return secret 
    +    // generate one 
    +    val sCookie = akka.util.Crypt.generateSecureCookie
    +
    +    // if we generate we must be the first so lets set it so its used by everyone else
    +    if (SparkHadoopUtil.get.isYarnMode) {
    +      val creds = new Credentials()
    +      creds.addSecretKey(new Text(""akkaCookie""), sCookie.getBytes())
    +      SparkHadoopUtil.get.addCurrentUserCredentials(creds)
    +      logDebug(""adding secret to credentials yarn mode"")
    +    } else {
    +      System.setProperty(""SPARK_SECRET"", sCookie)
    +      logDebug(""adding secret to java property"")
    +    }
    +    return sCookie
    --- End diff --
    
    The problem here is that it is not really how it does it between hadoop versions, which is what I think of SparkHadoopUtil being used for.  Its either Yarn deployed or its for instance standalone deploy.   We can move a bit of the logic into SparkHadoopUtil, like the code inside of the isYarnMode blocks, but we would still need the yarn check or abstract that out somewhere else.
    
    is sounds like we are going to want to add better support for this for the standalone deploy so for now I suggest we leave this as is until we get a better idea of how that is going to work.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
ahirreddy <git@git.apache.org>,"Thu, 20 Feb 2014 22:35:49 +0000 (UTC)",[GitHub] incubator-spark pull request: Allow PySpark to use existing JVM an...,dev@spark.incubator.apache.org,"GitHub user ahirreddy opened a pull request:

    https://github.com/apache/incubator-spark/pull/622

    Allow PySpark to use existing JVM and Gateway

    Patch to allow PySpark to use existing JVM and Gateway. Changes to PySpark implementation of SparkConf to take existing SparkConf JVM handle. Change to PySpark SparkContext to allow subclass specific context initialization.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/ahirreddy/incubator-spark pyspark-existing-jvm

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/622.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #622
    
----
commit a86f45721c7f009adb3fc7070d3641569e999ffd
Author: Ahir Reddy <ahirreddy@gmail.com>
Date:   2014-02-20T22:13:40Z

    Patch to allow PySpark to use existing JVM and Gateway. Changes to
    PySpark implementation of SparkConf to take existing SparkConf JVM
    handle. Change to PySpark SparkContext to allow subclass specific
    context initialization.

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 20 Feb 2014 22:38:00 +0000 (UTC)",[GitHub] incubator-spark pull request: Allow PySpark to use existing JVM an...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/622#issuecomment-35678137
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 20 Feb 2014 22:38:00 +0000 (UTC)",[GitHub] incubator-spark pull request: Allow PySpark to use existing JVM an...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/622#issuecomment-35678138
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Thu, 20 Feb 2014 23:00:36 +0000 (UTC)","[GitHub] incubator-spark pull request: doctest updated for mapValues, flatM...",dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/621#issuecomment-35680133
  
    Jenkins, test this please.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 20 Feb 2014 23:06:40 +0000 (UTC)",[GitHub] incubator-spark pull request: Allow PySpark to use existing JVM an...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/622#issuecomment-35680690
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12787/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 20 Feb 2014 23:06:41 +0000 (UTC)",[GitHub] incubator-spark pull request: Allow PySpark to use existing JVM an...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/622#issuecomment-35680688
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 20 Feb 2014 23:07:11 +0000 (UTC)","[GitHub] incubator-spark pull request: doctest updated for mapValues, flatM...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/621#issuecomment-35680739
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 20 Feb 2014 23:07:11 +0000 (UTC)","[GitHub] incubator-spark pull request: doctest updated for mapValues, flatM...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/621#issuecomment-35680738
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Thu, 20 Feb 2014 23:18:39 +0000 (UTC)",[GitHub] incubator-spark pull request: Fix ExternalMap on case of key's has...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/612#issuecomment-35681623
  
    @guojc - hey since Andrew may propose a slightly different fix, I want to make sure you are credited with this in our release notes. Are you Jiacheng Guo? Found this name looking at twitter.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
andrewor14 <git@git.apache.org>,"Thu, 20 Feb 2014 23:23:28 +0000 (UTC)",[GitHub] incubator-spark pull request: Fix ExternalMap on case of key's has...,dev@spark.incubator.apache.org,"Github user andrewor14 commented on the pull request:

    https://github.com/apache/incubator-spark/pull/612#issuecomment-35681966
  
    @guojc Hey, we discussed about this a little more and we thought of a different way of solving this that also simplifies some of the existing logic. I will create a new PR and make the appropriate changes there.
    
    This is a serious bug and we intend to add it back into the 0.9 release. Thanks and I'll be sure to include you in the credits.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Patrick Wendell <pwendell@gmail.com>,"Thu, 20 Feb 2014 15:38:30 -0800",Re: Signal/Noise Ratio,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I'd personally like to see this go to a separate list.

Until then I'd strongly recommended using filters to get rid of them.
In gmail it's trivial...


"
Patrick Wendell <pwendell@gmail.com>,"Thu, 20 Feb 2014 15:40:37 -0800",Re: Problem with akka.frameSize,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Thanks for this bug report... we'll look into this!


m

2
t
e
"
Xiangrui Meng <mengxr@gmail.com>,"Thu, 20 Feb 2014 15:44:23 -0800","Re: [GitHub] incubator-spark pull request: MLLIB-24: url of
 ""Collaborative Filt...",dev@spark.incubator.apache.org,"Just want to test whether this message will be forwarded to github. -Xiangrui


"
aarondav <git@git.apache.org>,"Thu, 20 Feb 2014 23:45:53 +0000 (UTC)",[GitHub] incubator-spark pull request: Super minor: Add require for mergeCo...,dev@spark.incubator.apache.org,"GitHub user aarondav opened a pull request:

    https://github.com/apache/incubator-spark/pull/623

    Super minor: Add require for mergeCombiners in combineByKey

    We changed the behavior in 0.9.0 from requiring that mergeCombiners be null when mapSideCombine was false to requiring that mergeCombiners *never* be null, for external sorting. This patch adds a require() to make this behavior change explicitly messaged rather than resulting in a NPE.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/aarondav/incubator-spark master

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/623.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #623
    
----
commit 520b80c7bef100e7b1c2b0fb6388569ac0335681
Author: Aaron Davidson <aaron@databricks.com>
Date:   2014-02-20T23:41:20Z

    Super minor: Add require for mergeCombiners in combineByKey
    
    We changed the behavior in 0.9.0 from requiring that mergeCombiners
    be null when mapSideCombine was false to requiring that mergeCombiners
    *never* be null, for external sorting. This patch adds a require()
    to make this behavior change explicitly messaged rather than resulting in
    a NPE.

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Thu, 20 Feb 2014 23:46:21 +0000 (UTC)",[GitHub] incubator-spark pull request: Super minor: Add require for mergeCo...,dev@spark.incubator.apache.org,"Github user aarondav commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/623#discussion_r9930713
  
    --- Diff: core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala ---
    @@ -77,6 +77,7 @@ class PairRDDFunctions[K: ClassTag, V: ClassTag](self: RDD[(K, V)])
           partitioner: Partitioner,
           mapSideCombine: Boolean = true,
           serializerClass: String = null): RDD[(K, C)] = {
    +    require(mergeCombiners != null, ""mergeCombiners must be defined"") // required as of Spark 0.9.0
    --- End diff --
    
    fyi: this line is <100ch.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
sryza <git@git.apache.org>,"Thu, 20 Feb 2014 23:54:32 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-929: Fully deprecate usage of ...,dev@spark.incubator.apache.org,"Github user sryza commented on the pull request:

    https://github.com/apache/incubator-spark/pull/615#issuecomment-35684281
  
    Right. What I mean is that calling the variable SPARK_DRIVER_MEMORY might be confusing in the context of yarn-standalone because its value would apply to the client and not the driver (if that's the right terminology).  Would SPARK_CLIENT_MEMORY possibly make more sense?



---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
sryza <git@git.apache.org>,"Thu, 20 Feb 2014 23:54:42 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-929: Fully deprecate usage of ...,dev@spark.incubator.apache.org,"Github user sryza commented on the pull request:

    https://github.com/apache/incubator-spark/pull/615#issuecomment-35684291
  
    Right. What I mean is that calling the variable SPARK_DRIVER_MEMORY might be confusing in the context of yarn-standalone because its value would apply to the client and not the driver (if that's the right terminology).  Would SPARK_CLIENT_MEMORY possibly make more sense?



---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
sryza <git@git.apache.org>,"Thu, 20 Feb 2014 23:54:43 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-929: Fully deprecate usage of ...,dev@spark.incubator.apache.org,"Github user sryza commented on the pull request:

    https://github.com/apache/incubator-spark/pull/615#issuecomment-35684293
  
    Right. What I mean is that calling the variable SPARK_DRIVER_MEMORY might be confusing in the context of yarn-standalone because its value would apply to the client and not the driver (if that's the right terminology).  Would SPARK_CLIENT_MEMORY possibly make more sense?



---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Xiangrui Meng <mengxr@gmail.com>,"Thu, 20 Feb 2014 15:57:03 -0800",Re: Signal/Noise Ratio,dev@spark.incubator.apache.org,"+1 If someone replies a github thread via spark-dev, it won't show up
on github and it gets filtered by most people. -Xiangrui


"
pwendell <git@git.apache.org>,"Thu, 20 Feb 2014 23:57:25 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-929: Fully deprecate usage of ...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/615#issuecomment-35684474
  
    I think SPARK_CLIENT_MEMORY isn't so hot either because most often `spark-class` isn't used to run a client, it's most often used by users to run examples. Maybe SPARK_CLASS_MEMORY? @asfgit 


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Fri, 21 Feb 2014 00:01:16 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-929: Fully deprecate usage of ...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/615#issuecomment-35684716
  
    Hm actually sorry that was totally wrong. Who uses this script externally at all? Why don't we just _not_ document this...


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 00:07:17 +0000 (UTC)",[GitHub] incubator-spark pull request: Super minor: Add require for mergeCo...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/623#issuecomment-35685132
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 00:07:17 +0000 (UTC)",[GitHub] incubator-spark pull request: Super minor: Add require for mergeCo...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/623#issuecomment-35685133
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 00:35:10 +0000 (UTC)",[GitHub] incubator-spark pull request: Super minor: Add require for mergeCo...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/623#issuecomment-35686975
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12789/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 00:35:10 +0000 (UTC)",[GitHub] incubator-spark pull request: Super minor: Add require for mergeCo...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/623#issuecomment-35686973
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Fri, 21 Feb 2014 00:45:09 +0000 (UTC)",[GitHub] incubator-spark pull request: Super minor: Add require for mergeCo...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/623#issuecomment-35687583
  
    Thanks aaron looks good. I'll merge this into master and 0.9.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
asfgit <git@git.apache.org>,"Fri, 21 Feb 2014 00:54:03 +0000 (UTC)",[GitHub] incubator-spark pull request: Super minor: Add require for mergeCo...,dev@spark.incubator.apache.org,"Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-spark/pull/623


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Fri, 21 Feb 2014 01:17:16 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/585#discussion_r9933131
  
    --- Diff: project/MimaBuild.scala ---
    @@ -0,0 +1,105 @@
    +import com.typesafe.tools.mima.plugin.MimaKeys.{binaryIssueFilters, previousArtifact}
    +import com.typesafe.tools.mima.plugin.MimaPlugin.mimaDefaultSettings
    +
    +object MimaBuild {
    +
    +  val ignoredABIProblems = {
    +    import com.typesafe.tools.mima.core._
    +    import com.typesafe.tools.mima.core.ProblemFilters._
    +    /**
    +     * A: Detections are semi private or likely to become semi private at some point.
    +     */
    +    Seq(exclude[MissingClassProblem](""org.apache.spark.util.XORShiftRandom""),
    +      exclude[MissingClassProblem](""org.apache.spark.util.XORShiftRandom$""),
    +      exclude[MissingMethodProblem](""org.apache.spark.util.Utils.cloneWritables""),
    +      // Scheduler is not considered a public API.
    +      excludePackage(""org.apache.spark.deploy""),
    +      // Was made private in 1.0
    --- End diff --
    
    It's sort of a hack but you can exclude these as packages like this:
    
    ```
    ```


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Fri, 21 Feb 2014 01:18:53 +0000 (UTC)",[GitHub] incubator-spark pull request: Deprecated and added a few java api ...,dev@spark.incubator.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/402#discussion_r9933178
  
    --- Diff: core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala ---
    @@ -73,7 +74,7 @@ trait JavaRDDLike[T, This <: JavaRDDLike[T, This]] extends Serializable {
        * of the original partition.
        */
       def mapPartitionsWithIndex[R: ClassTag](
    -      f: JFunction2[Int, java.util.Iterator[T], java.util.Iterator[R]],
    +      f: JFunction2[Integer, java.util.Iterator[T], java.util.Iterator[R]],
    --- End diff --
    
    yep, could you do this?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Fri, 21 Feb 2014 01:56:12 +0000 (UTC)",[GitHub] incubator-spark pull request: Deprecated and added a few java api ...,dev@spark.incubator.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/402#discussion_r9934007
  
    --- Diff: core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala ---
    @@ -73,7 +74,7 @@ trait JavaRDDLike[T, This <: JavaRDDLike[T, This]] extends Serializable {
        * of the original partition.
        */
       def mapPartitionsWithIndex[R: ClassTag](
    -      f: JFunction2[Int, java.util.Iterator[T], java.util.Iterator[R]],
    +      f: JFunction2[Integer, java.util.Iterator[T], java.util.Iterator[R]],
    --- End diff --
    
    @ScrapCodes I think @rxin is suggesting that you should actually write `java.lang.Integer` to make it more explicit.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Fri, 21 Feb 2014 01:56:30 +0000 (UTC)",[GitHub] incubator-spark pull request: Allow PySpark to use existing JVM an...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/622#issuecomment-35691749
  
    @ahirreddy Could you create a JIRA for this? Thanks


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
ahirreddy <git@git.apache.org>,"Fri, 21 Feb 2014 02:04:45 +0000 (UTC)",[GitHub] incubator-spark pull request: Allow PySpark to use existing JVM an...,dev@spark.incubator.apache.org,"Github user ahirreddy commented on the pull request:

    https://github.com/apache/incubator-spark/pull/622#issuecomment-35692179
  
    https://spark-project.atlassian.net/browse/SPARK-1114


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
guojc <git@git.apache.org>,"Fri, 21 Feb 2014 02:44:40 +0000 (UTC)",[GitHub] incubator-spark pull request: Fix ExternalMap on case of key's has...,dev@spark.incubator.apache.org,"Github user guojc commented on the pull request:

    https://github.com/apache/incubator-spark/pull/612#issuecomment-35694101
  
    Yes, I'm Jiacheng Guo. It's ok if you can find another good solution for this bug. Thanks for your guy's work.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
guojc <git@git.apache.org>,"Fri, 21 Feb 2014 02:44:41 +0000 (UTC)",[GitHub] incubator-spark pull request: Fix ExternalMap on case of key's has...,dev@spark.incubator.apache.org,"Github user guojc closed the pull request at:

    https://github.com/apache/incubator-spark/pull/612


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
andrewor14 <git@git.apache.org>,"Fri, 21 Feb 2014 03:17:40 +0000 (UTC)",[GitHub] incubator-spark pull request: External spilling - fix Int.MaxValue...,dev@spark.incubator.apache.org,"GitHub user andrewor14 opened a pull request:

    https://github.com/apache/incubator-spark/pull/624

    External spilling - fix Int.MaxValue hash code collision bug

    The original poster of this bug is @guojc, who opened a PR that preceded this one at https://github.com/apache/incubator-spark/pull/612.
    
    
    The fix is to maintain the invariant that empty buffer streams are never added back to the queue to be considered. This guarantees that we never read from an empty buffer stream, ever again.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/andrewor14/incubator-spark spilling-bug

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/624.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #624
    
----
commit 21c1a39ebe7d0f10519621f3ef54aa6e89c08441
Author: Andrew Or <andrewor14@gmail.com>
Date:   2014-02-21T02:04:27Z

    
    As of now, the test ""spilling with hash collisions using the Int.MaxValue key"" fails.
    Fixing this behavior is the main goal of this PR.

commit c11f03b6e6e82617a826dc3acbd09a52760f143b
Author: Andrew Or <andrewor14@gmail.com>
Date:   2014-02-21T02:58:18Z

    
    The solution is to maintain the invariant that mergeHeap contains only non-empty
    StreamBuffer's at the time next() is called.

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 03:17:58 +0000 (UTC)",[GitHub] incubator-spark pull request: External spilling - fix Int.MaxValue...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/624#issuecomment-35695580
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 03:17:58 +0000 (UTC)",[GitHub] incubator-spark pull request: External spilling - fix Int.MaxValue...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/624#issuecomment-35695581
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
ScrapCodes <git@git.apache.org>,"Fri, 21 Feb 2014 03:44:26 +0000 (UTC)",[GitHub] incubator-spark pull request: Deprecated and added a few java api ...,dev@spark.incubator.apache.org,"Github user ScrapCodes commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/402#discussion_r9935572
  
    --- Diff: core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala ---
    @@ -73,7 +74,7 @@ trait JavaRDDLike[T, This <: JavaRDDLike[T, This]] extends Serializable {
        * of the original partition.
        */
       def mapPartitionsWithIndex[R: ClassTag](
    -      f: JFunction2[Int, java.util.Iterator[T], java.util.Iterator[R]],
    +      f: JFunction2[Integer, java.util.Iterator[T], java.util.Iterator[R]],
    --- End diff --
    
    Well I think there was a context to that comment, Earlier right above that
    we were importing java.lang.Integer and then using it. He asked me to
    remove and make it java.lang.Integer, but later I discovered import was not
    necessary. But if you think having that specified explicitly is good, it
    can be done in a moment ?
    
    
    
    > In core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala:
    >
    > > @@ -73,7 +74,7 @@ trait JavaRDDLike[T, This <: JavaRDDLike[T, This]] extends Serializable {
    > >     * of the original partition.
    > >     */
    > >    def mapPartitionsWithIndex[R: ClassTag](
    > > -      f: JFunction2[Int, java.util.Iterator[T], java.util.Iterator[R]],
    > > +      f: JFunction2[Integer, java.util.Iterator[T], java.util.Iterator[R]],
    >
    > @ScrapCodes <https://github.com/ScrapCodes> I think @rxin<https://github.com/rxin>is suggesting that you should actually write
    > java.lang.Integer to make it more explicit.
    >
    > --
    > Reply to this email directly or view it on GitHub<https://github.com/apache/incubator-spark/pull/402/files#r9934007>
    > .
    >
    
    
    
    -- 
    Prashant


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Patrick Wendell <pwendell@gmail.com>,"Thu, 20 Feb 2014 20:03:46 -0800",[DISCUSS] Necessity of Maven *and* SBT Build in Spark,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey All,

It's very high overhead having two build systems in Spark. Before
getting into a long discussion about the merits of sbt vs maven, I
wanted to pose a simple question to the dev list:

Is there anyone who feels that dropping either sbt or maven would have
a major consequence for them?

And I say ""major consequence"" meaning something becomes completely
impossible now and can't be worked around. This is different from an
""inconvenience"", i.e., something which can be worked around but will
require some investment.

I'm posing the question in this way because, if there are features in
either build system that are absolutely-un-available in the other,
then we'll have to maintain both for the time being. I'm merely trying
to see whether this is the case...

- Patrick

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 04:06:53 +0000 (UTC)",[GitHub] incubator-spark pull request: External spilling - fix Int.MaxValue...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/624#issuecomment-35697471
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12790/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 04:06:53 +0000 (UTC)",[GitHub] incubator-spark pull request: External spilling - fix Int.MaxValue...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/624#issuecomment-35697469
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Mark Hamstra <mark@clearstorydata.com>,"Thu, 20 Feb 2014 20:16:11 -0800",Re: [DISCUSS] Necessity of Maven *and* SBT Build in Spark,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Is dropping Maven an option, or must we have it to comply with the Apache
release process?



"
Henry Saputra <henry.saputra@gmail.com>,"Thu, 20 Feb 2014 20:50:44 -0800",Re: [DISCUSS] Necessity of Maven *and* SBT Build in Spark,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Thanks for bringing back the build systems discussions, Patrick.
There was a long discussion way back before Spark joining ASF and as I
remember there has not been clear ""winner"" between using sbt or maven.

Maven makes it easier to publish the artifacts to Nexus repository,
not sure if sbt can do  the same, and as I remember one of the
limitations or drawbacks about maven is the use of profiles.
Matei had suggested using some kind of Hadoop client detection as in
Parquet project to manage the Hadoop versions to avoid profiles.


- Henry


"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 05:07:37 +0000 (UTC)",[GitHub] incubator-spark pull request: External spilling - fix Int.MaxValue...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/624#issuecomment-35699690
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 05:07:37 +0000 (UTC)",[GitHub] incubator-spark pull request: External spilling - fix Int.MaxValue...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/624#issuecomment-35699691
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 05:10:23 +0000 (UTC)",[GitHub] incubator-spark pull request: External spilling - fix Int.MaxValue...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/624#issuecomment-35699761
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 05:10:23 +0000 (UTC)",[GitHub] incubator-spark pull request: External spilling - fix Int.MaxValue...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/624#issuecomment-35699760
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Henry Saputra <henry.saputra@gmail.com>,"Thu, 20 Feb 2014 21:11:35 -0800",Re: Signal/Noise Ratio,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Daniel Gruno from ASF infra mentioned this when adding Github plugin
to dev@ list support :
""We may, in the future, add the possibility to filter out certain
comments from being relayed to the ML (such as jenkins workflows etc),
but this will all depend on how this initial phase goes along.""

Looks like for Apache Spark we need ability to filter comments from Jenkins.

So if we could ""filter"" the Jenkins comment fro being sent to dev@
list would this help reduce the noise?

- Henry


"
Patrick Wendell <pwendell@gmail.com>,"Thu, 20 Feb 2014 21:18:54 -0800",Re: [DISCUSS] Necessity of Maven *and* SBT Build in Spark,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey Henry,

Yep, I wanted to reboot this since some time has passed and people may
have new or changed ways of using the build.

Maven makes the Apache publishing fairly seamless, but after the last
two releases I believe we could make it work with sbt as well. sbt
also supports publishing and other Apache projects such as Kafka
publish with sbt.


"
mateiz <git@git.apache.org>,"Fri, 21 Feb 2014 05:21:45 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1114: Allow PySpark to use exi...,dev@spark.incubator.apache.org,"Github user mateiz commented on the pull request:

    https://github.com/apache/incubator-spark/pull/622#issuecomment-35700134
  
    Looks good, I've merged this in.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
ahirreddy <git@git.apache.org>,"Fri, 21 Feb 2014 05:27:02 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1114: Allow PySpark to use exi...,dev@spark.incubator.apache.org,"Github user ahirreddy commented on the pull request:

    https://github.com/apache/incubator-spark/pull/622#issuecomment-35700324
  
    Great! Thanksâ€”
    Sent from Mailbox for iPhone
    
    
    > Looks good, I've merged this in.
    > ---
    > Reply to this email directly or view it on GitHub:
    > https://github.com/apache/incubator-spark/pull/622#issuecomment-35700134


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Fri, 21 Feb 2014 05:31:20 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1111: URL Validation Throws Er...,dev@spark.incubator.apache.org,"GitHub user pwendell opened a pull request:

    https://github.com/apache/incubator-spark/pull/625

    SPARK-1111: URL Validation Throws Error for HDFS URL's

    Fixes an error where HDFS URL's cause an exception. Should be merged into master and 0.9.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/pwendell/incubator-spark url-validation

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/625.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #625
    
----
commit fa25ee2b02aa5b4518e76938cce71aad7239ba31
Author: Patrick Wendell <pwendell@gmail.com>
Date:   2014-02-21T05:29:19Z

    SPARK-1111: URL Validation Throws Error for HDFS URL's

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 05:33:00 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1111: URL Validation Throws Er...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/625#issuecomment-35700516
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 05:33:00 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1111: URL Validation Throws Er...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/625#issuecomment-35700515
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
asfgit <git@git.apache.org>,"Fri, 21 Feb 2014 05:36:08 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1114: Allow PySpark to use exi...,dev@spark.incubator.apache.org,"Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-spark/pull/622


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
CodingCat <git@git.apache.org>,"Fri, 21 Feb 2014 05:44:08 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1100] prevent Spark from over...,dev@spark.incubator.apache.org,"GitHub user CodingCat opened a pull request:

    https://github.com/apache/incubator-spark/pull/626

    [SPARK-1100] prevent Spark from overwriting directory silently and leaving dirty directory

    Thanks for Diana Carroll to report this issue
    
    the current saveAsTextFile/SequenceFile will overwrite the output directory silently if the directory already exists, this behaviour is not desirable because
    
    1. overwriting the data silently is not user-friendly
    
    2. if the partition number of two writing operation changed, then the output directory will contain the results generated by two runnings
    
    My fix includes:
    
    1. add some new APIs with a flag for users to define whether he/she wants to overwrite the directory:
    
    if the flag is set to true, then the output directory is deleted first and then written into the new data to prevent the output directory contains results from multiple rounds of running; 
    
    if the flag is set to false, Spark will throw an exception if the output directory already exists
    
    2. I didn't change saveNewHadoopAPI because in the new API, the overwrite flag is defined by the implementation of RecordWriter, we don't need to control that in Spark
    
    3. changed JavaAPI part
    
    4. default behaviour is overwriting
    
    -----
    
    Two questions
    
    1. should we deprecate the old APIs without such a flag?
    
    2. I noticed that Spark Streaming also called these APIs, I thought we don't need to change the related part in streaming? @tdas 


You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/CodingCat/incubator-spark SPARK-1100

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/626.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #626
    
----
commit 2ec87a1f63b4650036691e5bf5d484aae4e6d470
Author: CodingCat <zhunansjtu@gmail.com>
Date:   2014-02-21T05:32:17Z

    add new APIs to enable users define whether to overwrite the output directory

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 06:06:58 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1111: URL Validation Throws Er...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/625#issuecomment-35701663
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 06:06:58 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1111: URL Validation Throws Er...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/625#issuecomment-35701664
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12793/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 06:07:02 +0000 (UTC)",[GitHub] incubator-spark pull request: External spilling - fix Int.MaxValue...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/624#issuecomment-35701666
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 06:07:02 +0000 (UTC)",[GitHub] incubator-spark pull request: External spilling - fix Int.MaxValue...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/624#issuecomment-35701667
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 06:07:03 +0000 (UTC)",[GitHub] incubator-spark pull request: External spilling - fix Int.MaxValue...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/624#issuecomment-35701669
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12791/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 06:07:03 +0000 (UTC)",[GitHub] incubator-spark pull request: External spilling - fix Int.MaxValue...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/624#issuecomment-35701668
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12792/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 06:10:01 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1100] prevent Spark from over...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/626#issuecomment-35701775
  
    Can one of the admins verify this patch?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
ScrapCodes <git@git.apache.org>,"Fri, 21 Feb 2014 06:11:36 +0000 (UTC)",[GitHub] incubator-spark pull request: Deprecated and added a few java api ...,dev@spark.incubator.apache.org,"Github user ScrapCodes commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/402#discussion_r9937176
  
    --- Diff: core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala ---
    @@ -73,7 +74,7 @@ trait JavaRDDLike[T, This <: JavaRDDLike[T, This]] extends Serializable {
        * of the original partition.
        */
       def mapPartitionsWithIndex[R: ClassTag](
    -      f: JFunction2[Int, java.util.Iterator[T], java.util.Iterator[R]],
    +      f: JFunction2[Integer, java.util.Iterator[T], java.util.Iterator[R]],
    --- End diff --
    
    I am sorry, I think that was some other PR. Going to change this right away. 


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Mridul Muralidharan <mridul@gmail.com>,"Fri, 21 Feb 2014 12:06:03 +0530",Re: [DISCUSS] Necessity of Maven *and* SBT Build in Spark,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I am not sure if this is resolved now - but maven was better at
building the assembly jars compared to sbt.
To the point where I stopped using sbt due to unpredictable order in
which it unjars the dependencies to create the assembled jar (we do
have quite a lot of conflicting classes in our dependency tree :-( ).
I dont know if this is an artifact of how we specify it in sbt
project, or something else ...

If this is still an issue, then using sbt only is a non starter.

Regards,
Mridul





"
mridulm <git@git.apache.org>,"Fri, 21 Feb 2014 06:50:21 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1100] prevent Spark from over...,dev@spark.incubator.apache.org,"Github user mridulm commented on the pull request:

    https://github.com/apache/incubator-spark/pull/626#issuecomment-35703455
  
    Typically, the way this gets done is - write to a temporary directory, taking care of multiple attempts for same partition (failure case)/multiple concurrent executions on same partition (speculative execution case) and once job is done,  move to the desired destination (or delete dir if job fails) - like what mapred does for example.
    (Moves are atomic NN operations).
    
    So when output directory is ""done"", it is fully done : not partially/in progress/etc.
    Particularly the bug mentioned - of left over files from previous jobs, etc - is just scarey !


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
mateiz <git@git.apache.org>,"Fri, 21 Feb 2014 07:07:28 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user mateiz commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35704145
  
    Hey Prashant, this looks pretty good at first glance. Can you also create a Java 8 version of the Streaming suite?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 07:07:47 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35704161
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 07:07:48 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35704162
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 07:08:07 +0000 (UTC)",[GitHub] incubator-spark pull request: Deprecated and added a few java api ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/402#issuecomment-35704179
  
    Build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 07:08:07 +0000 (UTC)",[GitHub] incubator-spark pull request: Deprecated and added a few java api ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/402#issuecomment-35704178
  
     Build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
mateiz <git@git.apache.org>,"Fri, 21 Feb 2014 07:08:45 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user mateiz commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35704205
  
    Also, can you tell me where those implicits in JavaPairRDD are used? Can't we manually do the conversions there? At the very least, the implicits should be private[spark] so that Java users don't try to call them.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 07:10:36 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35704289
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 07:10:37 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35704292
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12794/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
jyotiska <git@git.apache.org>,"Fri, 21 Feb 2014 07:15:45 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1100] prevent Spark from over...,dev@spark.incubator.apache.org,"Github user jyotiska commented on the pull request:

    https://github.com/apache/incubator-spark/pull/626#issuecomment-35704511
  
    I think it is a good idea to add an extra flag for overwriting. If the flag is not present, Spark should throw an exception. I will see if the bug is also present in PySpark.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 07:23:07 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35704782
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 07:23:07 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35704781
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 07:26:13 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35704911
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12796/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 07:26:13 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35704910
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Fri, 21 Feb 2014 07:28:06 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1111: URL Validation Throws Er...,dev@spark.incubator.apache.org,"Github user aarondav commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/625#discussion_r9938103
  
    --- Diff: core/src/main/scala/org/apache/spark/deploy/ClientArguments.scala ---
    @@ -115,3 +110,7 @@ private[spark] class ClientArguments(args: Array[String]) {
         System.exit(exitCode)
       }
     }
    +
    +object ClientArguments {
    +  def isValidJarUrl(s: String) = s.matches(""^(.+):(.+)jar"")
    --- End diff --
    
    ""^"" is technically not needed since matches is full-string anyway


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Fri, 21 Feb 2014 07:28:20 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1111: URL Validation Throws Er...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/625#issuecomment-35704996
  
    LGTM save the minor regex nit


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Fri, 21 Feb 2014 07:38:12 +0000 (UTC)",[GitHub] incubator-spark pull request: External spilling - fix Int.MaxValue...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/624#issuecomment-35705443
  
    Thanks @guojc and @andrewor14! LGTM. Maybe @pwendell wants to take a look as well?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Fri, 21 Feb 2014 07:50:46 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1111: URL Validation Throws Er...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/625#issuecomment-35705998
  
    @aarondav fixed the nit, waiting for tests.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Sean Owen <sowen@cloudera.com>,"Fri, 21 Feb 2014 07:56:54 +0000",Re: [DISCUSS] Necessity of Maven *and* SBT Build in Spark,dev@spark.incubator.apache.org,"Two builds is indeed a pain, since it's an ongoing chore to keep them
in sync. For example, I am already seeing that the two do not quite
declare the same dependencies (see recent patch).

I think publishing artifacts to Maven central should be considered a
hard requirement if it isn't already one from the ASF, and it may be?
Certainly most people out there would be shocked if you told them
Spark is not in the repo at all. And that requires at least
maintaining a pom that declares the structure of the project.

This does not necessarily mean using Maven to build, but is a reason
that removing the pom is going to make this a lot harder for people to
consume as a project.

Maven has its pros and cons but there are plenty of people lurking
around who know it quite well. Certainly it's easier for the Hadoop
Scala although only via a plugin, which is weaker support. sbt seems
like a fairly new, basic, ad-hoc tool. Is there an advantage to it,
other than being Scala (which is an advantage)?

--
Sean Owen | Director, Data Science | London



"
pwendell <git@git.apache.org>,"Fri, 21 Feb 2014 08:00:42 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1111: URL Validation Throws Er...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/625#issuecomment-35706454
  
    Jenkins, test this please.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 08:06:58 +0000 (UTC)",[GitHub] incubator-spark pull request: Deprecated and added a few java api ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/402#issuecomment-35706774
  
    Build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 08:06:59 +0000 (UTC)",[GitHub] incubator-spark pull request: Deprecated and added a few java api ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/402#issuecomment-35706775
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12795/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 08:08:00 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1111: URL Validation Throws Er...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/625#issuecomment-35706836
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 08:08:01 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1111: URL Validation Throws Er...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/625#issuecomment-35706835
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 08:08:09 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35706840
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 08:08:09 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35706841
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 08:09:24 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35706916
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12798/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 08:09:24 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35706914
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
ScrapCodes <git@git.apache.org>,"Fri, 21 Feb 2014 08:20:14 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user ScrapCodes commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35707482
  
    These implicits are used in JavaPairRDD itself, JavaRDDLike and JavaDStreamLike. The reason they were used is the number of occurrence for this trivial wrap is high.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Paul Brown <prb@mult.ifario.us>,"Fri, 21 Feb 2014 00:28:55 -0800",Re: [DISCUSS] Necessity of Maven *and* SBT Build in Spark,dev@spark.incubator.apache.org,"As a customer of the code, I don't care *how* the code gets built, but it
is important to me that the Maven artifacts (POM files, binaries, sources,
javadocs) are clean, accurate, up to date, and published on Maven Central.

Some examples where structure/publishing failures have been bad for users:

- For a long time (and perhaps still), Solr and Lucene were built by an Ant
build that produced incorrect POMs and required potential developers to
manually configure their IDEs.

- For a long time (and perhaps still), Pig was built by Ant, published
incorrect POMs, and failed to publish useful auxiliary artifacts like
PigUnit and the PiggyBank as Maven-addressable artifacts.  (That said,
thanks to Spark, we no longer use Pig...)

- For a long time (and perhaps still), Cassandra depended on
non-generally-available libraries (high-scale, etc.) that made it
inconvenient to embed Cassandra in a larger system.  Cassandra gets a
little slack because the build/structure was almost too terrible to look at
prior to incubation and it's gotten better...

And those are just a few projects at Apache that come to mind; I could make
a longish list of offenders.

btw, among other things that the Spark project probably *should* do would
be to publish artifacts with a classifier to distinguish the Hadoop version
linked against.

I'll be a happy user of sbt-built artifacts, or if the project goes/sticks
with Maven I'm more than willing to help answer questions or provide PRs
for stickier items around assemblies, multiple artifacts, etc.


â€”
prb@mult.ifario.us | Multifarious, Inc. | http://mult.ifario.us/



"
hsaputra <git@git.apache.org>,"Fri, 21 Feb 2014 08:34:41 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1111: URL Validation Throws Er...,dev@spark.incubator.apache.org,"Github user hsaputra commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/625#discussion_r9939149
  
    --- Diff: core/src/test/scala/org/apache/spark/deploy/ClientSuite.scala ---
    @@ -0,0 +1,17 @@
    +package org.apache.spark.deploy
    --- End diff --
    
    Missing license ASF header?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Pascal Voitot Dev <pascal.voitot.dev@gmail.com>,"Fri, 21 Feb 2014 09:40:21 +0100",Re: [DISCUSS] Necessity of Maven *and* SBT Build in Spark,dev <dev@spark.incubator.apache.org>,"Hi,

My small contrib to the discussion.
SBT is able to publish Maven artifacts generating the POM and all JAR &
signed files.
So even if not in the project, a Pom can be found somewhere.

Pascal




"
hsaputra <git@git.apache.org>,"Fri, 21 Feb 2014 08:59:04 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1111: URL Validation Throws Er...,dev@spark.incubator.apache.org,"Github user hsaputra commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/625#discussion_r9939595
  
    --- Diff: core/src/main/scala/org/apache/spark/deploy/ClientArguments.scala ---
    @@ -115,3 +110,7 @@ private[spark] class ClientArguments(args: Array[String]) {
         System.exit(exitCode)
       }
     }
    +
    +object ClientArguments {
    +  def isValidJarUrl(s: String) = s.matches(""(.+):(.+)jar"")
    --- End diff --
    
    Should we also check for ""//"" in the String to prevent something like ""hdfs:test.jar""?
    Something like: s.matches(""(.+)://([^/]+)jar"")
    
    Tried with REPL:
    val s = ""hdfs://test.jar""
      s: java.lang.String = hdfs://test.jar
    s.matches(""(.+)://([^/]+)jar"")
      res3: Boolean = true
    s.matches(""(.+):(.+)jar"")
      res4: Boolean = true
    val s2 = ""hdfs:test.jar""
      s2: java.lang.String = hdfs:test.jar
    s2.matches(""(.+)://(.+)jar"")
      res5: Boolean = false
    s2.matches(""(.+):(.+)jar"")
      res6: Boolean = true
    
    We do not want dfs:test.jar to be true.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 09:07:03 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1111: URL Validation Throws Er...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/625#issuecomment-35710134
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 09:07:03 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1111: URL Validation Throws Er...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/625#issuecomment-35710135
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12797/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 09:07:16 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35710155
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 09:07:16 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35710156
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 09:10:15 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35710339
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12799/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 09:10:15 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35710338
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 09:23:07 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35711092
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 09:23:07 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35711093
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
jyotiska <git@git.apache.org>,"Fri, 21 Feb 2014 09:39:14 +0000 (UTC)","[GitHub] incubator-spark pull request: doctest updated for mapValues, flatM...",dev@spark.incubator.apache.org,"Github user jyotiska commented on the pull request:

    https://github.com/apache/incubator-spark/pull/621#issuecomment-35712122
  
    @pwendell can you merge this?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
berngp <git@git.apache.org>,"Fri, 21 Feb 2014 09:54:06 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-3: The spark-shell fails under...,dev@spark.incubator.apache.org,"GitHub user berngp opened a pull request:

    https://github.com/apache/incubator-spark/pull/627

    SPARK-3: The spark-shell fails under a dist.

    Problem:
    The spark-shell will fail to start when Spark is deployed using the
    tar.gz file built by ./make-distribution. The problem is generated
    by the spark-class shell that fails to setup a CLASSPATH if
    Spark is a RELEASE.
    
    Fix:
    The script `spark-class` now sets the CLASSPATH if it is a RELEASE/dist
    by pointing to the only assembly that should exist in the `./jars`
    
    Environment:
    *NIX environments. A similar change should be applied to
    `spark-class.cmd`.
    
    [ticket: SPARK-3] : https://issues.apache.org/jira/browse/SPARK-3

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/berngp/incubator-spark hotfix/SPARK-3

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/627.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #627
    
----
commit f1d6fdc3c119b972dd2f093c45c6f91a7c8d4a82
Author: Bernardo Gomez Palacio <bernardo.gomezpalacio@gmail.com>
Date:   2014-02-21T09:44:41Z

    SPARK-3: The spark-shell fails under a dist.
    
    Problem:
    The spark-shell will fail to start when Spark is deployed using the
    tar.gz file built by ./make-distribution. The problem is generated
    by the spark-class shell that fails to setup a CLASSPATH if
    Spark is a RELEASE.
    
    Fix:
    The script `spark-class` now sets the CLASSPATH if it is a RELEASE/dist
    by pointing to the only assembly that should exist in the `./jars`
    
    Environment:
    *NIX environments. A similar change should be applied to
    `spark-class.cmd`.
    
    [ticket: SPARK-3] : https://issues.apache.org/jira/browse/SPARK-3

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 10:07:14 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35714542
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12800/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 10:07:14 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1094] Support MiMa for report...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/585#issuecomment-35714541
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 10:07:22 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-3: The spark-shell fails under...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/627#issuecomment-35714571
  
    Can one of the admins verify this patch?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 13:07:45 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35728700
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 13:07:45 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35728699
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
CodingCat <git@git.apache.org>,"Fri, 21 Feb 2014 13:44:46 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1100] prevent Spark from over...,dev@spark.incubator.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/incubator-spark/pull/626#issuecomment-35731089
  
    @mridulm Thank you for telling me the standard solution, I will revise my patch today. I learnt a lot from the discussion with you in my other patches


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
CodingCat <git@git.apache.org>,"Fri, 21 Feb 2014 13:47:21 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1100] prevent Spark from over...,dev@spark.incubator.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/incubator-spark/pull/626#issuecomment-35731273
  
    @jyotiska that would be nice!


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 14:07:33 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35732711
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12801/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 14:07:33 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35732710
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
wchswchs <git@git.apache.org>,"Fri, 21 Feb 2014 15:11:28 +0000 (UTC)",[GitHub] incubator-spark pull request: add threadPool shutdown hook when ki...,dev@spark.incubator.apache.org,"GitHub user wchswchs opened a pull request:

    https://github.com/apache/incubator-spark/pull/628

    add threadPool shutdown hook when kill task

    is it rightï¼Ÿ

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/wchswchs/incubator-spark master

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/628.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #628
    
----
commit a8441c8da723afcba1837a1fe6eb884ef19288d8
Author: jobs wang <wchswchs@gmail.com>
Date:   2014-02-21T14:43:02Z

    add threadPool shutdown hook when kill task

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 15:11:56 +0000 (UTC)",[GitHub] incubator-spark pull request: add threadPool shutdown hook when ki...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/628#issuecomment-35738561
  
    Can one of the admins verify this patch?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
mridulm <git@git.apache.org>,"Fri, 21 Feb 2014 15:43:22 +0000 (UTC)",[GitHub] incubator-spark pull request: add threadPool shutdown hook when ki...,dev@spark.incubator.apache.org,"Github user mridulm commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/628#discussion_r9949391
  
    --- Diff: core/src/main/scala/org/apache/spark/executor/Executor.scala ---
    @@ -141,6 +141,7 @@ private[spark] class Executor(
         val tr = runningTasks.get(taskId)
         if (tr != null) {
           tr.kill()
    +      threadPool.shutdownNow()
         }
    --- End diff --
    
    This would end up killing not just the requested task - but all tasks in the thread pool - not what is required.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
wchswchs <git@git.apache.org>,"Fri, 21 Feb 2014 15:54:18 +0000 (UTC)",[GitHub] incubator-spark pull request: add threadPool shutdown hook when ki...,dev@spark.incubator.apache.org,"Github user wchswchs commented on the pull request:

    https://github.com/apache/incubator-spark/pull/628#issuecomment-35743010
  
    just use threadPool.remove(tr) ?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
wchswchs <git@git.apache.org>,"Fri, 21 Feb 2014 16:00:59 +0000 (UTC)",[GitHub] incubator-spark pull request: add threadPool shutdown hook when ki...,dev@spark.incubator.apache.org,"Github user wchswchs commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/628#discussion_r9950139
  
    --- Diff: core/src/main/scala/org/apache/spark/executor/Executor.scala ---
    @@ -141,6 +141,7 @@ private[spark] class Executor(
         val tr = runningTasks.get(taskId)
         if (tr != null) {
           tr.kill()
    +      threadPool.shutdownNow()
         }
    --- End diff --
    
    just call threadPool.remove(tr) to remove the task?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Ethan Jewett <esjewett@gmail.com>,"Fri, 21 Feb 2014 10:05:28 -0600",Re: Signal/Noise Ratio,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","It would help but wouldn't solve the formatting problem or the one-way
communication problem.


"
mridulm <git@git.apache.org>,"Fri, 21 Feb 2014 16:06:25 +0000 (UTC)",[GitHub] incubator-spark pull request: add threadPool shutdown hook when ki...,dev@spark.incubator.apache.org,"Github user mridulm commented on the pull request:

    https://github.com/apache/incubator-spark/pull/628#issuecomment-35744380
  
    remove will remove the task from the queue assuming it is not yet started : but that should not be a problem - since task.kill sets flags such that it will not execute for long even if it was not removed.
    
    But once the task starts executing, unfortunately, there is no guaranteed way to kill it.
    You can try interrupting it - assuming the task responds to interruptions (and does not gobble up InterruptedException : particularly since it is executing user code).
    But other than that, I can think of a good solution ... 


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Ethan Jewett <esjewett@gmail.com>,"Fri, 21 Feb 2014 10:07:36 -0600",Re: Signal/Noise Ratio,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi Aaron,

Off-list message here. Can you point me to this policy? Due to some
previous experiences here, I'm under the impression that it doesn't exist.
I can't find it on the Apache website.

Thanks,
Ethan


"
Ethan Jewett <esjewett@gmail.com>,"Fri, 21 Feb 2014 10:08:53 -0600",Re: Signal/Noise Ratio,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Or not off-list. Sorry folks :-) Anyone should feel free to educate me
either on the policy or on mailing list use ;-)


"
wchswchs <git@git.apache.org>,"Fri, 21 Feb 2014 16:09:27 +0000 (UTC)",[GitHub] incubator-spark pull request: add threadPool shutdown hook when ki...,dev@spark.incubator.apache.org,"Github user wchswchs commented on the pull request:

    https://github.com/apache/incubator-spark/pull/628#issuecomment-35744706
  
    ok ,i know, thx


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
srowen <git@git.apache.org>,"Fri, 21 Feb 2014 16:34:19 +0000 (UTC)",[GitHub] incubator-spark pull request: MLLIB-25: Implicit ALS runs out of m...,dev@spark.incubator.apache.org,"GitHub user srowen opened a pull request:

    https://github.com/apache/incubator-spark/pull/629

    MLLIB-25: Implicit ALS runs out of memory for moderately large numbers of features

    There's a step in implicit ALS where the matrix `Yt * Y` is computed. It's computed as the sum of matrices; an f x f matrix is created for each of n user/item rows in a partition. In `ALS.scala:214`:
    
    ```
            factors.flatMapValues{ case factorArray =>
              factorArray.map{ vector =>
                val x = new DoubleMatrix(vector)
                x.mmul(x.transpose())
              }
            }.reduceByKeyLocally((a, b) => a.addi(b))
             .values
             .reduce((a, b) => a.addi(b))
    ```
    
    Completely correct, but there's a subtle but quite large memory problem here. map() is going to create all of these matrices in memory at once, when they don't need to ever all exist at the same time.
    For example, if a partition has n = 100000 rows, and f = 200, then this intermediate product requires 32GB of heap. The computation will never work unless you can cough up workers with (more than) that much heap.
    
    Fortunately there's a trivial change that fixes it; just add `.view` in there.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/srowen/incubator-spark ALSMatrixAllocationOptimization

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/629.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #629
    
----
commit e9a5d636b8a5d6288924ddf2871645c4eea41ffe
Author: Sean Owen <sowen@cloudera.com>
Date:   2014-02-21T09:37:39Z

    Avoid unnecessary out of memory situation by not simultaneously allocating lots of matrices

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
mengxr <git@git.apache.org>,"Fri, 21 Feb 2014 17:06:33 +0000 (UTC)",[GitHub] incubator-spark pull request: MLLIB-25: Implicit ALS runs out of m...,dev@spark.incubator.apache.org,"Github user mengxr commented on the pull request:

    https://github.com/apache/incubator-spark/pull/629#issuecomment-35750636
  
    LGTM if Travis passes (no reason not). Thanks for the fix! 


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 17:07:54 +0000 (UTC)",[GitHub] incubator-spark pull request: MLLIB-25: Implicit ALS runs out of m...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/629#issuecomment-35750759
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 17:07:55 +0000 (UTC)",[GitHub] incubator-spark pull request: MLLIB-25: Implicit ALS runs out of m...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/629#issuecomment-35750760
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Fri, 21 Feb 2014 17:30:05 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1111: URL Validation Throws Er...,dev@spark.incubator.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/625#discussion_r9953789
  
    --- Diff: core/src/main/scala/org/apache/spark/deploy/ClientArguments.scala ---
    @@ -115,3 +110,7 @@ private[spark] class ClientArguments(args: Array[String]) {
         System.exit(exitCode)
       }
     }
    +
    +object ClientArguments {
    +  def isValidJarUrl(s: String) = s.matches(""(.+):(.+)jar"")
    --- End diff --
    
    @hsaputra - actually the `//` is not a require part of a URI, it is a scheme-dependent part that schemes chose to handle differently. For instance I think that Hadoop is alright with `file:/path/to/my/file` or even `file:path/to/my/file`.
    
    This particular piece of code is just providing a quick sanity check. If the user enters an invalid URL they will get a specific error message downstream...


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Patrick Wendell <pwendell@gmail.com>,"Fri, 21 Feb 2014 09:36:57 -0800",Re: [DISCUSS] Necessity of Maven *and* SBT Build in Spark,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey Everyone,

We are going to publish artifacts to maven central in the exact same
format no matter which build system we use.

For normal consumers of Spark {maven vs sbt} won't make a difference.
It will make a difference for people who are extended the Spark build
to do their own packaging. This is what I'm trying to gauge - does
anyone do this in a way where they feel only maven or only sbt
supports their particular issue.

- Patrick


"
Mark Hamstra <mark@clearstorydata.com>,"Fri, 21 Feb 2014 09:53:53 -0800",Re: [DISCUSS] Necessity of Maven *and* SBT Build in Spark,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","As long as the SBT build doesn't start depending on some new functionality
that doesn't have an easy analog in Maven, the canonical build being done
only via SBT doesn't make too much difference to me.  Regardless, I'm going
to need to continue to support customized builds that fit into my
Maven-ized environment.  The way things work currently, I need at some
point to examine every change to SparkBuild.scala and to the POM files to
make sure that they are still in sync and that I have picked up all the
appropriate changes into my builds.  If there are no more POM files to look
at in the future (other than the SBT-generated ones), that actually
simplifies my job in some respect (only one place to look for changes), as
long as the translation from changed-Apache-SBT to changed-ClearStory-POM
remains fairly obvious -- that's my basic requirement, as I said
previously.



"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 18:07:40 +0000 (UTC)",[GitHub] incubator-spark pull request: MLLIB-25: Implicit ALS runs out of m...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/629#issuecomment-35756551
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12802/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 18:07:40 +0000 (UTC)",[GitHub] incubator-spark pull request: MLLIB-25: Implicit ALS runs out of m...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/629#issuecomment-35756550
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 18:08:29 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1111: URL Validation Throws Er...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/625#issuecomment-35756615
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 18:08:29 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1111: URL Validation Throws Er...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/625#issuecomment-35756614
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Fri, 21 Feb 2014 18:29:43 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1111: URL Validation Throws Er...,dev@spark.incubator.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/625#discussion_r9955905
  
    --- Diff: core/src/test/scala/org/apache/spark/deploy/ClientSuite.scala ---
    @@ -0,0 +1,17 @@
    +package org.apache.spark.deploy
    --- End diff --
    
    @hsaputra good catch. Fixed!


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
hsaputra <git@git.apache.org>,"Fri, 21 Feb 2014 18:31:57 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1111: URL Validation Throws Er...,dev@spark.incubator.apache.org,"Github user hsaputra commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/625#discussion_r9955979
  
    --- Diff: core/src/main/scala/org/apache/spark/deploy/ClientArguments.scala ---
    @@ -115,3 +110,7 @@ private[spark] class ClientArguments(args: Array[String]) {
         System.exit(exitCode)
       }
     }
    +
    +object ClientArguments {
    +  def isValidJarUrl(s: String) = s.matches(""(.+):(.+)jar"")
    --- End diff --
    
    Ah, I did not know Hadoop is ok with file:/ pattern, I always use file://
    
    Thanks for the response, @pwendell
    
    But we do want URL instead of URI for the JAR path location I suppose.
    So this PR is just to ""relax"" java.net.URL requirement to have the double slashes?
    If that is the case, then maybe we could update the comment to include the ones without ""//""
    
    e.g. hdfs://XX.jar, file:/XX.jar, file:XX.jar
    
    Sorry about the questions for this small PR, just wanted to know more about the reasoning.
    Always nervous whenever we add custom URL/ URI validation.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Fri, 21 Feb 2014 18:47:34 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1111: URL Validation Throws Er...,dev@spark.incubator.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/625#discussion_r9956570
  
    --- Diff: core/src/main/scala/org/apache/spark/deploy/ClientArguments.scala ---
    @@ -115,3 +110,7 @@ private[spark] class ClientArguments(args: Array[String]) {
         System.exit(exitCode)
       }
     }
    +
    +object ClientArguments {
    +  def isValidJarUrl(s: String) = s.matches(""(.+):(.+)jar"")
    --- End diff --
    
    Hey @hsaputra - I'd think we should recommend ""//"" to users since it's the widely used way. Unfortunately URL naming is totally messed up and inconsistent not only in Hadoop but in the broader internet ecosystem. The recommended philosphy in the IETF is to be ""liberal in what you accept, and conservative in what you send"". I think we should be liberal in accepting weird URL's, but we should not advise users to make quirky URL's.
    
    http://en.wikipedia.org/wiki/Robustness_principle
    
    That said, this PR is mainly because java.net.URL (incorrectly) considers `hdfs` an invalid scheme. So we can't rely on java.net.URL to validate this.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
CodingCat <git@git.apache.org>,"Fri, 21 Feb 2014 18:48:52 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1100] prevent Spark from over...,dev@spark.incubator.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/incubator-spark/pull/626#issuecomment-35760516
  
    @mridulm I tested that and found that it is actually not handled in Spark, 
    
    ![abc](https://f.cloud.github.com/assets/678008/2233295/4a6efde6-9b28-11e3-9877-02a8829ac056.png)
    
    part-00000 and part-00001 are generated by the second run of the job (the partition is set to 2)
    
    part-00002 and part-00003 are generated by the first run of (partition number of is set to 4), 
    
    I also found this when I run Spark over S3, the old file is still left over in the output directory



---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Aaron Davidson <ilikerps@gmail.com>,"Fri, 21 Feb 2014 10:51:20 -0800",Re: Signal/Noise Ratio,dev@spark.incubator.apache.org,"I don't have an official policy to point you to, but Chris Mattmann (our
Apache project mentor) summarized some of the points in this thread, and
here is the original concern that caused us to make this change:

http://mail-archives.apache.org/mod_mbox/incubator-general/201402.mbox/%3CCAAS6=7hkCiT093nXVMcUus8Z-5XCDn=cQ5trjN_Kz9ARe9H=RA@mail.gmail.com%3E



"
rxin <git@git.apache.org>,"Fri, 21 Feb 2014 18:51:57 +0000 (UTC)",[GitHub] incubator-spark pull request: MLLIB-25: Implicit ALS runs out of m...,dev@spark.incubator.apache.org,"Github user rxin commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/629#discussion_r9956734
  
    --- Diff: mllib/src/main/scala/org/apache/spark/mllib/recommendation/ALS.scala ---
    @@ -212,7 +212,7 @@ class ALS private (var numBlocks: Int, var rank: Int, var iterations: Int, var l
         if (implicitPrefs) {
           Option(
             factors.flatMapValues{ case factorArray =>
    -          factorArray.map{ vector =>
    +          factorArray.view.map{ vector =>
    --- End diff --
    
    mind adding a space after map? technically not your problem, but while you are at it ...


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
rxin <git@git.apache.org>,"Fri, 21 Feb 2014 18:52:06 +0000 (UTC)",[GitHub] incubator-spark pull request: MLLIB-25: Implicit ALS runs out of m...,dev@spark.incubator.apache.org,"Github user rxin commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/629#discussion_r9956743
  
    --- Diff: mllib/src/main/scala/org/apache/spark/mllib/recommendation/ALS.scala ---
    @@ -212,7 +212,7 @@ class ALS private (var numBlocks: Int, var rank: Int, var iterations: Int, var l
         if (implicitPrefs) {
           Option(
             factors.flatMapValues{ case factorArray =>
    -          factorArray.map{ vector =>
    +          factorArray.view.map{ vector =>
    --- End diff --
    
    and for the above flatMapValues


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Konstantin Boudnik <cos@apache.org>,"Fri, 21 Feb 2014 10:52:56 -0800",Re: [DISCUSS] Necessity of Maven *and* SBT Build in Spark,dev@spark.incubator.apache.org,"I'd like to chime in as one of downstream integrators (Bigtop, etc.). As the
original author of Maven packaging assemblies I might be able shed some light
on the history behind of it:

  - in order to integrate Spark well into existing Hadoop stack it was
    necessary to have a way to avoid transitive dependencies duplications and
    possible conflicts.

    E.g. Maven assembly allows us to avoid adding _all_ Hadoop libs and later
    merely declare Spark package dependency on standard Bigtop Hadoop
    packages. And yes - Bigtop packaging means the naming and layout would be
    standard across all commercial Hadoop distributions that are worth
    mentioning: ASF Bigtop convenience binary packages, and Cloudera or
    Hortonworks packages. Hence, the downstream user doesn't need to spend any
    effort to make sure that Spark ""clicks-in"" properly.

  - Maven provides a relatively easy way to deal with the jar-hell problem,
    although the original maven build was just Shader'ing everything into a
    huge lump of class files. Oftentimes ending up with classes slamming on
    top of each other from different transitive dependencies.

Artifact publishing isn't a deciding concern when it comes to Sbt vs Maven: it
seems to be a no-brainer in both cases. I don't know Sbt that well to say that
its assemblies do not or can not provide the same level of segregation as
Maven's, but it seems this way. And that along is the huge blocker of dropping
the support of Maven build.

Now, what's the great deal of benefits supplemented by Sbt?

Regards,
    Cos


"
CodingCat <git@git.apache.org>,"Fri, 21 Feb 2014 18:55:34 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1100] prevent Spark from over...,dev@spark.incubator.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/incubator-spark/pull/626#issuecomment-35761128
  
    I run the following command
    
    scala> val a = sc.textFile(""/Users/nanzhu/code/incubator-spark/LICENSE"", 4).map(line => (""a"", ""b""))
    
    scala> val a = sc.textFile(""/Users/nanzhu/code/incubator-spark/LICENSE"", 2).map(line => (""a"", ""b""))
    
    Sorry, uploaded the wrong image, look at here
    ![abc](https://f.cloud.github.com/assets/678008/2233419/bb079c06-9b29-11e3-8ad5-ca57a0d9c480.png)



---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
hsaputra <git@git.apache.org>,"Fri, 21 Feb 2014 18:56:53 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1111: URL Validation Throws Er...,dev@spark.incubator.apache.org,"Github user hsaputra commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/625#discussion_r9956945
  
    --- Diff: core/src/main/scala/org/apache/spark/deploy/ClientArguments.scala ---
    @@ -115,3 +110,7 @@ private[spark] class ClientArguments(args: Array[String]) {
         System.exit(exitCode)
       }
     }
    +
    +object ClientArguments {
    +  def isValidJarUrl(s: String) = s.matches(""(.+):(.+)jar"")
    --- End diff --
    
    Ah yes
    
    scala> val test = new java.net.URL(""hdfs://test.jar"")
    java.net.MalformedURLException: unknown protocol: hdfs
    	at java.net.URL.<init>(URL.java:574)
    
    I thought java.net.URL accepts custom scheme =(
    Agree about messed up naming usage. 
    
    +1 
    
    Thanks for the explanation! =)


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 19:07:47 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1111: URL Validation Throws Er...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/625#issuecomment-35762279
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 19:07:47 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1111: URL Validation Throws Er...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/625#issuecomment-35762280
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12803/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Patrick Wendell <pwendell@gmail.com>,"Fri, 21 Feb 2014 11:11:32 -0800",Re: [DISCUSS] Necessity of Maven *and* SBT Build in Spark,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Kos - thanks for chiming in. Could you be more specific about what is
available in maven and not in sbt for these issues? I took a look at
the bigtop code relating to Spark. As far as I could tell [1] was the
main point of integration with the build system (maybe there are other
integration points)?


The sbt build also allows you to plug in a Hadoop version similar to
the maven build.


AFIAK we are only using the shade plug-in to deal with conflict
resolution in the assembly jar. These are dealt with in sbt via the
sbt assembly plug-in in an identical way. Is there a difference?

[1] https://git-wip-us.apache.org/repos/asf?p=bigtop.git;a=blob;f=bigtop-packages/src/common/spark/do-component-build;h=428540e0f6aa56cd7e78eb1c831aa7fe9496a08f;hb=master

"
aarondav <git@git.apache.org>,"Fri, 21 Feb 2014 19:13:24 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1111: URL Validation Throws Er...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/625#issuecomment-35762793
  
    Thanks! Merged in master and branch-0.9.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Ethan Jewett <esjewett@gmail.com>,"Fri, 21 Feb 2014 13:14:27 -0600",Re: Signal/Noise Ratio,dev@spark.incubator.apache.org,"Thanks for the pointer Aaron. Very helpful.

I won't harp on this any more after this email: my reading is that the main
concern is archiving discussion, which could be achieved using a separate
mailing list. Major decisions should clearly happen on the dev list so
everyone is informed, but I don't see a situation where that hadn't been
happening anyway (which is why I read the dev list regularly, sometimes
look at the archives, and am struggling with the Github messages and
pitying those not using Gmail filters).



"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 19:15:54 +0000 (UTC)",[GitHub] incubator-spark pull request: MLLIB-25: Implicit ALS runs out of m...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/629#issuecomment-35763013
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 19:15:54 +0000 (UTC)",[GitHub] incubator-spark pull request: MLLIB-25: Implicit ALS runs out of m...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/629#issuecomment-35763015
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Ameet Kini <ameetkini@gmail.com>,"Fri, 21 Feb 2014 14:16:10 -0500",why is NextIterator private?,dev@spark.incubator.apache.org,"I'm looking to subclass HadoopRDD and was hoping to subclass NextIterator
in compute().

Thanks,
Ameet
"
srowen <git@git.apache.org>,"Fri, 21 Feb 2014 19:18:21 +0000 (UTC)",[GitHub] incubator-spark pull request: MLLIB-25: Implicit ALS runs out of m...,dev@spark.incubator.apache.org,"Github user srowen commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/629#discussion_r9957753
  
    --- Diff: mllib/src/main/scala/org/apache/spark/mllib/recommendation/ALS.scala ---
    @@ -212,7 +212,7 @@ class ALS private (var numBlocks: Int, var rank: Int, var iterations: Int, var l
         if (implicitPrefs) {
           Option(
             factors.flatMapValues{ case factorArray =>
    -          factorArray.map{ vector =>
    +          factorArray.view.map{ vector =>
    --- End diff --
    
    No problem, it's done


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Ameet Kini <ameetkini@gmail.com>,"Fri, 21 Feb 2014 14:26:39 -0500",Re: why is NextIterator private?,dev@spark.incubator.apache.org,"
Yes, I can copy them verbatim and change them as they are small enough. But
sub-classing would have been nicer.

Ameet



"
asfgit <git@git.apache.org>,"Fri, 21 Feb 2014 19:34:07 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1111: URL Validation Throws Er...,dev@spark.incubator.apache.org,"Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-spark/pull/625


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Ameet Kini <ameetkini@gmail.com>,"Fri, 21 Feb 2014 14:35:05 -0500",Re: why is NextIterator private?,dev@spark.incubator.apache.org,"I'm looking at NewHadoopRDD and looks like there is no equivalent of
NextIterator there. I'm tempted to go with extending that.

Interestingly, in NewHadoopRDD, there is no concept of caching the
configuration if one has not been broadcasted, whereas in HadoopRDD, there
is a cache. I'm now tempted to go with extending NewHadoopRDD, and would
appreciate some insight from the developers on what was the thinking behind
keeping that out.

Thanks,
Ameet



"
Jey Kottalam <jey@cs.berkeley.edu>,"Fri, 21 Feb 2014 11:37:08 -0800",Re: why is NextIterator private?,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","What's the motivation for subclassing HadoopRDD? I don't believe
that's a supported use case. Is it not possible to do what you need
with a Hadoop InputFormat?


"
jyotiska <git@git.apache.org>,"Fri, 21 Feb 2014 19:40:25 +0000 (UTC)",[GitHub] incubator-spark pull request: Fixed minor typo in worker.py,dev@spark.incubator.apache.org,"GitHub user jyotiska opened a pull request:

    https://github.com/apache/incubator-spark/pull/630

    Fixed minor typo in worker.py

    Fixed minor typo in worker.py

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/jyotiska/incubator-spark pyspark_code

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/630.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #630
    
----
commit 716f7cda5e567c7de6f00246bdb427e87b6bbf82
Author: jyotiska <jyotiska123@gmail.com>
Date:   2014-02-20T16:51:01Z

    doctest updated for mapValues, flatMapValues in rdd.py

commit f56c32c9b390bc2d5114c7c0f409d0f82d579644
Author: jyotiska <jyotiska123@gmail.com>
Date:   2014-02-21T19:38:57Z

    typo fixed in worker.py

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Fri, 21 Feb 2014 19:43:27 +0000 (UTC)",[GitHub] incubator-spark pull request: Fixed minor typo in worker.py,dev@spark.incubator.apache.org,"Github user aarondav commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/630#discussion_r9958810
  
    --- Diff: python/pyspark/rdd.py ---
    @@ -916,6 +916,11 @@ def flatMapValues(self, f):
             Pass each value in the key-value pair RDD through a flatMap function
             without changing the keys; this also retains the original RDD's
             partitioning.
    +
    +        >>> x = sc.parallelize([(""a"", [""x"", ""y"", ""z""]), (""b"", [""p"", ""r""])])
    --- End diff --
    
    Were these changes intended in this PR? You might want to update the PR description if so..


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Fri, 21 Feb 2014 19:44:49 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1113] External spilling - fix...,dev@spark.incubator.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/624#discussion_r9958856
  
         private case class StreamBuffer(iterator: Iterator[(K, C)], pairs: ArrayBuffer[(K, C)])
           extends Comparable[StreamBuffer] {
     
    -      def minKeyHash: Int = {
    -        if (pairs.length > 0){
    -          // pairs are already sorted by key hash
    -          pairs(0)._1.hashCode()
    -        } else {
    -          Int.MaxValue
    -        }
    +      def isEmpty = pairs.isEmpty
    +
    +      // Invalid if there are no more pairs in this stream
    --- End diff --
    
    nit: but maybe put this comment above the assert line rather than the function?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Fri, 21 Feb 2014 19:45:22 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1113] External spilling - fix...,dev@spark.incubator.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/624#discussion_r9958879
  
         private case class StreamBuffer(iterator: Iterator[(K, C)], pairs: ArrayBuffer[(K, C)])
           extends Comparable[StreamBuffer] {
     
    -      def minKeyHash: Int = {
    -        if (pairs.length > 0){
    -          // pairs are already sorted by key hash
    -          pairs(0)._1.hashCode()
    -        } else {
    -          Int.MaxValue
    -        }
    +      def isEmpty = pairs.isEmpty
    +
    +      // Invalid if there are no more pairs in this stream
    --- End diff --
    
    ah nvm, after thinking more I guess it's better where it is


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Fri, 21 Feb 2014 19:48:55 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1113] External spilling - fix...,dev@spark.incubator.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/624#discussion_r9959040
  
             mergedBuffers += newBuffer
           }
     
    -      // Repopulate each visited stream buffer and add it back to the merge heap
    +      // Repopulate each visited stream buffer and add it back to the queue if it is non-empty
           mergedBuffers.foreach { buffer =>
    -        if (buffer.pairs.length == 0) {
    +        if (buffer.isEmpty) {
               buffer.pairs ++= getMorePairs(buffer.iterator)
             }
    -        mergeHeap.enqueue(buffer)
    +        if (!buffer.isEmpty) {
    --- End diff --
    
    shouldn't this just be an `else`?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
jyotiska <git@git.apache.org>,"Fri, 21 Feb 2014 19:53:32 +0000 (UTC)",[GitHub] incubator-spark pull request: Fixed minor typo in worker.py,dev@spark.incubator.apache.org,"Github user jyotiska commented on the pull request:

    https://github.com/apache/incubator-spark/pull/630#issuecomment-35766856
  
    oops, this was in PR #621 . If you want I can close the previous PR, rename the PR description and you can merge all the changes.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Ameet Kini <ameetkini@gmail.com>,"Fri, 21 Feb 2014 14:54:11 -0500",Re: why is NextIterator private?,"dev@spark.incubator.apache.org, jey@cs.berkeley.edu","The use case is to control the partitions as they come out of the
HadoopRDD.
1. Have my own HadoopPartition that has fields specific to my application.
These fields would then be used by other RDD operations (also overridden by
me). This is why I was looking to extend HadoopPartition.
2. Have my own getPartitions which has slightly different partitioning
logic. This can almost be solved by subclassing InputFormat and its
getSplits method, but I still need to have getPartitions create
MyHadoopPartition instead of HadoopPartition.

Ameet



"
Scott walent <scottwalent@gmail.com>,"Fri, 21 Feb 2014 11:57:09 -0800",Spark Summit 2014,"user@spark.incubator.apache.org, dev@spark.incubator.apache.org","Hello Spark User,









*Thank you for being a part of the Apache Spark community. Last December,
over 450 Spark users from 180 companies and 13 countries came together at
the first Spark Summit <http://spark-summit.org>.  The Summit was a 2-day
event that started off with a full day of presentations from 30 different
training with 200 participants. This showed us just how active and
enthusiastic the Spark community is. With this in mind we have been
planning the next Spark Summit. Spark Summit 2014
<http://spark-summit.org/summit-2014> will be held in downtown San
Francisco from June 30 - July 2. We have expanded our agenda to contain 2
days of presentations followed by our most ambitious Spark training program
yet.  With all the new space in our agenda, we are calling on our community
to step up. We want to hear about what you have been up to and get a
glimpse at all the exciting projects that involve Spark and Spark-related
technologies. The call for submissions for Spark Summit 2014 is now open.
Please visit our submissions page <http://sparksummit.submittable.com> to
submit an abstract or two.  The deadline for submissions is Friday, April
11. As always, we love feedback from our community. If you have any
feedback please let us know your thoughts. We look forward to seeing you
this summer. Sincerely, Scott WalentSpark Summit Organizer*
"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 20:07:53 +0000 (UTC)",[GitHub] incubator-spark pull request: MLLIB-25: Implicit ALS runs out of m...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/629#issuecomment-35768220
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 20:07:53 +0000 (UTC)",[GitHub] incubator-spark pull request: MLLIB-25: Implicit ALS runs out of m...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/629#issuecomment-35768223
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12804/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 20:09:39 +0000 (UTC)",[GitHub] incubator-spark pull request: Fixed minor typo in worker.py,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/630#issuecomment-35768389
  
    Can one of the admins verify this patch?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Jey Kottalam <jey@cs.berkeley.edu>,"Fri, 21 Feb 2014 12:24:43 -0800",Re: why is NextIterator private?,Ameet Kini <ameetkini@gmail.com>,"What's the motivation for having your own subclass of HadoopPartition?
As far as I know, that's not a supported use case either.


"
rxin <git@git.apache.org>,"Fri, 21 Feb 2014 20:46:05 +0000 (UTC)",[GitHub] incubator-spark pull request: MLLIB-25: Implicit ALS runs out of m...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/629#issuecomment-35771699
  
    Thanks. I've merged this.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
tgravescs <git@git.apache.org>,"Fri, 21 Feb 2014 21:00:54 +0000 (UTC)","[GitHub] incubator-spark pull request: Add Security to Spark - Akka, Http, ...",dev@spark.incubator.apache.org,"Github user tgravescs commented on the pull request:

    https://github.com/apache/incubator-spark/pull/332#issuecomment-35773048
  
    I think having a specialized Exception type could be useful.  The times that could be useful that come to mind are:
    - if we expose it as API to users 
    - Other parts of the Spark Code need to catch it and handle it in some way and we perhaps need different types - authentication vs authorized, etc..
    Being security aware of course also means you don't want to give the user too much information as a hacker could then use that to focus their attack.
    
    I'm fine with changing it here if you want, otherwise perhaps we file a followup jira or wait til we have more of a use. Thoughts?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Fri, 21 Feb 2014 21:23:49 +0000 (UTC)",[GitHub] incubator-spark pull request: MLLIB-25: Implicit ALS runs out of m...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/629#issuecomment-35775112
  
    @rxin would it make sense to backport this? Seems like a bug fix


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
asfgit <git@git.apache.org>,"Fri, 21 Feb 2014 21:35:51 +0000 (UTC)",[GitHub] incubator-spark pull request: MLLIB-25: Implicit ALS runs out of m...,dev@spark.incubator.apache.org,"Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-spark/pull/629


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Fri, 21 Feb 2014 21:36:50 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1113] External spilling - fix...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/624#issuecomment-35776248
  
    @andrewor14 Looks good to me. This adds a few function calls in the `next` path... would you mind doing some local regression testing just to make sure there are no regressions? I doubt there are but if you did a local test I'd feel better about it before merging in into 0.9.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Fri, 21 Feb 2014 21:38:29 +0000 (UTC)",[GitHub] incubator-spark pull request: Default log4j initialization causes ...,dev@spark.incubator.apache.org,"Github user pwendell closed the pull request at:

    https://github.com/apache/incubator-spark/pull/573


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
rxin <git@git.apache.org>,"Fri, 21 Feb 2014 21:39:43 +0000 (UTC)",[GitHub] incubator-spark pull request: MLLIB-25: Implicit ALS runs out of m...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/629#issuecomment-35776515
  
    Yup, good idea. I also cherry picked it into branch-0.9.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Gary Malouf <malouf.gary@gmail.com>,"Fri, 21 Feb 2014 17:22:42 -0500",Planned 0.9.1 release,dev@spark.incubator.apache.org,"My team has avoided upgrading to 0.9 to this point because of the Mesos bug
that has since been fixed in master.  For ease of tracking, we are trying
to only use tagged releases going forward as long as they will continue to
be frequent or become more stable over time.

Is there any timeline on cutting a tag for the 0.9.1 bug fix release?
"
mengxr <git@git.apache.org>,"Fri, 21 Feb 2014 22:24:02 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1117: update accumulator docs,dev@spark.incubator.apache.org,"GitHub user mengxr opened a pull request:

    https://github.com/apache/incubator-spark/pull/631

    SPARK-1117: update accumulator docs

    The current doc hints spark doesn't support accumulators of type `Long`, which is wrong.
    
    JIRA: https://spark-project.atlassian.net/browse/SPARK-1117

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/mengxr/incubator-spark acc

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/631.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #631
    
----
commit 45ecd25d02534594427982492946c07c09bf67ea
Author: Xiangrui Meng <meng@databricks.com>
Date:   2014-02-21T22:18:12Z

    update accumulator docs

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Patrick Wendell <pwendell@gmail.com>,"Fri, 21 Feb 2014 14:29:49 -0800",Re: Planned 0.9.1 release,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","We back port bug fixes into the 0.9 branch as they come in, so if
there is a particular fix you want to get you can always build from
the head of branch-0.9 and expect only stability improvements compared
with Spark 0.9.0.

The timing of the maintenance releases depends a bit on what bug fixes
come in and their importance. I'm thinking we should propose a release
pretty soon (order weeks) since there are some valuable bug fixes that
came in this week.

- Patrick


"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 23:09:03 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1117: update accumulator docs,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/631#issuecomment-35783476
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 21 Feb 2014 23:09:04 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1117: update accumulator docs,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/631#issuecomment-35783477
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Fri, 21 Feb 2014 23:14:51 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1071: Tidy logging strategy an...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/570#issuecomment-35783848
  
    @srowen - hey just wondering, why are you including the jul and jcl bindings? Are there random dependencies that are outputting logs through this?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
srowen <git@git.apache.org>,"Fri, 21 Feb 2014 23:22:30 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1071: Tidy logging strategy an...,dev@spark.incubator.apache.org,"Github user srowen commented on the pull request:

    https://github.com/apache/incubator-spark/pull/570#issuecomment-35784348
  
    @pwendell For jcl, yeah have a look at places where this PR excludes commons-logging, like for netty and jets3t. jul is used by, well, Java's own libraries I suppose. I suppose the thought was to let everything be controlled centrally. Shimming a java.* class always felt a bit funny, but, on the other hand I trust slf4j works. FWIW those were the thoughts.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sat, 22 Feb 2014 00:08:16 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1117: update accumulator docs,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/631#issuecomment-35786989
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sat, 22 Feb 2014 00:08:16 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1117: update accumulator docs,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/631#issuecomment-35786990
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12805/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
nicklan <git@git.apache.org>,"Sat, 22 Feb 2014 01:29:28 +0000 (UTC)",[GitHub] incubator-spark pull request: Add scripts using tachyon tarball,dev@spark.incubator.apache.org,"GitHub user nicklan opened a pull request:

    https://github.com/apache/incubator-spark/pull/632

    Add scripts using tachyon tarball

    - currently won't work as we're waiting on the new bin/tachyon script
     from tachyon

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/nicklan/incubator-spark tachyon-scripts-take2

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/632.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #632
    
----
commit dac07b866fa30e1081a1987473808f390a2f6530
Author: Nick Lanham <nick@afternight.org>
Date:   2014-02-22T01:26:55Z

    Add scripts using tachyon tarball
    
    - currently won't work as we're waiting on the new bin/tachyon script
      from tachyon

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sat, 22 Feb 2014 02:08:34 +0000 (UTC)",[GitHub] incubator-spark pull request: Add scripts using tachyon tarball,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/632#issuecomment-35791747
  
    Can one of the admins verify this patch?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Reynold Xin <rxin@databricks.com>,"Fri, 21 Feb 2014 18:08:35 -0800",Re: Signal/Noise Ratio,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","FYI I submitted an ASF INFRA ticket on granting the AMPLab Jenkins
permission to use the github commit status API.

If that goes through, we can configure Jenkins to use the commit status API
without leaving comments on the pull requests.

https://issues.apache.org/jira/browse/INFRA-7367




"
"""Mattmann, Chris A (3980)"" <chris.a.mattmann@jpl.nasa.gov>","Sat, 22 Feb 2014 02:18:55 +0000",Re: Signal/Noise Ratio,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Sweet great job Reynold.

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Chris Mattmann, Ph.D.
Chief Architect
Instrument Software and Science Data Systems Section (398)
NASA Jet Propulsion Laboratory Pasadena, CA 91109 USA
Office: 171-283, Mailstop: 171-246
Email: chris.a.mattmann@nasa.gov
WWW:  http://sunset.usc.edu/~mattmann/
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Adjunct Associate Professor, Computer Science Department
University of Southern California, Los Angeles, CA 90089 USA
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++






%3E


"
andrewor14 <git@git.apache.org>,"Sat, 22 Feb 2014 02:33:49 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1113] External spilling - fix...,dev@spark.incubator.apache.org,"Github user andrewor14 commented on the pull request:

    https://github.com/apache/incubator-spark/pull/624#issuecomment-35792355
  
    @pwendell I did a couple of tests locally and found a very slight speed-up with the latest commit, which replaces the usages of ```isEmpty``` and ```size``` with ```length```. More details are included in the commit message but I think this is ready for merge.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Ameet Kini <ameetkini@gmail.com>,"Fri, 21 Feb 2014 21:36:29 -0500",Re: why is NextIterator private?,jey@cs.berkeley.edu,"Right, I'm clearly facing headwinds. getPartitions returns
Array[Partition], so sub-classing HadoopPartition wouldn't help. Maybe I'm
better off just having a custom InputFormat. I'll explore that option some
more. Thanks for your input.

Ameet



"
AmplabJenkins <git@git.apache.org>,"Sat, 22 Feb 2014 03:09:11 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1113] External spilling - fix...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/624#issuecomment-35793074
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sat, 22 Feb 2014 03:09:11 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1113] External spilling - fix...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/624#issuecomment-35793075
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Sat, 22 Feb 2014 03:41:25 +0000 (UTC)",[GitHub] incubator-spark pull request: Switch from MUTF8 to UTF8 in PySpark...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/523#issuecomment-35793620
  
    @JoshRosen should we pick this into the 0.9 branch?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
JoshRosen <git@git.apache.org>,"Sat, 22 Feb 2014 03:43:42 +0000 (UTC)",[GitHub] incubator-spark pull request: Switch from MUTF8 to UTF8 in PySpark...,dev@spark.incubator.apache.org,"Github user JoshRosen commented on the pull request:

    https://github.com/apache/incubator-spark/pull/523#issuecomment-35793654
  
    @pwendell Yes, I'd merge it since it fixes a fairly serious bug.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Kay Ousterhout <keo@eecs.berkeley.edu>,"Fri, 21 Feb 2014 19:54:19 -0800",Re: Signal/Noise Ratio,dev@spark.incubator.apache.org,"It looks like there's at least one other apache project, jclouds, that
sends the github notifications to a separate notifications@ list (see
http://mail-archives.apache.org/mod_mbox/incubator-general/201402.mbox/%3C1391721862.67613.YahooMailNeo%40web172602.mail.ir2.yahoo.com%3E).
 Given that many people are annoyed by getting the messages on this list,
and that there is some precedent for sending them to a different list, I'd
be in favor of doing that.



"
pwendell <git@git.apache.org>,"Sat, 22 Feb 2014 03:57:51 +0000 (UTC)",[GitHub] incubator-spark pull request: Switch from MUTF8 to UTF8 in PySpark...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/523#issuecomment-35793875
  
    I realize now I back-ported this already:
    https://git-wip-us.apache.org/repos/asf?p=incubator-spark.git;a=commit;h=5edbd175e07dc9704b1babb9c5e8d97fb644be65


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Sat, 22 Feb 2014 04:04:06 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1113] External spilling - fix...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/624#issuecomment-35793977
  
    Thanks @andrewor14 I'll merge this.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Sat, 22 Feb 2014 04:05:16 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1113] External spilling - fix...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/624#issuecomment-35793996
  
    Hmm... github API having issues so it'll have to wait.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sat, 22 Feb 2014 04:08:27 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1113] External spilling - fix...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/624#issuecomment-35794056
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sat, 22 Feb 2014 04:08:28 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1113] External spilling - fix...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/624#issuecomment-35794057
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12806/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
asfgit <git@git.apache.org>,"Sat, 22 Feb 2014 04:36:34 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1113] External spilling - fix...,dev@spark.incubator.apache.org,"Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-spark/pull/624


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
jyotiska <git@git.apache.org>,"Sat, 22 Feb 2014 06:02:01 +0000 (UTC)",[GitHub] incubator-spark pull request: Fixed minor typo in worker.py,dev@spark.incubator.apache.org,"Github user jyotiska commented on the pull request:

    https://github.com/apache/incubator-spark/pull/630#issuecomment-35795765
  
    @aarondav I have removed the other commit, can you merge this?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Sat, 22 Feb 2014 06:45:34 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1117: update accumulator docs,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/631#issuecomment-35796348
  
    Thanks @mengxr. Merged into master and 0.9


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Tathagata Das <tathagata.das1565@gmail.com>,"Fri, 21 Feb 2014 23:09:32 -0800",Re: Spark Streaming : Not working with TextFileStream on HDFS,dev@spark.incubator.apache.org,"For the log trace, it does not seem like it is finding any new file, which
is why it is not creating any output data. Are you sure your inserting text
files correctly into the directory that you have created textStream on?
Does this program work locally with local file system using the same
mechanism of adding files to a local directory?

TD



"
asfgit <git@git.apache.org>,"Sat, 22 Feb 2014 07:34:54 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1117: update accumulator docs,dev@spark.incubator.apache.org,"Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-spark/pull/631


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sat, 22 Feb 2014 08:11:22 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35797496
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sat, 22 Feb 2014 08:11:22 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35797497
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sat, 22 Feb 2014 09:08:52 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35798351
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sat, 22 Feb 2014 09:08:52 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35798352
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12807/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
dbtsai <git@git.apache.org>,"Sat, 22 Feb 2014 11:10:56 +0000 (UTC)",[GitHub] incubator-spark pull request: Initialized the regVal for first ite...,dev@spark.incubator.apache.org,"GitHub user dbtsai opened a pull request:

    https://github.com/apache/incubator-spark/pull/633

    Initialized the regVal for first iteration in SGD optimizer

    In runMiniBatchSGD, the regVal (for 1st iter) should be initialized
    as sum of sqrt of weights if it's L2 update; for L1 update, the same logic is followed.
    
    It maybe not be important here for SGD since the updater doesn't take the loss
    as parameter to find the new weights. But it will give us the correct history of loss.
    However, for LBFGS optimizer we implemented, the correct loss with regVal is crucial to
    find the new weights.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/AlpineNow/incubator-spark dbtsai-smallRegValFix

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/633.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #633
    
----
commit 9d2670330ebcde4240d97f0f51d7cfbc71509780
Author: DB Tsai <dbtsai@dbtsai.com>
Date:   2014-02-22T10:59:00Z

    In runMiniBatchSGD, the regVal (for 1st iter) should be initialized
    as sum of sqrt of weights if it's L2 update; for L1 update, the same logic is followed.
    
    It maybe not be important here for SGD since the updater doesn't take the loss
    as parameter to find the new weights. But it will give us the correct history of loss.
    However, for LBFGS optimizer we implemented, the correct loss with regVal is crucial to
    find the new weights.

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sat, 22 Feb 2014 11:11:23 +0000 (UTC)",[GitHub] incubator-spark pull request: Initialized the regVal for first ite...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/633#issuecomment-35800239
  
    Can one of the admins verify this patch?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
DB Tsai <dbtsai@alpinenow.com>,"Sat, 22 Feb 2014 04:28:24 -0800",MLLib - Thoughts about refactoring Updater for LBFGS?,dev <dev@spark.incubator.apache.org>,"Hi guys,

First of all, we would like to thank all the Spark community for
building such great platform for big data processing. We built the
multinomial logistic regression with LBFGS optimizer in Spark, and
LBFGS is a limited memory version of quasi-newton method which allows
us to train a very high-dimensional data without computing the Hessian
matrix as newton method required.

In Strata Conference, we did a great demo using Spark with our MLOR to
train mnist8m dataset. We're able to train the model in 5 mins with 50
iterations, and get 86% accuracy. The first iteration takes 19.8s, and
the remaining iterations take about 5~7s.

We did comparison between LBFGS and SGD, and often we saw 10x less
steps in LBFGS while the cost of per step is the same (just computing
the gradient).

The following is the paper by Prof. Ng at Stanford comparing different
optimizers including LBFGS and SGD. They use them in the context of
deep learning, but worth as reference.
http://cs.stanford.edu/~jngiam/papers/LeNgiamCoatesLahiriProchnowNg2011.pdf

We would like to break our MLOR with LBFGS into three patches to
contribute to the community.

1) LBFGS optimizer - which can be used in logistic regression, and
liner regression or replacing any algorithms using SGD.
The core underneath LBFGS Java implementation we used is from RISO
project, and the author, Robert is so kind to relicense it to GPL and
Apache2 dual license.

We're almost ready to submit a PR for LBFGS, see our github fork,
https://github.com/AlpineNow/incubator-spark/commits/dbtsai-LBFGS

However, we don't use Updater in LBFGS since it designs for GD, and
for LBFGS, we don't need stepSize, and adaptive learning rate, etc.
While it seems to be difficult to fit the LBFGS updater logic (well,
in lbfgs library, the new weights is returned given old weights, loss,
and gradient) into the current framework, I was thinking to abstract
out the code computing the gradient and loss terms of regularization
into different place so that different optimizer can also use it. Any
suggestion about this?

2) and 3), we will add the MLOR gradient to MLLib, and add a few
examples. Finally, we will have some tweak using mapPartition instead
of map to further improve the performance.

Thanks.

Sincerely,

DB Tsai
Machine Learning Engineer
Alpine Data Labs
--------------------------------------
Web: http://alpinenow.com/

"
srowen <git@git.apache.org>,"Sat, 22 Feb 2014 12:51:29 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1071: Tidy logging strategy an...,dev@spark.incubator.apache.org,"Github user srowen commented on the pull request:

    https://github.com/apache/incubator-spark/pull/570#issuecomment-35801874
  
    @pwendell I rebased this PR to handle one tiny conflict. Following the conversation yesterday -- yeah the Spark build has core depending on both log4j and slf4j-log4j12. This shouldn't be so, and the Maven build doesn't do this. I think the Spark build should just drop the latter dependency. Seem OK?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sat, 22 Feb 2014 13:09:16 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1071: Tidy logging strategy an...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/570#issuecomment-35802165
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sat, 22 Feb 2014 13:09:16 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1071: Tidy logging strategy an...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/570#issuecomment-35802166
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sat, 22 Feb 2014 14:09:08 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1071: Tidy logging strategy an...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/570#issuecomment-35803372
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12808/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sat, 22 Feb 2014 14:09:08 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1071: Tidy logging strategy an...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/570#issuecomment-35803371
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sat, 22 Feb 2014 14:09:33 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1084. Fix most build warnings,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/586#issuecomment-35803384
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sat, 22 Feb 2014 14:09:34 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1084. Fix most build warnings,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/586#issuecomment-35803385
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sat, 22 Feb 2014 15:09:16 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1084. Fix most build warnings,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/586#issuecomment-35804723
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sat, 22 Feb 2014 15:09:16 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1084. Fix most build warnings,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/586#issuecomment-35804725
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12809/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
CodingCat <git@git.apache.org>,"Sat, 22 Feb 2014 15:10:41 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1055] fix the SCALA_VERSION a...,dev@spark.incubator.apache.org,"GitHub user CodingCat opened a pull request:

    https://github.com/apache/incubator-spark/pull/634

    [SPARK-1055] fix the SCALA_VERSION and SPARK_VERSION in docker file

    As reported in https://spark-project.atlassian.net/browse/SPARK-1055
    
    ""The used Spark version in the .../base/Dockerfile is stale on 0.8.1 and should be updated to 0.9.x to match the release.""

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/CodingCat/incubator-spark SPARK-1055

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/634.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #634
    
----
commit adf82594974f0a7bbffc7e7536836b2fa03a74cb
Author: CodingCat <zhunansjtu@gmail.com>
Date:   2014-02-22T15:12:26Z

    fix the SCALA_VERSION and SPARK_VERSION in docker file

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sat, 22 Feb 2014 15:10:45 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1055] fix the SCALA_VERSION a...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/634#issuecomment-35804763
  
    Can one of the admins verify this patch?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
CodingCat <git@git.apache.org>,"Sat, 22 Feb 2014 15:51:40 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-979] a LRU scheduler for load...,dev@spark.incubator.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/incubator-spark/pull/548#issuecomment-35805718
  
    rebased


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
CodingCat <git@git.apache.org>,"Sat, 22 Feb 2014 15:54:29 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1039] Set the upper bound for...,dev@spark.incubator.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/incubator-spark/pull/472#issuecomment-35805772
  
    rebased


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
CodingCat <git@git.apache.org>,"Sat, 22 Feb 2014 16:03:27 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1041] remove dead code in sta...,dev@spark.incubator.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/incubator-spark/pull/588#issuecomment-35806076
  
    anyone noticed this PR?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
CodingCat <git@git.apache.org>,"Sat, 22 Feb 2014 16:04:20 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1089] fix the regression prob...,dev@spark.incubator.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/incubator-spark/pull/614#issuecomment-35806096
  
    any discussion on this?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
tgravescs <git@git.apache.org>,"Sat, 22 Feb 2014 16:42:47 +0000 (UTC)","[GitHub] incubator-spark pull request: Add Security to Spark - Akka, Http, ...",dev@spark.incubator.apache.org,"Github user tgravescs commented on the pull request:

    https://github.com/apache/incubator-spark/pull/332#issuecomment-35807187
  
    I think I have addressed all the comments and questions.  If I missed something please let me know.
    Note I thought about moving the security files (SparkSaslClient/SparkSaslServer/SecurityManager) into a security sub-directory. Let me know what you think about that.
    
    Also if there is preferred or more optimal way to do the SparkSaslDigestCallbackHandler and SparkSaslClientCallbackHandler rather then having the private nested class let me know.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sat, 22 Feb 2014 17:10:01 +0000 (UTC)","[GitHub] incubator-spark pull request: Add Security to Spark - Akka, Http, ...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/332#issuecomment-35807936
  
    Build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sat, 22 Feb 2014 17:10:01 +0000 (UTC)","[GitHub] incubator-spark pull request: Add Security to Spark - Akka, Http, ...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/332#issuecomment-35807935
  
     Build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sat, 22 Feb 2014 17:11:09 +0000 (UTC)","[GitHub] incubator-spark pull request: Add Security to Spark - Akka, Http, ...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/332#issuecomment-35807967
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12810/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sat, 22 Feb 2014 17:11:09 +0000 (UTC)","[GitHub] incubator-spark pull request: Add Security to Spark - Akka, Http, ...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/332#issuecomment-35807966
  
    Build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Sat, 22 Feb 2014 18:00:18 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1055] fix the SCALA_VERSION a...,dev@spark.incubator.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/634#discussion_r9972227
  
    --- Diff: docker/spark-test/base/Dockerfile ---
    @@ -25,8 +25,8 @@ RUN apt-get update
     # install a few other useful packages plus Open Jdk 7
     RUN apt-get install -y less openjdk-7-jre-headless net-tools vim-tiny sudo openssh-server
     
    -ENV SCALA_VERSION 2.9.3
    -ENV SPARK_VERSION 0.8.1
    +ENV SCALA_VERSION 2.10.3
    +ENV SPARK_VERSION 0.9
    --- End diff --
    
    0.9 is not a valid spark version. Also, how is this actually used in the docker tests? did you run them with this change?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sat, 22 Feb 2014 18:10:23 +0000 (UTC)","[GitHub] incubator-spark pull request: Add Security to Spark - Akka, Http, ...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/332#issuecomment-35809605
  
    Build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sat, 22 Feb 2014 18:10:23 +0000 (UTC)","[GitHub] incubator-spark pull request: Add Security to Spark - Akka, Http, ...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/332#issuecomment-35809604
  
     Build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sat, 22 Feb 2014 18:11:08 +0000 (UTC)","[GitHub] incubator-spark pull request: Add Security to Spark - Akka, Http, ...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/332#issuecomment-35809624
  
    Build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sat, 22 Feb 2014 18:11:08 +0000 (UTC)","[GitHub] incubator-spark pull request: Add Security to Spark - Akka, Http, ...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/332#issuecomment-35809625
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12811/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Sat, 22 Feb 2014 18:13:24 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1089] fix the regression prob...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/614#issuecomment-35809685
  
    // @scrapcodes - mind reviewing this?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Sat, 22 Feb 2014 18:13:35 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1089] fix the regression prob...,dev@spark.incubator.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/614#discussion_r9972286
  
    --- Diff: repl/src/main/scala/org/apache/spark/repl/SparkILoop.scala ---
    @@ -876,7 +876,14 @@ class SparkILoop(in0: Option[BufferedReader], protected val out: JPrintWriter,
           })
     
       def process(settings: Settings): Boolean = savingContextLoader {
    +
    +    SparkILoop.getAddedJars.foreach(settings.classpath.append(_))
         this.settings = settings
    +    //work around for Scala bug
    --- End diff --
    
    Needs a space


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Sat, 22 Feb 2014 18:16:20 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1089] fix the regression prob...,dev@spark.incubator.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/614#discussion_r9972298
  
    --- Diff: repl/src/main/scala/org/apache/spark/repl/SparkILoop.scala ---
    @@ -876,7 +876,14 @@ class SparkILoop(in0: Option[BufferedReader], protected val out: JPrintWriter,
           })
     
       def process(settings: Settings): Boolean = savingContextLoader {
    +
    +    SparkILoop.getAddedJars.foreach(settings.classpath.append(_))
         this.settings = settings
    +    //work around for Scala bug
    +    if (settings.classpath != null)
    +      this.settings.classpath.value_= (settings.classpath.value)
    --- End diff --
    
    Space


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Sat, 22 Feb 2014 18:16:31 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1089] fix the regression prob...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/614#issuecomment-35809765
  
    Jenkins, test this please.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sat, 22 Feb 2014 18:17:25 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1089] fix the regression prob...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/614#issuecomment-35809789
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sat, 22 Feb 2014 18:17:25 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1089] fix the regression prob...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/614#issuecomment-35809788
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
asfgit <git@git.apache.org>,"Sat, 22 Feb 2014 18:34:53 +0000 (UTC)",[GitHub] incubator-spark pull request: Fixed minor typo in worker.py,dev@spark.incubator.apache.org,"Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-spark/pull/630


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
CodingCat <git@git.apache.org>,"Sat, 22 Feb 2014 18:35:55 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1089] fix the regression prob...,dev@spark.incubator.apache.org,"Github user CodingCat commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/614#discussion_r9972360
  
    --- Diff: repl/src/main/scala/org/apache/spark/repl/SparkILoop.scala ---
    @@ -876,7 +876,14 @@ class SparkILoop(in0: Option[BufferedReader], protected val out: JPrintWriter,
           })
     
       def process(settings: Settings): Boolean = savingContextLoader {
    +
    +    SparkILoop.getAddedJars.foreach(settings.classpath.append(_))
         this.settings = settings
    +    //work around for Scala bug
    +    if (settings.classpath != null)
    +      this.settings.classpath.value_= (settings.classpath.value)
    --- End diff --
    
    Hi, @pwendell , actually it is value_= ...(weird name..., but it is the only way to make setByUser to be true....)
    
    def value_=(arg: T) = {
          setByUser = true
          v = arg
          postSetHook()
        }
    
    so 


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sat, 22 Feb 2014 19:09:30 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1089] fix the regression prob...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/614#issuecomment-35811284
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sat, 22 Feb 2014 19:09:31 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1089] fix the regression prob...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/614#issuecomment-35811286
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12812/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
CodingCat <git@git.apache.org>,"Sat, 22 Feb 2014 19:12:35 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1055] fix the SCALA_VERSION a...,dev@spark.incubator.apache.org,"Github user CodingCat commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/634#discussion_r9972514
  
    --- Diff: docker/spark-test/base/Dockerfile ---
    @@ -25,8 +25,8 @@ RUN apt-get update
     # install a few other useful packages plus Open Jdk 7
     RUN apt-get install -y less openjdk-7-jre-headless net-tools vim-tiny sudo openssh-server
     
    -ENV SCALA_VERSION 2.9.3
    -ENV SPARK_VERSION 0.8.1
    +ENV SCALA_VERSION 2.10.3
    +ENV SPARK_VERSION 0.9
    --- End diff --
    
    I think it's just setting the environment in your container, and this SPARK_VERSION...do you mean the right setup should be the one in config.yml?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sat, 22 Feb 2014 19:13:00 +0000 (UTC)","[GitHub] incubator-spark pull request: Add Security to Spark - Akka, Http, ...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/332#issuecomment-35811391
  
    Build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sat, 22 Feb 2014 19:12:59 +0000 (UTC)","[GitHub] incubator-spark pull request: Add Security to Spark - Akka, Http, ...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/332#issuecomment-35811390
  
     Build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Sandy Ryza <sandy.ryza@cloudera.com>,"Sat, 22 Feb 2014 11:19:43 -0800",Re: Signal/Noise Ratio,dev@spark.incubator.apache.org,"Hadoop subprojects (MR, YARN, HDFS) each have a ""dev"" list that contains
discussion as well as a single email whenever a JIRA is filed, and an
""issues"" list with all the JIRA activity.  I think this works out pretty
well.  Subscribing just to the dev list, I can keep up with changes that
are going to be made and follow the ones I care about.  And the issues list
is there if I want the firehose.

Is Apache actually prescriptive that a list with ""dev"" in its name needs to
contain all discussion?  If so, most projects I've followed are violating
this.



"
"""Mattmann, Chris A (3980)"" <chris.a.mattmann@jpl.nasa.gov>","Sat, 22 Feb 2014 19:34:55 +0000",Re: Signal/Noise Ratio,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi Everyone,

The biggest thing is simply making sure that the dev@<project>a.o list is
meaningful
and that meaningful development isn't going on elsewhere that constitute
""decisions"" for the Apache project as reified in code contributions and
overall
stewardship of the effort.

I noticed in a few emails from Github relating to comments on Github Pull
Requests
some conversation which I deemed to be relevant to the project, so I
brought this
up and it came up during graduation.

Here's a general rule of thumb: it's fine if devs converse e.g., on
Github, etc.,
and even if it's project discussion *so long as* that relevant project
discussion
makes it way in some form to the actual, bona fide project's
""dev@<project>a.o list"",
giving others in the community not necessarily on Github or watching
Github or part
of that non Apache conversation to comment, and be part of the community
led decisions
for the project there.

Making its way to that bona fide Apache project dev list can happen in
several ways. 

1. by simply direct 1:1 mapping from Github comments which I see Apache
project
related dev discussion on from time to time and believe fits the criteria
I'm describing
above to the project's dev@<project>.a.o list.

2. by not 1:1 mapping all Github conversation to the dev@<project>.a.o
list, but to 
some other list, e.g., github@<project>a.o, for example (or any of the
others being
discussed) *so long as*, and this is key, that those discussions on Github
get summarized
on the dev@<project>.a.o list giving everyone an opportunity to
participate in the development
by being *here at Apache*.

3. By not worrying about Github at all and simply doing all the
development here at
the ASF. 

4. Others..

My feeling is that some combination of #1 and #2 can pass muster, and the
Apache Spark 
community can decide. That said, noise reduction can also lead to loss of
precision and
accuracy and don't be surprised in reducing that noise if some key thing
makes it onto
a Github PR but didn't make it onto the dev list b/c we are all human and
forgot to summarize
it there. Even if that happens, we assume everyone has good intentions and
we simply
address those issues when/if they come up.

Cheers,
Chris




om%3E


"
Xiangrui Meng <mengxr@gmail.com>,"Sat, 22 Feb 2014 11:49:43 -0800",Re: MLLib - Thoughts about refactoring Updater for LBFGS?,dev@spark.incubator.apache.org,"Hi DB,

It is great to have the L-BFGS optimizer in MLlib and thank you for taking
care of the license issue. I looked through your PR briefly. It contains a
Java translation of the L-BFGS implementation, which is part of the RISO
package. Is it possible that we ask its author to make a release on maven
central and then we add it as a dependency instead of including the code
directly?

Best,
Xiangrui



"
AmplabJenkins <git@git.apache.org>,"Sat, 22 Feb 2014 20:09:32 +0000 (UTC)","[GitHub] incubator-spark pull request: Add Security to Spark - Akka, Http, ...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/332#issuecomment-35813135
  
    Build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sat, 22 Feb 2014 20:09:32 +0000 (UTC)","[GitHub] incubator-spark pull request: Add Security to Spark - Akka, Http, ...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/332#issuecomment-35813136
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12813/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Sat, 22 Feb 2014 20:24:13 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1089] fix the regression prob...,dev@spark.incubator.apache.org,"Github user aarondav commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/614#discussion_r9972843
  
    --- Diff: repl/src/main/scala/org/apache/spark/repl/SparkILoop.scala ---
    @@ -876,7 +876,14 @@ class SparkILoop(in0: Option[BufferedReader], protected val out: JPrintWriter,
           })
     
       def process(settings: Settings): Boolean = savingContextLoader {
    +
    +    SparkILoop.getAddedJars.foreach(settings.classpath.append(_))
         this.settings = settings
    +    // work around for Scala bug
    +    if (settings.classpath != null)
    +      this.settings.classpath.value_= (settings.classpath.value)
    --- End diff --
    
    This is actually a proper setter (see http://dustinmartin.net/getters-and-setters-in-scala/) and you should be able to do
    `this.settings.classpath.value = (settings.classpath.value)`


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
CodingCat <git@git.apache.org>,"Sat, 22 Feb 2014 20:33:52 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1089] fix the regression prob...,dev@spark.incubator.apache.org,"Github user CodingCat commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/614#discussion_r9972883
  
    --- Diff: repl/src/main/scala/org/apache/spark/repl/SparkILoop.scala ---
    @@ -876,7 +876,14 @@ class SparkILoop(in0: Option[BufferedReader], protected val out: JPrintWriter,
           })
     
       def process(settings: Settings): Boolean = savingContextLoader {
    +
    +    SparkILoop.getAddedJars.foreach(settings.classpath.append(_))
         this.settings = settings
    +    // work around for Scala bug
    +    if (settings.classpath != null)
    +      this.settings.classpath.value_= (settings.classpath.value)
    --- End diff --
    
    I tried it before, I remember this much more straightforward way doesn't work...I will try it again right now


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
shivaram <git@git.apache.org>,"Sat, 22 Feb 2014 20:34:58 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1041] remove dead code in sta...,dev@spark.incubator.apache.org,"Github user shivaram commented on the pull request:

    https://github.com/apache/incubator-spark/pull/588#issuecomment-35813839
  
    Jenkins, test this please.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Aaron Davidson <ilikerps@gmail.com>,"Sat, 22 Feb 2014 12:34:56 -0800",Re: Signal/Noise Ratio,dev@spark.incubator.apache.org,"I think #2 sounds totally reasonable (and low-noise). I think we've been
pretty good in the past about making sure dev discussion has happened on
the dev@ mailing list, but there were definitely exceptions simply because
not everyone may have realized that this was the policy.

sparkdev or something so we can explicitly CC dev@ from GitHub, if the
discussion becomes relevant.



"
Patrick Wendell <pwendell@gmail.com>,"Sat, 22 Feb 2014 12:35:58 -0800",Re: Signal/Noise Ratio,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey Chris,

Would the following be consistent with the Apache guidelines?

(a) We establish a culture of not having overall design discussions on
github. Design discussions should to occur on JIRA or on the dev list.
IMO this is pretty much already true, but there are a few exceptions.
(b) We add a mailing list called github@s.a.o which receives the
github traffic. This way everything is available in Apache infra.
(c) Because of our use of JIRA it might make sense to have an
issues@s.a.o list as well similar to what YARN and other projects use.

The github chatter is so noisy that I think, overall, it decreases
engagement with the official developer list. This is the opposite of
what we want.

- Patrick


"
shivaram <git@git.apache.org>,"Sat, 22 Feb 2014 20:36:04 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1041] remove dead code in sta...,dev@spark.incubator.apache.org,"Github user shivaram commented on the pull request:

    https://github.com/apache/incubator-spark/pull/588#issuecomment-35813871
  
    Just checking: Did you test if the Standalone Web UI on EC2 works correctly with this patch ? We've had a bunch of problems before where the link from the Standalone master to the Spark Context Web UI used internal IPs.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Patrick Wendell <pwendell@gmail.com>,"Sat, 22 Feb 2014 12:37:34 -0800",Re: Signal/Noise Ratio,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","btw - I'd prefer reviews@s.a.o instead of github@ to remain more
neutral and flexible.


"
CodingCat <git@git.apache.org>,"Sat, 22 Feb 2014 20:38:10 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1041] remove dead code in sta...,dev@spark.incubator.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/incubator-spark/pull/588#issuecomment-35813920
  
    @shivaram yes, I checked that as long as the user set SPARK_PUBLIC_DNS in spark-env.sh, I remember I made a PR to spark-ec2, and you merged that....


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
CodingCat <git@git.apache.org>,"Sat, 22 Feb 2014 20:42:49 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1089] fix the regression prob...,dev@spark.incubator.apache.org,"Github user CodingCat commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/614#discussion_r9972921
  
    --- Diff: repl/src/main/scala/org/apache/spark/repl/SparkILoop.scala ---
    @@ -876,7 +876,14 @@ class SparkILoop(in0: Option[BufferedReader], protected val out: JPrintWriter,
           })
     
       def process(settings: Settings): Boolean = savingContextLoader {
    +
    +    SparkILoop.getAddedJars.foreach(settings.classpath.append(_))
         this.settings = settings
    +    // work around for Scala bug
    +    if (settings.classpath != null)
    +      this.settings.classpath.value_= (settings.classpath.value)
    --- End diff --
    
    it works....what happened several days ago.........


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
"""Mattmann, Chris A (3980)"" <chris.a.mattmann@jpl.nasa.gov>","Sat, 22 Feb 2014 21:04:51 +0000",Re: Signal/Noise Ratio,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","+1


++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Chris Mattmann, Ph.D.
Chief Architect
Instrument Software and Science Data Systems Section (398)
NASA Jet Propulsion Laboratory Pasadena, CA 91109 USA
Office: 171-283, Mailstop: 171-2"
"""Mattmann, Chris A (3980)"" <chris.a.mattmann@jpl.nasa.gov>","Sat, 22 Feb 2014 21:04:49 +0000",Re: Signal/Noise Ratio,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Patrick, +1 to the below. Great summary and yes I think that would
work great.

Cheers,
Chris

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Chris Mattmann, Ph.D.
Chief Architect
Instrument Software and Science Data Systems Section (398)
NASA Jet Propulsion Laboratory Pasadena, CA 91109 USA
Office: 171-283, Mailstop: 171-246
Email: chris.a.mattmann@nasa.gov
WWW:  http://sunset.usc.edu/~mattmann/
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Adjunct Associate Professor, Computer Science Department
University of Southern California, Los Angeles, CA 90089 USA
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++






.com%3


"
AmplabJenkins <git@git.apache.org>,"Sat, 22 Feb 2014 21:10:45 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1041] remove dead code in sta...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/588#issuecomment-35814910
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sat, 22 Feb 2014 21:10:45 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1041] remove dead code in sta...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/588#issuecomment-35814909
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
shivaram <git@git.apache.org>,"Sat, 22 Feb 2014 21:33:59 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1041] remove dead code in sta...,dev@spark.incubator.apache.org,"Github user shivaram commented on the pull request:

    https://github.com/apache/incubator-spark/pull/588#issuecomment-35815531
  
    Ah right - I just checked the spark-ec2 commit. LGTM. Lets just wait for Jenkins.
    cc @pwendell 


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Sandy Ryza <sandy.ryza@cloudera.com>,"Sat, 22 Feb 2014 14:08:11 -0800",standard way of running a compiled jar,dev@spark.incubator.apache.org,"Hey All,

I've encountered some confusion about how to run a Spark app from a
compiled jar and wanted to bring up the recommended way.

It seems like the current standard options are:
* Build an uber jar that contains the user jar and all of Spark.
* Explicitly include the locations of the Spark jars on the client
machine in the classpath.

Both of these options have a couple issues.

For the uber jar, this means unnecessarily sending all of Spark (and its
dependencies) to every executor, as well as including Spark twice in the
executor classpaths.  This also requires recompiling binaries against the
latest version whenever the cluster version is upgraded, lest executor
classpaths include two different versions of Spark at the same time.

Explicitly including the Spark jars in the classpath is a huge pain because
their locations can vary significantly between different installations and
platforms, and makes the invocation more verbose.

What seems ideal to me is a script that takes a user jar, sets up the Spark
classpath, and runs it.  This means only the user jar gets shipped across
the cluster, but the user doesn't need to figure out how to get the Spark
jars onto the client classpath.  This is similar to the ""hadoop jar""
command commonly used for running MapReduce jobs.

The spark-class script seems to do almost exactly this, but I've been told
it's meant only for internal Spark use (with the possible exception of
yarn-standalone mode). It doesn't take a user jar as an argument, but one
can be added by setting the SPARK_CLASSPATH variable.  This script could be
stabilized for user use.

Another option would be to have a ""spark-app"" script that does what
spark-class does, but also masks the decision of whether to run the driver
in the client process or on the cluster (both standalone and YARN have
modes for both of these).

Does this all make sense?
-Sandy
"
AmplabJenkins <git@git.apache.org>,"Sat, 22 Feb 2014 22:09:50 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1041] remove dead code in sta...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/588#issuecomment-35816464
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12814/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sat, 22 Feb 2014 22:09:50 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1041] remove dead code in sta...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/588#issuecomment-35816463
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Sat, 22 Feb 2014 22:21:28 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1055] fix the SCALA_VERSION a...,dev@spark.incubator.apache.org,"Github user aarondav commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/634#discussion_r9973330
  
    --- Diff: docker/spark-test/base/Dockerfile ---
    @@ -25,8 +25,8 @@ RUN apt-get update
     # install a few other useful packages plus Open Jdk 7
     RUN apt-get install -y less openjdk-7-jre-headless net-tools vim-tiny sudo openssh-server
     
    -ENV SCALA_VERSION 2.9.3
    -ENV SPARK_VERSION 0.8.1
    +ENV SCALA_VERSION 2.10.3
    +ENV SPARK_VERSION 0.9
    --- End diff --
    
    I agree that SPARK_VERSION does not seem to be used in these dockerfiles. It was used in the [original scripts](https://github.com/amplab/docker-scripts/search?q=SPARK_VERSION&ref=cmdform&type=Code) I drew these from, but we don't actually rely on it.
    
    I think we can just remove SPARK_VERSION here so we don't have to think about maintaining it in the future.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Sat, 22 Feb 2014 22:33:26 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1078: Replace lift-json with j...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/582#issuecomment-35817227
  
    There is some desire to get this change in as we have a PR in the pipeline that will use our JSON serialization library more heavily to serialize and log SparkListener events, so it would be nice to test that with this patch in.
    
    Do you think it'd be possible to bring this patch up to date with the comments previously given?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Patrick Wendell <pwendell@gmail.com>,"Sat, 22 Feb 2014 14:53:15 -0800",Re: Signal/Noise Ratio,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey All,

I created a JIRA to ask infra to create a dedicated reviews@ mailing
list for this purpose.

https://issues.apache.org/jira/browse/INFRA-7368

Hopefully they can migrate the github stream to this list so that
people can distinguish it from developer discussions. In parallel, we
are also trying to see if we can use the github status notifier rather
than the constant comments from jenkins.

- Patrick


"
aarondav <git@git.apache.org>,"Sat, 22 Feb 2014 23:05:59 +0000 (UTC)","[GitHub] incubator-spark pull request: For SPARK-1082, Use Curator for ZK i...",dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/611#issuecomment-35818159
  
    This looks great to me. ""curator-recipes-2.4.0"" includes a total of 3 curator dependencies, ZK 3.4.5 (same version as before), and guava 14.0.1 (same version we use). I'll merge this soon if no one else has comments.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Sat, 22 Feb 2014 23:10:51 +0000 (UTC)","[GitHub] incubator-spark pull request: doctest updated for mapValues, flatM...",dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/621#issuecomment-35818246
  
    Thanks! Merged into master.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Sat, 22 Feb 2014 23:32:35 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-929: Fully deprecate usage of ...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/615#issuecomment-35818768
  
    Updated to make de-publicize SPARK_DRIVER_MEMORY. `spark-class` is currently only used in the following cases:
    - spark-shell (which has a memory command-line option that under the hood uses SPARK_DRIVER_MEMORY)
    - developer examples (e.g., StoragePerfTester and FaultToleranceTest; probably do not need memory configuration)
    - launching Mesos executors (SPARK_EXECUTOR_MEMORY is used here)
    - launching standalone workers and masters (SPARK_MASTER/WORKER_MEMORY is used here)
    - and starting YARN and Standalone ""Clients"" (memory probably does not need to be set here, as the Client just sends a request to start a driver within the cluster, and driver memory is set via command line options)
    
    The last point is the most arguable, if for some reason YARN or Standalone Clients may need more than 512 MB. We could always introduce a public SPARK_CLIENT_MEMORY that is used only when launching YARN and Standalone Clients, but I am not sure it's necessary. Otherwise, all of our cases seem covered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
asfgit <git@git.apache.org>,"Sat, 22 Feb 2014 23:34:29 +0000 (UTC)","[GitHub] incubator-spark pull request: doctest updated for mapValues, flatM...",dev@spark.incubator.apache.org,"Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-spark/pull/621


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Sat, 22 Feb 2014 23:37:52 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1055] fix the SCALA_VERSION a...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/634#issuecomment-35818896
  
    Looks good to me. Just ran FaultToleranceTest with these updates and it ran just fine. Merging into master and branch-0.9.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
CodingCat <git@git.apache.org>,"Sat, 22 Feb 2014 23:56:56 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1055] fix the SCALA_VERSION a...,dev@spark.incubator.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/incubator-spark/pull/634#issuecomment-35819274
  
    @pwendell @aarondav thanks


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sun, 23 Feb 2014 00:12:19 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-929: Fully deprecate usage of ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/615#issuecomment-35819593
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sun, 23 Feb 2014 00:12:19 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-929: Fully deprecate usage of ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/615#issuecomment-35819591
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Sun, 23 Feb 2014 00:14:27 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1084. Fix most build warnings,dev@spark.incubator.apache.org,"Github user aarondav commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/586#discussion_r9973707
  
    --- Diff: project/SparkBuild.scala ---
    @@ -340,7 +336,8 @@ object SparkBuild extends Build {
       def streamingSettings = sharedSettings ++ Seq(
         name := ""spark-streaming"",
         libraryDependencies ++= Seq(
    -      ""commons-io"" % ""commons-io"" % ""2.4""
    +      ""commons-io"" % ""commons-io"" % ""2.4"",
    +      ""org.codehaus.jackson"" % ""jackson-mapper-asl"" % ""1.9.11""
    --- End diff --
    
    Looking through the dependency graph of master, I do not see any dependencies that include 1.9.11. 1.8.8 seems to be the version used. Why do you say streaming needs 1.9.11?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
asfgit <git@git.apache.org>,"Sun, 23 Feb 2014 00:33:33 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1055] fix the SCALA_VERSION a...,dev@spark.incubator.apache.org,"Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-spark/pull/634


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Sun, 23 Feb 2014 00:38:29 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1084. Fix most build warnings,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/586#issuecomment-35820063
  
    Do you think it would be possible to separate this into two patches, one for the code warnings and one for the build issues? Code warnings are relatively easy to clean up without issue (and thus merge quickly), but we need to be very careful with dependencies; i.e., we need to go in and confirm that every change is not adding or removing actual dependencies.
    
    The dependencies changes can also be incorporated into a more general ""clean up the build file"" type patch, since I noticed that there may be other issues, like spark streaming including commons-io when it is already included in sharedSettings.



---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Sun, 23 Feb 2014 00:43:24 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1063 Add .sortBy(f) method on ...,dev@spark.incubator.apache.org,"Github user aarondav commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/508#discussion_r9973811
  
    --- Diff: core/src/main/scala/org/apache/spark/rdd/RDD.scala ---
    @@ -394,6 +394,17 @@ abstract class RDD[T: ClassTag](
       def ++(other: RDD[T]): RDD[T] = this.union(other)
     
       /**
    +   * Return this RDD sorted by the given key function.
    +   */
    +  def sortBy[K: Ordering: ClassTag](
    +      f: (T) â‡’ K,
    +      ascending: Boolean = true,
    --- End diff --
    
    It's nice that you've added parameters for ascending and numPartitions, but nicer still would be to use them :)


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Sun, 23 Feb 2014 00:58:39 +0000 (UTC)",[GitHub] incubator-spark pull request: Code cleanup for the `examples` subp...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/419#issuecomment-35820427
  
    Looks like all comments were addressed (...a while ago). Unfortunately, this patch conflicts with the style cleanup patch, and is not currently in a mergeable state.
    
    Please let me know if you can bring this up to date to master. Apologies for our dropping the ball on this one.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sun, 23 Feb 2014 01:10:03 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-929: Fully deprecate usage of ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/615#issuecomment-35820623
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sun, 23 Feb 2014 01:10:03 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-929: Fully deprecate usage of ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/615#issuecomment-35820624
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12815/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Punya Biswal <pbiswal@palantir.com>,"Sun, 23 Feb 2014 01:17:20 +0000",Request to review PR #605,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi all,

Can someone review and/or merge PR #605
<https://github.com/apache/incubator-spark/pull/605>  (convert or move Java
code)? It¹s been sitting for four days.

Thanks!
Punya




"
pwendell <git@git.apache.org>,"Sun, 23 Feb 2014 01:31:16 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1071: Tidy logging strategy an...,dev@spark.incubator.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/570#discussion_r9973958
  
    --- Diff: bagel/pom.xml ---
    @@ -51,6 +51,11 @@
           <artifactId>scalacheck_${scala.binary.version}</artifactId>
           <scope>test</scope>
         </dependency>
    +    <dependency>
    +      <groupId>org.slf4j</groupId>
    +      <artifactId>slf4j-log4j12</artifactId>
    +      <scope>test</scope>
    --- End diff --
    
    I did some auditing of this - I think what we want is that in `core/pom.xml` there should be a dependency on `slf4j-log4j12` that is at the default scope (compile) similar to the sbt build. Then all of the sub projects will pick it up automatically and it won't need to exist anywhere in the sub projects.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Sun, 23 Feb 2014 01:31:31 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1071: Tidy logging strategy an...,dev@spark.incubator.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/570#discussion_r9973960
  
    --- Diff: bagel/pom.xml ---
    @@ -51,6 +51,11 @@
           <artifactId>scalacheck_${scala.binary.version}</artifactId>
           <scope>test</scope>
         </dependency>
    +    <dependency>
    +      <groupId>org.slf4j</groupId>
    +      <artifactId>slf4j-log4j12</artifactId>
    +      <scope>test</scope>
    --- End diff --
    
    currently the one in core is only at the `test` scope.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Fabrizio Milo aka misto <mistobaan@gmail.com>,"Sat, 22 Feb 2014 17:35:24 -0800",scala.collection.immutable.Nil$ cannot be cast to org.apache.spark.util.BoundedPriorityQueue,dev@spark.incubator.apache.org,"Hello Spark Developers,

While trying to use the takeOrdered method of RDD in the following way:

  object AceScoreOrdering extends Ordering[Record] {
      def compare(a:Record, b:Record) = a.score.ace_score compare
b.score.ace_score
    }

    val collected = dataset.takeOrdered(topN)(AceScoreOrdering)

I got this error:

scala.collection.immutable.Nil$ cannot be cast to
org.apache.spark.util.BoundedPriorityQueue
java.lang.ClassCastException: scala.collection.immutable.Nil$ cannot
be cast to org.apache.spark.util.BoundedPriorityQueue
at org.apache.spark.rdd.RDD$$anonfun$top$2.apply(RDD.scala:941)
at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:727)
at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:724)
at org.apache.spark.scheduler.JobWaiter.taskSucceeded(JobWaiter.scala:56)
at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:843)
at org.apache.spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:598)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$start$1$$anon$2$$anonfun$receive$1.applyOrElse(DAGScheduler.scala:190)
at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
at akka.actor.ActorCell.invoke(ActorCell.scala:456)
at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
at akka.dispatch.Mailbox.run(Mailbox.scala:219)
at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)


The error happens in this piece of code ( this is from today's TIP on github) :

  def top(num: Int)(implicit ord: Ordering[T]): Array[T] = {
    mapPartitions { items =>
      val queue = new BoundedPriorityQueue[T](num)
      queue ++= items
      Iterator.single(queue)
    }.reduce { (queue1, queue2) =>
      queue1 ++= queue2
      queue1
    }.toArray.sorted(ord.reverse)
  }

I am not an expert of scala but looks like in case one of the
partition returns a completely empty
collection ( scala.collection.immutable.Nil ? ) then the system is not
able to reduce it to a queue.

Now the real question is that I was trying to emulate this behavior
with a simple test inside RDDSuite.scala:


test(""takeOrdered with nil partition"") {
    case class Custom(value: Int) extends Serializable
    object CustomOrdering extends Ordering[Custom] {
      def compare(a:Custom, b:Custom) = a.value compare b.value
    }
    val nums = Array(Custom(1), Custom(2))
     val rdd = sc.makeRDD(nums, 2)
    val sortedTopK = rdd.takeOrdered(3)(CustomOrdering)
    assert(sortedTopK.size === 2)
    assert(sortedTopK === Array(Custom(1), Custom(2)))
    assert(sortedTopK === nums.sorted(CustomOrdering).take(2))
  }


But out of no where I get this error:

Job aborted: Task not serializable: java.io.NotSerializableException:
org.apache.spark.SparkConf
org.apache.spark.SparkException: Job aborted: Task not serializable:
java.io.NotSerializableException: org.apache.spark.SparkConf
at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1017)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1015)
at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$abortStage(DAGScheduler.scala:1015)
at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitMissingTasks(DAGScheduler.scala:778)
at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitStage(DAGScheduler.scala:721)
at org.apache.spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:551)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$start$1$$anon$2$$anonfun$receive$1.applyOrElse(DAGScheduler.scala:190)
at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
at akka.actor.ActorCell.invoke(ActorCell.scala:456)
at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
at akka.dispatch.Mailbox.run(Mailbox.scala:219)
at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)


Can someone explain me why do I get SparkConf not serializable error ?
out of where ?

Thank you for you time

Fabrizio
-- 
LinkedIn: http://linkedin.com/in/fmilo
Twitter: @fabmilo
Github: http://github.com/Mistobaan/
-----------------------
Simplicity, consistency, and repetition - that's how you get through.
(Jack Welch)
Perfection must be reached by degrees; she requires the slow hand of
time (Voltaire)
The best way to predict the future is to invent it (Alan Kay)

"
pwendell <git@git.apache.org>,"Sun, 23 Feb 2014 01:40:41 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1071: Tidy logging strategy an...,dev@spark.incubator.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/570#discussion_r9973984
  
    --- Diff: project/SparkBuild.scala ---
    @@ -268,9 +272,9 @@ object SparkBuild extends Build {
             ""it.unimi.dsi""             % ""fastutil""         % ""6.4.4"",
             ""colt""                     % ""colt""             % ""1.2.0"",
             ""org.apache.mesos""         % ""mesos""            % ""0.13.0"",
    -        ""net.java.dev.jets3t""      % ""jets3t""           % ""0.7.1"",
    +        ""net.java.dev.jets3t""      % ""jets3t""           % ""0.7.1"" excludeAll(excludeCommonsLogging),
             ""org.apache.derby""         % ""derby""            % ""10.4.2.0""                     % ""test"",
    -        ""org.apache.hadoop""        % hadoopClient       % hadoopVersion excludeAll(excludeJackson, excludeNetty, excludeAsm, excludeCglib),
    +        ""org.apache.hadoop""        % hadoopClient    % hadoopVersion excludeAll(excludeJackson, excludeNetty, excludeAsm, excludeCglib, excludeCommonsLogging, excludeSLF4J),
    --- End diff --
    
    mind making this line up prettily?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Sun, 23 Feb 2014 01:54:20 +0000 (UTC)",[GitHub] incubator-spark pull request: Migrate Java code to Scala or move i...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/605#issuecomment-35821340
  
    Thanks, LGTM! Merged into master.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Sun, 23 Feb 2014 01:54:25 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1071: Tidy logging strategy an...,dev@spark.incubator.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/570#discussion_r9974017
  
    --- Diff: project/SparkBuild.scala ---
    @@ -236,13 +236,15 @@ object SparkBuild extends Build {
       ) ++ net.virtualvoid.sbt.graph.Plugin.graphSettings ++ ScalaStyleSettings
     
    -  val slf4jVersion = ""1.7.2""
    +  val slf4jVersion = ""1.7.5""
     
       val excludeCglib = ExclusionRule(organization = ""org.sonatype.sisu.inject"")
       val excludeJackson = ExclusionRule(organization = ""org.codehaus.jackson"")
       val excludeNetty = ExclusionRule(organization = ""org.jboss.netty"")
       val excludeAsm = ExclusionRule(organization = ""asm"")
       val excludeSnappy = ExclusionRule(organization = ""org.xerial.snappy"")
    +  val excludeCommonsLogging = ExclusionRule(organization = ""commons-logging"")
    +  val excludeSLF4J = ExclusionRule(organization = ""org.slf4j"")
    --- End diff --
    
    It appears that a few other things include slf4j: (zookeeper, the coda hale metrics libs, avro, akka). Should we apply this rule to those as well? Or were you focused only on libraries which might return _newer_ versions?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Sun, 23 Feb 2014 02:06:42 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1071: Tidy logging strategy an...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/570#issuecomment-35821527
  
    hey @srowen this is looking good. My main observation was I think we should add the slf4j-log4j bridge as a compile dependency in core in the maven build and then remove it from all the modules. Don't know why that's different than the current sbt build - I think we never noticed because it ends up getting bundled anyways since some of the sub modules include it.
    
    I also added instructions for generating the dependency graph in both builds on the wiki:
    https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools
    
    It might be useful for you when auditing the effects of these changes.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
asfgit <git@git.apache.org>,"Sun, 23 Feb 2014 02:35:42 +0000 (UTC)",[GitHub] incubator-spark pull request: Migrate Java code to Scala or move i...,dev@spark.incubator.apache.org,"Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-spark/pull/605


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Patrick Wendell <pwendell@gmail.com>,"Sat, 22 Feb 2014 19:05:49 -0800",Re: Request to review PR #605,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey Punya,

It's sufficient to just ping the request on github rather than e-mail
the dev list. Sometimes it can takes a few days for people to get to
looking at patches...

- Patrick


"
pwendell <git@git.apache.org>,"Sun, 23 Feb 2014 03:52:32 +0000 (UTC)",[GitHub] incubator-spark pull request: Allow Bundling of Tachyon with Spark...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/632#issuecomment-35823177
  
    Jenkins, test this please.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sun, 23 Feb 2014 04:10:54 +0000 (UTC)",[GitHub] incubator-spark pull request: Allow Bundling of Tachyon with Spark...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/632#issuecomment-35823421
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sun, 23 Feb 2014 04:10:54 +0000 (UTC)",[GitHub] incubator-spark pull request: Allow Bundling of Tachyon with Spark...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/632#issuecomment-35823422
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
shivaram <git@git.apache.org>,"Sun, 23 Feb 2014 04:22:10 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1041] remove dead code in sta...,dev@spark.incubator.apache.org,"Github user shivaram commented on the pull request:

    https://github.com/apache/incubator-spark/pull/588#issuecomment-35823579
  
    Thanks for the fix. Merged this


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
CodingCat <git@git.apache.org>,"Sun, 23 Feb 2014 04:23:44 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1041] remove dead code in sta...,dev@spark.incubator.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/incubator-spark/pull/588#issuecomment-35823609
  
    @shivaram thanks


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
asfgit <git@git.apache.org>,"Sun, 23 Feb 2014 04:34:35 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1041] remove dead code in sta...,dev@spark.incubator.apache.org,"Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-spark/pull/588


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sun, 23 Feb 2014 05:10:27 +0000 (UTC)",[GitHub] incubator-spark pull request: Allow Bundling of Tachyon with Spark...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/632#issuecomment-35824152
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sun, 23 Feb 2014 05:10:27 +0000 (UTC)",[GitHub] incubator-spark pull request: Allow Bundling of Tachyon with Spark...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/632#issuecomment-35824153
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12816/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
coderxiang <git@git.apache.org>,"Sun, 23 Feb 2014 05:30:21 +0000 (UTC)",[GitHub] incubator-spark pull request: allCollect functions for RDD,dev@spark.incubator.apache.org,"GitHub user coderxiang opened a pull request:

    https://github.com/apache/incubator-spark/pull/635

    allCollect functions for RDD

     Two methods (`allCollect`, `allCollectBroadcast`) are added to `RDD[T]`, which output a new `RDD[Array[T]]` instance with each partition containing all of the records of the original RDD stored in a single `Array[T]` instance (the same as RDD.collect). This functionality can be useful in machine learning tasks that require sharing updated parameters across partitions.
    
    Method `allCollect` creates a new `AllCollectedRDD` while method `allCollectBroadcast` applies broadcasting. Both of them need collecting the data and therefore should deliver similar performance.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/coderxiang/incubator-spark allCollect

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/635.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #635
    
----
commit 74e7817d73bf635f9d27149d609b4da1c1e78f51
Author: lebesgue <lebesgue@lebesgue.net>
Date:   2014-02-21T03:46:30Z

    add a simple implementation of allCollect using AllCollectedRDD

commit b619cbd2642502a80f53bf6f4b753301cb157956
Author: lebesgue <lebesgue@lebesgue.net>
Date:   2014-02-21T03:51:07Z

    code reorganization

commit f727cd936dfad513238d8a127fdc15507a4025b0
Author: lebesgue <lebesgue@lebesgue.net>
Date:   2014-02-21T06:21:09Z

    add the implementation of allCollect using a broadcast variable

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
CodingCat <git@git.apache.org>,"Sun, 23 Feb 2014 05:34:29 +0000 (UTC)",[GitHub] incubator-spark pull request: allCollect functions for RDD,dev@spark.incubator.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/incubator-spark/pull/635#issuecomment-35824430
  
    I think now they require that every PR corresponds to a certain JIRA...https://spark-project.atlassian.net/browse/SPARK


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Sun, 23 Feb 2014 05:35:19 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1116: The spark-shell fails un...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/627#issuecomment-35824436
  
    Hey @berngp - I can't reproduce this problem. There is already a line in `spark-classpath` that adds this correctly:
    ```
    else
      # Else use spark-assembly jar from either RELEASE or assembly directory
      if [ -f ""$FWDIR/RELEASE"" ]; then
        ASSEMBLY_JAR=`ls ""$FWDIR""/jars/spark-assembly*.jar`
      else
    ```
    
    I also did a made fresh distribution and tested the shell and it worked properly.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
coderxiang <git@git.apache.org>,"Sun, 23 Feb 2014 05:38:20 +0000 (UTC)",[GitHub] incubator-spark pull request: allCollect functions for RDD,dev@spark.incubator.apache.org,"Github user coderxiang commented on the pull request:

    https://github.com/apache/incubator-spark/pull/635#issuecomment-35824486
  
    @CodingCat  Thanks for mentioning. I do create a JIRA here: https://spark-project.atlassian.net/browse/SPARK-1122


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
CodingCat <git@git.apache.org>,"Sun, 23 Feb 2014 05:41:45 +0000 (UTC)",[GitHub] incubator-spark pull request: allCollect functions for RDD,dev@spark.incubator.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/incubator-spark/pull/635#issuecomment-35824526
  
    @coderxiang I mean they suggest including the JIRA id in the PR title,I remember so...there was a discussion in dev list...cc: @pwendell


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
CodingCat <git@git.apache.org>,"Sun, 23 Feb 2014 05:45:11 +0000 (UTC)",[GitHub] incubator-spark pull request: allCollect functions for RDD,dev@spark.incubator.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/incubator-spark/pull/635#issuecomment-35824561
  
    ah, it's here: http://apache-spark-developers-list.1001551.n3.nabble.com/Proposal-for-JIRA-and-Pull-Request-Policy-td505.html


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
coderxiang <git@git.apache.org>,"Sun, 23 Feb 2014 05:51:30 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1122: allCollect functions for...,dev@spark.incubator.apache.org,"Github user coderxiang commented on the pull request:

    https://github.com/apache/incubator-spark/pull/635#issuecomment-35824621
  
    @CodingCat Updated. Thanks!


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Mridul Muralidharan <mridul@gmail.com>,"Sun, 23 Feb 2014 11:36:05 +0530",[DISCUSS] Extending public API,dev@spark.incubator.apache.org,"Hi,

  Over the past few months, I have seen a bunch of pull requests which have
extended spark api ... most commonly RDD itself.

Most of them are either relatively niche case of specialization (which
might not be useful for most cases) or idioms which can be expressed
(sometimes with minor perf penalty) using existing api.

While all of them have non zero value (hence the effort to contribute, and
gladly welcomed !) they are extending the api in nontrivial ways and have a
maintenance cost ... and we already have a pending effort to clean up our
interfaces prior to 1.0

I believe there is a need to keep exposed api succint, expressive and
functional in spark; while at the same time, encouraging extensions and
specialization within spark codebase so that other users can benefit from
the shared contributions.

contribute user generated specializations, helper utils, etc : bundled as
part of spark, but not part of core itself.

Thoughts, comments ?

Regards,
Mridul
"
Nan Zhu <zhunanmcgill@gmail.com>,"Sun, 23 Feb 2014 01:14:12 -0500",Re: [DISCUSS] Extending public API,dev@spark.incubator.apache.org,"piggybank-like +1 

-- 
Nan Zhu





"
AmplabJenkins <git@git.apache.org>,"Sun, 23 Feb 2014 06:11:25 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1122: allCollect functions for...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/635#issuecomment-35824836
  
    Can one of the admins verify this patch?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Surendranauth Hiraman <suren.hiraman@velos.io>,"Sun, 23 Feb 2014 01:11:57 -0500",Re: [DISCUSS] Extending public API,dev@spark.incubator.apache.org,"+1






-- 

SUREN HIRAMAN, VP TECHNOLOGY
Velos
Simple. Powerful. Predictions.

440 NINTH AVENUE, 11TH FLOOR
NEW YORK, NY 10001
O: (917) 525-2466 ext. 105
F: 646.349.4063
E: suren.hiraman@v <suren.hiraman@sociocast.com>elos.io
W: www.velos.io
"
Cheng Lian <rhythm.mail@gmail.com>,"Sun, 23 Feb 2014 14:16:17 +0800",Re: [DISCUSS] Extending public API,dev@spark.incubator.apache.org,"+1

We can maintain a contrib component just like what Akka and many other projects do.  With the help of Scala implicits, extensions to the RDD API can be done in a non-intrusive way and leave spark-core untouched.


have
and
have a
our
and
from
as


"
Ryan LeCompte <lecompte@gmail.com>,"Sat, 22 Feb 2014 22:16:36 -0800",Re: [DISCUSS] Extending public API,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Perhaps something similar to akka-contrib? Maybe spark-contrib?

https://github.com/akka/akka/tree/master/akka-contrib


"
rezazadeh <git@git.apache.org>,"Sun, 23 Feb 2014 06:16:55 +0000 (UTC)",[GitHub] incubator-spark pull request: Principal Component Analysis,dev@spark.incubator.apache.org,"Github user rezazadeh commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/564#discussion_r9974545
  
    --- Diff: mllib/src/main/scala/org/apache/spark/mllib/linalg/PCA.scala ---
    @@ -0,0 +1,119 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one or more
    + * contributor license agreements.  See the NOTICE file distributed with
    + * this work for additional information regarding copyright ownership.
    + * The ASF licenses this file to You under the Apache License, Version 2.0
    + * (the ""License""); you may not use this file except in compliance with
    + * the License.  You may obtain a copy of the License at
    + *
    + *    http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.spark.mllib.linalg
    +
    +import org.apache.spark.SparkContext
    +import org.apache.spark.SparkContext._
    +import org.apache.spark.rdd.RDD
    +
    +import org.apache.spark.mllib.util._
    +
    +
    +/**
    + * Class used to obtain principal components
    + */
    +class PCA {
    +  private var k: Int = 1
    +
    +  /**
    +   * Set the number of top-k principle components to return
    +   */
    +  def setK(k: Int): PCA = {
    +    this.k = k
    +    this
    +  }
    +
    +   /**
    +   * Compute PCA using the current set parameters
    +   */
    +  def compute(matrix: DenseMatrix): DenseMatrix = {
    +    computePCA(matrix, k)
    +  }
    +
    +  /**
    +  * Principal Component Analysis.
    +  * Computes the top k principal component coefficients for the m-by-n data matrix X.
    +  * Rows of X correspond to observations and columns correspond to variables. 
    +  * The coefficient matrix is n-by-k. Each column of coeff contains coefficients
    +  * for one principal component, and the columns are in descending 
    +  * order of component variance.
    +  * This function centers the data and uses the 
    +  * singular value decomposition (SVD) algorithm. 
    +  *
    +  * All input and output is expected in DenseMatrix format
    +  *
    +  * @param matrix dense matrix to perform pca on
    +  * @param k Recover k principal components
    +  * @return An nxk matrix of principal components
    +  */
    +  def computePCA(matrix: DenseMatrix, k: Int): DenseMatrix = {
    +    val m = matrix.m
    +    val n = matrix.n
    +
    +    if (m <= 0 || n <= 0) {
    +      throw new IllegalArgumentException(""Expecting a well-formed matrix"")
    +    }
    +
    +    // compute column sums and normalize matrix
    +    val rawData = matrix.rows.flatMap{
    +      x => Array.tabulate(x.data.size)(idx => MatrixEntry(x.i, idx, x.data(idx)))
    +    }
    +    val colSums = rawData.map(entry => (entry.j, entry.mval)).reduceByKey(_ + _)
    +    val data = rawData.map(entry => (entry.j, (entry.i, entry.mval))).join(colSums).map{
    +      case (col, ((row, mval), colsum)) =>
    +        MatrixEntry(row, col, (mval - colsum / m.toDouble) / Math.sqrt(n-1)) }
    --- End diff --
    
    @mengxr Note that there are two maps done on lines 73 and 76. These won't create any shuffling of the data. The reduce on line 76 will create a shuffle, which will be combined, so that each node does not output anything other than one small vector of column -> sum. Then those few vectors get shuffled. Building a combiner for Array[Double] here and summing those isn't going to buy us anything.
    
    Building a Dense SVD warrants another PR, since that will need a new implementation for SVD, new tests, documentation, and examples. I will do this in another PR, maybe at the same time as Dense square SVD, depending on what algorithm we decide.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Amandeep Khurana <amansk@gmail.com>,"Sat, 22 Feb 2014 22:23:27 -0800",Re: [DISCUSS] Extending public API,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Mridul,

Can you give examples of APIs that people have contributed (or wanted
to contribute) but you categorize as something that would go into
piggybank-like (sparkbank)? Curious to know how you'd decide what
should go where.

Amandeep


"
Matei Zaharia <matei.zaharia@gmail.com>,"Sat, 22 Feb 2014 22:48:41 -0800",Re: standard way of running a compiled jar,dev@spark.incubator.apache.org,"Hey Sandy,

In the long run, the ability to submit driver programs to run in the cluster (added in 0.9 as org.apache.spark.deploy.Client) might solve this. This is a feature currently available in the standalone mode that runs the driver on a worker node, but it is also how YARN works by default, and it wouldn’t be too bad to do in Mesos. With this, the user could compile a JAR that excludes Spark and still get Spark on the classpath.

This was added in 0.9 as a slightly harder to invoke feature mainly to be used for Spark Streaming (since the cluster can also automatically restart your driver), but we can create a script around it for submissions. I’d like to see a design for such a script that takes into account all the deploy modes though, because it would be confusing to use it one way on YARN and another way on standalone for instance. Already the YARN submit client kind of does what you’re looking for.

Matei


its
the
the
because
and
Spark
across
Spark
told
one
could be
driver


"
mateiz <git@git.apache.org>,"Sun, 23 Feb 2014 06:57:27 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user mateiz commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35825377
  
    Jenkins, retest this please


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
berngp <git@git.apache.org>,"Sun, 23 Feb 2014 07:03:55 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1116: The spark-shell fails un...,dev@spark.incubator.apache.org,"Github user berngp commented on the pull request:

    https://github.com/apache/incubator-spark/pull/627#issuecomment-35825450
  
    @pwendell sorry, not an issue.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
berngp <git@git.apache.org>,"Sun, 23 Feb 2014 07:03:55 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1116: The spark-shell fails un...,dev@spark.incubator.apache.org,"Github user berngp closed the pull request at:

    https://github.com/apache/incubator-spark/pull/627


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sun, 23 Feb 2014 07:11:48 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35825552
  
     Build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sun, 23 Feb 2014 07:11:48 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35825553
  
    Build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
CodingCat <git@git.apache.org>,"Sun, 23 Feb 2014 07:30:57 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1102] Create a saveAsNewAPIHa...,dev@spark.incubator.apache.org,"GitHub user CodingCat opened a pull request:

    https://github.com/apache/incubator-spark/pull/636

    [SPARK-1102] Create a saveAsNewAPIHadoopDataset method

    Create a saveAsNewAPIHadoopDataset method
    
    By @mateiz: ""Right now RDDs can only be saved as files using the new Hadoop API, not as ""datasets"" with no filename and just a JobConf. See http://codeforhire.com/2014/02/18/using-spark-with-mongodb/ for an example of how you have to give a bogus filename. For the old Hadoop API, we have saveAsHadoopDataset.""

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/CodingCat/incubator-spark SPARK-1102

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/636.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #636
    
----
commit fac89212f5b964eabfb316256daef774dffc7a5f
Author: CodingCat <zhunansjtu@gmail.com>
Date:   2014-02-23T07:18:36Z

    Create a saveAsNewAPIHadoopDataset method

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
markhamstra <git@git.apache.org>,"Sun, 23 Feb 2014 07:40:28 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1122: allCollect functions for...,dev@spark.incubator.apache.org,"Github user markhamstra commented on the pull request:

    https://github.com/apache/incubator-spark/pull/635#issuecomment-35825888
  
    Huh?  I don't get the point of these at all.
    
    At first glance, allCollect looks like a really bad idea.  Collecting the entire contents of an RDD to the driver process only to immediately turn around and push all of that data (or in this case, multiple copies of the data!) back across the network is an anti-pattern and generally a very poor design choice that cannot scale to large data -- if you can handle all of the data within the driver process, then why are you using a distributed, big-data framework in the first place?
    
    allCollectBroadcast makes even less sense to me.  Some workflows do demand collecting a relatively small amount of data to the driver and then broadcasting a small amount back to the workers for use in further computations, but why would I then want to go through the extra step of pushing the broadcast values into a strange-looking RDD instead of just using the broadcast variable directly?
    
    It's going to take a lot of persuading to convince me that either of these are things we want to promote and support in the 1.0 API.  That doesn't mean that I'm not listening, but I am far from convinced at this point.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Mark Hamstra <mark@clearstorydata.com>,"Sat, 22 Feb 2014 23:46:52 -0800",Re: [DISCUSS] Extending public API,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","I'm also curious what the vetting process will be for this spark-contrib
code?  Does inclusion in spark-contrib mean that it has received some sort
of review and official blessing, or is contrib just a dumping ground for
code of questionable quality, utility, maintenance, etc.?



"
AmplabJenkins <git@git.apache.org>,"Sun, 23 Feb 2014 08:10:44 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35826379
  
    Build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sun, 23 Feb 2014 08:10:44 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35826380
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12817/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sun, 23 Feb 2014 08:11:31 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1102] Create a saveAsNewAPIHa...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/636#issuecomment-35826394
  
    Can one of the admins verify this patch?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Nan Zhu <zhunanmcgill@gmail.com>,"Sun, 23 Feb 2014 03:48:33 -0500",Anyone wants to look at SPARK-1123?,dev@spark.incubator.apache.org,"Hi, all  

I found the weird thing on saveAsNewAPIHadoopFile  in PairRDDFunctions.scala when working on the other issue,  

saveAsNewAPIHadoopFile throws java.lang.InstantiationException all the time

I checked the commit history of the file, it seems that the API exists for a long time, no one else found this? (thatâ€™s the reason Iâ€™m confusing)

Best,  

--  
Nan Zhu

"
Cheng Lian <rhythm.mail@gmail.com>,"Sun, 23 Feb 2014 17:15:16 +0800",Re: [DISCUSS] Extending public API,dev@spark.incubator.apache.org,"I think SPARK-1063 (PR-503) “Add .sortBy(f) method on RDD” would be a good example. Note that I’m not saying that this PR is already qualified to be accepted, just take it as an example:
JIRA issue: https://spark-project.atlassian.net/browse/SPARK-1063
GitHub PR: https://github.com/apache/incubator-spark/pull/508


have
(which
contribute, and
have a
our
and
from
bundled as

"
Sean Owen <sowen@cloudera.com>,"Sun, 23 Feb 2014 09:33:55 +0000",Re: [DISCUSS] Extending public API,dev@spark.incubator.apache.org,"Thank you for bringing this up. I think the current committers are
bravely facing down a flood of PRs, and this (among other things) is a
step that needs to be taken to scale up and keep this fun. I'd love to
have a separate discussion about more steps, but for here I offer two
bits of advice from experience:

First, you guys most certainly can and should say 'no' to some
changes. It's part of keeping the project coherent. It's always good
to try to include all contributions, but, appreciating contributions
does not always mean accepting them. I have seen projects turned to
mush by the 'anything's welcome' mentality. Push back on contributors
to contribute the thing you think is right. Please keep the API
succinct, yes.

Second, contrib/ modules are problematic. It becomes a ball of legacy
code that you still have to keep maintaining to compile and run. In a
world of Github, I think 'contrib' stuff just belongs in other repos.
I know it sounds harmless to have a contrib, but I think you'd find
the consensus here is that contrib is a mistake.

$0.02 --
--
Sean Owen | Director, Data Science | London



"
srowen <git@git.apache.org>,"Sun, 23 Feb 2014 09:36:48 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1071: Tidy logging strategy an...,dev@spark.incubator.apache.org,"Github user srowen commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/570#discussion_r9975070
  
    --- Diff: project/SparkBuild.scala ---
    @@ -236,13 +236,15 @@ object SparkBuild extends Build {
       ) ++ net.virtualvoid.sbt.graph.Plugin.graphSettings ++ ScalaStyleSettings
     
    -  val slf4jVersion = ""1.7.2""
    +  val slf4jVersion = ""1.7.5""
     
       val excludeCglib = ExclusionRule(organization = ""org.sonatype.sisu.inject"")
       val excludeJackson = ExclusionRule(organization = ""org.codehaus.jackson"")
       val excludeNetty = ExclusionRule(organization = ""org.jboss.netty"")
       val excludeAsm = ExclusionRule(organization = ""asm"")
       val excludeSnappy = ExclusionRule(organization = ""org.xerial.snappy"")
    +  val excludeCommonsLogging = ExclusionRule(organization = ""commons-logging"")
    +  val excludeSLF4J = ExclusionRule(organization = ""org.slf4j"")
    --- End diff --
    
    I thought I got all of them but let me double-check with mvn dependency:tree, and then check that the sbt build does the same.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
srowen <git@git.apache.org>,"Sun, 23 Feb 2014 09:36:53 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1071: Tidy logging strategy an...,dev@spark.incubator.apache.org,"Github user srowen commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/570#discussion_r9975071
  
    --- Diff: project/SparkBuild.scala ---
    @@ -268,9 +272,9 @@ object SparkBuild extends Build {
             ""it.unimi.dsi""             % ""fastutil""         % ""6.4.4"",
             ""colt""                     % ""colt""             % ""1.2.0"",
             ""org.apache.mesos""         % ""mesos""            % ""0.13.0"",
    -        ""net.java.dev.jets3t""      % ""jets3t""           % ""0.7.1"",
    +        ""net.java.dev.jets3t""      % ""jets3t""           % ""0.7.1"" excludeAll(excludeCommonsLogging),
             ""org.apache.derby""         % ""derby""            % ""10.4.2.0""                     % ""test"",
    -        ""org.apache.hadoop""        % hadoopClient       % hadoopVersion excludeAll(excludeJackson, excludeNetty, excludeAsm, excludeCglib),
    +        ""org.apache.hadoop""        % hadoopClient    % hadoopVersion excludeAll(excludeJackson, excludeNetty, excludeAsm, excludeCglib, excludeCommonsLogging, excludeSLF4J),
    --- End diff --
    
    Will do.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
srowen <git@git.apache.org>,"Sun, 23 Feb 2014 09:37:09 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1071: Tidy logging strategy an...,dev@spark.incubator.apache.org,"Github user srowen commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/570#discussion_r9975073
  
    --- Diff: bagel/pom.xml ---
    @@ -51,6 +51,11 @@
           <artifactId>scalacheck_${scala.binary.version}</artifactId>
           <scope>test</scope>
         </dependency>
    +    <dependency>
    +      <groupId>org.slf4j</groupId>
    +      <artifactId>slf4j-log4j12</artifactId>
    +      <scope>test</scope>
    --- End diff --
    
    Yeah I think that's best, will modify it accordingly.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Mridul Muralidharan <mridul@gmail.com>,"Sun, 23 Feb 2014 15:10:53 +0530",Re: [DISCUSS] Extending public API,dev@spark.incubator.apache.org,"Good point, and I was purposefully vague on that since that is something
which our community should evolve imo : this was just an initial proposal
:-)

For example: there are multiple ways to do cartesian - and each has its own
trade offs.

Another candidate could be, as I mentioned, new methods which can be
expressed as sequences of existing methods but would be slightly more
performent if done in one shot - like the self cartesian pr, various types
of join (which can become a contrib of its own btw !), experiments using
key indexes, ordering, etc.

Addition into sparkbank or contrib (or something bettrr named !) does not
preclude future migration into core ... just an initial staging area for us
to e olve the api and get user feedback; without necessarily making spark
core api unstable.

Obviously, it is not a dumping ground for broken code/ideas ... and must
follow same level of scrutiny and rigour before committing.
Regards
Mridul

"
srowen <git@git.apache.org>,"Sun, 23 Feb 2014 09:41:01 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1084. Fix most build warnings,dev@spark.incubator.apache.org,"Github user srowen commented on the pull request:

    https://github.com/apache/incubator-spark/pull/586#issuecomment-35827729
  
    @aarondav Sure, it's already split into commits, and one of them has the dependency changes: https://github.com/srowen/incubator-spark/commit/6f2f67974bfedd40bafccd77abd0860dcbba4061 Move this to another PR?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
srowen <git@git.apache.org>,"Sun, 23 Feb 2014 09:46:20 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1084. Fix most build warnings,dev@spark.incubator.apache.org,"Github user srowen commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/586#discussion_r9975099
  
    --- Diff: project/SparkBuild.scala ---
    @@ -340,7 +336,8 @@ object SparkBuild extends Build {
       def streamingSettings = sharedSettings ++ Seq(
         name := ""spark-streaming"",
         libraryDependencies ++= Seq(
    -      ""commons-io"" % ""commons-io"" % ""2.4""
    +      ""commons-io"" % ""commons-io"" % ""2.4"",
    +      ""org.codehaus.jackson"" % ""jackson-mapper-asl"" % ""1.9.11""
    --- End diff --
    
    This was just making the sbt build consistent with Maven. But yeah on second glance it does look like Streaming doesn't even use Jackson! This can be removed in both places. Commons IO is used. I'll wait on your comment about splitting into a PR to move forward with fixes like this in this PR.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
srowen <git@git.apache.org>,"Sun, 23 Feb 2014 09:47:26 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1084. Fix most build warnings,dev@spark.incubator.apache.org,"Github user srowen commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/586#discussion_r9975107
  
    --- Diff: project/SparkBuild.scala ---
    @@ -340,7 +336,8 @@ object SparkBuild extends Build {
       def streamingSettings = sharedSettings ++ Seq(
         name := ""spark-streaming"",
         libraryDependencies ++= Seq(
    -      ""commons-io"" % ""commons-io"" % ""2.4""
    +      ""commons-io"" % ""commons-io"" % ""2.4"",
    +      ""org.codehaus.jackson"" % ""jackson-mapper-asl"" % ""1.9.11""
    --- End diff --
    
    Also, then I don't see a particular reason to bother excluding jackson (1.8.8) dependencies from Hadoop. It could be a problem to have no Jackson at all. I can undo that.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
mridulm <git@git.apache.org>,"Sun, 23 Feb 2014 14:35:56 +0000 (UTC)","[GitHub] incubator-spark pull request: Add Security to Spark - Akka, Http, ...",dev@spark.incubator.apache.org,"Github user mridulm commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/332#discussion_r9975936
  
    --- Diff: core/src/main/scala/org/apache/spark/network/ConnectionManager.scala ---
    @@ -483,10 +496,131 @@ private[spark] class ConnectionManager(port: Int, conf: SparkConf) extends Loggi
         /*handleMessage(connection, message)*/
       }
     
    -  private def handleMessage(connectionManagerId: ConnectionManagerId, message: Message) {
    +  private def handleClientAuthNeg(
    +      waitingConn: SendingConnection,
    +      securityMsg: SecurityMessage, 
    +      connectionId : ConnectionId) {
    +    if (waitingConn.isSaslComplete()) {
    +      logDebug(""Client sasl completed for id: ""  + waitingConn.connectionId)
    +      connectionsAwaitingSasl -= waitingConn.connectionId
    +      waitingConn.getAuthenticated().synchronized {
    +        waitingConn.getAuthenticated().notifyAll();
    +      }
    +      return
    +    } else {
    +      var replyToken : Array[Byte] = null
    +      try {
    +        replyToken = waitingConn.sparkSaslClient.saslResponse(securityMsg.getToken);
    +        if (waitingConn.isSaslComplete()) {
    +          logDebug(""Client sasl completed after evaluate for id: "" + waitingConn.connectionId)
    +          connectionsAwaitingSasl -= waitingConn.connectionId
    +          waitingConn.getAuthenticated().synchronized {
    +            waitingConn.getAuthenticated().notifyAll()
    +          }
    +          return
    +        }
    +        var securityMsgResp = SecurityMessage.fromResponse(replyToken, securityMsg.getConnectionId)
    +        var message = securityMsgResp.toBufferMessage
    +        if (message == null) throw new Exception(""Error creating security message"")
    +        sendSecurityMessage(waitingConn.getRemoteConnectionManagerId(), message)
    +      } catch  {
    +        case e: Exception => {
    +          logError(""Error doing sasl client: "" + e)
    +          waitingConn.close()
    +          throw new Exception(""error evaluating sasl response: "" + e)
    +        }
    +      }
    +    }
    +  }
    +
    +  private def handleServerAuthNeg(
    +      connection: Connection, 
    +      securityMsg: SecurityMessage,
    +      connectionId: ConnectionId) {
    +    if (!connection.isSaslComplete()) {
    +      logDebug(""saslContext not established"")
    +      var replyToken : Array[Byte] = null
    +      try {
    +        connection.synchronized {
    +          if (connection.sparkSaslServer == null) {
    +            logDebug(""Creating sasl Server"")
    +            connection.sparkSaslServer = new SparkSaslServer(securityManager)
    +          }
    +        }
    +        replyToken = connection.sparkSaslServer.response(securityMsg.getToken)
    +        if (connection.isSaslComplete()) {
    +          logDebug(""Server sasl completed: "" + connection.connectionId) 
    +        } else {
    +          logDebug(""Server sasl not completed: "" + connection.connectionId)
    +        }
    +        if (replyToken != null) {
    +          var securityMsgResp = SecurityMessage.fromResponse(replyToken, securityMsg.getConnectionId)
    +          var message = securityMsgResp.toBufferMessage
    +          if (message == null) throw new Exception(""Error creating security Message"")
    +          sendSecurityMessage(connection.getRemoteConnectionManagerId(), message)
    +        } 
    +      } catch {
    +        case e: Exception => {
    +          logError(""Error in server auth negotiation: "" + e)
    +          // It would probably be better to send an error message telling other side auth failed
    +          // but for now just close
    +          connection.close()
    +        }
    +      }
    +    } else {
    +      logDebug(""connection already established for this connection id: "" + connection.connectionId) 
    +    }
    +  }
    +
    +
    +  private def handleAuthentication(conn: Connection, bufferMessage: BufferMessage): Boolean = {
    +    if (bufferMessage.isSecurityNeg) {
    +      logDebug(""This is security neg message"")
    +
    +      // parse as SecurityMessage
    +      val securityMsg = SecurityMessage.fromBufferMessage(bufferMessage)
    +      val connectionId = new ConnectionId(securityMsg.getConnectionId)
    +
    +      connectionsAwaitingSasl.get(connectionId) match {
    +        case Some(waitingConn) => {
    +          // Client - this must be in response to us doing Send
    +          logDebug(""Client handleAuth for id: "" +  waitingConn.connectionId)
    +          handleClientAuthNeg(waitingConn, securityMsg, connectionId)
    +        }
    +        case None => {
    +          // Server - someone sent us something and we haven't authenticated yet
    +          logDebug(""Server handleAuth for id: "" + connectionId)
    +          handleServerAuthNeg(conn, securityMsg, connectionId)
    +        }
    +      }
    --- End diff --
    
    sigh, you are right - we have the weird split brain for connections in spark effectively doubling the number of sockets.
    dont think there is an alternative to this ...


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Nick Pentreath <nick.pentreath@gmail.com>,"Sun, 23 Feb 2014 18:06:24 +0200",Re: Anyone wants to look at SPARK-1123?,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi

What KeyClass and ValueClass are you trying to save as the keys/values of
your dataset?




"
Lianhui Wang <lianhuiwang09@gmail.com>,"Sun, 23 Feb 2014 22:49:07 +0800",ask for receiving spark user mailing list,dev@spark.incubator.apache.org,"hi
  i want to  ask for receiving spark user mailing list

-- 
thanks

ÍõÁª»Ô(Lianhui Wang)
blog; http://blog.csdn.net/lance_123
ÐËÈ¤·½Ïò£ºÊý¾Ý¿â£¬·Ö²¼Ê½£¬Êý¾ÝÍÚ¾ò£¬±à³ÌÓïÑÔ£¬»¥ÁªÍø¼¼ÊõµÈ
"
MLnick <git@git.apache.org>,"Sun, 23 Feb 2014 16:33:41 +0000 (UTC)",[GitHub] incubator-spark pull request: MLLIB-25: Implicit ALS runs out of m...,dev@spark.incubator.apache.org,"Github user MLnick commented on the pull request:

    https://github.com/apache/incubator-spark/pull/629#issuecomment-35835626
  
    @srowen good catch, thanks Sean. Didn't really think about this when I wrote it. Shows that testing on larger scale input data / params is always required!


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
willb <git@git.apache.org>,"Sun, 23 Feb 2014 17:25:52 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1078: Replace lift-json with j...,dev@spark.incubator.apache.org,"Github user willb commented on the pull request:

    https://github.com/apache/incubator-spark/pull/582#issuecomment-35837078
  
    Yes, I'll make the changes today.  Thanks, Aaron!


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
srowen <git@git.apache.org>,"Sun, 23 Feb 2014 17:32:39 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1071: Tidy logging strategy an...,dev@spark.incubator.apache.org,"Github user srowen commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/570#discussion_r9976665
  
    --- Diff: project/SparkBuild.scala ---
    @@ -236,13 +236,15 @@ object SparkBuild extends Build {
       ) ++ net.virtualvoid.sbt.graph.Plugin.graphSettings ++ ScalaStyleSettings
     
    -  val slf4jVersion = ""1.7.2""
    +  val slf4jVersion = ""1.7.5""
     
       val excludeCglib = ExclusionRule(organization = ""org.sonatype.sisu.inject"")
       val excludeJackson = ExclusionRule(organization = ""org.codehaus.jackson"")
       val excludeNetty = ExclusionRule(organization = ""org.jboss.netty"")
       val excludeAsm = ExclusionRule(organization = ""asm"")
       val excludeSnappy = ExclusionRule(organization = ""org.xerial.snappy"")
    +  val excludeCommonsLogging = ExclusionRule(organization = ""commons-logging"")
    +  val excludeSLF4J = ExclusionRule(organization = ""org.slf4j"")
    --- End diff --
    
    @pwendell What I see left are dependencies from third-party libraries on slf4j-api, which is fine. Most depend on 1.7.5 (so good that the version in Spark is bumped to 1.7.5), and a few use 1.6.x, which should be entirely compatible. It's also OK for dependencies to have slf4j-over-log4j12. So AFAICT it's fine in this regard.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
srowen <git@git.apache.org>,"Sun, 23 Feb 2014 17:33:00 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1071: Tidy logging strategy an...,dev@spark.incubator.apache.org,"Github user srowen commented on the pull request:

    https://github.com/apache/incubator-spark/pull/570#issuecomment-35837259
  
    @pwendell I addressed the last point about pulling up slf4j-over-log4j12 into core (non-test), and the indentation issue. Tests look good.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Nan Zhu <zhunanmcgill@gmail.com>,"Sun, 23 Feb 2014 12:50:13 -0500",Re: Anyone wants to look at SPARK-1123?,dev@spark.incubator.apache.org,"String, it should be get the following helper function 

private[spark] def getKeyClass() = implicitly[ClassTag[K]].runtimeClass

private[spark] def getValueClass() = implicitly[ClassTag[V]].runtimeClass

and this is what I run 

scala> val a = sc.textFile(""/Users/nanzhu/code/incubator-spark/LICENSE"", 2).map(line => (""a"", ""b""))

scala> a.saveAsNewAPIHadoopFile(""/Users/nanzhu/code/output_rdd"")
java.lang.InstantiationException
at sun.reflect.InstantiationExceptionConstructorAccessorImpl.newInstance(InstantiationExceptionConstructorAccessorImpl.java:48)
at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
at java.lang.Class.newInstance(Class.java:374)
at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:632)
at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:590)
at $iwC$$iwC$$iwC$$iwC.<init>(<console>:15)
at $iwC$$iwC$$iwC.<init>(<console>:20)
at $iwC$$iwC.<init>(<console>:22)
at $iwC.<init>(<console>:24)
at <init>(<console>:26)
at .<init>(<console>:30)
at .<clinit>(<console>)
at .<init>(<console>:7)
at .<clinit>(<console>)
at $print(<console>)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:774)
at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1042)
at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:611)
at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:642)
at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:606)
at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:790)
at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:835)
at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:747)
at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:595)
at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:602)
at org.apache.spark.repl.SparkILoop.loop(SparkILoop.scala:605)
at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:928)
at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:878)
at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:878)
at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:878)
at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:970)
at org.apache.spark.repl.Main$.main(Main.scala:31)
at org.apache.spark.repl.Main.main(Main.scala)






-- 
Nan Zhu





"
Nan Zhu <zhunanmcgill@gmail.com>,"Sun, 23 Feb 2014 13:11:24 -0500",Re: Anyone wants to look at SPARK-1123?,dev@spark.incubator.apache.org,"OK, I know where I was wrong 


Best, 

-- 
Nan Zhu
Sent with Sparrow (http://www.sparrowmailapp.com/?sig)




"
AmplabJenkins <git@git.apache.org>,"Sun, 23 Feb 2014 18:12:47 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1071: Tidy logging strategy an...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/570#issuecomment-35838296
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sun, 23 Feb 2014 18:12:47 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1071: Tidy logging strategy an...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/570#issuecomment-35838298
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
aarondav <git@git.apache.org>,"Sun, 23 Feb 2014 18:18:29 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1084. Fix most build warnings,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/586#issuecomment-35838441
  
    Ah, great, that'll make it simple. We can only merge at the granularity of PRs, so it'd be great if you could split the dependency stuff into its own.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Amandeep Khurana <amansk@gmail.com>,"Sun, 23 Feb 2014 10:18:50 -0800",Re: [DISCUSS] Extending public API,dev@spark.incubator.apache.org,"This makes sense. Thanks for clarifying, Mridul.

As Sean pointed out - a contrib module quickly turns into a legacy code
base that becomes hard to maintain. From that perspective, I think the idea
of a separate sparkbank github that is maintained by Spark contributors
(along with users who wish to contribute add-ons like you've described) and
adhere to the code quality and reviews like the main project seems
appealing. And then not just sparkbank but other things that people might
want to have as a part of the project but doesn't belong to the core
codebase can go there? I don't know if things like this have come up in the
past pull requests.

-Amandeep

PS: I'm not a spark committer/contributor so take my opinion fwiw. :)



"
CodingCat <git@git.apache.org>,"Sun, 23 Feb 2014 18:27:44 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1100] prevent Spark from over...,dev@spark.incubator.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/incubator-spark/pull/626#issuecomment-35838665
  
    OK, fixed some bugs and squashed the commits, I think it's ready for further review


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
fommil <git@git.apache.org>,"Sun, 23 Feb 2014 18:41:05 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user fommil commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35839024
  
    @mengxr looking through all the Apache authorised licenses, it would appear that the Mozilla license is a better fit with my goals since it would require distributors to make source code available if they make any modifications to `JNILoader`. Does that fit well with your project's goals? I'd rather have this than the Apache License.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sun, 23 Feb 2014 19:12:03 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1071: Tidy logging strategy an...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/570#issuecomment-35839844
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12818/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Sun, 23 Feb 2014 19:40:37 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1071: Tidy logging strategy an...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/570#issuecomment-35840761
  
    @srowen thanks for this clean-up. I'm going to merge this into master.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Sun, 23 Feb 2014 20:02:46 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1100] prevent Spark from over...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/626#issuecomment-35841445
  
    Hey @CodingCat this approach has a few drawbacks. First, it will mean a pretty bad regression for some users. For instance, say that a user is calling saveAsHadoopFile(/my-dir) and that directory has some other random stuff in at as well. Previously it would have written spark files alongside the other stuff, but with this patch it will silently delete the other data and create the directory. Second, this changes the API's all over the place which we are trying not to do. Third, it's a little scary to have code in spark that's deleting HDFS directories - I'd rather make the user do it explicitly.
    
    What if we did the following: We look in the output directory and see if there are any part-XX files in there already, and if so we throw an exception and say that the directory already has output data in it.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
CodingCat <git@git.apache.org>,"Sun, 23 Feb 2014 20:09:35 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1100] prevent Spark from over...,dev@spark.incubator.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/incubator-spark/pull/626#issuecomment-35841703
  
    @pwendell Thanks for the comments, I also considered what you mentioned, but will that prevent other components like Spark Streaming from doing the right job? (I'm not familiar with streaming, but it seems that it will overwrite the existing directory...)
    
    Also how to prevent the situation that the user occasionally run the job over the same directory for two times, but with different partition number (the second running has smaller value); eventually, the directory will contain the results from two runnings. 
    



---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Sun, 23 Feb 2014 20:11:29 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1122: allCollect functions for...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/635#issuecomment-35841766
  
    Hey @coderxiang - this is interesting functionality but I'm -1 on including it in the standard API. The main reason is that this will perform poorly on most large datasets and make it easy for people to shoot themselves in the foot. A second reason is that the use case isn't totally clear - as per some of @markhamstra's comments.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
srowen <git@git.apache.org>,"Sun, 23 Feb 2014 20:23:51 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user srowen commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35842122
  
    @fommil ASF is silent on the MPL: http://www.apache.org/legal/resolved.html#category-a
    But Mozilla says it's compatible with AL2: http://www.mozilla.org/MPL/license-policy.html
    Given the nature of the MPL, I suspect there is no issue. But IANAL.
    
    Sam you see what happens when you poke the hornet's nest! I can tell you have pointed opinions about licensing, and encourage you argue the case as long as you care to. The squabble is unlikely to conclude with ASF beards saying ""LGPL is cool"".
    
    I suggest filing a calm second JIRA to ask if there is any official stance on MPL, as that may solve the issue. (Want me to do it?)
    
    If not, I think Spark should just go with a different library.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
dlwh <git@git.apache.org>,"Sun, 23 Feb 2014 20:27:36 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user dlwh commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35842233
  
    @srowen @fommil Breeze is flexible enough that we can swap out different back ends quickly (and let users decide at runtime). So if need be, I can do the work to make both jblas and netlib-java work.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
CodingCat <git@git.apache.org>,"Sun, 23 Feb 2014 20:29:08 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1100] prevent Spark from over...,dev@spark.incubator.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/incubator-spark/pull/626#issuecomment-35842285
  
    @pwendell the second situation can be avoided, sorry, just brain damaged......the only issue is if there is a component relies on the fact that Spark allows the overwriting the directory before~


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
asfgit <git@git.apache.org>,"Sun, 23 Feb 2014 20:32:03 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1071: Tidy logging strategy an...,dev@spark.incubator.apache.org,"Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-spark/pull/570


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
srowen <git@git.apache.org>,"Sun, 23 Feb 2014 21:00:43 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1084. Fix most build warnings,dev@spark.incubator.apache.org,"Github user srowen commented on the pull request:

    https://github.com/apache/incubator-spark/pull/586#issuecomment-35843240
  


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
srowen <git@git.apache.org>,"Sun, 23 Feb 2014 21:00:43 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1084. Fix most build warnings,dev@spark.incubator.apache.org,"Github user srowen closed the pull request at:

    https://github.com/apache/incubator-spark/pull/586


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
fommil <git@git.apache.org>,"Sun, 23 Feb 2014 21:43:34 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user fommil commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35844613
  
    @srowen hehe, oh, I know. Actually I'm more interested in knowing exactly *why* they don't like LGPL. There have been so many discussions in the past between FSF and ASF that they don't quite appreciate that the rest of us don't understand either side's goals or have the memory of those previous discussions. I am at least confident that the thread has dusted off a lot of misconceptions about the LGPL and ASF's licensing goals.
    
    Re: Mozilla license, it's definitely listed under category B in that list.
    
    Don't worry, `JNILoader` can be made with AL2 if it needs to be... it's only a file or two anyway.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
fommil <git@git.apache.org>,"Sun, 23 Feb 2014 21:53:34 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user fommil commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35844934
  
    Actually, if somebody creates a ticket for me on https://github.com/fommil/jniloader that's the best way to ensure that I'll actually update the license and release it. I would prefer to use Mozilla if you are happy with that, so please do let me know what you discover. See http://www.apache.org/legal/resolved.html#category-b



---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Sandy Ryza <sandy.ryza@cloudera.com>,"Sun, 23 Feb 2014 13:55:55 -0800",Re: standard way of running a compiled jar,dev@spark.incubator.apache.org,"Is the client=driver mode still a supported option (outside of the REPLs),
at least for the medium term?  My impression from reading the docs is that
it's the most common, if not recommended, way to submit jobs.  If that's
the case, I still think it's important, or at least helpful, to have
something for this mode that addresses the issues below.



"
AmplabJenkins <git@git.apache.org>,"Sun, 23 Feb 2014 22:18:13 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1078: Replace lift-json with j...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/582#issuecomment-35845761
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sun, 23 Feb 2014 22:18:13 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1078: Replace lift-json with j...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/582#issuecomment-35845760
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sun, 23 Feb 2014 23:12:16 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1078: Replace lift-json with j...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/582#issuecomment-35847506
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sun, 23 Feb 2014 23:12:16 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1078: Replace lift-json with j...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/582#issuecomment-35847507
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12819/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sun, 23 Feb 2014 23:22:58 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1078: Replace lift-json with j...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/582#issuecomment-35847834
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Sun, 23 Feb 2014 23:22:58 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1078: Replace lift-json with j...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/582#issuecomment-35847835
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
srowen <git@git.apache.org>,"Sun, 23 Feb 2014 23:29:53 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1084 (part 1). Fix most build ...,dev@spark.incubator.apache.org,"GitHub user srowen opened a pull request:

    https://github.com/apache/incubator-spark/pull/637

    SPARK-1084 (part 1). Fix most build warnings.

    This is a redo of https://github.com/apache/incubator-spark/pull/586
    
    This contains all the same changes, minus dependency changes. It also rebases and squashes some commits that could be combined.
    
    After this is in I'll propose part 2, which concerns dependencies.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/srowen/incubator-spark SPARK-1084.1

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/637.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #637
    
----
commit 2e52f136474abf911472af2bb639d704605cd171
Author: Sean Owen <sowen@cloudera.com>
Date:   2014-02-11T14:37:03Z

    Replace deprecated Ant <tasks> with <target>

commit a82b841df207128aec23ae9eb3a297e41d1bcc49
Author: Sean Owen <sowen@cloudera.com>
Date:   2014-02-11T14:38:23Z

    Remove dead scaladoc links

commit 3b7b2ad9c9a2536da51a1b6af7ebf2aff77fef32
Author: Sean Owen <sowen@cloudera.com>
Date:   2014-02-11T14:39:48Z

    Fix scaladoc invocation warning, and enable javac warnings properly, with plugin config updates

commit b5ccbc9c6360437afabcfc14e81321e5b7b38e4c
Author: Sean Owen <sowen@cloudera.com>
Date:   2014-02-12T13:45:04Z

    Fix one new style error introduced in scaladoc warning commit

commit 79f1c7acdb9634128d417d704a234058d2993bea
Author: Sean Owen <sowen@cloudera.com>
Date:   2014-02-23T21:27:02Z

    Fix two misc javadoc problems

commit ee1c1150d482243c190c71931852f2797ec79120
Author: Sean Owen <sowen@cloudera.com>
Date:   2014-02-23T23:27:21Z

    Suppress warnings about legitimate unchecked array creations, or change code to avoid it

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Sun, 23 Feb 2014 23:51:42 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1122: allCollect functions for...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/635#issuecomment-35848534
  
    @coderxiang btw - it might be something where we make it a private API so it can be used inside of Spark if other packages need this to do broadcast joins. It would be good to understand a bit the intended use case though.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 00:12:19 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1078: Replace lift-json with j...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/582#issuecomment-35849113
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12820/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 00:12:19 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1078: Replace lift-json with j...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/582#issuecomment-35849112
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 00:13:01 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1084 (part 1). Fix most build ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/637#issuecomment-35849130
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 00:13:01 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1084 (part 1). Fix most build ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/637#issuecomment-35849129
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
mengxr <git@git.apache.org>,"Mon, 24 Feb 2014 00:19:44 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1122: allCollect functions for...,dev@spark.incubator.apache.org,"Github user mengxr commented on the pull request:

    https://github.com/apache/incubator-spark/pull/635#issuecomment-35849312
  
    @markhamstra @pwendell For the use cases, this allCollect operation may be useful in the grid search for a good set of training parameters for machine learning problems. For example, if the dataset is only 500MB but training takes half an hour to finish and we have to try 100 different combinations of training parameters (e.g., rank, regularization constants, and termination tolerance), the wall-clock time can be reduced by distributing the dataset to multiple nodes and training in parallel. Another use case is the replicated join, though locality issues need to be addressed. I agree with you that the implementation is not efficient, which puts heavy load on the driver.
    
    @coderxiang , could you try to improve the implementation? 


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
xoltar <git@git.apache.org>,"Mon, 24 Feb 2014 01:05:24 +0000 (UTC)",[GitHub] incubator-spark pull request: For outputformats that are Configura...,dev@spark.incubator.apache.org,"GitHub user xoltar opened a pull request:

    https://github.com/apache/incubator-spark/pull/638

    For outputformats that are Configurable, call setConf before sending data to them.

    This allows us to use, e.g. HBase's TableOutputFormat with PairRDDFunctions.saveAsNewAPIHadoopFile, which otherwise would throw NullPointerException because the output table name hasn't been configured.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/xoltar/incubator-spark SPARK-1108

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/638.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #638
    
----
commit 7cbcaa10bbf01cf04bba7f2883d1fb9564cd3660
Author: Bryn Keller <bryn.keller@intel.com>
Date:   2014-02-20T06:00:44Z

    For outputformats that are Configurable, call setConf before sending data to them. This allows us to use, e.g. HBase TableOutputFormat, which otherwise would throw NullPointerException because the output table name hasn't been configured

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 01:12:27 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1084 (part 1). Fix most build ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/637#issuecomment-35850736
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12821/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 01:12:28 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1084 (part 1). Fix most build ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/637#issuecomment-35850735
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 01:13:12 +0000 (UTC)",[GitHub] incubator-spark pull request: For outputformats that are Configura...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/638#issuecomment-35850754
  
    Can one of the admins verify this patch?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 01:13:18 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark-615: make mapPartitionsWithInd...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/606#issuecomment-35850759
  
     Build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 01:13:19 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark-615: make mapPartitionsWithInd...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/606#issuecomment-35850760
  
    Build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Mon, 24 Feb 2014 01:22:39 +0000 (UTC)",[GitHub] incubator-spark pull request: For outputformats that are Configura...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/638#issuecomment-35851045
  
    Jenkins, test this please.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
CodingCat <git@git.apache.org>,"Mon, 24 Feb 2014 01:38:19 +0000 (UTC)",[GitHub] incubator-spark pull request: For outputformats that are Configura...,dev@spark.incubator.apache.org,"Github user CodingCat commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/638#discussion_r9979453
  
    --- Diff: core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala ---
    @@ -617,6 +617,10 @@ class PairRDDFunctions[K: ClassTag, V: ClassTag](self: RDD[(K, V)])
             attemptNumber)
           val hadoopContext = newTaskAttemptContext(wrappedConf.value, attemptId)
           val format = outputFormatClass.newInstance
    +      format match {
    +        case c:Configurable => c.setConf(wrappedConf.value)
    --- End diff --
    
    do we need some comments here to indicate that this line is to support a special case in HBase?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
CodingCat <git@git.apache.org>,"Mon, 24 Feb 2014 01:43:07 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1100] prevent Spark from over...,dev@spark.incubator.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/incubator-spark/pull/626#issuecomment-35851751
  
    I just went through the Spark Streaming document, it seems that it's safe to follow your suggestion @pwendell 


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
CodingCat <git@git.apache.org>,"Mon, 24 Feb 2014 01:50:45 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1100] prevent Spark from over...,dev@spark.incubator.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/incubator-spark/pull/626#issuecomment-35851996
  
    but why not just preventing users from overwriting the directory, no matter whether there is part-*?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 02:12:30 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark-615: make mapPartitionsWithInd...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/606#issuecomment-35852723
  
    Build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 02:12:30 +0000 (UTC)",[GitHub] incubator-spark pull request: Spark-615: make mapPartitionsWithInd...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/606#issuecomment-35852724
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12822/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 02:12:40 +0000 (UTC)",[GitHub] incubator-spark pull request: For outputformats that are Configura...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/638#issuecomment-35852730
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 02:12:40 +0000 (UTC)",[GitHub] incubator-spark pull request: For outputformats that are Configura...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/638#issuecomment-35852729
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 02:12:49 +0000 (UTC)",[GitHub] incubator-spark pull request: MLI-2: Start adding k-fold cross val...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/572#issuecomment-35852739
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 02:12:49 +0000 (UTC)",[GitHub] incubator-spark pull request: MLI-2: Start adding k-fold cross val...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/572#issuecomment-35852737
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Matei Zaharia <matei.zaharia@gmail.com>,"Sun, 23 Feb 2014 18:58:11 -0800",Re: standard way of running a compiled jar,dev@spark.incubator.apache.org,"Yes, it is a supported option. I’m just wondering whether we want to create a script for it specifically. Maybe the same script could also allow submitting to the cluster or something.

Matei


REPLs),
that
that's
this.
runs the
and it
a JAR
to be
restart
I'd
the
on
submit
its
the
against the
executor
installations
the
across
Spark
been
of
but one
could
have


"
Matei Zaharia <matei.zaharia@gmail.com>,"Sun, 23 Feb 2014 19:04:27 -0800",Re: [DISCUSS] Extending public API,dev@spark.incubator.apache.org,"My sense on all this is that it should be done on a case-by-case basis. To add a new API, it needs to be general enough that a lot of users will want to use it. If adding that API confuses users, that’s a problem. However, on the flip side, if it’s not a super-popular function but it’s just 10-20 lines of code, it may still be worth having. The maintenance burden on that is not too high, and users are used to fairly extensive collection libraries.

For the joins in particular, we added them because it’s quite easy to mess up writing joins by hand, even once you have cogroup().

functionality, like statistics functions, in separate libraries. Right now there are some functions in the RDD API (e.g. sums, means, histograms, etc) that are fairly specific to this domain.

Matei


code
the idea
contributors
described) and
might
in the
something
proposal
its own
types
using
not
for us
spark
must
wanted
<mridul@gmail.com>
which
(which
expressed
contribute,
and
up
and
and
benefit
to
bundled


"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 03:12:35 +0000 (UTC)",[GitHub] incubator-spark pull request: For outputformats that are Configura...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/638#issuecomment-35854652
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 03:12:35 +0000 (UTC)",[GitHub] incubator-spark pull request: MLI-2: Start adding k-fold cross val...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/572#issuecomment-35854653
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12824/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 03:12:35 +0000 (UTC)",[GitHub] incubator-spark pull request: MLI-2: Start adding k-fold cross val...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/572#issuecomment-35854651
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 03:12:35 +0000 (UTC)",[GitHub] incubator-spark pull request: For outputformats that are Configura...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/638#issuecomment-35854654
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12823/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Mon, 24 Feb 2014 04:04:05 +0000 (UTC)",[GitHub] incubator-spark pull request: For outputformats that are Configura...,dev@spark.incubator.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/638#discussion_r9980719
  
    --- Diff: core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala ---
    @@ -617,6 +617,10 @@ class PairRDDFunctions[K: ClassTag, V: ClassTag](self: RDD[(K, V)])
             attemptNumber)
           val hadoopContext = newTaskAttemptContext(wrappedConf.value, attemptId)
           val format = outputFormatClass.newInstance
    +      format match {
    +        case c:Configurable => c.setConf(wrappedConf.value)
    --- End diff --
    
    I don't think this is specific to hbase - I think this is something we should really have been doing always but it only was noticed due to the fact that hbase replies on this configuration.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Mon, 24 Feb 2014 04:04:22 +0000 (UTC)",[GitHub] incubator-spark pull request: For outputformats that are Configura...,dev@spark.incubator.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/638#discussion_r9980723
  
    --- Diff: core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala ---
    @@ -617,6 +617,10 @@ class PairRDDFunctions[K: ClassTag, V: ClassTag](self: RDD[(K, V)])
             attemptNumber)
           val hadoopContext = newTaskAttemptContext(wrappedConf.value, attemptId)
           val format = outputFormatClass.newInstance
    +      format match {
    +        case c:Configurable => c.setConf(wrappedConf.value)
    --- End diff --
    
    Add a space after the colon: `case c: Configurable => `


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Mon, 24 Feb 2014 04:05:41 +0000 (UTC)",[GitHub] incubator-spark pull request: For outputformats that are Configura...,dev@spark.incubator.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/638#discussion_r9980734
  
    --- Diff: core/src/test/scala/org/apache/spark/rdd/PairRDDFunctionsSuite.scala ---
    @@ -26,6 +26,11 @@ import com.google.common.io.Files
     
     import org.apache.spark.SparkContext._
     import org.apache.spark.{Partitioner, SharedSparkContext}
    +import org.apache.hadoop.mapreduce._
    --- End diff --
    
    Mind making your new imports fit the normal style?
    
    https://cwiki.apache.org/confluence/display/SPARK/Spark+Code+Style+Guide#SparkCodeStyleGuide-Imports


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
witgo <git@git.apache.org>,"Mon, 24 Feb 2014 04:05:50 +0000 (UTC)",[GitHub] incubator-spark pull request: fix building with maven on Mac OS X,dev@spark.incubator.apache.org,"GitHub user witgo opened a pull request:

    https://github.com/apache/incubator-spark/pull/639

    fix building with maven on Mac OS X

    fix building with maven on Mac OS X throw Failure to find org.eclipse.paho:mqtt-client:jar:0.4.0 in https://repository.apache.org/content/repositories/releases was cached in the local repository, resolution will not be reattempted until the update interval of apache-repo has elapsed or updates are forced 

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/witgo/incubator-spark master

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/639.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #639
    
----
commit 27c612fb0dbbf27ba5a20d870a5cbb5cf33f4d9f
Author: liguoqiang <liguoqiang@rd.tuan800.com>
Date:   2014-02-24T04:00:36Z

    fix building with maven on Mac OS X throw Failure to find org.eclipse.paho:mqtt-client:jar:0.4.0 in https://repository.apache.org/content/repositories/releases was cached in the local repository, resolution will not be reattempted until the update interval of apache-repo has elapsed or updates are forced

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Mon, 24 Feb 2014 04:06:58 +0000 (UTC)",[GitHub] incubator-spark pull request: For outputformats that are Configura...,dev@spark.incubator.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/638#discussion_r9980747
  
    --- Diff: core/src/test/scala/org/apache/spark/rdd/PairRDDFunctionsSuite.scala ---
    @@ -330,4 +335,74 @@ class PairRDDFunctionsSuite extends FunSuite with SharedSparkContext {
           (1, ArrayBuffer(1)),
           (2, ArrayBuffer(1))))
       }
    +
    +  test(""saveNewAPIHadoopFile should call setConf if format is configurable"") {
    +    val pairs = sc.parallelize(Array((new Integer(1), new Integer(1))))
    +    val conf = new Configuration()
    +
    +    //No error, non-configurable formats still work
    --- End diff --
    
    Mind adding spaces after these? `// No error, non-configurable formats`... Also it would be nice (but up to you) to use `/* ... */` for multi-line comments.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Mon, 24 Feb 2014 04:07:13 +0000 (UTC)",[GitHub] incubator-spark pull request: For outputformats that are Configura...,dev@spark.incubator.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/638#discussion_r9980751
  
    --- Diff: core/src/test/scala/org/apache/spark/rdd/PairRDDFunctionsSuite.scala ---
    @@ -330,4 +335,74 @@ class PairRDDFunctionsSuite extends FunSuite with SharedSparkContext {
           (1, ArrayBuffer(1)),
           (2, ArrayBuffer(1))))
       }
    +
    +  test(""saveNewAPIHadoopFile should call setConf if format is configurable"") {
    +    val pairs = sc.parallelize(Array((new Integer(1), new Integer(1))))
    +    val conf = new Configuration()
    +
    +    //No error, non-configurable formats still work
    +    pairs.saveAsNewAPIHadoopFile[FakeFormat](""ignored"")
    +
    +    //Configurable intercepts get configured
    +    //ConfigTestFormat throws an exception if we try to write to it
    +    //when setConf hasn't been thrown first.
    +    //Assertion is in ConfigTestFormat.getRecordWriter
    +    pairs.saveAsNewAPIHadoopFile[ConfigTestFormat](""ignored"")
    +  }
    +}
    +
    +// These classes are fakes for testing
    +// ""saveNewAPIHadoopFile should call setConf if format is configurable"".
    +// Unfortunately, they have to be top level classes, and not defined in
    +// the test method, because otherwise Scala won't generate no-args constructors
    +// and the test will therefore throw InstantiationException when saveAsNewAPIHadoopFile
    +// tries to instantiate them with Class.newInstance.
    +class FakeWriter extends RecordWriter[Integer,Integer] {
    --- End diff --
    
    `Integer, Integer`


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Mon, 24 Feb 2014 04:08:35 +0000 (UTC)",[GitHub] incubator-spark pull request: For outputformats that are Configura...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/638#issuecomment-35856419
  
    Thanks a lot for tracking this down, fixing it, and adding tests! I added some minor style comments, modulo those comments LGTM.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
pwendell <git@git.apache.org>,"Mon, 24 Feb 2014 04:08:52 +0000 (UTC)",[GitHub] incubator-spark pull request: For outputformats that are Configura...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/638#issuecomment-35856428
  
    We should put this fix in 0.9 as well once it's ready to merge.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 04:13:40 +0000 (UTC)",[GitHub] incubator-spark pull request: fix building with maven on Mac OS X,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/639#issuecomment-35856579
  
    Can one of the admins verify this patch?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
xoltar <git@git.apache.org>,"Mon, 24 Feb 2014 05:00:25 +0000 (UTC)",[GitHub] incubator-spark pull request: For outputformats that are Configura...,dev@spark.incubator.apache.org,"Github user xoltar commented on the pull request:

    https://github.com/apache/incubator-spark/pull/638#issuecomment-35857946
  
    Thanks, last change should address all code review comments. Also cleaned up some imports in PairRDDFunctionsSuite that weren't needed.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
ScrapCodes <git@git.apache.org>,"Mon, 24 Feb 2014 05:59:35 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1089] fix the regression prob...,dev@spark.incubator.apache.org,"Github user ScrapCodes commented on the pull request:

    https://github.com/apache/incubator-spark/pull/614#issuecomment-35859704
  
    Nice catch ! and thanks for taking the time to dig this. I am okay with this way of doing it, however if you and others prefer we can move this code to createInterpreter before creating SparkILoopInterpreter. Even if we don't I think its fine to merge it. 



---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
ScrapCodes <git@git.apache.org>,"Mon, 24 Feb 2014 07:18:26 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1089] fix the regression prob...,dev@spark.incubator.apache.org,"Github user ScrapCodes commented on the pull request:

    https://github.com/apache/incubator-spark/pull/614#issuecomment-35862636
  
    Also from this I just went ahead and tried fixing this problem in scala and it worked.
    https://github.com/ScrapCodes/scala/tree/si-6502-fix


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
sryza <git@git.apache.org>,"Mon, 24 Feb 2014 07:33:51 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1004: PySpark on YARN,dev@spark.incubator.apache.org,"GitHub user sryza opened a pull request:

    https://github.com/apache/incubator-spark/pull/640

    SPARK-1004: PySpark on YARN

    Make pyspark work in yarn-client mode.  This build's on Josh's work.  I tested verified it works on a 5-node cluster.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/sryza/incubator-spark sandy-spark-1004

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/640.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #640
    
----
commit e752a6a1c8a9d7cbc31d7b911800e22db6fcb2b0
Author: Josh Rosen <joshrosen@apache.org>
Date:   2014-01-24T18:19:58Z

    Automatically set Yarn env vars in PySpark (SPARK-1030).

commit 0adcaa971086853b254baf32748811561bb6e209
Author: Josh Rosen <joshrosen@apache.org>
Date:   2014-01-25T23:28:56Z

    WIP towards PySpark on YARN:
    
      PySpark Python libraries are added to the PYTHONPATH.
    
    - Add a Makefile for generating a ""fat zip"" that contains PySpark's
      Python dependencies.  This is a bit of a hack and I'd be open to
      better packaging tools, but this doesn't require any extra Python
      libraries.  This use case doesn't seem to be well-addressed by the
      existing Python packaging tools: there are plenty of tools to package
      complete Python environments (such as pyinstaller and virtualenv) or
      to bundle *individual* libraries (e.g. distutils), but few to generate
      portable fat zips or eggs.
    
    This hasn't been tested with YARN and may not actually compile.

commit d4a71d0495d072d5b5364601e7cd0dc9a7c9c9b9
Author: Josh Rosen <joshrosen@apache.org>
Date:   2014-02-19T06:27:21Z

    Add missing setup.py file for PySpark.

commit dcda63863a41414ba5e410092dc4fbab2e353543
Author: Sandy Ryza <sandy@cloudera.com>
Date:   2014-02-24T07:06:42Z

    Improvements

commit 38546d4f282727f3ae112f1e564df72443b726f5
Author: Sandy Ryza <sandy@cloudera.com>
Date:   2014-02-24T07:26:01Z

    Don't set SPARK_JAR

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 07:38:37 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1004: PySpark on YARN,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/640#issuecomment-35863414
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 07:38:37 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1004: PySpark on YARN,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/640#issuecomment-35863413
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
mateiz <git@git.apache.org>,"Mon, 24 Feb 2014 07:39:51 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1102] Create a saveAsNewAPIHa...,dev@spark.incubator.apache.org,"Github user mateiz commented on the pull request:

    https://github.com/apache/incubator-spark/pull/636#issuecomment-35863485
  
    Jenkins, this is OK to test


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
mateiz <git@git.apache.org>,"Mon, 24 Feb 2014 07:41:29 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1102] Create a saveAsNewAPIHa...,dev@spark.incubator.apache.org,"Github user mateiz commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/636#discussion_r9983025
  
    --- Diff: core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala ---
    @@ -686,6 +649,47 @@ class PairRDDFunctions[K: ClassTag, V: ClassTag](self: RDD[(K, V)])
       }
     
       /**
    +   * Output the RDD to any Hadoop-supported storage system with new Hadoop API, using a Hadoop
    +   * Job object for that storage system. The Job should set an OutputFormat and any output paths
    +   * required (e.g. a table name to write to) in the same way as it would be configured for a Hadoop
    +   * MapReduce job.
    +   */
    +  def saveAsNewAPIHadoopDataset(job: NewAPIHadoopJob) {
    --- End diff --
    
    In the new Hadoop API, does this really require a Job or just a Configuration? In the old API we only needed a configuration.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
mateiz <git@git.apache.org>,"Mon, 24 Feb 2014 07:43:04 +0000 (UTC)",[GitHub] incubator-spark pull request: add threadPool shutdown hook when ki...,dev@spark.incubator.apache.org,"Github user mateiz commented on the pull request:

    https://github.com/apache/incubator-spark/pull/628#issuecomment-35863650
  
    Given this, can you close the pull request? Or do you plan to try interrupt? That may also not fix the issue.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
wchswchs <git@git.apache.org>,"Mon, 24 Feb 2014 07:44:31 +0000 (UTC)",[GitHub] incubator-spark pull request: add threadPool shutdown hook when ki...,dev@spark.incubator.apache.org,"Github user wchswchs closed the pull request at:

    https://github.com/apache/incubator-spark/pull/628


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
wchswchs <git@git.apache.org>,"Mon, 24 Feb 2014 07:44:57 +0000 (UTC)",[GitHub] incubator-spark pull request: add threadPool shutdown hook when ki...,dev@spark.incubator.apache.org,"Github user wchswchs commented on the pull request:

    https://github.com/apache/incubator-spark/pull/628#issuecomment-35863734
  
    okï¼Œi have closed itï¼


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
mateiz <git@git.apache.org>,"Mon, 24 Feb 2014 07:51:10 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1124: Fix infinite retries of ...,dev@spark.incubator.apache.org,"GitHub user mateiz opened a pull request:

    https://github.com/apache/incubator-spark/pull/641

    SPARK-1124: Fix infinite retries of reduce stage when a map stage failed

    In the previous code, if you had a failing map stage and then tried to run reduce stages on it repeatedly, the first reduce stage would fail correctly, but the later ones would mistakenly believe that all map outputs are available and start failing infinitely with fetch failures from ""null"". See https://spark-project.atlassian.net/browse/SPARK-1124 for an example.
    
    This PR also cleans up code style slightly where there was a variable named ""s"" and some weird map manipulation.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/mateiz/incubator-spark spark-1124-master

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/641.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #641
    
----
commit cd32d5e4dee1291e4509e5965322b7ffe620b1f3
Author: Matei Zaharia <matei@databricks.com>
Date:   2014-02-24T07:45:48Z

    SPARK-1124: Fix infinite retries of reduce stage when a map stage failed
    
    In the previous code, if you had a failing map stage and then tried to
    run reduce stages on it repeatedly, the first reduce stage would fail
    correctly, but the later ones would mistakenly believe that all map
    outputs are available and start failing infinitely with fetch failures
    from ""null"".

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 07:53:36 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1124: Fix infinite retries of ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/641#issuecomment-35864121
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 07:53:36 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1124: Fix infinite retries of ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/641#issuecomment-35864120
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 08:12:48 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1004: PySpark on YARN,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/640#issuecomment-35865040
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12825/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 08:12:48 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1004: PySpark on YARN,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/640#issuecomment-35865039
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
jyotiska <git@git.apache.org>,"Mon, 24 Feb 2014 08:15:30 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1004: PySpark on YARN,dev@spark.incubator.apache.org,"Github user jyotiska commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/640#discussion_r9983532
  
    --- Diff: python/setup.py ---
    @@ -0,0 +1,30 @@
    +#
    +# Licensed to the Apache Software Foundation (ASF) under one or more
    +# contributor license agreements.  See the NOTICE file distributed with
    +# this work for additional information regarding copyright ownership.
    +# The ASF licenses this file to You under the Apache License, Version 2.0
    +# (the ""License""); you may not use this file except in compliance with
    +# the License.  You may obtain a copy of the License at
    +#
    +#    http://www.apache.org/licenses/LICENSE-2.0
    +#
    +# Unless required by applicable law or agreed to in writing, software
    +# distributed under the License is distributed on an ""AS IS"" BASIS,
    +# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    +# See the License for the specific language governing permissions and
    +# limitations under the License.
    +#
    +
    +from distutils.core import setup
    +
    +
    +setup(
    +    name='pyspark',
    +    version='0.9.0-incubating-SNAPSHOT',
    +    description='Python API for Spark',
    +    author='The Apache Software Foundation',
    +    author_email='user@spark.incubator.apache.org',
    +    license='Apache License 2.0',
    +    url='spark-project.org',
    +    packages=['pyspark'],
    +)
    --- End diff --
    
    Should we specify any other packages such as numpy inside a separate <code>install_requires</code> field?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 08:22:00 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1124: Fix infinite retries of ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/641#issuecomment-35865525
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12826/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 08:21:59 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1124: Fix infinite retries of ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/641#issuecomment-35865523
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
srowen <git@git.apache.org>,"Mon, 24 Feb 2014 08:36:49 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1125: Fix building with maven ...,dev@spark.incubator.apache.org,"Github user srowen commented on the pull request:

    https://github.com/apache/incubator-spark/pull/639#issuecomment-35866347
  
    I am not seeing this problem with the build, and I am using OS X. (It would not be OS X-specific anyway.) Try running with ""mvn -U ..."" -- this is kind of what the error is suggesting you do. 


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
mengxr <git@git.apache.org>,"Mon, 24 Feb 2014 08:42:14 +0000 (UTC)",[GitHub] incubator-spark pull request: Principal Component Analysis,dev@spark.incubator.apache.org,"Github user mengxr commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/564#discussion_r9983939
  
    --- Diff: mllib/src/main/scala/org/apache/spark/mllib/linalg/PCA.scala ---
    @@ -0,0 +1,119 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one or more
    + * contributor license agreements.  See the NOTICE file distributed with
    + * this work for additional information regarding copyright ownership.
    + * The ASF licenses this file to You under the Apache License, Version 2.0
    + * (the ""License""); you may not use this file except in compliance with
    + * the License.  You may obtain a copy of the License at
    + *
    + *    http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.spark.mllib.linalg
    +
    +import org.apache.spark.SparkContext
    +import org.apache.spark.SparkContext._
    +import org.apache.spark.rdd.RDD
    +
    +import org.apache.spark.mllib.util._
    +
    +
    +/**
    + * Class used to obtain principal components
    + */
    +class PCA {
    +  private var k: Int = 1
    +
    +  /**
    +   * Set the number of top-k principle components to return
    +   */
    +  def setK(k: Int): PCA = {
    +    this.k = k
    +    this
    +  }
    +
    +   /**
    +   * Compute PCA using the current set parameters
    +   */
    +  def compute(matrix: DenseMatrix): DenseMatrix = {
    +    computePCA(matrix, k)
    +  }
    +
    +  /**
    +  * Principal Component Analysis.
    +  * Computes the top k principal component coefficients for the m-by-n data matrix X.
    +  * Rows of X correspond to observations and columns correspond to variables. 
    +  * The coefficient matrix is n-by-k. Each column of coeff contains coefficients
    +  * for one principal component, and the columns are in descending 
    +  * order of component variance.
    +  * This function centers the data and uses the 
    +  * singular value decomposition (SVD) algorithm. 
    +  *
    +  * All input and output is expected in DenseMatrix format
    +  *
    +  * @param matrix dense matrix to perform pca on
    +  * @param k Recover k principal components
    +  * @return An nxk matrix of principal components
    +  */
    +  def computePCA(matrix: DenseMatrix, k: Int): DenseMatrix = {
    +    val m = matrix.m
    +    val n = matrix.n
    +
    +    if (m <= 0 || n <= 0) {
    +      throw new IllegalArgumentException(""Expecting a well-formed matrix"")
    +    }
    +
    +    // compute column sums and normalize matrix
    +    val rawData = matrix.rows.flatMap{
    +      x => Array.tabulate(x.data.size)(idx => MatrixEntry(x.i, idx, x.data(idx)))
    +    }
    +    val colSums = rawData.map(entry => (entry.j, entry.mval)).reduceByKey(_ + _)
    +    val data = rawData.map(entry => (entry.j, (entry.i, entry.mval))).join(colSums).map{
    +      case (col, ((row, mval), colsum)) =>
    +        MatrixEntry(row, col, (mval - colsum / m.toDouble) / Math.sqrt(n-1)) }
    --- End diff --
    
    It creates many small objects and breaks the data continuity of each row. The following code should be adequate for computing column sums:
    ~~~
    def addi(a: Array[Double], b: Array[Double]): Array[Double] = ...
    val colsums = matrix.rows.fold(new Array[Double](matrix.n))(addi)
    ~~~
    I ran a small test of size 1000000x100, where it is about 50x faster than flat-mapping rows to entries and then reducing by column indices.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Sandy Ryza <sandy.ryza@cloudera.com>,"Mon, 24 Feb 2014 00:49:34 -0800",Re: standard way of running a compiled jar,dev@spark.incubator.apache.org,"That makes sense to me.  I filed
SPARK-1126<https://spark-project.atlassian.net/browse/SPARK-1126> to
create a spark-jar script that provides a layer over these.



"
witgo <git@git.apache.org>,"Mon, 24 Feb 2014 09:00:34 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1125: Fix building with maven ...,dev@spark.incubator.apache.org,"Github user witgo commented on the pull request:

    https://github.com/apache/incubator-spark/pull/639#issuecomment-35867784
  
    	mvn -v
    
    	
    	Apache Maven 3.1.1 (0728685237757ffbf44136acec0402957f723d9a; 2013-09-17 23:22:22+0800)
    	Maven home: /usr/local/Cellar/maven/3.1.1/libexec
    	Java version: 1.7.0_51, vendor: Oracle Corporation
    	Java home: /Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre
    	Default locale: zh_CN, platform encoding: UTF-8
    	OS name: ""mac os x"", version: ""10.9.1"", arch: ""x86_64"", family: ""mac""
    
    
        mvn -U -Pyarn -Dhadoop.version=2.2.0 -Dyarn.version=2.2.0 -DskipTests compile
     
     ERROR:
    
        Failed to execute goal org.apache.maven.plugins:maven-remote-resources-plugin:1.4:process (default) on project spark-examples_2.10: Error resolving project artifact: Could not transfer artifact org.eclipse.paho:mqtt-client:pom:0.4.0 from/to apache-repo (https://repository.apache.org/content/repositories/releases): peer not authenticated for project org.eclipse.paho:mqtt-client:jar:0.4.0 


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
srowen <git@git.apache.org>,"Mon, 24 Feb 2014 09:45:32 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1125: Fix building with maven ...,dev@spark.incubator.apache.org,"Github user srowen commented on the pull request:

    https://github.com/apache/incubator-spark/pull/639#issuecomment-35870621
  
    Still works fine for me even with clearing the local artifact and cleaning. I have the exact same Maven from brew.
    
    ```
    rm -fr ~/.m2/repository/org/eclipse/paho/
    mvn -U -Pyarn -Dhadoop.version=2.2.0 -Dyarn.version=2.2.0 -DskipTests clean compile
    ```
    
    I show that it is downloading from the eclipse repo:
    
    ```
    Downloading: https://repo.eclipse.org/content/repositories/paho-releases/org/eclipse/paho/mqtt-client/0.4.0/mqtt-client-0.4.0.pom
    Downloaded: https://repo.eclipse.org/content/repositories/paho-releases/org/eclipse/paho/mqtt-client/0.4.0/mqtt-client-0.4.0.pom (4 KB at 1.1 KB/sec)
    ```
    
    That's to be expected since this repo is configured in the `external/mqtt` module. I see code usages in this module, and in examples (where you see the failure), but the example module does depend on the MQTT module.
    
    Try the `install` goal instead? maybe it's not finding your copy of the snapshot build locally.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
witgo <git@git.apache.org>,"Mon, 24 Feb 2014 09:56:11 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1125: Fix building with maven ...,dev@spark.incubator.apache.org,"Github user witgo commented on the pull request:

    https://github.com/apache/incubator-spark/pull/639#issuecomment-35871268
  
    add exclude dependencies on mqtt-client from Spark Examples 
    throw error:
    
         [ERROR] /Users/witgo/work/code/java/github/witgo/incubator-spark/examples/src/main/scala/org/apache/spark/streaming/examples/MQTTWordCount.scala:20: object paho is not a member of package org.eclipse



---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
srowen <git@git.apache.org>,"Mon, 24 Feb 2014 09:57:14 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1125: Fix building with maven ...,dev@spark.incubator.apache.org,"Github user srowen commented on the pull request:

    https://github.com/apache/incubator-spark/pull/639#issuecomment-35871340
  
    Why would you exclude the dependency? It is necessary to work, as you show.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
witgo <git@git.apache.org>,"Mon, 24 Feb 2014 10:09:35 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1125: Fix building with maven ...,dev@spark.incubator.apache.org,"Github user witgo commented on the pull request:

    https://github.com/apache/incubator-spark/pull/639#issuecomment-35872264
  
    `mvn -U -Pyarn -Dhadoop.version=2.2.0 -Dyarn.version=2.2.0 -DskipTests  -Dmaven.wagon.http.ssl.insecure=true -Dmaven.wagon.http.ssl.allowall=true clean compile ` 
     works fine for meã€‚ What went wrong? 


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
srowen <git@git.apache.org>,"Mon, 24 Feb 2014 10:14:24 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1125: Fix building with maven ...,dev@spark.incubator.apache.org,"Github user srowen commented on the pull request:

    https://github.com/apache/incubator-spark/pull/639#issuecomment-35872621
  
    Hmm! is there a proxy between you and the outside world? anything that would break the SSL connection? I think it's environment-specific in any event. Great that you spotted the workaround. Does a better error show up with -X ?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
witgo <git@git.apache.org>,"Mon, 24 Feb 2014 10:22:59 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1125: Fix building with maven ...,dev@spark.incubator.apache.org,"Github user witgo commented on the pull request:

    https://github.com/apache/incubator-spark/pull/639#issuecomment-35873227
  
    You're right, there a proxy .
    `mvn -U -Pyarn -Dhadoop.version=2.2.0 -Dyarn.version=2.2.0 -DskipTests clean compile` works fine by VPN. I close this PR.
    Thank you very much


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
witgo <git@git.apache.org>,"Mon, 24 Feb 2014 10:25:29 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1125: Fix building with maven ...,dev@spark.incubator.apache.org,"Github user witgo closed the pull request at:

    https://github.com/apache/incubator-spark/pull/639


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 10:28:50 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35873697
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 10:28:51 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35873699
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 10:31:59 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35873917
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12827/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 10:31:59 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35873916
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
ScrapCodes <git@git.apache.org>,"Mon, 24 Feb 2014 10:43:42 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user ScrapCodes commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35874707
  
    I messed up rebase :( and had to flatten all the commits. 


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 10:53:49 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35875416
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 10:53:50 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35875417
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 11:22:17 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35877170
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12828/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 11:22:17 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35877169
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
fommil <git@git.apache.org>,"Mon, 24 Feb 2014 12:06:51 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user fommil commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35879891
  
    Hi all,
    
    The discussions with ASF on the LEGAL ticket has exposed some concerns - **unrelated to the LGPL** -
    that I think everybody needs to be aware of regarding native BLAS/LAPACK libraries.
    
    Basically, the ASF need to bundle and license their projects in a way that is easy for distributors and end
    users to understand. They have gone to a lot of effort to authorise ""Category A"" and ""Category B"" licenses
    so that there are no surprises.
    
    However, native loading af system-provided BLAS/LAPACK **is** a surprise in this context.
    I don't want ASF's commercial distributors to get into a flap about dynamically loading
    binaries that were created by Apple, Intel, NVIDIA or AMD's. These binaries would not be
    explicitly listed in the software license list that Apache carefully construct.
    
    I propose a simple solution, which really is just the ASF's recommendation: we make the native
    components ""optional"" and make it very easy for distributors to turn them on if they understand
    the additional legal and technical implications.
    
    In fact, `netlib-java` already supports this... conservative upstream projects need only depend on the `core`
    artefact, and then end-users who want the native performance improvements can depend on `all` (or a more
    specific artefact, including their own): natives are an optional runtime dependency.
    
    @dlwh would you be happy enough to change breeze's dependency to depend on `com.github.fommil.netlib:core`
    and give easy instructions to your upstream users to depend on `com.github.fommil.netlib:all`
    in order to get the native speedups? (I can even write a this to be included in your `README`).
    
    To be honest, it would actually help clean my inbox because I get a lot of bug reports from users of
    Breeze who are confused about logging messages regarding natives failing to load because they have not
    followed the system natives instructions (e.g. they haven't installed ATLAS, so they get a warning message
    and then it harmlessly falls back to the Fortran or F2J implementations).
    
    Note that the Fortran reference natives - or ATLAS binaries - are not necessarily a problem from a licensing
    perspective, because we can explicitly list them. But, from a
    technical point of view I don't think it's really worth the extra efforts to give them
    special attention. The performance results (above) agree with all industry benchmarks (including my own
    and the Java Matrix Benchmarks) that say system optimised natives greatly outperform generically tuned
    implementations. Also, the Fortran implementation is only marginally faster than the F2J implementation
    (JVM JIT for the win!).
    
    BTW, it might be interesting for you to run the performance tests when using the F2J backend of
    `netlib-java` to convince yourself of the benefit of the system natives.
    
    Does this sound sensible?



---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
CodingCat <git@git.apache.org>,"Mon, 24 Feb 2014 12:08:08 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1102] Create a saveAsNewAPIHa...,dev@spark.incubator.apache.org,"Github user CodingCat commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/636#discussion_r9988672
  
    --- Diff: core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala ---
    @@ -686,6 +649,47 @@ class PairRDDFunctions[K: ClassTag, V: ClassTag](self: RDD[(K, V)])
       }
     
       /**
    +   * Output the RDD to any Hadoop-supported storage system with new Hadoop API, using a Hadoop
    +   * Job object for that storage system. The Job should set an OutputFormat and any output paths
    +   * required (e.g. a table name to write to) in the same way as it would be configured for a Hadoop
    +   * MapReduce job.
    +   */
    +  def saveAsNewAPIHadoopDataset(job: NewAPIHadoopJob) {
    --- End diff --
    
    Hi @mateiz in the new API, the old JobConf is replaced by mapreduce.Job (it's different from mapred.Job), I got this from here http://www.slideshare.net/sh1mmer/upgrading-to-the-new-map-reduce-api (page 10)


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
CodingCat <git@git.apache.org>,"Mon, 24 Feb 2014 12:18:04 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1089] fix the regression prob...,dev@spark.incubator.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/incubator-spark/pull/614#issuecomment-35880521
  
    @ScrapCodes thank you very much


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
ScrapCodes <git@git.apache.org>,"Mon, 24 Feb 2014 12:55:31 +0000 (UTC)",[GitHub] incubator-spark pull request: [WIP] SPARK-1096 A new scalastyle fo...,dev@spark.incubator.apache.org,"GitHub user ScrapCodes opened a pull request:

    https://github.com/apache/incubator-spark/pull/642

    [WIP] SPARK-1096 A new scalastyle for checking comments.

    Unfortunately scalastyle sbt plugin does not pick it. TBD: have to figure out how that can be done.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/ScrapCodes/incubator-spark scalastyle-comment-rule

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/642.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #642
    
----
commit 2c3441339ac8c5fe18690a2bbd5468e219c3e8f7
Author: Prashant Sharma <prashant.s@imaginea.com>
Date:   2014-02-24T12:24:32Z

    A new scalastyle for checking comments, unfortunately scalastyle sbt plugin does not pick it.

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
ScrapCodes <git@git.apache.org>,"Mon, 24 Feb 2014 13:04:16 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user ScrapCodes commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35884071
  
    Did another pass and things look okay, atleast from my side.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
witgo <git@git.apache.org>,"Mon, 24 Feb 2014 13:10:25 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1125: Fix building with maven ...,dev@spark.incubator.apache.org,"GitHub user witgo reopened a pull request:

    https://github.com/apache/incubator-spark/pull/639

    SPARK-1125: Fix building with maven on Mac OS X

    fix building with maven on Mac OS X throw Failure to find org.eclipse.paho:mqtt-client:jar:0.4.0 in https://repository.apache.org/content/repositories/releases was cached in the local repository, resolution will not be reattempted until the update interval of apache-repo has elapsed or updates are forced 

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/witgo/incubator-spark master

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/639.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #639
    
----
commit 27c612fb0dbbf27ba5a20d870a5cbb5cf33f4d9f
Author: liguoqiang <liguoqiang@rd.tuan800.com>
Date:   2014-02-24T04:00:36Z

    fix building with maven on Mac OS X throw Failure to find org.eclipse.paho:mqtt-client:jar:0.4.0 in https://repository.apache.org/content/repositories/releases was cached in the local repository, resolution will not be reattempted until the update interval of apache-repo has elapsed or updates are forced

commit 1ac1b4948cf54b7e7873c9aaa55de395f3926313
Author: liguoqiang <liguoqiang@rd.tuan800.com>
Date:   2014-02-24T09:44:39Z

    Exclude dependencies on log4j, slf4j-log4j12 from Spark Examples

commit f60016ff3828db0a9f451e04d72bc285b866d6e0
Author: liguoqiang <liguoqiang@rd.tuan800.com>
Date:   2014-02-24T09:50:17Z

    undo Exclude dependencies on mqtt-client  from Spark Examples

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 13:13:32 +0000 (UTC)",[GitHub] incubator-spark pull request: [WIP] SPARK-1096 A new scalastyle fo...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/642#issuecomment-35884696
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 13:13:32 +0000 (UTC)",[GitHub] incubator-spark pull request: [WIP] SPARK-1096 A new scalastyle fo...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/642#issuecomment-35884694
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 13:13:32 +0000 (UTC)","[GitHub] incubator-spark pull request: SPARK-1125: When using a http proxy,...",dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/639#issuecomment-35884698
  
    Can one of the admins verify this patch?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 13:13:46 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35884717
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 13:13:46 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35884716
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
Mridul Muralidharan <mridul@gmail.com>,"Mon, 24 Feb 2014 18:47:20 +0530",Re: Anyone wants to look at SPARK-1123?,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Curious, what was the issue ?

- Mridul


"
mridulm <git@git.apache.org>,"Mon, 24 Feb 2014 13:25:12 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1124: Fix infinite retries of ...,dev@spark.incubator.apache.org,"Github user mridulm commented on the pull request:

    https://github.com/apache/incubator-spark/pull/641#issuecomment-35885498
  
    Looks good, nice catch !


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. To do so, please top-post your response.
If your project does not have this feature enabled and wishes so, or if the
feature is enabled but not working, please contact infrastructure at
infrastructure@apache.org or file a JIRA ticket with INFRA.
---

"
mridulm <git@git.apache.org>,"Mon, 24 Feb 2014 13:39:31 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1124: Fix infinite retries of ...,dev@spark.incubator.apache.org,"Github user mridulm commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/641#discussion_r9990785
  
    --- Diff: core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala ---
    @@ -373,25 +375,26 @@ class DAGScheduler(
               } else {
                 def removeStage(stageId: Int) {
                   // data structures based on Stage
    -              stageIdToStage.get(stageId).foreach { s =>
    -                if (running.contains(s)) {
    +              for (stage <- stageIdToStage.get(stageId)) {
    +                if (running.contains(stage)) {
                       logDebug(""Removing running stage %d"".format(stageId))
    -                  running -= s
    +                  running -= stage
    +                }
    +                stageToInfos -= stage
    +                for (shuffleDep <- stage.shuffleDep) {
    +                  shuffleToMapStage.remove(shuffleDep.shuffleId)
                     }
    -                stageToInfos -= s
    -                shuffleToMapStage.keys.filter(shuffleToMapStage(_) == s).foreach(shuffleId =>
    -                  shuffleToMapStage.remove(shuffleId))
    -                if (pendingTasks.contains(s) && !pendingTasks(s).isEmpty) {
    +                if (pendingTasks.contains(stage) && !pendingTasks(stage).isEmpty) {
                       logDebug(""Removing pending status for stage %d"".format(stageId))
                     }
    -                pendingTasks -= s
    --- End diff --
    
    Unrelated, but we could move this into the condition above ... the checks are done anyway !


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 13:41:30 +0000 (UTC)",[GitHub] incubator-spark pull request: [WIP] SPARK-1096 A new scalastyle fo...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/642#issuecomment-35886690
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12829/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 13:41:30 +0000 (UTC)",[GitHub] incubator-spark pull request: [WIP] SPARK-1096 A new scalastyle fo...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/642#issuecomment-35886689
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 13:42:45 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35886773
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 13:42:45 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35886774
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12830/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
willb <git@git.apache.org>,"Mon, 24 Feb 2014 13:50:58 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1078: Replace lift-json with j...,dev@spark.incubator.apache.org,"Github user willb commented on the pull request:

    https://github.com/apache/incubator-spark/pull/582#issuecomment-35887433
  
    I'm not able to reproduce [this failure](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12819/testReport/) locally (either with my patch atop 437b62fcb, as it was when I updated my branch, or atop c0ef3afa, which Jenkins merged into when testing).  Does anyone have any advice?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
srowen <git@git.apache.org>,"Mon, 24 Feb 2014 13:59:25 +0000 (UTC)","[GitHub] incubator-spark pull request: SPARK-1125: When using a http proxy,...",dev@spark.incubator.apache.org,"Github user srowen commented on the pull request:

    https://github.com/apache/incubator-spark/pull/639#issuecomment-35888184
  
    Did you mean to reopen this? it seems entirely specific to your network and proxy requirements.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
witgo <git@git.apache.org>,"Mon, 24 Feb 2014 14:18:39 +0000 (UTC)","[GitHub] incubator-spark pull request: SPARK-1125: When using a http proxy,...",dev@spark.incubator.apache.org,"Github user witgo commented on the pull request:

    https://github.com/apache/incubator-spark/pull/639#issuecomment-35889797
  
    I think this PR can be configured with the http proxy maven who can properly compile spark


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
markhamstra <git@git.apache.org>,"Mon, 24 Feb 2014 16:03:34 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1124: Fix infinite retries of ...,dev@spark.incubator.apache.org,"Github user markhamstra commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/641#discussion_r9996518
  
    --- Diff: core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala ---
    @@ -373,25 +375,26 @@ class DAGScheduler(
               } else {
                 def removeStage(stageId: Int) {
                   // data structures based on Stage
    -              stageIdToStage.get(stageId).foreach { s =>
    -                if (running.contains(s)) {
    +              for (stage <- stageIdToStage.get(stageId)) {
    +                if (running.contains(stage)) {
                       logDebug(""Removing running stage %d"".format(stageId))
    -                  running -= s
    +                  running -= stage
    +                }
    +                stageToInfos -= stage
    +                for (shuffleDep <- stage.shuffleDep) {
    +                  shuffleToMapStage.remove(shuffleDep.shuffleId)
                     }
    -                stageToInfos -= s
    -                shuffleToMapStage.keys.filter(shuffleToMapStage(_) == s).foreach(shuffleId =>
    -                  shuffleToMapStage.remove(shuffleId))
    -                if (pendingTasks.contains(s) && !pendingTasks(s).isEmpty) {
    +                if (pendingTasks.contains(stage) && !pendingTasks(stage).isEmpty) {
                       logDebug(""Removing pending status for stage %d"".format(stageId))
                     }
    -                pendingTasks -= s
    --- End diff --
    
    Doing so would introduce both a bug and a logging error.  Removing an empty Set from pendingTasks is normal, expected and desired behavior at this point.  And only unexpected clean-up is intended to be logged.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
markhamstra <git@git.apache.org>,"Mon, 24 Feb 2014 16:10:26 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1124: Fix infinite retries of ...,dev@spark.incubator.apache.org,"Github user markhamstra commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/641#discussion_r9996859
  
    --- Diff: core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala ---
    @@ -272,8 +272,10 @@ class DAGScheduler(
         if (mapOutputTracker.has(shuffleDep.shuffleId)) {
           val serLocs = mapOutputTracker.getSerializedMapOutputStatuses(shuffleDep.shuffleId)
           val locs = MapOutputTracker.deserializeMapStatuses(serLocs)
    -      for (i <- 0 until locs.size) stage.outputLocs(i) = List(locs(i))
    -      stage.numAvailableOutputs = locs.size
    +      for (i <- 0 until locs.size) {
    +        stage.outputLocs(i) = Option(locs(i)).toList   // locs(i) will be null if missing
    --- End diff --
    
    Ok, it was a bad assumption on my part that the mapOutputTracker would only be returning valid locations, and this PR does address the possibility of null values here.  However, it feels to me like it is addressing the symptom instead of the cause.  Wouldn't we be better off either not putting nulls into the MapOutputTracker in the first place, or at least not returning them when getting map output statuses?  As it stands, null checks are probably needed everywhere the output statuses are fetched.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
markhamstra <git@git.apache.org>,"Mon, 24 Feb 2014 16:16:19 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1124: Fix infinite retries of ...,dev@spark.incubator.apache.org,"Github user markhamstra commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/641#discussion_r9997152
  
    --- Diff: core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala ---
    @@ -373,25 +375,26 @@ class DAGScheduler(
               } else {
                 def removeStage(stageId: Int) {
                   // data structures based on Stage
    -              stageIdToStage.get(stageId).foreach { s =>
    -                if (running.contains(s)) {
    +              for (stage <- stageIdToStage.get(stageId)) {
    +                if (running.contains(stage)) {
                       logDebug(""Removing running stage %d"".format(stageId))
    -                  running -= s
    +                  running -= stage
    +                }
    +                stageToInfos -= stage
    +                for (shuffleDep <- stage.shuffleDep) {
    --- End diff --
    
    At this point, the need is to clean up the DAGScheduler's data structures that reference the removed stage.  If you are 100% certain that the stage's notion of shuffleDeps contains every shuffleId that shuffleToMapStage is tracking for the stage, then you can take the simpler route that you have of working from the stage's understanding.  I wasn't 100% certain of that (which is not the same thing as saying that I have a good reason to believe that the stage's understanding of shuffleDeps will diverge from shuffleToMapStage's understanding), so I took the safer route of working from what shuffleToMapStage's knows instead of from what the stage knows.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
jyotiska <git@git.apache.org>,"Mon, 24 Feb 2014 16:36:40 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-972] Added detailed callsite ...,dev@spark.incubator.apache.org,"GitHub user jyotiska opened a pull request:

    https://github.com/apache/incubator-spark/pull/643

    [SPARK-972] Added detailed callsite info for ValueError in context.py

    This patch gives detailed information when user tries to create a spark context while one context is already running. The JIRA ticket is in SPARK-972. 

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/jyotiska/incubator-spark pyspark_code

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/643.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #643
    
----
commit 10c04aa08cd5a9f5277809f5d404c2c5cf8c3ac5
Author: jyotiska <jyotiska123@gmail.com>
Date:   2014-02-24T15:13:17Z

    added callsite info on value error for context.py

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 16:38:37 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-972] Added detailed callsite ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/643#issuecomment-35905556
  
    Can one of the admins verify this patch?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
CodingCat <git@git.apache.org>,"Mon, 24 Feb 2014 16:41:35 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1102] Create a saveAsNewAPIHa...,dev@spark.incubator.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/incubator-spark/pull/636#issuecomment-35905921
  
    Jenkins....are you OK?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
dlwh <git@git.apache.org>,"Mon, 24 Feb 2014 16:48:04 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user dlwh commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35906687
  
    @fommil fine by me. I'll get on it.
    
    > Hi all,
    >
    > The discussions with ASF on the LEGAL ticket has exposed some concerns - *unrelated
    > to the LGPL* -
    > that I think everybody needs to be aware of regarding native BLAS/LAPACK
    > libraries.
    >
    > Basically, the ASF need to bundle and license their projects in a way that
    > is easy for distributors and end
    > users to understand. They have gone to a lot of effort to authorise
    > ""Category A"" and ""Category B"" licenses
    > so that there are no surprises.
    >
    > However, native loading af system-provided BLAS/LAPACK *is* a surprise in
    > this context.
    > I don't want ASF's commercial distributors to get into a flap about
    > dynamically loading
    > binaries that were created by Apple, Intel, NVIDIA or AMD's. These
    > binaries would not be
    > explicitly listed in the software license list that Apache carefully
    > construct.
    >
    > I propose a simple solution, which really is just the ASF's
    > recommendation: we make the native
    > components ""optional"" and make it very easy for distributors to turn them
    > on if they understand
    > the additional legal and technical implications.
    >
    > In fact, netlib-java already supports this... conservative upstream
    > projects need only depend on the core
    > artefact, and then end-users who want the native performance improvements
    > can depend on all (or a more
    > specific artefact, including their own): natives are an optional runtime
    > dependency.
    >
    > @dlwh <https://github.com/dlwh> would you be happy enough to change
    > breeze's dependency to depend on com.github.fommil.netlib:core
    > and give easy instructions to your upstream users to depend on
    > com.github.fommil.netlib:all
    > in order to get the native speedups? (I can even write a this to be
    > included in your README).
    >
    > To be honest, it would actually help clean my inbox because I get a lot of
    > bug reports from users of
    > Breeze who are confused about logging messages regarding natives failing
    > to load because they have not
    > followed the system natives instructions (e.g. they haven't installed
    > ATLAS, so they get a warning message
    > and then it harmlessly falls back to the Fortran or F2J implementations).
    >
    > Note that the Fortran reference natives - or ATLAS binaries - are not
    > necessarily a problem from a licensing
    > perspective, because we can explicitly list them. But, from a
    > technical point of view I don't think it's really worth the extra efforts
    > to give them
    > special attention. The performance results (above) agree with all industry
    > benchmarks (including my own
    > and the Java Matrix Benchmarks) that say system optimised natives greatly
    > outperform generically tuned
    > implementations. Also, the Fortran implementation is only marginally
    > faster than the F2J implementation
    > (JVM JIT for the win!).
    >
    > BTW, it might be interesting for you to run the performance tests when
    > using the F2J backend of
    > netlib-java to convince yourself of the benefit of the system natives.
    >
    > Does this sound sensible?
    >
    > â€”
    > Reply to this email directly or view it on GitHub<https://github.com/apache/incubator-spark/pull/575#issuecomment-35879891>
    > .
    >


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
fommil <git@git.apache.org>,"Mon, 24 Feb 2014 16:50:47 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user fommil commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35907015
  
    this also means that the JNILoader license simply doesn't matter anymore, which saves me from having to issue a new release.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
CodingCat <git@git.apache.org>,"Mon, 24 Feb 2014 17:01:02 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-1089] fix the regression prob...,dev@spark.incubator.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/incubator-spark/pull/614#issuecomment-35908324
  
    @ScrapCodes I'm looking at your suggestion, the difficulty to move to createInterpreter() is that you cannot not pass the parameter ""settings"" to there, do you want me to change the createInterpreter() API (I don't think so)?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
mridulm <git@git.apache.org>,"Mon, 24 Feb 2014 17:34:47 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1124: Fix infinite retries of ...,dev@spark.incubator.apache.org,"Github user mridulm commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/641#discussion_r10000592
  
    --- Diff: core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala ---
    @@ -373,25 +375,26 @@ class DAGScheduler(
               } else {
                 def removeStage(stageId: Int) {
                   // data structures based on Stage
    -              stageIdToStage.get(stageId).foreach { s =>
    -                if (running.contains(s)) {
    +              for (stage <- stageIdToStage.get(stageId)) {
    +                if (running.contains(stage)) {
                       logDebug(""Removing running stage %d"".format(stageId))
    -                  running -= s
    +                  running -= stage
    +                }
    +                stageToInfos -= stage
    +                for (shuffleDep <- stage.shuffleDep) {
    +                  shuffleToMapStage.remove(shuffleDep.shuffleId)
                     }
    -                stageToInfos -= s
    -                shuffleToMapStage.keys.filter(shuffleToMapStage(_) == s).foreach(shuffleId =>
    -                  shuffleToMapStage.remove(shuffleId))
    -                if (pendingTasks.contains(s) && !pendingTasks(s).isEmpty) {
    +                if (pendingTasks.contains(stage) && !pendingTasks(stage).isEmpty) {
                       logDebug(""Removing pending status for stage %d"".format(stageId))
                     }
    -                pendingTasks -= s
    --- End diff --
    
    I am not sure if we are talking about the same thing here ...
    I wanted 'pendingTasks -= stage' to be moved into the if() condition.
    I dont see how it is a behavior change or a bug


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
mridulm <git@git.apache.org>,"Mon, 24 Feb 2014 17:37:55 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1124: Fix infinite retries of ...,dev@spark.incubator.apache.org,"Github user mridulm commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/641#discussion_r10000699
  
    --- Diff: core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala ---
    @@ -373,25 +375,26 @@ class DAGScheduler(
               } else {
                 def removeStage(stageId: Int) {
                   // data structures based on Stage
    -              stageIdToStage.get(stageId).foreach { s =>
    -                if (running.contains(s)) {
    +              for (stage <- stageIdToStage.get(stageId)) {
    +                if (running.contains(stage)) {
                       logDebug(""Removing running stage %d"".format(stageId))
    -                  running -= s
    +                  running -= stage
    +                }
    +                stageToInfos -= stage
    +                for (shuffleDep <- stage.shuffleDep) {
    +                  shuffleToMapStage.remove(shuffleDep.shuffleId)
                     }
    -                stageToInfos -= s
    -                shuffleToMapStage.keys.filter(shuffleToMapStage(_) == s).foreach(shuffleId =>
    -                  shuffleToMapStage.remove(shuffleId))
    -                if (pendingTasks.contains(s) && !pendingTasks(s).isEmpty) {
    +                if (pendingTasks.contains(stage) && !pendingTasks(stage).isEmpty) {
                       logDebug(""Removing pending status for stage %d"".format(stageId))
                     }
    -                pendingTasks -= s
    --- End diff --
    
    I should elaborate - I made the assumption that the change was obvious.
    
    if (pendingTasks.contains(stage)) {
      if (!pendingTasks(stage).isEmpty) logDebug ...
       pendingTasks -= stage
    }
    
    Update pendingTasks only if it is known to contain the stage.
    I am not sure how common it is for pendingTasks not to have the stage - but I would assume it is fairly common : so avoid unnecessary re-search


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
markhamstra <git@git.apache.org>,"Mon, 24 Feb 2014 17:41:14 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1124: Fix infinite retries of ...,dev@spark.incubator.apache.org,"Github user markhamstra commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/641#discussion_r10000819
  
    --- Diff: core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala ---
    @@ -373,25 +375,26 @@ class DAGScheduler(
               } else {
                 def removeStage(stageId: Int) {
                   // data structures based on Stage
    -              stageIdToStage.get(stageId).foreach { s =>
    -                if (running.contains(s)) {
    +              for (stage <- stageIdToStage.get(stageId)) {
    +                if (running.contains(stage)) {
                       logDebug(""Removing running stage %d"".format(stageId))
    -                  running -= s
    +                  running -= stage
    +                }
    +                stageToInfos -= stage
    +                for (shuffleDep <- stage.shuffleDep) {
    +                  shuffleToMapStage.remove(shuffleDep.shuffleId)
                     }
    -                stageToInfos -= s
    -                shuffleToMapStage.keys.filter(shuffleToMapStage(_) == s).foreach(shuffleId =>
    -                  shuffleToMapStage.remove(shuffleId))
    -                if (pendingTasks.contains(s) && !pendingTasks(s).isEmpty) {
    +                if (pendingTasks.contains(stage) && !pendingTasks(stage).isEmpty) {
                       logDebug(""Removing pending status for stage %d"".format(stageId))
                     }
    -                pendingTasks -= s
    --- End diff --
    
    That works, but splitting the conditional is probably less performant than asking the Set to remove an element that it doesn't contain.  Either way, it's a trivial difference.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
Nan Zhu <zhunanmcgill@gmail.com>,"Mon, 24 Feb 2014 12:46:16 -0500",Re: Anyone wants to look at SPARK-1123?,dev@spark.incubator.apache.org,"I just misread the API doc, and forgot to pass the type information when calling this 

Best, 

-- 
Nan Zhu





"
mridulm <git@git.apache.org>,"Mon, 24 Feb 2014 17:43:51 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1124: Fix infinite retries of ...,dev@spark.incubator.apache.org,"Github user mridulm commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/641#discussion_r10000899
  
    --- Diff: core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala ---
    @@ -373,25 +375,26 @@ class DAGScheduler(
               } else {
                 def removeStage(stageId: Int) {
                   // data structures based on Stage
    -              stageIdToStage.get(stageId).foreach { s =>
    -                if (running.contains(s)) {
    +              for (stage <- stageIdToStage.get(stageId)) {
    +                if (running.contains(stage)) {
                       logDebug(""Removing running stage %d"".format(stageId))
    -                  running -= s
    +                  running -= stage
    +                }
    +                stageToInfos -= stage
    +                for (shuffleDep <- stage.shuffleDep) {
    +                  shuffleToMapStage.remove(shuffleDep.shuffleId)
                     }
    -                stageToInfos -= s
    -                shuffleToMapStage.keys.filter(shuffleToMapStage(_) == s).foreach(shuffleId =>
    -                  shuffleToMapStage.remove(shuffleId))
    -                if (pendingTasks.contains(s) && !pendingTasks(s).isEmpty) {
    +                if (pendingTasks.contains(stage) && !pendingTasks(stage).isEmpty) {
                       logDebug(""Removing pending status for stage %d"".format(stageId))
                     }
    -                pendingTasks -= s
    --- End diff --
    
    That is surprising !
    I would assume searching for an entry in a Set is atleast a few dozen opcodes, if not more - even if already searched (and so bpt's are favourable); would still be much more expensive than splitting an if for sure - I have not looked into scala's impl : but assuming reasonable implementation of 'if', this is definitely unexpected :-)


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
markhamstra <git@git.apache.org>,"Mon, 24 Feb 2014 17:46:15 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1124: Fix infinite retries of ...,dev@spark.incubator.apache.org,"Github user markhamstra commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/641#discussion_r10000970
  
    --- Diff: core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala ---
    @@ -373,25 +375,26 @@ class DAGScheduler(
               } else {
                 def removeStage(stageId: Int) {
                   // data structures based on Stage
    -              stageIdToStage.get(stageId).foreach { s =>
    -                if (running.contains(s)) {
    +              for (stage <- stageIdToStage.get(stageId)) {
    +                if (running.contains(stage)) {
                       logDebug(""Removing running stage %d"".format(stageId))
    -                  running -= s
    +                  running -= stage
    +                }
    +                stageToInfos -= stage
    +                for (shuffleDep <- stage.shuffleDep) {
    +                  shuffleToMapStage.remove(shuffleDep.shuffleId)
                     }
    -                stageToInfos -= s
    -                shuffleToMapStage.keys.filter(shuffleToMapStage(_) == s).foreach(shuffleId =>
    -                  shuffleToMapStage.remove(shuffleId))
    -                if (pendingTasks.contains(s) && !pendingTasks(s).isEmpty) {
    +                if (pendingTasks.contains(stage) && !pendingTasks(stage).isEmpty) {
                       logDebug(""Removing pending status for stage %d"".format(stageId))
                     }
    -                pendingTasks -= s
    --- End diff --
    
    And the expected, common case when a stage is being removed is for pendingTasks to have an empty Set of tasks associated with that stage.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
mridulm <git@git.apache.org>,"Mon, 24 Feb 2014 17:46:39 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1124: Fix infinite retries of ...,dev@spark.incubator.apache.org,"Github user mridulm commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/641#discussion_r10000990
  
    --- Diff: core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala ---
    @@ -373,25 +375,26 @@ class DAGScheduler(
               } else {
                 def removeStage(stageId: Int) {
                   // data structures based on Stage
    -              stageIdToStage.get(stageId).foreach { s =>
    -                if (running.contains(s)) {
    +              for (stage <- stageIdToStage.get(stageId)) {
    +                if (running.contains(stage)) {
                       logDebug(""Removing running stage %d"".format(stageId))
    -                  running -= s
    +                  running -= stage
    +                }
    +                stageToInfos -= stage
    +                for (shuffleDep <- stage.shuffleDep) {
    +                  shuffleToMapStage.remove(shuffleDep.shuffleId)
                     }
    -                stageToInfos -= s
    -                shuffleToMapStage.keys.filter(shuffleToMapStage(_) == s).foreach(shuffleId =>
    -                  shuffleToMapStage.remove(shuffleId))
    -                if (pendingTasks.contains(s) && !pendingTasks(s).isEmpty) {
    +                if (pendingTasks.contains(stage) && !pendingTasks(stage).isEmpty) {
                       logDebug(""Removing pending status for stage %d"".format(stageId))
                     }
    -                pendingTasks -= s
    --- End diff --
    
    To add, this was just a suggestion given the code was already changing there; and was more readable.
    Punting on it is perfectly fine - not a big deal


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
mridulm <git@git.apache.org>,"Mon, 24 Feb 2014 17:54:30 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1124: Fix infinite retries of ...,dev@spark.incubator.apache.org,"Github user mridulm commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/641#discussion_r10001290
  
    --- Diff: core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala ---
    @@ -373,25 +375,26 @@ class DAGScheduler(
               } else {
                 def removeStage(stageId: Int) {
                   // data structures based on Stage
    -              stageIdToStage.get(stageId).foreach { s =>
    -                if (running.contains(s)) {
    +              for (stage <- stageIdToStage.get(stageId)) {
    +                if (running.contains(stage)) {
                       logDebug(""Removing running stage %d"".format(stageId))
    -                  running -= s
    +                  running -= stage
    +                }
    +                stageToInfos -= stage
    +                for (shuffleDep <- stage.shuffleDep) {
    +                  shuffleToMapStage.remove(shuffleDep.shuffleId)
                     }
    -                stageToInfos -= s
    -                shuffleToMapStage.keys.filter(shuffleToMapStage(_) == s).foreach(shuffleId =>
    -                  shuffleToMapStage.remove(shuffleId))
    -                if (pendingTasks.contains(s) && !pendingTasks(s).isEmpty) {
    +                if (pendingTasks.contains(stage) && !pendingTasks(stage).isEmpty) {
                       logDebug(""Removing pending status for stage %d"".format(stageId))
                     }
    -                pendingTasks -= s
    --- End diff --
    


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
mengxr <git@git.apache.org>,"Mon, 24 Feb 2014 18:06:22 +0000 (UTC)",[GitHub] incubator-spark pull request: Initialized the regVal for first ite...,dev@spark.incubator.apache.org,"Github user mengxr commented on the pull request:

    https://github.com/apache/incubator-spark/pull/633#issuecomment-35915806
  
    @dbtsai Since regVal remains 0.0 for any existing updater in MLlib, it would make more sense if this change comes with the L-BFGS PR you are working on.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
kellrott <git@git.apache.org>,"Mon, 24 Feb 2014 18:16:29 +0000 (UTC)",[GitHub] incubator-spark pull request: Patch for SPARK-942,dev@spark.incubator.apache.org,"Github user kellrott commented on the pull request:

    https://github.com/apache/incubator-spark/pull/180#issuecomment-35916887
  
    Pull Request log, day 100. Itâ€™s been three months since I was submitted, and almost 2 months since I last heard from an admin. I pass all unit tests and fix and known issues, but Iâ€™ve reached the last page of the request list, and it looking like Iâ€™ve been forgotten about. Itâ€™s getting cold and Iâ€™m starting to lose hope. 


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
mengxr <git@git.apache.org>,"Mon, 24 Feb 2014 18:33:30 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user mengxr commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35918773
  
    @fommil Either AL2 or MPL should work. We only need appropriate labeling for MPL, which is trivial. And thanks for the suggestion of making native libraries optional. It simplifies the problem.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
aarondav <git@git.apache.org>,"Mon, 24 Feb 2014 18:39:08 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1078: Replace lift-json with j...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/582#issuecomment-35919353
  
    Let's just try it again - Jenkins, retest this please.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
rxin <git@git.apache.org>,"Mon, 24 Feb 2014 18:52:30 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1078: Replace lift-json with j...,dev@spark.incubator.apache.org,"Github user rxin commented on the pull request:

    https://github.com/apache/incubator-spark/pull/582#issuecomment-35920859
  
    Jenkins, retest this please.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
mateiz <git@git.apache.org>,"Mon, 24 Feb 2014 18:55:36 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1124: Fix infinite retries of ...,dev@spark.incubator.apache.org,"Github user mateiz commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/641#discussion_r10003953
  
    --- Diff: core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala ---
    @@ -272,8 +272,10 @@ class DAGScheduler(
         if (mapOutputTracker.has(shuffleDep.shuffleId)) {
           val serLocs = mapOutputTracker.getSerializedMapOutputStatuses(shuffleDep.shuffleId)
           val locs = MapOutputTracker.deserializeMapStatuses(serLocs)
    -      for (i <- 0 until locs.size) stage.outputLocs(i) = List(locs(i))
    -      stage.numAvailableOutputs = locs.size
    +      for (i <- 0 until locs.size) {
    +        stage.outputLocs(i) = Option(locs(i)).toList   // locs(i) will be null if missing
    --- End diff --
    
    Yeah, the problem is that the MapOutputTracker returns an array of MapStatus, where position i of the array is supposed to be the location of the output of task i. This array contains nulls initially to denote a missing location, because a MapStatus can only indicate a real location. So I think that either way we'll have to check for some ""missing output"" condition.
    
    With this fix I believe we won't be launching reduce stages when some maps are missing anymore, so we should be okay.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
mateiz <git@git.apache.org>,"Mon, 24 Feb 2014 19:00:35 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1124: Fix infinite retries of ...,dev@spark.incubator.apache.org,"Github user mateiz commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/641#discussion_r10004187
  
    --- Diff: core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala ---
    @@ -373,25 +375,26 @@ class DAGScheduler(
               } else {
                 def removeStage(stageId: Int) {
                   // data structures based on Stage
    -              stageIdToStage.get(stageId).foreach { s =>
    -                if (running.contains(s)) {
    +              for (stage <- stageIdToStage.get(stageId)) {
    +                if (running.contains(stage)) {
                       logDebug(""Removing running stage %d"".format(stageId))
    -                  running -= s
    +                  running -= stage
    +                }
    +                stageToInfos -= stage
    +                for (shuffleDep <- stage.shuffleDep) {
    --- End diff --
    
    Good point, maybe I'll change this back. It seemed that each stage is added using its own shuffleID, but perhaps with failures we will sometimes replace it with a newer stage and then we'd be removing that one.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 19:14:30 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1078: Replace lift-json with j...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/582#issuecomment-35923427
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 19:14:30 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1078: Replace lift-json with j...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/582#issuecomment-35923428
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
fommil <git@git.apache.org>,"Mon, 24 Feb 2014 19:36:52 +0000 (UTC)",[GitHub] incubator-spark pull request: [Proposal] Adding sparse data suppor...,dev@spark.incubator.apache.org,"Github user fommil commented on the pull request:

    https://github.com/apache/incubator-spark/pull/575#issuecomment-35926018
  
    @mengxr I don't think we need to change the license now since it's optional... I think the LGPL are the least contentious of the possible licenses at play here :-) Of the core `netlib-java` libraries, this is the license for you to check: https://github.com/fommil/netlib-java/blob/master/LICENSE.txt I think it's actually three clause BSD, not MIT, sorry about that.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
markhamstra <git@git.apache.org>,"Mon, 24 Feb 2014 19:56:57 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1124: Fix infinite retries of ...,dev@spark.incubator.apache.org,"Github user markhamstra commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/641#discussion_r10006949
  
    --- Diff: core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala ---
    @@ -272,8 +272,10 @@ class DAGScheduler(
         if (mapOutputTracker.has(shuffleDep.shuffleId)) {
           val serLocs = mapOutputTracker.getSerializedMapOutputStatuses(shuffleDep.shuffleId)
           val locs = MapOutputTracker.deserializeMapStatuses(serLocs)
    -      for (i <- 0 until locs.size) stage.outputLocs(i) = List(locs(i))
    -      stage.numAvailableOutputs = locs.size
    +      for (i <- 0 until locs.size) {
    +        stage.outputLocs(i) = Option(locs(i)).toList   // locs(i) will be null if missing
    --- End diff --
    
    Yup, as long as the underlying data structure is an array of length numTasks, null is as good as any other ""missing output"" flag.  Changing to a Map containing only valid locations instead of an array might make sense if the need to get the output statuses grows beyond the present cases in MapOutputTracker and DAGScheduler (and if the performance difference is acceptable), but now that we're handling the nulls properly in those existing cases, we're good at least for now. 


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
Punya Biswal <pbiswal@palantir.com>,"Mon, 24 Feb 2014 20:01:03 +0000",Re: Request to review PR #605,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi Patrick,

Sorry, it¹s hard to tell whether people follow the Github comments feed. I
did ping Aaron in the Github comments about three days ago and it didn¹t
seem like anything was happening. In an earlier conversation, he indicated
that 2-3 days was a reasonable time frame for alerting people.

Do you think I should write to him (or whoever I¹m working with for the
review) personally instead of spamming the list? I really don¹t want to
create more noise for you guys but it¹s hard to tell when a patch has been
forgotten.

Punya


"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 20:13:40 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1078: Replace lift-json with j...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/582#issuecomment-35930342
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 20:13:40 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1078: Replace lift-json with j...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/582#issuecomment-35930343
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12831/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
Andrew Ash <andrew@andrewash.com>,"Mon, 24 Feb 2014 13:07:21 -0800",SPARK-942 patch review,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi Spark devs,

Kyle identified a deficiency in Spark where generating iterators are
unrolled into memory and then flushed to disk rather than sent straight to
disk when possible.

He's had a patch sitting ready for code review for quite some time now (100
days) but no response.

Is this something that an admin would be able to review?  I for one would
find this quite valuable.

Thanks!
Andrew


https://spark-project.atlassian.net/browse/SPARK-942
https://github.com/apache/incubator-spark/pull/180
"
ash211 <git@git.apache.org>,"Mon, 24 Feb 2014 21:10:35 +0000 (UTC)",[GitHub] incubator-spark pull request: Patch for SPARK-942,dev@spark.incubator.apache.org,"Github user ash211 commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/180#discussion_r10010480
  
    --- Diff: core/src/main/scala/org/apache/spark/CacheManager.scala ---
    @@ -71,10 +71,21 @@ private[spark] class CacheManager(blockManager: BlockManager) extends Logging {
               val computedValues = rdd.computeOrReadCheckpoint(split, context)
               // Persist the result, so long as the task is not running locally
               if (context.runningLocally) { return computedValues }
    -          val elements = new ArrayBuffer[Any]
    -          elements ++= computedValues
    -          blockManager.put(key, elements, storageLevel, tellMaster = true)
    -          elements.iterator.asInstanceOf[Iterator[T]]
    +          if (storageLevel == StorageLevel.DISK_ONLY || storageLevel == StorageLevel.DISK_ONLY_2) {
    --- End diff --
    
    StorageLevels have .useDisk() and .useMemory() methods.  https://github.com/apache/incubator-spark/blob/master/core/src/main/scala/org/apache/spark/storage/StorageLevel.scala Maybe it makes sense to replace this with storageLevel.useDisk() && !storageLevel.useMemory()


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
Nan Zhu <zhunanmcgill@gmail.com>,"Mon, 24 Feb 2014 16:16:20 -0500",Re: SPARK-942 patch review,dev@spark.incubator.apache.org,"yet another email about forgotten PR

I think Sean would like to start some discussion on the current situation where committers are facing a flood of PRs recently (as he said in the discussion thread about how to prevent the blob of RDD API)?

Best, 

-- 
Nan Zhu





"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 21:16:08 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1124: Fix infinite retries of ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/641#issuecomment-35939005
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 21:16:08 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1124: Fix infinite retries of ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/641#issuecomment-35939003
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
mateiz <git@git.apache.org>,"Mon, 24 Feb 2014 21:17:45 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1124: Fix infinite retries of ...,dev@spark.incubator.apache.org,"Github user mateiz commented on the pull request:

    https://github.com/apache/incubator-spark/pull/641#issuecomment-35939422
  
    Okay, I think I've fixed the shuffleToMapStage removal part: https://github.com/mateiz/incubator-spark/commit/0187cef0f284e6cb22cb3986c327c43304daf57d.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
Andrew Ash <andrew@andrewash.com>,"Mon, 24 Feb 2014 13:18:08 -0800",Re: SPARK-942 patch review,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Would love to have a discussion since I know the core contributors are
facing a barrage of PRs and things are falling through the cracks.

Is there a list of who can commit to core Spark somewhere?  Maybe that list
should be expanded or there should be a rotation of PR duty of some sort.

way more contributions than you expected!



"
Nan Zhu <zhunanmcgill@gmail.com>,"Mon, 24 Feb 2014 16:24:16 -0500",Re: SPARK-942 patch review,dev@spark.incubator.apache.org,"Do you mean this https://cwiki.apache.org/confluence/display/SPARK/Committers?

-- 
Nan Zhu





"
markhamstra <git@git.apache.org>,"Mon, 24 Feb 2014 21:29:42 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1124: Fix infinite retries of ...,dev@spark.incubator.apache.org,"Github user markhamstra commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/641#discussion_r10011413
  
    --- Diff: core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala ---
    @@ -373,25 +375,26 @@ class DAGScheduler(
               } else {
                 def removeStage(stageId: Int) {
                   // data structures based on Stage
    -              stageIdToStage.get(stageId).foreach { s =>
    -                if (running.contains(s)) {
    +              for (stage <- stageIdToStage.get(stageId)) {
    +                if (running.contains(stage)) {
                       logDebug(""Removing running stage %d"".format(stageId))
    -                  running -= s
    +                  running -= stage
    +                }
    +                stageToInfos -= stage
    +                for ((k, v) <- shuffleToMapStage.find(_._2 == stage)) {
    --- End diff --
    
    nit: for((k, _) <- ...)


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
Andrew Ash <andrew@andrewash.com>,"Mon, 24 Feb 2014 13:30:01 -0800",Re: SPARK-942 patch review,dev@spark.incubator.apache.org,"Yep that's the one thanks! That's quite a few more people than I thought

Sent from my mobile phone

"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 24 Feb 2014 13:39:12 -0800",Re: SPARK-942 patch review,dev@spark.incubator.apache.org,"old inactive PRs on GitHub are not really getting closed, so active ones might be lost between those. For now please just post on the dev list if your PR is being ignored. We’ll implement some kind of cleanup (at least manually) to close the old ones.

Matei


thought
are
that
sort.
get
<zhunanmcgill@gmail.com(mailto:
the
are


"
mateiz <git@git.apache.org>,"Mon, 24 Feb 2014 21:40:57 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1124: Fix infinite retries of ...,dev@spark.incubator.apache.org,"Github user mateiz commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/641#discussion_r10011948
  
    --- Diff: core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala ---
    @@ -373,25 +375,26 @@ class DAGScheduler(
               } else {
                 def removeStage(stageId: Int) {
                   // data structures based on Stage
    -              stageIdToStage.get(stageId).foreach { s =>
    -                if (running.contains(s)) {
    +              for (stage <- stageIdToStage.get(stageId)) {
    +                if (running.contains(stage)) {
                       logDebug(""Removing running stage %d"".format(stageId))
    -                  running -= s
    +                  running -= stage
    +                }
    +                stageToInfos -= stage
    +                for ((k, v) <- shuffleToMapStage.find(_._2 == stage)) {
    --- End diff --
    
    Haha, I wanted to avoid lots of underscores, but I can do it if you think it's more idiomatic.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Mon, 24 Feb 2014 21:41:09 +0000 (UTC)",[GitHub] incubator-spark pull request: Patch for SPARK-942,dev@spark.incubator.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/180#discussion_r10011961
  
    --- Diff: core/src/test/scala/org/apache/spark/storage/LargeIteratorSuite.scala ---
    @@ -0,0 +1,55 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one or more
    + * contributor license agreements.  See the NOTICE file distributed with
    + * this work for additional information regarding copyright ownership.
    + * The ASF licenses this file to You under the Apache License, Version 2.0
    + * (the ""License""); you may not use this file except in compliance with
    + * the License.  You may obtain a copy of the License at
    + *
    + *    http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.spark.storage
    +
    +import org.scalatest.FunSuite
    +import org.apache.spark.{LocalSparkContext, SparkContext}
    +
    +class Expander(base:String, count:Int) extends Iterator[String] {
    +  var i = 0;
    +  def next() : String = {
    +    i += 1;
    +    return base + i.toString;
    +  }
    +  def hasNext() : Boolean = i < count;
    +}
    +
    +object Expander {
    +  def expand(s:String, i:Int) : Iterator[String] = {
    +    return new Expander(s,i)
    +  }
    +}
    +
    +class LargeIteratorSuite extends FunSuite with LocalSparkContext {
    +  /* Tests the ability of Spark to deal with user provided iterators that
    +   * generate more data then available memory. In any memory based persistance
    +   * Spark will unroll the iterator into an ArrayBuffer for caching, however in
    +   * the case that the use defines DISK_ONLY persistance, the iterator will be 
    +   * fed directly to the serializer and written to disk.
    +   */
    +  val clusterUrl = ""local-cluster[1,1,512]""
    +  test(""Flatmap iterator"") {
    +    sc = new SparkContext(clusterUrl, ""mem_test"");
    +    val seeds = sc.parallelize( Array(
    +      ""This is the first sentence that we will test:"",
    +      ""This is the second sentence that we will test:"",
    +      ""This is the third sentence that we will test:""
    +    ) );
    +    val out = seeds.flatMap(Expander.expand(_,10000000));
    --- End diff --
    
    Could this test the read-side as well... right now it seems to only test that there is not an exception when writing - but there is no test that the correctness is preserved.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
markhamstra <git@git.apache.org>,"Mon, 24 Feb 2014 21:47:03 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1124: Fix infinite retries of ...,dev@spark.incubator.apache.org,"Github user markhamstra commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/641#discussion_r10012275
  
    --- Diff: core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala ---
    @@ -373,25 +375,26 @@ class DAGScheduler(
               } else {
                 def removeStage(stageId: Int) {
                   // data structures based on Stage
    -              stageIdToStage.get(stageId).foreach { s =>
    -                if (running.contains(s)) {
    +              for (stage <- stageIdToStage.get(stageId)) {
    +                if (running.contains(stage)) {
                       logDebug(""Removing running stage %d"".format(stageId))
    -                  running -= s
    +                  running -= stage
    +                }
    +                stageToInfos -= stage
    +                for ((k, v) <- shuffleToMapStage.find(_._2 == stage)) {
    --- End diff --
    
    No big deal either way.  It's just that when I see a bound pattern I expect it to be used; whereas if the pattern is only partially bound, then I know to expect that parts of the complete pattern won't be used.  In this case, the relevant code is short and obvious enough that there's not much difference.  


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
mateiz <git@git.apache.org>,"Mon, 24 Feb 2014 21:49:09 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1124: Fix infinite retries of ...,dev@spark.incubator.apache.org,"Github user mateiz commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/641#discussion_r10012382
  
    --- Diff: core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala ---
    @@ -373,25 +375,26 @@ class DAGScheduler(
               } else {
                 def removeStage(stageId: Int) {
                   // data structures based on Stage
    -              stageIdToStage.get(stageId).foreach { s =>
    -                if (running.contains(s)) {
    +              for (stage <- stageIdToStage.get(stageId)) {
    +                if (running.contains(stage)) {
                       logDebug(""Removing running stage %d"".format(stageId))
    -                  running -= s
    +                  running -= stage
    +                }
    +                stageToInfos -= stage
    +                for ((k, v) <- shuffleToMapStage.find(_._2 == stage)) {
    --- End diff --
    
    Alright, I'll probably leave it as is then, but thanks for taking a look.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 22:13:46 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1124: Fix infinite retries of ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/641#issuecomment-35946252
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12832/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 22:13:47 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1124: Fix infinite retries of ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/641#issuecomment-35946250
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
CodingCat <git@git.apache.org>,"Mon, 24 Feb 2014 22:24:04 +0000 (UTC)",[GitHub] incubator-spark pull request: Whitelist+spreadout,dev@spark.incubator.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/incubator-spark/pull/136#issuecomment-35947344
  
    anyone is still looking at this? I think application-specific spreadout option is good


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
bouk <git@git.apache.org>,"Mon, 24 Feb 2014 22:42:01 +0000 (UTC)",[GitHub] incubator-spark pull request: Catch depickling errors,dev@spark.incubator.apache.org,"GitHub user bouk opened a pull request:

    https://github.com/apache/incubator-spark/pull/644

    Catch depickling errors

    This surroungs the complete worker code in a try/except block so we catch any error that arrives. An example would be the depickling failing for some reason
    
    @JoshRosen

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/Shopify/spark catch-depickling-errors

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/644.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #644
    
----
commit 0e4d504941528451af570b796a37c9a4c579d2b0
Author: Bouke van der Bijl <boukevanderbijl@gmail.com>
Date:   2014-02-24T20:52:44Z

    Surround the complete python worker with the try block
    
    This is for making sure depickling and any other nasty errors are
    captured when they arrive before the function is actually being executed

commit f0f67cc7b1478de82f750fc068695caff4079eb1
Author: Bouke van der Bijl <boukevanderbijl@gmail.com>
Date:   2014-02-24T21:01:31Z

    Lol indentation

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Mon, 24 Feb 2014 23:14:45 +0000 (UTC)",[GitHub] incubator-spark pull request: Catch depickling errors,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/644#issuecomment-35952358
  
    Can one of the admins verify this patch?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Mon, 24 Feb 2014 23:33:49 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1051. Executors should doAs su...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/538#issuecomment-35954077
  
    Hey @tgraves do you mind taking a quick look? I was hoping to merge this one.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Mon, 24 Feb 2014 23:34:00 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1053. Don't require SPARK_YARN...,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/553#issuecomment-35954097
  
    @tgravescs mind taking a look here as well?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
aarondav <git@git.apache.org>,"Mon, 24 Feb 2014 23:35:54 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1078: Replace lift-json with j...,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/582#issuecomment-35954244
  
    Looks good to me! Will merge soon, if no one else has comments.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
mateiz <git@git.apache.org>,"Tue, 25 Feb 2014 00:08:46 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user mateiz commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35959525
  
    Hey Prashant, one other thing, can you add the Java 8 tests in the Maven build too? Is there a way to only optionally run them there?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
mateiz <git@git.apache.org>,"Tue, 25 Feb 2014 00:09:46 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user mateiz commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/539#discussion_r10018257
  
    --- Diff: core/src/main/java/org/apache/spark/api/java/function/DoubleFunction.java ---
    @@ -15,13 +15,10 @@
      * limitations under the License.
      */
     
    -package org.apache.spark.api.java.function
    +package org.apache.spark.api.java.function;
     
    -import scala.reflect.ClassTag
    +import java.io.Serializable;
     
    -/**
    - * A function that takes two inputs and returns zero or more output records.
    - */
    -abstract class FlatMapFunction2[A, B, C] extends Function2[A, B, java.lang.Iterable[C]] {
    -  def elementType() : ClassTag[C] = ClassTag.Any.asInstanceOf[ClassTag[C]]
    +public interface DoubleFunction<T> extends Serializable {
    +  public Double call(T t) throws Exception;
    --- End diff --
    
    Would this work if it just returned a primitive `double`?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
mateiz <git@git.apache.org>,"Tue, 25 Feb 2014 00:10:07 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user mateiz commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/539#discussion_r10018267
  
    --- Diff: core/src/main/java/org/apache/spark/api/java/function/DoubleFunction.java ---
    @@ -15,13 +15,10 @@
      * limitations under the License.
      */
     
    -package org.apache.spark.api.java.function
    +package org.apache.spark.api.java.function;
     
    -import scala.reflect.ClassTag
    +import java.io.Serializable;
     
    -/**
    - * A function that takes two inputs and returns zero or more output records.
    - */
    -abstract class FlatMapFunction2[A, B, C] extends Function2[A, B, java.lang.Iterable[C]] {
    -  def elementType() : ClassTag[C] = ClassTag.Any.asInstanceOf[ClassTag[C]]
    +public interface DoubleFunction<T> extends Serializable {
    +  public Double call(T t) throws Exception;
    --- End diff --
    
    Actually I guess that would break our previous API.. maybe it's better to keep it as `Double`.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 25 Feb 2014 00:15:24 +0000 (UTC)",[GitHub] incubator-spark pull request: Patch for SPARK-942,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/180#issuecomment-35959962
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 25 Feb 2014 00:15:24 +0000 (UTC)",[GitHub] incubator-spark pull request: Patch for SPARK-942,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/180#issuecomment-35959961
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 25 Feb 2014 00:16:29 +0000 (UTC)",[GitHub] incubator-spark pull request: Patch for SPARK-942,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/180#issuecomment-35960041
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12833/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 25 Feb 2014 00:16:29 +0000 (UTC)",[GitHub] incubator-spark pull request: Patch for SPARK-942,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/180#issuecomment-35960040
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 25 Feb 2014 00:19:59 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1063 Add .sortBy(f) method on ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/508#issuecomment-35960321
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 25 Feb 2014 00:19:59 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1063 Add .sortBy(f) method on ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/508#issuecomment-35960323
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
ash211 <git@git.apache.org>,"Tue, 25 Feb 2014 00:21:07 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1063 Add .sortBy(f) method on ...,dev@spark.incubator.apache.org,"Github user ash211 commented on the pull request:

    https://github.com/apache/incubator-spark/pull/508#issuecomment-35960401
  
    Hey I was only asked to include the parameters, not to actually use them! :D  A silly oversight


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 25 Feb 2014 00:22:16 +0000 (UTC)",[GitHub] incubator-spark pull request: Patch for SPARK-942,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/180#issuecomment-35960479
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 25 Feb 2014 00:22:16 +0000 (UTC)",[GitHub] incubator-spark pull request: Patch for SPARK-942,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/180#issuecomment-35960478
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
ash211 <git@git.apache.org>,"Tue, 25 Feb 2014 00:22:30 +0000 (UTC)",[GitHub] incubator-spark pull request: Patch for SPARK-942,dev@spark.incubator.apache.org,"Github user ash211 commented on the pull request:

    https://github.com/apache/incubator-spark/pull/180#issuecomment-35960492
  
    The test that failed was a line-too-long error:
    
    error file=/root/workspace/SparkPullRequestBuilder/core/src/main/scala/org/apache/spark/serializer/JavaSerializer.scala message=File line length exceeds 100 characters line=29
    
    Can you wrap that line to under 100 characters?  I think it's the one with the URL


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 25 Feb 2014 00:23:24 +0000 (UTC)",[GitHub] incubator-spark pull request: Patch for SPARK-942,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/180#issuecomment-35960555
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12835/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 25 Feb 2014 00:23:24 +0000 (UTC)",[GitHub] incubator-spark pull request: Patch for SPARK-942,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/180#issuecomment-35960554
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
kellrott <git@git.apache.org>,"Tue, 25 Feb 2014 00:24:47 +0000 (UTC)",[GitHub] incubator-spark pull request: Patch for SPARK-942,dev@spark.incubator.apache.org,"Github user kellrott commented on the pull request:

    https://github.com/apache/incubator-spark/pull/180#issuecomment-35960651
  
    Thanks, I was trying to figure out what the heck that was all about. Must be a new code check they recently added.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
ash211 <git@git.apache.org>,"Tue, 25 Feb 2014 00:26:35 +0000 (UTC)",[GitHub] incubator-spark pull request: Patch for SPARK-942,dev@spark.incubator.apache.org,"Github user ash211 commented on the pull request:

    https://github.com/apache/incubator-spark/pull/180#issuecomment-35960770
  
    The style rules are defined in scalastyle-config.xml but I'm unsure how to
    run that test locally.  Does it not fail when you compile with ""sbt/sbt
    assemble"" ?
    
    
    
    > Thanks, I was trying to figure out what the heck that was all about. Must
    > be a new code check they recently added.
    >
    > â€”
    > Reply to this email directly or view it on GitHub<https://github.com/apache/incubator-spark/pull/180#issuecomment-35960651>
    > .
    >


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
kellrott <git@git.apache.org>,"Tue, 25 Feb 2014 00:30:02 +0000 (UTC)",[GitHub] incubator-spark pull request: Patch for SPARK-942,dev@spark.incubator.apache.org,"Github user kellrott commented on the pull request:

    https://github.com/apache/incubator-spark/pull/180#issuecomment-35961011
  
    sbt/sbt assembly runs fine. But I haven't re-merged master since 0.9 was released (about 20 days...)


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
mateiz <git@git.apache.org>,"Tue, 25 Feb 2014 00:35:38 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user mateiz commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/539#discussion_r10019048
  
    --- Diff: core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala ---
    @@ -699,8 +699,14 @@ object JavaPairRDD {
       }
     
       implicit def toRDD[K, V](rdd: JavaPairRDD[K, V]): RDD[(K, V)] = rdd.rdd
    -
    -
    +  private[spark]
    +  implicit def toJFunction2[T1, T2, R](fun: JFunction2[T1, T2, R])
    +  : Function2[T1, T2, R] = (x: T1, x1: T2) => fun.call(x, x1)
    --- End diff --
    
    This looks pretty weird; add a space between the different implicits, and maybe write them like this:
    ```
    private[spark]
    implicit def toJFunction2[T1, T2, R](fun: JFunction2[T1, T2, R]): Function2[T1, T2, R] = {
      (x: T1, x1: T2) => fun.call(x, x1)
    }
    ```
    
    Note that our line length limit is 100 characters, so this should fit.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
mateiz <git@git.apache.org>,"Tue, 25 Feb 2014 00:36:08 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user mateiz commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/539#discussion_r10019064
  
    --- Diff: core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala ---
    @@ -699,8 +699,14 @@ object JavaPairRDD {
       }
     
       implicit def toRDD[K, V](rdd: JavaPairRDD[K, V]): RDD[(K, V)] = rdd.rdd
    -
    -
    +  private[spark]
    +  implicit def toJFunction2[T1, T2, R](fun: JFunction2[T1, T2, R])
    +  : Function2[T1, T2, R] = (x: T1, x1: T2) => fun.call(x, x1)
    --- End diff --
    
    Also instead of being called toJFunction, these should be called toScalaFunction, since the return type is a Scala function.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
mateiz <git@git.apache.org>,"Tue, 25 Feb 2014 00:37:16 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user mateiz commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/539#discussion_r10019102
  
    --- Diff: core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala ---
    @@ -82,15 +82,16 @@ trait JavaRDDLike[T, This <: JavaRDDLike[T, This]] extends Serializable {
       /**
        * Return a new RDD by applying a function to all elements of this RDD.
        */
    -  def map[R](f: DoubleFunction[T]): JavaDoubleRDD =
    -    new JavaDoubleRDD(rdd.map(x => f(x).doubleValue()))
    +  def mapToDouble[R](f: DoubleFunction[T]): JavaDoubleRDD = {
    +    new JavaDoubleRDD(rdd.map(x => f.call(x).doubleValue()))
    +  }
     
       /**
        * Return a new RDD by applying a function to all elements of this RDD.
        */
    -  def map[K2, V2](f: PairFunction[T, K2, V2]): JavaPairRDD[K2, V2] = {
    -    val ctag = implicitly[ClassTag[Tuple2[K2, V2]]]
    -    new JavaPairRDD(rdd.map(f)(ctag))(f.keyType(), f.valueType())
    +  def mapToPair[K2, V2](f: PairFunction[T, K2, V2]): JavaPairRDD[K2, V2] = {
    +    def cm = implicitly[ClassTag[Tuple2[_, _]]].asInstanceOf[ClassTag[Tuple2[K2, V2]]]
    --- End diff --
    
    Why can't you do `implicitly[ClassTag[Tuple2[K2, V2]]]` as before?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
mateiz <git@git.apache.org>,"Tue, 25 Feb 2014 00:37:40 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user mateiz commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/539#discussion_r10019117
  
    --- Diff: core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala ---
    @@ -117,19 +118,19 @@ trait JavaRDDLike[T, This <: JavaRDDLike[T, This]] extends Serializable {
        *  Return a new RDD by first applying a function to all elements of this
        *  RDD, and then flattening the results.
        */
    -  def flatMap[K2, V2](f: PairFlatMapFunction[T, K2, V2]): JavaPairRDD[K2, V2] = {
    +  def flatMapToPair[K2, V2](f: PairFlatMapFunction[T, K2, V2]): JavaPairRDD[K2, V2] = {
         import scala.collection.JavaConverters._
    -    def fn = (x: T) => f.apply(x).asScala
    -    val ctag = implicitly[ClassTag[Tuple2[K2, V2]]]
    -    JavaPairRDD.fromRDD(rdd.flatMap(fn)(ctag))(f.keyType(), f.valueType())
    +    def fn = (x: T) => f.call(x).asScala
    +    def cm = implicitly[ClassTag[Tuple2[_, _]]].asInstanceOf[ClassTag[Tuple2[K2, V2]]]
    --- End diff --
    
    Ditto here on the ClassTag


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
mateiz <git@git.apache.org>,"Tue, 25 Feb 2014 00:42:13 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user mateiz commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/539#discussion_r10019275
  
    --- Diff: java8-tests/src/test/java/org/apache/spark/Java8APISuite.java ---
    @@ -0,0 +1,377 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one or more
    + * contributor license agreements.  See the NOTICE file distributed with
    + * this work for additional information regarding copyright ownership.
    + * The ASF licenses this file to You under the Apache License, Version 2.0
    + * (the ""License""); you may not use this file except in compliance with
    + * the License.  You may obtain a copy of the License at
    + *
    + *    http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.spark;
    +
    +import com.google.common.base.Optional;
    +import com.google.common.io.Files;
    +import org.apache.hadoop.io.IntWritable;
    +import org.apache.hadoop.io.Text;
    +import org.apache.hadoop.mapred.SequenceFileOutputFormat;
    +import org.apache.spark.api.java.JavaDoubleRDD;
    +import org.apache.spark.api.java.JavaPairRDD;
    +import org.apache.spark.api.java.JavaRDD;
    +import org.apache.spark.api.java.JavaSparkContext;
    +import org.apache.spark.api.java.function.*;
    +import org.junit.After;
    +import org.junit.Assert;
    +import org.junit.Before;
    +import org.junit.Test;
    +import scala.Tuple2;
    +
    +import java.io.File;
    +import java.io.Serializable;
    +import java.util.*;
    +
    +
    +/**
    + * Most of these tests replicate org.apache.spark.JavaAPISuite using java 8
    + * lambda syntax.
    + */
    +public class Java8APISuite implements Serializable {
    +  static int foreachCalls = 0;
    +  private transient JavaSparkContext sc;
    +
    +  @Before
    +  public void setUp() {
    +    sc = new JavaSparkContext(""local"", ""JavaAPISuite"");
    +  }
    +
    +  @After
    +  public void tearDown() {
    +    sc.stop();
    +    sc = null;
    +    // To avoid Akka rebinding to the same port, since it doesn't unbind immediately on shutdown
    +    System.clearProperty(""spark.driver.port"");
    +  }
    +
    +  @Test
    +  public void foreach() {
    +    foreachCalls = 0;
    +    JavaRDD<String> rdd = sc.parallelize(Arrays.asList(""Hello"", ""World""));
    +    rdd.foreach((x) -> foreachCalls++);
    +    Assert.assertEquals(2, foreachCalls);
    +  }
    +
    +  @Test
    +  public void groupBy() {
    +    JavaRDD<Integer> rdd = sc.parallelize(Arrays.asList(1, 1, 2, 3, 5, 8, 13));
    +    Function<Integer, Boolean> isOdd = x -> x % 2 == 0;
    +    JavaPairRDD<Boolean, List<Integer>> oddsAndEvens = rdd.groupBy(isOdd);
    +    Assert.assertEquals(2, oddsAndEvens.count());
    +    Assert.assertEquals(2, oddsAndEvens.lookup(true).get(0).size());  // Evens
    +    Assert.assertEquals(5, oddsAndEvens.lookup(false).get(0).size()); // Odds
    +
    +    oddsAndEvens = rdd.groupBy(isOdd, 1);
    +    Assert.assertEquals(2, oddsAndEvens.count());
    +    Assert.assertEquals(2, oddsAndEvens.lookup(true).get(0).size());  // Evens
    +    Assert.assertEquals(5, oddsAndEvens.lookup(false).get(0).size()); // Odds
    +  }
    +
    +  @Test
    +  public void leftOuterJoin() {
    +    JavaPairRDD<Integer, Integer> rdd1 = sc.parallelizePairs(Arrays.asList(
    +      new Tuple2<Integer, Integer>(1, 1),
    +      new Tuple2<Integer, Integer>(1, 2),
    +      new Tuple2<Integer, Integer>(2, 1),
    +      new Tuple2<Integer, Integer>(3, 1)
    +    ));
    +    JavaPairRDD<Integer, Character> rdd2 = sc.parallelizePairs(Arrays.asList(
    +      new Tuple2<Integer, Character>(1, 'x'),
    +      new Tuple2<Integer, Character>(2, 'y'),
    +      new Tuple2<Integer, Character>(2, 'z'),
    +      new Tuple2<Integer, Character>(4, 'w')
    +    ));
    +    List<Tuple2<Integer, Tuple2<Integer, Optional<Character>>>> joined =
    +      rdd1.leftOuterJoin(rdd2).collect();
    +    Assert.assertEquals(5, joined.size());
    +    Tuple2<Integer, Tuple2<Integer, Optional<Character>>> firstUnmatched =
    +      rdd1.leftOuterJoin(rdd2).filter(tup -> !tup._2()._2().isPresent()).first();
    +    Assert.assertEquals(3, firstUnmatched._1().intValue());
    +  }
    +
    +  @Test
    +  public void foldReduce() {
    +    JavaRDD<Integer> rdd = sc.parallelize(Arrays.asList(1, 1, 2, 3, 5, 8, 13));
    +    Function2<Integer, Integer, Integer> add = (a, b) -> a + b;
    +
    +    int sum = rdd.fold(0, add);
    +    Assert.assertEquals(33, sum);
    +
    +    sum = rdd.reduce(add);
    +    Assert.assertEquals(33, sum);
    +  }
    +
    +  @Test
    +  public void foldByKey() {
    +    List<Tuple2<Integer, Integer>> pairs = Arrays.asList(
    +      new Tuple2<Integer, Integer>(2, 1),
    +      new Tuple2<Integer, Integer>(2, 1),
    +      new Tuple2<Integer, Integer>(1, 1),
    +      new Tuple2<Integer, Integer>(3, 2),
    +      new Tuple2<Integer, Integer>(3, 1)
    +    );
    +    JavaPairRDD<Integer, Integer> rdd = sc.parallelizePairs(pairs);
    +    JavaPairRDD<Integer, Integer> sums = rdd.foldByKey(0, (a, b) -> a + b);
    +    Assert.assertEquals(1, sums.lookup(1).get(0).intValue());
    +    Assert.assertEquals(2, sums.lookup(2).get(0).intValue());
    +    Assert.assertEquals(3, sums.lookup(3).get(0).intValue());
    +  }
    +
    +  @Test
    +  public void reduceByKey() {
    +    List<Tuple2<Integer, Integer>> pairs = Arrays.asList(
    +      new Tuple2<Integer, Integer>(2, 1),
    +      new Tuple2<Integer, Integer>(2, 1),
    +      new Tuple2<Integer, Integer>(1, 1),
    +      new Tuple2<Integer, Integer>(3, 2),
    +      new Tuple2<Integer, Integer>(3, 1)
    +    );
    +    JavaPairRDD<Integer, Integer> rdd = sc.parallelizePairs(pairs);
    +    JavaPairRDD<Integer, Integer> counts = rdd.reduceByKey((a, b) -> a + b);
    +    Assert.assertEquals(1, counts.lookup(1).get(0).intValue());
    +    Assert.assertEquals(2, counts.lookup(2).get(0).intValue());
    +    Assert.assertEquals(3, counts.lookup(3).get(0).intValue());
    +
    +    Map<Integer, Integer> localCounts = counts.collectAsMap();
    +    Assert.assertEquals(1, localCounts.get(1).intValue());
    +    Assert.assertEquals(2, localCounts.get(2).intValue());
    +    Assert.assertEquals(3, localCounts.get(3).intValue());
    +
    +    localCounts = rdd.reduceByKeyLocally((a, b) -> a + b);
    +    Assert.assertEquals(1, localCounts.get(1).intValue());
    +    Assert.assertEquals(2, localCounts.get(2).intValue());
    +    Assert.assertEquals(3, localCounts.get(3).intValue());
    +  }
    +
    +  @Test
    +  public void map() {
    +    JavaRDD<Integer> rdd = sc.parallelize(Arrays.asList(1, 2, 3, 4, 5));
    +    JavaDoubleRDD doubles = rdd.mapToDouble(x -> 1.0 * x).cache();
    +    doubles.collect();
    +    JavaPairRDD<Integer, Integer> pairs = rdd.mapToPair(x -> new Tuple2<Integer, Integer>(x, x))
    +      .cache();
    +    pairs.collect();
    +    JavaRDD<String> strings = rdd.map(x -> x.toString()).cache();
    +    strings.collect();
    +  }
    +
    +  @Test
    +  public void flatMap() {
    +    JavaRDD<String> rdd = sc.parallelize(Arrays.asList(""Hello World!"",
    +      ""The quick brown fox jumps over the lazy dog.""));
    +    JavaRDD<String> words = rdd.flatMap(x -> Arrays.asList(x.split("" "")));
    +
    +    Assert.assertEquals(""Hello"", words.first());
    +    Assert.assertEquals(11, words.count());
    +
    +    JavaPairRDD<String, String> pairs = rdd.flatMapToPair(s -> {
    +      List<Tuple2<String, String>> pairs2 = new LinkedList<Tuple2<String, String>>();
    +      for (String word : s.split("" "")) pairs2.add(new Tuple2<String, String>(word, word));
    +      return pairs2;
    +    });
    +
    +    Assert.assertEquals(new Tuple2<String, String>(""Hello"", ""Hello""), pairs.first());
    +    Assert.assertEquals(11, pairs.count());
    +
    +    JavaDoubleRDD doubles = rdd.flatMapToDouble(s -> {
    +      List<Double> lengths = new LinkedList<Double>();
    +      for (String word : s.split("" "")) lengths.add(word.length() * 1.0);
    +      return lengths;
    +    });
    +
    +    Double x = doubles.first();
    +    Assert.assertEquals(5.0, doubles.first().doubleValue(), 0.01);
    +    Assert.assertEquals(11, pairs.count());
    +  }
    +
    +  @Test
    +  public void mapsFromPairsToPairs() {
    +    List<Tuple2<Integer, String>> pairs = Arrays.asList(
    +      new Tuple2<Integer, String>(1, ""a""),
    +      new Tuple2<Integer, String>(2, ""aa""),
    +      new Tuple2<Integer, String>(3, ""aaa"")
    +    );
    +    JavaPairRDD<Integer, String> pairRDD = sc.parallelizePairs(pairs);
    +
    +    // Regression test for SPARK-668:
    +    JavaPairRDD<String, Integer> swapped =
    +      pairRDD.flatMapToPair(x -> Collections.singletonList(x.swap()));
    +    swapped.collect();
    +
    +    // There was never a bug here, but it's worth testing:
    +    pairRDD.map(item -> item.swap()).collect();
    +  }
    +
    +  @Test
    +  public void mapPartitions() {
    +    JavaRDD<Integer> rdd = sc.parallelize(Arrays.asList(1, 2, 3, 4), 2);
    +    JavaRDD<Integer> partitionSums = rdd.mapPartitions(iter -> {
    +        int sum = 0;
    +        while (iter.hasNext()) {
    +          sum += iter.next();
    +        }
    +        return Collections.singletonList(sum);
    +      });
    --- End diff --
    
    This probably needs to be shifted back 2 spaces similar to how we'd write this in Scala


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
mateiz <git@git.apache.org>,"Tue, 25 Feb 2014 00:42:53 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user mateiz commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/539#discussion_r10019296
  
    --- Diff: java8-tests/src/test/java/org/apache/spark/Java8APISuite.java ---
    @@ -0,0 +1,377 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one or more
    + * contributor license agreements.  See the NOTICE file distributed with
    + * this work for additional information regarding copyright ownership.
    + * The ASF licenses this file to You under the Apache License, Version 2.0
    + * (the ""License""); you may not use this file except in compliance with
    + * the License.  You may obtain a copy of the License at
    + *
    + *    http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.spark;
    +
    +import com.google.common.base.Optional;
    +import com.google.common.io.Files;
    +import org.apache.hadoop.io.IntWritable;
    +import org.apache.hadoop.io.Text;
    +import org.apache.hadoop.mapred.SequenceFileOutputFormat;
    +import org.apache.spark.api.java.JavaDoubleRDD;
    +import org.apache.spark.api.java.JavaPairRDD;
    +import org.apache.spark.api.java.JavaRDD;
    +import org.apache.spark.api.java.JavaSparkContext;
    +import org.apache.spark.api.java.function.*;
    +import org.junit.After;
    +import org.junit.Assert;
    +import org.junit.Before;
    +import org.junit.Test;
    +import scala.Tuple2;
    +
    +import java.io.File;
    +import java.io.Serializable;
    +import java.util.*;
    +
    +
    +/**
    + * Most of these tests replicate org.apache.spark.JavaAPISuite using java 8
    + * lambda syntax.
    + */
    +public class Java8APISuite implements Serializable {
    +  static int foreachCalls = 0;
    +  private transient JavaSparkContext sc;
    +
    +  @Before
    +  public void setUp() {
    +    sc = new JavaSparkContext(""local"", ""JavaAPISuite"");
    +  }
    +
    +  @After
    +  public void tearDown() {
    +    sc.stop();
    +    sc = null;
    +    // To avoid Akka rebinding to the same port, since it doesn't unbind immediately on shutdown
    +    System.clearProperty(""spark.driver.port"");
    +  }
    +
    +  @Test
    +  public void foreach() {
    +    foreachCalls = 0;
    +    JavaRDD<String> rdd = sc.parallelize(Arrays.asList(""Hello"", ""World""));
    +    rdd.foreach((x) -> foreachCalls++);
    +    Assert.assertEquals(2, foreachCalls);
    +  }
    +
    +  @Test
    +  public void groupBy() {
    +    JavaRDD<Integer> rdd = sc.parallelize(Arrays.asList(1, 1, 2, 3, 5, 8, 13));
    +    Function<Integer, Boolean> isOdd = x -> x % 2 == 0;
    +    JavaPairRDD<Boolean, List<Integer>> oddsAndEvens = rdd.groupBy(isOdd);
    +    Assert.assertEquals(2, oddsAndEvens.count());
    +    Assert.assertEquals(2, oddsAndEvens.lookup(true).get(0).size());  // Evens
    +    Assert.assertEquals(5, oddsAndEvens.lookup(false).get(0).size()); // Odds
    +
    +    oddsAndEvens = rdd.groupBy(isOdd, 1);
    +    Assert.assertEquals(2, oddsAndEvens.count());
    +    Assert.assertEquals(2, oddsAndEvens.lookup(true).get(0).size());  // Evens
    +    Assert.assertEquals(5, oddsAndEvens.lookup(false).get(0).size()); // Odds
    +  }
    +
    +  @Test
    +  public void leftOuterJoin() {
    +    JavaPairRDD<Integer, Integer> rdd1 = sc.parallelizePairs(Arrays.asList(
    +      new Tuple2<Integer, Integer>(1, 1),
    +      new Tuple2<Integer, Integer>(1, 2),
    +      new Tuple2<Integer, Integer>(2, 1),
    +      new Tuple2<Integer, Integer>(3, 1)
    +    ));
    +    JavaPairRDD<Integer, Character> rdd2 = sc.parallelizePairs(Arrays.asList(
    +      new Tuple2<Integer, Character>(1, 'x'),
    +      new Tuple2<Integer, Character>(2, 'y'),
    +      new Tuple2<Integer, Character>(2, 'z'),
    +      new Tuple2<Integer, Character>(4, 'w')
    +    ));
    +    List<Tuple2<Integer, Tuple2<Integer, Optional<Character>>>> joined =
    +      rdd1.leftOuterJoin(rdd2).collect();
    +    Assert.assertEquals(5, joined.size());
    +    Tuple2<Integer, Tuple2<Integer, Optional<Character>>> firstUnmatched =
    +      rdd1.leftOuterJoin(rdd2).filter(tup -> !tup._2()._2().isPresent()).first();
    +    Assert.assertEquals(3, firstUnmatched._1().intValue());
    +  }
    +
    +  @Test
    +  public void foldReduce() {
    +    JavaRDD<Integer> rdd = sc.parallelize(Arrays.asList(1, 1, 2, 3, 5, 8, 13));
    +    Function2<Integer, Integer, Integer> add = (a, b) -> a + b;
    +
    +    int sum = rdd.fold(0, add);
    +    Assert.assertEquals(33, sum);
    +
    +    sum = rdd.reduce(add);
    +    Assert.assertEquals(33, sum);
    +  }
    +
    +  @Test
    +  public void foldByKey() {
    +    List<Tuple2<Integer, Integer>> pairs = Arrays.asList(
    +      new Tuple2<Integer, Integer>(2, 1),
    +      new Tuple2<Integer, Integer>(2, 1),
    +      new Tuple2<Integer, Integer>(1, 1),
    +      new Tuple2<Integer, Integer>(3, 2),
    +      new Tuple2<Integer, Integer>(3, 1)
    +    );
    +    JavaPairRDD<Integer, Integer> rdd = sc.parallelizePairs(pairs);
    +    JavaPairRDD<Integer, Integer> sums = rdd.foldByKey(0, (a, b) -> a + b);
    +    Assert.assertEquals(1, sums.lookup(1).get(0).intValue());
    +    Assert.assertEquals(2, sums.lookup(2).get(0).intValue());
    +    Assert.assertEquals(3, sums.lookup(3).get(0).intValue());
    +  }
    +
    +  @Test
    +  public void reduceByKey() {
    +    List<Tuple2<Integer, Integer>> pairs = Arrays.asList(
    +      new Tuple2<Integer, Integer>(2, 1),
    +      new Tuple2<Integer, Integer>(2, 1),
    +      new Tuple2<Integer, Integer>(1, 1),
    +      new Tuple2<Integer, Integer>(3, 2),
    +      new Tuple2<Integer, Integer>(3, 1)
    +    );
    +    JavaPairRDD<Integer, Integer> rdd = sc.parallelizePairs(pairs);
    +    JavaPairRDD<Integer, Integer> counts = rdd.reduceByKey((a, b) -> a + b);
    +    Assert.assertEquals(1, counts.lookup(1).get(0).intValue());
    +    Assert.assertEquals(2, counts.lookup(2).get(0).intValue());
    +    Assert.assertEquals(3, counts.lookup(3).get(0).intValue());
    +
    +    Map<Integer, Integer> localCounts = counts.collectAsMap();
    +    Assert.assertEquals(1, localCounts.get(1).intValue());
    +    Assert.assertEquals(2, localCounts.get(2).intValue());
    +    Assert.assertEquals(3, localCounts.get(3).intValue());
    +
    +    localCounts = rdd.reduceByKeyLocally((a, b) -> a + b);
    +    Assert.assertEquals(1, localCounts.get(1).intValue());
    +    Assert.assertEquals(2, localCounts.get(2).intValue());
    +    Assert.assertEquals(3, localCounts.get(3).intValue());
    +  }
    +
    +  @Test
    +  public void map() {
    +    JavaRDD<Integer> rdd = sc.parallelize(Arrays.asList(1, 2, 3, 4, 5));
    +    JavaDoubleRDD doubles = rdd.mapToDouble(x -> 1.0 * x).cache();
    +    doubles.collect();
    +    JavaPairRDD<Integer, Integer> pairs = rdd.mapToPair(x -> new Tuple2<Integer, Integer>(x, x))
    +      .cache();
    +    pairs.collect();
    +    JavaRDD<String> strings = rdd.map(x -> x.toString()).cache();
    +    strings.collect();
    +  }
    +
    +  @Test
    +  public void flatMap() {
    +    JavaRDD<String> rdd = sc.parallelize(Arrays.asList(""Hello World!"",
    +      ""The quick brown fox jumps over the lazy dog.""));
    +    JavaRDD<String> words = rdd.flatMap(x -> Arrays.asList(x.split("" "")));
    +
    +    Assert.assertEquals(""Hello"", words.first());
    +    Assert.assertEquals(11, words.count());
    +
    +    JavaPairRDD<String, String> pairs = rdd.flatMapToPair(s -> {
    +      List<Tuple2<String, String>> pairs2 = new LinkedList<Tuple2<String, String>>();
    +      for (String word : s.split("" "")) pairs2.add(new Tuple2<String, String>(word, word));
    +      return pairs2;
    +    });
    +
    +    Assert.assertEquals(new Tuple2<String, String>(""Hello"", ""Hello""), pairs.first());
    +    Assert.assertEquals(11, pairs.count());
    +
    +    JavaDoubleRDD doubles = rdd.flatMapToDouble(s -> {
    +      List<Double> lengths = new LinkedList<Double>();
    +      for (String word : s.split("" "")) lengths.add(word.length() * 1.0);
    +      return lengths;
    +    });
    +
    +    Double x = doubles.first();
    +    Assert.assertEquals(5.0, doubles.first().doubleValue(), 0.01);
    +    Assert.assertEquals(11, pairs.count());
    +  }
    +
    +  @Test
    +  public void mapsFromPairsToPairs() {
    +    List<Tuple2<Integer, String>> pairs = Arrays.asList(
    +      new Tuple2<Integer, String>(1, ""a""),
    +      new Tuple2<Integer, String>(2, ""aa""),
    +      new Tuple2<Integer, String>(3, ""aaa"")
    +    );
    +    JavaPairRDD<Integer, String> pairRDD = sc.parallelizePairs(pairs);
    +
    +    // Regression test for SPARK-668:
    +    JavaPairRDD<String, Integer> swapped =
    +      pairRDD.flatMapToPair(x -> Collections.singletonList(x.swap()));
    +    swapped.collect();
    +
    +    // There was never a bug here, but it's worth testing:
    +    pairRDD.map(item -> item.swap()).collect();
    +  }
    +
    +  @Test
    +  public void mapPartitions() {
    +    JavaRDD<Integer> rdd = sc.parallelize(Arrays.asList(1, 2, 3, 4), 2);
    +    JavaRDD<Integer> partitionSums = rdd.mapPartitions(iter -> {
    +        int sum = 0;
    +        while (iter.hasNext()) {
    +          sum += iter.next();
    +        }
    +        return Collections.singletonList(sum);
    +      });
    +
    +    Assert.assertEquals(""[3, 7]"", partitionSums.collect().toString());
    +  }
    +
    +  @Test
    +  public void sequenceFile() {
    +    File tempDir = Files.createTempDir();
    +    String outputDir = new File(tempDir, ""output"").getAbsolutePath();
    +    List<Tuple2<Integer, String>> pairs = Arrays.asList(
    +      new Tuple2<Integer, String>(1, ""a""),
    +      new Tuple2<Integer, String>(2, ""aa""),
    +      new Tuple2<Integer, String>(3, ""aaa"")
    +    );
    +    JavaPairRDD<Integer, String> rdd = sc.parallelizePairs(pairs);
    +
    +    rdd.mapToPair(
    +      pair -> new Tuple2<IntWritable, Text>(new IntWritable(pair._1()), new Text(pair._2())))
    +      .saveAsHadoopFile(outputDir, IntWritable.class, Text.class, SequenceFileOutputFormat.class);
    --- End diff --
    
    Probably want to put the `pair ->` on the first line and double-indent the body of the closure


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
mateiz <git@git.apache.org>,"Tue, 25 Feb 2014 00:45:18 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user mateiz commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/539#discussion_r10019362
  
    --- Diff: project/SparkBuild.scala ---
    @@ -88,6 +88,11 @@ object SparkBuild extends Build {
       }
       lazy val hadoopClient = if (hadoopVersion.startsWith(""0.20."") || hadoopVersion == ""1.0.0"") ""hadoop-core"" else ""hadoop-client""
     
    +  lazy val javaVersion = System.getProperty(""java.specification.version"")
    +  lazy val isJava8Enabled = if (javaVersion == ""1.8"") true else false
    --- End diff --
    
    You can simplify this to `lazy val isJava8Enabled = (javaVersion >= ""1.8"")`
    
    Note that it's better to use `>=` because 1.9 will sometime come out.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
mateiz <git@git.apache.org>,"Tue, 25 Feb 2014 00:45:53 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user mateiz commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/539#discussion_r10019378
  
    --- Diff: project/SparkBuild.scala ---
    @@ -88,6 +88,11 @@ object SparkBuild extends Build {
       }
       lazy val hadoopClient = if (hadoopVersion.startsWith(""0.20."") || hadoopVersion == ""1.0.0"") ""hadoop-core"" else ""hadoop-client""
     
    +  lazy val javaVersion = System.getProperty(""java.specification.version"")
    +  lazy val isJava8Enabled = if (javaVersion == ""1.8"") true else false
    +  // Conditionally include the yarn sub-project
    +  val maybeJava8Tests = if (isJava8Enabled) Seq[ProjectReference](java8Tests) else Seq[ProjectReference]()
    --- End diff --
    
    The comment on this is wrong (it says ""yarn""); also add a space below this


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
mateiz <git@git.apache.org>,"Tue, 25 Feb 2014 00:46:45 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user mateiz commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/539#discussion_r10019414
  
    --- Diff: project/SparkBuild.scala ---
    @@ -116,10 +121,10 @@ object SparkBuild extends Build {
       lazy val examples = Project(""examples"", file(""examples""), settings = examplesSettings)
     
    -  // Everything except assembly, tools and examples belong to packageProjects
    +  // Everything except assembly, tools, java8Tests and examples belong to packageProjects
       lazy val packageProjects = Seq[ProjectReference](core, repl, bagel, streaming, mllib, graphx) ++ maybeYarnRef
     
    -  lazy val allProjects = packageProjects ++ allExternalRefs ++ Seq[ProjectReference](examples, tools, assemblyProj)
    +  lazy val allProjects = packageProjects ++ allExternalRefs ++ Seq[ProjectReference](examples, tools, assemblyProj) ++ maybeJava8Tests
    --- End diff --
    
    This line is kind of long now, probably want to split it


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
mateiz <git@git.apache.org>,"Tue, 25 Feb 2014 00:48:51 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user mateiz commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/539#discussion_r10019494
  
    --- Diff: streaming/src/test/java/org/apache/spark/streaming/JavaAPISuite.java ---
    @@ -718,7 +717,7 @@ public void testPairFlatMap() {
                 new Tuple2<Integer, String>(9, ""s"")));
     
         JavaDStream<String> stream = JavaTestUtils.attachTestInputStream(ssc, inputData, 1);
    -    JavaPairDStream<Integer,String> flatMapped = stream.flatMap(new PairFlatMapFunction<String, Integer, String>() {
    +    JavaPairDStream<Integer,String> flatMapped = stream.flatMapToPair(new PairFlatMapFunction<String, Integer, String>() {
    --- End diff --
    
    This line now looks too long


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
mateiz <git@git.apache.org>,"Tue, 25 Feb 2014 00:50:28 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user mateiz commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/539#discussion_r10019552
  
    --- Diff: core/src/main/java/org/apache/spark/api/java/function/PairFlatMapFunction.java ---
    @@ -0,0 +1,26 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one or more
    + * contributor license agreements.  See the NOTICE file distributed with
    + * this work for additional information regarding copyright ownership.
    + * The ASF licenses this file to You under the Apache License, Version 2.0
    + * (the ""License""); you may not use this file except in compliance with
    + * the License.  You may obtain a copy of the License at
    + *
    + *    http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.spark.api.java.function;
    +
    +import scala.Tuple2;
    +
    +import java.io.Serializable;
    +
    +public interface PairFlatMapFunction<T, K, V> extends Serializable {
    +    public Iterable<Tuple2<K, V>> call(T t) throws Exception;
    --- End diff --
    
    Use only 2 spaces for indent here


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
mateiz <git@git.apache.org>,"Tue, 25 Feb 2014 00:51:20 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user mateiz commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35962439
  
    Hey Prashant, I did another pass through this and it looks quite good, but I have a few comments:
    
    * Is there a way to add the Java 8 test projects to the Maven build? I'm not 100% sure it's possible but please look into it.
    * Add doc comments above the Function classes. Note that the previous Scala classes for these had comments already (e.g. https://github.com/apache/incubator-spark/pull/539/files#diff-16).


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Tue, 25 Feb 2014 00:53:23 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/539#discussion_r10019661
  
    --- Diff: core/src/main/java/org/apache/spark/api/java/function/DoubleFunction.java ---
    @@ -15,13 +15,10 @@
      * limitations under the License.
      */
     
    -package org.apache.spark.api.java.function
    +package org.apache.spark.api.java.function;
     
    -import scala.reflect.ClassTag
    +import java.io.Serializable;
     
    -/**
    - * A function that takes two inputs and returns zero or more output records.
    - */
    -abstract class FlatMapFunction2[A, B, C] extends Function2[A, B, java.lang.Iterable[C]] {
    -  def elementType() : ClassTag[C] = ClassTag.Any.asInstanceOf[ClassTag[C]]
    +public interface DoubleFunction<T> extends Serializable {
    +  public Double call(T t) throws Exception;
    --- End diff --
    
    I'd actually be inclined to go with `double` here as we are already changing some API's around.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
mateiz <git@git.apache.org>,"Tue, 25 Feb 2014 00:55:52 +0000 (UTC)",[GitHub] incubator-spark pull request: [java8API] SPARK-964 Investigate the...,dev@spark.incubator.apache.org,"Github user mateiz commented on the pull request:

    https://github.com/apache/incubator-spark/pull/539#issuecomment-35962751
  
    Also one other thing, can you look through the Java programming guide and the Quick Start guide in the `docs/` folder and make sure they have valid Java code now that we changed some methods? You should also note that Java has different versions of `map` and `flatMap` for different return types in the Java programming guide (it has a section on differences from Scala).


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
mateiz <git@git.apache.org>,"Tue, 25 Feb 2014 01:03:34 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1124: Fix infinite retries of ...,dev@spark.incubator.apache.org,"Github user mateiz commented on the pull request:

    https://github.com/apache/incubator-spark/pull/641#issuecomment-35963256
  
    Merged into master and branch-0.9.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
aarondav <git@git.apache.org>,"Tue, 25 Feb 2014 01:07:45 +0000 (UTC)",[GitHub] incubator-spark pull request: Patch for SPARK-942,dev@spark.incubator.apache.org,"Github user aarondav commented on the pull request:

    https://github.com/apache/incubator-spark/pull/180#issuecomment-35963553
  
    (`sbt/sbt scalastyle` runs its namesake, by the way)


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 25 Feb 2014 01:13:57 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1063 Add .sortBy(f) method on ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/508#issuecomment-35963953
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12834/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 25 Feb 2014 01:13:57 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1063 Add .sortBy(f) method on ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/508#issuecomment-35963952
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 25 Feb 2014 01:14:13 +0000 (UTC)",[GitHub] incubator-spark pull request: Patch for SPARK-942,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/180#issuecomment-35963965
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 25 Feb 2014 01:14:13 +0000 (UTC)",[GitHub] incubator-spark pull request: Patch for SPARK-942,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/180#issuecomment-35963966
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
JoshRosen <git@git.apache.org>,"Tue, 25 Feb 2014 01:21:37 +0000 (UTC)",[GitHub] incubator-spark pull request: Catch depickling errors,dev@spark.incubator.apache.org,"Github user JoshRosen commented on the pull request:

    https://github.com/apache/incubator-spark/pull/644#issuecomment-35964416
  
    Jenkins, add to whitelist and test this please.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 25 Feb 2014 01:22:31 +0000 (UTC)",[GitHub] incubator-spark pull request: Catch depickling errors,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/644#issuecomment-35964485
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 25 Feb 2014 01:22:31 +0000 (UTC)",[GitHub] incubator-spark pull request: Catch depickling errors,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/644#issuecomment-35964484
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 25 Feb 2014 01:24:07 +0000 (UTC)",[GitHub] incubator-spark pull request: Patch for SPARK-942,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/180#issuecomment-35964570
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 25 Feb 2014 01:24:08 +0000 (UTC)",[GitHub] incubator-spark pull request: Patch for SPARK-942,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/180#issuecomment-35964569
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
JoshRosen <git@git.apache.org>,"Tue, 25 Feb 2014 01:26:08 +0000 (UTC)",[GitHub] incubator-spark pull request: Catch depickling errors,dev@spark.incubator.apache.org,"Github user JoshRosen commented on the pull request:

    https://github.com/apache/incubator-spark/pull/644#issuecomment-35964696
  
    Per our new PR policies, this needs to reference a JIRA issue.
    
    [SPARK-1115](https://spark-project.atlassian.net/browse/SPARK-1115), reported by (@sryza) might actually be fixed by this, so maybe we can use that issue.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Tue, 25 Feb 2014 01:26:29 +0000 (UTC)",[GitHub] incubator-spark pull request: Patch for SPARK-942,dev@spark.incubator.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/180#discussion_r10020702
  
    --- Diff: core/src/main/scala/org/apache/spark/serializer/JavaSerializer.scala ---
    @@ -25,7 +25,22 @@ import org.apache.spark.SparkConf
     
     private[spark] class JavaSerializationStream(out: OutputStream) extends SerializationStream {
       val objOut = new ObjectOutputStream(out)
    -  def writeObject[T](t: T): SerializationStream = { objOut.writeObject(t); this }
    +  var counter = 0;
    +  /* Calling reset to avoid memory leak:
    +   * http://stackoverflow.com/questions/1281549/memory-leak-traps-in-the-java-standard-api
    +   * But only call it every 1000th time to avoid bloated serialization streams (when
    +   * the stream 'resets' object class descriptions have to be re-written)  
    +   */
    +  def writeObject[T](t: T): SerializationStream = {
    +    objOut.writeObject(t);
    +    if (counter >= 1000) {
    --- End diff --
    
    It's funny we had the same problem earlier in Spark and I looked for a long time and was convinced this reset() thing didn't work this way. We ended up solving it another way. But after reading the OOS source code (again) it looks like this is what we want. Good catch.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Tue, 25 Feb 2014 01:26:35 +0000 (UTC)",[GitHub] incubator-spark pull request: Patch for SPARK-942,dev@spark.incubator.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/180#discussion_r10020704
  
    --- Diff: core/src/main/scala/org/apache/spark/serializer/JavaSerializer.scala ---
    @@ -25,7 +25,22 @@ import org.apache.spark.SparkConf
     
     private[spark] class JavaSerializationStream(out: OutputStream) extends SerializationStream {
       val objOut = new ObjectOutputStream(out)
    -  def writeObject[T](t: T): SerializationStream = { objOut.writeObject(t); this }
    +  var counter = 0;
    +  /* Calling reset to avoid memory leak:
    +   * http://stackoverflow.com/questions/1281549/memory-leak-traps-in-the-java-standard-api
    +   * But only call it every 1000th time to avoid bloated serialization streams (when
    +   * the stream 'resets' object class descriptions have to be re-written)  
    +   */
    +  def writeObject[T](t: T): SerializationStream = {
    +    objOut.writeObject(t);
    +    if (counter >= 1000) {
    +      objOut.reset();
    +      counter = 0;
    +    } else {
    +      counter+=1;
    --- End diff --
    
    `counter += 1`


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Tue, 25 Feb 2014 01:26:54 +0000 (UTC)",[GitHub] incubator-spark pull request: Patch for SPARK-942,dev@spark.incubator.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/180#discussion_r10020710
  
    --- Diff: core/src/main/scala/org/apache/spark/serializer/JavaSerializer.scala ---
    @@ -25,7 +25,22 @@ import org.apache.spark.SparkConf
     
     private[spark] class JavaSerializationStream(out: OutputStream) extends SerializationStream {
       val objOut = new ObjectOutputStream(out)
    -  def writeObject[T](t: T): SerializationStream = { objOut.writeObject(t); this }
    +  var counter = 0;
    +  /* Calling reset to avoid memory leak:
    +   * http://stackoverflow.com/questions/1281549/memory-leak-traps-in-the-java-standard-api
    +   * But only call it every 1000th time to avoid bloated serialization streams (when
    +   * the stream 'resets' object class descriptions have to be re-written)  
    +   */
    +  def writeObject[T](t: T): SerializationStream = {
    +    objOut.writeObject(t);
    --- End diff --
    
    could you remove this semi-colon and all the other ones in this PR?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
JoshRosen <git@git.apache.org>,"Tue, 25 Feb 2014 01:28:11 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-972] Added detailed callsite ...,dev@spark.incubator.apache.org,"Github user JoshRosen commented on the pull request:

    https://github.com/apache/incubator-spark/pull/643#issuecomment-35964807
  
    Jenkins, add to whitelist and test this please.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Tue, 25 Feb 2014 01:32:21 +0000 (UTC)",[GitHub] incubator-spark pull request: Patch for SPARK-942,dev@spark.incubator.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/180#discussion_r10020880
  
    --- Diff: core/src/test/scala/org/apache/spark/storage/LargeIteratorSuite.scala ---
    @@ -0,0 +1,61 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one or more
    + * contributor license agreements.  See the NOTICE file distributed with
    + * this work for additional information regarding copyright ownership.
    + * The ASF licenses this file to You under the Apache License, Version 2.0
    + * (the ""License""); you may not use this file except in compliance with
    + * the License.  You may obtain a copy of the License at
    + *
    + *    http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.spark.storage
    +
    +import org.scalatest.FunSuite
    +import org.apache.spark.{LocalSparkContext, SparkContext}
    +import org.apache.commons.io.FileUtils
    +import java.io.File
    +
    +class Expander(base:String, count:Int) extends Iterator[String] {
    --- End diff --
    
    Hey just wondering about this class... is this necessary? It might be possible to write more concise tests:
    
    ```
    sc.makeRDD(1 to 10000).flatMap(1 to 1000).saveAsTextFile 
    ```


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
bouk <git@git.apache.org>,"Tue, 25 Feb 2014 01:32:57 +0000 (UTC)",[GitHub] incubator-spark pull request: Catch depickling errors,dev@spark.incubator.apache.org,"Github user bouk commented on the pull request:

    https://github.com/apache/incubator-spark/pull/644#issuecomment-35965102
  
    Yeah that's probably fixed by this, just use that as reference
    
    > Per our new PR policies, this needs to reference a JIRA issue.
    >
    > SPARK-1115 <https://spark-project.atlassian.net/browse/SPARK-1115>,
    > reported by (@sryza <https://github.com/sryza>) might actually be fixed
    > by this, so maybe we can use that issue.
    >
    > â€”
    > Reply to this email directly or view it on GitHub<https://github.com/apache/incubator-spark/pull/644#issuecomment-35964696>
    > .
    >


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Tue, 25 Feb 2014 01:33:22 +0000 (UTC)",[GitHub] incubator-spark pull request: Patch for SPARK-942,dev@spark.incubator.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/180#discussion_r10020917
  
    --- Diff: core/src/test/scala/org/apache/spark/storage/LargeIteratorSuite.scala ---
    @@ -0,0 +1,61 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one or more
    + * contributor license agreements.  See the NOTICE file distributed with
    + * this work for additional information regarding copyright ownership.
    + * The ASF licenses this file to You under the Apache License, Version 2.0
    + * (the ""License""); you may not use this file except in compliance with
    + * the License.  You may obtain a copy of the License at
    + *
    + *    http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.spark.storage
    +
    +import org.scalatest.FunSuite
    +import org.apache.spark.{LocalSparkContext, SparkContext}
    +import org.apache.commons.io.FileUtils
    +import java.io.File
    +
    +class Expander(base:String, count:Int) extends Iterator[String] {
    +  var i = 0;
    +  def next() : String = {
    +    i += 1;
    +    return base + i.toString;
    +  }
    +  def hasNext() : Boolean = i < count;
    +}
    +
    +object Expander {
    +  def expand(s:String, i:Int) : Iterator[String] = {
    +    return new Expander(s,i)
    +  }
    +}
    +
    +class LargeIteratorSuite extends FunSuite with LocalSparkContext {
    +  /* Tests the ability of Spark to deal with user provided iterators that
    +   * generate more data then available memory. In any memory based persistance
    +   * Spark will unroll the iterator into an ArrayBuffer for caching, however in
    +   * the case that the use defines DISK_ONLY persistance, the iterator will be 
    +   * fed directly to the serializer and written to disk.
    +   */
    +  val clusterUrl = ""local-cluster[1,1,512]""
    +  test(""Flatmap iterator"") {
    +    sc = new SparkContext(clusterUrl, ""mem_test"");
    +    val seeds = sc.parallelize( Array(
    +      ""This is the first sentence that we will test:"",
    +      ""This is the second sentence that we will test:"",
    +      ""This is the third sentence that we will test:""
    +    ) );
    +    val expand_size = 10000000;
    --- End diff --
    
    It seems like this test writes a very large amount of data to disk. Spark has hundreds of tests and each one can only take a short amount of time. So it would be great if you could write a test that just writes a small amount of data to disk.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Tue, 25 Feb 2014 01:33:47 +0000 (UTC)",[GitHub] incubator-spark pull request: Patch for SPARK-942,dev@spark.incubator.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/180#discussion_r10020928
  
    --- Diff: core/src/test/scala/org/apache/spark/storage/LargeIteratorSuite.scala ---
    @@ -0,0 +1,61 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one or more
    + * contributor license agreements.  See the NOTICE file distributed with
    + * this work for additional information regarding copyright ownership.
    + * The ASF licenses this file to You under the Apache License, Version 2.0
    + * (the ""License""); you may not use this file except in compliance with
    + * the License.  You may obtain a copy of the License at
    + *
    + *    http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.spark.storage
    +
    +import org.scalatest.FunSuite
    +import org.apache.spark.{LocalSparkContext, SparkContext}
    +import org.apache.commons.io.FileUtils
    +import java.io.File
    +
    +class Expander(base:String, count:Int) extends Iterator[String] {
    +  var i = 0;
    +  def next() : String = {
    +    i += 1;
    +    return base + i.toString;
    +  }
    +  def hasNext() : Boolean = i < count;
    +}
    +
    +object Expander {
    +  def expand(s:String, i:Int) : Iterator[String] = {
    +    return new Expander(s,i)
    +  }
    +}
    +
    +class LargeIteratorSuite extends FunSuite with LocalSparkContext {
    +  /* Tests the ability of Spark to deal with user provided iterators that
    +   * generate more data then available memory. In any memory based persistance
    +   * Spark will unroll the iterator into an ArrayBuffer for caching, however in
    +   * the case that the use defines DISK_ONLY persistance, the iterator will be 
    +   * fed directly to the serializer and written to disk.
    +   */
    +  val clusterUrl = ""local-cluster[1,1,512]""
    +  test(""Flatmap iterator"") {
    +    sc = new SparkContext(clusterUrl, ""mem_test"");
    +    val seeds = sc.parallelize( Array(
    +      ""This is the first sentence that we will test:"",
    +      ""This is the second sentence that we will test:"",
    +      ""This is the third sentence that we will test:""
    +    ) );
    +    val expand_size = 10000000;
    --- End diff --
    
    Also if you write a smaller test I'd just have the test write and then read back an RDD and make sure they are exactly the same.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
asfgit <git@git.apache.org>,"Tue, 25 Feb 2014 01:42:01 +0000 (UTC)",[GitHub] incubator-spark pull request: For outputformats that are Configura...,dev@spark.incubator.apache.org,"Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-spark/pull/638


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
asfgit <git@git.apache.org>,"Tue, 25 Feb 2014 01:42:07 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1124: Fix infinite retries of ...,dev@spark.incubator.apache.org,"Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-spark/pull/641


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Tue, 25 Feb 2014 01:50:57 +0000 (UTC)",[GitHub] incubator-spark pull request: Patch for SPARK-942,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/180#issuecomment-35966227
  
    Hey @kellrott - I started to do a review on this focused on the tests and smaller stuff. But I realized, this makes a fairly major change to the block manager API in that it changes it to accept iterators instead of existing buffers. This means you do a copy into a new buffer in the case where an iterator is not used - which is expensive and will regress behavior for existing users. That I think blocks this patch as-is from being merged.
    
    I need to think about this a bit more and see if there is a more surgical/simple solution to fixing this. Since this is not a super common issue (although agreed it would be way better to pipeline this write directly to Disk) it would be nice if we could avoid changing the codepath for normal users. 


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Tue, 25 Feb 2014 01:53:22 +0000 (UTC)",[GitHub] incubator-spark pull request: Patch for SPARK-942,dev@spark.incubator.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/180#discussion_r10021400
  
    --- Diff: core/src/main/scala/org/apache/spark/storage/MemoryStore.scala ---
    @@ -65,17 +65,19 @@ private class MemoryStore(blockManager: BlockManager, maxMemory: Long)
     
       override def putValues(
           blockId: BlockId,
    -      values: ArrayBuffer[Any],
    +      values: Iterator[Any],
           level: StorageLevel,
           returnValues: Boolean)
         : PutResult = {
     
         if (level.deserialized) {
    -      val sizeEstimate = SizeEstimator.estimate(values.asInstanceOf[AnyRef])
    -      tryToPut(blockId, values, sizeEstimate, true)
    -      PutResult(sizeEstimate, Left(values.iterator))
    +      val valueEntries = new ArrayBuffer[Any]()
    +      valueEntries ++= values
    --- End diff --
    
    Making a new buffer here is the issue


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Tue, 25 Feb 2014 01:54:45 +0000 (UTC)",[GitHub] incubator-spark pull request: Patch for SPARK-942,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/180#issuecomment-35966452
  
    @kellrott  - ah I see, this has just moved the copy from one location to another... I retract my comment.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Tue, 25 Feb 2014 01:56:06 +0000 (UTC)",[GitHub] incubator-spark pull request: Patch for SPARK-942,dev@spark.incubator.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/180#discussion_r10021460
  
    --- Diff: core/src/main/scala/org/apache/spark/CacheManager.scala ---
    @@ -71,10 +71,21 @@ private[spark] class CacheManager(blockManager: BlockManager) extends Logging {
               val computedValues = rdd.computeOrReadCheckpoint(split, context)
               // Persist the result, so long as the task is not running locally
               if (context.runningLocally) { return computedValues }
    -          val elements = new ArrayBuffer[Any]
    -          elements ++= computedValues
    -          blockManager.put(key, elements, storageLevel, tellMaster = true)
    -          elements.iterator.asInstanceOf[Iterator[T]]
    +          if (storageLevel.useDisk && !storageLevel.useMemory) {
    +            blockManager.put(key, computedValues, storageLevel, tellMaster = true)
    +            return blockManager.get(key)  match {
    --- End diff --
    
    `get(key) match` (one space)


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
mengxr <git@git.apache.org>,"Tue, 25 Feb 2014 02:11:04 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1129: use a predefined seed wh...,dev@spark.incubator.apache.org,"GitHub user mengxr opened a pull request:

    https://github.com/apache/incubator-spark/pull/645

    SPARK-1129: use a predefined seed when seed is zero in XORShiftRandom

    If the seed is zero, XORShift generates all zeros, which would create unexpected result.
    
    JIRA: https://spark-project.atlassian.net/browse/SPARK-1129

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/mengxr/incubator-spark xor

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-spark/pull/645.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #645
    
----
commit 51f4050430a6627f979e4b044e6614c667931212
Author: Xiangrui Meng <meng@databricks.com>
Date:   2014-02-25T02:08:07Z

    use a predefined seed when seed is zero in XORShiftRandom

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
JoshRosen <git@git.apache.org>,"Tue, 25 Feb 2014 02:11:06 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1122: allCollect functions for...,dev@spark.incubator.apache.org,"Github user JoshRosen commented on the pull request:

    https://github.com/apache/incubator-spark/pull/635#issuecomment-35967399
  
    @mengxr For your examples (training multiple models in parallel and broadcast join), why is `allCollect` better than directly using broadcast variables?  Why do you want to access your model/table through an RDD with a single array value?  It seems like you'd need to use something like `zipPartitions()` to pair up the model with your data, so why not just directly reference the broadcast variable in a `mapPartitions()` on your varying dataset?
    
    In the past, I've thought about implementing a `SparkContext.broadcast[T](rdd: RDD[T]): Broadcast[T]` for creating broadcast variables from RDDs.  This can be implemented through peer-to-peer broadcasting of the  RDD fragments rather than broadcasting the entire collected RDD from the driver; this avoids bottlenecking on the driver's send bandwidth.
    
    I tried to prototype efficient broadcasting of RDDs, but ran into some difficulties.  My implementation used its own Broadcast subclass that fetched RDD partitions from remote block managers when trying to access the broadcast variable's value.  The problem is that we need to run some job on the broadcasted RDD to generate the partitions that the Broadcast subclass will fetch, and we need to ensure that those partitions are computed and stored before we attempt to fetch them.  Thus, any transformation that references a BroadcastRDD variable must declare a dependency on the stage that produces those blocks.  Usually, RDDs declare their dependencies when they're constructed; with BroadcastRDDs, we'd need to actually look inside the UDF to find any references in order to build the proper lineage.
    


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 25 Feb 2014 02:13:58 +0000 (UTC)",[GitHub] incubator-spark pull request: Catch depickling errors,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/644#issuecomment-35967537
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12837/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 25 Feb 2014 02:13:58 +0000 (UTC)",[GitHub] incubator-spark pull request: Patch for SPARK-942,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/180#issuecomment-35967538
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12836/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 25 Feb 2014 02:14:02 +0000 (UTC)",[GitHub] incubator-spark pull request: Patch for SPARK-942,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/180#issuecomment-35967549
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12838/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 25 Feb 2014 02:13:58 +0000 (UTC)",[GitHub] incubator-spark pull request: Patch for SPARK-942,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/180#issuecomment-35967536
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 25 Feb 2014 02:13:58 +0000 (UTC)",[GitHub] incubator-spark pull request: Catch depickling errors,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/644#issuecomment-35967535
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 25 Feb 2014 02:14:01 +0000 (UTC)",[GitHub] incubator-spark pull request: Patch for SPARK-942,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/180#issuecomment-35967547
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 25 Feb 2014 02:15:04 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-972] Added detailed callsite ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/643#issuecomment-35967618
  
    Build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 25 Feb 2014 02:15:03 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1129: use a predefined seed wh...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/645#issuecomment-35967610
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 25 Feb 2014 02:15:04 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-972] Added detailed callsite ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/643#issuecomment-35967617
  
     Build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 25 Feb 2014 02:15:03 +0000 (UTC)",[GitHub] incubator-spark pull request: SPARK-1129: use a predefined seed wh...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/645#issuecomment-35967609
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 25 Feb 2014 02:17:23 +0000 (UTC)",[GitHub] incubator-spark pull request: Patch for SPARK-942,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/180#issuecomment-35967757
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 25 Feb 2014 02:17:23 +0000 (UTC)",[GitHub] incubator-spark pull request: Patch for SPARK-942,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/180#issuecomment-35967758
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 25 Feb 2014 02:17:33 +0000 (UTC)",[GitHub] incubator-spark pull request: Patch for SPARK-942,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/180#issuecomment-35967767
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12841/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 25 Feb 2014 02:17:33 +0000 (UTC)",[GitHub] incubator-spark pull request: Patch for SPARK-942,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/180#issuecomment-35967766
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Tue, 25 Feb 2014 02:23:44 +0000 (UTC)",[GitHub] incubator-spark pull request: Patch for SPARK-942,dev@spark.incubator.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/180#discussion_r10022071
  
    --- Diff: core/src/main/scala/org/apache/spark/CacheManager.scala ---
    @@ -71,10 +71,21 @@ private[spark] class CacheManager(blockManager: BlockManager) extends Logging {
               val computedValues = rdd.computeOrReadCheckpoint(split, context)
               // Persist the result, so long as the task is not running locally
               if (context.runningLocally) { return computedValues }
    -          val elements = new ArrayBuffer[Any]
    -          elements ++= computedValues
    -          blockManager.put(key, elements, storageLevel, tellMaster = true)
    -          elements.iterator.asInstanceOf[Iterator[T]]
    +          if (storageLevel.useDisk && !storageLevel.useMemory) {
    --- End diff --
    
    /cc @RongGu make sure you catch this correctly in the Tachyon PR if this is merged... this will be easy to miss.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 25 Feb 2014 02:25:08 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-972] Added detailed callsite ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/643#issuecomment-35968221
  
    Build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Tue, 25 Feb 2014 02:25:08 +0000 (UTC)",[GitHub] incubator-spark pull request: [SPARK-972] Added detailed callsite ...,dev@spark.incubator.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/incubator-spark/pull/643#issuecomment-35968223
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12840/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
mridulm <git@git.apache.org>,"Tue, 25 Feb 2014 02:29:24 +0000 (UTC)",[GitHub] incubator-spark pull request: Patch for SPARK-942,dev@spark.incubator.apache.org,"Github user mridulm commented on a diff in the pull request:

    https://github.com/apache/incubator-spark/pull/180#discussion_r10022161
  
    --- Diff: core/src/main/scala/org/apache/spark/serializer/JavaSerializer.scala ---
    @@ -25,7 +25,22 @@ import org.apache.spark.SparkConf
     
     private[spark] class JavaSerializationStream(out: OutputStream) extends SerializationStream {
       val objOut = new ObjectOutputStream(out)
    -  def writeObject[T](t: T): SerializationStream = { objOut.writeObject(t); this }
    +  var counter = 0
    +  /* Calling reset to avoid memory leak:
    +   * http://stackoverflow.com/questions/1281549/memory-leak-traps-in-the-java-standard-api
    +   * But only call it every 1000th time to avoid bloated serialization streams (when
    +   * the stream 'resets' object class descriptions have to be re-written)  
    +   */
    +  def writeObject[T](t: T): SerializationStream = {
    +    objOut.writeObject(t)
    +    if (counter >= 1000) {
    +      objOut.reset()
    +      counter = 0
    +    } else {
    --- End diff --
    
    This (1000) should be configurable - really helps when using flatMap (in particular) to not create multiple instances of same object during ser-deser time.
    But otherwise, very good idea !


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Tue, 25 Feb 2014 02:31:55 +0000 (UTC)",[GitHub] incubator-spark pull request: Patch for SPARK-942,dev@spark.incubator.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/incubator-spark/pull/180#issuecomment-35968583
  
    Hmm okay I'm still wondering if there is a path for in-memory data where two copies are created:
    
    Cache manager runs:
    ```
    val elements = new ArrayBuffer[Any]
    elements ++= computedValues
    blockManager.put(key, elements, storageLevel, tellMaster = true)
    ```
    
    Block manager runs:
    ```
    doPut(blockId, Left(values.toIterator), level, tellMaster)
    ```
    
    DoPut runs:
    ```
    val res = memoryStore.putValues(blockId, values, level, true)
    ```
    
    MemoryStore runs:
    ```
    val valueEntries = new ArrayBuffer[Any]()
          valueEntries ++= values
    ```


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
=?UTF-8?B?5bC557uq5qOu?= <yinxusen@gmail.com>,"Tue, 25 Feb 2014 10:35:43 +0800",Preparing to provide a small text files input API in mllib,dev@spark.incubator.apache.org,"Hi community,

As I moving forward to write a LDA (Latent Dirichlet Allocation) to Spark
mllib, I find that a small files input API is useful, so I am writing a
smallTextFiles() to support it.

smallTextFiles() will digest a directory of text files, and return an
RDD[(String, String)], the former String is the file name, while the latter
one is the contents of the text file.

smallTextFiles() can be used for local disk IO, or HDFS IO, just like the
textFiles() in SparkContext. In the scenario of LDA, there are 2 common
uses:

1. We use smallTextFiles() to preprocess local disk files, i.e. combine
those files into a huge one, then transfer it onto HDFS to do further
process, such as LDA clustering.

2. We can also transfer the raw directory of small files onto HDFS (though
it is not recommended, because it will cost too many namenode entries),
then clustering it directly with LDA.

I also find in the Spark mail list that there are some users need this
function.

I have already finished it, but I am trying to remove a useless shuffle to
improve the performance now. Here is my code and all testsuites have passed.
https://github.com/yinxusen/incubator-spark/commit/ef418ea73e3cdaea9e45f60ce28fef3474872ade

What do you think about that ? I wish for your advises, thanks !

-- 
Best Regards
-----------------------------------
Xusen Yin    å°¹ç»ªæ£®
Beijing Key Laboratory of Intelligent Telecommunications Software and
Multimedia
Beijing University of Posts & Telecommunications
Intel Labs China
Homepage: *http://yinxusen.github.io/ <http://yinxusen.github.io/>*
"
=?UTF-8?B?5bC557uq5qOu?= <yinxusen@gmail.com>,"Tue, 25 Feb 2014 10:46:11 +0800",Re: MLLib - Thoughts about refactoring Updater for LBFGS?,dev@spark.incubator.apache.org,"Hi DB and Xiangrui,

L-BFGS is really useful. I think it will be cool if we can refactor the
interface of runMiniBatchSGD, so as to plug in new optimizer more easily
and compatible. What do you think of that ?


2014-02-23 3:49 GMT+08:00 Xiangrui Meng <mengxr@gmail.com>:

g
a
df



-- 
Best Regards
-----------------------------------
Xusen Yin    å°¹ç»ªæ£®
Beijing Key Laboratory of Intelligent Telecommunications Software and
Multimedia
Beijing University of Posts & Telecommunications
Intel Labs China
Homepage: *http://yinxusen.github.io/ <http://yinxusen.github.io/>*
"
DB Tsai <dbtsai@alpinenow.com>,"Mon, 24 Feb 2014 19:05:25 -0800",Re: MLLib - Thoughts about refactoring Updater for LBFGS?,dev@spark.apache.org,"Hi Yinxusen,

I agree. I found a hack to use the current Updater.scala (it contains
not only the logic of regularization but also the adaptive learning
rate in SGD which will not be used in general for other optimizers),
but I think we still need to have a better design to deal with it.

Sincerely,

DB Tsai
Machine Learning Engineer
Alpine Data Labs
--------------------------------------
Web: http://alpinenow.com/


ng
 a
n
pdf

"
DB Tsai <dbtsai@alpinenow.com>,"Mon, 24 Feb 2014 19:05:25 -0800",Re: MLLib - Thoughts about refactoring Updater for LBFGS?,dev@spark.apache.org,"Hi Yinxusen,

I agree. I found a hack to use the current Updater.scala (it contains
not only the logic of regularization but also the adaptive learning
rate in SGD which will not be used in general for other optimizers),
but I think we still need to have a better design to deal with it.

Sincerely,

DB Tsai
Machine Learning Engineer
Alpine Data Labs
--------------------------------------
Web: http://alpinenow.com/


ng
 a
n
pdf

"
Andrew Ash <andrew@andrewash.com>,"Mon, 24 Feb 2014 19:55:33 -0800",Kryo docs: do we include twitter/chill by default?,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Spark devs,

I picked up somewhere that the Spark 0.9.0 release included Twitter's chill
library of default-registered Kryo serialization classes.  Is that the case?

If so I'd like to mention in the data serialization docs that many things
are registered by default and include a link to the relevant Chill
documentation of what those default-registered classes are.  Chill isn't
mentioned in the below docs right now.

http://spark.incubator.apache.org/docs/latest/tuning.html#data-serialization

Thanks!
Andrew
"
Reynold Xin <rxin@databricks.com>,"Mon, 24 Feb 2014 20:30:22 -0800",Re: Kryo docs: do we include twitter/chill by default?,dev@spark.apache.org,"We do include Chill by default. It's a good idea to update the doc to
include chill.



"
Andrew Ash <andrew@andrewash.com>,"Mon, 24 Feb 2014 20:59:46 -0800",Re: Kryo docs: do we include twitter/chill by default?,dev@spark.apache.org,"https://github.com/apache/incubator-spark/pull/647

I was going to say in the addition that twitter/chill was new in 0.9.0 but
decided against it since there aren't any other references to specific
spark versions in the documentation (and I'm not confident that was
actually the new version).



"
Mark Hamstra <mark@clearstorydata.com>,"Mon, 24 Feb 2014 21:07:03 -0800",Re: Kryo docs: do we include twitter/chill by default?,dev@spark.apache.org,"Chill has been part of Spark releases since 0.8.0-incubating.



"
Mridul Muralidharan <mridul@gmail.com>,"Tue, 25 Feb 2014 11:43:53 +0530",Re: Preparing to provide a small text files input API in mllib,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi,

  I have not looked into why this would be needed, but given it is
needed, I added a couple of comments to the PR.
Overall, it looks promising.

Regards,
Mridul


ote:
er
h
o
ed.
0ce28fef3474872ade

"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 24 Feb 2014 22:42:59 -0800",WARNING: Spark lists moving to spark.apache.org domain name,"user@spark.incubator.apache.org,
 dev@spark.incubator.apache.org","Hi everyone,

As you may have noticed, our lists are currently in the process of being migrated from @spark.incubator.apache.org domain names to @spark.apache.org, as part of the project becoming a top-level project. Please beware that messages will come to the new lists and you’ll have to adjust email filters accordingly. This is hopefully our last move of mailing list — thanks for bearing with us through the moves in the past year.

Matei
"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 24 Feb 2014 22:55:24 -0800",Re: Github emails,"dev@spark.apache.org,
 infrastructure-dev@apache.org","This is probably a snafu because we had a GitHub hook that was sending messages to dev@spark.incubator.apache.org, and that list was recently moved (or is in the process of being moved?) to dev@spark.apache.org. Unfortunately there’s nothing we can do to change it on our end, but this was originally set up by Daniel Gruno and Jake Farrel: https://issues.apache.org/jira/browse/INFRA-7276.

Matei




"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 24 Feb 2014 22:55:24 -0800",Re: Github emails,"dev@spark.apache.org,
 infrastructure-dev@apache.org","This is probably a snafu because we had a GitHub hook that was sending messages to dev@spark.incubator.apache.org, and that list was recently moved (or is in the process of being moved?) to dev@spark.apache.org. Unfortunately there’s nothing we can do to change it on our end, but this was originally set up by Daniel Gruno and Jake Farrel: https://issues.apache.org/jira/browse/INFRA-7276.

Matei




"
Aaron Davidson <ilikerps@gmail.com>,"Mon, 24 Feb 2014 23:32:00 -0800",Re: Github emails,dev@spark.apache.org,"By the way, we still need to get our JIRAs migrated over to the Apache
system. Unrelated, just... saying.



"
Aaron Davidson <ilikerps@gmail.com>,"Mon, 24 Feb 2014 23:32:00 -0800",Re: Github emails,dev@spark.apache.org,"By the way, we still need to get our JIRAs migrated over to the Apache
system. Unrelated, just... saying.



"
Debasish Das <debasish.das83@gmail.com>,"Tue, 25 Feb 2014 06:53:05 -0800",Re: MLLib - Thoughts about refactoring Updater for LBFGS?,dev@spark.incubator.apache.org,"Hi DB, Xiangrui,

Mallet from cmu also has bfgs cg and a good optimization package.  Do you
know if cpl license si

"
Debasish Das <debasish.das83@gmail.com>,"Tue, 25 Feb 2014 07:07:18 -0800",Re: MLLib - Thoughts about refactoring Updater for LBFGS?,dev@spark.incubator.apache.org,"Continuation on last email sent by mistake:

Is cpl license is compatible with apache ?

http://opensource.org/licenses/cpl1.0.php

Mallet jars are available on maven. They have hessian based solvers which
looked interesting along with bfgs and cg.

Definitely the lbfgs f2j looks promising as the base fortran code is
perhaps the best bfgs code I have used. Also the bounded bfgs is useful for
many cases to enforce constraints on say pca....

Note that right now the version is not blas optimized. With jblas or
netlib-java discussions that's going on it can be improved. Also it runs on
a single thread which can be improved...so there is scope for further
improvements in the code.

Basically Xiangrui, is there a push back on making optimizers part of spark
mllib ? I am exploring cg and qp solvers for spark mllib as well and I am
developing these as part of mllib optimization. I was hoping we should be
able to publish mllib as a maven artifact later.

Thanks.
Deb

"
Daniel Gruno <rumble@cord.dk>,"Tue, 25 Feb 2014 12:35:06 +0100",Re: Github emails,"infrastructure-dev@apache.org, dev@spark.apache.org","No, it was because the incubator-spark repository did not exist anymore,
so our hook did not know where to send to.

requests from the incubator repo on GitHub to your new TLP repo.
Hopefully, this shouldn't be a problem.

"
Xiangrui Meng <mengxr@gmail.com>,"Tue, 25 Feb 2014 11:14:34 -0800",Re: MLLib - Thoughts about refactoring Updater for LBFGS?,dev@spark.apache.org,"Hi Deb,

CPL 1.0 is compatible if the inclusion is appropriately labeled
(https://www.apache.org/legal/3party.html). I think it is great to
have an L-BFGS optimizer in mllib, but we need to investigate some
time to figure out which one to use. I'm not sure whether jblas or
netlib-java will make a big difference here, because L-BFGS doesn't
have dense level-2 BLAS operations. But it would be great if someone
can do the comparison and show some numbers. FYI, spark-mllib is
published in maven central.

Best,
Xiangrui


"
Xiangrui Meng <mengxr@gmail.com>,"Tue, 25 Feb 2014 11:14:34 -0800",Re: MLLib - Thoughts about refactoring Updater for LBFGS?,dev@spark.apache.org,"Hi Deb,

CPL 1.0 is compatible if the inclusion is appropriately labeled
(https://www.apache.org/legal/3party.html). I think it is great to
have an L-BFGS optimizer in mllib, but we need to investigate some
time to figure out which one to use. I'm not sure whether jblas or
netlib-java will make a big difference here, because L-BFGS doesn't
have dense level-2 BLAS operations. But it would be great if someone
can do the comparison and show some numbers. FYI, spark-mllib is
published in maven central.

Best,
Xiangrui


"
DB Tsai <dbtsai@alpinenow.com>,"Tue, 25 Feb 2014 11:16:00 -0800",Re: MLLib - Thoughts about refactoring Updater for LBFGS?,dev@spark.apache.org,"Hi Deb,


Based on what I read here, there is no problem to include CPL code in
apache project
as long as the code isn't modified, and we include the maven binary.
https://www.apache.org/legal/3party.html


We found that hessian based solvers don't scale as the # of features grow, and
we have lots of customers trying to train sparse input. That's our motivation to
work on L-BFGS which approximate hessian using just a few vectors.

Just take a look at MALLET, and it does have L-BFGS and its variant OWL-QN
which can tackle L1 problem. Since implementing L-BFGS is very subtle, I don't
know the quality of the mallet implementation. Personally, I
implemented one based
on textbook, and not very stable. If MALLET is robust, I'll go for it
since it has more
features, and already in maven.


I think it will not impact performance even it's not blas optimized
nor multi-threaded,
since most of the parallelization is in computing gradientSum and
lossSum in Spark,
and the optimizer just takes gradientSum, lossSum, and weights to get
the newWeights.

As a result, 99.9% of time is in computing gradientSum and lossSum.
of time is in optimization.


Thanks.

Sincerely,

DB Tsai
Machine Learning Engineer
Alpine Data Labs
--------------------------------------
Web: http://alpinenow.com/

"
DB Tsai <dbtsai@alpinenow.com>,"Tue, 25 Feb 2014 11:16:00 -0800",Re: MLLib - Thoughts about refactoring Updater for LBFGS?,dev@spark.apache.org,"Hi Deb,


Based on what I read here, there is no problem to include CPL code in
apache project
as long as the code isn't modified, and we include the maven binary.
https://www.apache.org/legal/3party.html


We found that hessian based solvers don't scale as the # of features grow, and
we have lots of customers trying to train sparse input. That's our motivation to
work on L-BFGS which approximate hessian using just a few vectors.

Just take a look at MALLET, and it does have L-BFGS and its variant OWL-QN
which can tackle L1 problem. Since implementing L-BFGS is very subtle, I don't
know the quality of the mallet implementation. Personally, I
implemented one based
on textbook, and not very stable. If MALLET is robust, I'll go for it
since it has more
features, and already in maven.


I think it will not impact performance even it's not blas optimized
nor multi-threaded,
since most of the parallelization is in computing gradientSum and
lossSum in Spark,
and the optimizer just takes gradientSum, lossSum, and weights to get
the newWeights.

As a result, 99.9% of time is in computing gradientSum and lossSum.
of time is in optimization.


Thanks.

Sincerely,

DB Tsai
Machine Learning Engineer
Alpine Data Labs
--------------------------------------
Web: http://alpinenow.com/

"
DB Tsai <dbtsai@alpinenow.com>,"Tue, 25 Feb 2014 11:36:34 -0800",Re: MLLib - Thoughts about refactoring Updater for LBFGS?,dev@spark.apache.org,"I find some comparison between Mallet vs Fortran version. The result
is closed but not the same.

http://t3827.ai-mallet-development.aitalk.info/help-with-l-bfgs-t3827.html

Here is LBFGS-B
Cost: 0.6902411220175793
Gradient: -5.453609E-007, -2.858372E-008, -1.369706E-007
Theta: -0.014186210102171406, -0.303521206706629, -0.018132348904129902

And Mallet LBFGS (Tollerance .000000000000001)
Cost: 0.6902412268833071
Gradient: 0.000117, -4.615523E-005, 0.000114
Theta: -0.013914961040040107, -0.30419883021414335, -0.016838481937958744

So this shows me, that Mallet is close, but Plain ol Gradient Descent
and LBFGS-B are really close.
I see that Mallet also has a ""LineOptimizer"" and ""Evaluator"" that I
have yet to explore...

Sincerely,

DB Tsai
Machine Learning Engineer
Alpine Data Labs
--------------------------------------
Web: http://alpinenow.com/



"
DB Tsai <dbtsai@alpinenow.com>,"Tue, 25 Feb 2014 11:36:34 -0800",Re: MLLib - Thoughts about refactoring Updater for LBFGS?,dev@spark.apache.org,"I find some comparison between Mallet vs Fortran version. The result
is closed but not the same.

http://t3827.ai-mallet-development.aitalk.info/help-with-l-bfgs-t3827.html

Here is LBFGS-B
Cost: 0.6902411220175793
Gradient: -5.453609E-007, -2.858372E-008, -1.369706E-007
Theta: -0.014186210102171406, -0.303521206706629, -0.018132348904129902

And Mallet LBFGS (Tollerance .000000000000001)
Cost: 0.6902412268833071
Gradient: 0.000117, -4.615523E-005, 0.000114
Theta: -0.013914961040040107, -0.30419883021414335, -0.016838481937958744

So this shows me, that Mallet is close, but Plain ol Gradient Descent
and LBFGS-B are really close.
I see that Mallet also has a ""LineOptimizer"" and ""Evaluator"" that I
have yet to explore...

Sincerely,

DB Tsai
Machine Learning Engineer
Alpine Data Labs
--------------------------------------
Web: http://alpinenow.com/



"
Debasish Das <debasish.das83@gmail.com>,"Tue, 25 Feb 2014 12:07:55 -0800",Re: MLLib - Thoughts about refactoring Updater for LBFGS?,dev@spark.apache.org,"Hi DB,

I am considering building on your PR and add Mallet as the dependency so
that we can run some basic comparisons test on large scale sparse datasets
that I have.

In the meantime, let's discuss if there are other optimization packages
that we should try.

My wishlist has bounded bfgs as well and I will add it to the PR.

About the PR getting merged to mllib, we can plan that later.

Thanks.
Deb




"
Debasish Das <debasish.das83@gmail.com>,"Tue, 25 Feb 2014 12:07:55 -0800",Re: MLLib - Thoughts about refactoring Updater for LBFGS?,dev@spark.apache.org,"Hi DB,

I am considering building on your PR and add Mallet as the dependency so
that we can run some basic comparisons test on large scale sparse datasets
that I have.

In the meantime, let's discuss if there are other optimization packages
that we should try.

My wishlist has bounded bfgs as well and I will add it to the PR.

About the PR getting merged to mllib, we can plan that later.

Thanks.
Deb




"
Konstantin Boudnik <cos@apache.org>,"Tue, 25 Feb 2014 13:24:37 -0800",Re: [DISCUSS] Necessity of Maven *and* SBT Build in Spark,dev@spark.incubator.apache.org,"
I am actually talking about an ability to exclude a set of dependencies from an
assembly, similarly to what's happening in dependencySet sections of  
    assembly/src/main/assembly/assembly.xml
If there is a comparable functionality in Sbt, that would help quite a bit,
apparently.

Cos


I am bringing up the Sharder, because it is an awful hack, which is can't be
used in real controlled deployment.

Cos


"
Koert Kuipers <koert@tresata.com>,"Tue, 25 Feb 2014 17:45:21 -0500",Re: [DISCUSS] Necessity of Maven *and* SBT Build in Spark,dev@spark.apache.org,"yes in sbt assembly you can exclude jars (although i never had a need for
this) and files in jars.

for example i frequently remove log4j.properties, because for whatever
reason hadoop decided to include it making it very difficult to use our own
logging config.




"
Evan Chan <ev@ooyala.com>,"Tue, 25 Feb 2014 15:20:47 -0800",Re: [DISCUSS] Necessity of Maven *and* SBT Build in Spark,dev@spark.apache.org,"The correct way to exclude dependencies in SBT is actually to declare
a dependency as ""provided"".   I'm not familiar with Maven or its
dependencySet, but provided will mark the entire dependency tree as
excluded.   It is also possible to exclude jar by jar, but this is
pretty error prone and messy.




-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

"
Sravya Tirukkovalur <sravya@cloudera.com>,"Tue, 25 Feb 2014 15:26:40 -0800",Re: [DISCUSS] Necessity of Maven *and* SBT Build in Spark,dev@spark.apache.org,"I am no sbt guru, but I could exclude transitive dependencies this way:

libraryDependencies +=
  ""log4j"" % ""log4j"" % ""1.2.15"" exclude(""javax.jms"", ""jms"")

Thanks!







-- 
Sravya Tirukkovalur
"
yao <yaoshengzhe@gmail.com>,"Tue, 25 Feb 2014 15:31:16 -0800",Re: [DISCUSS] Necessity of Maven *and* SBT Build in Spark,dev@spark.apache.org,"I would prefer keep both of them, it would be better even if that means
pom.xml will be generated using sbt. Some company, like my current one,
have their own build infrastructures built on top of maven. It is not easy
to support sbt for these potential spark clients. But I do agree to only
keep one if there is a promising way to generate correct configuration from
the other.

-Shengzhe



"
Evan Chan <ev@ooyala.com>,"Tue, 25 Feb 2014 15:39:36 -0800",Re: [DISCUSS] Necessity of Maven *and* SBT Build in Spark,dev@spark.apache.org,"The problem is that plugins are not equivalent.  There is AFAIK no
equivalent to the maven shader plugin for SBT.
There is an SBT plugin which can apparently read POM XML files
(sbt-pom-reader).   However, it can't possibly handle plugins, which
is still problematic.




-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

"
Patrick Wendell <pwendell@gmail.com>,"Tue, 25 Feb 2014 15:40:35 -0800",Re: [DISCUSS] Necessity of Maven *and* SBT Build in Spark,dev@spark.apache.org,"Hey Yao,

Would you mind explaining exactly how your company extends the Spark
maven build? For instance:

(a) You are depending on Spark in your build and your build is using Maven.
(b) You have downloaded Spark and forked it's maven build to change
around the dependencies.
(c) You are writing pom files that extend the Spark pom.

If it's just (a) - then whether Spark itself uses sbt/maven will make
no difference. We'd publish identical poms.

- Patrick


"
yao <yaoshengzhe@gmail.com>,"Tue, 25 Feb 2014 15:51:07 -0800",Re: [DISCUSS] Necessity of Maven *and* SBT Build in Spark,dev@spark.apache.org,"Hi Patrick,




We go with this approach. We've cloned Spark repo and currently maintain
our own branch. The idea is to fix Spark issues found in our production
system first and contribute back to community later (if it is accepted). We
use maven + jenkins here and our deployment engineer customize their
configuration. We might lose some build features (spell check, coding style
check) if make a exception for Spark (using sbt), even for maven, we make a
special configuration just for Spark. Java is still widely used and only
few teams start experimenting Scala. I would say this change will affect
people like us who maintains their own Spark branch. It's okay to go with
sbt since it's the standard build tool for scala, but I think we still want
the ability to use maven as alternative.

Thanks
-Shengzhe



"
Patrick Wendell <pwendell@gmail.com>,"Tue, 25 Feb 2014 16:01:37 -0800",Re: [DISCUSS] Necessity of Maven *and* SBT Build in Spark,"dev@spark.apache.org, Evan Chan <ev@ooyala.com>","Evan - this is a good thing to bring up. Wrt the shader plug-in -
right now we don't actually use it for bytecode shading - we simply
use it for creating the uber jar with excludes (which sbt supports
just fine via assembly).

I was wondering actually, do you know if it's possible to added shaded
artifacts to the *spark jar* using this plug-in (e.g. not an uber
jar)? That's something I could see being really handy in the future.

- Patrick


"
Evan Chan <ev@ooyala.com>,"Tue, 25 Feb 2014 16:04:31 -0800",Re: [DISCUSS] Necessity of Maven *and* SBT Build in Spark,Patrick Wendell <pwendell@gmail.com>,"Patrick -- not sure I understand your request, do you mean
- somehow creating a shaded jar (eg with maven shader plugin)
- then including it in the spark jar (which would then be an assembly)?




-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

"
Patrick Wendell <pwendell@gmail.com>,"Tue, 25 Feb 2014 16:09:34 -0800",Re: [DISCUSS] Necessity of Maven *and* SBT Build in Spark,Evan Chan <ev@ooyala.com>,"What I mean is this. AFIAK the shader plug-in is primarily designed
for creating uber jars which contain spark and all dependencies. But
since Spark is something people depend on in Maven, what I actually
want is to create the normal old Spark jar [1], but then include
shaded versions of some of our dependencies inside of it. Not sure if
that's even possible.

The way we do shading now is we manually publish shaded versions of
some dependencies to maven central as their own artifacts.

http://search.maven.org/remotecontent?filepath=org/apache/spark/spark-core_2.10/0.9.0-incubating/spark-core_2.10-0.9.0-incubating.jar


"
Evan Chan <ev@ooyala.com>,"Tue, 25 Feb 2014 16:23:42 -0800",Re: [DISCUSS] Necessity of Maven *and* SBT Build in Spark,Patrick Wendell <pwendell@gmail.com>,"Hi Patrick,

If you include shaded dependencies inside of the main Spark jar, such
that it would have combined classes from all dependencies, wouldn't
you end up with a sub-assembly jar?  It would be dangerous in that
since it is a single unit, it would break normal packaging assumptions
that the jar only contains its own classes, and maven/sbt/ivy/etc is
used to resolve the remaining deps.... but maybe I don't know what you
mean.

The shader plugin in maven is apparently used to
1) build uber jars  - this is the part that sbt-assembly also does
2) ""shade"" existing jars, ie rename the classes and rewrite bytecode
depending on them such that it doesn't conflict with other jars having
the same classes  -- this is something sbt-assembly doesn't do, which
you point out is done manually.






-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

"
Sandy Ryza <sandy.ryza@cloudera.com>,"Tue, 25 Feb 2014 16:36:28 -0800",Re: [DISCUSS] Necessity of Maven *and* SBT Build in Spark,dev@spark.apache.org,"To perhaps restate what some have said, Maven is by far the most common
build tool for the Hadoop / JVM data ecosystem.  While Maven is less pretty
than SBT, expertise in it is abundant.  SBT requires contributors to
projects in the ecosystem to learn yet another tool.  If we think of Spark
as a project in that ecosystem that happens to be in Scala, as opposed to a
Scala project that happens to be part of that ecosystem, Maven seems like
the better choice to me.

helpful to us is that it makes it easy to harmonize dependency versions
across projects.  We modify project poms to include the ""CDH"" pom as a root
pom, allowing each project to reference variables defined in the root pom
like ${cdh.slf4j.version}.  Is there a way to make an SBT project inherit
from a Maven project that would allow this kind of thing?

-Sandy



"
Evan Chan <ev@ooyala.com>,"Tue, 25 Feb 2014 16:41:22 -0800",Re: [DISCUSS] Necessity of Maven *and* SBT Build in Spark,dev@spark.apache.org,"Sandy, I believe the sbt-pom-reader plugin might work very well for
this exact use case.   Otherwise, the SBT build file is just Scala
code, so it can easily read the pom XML directly if needed and parse
stuff out.




-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

"
DB Tsai <dbtsai@alpinenow.com>,"Tue, 25 Feb 2014 17:03:38 -0800",Re: MLLib - Thoughts about refactoring Updater for LBFGS?,dev@spark.apache.org,"Hi Deb, Xiangrui

I just moved the LBFGS code to maven central, and cleaned up the code
a little bit.

https://github.com/AlpineNow/incubator-spark/commits/dbtsai-LBFGS

After looking at Mallet, the api is pretty simple, and it's probably
can be easily tested
based on my PR.

It will be tricky to just benchmark the time of optimizers by
excluding the parallel gradientSum
and lossSum computation, and I don't have good approach yet. Let's
compare the accuracy for the time being.

Thanks.

Sincerely,

DB Tsai
Machine Learning Engineer
Alpine Data Labs
--------------------------------------
Web: http://alpinenow.com/



"
Debasish Das <debasish.das83@gmail.com>,"Tue, 25 Feb 2014 17:52:09 -0800",Re: MLLib - Thoughts about refactoring Updater for LBFGS?,dev@spark.apache.org,"Hi DB,

Could you please point me to your spark PR ?

Thanks.
Deb



"
Patrick Wendell <pwendell@gmail.com>,"Tue, 25 Feb 2014 18:32:30 -0800",Re: SPARK-942 patch review,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey Andrew,

Indeed, sometimes there are patches that sit around a while and in
this case it can be because it's unclear to the reviewers whether they
are features worth having - or just by accident.

To put things in perspective, Spark merges about 80% of the proposed
patches (if you look we are on around 600 since moving to the new repo
with 100 not merged) - so in general we try hard to be very supportive
of community patches, much more than other projects in this space.

- Patrick

ote:
 inactive PRs on GitHub are not really getting closed, so active ones might be lost between those. For now please just post on the dev list if your PR is being ignored. We'll implement some kind of cleanup (at least manually) to close the old ones.
rt.
et
o:
e

"
"""Evan R. Sparks"" <evan.sparks@gmail.com>","Tue, 25 Feb 2014 18:33:10 -0800",Re: MLLib - Thoughts about refactoring Updater for LBFGS?,dev@spark.apache.org,"Hi everyone,

Sorry I'm late to the thread here, but I want to point out a few things.
This is, of course, a most welcome contribution and it will be immediately
useful to everything currently using the stochastic gradient optimizers!

1) I'm all for refactoring the optimization methods to make them a little
more general - Perhaps there should be a ""FirstOrderUpdater"" that
subclasses updater which takes things like stepsize as paramters, but still
has an ""compute"" method. While the updater APIs are public, I'd be
surprised if anyone is using them directly, instead, I'd expect most people
to be using the APIs that rely on them (namely the SVM, Logistic, and
Linear regression classes). It should *definitely* be possible to keep the
loss function we're minimizing separate from the optimization method.

2) That said - LBFGS *does* rely on being able to take gradients of the
loss function, and is a gradient based method.

L-BFGS is basically identical to the code for minibatchSGD - I may be
missing something, but we really should try to factor out the parts that
are the same and avoid duplicating this logic. I *think* coming up with an
LBFGSUpdater with an appropriate compute method is all we need
(particularly since we're keeping track of the loss history), but I might
be wrong here.

4) In general, I think we should think about incurring technical debt
through duplicated (in functionality) code in the codebase (e.g. yet
another Vector sum/multiply class) and code written in languages other than
Scala in MLlib - the fortran/C++ implementations of L-BFGS aren't doing
anything magical, and as long as we can get similar performance it will be
much easier to maintain if everything is in Scala (with some critical bits
in other languages - but I don't think this falls into that case).
Additionally, we should think about whether we really need these additional
dependencies. While I'm sure Mallet is great, I'm a little worried that
adding it as a dependency for one or two functions we could pretty easily
reimplement might be a little heavy and present problems in the future.

Anyway, you should submit a PR and we can work on it!

- Evan





"
=?GBK?B?u8bUtse/?= <hyqgod@163.com>,"Wed, 26 Feb 2014 10:33:58 +0800 (CST)",[HELP] ask for some information about public  data set,dev@spark.apache.org,"Hi all:
I am a freshman in Spark community. i dream of being a expert in the field of big data.  But i have no idea where to start after i have gone through the published  documents in Spark website and examples in  Spark source code.  I want to know if there are some public data set in the internet that can be utilized  to learn Spark and test my some new ideas base on Spark.
      Thanks a lot.


---------------------------
Best regards
hyqgod"
"""Evan R. Sparks"" <evan.sparks@gmail.com>","Tue, 25 Feb 2014 18:45:02 -0800",Re: [HELP] ask for some information about public data set,user@spark.apache.org,"Hi hyqgod,

This is probably a better question for the spark user's list than the dev
list (cc'ing user and bcc'ing dev on this reply).

To answer your question, though:

Amazon's Public Datasets Page is a nice place to start:
http://aws.amazon.com/datasets/ - these work well with spark because
they're often stored on s3 (which spark can read from natively) and it's
very easy to spin up a spark cluster on EC2 to begin experimenting with the
data.

There's also a pretty good list of (mostly big) datasets that google has
released over the years here:
http://svonava.com/post/62186512058/datasets-released-by-google

- Evan


d
"
Mridul Muralidharan <mridul@gmail.com>,"Wed, 26 Feb 2014 08:20:48 +0530",Re: [DISCUSS] Necessity of Maven *and* SBT Build in Spark,dev@spark.apache.org,"

Not really - as I mentioned initially in this thread, sbt's assembly
does not take dependencies into account properly : and can overwrite
newer classes with older versions.
try it after 2.10 shift though (and probably wont, given the mess it
created last time).

Regards,
Mridul






"
Evan chan <ev@ooyala.com>,"Tue, 25 Feb 2014 19:26:55 -0800",Re: [DISCUSS] Necessity of Maven *and* SBT Build in Spark,"""dev@spark.apache.org"" <dev@spark.apache.org>","Actually you can control exactly how sbt assembly merges or resolves conflicts.  I believe the default settings however lead to order which cannot be controlled. 

I do wish for a smarter fat jar plugin.  

-Evan
To be free is not merely to cast off one's chains, but to live in a way that respects & enhances the freedom of others. (#NelsonMandela)


te:


asy
y
rom
ote:
 for
r
ur

s
t
he
her

t
o
o
ies

 a
g

gtop-packages/src/common/spark/do-component-build;h=428540e0f6aa56cd7e78eb1c831aa7fe9496a08f;hb=master

"
Qiuzhuang Lian <qiuzhuang.lian@gmail.com>,"Wed, 26 Feb 2014 11:31:09 +0800",Re: [DISCUSS] Necessity of Maven *and* SBT Build in Spark,dev@spark.apache.org,"We use jarjar Ant plugin task to assemble into one fat jar.

Qiuzhuang



"
Andrew Ash <andrew@andrewash.com>,"Tue, 25 Feb 2014 20:00:10 -0800",Re: SPARK-942 patch review,dev@spark.apache.org,"I've always felt that the Spark team was extremely responsive to PRs and
I've been very impressed over the past year with your output.  As Matei
said, probably the best thing to do here is to be more diligent about
closing PRs that are old/abandoned so that every PR is active.  Whenever I
comment I try to make it clear who has the next action to get the PR merged.

I definitely don't want you to think that I'm critiquing the process!  The
reason I brought this up in the first place was because I thought we were
about to lose a contributor because something fell through the cracks,
which would be unfortunate.



"
Andrew Ash <andrew@andrewash.com>,"Tue, 25 Feb 2014 20:00:10 -0800",Re: SPARK-942 patch review,dev@spark.apache.org,"I've always felt that the Spark team was extremely responsive to PRs and
I've been very impressed over the past year with your output.  As Matei
said, probably the best thing to do here is to be more diligent about
closing PRs that are old/abandoned so that every PR is active.  Whenever I
comment I try to make it clear who has the next action to get the PR merged.

I definitely don't want you to think that I'm critiquing the process!  The
reason I brought this up in the first place was because I thought we were
about to lose a contributor because something fell through the cracks,
which would be unfortunate.



"
Chester Chen <chesterxgchen@yahoo.com>,"Tue, 25 Feb 2014 20:52:39 -0800",Re: [DISCUSS] Necessity of Maven *and* SBT Build in Spark,"""dev@spark.apache.org"" <dev@spark.apache.org>","@Sandy

Yes, in sbt with multiple projects setup, you can easily set a variable in the build.scala and reference the version number from all dependent projects .


Regarding mix of java and scala projects, in my workplace , we have both java and scala cod"
Mridul Muralidharan <mridul@gmail.com>,"Wed, 26 Feb 2014 10:40:46 +0530",Re: [DISCUSS] Necessity of Maven *and* SBT Build in Spark,dev@spark.apache.org,"The problem is, the complete spark dependency graph is fairly large,
and there are lot of conflicting versions in there.
In particular, when we bump versions of dependencies - making managing
this messy at best.

Now, I have not looked in detail at how maven manages this - it might
just be accidental that we get a decent out-of-the-box assembled
shaded jar (since we dont do anything great to configure it).
With current state of sbt in spark, it definitely is not a good
solution : if we can enhance it (or it already is ?), while keeping
the management of the version/dependency graph manageable, I dont have
any objections to using sbt or maven !
Too many exclude versions, pinned versions, etc would just make things
unmanageable in future.


Regards,
Mridul





"
Patrick Wendell <pwendell@gmail.com>,"Tue, 25 Feb 2014 22:52:35 -0800",Re: SPARK-942 patch review,dev@spark.apache.org,"Hey Andrew,

Ah, I just meant to say that in cases like this it's usually a
mistake...  and we try to (in general) be inclusive about merging
patches :) Definitely appreciate you calling this one out... this is
what people should do in cases like this.

- Patrick


"
Patrick Wendell <pwendell@gmail.com>,"Tue, 25 Feb 2014 22:52:35 -0800",Re: SPARK-942 patch review,dev@spark.apache.org,"Hey Andrew,

Ah, I just meant to say that in cases like this it's usually a
mistake...  and we try to (in general) be inclusive about merging
patches :) Definitely appreciate you calling this one out... this is
what people should do in cases like this.

- Patrick


"
Sean Owen <sowen@cloudera.com>,"Wed, 26 Feb 2014 09:22:21 +0000",Re: [DISCUSS] Necessity of Maven *and* SBT Build in Spark,dev@spark.apache.org,"Side point -- ""provides"" scope is not the same as an exclude.
""provides"" means, this artifact is used directly by this code (compile
time), but it is not necessary to package it, since it will be
available from a runtime container. Exclusions make an artifact, that
would otherwise be available, unavailable at both compile time and
run-time.

SBT appears to have syntax for both, just like Maven. Surely these
have the same meanings in SBT, and excluding artifacts is accomplished
with exclude and excludeAll, as seen in the Spark build?

The assembly and shader stuff in Maven is more about controlling
exactly how it's put together into an artifact, at the level of files
even, to stick a license file in or exclude some data file cruft or
rename dependencies.

exclusions and shading are necessary evils to be used as sparingly as
possible. Dependency graphs get nuts fast here, and Spark is already
quite big. (Hence my recent PR to start touching it up -- more coming
for sure.)


"
Nan Zhu <zhunanmcgill@gmail.com>,"Wed, 26 Feb 2014 08:23:29 -0500",Discussion on SPARK-1139,"dev@spark.incubator.apache.org, dev@spark.apache.org","Hi, all  

I just created a JIRA https://spark-project.atlassian.net/browse/SPARK-1139 . The issue discusses that:

the new Hadoop API based Spark APIs are actually a mixture of old and new Hadoop API.

Spark APIs are still using JobConf (or Configuration) as one of the parameters, but actually Configuration has been replace by mapreduce.Job in the new Hadoop API

for example : http://codesfusion.blogspot.ca/2013/10/hadoop-wordcount-with-new-map-reduce-api.html  

&  

http://www.slideshare.net/sh1mmer/upgrading-to-the-new-map-reduce-api (p10)

Personally I think itâ€™s better to fix this design, but it will introduce some compatibility issue  

Just bring it here for your advices

Best,  

--  
Nan Zhu

"
Nan Zhu <zhunanmcgill@gmail.com>,"Wed, 26 Feb 2014 08:23:29 -0500",Discussion on SPARK-1139,"dev@spark.incubator.apache.org, dev@spark.apache.org","Hi, all  

I just created a JIRA https://spark-project.atlassian.net/browse/SPARK-1139 . The issue discusses that:

the new Hadoop API based Spark APIs are actually a mixture of old and new Hadoop API.

Spark APIs are still using JobConf (or Configuration) as one of the parameters, but actually Configuration has been replace by mapreduce.Job in the new Hadoop API

for example : http://codesfusion.blogspot.ca/2013/10/hadoop-wordcount-with-new-map-reduce-api.html  

&  

http://www.slideshare.net/sh1mmer/upgrading-to-the-new-map-reduce-api (p10)

Personally I think itâ€™s better to fix this design, but it will introduce some compatibility issue  

Just bring it here for your advices

Best,  

--  
Nan Zhu

"
Koert Kuipers <koert@tresata.com>,"Wed, 26 Feb 2014 10:17:29 -0500",Re: [DISCUSS] Necessity of Maven *and* SBT Build in Spark,dev@spark.apache.org,"We maintain in house spark build using sbt. We have no problem using sbt
assembly. We did add a few exclude statements for transitive dependencies.

The main enemy of assemblies are jars that include stuff they shouldn't
(kryo comes to mind, I think they include logback?), new versions of jars
that change the provider/artifact without changing the package (asm), and
incompatible new releases (protobuf). These break the transitive resolution
process. I imagine that's true for any build tool.

Besides shading I don't see anything maven can do sbt cannot, and if I
understand it correctly shading is not done currently using the build tool.

Since spark is primarily scala/akka based the main developer base will be
familiar with sbt (I think?). Switching build tool is always painful. I
personally think it is smarter to put this burden on a limited number of
upstream integrators than on the community. However that said I don't think
its a problem for us to maintain an sbt build in-house if spark switched to
maven.
The problem is, the complete spark dependency graph is fairly large,
and there are lot of conflicting versions in there.
In particular, when we bump versions of dependencies - making managing
this messy at best.

Now, I have not looked in detail at how maven manages this - it might
just be accidental that we get a decent out-of-the-box assembled
shaded jar (since we dont do anything great to configure it).
With current state of sbt in spark, it definitely is not a good
solution : if we can enhance it (or it already is ?), while keeping
the management of the version/dependency graph manageable, I dont have
any objections to using sbt or maven !
Too many exclude versions, pinned versions, etc would just make things
unmanageable in future.


Regards,
Mridul




conflicts.  I believe the default settings however lead to order which
cannot be controlled.
that respects & enhances the freedom of others. (#NelsonMandela)
means
one,
easy
only
configuration from
need for
whatever
our
what is
at
the
other
libs
layout
to
to
dependencies
of
quite a
everything
the
https://git-wip-us.apache.org/repos/asf?p=bigtop.git;a=blob;f=bigtop-packages/src/common/spark/do-component-build;h=428540e0f6aa56cd7e78eb1c831aa7fe9496a08f;hb=master
"
Evan Chan <ev@ooyala.com>,"Wed, 26 Feb 2014 09:31:59 -0800",Re: [DISCUSS] Necessity of Maven *and* SBT Build in Spark,dev@spark.apache.org,"I'd like to propose the following way to move forward, based on the
comments I've seen:

might work on if I have time is SPARK-681 which might remove the giant
fastutil dependency (~15MB by itself).

2.  Take an intermediate step by having only ONE source of truth
w.r.t. dependencies and versions.  This means either:
   a)  Using a maven POM as the spec for dependencies, Hadoop version,
etc.   Then, use sbt-pom-reader to import it.
   b)  Using the build.scala as the spec, and ""sbt make-pom"" to
generate the pom.xml for the dependencies

    The idea is to remove the pain and errors associated with manual
translation of dependency specs from one system to another, while
still maintaining the things which are hard to translate (plugins).





-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

"
Patrick Wendell <pwendell@gmail.com>,"Wed, 26 Feb 2014 09:42:11 -0800",Re: [DISCUSS] Necessity of Maven *and* SBT Build in Spark,dev@spark.apache.org,"@mridul - As far as I know both Maven and Sbt use fairly similar
processes for building the assembly/uber jar. We actually used to
package spark with sbt and there were no specific issues we
encountered and AFAIK sbt respects versioning of transitive
depe"
Sandy Ryza <sandy.ryza@cloudera.com>,"Wed, 26 Feb 2014 10:30:46 -0800",Re: [DISCUSS] Necessity of Maven *and* SBT Build in Spark,dev@spark.apache.org,"@patrick - It seems like my point about being able to inherit the root pom
was addressed and there's a way to handle this.

The larger point I meant to make is that Maven is by far the most common
build tool in projects that are likely to share contributo"
Mark Grover <grover.markgrover@gmail.com>,"Wed, 26 Feb 2014 10:38:29 -0800",Re: [DISCUSS] Necessity of Maven *and* SBT Build in Spark,dev@spark.apache.org,"Hi Patrick,
And, to pile on what Sandy said. In my opinion, it's definitely more than
just a matter of convenience. My comment below applies both to distribution
builders but also people who have their own internal ""distributions"" (a few
examples of which we have already seen on this thread already).

If one has to ensure consistent and harmonized versions of dependencies
(whether they are being built as a part of the distribution e.g. zookeeper
or pulled in transitively e.g. jersey), inheriting a root pom is the only
sane way I know of doing so. It's really painful and error prone for a
packager wanting to bump up jersey version for the entire stack, to have to
bump up the version in a root pom for all maven projects but have to also
go to ant's build properties file for all ant based projects and possibly
sbt's build properties file to bump up the version there. Now, it was
suggested that sbt can read such a pom file with use of a plugin and that
would work for me but I personally don't think the other alternative of
parsing out the pom file in scala would fly all that much.

And then, of course, there is this subjective point of people being very
familiar with maven as compared to sbt, it having a larger community base
and there is something to be said for that.

Mark



"
Mark Hamstra <mark@clearstorydata.com>,"Wed, 26 Feb 2014 10:54:12 -0800",Re: [DISCUSS] Necessity of Maven *and* SBT Build in Spark,dev@spark.apache.org,"Evan,

Have you actually tried to build Spark using its POM file and sbt-pom-reader?
 I just made a first, naive attempt, and I'm still sorting through just
what this did and didn't produce.  It looks like the basic jar files are at
least very close to correct, and may be just fine, but that building the
assembly jars failed completely.

It's not completely obvious to me how to proceed with what sbt-pom-reader
produces in order build the assemblies, run the test suites, etc., so I'm
wondering if you have already worked out what that requires?



"
Koert Kuipers <koert@tresata.com>,"Wed, 26 Feb 2014 13:59:29 -0500",Re: [DISCUSS] Necessity of Maven *and* SBT Build in Spark,dev@spark.apache.org,"i dont buy the argument that we should use it because its the most common.


if all we would do is use what is most common then we should switch to
java, svn and maven



"
Sean Owen <sowen@cloudera.com>,"Wed, 26 Feb 2014 19:11:40 +0000",Re: [DISCUSS] Necessity of Maven *and* SBT Build in Spark,dev@spark.apache.org,"I also favor Maven. I don't the the logic is ""because it's common"". As
Sandy says, it's because of the things that brings: more plugins,
easier to consume by more developers, etc. These are, however, just
some reasons 'for', and have to be considered against the other pros
and cons.

The choice of build tools affects users in a way that choice of, say,
version control does not.  As long as a usable pom pops out somehow,
and artifacts are on Maven Central, I think all users will be happy.
Whether SBT or Maven is the single source of truth is not that
important (and yes there should be only one).

What is the benefit to SBT? It seems like it's mostly replicating
Maven functionality. I don't doubt it, just wondering. Is it just that
it's in Scala too? They seem pretty equivalent.

Don't underestimate the power of Maven plugins though... especially
when trying to make all the release artifacts and push it! There are
known incantations for this, hard won by years of anguish with Maven.

My hunch is it will come down to whether it's easier to make the pom
from SBT, or the SBT build from a pom.
--
Sean Owen | Director, Data Science | London



"
Evan Chan <ev@ooyala.com>,"Wed, 26 Feb 2014 11:34:56 -0800",Re: [DISCUSS] Necessity of Maven *and* SBT Build in Spark,"""dev@spark.apache.org"" <dev@spark.apache.org>","Mark,

No, I haven't tried this myself yet  :-p   Also I would expect that
sbt-pom-reader does not do assemblies at all .... because that is an
SBT plugin, so we would still need code to include sbt-assembly.
There is also the trick question of how to include the assembly stuff
into sbt-pom-reader generated projects.  So, needs much more
investigation.....

My hunch is that it's easier to generate the pom from SBT (make-pom)
than the other way around.




-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

"
Mark Hamstra <mark@clearstorydata.com>,"Wed, 26 Feb 2014 11:55:43 -0800",Re: [DISCUSS] Necessity of Maven *and* SBT Build in Spark,dev@spark.apache.org,"Yes, but the POM generated in that fashion is only sufficient for linking
with Spark, not for building Spark or serving as a basis from which to
build a customized Spark with Maven.  So, starting from SparkBuild.scala
and generating a POM with make-pom, those who wish to build a customized
Spark with Maven will have to figure out how to add various Maven plugins
and other stuff to the generated POM to actually have something useful.
 Going the other way, starting from a POM that is sufficient to build Spark
and generating an SBT build with sbt-pom-reader, the Maven plugins in the
POM appear to be ignored cleanly, but then the developer wishing to build
Spark using SBT has the burden of figuring out how to add the equivalent of
the Maven plugins in order to build the assemblies, among other things.
 Neither way looks completely obvious to me to do programmatically.  Either
should be do-able given sufficient development and maintenance resources,
but that could be a pretty heavy commitment (and when Josh Suereth says wrt
to sbt-pom-reader that mapping maven plugins into sbt is practically a
failed task, I have every expectation that generating a completely
satisfactory SBT build from a Maven build would be quite challenging.)



"
Evan Chan <ev@ooyala.com>,"Wed, 26 Feb 2014 12:04:29 -0800",Re: [DISCUSS] Necessity of Maven *and* SBT Build in Spark,"""dev@spark.apache.org"" <dev@spark.apache.org>","Can't maven pom's include other ones?  So what if we remove the
artifact specs from the main pom, have them generated by sbt make-pom,
and include the generated file in the main pom.xml?    I guess, just
trying to figure out how much this would help (it seems at least it
would remove the issue of maintaining and translating dependencies and
exclusions).   If the burden of maintaining the plugins turns out to
be the heavier commitment then maybe it's not worth it.




-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

"
Koert Kuipers <koert@tresata.com>,"Wed, 26 Feb 2014 15:41:56 -0500",Re: [DISCUSS] Necessity of Maven *and* SBT Build in Spark,dev@spark.apache.org,"yes. the Build.scala file behaves like a configuration file mostly, but
because it is scala you can use the full power of a real language when
needed. also i found writing sbt plugins doable (but not easy).



"
Nathan Kronenfeld <nkronenfeld@oculusinfo.com>,"Wed, 26 Feb 2014 16:33:59 -0500",Re: [DISCUSS] Necessity of Maven *and* SBT Build in Spark,dev@spark.apache.org,"

There's nothing wrong with ""because it's more common"" as a reason - it just
isn't the strongest argument.

I tend to use both - I use sbt when trying to fix syntax problems or test
failures, since it gives me a quicker turn-around in the compiler phase,
but I switch to maven once I have those fixed and need to do a real build -
I find maven quicker and more reliable there (though the later is probably
more due to my own unfamiliarity with sbt)

So my vote would definitely be, if we can find some way of reliably keeping
both, I would very much prefer to do so.
"
Patrick Wendell <pwendell@gmail.com>,"Wed, 26 Feb 2014 14:37:46 -0800",[IMPORTANT] Github/jenkins migration,dev@spark.apache.org,"Hey All,

The github incubator-spark mirror has been migrated to [1] by Apache
infra and we've migrated Jenkins to reflect the new changes. This
means the existing ""incubator-spark"" mirror is becoming outdated and
no longer correctly displays pull request diff's.

We've asked apache infra to see if they can migrate existing pull
requests to incubator-spark. However since this relies on coordinating
with github, I'm not entirely sure whether they can do this or what
the timeline would be.

In the mean time it would be good for people to open new pull requests
against [1]. For pull requests that were *just* about to be merged, we
can go manually merge them, but ones that require feedback and more
rounds of testing will need to be done on the new one since
incubator-spark is now out of date.

Sorry about this inconvenience, it is a one-time transition and we
won't ever have to do it again.

[1] https://github.com/apache/spark

- Patrick

"
Nan Zhu <zhunanmcgill@gmail.com>,"Wed, 26 Feb 2014 17:45:39 -0500",Re: [IMPORTANT] Github/jenkins migration,dev@spark.apache.org," Hi, Patrick,

How to deal with the active pull requests in the old repository?

The contributors have to do something?

Best,

-- 
Nan Zhu


Hey All,

The github incubator-spark mirror has been migrated to [1] by Apache
infra and we've migrated Jenkins to reflect the new changes. This
means the existing ""incubator-spark"" mirror is becoming outdated and
no longer correctly displays pull request diff's.

We've asked apache infra to see if they can migrate existing pull
requests to incubator-spark. However since this relies on coordinating
with github, I'm not entirely sure whether they can do this or what
the timeline would be.

In the mean time it would be good for people to open new pull requests
against [1]. For pull requests that were *just* about to be merged, we
can go manually merge them, but ones that require feedback and more
rounds of testing will need to be done on the new one since
incubator-spark is now out of date.

Sorry about this inconvenience, it is a one-time transition and we
won't ever have to do it again.

[1] https://github.com/apache/spark

- Patrick
"
Patrick Wendell <pwendell@gmail.com>,"Wed, 26 Feb 2014 14:56:00 -0800",Re: [IMPORTANT] Github/jenkins migration,dev@spark.apache.org,"Sorry if this wasn't clear - If you are in the middle of a review
close it and re-open it in against [1]. The reason is we can't test
your changes against incubator-spark because it no longer exists.

[1] https://github.com/apache/spark

- Patrick


"
Koert Kuipers <koert@tresata.com>,"Wed, 26 Feb 2014 18:51:22 -0500",Re: [IMPORTANT] Github/jenkins migration,dev@spark.apache.org,"github is not aware of the new repo being a ""base-fork"", so its not easy to
re-point pull requests. i am guessing it didnt get cloned from the
incubator spark one?



"
Patrick Wendell <pwendell@gmail.com>,"Wed, 26 Feb 2014 16:24:08 -0800",Re: [IMPORTANT] Github/jenkins migration,dev@spark.apache.org,"You need to fork the new apache repository.

1. Fork https://github.com/apache/spark/ in github
2. Add your own fork as a remote in your local git
===> git remote add apache-pwendell git@github.com:pwendell/spark.git
3. Push your local branch the fork on github.
4. Make a pull request from your fork on github to apache.

Because the repo has migrated, anyone who wants to contribute will
need to fork the new repo at some point...

- Patrick


"
Koert Kuipers <koert@tresata.com>,"Wed, 26 Feb 2014 21:26:21 -0500",Re: [IMPORTANT] Github/jenkins migration,dev@spark.apache.org,"Thanks

"
Mridul Muralidharan <mridul@gmail.com>,"Thu, 27 Feb 2014 08:39:00 +0530",Re: [DISCUSS] Necessity of Maven *and* SBT Build in Spark,dev@spark.apache.org,"
Slightly longish ...

The assembled jar, generated via sbt broke all over the place while I was
adding yarn support in 0.6 - and I had to fix sbt project a fair bit to get
it to work : we need the assembled jar to submit a yarn job.

When I finally submitted those changes to 0.7, it broke even more - since
dependencies changed : someone else had thankfully already added maven
support by then - which worked remarkably well out of the box (with some
minor tweaks) !

In theory, they might be expected to work the same, but practically they
did not : as I mentioned,  it must just have been luck that maven worked
that well; but given multiple past nasty experiences with sbt, and the fact
that it does not bring anything compelling or new in contrast, I am fairly
against the idea of using only sbt - inspite of maven being unintuitive at
times.

Regards,
Mridul

sbt
dependencies.
jars
and
resolution
tool.
be
of
think
switched to
way
shaded
which
not
to
declare
as
is
koert@tresata.com>
use
cos@apache.org
look
was
are
stack it
are
need
similar
sections
jar-hell
classes
conflict
via
difference?
which is
https://git-wip-us.apache.org/repos/asf?p=bigtop.git;a=blob;f=bigtop-packages/src/common/spark/do-component-build;h=428540e0f6aa56cd7e78eb1c831aa7fe9496a08f;hb=master
"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 05:58:56 +0000 (UTC)",,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/6#issuecomment-36212896
  
     Build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 05:58:56 +0000 (UTC)",,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/6#issuecomment-36212897
  
    Build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 05:59:06 +0000 (UTC)",,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/6#issuecomment-36212905
  
     Build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
Bryn Keller <xoltar@xoltar.org>,"Wed, 26 Feb 2014 22:04:06 -0800",How to run a single test suite?,dev@spark.apache.org,"Hi Folks,

I've tried using ""sbt test-only '*PairRDDFunctionsSuite'"" to run only that
test suite, which is what I think is supposed to work with ScalaTest. I
have also tried the variant with the fully qualified name spelled out as
well. No matter what I try, it always runs *all* the test suites, which
makes it very tedious to do unit testing. What am I doing wrong?

Thanks,
Bryn
"
Reynold Xin <rxin@databricks.com>,"Wed, 26 Feb 2014 22:05:16 -0800",Re: How to run a single test suite?,dev@spark.apache.org,"You put your quotes in the wrong place. See
https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools




"
Bryn Keller <xoltar@xoltar.org>,"Wed, 26 Feb 2014 22:13:33 -0800",Re: How to run a single test suite?,dev@spark.apache.org,"Thanks, that was it!



"
asfgit <git@git.apache.org>,"Thu, 27 Feb 2014 06:24:29 +0000 (UTC)",[GitHub] spark pull request: Updated link for pyspark examples in docs,dev@spark.apache.org,"Github user asfgit closed the pull request at:

    https://github.com/apache/spark/pull/22


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
jyotiska <git@git.apache.org>,"Thu, 27 Feb 2014 06:27:29 +0000 (UTC)",[GitHub] spark pull request: Updated more links in documentation,dev@spark.apache.org,"GitHub user jyotiska opened a pull request:

    https://github.com/apache/spark/pull/23

    Updated more links in documentation

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/jyotiska/spark pyspark_docs2

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/spark/pull/23.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #23
    
----
commit dc94c4ad2ab01cb3c7133bf0b2c3de3ead6f59eb
Author: Jyotiska NK <jyotiska123@gmail.com>
Date:   2014-02-27T06:19:22Z

    Updated links in README.md

commit cc4c10e839fb39473effd9dca68285095f76bfc3
Author: Jyotiska NK <jyotiska123@gmail.com>
Date:   2014-02-27T06:23:59Z

    Updated links in index.md

commit 7f681a700797ad2359b0feaaff5f022703a4984e
Author: Jyotiska NK <jyotiska123@gmail.com>
Date:   2014-02-27T06:25:24Z

    Updated links in java programming guide

commit ad416f29729af1d13a7a615d2ff24e85bc9f7b3d
Author: Jyotiska NK <jyotiska123@gmail.com>
Date:   2014-02-27T06:26:26Z

    Updated links in scala programming guide

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 06:27:38 +0000 (UTC)",,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/6#issuecomment-36214461
  
    Build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 06:27:38 +0000 (UTC)",,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/6#issuecomment-36214463
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12905/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 06:28:51 +0000 (UTC)",[GitHub] spark pull request: Updated more links in documentation,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/23#issuecomment-36214522
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 06:28:51 +0000 (UTC)",[GitHub] spark pull request: Updated more links in documentation,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/23#issuecomment-36214523
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 06:29:23 +0000 (UTC)",[GitHub] spark pull request: Updated more links in documentation,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/23#issuecomment-36214553
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
colorant <git@git.apache.org>,"Thu, 27 Feb 2014 06:49:00 +0000 (UTC)",[GitHub] spark pull request: Show Master status on UI page,dev@spark.apache.org,"GitHub user colorant opened a pull request:

    https://github.com/apache/spark/pull/24

    Show Master status on UI page

    For standalone HA mode, A status is useful to identify the current master, already in json format too.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/colorant/spark status

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/spark/pull/24.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #24
    
----
commit df630b3fa2c233d1113ae7d47da3c27c7eb6ca0e
Author: Raymond Liu <raymond.liu@intel.com>
Date:   2014-02-27T02:55:14Z

    Show Master status on UI page

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Thu, 27 Feb 2014 06:52:55 +0000 (UTC)",[GitHub] spark pull request: Remove references to ClusterScheduler (SPARK-1...,dev@spark.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/spark/pull/9#issuecomment-36215860
  
    Thanks, merged into master.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 06:53:51 +0000 (UTC)",[GitHub] spark pull request: Show Master status on UI page,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/24#issuecomment-36215905
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 06:53:51 +0000 (UTC)",[GitHub] spark pull request: Show Master status on UI page,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/24#issuecomment-36215906
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 06:54:00 +0000 (UTC)",[GitHub] spark pull request: Show Master status on UI page,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/24#issuecomment-36215913
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Thu, 27 Feb 2014 06:54:24 +0000 (UTC)",[GitHub] spark pull request: Updated more links in documentation,dev@spark.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/spark/pull/23#issuecomment-36215928
  
    I think this is redundant with #2


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Thu, 27 Feb 2014 06:54:50 +0000 (UTC)",,dev@spark.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/spark/pull/6#issuecomment-36215951
  
    Thanks @ScrapCodes looks good.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Thu, 27 Feb 2014 06:55:14 +0000 (UTC)",,dev@spark.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/spark/pull/6#issuecomment-36215972
  
    hmm... apears it does not merge cleanly


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 06:57:37 +0000 (UTC)",[GitHub] spark pull request: Updated more links in documentation,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/23#issuecomment-36216060
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12906/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 06:57:37 +0000 (UTC)",[GitHub] spark pull request: Updated more links in documentation,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/23#issuecomment-36216059
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
witgo <git@git.apache.org>,"Thu, 27 Feb 2014 06:59:11 +0000 (UTC)","[GitHub] spark pull request: SPARK-1125: When using a http proxy,the maven ...",dev@spark.apache.org,"GitHub user witgo opened a pull request:

    https://github.com/apache/spark/pull/25

    SPARK-1125: When using a http proxy,the maven build error for Spark Examples

    building with maven When using a http proxy, throw Failure to find org.eclipse.paho:mqtt-client:jar:0.4.0 in https://repository.apache.org/content/repositories/releases was cached in the local repository, resolution will not be reattempted until the update interval of apache-repo has elapsed or updates are forced

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/witgo/spark SPARK-1125

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/spark/pull/25.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #25
    
----
commit 8faa8b236b97b595d56a5c8646a2b282c887d75d
Author: liguoqiang <liguoqiang@rd.tuan800.com>
Date:   2014-02-26T07:24:06Z

    SPARK-1125: When using a http proxy,the maven build error for Spark Examples
    Failure to find org.eclipse.paho:mqtt-client:jar:0.4.0 in https://repository.apache.org/content/repositories/releases was cached in the local repository, resolution will not be reattempted until the update interval of apache-repo has elapsed or updates are forced

commit 391c62fdcf4aa198059952c50b6982c411e63a71
Author: liguoqiang <liguoqiang@rd.tuan800.com>
Date:   2014-02-27T02:34:46Z

    Merge branch 'master' of github.com:witgo/spark

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 07:03:50 +0000 (UTC)","[GitHub] spark pull request: SPARK-1125: When using a http proxy,the maven ...",dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/25#issuecomment-36216306
  
    Can one of the admins verify this patch?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 07:03:58 +0000 (UTC)",,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/6#issuecomment-36216310
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 07:03:58 +0000 (UTC)",,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/6#issuecomment-36216313
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 07:03:59 +0000 (UTC)","[GitHub] spark pull request: SPARK-1125: When using a http proxy,the maven ...",dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/25#issuecomment-36216317
  
    Can one of the admins verify this patch?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 07:04:07 +0000 (UTC)",,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/6#issuecomment-36216320
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
jyotiska <git@git.apache.org>,"Thu, 27 Feb 2014 07:11:38 +0000 (UTC)",[GitHub] spark pull request: Updated more links in documentation,dev@spark.apache.org,"Github user jyotiska commented on the pull request:

    https://github.com/apache/spark/pull/23#issuecomment-36216611
  
    Right, didn't notice that PR.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 07:22:59 +0000 (UTC)",[GitHub] spark pull request: Show Master status on UI page,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/24#issuecomment-36217111
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12907/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 07:22:59 +0000 (UTC)",[GitHub] spark pull request: Show Master status on UI page,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/24#issuecomment-36217109
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
asfgit <git@git.apache.org>,"Thu, 27 Feb 2014 07:23:10 +0000 (UTC)",[GitHub] spark pull request: Remove references to ClusterScheduler (SPARK-1...,dev@spark.apache.org,"Github user asfgit closed the pull request at:

    https://github.com/apache/spark/pull/9


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
srowen <git@git.apache.org>,"Thu, 27 Feb 2014 07:23:42 +0000 (UTC)","[GitHub] spark pull request: SPARK-1125: When using a http proxy,the maven ...",dev@spark.apache.org,"Github user srowen commented on the pull request:

    https://github.com/apache/spark/pull/25#issuecomment-36217136
  
    I'm still confused why you are posting this pull request. You found this was a problem with your local proxy. This change does not fix that at all. Nor would any change that can be committed to the general project. It's specific to your environment.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 07:32:27 +0000 (UTC)",,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/6#issuecomment-36217566
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 07:32:27 +0000 (UTC)",,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/6#issuecomment-36217568
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12908/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
ScrapCodes <git@git.apache.org>,"Thu, 27 Feb 2014 07:34:33 +0000 (UTC)",,dev@spark.apache.org,"Github user ScrapCodes commented on the pull request:

    https://github.com/apache/spark/pull/6#issuecomment-36217654
  
    Rebased !!


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Thu, 27 Feb 2014 07:41:20 +0000 (UTC)",,dev@spark.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/spark/pull/6#issuecomment-36217973
  
    Merged !!


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Thu, 27 Feb 2014 07:43:47 +0000 (UTC)",[GitHub] spark pull request: [SPARK-1089] fix the regression problem on ADD...,dev@spark.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/spark/pull/13#issuecomment-36218095
  
    Thanks I've merged this into master and 0.9. Actually didn't notice tests hadn't gone through. We can revert if there are any issues. Jenkins, test this please.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 07:43:56 +0000 (UTC)",[GitHub] spark pull request: [SPARK-1089] fix the regression problem on ADD...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/13#issuecomment-36218102
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 07:43:56 +0000 (UTC)",[GitHub] spark pull request: [SPARK-1089] fix the regression problem on ADD...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/13#issuecomment-36218101
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 07:44:03 +0000 (UTC)",[GitHub] spark pull request: [SPARK-1089] fix the regression problem on ADD...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/13#issuecomment-36218110
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
witgo <git@git.apache.org>,"Thu, 27 Feb 2014 07:49:28 +0000 (UTC)","[GitHub] spark pull request: SPARK-1125: When using a http proxy,the maven ...",dev@spark.apache.org,"Github user witgo commented on the pull request:

    https://github.com/apache/spark/pull/25#issuecomment-36218375
  
    Perhaps no one encountered the same problem.Well, let me close this PR.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
witgo <git@git.apache.org>,"Thu, 27 Feb 2014 07:49:28 +0000 (UTC)","[GitHub] spark pull request: SPARK-1125: When using a http proxy,the maven ...",dev@spark.apache.org,"Github user witgo closed the pull request at:

    https://github.com/apache/spark/pull/25


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Thu, 27 Feb 2014 07:52:10 +0000 (UTC)",[GitHub] spark pull request: Show Master status on UI page,dev@spark.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/spark/pull/24#issuecomment-36218515
  
    Thanks, merged into master. I also made sure enumerations print nicely because I wasn't sure they do... turns out they do.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Thu, 27 Feb 2014 07:52:47 +0000 (UTC)",[GitHub] spark pull request: [SPARK-1100] prevent Spark from overwriting di...,dev@spark.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/spark/pull/11#issuecomment-36218547
  
    Jenkins, test this please.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Thu, 27 Feb 2014 07:53:28 +0000 (UTC)",[GitHub] spark pull request: [SPARK-1100] prevent Spark from overwriting di...,dev@spark.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/spark/pull/11#discussion_r10114812
  
    --- Diff: core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala ---
    @@ -77,7 +74,6 @@ class PairRDDFunctions[K: ClassTag, V: ClassTag](self: RDD[(K, V)])
           partitioner: Partitioner,
           mapSideCombine: Boolean = true,
           serializerClass: String = null): RDD[(K, C)] = {
    -    require(mergeCombiners != null, ""mergeCombiners must be defined"") // required as of Spark 0.9.0
    --- End diff --
    
    why did you remove this? We want to require this here.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 07:53:57 +0000 (UTC)",[GitHub] spark pull request: [SPARK-1100] prevent Spark from overwriting di...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/11#issuecomment-36218602
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 07:54:04 +0000 (UTC)",[GitHub] spark pull request: [SPARK-1100] prevent Spark from overwriting di...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/11#issuecomment-36218612
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Thu, 27 Feb 2014 07:56:11 +0000 (UTC)",[GitHub] spark pull request: [SPARK-1100] prevent Spark from overwriting di...,dev@spark.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/spark/pull/11#discussion_r10114841
  
    --- Diff: core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala ---
    @@ -618,10 +619,6 @@ class PairRDDFunctions[K: ClassTag, V: ClassTag](self: RDD[(K, V)])
             attemptNumber)
           val hadoopContext = newTaskAttemptContext(wrappedConf.value, attemptId)
           val format = outputFormatClass.newInstance
    -      format match {
    --- End diff --
    
    ah I think somehow you may have created a diff or something based on some older code... this also got updated recently and I think we want to keep it.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 08:00:56 +0000 (UTC)",[GitHub] spark pull request: [SPARK-1100] prevent Spark from overwriting di...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/11#issuecomment-36218942
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 08:13:07 +0000 (UTC)",[GitHub] spark pull request: [SPARK-1089] fix the regression problem on ADD...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/13#issuecomment-36219532
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12909/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 08:13:07 +0000 (UTC)",[GitHub] spark pull request: [SPARK-1089] fix the regression problem on ADD...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/13#issuecomment-36219531
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 08:27:54 +0000 (UTC)",[GitHub] spark pull request: [SPARK-1100] prevent Spark from overwriting di...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/11#issuecomment-36220356
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12910/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 08:27:54 +0000 (UTC)",[GitHub] spark pull request: [SPARK-1100] prevent Spark from overwriting di...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/11#issuecomment-36220353
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
asfgit <git@git.apache.org>,"Thu, 27 Feb 2014 08:32:25 +0000 (UTC)",[GitHub] spark pull request: [SPARK-1089] fix the regression problem on ADD...,dev@spark.apache.org,"Github user asfgit closed the pull request at:

    https://github.com/apache/spark/pull/13


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
asfgit <git@git.apache.org>,"Thu, 27 Feb 2014 08:32:25 +0000 (UTC)",,dev@spark.apache.org,"Github user asfgit closed the pull request at:

    https://github.com/apache/spark/pull/6


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
asfgit <git@git.apache.org>,"Thu, 27 Feb 2014 08:32:24 +0000 (UTC)",[GitHub] spark pull request: Show Master status on UI page,dev@spark.apache.org,"Github user asfgit closed the pull request at:

    https://github.com/apache/spark/pull/24


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
ngbinh <git@git.apache.org>,"Thu, 27 Feb 2014 08:36:19 +0000 (UTC)",[GitHub] spark pull request: Vagrant support for Spark,dev@spark.apache.org,"GitHub user ngbinh opened a pull request:

    https://github.com/apache/spark/pull/26

    Vagrant support for Spark

    This PR uses Vagrant to create a clusters of three VMs, one master and two workers. It allows running/testing Spark Cluster mode on one machine.
    
    My initial goal is to set up a stand alone cluster for now but I am open to others (YARN and Mesos) later.
    
    Note that this is a WIP.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/ngbinh/spark master

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/spark/pull/26.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #26
    
----
commit 4546248b5cbc445d636ece091e3a5463c4493f8a
Author: Binh Nguyen <ngbinh@gmail.com>
Date:   2014-02-27T07:30:08Z

    Initial import of Vagrant scripts and shells

commit 42907d051923ff815da7e59608d867df6ab859ca
Author: Binh Nguyen <ngbinh@gmail.com>
Date:   2014-02-27T08:25:05Z

    Shell scripts to copy public key to Vagrant nodes

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 08:38:51 +0000 (UTC)",[GitHub] spark pull request: Vagrant support for Spark,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/26#issuecomment-36220958
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 08:38:52 +0000 (UTC)",[GitHub] spark pull request: Vagrant support for Spark,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/26#issuecomment-36220959
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 08:38:58 +0000 (UTC)",[GitHub] spark pull request: Vagrant support for Spark,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/26#issuecomment-36220966
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
kayousterhout <git@git.apache.org>,"Thu, 27 Feb 2014 08:49:50 +0000 (UTC)",[GitHub] spark pull request: [SPARK-979] Randomize order of offers.,dev@spark.apache.org,"GitHub user kayousterhout opened a pull request:

    https://github.com/apache/spark/pull/27

    [SPARK-979] Randomize order of offers.

    This commit randomizes the order of resource offers to avoid scheduling
    all tasks on the same small set of machines.
    
    This is a much simpler solution to SPARK-979 than #7.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/kayousterhout/spark-1 randomize

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/spark/pull/27.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #27
    
----
commit 435d817424d3c9f3d900c65164ee5ed49b037b26
Author: Kay Ousterhout <kayousterhout@gmail.com>
Date:   2014-02-27T08:45:17Z

    [SPARK-979] Randomize order of offers.
    
    This commit randomizes the order of resource offers to avoid scheduling
    all tasks on the same small set of machines.

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 08:53:54 +0000 (UTC)",[GitHub] spark pull request: [SPARK-979] Randomize order of offers.,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/27#issuecomment-36221889
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 08:53:59 +0000 (UTC)",[GitHub] spark pull request: [SPARK-979] Randomize order of offers.,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/27#issuecomment-36221897
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 09:07:12 +0000 (UTC)",[GitHub] spark pull request: [SPARK-1146] Vagrant support for Spark,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/26#issuecomment-36222794
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 09:07:13 +0000 (UTC)",[GitHub] spark pull request: [SPARK-1146] Vagrant support for Spark,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/26#issuecomment-36222795
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12911/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
sryza <git@git.apache.org>,"Thu, 27 Feb 2014 09:20:30 +0000 (UTC)",[GitHub] spark pull request: SPARK-1032. If Yarn app fails before registeri...,dev@spark.apache.org,"GitHub user sryza opened a pull request:

    https://github.com/apache/spark/pull/28

    SPARK-1032. If Yarn app fails before registering, app master stays aroun...

    ...d long after
    
    This reopens https://github.com/apache/incubator-spark/pull/648 against the new repo.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/sryza/spark sandy-spark-1032

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/spark/pull/28.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #28
    
----
commit 5953f5035edc2e0d7f3ccac0d127874ebc3a221d
Author: Sandy Ryza <sandy@cloudera.com>
Date:   2014-02-25T04:38:43Z

    SPARK-1032. If Yarn app fails before registering, app master stays around long after

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
sryza <git@git.apache.org>,"Thu, 27 Feb 2014 09:21:33 +0000 (UTC)",,dev@spark.apache.org,"GitHub user sryza opened a pull request:

    https://github.com/apache/spark/pull/29


    This reopens https://github.com/apache/incubator-spark/pull/538 against the new repo

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/sryza/spark sandy-spark-1051

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/spark/pull/29.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #29
    
----
commit f2a3c24ebbc6f89be5f88c4a3f34a3c05ff8a5c2
Author: Sandy Ryza <sandy@cloudera.com>
Date:   2014-02-06T01:43:33Z

    SPARK-1051 second way

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
sryza <git@git.apache.org>,"Thu, 27 Feb 2014 09:22:35 +0000 (UTC)",[GitHub] spark pull request: SPARK-1004. PySpark on YARN,dev@spark.apache.org,"GitHub user sryza opened a pull request:

    https://github.com/apache/spark/pull/30

    SPARK-1004.  PySpark on YARN

    This reopens https://github.com/apache/incubator-spark/pull/640 against the new repo

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/sryza/spark sandy-spark-1004

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/spark/pull/30.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #30
    
----
commit e49ff667154de988a1cb58d90c9743c6c24ef5bc
Author: Josh Rosen <joshrosen@apache.org>
Date:   2014-01-24T18:19:58Z

    Automatically set Yarn env vars in PySpark (SPARK-1030).

commit 59ac972026a7600fded49d906ef27bbb017fc9d2
Author: Josh Rosen <joshrosen@apache.org>
Date:   2014-01-25T23:28:56Z

    WIP towards PySpark on YARN:
    
      PySpark Python libraries are added to the PYTHONPATH.
    
    - Add a Makefile for generating a ""fat zip"" that contains PySpark's
      Python dependencies.  This is a bit of a hack and I'd be open to
      better packaging tools, but this doesn't require any extra Python
      libraries.  This use case doesn't seem to be well-addressed by the
      existing Python packaging tools: there are plenty of tools to package
      complete Python environments (such as pyinstaller and virtualenv) or
      to bundle *individual* libraries (e.g. distutils), but few to generate
      portable fat zips or eggs.
    
    This hasn't been tested with YARN and may not actually compile.

commit 54bd8c0aec51d5d5cb24d6453dea2fb627db05cd
Author: Josh Rosen <joshrosen@apache.org>
Date:   2014-02-19T06:27:21Z

    Add missing setup.py file for PySpark.

commit 514b2d0cfc8995b86186d02aebf61500d25df7db
Author: Sandy Ryza <sandy@cloudera.com>
Date:   2014-02-24T07:06:42Z

    Improvements

commit ee3cc204dcabd7d092e3d6ed205e01c5deffc7ca
Author: Sandy Ryza <sandy@cloudera.com>
Date:   2014-02-24T07:26:01Z

    Don't set SPARK_JAR

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 09:23:51 +0000 (UTC)",[GitHub] spark pull request: SPARK-1004. PySpark on YARN,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/30#issuecomment-36223978
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 09:23:51 +0000 (UTC)",,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/29#issuecomment-36223980
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 09:23:52 +0000 (UTC)",,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/29#issuecomment-36223981
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 09:23:51 +0000 (UTC)",[GitHub] spark pull request: SPARK-1004. PySpark on YARN,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/30#issuecomment-36223976
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 09:23:52 +0000 (UTC)",[GitHub] spark pull request: SPARK-1032. If Yarn app fails before registeri...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/28#issuecomment-36223982
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 09:24:01 +0000 (UTC)",[GitHub] spark pull request: SPARK-1004. PySpark on YARN,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/30#issuecomment-36223993
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 09:23:53 +0000 (UTC)",[GitHub] spark pull request: SPARK-1032. If Yarn app fails before registeri...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/28#issuecomment-36223986
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 09:24:03 +0000 (UTC)",[GitHub] spark pull request: SPARK-1032. If Yarn app fails before registeri...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/28#issuecomment-36223996
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 09:24:02 +0000 (UTC)",,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/29#issuecomment-36223994
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 09:26:45 +0000 (UTC)",[GitHub] spark pull request: [SPARK-979] Randomize order of offers.,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/27#issuecomment-36224189
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 09:26:45 +0000 (UTC)",[GitHub] spark pull request: [SPARK-979] Randomize order of offers.,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/27#issuecomment-36224190
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12912/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
Qiuzhuang Lian <qiuzhuang.lian@gmail.com>,"Thu, 27 Feb 2014 17:28:21 +0800",question on removeRdd method in BlockManagerMasterActor.scala,dev@spark.incubator.apache.org,"Hi,

I have one question on removeRdd method in BlockManagerMasterActor.scala
about asking slave actor to remove RDD.

in this piece of code,

    Future.sequence(blockManagerInfo.values.map { bm =>
      bm.slaveActor.ask(removeMsg)(akkaTimeout).mapTo[Int]
    }.toSeq)

it asks all blockManagerInfo to remove rdd. Shouldn't we
filter blockManagerInfo so as to only pick up the BlockManagerInfo which
did contains that RDD?

I did my changes to see if making sense,

E:\projects\amplab\spark>git diff
diff --git
a/core/src/main/scala/org/apache/spark/storage/BlockManagerMasterActor.scala
b/core/src/main/scold mode 10064
4
new mode 100755
index a999d76..fccc5a9
---
a/core/src/main/scala/org/apache/spark/storage/BlockManagerMasterActor.scala
+++
b/core/src/main/scala/org/apache/spark/storage/BlockManagerMasterActor.scala
@@ -128,9 +128,15 @@ class BlockManagerMasterActor(val isLocal: Boolean,
conf: SparkConf) extends Act
     // Find all blocks for the given RDD, remove the block from both
blockLocations and
     // the blockManagerInfo that is tracking the blocks.
     val blocks = blockLocations.keys.flatMap(_.asRDDId).filter(_.rddId ==
rddId)
+    val bmInfos = new
mutable.HashSet[BlockManagerMasterActor.BlockManagerInfo]
     blocks.foreach { blockId =>
       val bms: mutable.HashSet[BlockManagerId] =
blockLocations.get(blockId)
-      bms.foreach(bm =>
blockManagerInfo.get(bm).foreach(_.removeBlock(blockId)))
+      bms.foreach{ bm =>
+        blockManagerInfo.get(bm).foreach{ bmInfo =>
+          bmInfos += bmInfo
+          bmInfo.removeBlock(blockId)
+        }
+      }
       blockLocations.remove(blockId)
     }

@@ -138,7 +144,7 @@ class BlockManagerMasterActor(val isLocal: Boolean,
conf: SparkConf) extends Act
     // The dispatcher is used as an implicit argument into the Future
sequence construction.
     import context.dispatcher
     val removeMsg = RemoveRdd(rddId)
-    Future.sequence(blockManagerInfo.values.map { bm =>
+    Future.sequence(bmInfos.map { bm =>
       bm.slaveActor.ask(removeMsg)(akkaTimeout).mapTo[Int]
     }.toSeq)
   }


Thanks,
Qiuzhuang
"
Qiuzhuang Lian <qiuzhuang.lian@gmail.com>,"Thu, 27 Feb 2014 17:32:01 +0800",Fwd: question on removeRdd method in BlockManagerMasterActor.scala,dev@spark.apache.org,"Sorry, I should send to the new dev spark address instead.


Hi,

I have one question on removeRdd method in BlockManagerMasterActor.scala
about asking slave actor to remove RDD.

in this piece of code,

    Future.sequence(blockManagerInfo.values.map { bm =>
      bm.slaveActor.ask(removeMsg)(akkaTimeout).mapTo[Int]
    }.toSeq)

it asks all blockManagerInfo to remove rdd. Shouldn't we
filter blockManagerInfo so as to only pick up the BlockManagerInfo which
did contains that RDD?

I did my changes to see if making sense,

E:\projects\amplab\spark>git diff
diff --git
a/core/src/main/scala/org/apache/spark/storage/BlockManagerMasterActor.scala
b/core/src/main/scold mode 10064
4
new mode 100755
index a999d76..fccc5a9
---
a/core/src/main/scala/org/apache/spark/storage/BlockManagerMasterActor.scala
+++
b/core/src/main/scala/org/apache/spark/storage/BlockManagerMasterActor.scala
@@ -128,9 +128,15 @@ class BlockManagerMasterActor(val isLocal: Boolean,
conf: SparkConf) extends Act
     // Find all blocks for the given RDD, remove the block from both
blockLocations and
     // the blockManagerInfo that is tracking the blocks.
     val blocks = blockLocations.keys.flatMap(_.asRDDId).filter(_.rddId ==
rddId)
+    val bmInfos = new
mutable.HashSet[BlockManagerMasterActor.BlockManagerInfo]
     blocks.foreach { blockId =>
       val bms: mutable.HashSet[BlockManagerId] =
blockLocations.get(blockId)
-      bms.foreach(bm =>
blockManagerInfo.get(bm).foreach(_.removeBlock(blockId)))
+      bms.foreach{ bm =>
+        blockManagerInfo.get(bm).foreach{ bmInfo =>
+          bmInfos += bmInfo
+          bmInfo.removeBlock(blockId)
+        }
+      }
       blockLocations.remove(blockId)
     }

@@ -138,7 +144,7 @@ class BlockManagerMasterActor(val isLocal: Boolean,
conf: SparkConf) extends Act
     // The dispatcher is used as an implicit argument into the Future
sequence construction.
     import context.dispatcher
     val removeMsg = RemoveRdd(rddId)
-    Future.sequence(blockManagerInfo.values.map { bm =>
+    Future.sequence(bmInfos.map { bm =>
       bm.slaveActor.ask(removeMsg)(akkaTimeout).mapTo[Int]
     }.toSeq)
   }


Thanks,
Qiuzhuang
"
markhamstra <git@git.apache.org>,"Thu, 27 Feb 2014 09:34:38 +0000 (UTC)",[GitHub] spark pull request: [SPARK-1146] Vagrant support for Spark,dev@spark.apache.org,"Github user markhamstra commented on the pull request:

    https://github.com/apache/spark/pull/26#issuecomment-36224757
  
    I'm bothered by the idea of vagrant, docker, ec2, and potentially other virtualization and cloud environments (EMR, etc.) all becoming supported and maintained parts of the core Spark distribution.  Where do we draw the line as to which should and should not be included?  Could they perhaps be better located in a separate project or projects similar to the idea for a separate spark-contrib project and repository?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
jyotiska <git@git.apache.org>,"Thu, 27 Feb 2014 09:44:28 +0000 (UTC)",[GitHub] spark pull request: [SPARK-1146] Vagrant support for Spark,dev@spark.apache.org,"Github user jyotiska commented on the pull request:

    https://github.com/apache/spark/pull/26#issuecomment-36225645
  
    +1 It will be better if these projects were made separate from core-spark project and grown as independent projects. This keeps the core project lean and helps to grow the ecosystem around spark.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
srowen <git@git.apache.org>,"Thu, 27 Feb 2014 09:46:54 +0000 (UTC)",[GitHub] spark pull request: [SPARK-1146] Vagrant support for Spark,dev@spark.apache.org,"Github user srowen commented on the pull request:

    https://github.com/apache/spark/pull/26#issuecomment-36225852
  
    FWIW I agree. The tendency is almost always to include a bunch of modules that are really separate, slightly-downstream projects. You could make similar arguments for even mllib. For me, my line would have things like this as separate repos and things like mllib as modules.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
ngbinh <git@git.apache.org>,"Thu, 27 Feb 2014 09:49:07 +0000 (UTC)",[GitHub] spark pull request: [SPARK-1146] Vagrant support for Spark,dev@spark.apache.org,"Github user ngbinh commented on the pull request:

    https://github.com/apache/spark/pull/26#issuecomment-36226015
  
    I agree that while they are not necessary be a part of Spark core because there is usually no direct dependencies between them. But I feel like they make Spark more accessible. 


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
jyotiska <git@git.apache.org>,"Thu, 27 Feb 2014 09:49:40 +0000 (UTC)",[GitHub] spark pull request: SPARK-1004. PySpark on YARN,dev@spark.apache.org,"Github user jyotiska commented on a diff in the pull request:

    https://github.com/apache/spark/pull/30#discussion_r10117264
  
    --- Diff: python/setup.py ---
    @@ -0,0 +1,30 @@
    +#
    +# Licensed to the Apache Software Foundation (ASF) under one or more
    +# contributor license agreements.  See the NOTICE file distributed with
    +# this work for additional information regarding copyright ownership.
    +# The ASF licenses this file to You under the Apache License, Version 2.0
    +# (the ""License""); you may not use this file except in compliance with
    +# the License.  You may obtain a copy of the License at
    +#
    +#    http://www.apache.org/licenses/LICENSE-2.0
    +#
    +# Unless required by applicable law or agreed to in writing, software
    +# distributed under the License is distributed on an ""AS IS"" BASIS,
    +# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    +# See the License for the specific language governing permissions and
    +# limitations under the License.
    +#
    +
    +from distutils.core import setup
    +
    +
    +setup(
    +    name='pyspark',
    +    version='0.9.0-incubating-SNAPSHOT',
    +    description='Python API for Spark',
    +    author='The Apache Software Foundation',
    +    author_email='user@spark.incubator.apache.org',
    +    license='Apache License 2.0',
    +    url='spark-project.org',
    +    packages=['pyspark'],
    --- End diff --
    
    I believe we should add numpy as required dependency. Also, ""incubating"" should be removed from <code>version</code> and <code>author-email</code>


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
markhamstra <git@git.apache.org>,"Thu, 27 Feb 2014 09:51:31 +0000 (UTC)",[GitHub] spark pull request: [SPARK-1146] Vagrant support for Spark,dev@spark.apache.org,"Github user markhamstra commented on the pull request:

    https://github.com/apache/spark/pull/26#issuecomment-36226183
  
    Yes, they definitely have value, but putting them directly into Spark also has costs and imposes responsibilities on the maintainers.  The question is how to get the best cost:benefit ratio.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 09:52:28 +0000 (UTC)",[GitHub] spark pull request: SPARK-1004. PySpark on YARN,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/30#issuecomment-36226247
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12913/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 09:52:28 +0000 (UTC)",[GitHub] spark pull request: SPARK-1004. PySpark on YARN,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/30#issuecomment-36226245
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 09:52:56 +0000 (UTC)",,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/29#issuecomment-36226279
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 09:52:56 +0000 (UTC)",[GitHub] spark pull request: SPARK-1032. If Yarn app fails before registeri...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/28#issuecomment-36226280
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 09:52:57 +0000 (UTC)",[GitHub] spark pull request: SPARK-1032. If Yarn app fails before registeri...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/28#issuecomment-36226282
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12915/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 09:52:56 +0000 (UTC)",,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/29#issuecomment-36226281
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12914/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
ngbinh <git@git.apache.org>,"Thu, 27 Feb 2014 09:57:25 +0000 (UTC)",[GitHub] spark pull request: [SPARK-1146] Vagrant support for Spark,dev@spark.apache.org,"Github user ngbinh commented on the pull request:

    https://github.com/apache/spark/pull/26#issuecomment-36226574
  
    I can argue that having ec2, stand alone cluster scripts inside the core repo is important for Spark adoption. 
    
    @markhamstra I agree. My feeling is the benefit is still outweigh the cost. 


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
ngbinh <git@git.apache.org>,"Thu, 27 Feb 2014 10:01:52 +0000 (UTC)",[GitHub] spark pull request: [SPARK-1146] Vagrant support for Spark,dev@spark.apache.org,"Github user ngbinh commented on the pull request:

    https://github.com/apache/spark/pull/26#issuecomment-36226895
  


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
srowen <git@git.apache.org>,"Thu, 27 Feb 2014 11:51:10 +0000 (UTC)",[GitHub] spark pull request: SPARK 1084.1 (resubmitted),dev@spark.apache.org,"GitHub user srowen opened a pull request:

    https://github.com/apache/spark/pull/31

    SPARK 1084.1 (resubmitted)

    (Ported from https://github.com/apache/incubator-spark/pull/637 )

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/srowen/spark SPARK-1084.1

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/spark/pull/31.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #31
    
----
commit b8ff8cb5ebf9ac575d1f7ef6bf9a7c4784b8a5c8
Author: Sean Owen <sowen@cloudera.com>
Date:   2014-02-27T08:01:23Z

    Replace deprecated Ant <tasks> with <target>

commit 007762ba2ffa9c3b396de95e64652935a07e74e0
Author: Sean Owen <sowen@cloudera.com>
Date:   2014-02-27T08:02:13Z

    Remove dead scaladoc links

commit 5b2fce25a9620d3e7f36c311c38c44f85c7684a8
Author: Sean Owen <sowen@cloudera.com>
Date:   2014-02-27T08:03:41Z

    Fix scaladoc invocation warning, and enable javac warnings properly, with plugin config updates

commit 254e8efb811390518298fc5532b9bbeaca66fc77
Author: Sean Owen <sowen@cloudera.com>
Date:   2014-02-27T08:04:20Z

    Fix one new style error introduced in scaladoc warning commit

commit f35b833041dcbc12b95898b7be2a43c77ec14368
Author: Sean Owen <sowen@cloudera.com>
Date:   2014-02-27T08:05:14Z

    Fix two misc javadoc problems

commit 6c4a32c2e2b9bc57d249bb72fad9fc2adc12cb69
Author: Sean Owen <sowen@cloudera.com>
Date:   2014-02-27T08:05:54Z

    Suppress warnings about legitimate unchecked array creations, or change code to avoid it

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
srowen <git@git.apache.org>,"Thu, 27 Feb 2014 11:52:08 +0000 (UTC)",[GitHub] spark pull request: SPARK-1084.2 (resubmitted),dev@spark.apache.org,"GitHub user srowen opened a pull request:

    https://github.com/apache/spark/pull/32

    SPARK-1084.2 (resubmitted)

    (Ported from https://github.com/apache/incubator-spark/pull/650 )
    
    This adds one more change though, to fix the scala version warning introduced by json4s recently.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/srowen/spark SPARK-1084.2

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/spark/pull/32.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #32
    
----
commit fa3e4d2a90c27464251bd2b25f537d920acc6d1d
Author: Sean Owen <sowen@cloudera.com>
Date:   2014-02-27T10:11:10Z

    Remove ""exclude *"" dependencies that are causing Maven warnings, and that are apparently unneeded anyway

commit 6a9820248a13d539b52deaff7798b5a27e07e531
Author: Sean Owen <sowen@cloudera.com>
Date:   2014-02-27T11:49:46Z

    Avoid scala version conflict in scalap induced by json4s dependency

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 11:53:52 +0000 (UTC)",[GitHub] spark pull request: SPARK 1084.1 (resubmitted),dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/31#issuecomment-36234486
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 11:53:52 +0000 (UTC)",[GitHub] spark pull request: SPARK 1084.1 (resubmitted),dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/31#issuecomment-36234488
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 11:53:53 +0000 (UTC)",[GitHub] spark pull request: SPARK-1084.2 (resubmitted),dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/32#issuecomment-36234483
  
    Build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 11:53:52 +0000 (UTC)",[GitHub] spark pull request: SPARK-1084.2 (resubmitted),dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/32#issuecomment-36234482
  
     Build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 11:54:01 +0000 (UTC)",[GitHub] spark pull request: SPARK-1084.2 (resubmitted),dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/32#issuecomment-36234494
  
     Build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 11:54:04 +0000 (UTC)",[GitHub] spark pull request: SPARK 1084.1 (resubmitted),dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/31#issuecomment-36234496
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 12:23:06 +0000 (UTC)",[GitHub] spark pull request: SPARK-1084.2 (resubmitted),dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/32#issuecomment-36236450
  
    Build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 12:23:06 +0000 (UTC)",[GitHub] spark pull request: SPARK-1084.2 (resubmitted),dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/32#issuecomment-36236453
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12916/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 12:23:06 +0000 (UTC)",[GitHub] spark pull request: SPARK 1084.1 (resubmitted),dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/31#issuecomment-36236452
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12917/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 12:23:06 +0000 (UTC)",[GitHub] spark pull request: SPARK 1084.1 (resubmitted),dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/31#issuecomment-36236449
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
CodingCat <git@git.apache.org>,"Thu, 27 Feb 2014 12:47:30 +0000 (UTC)",[GitHub] spark pull request: [SPARK-1100] prevent Spark from overwriting di...,dev@spark.apache.org,"Github user CodingCat commented on a diff in the pull request:

    https://github.com/apache/spark/pull/11#discussion_r10121533
  
    --- Diff: core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala ---
    @@ -618,10 +619,6 @@ class PairRDDFunctions[K: ClassTag, V: ClassTag](self: RDD[(K, V)])
             attemptNumber)
           val hadoopContext = newTaskAttemptContext(wrappedConf.value, attemptId)
           val format = outputFormatClass.newInstance
    -      format match {
    --- End diff --
    
    yeah, I think so...I just synced the code


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
CodingCat <git@git.apache.org>,"Thu, 27 Feb 2014 12:48:31 +0000 (UTC)",[GitHub] spark pull request: [SPARK-979] Randomize order of offers.,dev@spark.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/spark/pull/27#issuecomment-36238278
  
    en...it's much simpler...but randomization can just mitigate the issue with some probability? 


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
tgravescs <git@git.apache.org>,"Thu, 27 Feb 2014 13:34:29 +0000 (UTC)","[GitHub] spark pull request: Add Security to Spark - Akka, Http, Connection...",dev@spark.apache.org,"GitHub user tgravescs opened a pull request:

    https://github.com/apache/spark/pull/33

    Add Security to Spark - Akka, Http, ConnectionManager, UI use servlets

    resubmit pull request.  was https://github.com/apache/incubator-spark/pull/332.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/tgravescs/spark security-branch-0.9-with-client-rebase

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/spark/pull/33.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #33
    
----
commit f35176352b77d8a295601479879a4666501eabe8
Author: Thomas Graves <tgraves@apache.org>
Date:   2014-01-19T19:35:32Z

    Add Security to Spark - Akka, Http, ConnectionManager, UI to use servlets

commit 5721c5ac83b62afb8e8201730e4fc6bc76556e5b
Author: Thomas Graves <tgraves@apache.org>
Date:   2014-01-20T15:17:56Z

    update AkkaUtilsSuite test for the actorSelection changes, fix typos based on comments, and remove extra lines I missed in rebase from AkkaUtils

commit 2d9e23e1739f52a858c34b7e85c7625b43ad3833
Author: Thomas Graves <tgraves@apache.org>
Date:   2014-02-18T16:38:54Z

    Merge remote-tracking branch 'upstream/master' into security-branch-0.9-with-client-rebase_rework
    
    Conflicts:
    	core/src/main/scala/org/apache/spark/SparkEnv.scala
    	core/src/main/scala/org/apache/spark/network/ConnectionManager.scala
    	core/src/main/scala/org/apache/spark/ui/JettyUtils.scala
    	repl/src/main/scala/org/apache/spark/repl/ExecutorClassLoader.scala

commit 6f7ddf38d3b3f3c367df4d0b9a6be3a0bc644e1d
Author: Thomas Graves <tgraves@apache.org>
Date:   2014-02-22T16:37:03Z

    Convert SaslClient and SaslServer to scala, change spark.authenticate.ui to spark.ui.acls.enable, and fix up various
    other things from review comments

commit ed3d1c16cf9a0af6530d2c37e62fb9cdc92ddfcb
Author: Thomas Graves <tgraves@apache.org>
Date:   2014-02-22T16:50:30Z

    Add security.md

commit b514becd7a0173ebeb209c0436e3c2c9f2f40a64
Author: Thomas Graves <tgraves@apache.org>
Date:   2014-02-22T16:52:56Z

    Fix reference to config

commit ecbfb65860e4fea722537802cf036f0b505d7da9
Author: Thomas Graves <tgraves@apache.org>
Date:   2014-02-22T17:37:41Z

    Fix spacing and formatting

commit 50dd9f2438356117e749d4cbd8d0ea8c25746166
Author: Thomas Graves <tgraves@apache.org>
Date:   2014-02-22T18:35:34Z

    fix header in SecurityManager

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 13:38:52 +0000 (UTC)","[GitHub] spark pull request: Add Security to Spark - Akka, Http, Connection...",dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/33#issuecomment-36241944
  
     Build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 13:38:52 +0000 (UTC)","[GitHub] spark pull request: Add Security to Spark - Akka, Http, Connection...",dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/33#issuecomment-36241945
  
    Build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 13:39:04 +0000 (UTC)","[GitHub] spark pull request: Add Security to Spark - Akka, Http, Connection...",dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/33#issuecomment-36241960
  
     Build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 14:07:24 +0000 (UTC)","[GitHub] spark pull request: Add Security to Spark - Akka, Http, Connection...",dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/33#issuecomment-36244602
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12918/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 14:07:23 +0000 (UTC)","[GitHub] spark pull request: Add Security to Spark - Akka, Http, Connection...",dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/33#issuecomment-36244600
  
    Build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
jyotiska <git@git.apache.org>,"Thu, 27 Feb 2014 14:37:20 +0000 (UTC)",[GitHub] spark pull request: [SPARK-972] Added detailed callsite info for V...,dev@spark.apache.org,"GitHub user jyotiska opened a pull request:

    https://github.com/apache/spark/pull/34

    [SPARK-972] Added detailed callsite info for ValueError in context.py

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/jyotiska/spark pyspark_code

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/spark/pull/34.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #34
    
----
commit a6bf4cd0e7ed86a4ca19d4d240ae87c95d54642e
Author: jyotiska <jyotiska123@gmail.com>
Date:   2014-02-27T14:13:19Z

    added callsite info for context.py

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 14:38:51 +0000 (UTC)",[GitHub] spark pull request: [SPARK-972] Added detailed callsite info for V...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/34#issuecomment-36247608
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 14:38:51 +0000 (UTC)",[GitHub] spark pull request: [SPARK-972] Added detailed callsite info for V...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/34#issuecomment-36247606
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 14:39:02 +0000 (UTC)",[GitHub] spark pull request: [SPARK-972] Added detailed callsite info for V...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/34#issuecomment-36247626
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 15:07:42 +0000 (UTC)",[GitHub] spark pull request: [SPARK-972] Added detailed callsite info for V...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/34#issuecomment-36250827
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 15:07:42 +0000 (UTC)",[GitHub] spark pull request: [SPARK-972] Added detailed callsite info for V...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/34#issuecomment-36250828
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12919/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
tgravescs <git@git.apache.org>,"Thu, 27 Feb 2014 15:34:59 +0000 (UTC)",,dev@spark.apache.org,"Github user tgravescs commented on the pull request:

    https://github.com/apache/spark/pull/29#issuecomment-36253998
  
    Thats really unfortunately that hadoop 1.x doesn't support it, as I would prefer to use the addCredentials since it also handles secrets.    
    
    Our only option right now would be to add a separate function in SparkHadoopUtils and YarnSparkHadoopUtils. This would allow spark on yarn to use the addCredentials at least.  Maybe its not critical right now. thoughts?
    
    Otherwise changes look good.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
tgravescs <git@git.apache.org>,"Thu, 27 Feb 2014 15:46:13 +0000 (UTC)",[GitHub] spark pull request: SPARK-1032. If Yarn app fails before registeri...,dev@spark.apache.org,"Github user tgravescs commented on the pull request:

    https://github.com/apache/spark/pull/28#issuecomment-36255394
  
    I personally like the way 4 spaces looks too.  The style guide isn't clear on what its supposed to be.  I'll assume it falls under the 4 space rule similar to functions: https://cwiki.apache.org/confluence/display/SPARK/Spark+Code+Style+Guide#SparkCodeStyleGuide-Indentation
    
    @pwendell - comment on the what the indentation is supposed to be?  If 4 perhaps we should update the guide.
    
    Otherwise looks good to me. Thanks Sandy! 


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
Nan Zhu <zhunanmcgill@gmail.com>,"Thu, 27 Feb 2014 11:13:14 -0500","test cases stuck on ""local-cluster mode"" of ReplSuite?",dev@spark.apache.org,"Hi, all  

Actually this problem exists for months in my side, when I run the test cases, it will stop (actually pause?) at the ReplSuite

[info] ReplSuite:  
2014-02-27 10:57:37.220 java[3911:1303] Unable to load realm info from SCDynamicStore
[info] - propagation of local properties (7 seconds, 646 milliseconds)
[info] - simple foreach with accumulator (6 seconds, 204 milliseconds)
[info] - external vars (4 seconds, 271 milliseconds)
[info] - external classes (3 seconds, 186 milliseconds)
[info] - external functions (4 seconds, 843 milliseconds)
[info] - external functions that access vars (3 seconds, 503 milliseconds)
[info] - broadcast vars (4 seconds, 313 milliseconds)
[info] - interacting with files (2 seconds, 492 milliseconds)



The next test case should be  

test(""local-cluster mode"") {
    val output = runInterpreter(""local-cluster[1,1,512]"",
      """"""
        |var v = 7
        |def getV() = v
        |sc.parallelize(1 to 10).map(x => getV()).collect.reduceLeft(_+_)
        |v = 10
        |sc.parallelize(1 to 10).map(x => getV()).collect.reduceLeft(_+_)
        |var array = new Array[Int](5)
        |val broadcastArray = sc.broadcast(array)
        |sc.parallelize(0 to 4).map(x => broadcastArray.value(x)).collect
        |array(0) = 5
        |sc.parallelize(0 to 4).map(x => broadcastArray.value(x)).collect
      """""".stripMargin)
    assertDoesNotContain(""error:"", output)
    assertDoesNotContain(""Exception"", output)
    assertContains(""res0: Int = 70"", output)
    assertContains(""res1: Int = 100"", output)
    assertContains(""res2: Array[Int] = Array(0, 0, 0, 0, 0)"", output)
    assertContains(""res4: Array[Int] = Array(0, 0, 0, 0, 0)"", output)
  }



I didnâ€™t see any reason for it spending so much time on itâ€¦.

Any idea? Iâ€™m using mbp, OS X 10.9.1, Intel Core i7 2.9 GHz, Memory 8GB 1600 MHz DDR3

Best,

--  
Nan Zhu

"
markhamstra <git@git.apache.org>,"Thu, 27 Feb 2014 17:30:30 +0000 (UTC)",,dev@spark.apache.org,"Github user markhamstra commented on the pull request:

    https://github.com/apache/spark/pull/6#issuecomment-36267797
  
    This broke the maven build.  Also, both SBT and Maven are still building artifacts with ""incubating"".
    
    [ERROR]   The project org.apache.spark:spark-core_2.10:1.0.0-incubating-SNAPSHOT (/home/mark/Apache/spark/core/pom.xml) has 2 errors
    [ERROR]     'dependencies.dependency.version' for org.apache.avro:avro:jar is missing. @ line 49, column 21
    [ERROR]     'dependencies.dependency.version' for org.apache.avro:avro-ipc:jar is missing. @ line 53, column 21


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
CodingCat <git@git.apache.org>,"Thu, 27 Feb 2014 18:00:45 +0000 (UTC)",[GitHub] spark pull request: [SPARK-1104] kill Process in workerThread,dev@spark.apache.org,"GitHub user CodingCat opened a pull request:

    https://github.com/apache/spark/pull/35

    [SPARK-1104] kill Process in workerThread

    As reported in https://spark-project.atlassian.net/browse/SPARK-1104
    
    By @pwendell: ""Sometimes due to large shuffles executors will take a long time shutting down. In particular this can happen if large numbers of shuffle files are around (this will be alleviated by SPARK-1103, but nonetheless...).
    
    The symptom is you have DEAD workers sitting around in the UI and the existing workers keep trying to re-register but can't because they've been assumed dead.""
    
    In this patch, I add lines in the handler of InterruptedException in workerThread of executorRunner, so that the process.destroy() and process.waitFor() can only block the workerThread instead of blocking the worker Actor... 

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/CodingCat/spark SPARK-1104

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/spark/pull/35.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #35
    
----
commit 48a88d9c2cee13410c2a7a7891566fae6609fcd8
Author: CodingCat <zhunansjtu@gmail.com>
Date:   2014-02-27T15:22:30Z

    kill Process in workerThread

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 18:03:51 +0000 (UTC)",[GitHub] spark pull request: [SPARK-1104] kill Process in workerThread,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/35#issuecomment-36271507
  
    Can one of the admins verify this patch?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 18:04:01 +0000 (UTC)",[GitHub] spark pull request: [SPARK-1104] kill Process in workerThread,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/35#issuecomment-36271526
  
    Can one of the admins verify this patch?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
shivaram <git@git.apache.org>,"Thu, 27 Feb 2014 18:06:06 +0000 (UTC)",[GitHub] spark pull request: [SPARK-979] Randomize order of offers.,dev@spark.apache.org,"Github user shivaram commented on the pull request:

    https://github.com/apache/spark/pull/27#issuecomment-36271766
  
    IMHO it would be better to try the simpler solution first and see how it works -- With a large enough number of executors I think the probability of seeing the same executors repeatedly should be pretty small 
    
    P.S: (It looks like an instance of balls-bins problem to me, so the imbalance should be at worst log N ?).  


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Thu, 27 Feb 2014 18:31:11 +0000 (UTC)","[GitHub] spark pull request: Add Security to Spark - Akka, Http, Connection...",dev@spark.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/spark/pull/33#issuecomment-36274459
  
    @tgravescs is this ready for another round of review or are you still working on it?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
hsaputra <git@git.apache.org>,"Thu, 27 Feb 2014 18:38:24 +0000 (UTC)",[GitHub] spark pull request: [SPARK-1146] Vagrant support for Spark,dev@spark.apache.org,"Github user hsaputra commented on the pull request:

    https://github.com/apache/spark/pull/26#issuecomment-36275259
  
    Hi @ngbinh, do you mind tagging the PR with ""[WIP]"" prefix to help indicate you are still working on this?
    
    Thx!


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
CodingCat <git@git.apache.org>,"Thu, 27 Feb 2014 18:38:57 +0000 (UTC)",[GitHub] spark pull request: [SPARK-979] Randomize order of offers.,dev@spark.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/spark/pull/27#issuecomment-36275312
  
    @shivaram I understand your cautiousness and I agree with Kay on that we would be careful when adding the complexity to the already-complex code base. So, I don't mind closing my PR if we decide to use randomization to resolve the issue...



---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
tgravescs <git@git.apache.org>,"Thu, 27 Feb 2014 18:39:11 +0000 (UTC)","[GitHub] spark pull request: Add Security to Spark - Akka, Http, Connection...",dev@spark.apache.org,"Github user tgravescs commented on the pull request:

    https://github.com/apache/spark/pull/33#issuecomment-36275335
  
    its ready for review.  I believe I've addressed all the comments from the previous PR.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
ngbinh <git@git.apache.org>,"Thu, 27 Feb 2014 18:42:09 +0000 (UTC)",[GitHub] spark pull request: [SPARK-1146] Vagrant support for Spark,dev@spark.apache.org,"Github user ngbinh closed the pull request at:

    https://github.com/apache/spark/pull/26


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
ngbinh <git@git.apache.org>,"Thu, 27 Feb 2014 18:42:09 +0000 (UTC)",[GitHub] spark pull request: [SPARK-1146] Vagrant support for Spark,dev@spark.apache.org,"Github user ngbinh commented on the pull request:

    https://github.com/apache/spark/pull/26#issuecomment-36275658
  
    Thanks for reminding me.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
ngbinh <git@git.apache.org>,"Thu, 27 Feb 2014 18:42:46 +0000 (UTC)",[GitHub] spark pull request: [SPARK-1146] Vagrant support for Spark,dev@spark.apache.org,"GitHub user ngbinh reopened a pull request:

    https://github.com/apache/spark/pull/26

    [SPARK-1146] Vagrant support for Spark

    This PR uses Vagrant to create a clusters of three VMs, one master and two workers. It allows running/testing Spark Cluster mode on one machine.
    
    My initial goal is to set up a stand alone cluster for now but I am open to others (YARN and Mesos) later.
    
    Note that this is a WIP.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/ngbinh/spark master

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/spark/pull/26.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #26
    
----
commit 4546248b5cbc445d636ece091e3a5463c4493f8a
Author: Binh Nguyen <ngbinh@gmail.com>
Date:   2014-02-27T07:30:08Z

    Initial import of Vagrant scripts and shells

commit 42907d051923ff815da7e59608d867df6ab859ca
Author: Binh Nguyen <ngbinh@gmail.com>
Date:   2014-02-27T08:25:05Z

    Shell scripts to copy public key to Vagrant nodes

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
sryza <git@git.apache.org>,"Thu, 27 Feb 2014 18:45:13 +0000 (UTC)",,dev@spark.apache.org,"Github user sryza commented on the pull request:

    https://github.com/apache/spark/pull/29#issuecomment-36275981
  
    I'm not familiar with what the secrets are used for as opposed to the tokens.  Do you know the reason they're not needed in the MR1 code? Either way, having YarnSparkHadoopUtils override runAsUser with something that calls addCredentials doesn't sound too difficult.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
markhamstra <git@git.apache.org>,"Thu, 27 Feb 2014 18:59:16 +0000 (UTC)",[GitHub] spark pull request: [SPARK-979] Randomize order of offers.,dev@spark.apache.org,"Github user markhamstra commented on the pull request:

    https://github.com/apache/spark/pull/27#issuecomment-36277558
  
    I see two issues: 1) The deterministic nature of the current scheduler places tasks on the same small set of machines while leaving others largely unused; 2) There is no rebalancing of partitions across worker nodes when new nodes are added to the cluster.  Neither LRU nor randomization really addresses the rebalancing issue, and LRU is only a little better than randomization in addressing the unused workers issue, so I think the additional complexity of LRU weighs against it -- at least until such time as we have evidence that random isn't adequate.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
xoltar <git@git.apache.org>,"Thu, 27 Feb 2014 19:00:19 +0000 (UTC)",[GitHub] spark pull request: Added a unit test for PairRDDFunctions.lookup,dev@spark.apache.org,"GitHub user xoltar opened a pull request:

    https://github.com/apache/spark/pull/36

    Added a unit test for PairRDDFunctions.lookup

    Lookup didn't have a unit test. Added two tests, one for with a partitioner, and one for without.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/xoltar/spark lookup

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/spark/pull/36.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #36
    
----
commit 3bc0d44a9aaf23c996f840dd26792cff341b828b
Author: Bryn Keller <bryn.keller@intel.com>
Date:   2014-02-27T05:51:46Z

    Added a unit test for PairRDDFunctions.lookup

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 19:03:58 +0000 (UTC)",[GitHub] spark pull request: Added a unit test for PairRDDFunctions.lookup,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/36#issuecomment-36278028
  
    Can one of the admins verify this patch?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 19:04:18 +0000 (UTC)",[GitHub] spark pull request: Added a unit test for PairRDDFunctions.lookup,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/36#issuecomment-36278062
  
    Can one of the admins verify this patch?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Thu, 27 Feb 2014 19:05:54 +0000 (UTC)",[GitHub] spark pull request: [WIP] [SPARK-1146] Vagrant support for Spark,dev@spark.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/spark/pull/26#issuecomment-36278255
  
    Hey @ngbinh this is a cool feature, but it might be best as a standalone project that we can refer to on the Spark website or in a separate contrib repo (similar to https://github.com/nathanmarz/storm-contrib - an idea we've been kicking around). The only thing really similar to this is the EC2 scripts, which are actually hosted largely outside of Spark's own repo, and we are thinking of further de-coupling them from the spark codebase. 
    
    Given that the number of ways people deploy spark is proliferating, I think the project is best off pointing people towards other maintained repositories rather than embedding and supporting a bunch of different ways of doing this. For instance, a while before there was a proposed 1000-line patch to add openstack support:
    
    https://github.com/apache/incubator-spark/pull/91
    
    And offline people have talked to me about contributing patches for docker (a user facing docker thing, not just one we use for tests), google compute engine, and other deployment modes. 
    
    Merging all of these into Spark would make the project unwieldy. However, pointing people towards them would be great. So if you are interested in maintaining this as its own repo, we could probably codify that and point users towards it.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Thu, 27 Feb 2014 19:07:13 +0000 (UTC)",[GitHub] spark pull request: Added a unit test for PairRDDFunctions.lookup,dev@spark.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/spark/pull/36#issuecomment-36278409
  
    Jenkins, test this please.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 19:08:56 +0000 (UTC)",[GitHub] spark pull request: Added a unit test for PairRDDFunctions.lookup,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/36#issuecomment-36278596
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 19:08:56 +0000 (UTC)",[GitHub] spark pull request: Added a unit test for PairRDDFunctions.lookup,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/36#issuecomment-36278594
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 19:09:22 +0000 (UTC)",[GitHub] spark pull request: Added a unit test for PairRDDFunctions.lookup,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/36#issuecomment-36278643
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Thu, 27 Feb 2014 19:12:11 +0000 (UTC)",[GitHub] spark pull request: SPARK 1084.1 (resubmitted),dev@spark.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/spark/pull/31#issuecomment-36278982
  
    Looks good, merging this into master. Thanks!


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
asfgit <git@git.apache.org>,"Thu, 27 Feb 2014 19:14:21 +0000 (UTC)",[GitHub] spark pull request: SPARK 1084.1 (resubmitted),dev@spark.apache.org,"Github user asfgit closed the pull request at:

    https://github.com/apache/spark/pull/31


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
ngbinh <git@git.apache.org>,"Thu, 27 Feb 2014 19:19:55 +0000 (UTC)",[GitHub] spark pull request: [WIP] [SPARK-1146] Vagrant support for Spark,dev@spark.apache.org,"Github user ngbinh commented on the pull request:

    https://github.com/apache/spark/pull/26#issuecomment-36279863
  
    @pwendell makes sense! I will maintain this on its own repo. Close for now!


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
ngbinh <git@git.apache.org>,"Thu, 27 Feb 2014 19:19:55 +0000 (UTC)",[GitHub] spark pull request: [WIP] [SPARK-1146] Vagrant support for Spark,dev@spark.apache.org,"Github user ngbinh closed the pull request at:

    https://github.com/apache/spark/pull/26


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 19:37:30 +0000 (UTC)",[GitHub] spark pull request: Added a unit test for PairRDDFunctions.lookup,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/36#issuecomment-36281796
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12920/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 19:37:30 +0000 (UTC)",[GitHub] spark pull request: Added a unit test for PairRDDFunctions.lookup,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/36#issuecomment-36281795
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
Nan Zhu <zhunanmcgill@gmail.com>,"Thu, 27 Feb 2014 15:17:30 -0500",Re: Discussion on SPARK-1139,dev@spark.apache.org,"any discussion on this?  

I would like to hear more advices from the community before I create the PR,

an example is how to create a NewHadoopRDD


we get a configuration from JobContext

val updatedConf = job.getConfiguration
new NewHadoopRDD(this, fClass, kClass, vClass, updatedConf)


then we create a jobContext based on this configuration object

NewHadoopRDD.scala (L74)
val jobContext = newJobContext(conf, jobId)
val rawSplits = inputFormat.getSplits(jobContext).toArray


because inputFormat is from mapreduce package, it only accept a JobContext as the parameter in its methods


I think we should avoid introduce Configuration as the parameter, but same thing as before, it will change the APIs


Best,  

--  
Nan Zhu



1139 . The issue discusses that:
ew Hadoop API.
ameters, but actually Configuration has been replaced by mapreduce.Job in the new Hadoop API
ith-new-map-reduce-api.html  
p10)
introduce some compatibility issue  

"
tgravescs <git@git.apache.org>,"Thu, 27 Feb 2014 20:18:13 +0000 (UTC)",,dev@spark.apache.org,"Github user tgravescs commented on the pull request:

    https://github.com/apache/spark/pull/29#issuecomment-36286358
  
    I believe secrets are mostly for users adding secrets for other services. secrets are also used by the MR2 framework.  secret keys are supported in MR1 via Credentials, I believe they just added the convenience methods to UserGroupInformation in mr2 for dealing with them.  It looks like MR1 handled credentials through the JobConf.
    
    So I guess if you don't mind, overriding it for Yarn at least to pass them along might be best.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Thu, 27 Feb 2014 20:21:02 +0000 (UTC)","[GitHub] spark pull request: Add Security to Spark - Akka, Http, Connection...",dev@spark.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/spark/pull/33#discussion_r10140602
  
    --- Diff: core/src/main/scala/org/apache/spark/SecurityManager.scala ---
    @@ -0,0 +1,259 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one or more
    + * contributor license agreements.  See the NOTICE file distributed with
    + * this work for additional information regarding copyright ownership.
    + * The ASF licenses this file to You under the Apache License, Version 2.0
    + * (the ""License""); you may not use this file except in compliance with
    + * the License.  You may obtain a copy of the License at
    + *
    + *    http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.spark
    +
    +import java.net.{Authenticator, PasswordAuthentication}
    +import org.apache.hadoop.io.Text
    +import org.apache.hadoop.security.Credentials
    +import org.apache.hadoop.security.UserGroupInformation
    +import org.apache.spark.deploy.SparkHadoopUtil
    +
    +import scala.collection.mutable.ArrayBuffer
    +
    +/** 
    + * Spark class responsible for security. 
    + * 
    + * In general this class should be instantiated by the SparkEnv and most components
    + * should access it from that. There are some cases where the SparkEnv hasn't been 
    + * initialized yet and this class must be instantiated directly.
    + * 
    + * Spark currently supports authentication via a shared secret.
    + * Authentication can be configured to be on via the 'spark.authenticate' configuration
    + * parameter. This parameter controls whether the Spark communication protocols do 
    + * authentication using the shared secret. This authentication is a basic handshake to
    + * make sure both sides have the same shared secret and are allowed to communicate.
    + * If the shared secret is not identical they will not be allowed to communicate. 
    + * 
    + * The Spark UI can also be secured by using javax servlet filters. A user may want to 
    + * secure the UI if it has data that other users should not be allowed to see. The javax 
    + * servlet filter specified by the user can authenticate the user and then once the user 
    + * is logged in, Spark can compare that user versus the view acls to make sure they are 
    + * authorized to view the UI. The configs 'spark.ui.acls.enable' and 'spark.ui.view.acls' 
    + * control the behavior of the acls. Note that the person who started the application
    + * always has view access to the UI.
    + *
    + * Spark does not currently support encryption after authentication.
    + * 
    + * At this point spark has multiple communication protocols that need to be secured and
    + * different underlying mechisms are used depending on the protocol:
    --- End diff --
    
    mechanisms


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Thu, 27 Feb 2014 20:24:18 +0000 (UTC)","[GitHub] spark pull request: Add Security to Spark - Akka, Http, Connection...",dev@spark.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/spark/pull/33#discussion_r10140725
  
    --- Diff: core/src/main/scala/org/apache/spark/SecurityManager.scala ---
    @@ -0,0 +1,259 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one or more
    + * contributor license agreements.  See the NOTICE file distributed with
    + * this work for additional information regarding copyright ownership.
    + * The ASF licenses this file to You under the Apache License, Version 2.0
    + * (the ""License""); you may not use this file except in compliance with
    + * the License.  You may obtain a copy of the License at
    + *
    + *    http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.spark
    +
    +import java.net.{Authenticator, PasswordAuthentication}
    +import org.apache.hadoop.io.Text
    +import org.apache.hadoop.security.Credentials
    +import org.apache.hadoop.security.UserGroupInformation
    +import org.apache.spark.deploy.SparkHadoopUtil
    +
    +import scala.collection.mutable.ArrayBuffer
    +
    +/** 
    + * Spark class responsible for security. 
    + * 
    + * In general this class should be instantiated by the SparkEnv and most components
    + * should access it from that. There are some cases where the SparkEnv hasn't been 
    + * initialized yet and this class must be instantiated directly.
    + * 
    + * Spark currently supports authentication via a shared secret.
    + * Authentication can be configured to be on via the 'spark.authenticate' configuration
    + * parameter. This parameter controls whether the Spark communication protocols do 
    + * authentication using the shared secret. This authentication is a basic handshake to
    + * make sure both sides have the same shared secret and are allowed to communicate.
    + * If the shared secret is not identical they will not be allowed to communicate. 
    + * 
    + * The Spark UI can also be secured by using javax servlet filters. A user may want to 
    + * secure the UI if it has data that other users should not be allowed to see. The javax 
    + * servlet filter specified by the user can authenticate the user and then once the user 
    + * is logged in, Spark can compare that user versus the view acls to make sure they are 
    + * authorized to view the UI. The configs 'spark.ui.acls.enable' and 'spark.ui.view.acls' 
    + * control the behavior of the acls. Note that the person who started the application
    + * always has view access to the UI.
    + *
    + * Spark does not currently support encryption after authentication.
    + * 
    + * At this point spark has multiple communication protocols that need to be secured and
    + * different underlying mechisms are used depending on the protocol:
    + *
    + *  - Akka -> The only option here is to use the Akka Remote secure-cookie functionality. 
    + *            Akka remoting allows you to specify a secure cookie that will be exchanged 
    + *            and ensured to be identical in the connection handshake between the client 
    + *            and the server. If they are not identical then the client will be refused 
    + *            to connect to the server. There is no control of the underlying 
    + *            authentication mechanism so its not clear if the password is passed in 
    + *            plaintext or uses DIGEST-MD5 or some other mechanism.
    + *            Akka also has an option to turn on SSL, this option is not currently supported
    + *            but we could add a configuration option in the future.
    + * 
    + *  - HTTP for broadcast and file server (via HttpServer) ->  Spark currently uses Jetty 
    + *            for the HttpServer. Jetty supports multiple authentication mechanisms - 
    + *            Basic, Digest, Form, Spengo, etc. It also supports multiple different login 
    + *            services - Hash, JAAS, Spnego, JDBC, etc.  Spark currently uses the HashLoginService
    + *            to authenticate using DIGEST-MD5 via a single user and the shared secret. 
    + *            Since we are using DIGEST-MD5, the shared secret is not passed on the wire
    + *            in plaintext.
    + *            We currently do not support SSL (https), but Jetty can be configured to use it
    + *            so we could add a configuration option for this in the future.
    + *            
    + *            The Spark HttpServer installs the HashLoginServer and configures it to DIGEST-MD5.
    + *            Any clients must specify the user and password. There is a default 
    + *            Authenticator installed in the SecurityManager to how it does the authentication
    + *            and in this case gets the user name and password from the request.
    + *
    + *  - ConnectionManager -> The Spark ConnectionManager uses java nio to asynchronously 
    + *            exchange messages.  For this we use the Java SASL 
    + *            (Simple Authentication and Security Layer) API and again use DIGEST-MD5 
    + *            as the authentication mechanism. This means the shared secret is not passed
    + *            over the wire in plaintext.
    + *            Note that SASL is pluggable as to what mechanism it uses.  We currently use
    + *            DIGEST-MD5 but this could be changed to use Kerberos or other in the future.
    + *            Spark currently supports ""auth"" for the quality of protection, which means
    + *            the connection is not supporting integrity or privacy protection (encryption)
    + *            after authentication. SASL also supports ""auth-int"" and ""auth-conf"" which 
    + *            SPARK could be support in the future to allow the user to specify the quality
    + *            of protection they want. If we support those, the messages will also have to 
    + *            be wrapped and unwrapped via the SaslServer/SaslClient.wrap/unwrap API's.
    + * 
    + *            Since the connectionManager does asynchronous messages passing, the SASL 
    + *            authentication is a bit more complex. A ConnectionManager can be both a client
    + *            and a Server, so for a particular connection is has to determine what to do.
    + *            A ConnectionId was added to be able to track connections and is used to 
    + *            match up incoming messages with connections waiting for authentication.
    + *            If its acting as a client and trying to send a message to another ConnectionManager,
    + *            it blocks the thread calling sendMessage until the SASL negotiation has occurred.
    + *            The ConnectionManager tracks all the sendingConnections using the ConnectionId
    + *            and waits for the response from the server and does the handshake.
    + *
    + *  - HTTP for the Spark UI -> the UI was changed to use servlets so that javax servlet filters 
    + *            can be used. Yarn requires a specific AmIpFilter be installed for security to work
    + *            properly. For non-Yarn deployments, users can write a filter to go through a
    + *            companies normal login service. If an authentication filter is in place then the
    + *            SparkUI can be configured to check the logged in user against the list of users who
    + *            have view acls to see if that user is authorized.
    + *            The filters can also be used for many different purposes. For instance filters 
    + *            could be used for logging, encypryption, or compression.
    --- End diff --
    
    encryption


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Thu, 27 Feb 2014 20:29:43 +0000 (UTC)","[GitHub] spark pull request: Add Security to Spark - Akka, Http, Connection...",dev@spark.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/spark/pull/33#discussion_r10140917
  
    --- Diff: core/src/main/scala/org/apache/spark/SecurityManager.scala ---
    @@ -0,0 +1,259 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one or more
    + * contributor license agreements.  See the NOTICE file distributed with
    + * this work for additional information regarding copyright ownership.
    + * The ASF licenses this file to You under the Apache License, Version 2.0
    + * (the ""License""); you may not use this file except in compliance with
    + * the License.  You may obtain a copy of the License at
    + *
    + *    http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.spark
    +
    +import java.net.{Authenticator, PasswordAuthentication}
    +import org.apache.hadoop.io.Text
    +import org.apache.hadoop.security.Credentials
    +import org.apache.hadoop.security.UserGroupInformation
    +import org.apache.spark.deploy.SparkHadoopUtil
    +
    +import scala.collection.mutable.ArrayBuffer
    +
    +/** 
    + * Spark class responsible for security. 
    + * 
    + * In general this class should be instantiated by the SparkEnv and most components
    + * should access it from that. There are some cases where the SparkEnv hasn't been 
    + * initialized yet and this class must be instantiated directly.
    + * 
    + * Spark currently supports authentication via a shared secret.
    + * Authentication can be configured to be on via the 'spark.authenticate' configuration
    + * parameter. This parameter controls whether the Spark communication protocols do 
    + * authentication using the shared secret. This authentication is a basic handshake to
    + * make sure both sides have the same shared secret and are allowed to communicate.
    + * If the shared secret is not identical they will not be allowed to communicate. 
    + * 
    + * The Spark UI can also be secured by using javax servlet filters. A user may want to 
    + * secure the UI if it has data that other users should not be allowed to see. The javax 
    + * servlet filter specified by the user can authenticate the user and then once the user 
    + * is logged in, Spark can compare that user versus the view acls to make sure they are 
    + * authorized to view the UI. The configs 'spark.ui.acls.enable' and 'spark.ui.view.acls' 
    + * control the behavior of the acls. Note that the person who started the application
    + * always has view access to the UI.
    + *
    + * Spark does not currently support encryption after authentication.
    + * 
    + * At this point spark has multiple communication protocols that need to be secured and
    + * different underlying mechisms are used depending on the protocol:
    + *
    + *  - Akka -> The only option here is to use the Akka Remote secure-cookie functionality. 
    + *            Akka remoting allows you to specify a secure cookie that will be exchanged 
    + *            and ensured to be identical in the connection handshake between the client 
    + *            and the server. If they are not identical then the client will be refused 
    + *            to connect to the server. There is no control of the underlying 
    + *            authentication mechanism so its not clear if the password is passed in 
    + *            plaintext or uses DIGEST-MD5 or some other mechanism.
    + *            Akka also has an option to turn on SSL, this option is not currently supported
    + *            but we could add a configuration option in the future.
    + * 
    + *  - HTTP for broadcast and file server (via HttpServer) ->  Spark currently uses Jetty 
    + *            for the HttpServer. Jetty supports multiple authentication mechanisms - 
    + *            Basic, Digest, Form, Spengo, etc. It also supports multiple different login 
    + *            services - Hash, JAAS, Spnego, JDBC, etc.  Spark currently uses the HashLoginService
    + *            to authenticate using DIGEST-MD5 via a single user and the shared secret. 
    + *            Since we are using DIGEST-MD5, the shared secret is not passed on the wire
    + *            in plaintext.
    + *            We currently do not support SSL (https), but Jetty can be configured to use it
    + *            so we could add a configuration option for this in the future.
    + *            
    + *            The Spark HttpServer installs the HashLoginServer and configures it to DIGEST-MD5.
    + *            Any clients must specify the user and password. There is a default 
    + *            Authenticator installed in the SecurityManager to how it does the authentication
    + *            and in this case gets the user name and password from the request.
    + *
    + *  - ConnectionManager -> The Spark ConnectionManager uses java nio to asynchronously 
    + *            exchange messages.  For this we use the Java SASL 
    + *            (Simple Authentication and Security Layer) API and again use DIGEST-MD5 
    + *            as the authentication mechanism. This means the shared secret is not passed
    + *            over the wire in plaintext.
    + *            Note that SASL is pluggable as to what mechanism it uses.  We currently use
    + *            DIGEST-MD5 but this could be changed to use Kerberos or other in the future.
    + *            Spark currently supports ""auth"" for the quality of protection, which means
    + *            the connection is not supporting integrity or privacy protection (encryption)
    + *            after authentication. SASL also supports ""auth-int"" and ""auth-conf"" which 
    + *            SPARK could be support in the future to allow the user to specify the quality
    + *            of protection they want. If we support those, the messages will also have to 
    + *            be wrapped and unwrapped via the SaslServer/SaslClient.wrap/unwrap API's.
    + * 
    + *            Since the connectionManager does asynchronous messages passing, the SASL 
    + *            authentication is a bit more complex. A ConnectionManager can be both a client
    + *            and a Server, so for a particular connection is has to determine what to do.
    + *            A ConnectionId was added to be able to track connections and is used to 
    + *            match up incoming messages with connections waiting for authentication.
    + *            If its acting as a client and trying to send a message to another ConnectionManager,
    + *            it blocks the thread calling sendMessage until the SASL negotiation has occurred.
    + *            The ConnectionManager tracks all the sendingConnections using the ConnectionId
    + *            and waits for the response from the server and does the handshake.
    + *
    + *  - HTTP for the Spark UI -> the UI was changed to use servlets so that javax servlet filters 
    + *            can be used. Yarn requires a specific AmIpFilter be installed for security to work
    + *            properly. For non-Yarn deployments, users can write a filter to go through a
    + *            companies normal login service. If an authentication filter is in place then the
    + *            SparkUI can be configured to check the logged in user against the list of users who
    + *            have view acls to see if that user is authorized.
    + *            The filters can also be used for many different purposes. For instance filters 
    + *            could be used for logging, encypryption, or compression.
    + *            
    + *  The exact mechanisms used to generate/distributed the shared secret is deployment specific.
    + * 
    + *  For Yarn deployments, the secret is automatically generated using the Akka remote
    + *  Crypt.generateSecureCookie() API. The secret is placed in the Hadoop UGI which gets passed
    + *  around via the Hadoop RPC mechanism. Hadoop RPC can be configured to support different levels
    + *  of protection. See the Hadoop documentation for more details. Each Spark application on Yarn
    + *  AmIpFilter which requires the user to go through the ResourceManager Proxy. That Proxy is there
    + *  to reduce the possibility of web based attacks through YARN. Hadoop can be configured to use
    + *  filters to do authentication. That authentication then happens via the ResourceManager Proxy
    + *  and Spark will use that to do authorization against the view acls.
    + * 
    + *  For other Spark deployments, the shared secret should be specified via the SPARK_SECRET 
    + *  environment variable. This isn't ideal but it means only the user who starts the process 
    + *  has access to view that variable. Note that Spark does try to generate a secret for
    + *  you if the SPARK_SECRET environment variable is not set, but it gets put into the java
    + *  system property which can be viewed by other users, so setting the SPARK_SECRET environment
    + *  variable is recommended.
    + *  All the nodes (Master and Workers) need to have the same shared secret
    --- End diff --
    
    This could be interpreted in two ways:
    
    (a) The entire spark deployment, including the master/workers and all applications, must share the same secret.
    
    (b) The standalone cluster (master and workers) must all share a secret and all applications must share a secret, but these need not be the same.
    
    Maybe change the wording a bit to make it more clear? I'm not sure which of these it is.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Thu, 27 Feb 2014 20:37:53 +0000 (UTC)","[GitHub] spark pull request: Add Security to Spark - Akka, Http, Connection...",dev@spark.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/spark/pull/33#discussion_r10141220
  
    --- Diff: core/src/main/scala/org/apache/spark/SecurityManager.scala ---
    @@ -0,0 +1,259 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one or more
    + * contributor license agreements.  See the NOTICE file distributed with
    + * this work for additional information regarding copyright ownership.
    + * The ASF licenses this file to You under the Apache License, Version 2.0
    + * (the ""License""); you may not use this file except in compliance with
    + * the License.  You may obtain a copy of the License at
    + *
    + *    http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.spark
    +
    +import java.net.{Authenticator, PasswordAuthentication}
    +import org.apache.hadoop.io.Text
    +import org.apache.hadoop.security.Credentials
    +import org.apache.hadoop.security.UserGroupInformation
    +import org.apache.spark.deploy.SparkHadoopUtil
    +
    +import scala.collection.mutable.ArrayBuffer
    +
    +/** 
    + * Spark class responsible for security. 
    + * 
    + * In general this class should be instantiated by the SparkEnv and most components
    + * should access it from that. There are some cases where the SparkEnv hasn't been 
    + * initialized yet and this class must be instantiated directly.
    + * 
    + * Spark currently supports authentication via a shared secret.
    + * Authentication can be configured to be on via the 'spark.authenticate' configuration
    + * parameter. This parameter controls whether the Spark communication protocols do 
    + * authentication using the shared secret. This authentication is a basic handshake to
    + * make sure both sides have the same shared secret and are allowed to communicate.
    + * If the shared secret is not identical they will not be allowed to communicate. 
    + * 
    + * The Spark UI can also be secured by using javax servlet filters. A user may want to 
    + * secure the UI if it has data that other users should not be allowed to see. The javax 
    + * servlet filter specified by the user can authenticate the user and then once the user 
    + * is logged in, Spark can compare that user versus the view acls to make sure they are 
    + * authorized to view the UI. The configs 'spark.ui.acls.enable' and 'spark.ui.view.acls' 
    + * control the behavior of the acls. Note that the person who started the application
    + * always has view access to the UI.
    + *
    + * Spark does not currently support encryption after authentication.
    + * 
    + * At this point spark has multiple communication protocols that need to be secured and
    + * different underlying mechisms are used depending on the protocol:
    + *
    + *  - Akka -> The only option here is to use the Akka Remote secure-cookie functionality. 
    + *            Akka remoting allows you to specify a secure cookie that will be exchanged 
    + *            and ensured to be identical in the connection handshake between the client 
    + *            and the server. If they are not identical then the client will be refused 
    + *            to connect to the server. There is no control of the underlying 
    + *            authentication mechanism so its not clear if the password is passed in 
    + *            plaintext or uses DIGEST-MD5 or some other mechanism.
    + *            Akka also has an option to turn on SSL, this option is not currently supported
    + *            but we could add a configuration option in the future.
    + * 
    + *  - HTTP for broadcast and file server (via HttpServer) ->  Spark currently uses Jetty 
    + *            for the HttpServer. Jetty supports multiple authentication mechanisms - 
    + *            Basic, Digest, Form, Spengo, etc. It also supports multiple different login 
    + *            services - Hash, JAAS, Spnego, JDBC, etc.  Spark currently uses the HashLoginService
    + *            to authenticate using DIGEST-MD5 via a single user and the shared secret. 
    + *            Since we are using DIGEST-MD5, the shared secret is not passed on the wire
    + *            in plaintext.
    + *            We currently do not support SSL (https), but Jetty can be configured to use it
    + *            so we could add a configuration option for this in the future.
    + *            
    + *            The Spark HttpServer installs the HashLoginServer and configures it to DIGEST-MD5.
    + *            Any clients must specify the user and password. There is a default 
    + *            Authenticator installed in the SecurityManager to how it does the authentication
    + *            and in this case gets the user name and password from the request.
    + *
    + *  - ConnectionManager -> The Spark ConnectionManager uses java nio to asynchronously 
    + *            exchange messages.  For this we use the Java SASL 
    + *            (Simple Authentication and Security Layer) API and again use DIGEST-MD5 
    + *            as the authentication mechanism. This means the shared secret is not passed
    + *            over the wire in plaintext.
    + *            Note that SASL is pluggable as to what mechanism it uses.  We currently use
    + *            DIGEST-MD5 but this could be changed to use Kerberos or other in the future.
    + *            Spark currently supports ""auth"" for the quality of protection, which means
    + *            the connection is not supporting integrity or privacy protection (encryption)
    + *            after authentication. SASL also supports ""auth-int"" and ""auth-conf"" which 
    + *            SPARK could be support in the future to allow the user to specify the quality
    + *            of protection they want. If we support those, the messages will also have to 
    + *            be wrapped and unwrapped via the SaslServer/SaslClient.wrap/unwrap API's.
    + * 
    + *            Since the connectionManager does asynchronous messages passing, the SASL 
    + *            authentication is a bit more complex. A ConnectionManager can be both a client
    + *            and a Server, so for a particular connection is has to determine what to do.
    + *            A ConnectionId was added to be able to track connections and is used to 
    + *            match up incoming messages with connections waiting for authentication.
    + *            If its acting as a client and trying to send a message to another ConnectionManager,
    + *            it blocks the thread calling sendMessage until the SASL negotiation has occurred.
    + *            The ConnectionManager tracks all the sendingConnections using the ConnectionId
    + *            and waits for the response from the server and does the handshake.
    + *
    + *  - HTTP for the Spark UI -> the UI was changed to use servlets so that javax servlet filters 
    + *            can be used. Yarn requires a specific AmIpFilter be installed for security to work
    + *            properly. For non-Yarn deployments, users can write a filter to go through a
    + *            companies normal login service. If an authentication filter is in place then the
    + *            SparkUI can be configured to check the logged in user against the list of users who
    + *            have view acls to see if that user is authorized.
    + *            The filters can also be used for many different purposes. For instance filters 
    + *            could be used for logging, encypryption, or compression.
    + *            
    + *  The exact mechanisms used to generate/distributed the shared secret is deployment specific.
    + * 
    + *  For Yarn deployments, the secret is automatically generated using the Akka remote
    + *  Crypt.generateSecureCookie() API. The secret is placed in the Hadoop UGI which gets passed
    + *  around via the Hadoop RPC mechanism. Hadoop RPC can be configured to support different levels
    + *  of protection. See the Hadoop documentation for more details. Each Spark application on Yarn
    + *  AmIpFilter which requires the user to go through the ResourceManager Proxy. That Proxy is there
    + *  to reduce the possibility of web based attacks through YARN. Hadoop can be configured to use
    + *  filters to do authentication. That authentication then happens via the ResourceManager Proxy
    + *  and Spark will use that to do authorization against the view acls.
    + * 
    + *  For other Spark deployments, the shared secret should be specified via the SPARK_SECRET 
    + *  environment variable. This isn't ideal but it means only the user who starts the process 
    + *  has access to view that variable. Note that Spark does try to generate a secret for
    + *  you if the SPARK_SECRET environment variable is not set, but it gets put into the java
    + *  system property which can be viewed by other users, so setting the SPARK_SECRET environment
    + *  variable is recommended.
    + *  All the nodes (Master and Workers) need to have the same shared secret
    + *  and all the applications running need to have that same shared secret. This again
    + *  is not ideal as one user could potentially affect another users application.
    + *  This should be enhanced in the future to provide better protection.
    + *  If the UI needs to be secured the user needs to install a javax servlet filter to do the
    + *  authentication. Spark will then use that user to compare against the view acls to do
    + *  authorization. If not filter is in place the user is generally null and no authorization
    + *  can take place.
    + */
    +private[spark] class SecurityManager extends Logging {
    +
    +  // key used to store the spark secret in the Hadoop UGI
    +  private val sparkSecretLookupKey = ""sparkCookie""
    +
    +
    +  // always add the current user and SPARK_USER to the viewAcls
    +  private val aclUsers = ArrayBuffer[String](System.getProperty(""user.name"", """"),
    +    Option(System.getenv(""SPARK_USER"")).getOrElse(""""))
    +  aclUsers ++= System.getProperty(""spark.ui.view.acls"", """").split(',')
    +  private val viewAcls = aclUsers.map(_.trim()).filter(!_.isEmpty).toSet
    +
    +  private val secretKey = generateSecretKey()
    +
    +  // Set our own authenticator to properly negotiate user/password for HTTP connections.
    +  // This is needed by the HTTP client fetching from the HttpServer. Put here so its 
    +  // only set once.
    +    Authenticator.setDefault(
    +      new Authenticator() {
    +        override def getPasswordAuthentication(): PasswordAuthentication = {
    +          var passAuth: PasswordAuthentication = null
    +          val userInfo = getRequestingURL().getUserInfo()
    +          if (userInfo != null) {
    +            val  parts = userInfo.split("":"", 2)
    +            passAuth = new PasswordAuthentication(parts(0), parts(1).toCharArray())
    +          }
    +          return passAuth
    +        }
    +      }
    +    );
    +  }
    +
    +  /**
    +   * Generates or looks up the secret key.
    +   *
    +   * The way the key is stored depends on the Spark deployment mode. Yarn
    +   * uses the Hadoop UGI.
    +   *
    +   * For non-Yarn deployments, If the environment variable is not set already 
    +   * we generate a secret and since we can't set an environment variable dynamically 
    +   * we set the java system property SPARK_SECRET. This will allow it to automatically
    +   * work in certain situations.  Others this still will not work and this definitely is 
    +   * not ideal since other users can see it. We should switch to put it in 
    +   * a config once Spark supports configs.
    +   */
    +  private def generateSecretKey(): String = {
    +    if (!isAuthenticationEnabled) return null
    +    // first check to see if the secret is already set, else generate a new one
    +    if (SparkHadoopUtil.get.isYarnMode) {
    +      val secretKey = SparkHadoopUtil.get.getSecretKeyFromUserCredentials(sparkSecretLookupKey)
    +      if (secretKey != null) {
    +        logDebug(""in yarn mode, getting secret from credentials"")
    +        return new Text(secretKey).toString
    +      } else {
    +        logDebug(""getSecretKey: yarn mode, secret key from credentials is null"")
    +      }
    +    }
    +    val secret = System.getProperty(""SPARK_SECRET"", System.getenv(""SPARK_SECRET"")) 
    +    if (secret != null && !secret.isEmpty()) return secret 
    +    // generate one 
    +    val sCookie = akka.util.Crypt.generateSecureCookie
    +
    +    // if we generated the secret then we must be the first so lets set it so t 
    +    // gets used by everyone else
    +    if (SparkHadoopUtil.get.isYarnMode) {
    +      SparkHadoopUtil.get.addSecretKeyToUserCredentials(sparkSecretLookupKey, sCookie)
    +      logDebug(""adding secret to credentials yarn mode"")
    --- End diff --
    
    This might be worth logging at INFO level since it happens at most once per application.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Thu, 27 Feb 2014 20:39:16 +0000 (UTC)","[GitHub] spark pull request: Add Security to Spark - Akka, Http, Connection...",dev@spark.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/spark/pull/33#discussion_r10141281
  
    --- Diff: core/src/main/scala/org/apache/spark/SecurityManager.scala ---
    @@ -0,0 +1,259 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one or more
    + * contributor license agreements.  See the NOTICE file distributed with
    + * this work for additional information regarding copyright ownership.
    + * The ASF licenses this file to You under the Apache License, Version 2.0
    + * (the ""License""); you may not use this file except in compliance with
    + * the License.  You may obtain a copy of the License at
    + *
    + *    http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.spark
    +
    +import java.net.{Authenticator, PasswordAuthentication}
    +import org.apache.hadoop.io.Text
    +import org.apache.hadoop.security.Credentials
    +import org.apache.hadoop.security.UserGroupInformation
    +import org.apache.spark.deploy.SparkHadoopUtil
    +
    +import scala.collection.mutable.ArrayBuffer
    +
    +/** 
    + * Spark class responsible for security. 
    + * 
    + * In general this class should be instantiated by the SparkEnv and most components
    + * should access it from that. There are some cases where the SparkEnv hasn't been 
    + * initialized yet and this class must be instantiated directly.
    + * 
    + * Spark currently supports authentication via a shared secret.
    + * Authentication can be configured to be on via the 'spark.authenticate' configuration
    + * parameter. This parameter controls whether the Spark communication protocols do 
    + * authentication using the shared secret. This authentication is a basic handshake to
    + * make sure both sides have the same shared secret and are allowed to communicate.
    + * If the shared secret is not identical they will not be allowed to communicate. 
    + * 
    + * The Spark UI can also be secured by using javax servlet filters. A user may want to 
    + * secure the UI if it has data that other users should not be allowed to see. The javax 
    + * servlet filter specified by the user can authenticate the user and then once the user 
    + * is logged in, Spark can compare that user versus the view acls to make sure they are 
    + * authorized to view the UI. The configs 'spark.ui.acls.enable' and 'spark.ui.view.acls' 
    + * control the behavior of the acls. Note that the person who started the application
    + * always has view access to the UI.
    + *
    + * Spark does not currently support encryption after authentication.
    + * 
    + * At this point spark has multiple communication protocols that need to be secured and
    + * different underlying mechisms are used depending on the protocol:
    + *
    + *  - Akka -> The only option here is to use the Akka Remote secure-cookie functionality. 
    + *            Akka remoting allows you to specify a secure cookie that will be exchanged 
    + *            and ensured to be identical in the connection handshake between the client 
    + *            and the server. If they are not identical then the client will be refused 
    + *            to connect to the server. There is no control of the underlying 
    + *            authentication mechanism so its not clear if the password is passed in 
    + *            plaintext or uses DIGEST-MD5 or some other mechanism.
    + *            Akka also has an option to turn on SSL, this option is not currently supported
    + *            but we could add a configuration option in the future.
    + * 
    + *  - HTTP for broadcast and file server (via HttpServer) ->  Spark currently uses Jetty 
    + *            for the HttpServer. Jetty supports multiple authentication mechanisms - 
    + *            Basic, Digest, Form, Spengo, etc. It also supports multiple different login 
    + *            services - Hash, JAAS, Spnego, JDBC, etc.  Spark currently uses the HashLoginService
    + *            to authenticate using DIGEST-MD5 via a single user and the shared secret. 
    + *            Since we are using DIGEST-MD5, the shared secret is not passed on the wire
    + *            in plaintext.
    + *            We currently do not support SSL (https), but Jetty can be configured to use it
    + *            so we could add a configuration option for this in the future.
    + *            
    + *            The Spark HttpServer installs the HashLoginServer and configures it to DIGEST-MD5.
    + *            Any clients must specify the user and password. There is a default 
    + *            Authenticator installed in the SecurityManager to how it does the authentication
    + *            and in this case gets the user name and password from the request.
    + *
    + *  - ConnectionManager -> The Spark ConnectionManager uses java nio to asynchronously 
    + *            exchange messages.  For this we use the Java SASL 
    + *            (Simple Authentication and Security Layer) API and again use DIGEST-MD5 
    + *            as the authentication mechanism. This means the shared secret is not passed
    + *            over the wire in plaintext.
    + *            Note that SASL is pluggable as to what mechanism it uses.  We currently use
    + *            DIGEST-MD5 but this could be changed to use Kerberos or other in the future.
    + *            Spark currently supports ""auth"" for the quality of protection, which means
    + *            the connection is not supporting integrity or privacy protection (encryption)
    + *            after authentication. SASL also supports ""auth-int"" and ""auth-conf"" which 
    + *            SPARK could be support in the future to allow the user to specify the quality
    + *            of protection they want. If we support those, the messages will also have to 
    + *            be wrapped and unwrapped via the SaslServer/SaslClient.wrap/unwrap API's.
    + * 
    + *            Since the connectionManager does asynchronous messages passing, the SASL 
    + *            authentication is a bit more complex. A ConnectionManager can be both a client
    + *            and a Server, so for a particular connection is has to determine what to do.
    + *            A ConnectionId was added to be able to track connections and is used to 
    + *            match up incoming messages with connections waiting for authentication.
    + *            If its acting as a client and trying to send a message to another ConnectionManager,
    + *            it blocks the thread calling sendMessage until the SASL negotiation has occurred.
    + *            The ConnectionManager tracks all the sendingConnections using the ConnectionId
    + *            and waits for the response from the server and does the handshake.
    + *
    + *  - HTTP for the Spark UI -> the UI was changed to use servlets so that javax servlet filters 
    + *            can be used. Yarn requires a specific AmIpFilter be installed for security to work
    + *            properly. For non-Yarn deployments, users can write a filter to go through a
    + *            companies normal login service. If an authentication filter is in place then the
    + *            SparkUI can be configured to check the logged in user against the list of users who
    + *            have view acls to see if that user is authorized.
    + *            The filters can also be used for many different purposes. For instance filters 
    + *            could be used for logging, encypryption, or compression.
    + *            
    + *  The exact mechanisms used to generate/distributed the shared secret is deployment specific.
    + * 
    + *  For Yarn deployments, the secret is automatically generated using the Akka remote
    + *  Crypt.generateSecureCookie() API. The secret is placed in the Hadoop UGI which gets passed
    + *  around via the Hadoop RPC mechanism. Hadoop RPC can be configured to support different levels
    + *  of protection. See the Hadoop documentation for more details. Each Spark application on Yarn
    + *  AmIpFilter which requires the user to go through the ResourceManager Proxy. That Proxy is there
    + *  to reduce the possibility of web based attacks through YARN. Hadoop can be configured to use
    + *  filters to do authentication. That authentication then happens via the ResourceManager Proxy
    + *  and Spark will use that to do authorization against the view acls.
    + * 
    + *  For other Spark deployments, the shared secret should be specified via the SPARK_SECRET 
    + *  environment variable. This isn't ideal but it means only the user who starts the process 
    + *  has access to view that variable. Note that Spark does try to generate a secret for
    + *  you if the SPARK_SECRET environment variable is not set, but it gets put into the java
    + *  system property which can be viewed by other users, so setting the SPARK_SECRET environment
    + *  variable is recommended.
    + *  All the nodes (Master and Workers) need to have the same shared secret
    + *  and all the applications running need to have that same shared secret. This again
    + *  is not ideal as one user could potentially affect another users application.
    + *  This should be enhanced in the future to provide better protection.
    + *  If the UI needs to be secured the user needs to install a javax servlet filter to do the
    + *  authentication. Spark will then use that user to compare against the view acls to do
    + *  authorization. If not filter is in place the user is generally null and no authorization
    + *  can take place.
    + */
    +private[spark] class SecurityManager extends Logging {
    +
    +  // key used to store the spark secret in the Hadoop UGI
    +  private val sparkSecretLookupKey = ""sparkCookie""
    +
    +
    +  // always add the current user and SPARK_USER to the viewAcls
    +  private val aclUsers = ArrayBuffer[String](System.getProperty(""user.name"", """"),
    +    Option(System.getenv(""SPARK_USER"")).getOrElse(""""))
    +  aclUsers ++= System.getProperty(""spark.ui.view.acls"", """").split(',')
    +  private val viewAcls = aclUsers.map(_.trim()).filter(!_.isEmpty).toSet
    +
    +  private val secretKey = generateSecretKey()
    +
    +  // Set our own authenticator to properly negotiate user/password for HTTP connections.
    +  // This is needed by the HTTP client fetching from the HttpServer. Put here so its 
    +  // only set once.
    +    Authenticator.setDefault(
    +      new Authenticator() {
    +        override def getPasswordAuthentication(): PasswordAuthentication = {
    +          var passAuth: PasswordAuthentication = null
    +          val userInfo = getRequestingURL().getUserInfo()
    +          if (userInfo != null) {
    +            val  parts = userInfo.split("":"", 2)
    +            passAuth = new PasswordAuthentication(parts(0), parts(1).toCharArray())
    +          }
    +          return passAuth
    +        }
    +      }
    +    );
    +  }
    +
    +  /**
    +   * Generates or looks up the secret key.
    +   *
    +   * The way the key is stored depends on the Spark deployment mode. Yarn
    +   * uses the Hadoop UGI.
    +   *
    +   * For non-Yarn deployments, If the environment variable is not set already 
    +   * we generate a secret and since we can't set an environment variable dynamically 
    +   * we set the java system property SPARK_SECRET. This will allow it to automatically
    +   * work in certain situations.  Others this still will not work and this definitely is 
    +   * not ideal since other users can see it. We should switch to put it in 
    +   * a config once Spark supports configs.
    +   */
    +  private def generateSecretKey(): String = {
    +    if (!isAuthenticationEnabled) return null
    +    // first check to see if the secret is already set, else generate a new one
    +    if (SparkHadoopUtil.get.isYarnMode) {
    +      val secretKey = SparkHadoopUtil.get.getSecretKeyFromUserCredentials(sparkSecretLookupKey)
    +      if (secretKey != null) {
    +        logDebug(""in yarn mode, getting secret from credentials"")
    +        return new Text(secretKey).toString
    +      } else {
    +        logDebug(""getSecretKey: yarn mode, secret key from credentials is null"")
    +      }
    +    }
    +    val secret = System.getProperty(""SPARK_SECRET"", System.getenv(""SPARK_SECRET"")) 
    +    if (secret != null && !secret.isEmpty()) return secret 
    +    // generate one 
    +    val sCookie = akka.util.Crypt.generateSecureCookie
    --- End diff --
    
    It seems a little scary to silently generate a secret here. What about just throwing an exception if SPARK_SECRET is not set? I'm just wondering if maybe this could cause some silent failures that are very hard to debug.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Thu, 27 Feb 2014 20:45:52 +0000 (UTC)","[GitHub] spark pull request: Add Security to Spark - Akka, Http, Connection...",dev@spark.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/spark/pull/33#discussion_r10141554
  
    --- Diff: core/src/main/scala/org/apache/spark/SecurityManager.scala ---
    @@ -0,0 +1,259 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one or more
    + * contributor license agreements.  See the NOTICE file distributed with
    + * this work for additional information regarding copyright ownership.
    + * The ASF licenses this file to You under the Apache License, Version 2.0
    + * (the ""License""); you may not use this file except in compliance with
    + * the License.  You may obtain a copy of the License at
    + *
    + *    http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.spark
    +
    +import java.net.{Authenticator, PasswordAuthentication}
    +import org.apache.hadoop.io.Text
    +import org.apache.hadoop.security.Credentials
    +import org.apache.hadoop.security.UserGroupInformation
    +import org.apache.spark.deploy.SparkHadoopUtil
    +
    +import scala.collection.mutable.ArrayBuffer
    +
    +/** 
    + * Spark class responsible for security. 
    + * 
    + * In general this class should be instantiated by the SparkEnv and most components
    + * should access it from that. There are some cases where the SparkEnv hasn't been 
    + * initialized yet and this class must be instantiated directly.
    + * 
    + * Spark currently supports authentication via a shared secret.
    + * Authentication can be configured to be on via the 'spark.authenticate' configuration
    + * parameter. This parameter controls whether the Spark communication protocols do 
    + * authentication using the shared secret. This authentication is a basic handshake to
    + * make sure both sides have the same shared secret and are allowed to communicate.
    + * If the shared secret is not identical they will not be allowed to communicate. 
    + * 
    + * The Spark UI can also be secured by using javax servlet filters. A user may want to 
    + * secure the UI if it has data that other users should not be allowed to see. The javax 
    + * servlet filter specified by the user can authenticate the user and then once the user 
    + * is logged in, Spark can compare that user versus the view acls to make sure they are 
    + * authorized to view the UI. The configs 'spark.ui.acls.enable' and 'spark.ui.view.acls' 
    + * control the behavior of the acls. Note that the person who started the application
    + * always has view access to the UI.
    + *
    + * Spark does not currently support encryption after authentication.
    + * 
    + * At this point spark has multiple communication protocols that need to be secured and
    + * different underlying mechisms are used depending on the protocol:
    + *
    + *  - Akka -> The only option here is to use the Akka Remote secure-cookie functionality. 
    + *            Akka remoting allows you to specify a secure cookie that will be exchanged 
    + *            and ensured to be identical in the connection handshake between the client 
    + *            and the server. If they are not identical then the client will be refused 
    + *            to connect to the server. There is no control of the underlying 
    + *            authentication mechanism so its not clear if the password is passed in 
    + *            plaintext or uses DIGEST-MD5 or some other mechanism.
    + *            Akka also has an option to turn on SSL, this option is not currently supported
    + *            but we could add a configuration option in the future.
    + * 
    + *  - HTTP for broadcast and file server (via HttpServer) ->  Spark currently uses Jetty 
    + *            for the HttpServer. Jetty supports multiple authentication mechanisms - 
    + *            Basic, Digest, Form, Spengo, etc. It also supports multiple different login 
    + *            services - Hash, JAAS, Spnego, JDBC, etc.  Spark currently uses the HashLoginService
    + *            to authenticate using DIGEST-MD5 via a single user and the shared secret. 
    + *            Since we are using DIGEST-MD5, the shared secret is not passed on the wire
    + *            in plaintext.
    + *            We currently do not support SSL (https), but Jetty can be configured to use it
    + *            so we could add a configuration option for this in the future.
    + *            
    + *            The Spark HttpServer installs the HashLoginServer and configures it to DIGEST-MD5.
    + *            Any clients must specify the user and password. There is a default 
    + *            Authenticator installed in the SecurityManager to how it does the authentication
    + *            and in this case gets the user name and password from the request.
    + *
    + *  - ConnectionManager -> The Spark ConnectionManager uses java nio to asynchronously 
    + *            exchange messages.  For this we use the Java SASL 
    + *            (Simple Authentication and Security Layer) API and again use DIGEST-MD5 
    + *            as the authentication mechanism. This means the shared secret is not passed
    + *            over the wire in plaintext.
    + *            Note that SASL is pluggable as to what mechanism it uses.  We currently use
    + *            DIGEST-MD5 but this could be changed to use Kerberos or other in the future.
    + *            Spark currently supports ""auth"" for the quality of protection, which means
    + *            the connection is not supporting integrity or privacy protection (encryption)
    + *            after authentication. SASL also supports ""auth-int"" and ""auth-conf"" which 
    + *            SPARK could be support in the future to allow the user to specify the quality
    + *            of protection they want. If we support those, the messages will also have to 
    + *            be wrapped and unwrapped via the SaslServer/SaslClient.wrap/unwrap API's.
    + * 
    + *            Since the connectionManager does asynchronous messages passing, the SASL 
    + *            authentication is a bit more complex. A ConnectionManager can be both a client
    + *            and a Server, so for a particular connection is has to determine what to do.
    + *            A ConnectionId was added to be able to track connections and is used to 
    + *            match up incoming messages with connections waiting for authentication.
    + *            If its acting as a client and trying to send a message to another ConnectionManager,
    + *            it blocks the thread calling sendMessage until the SASL negotiation has occurred.
    + *            The ConnectionManager tracks all the sendingConnections using the ConnectionId
    + *            and waits for the response from the server and does the handshake.
    + *
    + *  - HTTP for the Spark UI -> the UI was changed to use servlets so that javax servlet filters 
    + *            can be used. Yarn requires a specific AmIpFilter be installed for security to work
    + *            properly. For non-Yarn deployments, users can write a filter to go through a
    + *            companies normal login service. If an authentication filter is in place then the
    + *            SparkUI can be configured to check the logged in user against the list of users who
    + *            have view acls to see if that user is authorized.
    + *            The filters can also be used for many different purposes. For instance filters 
    + *            could be used for logging, encypryption, or compression.
    + *            
    + *  The exact mechanisms used to generate/distributed the shared secret is deployment specific.
    + * 
    + *  For Yarn deployments, the secret is automatically generated using the Akka remote
    + *  Crypt.generateSecureCookie() API. The secret is placed in the Hadoop UGI which gets passed
    + *  around via the Hadoop RPC mechanism. Hadoop RPC can be configured to support different levels
    + *  of protection. See the Hadoop documentation for more details. Each Spark application on Yarn
    + *  AmIpFilter which requires the user to go through the ResourceManager Proxy. That Proxy is there
    + *  to reduce the possibility of web based attacks through YARN. Hadoop can be configured to use
    + *  filters to do authentication. That authentication then happens via the ResourceManager Proxy
    + *  and Spark will use that to do authorization against the view acls.
    + * 
    + *  For other Spark deployments, the shared secret should be specified via the SPARK_SECRET 
    + *  environment variable. This isn't ideal but it means only the user who starts the process 
    + *  has access to view that variable. Note that Spark does try to generate a secret for
    + *  you if the SPARK_SECRET environment variable is not set, but it gets put into the java
    + *  system property which can be viewed by other users, so setting the SPARK_SECRET environment
    + *  variable is recommended.
    + *  All the nodes (Master and Workers) need to have the same shared secret
    + *  and all the applications running need to have that same shared secret. This again
    + *  is not ideal as one user could potentially affect another users application.
    + *  This should be enhanced in the future to provide better protection.
    + *  If the UI needs to be secured the user needs to install a javax servlet filter to do the
    + *  authentication. Spark will then use that user to compare against the view acls to do
    + *  authorization. If not filter is in place the user is generally null and no authorization
    + *  can take place.
    + */
    +private[spark] class SecurityManager extends Logging {
    +
    +  // key used to store the spark secret in the Hadoop UGI
    +  private val sparkSecretLookupKey = ""sparkCookie""
    +
    +
    +  // always add the current user and SPARK_USER to the viewAcls
    +  private val aclUsers = ArrayBuffer[String](System.getProperty(""user.name"", """"),
    +    Option(System.getenv(""SPARK_USER"")).getOrElse(""""))
    +  aclUsers ++= System.getProperty(""spark.ui.view.acls"", """").split(',')
    +  private val viewAcls = aclUsers.map(_.trim()).filter(!_.isEmpty).toSet
    +
    +  private val secretKey = generateSecretKey()
    +
    +  // Set our own authenticator to properly negotiate user/password for HTTP connections.
    +  // This is needed by the HTTP client fetching from the HttpServer. Put here so its 
    +  // only set once.
    +    Authenticator.setDefault(
    +      new Authenticator() {
    +        override def getPasswordAuthentication(): PasswordAuthentication = {
    +          var passAuth: PasswordAuthentication = null
    +          val userInfo = getRequestingURL().getUserInfo()
    +          if (userInfo != null) {
    +            val  parts = userInfo.split("":"", 2)
    +            passAuth = new PasswordAuthentication(parts(0), parts(1).toCharArray())
    +          }
    +          return passAuth
    +        }
    +      }
    +    );
    +  }
    +
    +  /**
    +   * Generates or looks up the secret key.
    +   *
    +   * The way the key is stored depends on the Spark deployment mode. Yarn
    +   * uses the Hadoop UGI.
    +   *
    +   * For non-Yarn deployments, If the environment variable is not set already 
    +   * we generate a secret and since we can't set an environment variable dynamically 
    +   * we set the java system property SPARK_SECRET. This will allow it to automatically
    +   * work in certain situations.  Others this still will not work and this definitely is 
    +   * not ideal since other users can see it. We should switch to put it in 
    +   * a config once Spark supports configs.
    +   */
    +  private def generateSecretKey(): String = {
    +    if (!isAuthenticationEnabled) return null
    +    // first check to see if the secret is already set, else generate a new one
    +    if (SparkHadoopUtil.get.isYarnMode) {
    +      val secretKey = SparkHadoopUtil.get.getSecretKeyFromUserCredentials(sparkSecretLookupKey)
    +      if (secretKey != null) {
    +        logDebug(""in yarn mode, getting secret from credentials"")
    +        return new Text(secretKey).toString
    +      } else {
    +        logDebug(""getSecretKey: yarn mode, secret key from credentials is null"")
    +      }
    +    }
    +    val secret = System.getProperty(""SPARK_SECRET"", System.getenv(""SPARK_SECRET"")) 
    +    if (secret != null && !secret.isEmpty()) return secret 
    +    // generate one 
    +    val sCookie = akka.util.Crypt.generateSecureCookie
    +
    +    // if we generated the secret then we must be the first so lets set it so t 
    +    // gets used by everyone else
    +    if (SparkHadoopUtil.get.isYarnMode) {
    +      SparkHadoopUtil.get.addSecretKeyToUserCredentials(sparkSecretLookupKey, sCookie)
    +      logDebug(""adding secret to credentials yarn mode"")
    +    } else {
    +      System.setProperty(""SPARK_SECRET"", sCookie)
    +      logDebug(""adding secret to java property"")
    +    }
    +    sCookie
    +  }
    +
    +  /**
    +   * Check to see if Acls for the UI are enabled
    +   * @return true if UI authentication is enabled, otherwise false
    +   */
    +
    +  /**
    +   * Checks the given user against the view acl list to see if they have 
    +   * authorization to view the UI.
    +   * @param user to see if is authorized
    +   * @return true is the user has permission, otherwise false 
    +   */
    +  def checkUIViewPermissions(user: String): Boolean = {
    +    if (uiAclsEnabled() && (user != null) && (!viewAcls.contains(user))) false else true
    --- End diff --
    
    It might be good to note in the javadoc that if ACL's are defined but disabled all viewers are assumed to have access regardless of the ACL. Currently it's sort of a hidden semantic in this function. Alternatively, maybe this could be restructured so that the caller is expected to check `uiAclsEnabled` first - and this only returns permissions based on the defined ACL if present. Since it's only called once that probably wouldn't be so bad.
    
    Up to you but it would be great to do one or the other.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Thu, 27 Feb 2014 21:09:24 +0000 (UTC)","[GitHub] spark pull request: Add Security to Spark - Akka, Http, Connection...",dev@spark.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/spark/pull/33#discussion_r10142402
  
    --- Diff: core/src/main/scala/org/apache/spark/SparkContext.scala ---
    @@ -135,6 +135,8 @@ class SparkContext(
     
       val isLocal = (master == ""local"" || master.startsWith(""local[""))
     
    +  if (master == ""yarn-client"") System.setProperty(""SPARK_YARN_MODE"", ""true"")
    --- End diff --
    
    Wondering - is there any reason not to make SPARK_YARN_MODE into a SparkConf value. E.g. spark.yarnMode? It seems like we only use it in places where spark conf is visible. Look, for instance at spark.driver.host and so on where we detect it at the driver and then set it to be passed to executors. It would be nice to try and standardize on SparkConf as the way of doing this unless there is some reason it won't work (which there may be...).


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Thu, 27 Feb 2014 21:11:41 +0000 (UTC)",[GitHub] spark pull request: [WIP] [SPARK-1146] Vagrant support for Spark,dev@spark.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/spark/pull/26#issuecomment-36291938
  
    @ngbinh when we launch a list of contrib projects, would you be interested in being included in it?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
ngbinh <git@git.apache.org>,"Thu, 27 Feb 2014 21:14:08 +0000 (UTC)",[GitHub] spark pull request: [WIP] [SPARK-1146] Vagrant support for Spark,dev@spark.apache.org,"Github user ngbinh commented on the pull request:

    https://github.com/apache/spark/pull/26#issuecomment-36292181
  
    @pwendell Yes! You can point to https://github.com/ngbinh/spark-vagrant


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
sryza <git@git.apache.org>,"Thu, 27 Feb 2014 21:28:26 +0000 (UTC)",,dev@spark.apache.org,"Github user sryza commented on the pull request:

    https://github.com/apache/spark/pull/29#issuecomment-36293609
  
    No problem.  I attached a new patch that does this.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Thu, 27 Feb 2014 21:34:56 +0000 (UTC)",[GitHub] spark pull request: [HOTFIX] Patching maven build after #6 (SPARK-...,dev@spark.apache.org,"GitHub user pwendell opened a pull request:

    https://github.com/apache/spark/pull/37

    [HOTFIX] Patching maven build after #6 (SPARK-1121).

    That patch removed the Maven avro declaration but didn't remove the
    actual dependency in core. /cc @scrapcodes

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/pwendell/spark master

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/spark/pull/37.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #37
    
----
commit 0ef3008ed3a27f26e76da1dd0d3a90d9310b625b
Author: Patrick Wendell <pwendell@gmail.com>
Date:   2014-02-27T21:23:11Z

    [HOTFIX] Patching maven build after #6 (SPARK-1121).
    
    That patch removed the Maven avro declaration but didn't remove the
    actual dependency in core.

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Thu, 27 Feb 2014 21:35:14 +0000 (UTC)",[GitHub] spark pull request: [HOTFIX] Patching maven build after #6 (SPARK-...,dev@spark.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/spark/pull/37#issuecomment-36294278
  
    Jenkins, test this please.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 21:38:52 +0000 (UTC)",[GitHub] spark pull request: [HOTFIX] Patching maven build after #6 (SPARK-...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/37#issuecomment-36294680
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 21:38:52 +0000 (UTC)",[GitHub] spark pull request: [HOTFIX] Patching maven build after #6 (SPARK-...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/37#issuecomment-36294679
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 21:39:04 +0000 (UTC)",[GitHub] spark pull request: [HOTFIX] Patching maven build after #6 (SPARK-...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/37#issuecomment-36294696
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Thu, 27 Feb 2014 21:40:32 +0000 (UTC)","[GitHub] spark pull request: Add Security to Spark - Akka, Http, Connection...",dev@spark.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/spark/pull/33#discussion_r10143746
  
    --- Diff: core/src/main/scala/org/apache/spark/SecurityManager.scala ---
    @@ -0,0 +1,259 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one or more
    + * contributor license agreements.  See the NOTICE file distributed with
    + * this work for additional information regarding copyright ownership.
    + * The ASF licenses this file to You under the Apache License, Version 2.0
    + * (the ""License""); you may not use this file except in compliance with
    + * the License.  You may obtain a copy of the License at
    + *
    + *    http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.spark
    +
    +import java.net.{Authenticator, PasswordAuthentication}
    +import org.apache.hadoop.io.Text
    +import org.apache.hadoop.security.Credentials
    +import org.apache.hadoop.security.UserGroupInformation
    +import org.apache.spark.deploy.SparkHadoopUtil
    +
    +import scala.collection.mutable.ArrayBuffer
    +
    +/** 
    + * Spark class responsible for security. 
    + * 
    + * In general this class should be instantiated by the SparkEnv and most components
    + * should access it from that. There are some cases where the SparkEnv hasn't been 
    + * initialized yet and this class must be instantiated directly.
    + * 
    + * Spark currently supports authentication via a shared secret.
    + * Authentication can be configured to be on via the 'spark.authenticate' configuration
    + * parameter. This parameter controls whether the Spark communication protocols do 
    + * authentication using the shared secret. This authentication is a basic handshake to
    + * make sure both sides have the same shared secret and are allowed to communicate.
    + * If the shared secret is not identical they will not be allowed to communicate. 
    + * 
    + * The Spark UI can also be secured by using javax servlet filters. A user may want to 
    + * secure the UI if it has data that other users should not be allowed to see. The javax 
    + * servlet filter specified by the user can authenticate the user and then once the user 
    + * is logged in, Spark can compare that user versus the view acls to make sure they are 
    + * authorized to view the UI. The configs 'spark.ui.acls.enable' and 'spark.ui.view.acls' 
    + * control the behavior of the acls. Note that the person who started the application
    + * always has view access to the UI.
    + *
    + * Spark does not currently support encryption after authentication.
    + * 
    + * At this point spark has multiple communication protocols that need to be secured and
    + * different underlying mechisms are used depending on the protocol:
    + *
    + *  - Akka -> The only option here is to use the Akka Remote secure-cookie functionality. 
    + *            Akka remoting allows you to specify a secure cookie that will be exchanged 
    + *            and ensured to be identical in the connection handshake between the client 
    + *            and the server. If they are not identical then the client will be refused 
    + *            to connect to the server. There is no control of the underlying 
    + *            authentication mechanism so its not clear if the password is passed in 
    + *            plaintext or uses DIGEST-MD5 or some other mechanism.
    + *            Akka also has an option to turn on SSL, this option is not currently supported
    + *            but we could add a configuration option in the future.
    + * 
    + *  - HTTP for broadcast and file server (via HttpServer) ->  Spark currently uses Jetty 
    + *            for the HttpServer. Jetty supports multiple authentication mechanisms - 
    + *            Basic, Digest, Form, Spengo, etc. It also supports multiple different login 
    + *            services - Hash, JAAS, Spnego, JDBC, etc.  Spark currently uses the HashLoginService
    + *            to authenticate using DIGEST-MD5 via a single user and the shared secret. 
    + *            Since we are using DIGEST-MD5, the shared secret is not passed on the wire
    + *            in plaintext.
    + *            We currently do not support SSL (https), but Jetty can be configured to use it
    + *            so we could add a configuration option for this in the future.
    + *            
    + *            The Spark HttpServer installs the HashLoginServer and configures it to DIGEST-MD5.
    + *            Any clients must specify the user and password. There is a default 
    + *            Authenticator installed in the SecurityManager to how it does the authentication
    + *            and in this case gets the user name and password from the request.
    + *
    + *  - ConnectionManager -> The Spark ConnectionManager uses java nio to asynchronously 
    + *            exchange messages.  For this we use the Java SASL 
    + *            (Simple Authentication and Security Layer) API and again use DIGEST-MD5 
    + *            as the authentication mechanism. This means the shared secret is not passed
    + *            over the wire in plaintext.
    + *            Note that SASL is pluggable as to what mechanism it uses.  We currently use
    + *            DIGEST-MD5 but this could be changed to use Kerberos or other in the future.
    + *            Spark currently supports ""auth"" for the quality of protection, which means
    + *            the connection is not supporting integrity or privacy protection (encryption)
    + *            after authentication. SASL also supports ""auth-int"" and ""auth-conf"" which 
    + *            SPARK could be support in the future to allow the user to specify the quality
    + *            of protection they want. If we support those, the messages will also have to 
    + *            be wrapped and unwrapped via the SaslServer/SaslClient.wrap/unwrap API's.
    + * 
    + *            Since the connectionManager does asynchronous messages passing, the SASL 
    + *            authentication is a bit more complex. A ConnectionManager can be both a client
    + *            and a Server, so for a particular connection is has to determine what to do.
    + *            A ConnectionId was added to be able to track connections and is used to 
    + *            match up incoming messages with connections waiting for authentication.
    + *            If its acting as a client and trying to send a message to another ConnectionManager,
    + *            it blocks the thread calling sendMessage until the SASL negotiation has occurred.
    + *            The ConnectionManager tracks all the sendingConnections using the ConnectionId
    + *            and waits for the response from the server and does the handshake.
    + *
    + *  - HTTP for the Spark UI -> the UI was changed to use servlets so that javax servlet filters 
    + *            can be used. Yarn requires a specific AmIpFilter be installed for security to work
    + *            properly. For non-Yarn deployments, users can write a filter to go through a
    + *            companies normal login service. If an authentication filter is in place then the
    + *            SparkUI can be configured to check the logged in user against the list of users who
    + *            have view acls to see if that user is authorized.
    + *            The filters can also be used for many different purposes. For instance filters 
    + *            could be used for logging, encypryption, or compression.
    + *            
    + *  The exact mechanisms used to generate/distributed the shared secret is deployment specific.
    + * 
    + *  For Yarn deployments, the secret is automatically generated using the Akka remote
    + *  Crypt.generateSecureCookie() API. The secret is placed in the Hadoop UGI which gets passed
    + *  around via the Hadoop RPC mechanism. Hadoop RPC can be configured to support different levels
    + *  of protection. See the Hadoop documentation for more details. Each Spark application on Yarn
    + *  AmIpFilter which requires the user to go through the ResourceManager Proxy. That Proxy is there
    + *  to reduce the possibility of web based attacks through YARN. Hadoop can be configured to use
    + *  filters to do authentication. That authentication then happens via the ResourceManager Proxy
    + *  and Spark will use that to do authorization against the view acls.
    + * 
    + *  For other Spark deployments, the shared secret should be specified via the SPARK_SECRET 
    + *  environment variable. This isn't ideal but it means only the user who starts the process 
    + *  has access to view that variable. Note that Spark does try to generate a secret for
    + *  you if the SPARK_SECRET environment variable is not set, but it gets put into the java
    + *  system property which can be viewed by other users, so setting the SPARK_SECRET environment
    + *  variable is recommended.
    + *  All the nodes (Master and Workers) need to have the same shared secret
    + *  and all the applications running need to have that same shared secret. This again
    + *  is not ideal as one user could potentially affect another users application.
    + *  This should be enhanced in the future to provide better protection.
    + *  If the UI needs to be secured the user needs to install a javax servlet filter to do the
    + *  authentication. Spark will then use that user to compare against the view acls to do
    + *  authorization. If not filter is in place the user is generally null and no authorization
    + *  can take place.
    + */
    +private[spark] class SecurityManager extends Logging {
    +
    +  // key used to store the spark secret in the Hadoop UGI
    +  private val sparkSecretLookupKey = ""sparkCookie""
    +
    +
    +  // always add the current user and SPARK_USER to the viewAcls
    +  private val aclUsers = ArrayBuffer[String](System.getProperty(""user.name"", """"),
    +    Option(System.getenv(""SPARK_USER"")).getOrElse(""""))
    +  aclUsers ++= System.getProperty(""spark.ui.view.acls"", """").split(',')
    +  private val viewAcls = aclUsers.map(_.trim()).filter(!_.isEmpty).toSet
    +
    +  private val secretKey = generateSecretKey()
    +
    +  // Set our own authenticator to properly negotiate user/password for HTTP connections.
    +  // This is needed by the HTTP client fetching from the HttpServer. Put here so its 
    +  // only set once.
    +    Authenticator.setDefault(
    +      new Authenticator() {
    +        override def getPasswordAuthentication(): PasswordAuthentication = {
    +          var passAuth: PasswordAuthentication = null
    +          val userInfo = getRequestingURL().getUserInfo()
    +          if (userInfo != null) {
    +            val  parts = userInfo.split("":"", 2)
    +            passAuth = new PasswordAuthentication(parts(0), parts(1).toCharArray())
    +          }
    +          return passAuth
    +        }
    +      }
    +    );
    --- End diff --
    
    I think this semi-colon is extra


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Thu, 27 Feb 2014 21:43:05 +0000 (UTC)","[GitHub] spark pull request: Add Security to Spark - Akka, Http, Connection...",dev@spark.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/spark/pull/33#discussion_r10143842
  
    --- Diff: core/src/main/scala/org/apache/spark/SecurityManager.scala ---
    @@ -0,0 +1,259 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one or more
    + * contributor license agreements.  See the NOTICE file distributed with
    + * this work for additional information regarding copyright ownership.
    + * The ASF licenses this file to You under the Apache License, Version 2.0
    + * (the ""License""); you may not use this file except in compliance with
    + * the License.  You may obtain a copy of the License at
    + *
    + *    http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.spark
    +
    +import java.net.{Authenticator, PasswordAuthentication}
    +import org.apache.hadoop.io.Text
    +import org.apache.hadoop.security.Credentials
    +import org.apache.hadoop.security.UserGroupInformation
    +import org.apache.spark.deploy.SparkHadoopUtil
    +
    +import scala.collection.mutable.ArrayBuffer
    +
    +/** 
    + * Spark class responsible for security. 
    + * 
    + * In general this class should be instantiated by the SparkEnv and most components
    + * should access it from that. There are some cases where the SparkEnv hasn't been 
    + * initialized yet and this class must be instantiated directly.
    + * 
    + * Spark currently supports authentication via a shared secret.
    + * Authentication can be configured to be on via the 'spark.authenticate' configuration
    + * parameter. This parameter controls whether the Spark communication protocols do 
    + * authentication using the shared secret. This authentication is a basic handshake to
    + * make sure both sides have the same shared secret and are allowed to communicate.
    + * If the shared secret is not identical they will not be allowed to communicate. 
    + * 
    + * The Spark UI can also be secured by using javax servlet filters. A user may want to 
    + * secure the UI if it has data that other users should not be allowed to see. The javax 
    + * servlet filter specified by the user can authenticate the user and then once the user 
    + * is logged in, Spark can compare that user versus the view acls to make sure they are 
    + * authorized to view the UI. The configs 'spark.ui.acls.enable' and 'spark.ui.view.acls' 
    + * control the behavior of the acls. Note that the person who started the application
    + * always has view access to the UI.
    + *
    + * Spark does not currently support encryption after authentication.
    + * 
    + * At this point spark has multiple communication protocols that need to be secured and
    + * different underlying mechisms are used depending on the protocol:
    + *
    + *  - Akka -> The only option here is to use the Akka Remote secure-cookie functionality. 
    + *            Akka remoting allows you to specify a secure cookie that will be exchanged 
    + *            and ensured to be identical in the connection handshake between the client 
    + *            and the server. If they are not identical then the client will be refused 
    + *            to connect to the server. There is no control of the underlying 
    + *            authentication mechanism so its not clear if the password is passed in 
    + *            plaintext or uses DIGEST-MD5 or some other mechanism.
    + *            Akka also has an option to turn on SSL, this option is not currently supported
    + *            but we could add a configuration option in the future.
    + * 
    + *  - HTTP for broadcast and file server (via HttpServer) ->  Spark currently uses Jetty 
    + *            for the HttpServer. Jetty supports multiple authentication mechanisms - 
    + *            Basic, Digest, Form, Spengo, etc. It also supports multiple different login 
    + *            services - Hash, JAAS, Spnego, JDBC, etc.  Spark currently uses the HashLoginService
    + *            to authenticate using DIGEST-MD5 via a single user and the shared secret. 
    + *            Since we are using DIGEST-MD5, the shared secret is not passed on the wire
    + *            in plaintext.
    + *            We currently do not support SSL (https), but Jetty can be configured to use it
    + *            so we could add a configuration option for this in the future.
    + *            
    + *            The Spark HttpServer installs the HashLoginServer and configures it to DIGEST-MD5.
    + *            Any clients must specify the user and password. There is a default 
    + *            Authenticator installed in the SecurityManager to how it does the authentication
    + *            and in this case gets the user name and password from the request.
    + *
    + *  - ConnectionManager -> The Spark ConnectionManager uses java nio to asynchronously 
    + *            exchange messages.  For this we use the Java SASL 
    + *            (Simple Authentication and Security Layer) API and again use DIGEST-MD5 
    + *            as the authentication mechanism. This means the shared secret is not passed
    + *            over the wire in plaintext.
    + *            Note that SASL is pluggable as to what mechanism it uses.  We currently use
    + *            DIGEST-MD5 but this could be changed to use Kerberos or other in the future.
    + *            Spark currently supports ""auth"" for the quality of protection, which means
    + *            the connection is not supporting integrity or privacy protection (encryption)
    + *            after authentication. SASL also supports ""auth-int"" and ""auth-conf"" which 
    + *            SPARK could be support in the future to allow the user to specify the quality
    + *            of protection they want. If we support those, the messages will also have to 
    + *            be wrapped and unwrapped via the SaslServer/SaslClient.wrap/unwrap API's.
    + * 
    + *            Since the connectionManager does asynchronous messages passing, the SASL 
    + *            authentication is a bit more complex. A ConnectionManager can be both a client
    + *            and a Server, so for a particular connection is has to determine what to do.
    + *            A ConnectionId was added to be able to track connections and is used to 
    + *            match up incoming messages with connections waiting for authentication.
    + *            If its acting as a client and trying to send a message to another ConnectionManager,
    + *            it blocks the thread calling sendMessage until the SASL negotiation has occurred.
    + *            The ConnectionManager tracks all the sendingConnections using the ConnectionId
    + *            and waits for the response from the server and does the handshake.
    + *
    + *  - HTTP for the Spark UI -> the UI was changed to use servlets so that javax servlet filters 
    + *            can be used. Yarn requires a specific AmIpFilter be installed for security to work
    + *            properly. For non-Yarn deployments, users can write a filter to go through a
    + *            companies normal login service. If an authentication filter is in place then the
    + *            SparkUI can be configured to check the logged in user against the list of users who
    + *            have view acls to see if that user is authorized.
    + *            The filters can also be used for many different purposes. For instance filters 
    + *            could be used for logging, encypryption, or compression.
    + *            
    + *  The exact mechanisms used to generate/distributed the shared secret is deployment specific.
    + * 
    + *  For Yarn deployments, the secret is automatically generated using the Akka remote
    + *  Crypt.generateSecureCookie() API. The secret is placed in the Hadoop UGI which gets passed
    + *  around via the Hadoop RPC mechanism. Hadoop RPC can be configured to support different levels
    + *  of protection. See the Hadoop documentation for more details. Each Spark application on Yarn
    + *  AmIpFilter which requires the user to go through the ResourceManager Proxy. That Proxy is there
    + *  to reduce the possibility of web based attacks through YARN. Hadoop can be configured to use
    + *  filters to do authentication. That authentication then happens via the ResourceManager Proxy
    + *  and Spark will use that to do authorization against the view acls.
    + * 
    + *  For other Spark deployments, the shared secret should be specified via the SPARK_SECRET 
    + *  environment variable. This isn't ideal but it means only the user who starts the process 
    + *  has access to view that variable. Note that Spark does try to generate a secret for
    + *  you if the SPARK_SECRET environment variable is not set, but it gets put into the java
    + *  system property which can be viewed by other users, so setting the SPARK_SECRET environment
    + *  variable is recommended.
    + *  All the nodes (Master and Workers) need to have the same shared secret
    + *  and all the applications running need to have that same shared secret. This again
    + *  is not ideal as one user could potentially affect another users application.
    + *  This should be enhanced in the future to provide better protection.
    + *  If the UI needs to be secured the user needs to install a javax servlet filter to do the
    + *  authentication. Spark will then use that user to compare against the view acls to do
    + *  authorization. If not filter is in place the user is generally null and no authorization
    + *  can take place.
    + */
    +private[spark] class SecurityManager extends Logging {
    +
    +  // key used to store the spark secret in the Hadoop UGI
    +  private val sparkSecretLookupKey = ""sparkCookie""
    +
    +
    +  // always add the current user and SPARK_USER to the viewAcls
    +  private val aclUsers = ArrayBuffer[String](System.getProperty(""user.name"", """"),
    +    Option(System.getenv(""SPARK_USER"")).getOrElse(""""))
    +  aclUsers ++= System.getProperty(""spark.ui.view.acls"", """").split(',')
    +  private val viewAcls = aclUsers.map(_.trim()).filter(!_.isEmpty).toSet
    +
    +  private val secretKey = generateSecretKey()
    --- End diff --
    
    it might be good to log at INFO level some details about the security set-up. That way people can understand the state of their job.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Thu, 27 Feb 2014 21:48:35 +0000 (UTC)","[GitHub] spark pull request: Add Security to Spark - Akka, Http, Connection...",dev@spark.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/spark/pull/33#discussion_r10144073
  
    --- Diff: core/src/main/scala/org/apache/spark/SecurityManager.scala ---
    @@ -0,0 +1,259 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one or more
    + * contributor license agreements.  See the NOTICE file distributed with
    + * this work for additional information regarding copyright ownership.
    + * The ASF licenses this file to You under the Apache License, Version 2.0
    + * (the ""License""); you may not use this file except in compliance with
    + * the License.  You may obtain a copy of the License at
    + *
    + *    http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.spark
    +
    +import java.net.{Authenticator, PasswordAuthentication}
    +import org.apache.hadoop.io.Text
    +import org.apache.hadoop.security.Credentials
    +import org.apache.hadoop.security.UserGroupInformation
    +import org.apache.spark.deploy.SparkHadoopUtil
    +
    +import scala.collection.mutable.ArrayBuffer
    +
    +/** 
    + * Spark class responsible for security. 
    + * 
    + * In general this class should be instantiated by the SparkEnv and most components
    + * should access it from that. There are some cases where the SparkEnv hasn't been 
    + * initialized yet and this class must be instantiated directly.
    + * 
    + * Spark currently supports authentication via a shared secret.
    + * Authentication can be configured to be on via the 'spark.authenticate' configuration
    + * parameter. This parameter controls whether the Spark communication protocols do 
    + * authentication using the shared secret. This authentication is a basic handshake to
    + * make sure both sides have the same shared secret and are allowed to communicate.
    + * If the shared secret is not identical they will not be allowed to communicate. 
    + * 
    + * The Spark UI can also be secured by using javax servlet filters. A user may want to 
    + * secure the UI if it has data that other users should not be allowed to see. The javax 
    + * servlet filter specified by the user can authenticate the user and then once the user 
    + * is logged in, Spark can compare that user versus the view acls to make sure they are 
    + * authorized to view the UI. The configs 'spark.ui.acls.enable' and 'spark.ui.view.acls' 
    + * control the behavior of the acls. Note that the person who started the application
    + * always has view access to the UI.
    + *
    + * Spark does not currently support encryption after authentication.
    + * 
    + * At this point spark has multiple communication protocols that need to be secured and
    + * different underlying mechisms are used depending on the protocol:
    + *
    + *  - Akka -> The only option here is to use the Akka Remote secure-cookie functionality. 
    + *            Akka remoting allows you to specify a secure cookie that will be exchanged 
    + *            and ensured to be identical in the connection handshake between the client 
    + *            and the server. If they are not identical then the client will be refused 
    + *            to connect to the server. There is no control of the underlying 
    + *            authentication mechanism so its not clear if the password is passed in 
    + *            plaintext or uses DIGEST-MD5 or some other mechanism.
    + *            Akka also has an option to turn on SSL, this option is not currently supported
    + *            but we could add a configuration option in the future.
    + * 
    + *  - HTTP for broadcast and file server (via HttpServer) ->  Spark currently uses Jetty 
    + *            for the HttpServer. Jetty supports multiple authentication mechanisms - 
    + *            Basic, Digest, Form, Spengo, etc. It also supports multiple different login 
    + *            services - Hash, JAAS, Spnego, JDBC, etc.  Spark currently uses the HashLoginService
    + *            to authenticate using DIGEST-MD5 via a single user and the shared secret. 
    + *            Since we are using DIGEST-MD5, the shared secret is not passed on the wire
    + *            in plaintext.
    + *            We currently do not support SSL (https), but Jetty can be configured to use it
    + *            so we could add a configuration option for this in the future.
    + *            
    + *            The Spark HttpServer installs the HashLoginServer and configures it to DIGEST-MD5.
    + *            Any clients must specify the user and password. There is a default 
    + *            Authenticator installed in the SecurityManager to how it does the authentication
    + *            and in this case gets the user name and password from the request.
    + *
    + *  - ConnectionManager -> The Spark ConnectionManager uses java nio to asynchronously 
    + *            exchange messages.  For this we use the Java SASL 
    + *            (Simple Authentication and Security Layer) API and again use DIGEST-MD5 
    + *            as the authentication mechanism. This means the shared secret is not passed
    + *            over the wire in plaintext.
    + *            Note that SASL is pluggable as to what mechanism it uses.  We currently use
    + *            DIGEST-MD5 but this could be changed to use Kerberos or other in the future.
    + *            Spark currently supports ""auth"" for the quality of protection, which means
    + *            the connection is not supporting integrity or privacy protection (encryption)
    + *            after authentication. SASL also supports ""auth-int"" and ""auth-conf"" which 
    + *            SPARK could be support in the future to allow the user to specify the quality
    + *            of protection they want. If we support those, the messages will also have to 
    + *            be wrapped and unwrapped via the SaslServer/SaslClient.wrap/unwrap API's.
    + * 
    + *            Since the connectionManager does asynchronous messages passing, the SASL 
    + *            authentication is a bit more complex. A ConnectionManager can be both a client
    + *            and a Server, so for a particular connection is has to determine what to do.
    + *            A ConnectionId was added to be able to track connections and is used to 
    + *            match up incoming messages with connections waiting for authentication.
    + *            If its acting as a client and trying to send a message to another ConnectionManager,
    + *            it blocks the thread calling sendMessage until the SASL negotiation has occurred.
    + *            The ConnectionManager tracks all the sendingConnections using the ConnectionId
    + *            and waits for the response from the server and does the handshake.
    + *
    + *  - HTTP for the Spark UI -> the UI was changed to use servlets so that javax servlet filters 
    + *            can be used. Yarn requires a specific AmIpFilter be installed for security to work
    + *            properly. For non-Yarn deployments, users can write a filter to go through a
    + *            companies normal login service. If an authentication filter is in place then the
    + *            SparkUI can be configured to check the logged in user against the list of users who
    + *            have view acls to see if that user is authorized.
    + *            The filters can also be used for many different purposes. For instance filters 
    + *            could be used for logging, encypryption, or compression.
    + *            
    + *  The exact mechanisms used to generate/distributed the shared secret is deployment specific.
    + * 
    + *  For Yarn deployments, the secret is automatically generated using the Akka remote
    + *  Crypt.generateSecureCookie() API. The secret is placed in the Hadoop UGI which gets passed
    + *  around via the Hadoop RPC mechanism. Hadoop RPC can be configured to support different levels
    + *  of protection. See the Hadoop documentation for more details. Each Spark application on Yarn
    + *  AmIpFilter which requires the user to go through the ResourceManager Proxy. That Proxy is there
    + *  to reduce the possibility of web based attacks through YARN. Hadoop can be configured to use
    + *  filters to do authentication. That authentication then happens via the ResourceManager Proxy
    + *  and Spark will use that to do authorization against the view acls.
    + * 
    + *  For other Spark deployments, the shared secret should be specified via the SPARK_SECRET 
    + *  environment variable. This isn't ideal but it means only the user who starts the process 
    + *  has access to view that variable. Note that Spark does try to generate a secret for
    + *  you if the SPARK_SECRET environment variable is not set, but it gets put into the java
    + *  system property which can be viewed by other users, so setting the SPARK_SECRET environment
    + *  variable is recommended.
    + *  All the nodes (Master and Workers) need to have the same shared secret
    + *  and all the applications running need to have that same shared secret. This again
    + *  is not ideal as one user could potentially affect another users application.
    + *  This should be enhanced in the future to provide better protection.
    + *  If the UI needs to be secured the user needs to install a javax servlet filter to do the
    + *  authentication. Spark will then use that user to compare against the view acls to do
    + *  authorization. If not filter is in place the user is generally null and no authorization
    + *  can take place.
    + */
    +private[spark] class SecurityManager extends Logging {
    +
    +  // key used to store the spark secret in the Hadoop UGI
    +  private val sparkSecretLookupKey = ""sparkCookie""
    +
    --- End diff --
    
    System properties are used a lot here, but it would be good to read these things from the SparkConf that is contained in SparkEnv. Then SecurityManager could just take a conf as a constructor. For the standalone cluster you can just create a new SparkConf() and it will load system properties automatically.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Thu, 27 Feb 2014 21:50:35 +0000 (UTC)","[GitHub] spark pull request: Add Security to Spark - Akka, Http, Connection...",dev@spark.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/spark/pull/33#discussion_r10144156
  
    --- Diff: core/src/main/scala/org/apache/spark/ui/JettyUtils.scala ---
    @@ -100,17 +152,12 @@ private[spark] object JettyUtils extends Logging {
        * If the desired port number is contented, continues incrementing ports until a free port is
        * found. Returns the chosen port and the jetty Server object.
        */
    -  def startJettyServer(hostName: String, port: Int, handlers: Seq[(String, Handler)]): (Server, Int)
    -  = {
    -
    -    val handlersToRegister = handlers.map { case(path, handler) =>
    -      val contextHandler = new ContextHandler(path)
    -      contextHandler.setHandler(handler)
    -      contextHandler.asInstanceOf[org.eclipse.jetty.server.Handler]
    -    }
    +  def startJettyServer(hostName: String, port: Int, handlers: Seq[ServletContextHandler]): 
    --- End diff --
    
    I would take a SparkConf here and pass it down to get the filters from that.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Thu, 27 Feb 2014 21:55:54 +0000 (UTC)","[GitHub] spark pull request: Add Security to Spark - Akka, Http, Connection...",dev@spark.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/spark/pull/33#discussion_r10144358
  
    --- Diff: core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerWebUI.scala ---
    @@ -198,6 +199,6 @@ class WorkerWebUI(val worker: Worker, val workDir: File, requestedPort: Option[I
     }
     
     private[spark] object WorkerWebUI {
    -  val STATIC_RESOURCE_DIR = ""org/apache/spark/ui/static""
    +  val STATIC_RESOURCE_DIR = ""org/apache/spark/ui""
    --- End diff --
    
    maybe change this to STATIC_RESOURCE_BASE or something since it's no longer the directory that contains them.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Thu, 27 Feb 2014 22:05:25 +0000 (UTC)","[GitHub] spark pull request: Add Security to Spark - Akka, Http, Connection...",dev@spark.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/spark/pull/33#discussion_r10144726
  
    --- Diff: core/src/main/scala/org/apache/spark/network/Connection.scala ---
    @@ -18,25 +18,27 @@
     package org.apache.spark.network
     
     import org.apache.spark._
    +import org.apache.spark.SparkSaslServer
     
     import scala.collection.mutable.{HashMap, Queue, ArrayBuffer}
     
    -import java.io._
     import java.nio._
     import java.nio.channels._
    -import java.nio.channels.spi._
     import java.net._
     
     
     private[spark]
     abstract class Connection(val channel: SocketChannel, val selector: Selector,
    -    val socketRemoteConnectionManagerId: ConnectionManagerId)
    +    val socketRemoteConnectionManagerId: ConnectionManagerId, val connectionId: ConnectionId)
       extends Logging {
     
    -  def this(channel_ : SocketChannel, selector_ : Selector) = {
    +  var sparkSaslServer : SparkSaslServer = null
    --- End diff --
    
    minor style thing, but these should all be `val x: Type = yy` rather than `val x : Type = y` same below in `def this`.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Thu, 27 Feb 2014 22:06:50 +0000 (UTC)","[GitHub] spark pull request: Add Security to Spark - Akka, Http, Connection...",dev@spark.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/spark/pull/33#discussion_r10144775
  
    --- Diff: core/src/main/scala/org/apache/spark/network/Connection.scala ---
    @@ -18,25 +18,27 @@
     package org.apache.spark.network
     
     import org.apache.spark._
    +import org.apache.spark.SparkSaslServer
     
     import scala.collection.mutable.{HashMap, Queue, ArrayBuffer}
     
    -import java.io._
     import java.nio._
     import java.nio.channels._
    -import java.nio.channels.spi._
     import java.net._
     
     
     private[spark]
     abstract class Connection(val channel: SocketChannel, val selector: Selector,
    -    val socketRemoteConnectionManagerId: ConnectionManagerId)
    +    val socketRemoteConnectionManagerId: ConnectionManagerId, val connectionId: ConnectionId)
       extends Logging {
     
    -  def this(channel_ : SocketChannel, selector_ : Selector) = {
    +  var sparkSaslServer : SparkSaslServer = null
    --- End diff --
    
    There are a bunch of instances also below in this file 


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 22:08:00 +0000 (UTC)",[GitHub] spark pull request: [HOTFIX] Patching maven build after #6 (SPARK-...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/37#issuecomment-36297710
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Thu, 27 Feb 2014 22:08:05 +0000 (UTC)","[GitHub] spark pull request: Add Security to Spark - Akka, Http, Connection...",dev@spark.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/spark/pull/33#discussion_r10144822
  
    --- Diff: core/src/main/scala/org/apache/spark/network/ConnectionManager.scala ---
    @@ -53,6 +54,10 @@ private[spark] class ConnectionManager(port: Int, conf: SparkConf) extends Loggi
     
       private val selector = SelectorProvider.provider.openSelector()
     
    +  // default to 30 second timeout waiting for authentication
    +  private val authTimeout = System.getProperty(""spark.core.connection.auth.wait.timeout"",
    --- End diff --
    
    Use conf here.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 22:08:00 +0000 (UTC)",[GitHub] spark pull request: [HOTFIX] Patching maven build after #6 (SPARK-...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/37#issuecomment-36297711
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12921/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Thu, 27 Feb 2014 22:09:20 +0000 (UTC)","[GitHub] spark pull request: Add Security to Spark - Akka, Http, Connection...",dev@spark.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/spark/pull/33#discussion_r10144869
  
    --- Diff: docs/configuration.md ---
    @@ -477,6 +505,21 @@ Apart from these, the following properties are also available, and may be useful
       <td>
         Whether to overwrite files added through SparkContext.addFile() when the target file exists and its contents do not match those of the source.
       </td>
    +<tr>  
    +  <td>spark.authenticate</td>
    +  <td>false</td>
    +  <td>
    +    Whether spark authenticates its internal connections. See <code>SPARK_SECRET</code> if not
    +    running on Yarn.
    +  </td>
    +</tr>
    +<tr>  
    +  <td>spark.core.connection.auth.wait.timeout</td>
    --- End diff --
    
    Since `spark.worker.timeout` and `spark.akka.timeout` are in seconds, maybe it makes sense for this to also be in seconds, rather than milliseconds?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
dianacarroll <git@git.apache.org>,"Thu, 27 Feb 2014 22:43:25 +0000 (UTC)",[GitHub] spark pull request: SPARK-1134 ,dev@spark.apache.org,"GitHub user dianacarroll opened a pull request:

    https://github.com/apache/spark/pull/38

    SPARK-1134 

    bug with ipython: startup scripts are called in both interactive and non-interactive mode.  This prevents non-interactive use with spark because the spark startup  script and user script both attempt to create a SparkContext.  This changes the pyspark script to only call ipython if no command line arguments were supplied, ie: call ipython for interactive use only.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/dianacarroll/spark master

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/spark/pull/38.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #38
    
----
commit 0309cf911cc0379b7de871463c25a7c105202f4b
Author: Diana Carroll <dcarroll@cloudera.com>
Date:   2014-02-27T22:39:43Z

    SPARK-1134 bug with ipython prevents non-interactive use with spark; only call ipython if no command line arguments were supplied

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 22:43:51 +0000 (UTC)",[GitHub] spark pull request: SPARK-1134 ,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/38#issuecomment-36301181
  
    Can one of the admins verify this patch?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 22:44:02 +0000 (UTC)",[GitHub] spark pull request: SPARK-1134 ,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/38#issuecomment-36301198
  
    Can one of the admins verify this patch?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Thu, 27 Feb 2014 23:06:36 +0000 (UTC)",[GitHub] spark pull request: [HOTFIX] Patching maven build after #6 (SPARK-...,dev@spark.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/spark/pull/37#issuecomment-36303359
  
    Merged into master.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
asfgit <git@git.apache.org>,"Thu, 27 Feb 2014 23:22:17 +0000 (UTC)",[GitHub] spark pull request: [HOTFIX] Patching maven build after #6 (SPARK-...,dev@spark.apache.org,"Github user asfgit closed the pull request at:

    https://github.com/apache/spark/pull/37


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
kayousterhout <git@git.apache.org>,"Thu, 27 Feb 2014 23:35:39 +0000 (UTC)",[GitHub] spark pull request: Remote BlockFetchTracker trait,dev@spark.apache.org,"GitHub user kayousterhout opened a pull request:

    https://github.com/apache/spark/pull/39

    Remote BlockFetchTracker trait

    This trait seems to have been created a while ago when there
    were multiple implementations; now that there's just one, I think it
    makes sense to merge it into the BlockFetcherIterator trait.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/kayousterhout/spark-1 remove_tracker

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/spark/pull/39.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #39
    
----
commit 56f84a8c99e13218f1abfdbe17961756a13b7688
Author: Kay Ousterhout <kayousterhout@gmail.com>
Date:   2014-02-27T23:28:20Z

    Remote BlockFetchTracker.
    
    This trait seems to have been created a while ago when there
    were multiple implementations; now that there's just one, there's
    no reason not to merge it into the BlockFetcherIterator trait.

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 23:38:52 +0000 (UTC)",[GitHub] spark pull request: Remote BlockFetchTracker trait,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/39#issuecomment-36305800
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 23:38:53 +0000 (UTC)",[GitHub] spark pull request: Remote BlockFetchTracker trait,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/39#issuecomment-36305801
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 23:39:04 +0000 (UTC)",[GitHub] spark pull request: Remote BlockFetchTracker trait,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/39#issuecomment-36305816
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
dbtsai <git@git.apache.org>,"Thu, 27 Feb 2014 23:43:34 +0000 (UTC)",[GitHub] spark pull request: Initialized the regVal for first iteration in ...,dev@spark.apache.org,"GitHub user dbtsai opened a pull request:

    https://github.com/apache/spark/pull/40

    Initialized the regVal for first iteration in SGD optimizer 

    Ported from https://github.com/apache/incubator-spark/pull/633
    
    In runMiniBatchSGD, the regVal (for 1st iter) should be initialized
    as sum of sqrt of weights if it's L2 update; for L1 update, the same logic is followed.
    
    It maybe not be important here for SGD since the updater doesn't take the loss
    as parameter to find the new weights. But it will give us the correct history of loss.
    However, for LBFGS optimizer we implemented, the correct loss with regVal is crucial to
    find the new weights.


You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/AlpineNow/spark dbtsai-smallRegValFix

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/spark/pull/40.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #40
    
----
commit 9d2670330ebcde4240d97f0f51d7cfbc71509780
Author: DB Tsai <dbtsai@dbtsai.com>
Date:   2014-02-22T10:59:00Z

    In runMiniBatchSGD, the regVal (for 1st iter) should be initialized
    as sum of sqrt of weights if it's L2 update; for L1 update, the same logic is followed.
    
    It maybe not be important here for SGD since the updater doesn't take the loss
    as parameter to find the new weights. But it will give us the correct history of loss.
    However, for LBFGS optimizer we implemented, the correct loss with regVal is crucial to
    find the new weights.

commit 1c15cfebf8a130744ddea915dd457a8e4291bdfe
Author: DB Tsai <dbtsai@dbtsai.com>
Date:   2014-02-25T22:02:12Z

    Added unittest for the change in runMiniBatchSGD

commit 594a288acd91dd9b426d9327f2b8f4db828321d3
Author: DB Tsai <dbtsai@dbtsai.com>
Date:   2014-02-27T09:37:39Z

    Removed unnecessary parentheses, and fixed a typo.

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 23:43:52 +0000 (UTC)",[GitHub] spark pull request: Initialized the regVal for first iteration in ...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/40#issuecomment-36306177
  
    Can one of the admins verify this patch?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 23:44:04 +0000 (UTC)",[GitHub] spark pull request: Initialized the regVal for first iteration in ...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/40#issuecomment-36306198
  
    Can one of the admins verify this patch?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
ngbinh <git@git.apache.org>,"Thu, 27 Feb 2014 23:52:55 +0000 (UTC)",[GitHub] spark pull request: Update io.netty from 4.0.13 Final to 4.0.17.Fi...,dev@spark.apache.org,"GitHub user ngbinh opened a pull request:

    https://github.com/apache/spark/pull/41

    Update io.netty from 4.0.13 Final to 4.0.17.Final

    This update contains a lot of bug fixes and some new perf improvements.
    It is also binary compatible with the current 4.0.13.Final
    
    For more information: http://netty.io/news/2014/02/25/4-0-17-Final.html

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/ngbinh/spark master

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/spark/pull/41.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #41
    
----
commit c782207f90cf66339268b5265b49b2e50a22a1a5
Author: Binh Nguyen <bnguyen@palantir.com>
Date:   2014-02-27T23:48:38Z

    update io.netty to 4.0.17.Final
    
    This update contains a lot of bug fixes and some new perf improvements.
    It is also binary compatible with the current 4.0.13.Final
    
    For more information: http://netty.io/news/2014/02/25/4-0-17-Final.html

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 23:53:54 +0000 (UTC)",[GitHub] spark pull request: Update io.netty from 4.0.13 Final to 4.0.17.Fi...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/41#issuecomment-36306872
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 23:53:54 +0000 (UTC)",[GitHub] spark pull request: Update io.netty from 4.0.13 Final to 4.0.17.Fi...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/41#issuecomment-36306871
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Thu, 27 Feb 2014 23:54:04 +0000 (UTC)",[GitHub] spark pull request: Update io.netty from 4.0.13 Final to 4.0.17.Fi...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/41#issuecomment-36306883
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 00:07:27 +0000 (UTC)",[GitHub] spark pull request: Remote BlockFetchTracker trait,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/39#issuecomment-36307822
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 00:07:28 +0000 (UTC)",[GitHub] spark pull request: Remote BlockFetchTracker trait,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/39#issuecomment-36307824
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12922/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Fri, 28 Feb 2014 00:12:49 +0000 (UTC)","[GitHub] spark pull request: Add Security to Spark - Akka, Http, Connection...",dev@spark.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/spark/pull/33#discussion_r10149145
  
    --- Diff: core/src/main/scala/org/apache/spark/network/ConnectionId.scala ---
    @@ -0,0 +1,34 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one or more
    + * contributor license agreements.  See the NOTICE file distributed with
    + * this work for additional information regarding copyright ownership.
    + * The ASF licenses this file to You under the Apache License, Version 2.0
    + * (the ""License""); you may not use this file except in compliance with
    + * the License.  You may obtain a copy of the License at
    + *
    + *    http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.spark.network
    +
    +private[spark] case class ConnectionId(connectionManagerId: ConnectionManagerId, uniqId: Int) {
    +  override def toString = connectionManagerId.host + ""_"" + connectionManagerId.port + ""_"" + uniqId  
    +}
    +
    +private[spark] object ConnectionId {
    +
    +  def createConnectionIdFromString(connectionIdString: String) : ConnectionId = {
    --- End diff --
    
        def createConnectionIdFromString(connectionIdString: String): ConnectionId = {


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Fri, 28 Feb 2014 00:17:17 +0000 (UTC)","[GitHub] spark pull request: Add Security to Spark - Akka, Http, Connection...",dev@spark.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/spark/pull/33#discussion_r10149280
  
    --- Diff: core/src/main/scala/org/apache/spark/network/ConnectionManager.scala ---
    @@ -557,7 +754,54 @@ private[spark] class ConnectionManager(port: Int, conf: SparkConf) extends Loggi
         // useful in our test-env ... If we do re-add it, we should consistently use it everywhere I
         // guess ?
         val connection = connectionsById.getOrElseUpdate(connectionManagerId, startNewConnection())
    +    if (authEnabled) {
    +      checkSendAuthFirst(connectionManagerId, connection)
    +    }
         message.senderAddress = id.toSocketAddress()
    +    logDebug(""Before Sending ["" + message + ""] to ["" + connectionManagerId + ""]"" + "" "" +
    --- End diff --
    
    is this log statement O(messages sent) or just O(authentication messages). If it's the former, it would be good to make it logTrace. Also, if there is any other code in here that logs on every message it would be good to make that trace-level. Otherwise the debug log is not consumable.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 00:23:08 +0000 (UTC)",[GitHub] spark pull request: Update io.netty from 4.0.13 Final to 4.0.17.Fi...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/41#issuecomment-36308969
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12923/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 00:23:08 +0000 (UTC)",[GitHub] spark pull request: Update io.netty from 4.0.13 Final to 4.0.17.Fi...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/41#issuecomment-36308968
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Fri, 28 Feb 2014 00:53:13 +0000 (UTC)","[GitHub] spark pull request: Add Security to Spark - Akka, Http, Connection...",dev@spark.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/spark/pull/33#discussion_r10150320
  
    --- Diff: core/src/main/scala/org/apache/spark/ui/JettyUtils.scala ---
    @@ -41,56 +46,103 @@ private[spark] object JettyUtils extends Logging {
       type Responder[T] = HttpServletRequest => T
     
       // Conversions from various types of Responder's to jetty Handlers
    -  implicit def jsonResponderToHandler(responder: Responder[JValue]): Handler =
    +  implicit def jsonResponderToHandler(responder: Responder[JValue]): HttpServlet =
    --- End diff --
    
    should these functions be renamed now to `xxxToServlet`?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Fri, 28 Feb 2014 00:53:34 +0000 (UTC)","[GitHub] spark pull request: Add Security to Spark - Akka, Http, Connection...",dev@spark.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/spark/pull/33#discussion_r10150334
  
    --- Diff: core/src/main/scala/org/apache/spark/ui/JettyUtils.scala ---
    @@ -41,56 +46,103 @@ private[spark] object JettyUtils extends Logging {
       type Responder[T] = HttpServletRequest => T
     
       // Conversions from various types of Responder's to jetty Handlers
    -  implicit def jsonResponderToHandler(responder: Responder[JValue]): Handler =
    +  implicit def jsonResponderToHandler(responder: Responder[JValue]): HttpServlet =
         createHandler(responder, ""text/json"", (in: JValue) => pretty(render(in)))
     
    -  implicit def htmlResponderToHandler(responder: Responder[Seq[Node]]): Handler =
    +  implicit def htmlResponderToHandler(responder: Responder[Seq[Node]]): HttpServlet =
         createHandler(responder, ""text/html"", (in: Seq[Node]) => ""<!DOCTYPE html>"" + in.toString)
     
    -  implicit def textResponderToHandler(responder: Responder[String]): Handler =
    +  implicit def textResponderToHandler(responder: Responder[String]): HttpServlet =
         createHandler(responder, ""text/plain"")
     
    -  def createHandler[T <% AnyRef](responder: Responder[T], contentType: String,
    -                                 extractFn: T => String = (in: Any) => in.toString): Handler = {
    -    new AbstractHandler {
    -      def handle(target: String,
    -                 baseRequest: Request,
    -                 request: HttpServletRequest,
    +  def createHandler[T <% AnyRef](responder: Responder[T], contentType: String, 
    --- End diff --
    
    and this could be... `createServlet`


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
sryza <git@git.apache.org>,"Fri, 28 Feb 2014 01:00:25 +0000 (UTC)",,dev@spark.apache.org,"Github user sryza commented on the pull request:

    https://github.com/apache/spark/pull/29#issuecomment-36311392
  
    Oops, actually pushed it this time.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 01:03:57 +0000 (UTC)",,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/29#issuecomment-36311619
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 01:03:57 +0000 (UTC)",,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/29#issuecomment-36311620
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 01:04:08 +0000 (UTC)",,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/29#issuecomment-36311628
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 01:32:25 +0000 (UTC)",,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/29#issuecomment-36313361
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12924/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 01:32:25 +0000 (UTC)",,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/29#issuecomment-36313360
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
andrewor14 <git@git.apache.org>,"Fri, 28 Feb 2014 01:36:17 +0000 (UTC)",[GitHub] spark pull request: [WIP] [SPARK-1132] Persisting Web UI through r...,dev@spark.apache.org,"GitHub user andrewor14 opened a pull request:

    https://github.com/apache/spark/pull/42

    [WIP] [SPARK-1132] Persisting Web UI through refactoring the SparkListener interface

    The fleeting nature of the Spark Web UI has long been a problem reported by many users: The existing Web UI disappears as soon as the associated application terminates. This is because SparkUI is tightly coupled with SparkContext, and cannot be instantiated independently from it. To solve this, some state must be saved to persistent storage while the application is still running.
    
    The approach taken by this PR involves persisting the UI state through SparkListenerEvents. This requires a major refactor of the SparkListener interface because existing events (1) maintain deep references, making de/serialization is difficult, and (2) do not encode all the information displayed on the UI.
    
    The new architecture is as follows - The SparkUI registers a central gateway listener with the SparkContext for events. For each event this gateway listener receives, it logs the event to a file and relays it to all of its children listeners (e.g. ExecutorsListener) used by the UI. Each of these children listeners then construct the appropriate information from these events and supplies it to its parent UI (e.g. ExecutorsUI), which renders the associated page(s) on demand. Then, after the SparkContext has stopped, the SparkUI can be revived by replaying all the logged events to the children listeners through the gateway listener.
    
    This patch is WIP and still expects additional features. The main TODO's at this point include adding support for logging into HDFS, handling long running jobs (perhaps through checkpointing), and performance testing. To try this patch out, run your Spark application to completion as usual, then run ```bin/spark-class org.apache.spark.ui.UIReloader /tmp/spark-<user name>```. Your revived Spark Web UI awaits on port 14040.
    
    More details can be found in the commit messages, comments within the code, and the [design doc](https://spark-project.atlassian.net/secure/attachment/12900/PersistingSparkWebUI.pdf). Comments and feedback are most welcome.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/andrewor14/spark master

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/spark/pull/42.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #42
    
----
commit 164489d6f176bdecfa9dabec2dfce5504d1ee8af
Author: Andrew Or <andrewor14@gmail.com>
Date:   2014-02-04T02:18:04Z

    Relax assumptions on compressors and serializers when batching
    
    This commit introduces an intermediate layer of an input stream on the batch level.
    This guards against interference from higher level streams (i.e. compression and
    deserialization streams), especially pre-fetching, without specifically targeting
    particular libraries (Kryo) and forcing shuffle spill compression to use LZF.

commit a531d2e347acdcecf2d0ab72cd4f965ab5e145d8
Author: Andrew Or <andrewor14@gmail.com>
Date:   2014-02-04T02:18:04Z

    Relax assumptions on compressors and serializers when batching
    
    This commit introduces an intermediate layer of an input stream on the batch level.
    This guards against interference from higher level streams (i.e. compression and
    deserialization streams), especially pre-fetching, without specifically targeting
    particular libraries (Kryo) and forcing shuffle spill compression to use LZF.

commit 3df700509955f7074821e9aab1e74cb53c58b5a5
Author: Andrew Or <andrewor14@gmail.com>
Date:   2014-02-04T02:27:49Z

    Merge branch 'master' of github.com:andrewor14/incubator-spark

commit 287ef44e593ad72f7434b759be3170d9ee2723d2
Author: Andrew Or <andrewor14@gmail.com>
Date:   2014-02-04T21:38:32Z

    Avoid reading the entire batch into memory; also simplify streaming logic
    
    Additionally, address formatting comments.

commit bd5a1d7350467ed3dc19c2de9b2c9f531f0e6aa3
Author: Andrew Or <andrewor14@gmail.com>
Date:   2014-02-04T21:44:24Z

    Typo: phyiscal -> physical

commit 13920c918efe22e66a1760b14beceb17a61fd8cc
Author: Andrew Or <andrewor14@gmail.com>
Date:   2014-02-05T00:34:15Z

    Update docs

commit 090544a87a0767effd0c835a53952f72fc8d24f0
Author: Andrew Or <andrewor14@gmail.com>
Date:   2014-02-05T18:58:23Z

    Privatize methods

commit 3ddeb7ef89a0af2b685fb5d071aa0f71c975cc82
Author: Andrew Or <andrewor14@gmail.com>
Date:   2014-02-05T20:09:32Z

    Also privatize fields

commit e3ae35f4fb1ce8e2d7398afdbabab3dbf4bb2ffe
Author: Andrew Or <andrewor14@gmail.com>
Date:   2014-02-11T00:15:15Z

    Merge github.com:apache/incubator-spark
    
    Conflicts:

commit 8e09306f6dd4ab421447d769572de58035d3d66a
Author: Andrew Or <andrewor14@gmail.com>
Date:   2014-02-12T01:48:16Z

    Use JSON for ExecutorsUI

commit 10ed49dffe4a515bff42762cb025a3f64d9cd407
Author: Andrew Or <andrewor14@gmail.com>
Date:   2014-02-12T18:53:32Z

    Merge github.com:apache/incubator-spark into persist-ui

commit dcbd312b1e4585445868dfb562f9c64ac2fc8cda
Author: Andrew Or <andrewor14@gmail.com>
Date:   2014-02-12T23:58:39Z

    Add JSON Serializability for all SparkListenerEvent's
    
    This also involves a clean-up in the way these events are structured. The existing way
    in which these events are defined maintains a lot of extraneous information. To avoid
    serializing the whole tree of RDD dependencies, for instance, this commit cherry-picks
    only the relevant fields. However, this means sacrificing JobLogger's functionality of
    tracing the entire RDD tree.
    
    Additionally, this commit also involves minor formatting and naming clean-ups within
    the scope of the above changes.

commit bb222b9f7422cdf9e3a4c682bb271da1f75f4f75
Author: Andrew Or <andrewor14@gmail.com>
Date:   2014-02-13T04:35:09Z

    ExecutorUI: render completely from JSON
    
    Additionally, this commit fixes the bug in the local mode, where executor IDs of tasks
    do not match those of storage statuses (more detail in ExecutorsUI.scala).
    
    This commit currently does not serialize the SparkListenerEvents yet, but instead
    serializes changes to each executor JSON. This is a big TODO in the upcoming commit.

commit bf0b2e9e92d760d49ba7b26aaa41b9e3aef2420f
Author: Andrew Or <andrewor14@gmail.com>
Date:   2014-02-14T03:12:53Z

    ExecutorUI: Serialize events rather than arbitary executor information
    
    This involves adding a new SparkListenerStorageFetchEvent, and adding JSON serializability
    to all of the objects it depends on.

commit de8a1cdb833d80423aba629ba932b6f403ecd4ab
Author: Andrew Or <andrewor14@gmail.com>
Date:   2014-02-15T03:22:50Z

    Serialize events both to and from JSON (rather than just to)
    
    This requires every field of every event to be completely reconstructible from its
    JSON representation. This commit may contain incomplete state.

commit 8a2ebe6ba37b2d5efe344aa3bea343cda1411212
Author: Andrew Or <andrewor14@gmail.com>
Date:   2014-02-15T06:01:21Z

    Fix bugs for EnvironmentUI and ExecutorsUI
    
    In particular, EnvironmentUI was not rendering until a job begins, and ExecutorsUI
    reports an incorrect number (format) of total tasks.

commit c4cd48022b3a8dbf60f458196e21ba8c9cb3b88f
Author: Andrew Or <andrewor14@gmail.com>
Date:   2014-02-15T06:53:43Z

    Also deserialize new events
    
    This includes SparkListenerLoadEnvironment and SparkListenerStorageStatusFetch

commit d859efc34c9a5f07bae7eca7b4ab72fa19fb7e29
Author: Andrew Or <andrewor14@gmail.com>
Date:   2014-02-15T22:01:14Z

    BlockManagerUI: Add JSON functionality

commit 8add36bb08126fbcd02d23c446dd3ec970f1f549
Author: Andrew Or <andrewor14@gmail.com>
Date:   2014-02-15T22:40:49Z

    JobProgressUI: Add JSON functionality
    
    In addition, refactor FileLogger to log in one directory per logger

commit b3976b0a2eb21b4a887d01fd16869a0f37c36f8b
Author: Andrew Or <andrewor14@gmail.com>
Date:   2014-02-16T06:52:43Z

    Add functionality of reconstructing a persisted UI from SparkContext
    
    With this commit, any reconstruct SparkUI resides on default port of 14040 onwards.
    Logged events are posted separately from live events, such that the live SparkListeners
    are not affected.
    
    This commit also fixes a few JSON de/serialization bugs.

commit 4dfcd224504f392302a49ac82280b294c381f381
Author: Andrew Or <andrewor14@gmail.com>
Date:   2014-02-17T19:14:20Z

    Merge git://git.apache.org/incubator-spark into persist-ui

commit f3fc13b53725cdfeddcecb2068ab5a533566772f
Author: Andrew Or <andrewor14@gmail.com>
Date:   2014-02-17T21:22:01Z

    General refactor
    
    This includes reverting previous formatting and naming changes that are irrelevant to
    this patch.

commit 3fd584e30aaf6552179bf9e9b350b130fa92d0ad
Author: Andrew Or <andrewor14@gmail.com>
Date:   2014-02-18T02:01:12Z

    Fix two major bugs
    
    First, JobProgessListener uses HashSets of TaskInfo and StageInfo, and relies on the equality
    of these objects to remove from the corresponding HashSets correctly. This is not a luxury that
    deserialized StageInfo's and TaskInfo's have. Instead, when removing from these collections, we
    must match by the ID rather than the object itself.
    
    Second, although SparkUI differentiates between persisted and live UI's, its children UI's and
    their corresponding listeners do not. Thus, each revived UI essentially duplicated all the logs
    that reconstructed it in the first place. Further, these zombie UI's continued to respond to
    live SparkListenerEvents. This has been fixed by requiring that revived UI's do not register
    their listeners with the current SparkContext.
    
    With the former fix, there were major incompatibility issues with the existing way UI classes
    access and mutate the collections. Formatting improvements associated with smoothing out these
    inconsistencies are included as part of this commit.

commit 5ac906d4dfd546c5d6b6e80540c8774f3985fecc
Author: Andrew Or <andrewor14@gmail.com>
Date:   2014-02-18T05:38:16Z

    Mostly naming, formatting, and code style changes

commit 904c7294ac221a0cd9806af843219aaa8a847085
Author: Andrew Or <andrewor14@gmail.com>
Date:   2014-02-18T06:06:46Z

    Fix another major bug
    
    Previously, rendering the old, persisted UI continues to trigger load environment and
    storage status fetch events. These are now only triggered for the live UI.
    
    A related TODO: Under JobProgressUI, the total duration is inaccurate; right now it uses
    the time when the old UI is revived, rather than when it was live. This should be fixed.

commit 427301371117e9e7889f5df0f6bba51e5916e425
Author: Andrew Or <andrewor14@gmail.com>
Date:   2014-02-18T23:27:39Z

    Add a gateway SparkListener to simplify event logging
    
    Instead of having each SparkListener log an independent set of events, centralize event
    logging to avoid differentiating events across UI's and thus duplicating logged events.
    Also rename the ""fromDisk"" parameter to ""live"".
    
    TODO: Storage page currently still relies on the previous SparkContext and is not
    rendering correctly.

commit 64d2ce1efee3aa5a8166c5fe108932b2279217fc
Author: Andrew Or <andrewor14@gmail.com>
Date:   2014-02-19T02:29:21Z

    Fix BlockManagerUI bug by introducing new event
    
    Previously, the storage information of persisted RDD's continued to rely on the old SparkContext,
    which is no longer accessible if the UI is rendered from disk. This fix solves it by introducing
    an event, SparkListenerGetRDDInfo, which captures this information.
    
    Per discussion with Patrick, an alternative is to encapsulate this information within
    SparkListenerTaskEnd. This would bypass the need to create a new event, but would also require
    a non-trivial refactor of BlockManager / BlockStore.

commit 6814da0cf9af2a29810b6773463acee3b259c95f
Author: Andrew Or <andrewor14@gmail.com>
Date:   2014-02-19T18:36:01Z

    Explicitly register each UI listener rather than through some magic
    
    This (1) allows UISparkListener to be a simple trait and (2) is more intuitive, since it
    mirrors sc.addSparkListener(listener), for all other non-UI listeners.

commit d646df6786737d67d5ca1dbf593740a02a600991
Author: Andrew Or <andrewor14@gmail.com>
Date:   2014-02-20T02:47:35Z

    Completely decouple SparkUI from SparkContext
    
    This involves storing additional fields, such as the scheduling mode and the app name, into the
    new event, SparkListenerApplicationStart, since these attributes are no longer accessible without
    a SparkContext. Further, environment information is refactored to be loaded on application start
    (rather than on job start).
    
    Persisted Spark UI's can no longer be created from SparkContext. The new way of constructing them
    is through a standalone scala program. org.apache.spark.ui.UIReloader is introduced as an example
    of how to do this.

commit e9e1c6dede36788d3cefe3c65366f5a79be97a1d
Author: Andrew Or <andrewor14@gmail.com>
Date:   2014-02-21T07:51:08Z

    Move all JSON de/serialization logic to JsonProtocol
    
    This makes all classes involved appear less cluttered.

commit 70e7e7acf09d8efd2c7e459ee450c1db140b8f5a
Author: Andrew Or <andrewor14@gmail.com>
Date:   2014-02-22T02:56:26Z

    Formatting changes

commit 6631c02a8791d0321f003bb339344445f4dd0cab
Author: Andrew Or <andrewor14@gmail.com>
Date:   2014-02-24T18:52:21Z

    More formatting changes, this time mainly for Json DSL

commit bbe3501c63029ffa9c1fd9053e7ab868d0f28b10
Author: Andrew Or <andrewor14@gmail.com>
Date:   2014-02-26T23:27:43Z

    Embed storage status and RDD info in Task events
    
    This commit achieves three main things. First and foremost, it embeds the information
    from the SparkListenerFetchStorageStatus and SparkListenerGetRDDInfo events into events
    that are more descriptive of the SparkListenerInterface. In particular, every Task now
    maintains a list of blocks whose storage status have been updated as a result of the task.
    Previously, this information is retrieved from fetching storage status from the driver,
    an action arbitrarily associated with a stage. This change involves keeping track of
    what blocks are dropped during each call to an RDD persist. A big TODO is to also capture
    the behavior of an RDD unpersist in a SparkListenerEvent.
    
    Second, the SparkListenerEvent interface now handles the dynamic nature of Executors.
    In particular, a new event, SparkListenerExecutorStateChange, is introduced, which triggers
    a storage status fetch from the driver. The purpose of this is mainly to decouple fetching
    storage status from the driver from the Stage. Note that storage status is not ready until
    the remote BlockManagers have been registered, so this involves attaching a registration
    listener to the BlockManagerMasterActor.
    
    Third, changes in environment properties is now supported. This accounts for the fact that
    the user can invoke sc.addFile and sc.addJar in his/her own application, which should be
    reflected appropriately on the EnvironmentUI. In the previous implementation, coupling this
    information with application start prevents this from happening.
    
    Other relatively minor changes include: 1) Refactoring BlockStatus and BlockManagerInfo to
    not be a part of the BlockManagerMasterActor object, 2) Formatting changes, especially those
    involving multi-line arguments, and 3) Making all UI widgets and listeners private[ui] instead
    of private[spark].

commit 28019caa5712b8d7f1db039dc41876d91e530998
Author: Andrew Or <andrewor14@gmail.com>
Date:   2014-02-27T00:47:00Z

    Merge github.com:apache/spark
    
    Conflicts:
    	core/src/main/scala/org/apache/spark/CacheManager.scala
    	core/src/main/scala/org/apache/spark/SparkEnv.scala
    	core/src/main/scala/org/apache/spark/scheduler/JobLogger.scala
    	core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala
    	core/src/main/scala/org/apache/spark/storage/BlockManager.scala
    	core/src/main/scala/org/apache/spark/storage/MemoryStore.scala
    	core/src/main/scala/org/apache/spark/storage/StorageUtils.scala
    	core/src/main/scala/org/apache/spark/ui/SparkUI.scala
    	core/src/main/scala/org/apache/spark/ui/env/EnvironmentUI.scala
    	core/src/main/scala/org/apache/spark/ui/exec/ExecutorsUI.scala
    	core/src/main/scala/org/apache/spark/ui/jobs/IndexPage.scala
    	core/src/main/scala/org/apache/spark/ui/jobs/JobProgressListener.scala
    	core/src/main/scala/org/apache/spark/ui/jobs/JobProgressUI.scala
    	core/src/main/scala/org/apache/spark/ui/jobs/PoolPage.scala
    	core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala
    	core/src/main/scala/org/apache/spark/ui/jobs/StageTable.scala
    	core/src/main/scala/org/apache/spark/ui/storage/IndexPage.scala
    	core/src/main/scala/org/apache/spark/ui/storage/RDDPage.scala
    	core/src/main/scala/org/apache/spark/util/TimeStampedHashMap.scala
    	core/src/main/scala/org/apache/spark/util/Utils.scala
    	core/src/test/scala/org/apache/spark/ui/jobs/JobProgressListenerSuite.scala

commit d1f428591d6c33c2bb86f85468c7842b5ca00311
Author: Andrew Or <andrewor14@gmail.com>
Date:   2014-02-27T01:19:20Z

    Migrate from lift-json to json4s-jackson

commit 7b2f8112795a53c35b10bc3d72e5be7b699ceb65
Author: Andrew Or <andrewor14@gmail.com>
Date:   2014-02-27T19:24:32Z

    Guard against TaskMetrics NPE + Fix tests

commit 996d7a2f42d4e02c1e40ec22b0c4d7db86aa03e3
Author: Andrew Or <andrewor14@gmail.com>
Date:   2014-02-27T20:26:23Z

    Reflect RDD unpersist on UI
    
    This introduces a new event, SparkListenerUnpersistRDD.

commit 472fd8a4845e39a38f8d993a3527a7e77571ffad
Author: Andrew Or <andrewor14@gmail.com>
Date:   2014-02-27T23:03:59Z

    Fix a couple of tests

commit d47585f22f243fc7e840af90132edb7e84b003ed
Author: Andrew Or <andrewor14@gmail.com>
Date:   2014-02-28T00:15:21Z

    Clean up FileLogger

commit faa113e674a276ddf5cd7dc643c16b7bed2b5e44
Author: Andrew Or <andrewor14@gmail.com>
Date:   2014-02-28T01:12:19Z

    General clean up

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 01:38:53 +0000 (UTC)",[GitHub] spark pull request: [WIP] [SPARK-1132] Persisting Web UI through r...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/42#issuecomment-36313763
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 01:38:54 +0000 (UTC)",[GitHub] spark pull request: [WIP] [SPARK-1132] Persisting Web UI through r...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/42#issuecomment-36313766
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 01:39:05 +0000 (UTC)",[GitHub] spark pull request: [WIP] [SPARK-1132] Persisting Web UI through r...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/42#issuecomment-36313780
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Fri, 28 Feb 2014 01:40:10 +0000 (UTC)","[GitHub] spark pull request: Add Security to Spark - Akka, Http, Connection...",dev@spark.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/spark/pull/33#discussion_r10151554
  
    --- Diff: core/src/main/scala/org/apache/spark/ui/JettyUtils.scala ---
    @@ -41,56 +46,103 @@ private[spark] object JettyUtils extends Logging {
       type Responder[T] = HttpServletRequest => T
     
       // Conversions from various types of Responder's to jetty Handlers
    -  implicit def jsonResponderToHandler(responder: Responder[JValue]): Handler =
    +  implicit def jsonResponderToHandler(responder: Responder[JValue]): HttpServlet =
         createHandler(responder, ""text/json"", (in: JValue) => pretty(render(in)))
     
    -  implicit def htmlResponderToHandler(responder: Responder[Seq[Node]]): Handler =
    +  implicit def htmlResponderToHandler(responder: Responder[Seq[Node]]): HttpServlet =
         createHandler(responder, ""text/html"", (in: Seq[Node]) => ""<!DOCTYPE html>"" + in.toString)
     
    -  implicit def textResponderToHandler(responder: Responder[String]): Handler =
    +  implicit def textResponderToHandler(responder: Responder[String]): HttpServlet =
         createHandler(responder, ""text/plain"")
     
    -  def createHandler[T <% AnyRef](responder: Responder[T], contentType: String,
    -                                 extractFn: T => String = (in: Any) => in.toString): Handler = {
    -    new AbstractHandler {
    -      def handle(target: String,
    -                 baseRequest: Request,
    -                 request: HttpServletRequest,
    +  def createHandler[T <% AnyRef](responder: Responder[T], contentType: String, 
    +                                 extractFn: T => String = (in: Any) => in.toString): HttpServlet = {
    +    new HttpServlet {
    +      override def doGet(request: HttpServletRequest,
                      response: HttpServletResponse) {
    -        response.setContentType(""%s;charset=utf-8"".format(contentType))
    -        response.setStatus(HttpServletResponse.SC_OK)
    -        baseRequest.setHandled(true)
    -        val result = responder(request)
    -        response.setHeader(""Cache-Control"", ""no-cache, no-store, must-revalidate"")
    -        response.getWriter().println(extractFn(result))
    +        // First try to get the security Manager from the SparkEnv. If that doesn't exist, create
    +        // a new one and rely on the configs being set
    +        val sparkEnv = SparkEnv.get
    +        val securityMgr = if (sparkEnv != null) sparkEnv.securityManager else new SecurityManager()
    --- End diff --
    
    Ideally the way this works is that we would just pass a security manager to this function rather than initializing some configuration inside of a static utility like this.
    
    The model going forward with SparkConf is that all components should configure themselves via a SparkConf and we shouldn't directly use system properties or thread locals to do various configurations.
    
    So the ideal way this would work is that we create a single SecurityManager inside the Master and Worker classes (based on the SparkConf they each create) and then that gets passed into all calls of these static utilities. For the UI's inside of an active SparkContext we'd just pass `sc.env.securityManager`. 
    
    The downside is it will make using these functions a little clunkier because really you need a reference to the security manager for every handler you create. That's a bit clunkier but ultimately better (I think). Doing this initialization inside of each servlet isn't so good in the SparkConf world.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 01:40:20 +0000 (UTC)",[GitHub] spark pull request: [WIP] [SPARK-1132] Persisting Web UI through r...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/42#issuecomment-36313839
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 01:40:20 +0000 (UTC)",[GitHub] spark pull request: [WIP] [SPARK-1132] Persisting Web UI through r...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/42#issuecomment-36313840
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12925/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Fri, 28 Feb 2014 01:52:24 +0000 (UTC)","[GitHub] spark pull request: Add Security to Spark - Akka, Http, Connection...",dev@spark.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/spark/pull/33#discussion_r10151846
  
    --- Diff: core/src/main/scala/org/apache/spark/network/SecurityMessage.scala ---
    @@ -0,0 +1,110 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one or more
    + * contributor license agreements.  See the NOTICE file distributed with
    + * this work for additional information regarding copyright ownership.
    + * The ASF licenses this file to You under the Apache License, Version 2.0
    + * (the ""License""); you may not use this file except in compliance with
    + * the License.  You may obtain a copy of the License at
    + *
    + *    http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.spark.network
    +
    +import java.nio.ByteBuffer
    +
    +import scala.collection.mutable.ArrayBuffer
    +import scala.collection.mutable.StringBuilder
    +
    +import org.apache.spark._
    +import org.apache.spark.network._
    +
    +private[spark] class SecurityMessage() extends Logging {
    --- End diff --
    
    This class might benefit from a little bit of documentation... the interactions here are fairly complicated and the responsibility breakdown between this and BufferMessage are a little confusing (at least for me).


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 01:53:53 +0000 (UTC)",[GitHub] spark pull request: [WIP] [SPARK-1132] Persisting Web UI through r...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/42#issuecomment-36314537
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 01:53:53 +0000 (UTC)",[GitHub] spark pull request: [WIP] [SPARK-1132] Persisting Web UI through r...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/42#issuecomment-36314536
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 01:54:08 +0000 (UTC)",[GitHub] spark pull request: [WIP] [SPARK-1132] Persisting Web UI through r...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/42#issuecomment-36314544
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Fri, 28 Feb 2014 02:05:31 +0000 (UTC)","[GitHub] spark pull request: Add Security to Spark - Akka, Http, Connection...",dev@spark.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/spark/pull/33#discussion_r10152124
  
    --- Diff: docs/security.md ---
    @@ -0,0 +1,14 @@
    +---
    +layout: global
    +title: Spark Security
    +---
    +
    +Spark currently supports authentication via a shared secret. Authentication can be configured to be on via the `spark.authenticate` configuration parameter. This parameter controls whether the Spark communication protocols do authentication using the shared secret. This authentication is a basic handshake to make sure both sides have the same shared secret and are allowed to communicate. If the shared secret is not identical they will not be allowed to communicate.
    +
    +The Spark UI can also be secured by using javax servlet filters. A user may want to secure the UI if it has data that other users should not be allowed to see. The javax servlet filter specified by the user can authenticate the user and then once the user is logged in, Spark can compare that user versus the view acls to make sure they are authorized to view the UI. The configs 'spark.ui.acls.enable' and 'spark.ui.view.acls' control the behavior of the acls. Note that the person who started the application always has view access to the UI.
    +
    +For Spark on Yarn deployments, configuring `spark.authenticate` to true will automatically handle generating and distributing the shared secret. Each application will use a unique shared secret. The Spark UI uses the standard YARN web application proxy mechanism and will authenticate via any installed Hadoop filters. If an authentication filter is enabled, the acls controls can be used by control which users can via the Spark UI. 
    +
    +For other types of Spark deployments, the environment variable `SPARK_SECRET` should be configured on each of the nodes. This secret will be used by all the Master/Workers and applications. The UI can be secured using a javax servlet filter installed via `spark.ui.filters`. If an authentication filter is enabled, the acls controls can be used by control which users can via the Spark UI.
    +
    +See [Spark Configuration](configuration.html) for more details on the security configs.
    --- End diff --
    
    Here it might also be good to link on github to the javadoc of the SecurityManager class (now that we have a stable home on github). Since it's so thorough :)


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Fri, 28 Feb 2014 02:05:45 +0000 (UTC)","[GitHub] spark pull request: Add Security to Spark - Akka, Http, Connection...",dev@spark.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/spark/pull/33#issuecomment-36315155
  
    Hey Tom,
    
    This patch is looking really great. I took a pretty thorough look through this and there were really only two high level things:
    
    1. Configuration. The model we've move towards is that a SparkContext is configured by a SparkConf and that Conf is used to configure all components. For the master/worker they each instantiate a conf as well. It would be good if instead of system properties and environment variables, a security manager takes a SparkConf on initialization. And other parts of the code use SparkConf as well. If it seems like this isn't possible anywhere, let me know, that might represent a shortfall in the current design of SparkConf.
    
    2. Default behavior and logging. Thinking a bit about environments less homogeneous than Yahoo, we should try to minimize the potential for confusion and mistakes when configuring security. So I was thinking maybe if users, e.g., don't set spark secret we should throw an exception, and suggested logging more detail about the security set-up at info level.
    
    Some other smaller comments as well.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
"""=?gb18030?B?bGlncQ==?="" <witgo@qq.com>","Fri, 28 Feb 2014 10:07:03 +0800",Re: Discussion on SPARK-1139,"""=?gb18030?B?ZGV2?="" <dev@spark.apache.org>","You can make the patch so that everyone review


On Wednesday, February 26, 2014 at 8:23 AM, Nan Zhu wrote:

> Hi, all  
>  
> I just created a JIRA https://spark-project.atlassian.net/browse/SPARK-1139 . The issue discusses that:
>  
> the new Hadoop API based Spark APIs are actually a mixture of old and new Hadoop API.
>  
> Spark APIs are still using JobConf (or Configuration) as one of the parameters, but actually Configuration has been replaced by mapreduce.Job in the new Hadoop API
>  
> for example : http://codesfusion.blogspot.ca/2013/10/hadoop-wordcount-with-new-map-reduce-api.html  
>  
> &  
>  
> http://www.slideshare.net/sh1mmer/upgrading-to-the-new-map-reduce-api (p10)
>  
> Personally I think it¡¯s better to fix this design, but it will introduce some compatibility issue  
>  
> Just bring it here for your advices
>  
> Best,  
>  
> --  
> Nan Zhu
>"
pwendell <git@git.apache.org>,"Fri, 28 Feb 2014 02:11:20 +0000 (UTC)",[GitHub] spark pull request: Remote BlockFetchTracker trait,dev@spark.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/spark/pull/39#discussion_r10152243
  
    --- Diff: core/src/main/scala/org/apache/spark/storage/BlockFetcherIterator.scala ---
    @@ -44,9 +44,14 @@ import org.apache.spark.util.Utils
      */
     
     private[storage]
    -trait BlockFetcherIterator extends Iterator[(BlockId, Option[Iterator[Any]])]
    -  with Logging with BlockFetchTracker {
    +trait BlockFetcherIterator extends Iterator[(BlockId, Option[Iterator[Any]])] with Logging {
       def initialize()
    +  def totalBlocks : Int
    --- End diff --
    
    mind fixing the spacing while you are cleaning this up: `def totalBlocks: Int`?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Fri, 28 Feb 2014 02:21:57 +0000 (UTC)",[GitHub] spark pull request: Remote BlockFetchTracker trait,dev@spark.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/spark/pull/39#discussion_r10152493
  
    --- Diff: core/src/main/scala/org/apache/spark/storage/BlockFetcherIterator.scala ---
    @@ -233,7 +238,16 @@ object BlockFetcherIterator {
           logDebug(""Got local blocks in "" + Utils.getUsedTimeMs(startTime) + "" ms"")
         }
     
    -    //an iterator that will read fetched blocks off the queue as they arrive.
    +    override def totalBlocks: Int = numLocal + numRemote
    --- End diff --
    
    Any particular reason to move this? Code clarity?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 02:22:36 +0000 (UTC)",[GitHub] spark pull request: [WIP] [SPARK-1132] Persisting Web UI through r...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/42#issuecomment-36315930
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12926/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 02:22:36 +0000 (UTC)",[GitHub] spark pull request: [WIP] [SPARK-1132] Persisting Web UI through r...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/42#issuecomment-36315929
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
CodingCat <git@git.apache.org>,"Fri, 28 Feb 2014 04:53:43 +0000 (UTC)",[GitHub] spark pull request: [SPARK-1102] Create a saveAsNewAPIHadoopDatase...,dev@spark.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/spark/pull/12#issuecomment-36321849
  
    this is a re-opened PR, in the old PR, https://github.com/apache/incubator-spark/pull/636, all test cases have passed
    
    Can anyone verify that and make further review?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
JoshRosen <git@git.apache.org>,"Fri, 28 Feb 2014 05:15:22 +0000 (UTC)",[GitHub] spark pull request: SPARK-1134 pyspark only uses ipython if starte...,dev@spark.apache.org,"Github user JoshRosen commented on the pull request:

    https://github.com/apache/spark/pull/38#issuecomment-36322600
  
    Jenkins, this is okay to test.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
asfgit <git@git.apache.org>,"Fri, 28 Feb 2014 05:17:13 +0000 (UTC)",[GitHub] spark pull request: Removed reference to incubation in Spark user ...,dev@spark.apache.org,"Github user asfgit closed the pull request at:

    https://github.com/apache/spark/pull/2


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
Bryn Keller <xoltar@xoltar.org>,"Thu, 27 Feb 2014 21:34:35 -0800",How best to ensure partitioners behave?,dev@spark.apache.org,"Hi Folks,

I just filed https://spark-project.atlassian.net/browse/SPARK-1149 - I'm
happy to fix it, but I'd like input about how best to go about it. The
problem is, if a partitioner misbehaves by, say, returning a negative
partition number, Spark hangs. This is easier to do than it sounds.

I'd like to fix that so that instead we'd get an exception that let the
developer know their partitioner had done something wrong.

Unfortunately I don't see an easy way to do that without changing method
signatures. Would a reasonable compromise be to write a utility method that
checks the partition number against Partitioner.numPartitions, and update
the code to use that method everywhere that it directly calls
Partitioner.getPartition now?

Anyone have a better suggestion?

Thanks,
Bryn
"
kayousterhout <git@git.apache.org>,"Fri, 28 Feb 2014 05:48:51 +0000 (UTC)",[GitHub] spark pull request: Remote BlockFetchTracker trait,dev@spark.apache.org,"Github user kayousterhout commented on a diff in the pull request:

    https://github.com/apache/spark/pull/39#discussion_r10155351
  
    --- Diff: core/src/main/scala/org/apache/spark/storage/BlockFetcherIterator.scala ---
    @@ -233,7 +238,16 @@ object BlockFetcherIterator {
           logDebug(""Got local blocks in "" + Utils.getUsedTimeMs(startTime) + "" ms"")
         }
     
    -    //an iterator that will read fetched blocks off the queue as they arrive.
    +    override def totalBlocks: Int = numLocal + numRemote
    --- End diff --
    
    Yeah trying to group the functions that implement the same interface.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
kayousterhout <git@git.apache.org>,"Fri, 28 Feb 2014 05:49:56 +0000 (UTC)",[GitHub] spark pull request: Remote BlockFetchTracker trait,dev@spark.apache.org,"Github user kayousterhout commented on the pull request:

    https://github.com/apache/spark/pull/39#issuecomment-36323841
  
    Thanks @pwendell  -- fixed the style!


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Fri, 28 Feb 2014 05:53:13 +0000 (UTC)",[GitHub] spark pull request: Remote BlockFetchTracker trait,dev@spark.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/spark/pull/39#issuecomment-36323971
  
    thanks I've merged this.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 05:53:54 +0000 (UTC)",[GitHub] spark pull request: Remote BlockFetchTracker trait,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/39#issuecomment-36324005
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 05:53:54 +0000 (UTC)",[GitHub] spark pull request: Remote BlockFetchTracker trait,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/39#issuecomment-36324006
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 05:54:05 +0000 (UTC)",[GitHub] spark pull request: Remote BlockFetchTracker trait,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/39#issuecomment-36324015
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
asfgit <git@git.apache.org>,"Fri, 28 Feb 2014 06:21:41 +0000 (UTC)",[GitHub] spark pull request: Remote BlockFetchTracker trait,dev@spark.apache.org,"Github user asfgit closed the pull request at:

    https://github.com/apache/spark/pull/39


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 06:22:55 +0000 (UTC)",[GitHub] spark pull request: Remote BlockFetchTracker trait,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/39#issuecomment-36325078
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 06:22:55 +0000 (UTC)",[GitHub] spark pull request: Remote BlockFetchTracker trait,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/39#issuecomment-36325079
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12927/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Fri, 28 Feb 2014 06:40:37 +0000 (UTC)",[GitHub] spark pull request: SPARK-1134 pyspark only uses ipython if starte...,dev@spark.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/spark/pull/38#issuecomment-36325759
  
    Jenkins, test this please.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 06:43:53 +0000 (UTC)",[GitHub] spark pull request: SPARK-1134 pyspark only uses ipython if starte...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/38#issuecomment-36325871
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 06:43:53 +0000 (UTC)",[GitHub] spark pull request: SPARK-1134 pyspark only uses ipython if starte...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/38#issuecomment-36325872
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 06:44:04 +0000 (UTC)",[GitHub] spark pull request: SPARK-1134 pyspark only uses ipython if starte...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/38#issuecomment-36325879
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Fri, 28 Feb 2014 06:45:32 +0000 (UTC)",[GitHub] spark pull request: SPARK-1145: Memory mapping with many small blo...,dev@spark.apache.org,"GitHub user pwendell opened a pull request:

    https://github.com/apache/spark/pull/43

    SPARK-1145: Memory mapping with many small blocks can cause JVM allocation failures

    This includes some minor code clean-up as well. The main change is that small files are not memory mapped. There is a nicer way to write that code block using Scala's `Try` but to make it easy to back port and as simple as possible, I opted for the more explicit but less pretty format.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/pwendell/spark block-iter-logging

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/spark/pull/43.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #43
    
----
commit d238b885d6974283f3b96c26e26693074ff557e6
Author: Patrick Wendell <pwendell@gmail.com>
Date:   2014-02-26T23:44:18Z

    Some logging and clean-up

commit 4e1514e666624d2285b190ff1419a9fae2ae929f
Author: Patrick Wendell <pwendell@gmail.com>
Date:   2014-02-27T02:34:08Z

    Don't memory map for small files

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 06:48:53 +0000 (UTC)",[GitHub] spark pull request: SPARK-1145: Memory mapping with many small blo...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/43#issuecomment-36326074
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 06:48:53 +0000 (UTC)",[GitHub] spark pull request: SPARK-1145: Memory mapping with many small blo...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/43#issuecomment-36326073
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 06:49:05 +0000 (UTC)",[GitHub] spark pull request: SPARK-1145: Memory mapping with many small blo...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/43#issuecomment-36326084
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
mridulm <git@git.apache.org>,"Fri, 28 Feb 2014 06:59:44 +0000 (UTC)",[GitHub] spark pull request: SPARK-1145: Memory mapping with many small blo...,dev@spark.apache.org,"Github user mridulm commented on a diff in the pull request:

    https://github.com/apache/spark/pull/43#discussion_r10156300
  
    --- Diff: core/src/main/scala/org/apache/spark/storage/BlockFetcherIterator.scala ---
    @@ -146,6 +146,12 @@ object BlockFetcherIterator {
         }
     
         protected def splitLocalRemoteBlocks(): ArrayBuffer[FetchRequest] = {
    +      // Make remote requests at most maxBytesInFlight / 5 in length; the reason to keep them
    +      // smaller than maxBytesInFlight is to allow multiple, parallel fetches from up to 5
    +      // nodes, rather than blocking on reading output from one node.
    +      val maxRequestSize = math.max(maxBytesInFlight / 5, 1L)
    +      logInfo(""maxBytesInFlight: "" + maxBytesInFlight + "", maxRequestSize: "" + maxRequestSize)
    +
    --- End diff --
    
    This assumes you have atleast 5 nodes - which need not be the general case.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
mridulm <git@git.apache.org>,"Fri, 28 Feb 2014 07:00:27 +0000 (UTC)",[GitHub] spark pull request: SPARK-1145: Memory mapping with many small blo...,dev@spark.apache.org,"Github user mridulm commented on a diff in the pull request:

    https://github.com/apache/spark/pull/43#discussion_r10156309
  
    --- Diff: core/src/main/scala/org/apache/spark/storage/BlockFetcherIterator.scala ---
    @@ -146,6 +146,12 @@ object BlockFetcherIterator {
         }
     
         protected def splitLocalRemoteBlocks(): ArrayBuffer[FetchRequest] = {
    +      // Make remote requests at most maxBytesInFlight / 5 in length; the reason to keep them
    +      // smaller than maxBytesInFlight is to allow multiple, parallel fetches from up to 5
    +      // nodes, rather than blocking on reading output from one node.
    +      val maxRequestSize = math.max(maxBytesInFlight / 5, 1L)
    +      logInfo(""maxBytesInFlight: "" + maxBytesInFlight + "", maxRequestSize: "" + maxRequestSize)
    +
    --- End diff --
    
    Was always bothered by this ... not specifically related to this PR btw !


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
mridulm <git@git.apache.org>,"Fri, 28 Feb 2014 07:03:33 +0000 (UTC)",[GitHub] spark pull request: SPARK-1145: Memory mapping with many small blo...,dev@spark.apache.org,"Github user mridulm commented on a diff in the pull request:

    https://github.com/apache/spark/pull/43#discussion_r10156343
  
    --- Diff: core/src/main/scala/org/apache/spark/storage/BlockFetcherIterator.scala ---
    @@ -146,6 +146,12 @@ object BlockFetcherIterator {
         }
     
         protected def splitLocalRemoteBlocks(): ArrayBuffer[FetchRequest] = {
    +      // Make remote requests at most maxBytesInFlight / 5 in length; the reason to keep them
    +      // smaller than maxBytesInFlight is to allow multiple, parallel fetches from up to 5
    +      // nodes, rather than blocking on reading output from one node.
    +      val maxRequestSize = math.max(maxBytesInFlight / 5, 1L)
    +      logInfo(""maxBytesInFlight: "" + maxBytesInFlight + "", maxRequestSize: "" + maxRequestSize)
    +
    --- End diff --
    
    Btw, any reason to move this up ?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
mridulm <git@git.apache.org>,"Fri, 28 Feb 2014 07:05:23 +0000 (UTC)",[GitHub] spark pull request: SPARK-1145: Memory mapping with many small blo...,dev@spark.apache.org,"Github user mridulm commented on a diff in the pull request:

    https://github.com/apache/spark/pull/43#discussion_r10156359
  
    --- Diff: core/src/main/scala/org/apache/spark/storage/DiskStore.scala ---
    @@ -84,12 +84,27 @@ private class DiskStore(blockManager: BlockManager, diskManager: DiskBlockManage
       override def getBytes(blockId: BlockId): Option[ByteBuffer] = {
         val segment = diskManager.getBlockLocation(blockId)
         val channel = new RandomAccessFile(segment.file, ""r"").getChannel()
    -    val buffer = try {
    -      channel.map(MapMode.READ_ONLY, segment.offset, segment.length)
    -    } finally {
    -      channel.close()
    +
    +    val buffer =
    +      // For small files, directly read rather than memory map
    +      if (segment.length < 2 * 4096) {
    --- End diff --
    
    How was this 8k limit arrived at ?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 07:12:34 +0000 (UTC)",[GitHub] spark pull request: SPARK-1134 pyspark only uses ipython if starte...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/38#issuecomment-36326993
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 07:12:34 +0000 (UTC)",[GitHub] spark pull request: SPARK-1134 pyspark only uses ipython if starte...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/38#issuecomment-36326995
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12928/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 07:16:12 +0000 (UTC)",[GitHub] spark pull request: SPARK-1145: Memory mapping with many small blo...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/43#issuecomment-36327168
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 07:16:13 +0000 (UTC)",[GitHub] spark pull request: SPARK-1145: Memory mapping with many small blo...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/43#issuecomment-36327170
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12929/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
witgo <git@git.apache.org>,"Fri, 28 Feb 2014 08:58:44 +0000 (UTC)",[GitHub] spark pull request: fix #SPARK-1149 Bad partitioners can cause Spa...,dev@spark.apache.org,"GitHub user witgo opened a pull request:

    https://github.com/apache/spark/pull/44

    fix #SPARK-1149 Bad partitioners can cause Spark to hang

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/witgo/spark SPARK-1149

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/spark/pull/44.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #44
    
----
commit 6bb725e397471ca719db028318776775cb6a17bf
Author: liguoqiang <liguoqiang@rd.tuan800.com>
Date:   2014-02-28T08:57:27Z

    fix #SPARK-1149 Bad partitioners can cause Spark to hang

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 08:58:53 +0000 (UTC)",[GitHub] spark pull request: fix #SPARK-1149 Bad partitioners can cause Spa...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/44#issuecomment-36332248
  
    Can one of the admins verify this patch?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 08:59:06 +0000 (UTC)",[GitHub] spark pull request: fix #SPARK-1149 Bad partitioners can cause Spa...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/44#issuecomment-36332257
  
    Can one of the admins verify this patch?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
coderh <git@git.apache.org>,"Fri, 28 Feb 2014 09:43:09 +0000 (UTC)","[GitHub] spark pull request: remove ""sbt/"" prefix in $JAR variable make the...",dev@spark.apache.org,"GitHub user coderh opened a pull request:

    https://github.com/apache/spark/pull/45

    remove ""sbt/"" prefix in $JAR variable make the script pass if test

    There is no sbt/ dir under sbt/sbt. ""if"" test on line 29 can not pass.
    
    remove ""sbt/"" prefix in $JAR variable make the script pass if test

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/ClaraVista-IT/spark master

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/spark/pull/45.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #45
    
----
commit e68150b9e743aa7a7ea6635149c1495e055c61fb
Author: Hao Ren <ren.hao.invictus@gmail.com>
Date:   2014-02-27T17:47:38Z

    remove ""sbt/"" prefix in $JAR variable make the script pass if test

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 09:43:52 +0000 (UTC)","[GitHub] spark pull request: remove ""sbt/"" prefix in $JAR variable make the...",dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/45#issuecomment-36335137
  
    Can one of the admins verify this patch?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 09:44:05 +0000 (UTC)","[GitHub] spark pull request: remove ""sbt/"" prefix in $JAR variable make the...",dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/45#issuecomment-36335150
  
    Can one of the admins verify this patch?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
coderh <git@git.apache.org>,"Fri, 28 Feb 2014 09:54:01 +0000 (UTC)","[GitHub] spark pull request: remove ""sbt/"" prefix in $JAR variable make the...",dev@spark.apache.org,"Github user coderh closed the pull request at:

    https://github.com/apache/spark/pull/45


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
ScrapCodes <git@git.apache.org>,"Fri, 28 Feb 2014 09:54:23 +0000 (UTC)",[GitHub] spark pull request: [HOTFIX] Patching maven build after #6 (SPARK-...,dev@spark.apache.org,"Github user ScrapCodes commented on the pull request:

    https://github.com/apache/spark/pull/37#issuecomment-36335799
  
    Hey Patrick, 
    
    Forgive me for this, this is the second time I have messed up maven build. 


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
coderh <git@git.apache.org>,"Fri, 28 Feb 2014 10:29:40 +0000 (UTC)","[GitHub] spark pull request: remove ""sbt/"" prefix in $JAR variable make the...",dev@spark.apache.org,"GitHub user coderh opened a pull request:

    https://github.com/apache/spark/pull/46

    remove ""sbt/"" prefix in $JAR variable make the script pass if test

    There is no sbt/ dir under sbt/sbt. ""if"" test on line 29 can not pass.
    
    remove ""sbt/"" prefix in $JAR variable make the script pass if test

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/ClaraVista-IT/spark master

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/spark/pull/46.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #46
    
----
commit 880bc1e9c15ac51b4e3291b815034b8ed4aae36c
Author: Hao Ren <ren.hao.invictus@gmail.com>
Date:   2014-02-28T10:27:31Z

    remove ""sbt/"" prefix in $JAR variable make the script pass if test

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 10:33:53 +0000 (UTC)","[GitHub] spark pull request: remove ""sbt/"" prefix in $JAR variable make the...",dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/46#issuecomment-36338504
  
    Can one of the admins verify this patch?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 10:34:06 +0000 (UTC)","[GitHub] spark pull request: remove ""sbt/"" prefix in $JAR variable make the...",dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/46#issuecomment-36338520
  
    Can one of the admins verify this patch?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
lfrancke <git@git.apache.org>,"Fri, 28 Feb 2014 11:48:51 +0000 (UTC)",,dev@spark.apache.org,"Github user lfrancke commented on the pull request:

    https://github.com/apache/spark/pull/6#issuecomment-36343187
  
    The Avro dependency is still in the yarn/pom.xml file and causes Maven to fail early. I'm happy to submit a pull request/issue but maybe you want to do another Hotfix instead?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
Lars Francke <lars.francke@gmail.com>,"Fri, 28 Feb 2014 13:13:17 +0100",YARN Maven build questions,dev@spark.apache.org,"Hey,

I'm trying to dig into Spark's code but am running into a couple of problems.

1) The yarn-common directory is not included in the Maven build
causing things to fail because the dependency is missing. If I see the
history correct it used to be a Maven module but is not anymore.

2) When I try to include the yarn-common directory in the build things
start going bad. Compilation failures all over the place and I think
there are some dependency issues in there as well.

This leads me to believe that either the Maven build system isn't
maintained for YARN or the whole YARN branch isn't. What's the status
here?

Without YARN things build fine for me using Maven.

Thanks for your help.

Cheers,
Lars

"
coderh <git@git.apache.org>,"Fri, 28 Feb 2014 13:13:13 +0000 (UTC)","[GitHub] spark pull request: remove ""sbt/"" prefix in $JAR variable make the...",dev@spark.apache.org,"Github user coderh closed the pull request at:

    https://github.com/apache/spark/pull/46


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
Tom Graves <tgraves_cs@yahoo.com>,"Fri, 28 Feb 2014 06:05:17 -0800 (PST)",Re: YARN Maven build questions,"""dev@spark.apache.org"" <dev@spark.apache.org>","what build command are you using?    What do you mean when you say YARN branch? 

The yarn builds have been working fine for me with maven.   Build command I use against hadoop 2.2 or higher: mvn -Dyarn.version=2.2.0 -Dhadoop.version=2.2.0 -Pyarn clean package -DskipTests

Tom am running into a couple of problems.

1) The yarn-common directory is not included in the Maven build
causing things to fail because the dependency is missing. If I see the
history correct it used to be a Maven module but is not anymore.

2) When I try to include the yarn-common directory in the build things
start going bad. Compilation failures all over the place and I think
there are some dependency issues in there as well.

This leads me to believe that either the Maven build system isn't
maintained for YARN or the whole YARN branch isn't. What's the status
here?

Without YARN things build fine for me using Maven.

Thanks for your help.

Cheers,
Lars"
CodingCat <git@git.apache.org>,"Fri, 28 Feb 2014 14:09:27 +0000 (UTC)",[GitHub] spark pull request: fix #SPARK-1149 Bad partitioners can cause Spa...,dev@spark.apache.org,"Github user CodingCat commented on a diff in the pull request:

    https://github.com/apache/spark/pull/44#discussion_r10164732
  
    --- Diff: core/src/main/scala/org/apache/spark/SparkContext.scala ---
    @@ -847,6 +847,8 @@ class SparkContext(
           partitions: Seq[Int],
           allowLocal: Boolean,
           resultHandler: (Int, U) => Unit) {
    +    val rddPartitions = rdd.partitions.map(_.index)
    +    partitions.foreach(p =>require(rddPartitions.contains(p), ""partition index out of range: "" +p))
    --- End diff --
    
    require(partitions.forall(rddPartitions.contains, ""partition index out of range"")), more intuitive?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
CodingCat <git@git.apache.org>,"Fri, 28 Feb 2014 14:15:01 +0000 (UTC)",[GitHub] spark pull request: [SPARK-1102] Create a saveAsNewAPIHadoopDatase...,dev@spark.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/spark/pull/12#issuecomment-36353844
  
    I changed back the parameter type of the new method to Configuration for keeping consistent with other APIs, and whether Job should be parameter type is still under discussion
    
    https://spark-project.atlassian.net/browse/SPARK-1139


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
witgo <git@git.apache.org>,"Fri, 28 Feb 2014 15:08:00 +0000 (UTC)",[GitHub] spark pull request: fix #SPARK-1149 Bad partitioners can cause Spa...,dev@spark.apache.org,"Github user witgo commented on a diff in the pull request:

    https://github.com/apache/spark/pull/44#discussion_r10166854
  
    --- Diff: core/src/main/scala/org/apache/spark/SparkContext.scala ---
    @@ -847,6 +847,8 @@ class SparkContext(
           partitions: Seq[Int],
           allowLocal: Boolean,
           resultHandler: (Int, U) => Unit) {
    +    val rddPartitions = rdd.partitions.map(_.index)
    +    partitions.foreach(p =>require(rddPartitions.contains(p), ""partition index out of range: "" +p))
    --- End diff --
    
    ` require(partitions.forall(rddPartitions.contains(_)), ""partition index out of range"")`  is more readable?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
CodingCat <git@git.apache.org>,"Fri, 28 Feb 2014 15:10:33 +0000 (UTC)",[GitHub] spark pull request: fix #SPARK-1149 Bad partitioners can cause Spa...,dev@spark.apache.org,"Github user CodingCat commented on a diff in the pull request:

    https://github.com/apache/spark/pull/44#discussion_r10166948
  
    --- Diff: core/src/main/scala/org/apache/spark/SparkContext.scala ---
    @@ -847,6 +847,8 @@ class SparkContext(
           partitions: Seq[Int],
           allowLocal: Boolean,
           resultHandler: (Int, U) => Unit) {
    +    val rddPartitions = rdd.partitions.map(_.index)
    +    partitions.foreach(p =>require(rddPartitions.contains(p), ""partition index out of range: "" +p))
    --- End diff --
    
    Oops, I found that the parentheses are mismatching in my comments 


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
tgravescs <git@git.apache.org>,"Fri, 28 Feb 2014 15:41:48 +0000 (UTC)",[GitHub] spark pull request: SPARK-1032. If Yarn app fails before registeri...,dev@spark.apache.org,"Github user tgravescs commented on the pull request:

    https://github.com/apache/spark/pull/28#issuecomment-36362791
  
    I committed this. Thanks Sandy!


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
tgravescs <git@git.apache.org>,"Fri, 28 Feb 2014 15:43:39 +0000 (UTC)",[GitHub] spark pull request: Update dev merge script to use spark.git inste...,dev@spark.apache.org,"GitHub user tgravescs opened a pull request:

    https://github.com/apache/spark/pull/47

    Update dev merge script to use spark.git instead of incubator-spark

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/tgravescs/spark fix_merge_script

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/spark/pull/47.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #47
    
----
commit 8209ab152dc14f534585d1235b32f6d22d8952a5
Author: Thomas Graves <tgraves@apache.org>
Date:   2014-02-28T15:42:54Z

    Update dev merge script to use spark.git instead of incubator-spark

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 15:43:53 +0000 (UTC)",[GitHub] spark pull request: Update dev merge script to use spark.git inste...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/47#issuecomment-36363011
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 15:43:53 +0000 (UTC)",[GitHub] spark pull request: Update dev merge script to use spark.git inste...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/47#issuecomment-36363010
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 15:44:06 +0000 (UTC)",[GitHub] spark pull request: Update dev merge script to use spark.git inste...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/47#issuecomment-36363038
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
CodingCat <git@git.apache.org>,"Fri, 28 Feb 2014 15:54:24 +0000 (UTC)",[GitHub] spark pull request: fix repo location in create script,dev@spark.apache.org,"GitHub user CodingCat opened a pull request:

    https://github.com/apache/spark/pull/48

    fix repo location in create script

    fix the repo location in create_release script

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/CodingCat/spark script_fixes

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/spark/pull/48.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #48
    
----
commit fc05a7167c079543203d4dc7951b383c154c8cdd
Author: CodingCat <zhunansjtu@gmail.com>
Date:   2014-02-28T15:57:25Z

    fix repo location in create script

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 15:58:52 +0000 (UTC)",[GitHub] spark pull request: [SPARK-1150] fix repo location in create scrip...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/48#issuecomment-36364594
  
    Can one of the admins verify this patch?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 15:59:05 +0000 (UTC)",[GitHub] spark pull request: [SPARK-1150] fix repo location in create scrip...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/48#issuecomment-36364625
  
    Can one of the admins verify this patch?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 16:12:32 +0000 (UTC)",[GitHub] spark pull request: Update dev merge script to use spark.git inste...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/47#issuecomment-36366006
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 16:12:32 +0000 (UTC)",[GitHub] spark pull request: Update dev merge script to use spark.git inste...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/47#issuecomment-36366007
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12930/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
asfgit <git@git.apache.org>,"Fri, 28 Feb 2014 16:19:53 +0000 (UTC)",[GitHub] spark pull request: SPARK-1032. If Yarn app fails before registeri...,dev@spark.apache.org,"Github user asfgit closed the pull request at:

    https://github.com/apache/spark/pull/28


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
tgravescs <git@git.apache.org>,"Fri, 28 Feb 2014 16:26:29 +0000 (UTC)",,dev@spark.apache.org,"Github user tgravescs commented on the pull request:

    https://github.com/apache/spark/pull/29#issuecomment-36367636
  
    +1. Looks Good. Thanks Sandy!


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
tgravescs <git@git.apache.org>,"Fri, 28 Feb 2014 16:33:37 +0000 (UTC)",,dev@spark.apache.org,"Github user tgravescs commented on the pull request:

    https://github.com/apache/spark/pull/29#issuecomment-36368483
  
    @sryza it looks like this is no longer mergeable due to other check ins. Can you please update it to the latest.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
sryza <git@git.apache.org>,"Fri, 28 Feb 2014 16:36:47 +0000 (UTC)",[GitHub] spark pull request: SPARK-1032. If Yarn app fails before registeri...,dev@spark.apache.org,"Github user sryza commented on the pull request:

    https://github.com/apache/spark/pull/28#issuecomment-36368855
  
    Thanks Tom!


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
tgravescs <git@git.apache.org>,"Fri, 28 Feb 2014 16:47:50 +0000 (UTC)",,dev@spark.apache.org,"Github user tgravescs commented on the pull request:

    https://github.com/apache/spark/pull/6#issuecomment-36370069
  
    I disagree with this change going in and breaking the yarn hadoop0.23 build. If we are going to support maven and hadoop 0.23 you should be able to build it without manually looking though jira/pull request to figure out what to add back in to get it to build.    I unfortunately didn't see it before it went in.  
    
    Note I think we should revert this unless someone is going to fix it soon.  If we aren't going to support maven it should be done globally, as far as I've seen that discussion hasn't finished.



---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
sryza <git@git.apache.org>,"Fri, 28 Feb 2014 17:11:53 +0000 (UTC)",,dev@spark.apache.org,"Github user sryza commented on the pull request:

    https://github.com/apache/spark/pull/29#issuecomment-36372574
  
    Uploaded a rebased patch


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 17:13:57 +0000 (UTC)",,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/29#issuecomment-36372766
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 17:13:58 +0000 (UTC)",,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/29#issuecomment-36372765
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 17:14:13 +0000 (UTC)",,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/29#issuecomment-36372790
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
Lars Francke <lars.francke@gmail.com>,"Fri, 28 Feb 2014 18:41:00 +0100",Re: YARN Maven build questions,"dev@spark.apache.org, Tom Graves <tgraves_cs@yahoo.com>","Hey,

so currently it doesn't work because of
<https://github.com/apache/spark/pull/6#issuecomment-36343187>

IntelliJ reports a lot of warnings with default settings and I haven't
found a way to tell IntellJ to use different Hadoop versions yet.
mvn clean compile -Pyarn fails as well (compilation errror

Your command works indeed. Default yarn version is 0.23.7 which
doesn't seem to work with the default 2.2.0 Hadoop version (anymore?)

I was basically trying to follow the documentation:
<http://spark.incubator.apache.org/docs/latest/building-with-maven.html>

mvn clean compile -Pyarn-alpha -Dhadoop.version=2.0.0-cdh4.5.0
-Dyarn.version=2.0.0-cdh4.5.0 fails as well as does mvn clean compile
-Pyarn-alpha

Thanks for showing me a configuration that works. Unfortunately the
default ones and at least one of the documented ones fail.

Cheers,
Lars



"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 17:42:28 +0000 (UTC)",,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/29#issuecomment-36375413
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12931/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 17:42:28 +0000 (UTC)",,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/29#issuecomment-36375411
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Fri, 28 Feb 2014 18:15:42 +0000 (UTC)",,dev@spark.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/spark/pull/6#issuecomment-36378496
  
    @tgravescs hey tom if we can't do this with a profile we can revert it. Didn't realize 0.23.X was widely used.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
mengxr <git@git.apache.org>,"Fri, 28 Feb 2014 18:20:02 +0000 (UTC)",[GitHub] spark pull request: MLI-2: Start adding k-fold cross validation to...,dev@spark.apache.org,"Github user mengxr commented on a diff in the pull request:

    https://github.com/apache/spark/pull/18#discussion_r10174437
  
    --- Diff: mllib/src/main/scala/org/apache/spark/mllib/util/MLUtils.scala ---
    @@ -62,6 +67,20 @@ object MLUtils {
       }
     
       /**
    +   * Return a k element list of pairs of RDDs with the first element of each pair
    +   * containing a unique 1/Kth of the data and the second element contain the compliment of that. 
    +   */
    +  def kFold[T : ClassTag](rdd: RDD[T], folds: Int, seed: Int): List[Pair[RDD[T], RDD[T]]] = {
    --- End diff --
    
    folds -> numFolds?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
mengxr <git@git.apache.org>,"Fri, 28 Feb 2014 18:20:05 +0000 (UTC)",[GitHub] spark pull request: MLI-2: Start adding k-fold cross validation to...,dev@spark.apache.org,"Github user mengxr commented on a diff in the pull request:

    https://github.com/apache/spark/pull/18#discussion_r10174439
  
    --- Diff: mllib/src/main/scala/org/apache/spark/mllib/util/MLUtils.scala ---
    @@ -62,6 +67,20 @@ object MLUtils {
       }
     
       /**
    +   * Return a k element list of pairs of RDDs with the first element of each pair
    +   * containing a unique 1/Kth of the data and the second element contain the compliment of that. 
    +   */
    +  def kFold[T : ClassTag](rdd: RDD[T], folds: Int, seed: Int): List[Pair[RDD[T], RDD[T]]] = {
    +    val foldsF = folds.toFloat
    +    1.to(folds).map  { fold =>
    --- End diff --
    
    (1 to folds).map {


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Fri, 28 Feb 2014 18:26:00 +0000 (UTC)",[GitHub] spark pull request: SPARK-1145: Memory mapping with many small blo...,dev@spark.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/spark/pull/43#discussion_r10174657
  
    --- Diff: core/src/main/scala/org/apache/spark/storage/DiskStore.scala ---
    @@ -84,12 +84,27 @@ private class DiskStore(blockManager: BlockManager, diskManager: DiskBlockManage
       override def getBytes(blockId: BlockId): Option[ByteBuffer] = {
         val segment = diskManager.getBlockLocation(blockId)
         val channel = new RandomAccessFile(segment.file, ""r"").getChannel()
    -    val buffer = try {
    -      channel.map(MapMode.READ_ONLY, segment.offset, segment.length)
    -    } finally {
    -      channel.close()
    +
    +    val buffer =
    +      // For small files, directly read rather than memory map
    +      if (segment.length < 2 * 4096) {
    --- End diff --
    
    somewhat arbitrary, but related to the page size. If the segment is smaller than a couple of pages the fact that memory mapping aligns on page boundaries means a lot of wasted memory.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Fri, 28 Feb 2014 18:27:55 +0000 (UTC)",[GitHub] spark pull request: SPARK-1145: Memory mapping with many small blo...,dev@spark.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/spark/pull/43#discussion_r10174727
  
    --- Diff: core/src/main/scala/org/apache/spark/storage/BlockFetcherIterator.scala ---
    @@ -146,6 +146,12 @@ object BlockFetcherIterator {
         }
     
         protected def splitLocalRemoteBlocks(): ArrayBuffer[FetchRequest] = {
    +      // Make remote requests at most maxBytesInFlight / 5 in length; the reason to keep them
    +      // smaller than maxBytesInFlight is to allow multiple, parallel fetches from up to 5
    +      // nodes, rather than blocking on reading output from one node.
    +      val maxRequestSize = math.max(maxBytesInFlight / 5, 1L)
    +      logInfo(""maxBytesInFlight: "" + maxBytesInFlight + "", maxRequestSize: "" + maxRequestSize)
    +
    --- End diff --
    
    The only change here is to move this up. The reason is that there is no reason to calculate and log this in the inner loop, because its the same for all iterations of that loop. Logging it in the inner loop also means the same message is logged a bunch of times.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
kayousterhout <git@git.apache.org>,"Fri, 28 Feb 2014 18:34:20 +0000 (UTC)",[GitHub] spark pull request: [WIP] [SPARK-1132] Persisting Web UI through r...,dev@spark.apache.org,"Github user kayousterhout commented on a diff in the pull request:

    https://github.com/apache/spark/pull/42#discussion_r10175006
  
    --- Diff: core/src/main/scala/org/apache/spark/scheduler/JobLogger.scala ---
    @@ -339,8 +217,8 @@ class JobLogger(val user: String, val logDirName: String)
        * @param jobEnd Job end event
        */
       override def onJobEnd(jobEnd: SparkListenerJobEnd) {
    -    val job = jobEnd.job
    -    var info = ""JOB_ID="" + job.jobId
    +    val jobID = jobEnd.jobId
    --- End diff --
    
    nit: camel case here? (jobId)


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Fri, 28 Feb 2014 18:36:24 +0000 (UTC)",[GitHub] spark pull request: [WIP] [SPARK-1132] Persisting Web UI through r...,dev@spark.apache.org,"Github user pwendell commented on the pull request:

    https://github.com/apache/spark/pull/42#issuecomment-36380483
  
    Jenkins, test this please.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 18:38:54 +0000 (UTC)",[GitHub] spark pull request: [WIP] [SPARK-1132] Persisting Web UI through r...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/42#issuecomment-36380718
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 18:38:55 +0000 (UTC)",[GitHub] spark pull request: [WIP] [SPARK-1132] Persisting Web UI through r...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/42#issuecomment-36380717
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 18:39:08 +0000 (UTC)",[GitHub] spark pull request: [WIP] [SPARK-1132] Persisting Web UI through r...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/42#issuecomment-36380733
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 19:08:05 +0000 (UTC)",[GitHub] spark pull request: [WIP] [SPARK-1132] Persisting Web UI through r...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/42#issuecomment-36383623
  
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12932/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 19:08:05 +0000 (UTC)",[GitHub] spark pull request: [WIP] [SPARK-1132] Persisting Web UI through r...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/42#issuecomment-36383621
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
asfgit <git@git.apache.org>,"Fri, 28 Feb 2014 19:14:16 +0000 (UTC)",,dev@spark.apache.org,"Github user asfgit closed the pull request at:

    https://github.com/apache/spark/pull/29


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
Evan Chan <ev@ooyala.com>,"Fri, 28 Feb 2014 11:29:56 -0800",Spark JIRA,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey guys,

There is no plan to move the Spark JIRA from the current
https://spark-project.atlassian.net/

right?

-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

"
Nan Zhu <zhunanmcgill@gmail.com>,"Fri, 28 Feb 2014 14:36:43 -0500",Re: Spark JIRA,dev@spark.apache.org,"I think they are working on it? https://issues.apache.org/jira/browse/SPARK 

Best, 

-- 
Nan Zhu





"
Josh Rosen <rosenville@gmail.com>,"Fri, 28 Feb 2014 11:36:51 -0800",Re: Spark JIRA,dev@spark.apache.org,"Apache INFRA has been trying to import our JIRA issues since June 2013:
https://issues.apache.org/jira/browse/INFRA-6419.  It seems that JIRA
introduces minor import incompatibilities with every update to JIRA
versions to sync up.





"
Evan Chan <ev@ooyala.com>,"Fri, 28 Feb 2014 11:40:04 -0800",Re: Spark JIRA,"""dev@spark.apache.org"" <dev@spark.apache.org>","Ok, will continue to work with the existing one for now.




-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

"
Evan Chan <ev@ooyala.com>,"Fri, 28 Feb 2014 11:43:01 -0800",New JIRA ticket: cleaning up app-* folders,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey guys,

FYI, I just filed a new ticket,
https://spark-project.atlassian.net/browse/SPARK-1154,
concerning that Spark leaves tons of app-* folders on disk, which can
quickly fill up the disk.
Feel free to comment.

I believe it is better for Spark to clean itself up; ie users should
not need a cron job to clean up old folders.

thanks,
-Evan

-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

"
JoshRosen <git@git.apache.org>,"Fri, 28 Feb 2014 19:52:29 +0000 (UTC)",[GitHub] spark pull request: SPARK-1134 pyspark only uses ipython if starte...,dev@spark.apache.org,"Github user JoshRosen commented on the pull request:

    https://github.com/apache/spark/pull/38#issuecomment-36387844
  
    I'd like to hold off on merging this until we see how https://github.com/ipython/ipython/pull/5226 is resolved.  It looks like IPython executes `PYTHONSTARTUP` for non-interactive shells, too, which is different from the [behavior in the regular Python interpreter](http://docs.python.org/2/tutorial/interpreter.html#the-interactive-startup-file).  If IPython decides to adopt Python's default `PYTHONSTARTUP` behavior, then I think we'd want to implement a different fix.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
JoshRosen <git@git.apache.org>,"Fri, 28 Feb 2014 19:54:22 +0000 (UTC)",[GitHub] spark pull request: SPARK-1134 pyspark only uses ipython if starte...,dev@spark.apache.org,"Github user JoshRosen commented on a diff in the pull request:

    https://github.com/apache/spark/pull/38#discussion_r10178681
  
    --- Diff: bin/pyspark ---
    @@ -58,7 +58,8 @@ if [ -n ""$IPYTHON_OPTS"" ]; then
       IPYTHON=1
     fi
     
    -if [[ ""$IPYTHON"" = ""1"" ]] ; then
    +if [[ ""$IPYTHON"" = ""1"" && $# = 0 ]] ; then
       exec ipython $IPYTHON_OPTS
    --- End diff --
    
    If we can, I'd like to remove `IPYTHON_OPTS`, since it makes it a little awkward to start the notebook server:
    
    ```
    IPYTHON=1 IPYTHON_OPTS=notebook ./pyspark
    ```
    versus
    
    ```
    IPYTHON=1 ./pyspark notebook
    ```


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
mengxr <git@git.apache.org>,"Fri, 28 Feb 2014 19:57:41 +0000 (UTC)",[GitHub] spark pull request: Initialized the regVal for first iteration in ...,dev@spark.apache.org,"Github user mengxr commented on a diff in the pull request:

    https://github.com/apache/spark/pull/40#discussion_r10178847
  
    --- Diff: mllib/src/main/scala/org/apache/spark/mllib/optimization/GradientDescent.scala ---
    @@ -149,7 +149,13 @@ object GradientDescent extends Logging {
     
         // Initialize weights as a column vector
         var weights = new DoubleMatrix(initialWeights.length, 1, initialWeights:_*)
    -    var regVal = 0.0
    +
    +    /**
    +     * For the first iteration, the regVal will be initialized as sum of sqrt of
    +     * weights if it's L2 update; for L1 update; the same logic is followed.
    +     */
    +    var regVal = updater.compute(weights,
    +      new DoubleMatrix(initialWeights.length, 1), 0, 1, regParam)._2
    --- End diff --
    
    The following looks better to me, but I'm not sure. @rxin ?
    ~~~
    var regVal = updater.compute(
      weights,
      new DoubleMatrix(initialWeights.length, 1), 0, 1, regParam
    )._2
    ~~~


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
mengxr <git@git.apache.org>,"Fri, 28 Feb 2014 19:59:01 +0000 (UTC)",[GitHub] spark pull request: Initialized the regVal for first iteration in ...,dev@spark.apache.org,"Github user mengxr commented on a diff in the pull request:

    https://github.com/apache/spark/pull/40#discussion_r10178907
  
    --- Diff: mllib/src/test/scala/org/apache/spark/mllib/optimization/GradientDescentSuite.scala ---
    @@ -104,4 +104,45 @@ class GradientDescentSuite extends FunSuite with LocalSparkContext with ShouldMa
         val lossDiff = loss.init.zip(loss.tail).map { case (lhs, rhs) => lhs - rhs }
         assert(lossDiff.count(_ > 0).toDouble / lossDiff.size > 0.8)
       }
    +
    +  test(""Test the loss and gradient of first iteration with regularization."") {
    +
    +    val gradient = new LogisticGradient()
    +    val updater = new SquaredL2Updater()
    +
    +    // Add a extra variable consisting of all 1.0's for the intercept.
    +    val testData = GradientDescentSuite.generateGDInput(2.0, -1.5, 10000, 42)
    +    val data = testData.map { case LabeledPoint(label, features) =>
    +      label -> Array(1.0, features: _*)
    +    }
    +
    +    val dataRDD = sc.parallelize(data, 2).cache()
    +
    +    // Prepare non-zero weights
    +    val initialWeightsWithIntercept = Array(1.0, 0.5)
    +
    +    val regParam0 = 0
    +    val (newWeights0, loss0) = GradientDescent.runMiniBatchSGD(
    +      dataRDD, gradient, updater, 1, 1, regParam0, 1.0, initialWeightsWithIntercept)
    +
    +    val regParam1 = 1
    +    val (newWeights1, loss1) = GradientDescent.runMiniBatchSGD(
    +      dataRDD, gradient, updater, 1, 1, regParam1, 1.0, initialWeightsWithIntercept)
    +
    +    def compareDouble(x: Double, y: Double, tol: Double = 1E-3): Boolean = {
    +      math.abs(x - y) / (math.abs(y) + 1e-15) < tol
    +    }
    +
    +    assert(compareDouble(
    +      loss1(0),
    +      loss0(0) + (math.pow(initialWeightsWithIntercept(0), 2) +
    +        math.pow(initialWeightsWithIntercept(1), 2)) / 2),
    +      """"""For non-zero weights, the regVal should be \frac{1}{2}\sum_i w_i^2."""""")
    +
    +    assert(
    +      compareDouble(newWeights1(0) , newWeights0(0) - initialWeightsWithIntercept(0)) &&
    +      compareDouble(newWeights1(1) , newWeights0(1) - initialWeightsWithIntercept(1)),
    +      ""The different between newWeights with/without regularization "" +
    +        ""should be initialWeightsWithIntercept"")
    --- End diff --
    
    Missing a period at the end.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Fri, 28 Feb 2014 20:12:38 +0000 (UTC)",[GitHub] spark pull request: SPARK-1121: Use 'avro' profile in Maven.,dev@spark.apache.org,"GitHub user pwendell opened a pull request:

    https://github.com/apache/spark/pull/49

    SPARK-1121: Use 'avro' profile in Maven.

    This lets us explicitly include Avro based on a profile for 0.23.X
    builds. It makes me sad how convoluted it is to express this logic
    in Maven. @tgraves and @sryza curious if this works for you.
    
    I'm also considering just reverting to how it was before. The only
    real problem was that Spark advertised a dependency on Avro
    even though it only really depends transitively on Avro through
    other deps.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/pwendell/spark avro-build-fix

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/spark/pull/49.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #49
    
----
commit 6faab259b6fe5cc58a8ffd2d53ca7e1a828359dc
Author: Patrick Wendell <pwendell@gmail.com>
Date:   2014-02-28T19:56:41Z

    SPARK-1121: Use 'avro' profile in Maven.
    
    This lets us explicitly include Avro based on a profile for 0.23.X
    builds. It makes me sad how convoluted it is to express this logic
    in Maven.

----


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 20:13:52 +0000 (UTC)",[GitHub] spark pull request: SPARK-1121: Use 'avro' profile in Maven.,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/49#issuecomment-36389807
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 20:13:53 +0000 (UTC)",[GitHub] spark pull request: SPARK-1121: Use 'avro' profile in Maven.,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/49#issuecomment-36389809
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 20:14:05 +0000 (UTC)",[GitHub] spark pull request: SPARK-1121: Use 'avro' profile in Maven.,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/49#issuecomment-36389827
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
mridulm <git@git.apache.org>,"Fri, 28 Feb 2014 20:25:05 +0000 (UTC)",[GitHub] spark pull request: SPARK-1145: Memory mapping with many small blo...,dev@spark.apache.org,"Github user mridulm commented on a diff in the pull request:

    https://github.com/apache/spark/pull/43#discussion_r10180027
  
    --- Diff: core/src/main/scala/org/apache/spark/storage/DiskStore.scala ---
    @@ -84,12 +84,27 @@ private class DiskStore(blockManager: BlockManager, diskManager: DiskBlockManage
       override def getBytes(blockId: BlockId): Option[ByteBuffer] = {
         val segment = diskManager.getBlockLocation(blockId)
         val channel = new RandomAccessFile(segment.file, ""r"").getChannel()
    -    val buffer = try {
    -      channel.map(MapMode.READ_ONLY, segment.offset, segment.length)
    -    } finally {
    -      channel.close()
    +
    +    val buffer =
    +      // For small files, directly read rather than memory map
    +      if (segment.length < 2 * 4096) {
    --- End diff --
    
    I was hoping it was obtained in a more principled manner ... ah well
    in which case, atleast make it configurable - dont think anyone is going to muck around with it; but you never know.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
Patrick Wendell <pwendell@gmail.com>,"Fri, 28 Feb 2014 12:35:04 -0800",Re: [DISCUSS] Necessity of Maven *and* SBT Build in Spark,dev@spark.apache.org,"Hey,

Thanks everyone for chiming in on this. I wanted to summarize these
issues a bit particularly wrt the constituents involved - does this
seem accurate?

= Spark Users =
In general those linking against Spark should be totally unaffected by
the build choice. Spark will continue to publish well-formed poms and
jars to maven central. This is a no-op wrt this decision.

= Spark Developers =
There are two concerns. (a) General day-to-day development and
packaging and (b) Spark binaries and packages for distribution.

For (a) - sbt seems better because it's just nicer for doing scala
development (incremental complication is simple, we have some
home-baked tools for compiling Spark vs. the spark deps etc). The
arguments that maven has more ""general know how"", at least so far,
haven't affected us in the ~2 years we've maintained both builds -
where adding stuff for Maven is typically just as annoying/difficult
with sbt.

For (b) - Some non-specific concerns were raised about bugs with the
sbt assembly package - we should look into this and see what is going
on. Maven has better out-of-the-box support for publishing to Maven
central, we'd have to do some manual work on our end to make this work
well with sbt.

= Downstream Integrators =
because of community awareness of Maven and comfort with Maven builds.
Some things like restructuring the Spark build to inherit config
values from a vendor build will be not possible with sbt (though
fairly straightforward to work around). Other cases where vendors have
directly modified or inherited the Spark build won't work anymore if
we standardize on SBT. These have no obvious work around at this point
as far as I see.

- Patrick


"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 20:42:41 +0000 (UTC)",[GitHub] spark pull request: SPARK-1121: Use 'avro' profile in Maven.,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/49#issuecomment-36392299
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 20:42:41 +0000 (UTC)",[GitHub] spark pull request: SPARK-1121: Use 'avro' profile in Maven.,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/49#issuecomment-36392301
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12933/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
Evan Chan <ev@ooyala.com>,"Fri, 28 Feb 2014 13:00:06 -0800",New blog post on Spark + Parquet + Scrooge,"""dev@spark.apache.org"" <dev@spark.apache.org>","http://engineering.ooyala.com/blog/using-parquet-and-scrooge-spark

Enjoy!

By the way, I was not able to subscribe to the user-digest list for
some reason.   The help email claims


Similar addresses exist for the digest list:
   <user-allow-digest-subscribe@spark.apache.org>
   <user-allow-digest-unsubscribe@spark.apache.org>


But if you send a email to the digest-subscribe it bounces back with a
help email.


-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

"
dbtsai <git@git.apache.org>,"Fri, 28 Feb 2014 21:11:37 +0000 (UTC)",[GitHub] spark pull request: Initialized the regVal for first iteration in ...,dev@spark.apache.org,"Github user dbtsai commented on a diff in the pull request:

    https://github.com/apache/spark/pull/40#discussion_r10181447
  
    --- Diff: mllib/src/main/scala/org/apache/spark/mllib/optimization/GradientDescent.scala ---
    @@ -149,7 +149,13 @@ object GradientDescent extends Logging {
     
         // Initialize weights as a column vector
         var weights = new DoubleMatrix(initialWeights.length, 1, initialWeights:_*)
    -    var regVal = 0.0
    +
    +    /**
    +     * For the first iteration, the regVal will be initialized as sum of sqrt of
    +     * weights if it's L2 update; for L1 update; the same logic is followed.
    +     */
    +    var regVal = updater.compute(weights,
    +      new DoubleMatrix(initialWeights.length, 1), 0, 1, regParam)._2
    --- End diff --
    
    I saw code in spark codebase written in this way. 
    
        var regVal = updater.compute(
          weights,
          new DoubleMatrix(initialWeights.length, 1), 0, 1, regParam)._2
    
    What do you think?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 21:53:54 +0000 (UTC)",[GitHub] spark pull request: [WIP] [SPARK-1132] Persisting Web UI through r...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/42#issuecomment-36398304
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 21:53:55 +0000 (UTC)",[GitHub] spark pull request: [WIP] [SPARK-1132] Persisting Web UI through r...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/42#issuecomment-36398305
  
    Merged build started.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 21:54:08 +0000 (UTC)",[GitHub] spark pull request: [WIP] [SPARK-1132] Persisting Web UI through r...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/42#issuecomment-36398321
  
     Merged build triggered.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
andrewor14 <git@git.apache.org>,"Fri, 28 Feb 2014 21:55:18 +0000 (UTC)",[GitHub] spark pull request: [WIP] [SPARK-1132] Persisting Web UI through r...,dev@spark.apache.org,"Github user andrewor14 commented on a diff in the pull request:

    https://github.com/apache/spark/pull/42#discussion_r10183037
  
    --- Diff: core/src/main/scala/org/apache/spark/scheduler/JobLogger.scala ---
    @@ -339,8 +217,8 @@ class JobLogger(val user: String, val logDirName: String)
        * @param jobEnd Job end event
        */
       override def onJobEnd(jobEnd: SparkListenerJobEnd) {
    -    val job = jobEnd.job
    -    var info = ""JOB_ID="" + job.jobId
    +    val jobID = jobEnd.jobId
    --- End diff --
    
    Yeah, actually I had jobId before, but to make it consistent with the rest of the file I reverted it back to jobID. I'll change it back.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
kayousterhout <git@git.apache.org>,"Fri, 28 Feb 2014 22:00:17 +0000 (UTC)",[GitHub] spark pull request: [WIP] [SPARK-1132] Persisting Web UI through r...,dev@spark.apache.org,"Github user kayousterhout commented on a diff in the pull request:

    https://github.com/apache/spark/pull/42#discussion_r10183222
  
    --- Diff: core/src/main/scala/org/apache/spark/scheduler/JobLogger.scala ---
    @@ -339,8 +217,8 @@ class JobLogger(val user: String, val logDirName: String)
        * @param jobEnd Job end event
        */
       override def onJobEnd(jobEnd: SparkListenerJobEnd) {
    -    val job = jobEnd.job
    -    var info = ""JOB_ID="" + job.jobId
    +    val jobID = jobEnd.jobId
    --- End diff --
    
    Yeah I realized after that comment that there seems to be inconsistency in
    the way ID is named so feel free to leave it however you prefer.
    
    
    
    > In core/src/main/scala/org/apache/spark/scheduler/JobLogger.scala:
    >
    > > @@ -339,8 +217,8 @@ class JobLogger(val user: String, val logDirName: String)
    > >     * @param jobEnd Job end event
    > >     */
    > >    override def onJobEnd(jobEnd: SparkListenerJobEnd) {
    > > -    val job = jobEnd.job
    > > -    var info = ""JOB_ID="" + job.jobId
    > > +    val jobID = jobEnd.jobId
    >
    > Yeah, actually I had jobId before, but to make it consistent with the rest
    > of the file I reverted it back to jobID. I'll change it back.
    >
    > --
    > Reply to this email directly or view it on GitHub<https://github.com/apache/spark/pull/42/files#r10183037>
    > .
    >


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
tgravescs <git@git.apache.org>,"Fri, 28 Feb 2014 22:01:05 +0000 (UTC)",[GitHub] spark pull request: SPARK-1121: Use 'avro' profile in Maven.,dev@spark.apache.org,"Github user tgravescs commented on the pull request:

    https://github.com/apache/spark/pull/49#issuecomment-36398886
  
    I'll give it a try. Any reason we don't just tie this to the yarn-alpha profile?  Or does it not apply to the hadoop 2.0.2 type builds?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Fri, 28 Feb 2014 22:08:10 +0000 (UTC)",[GitHub] spark pull request: [WIP] [SPARK-1132] Persisting Web UI through r...,dev@spark.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/spark/pull/42#discussion_r10183527
  
    --- Diff: core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala ---
    @@ -415,6 +415,7 @@ class JavaSparkContext(val sc: SparkContext) extends JavaSparkContextVarargsWork
        * Clear the job's list of JARs added by `addJar` so that they do not get downloaded to
        * any new nodes.
        */
    +  @deprecated(""added jars are now temporary files and need not be deleted manually"", ""1.0.0"")
    --- End diff --
    
    actually this is more like: ""add jars no longer creates local copies so they do not need to be deleted""


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Fri, 28 Feb 2014 22:14:10 +0000 (UTC)",[GitHub] spark pull request: [WIP] [SPARK-1132] Persisting Web UI through r...,dev@spark.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/spark/pull/42#discussion_r10183713
  
    --- Diff: core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala ---
    @@ -194,12 +194,19 @@ class DAGScheduler(
               }
           }
         }))
    +
    +    // Start listening for block manager registration
    +    blockManagerMaster.registrationListener.foreach(_.setListenerBus(listenerBus))
       }
     
       def addSparkListener(listener: SparkListener) {
         listenerBus.addListener(listener)
       }
     
    +  def post(event: SparkListenerEvent) {
    --- End diff --
    
    it would be better not to have other parts of Spark posting events to the DAGScheduler. Maybe we could do the following:
    
    1. Move the listener bus to the SparkContext (approved by @kayousterhout)
    2. Pass a SparkContext to the DAGScheduler and access it through that. Then we could remove the Spark env and other arguments currently passed in. 


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 22:22:36 +0000 (UTC)",[GitHub] spark pull request: [WIP] [SPARK-1132] Persisting Web UI through r...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/42#issuecomment-36400692
  
    Merged build finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
AmplabJenkins <git@git.apache.org>,"Fri, 28 Feb 2014 22:22:36 +0000 (UTC)",[GitHub] spark pull request: [WIP] [SPARK-1132] Persisting Web UI through r...,dev@spark.apache.org,"Github user AmplabJenkins commented on the pull request:

    https://github.com/apache/spark/pull/42#issuecomment-36400693
  
    All automated tests passed.
    Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/12934/


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Fri, 28 Feb 2014 22:35:55 +0000 (UTC)",[GitHub] spark pull request: [WIP] [SPARK-1132] Persisting Web UI through r...,dev@spark.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/spark/pull/42#discussion_r10184513
  
    --- Diff: core/src/main/scala/org/apache/spark/executor/TaskMetrics.scala ---
    @@ -17,21 +17,23 @@
     
     package org.apache.spark.executor
     
    +import org.apache.spark.storage.{BlockId, BlockStatus}
    +
     class TaskMetrics extends Serializable {
       /**
    -   * Host's name the task runs on 
    +   * Host's name the task runs on
        */
       var hostname: String = _
     
       /**
        * Time taken on the executor to deserialize this task
        */
    -  var executorDeserializeTime: Int = _
    +  var executorDeserializeTime: Long = _
    --- End diff --
    
    Any particular reason to change these to Long's?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
Mark Hamstra <mark@clearstorydata.com>,"Fri, 28 Feb 2014 15:10:40 -0800",Re: [DISCUSS] Necessity of Maven *and* SBT Build in Spark,dev@spark.apache.org,"Couple of comments: 1) Whether the Spark POM is produced by SBT or Maven
shouldn't matter for those who just need to link against published
artifacts, but right now SBT and Maven do not produce equivalent POMs for
Spark -- I think....  2) Incremental builds using Maven are trivially more
difficult than they are with SBT -- just start a Zinc daemon and forget
about it.



"
CodingCat <git@git.apache.org>,"Fri, 28 Feb 2014 23:42:05 +0000 (UTC)",[GitHub] spark pull request: [SPARK-1100] prevent Spark from overwriting di...,dev@spark.apache.org,"Github user CodingCat commented on the pull request:

    https://github.com/apache/spark/pull/11#issuecomment-36406325
  
    I changed the code and tested in local side, mind reviewing it again? @pwendell 


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Fri, 28 Feb 2014 23:45:32 +0000 (UTC)",[GitHub] spark pull request: [WIP] [SPARK-1132] Persisting Web UI through r...,dev@spark.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/spark/pull/42#discussion_r10186518
  
    --- Diff: core/src/main/scala/org/apache/spark/ui/UISparkListener.scala ---
    @@ -0,0 +1,210 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one or more
    + * contributor license agreements.  See the NOTICE file distributed with
    + * this work for additional information regarding copyright ownership.
    + * The ASF licenses this file to You under the Apache License, Version 2.0
    + * (the ""License""); you may not use this file except in compliance with
    + * the License.  You may obtain a copy of the License at
    + *
    + *    http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.spark.ui
    +
    +import scala.collection.mutable
    +import scala.collection.mutable.ArrayBuffer
    +
    +import org.json4s.jackson.JsonMethods._
    +
    +import org.apache.spark.scheduler._
    +import org.apache.spark.storage._
    +import org.apache.spark.util.FileLogger
    +import org.apache.spark.util.JsonProtocol
    +
    +private[ui] trait UISparkListener extends SparkListener
    +
    +/**
    + * A SparkListener that serves as an entry point for all events posted to the UI.
    + *
    + * GatewayUISparkListener achieves two functions:
    + *
    + *  (1) If the UI is live, GatewayUISparkListener posts each event to all attached listeners
    + *      then logs it as JSON. This centralizes event logging and avoids having all attached
    + *      listeners log the events on their own. By default, GatewayUISparkListener logs one
    + *      file per job, though this needs not be the case.
    --- End diff --
    
    need not


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Fri, 28 Feb 2014 23:52:41 +0000 (UTC)",[GitHub] spark pull request: [WIP] [SPARK-1132] Persisting Web UI through r...,dev@spark.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/spark/pull/42#discussion_r10186680
  
    --- Diff: core/src/main/scala/org/apache/spark/ui/UISparkListener.scala ---
    @@ -0,0 +1,210 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one or more
    + * contributor license agreements.  See the NOTICE file distributed with
    + * this work for additional information regarding copyright ownership.
    + * The ASF licenses this file to You under the Apache License, Version 2.0
    + * (the ""License""); you may not use this file except in compliance with
    + * the License.  You may obtain a copy of the License at
    + *
    + *    http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.spark.ui
    +
    +import scala.collection.mutable
    +import scala.collection.mutable.ArrayBuffer
    +
    +import org.json4s.jackson.JsonMethods._
    +
    +import org.apache.spark.scheduler._
    +import org.apache.spark.storage._
    +import org.apache.spark.util.FileLogger
    +import org.apache.spark.util.JsonProtocol
    +
    +private[ui] trait UISparkListener extends SparkListener
    +
    +/**
    + * A SparkListener that serves as an entry point for all events posted to the UI.
    + *
    + * GatewayUISparkListener achieves two functions:
    + *
    + *  (1) If the UI is live, GatewayUISparkListener posts each event to all attached listeners
    + *      then logs it as JSON. This centralizes event logging and avoids having all attached
    + *      listeners log the events on their own. By default, GatewayUISparkListener logs one
    + *      file per job, though this needs not be the case.
    + *
    + *  (2) If the UI is rendered from disk, GatewayUISparkListener replays each event deserialized
    + *      from the event logs to all attached listeners.
    + */
    +private[ui] class GatewayUISparkListener(parent: SparkUI, live: Boolean) extends SparkListener {
    --- End diff --
    
    I think as a starting point it would be better to just log at application boundaries rather than job boundaries. I.e. just have a single log for the entire spark application. This class makes the assumption that jobs are (mostly) sequential, but in fact stages and tasks from different jobs can be interleaved. So rolling the log based on onJobStart/onJobEnd is not correct.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Sat,  1 Mar 2014 00:00:18 +0000 (UTC)",[GitHub] spark pull request: [WIP] [SPARK-1132] Persisting Web UI through r...,dev@spark.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/spark/pull/42#discussion_r10186874
  
    --- Diff: core/src/main/scala/org/apache/spark/util/FileLogger.scala ---
    @@ -0,0 +1,103 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one or more
    + * contributor license agreements.  See the NOTICE file distributed with
    + * this work for additional information regarding copyright ownership.
    + * The ASF licenses this file to You under the Apache License, Version 2.0
    + * (the ""License""); you may not use this file except in compliance with
    + * the License.  You may obtain a copy of the License at
    + *
    + *    http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.spark.util
    +
    +import java.io._
    +import java.text.SimpleDateFormat
    +import java.util.Date
    +
    +import org.apache.spark.Logging
    +
    +/**
    + * A generic class for logging information to file
    + * @param logDir Path to the directory in which files are logged
    + * @param name An identifier of each FileLogger instance
    + */
    +class FileLogger(
    +    logDir: String = Option(System.getenv(""SPARK_LOG_DIR""))
    +      .getOrElse(""/tmp/spark-%s"".format(System.getProperty(""user.name"", ""user""))),
    +    name: String = String.valueOf(System.currentTimeMillis()))
    +  extends Logging {
    +
    +  private val logPath = logDir.stripSuffix(""/"") + ""/"" + name
    +  private val DATE_FORMAT = new SimpleDateFormat(""yyyy/MM/dd HH:mm:ss"")
    +  private var fileIndex = 0
    +
    +  private var writer: Option[PrintWriter] = {
    +    createLogDir()
    +    Some(createWriter())
    +  }
    +
    +  /** Create a logging directory with the given path */
    +  private def createLogDir() = {
    +    val dir = new File(logPath)
    +    if (dir.exists) {
    +      logWarning(""Logging directory already exists: "" + logDir)
    +    }
    +    if (!dir.exists && !dir.mkdirs()) {
    +      // Logger should throw a exception rather than continue to construct this object
    +      throw new IOException(""Error in creating log directory:"" + logDir)
    +    }
    +  }
    +
    +  /** Create a new writer to the file identified with the given path */
    +  private def createWriter() = {
    +    // Overwrite any existing file
    +    val fileWriter = new FileWriter(logPath + ""/"" + fileIndex)
    --- End diff --
    
    For this we should use the Hadoop FileSystem API rather than a java file writer. 


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Sat,  1 Mar 2014 00:01:33 +0000 (UTC)",[GitHub] spark pull request: [WIP] [SPARK-1132] Persisting Web UI through r...,dev@spark.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/spark/pull/42#discussion_r10186902
  
    --- Diff: core/src/main/scala/org/apache/spark/util/FileLogger.scala ---
    @@ -0,0 +1,103 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one or more
    + * contributor license agreements.  See the NOTICE file distributed with
    + * this work for additional information regarding copyright ownership.
    + * The ASF licenses this file to You under the Apache License, Version 2.0
    + * (the ""License""); you may not use this file except in compliance with
    + * the License.  You may obtain a copy of the License at
    + *
    + *    http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.spark.util
    +
    +import java.io._
    +import java.text.SimpleDateFormat
    +import java.util.Date
    +
    +import org.apache.spark.Logging
    +
    +/**
    + * A generic class for logging information to file
    + * @param logDir Path to the directory in which files are logged
    + * @param name An identifier of each FileLogger instance
    + */
    +class FileLogger(
    +    logDir: String = Option(System.getenv(""SPARK_LOG_DIR""))
    --- End diff --
    
    I think we want this to be a sparkConf value called `spark.eventLog.dir` or something similar.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
pwendell <git@git.apache.org>,"Sat,  1 Mar 2014 00:02:58 +0000 (UTC)",[GitHub] spark pull request: [WIP] [SPARK-1132] Persisting Web UI through r...,dev@spark.apache.org,"Github user pwendell commented on a diff in the pull request:

    https://github.com/apache/spark/pull/42#discussion_r10186927
  
    --- Diff: core/src/main/scala/org/apache/spark/ui/UISparkListener.scala ---
    @@ -0,0 +1,210 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one or more
    + * contributor license agreements.  See the NOTICE file distributed with
    + * this work for additional information regarding copyright ownership.
    + * The ASF licenses this file to You under the Apache License, Version 2.0
    + * (the ""License""); you may not use this file except in compliance with
    + * the License.  You may obtain a copy of the License at
    + *
    + *    http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an ""AS IS"" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package org.apache.spark.ui
    +
    +import scala.collection.mutable
    +import scala.collection.mutable.ArrayBuffer
    +
    +import org.json4s.jackson.JsonMethods._
    +
    +import org.apache.spark.scheduler._
    +import org.apache.spark.storage._
    +import org.apache.spark.util.FileLogger
    +import org.apache.spark.util.JsonProtocol
    +
    +private[ui] trait UISparkListener extends SparkListener
    +
    +/**
    + * A SparkListener that serves as an entry point for all events posted to the UI.
    + *
    + * GatewayUISparkListener achieves two functions:
    + *
    + *  (1) If the UI is live, GatewayUISparkListener posts each event to all attached listeners
    + *      then logs it as JSON. This centralizes event logging and avoids having all attached
    + *      listeners log the events on their own. By default, GatewayUISparkListener logs one
    + *      file per job, though this needs not be the case.
    + *
    + *  (2) If the UI is rendered from disk, GatewayUISparkListener replays each event deserialized
    + *      from the event logs to all attached listeners.
    + */
    +private[ui] class GatewayUISparkListener(parent: SparkUI, live: Boolean) extends SparkListener {
    +
    +  // Log events only if the UI is live
    +  private val logger: Option[FileLogger] = if (live) Some(new FileLogger()) else None
    --- End diff --
    
    We probably want to make the use of logging configurable. For instance by introducing a configuration value called `spark.eventLog.enabled` or something similar and have the default be ""false"". If it's not enabled then even for live code we won't turn it on. 


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---

"
