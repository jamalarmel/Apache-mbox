Andrew Or <andrew@databricks.com>,"Mon, 31 Mar 2014 17:21:34 -0700",Re: [VOTE] Release Apache Spark 0.9.1 (RC3),dev@spark.apache.org,"+1 tested on OSX



"
Tom Graves <tgraves_cs@yahoo.com>,"Tue, 1 Apr 2014 07:28:28 -0700 (PDT)",Re: [VOTE] Release Apache Spark 0.9.1 (RC3),"Tathagata Das <tathagata.das1565@gmail.com>,
  ""dev@spark.apache.org"" <dev@spark.apache.org>","Thanks for extending the voting.

Unfortunately I've found an issue with the spark-shell in yarn-client mode.  It doesn't work with secure HDFS unless you 
export SPARK_YARN_MODE=true before starting the shell, or if you happen to do something immediately with HDFS.  If you wait for the connection to the namenode to timeout it will fail. 

I think it was actually this way in the 0.9 release also so I thought I would send this and get peoples feedback to see if you want it fixed? 

Another option would be to document that you have to export SPARK_YARN_MODE=true for the shell.   The fix actually went in with the authentication changes I made in master but I never realized that change needed to apply to 0.9. 

https://github.com/apache/spark/commit/7edbea41b43e0dc11a2de156be220db8b7952d01#diff-0ae5b834ce90ec37c19af35aa7a5e1a0

See the SparkILoop diff.ays from now. So the vote is open till Wednesday, April 02, at 20:00 UTCI should probably pull this off into another thread, but going forward can
>> we try to not have the release votes end on a weekend? Since we only seem
>> to give 3 days, it makes it really hard for anyone who is offline for the
>> weekend to try it out.   Either that or extend the voting fnloaded and did some local testing. Looks good to me!
>>
>> +1
>>
> >
>> > > > Please vote on releasing the following candidate as Apache Spark
>> > version
>> > > 0.9.1
>> > > >
>> > > > A draft of the release notes along with the CHANGES.txt file is
>> > > > attached to this e-mail.
>> > > >
>> > > > The tag to be voted on is v0.9.1-rc3 (commit 4c43182b):
>> > > >
>> > >
>> >
>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=4c43182b6d1b0b7717423f386c0214fe93073208
>> > > >
>> > > > The release files, including signatures, digests, etc. can be found
>> at:
>> > > > http://people.apache.org/~tdas/spark-0.9.1-rc3/
>> > > >
>> > > > Release artifacts are signed with the following key:
>> > > > https://people.apache.org/keys/committer/tdas.asc
>> 
>> > > >
>> > https://repository.apache.org/content/repositories/orgapachespark-1009/
>> > > >
>> > > > The documentation corresponding to this release can be found at:
>> > > > http://people.apache.org/~tdas/spark-0.9.1-rc3-docs/
>> > > >
>> > > > Please vote on releasing this package as Apache Spark 0.9.1!
>> > > >
>> > > > The vote is open until Sunday, March 30, at 10:00 UTC and passes if
>> > > > a majority of at least 3 +1 PMC votes are cast.
>> > > >
>> > > > [ ] +1 Release this package as Apache Spark 0.9.1
>> > > > [ ] -1 Do not release this package because ...
>> > > >
>> > > > To learn more about Apache Spark, please see
>> > > > http://spark.apache.org/
>> > > > <CHANGES.txt><RELEASE_NOTES.txt>
>> > >
>> > >
>> >
>>
>"
Evan Chan <ev@ooyala.com>,"Tue, 1 Apr 2014 11:24:23 -0700",sbt-package-bin,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey folks,

We are in the middle of creating a Chef recipe for Spark.   As part of that
we want to create a Debian package for Spark.

What do folks think of adding the sbt-package-bin plugin to allow easy
creation of a Spark .deb file?  I believe it adds all dependency jars into
a single lib/ folder, so in some ways it's even easier to manage than the
assembly.

Also I'm not sure if there's an equivalent plugin for Maven.

thanks,
Evan


-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

<http://www.ooyala.com/>
<http://www.facebook.com/ooyala><http://www.linkedin.com/company/ooyala><http://www.twitter.com/ooyala>
"
Evan Chan <ev@ooyala.com>,"Tue, 1 Apr 2014 11:24:55 -0700",Re: sbt-package-bin,"""dev@spark.apache.org"" <dev@spark.apache.org>","Also, I understand this is the last week / merge window for 1.0, so if
folks are interested I'd like to get in a PR quickly.

thanks,
Evan






-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

<http://www.ooyala.com/>
<http://www.facebook.com/ooyala><http://www.linkedin.com/company/ooyala><http://www.twitter.com/ooyala>
"
Mark Hamstra <mark@clearstorydata.com>,"Tue, 1 Apr 2014 11:31:07 -0700",Re: sbt-package-bin,dev@spark.apache.org,"A basic Debian package can already be created from the Maven build: mvn
-Pdeb ...



"
Patrick Wendell <pwendell@gmail.com>,"Tue, 1 Apr 2014 11:36:42 -0700",Re: sbt-package-bin,"""dev@spark.apache.org"" <dev@spark.apache.org>","Ya there is already some fragmentation here. Maven has some ""dist"" targets
and there is also ./make-distribution.sh.



"
Patrick Wendell <pwendell@gmail.com>,"Tue, 1 Apr 2014 11:36:59 -0700",Re: sbt-package-bin,"""dev@spark.apache.org"" <dev@spark.apache.org>","And there is a deb target as well - ah didn't see Mark's email.



"
Patrick Wendell <pwendell@gmail.com>,"Tue, 1 Apr 2014 11:39:37 -0700",Re: [VOTE] Release Apache Spark 0.9.1 (RC3),"""dev@spark.apache.org"" <dev@spark.apache.org>, Tom Graves <tgraves_cs@yahoo.com>","Tom,

Given this is a pretty straightforward workaround, what do yo think about
the following course of action:

(a) We can put the workaround in the docs for 0.9.1. We don't need to do a
new RC/vote for this since we can update the published docs independently.

(b) We try to get a fix in for this into the 0.9 branch so it can end up in
0.9.2. But this takes the fix off the critical path for this release.

- Patrick



"
Tom Graves <tgraves_cs@yahoo.com>,"Tue, 1 Apr 2014 11:44:00 -0700 (PDT)",Re: [VOTE] Release Apache Spark 0.9.1 (RC3),"""dev@spark.apache.org"" <dev@spark.apache.org>","No one else has reported seeing the issue so I think documenting it is fineorward workaround, what do yo think about
the following course of action:

(a) We can put the workaround in the docs for 0.9.1. We don't need to do a
new RC/vote for this since we can update the published docs independently.

(b) We try to get a fix in for this into the 0.9 branch so it can end up in
0.9.2. But this takes the fix off the critical path for thi
>
> Unfortunately I've found an issue with the spark-shell in yarn-client
> mode.  It doesn't work with secure HDFS unless you
> export SPARK_YARN_MODE=true before starting the shell, or if you happen to
> do something immediately with HDFS.  If you wait for the connection to the
y in the 0.9 release also so I thought I
> would send this and get peoples feedback to see if you want it fixed?
>
> Another option would be to document that you have to export
> SPARK_YARN_MODE=true for the shell.   The fix actually went in with the
> authentication changes I made in master but I never realized that change
> needed to apply to 0.9.
>
b8b7952d01#diff-0ae5b834ce90ec37c19af35aa7a5e1a0
>
> See the SparkILoop vote for two more days from now. So the vote is open
> till Wednesday, All this off into another thread, but going forward
> can
> >> we try to not have the release votes end on a weekend? Since we only
> seem
> >> to give 3 days, it makes it really hard for anyone who is offline for
> the
> >> weekend to try it out.   Either that or extend the voting for 
> >>
> >> +1
> >>
> >> You should cast your own vote - at that point it's enough to pass.
> >>
> >> - Patrick
> >>
> >>
> >>
> >>te:
> >>
> >> > +1
> >> > tested on Ubuntu12.04 64bit
> >> >
> >>
> >> > >
> >> > > > Please vote on releasing the following candidate as Apache Spark
> >> > version
> >> > > 0.9.1
> >> > > >
> >> > > > A draft of the release notes along with the CHANGES.txt file is
> >> > >  is v0.9.1-rc3 (commit 4c43182b):
> >> > > >
> >> > >
> >> >
> >>
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=4c43182b6d1b0b7717423f386c0214fe93073208
> >> > > >
> >> > > > The release files, including signatures, digests, etc. can be
> found
> >> at:
> >> > > > http://people.apache.org/~tdas/spark-0.9.1-rc3/
> >> > > >
> >> > > > Release artifacts are signed with the following key:
> >> > > > https://people.apache.org/keys/committer/tdas.asc
> >> > > >
> >> 
> >> >
> https://repository.apache.org/content/repositories/orgapachespark-1009/
> >> > > >
> >> > > > The documentation corresponding to this release can be found at:
> >> > > > http://people.apache.org/~tdas/spark-0.9.1-rc3-docs/
> >> > > >
> >> > > > Please vote on releasing this package as Apache Spark 0.9.1!
> >> > > >
> >> > > > The vote is open until Sunday, March 30, at 10:00 UTC and passes
> if
> >> > > > a majority of at least 3 +1 PMC votes are cast.
> >> > > >
> >> > > > [ ] +1 Release this package as Apache Spark 0.9.1
> >> > > > [ ] -1 Do not release this package because ...
> >> > > >
> >> > > > To learn more about Apache Spark, please see
> >> > > > http://spark.apache.org/
> >> > > > <CHANGES.txt><RELEASE_NOTES.txt>
> >> > >
> >> > >
> >> >
> >>
> >
>"
Evan Chan <ev@ooyala.com>,"Tue, 1 Apr 2014 15:01:32 -0700",Re: sbt-package-bin,"""dev@spark.apache.org"" <dev@spark.apache.org>","Mark - sorry, would you mind expanding what the ""...."" is?

Something like

mvn -Pdeb package

?

I get:

[ERROR] Plugin org.apache.maven.plugins:maven-compiler-plugin:3.1 or one of
its dependencies could not be resolved: Failed to read artifact descriptor
for org.apache.maven.plugins:maven-compiler-plugin:jar:3.1: Could not find
artifact org.apache:apache:pom:13 -> [Help 1]






-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

<http://www.ooyala.com/>
<http://www.facebook.com/ooyala><http://www.linkedin.com/company/ooyala><http://www.twitter.com/ooyala>
"
Mark Hamstra <mark@clearstorydata.com>,"Tue, 1 Apr 2014 18:01:04 -0700",Re: sbt-package-bin,dev@spark.apache.org,"What the ""..."" is kind of depends on what you're trying to accomplish.  You
could be setting Hadoop version and other stuff in there, but if you go too
much beyond a pretty basic build, you're probably also going to have to
modify the <configuration> of the jdeb plugin in assembly/pom.xml to
include/exclude just what you want/don't want in the Debian package.

Anyway, a typical build would look something like 'mvn -U -Pdeb -DskipTests
clean package', after which you should be able to find your .deb in
assembly/target.



"
Mark Hamstra <mark@clearstorydata.com>,"Tue, 1 Apr 2014 18:03:20 -0700",Re: sbt-package-bin,dev@spark.apache.org,"...or at least you could do that if the Maven build wasn't broken right now.



"
Mark Hamstra <mark@clearstorydata.com>,"Tue, 1 Apr 2014 18:08:45 -0700",Re: sbt-package-bin,dev@spark.apache.org,"Whoops!  Looks like it was just my brain that was broken.



"
Lee Mighdoll <lee@underneath.ca>,"Tue, 1 Apr 2014 20:14:34 -0700",Re: sbt-package-bin,dev@spark.apache.org,"
You might also check out the
sbt-native-packager<https://github.com/sbt/sbt-native-packager>.


Cheers,
Lee
"
Tom Graves <tgraves_cs@yahoo.com>,"Wed, 2 Apr 2014 06:46:28 -0700 (PDT)",Re: [VOTE] Release Apache Spark 0.9.1 (RC3),"""dev@spark.apache.org"" <dev@spark.apache.org>,
  Tom Graves <tgraves_cs@yahoo.com>","Note I'm +1 with the doc changed to tell users to export SPARK_YARN_MODE=true before using spark-shell on yarn.

I tested it on both hadoop 0lse has reported seeing the issue so I think documenting it is fine.  
 workaround, what do yo think about
the following course of action:

(a) We can put the workaround in the docs for 0.9.1. We don't need to do a
new RC/vote for this since we can update the published docs independently.

(b) We try to get a fix in for this into the 0.9 branch so it can end up in
0.9.2. But this takes the fix off the critical path for this rel
> Unfortunately I've found an issue with the spark-shell in yarn-client
> mode.  It doesn't work with secure HDFS unless you
> export SPARK_YARN_MODE=true before starting the shell, or if you happen to
> do something immediately with HDFS.  If you wait for the connection to the
> namenode to timeout it will fail.
>
> I think it was actually this way in the 0.9 release also so I thought I
> would send this and get peoples feedback to see if you want it fixed?
>
> Another option would be to document that you have to export
> SPARK_YARN_MODE=true for the shell.   The fix actually went in with the
> authentication changes I made in master but I never realized that change
> needed to apply to 0.9.
>
>
> https://github.com/apache/spark/commit/7edbea41b43e0dc11a2de156be220db8b7952d01#diff-0ae5b834ce90ec37c19af35aa7a5e1a0
>
> See the SparkILoop vote for two more days from now. So the vote is open
> till Wednesday, Apl this off into another thread, but going forward
> can
> >> we try to not have the release votes end on a weekend? Since we only
> seem
> >> to give 3 days, it makes it really hard for anyone who is offline for
> the
> >> weekend to try it out.   Either that or extend the voting for >>
> >> TD - I downloaded and did some local testing. Looks good to me!
> >>
> >> +1
> >>
> >> You should cast your own vote - at that point it's enough to pass.
> >>
> >> - Patrick
> >>
> >>
> >>
> >>te:
> >>
> >> > +1
> >> > tested on Ubuntu12.04 64bit
> >> >
> >>
> >> > >
> >> > > > Please vote on releasing the following candidate as Apache Spark
> >> > version
> >> > > 0.9.1
> >> > > >
> >> > > > A draft of the release notes along with the CHANGES.txt file is
> >> > >  is v0.9.1-rc3 (commit 4c43182b):
> >> > > >
> >> > >
> >> >
> >>
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=4c43182b6d1b0b7717423f386c0214fe93073208
> >> > > >
> >> > > > The release files, including signatures, digests, etc. can be
> found
> >> at:
> >> > > > http://people.apache.org/~tdas/spark-0.9.1-rc3/
> >> > > >
> >> > > > Release artifacts are signed with the following key:
> >> > > > https://people.apache.org/keys/committer/tdas.asc
> >> > > >
> >> 
> >> >
> https://repository.apache.org/content/repositories/orgapachespark-1009/
> >> > > >
> >> > > > The documentation corresponding to this release can be found at:
> >> > > > http://people.apache.org/~tdas/spark-0.9.1-rc3-docs/
> >> > > >
> >> > > > Please vote on releasing this package as Apache Spark 0.9.1!
> >> > > >
> >> > > > The vote is open until Sunday, March 30, at 10:00 UTC and passes
> if
> >> > > > a majority of at least 3 +1 PMC votes are cast.
> >> > > >
> >> > > > [ ] +1 Release this package as Apache Spark 0.9.1
> >> > > > [ ] -1 Do not release this package because ...
> >> > > >
> >> > > > To learn more about Apache Spark, please see
> >> > > > http://spark.apache.org/
> >> > > > <CHANGES.txt><RELEASE_NOTES.txt>
> >> > >
> >> > >
> >> >
> >>
> >
>"
Evan Chan <ev@ooyala.com>,"Wed, 2 Apr 2014 10:22:24 -0700",Re: sbt-package-bin,"""dev@spark.apache.org"" <dev@spark.apache.org>, lee@underneath.ca","Lee, sorry, I actually meant exactly that, sbt-native-packager.






-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

<http://www.ooyala.com/>
<http://www.facebook.com/ooyala><http://www.linkedin.com/company/ooyala><http://www.twitter.com/ooyala>
"
Evan Chan <ev@ooyala.com>,"Wed, 2 Apr 2014 10:25:45 -0700",Would anyone mind having a quick look at PR#288?,"""dev@spark.apache.org"" <dev@spark.apache.org>","https://github.com/apache/spark/pull/288

It's for fixing SPARK-1154, which would help Spark be a better citizen for
most deploys, and should be really small and easy to review.

thanks,
Evan


-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

<http://www.ooyala.com/>
<http://www.facebook.com/ooyala><http://www.linkedin.com/company/ooyala><http://www.twitter.com/ooyala>
"
Patrick Wendell <pwendell@gmail.com>,"Wed, 2 Apr 2014 12:20:29 -0700",Re: Would anyone mind having a quick look at PR#288?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey Evan,

Ya thanks this is a pretty small patch. Should definitely be do-able for
1.0.

- Patrick



"
Dan <zsh912006@gmail.com>,"Thu, 3 Apr 2014 06:56:17 -0700 (PDT)",Re: The difference between driver and master in Spark,dev@spark.incubator.apache.org,"
Can I think driver as client? Driver and master can be located in a single
machine or different machines, right? 

Thanks,
Dan 



--

"
"""Xia, Junluan"" <junluan.xia@intel.com>","Thu, 3 Apr 2014 14:00:15 +0000",RE: The difference between driver and master in Spark,"""dev@spark.apache.org"" <dev@spark.apache.org>,
	""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Yes, driver is a client. Either single or different machine will be OK for driver and master

Can I think driver as client? Driver and master can be located in a single machine or different machines, right? 

Thanks,
Dan 



--
3.nabble.com/The-difference-between-driver-and-master-in-Spark-tp6158p6192.html
om.

"
"""Xia, Junluan"" <junluan.xia@intel.com>","Thu, 3 Apr 2014 14:00:15 +0000",RE: The difference between driver and master in Spark,"""dev@spark.apache.org"" <dev@spark.apache.org>,
	""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Yes, driver is a client. Either single or different machine will be OK for driver and master

Can I think driver as client? Driver and master can be located in a single machine or different machines, right? 

Thanks,
Dan 



--
3.nabble.com/The-difference-between-driver-and-master-in-Spark-tp6158p6192.html
om.

"
Tom Graves <tgraves_cs@yahoo.com>,"Thu, 3 Apr 2014 12:35:39 -0700 (PDT)",Re: [VOTE] Release Apache Spark 0.9.1 (RC3),"""dev@spark.apache.org"" <dev@spark.apache.org>,
  Tom Graves <tgraves_cs@yahoo.com>","I put up a pull request with documentation changes https://github.com/apaed to tell users to export SPARK_YARN_MODE=true before using spark-shell on yarn.

I tested it on both hadoop 0.23 and 2.3 clusters using secum,

Given this is a pretty straightforward workaround, what do yo think about
the following course of action:

(a) We can put the workaround in the docs for 0.9.1. We don't need to do a
new RC/vote for this since we can update the published docs independently.

(b) We try to get a fix in for this into the 0.9 branch so it can end up in
0.9.2. But this takes the fix off the critical path for this release.

- Patrick




> Thanks for extending the voting.
>
> Unfortunately I've found an issue with the spark-shell in yarn-client
> mode.  It doesn't work with secure HDFS unless you
> export SPARK_YARN_MODE=true before starting the shell, or if you happen to
> do something immediately with HDFS.  If you wait for the connection to the
> namenode to timeout it will fail.
>
> I think it was actually this way in the 0.9 release also so I thought I
> would send this and get peoples feedback to see if you want it fixed?
>
> Another option would be to document that you have to export
 the
> authentication changes I made in master but I never realized that change
> needed to apply to 0.9.
>
>
> https://github.com/apache/spark/commit/7edbea41b43e0dc11a2de156be220db8b7952d01#diff-0ae5b834ce90ec37c1Monday, March 31, 2014 1:33 PM, Tathagata Das <
> tathagata.das1565@gmailah good point. Let's just extend this vote another few days?
> >
> >
ut going forward
> can
> >> we try to not have the release votes end on a weekend? Since we only
> seem
> >> to give 3 days, it makes it really hard for anyone who is offline for
> the
> >> weekend to try it out.   Either that or extend the voting for more then
> 3
> >> days.
 did some local testing. Looks good to me!
> >>
> >> +1
> >>
> >> You should cast your own vote - at that point it's enough to pass.
> >>
 on releasing the following candidate as Apache Spark
> >> > version
> ith the CHANGES.txt file is
> >> > > > attached to this e-mail.
> >> > os/asf?p=spark.git;a=commit;h=4c43182b6d1b0b7717423f386c0214fe93073208
> >> > > >
> >> > > > The release files, including signatures, digests, etc. can be
> found
> >> at:
> >> > > > http://people.apache.org/~tdas/spark-0.9.1-rc3/
> >> > > >
> >> > > > Release artifacts are signed with the following key:
> >> > > > https://people.apache.org/keys/committer/tdas.asc
> >> > > >
> >> > > > The staging repository for this release can be found at:
> >> > > >
> >> >
> https://repository.apache.org/content/repositories/orgapachespark-1009/
> >> > > >
> >> > > > The documentation corresponding to this release can be found at:
> >> > > > http://people.apache.org/~tdas/spark-0.9.1-rc3-docs/
> >> > > >
> >> > > > Please vote on releasing this package as Apache Spark 0.9.1!
> >> > > sses
> if
> >> > > > a majority of at least 3 +1 PMC votes are cast.

> >> > > > [ ] -1 Do not release this package because ...
> >> > > >
> >> > > > To learn more about Apache Spark, please see
> >> > > > http://spark.apache.org/
> >> > > > <CHANGES.txt><RELEASE_NOTES.txt>
> >> "
Egor Pahomov <pahomov.egor@gmail.com>,"Fri, 4 Apr 2014 15:24:49 +0400",Re: Ping on SPARK-1177,dev@spark.apache.org,"Hi again. Can anyone help me create way to run spark from application by
reviewing it?


2014-03-16 13:10 GMT+04:00 Egor Pahomov <pahomov.egor@gmail.com>:




-- 



*Sincerely yoursEgor PakhomovScala Developer, Yandex*
"
Debasish Das <debasish.das83@gmail.com>,"Fri, 4 Apr 2014 18:13:45 -0700",Recent heartbeats,dev@spark.apache.org,"Hi,

Also posted it on user but then I realized it might be more involved.

In my ALS runs I am noticing messages that complain about heart beats:

14/04/04 20:43:09 WARN BlockManagerMasterActor: Removing BlockManager
BlockManagerId(17, machine1, 53419, 0) with no recent heart beats: 48476ms
exceeds 45000ms
14/04/04 20:43:09 WARN BlockManagerMasterActor: Removing BlockManager
BlockManagerId(12, machine2, 60714, 0) with no recent heart beats: 45328ms
exceeds 45000ms
14/04/04 20:43:09 WARN BlockManagerMasterActor: Removing BlockManager
BlockManagerId(19, machine3, 39496, 0) with no recent heart beats: 53259ms
exceeds 45000ms

Is this some issue with the underlying jvm over which akka is run ? Can I
increase the heartbeat somehow to get these messages resolved ?

Any more insight about the possible cause for the heartbeat will be
helpful...

Thanks.
Deb
"
Patrick Wendell <pwendell@gmail.com>,"Fri, 4 Apr 2014 22:35:48 -0700",Re: Recent heartbeats,"""dev@spark.apache.org"" <dev@spark.apache.org>","I answered this over on the user list...



"
Debasish Das <debasish.das83@gmail.com>,"Sat, 5 Apr 2014 01:17:12 -0700",Re: Recent heartbeats,dev@spark.apache.org,"Thanks Patrick...I searched in the archives and found the answer...tuning
the akka and gc params....



"
Debasish Das <debasish.das83@gmail.com>,"Sat, 5 Apr 2014 14:04:22 -0700",Master compilation,dev@spark.apache.org,"I am synced with apache/spark master but getting error in spark/sql
compilation...

Is the master broken ?

[info] Compiling 34 Scala sources to
/home/debasish/spark_deploy/sql/core/target/scala-2.10/classes...
[error]
/home/debasish/spark_deploy/sql/core/src/main/scala/org/apache/spark/sql/parquet/ParquetRelation.scala:106:
value getGlobal is not a member of object java.util.logging.Logger
[error]       logger.setParent(Logger.getGlobal)
[error]                               ^
[error] one error found
[error] (sql/compile:compile) Compilation failed
[error] Total time: 171 s, completed Apr 5, 2014 4:58:41 PM

Thanks.
Deb
"
Sean Owen <sowen@cloudera.com>,"Sat, 5 Apr 2014 22:19:35 +0100",Re: Master compilation,dev@spark.apache.org,"That method was added in Java 7. The project is on Java 6, so I think
this was just an inadvertent error in a recent PR (it was the 'Spark
parquet improvements' one).

I'll open a hot-fix PR after looking for other stuff like this that
might have snuck in.
--
Sean Owen | Director, Data Science | London



"
Debasish Das <debasish.das83@gmail.com>,"Sat, 5 Apr 2014 14:22:46 -0700",Re: Master compilation,dev@spark.apache.org,"I can compile with Java 7...let me try that...



"
Debasish Das <debasish.das83@gmail.com>,"Sat, 5 Apr 2014 14:30:23 -0700",Re: Master compilation,dev@spark.apache.org,"I verified this is happening for both CDH4.5 and 1.0.4...My deploy
environment is Java 6...so Java 7 compilation is not going to help...

Is this the PR which caused it ?

Andre Schumacher

    fbebaed    Spark parquet improvements A few improvements to the Parquet
support for SQL queries: - Instead of files a ParquetRelation is now backed
by a directory, which simplifies importing data from other sources -
InsertIntoParquetTable operation now supports switching between overwriting
or appending (at least in HiveQL) - tests now use the new API - Parquet
logging can be set to WARNING level (Default) - Default compression for
Parquet files (GZIP, as in parquet-mr) Author: Andre Schumacher &...    2
days ago    SPARK-1383

I will go to a stable checkin before this





"
Patrick Wendell <pwendell@gmail.com>,"Sat, 5 Apr 2014 15:06:31 -0700",Re: Master compilation,"""dev@spark.apache.org"" <dev@spark.apache.org>","If you want to submit a hot fix for this issue specifically please do. I'm
not sure why it didn't fail our build...



"
Debasish Das <debasish.das83@gmail.com>,"Sat, 5 Apr 2014 15:09:53 -0700",Re: Master compilation,dev@spark.apache.org,"@patrick our cluster still has java6 deployed...and I compiled using jdk6...

Sean is looking into it...this api is in java7 but not java6...




"
Sean Owen <sowen@cloudera.com>,"Sat, 5 Apr 2014 23:09:38 +0100",Re: Master compilation,dev@spark.apache.org,"Will do. I'm just finishing a recompile to check for anything else like this.

The reason is because the tests run with Java 7 (like lots of us do
including me) so it used the Java 7 classpath and found the class.
It's possible to use Java 7 with the Java 6 -bootclasspath. Or just
use Java 6.
--
Sean Owen | Director, Data Science | London



"
Mridul Muralidharan <mridul@gmail.com>,"Sun, 6 Apr 2014 05:00:02 +0530",ephemeral storage level in spark ?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

  We have a requirement to use a (potential) ephemeral storage, which
is not within the VM, which is strongly tied to a worker node. So
source of truth for a block would still be within spark; but to
actually do computation, we would need to copy data to external device
(where it might lie around for a while : so data locality really
really helps if we can avoid a subsequent copy if it is already
present on computations on same block again).

I was wondering if the recently added storage level for tachyon would
help in this case (note, tachyon wont help; just the storage level
might).
What sort of guarantees does it provide ? How extensible is it ? Or is
it strongly tied to tachyon with only a generic name ?


Thanks,
Mridul

"
Haoyuan Li <haoyuan.li@gmail.com>,"Sat, 5 Apr 2014 16:48:52 -0700",Re: ephemeral storage level in spark ?,dev@spark.apache.org,"Hi Mridul,

Do you mean the scenario that different Spark applications need to read the
same raw data, which is stored in a remote cluster or machines. And the
goal is to load the remote raw data only once?

Haoyuan






-- 
Haoyuan Li
Algorithms, Machines, People Lab, EECS, UC Berkeley
http://www.cs.berkeley.edu/~haoyuan/
"
Mridul Muralidharan <mridul@gmail.com>,"Sun, 6 Apr 2014 05:44:49 +0530",Re: ephemeral storage level in spark ?,dev@spark.apache.org,"No, I am thinking along lines of writing to an accelerator card or
dedicated card with its own memory.

Regards,
Mridul

"
Debasish Das <debasish.das83@gmail.com>,"Sat, 5 Apr 2014 18:10:31 -0700",Re: Master compilation,dev@spark.apache.org,"With jdk7 I could compile it fine:

java version ""1.7.0_51""
Java(TM) SE Runtime Environment (build 1.7.0_51-b13)
Java HotSpot(TM) 64-Bit Server VM (build 24.51-b03, mixed mode)

What happens if I say take the jar and try to deploy it on ancient centos6
default on cluster ?

java -version
java version ""1.6.0_31""
Java(TM) SE Runtime Environment (build 1.6.0_31-b04)
Java HotSpot(TM) 64-Bit Server VM (build 20.6-b01, mixed mode)

Breeze compilation also fails with jdk6, runs fine with jdk7 and breeze jar
is already included in spark mllib with Xiangrui's Sparse vector checkin....

Does that mean that classes compiled and generated using jdk7 will run fine
on jre6 ?

I am confused



"
Debasish Das <debasish.das83@gmail.com>,"Sat, 5 Apr 2014 22:53:36 -0700",ALS array index out of bound with 50 factors,dev@spark.apache.org,"Hi,

I deployed apache/spark master today and recently there were many ALS
related checkins and enhancements..

I am running ALS with explicit feedback and I remember most enhancements
were related to implicit feedback...

With 25 factors my runs were successful but with 50 factors I am getting
array index out of bound...

Note that I was hitting gc errors before with an older version of spark but
it seems like the sparse matrix partitioning scheme has changed now...data
caching looks much balanced now...earlier one node was becoming
bottleneck...Although I ran with 64g memory per node...

There are around 3M products, 25M users...

Anyone noticed this bug or something similar ?

14/04/05 23:03:15 WARN TaskSetManager: Loss was due to
java.lang.ArrayIndexOutOfBoundsException
java.lang.ArrayIndexOutOfBoundsException: 81029
    at
org.apache.spark.mllib.recommendation.ALS$$anonfun$org$apache$spark$mllib$recommendation$ALS$$updateBlock$1$$anonfun$apply$mcVI$sp$1.apply$mcVI$sp(ALS.scala:450)
    at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
    at
org.apache.spark.mllib.recommendation.ALS$$anonfun$org$apache$spark$mllib$recommendation$ALS$$updateBlock$1.apply$mcVI$sp(ALS.scala:446)
    at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
    at org.apache.spark.mllib.recommendation.ALS.org
$apache$spark$mllib$recommendation$ALS$$updateBlock(ALS.scala:445)
    at
org.apache.spark.mllib.recommendation.ALS$$anonfun$org$apache$spark$mllib$recommendation$ALS$$updateFeatures$2.apply(ALS.scala:416)
    at
org.apache.spark.mllib.recommendation.ALS$$anonfun$org$apache$spark$mllib$recommendation$ALS$$updateFeatures$2.apply(ALS.scala:415)
    at
org.apache.spark.rdd.MappedValuesRDD$$anonfun$compute$1.apply(MappedValuesRDD.scala:31)
    at
org.apache.spark.rdd.MappedValuesRDD$$anonfun$compute$1.apply(MappedValuesRDD.scala:31)
    at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
    at
org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$4.apply(CoGroupedRDD.scala:149)
    at
org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$4.apply(CoGroupedRDD.scala:147)
    at
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
    at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:147)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:220)
    at
org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:220)
    at
org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:220)
    at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:220)
    at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:161)
    at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)
    at org.apache.spark.scheduler.Task.run(Task.scala:52)
    at
org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:211)
    at
org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:43)
    at
org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:396)
    at
org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408)
    at
org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:42)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)
    at
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)

Thanks.
Deb
"
Koert Kuipers <koert@tresata.com>,"Sun, 6 Apr 2014 11:41:47 -0400",Re: Master compilation,dev@spark.apache.org,"classes compiled with java7 run fine on java6 if you specified ""-target
1.6"". however if thats the case generally you should also be able to also
then compile it with java 6 just fine.

something compiled with java7 with ""-target 1.7"" will not run on java 6




"
Debasish Das <debasish.das83@gmail.com>,"Sun, 6 Apr 2014 09:00:08 -0700",Re: Master compilation,dev@spark.apache.org,"Hi Koert,

How do I specify that in sbt ?

Is this the correct way ?
  javacOptions ++= Seq(""-target"", ""1.6"", ""-source"",""1.6"")

Breeze project for examples compiles fine with jdk7, fails with jdk6 and
the function it fails on:

error] /home/debasish/github/breeze/
src/main/scala/breeze/util/package.scala:200: value valueOf is not a member
of object java.util.BitSet
[error]       java.util.BitSet.valueOf(bs.toBitMask)

is not available in jdk6...

http://docs.oracle.com/javase/6/docs/api/java/util/BitSet.html

I have no clue how with target 1.6 solves the issue...are you saying jdk7
will put a function that's closest to java.util.BitSet.valueOf ?

Thanks.
Deb




"
Koert Kuipers <koert@tresata.com>,"Sun, 6 Apr 2014 12:03:26 -0400",Re: Master compilation,dev@spark.apache.org,"thats confusing. it seems to me the breeze dependency has been compiled
with java 6, since the mllib tests passed fine for me with java 6



"
Sean Owen <sowen@cloudera.com>,"Sun, 6 Apr 2014 17:07:05 +0100",Re: Master compilation,dev@spark.apache.org,"That's a Breeze question, no? you should not need to compile Breeze
yourself to compile Spark -- why do that?

That method indeed only exists in Java 7. But Breeze seems to target
Java 6 as expected:

https://github.com/scalanlp/breeze/blob/master/build.sbt#L59

I see this particular line of code was added after the last release:

https://github.com/scalanlp/breeze/commit/ff46ddfa66f98b8c8b0ef5b65a6e7a9f86b5a5c4

So it's possible it's an issue lurking in Breeze of just the same form
we just saw. It's worth opening an issue since, indeed, I would expect
exactly the compile error you see with Java 6.

But it should not stop you from building Spark.



"
Debasish Das <debasish.das83@gmail.com>,"Sun, 6 Apr 2014 09:12:54 -0700",Re: Master compilation,dev@spark.apache.org,"Yeah spark builds are fine...

For solvers we are planning to use breeze optimization since it has most of
the core functions we will need and we can enhance it further (QP solver
for example)

Right now sparse kmeans in spark mllib uses breeze and that might not even
need this line of code....But still I thought Xiangrui should be aware of
this issue...




"
Koert Kuipers <koert@tresata.com>,"Sun, 6 Apr 2014 12:13:56 -0400",Re: Master compilation,dev@spark.apache.org,"patrick,
this has happened before, that a commit introduced java 7 code/dependencies
and your build didnt fail, i think it was when reynold upgraded to jetty 9.
must be that your entire build infrastructure runs java 7...



"
Debasish Das <debasish.das83@gmail.com>,"Sun, 6 Apr 2014 10:21:13 -0700","Re: Any suggestion about JIRA 1006 ""MLlib ALS gets stack overflow
 with too many iterations""?",dev <dev@spark.incubator.apache.org>,"At the head I see persist option in implicitPrefs but more cases like the
ones mentioned above why don't we use similar technique and take an input
that which iteration should we persist in explicit runs as well ?

for (iter <- 1 to iterations) {
        // perform ALS update
        logInfo(""Re-computing I given U (Iteration %d/%d)"".format(iter,
iterations))
        products = updateFeatures(users, userOutLinks, productInLinks,
partitioner, rank, lambda,
          alpha, YtY = None)
        logInfo(""Re-computing U given I (Iteration %d/%d)"".format(iter,
iterations))
        users = updateFeatures(products, productOutLinks, userInLinks,
partitioner, rank, lambda,
          alpha, YtY = None)
      }

Say if I want to persist at every k iterations out of N iterations of ALS
explicit, there shoud be an option to do that...implicit right now uses
persist at each iteration...

Does this option make sense or you guys want this issue to be fixed in a
different way...

I definitely see that for my 25M x 3M run, with 64 gb executor memory,
something is going wrong after 5-th iteration and I wanted to run for 10
iterations...

So my k is 4/5 for this particular problem...

I can ask for the PR after testing the fix on the dataset I have...I will
also try to see if we can make such datasets public for more research...

For the LDA problem mentioned earlier in this email chain, k is 10...NMF
can generate topics similar to LDA as well...Carrot2 project uses it...




"
Koert Kuipers <koert@tresata.com>,"Sun, 6 Apr 2014 13:46:56 -0400",Re: Master compilation,dev@spark.apache.org,"also, i thought scala 2.10 was binary compatible, but does not seem to be
the case. the spark artifacts for scala 2.10.4 dont work for me, since we
are still on scala 2.10.3, but when i recompiled and published spark with
scala 2.10.3 everything was fine again.

errors i see:
java.lang.ClassNotFoundException: scala.None$

fun stuff!



"
Koert Kuipers <koert@tresata.com>,"Sun, 6 Apr 2014 15:21:43 -0400",Re: Master compilation,dev@spark.apache.org,"i suggest we stick to 2.10.3, since otherwise it seems that (surprisingly)
you force everyone to upgrade



"
Sean Owen <sowen@cloudera.com>,"Sun, 6 Apr 2014 21:09:33 +0100",Re: Master compilation,dev@spark.apache.org,"scala.None certainly isn't new in 2.10.4; it's ancient :
http://www.scala-lang.org/api/2.10.3/index.html#scala.None$

Surely this is some other problem?


"
Koert Kuipers <koert@tresata.com>,"Sun, 6 Apr 2014 18:31:47 -0400",Re: Master compilation,dev@spark.apache.org,"see here for similar issue

http://mail-archives.apache.org/mod_mbox/spark-user/201401.mbox/%3CCALNFXi2hBSyCkPpnBJBYJnPv3dSLNw8VpL_6caEn3yfXCykO=w@mail.gmail.com%3E

"
Matei Zaharia <matei.zaharia@gmail.com>,"Sun, 6 Apr 2014 19:59:23 -0700",Re: ephemeral storage level in spark ?,dev@spark.apache.org,"The off-heap storage level is currently tied to Tachyon, but it might support other forms of off-heap storage later. However it’s not really designed to be mixed with the other ones. For this use case you may want to rely on memory locality and have some custom code to push the data to the accelerator. If you can think of a way to extend the storage level concept to handle this that would be general though, do send a proposal.

Matei


read the
the
device
would
is


"
Xiangrui Meng <mengxr@gmail.com>,"Sun, 6 Apr 2014 21:08:00 -0700",Re: ALS array index out of bound with 50 factors,dev@spark.apache.org,"Hi Deb,

Are you using the master branch or a particular commit? Do you have
negative or out-of-integer-range user or product ids? There is an
issue with ALS' partitioning
(https://spark-project.atlassian.net/browse/SPARK-1281), but I'm not
sure whether that is the reason. Could you try to see whether you can
reproduce the error on a public data set, e.g., movielens? Thanks!

Best,
Xiangrui


"
Xiangrui Meng <mengxr@gmail.com>,"Sun, 6 Apr 2014 21:13:51 -0700","Re: Any suggestion about JIRA 1006 ""MLlib ALS gets stack overflow
 with too many iterations""?",dev@spark.apache.org,"The persist used in implicit ALS doesn't help StackOverflow problem.
Persist doesn't cut lineage. We need to call count() and then
checkpoint() to cut the lineage. Did you try the workaround mentioned
in https://issues.apache.org/jira/browse/SPARK-958:

""I tune JVM thread stack size to 512k via option -Xss512k and it works.""

Best,
Xiangrui


"
Xiangrui Meng <mengxr@gmail.com>,"Sun, 6 Apr 2014 21:13:51 -0700","Re: Any suggestion about JIRA 1006 ""MLlib ALS gets stack overflow
 with too many iterations""?",dev@spark.apache.org,"The persist used in implicit ALS doesn't help StackOverflow problem.
Persist doesn't cut lineage. We need to call count() and then
checkpoint() to cut the lineage. Did you try the workaround mentioned
in https://issues.apache.org/jira/browse/SPARK-958:

""I tune JVM thread stack size to 512k via option -Xss512k and it works.""

Best,
Xiangrui


"
Xiangrui Meng <mengxr@gmail.com>,"Sun, 6 Apr 2014 21:15:17 -0700","Re: Any suggestion about JIRA 1006 ""MLlib ALS gets stack overflow
 with too many iterations""?",dev@spark.apache.org,"Btw, explicit ALS doesn't need persist because each intermediate
factor is only used once. -Xiangrui


"
Xiangrui Meng <mengxr@gmail.com>,"Sun, 6 Apr 2014 21:15:17 -0700","Re: Any suggestion about JIRA 1006 ""MLlib ALS gets stack overflow
 with too many iterations""?",dev@spark.apache.org,"Btw, explicit ALS doesn't need persist because each intermediate
factor is only used once. -Xiangrui


"
Debasish Das <debasish.das83@gmail.com>,"Sun, 6 Apr 2014 23:01:51 -0700",Re: ALS array index out of bound with 50 factors,dev@spark.apache.org,"Hi Xiangrui,

With 4 ALS iterations it runs fine...If I run 10 I am failing...I believe I
have to cut the lineage chain and call checkpoint....Trying to follow the
other email chain on checkpointing...

Thanks.
Deb



"
Debasish Das <debasish.das83@gmail.com>,"Sun, 6 Apr 2014 23:09:26 -0700","Re: Any suggestion about JIRA 1006 ""MLlib ALS gets stack overflow
 with too many iterations""?",dev@spark.apache.org,"Sorry not persist...I meant adding a user parameter k which does checkpoint
after every k iterations...out of N ALS iterations...We have hdfs installed
so not a big deal...is there an issue of adding this user parameter in
ALS.scala ? If it is then I can add it to our internal branch...

For me tipping k seems like 4...With 4 iterations I can write out the
factors...if I run with 10 iterations, after 4 I can see that it restarts
the sparse matrix partition...tries to run all the iterations over again
and fails due to array index out of bound which does not seems like a real
bug...

Not sure if it can be reproduced in movielens as the dataset I have is 25M
x 3M (and counting)...whille movielens is tall and thin....

Another idea would be to give an option to restart ALS with previous
factors...that way ALS core algorithm does not need to change and it might
be more useful...and that way we can point to a location from where the old
factors can be load...I think @sean used similar idea in Oryx generations...

Let me know which way you guys prefer....I can add it in...





"
Debasish Das <debasish.das83@gmail.com>,"Sun, 6 Apr 2014 23:09:26 -0700","Re: Any suggestion about JIRA 1006 ""MLlib ALS gets stack overflow
 with too many iterations""?",dev@spark.apache.org,"Sorry not persist...I meant adding a user parameter k which does checkpoint
after every k iterations...out of N ALS iterations...We have hdfs installed
so not a big deal...is there an issue of adding this user parameter in
ALS.scala ? If it is then I can add it to our internal branch...

For me tipping k seems like 4...With 4 iterations I can write out the
factors...if I run with 10 iterations, after 4 I can see that it restarts
the sparse matrix partition...tries to run all the iterations over again
and fails due to array index out of bound which does not seems like a real
bug...

Not sure if it can be reproduced in movielens as the dataset I have is 25M
x 3M (and counting)...whille movielens is tall and thin....

Another idea would be to give an option to restart ALS with previous
factors...that way ALS core algorithm does not need to change and it might
be more useful...and that way we can point to a location from where the old
factors can be load...I think @sean used similar idea in Oryx generations...

Let me know which way you guys prefer....I can add it in...





"
Nick Pentreath <nick.pentreath@gmail.com>,"Mon, 7 Apr 2014 15:57:01 +0200",Re: ALS array index out of bound with 50 factors,dev@spark.apache.org,"feasible will it be to just allow the user and item ids to be strings? A
lot of the time these ids are strings anyway (UUIDs and so on), and it's
really painful to translate between String <-> Int the whole time.

Are there any obvious blockers to this? I am a bit rusty on the ALS code
but from a quick scan I think this may work. Performance may be an issue
with large String keys... Any majore issues/objections to this thinking?

I may be able to find time to take a stab at this if there is demand.



"
Debasish Das <debasish.das83@gmail.com>,"Mon, 7 Apr 2014 07:12:21 -0700",Re: ALS array index out of bound with 50 factors,dev@spark.apache.org,"Nick,

I already have this code which calls dictionary generation and then maps
string etc to ints...I think the core algorithm should stay in ints...if
you like I can add this code in MFUtils.scala....that's the convention I
followed similar to MLUtils.scala...actually these functions should be even
made part of MLUtils.scala...

dependent...sometimes people would like to do map side joins if their
dictionaries are small...in my case user dictionary has 25M rows and
product dictionary has 3M rows...so join optimization did not help...

Thanks.
Deb




"
Koert Kuipers <koert@tresata.com>,"Mon, 7 Apr 2014 10:42:23 -0400",tachyon dependency,dev@spark.apache.org,"i noticed there is a dependency on tachyon in spark core 1.0.0-SNAPSHOT.
how does that work? i believe tachyon is written in java 7, yet spark
claims to be java 6 compatible.
"
Haoyuan Li <haoyuan.li@gmail.com>,"Mon, 7 Apr 2014 07:59:10 -0700",Re: tachyon dependency,dev@spark.apache.org,"Tachyon is Java 6 compatible from version 0.4. Beside putting input/output
data in Tachyon ( http://tachyon-project.org/Running-Spark-on-Tachyon.html ),
Spark applications can also persist data into Tachyon (
https://github.com/apache/spark/blob/master/docs/scala-programming-guide.md
).






-- 
Haoyuan Li
Algorithms, Machines, People Lab, EECS, UC Berkeley
http://www.cs.berkeley.edu/~haoyuan/
"
Mukesh G <mukgbv@gmail.com>,"Mon, 7 Apr 2014 21:50:36 +0530",Contributing to Spark,dev@spark.apache.org,"Hi,

   How I contribute to Spark and it's associated projects?

Appreciate the help...

Thanks

Mukesh
"
Sujeet Varakhedi <svarakhedi@gopivotal.com>,"Mon, 7 Apr 2014 09:44:17 -0700",Re: Contributing to Spark,dev@spark.apache.org,"This is a good place to start:
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark

Sujeet



"
Xiangrui Meng <mengxr@gmail.com>,"Mon, 7 Apr 2014 09:53:38 -0700",Re: ALS array index out of bound with 50 factors,dev@spark.apache.org,"Hi Deb,

This thread is for the out-of-bound error you described. I don't think
the number of iterations has any effect here. My questions were:

1) Are you using the master branch or a particular commit?

2) Do you have negative or out-of-integer-range user or product ids?
Try to print out the max/min value of user/product ids.

Best,
Xiangrui


"
Debasish Das <debasish.das83@gmail.com>,"Mon, 7 Apr 2014 10:36:37 -0700",Re: ALS array index out of bound with 50 factors,dev@spark.apache.org,"I am using master...

No negative indexes...

If I run with 4 iterations it runs fine and I can generate factors...

With 10 iterations run fails with array index out of bound...

25m users and 3m products are within int limits....

Does it help if I can point the logs for both the runs to you ?

I will debug it further today...

"
Kay Ousterhout <keo@eecs.berkeley.edu>,"Mon, 7 Apr 2014 10:52:18 -0700",Flaky streaming tests,dev@spark.apache.org,"Hi all,

The InputStreamsSuite seems to have some serious flakiness issues -- I've
seen the file input stream fail many times and now I'm seeing some actor
input stream test failures (
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/13846/consoleFull)
on what I think is an unrelated change.  Does anyone know anything about
these?  Should we just remove some of these tests since they seem to be
constantly failing?

-Kay
"
Xiangrui Meng <mengxr@gmail.com>,"Mon, 7 Apr 2014 11:28:53 -0700",Re: ALS array index out of bound with 50 factors,dev@spark.apache.org,"Hi Deb,

It would be helpful if you can attached the logs. It is strange to see
that you can make 4 iterations but not 10.

Xiangrui


"
Nan Zhu <zhunanmcgill@gmail.com>,"Mon, 7 Apr 2014 14:30:50 -0400",Re: Flaky streaming tests,"""dev@spark.apache.org"" <dev@spark.apache.org>","I met this issue when Jenkins seems to be very busy....


"
Patrick Wendell <pwendell@gmail.com>,"Mon, 7 Apr 2014 11:32:02 -0700",Re: Flaky streaming tests,"""dev@spark.apache.org"" <dev@spark.apache.org>","TD - do you know what is going on here?

I looked into this ab it and at least a few of these that use
Thread.sleep() and assume the sleep will be exact, which is wrong. We
should disable all the tests that do and probably they should be re-written
to virtualize time.

- Patrick



"
Michael Armbrust <michael@databricks.com>,"Mon, 7 Apr 2014 11:33:25 -0700",Re: Flaky streaming tests,dev@spark.apache.org,"There is a JIRA for one of the flakey tests here:
https://issues.apache.org/jira/browse/SPARK-1409



"
Tathagata Das <tathagata.das1565@gmail.com>,"Mon, 7 Apr 2014 11:33:50 -0700",Re: Flaky streaming tests,dev@spark.apache.org,"Yes, I will take a look at those tests ASAP.

TD




"
Michael Armbrust <michael@databricks.com>,"Mon, 7 Apr 2014 11:34:03 -0700",Re: Flaky streaming tests,dev@spark.apache.org,"I agree these should be disabled right away, and the JIRA can be used to
track fixing / turning them back on.



"
Christophe Clapp <christophe@christophe.cc>,"Mon, 07 Apr 2014 12:19:43 -0700",Spark Streaming and Flume Avro RPC Servers,dev@spark.apache.org,"Hi,

 From my testing of Spark Streaming with Flume, it seems that there's 
only one of the Spark worker nodes that runs a Flume Avro RPC server to 
receive messages at any given time, as opposed to every Spark worker 
running an Avro RPC server to receive messages. Is this the case? Our 
use-case would benefit from balancing the load across Workers because of 
our volume of messages. We would be using a load balancer in front of 
the Spark workers running the Avro RPC servers, essentially 
round-robinning the messages across all of them.

If this is something that is currently not supported, I'd be interested 
in contributing to the code to make it happen.

- Christophe

"
Michael Ernest <mfernest@cloudera.com>,"Mon, 7 Apr 2014 15:23:38 -0400",Re: Spark Streaming and Flume Avro RPC Servers,dev@spark.apache.org,"You can configure your sinks to write to one or more Avro sources in a
load-balanced configuration.

https://flume.apache.org/FlumeUserGuide.html#flume-sink-processors

mfe






-- 
Michael Ernest
Sr. Solutions Consultant
West Coast
"
Christophe Clapp <christophe@christophe.cc>,"Mon, 7 Apr 2014 12:37:29 -0700",Re: Spark Streaming and Flume Avro RPC Servers,dev@spark.apache.org,"Right, but at least in my case, no avro RPC server was started on any of
the spark worker nodes except for one. I don't know if that's just some
configuration issue with my setup or if it's expected behavior. I would
need spark to start avro RPC servers on every worker rather than just one.

- Christophe

"
Christophe Clapp <christophe@christophe.cc>,"Mon, 07 Apr 2014 12:58:21 -0700",Re: Spark Streaming and Flume Avro RPC Servers,dev@spark.apache.org,"Based on the source code here:
https://github.com/apache/spark/blob/master/external/flume/src/main/scala/org/apache/spark/streaming/flume/FlumeUtils.scala

It looks like in its current version, FlumeUtils does not support 
starting an Avro RPC server on more than one worker.

- Christophe



"
Christophe Clapp <christophe@christophe.cc>,"Mon, 07 Apr 2014 13:16:18 -0700",Re: Spark Streaming and Flume Avro RPC Servers,dev@spark.apache.org,"Could it be as simple as just changing FlumeUtils to accept a list of 
host/port number pairs to start the RPC servers on?




"
Michael Ernest <mfernest@cloudera.com>,"Mon, 7 Apr 2014 16:50:25 -0400",Re: Spark Streaming and Flume Avro RPC Servers,dev@spark.apache.org,"I don't see why not. If one were doing something similar with straight
Flume, you'd start an agent on each node you care to receive Avro/RPC
events. In the absence of clearer insight to your use case, I'm puzzling
just a little why it's necessary for each Worker to be its own receiver,
but there's no real objection or concern to fuel the puzzlement, just
curiosity.





-- 
Michael Ernest
Sr. Solutions Consultant
West Coast
"
Christophe Clapp <christophe@christophe.cc>,"Mon, 7 Apr 2014 14:37:22 -0700",Re: Spark Streaming and Flume Avro RPC Servers,dev@spark.apache.org,"Cool. I'll look at making the code change in FlumeUtils and generating a
pull request.

As far as the use case, the volume of messages we have is currently about
30 MB per second which may grow to over what a 1 Gbit network adapter can
handle.

- Christophe

"
DB Tsai <dbtsai@stanford.edu>,"Mon, 7 Apr 2014 19:00:02 -0700",Re: MLLib - Thoughts about refactoring Updater for LBFGS?,"dlwh@cs.berkeley.edu, debasish.das83@gmail.com, mengxr@gmail.com, 
	dev@spark.apache.org","Hi guys,

The latest PR uses Breeze's L-BFGS implement which is introduced by
Xiangrui's sparse input format work in SPARK-1212.

https://github.com/apache/spark/pull/353

Now, it works with the new sparse framework!

Any feedback would be greatly appreciated.

Thanks.

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



"
Mukesh G <mukgbv@gmail.com>,"Tue, 8 Apr 2014 10:15:14 +0530",Re: Contributing to Spark,dev@spark.apache.org,"Hi Sujeet,

    Thanks. I went thru the website and looks great. Is there a list of
items that I can choose from, for contribution?

Thanks

Mukesh



"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 7 Apr 2014 22:22:21 -0700",Re: Contributing to Spark,dev@spark.apache.org,"I’d suggest looking for the issues labeled “Starter” on JIRA. You can find them here: https://issues.apache.org/jira/browse/SPARK-1438?jql=project%20%3D%20SPARK%20AND%20labels%20%3D%20Starter%20AND%20status%20in%20(Open%2C%20%22In%20Progress%22%2C%20Reopened)

Matei


https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark


"
Debasish Das <debasish.das83@gmail.com>,"Mon, 7 Apr 2014 23:23:20 -0700",Re: MLLib - Thoughts about refactoring Updater for LBFGS?,DB Tsai <dbtsai@stanford.edu>,"I got your checkin....I need to run logistic regression SGD vs BFGS for my
current usecases but your next checkin will update the logistic regression
with LBFGS right ? Are you adding it to regression package as well ?

Thanks.
Deb



"
Debasish Das <debasish.das83@gmail.com>,"Mon, 7 Apr 2014 23:41:56 -0700",Re: MLLib - Thoughts about refactoring Updater for LBFGS?,DB Tsai <dbtsai@stanford.edu>,"By the way...what's the idea...the labeled data set is a RDD which is
cached on all nodes..

The bfgs solver is maintained on the master or each worker is supposed to
maintain it's own bfgs...



"
Aaron Davidson <ilikerps@gmail.com>,"Tue, 8 Apr 2014 09:34:30 -0700",Re: Contributing to Spark,dev@spark.apache.org,"Matei's link seems to point to a specific starter project as part of the
starter list, but here is the list itself:
https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20labels%20%3D%20Starter%20AND%20status%20in%20(Open%2C%20%22In%20Progress%22%2C%20Reopened)



"
Matei Zaharia <matei.zaharia@gmail.com>,"Tue, 8 Apr 2014 09:37:24 -0700",Re: Contributing to Spark,dev@spark.apache.org,"Shh, maybe I really wanted people to fix that one issue.


the
https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20labels%20%3D%20Starter%20AND%20status%20in%20(Open%2C%20%22In%20Progress%22%2C%20Reopened)
find
https://issues.apache.org/jira/browse/SPARK-1438?jql=project%20%3D%20SPARK%20AND%20labels%20%3D%20Starter%20AND%20status%20in%20(Open%2C%20%22In%20Progress%22%2C%20Reopened)
of
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark


"
Debasish Das <debasish.das83@gmail.com>,"Tue, 8 Apr 2014 09:42:13 -0700",Re: MLLib - Thoughts about refactoring Updater for LBFGS?,DB Tsai <dbtsai@stanford.edu>,"Hi DB,

Are we going to clean up the function:

class LogisticRegressionWithSGD private (
    var stepSize: Double,
    var numIterations: Int,
    var regParam: Double,
    var miniBatchFraction: Double)
  extends GeneralizedLinearAlgorithm[LogisticRegressionModel] with
Serializable {

  val gradient = new LogisticGradient()
  val updater = new SimpleUpdater()
  override val optimizer = new GradientDescent(gradient, updater)

Or add a new one ?

class LogisticRegressionWithBFGS ?

The WithABC is optional since optimizer could be picked up either based on
a flag...there are only 3 options for optimizor:

1. GradientDescent
2. Quasi Newton
3. Newton

May be we add an enum for optimization type....and then under
GradientDescent family people can add their variants of SGD....Not sure if
ConjugateGradient comes under 1 or 2....may be we need 4 options...

Thanks.
Deb



"
Debasish Das <debasish.das83@gmail.com>,"Tue, 8 Apr 2014 09:44:49 -0700",Re: MLLib - Thoughts about refactoring Updater for LBFGS?,DB Tsai <dbtsai@stanford.edu>,"By the way these changes are needed in mllib.regression as well....

Right now my usecases need BFGS support in logistic regression and MLOR so
we can focus on cleaning up the classification package first ?




"
Michael Ernest <mfernest@cloudera.com>,"Tue, 8 Apr 2014 12:50:30 -0400",Re: Contributing to Spark,dev@spark.apache.org,"Ha ha! nice try, sheepherder! ;-)





-- 
Michael Ernest
Sr. Solutions Consultant
West Coast
"
love2dishtech <love2dishtech@gmail.com>,"Tue, 8 Apr 2014 13:02:27 -0700 (PDT)",Apache Spark and Graphx for Real Time Analytics,dev@spark.incubator.apache.org,"Hi,

Is Graphx on top of Apache Spark, is able to process the large scale
distributed graph traversal and compute, in real time. What is the query
execution engine distributing the query on top of graphx and apache spark.
My typical use case is a large scale distributed graph traversal in real
time, with billions of nodes.

Thanks,
Love.



--

"
Nick Pentreath <nick.pentreath@gmail.com>,"Tue, 8 Apr 2014 22:23:12 +0200",Re: Apache Spark and Graphx for Real Time Analytics,dev@spark.apache.org,"GraphX, like Spark, will not typically be ""real-time"" (where by ""real-time""
here I assume you mean of the order of a few 10s-100s ms, up to a few
seconds).

Spark can in some cases approach the upper boundary of this definition (a
second or two, possibly less) when data is cached in memory and the
computation is not ""too heavy"", while Spark Streaming may be able to get
closer to the mid-to-upper boundary of this under similar conditions,
especially if aggregating over relatively small windows.

However, for this use case (while I haven't used GraphX yet) I would say
something like Titan (https://github.com/thinkaurelius/titan/wiki) or a
similar OLTP graph DB may be what you're after. But this depends on what
kind of graph traversal you need.





"
Anurag <anurag.phadke@gmail.com>,"Tue, 8 Apr 2014 13:36:04 -0700",reading custom input format in Spark,dev@spark.apache.org,"Hi,
I am able to read a custom input format in spark.
scala> val inputRead = sc.newAPIHadoopFile(""hdfs://
127.0.0.1/user/cloudera/date_dataset/
"",classOf[io.reader.PatternInputFormat],classOf[org.apache.hadoop.io.LongWritable],classOf[org.apache.hadoop.io.Text])

However, doing a
inputRead.count()
results in null pointer exception.
14/04/08 13:33:39 INFO FileInputFormat: Total input paths to process : 1
14/04/08 13:33:39 INFO SparkContext: Starting job: count at <console>:15
14/04/08 13:33:39 INFO DAGScheduler: Got job 8 (count at <console>:15) with
1 output partitions (allowLocal=false)
14/04/08 13:33:39 INFO DAGScheduler: Final stage: Stage 9 (count at
<console>:15)
14/04/08 13:33:39 INFO DAGScheduler: Parents of final stage: List()
14/04/08 13:33:39 INFO DAGScheduler: Missing parents: List()
14/04/08 13:33:39 INFO DAGScheduler: Submitting Stage 9 (NewHadoopRDD[19]
at newAPIHadoopFile at <console>:12), which has no missing parents
14/04/08 13:33:39 INFO DAGScheduler: Submitting 1 missing tasks from Stage
9 (NewHadoopRDD[19] at newAPIHadoopFile at <console>:12)
14/04/08 13:33:39 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks
14/04/08 13:33:39 INFO TaskSetManager: Starting task 9.0:0 as TID 8 on
executor localhost: localhost (PROCESS_LOCAL)
14/04/08 13:33:39 INFO TaskSetManager: Serialized task 9.0:0 as 1297 bytes
in 0 ms
14/04/08 13:33:39 INFO Executor: Running task ID 8
14/04/08 13:33:39 INFO BlockManager: Found block broadcast_5 locally
14/04/08 13:33:39 INFO NewHadoopRDD: Input split: hdfs://
127.0.0.1/user/cloudera/date_dataset/sample.txt:0+759
14/04/08 13:33:39 WARN TaskSetManager: Lost TID 8 (task 9.0:0)
14/04/08 13:33:39 WARN TaskSetManager: Loss was due to
java.lang.NullPointerException
java.lang.NullPointerException
    at java.util.regex.Pattern.<init>(Pattern.java:1132)
    at java.util.regex.Pattern.compile(Pattern.java:823)
    at io.reader.PatternRecordReader.initialize(PatternRecordReader.java:42)
    at
org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:96)
    at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:84)
    at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:48)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109)
    at org.apache.spark.scheduler.Task.run(Task.scala:53)
    at
org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:213)
    at
org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)
    at
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
14/04/08 13:33:39 ERROR TaskSetManager: Task 9.0:0 failed 1 times; aborting
job
14/04/08 13:33:39 INFO DAGScheduler: Failed to run count at <console>:15
14/04/08 13:33:39 INFO TaskSchedulerImpl: Remove TaskSet 9.0 from pool
14/04/08 13:33:39 ERROR Executor: Exception in task ID 8
java.lang.NullPointerException
    at java.util.regex.Pattern.<init>(Pattern.java:1132)
    at java.util.regex.Pattern.compile(Pattern.java:823)
    at io.reader.PatternRecordReader.initialize(PatternRecordReader.java:42)
    at
org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:96)
    at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:84)
    at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:48)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109)
    at org.apache.spark.scheduler.Task.run(Task.scala:53)
    at
org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:213)
    at
org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)
    at
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
org.apache.spark.SparkException: Job aborted: Task 9.0:0 failed 1 times
(most recent failure: Exception failure: java.lang.NullPointerException)
    at
org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1028)
    at
org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1026)
    at
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
    at org.apache.spark.scheduler.DAGScheduler.org
$apache$spark$scheduler$DAGScheduler$$abortStage(DAGScheduler.scala:1026)
    at
org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:619)
    at
org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:619)
    at scala.Option.foreach(Option.scala:236)
    at
org.apache.spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:619)
    at
org.apache.spark.scheduler.DAGScheduler$$anonfun$start$1$$anon$2$$anonfun$receive$1.applyOrElse(DAGScheduler.scala:207)
    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
    at akka.actor.ActorCell.invoke(ActorCell.scala:456)
    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
    at akka.dispatch.Mailbox.run(Mailbox.scala:219)
    at
akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
    at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
    at
scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
    at
scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
    at
scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)


any idea what might be happening here?

-anurag



-- 
Twitter: @anuragphadke (https://twitter.com/#!/anuragphadke)
"
Nick Pentreath <nick.pentreath@gmail.com>,"Tue, 8 Apr 2014 22:42:14 +0200",Re: reading custom input format in Spark,dev@spark.apache.org,"Seems like you need to initialise a regex pattern for that inputformat. How
is this done? Perhaps via a config option?

In which case you need to first create a hadoop configuration, set the
appropriate config option for the regex, and pass that into
newAPIHadoopFile.



"
Andrew Ash <andrew@andrewash.com>,"Tue, 8 Apr 2014 13:41:57 -0700",Re: reading custom input format in Spark,dev@spark.apache.org,"Are you using the PatternInputFormat from this blog post?

https://hadoopi.wordpress.com/2013/05/31/custom-recordreader-processing-string-pattern-delimited-records/

If so you need to set the pattern in the configuration before attempting to
read data with that InputFormat:

String regex = ""^[A-Za-z]{3},\\s\\d{2}\\s[A-Za-z]{3}.*"";
 Configuration conf = new Configuration(true);
 conf.set(""record.delimiter.regex"", regex);



"
Anurag <anurag.phadke@gmail.com>,"Tue, 8 Apr 2014 13:47:08 -0700",Re: reading custom input format in Spark,dev <dev@spark.apache.org>,"andrew - yes, i am using the PatternInputFormat from the blog post you
referenced.
I know how to set the pattern in configuration while writing a MR job, how
do i do that from a spark shell?

-anurag







-- 
Twitter: @anuragphadke (https://twitter.com/#!/anuragphadke)
"
Andrew Ash <andrew@andrewash.com>,"Tue, 8 Apr 2014 13:53:25 -0700",Re: reading custom input format in Spark,dev@spark.apache.org,"Anurag,

There is another method called newAPIHadoopRDD that takes in a
Configuration object rather than a path.  Give that a shot?

https://spark.apache.org/docs/latest/api/core/index.html#org.apache.spark.SparkContext



"
Anurag <anurag.phadke@gmail.com>,"Tue, 8 Apr 2014 13:53:58 -0700",Re: reading custom input format in Spark,dev <dev@spark.apache.org>,"andrew/nick,
thx for the input, got it to work:

sc.hadoopConfiguration.set(""record.delimiter.regex"",
""^[A-Za-z]{3},\\s\\d{2}\\s[A-Za-z]{3}.*"")

:-)

-anurag







-- 
Twitter: @anuragphadke (https://twitter.com/#!/anuragphadke)
"
Evan Chan <ev@ooyala.com>,"Tue, 8 Apr 2014 13:56:01 -0700",Re: Apache Spark and Graphx for Real Time Analytics,"""dev@spark.apache.org"" <dev@spark.apache.org>","I doubt Titan would be able to give you traversal of billions of nodes in
real-time either.   In-memory traversal is typically much faster than
Cassandra-based tree traversal, even including in-memory caching.






-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

<http://www.ooyala.com/>
<http://www.facebook.com/ooyala><http://www.linkedin.com/company/ooyala><http://www.twitter.com/ooyala>
"
Koert Kuipers <koert@tresata.com>,"Tue, 8 Apr 2014 17:01:59 -0400",Re: Apache Spark and Graphx for Real Time Analytics,dev@spark.apache.org,"it all depends on what kind of traversing. if its point traversing then a
random access based something would be great.

if its more scan-like traversl then spark will fit



"
Nick Pentreath <nick.pentreath@gmail.com>,"Tue, 8 Apr 2014 23:03:16 +0200",Re: Apache Spark and Graphx for Real Time Analytics,dev@spark.apache.org,"Likely neither will give real-time for full-graph traversal, no. And once
in memory, GraphX would definitely be faster for ""breadth-first"" traversal.

But for ""vertex-centric"" traversals (starting from a vertex and traversing
edges from there, such as ""friends of friends"" queries etc) then Titan is
optimized for that use case.





"
Reynold Xin <rxin@databricks.com>,"Tue, 8 Apr 2014 14:12:53 -0700",Re: Apache Spark and Graphx for Real Time Analytics,dev@spark.apache.org,"Nick and Koert summarized it pretty well. Just to clarify and give some
concrete examples.

If you want to start with a specific vertex, and follow some path, it is
probably easier and faster to use some key values store or even MySQL or a
graph database.

If you want to count the average length of paths between all nodes, or if
you want to compute the pair wise shortest path for all vertices, GraphX
will likely be way faster.







"
DB Tsai <dbtsai@stanford.edu>,"Tue, 8 Apr 2014 15:44:11 -0700",Re: MLLib - Thoughts about refactoring Updater for LBFGS?,Debasish Das <debasish.das83@gmail.com>,"Hi Debasish,

The L-BFGS solver will be in the master like GD solver, and the part
that is parallelized is computing the gradient of each input row, and
summing them up.

I prefer to make the optimizer plug-able instead of adding new
LogisticRegressionWithLBFGS since 98% of the code will be the same.

Nice to have something like this,

class LogisticRegression private (
    var optimizer: Optimizer)
  extends GeneralizedLinearAlgorithm[LogisticRegressionModel]

The following parameters will be setup in the optimizers, and they
should because they are part of optimization parameters.

    var stepSize: Double,
    var numIterations: Int,
    var regParam: Double,
    var miniBatchFraction: Double

Xiangrui, what do you think?

For now, you can use my L-BFGS solver by copying and pasting the
LogisticRegressionWithSGD code, and changing the optimizer to L-BFGS.

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



"
Debasish Das <debasish.das83@gmail.com>,"Tue, 8 Apr 2014 16:05:18 -0700",Re: MLLib - Thoughts about refactoring Updater for LBFGS?,DB Tsai <dbtsai@stanford.edu>,"Yup that's what I expected...L-BFGS solver is in the master and gradient
computation per RDD is done on each of the workers...

This miniBatchFraction is also a heuristic which I don't think makes sense
for LogisticRegressionWithBFGS...does it ?



"
DB Tsai <dbtsai@stanford.edu>,"Tue, 8 Apr 2014 16:06:13 -0700",A series of meetups about machine learning with Spark in San Francisco,"dev@spark.apache.org, user@spark.apache.org","Hi guys,

We're going to hold a series of meetups about machine learning with
Spark in San Francisco.

The first one will be on April 24. Xiangrui Meng from Databricks will
talk about Spark, Spark/Python, features engineering, and MLlib.

See http://www.meetup.com/sfmachinelearning/events/174560212/ for detail.

The next one on May 1 will be join event with Cloudera talking about
unsupervised learning and multinomial logistic regression with L-BFGS
with Spark.

See http://www.meetup.com/sfmachinelearning/events/176105932/

If you would like to share anything related to machine learning with
Spark in SF Machine Learning Meetup, please let me know.

Thanks.

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai

"
DB Tsai <dbtsai@stanford.edu>,"Tue, 8 Apr 2014 16:19:37 -0700",Re: MLLib - Thoughts about refactoring Updater for LBFGS?,Debasish Das <debasish.das83@gmail.com>,"I think mini batch is still useful for L-BFGS.

the smaller subsamples of data using mini batch with L-BFGS.

Then we could use the weights trained with mini batch to start another
training process with full data.

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



"
Debasish Das <debasish.das83@gmail.com>,"Tue, 8 Apr 2014 16:45:46 -0700",Re: MLLib - Thoughts about refactoring Updater for LBFGS?,DB Tsai <dbtsai@stanford.edu>,"Have you experimented with it ? For logistic regression at least given
enough iterations/tolerance that you are giving, BFGS in both ways should
converge to same solution....



"
DB Tsai <dbtsai@stanford.edu>,"Tue, 8 Apr 2014 16:48:17 -0700",Re: MLLib - Thoughts about refactoring Updater for LBFGS?,Debasish Das <debasish.das83@gmail.com>,"I don't experiment it. That's the use-case in theory I could think of. ^^

However, from what I saw, BFGS converges really fast so that I only
need 20~30 iterations in general.

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



"
Patrick Wendell <pwendell@gmail.com>,"Wed, 9 Apr 2014 02:31:24 -0700",branch-1.0 cut,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey All,

In accordance with the scheduled window for the release I've cut a 1.0
branch. Thanks a ton to everyone for being so active in reviews during the
last week. In the last 7 days we've merged 66 new patches, and every one of
them has undergone thorough peer-review. Tons of committers have been
active in code review - pretty cool!

At this point the 1.0 branch transitions to a normal maintenance branch*.
Bug fixes, documentation are still welcome or additions to higher level
libraries (e.g. MLLib). The focus though is shifting to QA, fixes, and
documentation for the release.

Thanks again to everyone who participated in the last week!

- Patrick

*caveat: we will still merge in some API visibility patches and a few
remaining loose ends in the next day or two.
"
Andy Konwinski <andykonwinski@gmail.com>,"Wed, 9 Apr 2014 12:39:34 -0700",Re: branch-1.0 cut,dev@spark.apache.org,"Wow, great work. Very impressive sticking to the schedule!



"
Tathagata Das <tathagata.das1565@gmail.com>,"Wed, 9 Apr 2014 14:54:03 -0700",Spark 0.9.1 released,"dev@spark.apache.org, user@spark.apache.org","Hi everyone,

We have just posted Spark 0.9.1, which is a maintenance release with
bug fixes, performance improvements, better stability with YARN and
improved parity of the Scala and Python API. We recommend all 0.9.0
users to upgrade to this stable release.

This is the first release since Spark graduated as a top level Apache
project. Contributions to this release came from 37 developers.

The full release notes are at:
http://spark.apache.org/releases/spark-release-0-9-1.html

You can download the release at:
http://spark.apache.org/downloads.html

Thanks all the developers who contributed to this release:
Aaron Davidson, Aaron Kimball, Andrew Ash, Andrew Or, Andrew Tulloch,
Bijay Bisht, Bouke van der Bijl, Bryn Keller, Chen Chao,
Christian Lundgren, Diana Carroll, Emtiaz Ahmed, Frank Dai,
Henry Saputra, jianghan, Josh Rosen, Jyotiska NK, Kay Ousterhout,
Kousuke Saruta, Mark Grover, Matei Zaharia, Nan Zhu, Nick Lanham,
Patrick Wendell, Prabin Banka, Prashant Sharma, Qiuzhuang,
Raymond Liu, Reynold Xin, Sandy Ryza, Sean Owen, Shixiong Zhu,
shiyun.wxm, Stevo SlaviÄ‡, Tathagata Das, Tom Graves, Xiangrui Meng

TD

"
Tathagata Das <tathagata.das1565@gmail.com>,"Wed, 9 Apr 2014 14:59:07 -0700",Re: Spark 0.9.1 released,"dev@spark.apache.org, user@spark.apache.org","A small additional note: Please use the direct download links in the Spark
Downloads <http://spark.apache.org/downloads.html> page. The Apache mirrors
take a day or so to sync from the main repo, so may not work immediately.

TD



"
Sujeet Varakhedi <svsujeet@gmail.com>,"Wed, 9 Apr 2014 15:00:34 -0700",Re: Contributing to Spark,dev@spark.apache.org,"Another starter question which probably should have asked before is what is
the most efficient way to iterate quickly on dev/test. I am currently using
a local cluster (via vagrant and shared folders)  and also spark-shell.

Sujeet



"
Matei Zaharia <matei.zaharia@gmail.com>,"Wed, 9 Apr 2014 15:07:38 -0700",Re: Spark 0.9.1 released,user@spark.apache.org,"Thanks TD for managing this release, and thanks to everyone who contributed!

Matei


Spark Downloads page. The Apache mirrors take a day or so to sync from the main repo, so may not work immediately.
Meng

"
Reynold Xin <rxin@databricks.com>,"Wed, 9 Apr 2014 15:23:52 -0700",Re: Contributing to Spark,dev@spark.apache.org,"Usually you can just run Spark in local mode on a single machine for most
dev/testing.

If you want to simulate a cluster locally using multiple Spark worker
processes, you can use the undocumented local cluster mode, e.g.

local-cluster[2,1,512]

this launches two worker processes, each with one core and 512m of ram.






"
Tathagata Das <tathagata.das1565@gmail.com>,"Wed, 9 Apr 2014 16:21:38 -0700",Re: Spark 0.9.1 released,user@spark.apache.org,"Thanks Nick for pointing that out! I have updated the release
notes<http://spark.apache.org/releases/spark-release-0-9-1.html>.
But I see the new operations like repartition in the latest PySpark
RDD docs<http://spark.apache.org/docs/latest/api/pyspark/index.html>.
Maybe refresh the page couple of times?

TD


m

rk.apache.org/releases/spark-release-0-9-1.html>
ml>hasn't been updated yet to reflect the new additions to PySpark.
ote:
ork
"
Andy Konwinski <andykonwinski@gmail.com>,"Wed, 9 Apr 2014 17:48:00 -0700",Updating all references to github.com/apache/incubator-spark on spark website,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Since http://github.com/apache/incubator-spark and any links underneath it
now return 404, I propose we do a global search and replace to change all
instances to remove ""incubator-"", including those in docs/0.8.0 docs/0.8.1
and docs/0.9.0.

I'm happy to do this.

Any discussion before I do?

Andy
"
techaddict <sandeep@techaddict.me>,"Thu, 10 Apr 2014 12:56:12 -0700 (PDT)",org.apache.spark.util.Vector is deprecated what next ?,dev@spark.incubator.apache.org,"org.apache.spark.util.Vector is deprecated so what should be done to use say
if want to create a vector with zeros, def zeros(length: Int) in util.Vector
using new mllib.linalg.Vector ?



--

"
Patrick Wendell <pwendell@gmail.com>,"Thu, 10 Apr 2014 13:12:41 -0700",Re: org.apache.spark.util.Vector is deprecated what next ?,"""dev@spark.apache.org"" <dev@spark.apache.org>","You'll need to use the associated functionality in Breeze and then create a
dense vector from a Breeze vector. I have a JIRA for us to update the
examples for 1.0...  I'm hoping Xiangrui can take a look at it.

https://issues.apache.org/jira/browse/SPARK-1464

https://github.com/scalanlp/breeze/wiki/Breeze-Linear-Algebra




"
Patrick Wendell <pwendell@gmail.com>,"Thu, 10 Apr 2014 13:12:41 -0700",Re: org.apache.spark.util.Vector is deprecated what next ?,"""dev@spark.apache.org"" <dev@spark.apache.org>","You'll need to use the associated functionality in Breeze and then create a
dense vector from a Breeze vector. I have a JIRA for us to update the
examples for 1.0...  I'm hoping Xiangrui can take a look at it.

https://issues.apache.org/jira/browse/SPARK-1464

https://github.com/scalanlp/breeze/wiki/Breeze-Linear-Algebra




"
Ignacio Zendejas <ignacio.zendejas.cs@gmail.com>,"Thu, 10 Apr 2014 13:48:23 -0700",minor optimizations to get my feet wet,dev@spark.apache.org,"Hi, all -

First off, I want to say that I love spark and am very excited about
MLBase. I'd love to contribute now that I have some time, but before I do
that I'd like to familiarize myself with the process.

In looking for a few projects and settling on one which I'll discuss in
another thread, I found some very minor optimizations I could contribute,
again, as part of this first step.

Before I initiate a PR, I've gone ahead and tested style, ran tests, etc
per the instructions, but I'd still like to have someone quickly glance
over it and ensure that these are JIRA worthy.

Commit:
https://github.com/izendejas/spark/commit/81065aed9987c1b08cd5784b7a6153e26f3f7402

To summarize:

* I got rid of some SeqLike.reverse calls when sorting by descending order
* replaced slice(1, length) calls with the much safer (avoids IOOBEs) and
more readable .tail calls
* used a foldleft to avoid using mutable variables in NaiveBayes code

This last one is meant to understand what's valued more between idiomatic
Scala development or readability. I'm personally a fan of foldLefts where
applicable, but do think they're a bit less readable.

Thanks,
Ignacio
"
Reynold Xin <rxin@databricks.com>,"Thu, 10 Apr 2014 14:10:56 -0700",Re: minor optimizations to get my feet wet,dev@spark.apache.org,"Thanks for contributing!

I think often unless the feature is gigantic, you can send a pull request
we typically prefer readability over conciseness, and thus we tend to avoid
using too much Scala magic or operator overloading.

In this specific case, do you know if using - instead of reverse improve
performance? I personally find it slightly awkward to use underscore right
after negation ...


The tail change looks good to me.

For foldLeft, I agree with you that the old way is more readable (although
less idiomatic scala).





"
DB Tsai <dbtsai@stanford.edu>,"Thu, 10 Apr 2014 14:14:07 -0700",Re: org.apache.spark.util.Vector is deprecated what next ?,dev@spark.apache.org,"You can construct the Breeze vector by

    val breezeVector = breeze.linalg.DenseVector.zeros[Double](length)

If you want to convert to mllib vector, you can do

    val mllibVector = Vectors.fromBreeze(breezeVector)

If you want to convert back to breeze vector,

    val newBreezeVector = mllibVector.toBreeze

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



"
DB Tsai <dbtsai@stanford.edu>,"Thu, 10 Apr 2014 14:14:07 -0700",Re: org.apache.spark.util.Vector is deprecated what next ?,dev@spark.apache.org,"You can construct the Breeze vector by

    val breezeVector = breeze.linalg.DenseVector.zeros[Double](length)

If you want to convert to mllib vector, you can do

    val mllibVector = Vectors.fromBreeze(breezeVector)

If you want to convert back to breeze vector,

    val newBreezeVector = mllibVector.toBreeze

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



"
Ignacio Zendejas <ignacio.zendejas.cs@gmail.com>,"Thu, 10 Apr 2014 14:18:21 -0700",feature selection and sparse vector support,dev@spark.apache.org,"Hi, again -

As part of the next step, I'd like to make a more substantive contribution
and propose some initial work on feature selection, primarily as it relates
to text classification.

Specifically, I'd like to contribute very straightforward code to perform
information gain feature evaluation. Below's a good primer that shows that
Information Gain is a very good option in many cases. If successful, BNS
(introduced in the paper), would be another approach worth looking into as
it actually improves the f score with a smaller feature space.

http://machinelearning.wustl.edu/mlpapers/paper_files/Forman03.pdf

And here's my first cut:
https://github.com/izendejas/spark/commit/e5a0620838841c99865ffa4fb0d2b449751236a8

I don't like that I do two passes to compute the class priors and joint
distributions, so I'll look into using combineByKey as in the NaiveBayes
implementation.  Also, this is still untested code, but it gets my ideas
out there and think it'd be best to define a FeatureEval trait or whatnot
that helps with ranking and selecting.

I also realize the above methods are probably more suitable for MLI than
MLlib, but there doesn't seem to be much activity on the former.

Second, is there a plan to support sparse vector representations for
NaiveBayes. This will probably be more efficient in, for example, text
classification tasks with lots of features (consider the case where n-grams
with n > 1 are used).

And on a related note, MLUtils.loadLabeledData doesn't support loading
sparse data. Any plans here to do so? There also doesn't seem to be a
defined file format for MLlib. Has there been any consideration to support
multiple standard formats, rather than defining one: eg, csv, tsv, Weka's
arff, etc?

Thanks for your time,
Ignacio
"
Henry Saputra <henry.saputra@gmail.com>,"Thu, 10 Apr 2014 15:12:52 -0700",Re: minor optimizations to get my feet wet,"""dev@spark.apache.org"" <dev@spark.apache.org>","HI Ignacio,

Thank you for your contribution.

Just a friendly reminder, in case you have not contributed to Apache
Software Foundation projects before please submit ASF ICLA form [1] or
if you are sponsored by your company also ask the company to send CCLA
[2] to clear the intellectual property for your contributions.

You can ignore ""preferred Apache id"" section for now.


Thank you,

Henry Saputra

[1] https://www.apache.org/licenses/icla.txt
[2] http://www.apache.org/licenses/cla-corporate.txt



"
Ignacio Zendejas <ignacio.zendejas.cs@gmail.com>,"Thu, 10 Apr 2014 15:17:44 -0700",Re: minor optimizations to get my feet wet,dev@spark.apache.org,"I don't think there's a noticeable performance hit by the use of reverse in
those cases. It was a quick set of changes and it helped understand what
you look for. I didn't intend to nitpick, so I'll leave as is. I could have
used a scala.Ordering implicitly/explicitly also, but seems overkill and
don't want to necessarily start a discussion about what's best--unless one
of the admins deems this important.

I'll only keep the use of take and tail over using slice and switch over to
math.min where indicated.

This after I follow Henry's timely advice--thanks, Henry.

cheers.





"
Henry Saputra <henry.saputra@gmail.com>,"Thu, 10 Apr 2014 16:09:39 -0700",Re: minor optimizations to get my feet wet,"""dev@spark.apache.org"" <dev@spark.apache.org>","You are welcome, thanks again for contributing =)

- Henry


"
Marcelo Vanzin <vanzin@cloudera.com>,"Thu, 10 Apr 2014 17:16:36 -0700",RFC: varargs in Logging.scala?,dev@spark.apache.org,"Hey there,

While going through the try to get the hang of things, I've noticed
several different styles of logging. They all have some downside
(readability being one of them in certain cases), but all of the
suffer from the fact that the log message needs to be built even
though it might not be used.

I spent some time trying to add varargs support to Logging.scala (also
to learn more about Scala itself), and came up with this:
https://github.com/vanzin/spark/commit/a15c284d4aac3d645b13c0ef157787ba014840e4

The change may look large, but the only interesting changes are in
Logging.scala, I promise.

What do you guys think of this approach?

It should, at worst, be just as fast (or slow) as before for the
majority of cases (i.e., any case where variables were used in the log
message). Personally, I think it reads better.

It might be possible to have something similar using string
interpolation, but I'm not familiar enough with Scala yet to try my
hand at that. Also, I believe that would still require some kind of
formatting when you want to do calculations (e.g. turn a variable
holding milliseconds into seconds in the log message).

If people like it, I'll submit a proper pull request. I've run a few
things using this code, and also the tests (which caught a few type
mismatches in the format strings), and everything looks ok so far.

-- 
Marcelo

"
Michael Armbrust <michael@databricks.com>,"Thu, 10 Apr 2014 17:46:19 -0700",Re: RFC: varargs in Logging.scala?,dev@spark.apache.org,"Hi Marcelo,

Thanks for bringing this up here, as this has been a topic of debate
recently.  Some thoughts below.

... all of the suffer from the fact that the log message needs to be built

This is not true of the current implementation (and this is actually why
Spark has a logging trait instead of just using a logger directly.)

If you look at the original function signatures:

protected def logDebug(msg: => String) ...


The => implies that we are passing the msg by name instead of by value.
Under the covers, scala is creating a closure that can be used to calculate
the log message, only if its actually required.  This does result is a
significant performance improvement, but still requires allocating an
object for the closure.  The bytecode is really something like this:

val logMessage = new Function0() { def call() =  ""Log message"" +
someExpensiveComputation() }
log.debug(logMessage)


In Catalyst and Spark SQL we are using the scala-logging package, which
uses macros to automatically rewrite all of your log statements.

You write: logger.debug(s""Log message $someExpensiveComputation"")

You get:

if(logger.debugEnabled) {
  val logMsg = ""Log message"" + someExpensiveComputation()
  logger.debug(logMsg)
}

IMHO, this is the cleanest option (and is supported by Typesafe).  Based on
a micro-benchmark, it is also the fastest:

std logging: 19885.48ms
spark logging 914.408ms
scala logging 729.779ms

standardizing on scala-logging.

Michael
"
Michael Armbrust <michael@databricks.com>,"Thu, 10 Apr 2014 17:55:24 -0700",Re: RFC: varargs in Logging.scala?,dev@spark.apache.org,"BTW...

You can do calculations in string interpolation:
s""Time: ${timeMillis / 1000}s""

Or use format strings.
f""Float with two decimal places: $floatValue%.2f""

More info:
http://docs.scala-lang.org/overviews/core/string-interpolation.html



"
Jim Ancona <jim@anconafamily.com>,"Thu, 10 Apr 2014 21:57:28 -0400",Building Spark AMI,dev@spark.apache.org,"Are there scripts to build the AMI used by the spark-ec2 script?

Alternatively, is there a place to download the AMI. I'm interested in
using it to deploy into an internal Openstack cloud.

Thanks,

Jim

"
"""=?ISO-8859-1?B?d2l0Z28=?="" <witgo@qq.com>","Fri, 11 Apr 2014 10:20:51 +0800",Re:RFC: varargs in Logging.scala?,"""=?ISO-8859-1?B?ZGV2?="" <dev@spark.apache.org>","In the following PR, there are related discussions.
https://github.com/apache/spark/pull/332





------------------ Original ------------------
From:  ""Marcelo Vanzin"";<vanzin@cloudera.com>;
Date:  Fri, Apr 11, 2014 08:16 AM
To:  ""dev""<dev@spark.apache.org>; 

Subject:  RFC: varargs in Logging.scala?



Hey there,

While going through the try to get the hang of things, I've noticed
several different styles of logging. They all have some downside
(readability being one of them in certain cases), but all of the
suffer from the fact that the log message needs to be built even
though it might not be used.

I spent some time trying to add varargs support to Logging.scala (also
to learn more about Scala itself), and came up with this:
https://github.com/vanzin/spark/commit/a15c284d4aac3d645b13c0ef157787ba014840e4

The change may look large, but the only interesting changes are in
Logging.scala, I promise.

What do you guys think of this approach?

It should, at worst, be just as fast (or slow) as before for the
majority of cases (i.e., any case where variables were used in the log
message). Personally, I think it reads better.

It might be possible to have something similar using string
interpolation, but I'm not familiar enough with Scala yet to try my
hand at that. Also, I believe that would still require some kind of
formatting when you want to do calculations (e.g. turn a variable
holding milliseconds into seconds in the log message).

If people like it, I'll submit a proper pull request. I've run a few
things using this code, and also the tests (which caught a few type
mismatches in the format strings), and everything looks ok so far.

-- 
Marcelo
."
Xiangrui Meng <mengxr@gmail.com>,"Thu, 10 Apr 2014 23:20:44 -0400",Re: feature selection and sparse vector support,dev@spark.apache.org,"Hi Ignacio,

Please create a JIRA and send a PR for the information gain
computation, so it is easy to track the progress.

The sparse vector support for NaiveBayes is already implemented in
branch-1.0 and master. You only need to provide an RDD of sparse
vectors (created from Vectors.sparse).

MLUtils.loadLibSVMData reads sparse features in LIBSVM format.

Best,
Xiangrui


"
=?UTF-8?B?SMOpY3RvciBNb3VyacOxby1UYWzDrW4=?= <hmourit@gmail.com>,"Fri, 11 Apr 2014 14:44:00 +0200",Re: feature selection and sparse vector support,dev@spark.apache.org,"Hi,

Regarding the implementation of feature selection techniques, I'm
implementing some iterative algorithms based on a paper by Gavin Brown et
al. [1]. In this paper, he proposes a common framework for many Information
Theory-based criteria, namely those that use relevancy (mutual information
between one feature and the label; Information Gain), redundancy, and
conditional redundancy. The latter two are differently interpreted
depending on the criteria, but all of them play with the mutual information
between the feature being analyzed and the already selected ones and the
same mutual information conditioned to the label.

I think we should have a common interface to plug different Feature
Selection techniques. I already have the algorithm implemented, but still
have to do tests on it. Right now I'm working on the design. Next week I
can share with you a proposal, so we can work together to bring Feature
Selection to Spark.

[1] Brown, G., Pocock, A., Zhao, M. J., & LujÃ¡n, M. (2012). Conditional
likelihood maximisation: a unifying framework for information theoretic
feature selection.*The Journal of Machine Learning Research*, *13*, 27-66.

---
HÃ©ctor



rm
S
9751236a8
s
s
ot
n
's
"
Marcelo Vanzin <vanzin@cloudera.com>,"Fri, 11 Apr 2014 09:24:40 -0700",Re: RFC: varargs in Logging.scala?,dev@spark.apache.org,"
Hah. Interesting. Guess it's my noob Scala hat showing off.

I saw the PR about using scala-logging before, but didn't pay too
close attention to it.

Thanks for the info guys!

-- 
Marcelo

"
David Hall <dlwh@cs.berkeley.edu>,"Fri, 11 Apr 2014 11:01:48 -0700",Re: RFC: varargs in Logging.scala?,dev@spark.apache.org,"Another usage that's nice is:

logDebug {
   val timeS = timeMillis/1000.0
   s""Time: $timeS""
}

which can be useful for more complicated expressions.



"
Ignacio Zendejas <ignacio.zendejas.cs@gmail.com>,"Fri, 11 Apr 2014 11:26:58 -0700",Re: feature selection and sparse vector support,dev@spark.apache.org,"Thanks for the response, Xiangrui.

And sounds good, Héctor. Look forward to working on this together.

A common interface is definitely required.  I'll create a JIRA shortly and
will explore design options myself to bring ideas to the table.

cheers.




on
n
on
.
to
9751236a8
nt
t
g
"
priya arora <arora.priya4172@gmail.com>,"Fri, 11 Apr 2014 15:05:03 -0400",Suggestion,dev@spark.apache.org,"Hi,

May I know how one can contribute in this project
http://spark.apache.org/mllib/ or in any other project. I am very eager to
contribute. Do let me know.

Thanks & Regards,
Priya Arora
"
Sandy Ryza <sandy.ryza@cloudera.com>,"Fri, 11 Apr 2014 12:16:43 -0700",Re: Suggestion,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Priya,

Here's a good place to start:
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark

-Sandy



"
Ignacio Zendejas <ignacio.zendejas.cs@gmail.com>,"Fri, 11 Apr 2014 14:04:14 -0700",Re: feature selection and sparse vector support,dev@spark.apache.org,"Here's the JIRA:
https://issues.apache.org/jira/browse/SPARK-1473

Future discussions should take place in its comments section.

Thanks.





d
t
on
l
l
6.
s
49751236a8
xt
ng
a
"
DB Tsai <dbtsai@stanford.edu>,"Fri, 11 Apr 2014 14:35:08 -0700",It seems that jenkins for PR is not working,dev@spark.apache.org,"I always got
=========================================================================

Could not find Apache license headers in the following files:
 !????? /root/workspace/SparkPullRequestBuilder/python/metastore/db.lck
 !????? /root/workspace/SparkPullRequestBuilder/python/metastore/service.properties


Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai

"
Mayur Rustagi <mayur.rustagi@gmail.com>,"Fri, 11 Apr 2014 16:58:45 -0700",Re: Building Spark AMI,dev <dev@spark.apache.org>,"I am creating one fully configured & synced one. But you still need to send
over configuration. Do you plan to use chef for that ?

"
Jim Ancona <jim@anconafamily.com>,"Fri, 11 Apr 2014 22:05:56 -0400",Re: Building Spark AMI,dev@spark.apache.org,"Hi,

Right now my use case is setting up a small cluster for
prototyping/evaluation. My hope was that I could use the scripts that
come with Spark to get things up and running quickly. For a production
deploy we would probably roll our own using Puppet.

Jim


"
Nick Pentreath <nick.pentreath@gmail.com>,"Sat, 12 Apr 2014 14:36:58 +0200","Re: Any suggestion about JIRA 1006 ""MLlib ALS gets stack overflow
 with too many iterations""?",dev@spark.apache.org,"I think having the option of seeding the factors from HDFS rather than
random is a good one (well, actually providing additional optional
arguments initialUserFactors and initialItemFactors as RDD[(Int,
Array[Double])])



"
Ye Xianjin <advancedxy@gmail.com>,"Mon, 14 Apr 2014 18:14:45 +0800",Tests failed after assembling the latest code from github,dev@spark.apache.org,"Hi, everyone: 
I am new to Spark development. I download spark's latest code from github. After running sbt/sbt assembly,
I began running  sbt/sbt test in the spark source code dir. But it failed running the repl module test.

Here are some output details.

command:
sbt/sbt ""test-only org.apache.spark.repl.*""
output:

[info] Loading project definition from /Volumes/MacintoshHD/github/spark/project/project
[info] Loading project definition from /Volumes/MacintoshHD/github/spark/project
[info] Set current project to root (in build file:/Volumes/MacintoshHD/github/spark/)
[info] Passed: Total 0, Failed 0, Errors 0, Passed 0
[info] Passed: Total 0, Failed 0, Errors 0, Passed 0
[info] Passed: Total 0, Failed 0, Errors 0, Passed 0
[info] Passed: Total 0, Failed 0, Errors 0, Passed 0
[info] Passed: Total 0, Failed 0, Errors 0, Passed 0
[info] Passed: Total 0, Failed 0, Errors 0, Passed 0
[info] Passed: Total 0, Failed 0, Errors 0, Passed 0
[info] Passed: Total 0, Failed 0, Errors 0, Passed 0
[info] ExecutorClassLoaderSuite:
2014-04-14 16:59:31.247 java[8393:1003] Unable to load realm info from SCDynamicStore
[info] - child first *** FAILED *** (440 milliseconds)
[info]   java.lang.ClassNotFoundException: ReplFakeClass2
[info]   at java.lang.ClassLoader.findClass(ClassLoader.java:364)
[info]   at org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.scala:26)
[info]   at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
[info]   at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
[info]   at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:30)
[info]   at org.apache.spark.repl.ExecutorClassLoader$$anonfun$findClass$1.apply(ExecutorClassLoader.scala:57)
[info]   at org.apache.spark.repl.ExecutorClassLoader$$anonfun$findClass$1.apply(ExecutorClassLoader.scala:57)
[info]   at scala.Option.getOrElse(Option.scala:120)
[info]   at org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:57)
[info]   at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
[info]   at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
[info]   at org.apache.spark.repl.ExecutorClassLoaderSuite$$anonfun$1.apply$mcV$sp(ExecutorClassLoaderSuite.scala:47)
[info]   at org.apache.spark.repl.ExecutorClassLoaderSuite$$anonfun$1.apply(ExecutorClassLoaderSuite.scala:44)
[info]   at org.apache.spark.repl.ExecutorClassLoaderSuite$$anonfun$1.apply(ExecutorClassLoaderSuite.scala:44)
[info]   at org.scalatest.FunSuite$$anon$1.apply(FunSuite.scala:1265)
[info]   at org.scalatest.Suite$class.withFixture(Suite.scala:1974)
[info]   at org.apache.spark.repl.ExecutorClassLoaderSuite.withFixture(ExecutorClassLoaderSuite.scala:30)
[info]   at org.scalatest.FunSuite$class.invokeWithFixture$1(FunSuite.scala:1262)
[info]   at org.scalatest.FunSuite$$anonfun$runTest$1.apply(FunSuite.scala:1271)
[info]   at org.scalatest.FunSuite$$anonfun$runTest$1.apply(FunSuite.scala:1271)
[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:198)
[info]   at org.scalatest.FunSuite$class.runTest(FunSuite.scala:1271)
[info]   at org.apache.spark.repl.ExecutorClassLoaderSuite.runTest(ExecutorClassLoaderSuite.scala:30)
[info]   at org.scalatest.FunSuite$$anonfun$runTests$1.apply(FunSuite.scala:1304)
[info]   at org.scalatest.FunSuite$$anonfun$runTests$1.apply(FunSuite.scala:1304)
[info]   at org.scalatest.SuperEngine$$anonfun$org$scalatest$SuperEngine$$runTestsInBranch$1.apply(Engine.scala:260)
[info]   at org.scalatest.SuperEngine$$anonfun$org$scalatest$SuperEngine$$runTestsInBranch$1.apply(Engine.scala:249)
[info]   at scala.collection.immutable.List.foreach(List.scala:318)
[info]   at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:249)
[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:326)
[info]   at org.scalatest.FunSuite$class.runTests(FunSuite.scala:1304)
[info]   at org.apache.spark.repl.ExecutorClassLoaderSuite.runTests(ExecutorClassLoaderSuite.scala:30)
[info]   at org.scalatest.Suite$class.run(Suite.scala:2303)
[info]   at org.apache.spark.repl.ExecutorClassLoaderSuite.org$scalatest$FunSuite$$super$run(ExecutorClassLoaderSuite.scala:30)
[info]   at org.scalatest.FunSuite$$anonfun$run$1.apply(FunSuite.scala:1310)
[info]   at org.scalatest.FunSuite$$anonfun$run$1.apply(FunSuite.scala:1310)
[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:362)
[info]   at org.scalatest.FunSuite$class.run(FunSuite.scala:1310)
[info]   at org.apache.spark.repl.ExecutorClassLoaderSuite.org$scalatest$BeforeAndAfterAll$$super$run(ExecutorClassLoaderSuite.scala:30)
[info]   at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:213)
[info]   at org.apache.spark.repl.ExecutorClassLoaderSuite.run(ExecutorClassLoaderSuite.scala:30)
[info]   at org.scalatest.tools.ScalaTestFramework$ScalaTestRunner.run(ScalaTestFramework.scala:214)
[info]   at sbt.RunnerWrapper$1.runRunner2(FrameworkWrapper.java:220)
[info]   at sbt.RunnerWrapper$1.execute(FrameworkWrapper.java:233)
[info]   at sbt.ForkMain$Run.runTest(ForkMain.java:243)
[info]   at sbt.ForkMain$Run.runTestSafe(ForkMain.java:214)
[info]   at sbt.ForkMain$Run.runTests(ForkMain.java:190)
[info]   at sbt.ForkMain$Run.run(ForkMain.java:257)
[info]   at sbt.ForkMain.main(ForkMain.java:99)
[info] - parent first *** FAILED *** (59 milliseconds)
[info]   java.lang.ClassNotFoundException: ReplFakeClass1
...
[info]   Cause: java.lang.ClassNotFoundException: ReplFakeClass1
...
[info] - child first can fall back *** FAILED *** (39 milliseconds)
[info]   java.lang.ClassNotFoundException: ReplFakeClass3
...
[info] - child first can fail (46 milliseconds)
[info] ReplSuite:
[info] - propagation of local properties (9 seconds, 353 milliseconds)
[info] - simple foreach with accumulator (7 seconds, 608 milliseconds)
[info] - external vars (5 seconds, 783 milliseconds)
[info] - external classes (4 seconds, 341 milliseconds)
[info] - external functions (4 seconds, 106 milliseconds)
[info] - external functions that access vars (4 seconds, 538 milliseconds)
[info] - broadcast vars (4 seconds, 155 milliseconds)
[info] - interacting with files (3 seconds, 376 milliseconds)
Exception in thread ""Connection manager future execution context-0""


Some output is omitted.

Here are some more information:
ReplFakeClass1.class is in the {spark_source_dir}/repl/ReplFakeClass1.class, same as ReplFakeClass2 and 3.
ReplSuite failed in running test(""local-cluster mode""). The first time running this test throws OOM error. The exception shown in above is a second try
The test(""local-cluster mode"") jvm options are '-Xms512M -Xmx512M' which I see from the corresponding stderr log
I have .sbtconfig file in my home dir.  The content is 
export SBT_OPTS=""-XX:+CMSClassUnloadingEnabled -XX:PermSize=5120M -XX:MaxPermSize=10240M""


The test hung after the test failed in the ReplSuite. I have to Ctr-c to close the test.

Thank you for you advice.



-- 
Ye Xianjin
Sent with Sparrow (http://www.sparrowmailapp.com/?sig)

"
Gary Malouf <malouf.gary@gmail.com>,"Mon, 14 Apr 2014 11:12:20 -0400","Re: Akka problem when using scala command to launch Spark
 applications in the current 0.9.0-SNAPSHOT",dev@spark.incubator.apache.org,"Sorry to dig up an old issue.

We build an assembly against spark-0.9.0-RC3 to run on our Spark cluster on
top of Mesos.  When we upgraded to 0.9.0-RC3 from an earlier master cut
from November, we ran into Akka issues described above.

Is it supported to be able to deploy this jar using the Spark classpath
script and Java?  Putting Scala 2.10.3's libraries on the path seems to
break it at runtime.



"
Michael Armbrust <michael@databricks.com>,"Mon, 14 Apr 2014 10:39:06 -0700",Re: Tests failed after assembling the latest code from github,dev@spark.apache.org,"I believe you may need an assembly jar to run the ReplSuite. ""sbt/sbt
assembly/assembly"".

Michael



"
Ye Xianjin <advancedxy@gmail.com>,"Tue, 15 Apr 2014 03:04:15 +0800",Re: Tests failed after assembling the latest code from github,dev@spark.apache.org,"Thank you for your reply. 

After building the assembly jar, the repl test still failed. The error output is same as I post before. 

-- 
Ye Xianjin
Sent with Sparrow (http://www.sparrowmailapp.com/?sig)





"
Aaron Davidson <ilikerps@gmail.com>,"Mon, 14 Apr 2014 12:14:09 -0700",Re: Tests failed after assembling the latest code from github,dev@spark.apache.org,"This may have something to do with running the tests on a Mac, as there is
a lot of File/URI/URL stuff going on in that test which may just have
happened to work if run on a Linux system (like Jenkins). Note that this
suite was added relatively recently:
https://github.com/apache/spark/pull/217



"
Ye Xianjin <advancedxy@gmail.com>,"Tue, 15 Apr 2014 03:43:27 +0800",Re: Tests failed after assembling the latest code from github,dev@spark.apache.org,"well. This is very strange. 
I looked into ExecutorClassLoaderSuite.scala and ReplSuite.scala and made small changes to ExecutorClassLoaderSuite.scala (mostly output some internal variables). After that, when running repl test, I noticed the ReplSuite  
was tested first and the test result is ok. But the ExecutorClassLoaderSuite test was weird.
Here is the output:
[info] ExecutorClassLoaderSuite:
[error] Uncaught exception when running org.apache.spark.repl.ExecutorClassLoaderSuite: java.lang.OutOfMemoryError: PermGen space
[error] Uncaught exception when running org.apache.spark.repl.ExecutorClassLoaderSuite: java.lang.OutOfMemoryError: PermGen space
Internal error when running tests: java.lang.OutOfMemoryError: PermGen space
Exception in thread ""Thread-3"" java.io.EOFException
at java.io.ObjectInputStream$BlockDataInputStream.peekByte(ObjectInputStream.java:2577)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1297)
at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1685)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1323)
at java.io.ObjectInputStream.readObject(ObjectInputStream.java:349)
at sbt.React.react(ForkTests.scala:116)
at sbt.ForkTests$$anonfun$mainTestTask$1$Acceptor$2$.run(ForkTests.scala:75)
at java.lang.Thread.run(Thread.java:695)


I revert my changes. The test result is same.

 I touched the ReplSuite.scala file (use touch command), the test order is reversed, same as the very beginning. And the output is also the same.(The result in my first post).


-- 
Ye Xianjin
Sent with Sparrow (http://www.sparrowmailapp.com/?sig)





"
Nan Zhu <zhunanmcgill@gmail.com>,"Mon, 14 Apr 2014 16:32:08 -0400",Re: It seems that jenkins for PR is not working,dev@spark.apache.org,"+1â€¦.  

--  
Nan Zhu



=================================================
tore/db.lck
tore/service.properties


"
Ye Xianjin <advancedxy@gmail.com>,"Tue, 15 Apr 2014 13:34:53 +0800",Re: Tests failed after assembling the latest code from github,dev@spark.apache.org,"Hi, I think I have found the cause of the tests failing. 

I have two disks on my laptop. The spark project dir is on an HDD disk while the tempdir created by google.io.Files.createTempDir is the /var/folders/5q/.... ,which is on the system disk, an SSD.
The ExecutorLoaderSuite test uses org.apache.spark.TestUtils.createdCompiledClass methods.
The createCompiledClass method first generates the compiled class in the pwd(spark/repl), thens use renameTo to move
the file. The renameTo method fails because the dest file is in a different filesystem than the source file.

I modify the TestUtils.scala to first copy the file to dest then delete the original file. The tests go smoothly.
Should I issue an jira about this problem? Then I can send a pr on Github.

-- 
Ye Xianjin
Sent with Sparrow (http://www.sparrowmailapp.com/?sig)




"
Aaron Davidson <ilikerps@gmail.com>,"Mon, 14 Apr 2014 23:36:00 -0700",Re: Tests failed after assembling the latest code from github,dev@spark.apache.org,"By all means, it would be greatly appreciated!



"
Sean Owen <sowen@cloudera.com>,"Tue, 15 Apr 2014 07:41:34 +0100",Re: Tests failed after assembling the latest code from github,dev@spark.apache.org,"Good call -- indeed that same Files class has a move() method that
will try to use renameTo() and then fall back to copy() and delete()
if needed for this very reason.



"
Ye Xianjin <advancedxy@gmail.com>,"Tue, 15 Apr 2014 14:51:21 +0800",Re: Tests failed after assembling the latest code from github,dev@spark.apache.org,"@Sean Owen, Thanks for your advice.
 There are still some failing tests on my laptop. I will work on this issue(file move) as soon as I figure out other test related issues.


-- 
Ye Xianjin
Sent with Sparrow (http://www.sparrowmailapp.com/?sig)





"
Mark Hamstra <mark@clearstorydata.com>,"Tue, 15 Apr 2014 09:34:41 -0700",Re: It seems that jenkins for PR is not working,dev@spark.apache.org,"The RAT path issue is now fixed, but it appears to me that some recent
change has dramatically altered the behavior of the testing framework, so
that I am now seeing many individual tests taking more than a minute to run
and the complete test run taking a very, very long time.  I expect that
this is what is causing Jenkins to now timeout repeatedly.



"
Patrick Wendell <pwendell@gmail.com>,"Tue, 15 Apr 2014 22:14:37 -0700",Re: It seems that jenkins for PR is not working,"""dev@spark.apache.org"" <dev@spark.apache.org>","There are a few things going on here wrt tests.

1. I fixed up the RAT issues with a hotfix.

2. The Hive tests were actually disabled for a while accidentally. A recent
fix correctly re-enabled them. Without Hive Spark tests run in about 40
minutes and with Hive it runs in 1 hour and 15 minutes, so it's a big
difference.

To ease things I committed a patch today that only runs the Hive tests if
the change touches Spark SQL. So this should make it simpler for normal
tests.

We can actually generalize this to do much finer grained testing, e.g. if
something in MLLib changes we don't need to re-run the streaming tests.
I've added this JIRA to track it:
https://issues.apache.org/jira/browse/SPARK-1455

3. Overall we've experienced more race conditions with tests recently. I
noticed a few zombie test processes on Jenkins hogging up 100% of CPU so I
think this has triggered several previously unseen races due to CPU
contention on the test cluster. I killed them and we'll see if they crop up
again.

4. Please try to keep an eye on the length of new tests that get committed.
It's common to see people commit tests that e.g. sleep for several seconds
or do things that take a long time. Almost always this can be avoided and
usually avoiding it makes the test cleaner anyways (e.g. use proper
synchronization instead of sleeping).

- Patrick



"
Patrick Wendell <pwendell@gmail.com>,"Wed, 16 Apr 2014 10:28:18 -0700","Re: [jira] [Commented] (SPARK-1496) SparkContext.jarOfClass should
 return Option instead of a sequence",dev@spark.apache.org,"Just option [string] is fine. Happy to accept a fix but I'll probably
submit one tonight if no one else has.

---
sent from my phone

"
Dmitriy Lyubimov <dlieu.7@gmail.com>,"Thu, 17 Apr 2014 12:03:57 -0700",Double lhbase dependency in spark 0.9.1,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Not sure if I am seeing double.

SparkBuild.scala for 0.9.1 has dobule hbase declaration

      ""org.apache.hbase""     %  ""hbase""           % ""0.94.6""
excludeAll(excludeNetty, excludeAsm),
      ""org.apache.hbase"" % ""hbase"" % HBASE_VERSION excludeAll(excludeNetty,
excludeAsm),


as a result i am not getting the right version of hbase here. Perhaps the
old declaration crept in during a merge at some point?

-d
"
Sean Owen <sowen@cloudera.com>,"Thu, 17 Apr 2014 20:08:58 +0100",Re: Double lhbase dependency in spark 0.9.1,dev@spark.apache.org,"I remember that too, and it has been fixed already in master, but
maybe it was not included in 0.9.1:

https://github.com/apache/spark/blob/master/project/SparkBuild.scala#L367
--
Sean Owen | Director, Data Science | London



"
Sean Owen <sowen@cloudera.com>,"Thu, 17 Apr 2014 20:08:58 +0100",Re: Double lhbase dependency in spark 0.9.1,dev@spark.apache.org,"I remember that too, and it has been fixed already in master, but
maybe it was not included in 0.9.1:

https://github.com/apache/spark/blob/master/project/SparkBuild.scala#L367
--
Sean Owen | Director, Data Science | London



"
Zhan Zhang <zhazhan@gmail.com>,"Thu, 17 Apr 2014 13:58:34 -0700 (PDT)",Spark REPL question,dev@spark.incubator.apache.org,"Please help, I am knew to both Spark and scala. 

I am trying to figure out how spark distribute the task to workers in REPL.
I only found the place where task is serialized and sent, and workers
deserialize and load the task with the class name by ExecutorClassLoader.
But I didn't find how the driver uploaded the REPL generated .class/jar file
by REPL to file server/hdfs. My understanding is that the worker has to know
the class as well to instantiate the task.

Does anybody know where the code is (file or function name) or my
undertanding is wrong?

Thanks.



--

"
Michael Armbrust <michael@databricks.com>,"Thu, 17 Apr 2014 14:33:49 -0700",Re: Spark REPL question,dev@spark.apache.org,"The REPL spins up an org.apache.spark.HttpServer, which provides classes
that are generated by the REPL as well as jars from addJar.

Michael



"
Michael Armbrust <michael@databricks.com>,"Thu, 17 Apr 2014 14:33:49 -0700",Re: Spark REPL question,dev@spark.apache.org,"The REPL spins up an org.apache.spark.HttpServer, which provides classes
that are generated by the REPL as well as jars from addJar.

Michael



"
Zhan Zhang <zhazhan@gmail.com>,"Thu, 17 Apr 2014 14:47:21 -0700 (PDT)",Re: Spark REPL question,dev@spark.incubator.apache.org,"Thanks a lot.

By ""spins up"", do you mean using the same directory, specified by following?

      /** Local directory to save .class files too */
      val outputDir = {
        val tmp = System.getProperty(""java.io.tmpdir"")
        val rootDir = new SparkConf().get(""spark.repl.classdir"",  tmp)
        Utils.createTempDir(rootDir)
      }
    val virtualDirectory                              = new
PlainFile(outputDir) // ""directory"" for classfiles
    val classServer                                   = new
HttpServer(outputDir)     /** Jetty server that will serve our classes to
worker nodes */



--

"
Michael Armbrust <michael@databricks.com>,"Thu, 17 Apr 2014 14:53:05 -0700",Re: Spark REPL question,dev@spark.apache.org,"Yeah, I think that is correct.



"
Michael Armbrust <michael@databricks.com>,"Thu, 17 Apr 2014 14:53:05 -0700",Re: Spark REPL question,dev@spark.apache.org,"Yeah, I think that is correct.



"
Zhan Zhang <zhazhan@gmail.com>,"Thu, 17 Apr 2014 15:04:12 -0700 (PDT)",Re: Spark REPL question,dev@spark.incubator.apache.org,"Clear to me now.

Thanks.



--

"
Tathagata Das <tathagata.das1565@gmail.com>,"Thu, 17 Apr 2014 16:34:14 -0700",Re: Double lhbase dependency in spark 0.9.1,dev@spark.apache.org,"Aaah, this should have been ported to Spark 0.9.1!

TD



"
Tathagata Das <tathagata.das1565@gmail.com>,"Thu, 17 Apr 2014 16:34:14 -0700",Re: Double lhbase dependency in spark 0.9.1,dev@spark.apache.org,"Aaah, this should have been ported to Spark 0.9.1!

TD



"
Marcelo Vanzin <vanzin@cloudera.com>,"Fri, 18 Apr 2014 16:49:40 -0700",SparkListener questions,dev@spark.apache.org,"Hello all,

I'm currently taking a look at how to hook Spark with the Application
Timeline Server (ATS) work going on in Yarn (YARN-1530). I've got a
reasonable idea of how the Yarn part works, and the basic idea of what
needs to be done in Spark, but I've run into a couple of issues with
the current listener framework and I'd like to ask for some guidance.

(i) SparkContext.addSparkListener() may miss events

Notably, it's not possible to add a listener to catch the initial
SparkListenerEnvironmentUpdate and SparkListenerApplicationStart
events; the second one is particularly important for something like an
event logger. The current EventLoggingListener gets around that by
being initialized by SparkContext itself, but for other listeners,
that option isn't available (and that doesn't seem like a scalable
solution either).

My initial idea was to add a ""listeners"" argument to the ""big""
SparkContext constructor, but I'm open to different suggestions, since
I kinda dislike when constructors start growing too much. (It current
has 6 arguments, some of which have default values.) A builder-like
pattern could be an option (e.g.
SparkContext.newBuilder().setConf(...).addJars(...).addListener(...).build()).


(ii) Posting things to the ATS requires an ID.

If you look at the TimelineEntity class in Yarn, it requires both a
type (which would be something like ""SparkApplication"" for Spark) and
an ID. A SparkContext currently has no concept of an application ID (I
don't count name as an ID).

Using a random UUID is possible, but I think is ugly.

EventLoggingListener uses app name + System.currentTimeMillis, which
is better from a user-friendliness p.o.v. (if you ignore the
possibility of clashes).

But really my preferred solution here would be to use the Yarn
application id. That would make it easy to correlate this data with
more generic data kept by Yarn (see YARN-321).

The problem here is that we don't know this ID until way after the
SparkContext is created. I can see two different ways to solve this
issue.

A more hackish way would be expose the listeners to the Yarn code, so
that it can then find the Yarn-specific listener and trigger some
action to update the application id.

A more generic way would be to allow arbitrary events to be posted to
the bus, not just the ones declared in SparkListener.scala. This way,
the Yarn code could publish a ""SparkYarnApplicationStarted"" event that
has the information the listener wants, and other listeners could
potentially use that information too.

How do you guys feel about the latter?


Feedback here is greatly appreciated! :-)

-- 
Marcelo

"
Debasish Das <debasish.das83@gmail.com>,"Sat, 19 Apr 2014 16:45:23 -0700",Publish spark jars to artifactory,dev@spark.apache.org,"Hi,

I saw in the code that spark jars are published on sonatype but I was
wondering if you guys have published spark jars to artifactory
as...Cloudera uses artifactory...

Somehow I can publish maven projects to artifactory but after following the
sbt link:

http://www.scala-sbt.org/release/docs/Detailed-Topics/Publishing.html

And trying various things, still I have not been able to publish the jars
on artifactory...

Any github example that publishes sbt project to artifactory would be
really helpful...

Thanks.
Deb
"
qingyang li <liqingyang1985@gmail.com>,"Sun, 20 Apr 2014 18:45:08 +0800",does shark0.9.1 work well with hadoop2.2.0 ?,dev@spark.apache.org,"shark 0.9.1 is using protobuf 2.4.1 , but hadoop2.2.0 is using
protobuf2.5.0,
how can we make them work together?
I have tried replace protobuf2.4.1 in shark with protobuf2.5.0, it does not
work.
I have also tried replacing protobuf2.5.0 in hadoop with shark's 2.4.1, it
does not work too.
"
Ted Yu <yuzhihong@gmail.com>,"Sun, 20 Apr 2014 08:03:11 -0700",Re: does shark0.9.1 work well with hadoop2.2.0 ?,dev@spark.apache.org,"bq. I have tried replace protobuf2.4.1 in shark with protobuf2.5.0

Did you replace the jar file or did you change the following in pom.xml and
rebuild ?
    <protobuf.version>2.4.1</protobuf.version>

Cheers



"
Gordon Wang <gwang@gopivotal.com>,"Sun, 20 Apr 2014 23:53:09 +0800",Re: does shark0.9.1 work well with hadoop2.2.0 ?,dev@spark.apache.org,"replacing the jar is not enough.
You have to change protobuf dependency in shark's build script. and
recompile the source.

Protobuf 2.4.1 and 2.5.0 is not binary compatible.






-- 
Regards
Gordon Wang
"
Sandy Ryza <sandy.ryza@cloudera.com>,"Sun, 20 Apr 2014 11:16:47 -0700",all values for a key must fit in memory,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey all,

After a shuffle / groupByKey, Hadoop MapReduce allows the values for a key
to not all fit in memory.  The current ShuffleFetcher.fetch API, which
doesn't distinguish between keys and values, only returning an Iterator[P],
seems incompatible with this.

Any thoughts on how we could achieve parity here?

-Sandy
"
Mridul Muralidharan <mridul@gmail.com>,"Mon, 21 Apr 2014 00:06:53 +0530",Re: all values for a key must fit in memory,dev@spark.apache.org,"An iterator does not imply data has to be memory resident.
Think merge sort output as an iterator (disk backed).

Tom is actually planning to work on something similar with me on this
hopefully this or next month.

Regards,
Mridul



"
Sandy Ryza <sandy.ryza@cloudera.com>,"Sun, 20 Apr 2014 17:55:43 -0700",Re: all values for a key must fit in memory,"""dev@spark.apache.org"" <dev@spark.apache.org>","The issue isn't that the Iterator[P] can't be disk-backed.  It's that, with
a groupBy, each P is a (Key, Values) tuple, and the entire tuple is read
into memory at once.  The ShuffledRDD is agnostic to what goes inside P.


"
Matei Zaharia <matei.zaharia@gmail.com>,"Sun, 20 Apr 2014 20:13:29 -0700",Re: all values for a key must fit in memory,dev@spark.apache.org,"We’ve updated the user-facing API of groupBy in 1.0 to allow this: https://issues.apache.org/jira/browse/SPARK-1271. The ShuffleFetcher API is internal to Spark, it doesn’t really matter what it is because we can change it. But the problem before was that groupBy and cogroup were defined as returning (Key, Seq[Value]). Now they return (Key, Iterable[Value]), which will allow us to make the internal changes to allow spilling to disk within a key. This will happen after 1.0 though, but it will be doable without any changes to user programs.

Matei


with
read
P.
<sandy.ryza@cloudera.com>
a
which


"
Patrick Wendell <pwendell@gmail.com>,"Sun, 20 Apr 2014 20:38:15 -0700",Re: all values for a key must fit in memory,"""dev@spark.apache.org"" <dev@spark.apache.org>","Just wanted to mention - one common thing I've seen users do is use
groupByKey, then do something that is commutitive and associative once the
values are grouped. Really users here should be doing reduceByKey.

rdd.groupByKey().map{ case (key, values) => (key, values.sum))
rdd.reduceByKey(_ + _)

I've seen this happen particularly for users coming from MapReduce where
they are used to having to write their own combiners and it's not intuitive
that these functions are very different.

Sandy - have you heard from users who have a specific problems they can't
solve using an associative function? I'm sure they exist, but I wonder how
often it's this vs. they just don't understand they API.

I wonder if we should actually warn about this in the groupByKey
documentation.

- Patrick



"
Mridul Muralidharan <mridul@gmail.com>,"Mon, 21 Apr 2014 13:01:27 +0530",Re: all values for a key must fit in memory,dev@spark.apache.org,"As Matei mentioned, the Values is now an Iterable : which can be disk backed.
Does that not address the concern ?

@Patrick - we do have cases where the length of the sequence is large
and size per value is also non trivial : so we do need this :-)
Note that join is a trivial example where this is required (in our
current implementation).

Regards,
Mridul


"
Sandy Ryza <sandy.ryza@cloudera.com>,"Mon, 21 Apr 2014 02:49:52 -0700",Re: all values for a key must fit in memory,"""dev@spark.apache.org"" <dev@spark.apache.org>","Thanks Matei and Mridul - was basically wondering whether we would be able
to change the shuffle to accommodate this after 1.0, and from your answers
it sounds like we can.



"
Aliaksei Litouka <aliaksei.litouka@gmail.com>,"Mon, 21 Apr 2014 10:39:35 -0500",Any plans for new clustering algorithms?,dev@spark.apache.org,"Hi, Spark developers.
Are there any plans for implementing new clustering algorithms in MLLib? As
far as I understand, current version of Spark ships with only one
clustering algorithm - K-Means. I want to contribute to Spark and I'm
thinking of adding more clustering algorithms - maybe
DBSCAN<http://en.wikipedia.org/wiki/DBSCAN>.
I can start working on it. Does anyone want to join me?
"
Sean Owen <sowen@cloudera.com>,"Mon, 21 Apr 2014 16:58:59 +0100",Re: Any plans for new clustering algorithms?,dev@spark.apache.org,"Nobody asked me, and this is a comment on a broader question, not this
one, but:

In light of a number of recent items about adding more algorithms,
I'll say that I personally think an explosion of algorithms should
come after the MLlib ""core"" is more fully baked. I'm thinking of
finishing out the changes to vectors and matrices, for example. Things
are going to change significantly in the short term as people use the
algorithms and see how well the abstractions do or don't work. I've
seen another similar project suffer mightily from too many algorithms
too early, so maybe I'm just paranoid.

Anyway, long-term, I think lots of good algorithms is a right and
proper goal for MLlib, myself. Consistent approaches, representations
and APIs will make or break MLlib much more than having or not having
a particular algorithm. With the plumbing in place, writing the algo
is the fun easy part.
--
Sean Owen | Director, Data Science | London



"
"""Evan R. Sparks"" <evan.sparks@gmail.com>","Mon, 21 Apr 2014 09:07:45 -0700",Re: Any plans for new clustering algorithms?,dev@spark.apache.org,"While DBSCAN and others would be welcome contributions, I couldn't agree
more with Sean.





"
Sang Venkatraman <sang.venkatraman@gmail.com>,"Mon, 21 Apr 2014 12:32:30 -0400",Re: Any plans for new clustering algorithms?,dev@spark.apache.org,"Hi,

are there plans on reusing or porting over parts of apache mahout.

Thanks,
Sang



"
Nick Pentreath <nick.pentreath@gmail.com>,"Mon, 21 Apr 2014 18:40:04 +0200",Re: Any plans for new clustering algorithms?,dev@spark.apache.org,"I am very much +1 on Sean's comment.

I think the correct abstractions and API for Vectors, Matrices and
distributed matrices (distributed row matrix etc) will, once bedded down
and battle tested in the wild, allow a whole lot of flexibility for
developers of algorithms on top of MLlib core.

This is true whether the algorithm finds itself in MLlib, MLBase, or
resides in a separate contrib project. Just like Spark core sometimes risks
becoming ""trying to please everybody"" by having the kitchen sink in terms
of Hadoop integration aspects or RDD operations, and thus a spark-contrib
project may make a lot of sense. So too could ml-contrib hold a lot of
algorithms that are not core but still of wide interest. This can include,
for example, models that are still cutting edge and perhaps not as widely
used in production yet, or specialist models that are of interest to a more
niche group.

scikit-learn is very tough about this, requiring a very high bar for
including a new algorithm (many citations, dev support, proof of strong
performance and wide demand). And this leads to a very high quality code
base in general.

I'd say we should (if it hasn't been done already, I may have missed such a
discussion), decide precisely what does constitute MLlib's ""1.0.0"" goals
for algorithms. I'd say what we have in terms of clustering (K-Means||),
linear models, decision trees and collaborative filtering is pretty much a
good goal. Potentially the Random Forest implementation on top of the DT,
and perhaps another form of recommendation model (such as the co-occurrence
models cf. Mahout's) could be potential candidates for inclusion. I'd also
say any other optimization methods/procedures in addition to SGD and LBFGS
that are very strong and widely used for a variety of (distributed) ML
problems, could be candidates. And finally things like useful utils,
cross-validation and evaluation methods, etc.

So I'd say by all means, please work on a new model such as DBSCAN. Put it
in a new GitHub project, post some detailed performance comparisons vs
MLlib K-Means, and then in future if it gets included in MLlib core it's a
pretty easy to do.



"
Aliaksei Litouka <aliaksei.litouka@gmail.com>,"Mon, 21 Apr 2014 11:56:25 -0500",Re: Any plans for new clustering algorithms?,dev@spark.apache.org,"Thank you very much for detailed answers.
I can't but agree that a good MLLib core is a higher priority than
algorithms built on top of it. I'll check if I can contribute anything to
the core. I will also follow Nick Pentreath's recommendation to start a new
GitHub project. Actually, here is a link to repository:
https://github.com/alitouka/spark_dbscan . Currently it is empty - I've
just created it :)


2014-04-21 11:40 GMT-05:00 Nick Pentreath <nick.pentreath@gmail.com>:

"
Paul Brown <prb@mult.ifario.us>,"Mon, 21 Apr 2014 10:03:55 -0700",Re: Any plans for new clustering algorithms?,dev@spark.apache.org,"I agree that it will be good to see more algorithms added to the MLlib
universe, although this does bring to mind a couple of comments:

- MLlib as Mahout.next would be a unfortunate.  There are some gems in
Mahout, but there are also lots of rocks.  Setting a minimal bar of
working, correctly implemented, and documented requires a surprising amount
of work.

- Not getting any signal out of your data with an algorithm like K-means
implies one of the following: (1) there is no signal in your data, (2) you
should try tuning the algorithm differently, (3) you're using K-means
wrong, (4) you should try preparing the data differently, (5) all of the
above, or (6) none of the above.

My $0.02.
-- Paul


â€”
prb@mult.ifario.us | Multifarious, Inc. | http://mult.ifario.us/



?
"
Sean Owen <sowen@cloudera.com>,"Mon, 21 Apr 2014 18:23:04 +0100",Re: Any plans for new clustering algorithms?,dev@spark.apache.org,"
As someone with first-hand knowledge, this is correct. To Sang's
question, I can't see value in 'porting' Mahout since it is based on a
quite different paradigm. About the only part that translates is the
algorithm concept itself.

This is also the cautionary tale. The contents of the project have
ended up being a number of ""drive-by"" contributions of implementations
that, while individually perhaps brilliant (perhaps), didn't
necessarily match any other implementation in structure, input/output,
libraries used. The implementations were often a touch academic. The
result was hard to document, maintain, evolve or use.

Far more of the structure of the MLlib implementations are consistent
by virtue of being built around Spark core already. That's great.

implementations. To me, the existing implementations are almost
exactly the basics I would choose. They cover the bases and will
exercise the abstractions and structure. So that's also great IMHO.

"
Xiangrui Meng <mengxr@gmail.com>,"Mon, 21 Apr 2014 10:54:25 -0700",Re: Any plans for new clustering algorithms?,dev@spark.apache.org,"+1 on Sean's comment. MLlib covers the basic algorithms but we
definitely need to spend more time on how to make the design scalable.
For example, think about current ""ProblemWithAlgorithm"" naming scheme.
That being said, new algorithms are welcomed. I wi"
Sandy Ryza <sandy.ryza@cloudera.com>,"Mon, 21 Apr 2014 10:59:24 -0700",Re: Any plans for new clustering algorithms?,"""dev@spark.apache.org"" <dev@spark.apache.org>","If it's not done already, would it make sense to codify this philosophy
somewhere?  I imagine this won't be the first time this discussion comes
up, and it would be nice to have a doc to point to.  I'd be happy to take a
stab at this.



"
Nick Pentreath <nick.pentreath@gmail.com>,"Mon, 21 Apr 2014 22:41:36 +0200",Re: Any plans for new clustering algorithms?,"""dev@spark.apache.org"" <dev@spark.apache.org>","I'd say a section in the ""how to contribute"" page would be a good place to put this.

In general I'd say that the criteria for inclusion of an algorithm is it should be high quality, widely known, used and accepted (citations and concrete use cases as examples of this), scalable and parallelizable, well documented and with reasonable expectation of dev support

Sent from my iPhone





"
Xiangrui Meng <mengxr@gmail.com>,"Mon, 21 Apr 2014 15:19:31 -0700",Re: Any plans for new clustering algorithms?,dev@spark.apache.org,"Cannot agree more with your words. Could you add one section about
""how and what to contribute"" to MLlib's guide? -Xiangrui

o put this.
should be high quality, widely known, used and accepted (citations and concrete use cases as examples of this), scalable and parallelizable, well documented and with reasonable expectation of dev support
e a
e:
:
e:
n

"
Sandy Ryza <sandy.ryza@cloudera.com>,"Mon, 21 Apr 2014 18:01:51 -0700",Re: Any plans for new clustering algorithms?,"""dev@spark.apache.org"" <dev@spark.apache.org>","How do I get permissions to edit the wiki?



"
Xiangrui Meng <mengxr@gmail.com>,"Mon, 21 Apr 2014 18:09:54 -0700",Re: Any plans for new clustering algorithms?,dev@spark.apache.org,"The markdown files are under spark/docs. You can submit a PR for
changes. -Xiangrui


"
Sandy Ryza <sandy.ryza@cloudera.com>,"Mon, 21 Apr 2014 18:14:52 -0700",Re: Any plans for new clustering algorithms?,"""dev@spark.apache.org"" <dev@spark.apache.org>","I thought this might be a good thing to add to the wiki's ""How to
contribute"" page<https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark>,
as it's not tied to a release.



"
Nan Zhu <zhunanmcgill@gmail.com>,"Mon, 21 Apr 2014 21:22:58 -0400",Re: Any plans for new clustering algorithms?,dev@spark.apache.org,"I thought those are files of spark.apache.org? 

-- 
Nan Zhu





"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 21 Apr 2014 19:25:10 -0700",Re: Any plans for new clustering algorithms?,dev@spark.apache.org,"The wiki is actually maintained separately in https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage. We restricted editing of the wiki because bots would automatically add stuff. I’ve given you permissions now.

Matei


place
is it
and
parallelizable, well
philosophy
comes
to
<mengxr@gmail.com (mailto:mengxr@gmail.com)>
scalable.
scheme.
but
been
Collapsed
lack of
and
go.
(mailto:sowen@cloudera.com)>
<prb@mult.ifario.us (mailto:prb@mult.ifario.us)>
gems
bar of
surprising
Sang's
based on a
is the
have
implementations
input/output,
academic. The
consistent
great.
building any
will
IMHO.


"
prabeesh k <prabsmails@gmail.com>,"Tue, 22 Apr 2014 11:29:00 +0530",Link not working,dev <dev@spark.apache.org>,"For Spark-0.8.0, the download links are not working.

Please update the same

Regarding,
prabeesh
"
qingyang li <liqingyang1985@gmail.com>,"Tue, 22 Apr 2014 21:59:19 +0800",Re: does shark0.9.1 work well with hadoop2.2.0 ?,dev@spark.apache.org,"this can help resolve protobuf version problem, too.
https://groups.google.com/forum/#!msg/shark-users/0pGIVQvaYfo/-43oaK8scNAJ


2014-04-20 23:53 GMT+08:00 Gordon Wang <gwang@gopivotal.com>:

"
Sandy Ryza <sandy.ryza@cloudera.com>,"Tue, 22 Apr 2014 10:11:53 -0700",Re: Any plans for new clustering algorithms?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Thanks Matei.  I added a section ""How to contribute"" page.



"
Ajay Nair <prodigyaj@gmail.com>,"Wed, 23 Apr 2014 00:03:31 -0500",Spark on wikipedia dataset,dev@spark.apache.org,"I am going to perform some test experiments on the wikipedia dataset using
the spark framework. I know wikipedia data set might already have been
analyzed, but what are the potential explored/unexplored aspects of spark
that can be tested and benchmarked on wikipedia dataset?

Thanks
AJ
"
Andy Konwinski <andykonwinski@gmail.com>,"Tue, 22 Apr 2014 22:37:21 -0700",Re: Link not working,dev@spark.apache.org,"Should be fixed now, thanks for reporting this!

Andy



"
DB Tsai <dbtsai@dbtsai.com>,"Tue, 22 Apr 2014 23:26:01 -0700",Jekyll documentation generation error,dev@spark.apache.org,"Hi guys,

I'm trying to update LBFGS documentation so I need to generate html
document to see if everything looks great. However, mv I get the following
error.

Conversion error: There was an error converting 'docs/cluster-overview.md'.
error: MaRuKu encountered problem(s) while converting your markup.. Use
--trace to view backtrace


I'm using ruby 2.1.1p76 and jekyll 1.5.1 installed by gem as instruction.
Do I miss anything?

Also, in the instruction, if we want to skip the api docs, we can do the
following.

# Skip generating API docs (which takes a while)
$ SKIP_SCALADOC=1 jekyll build

But what does""SKIP_SCALADOC=1"" mean? export SKIP_SCALADOC=1?

Thanks.




Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai
"
DB Tsai <dbtsai@dbtsai.com>,"Tue, 22 Apr 2014 23:31:56 -0700",Re: Jekyll documentation generation error,dev@spark.apache.org,"This is the trace.

  Conversion error: There was an error converting 'docs/cluster-overview.md
'.

/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/lib/jekyll/converters/markdown/maruku_parser.rb:45:in
`print_errors_and_fail': MaRuKu encountered problem(s) while converting
your markup. (MaRuKu::Exception)

from
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/lib/jekyll/converters/markdown/maruku_parser.rb:50:in
`convert'

from
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/lib/jekyll/converters/markdown.rb:39:in
`convert'

from
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/lib/jekyll/convertible.rb:57:in
`transform'

from
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/lib/jekyll/convertible.rb:153:in
`do_layout'

from
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/lib/jekyll/page.rb:115:in
`render'

from
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/lib/jekyll/site.rb:239:in
`block in render'

from
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/lib/jekyll/site.rb:238:in
`each'

from
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/lib/jekyll/site.rb:238:in
`render'

from
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/lib/jekyll/site.rb:39:in
`process'

from
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/lib/jekyll/command.rb:18:in
`process_site'

from
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/lib/jekyll/commands/build.rb:23:in
`build'

from
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/lib/jekyll/commands/build.rb:7:in
`process'

from
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/bin/jekyll:77:in
`block (2 levels) in <top (required)>'

from
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/commander-4.1.6/lib/commander/command.rb:180:in
`call'

from
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/commander-4.1.6/lib/commander/command.rb:180:in
`call'

from
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/commander-4.1.6/lib/commander/command.rb:155:in
`run'

from
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/commander-4.1.6/lib/commander/runner.rb:422:in
`run_active_command'

from
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/commander-4.1.6/lib/commander/runner.rb:82:in
`run!'

from
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/commander-4.1.6/lib/commander/delegates.rb:8:in
`run!'

from
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/commander-4.1.6/lib/commander/import.rb:10:in
`block in <top (required)>'


Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



"
Matei Zaharia <matei.zaharia@gmail.com>,"Tue, 22 Apr 2014 23:38:34 -0700",Re: Jekyll documentation generation error,"dev@spark.apache.org,
 dbtsai@dbtsai.com","Try doing “gem install kramdown”. The maruku gem for Markdown throws these errors, but Kramdown doesn’t.

Matei


'docs/cluster-overview.md
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/lib/jekyll/converters/markdown/maruku_parser.rb:45:in
converting
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/lib/jekyll/converters/markdown/maruku_parser.rb:50:in
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/lib/jekyll/converters/markdown.rb:39:in
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/lib/jekyll/convertible.rb:57:in
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/lib/jekyll/convertible.rb:153:in
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/lib/jekyll/page.rb:115:in
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/lib/jekyll/site.rb:239:in
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/lib/jekyll/site.rb:238:in
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/lib/jekyll/site.rb:238:in
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/lib/jekyll/site.rb:39:in
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/lib/jekyll/command.rb:18:in
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/lib/jekyll/commands/build.rb:23:in
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/lib/jekyll/commands/build.rb:7:in
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/bin/jekyll:77:in
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/commander-4.1.6/lib/commander/command.rb:180:in
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/commander-4.1.6/lib/commander/command.rb:180:in
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/commander-4.1.6/lib/commander/command.rb:155:in
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/commander-4.1.6/lib/commander/runner.rb:422:in
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/commander-4.1.6/lib/commander/runner.rb:82:in
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/commander-4.1.6/lib/commander/delegates.rb:8:in
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/commander-4.1.6/lib/commander/import.rb:10:in
following
'docs/cluster-overview.md
Use
instruction.
the


"
DB Tsai <dbtsai@dbtsai.com>,"Tue, 22 Apr 2014 23:44:14 -0700",Re: Jekyll documentation generation error,dev@spark.apache.org,"Matei, thanks. It works with kramdown.


Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai


ote:

down throws these
/lib/jekyll/converters/markdown/maruku_parser.rb:45:in
/lib/jekyll/converters/markdown/maruku_parser.rb:50:in
/lib/jekyll/converters/markdown.rb:39:in
/lib/jekyll/convertible.rb:57:in
/lib/jekyll/convertible.rb:153:in
/lib/jekyll/page.rb:115:in
/lib/jekyll/site.rb:239:in
/lib/jekyll/site.rb:238:in
/lib/jekyll/site.rb:238:in
/lib/jekyll/site.rb:39:in
/lib/jekyll/command.rb:18:in
/lib/jekyll/commands/build.rb:23:in
/lib/jekyll/commands/build.rb:7:in
/bin/jekyll:77:in
1.6/lib/commander/command.rb:180:in
1.6/lib/commander/command.rb:180:in
1.6/lib/commander/command.rb:155:in
1.6/lib/commander/runner.rb:422:in
1.6/lib/commander/runner.rb:82:in
1.6/lib/commander/delegates.rb:8:in
1.6/lib/commander/import.rb:10:in
e
he
"
"""Saumitra Shahapure (Vizury)"" <saumitra.shahapure@vizury.com>","Wed, 23 Apr 2014 12:36:05 +0530",Sharing RDDs,dev@spark.apache.org,"Hello,

Is it possible in spark to reuse cached RDDs generated in earlier run?

Specifically, I am trying to have a setup where first scala script
generates cached RDDs. If another scala script tries to perform same
operations on same dataset, it should be able to get results from cache
generated in earlier run.

Is there any direct/indirect way to do this?

--
Regards,
Saumitra Shahapure
"
Mayur Rustagi <mayur.rustagi@gmail.com>,"Wed, 23 Apr 2014 13:33:25 +0530",Re: Spark on wikipedia dataset,dev <dev@spark.apache.org>,"Huge joins would be interesting. I do all my demos on wikipedia dataset for
Shark. Joins are typical pain to showcase & show off :)

Mayur Rustagi
Ph: +1 (760) 203 3257
http://www.sigmoidanalytics.com
@mayur_rustagi <https://twitter.com/mayur_rustagi>




"
qingyang li <liqingyang1985@gmail.com>,"Wed, 23 Apr 2014 17:19:20 +0800",get -101 error code when running select query,dev@spark.apache.org,"hi,  i have started one sharkserver2 ,  and using java code to send query
to this server by hive jdbc,  but i got such error:
------
FAILED: Execution Error, return code -101 from shark.execution.SparkTask
org.apache.hive.service.cli.HiveSQLException: Error while processing
statement: FAILED: Execution Error, return code -101 from
shark.execution.SparkTask
        at shark.server.SharkSQLOperation.run(SharkSQLOperation.scala:45)
        at
org.apache.hive.service.cli.session.HiveSessionImpl.executeStatement(HiveSessionImpl.java:180)
        at
org.apache.hive.service.cli.CLIService.executeStatement(CLIService.java:152)
        at
org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:203)
        at
org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1133)
        at
org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1118)
        at
org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
        at
org.apache.hive.service.auth.TUGIContainingProcessor$2.run(TUGIContainingProcessor.java:64)
        at
org.apache.hive.service.auth.TUGIContainingProcessor$2.run(TUGIContainingProcessor.java:61)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at
org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
        at
org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:524)
        at
org.apache.hive.service.auth.TUGIContainingProcessor.process(TUGIContainingProcessor.java:61)
        at
org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)
        at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)

-------
do anyone encounter this problem?
"
Mridul Muralidharan <mridul@gmail.com>,"Wed, 23 Apr 2014 18:37:21 +0530","Re: [jira] [Commented] (SPARK-1576) Passing of JAVA_OPTS to YARN on
 command line",dev@spark.apache.org,"This breaks all existing jobs which are not using spark-submit.
The consensus was not to break compatibility unless there was an overriding
reason to do so

"
Tom Graves <tgraves_cs@yahoo.com>,"Wed, 23 Apr 2014 08:51:07 -0700 (PDT)",Re: [jira] [Commented] (SPARK-1576) Passing of JAVA_OPTS to YARN on command line,"""dev@spark.apache.org"" <dev@spark.apache.org>","can you be more specific?  What breaks existing jobs?  If you are referring to my comment,  SPARK_JAVA_OPTS still works but I think the intent ijobs which are not using spark-submit.
The consensus was not to break com     [
> https://issues.apache.org/jira/browse/SPARK-1576?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13978164#comment-13978164]
>
> Thomas Graves commented on SPARK-1576:
> --------------------------------------
>
> Is this meant for the driver or the executors?  The spark-submit script
> has a command line option for the driver:  --driver-java-options.
> I believe the intent of https://github.com/apache/spark/pull/299 was to
> not expose SPARK_JAVA_OPTS to the user anymore.
>
> > Passing of JAVA_OPTS to YARN on command line
> > --------------------------------------------
> >
> >                 Key: SPARK-1576
> >                 URL: https://issues.apache.org/jira/browse/SPARK-1576
> >             Project: Spark
> >          Issue Type: Improvement
> >    Affects Versions: 0.9.0, 1.0.0, 0.9.1
> >            Reporter: Nishkam Ravi
> >             Fix For: 0.9.0, 1.0.0, 0.9.1
> >
> >         Attachments: SPARK-1576.patch
> >
> >
> > JAVA_OPTS can be passed by using either env variables (i.e.,
> SPARK_JAVA_OPTS) or as config vars (after Patrick's recent change). It
> would be good to allow the user to pass them on command line as well to
> restrict scope to single application invocation.
>
>
>
> --
> This message was sent by Atlassian JIRA
> (v6.2#6252)
>"
Mridul Muralidharan <mridul@gmail.com>,"Wed, 23 Apr 2014 23:24:20 +0530","Re: [jira] [Commented] (SPARK-1576) Passing of JAVA_OPTS to YARN on
 command line",dev@spark.apache.org,"Sorry, I misread - I meant SPARK_JAVA_OPTS - not JAVA_OPTS.
See here : https://issues.apache.org/jira/browse/SPARK-1588

Regards,
Mridul


"
Xiangrui Meng <mengxr@gmail.com>,"Wed, 23 Apr 2014 12:52:39 -0700",Re: ArrayIndexOutOfBoundsException in ALS.implicit,"user@spark.apache.org, j.barrett.strausser@gmail.com","Hi bearrito, this issue was fixed by Tor in
https://github.com/apache/spark/pull/407. You can either try the
master branch or wait for the 1.0 release. -Xiangrui


"
Madhu <madhu@madhu.com>,"Wed, 23 Apr 2014 18:26:41 -0700 (PDT)",Re: get -101 error code when running select query,dev@spark.incubator.apache.org,"I have seen a similar error message when connecting to Hive through JDBC.
This is just a guess on my part, but check your query. The error occurs if
you have a select that includes a null literal with an alias like this:

select a, b, null as c, d from foo

In my case, rewriting the query to use an empty string or other literal
instead of null worked:

select a, b, '' as c, d from foo

I think the problem is the lack of type information when supplying a null
literal.



--

"
Nishkam Ravi <nravi@cloudera.com>,"Wed, 23 Apr 2014 18:26:30 -0700","Re: [jira] [Commented] (SPARK-1576) Passing of JAVA_OPTS to YARN on
 command line",dev@spark.apache.org,"Bit of a race condition here it seems. Patrick made a few changes yesterday
around the same time as I did (in ClientBase.scala):

for ((k, v) <- sys.props.filterKeys(_.startsWith(""spark"")))
{ JAVA_OPTS += ""-D"" + k + ""="" + ""\\\"""" + v + ""\\\"""" }

This would allow JAVA_OPTS to be passed on the command line to the
ApplicationMaster, and accomplishes the same things as creation of a new
command line flag --spark-java-opts.


Mridul, the use of SPARK_JAVA_OPTS has been intentionally suppressed.



"
Nishkam Ravi <nravi@cloudera.com>,"Wed, 23 Apr 2014 18:29:42 -0700","Re: [jira] [Commented] (SPARK-1576) Passing of JAVA_OPTS to YARN on
 command line",dev@spark.apache.org,"It would probably be best to retain support for SPARK_JAVA_OPTS in
ClientBase though..for developers that may have been using it.



"
Nan Zhu <zhunanmcgill@gmail.com>,"Wed, 23 Apr 2014 21:52:05 -0400","Fw: Is there any way to make a quick test on some pre-commit
 code?",dev@spark.apache.org,"Iâ€™m just asked by others for the same question  

I think Reynold gave a pretty helpful tip on this,  

Shall we put this on Contribute-to-Spark wiki?  

--  
Nan Zhu


Forwarded message:


ode?
 code,
 test


"
DB Tsai <dbtsai@stanford.edu>,"Wed, 23 Apr 2014 21:21:32 -0700","MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result","dev@spark.apache.org, mengxr@gmail.com","Hi all,

I'm benchmarking Logistic Regression in MLlib using the newly added
optimizer LBFGS and GD. I'm using the same dataset and the same methodology
in this paper, http://www.csie.ntu.edu.tw/~cjlin/papers/l1.pdf

I want to know how Spark scale while adding workers, and how optimizers and
input format (sparse or dense) impact performance.

The benchmark code can be found here,
https://github.com/dbtsai/spark-lbfgs-benchmark

The first dataset I benchmarked is a9a which only has 2.2MB. I duplicated
the dataset, and made it 762MB to have 11M rows. This dataset has 123
features and 11% of the data are non-zero elements.

In this benchmark, all the dataset is cached in memory.

As we expect, LBFGS converges faster than GD, and at some point, no matter
how we push GD, it will converge slower and slower.

However, it's surprising that sparse format runs slower than dense format.
I did see that sparse format takes significantly smaller amount of memory
in caching RDD, but sparse is 40% slower than dense. I think sparse should
be fast since when we compute x wT, since x is sparse, we can do it faster.
I wonder if there is anything I'm doing wrong.

The attachment is the benchmark result.

Thanks.

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai
"
Evan Sparks <evan.sparks@gmail.com>,"Wed, 23 Apr 2014 21:30:33 -0700","Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense benchmark result","""dev@spark.apache.org"" <dev@spark.apache.org>","What is the number of non zeroes per row (and number of features) in the sparse case? We've hit some issues with breeze sparse support in the past but for sufficiently sparse data it's still pretty good. 

zer LBFGS and GD. I'm using the same dataset and the same methodology in this paper, http://www.csie.ntu.edu.tw/~cjlin/papers/l1.pdf
d input format (sparse or dense) impact performance. 
s-benchmark
he dataset, and made it 762MB to have 11M rows. This dataset has 123 features and 11% of the data are non-zero elements. 
 how we push GD, it will converge slower and slower. 
 I did see that sparse format takes significantly smaller amount of memory in caching RDD, but sparse is 40% slower than dense. I think sparse should be fast since when we compute x wT, since x is sparse, we can do it faster. I wonder if there is anything I'm doing wrong. 
"
Evan Sparks <evan.sparks@gmail.com>,"Wed, 23 Apr 2014 21:32:50 -0700","Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense benchmark result","""dev@spark.apache.org"" <dev@spark.apache.org>","Sorry - just saw the 11% number. That is around the spot where dense data is usually faster (blocking, cache coherence, etc) is there any chance you have a 1% (or so) sparse dataset to experiment with?

zer LBFGS and GD. I'm using the same dataset and the same methodology in this paper, http://www.csie.ntu.edu.tw/~cjlin/papers/l1.pdf
d input format (sparse or dense) impact performance. 
s-benchmark
he dataset, and made it 762MB to have 11M rows. This dataset has 123 features and 11% of the data are non-zero elements. 
 how we push GD, it will converge slower and slower. 
 I did see that sparse format takes significantly smaller amount of memory in caching RDD, but sparse is 40% slower than dense. I think sparse should be fast since when we compute x wT, since x is sparse, we can do it faster. I wonder if there is anything I'm doing wrong. 
"
DB Tsai <dbtsai@dbtsai.com>,"Wed, 23 Apr 2014 21:32:56 -0700","Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result",dev@spark.apache.org,"123 features per rows, and in average, 89% are zeros.

"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Wed, 23 Apr 2014 21:34:28 -0700","Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result","dev@spark.apache.org, dbtsai@dbtsai.com","I don't think the attachment came through in the list. Could you upload the
results somewhere and link to them ?



"
DB Tsai <dbtsai@dbtsai.com>,"Wed, 23 Apr 2014 21:36:15 -0700","Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result",dev@spark.apache.org,"Any suggestion for sparser dataset? Will test more tomorrow in the office.

"
David Hall <dlwh@cs.berkeley.edu>,"Wed, 23 Apr 2014 21:38:21 -0700","Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result",dev@spark.apache.org,"

Any chance you remember what the problems were? I'm sure it could be
better, but it's good to know where improvements need to happen.

-- David


"
DB Tsai <dbtsai@dbtsai.com>,"Wed, 23 Apr 2014 22:08:18 -0700","Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result",shivaram@eecs.berkeley.edu,"The figure showing the Log-Likelihood vs Time can be found here.

https://github.com/dbtsai/spark-lbfgs-benchmark/raw/fd703303fb1c16ef5714901739154728550becf4/result/a9a11M.pdf

Let me know if you can not open it.

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



"
David Hall <dlwh@cs.berkeley.edu>,"Wed, 23 Apr 2014 22:16:11 -0700","Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result","dev@spark.apache.org, dbtsai@dbtsai.com","Was the weight vector sparse? The gradients? Or just the feature vectors?



"
DB Tsai <dbtsai@dbtsai.com>,"Wed, 23 Apr 2014 22:17:44 -0700","Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result",David Hall <dlwh@cs.berkeley.edu>,"
Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



"
DB Tsai <dbtsai@dbtsai.com>,"Wed, 23 Apr 2014 22:18:56 -0700","Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result",David Hall <dlwh@cs.berkeley.edu>,"ps, it doesn't make sense to have weight and gradient sparse unless
with strong L1 penalty.

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



"
David Hall <dlwh@cs.berkeley.edu>,"Wed, 23 Apr 2014 22:22:35 -0700","Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result",dbtsai@dbtsai.com,"

Sure, I was just checking the obvious things. Have you run it through it a
profiler to see where the problem is?



"
DB Tsai <dbtsai@dbtsai.com>,"Wed, 23 Apr 2014 22:33:45 -0700","Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result",David Hall <dlwh@cs.berkeley.edu>,"Not yet since it's running in the cluster. Will run locally with
profiler. Thanks for help.

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



"
DB Tsai <dbtsai@stanford.edu>,"Wed, 23 Apr 2014 22:35:14 -0700","Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result",shivaram@eecs.berkeley.edu,"The figure showing the Log-Likelihood vs Time can be found here.

https://github.com/dbtsai/spark-lbfgs-benchmark/raw/fd703303fb1c16ef5714901739154728550becf4/result/a9a11M.pdf

Let me know if you can not open it. Thanks.

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



"
qingyang li <liqingyang1985@gmail.com>,"Thu, 24 Apr 2014 13:48:23 +0800",Re: get -101 error code when running select query,dev@spark.apache.org,"thanks for sharing,  my case is diffrent from yours,
i have set hive.server2.enable.doAs into false in  hive-site.xml,  then
that 101 error code disappeared.



2014-04-24 9:26 GMT+08:00 Madhu <madhu@madhu.com>:

"
Xiangrui Meng <mengxr@gmail.com>,"Thu, 24 Apr 2014 00:53:44 -0700","Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result","DB Tsai <dbtsai@stanford.edu>, David Hall <dlwh@cs.berkeley.edu>","I don't think it is easy to make sparse faster than dense with this
sparsity and feature dimension. You can try rcv1.binary, which should
show the difference easily.

David, the breeze operators used here are

1. DenseVector dot SparseVector
2. axpy DenseVector SparseVector

However, the SparseVector is passed in as Vector[Double] instead of
SparseVector[Double]. It might use the axpy impl of [DenseVector,
Vector] and call activeIterator. I didn't check whether you used
multimethods on axpy.

Best,
Xiangrui


"
Patrick Wendell <pwendell@gmail.com>,"Thu, 24 Apr 2014 01:02:45 -0700",Re: Fw: Is there any way to make a quick test on some pre-commit code?,"""dev@spark.apache.org"" <dev@spark.apache.org>","This is already on the wiki:

https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools




"
Xiangrui Meng <mengxr@gmail.com>,"Thu, 24 Apr 2014 01:09:54 -0700","Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result","DB Tsai <dbtsai@stanford.edu>, David Hall <dlwh@cs.berkeley.edu>","Hi DB,

I saw you are using yarn-cluster mode for the benchmark. I tested the
yarn-cluster mode and found that YARN does not always give you the
exact number of executors requested. Just want to confirm that you've
checked the number of executors.

The second thing to check is that in the benchmark code, after you
call cache, you should also call count() to materialize the RDD. I saw
in the result, the real difference is actually at the first step.
Adding intercept is not a cheap operation for sparse vectors.

Best,
Xiangrui


"
Prashant Sharma <scrapcodes@gmail.com>,"Thu, 24 Apr 2014 13:55:08 +0530",Re: Fw: Is there any way to make a quick test on some pre-commit code?,dev@spark.apache.org,"Not sure but I use sbt/sbt ~compile instead of package. Any reason we use
package instead of compile(which is slightly faster ofc.)


Prashant Sharma



"
=?UTF-8?Q?Piotr_Ko=C5=82aczkowski?= <pkolaczk@datastax.com>,"Thu, 24 Apr 2014 12:14:04 +0200",Problem creating objects through reflection,dev@spark.apache.org,"Hi,

I'm working on Cassandra-Spark integration and I hit a pretty severe
objects of user-defined classes. E.g. like this:

class MyRow(val key: String, val data: Int)
sc.cassandraTable(""keyspace"", ""table"").select(""key"", ""data"").as[MyRow]  //
returns CassandraRDD[MyRow]

In this example CassandraRDD creates MyRow instances by reflection, i.e.
matches selected fields from Cassandra table and passes them to the
constructor.

Unfortunately this does not work in Spark REPL.
Turns out any class declared on the REPL is an inner classes, and to be
successfully created, it needs a reference to the outer object, even though
it doesn't really use anything from the outer context.

scala> class SomeClass
defined class SomeClass

scala> classOf[SomeClass].getConstructors()(0)
res11: java.lang.reflect.Constructor[_] = public
$iwC$$iwC$SomeClass($iwC$$iwC)

I tried passing a null as a temporary workaround, and it also doesn't work
- I get NPE.
How can I get a reference to the current outer object representing the
context of the current line?

Also, plain non-spark Scala REPL doesn't exhibit this behaviour - and
classes declared on the REPL are proper top-most classes, not inner ones.
Why?

Thanks,
Piotr







-- 
Piotr Kolaczkowski, Lead Software Engineer
pkolaczk@datastax.com

777 Mariners Island Blvd., Suite 510
San Mateo, CA 94404
"
DB Tsai <dbtsai@stanford.edu>,"Thu, 24 Apr 2014 11:26:05 -0700","Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result",Xiangrui Meng <mengxr@gmail.com>,"Hi Xiangrui,

Yes, I'm using yarn-cluster mode, and I did check # of executors I
specified are the same as the actual running executors.

For caching and materialization, I've the timer in optimizer after calling
count(); as a result, the time for materialization in cache isn't in the
benchmark.

The difference you saw is actually from dense feature or sparse feature
vector. For LBFGS and GD dense feature, you can see the first iteration
takes the same time. It's true for GD.

I'm going to run rcv1.binary which only has 0.15% non-zero elements to
verify the hypothesis.


Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



"
Xiangrui Meng <mengxr@gmail.com>,"Thu, 24 Apr 2014 13:44:07 -0700","Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result",DB Tsai <dbtsai@stanford.edu>,"I don't understand why sparse falls behind dense so much at the very
first iteration. I didn't see count() is called in
https://github.com/dbtsai/spark-lbfgs-benchmark/blob/master/src/main/scala/org/apache/spark/mllib/benchmark/BinaryLogisticRegression.scala
. Maybe you have local uncommitted changes.

Best,
Xiangrui


"
DB Tsai <dbtsai@stanford.edu>,"Thu, 24 Apr 2014 13:54:13 -0700","Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result",Xiangrui Meng <mengxr@gmail.com>,"I'm doing the timer in runMiniBatchSGD after  val numExamples = data.count()

See the following. Running rcv1 dataset now, and will update soon.

    val startTime = System.nanoTime()
    for (i <- 1 to numIterations) {
      // Sample a subset (fraction miniBatchFraction) of the total data
      // compute and sum up the subgradients on this subset (this is one
map-reduce)
      val (gradientSum, lossSum) = data.sample(false, miniBatchFraction, 42
+ i)
        .aggregate((BDV.zeros[Double](weights.size), 0.0))(
          seqOp = (c, v) => (c, v) match { case ((grad, loss), (label,
features)) =>
            val l = gradient.compute(features, label, weights,
Vectors.fromBreeze(grad))
            (grad, loss + l)
          },
          combOp = (c1, c2) => (c1, c2) match { case ((grad1, loss1),
(grad2, loss2)) =>
            (grad1 += grad2, loss1 + loss2)
          })

      /**
       * NOTE(Xinghao): lossSum is computed using the weights from the
previous iteration
       * and regVal is the regularization value computed in the previous
iteration as well.
       */
      stochasticLossHistory.append(lossSum / miniBatchSize + regVal)
      val update = updater.compute(
        weights, Vectors.fromBreeze(gradientSum / miniBatchSize), stepSize,
i, regParam)
      weights = update._1
      regVal = update._2
      timeStamp.append(System.nanoTime() - startTime)
    }






Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



"
DB Tsai <dbtsai@stanford.edu>,"Thu, 24 Apr 2014 14:36:59 -0700","Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result",Xiangrui Meng <mengxr@gmail.com>,"rcv1.binary is too sparse (0.15% non-zero elements), so dense format will
not run due to out of memory. But sparse format runs really well.


Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



"
Michael Armbrust <michael@databricks.com>,"Thu, 24 Apr 2014 15:13:26 -0700",Re: Problem creating objects through reflection,dev@spark.apache.org,"The Spark REPL is slightly modified from the normal Scala REPL to prevent
work from being done twice when closures are deserialized on the workers.
 I'm not sure exactly why this causes your problem, but its probably worth
filing a JIRA about it.

Here is another issues with classes defined in the REPL.  Not sure if it is
related, but I'd be curious if the workaround helps you:
https://issues.apache.org/jira/browse/SPARK-1199

Michael



/
gh
k
"
=?UTF-8?Q?Piotr_Ko=C5=82aczkowski?= <pkolaczk@datastax.com>,"Fri, 25 Apr 2014 09:28:11 +0200",Re: Problem creating objects through reflection,dev@spark.apache.org,"Yeah, this is related.

From
https://groups.google.com/forum/#!msg/spark-users/bwAmbUgxWrA/HwP4Nv4adfEJ:
""This is a limitation that will hopefully go away in Scala 2.10 or 2.10 .1,
when we'll use macros to remove the need to do this. (Or more generally if
we get some changes in the Scala interpreter to do something smarter in
this case.) ""

We're using Spark 0.9.0, Scala 2.10.3 and the limitation is there. Any
ideas when it is going to be fixed?

The workaround with embedding everything inside a singleton object does not
work for me, because nested classes defined there are still inner  and
require additional argument to the constructor (when invoked by
reflection).

If I only had some reliable way to obtain a reference to that outer object
by reflection, we could somehow workaround it. E.g. saving it in some
singleton object, etc. However, a proper fix would be to make non-inner
classes properly non-inner.

Thanks,
Piotr




2014-04-25 0:13 GMT+02:00 Michael Armbrust <michael@databricks.com>:

h
is
to
.
s.



-- 
Piotr Kolaczkowski, Lead Software Engineer
pkolaczk@datastax.com

777 Mariners Island Blvd., Suite 510
San Mateo, CA 94404
"
Art Peel <foundart@gmail.com>,"Fri, 25 Apr 2014 00:51:36 -0700",thoughts on spark_ec2.py?,dev@spark.apache.org,"I've been setting up Spark cluster on EC2 using the provided
ec2/spark_ec2.py script and am very happy I didn't have to write it from
scratch. Thanks for providing it.

There have been some issues, though, and I have had to make some additions.
 So far, they are all additions of command-line options.  For example, the
original script allows access from anywhere to the various ports.  I've
added an option to specify what net/mask should be allowed to access those
ports.

I've filed a couple of pull requests, but they are not going anywhere.
 Given what I've seen of the traffic on this list, I don't feel that a lot
of the developers are thinking about EC2 setup. I totally agree that it is
not as important as improving the guts of Spark itself; nevertheless, I
feel that being able to run Spark on EC2 smartly and easily is valuable.

So, I have 2 questions for the committers:

1. Is ec2/spark_ec2.py something the committers
a. are not thinking about?
b. are planning to replace?
c. other

2. Should I just start a new project based on ec2/spark_ec2.py but without
all the other stuff and make (and share) my changes there?

Regards,

Art
"
Andrew Or <andrew@databricks.com>,"Fri, 25 Apr 2014 09:48:11 -0700",Re: thoughts on spark_ec2.py?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Art,

First of all thanks a lot for your PRs. We are currently in the middle of
all the Spark 1.0 release so most of us are swamped with the more core
features. To answer your questions:

1. Neither. We welcome changes from developers for all components of Spark,
to review the many PRs that we missed on the ride.

2. We prefer to keep the EC2 scripts within Spark, at least for now.

Cheers,
Andrew


"
DB Tsai <dbtsai@stanford.edu>,"Fri, 25 Apr 2014 14:57:46 -0700","Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result",Xiangrui Meng <mengxr@gmail.com>,"Another interesting benchmark.

*News20 dataset - 0.14M row, 1,355,191 features, 0.034% non-zero elements.*

LBFGS converges in 70 seconds, while GD seems to be not progressing.

Dense feature vector will be too big to fit in the memory, so only conduct
the sparse benchmark.

I saw the sometimes the loss bumps up, and it's weird for me. Since the
cost function of logistic regression is convex, it should be monotonically
decreasing.  David, any suggestion?

The detail figure:
https://github.com/dbtsai/spark-lbfgs-benchmark/raw/0b774682e398b4f7e0ce01a69c44000eb0e73454/result/news20.pdf


*Rcv1 dataset - 6.8M row, 677,399 features, 0.15% non-zero elements.*

LBFGS converges in 25 seconds, while GD also seems to be not progressing.

bumps up for unknown reason.

The detail figure:
https://github.com/dbtsai/spark-lbfgs-benchmark/raw/0b774682e398b4f7e0ce01a69c44000eb0e73454/result/rcv1.pdf


Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



"
David Hall <dlwh@cs.berkeley.edu>,"Fri, 25 Apr 2014 15:10:17 -0700","Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result",DB Tsai <dbtsai@stanford.edu>,"LBFGS will not take a step that sends the objective value up. It might try
a step that is ""too big"" and reject it, so if you're just logging
everything that gets tried by LBFGS, you could see that. The ""iterations""
method of the minimizer should never return an increasing objective value.
If you're regularizing, are you including the regularizer in the objective
value computation?

GD is almost never worth your time.

-- David


"
Tom Vacek <minnesota.cs@gmail.com>,"Fri, 25 Apr 2014 17:10:34 -0500","Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result",dev@spark.apache.org,"I don't know about Spark's implementation, but with LBFGS, there is a line
search step.  Since computing the line search takes roughly the same work
as one iteration, an efficient implementation will take a full step and
simultaneously compute the gradient for the next step and check if the
update satisfied the line search rules.  If it failed, then the new step is
abandoned and the previous step is revised, and the process repeats.  I
suspect the loss being reported doesn't distinguish between an update step
and a line search step, so it looks like the loss is increasing.



"
Ajay Nair <prodigyaj@gmail.com>,"Sat, 26 Apr 2014 16:20:03 -0500",Parsing wikipedia xml data in Spark,dev@spark.apache.org,"Is there a way in spark to parse wikipedia xml dump? It seems like the
freebase dump is longer available. Also does the spark shell support the
xml load file sax parser that is present in scala.

Thanks
AJ
"
DB Tsai <dbtsai@stanford.edu>,"Sun, 27 Apr 2014 23:28:24 -0700","Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result",David Hall <dlwh@cs.berkeley.edu>,"Hi David,

I'm recording the loss history in the DiffFunction implementation, and
that's why the rejected step is also recorded in my loss history.

Is there any api in Breeze LBFGS to get the history which already excludes
the reject step? Or should I just call ""iterations"" method and check
""iteratingShouldStop"" instead?

Thanks.


Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



"
DB Tsai <dbtsai@stanford.edu>,"Sun, 27 Apr 2014 23:31:30 -0700","Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result",David Hall <dlwh@cs.berkeley.edu>,"Also, how many failure of rejection will terminate the optimization
process? How is it related to ""numberOfImprovementFailures""?

Thanks.


Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



"
DB Tsai <dbtsai@stanford.edu>,"Sun, 27 Apr 2014 23:53:26 -0700","Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result",David Hall <dlwh@cs.berkeley.edu>,"I think I figure it out. Instead of calling minimize, and record the loss
in the DiffFunction, I should do the following.

val states = lbfgs.iterations(new CachedDiffFunction(costFun),
initialWeights.toBreeze.toDenseVector)
states.foreach(state => lossHistory.append(state.value))

All the losses in states should be decreasing now. Am I right?



Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



"
Geoffroy Fouquier <geoffroy.fouquier@exensa.com>,"Mon, 28 Apr 2014 10:36:59 +0200",Re: Parsing wikipedia xml data in Spark,dev@spark.apache.org,"
We did it using scala xml with spark

We start by creating a rdd containing each page is store as a single line :
   - split the xml dump with xml_split
   - process each split with a shell script which remove ""xml_split"" tag 
and siteinfo section, and put each page on a single line.
   - copy resulting files on hdfs

Then the dataset may be load as a text file and processed

  val rawDataset = sparkContext.textFile(input)
  val allDocuments = rawDataset.map{
     case document =>
         val page = scala.xml.XML.loadString(document)
         val pageTitle = (page \ ""title"").text
         [...]
  }

We create a demo using the dataset here: http://wikinsights.org

Le 26/04/2014 23:20, Ajay Nair a Ã©crit :

Geoffroy Fouquier
http://eXenSa.com


"
Art Peel <foundart@gmail.com>,"Mon, 28 Apr 2014 07:26:50 -0700",Re: thoughts on spark_ec2.py?,dev@spark.apache.org,"Thanks for the info and good luck with 1.0.

Regards,
Art




"
David Hall <dlwh@cs.berkeley.edu>,"Mon, 28 Apr 2014 08:55:35 -0700","Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result",DB Tsai <dbtsai@stanford.edu>,"That's right.

FWIW, caching should be automatic now, but it might be the version of
Breeze you're using doesn't do that yet.

Also, In breeze.util._ there's an implicit that adds a tee method to
iterator, and also a last method. Both are useful for things like this.

-- David


"
DB Tsai <dbtsai@stanford.edu>,"Mon, 28 Apr 2014 14:36:25 -0700","Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result",David Hall <dlwh@cs.berkeley.edu>,"Hi David,

I got most of the stuff working, and the loss is monotonically decreasing
by getting the history from iterator of state.

However, in the costFun, I need to know what current iteration is it for
miniBatch, which means for one iteration, if optimizer calls costFun
several times for line search, it should pass the same iteration into
costFun. So I pass the lbfgs optimizer into costFun as the following code,
and try to find the current iteration in lbfgs object. Unfortunately, it
seems that the current iteration is not available in this object.

Any idea for getting this in costFun? Originally, I've a counter inside
costFun which gives the # of iterations. However, it's not what I want now
since it also counts line search.

val lbfgs = new BreezeLBFGS[BDV[Double]](maxNumIterations, numCorrections,
convergenceTol)

val costFun =
      new CostFun(data, gradient, updater, miniBatchFraction, lbfgs,
miniBatchSize)

val states = lbfgs.iterations(new CachedDiffFunction(costFun),
initialWeights.toBreeze.toDenseVector)


Thanks.


Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



"
DB Tsai <dbtsai@stanford.edu>,"Mon, 28 Apr 2014 14:36:26 -0700","Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result",David Hall <dlwh@cs.berkeley.edu>,"Hi David,

I got most of the stuff working, and the loss is monotonically decreasing
by getting the history from iterator of state.

However, in the costFun, I need to know what current iteration is it for
miniBatch, which means for one iteration, if optimizer calls costFun
several times for line search, it should pass the same iteration into
costFun. So I pass the lbfgs optimizer into costFun as the following code,
and try to find the current iteration in lbfgs object. Unfortunately, it
seems that the current iteration is not available in this object.

Any idea for getting this in costFun? Originally, I've a counter inside
costFun which gives the # of iterations. However, it's not what I want now
since it also counts line search.

val lbfgs = new BreezeLBFGS[BDV[Double]](maxNumIterations, numCorrections,
convergenceTol)

val costFun =
      new CostFun(data, gradient, updater, miniBatchFraction, lbfgs,
miniBatchSize)

val states = lbfgs.iterations(new CachedDiffFunction(costFun),
initialWeights.toBreeze.toDenseVector)


Thanks.


Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



"
Patrick Wendell <pwendell@gmail.com>,"Tue, 29 Apr 2014 01:05:38 -0700",Spark 1.0.0 rc3,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey All,

This is not an official vote, but I wanted to cut an RC so that people can
test against the Maven artifacts, test building with their configuration,
etc. We are still chasing down a few issues and updating docs, etc.

If you have issues or bug reports for this release, please send an e-mail
to the Spark dev list and/or file a JIRA.

Commit: d636772 (v1.0.0-rc3)
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=d636772ea9f98e449a038567b7975b1a07de3221

Binaries:
http://people.apache.org/~pwendell/spark-1.0.0-rc3/

Docs:
http://people.apache.org/~pwendell/spark-1.0.0-rc3-docs/

Repository:
https://repository.apache.org/content/repositories/orgapachespark-1012/

== API Changes ==
If you want to test building against Spark there are some minor API
changes. We'll get these written up for the final release but I'm noting a
few here (not comprehensive):

changes to ML vector specification:
http://people.apache.org/~pwendell/spark-1.0.0-rc3-docs/mllib-guide.html#from-09-to-10

changes to the Java API:
http://people.apache.org/~pwendell/spark-1.0.0-rc3-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark

coGroup and related functions now return Iterable[T] instead of Seq[T]
==> Call toSeq on the result to restore the old behavior

SparkContext.jarOfClass returns Option[String] instead of Seq[String]
==> Call toSeq on the result to restore old behavior

Streaming classes have been renamed:
NetworkReceiver -> Receiver
"
Dean Wampler <deanwampler@gmail.com>,"Tue, 29 Apr 2014 11:43:13 -0500",Re: Spark 1.0.0 rc3,dev@spark.apache.org,"I'm observing one anomalous behavior. With the 1.0.0 libraries, it's using
HDFS classes for file I/O, while the same script compiled and running with
0.9.1 uses only the local-mode File IO.

The script is a variation of the Word Count script. Here are the ""guts"":

object WordCount2 {
  def main(args: Array[String]) = {

    val sc = new SparkContext(""local"", ""Word Count (2)"")

    val input = sc.textFile("".../some/local/file"").map(line =>
line.toLowerCase)
    input.cache

    val wc2 = input
      .flatMap(line => line.split(""""""\W+""""""))
      .map(word => (word, 1))
      .reduceByKey((count1, count2) => count1 + count2)

    wc2.saveAsTextFile(""output/some/directory"")

    sc.stop()

It works fine compiled and executed with 0.9.1. If I recompile and run with
1.0.0-RC1, where the same output directory still exists, I get this
familiar Hadoop-ish exception:

[error] (run-main-0) org.apache.hadoop.mapred.FileAlreadyExistsException:
Output directory
file:/Users/deanwampler/projects/typesafe/activator/activator-spark/output/kjv-wc
already exists
org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory
file:/Users/deanwampler/projects/typesafe/activator/activator-spark/output/kjv-wc
already exists
 at
org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:121)
at
org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:749)
 at
org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:662)
at
org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:581)
 at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1057)
at spark.activator.WordCount2$.main(WordCount2.scala:42)
 at spark.activator.WordCount2.main(WordCount2.scala)
...

Thoughts?






-- 
Dean Wampler, Ph.D.
Typesafe
@deanwampler
http://typesafe.com
http://polyglotprogramming.com
"
DB Tsai <dbtsai@stanford.edu>,"Tue, 29 Apr 2014 10:10:33 -0700",Code Review for SPARK-1516: Throw exception in yarn client instead of System.exit,dev@spark.apache.org,"Hi All,

Since we're launching Spark Yarn Job in our tomcat application, the default
behavior of calling System.exit when job is finished or runs into any error
isn't desirable.

We create this PR https://github.com/apache/spark/pull/490 to address this
issue. Since the logical is fairly straightforward, we wonder if this can
be reviewed and have this in 1.0.

Thanks.

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai
"
Marcelo Vanzin <vanzin@cloudera.com>,"Tue, 29 Apr 2014 11:23:43 -0700",Re: Spark 1.0.0 rc3,dev@spark.apache.org,"Hi Patrick,

What are the expectations / guarantees on binary compatibility between
0.9 and 1.0?

You mention some API changes, which kinda hint that binary
compatibility has already been broken, but just wanted to point out
there are other cases. e.g.:

Exception in thread ""main"" java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:236)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:47)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.NoSuchMethodError:
org.apache.spark.SparkContext$.rddToOrderedRDDFunctions(Lorg/apache/spark/rdd/RDD;Lscala/Function1;Lscala/reflect/ClassTag;Lscala/reflect/ClassTag;)Lorg/apache/spark/rdd/OrderedRDDFunctions;

(Compiled against 0.9, run against 1.0.)
Offending code:

      val top10 = counts.sortByKey(false).take(10)

Recompiling fixes the problem.





-- 
Marcelo

"
Patrick Wendell <pwendell@gmail.com>,"Tue, 29 Apr 2014 11:47:18 -0700",Re: Spark 1.0.0 rc3,"""dev@spark.apache.org"" <dev@spark.apache.org>","
There are not guarantees.

"
Patrick Wendell <pwendell@gmail.com>,"Tue, 29 Apr 2014 11:48:43 -0700",Re: Spark 1.0.0 rc3,"""dev@spark.apache.org"" <dev@spark.apache.org>","Sorry got cut off. For 0.9.0 and 1.0.0 they are not binary compatible
and in a few cases not source compatible. 1.X will be source
compatible. We are also planning to support binary compatibility in
1.X but I'm waiting util we make a few releases to officially promise
that, since Scala makes this pretty tricky.


"
Patrick Wendell <pwendell@gmail.com>,"Tue, 29 Apr 2014 11:54:31 -0700",Re: Spark 1.0.0 rc3,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Dean,

We always used the Hadoop libraries here to read and write local
files. In Spark 1.0 we started enforcing the rule that you can't
over-write an existing directory because it can cause
confusing/undefined behavior if multiple jobs output to the directory
(they partially clobber each other's output).

https://issues.apache.org/jira/browse/SPARK-1100
https://github.com/apache/spark/pull/11

In the JIRA I actually proposed slightly deviating from Hadoop
semantics and allowing the directory to exist if it is empty, but I
think in the end we decided to just go with the exact same semantics
as Hadoop (i.e. empty directories are a problem).

- Patrick


"
Dean Wampler <deanwampler@gmail.com>,"Tue, 29 Apr 2014 16:20:08 -0500",Re: Spark 1.0.0 rc3,dev@spark.apache.org,"Thanks. I'm fine with the logic change, although I was a bit surprised to
see Hadoop used for file I/O.

Anyway, the jira issue and pull request discussions mention a flag to
enable overwrites. That would be very convenient for a tutorial I'm
writing, although I wouldn't recommend it for normal use, of course.
However, I can't figure out if this actually exists. I found the
spark.files.overwrite property, but that doesn't apply.  Does this override
flag, method call, or method argument actually exist?

Thanks,
Dean






-- 
Dean Wampler, Ph.D.
Typesafe
@deanwampler
http://typesafe.com
http://polyglotprogramming.com
"
DB Tsai <dbtsai@stanford.edu>,"Tue, 29 Apr 2014 15:30:53 -0700","Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result",David Hall <dlwh@cs.berkeley.edu>,"Have a quick hack to understand the behavior of SLBFGS
(Stochastic-LBFGS) by overwriting the breeze iterations method to get the
current LBFGS step to ensure that the objective function is the same during
the line search step. David, the following is my code, have a better way to
inject into it?

https://github.com/dbtsai/spark/tree/dbtsai-lbfgshack

Couple findings,

1) miniBatch (using rdd sample api) for each iteration is slower than full
data training when the full data is cached. Probably because sample is not
efficiency in Spark.

2) Since in the line search steps, we use the same sample of data (the same
objective function), the SLBFGS actually converges well.

3) For news20 dataset, with 0.05 miniBatch size, it takes 14 SLBFGS steps
(29 data iterations, 74.5seconds) to converge to loss < 0.10. For LBFGS
with full data training, it takes 9 LBFGS steps (12 data iterations, 37.6
seconds) to converge to loss < 0.10.

It seems that as long as the noisy gradient happens in different SLBFGS
steps, it still works.

(ps, I also tried in line search step, I use different sample of data, and
it just doesn't work as we expect.)



Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



"
David Hall <dlwh@cs.berkeley.edu>,"Tue, 29 Apr 2014 21:50:50 -0700","Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result",DB Tsai <dbtsai@stanford.edu>,"Yeah, that's probably the easiest though obviously pretty hacky.

I'm surprised that the hessian approximation isn't worse than it is. (As
in, I'd expect error messages.) It's obviously line searching much more, so
the approximation must be worse. You might be interested in this online
bfgs:
http://jmlr.org/proceedings/papers/v2/schraudolph07a/schraudolph07a.pdf

-- David



"
DB Tsai <dbtsai@stanford.edu>,"Tue, 29 Apr 2014 22:18:14 -0700","Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result",David Hall <dlwh@cs.berkeley.edu>,"Yeah, the approximation of hssian in LBFGS isn't stateless, and it does
depend on previous LBFGS step as Xiangrui also pointed out. It's surprising
that it works without error message. I also saw the loss is fluctuating
like SGD during the training.

We will remove the miniBatch mode in LBFGS in Spark before we've deeper
understanding of how ""stochastic"" LBFGS works.


Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



"
Patrick Wendell <pwendell@gmail.com>,"Tue, 29 Apr 2014 22:43:25 -0700",Re: Spark 1.0.0 rc3,"""dev@spark.apache.org"" <dev@spark.apache.org>","That suggestion got lost along the way and IIRC the patch didn't have
that. It's a good idea though, if nothing else to provide a simple
means for backwards compatibility.

I created a JIRA for this. It's very straightforward so maybe someone
can pick it up quickly:
https://issues.apache.org/jira/browse/SPARK-1677



"
Han JU <ju.han.felix@gmail.com>,"Wed, 30 Apr 2014 10:13:43 +0200",Fwd: Spark RDD cache memory usage,dev@spark.incubator.apache.org,"Hi,

As I understand, by default in Spark a fraction of the executor memory
(60%) is reserved for RDD caching. So if there's no explicit caching in the
code (eg. rdd.cache() etc.), or if we persist RDD with
StorageLevel.DISK_ONLY, is this part of memory wasted? Does Spark allocates
the RDD cache memory dynamically? Or does spark automatically caches RDDs
when it can?

I've posted this question in user list but got no response there, so I try
the dev list. Sorry for spam.

Thanks.

-- 
*JU Han*

Data Engineer @ Botify.com

+33 0619608888
"
Marcelo Vanzin <vanzin@cloudera.com>,"Wed, 30 Apr 2014 11:29:03 -0700",SparkSubmit and --driver-java-options,dev@spark.apache.org,"Hello all,

Maybe my brain is not evolved enough to be able to trace through what
happens with command-line arguments as they're parsed through all the
shell scripts... but I really can't figure out how to pass more than a
single JVM option on the command line.

Unless someone has an obvious workaround that I'm missing, I'd like to
propose something that is actually pretty standard in JVM tools: using
-J. From javac:

  -J<flag>                   Pass <flag> directly to the runtime system

So ""javac -J-Xmx1g"" would pass ""-Xmx1g"" to the underlying JVM. You can
use several of those to pass multiple options (unlike
--driver-java-options), so it helps that it's a short syntax.

Unless someone has some issue with that I'll work on a patch for it...
(well, I'm going to do it locally for me anyway because I really can't
figure out how to do what I want to otherwise.)


-- 
Marcelo

"
Patrick Wendell <pwendell@gmail.com>,"Wed, 30 Apr 2014 12:49:19 -0700",Re: SparkSubmit and --driver-java-options,"""dev@spark.apache.org"" <dev@spark.apache.org>","I added a fix for this recently and it didn't require adding -J
notation - are you trying it with this patch?

https://issues.apache.org/jira/browse/SPARK-1654

 ./bin/spark-shell --driver-java-options ""-Dfoo=a -Dbar=b""
scala> sys.props.get(""foo"")
res0: Option[String] = Some(a)
scala> sys.props.get(""bar"")
res1: Option[String] = Some(b)

- Patrick


"
Marcelo Vanzin <vanzin@cloudera.com>,"Wed, 30 Apr 2014 13:06:56 -0700",Re: SparkSubmit and --driver-java-options,dev@spark.apache.org,"Just pulled again just in case. Verified your fix is there.

$ ./bin/spark-submit --master yarn --deploy-mode client
--driver-java-options ""-Dfoo -Dbar"" blah blah blah
error: Unrecognized option '-Dbar'.
run with --help for more information or --verbose for debugging output





-- 
Marcelo

"
Patrick Wendell <pwendell@gmail.com>,"Wed, 30 Apr 2014 13:41:00 -0700",Re: SparkSubmit and --driver-java-options,"""dev@spark.apache.org"" <dev@spark.apache.org>","Yeah I think the problem is that the spark-submit script doesn't pass
the argument array to spark-class in the right way, so any quoted
strings get flattened.

We do:
ORIG_ARGS=$@
$SPARK_HOME/bin/spark-class org.apache.spark.deploy.SparkSubmit $ORIG_ARGS

This works:
// remove all the code relating to `shift`ing the arguments
$SPARK_HOME/bin/spark-class org.apache.spark.deploy.SparkSubmit ""$@""

Not sure, but I think the issue is that when you make a copy of $@ in
bash the type actually changes from an array to something else.

My patch fixes this for spark-shell but I didn't realize that
spark-submit does the same thing.
https://github.com/apache/spark/pull/576/files#diff-bc287993dfd11fd18794041e169ffd72L23

I think we'll need to figure out how to do this correctly in the bash
script so that quoted strings get passed in the right way.


"
Marcelo Vanzin <vanzin@cloudera.com>,"Wed, 30 Apr 2014 13:48:53 -0700",Re: SparkSubmit and --driver-java-options,dev@spark.apache.org,"
I tried a few different approaches but finally ended up giving up; my
bash-fu is apparently not strong enough. If you can make it work
great, but I have ""-J"" working locally in case you give up like me.
:-)

-- 
Marcelo

"
Patrick Wendell <pwendell@gmail.com>,"Wed, 30 Apr 2014 13:51:03 -0700",Re: SparkSubmit and --driver-java-options,"""dev@spark.apache.org"" <dev@spark.apache.org>","So I reproduced the problem here:

== test.sh ==
#!/bin/bash
for x in ""$@""; do
  echo ""arg: $x""
done
ARGS_COPY=$@
for x in ""$ARGS_COPY""; do
  echo ""arg_copy: $x""
done
==

./test.sh a b ""c d e"" f
arg: a
arg: b
arg: c d e
arg: f
arg_copy: a b c d e f

I'll dig around a bit more and see if we can fix it. Pretty sure we
aren't passing these argument arrays around correctly in bash.


"
Dean Wampler <deanwampler@gmail.com>,"Wed, 30 Apr 2014 16:07:23 -0500",Re: SparkSubmit and --driver-java-options,dev@spark.apache.org,"Try this:

#!/bin/bash
for x in ""$@""; do
  echo ""arg: $x""
done
ARGS_COPY=(""$@"")     # Make ARGS_COPY an array with the array elements in $@

for x in ""${ARGS_COPY[@]}""; do                # preserve array arguments.
  echo ""arg_copy: $x""
done







-- 
Dean Wampler, Ph.D.
Typesafe
@deanwampler
http://typesafe.com
http://polyglotprogramming.com
"
Patrick Wendell <pwendell@gmail.com>,"Wed, 30 Apr 2014 14:09:47 -0700",Re: SparkSubmit and --driver-java-options,"""dev@spark.apache.org"" <dev@spark.apache.org>","Marcelo - Mind trying the following diff locally? If it works I can
send a patch:

patrick@patrick-t430s:~/Documents/spark$ git diff bin/spark-submit
diff --git a/bin/spark-submit b/bin/spark-submit
index dd0d95d..49bc262 100755
--- a/bin/spark-submit
+++ b/bin/spark-submit
@@ -18,7 +18,7 @@
 #

 export SPARK_HOME=""$(cd `dirname $0`/..; pwd)""
-ORIG_ARGS=$@
+ORIG_ARGS=(""$@"")

 while (($#)); do
   if [ ""$1"" = ""--deploy-mode"" ]; then
@@ -39,5 +39,5 @@ if [ ! -z $DRIVER_MEMORY ] && [ ! -z $DEPLOY_MODE ]
&& [ $DEPLOY_MODE = ""client""
   export SPARK_MEM=$DRIVER_MEMORY
 fi

-$SPARK_HOME/bin/spark-class org.apache.spark.deploy.SparkSubmit $ORIG_ARGS
+$SPARK_HOME/bin/spark-class org.apache.spark.deploy.SparkSubmit
""${ORIG_ARGS[@]}""


"
Marcelo Vanzin <vanzin@cloudera.com>,"Wed, 30 Apr 2014 14:14:42 -0700",Re: SparkSubmit and --driver-java-options,dev@spark.apache.org,"Cool, that seems to work. Thanks!




-- 
Marcelo

"
Patrick Wendell <pwendell@gmail.com>,"Wed, 30 Apr 2014 14:26:34 -0700",Re: SparkSubmit and --driver-java-options,"""dev@spark.apache.org"" <dev@spark.apache.org>","Dean - our e-mails crossed, but thanks for the tip. Was independently
arriving at your solution :)

Okay I'll submit something.

- Patrick


"
