Reynold Xin <rxin@databricks.com>,"Tue, 30 Jun 2015 17:27:23 -0700",Re: [VOTE] Release Apache Spark 1.4.1,Patrick Wendell <pwendell@gmail.com>,1
Joseph Bradley <joseph@databricks.com>,"Tue, 30 Jun 2015 19:38:44 -0700",Re: [VOTE] Release Apache Spark 1.4.1,Reynold Xin <rxin@databricks.com>,1
Bobby Chowdary <bobby.chowdary03@gmail.com>,"Tue, 30 Jun 2015 20:12:19 -0700",Re: [VOTE] Release Apache Spark 1.4.1,dev@spark.apache.org,"+1 Tested on CentOS 7

"
Tathagata Das <tathagata.das1565@gmail.com>,"Wed, 1 Jul 2015 02:28:24 -0700",Re: [VOTE] Release Apache Spark 1.4.1,Bobby Chowdary <bobby.chowdary03@gmail.com>,1
"""Nick Pentreath"" <nick.pentreath@gmail.com>","Wed, 01 Jul 2015 08:26:24 -0700 (PDT)",Re: HyperLogLogUDT,dev@spark.apache.org,"Any thoughts?



â€”
Sent from Mailbox

com>

structure
various
com/MLnick/hive-udf)
 not
as
with
 field"" for"
Daniel Darabos <daniel.darabos@lynxanalytics.com>,"Wed, 1 Jul 2015 18:32:46 +0200",Re: HyperLogLogUDT,Nick Pentreath <nick.pentreath@gmail.com>,"It's already possible to just copy the code from countApproxDistinct
<https://github.com/apache/spark/blob/v1.4.0/core/src/main/scala/org/apache/spark/rdd/RDD.scala#L1153>
and
access the HLL directly, or do anything you like.


m
e
s
or
"
Stephen Boesch <javadba@gmail.com>,"Wed, 1 Jul 2015 15:53:49 -0700",Re: enum-like types in Spark,Xiangrui Meng <mengxr@gmail.com>,"I am reviving an old thread here. The link for the example code for the
java enum based solution is now dead: would someone please post an updated
link showing the proper interop?

Specifically: it is my understanding that java enum's may not be created
within Scala.  So is the proposed solution requiring dropping out into Java
to create the enum's?

2015-04-09 17:16 GMT-07:00 Xiangrui Meng <mengxr@gmail.com>:

"
Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"Wed, 1 Jul 2015 21:10:16 -0700 (MST)","[pyspark] What is the best way to run a minimum unit testing
 related to our developing module?",dev@spark.apache.org,"Hi all,

When I develop pyspark modules, such as adding a spark.ml API in Python, I'd
like to run a minimum unit testing related to the developing module again
and again. 
In the previous version, that was easy with commenting out unrelated modules
in the ./python/run-tests script. So what is the best way to run a minimum
unit testing related to our developing modules under the current version?
Of course, I think it would be nice to be able to identify testing targets
with the script like scala's sbt.

Thanks,
Yu



-----
-- Yu Ishikawa
--

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Wed, 1 Jul 2015 21:13:03 -0700","Re: [pyspark] What is the best way to run a minimum unit testing
 related to our developing module?",Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"Run

./python/run-tests --help

and you will see. :)


"
Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"Wed, 1 Jul 2015 21:22:41 -0700 (MST)","Re: [pyspark] What is the best way to run a minimum unit testing
 related to our developing module?",dev@spark.apache.org,"Thanks! --Yu



-----
-- Yu Ishikawa
--

---------------------------------------------------------------------


"
Yu ISHIKAWA <yuu.ishikawa@gmail.com>,"Thu, 2 Jul 2015 13:19:47 +0900","Re: [pyspark] What is the best way to run a minimum unit testing
 related to our developing module?",Reynold Xin <rxin@databricks.com>,"Thanks!  --Yu

2015-07-02 13:13 GMT+09:00 Reynold Xin <rxin@databricks.com>:

"
"""Nick Pentreath"" <nick.pentreath@gmail.com>","Wed, 01 Jul 2015 23:00:11 -0700 (PDT)",Re: HyperLogLogUDT,"""Daniel Darabos"" <daniel.darabos@lynxanalytics.com>","Sure I can copy the code but my aim was more to understand:




(A) if this is broadly interesting enough to folks to think about updating / extending the existing UDAF within Spark

(b) how to register ones own custom UDAF - in which case it could be a Spark package for exampleÂ 




All examples deal with registering a UDF but nothing about UDAFs



â€”
Sent from Mailbox


he/spark/rdd/RDD.scala#L1153>
com>
com
structure
distinct
various
So
a
containing the
am
adapt
as
play
""default field"" for"
Reynold Xin <rxin@databricks.com>,"Wed, 1 Jul 2015 23:06:50 -0700",Re: HyperLogLogUDT,Nick Pentreath <nick.pentreath@gmail.com>,"Yes - it's very interesting. However, ideally we should have a version of
hyperloglog that can work directly against some raw bytes in memory (rather
than java objects), in order for this to fit the Tungsten execution model
where everything is operating directly against some memory address.


g
che/spark/rdd/RDD.scala#L1153> and
to
le to
m
pt
s
 for
"
Sean Owen <sowen@cloudera.com>,"Thu, 2 Jul 2015 08:57:41 +0100",Re: [VOTE] Release Apache Spark 1.4.1,Patrick Wendell <pwendell@gmail.com>,"I wanted to flag a potential blocker here, but pardon me if this is
still after all this time just my misunderstanding of the POM/build
theory --

So this is the final candiate release POM right?
https://repository.apache.org/content/repositories/orgapachespark-1118/org/apache/spark/spark-core_2.10/1.4.1/spark-core_2.10-1.4.1.pom

Compare to for example:
https://repo1.maven.org/maven2/org/apache/spark/spark-core_2.10/1.4.0/spark-core_2.10-1.4.0.pom

and see:

https://issues.apache.org/jira/browse/SPARK-8781

For instance, in 1.4.0 it had

<dependency>
<groupId>org.apache.spark</groupId>
<artifactId>spark-launcher_2.10</artifactId>
<version>1.4.0</version>
<scope>compile</scope>
</dependency>

but now that's:

<dependency>
<groupId>org.apache.spark</groupId>
<artifactId>spark-launcher_${scala.binary.version}</artifactId>
<version>${project.version}</version>
</dependency>

JIRA suggests it had to do with adding:

<createDependencyReducedPom>false</createDependencyReducedPom>

Am I missing something or is that indeed not going to work as a release POM?


---------------------------------------------------------------------


"
shenyan zhen <shenyanls@gmail.com>,"Thu, 2 Jul 2015 08:33:04 -0400",Re: [VOTE] Release Apache Spark 1.4.1,Reynold Xin <rxin@databricks.com>,1
Imran Rashid <irashid@cloudera.com>,"Thu, 2 Jul 2015 08:14:22 -0500",Re: enum-like types in Spark,Stephen Boesch <javadba@gmail.com>,"Hi Stephen,

I'm not sure which link you are referring to for the example code -- but
yes, the recommendation is that you create the enum in Java, eg. see

https://github.com/apache/spark/blob/v1.4.0/core/src/main/java/org/apache/spark/status/api/v1/StageStatus.java

Then nothing special is required to use it in scala.  This method both uses
the overall type of the enum in the return value, and uses specific values
in the body:

https://github.com/apache/spark/blob/v1.4.0/core/src/main/scala/org/apache/spark/status/api/v1/AllStagesResource.scala#L114

(I did delete the branches for the code that is *not* recommended anymore)

Imran



"
vaquar khan <vaquar.khan@gmail.com>,"Thu, 2 Jul 2015 19:51:28 +0530",Re: [VOTE] Release Apache Spark 1.4.1,shenyan zhen <shenyanls@gmail.com>,1
Patrick Wendell <pwendell@gmail.com>,"Thu, 2 Jul 2015 08:22:55 -0700",Re: [VOTE] Release Apache Spark 1.4.1,vaquar khan <vaquar.khan@gmail.com>,"Hey Sean - yes I think that is an issue. Our published poms need to
have the dependency versions inlined.

We probably need to revert that bit of the build patch.

- Patrick


---------------------------------------------------------------------


"
"""prateek3.14"" <prateek3.14@gmail.com>","Thu, 2 Jul 2015 06:24:52 -0700 (MST)",Size of RDD partitions,dev@spark.apache.org,"Hello everyone,
      Are there metrics for capturing the size of RDD partitions? Would the
memory usage of an executor be a good proxy for the same?

Thanks,
--Prateek




--"
Eron Wright <ewright@live.com>,"Thu, 2 Jul 2015 09:03:04 -0700",[SPARK-8794] [SQL] PrunedScan problem,"""dev@spark.apache.org"" <dev@spark.apache.org>","I filed an issue due to an issue I see with PrunedScan, that causes sub-optimal performance in ML pipelines.   
Sorry if the issue is already known.
Having tried a few approaches to working with large binary files with Spark ML, I prefer loading the data into a vector-type column from a relation supporting pruned scan.  This is better, I think, than a lazy-loading scheme based on binaryFiles/PortalDataStream.   SPARK-8794 undermines the approach.
Eron 		 	   		  "
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Thu, 2 Jul 2015 09:10:32 -0700",Re: [VOTE] Release Apache Spark 1.4.1,Patrick Wendell <pwendell@gmail.com>,"+1 Tested the EC2 launch scripts and the Spark version and EC2 branch etc.
look good.

Shivaram


"
Chris Harvey <ctharve@gmail.com>,"Thu, 2 Jul 2015 12:14:54 -0400",A proposal for Test matrix decompositions for speed/stability (SPARK-7210),dev@spark.apache.org,"Hello,

I am new to the Apache Spark project but I would like to contribute to
issue SPARK-7210. There has been come conversation on that issue and I
would like to take a shot at it. Before doing so, I want to run my plan by
everyone.

computing the  MVN pdf. The stated concern is that the SVD used is slow
despite it being numerically stable, and that speed and stability may
become problematic as the number of features grow.

In the comments, Feynman posted an R recipe for computing the pdf using a
Cholesky trick. I would like to compute the pdf by following that recipe
while using the Cholesky implementation found in Scalanlp Breeze. To test
speed I would estimate the pdf using the original method and the Cholesky
method across a range of simulated datasets with growing n and p. To test
stability I would estimate the pdf on simulated features with some
multicollinearity.

Does this sound like a good starting point? Am I thinking of this correctly?

Given that this is my first attempt at contributing to an Apache project,
might it be a good idea to do this through the Mentor Programme?

Please let me know how this sounds, and I can provide some personal details
about my experience and motivations.

Thanks,

Chris
"
Mohit Jaggi <mohitjaggi@gmail.com>,"Thu, 2 Jul 2015 10:27:43 -0700",Re: Grouping runs of elements in a RDD,RJ Nowling <rnowling@gmail.com>,"if you are joining successive lines together based on a predicate, then you
are doing a ""flatMap"" not an ""aggregate"". you are on the right track with a
multi-pass solution. i had the same challenge when i needed a sliding
window over an RDD(see below).

[ i had suggested that the sliding window API be moved to spark-core. not
sure if that happened ]

----- previous posts ---

http://spark.apache.org/docs/1.4.0/api/scala/index.html#org.apache.spark.mllib.rdd.RDDFunctions

pKN65rOLzbETC+Ddk4O+YJm+TfAF5DZ8EuCpL-2YHYPZA@mail.gmail.com%3E
s.
ave a map
â€™s
in
t it might be



le
d
k
 my
ons.
:
:
s
ory.
a
ich
"
Andrew Or <andrew@databricks.com>,"Thu, 2 Jul 2015 11:11:19 -0700",Re: [VOTE] Release Apache Spark 1.4.1,Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"@Sean I believe that is a real issue. I have submitted a patch to fix it:
https://github.com/apache/spark/pull/7193. Unfortunately this would mean we
need to cut a new RC to include it. When we do so we should also do another
careful pass over the commits"
RJ Nowling <rnowling@gmail.com>,"Thu, 2 Jul 2015 14:40:41 -0500",Re: Grouping runs of elements in a RDD,Mohit Jaggi <mohitjaggi@gmail.com>,"Thanks, Mohit.  It sounds like we're on the same page -- I used a similar
approach.


mllib.rdd.RDDFunctions
VTpKN65rOLzbETC+Ddk4O+YJm+TfAF5DZ8EuCpL-2YHYPZA@mail.gmail.com%3E
his.
 have a map
aâ€™s
d in
but it might be
gle
e
nd
s a
n my
ions.
e
mory.
 a
hich
"
jamaica <myunsoo@gmail.com>,"Thu, 2 Jul 2015 23:11:56 -0700 (MST)",Differential Equation Spark Solver,dev@spark.apache.org,"Dear Spark Devs,

I have written an  experimental 1d laplace parallel Spark solver
<http://myunsoo-dataworks.blogspot.kr/2015/06/solve-differential-equations-with-spark.html> 
, out of curiousity regarding  this
<http://apache-spark-user-list.1001560.n3.nabble.com/Solving-Systems-of-Linear-Equations-Using-Spark-td13674.html#a23354>  
thread. 
It still may have some unknown bugs and only works for very special cases
but still, i think it shows it can be done with Spark.

When solving large scale differential equations, one may consider using
MPI/PVM based parallel machines because of their raw power and efficiency.
But then again I think Spark (in-memory) map-reduce can also be reasonably
fast and easy to use, with some tweaks.

- For example a tweak such as transferring data only between specific
clusters will significantly improve both memory efficiency and speed.
(partial broadcasting?)

But still I do not know how many people may also be interested in this and
how much contribution to Spark scene it can make.
So if you are interested building Differential Equation Spark Solver, or can
suggest where I can discuss this with potentially interested parties, or
have any other suggestions / information, please let me know!

Thank you.

Best regards,
Myunsoo



--

---------------------------------------------------------------------


"
Krishna Sankar <ksankar42@gmail.com>,"Thu, 2 Jul 2015 23:54:58 -0700",except vs subtract,"""dev@spark.apache.org"" <dev@spark.apache.org>","Guys,
   Scala says except while python has subtract. (I verified that except
doesn't exist in python) Why the difference in syntax for the same
functionality ?
Cheers
<k/>
"
Reynold Xin <rxin@databricks.com>,"Thu, 2 Jul 2015 23:57:12 -0700",Re: except vs subtract,Krishna Sankar <ksankar42@gmail.com>,"""except"" is a keyword in Python unfortunately.




"
StanZhai <mail@zhaishidan.cn>,"Thu, 2 Jul 2015 23:58:50 -0700 (MST)","[SparkSQL 1.4.0]The result of SUM(xxx) in SparkSQL is 0.0 but not
 null when the column xxx is all null",dev@spark.apache.org,"Hi all, 

I have a table named test like this:

|  a  |  b  |
|  1  | null |
|  2  | null |

After upgraded the cluster from spark 1.3.1 to 1.4.0, I found the Sum
function in spark 1.4 and 1.3 are different.

The SQL is: select sum(b) from test

In Spark 1.4.0 the result is 0.0, in spark 1.3.1 the result is null. I think
the result should be null, why the result is 0.0 in 1.4.0 but not null? Is
this a bug?

Any hint is appreciated.



--

---------------------------------------------------------------------


"
Krishna Sankar <ksankar42@gmail.com>,"Fri, 3 Jul 2015 00:11:00 -0700",Re: except vs subtract,Reynold Xin <rxin@databricks.com>,"Thanks. Forgot about that ;o(


"
Sean Owen <sowen@cloudera.com>,"Fri, 3 Jul 2015 10:21:39 +0100",Re: [VOTE] Release Apache Spark 1.4.1,Andrew Or <andrew@databricks.com>,"Great, thanks for the fix.

Anything marked as fixed for 1.4.2 should now be marked as fixed for
1.4.1 right? I saw you were already updating many of those; OK to
finish that?

pretty safe. A few things are kind of minor behavior changes like
https://issues.apache.org/jira/browse/SPARK-8630  Still probably not
wrong to include.

Ideally these would not be merged in branch 1.4 while the RC process
1.4.1 is cut, should be mark everything else merged into branch 1.4 as
fixed for 1.4.2? and then if a new RC is cut, mark them as fixed for
1.4.1 instead? that's a nice easy convention.

Anything that might be mergeable for a later 1.4.x release but
shouldn't go into 1.4.1 could be left out of branch 1.4 and marked as
backport-needed. That should be a rare occasion for late in the RC
process.

Obviously, the faster the RC process goes the smaller this issue is -
the smaller and more disciplined the list of target issues is when the
process is, the better.


---------------------------------------------------------------------


"
=?UTF-8?Q?Zolt=C3=A1n_Zvara?= <zoltan.zvara@gmail.com>,"Fri, 03 Jul 2015 14:34:57 +0000",SparkSqlSerializer2,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

Is there any way to bypass the limitations of SparkSqlSerializer2 in module
SQL? Said that,
1) it does not support complex types,
2) assumes key-value pairs.

Is there any other pluggable serializer that can be used here?

Thanks!
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 03 Jul 2015 17:23:29 +0000",Should spark-ec2 get its own repo?,Spark dev list <dev@spark.apache.org>,"spark-ec2 is kind of a mini project within a project.

Itâ€™s composed of a set of EC2 AMIs
<https://github.com/mesos/spark-ec2/tree/branch-1.4/ami-list> under
someoneâ€™s account (maybe Patrickâ€™s?) plus the following 2 code bases:

   - Main command line tool: https://github.com/apache/spark/tree/master/ec2
   - Scripts used to install stuff on launched instances:
   https://github.com/mesos/spark-ec2

Youâ€™ll notice that part of the code lives under the Mesos GitHub
organization. This is an artifact of history, when Spark itself kinda grew
out of Mesos before becoming its own project.

There are a few issues with this state of affairs, none of which are major
but which nonetheless merit some discussion:

   - The spark-ec2 code is split across 2 repositories when it is not
   technically necessary.
   - Some of that code is owned by an organization that should technically
   not be owning Spark stuff.
   - Spark and spark-ec2 live in the same repo but spark-ec2 issues are
   often completely disjoint from issues with Spark itself. This has led in
   some cases to new Spark RCs being cut because of minor issues with
   spark-ec2 (like version strings not being updated).

I wanted to put up for discussion a few suggestions and see what people
agreed with.

   1. The current state of affairs is fine and it is not worth moving stuff
   around.
   2. spark-ec2 should get its own repo, and should be moved out of the
   main Spark repo. That means both of the code bases linked above would live
   in one place (maybe a spark-ec2/spark-ec2 repo).
   3. spark-ec2 should stay in the Spark repo, but the stuff under the
   Mesos organization should be moved elsewhere (again, perhaps under a
   spark-ec2/spark-ec2 repo).

What do you think?

Nick
â€‹
"
Sean Owen <sowen@cloudera.com>,"Fri, 3 Jul 2015 18:36:41 +0100",Re: Should spark-ec2 get its own repo?,Nicholas Chammas <nicholas.chammas@gmail.com>,"I'll render an opinion although I'm only barely qualified by having
just had a small discussion on this --

It does seem like mesos/spark-ec2 is in the wrong place, although
really, that is at best an issue for Mesos. But it does highlight that
the Spark EC2 support doesn't entirely live with and get distributed
with apache/spark.

It does feel like that should move and should not be separate from the
other half of EC2 support. Why not put it in apache/spark? I think the
problem is that the AMI process clones the repo, and the apache/spark
way of releasing the EC2 files as a downloadable archive.

However, if it is true that the Spark EC2 support doesn't need to live
with and get released with the rest of Spark, it might make more sense
to merge both halves into a new separate repo and run it separately
from apache/spark, like any other third-party repo.

I think that's less radical than it sounds, and has some benefits.
There is not quite the same argument of needing to build and maintain
this together like with language bindings and subprojects.

But is that something that people who use and maintain it agree with
or are advocating for?

t (maybe
w
r
lly
 be
ark
ace

---------------------------------------------------------------------


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Fri, 3 Jul 2015 10:47:11 -0700",Re: Should spark-ec2 get its own repo?,Sean Owen <sowen@cloudera.com>,"As the person maintaining the mesos/spark-ec2 repo, here are my 2 cents

- I don't think it makes sense to put the scripts in the Spark repo itself.
Cloning the scripts on the EC2 instances is an intentional design which
allows us to make minor config changes in EC2 launches without needing a
new Spark release.

- I think having some script to launch EC2 clusters that is a part of
mainline Spark is a nice feature to have. However this could be a very thin
wrapper instead of the big Python file we have right now.

- Moving the scripts from the Mesos organization to spark-ec2 or amplab is
fine by me. In fact one nice way to do this transition would be to move the
existing spark-ec2 repo to a new organization and then move the logic from
 the launcher script out of the Spark to the new repo.

Thanks
Shivaram





unt (maybe
b
n
"
Patrick Wendell <pwendell@gmail.com>,"Fri, 3 Jul 2015 13:12:21 -0700",[RESULT] [VOTE] Release Apache Spark 1.4.1,"""dev@spark.apache.org"" <dev@spark.apache.org>","This vote is cancelled in favor of RC2. Thanks very much to Sean Owen
for triaging an important bug associated with RC1.

I took a look at the branch-1.4 contents and I think its safe to cut
RC2 from the head of that branch (i.e no very high risk patches that I
could see). JIRA management around the time of the RC voting is an
interesting topic, Sean I like your most recent proposal. Maybe we can
put that on the wiki or start a DISCUSS thread to cover that topic.


---------------------------------------------------------------------


"
Tarek Auel <tarek.auel@gmail.com>,"Fri, 03 Jul 2015 20:13:19 +0000",Can not build master,dev <dev@spark.apache.org>,"Hi all,

I am trying to build the master, but it stucks and prints

[INFO] Dependency-reduced POM written at:
/Users/tarek/test/spark/bagel/dependency-reduced-pom.xml

build command:  mvn -DskipTests clean package

Do others have the same issue?

Regards,
Tarek
"
Patrick Wendell <pwendell@gmail.com>,"Fri, 3 Jul 2015 13:15:42 -0700",[VOTE] Release Apache Spark 1.4.1 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version 1.4.1!

This release fixes a handful of known issues in Spark 1.4.0, listed here:
http://s.apache.org/spark-1.4.1

The tag to be voted on is v1.4.1-rc2 (commit 07b95c7):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=
07b95c7adf88f0662b7ab1c47e302ff5e6859606

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.4.1-rc2-bin/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
[published as version: 1.4.1]
https://repository.apache.org/content/repositories/orgapachespark-1120/
[published as version: 1.4.1-rc2]
https://repository.apache.org/content/repositories/orgapachespark-1121/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.4.1-rc2-docs/

Please vote on releasing this package as Apache Spark 1.4.1!

The vote is open until Monday, July 06, at 22:00 UTC and passes
if a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.4.1
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Fri, 3 Jul 2015 14:21:12 -0700",Re: Can not build master,Tarek Auel <tarek.auel@gmail.com>,"This is what I got (the last line was repeated non-stop):

[INFO] Replacing original artifact with shaded artifact.
[INFO] Replacing
/home/hbase/spark/bagel/target/spark-bagel_2.10-1.5.0-SNAPSHOT.jar with
/home/hbase/spark/bagel/target/spark-bagel_2.10-1.5.0-SNAPSHOT-shaded.jar
[INFO] Dependency-reduced POM written at:
/home/hbase/spark/bagel/dependency-reduced-pom.xml
[INFO] Dependency-reduced POM written at:
/home/hbase/spark/bagel/dependency-reduced-pom.xml


"
Robin East <robin.east@xense.co.uk>,"Fri, 3 Jul 2015 22:28:25 +0100",Re: Can not build master,Ted Yu <yuzhihong@gmail.com>,"Yes me too
/home/hbase/spark/bagel/target/spark-bagel_2.10-1.5.0-SNAPSHOT.jar with /home/hbase/spark/bagel/target/spark-bagel_2.10-1.5.0-SNAPSHOT-shaded.jar
/home/hbase/spark/bagel/dependency-reduced-pom.xml
/home/hbase/spark/bagel/dependency-reduced-pom.xml
/Users/tarek/test/spark/bagel/dependency-reduced-pom.xml

"
Ted Yu <yuzhihong@gmail.com>,"Fri, 3 Jul 2015 14:40:46 -0700",Re: [VOTE] Release Apache Spark 1.4.1 (RC2),Patrick Wendell <pwendell@gmail.com>,"Patrick:
I used the following command:
~/apache-maven-3.3.1/bin/mvn -DskipTests -Phadoop-2.4 -Pyarn -Phive clean
package

The build doesn't seem to stop.
Here is tail of build output:

[INFO] Dependency-reduced POM written at:
/home/hbase/spark-1.4.1/bagel/dependency-reduced-pom.xml
[INFO] Dependency-reduced POM written at:
/home/hbase/spark-1.4.1/bagel/dependency-reduced-pom.xml

Here is part of the stack trace for the build process:

http://pastebin.com/xL2Y0QMU

FYI


"
Tarek Auel <tarek.auel@gmail.com>,"Fri, 03 Jul 2015 22:05:50 +0000",Re: Can not build master,"Robin East <robin.east@xense.co.uk>, Ted Yu <yuzhihong@gmail.com>","I found a solution, there might be a better one.

https://github.com/apache/spark/pull/7217


"
Ted Yu <yuzhihong@gmail.com>,"Fri, 3 Jul 2015 15:14:35 -0700",Re: Can not build master,Tarek Auel <tarek.auel@gmail.com>,"Please take a look at SPARK-8781 (https://github.com/apache/spark/pull/7193)

Cheers


"
Krishna Sankar <ksankar42@gmail.com>,"Fri, 3 Jul 2015 15:19:42 -0700",Re: [VOTE] Release Apache Spark 1.4.1 (RC2),Ted Yu <yuzhihong@gmail.com>,"Yep, happens to me as well. Build loops.
Cheers
<k/>


"
Patrick Wendell <pwendell@gmail.com>,"Fri, 3 Jul 2015 15:41:12 -0700",Re: [VOTE] Release Apache Spark 1.4.1 (RC2),Krishna Sankar <ksankar42@gmail.com>,"What if you use the built-in maven (i.e. build/mvn). It might be that
we require a newer version of maven than you have. The release itself
is built with maven 3.3.3:

https://github.com/apache/spark/blob/master/build/mvn#L72

- Patrick


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Fri, 3 Jul 2015 23:43:12 +0100",Re: [VOTE] Release Apache Spark 1.4.1 (RC2),Patrick Wendell <pwendell@gmail.com>,"Sorry to say same happens on 3.3.3. I tried Shade 2.4 too. It is
indeed MSHADE-148 that Andrew was trying to fix in the first place.
I'm also trying to think of workarounds here.


---------------------------------------------------------------------


"
Robin East <robin.east@xense.co.uk>,"Fri, 3 Jul 2015 23:44:01 +0100",Re: [VOTE] Release Apache Spark 1.4.1 (RC2),Patrick Wendell <pwendell@gmail.com>,"I used the following build command:

build/mvn -Pyarn -Phadoop-2.4 -Dhadoop.version=2.4.0 -DskipTests clean package

this also gave the â€˜Dependency-reduced POMâ€™ loop

Robin
clean
version
here:

at:
http://people.apache.org/~pwendell/spark-releases/spark-1.4.1-rc2-bin/
https://repository.apache.org/content/repositories/orgapachespark-1120/
https://repository.apache.org/content/repositories/orgapachespark-1121/
http://people.apache.org/~pwendell/spark-releases/spark-1.4.1-rc2-docs/
---------------------------------------------------------------------

"
Patrick Wendell <pwendell@gmail.com>,"Fri, 3 Jul 2015 15:45:13 -0700",Re: Can not build master,Ted Yu <yuzhihong@gmail.com>,"Can you try using the built in maven ""build/mvn...""? All of our builds
are passing on Jenkins so I wonder if it's a maven version issue:

https://amplab.cs.berkeley.edu/jenkins/view/Spark-QA-Compile/

- Patrick


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Fri, 3 Jul 2015 15:49:35 -0700",Re: [VOTE] Release Apache Spark 1.4.1 (RC2),Robin East <robin.east@xense.co.uk>,"Hm - what if you do a fresh git checkout (just to make sure you don't
have an older maven version downloaded). It also might be that this
really is an issue even with Maven 3.3.3. I just am not sure why it's
not reflected in our continuous integration or the build of the
release packages themselves:

https://amplab.cs.berkeley.edu/jenkins/view/Spark-QA-Compile/

It could be that it's dependent on which modules are enabled.

512M;
œmac""
e:

---------------------------------------------------------------------


"
Tarek Auel <tarek.auel@gmail.com>,"Fri, 03 Jul 2015 22:51:21 +0000",Re: Can not build master,"Patrick Wendell <pwendell@gmail.com>, Ted Yu <yuzhihong@gmail.com>","Doesn't change anything for me.


"
Krishna Sankar <ksankar42@gmail.com>,"Fri, 3 Jul 2015 16:02:37 -0700",Re: [VOTE] Release Apache Spark 1.4.1 (RC2),Patrick Wendell <pwendell@gmail.com>,"I have 3.3.3
USS-Defiant:NW ksankar$ mvn -version
Apache Maven 3.3.3 (7994120775791599e205a5524ec3e0dfe41d4a06;
2015-04-22T04:57:37-07:00)
Maven home: /usr/local/apache-maven-3.3.3
Java version: 1.7.0_60, vendor: Oracle Corporation
Java home:
/Library/Java/JavaVirtualMachines/jdk1.7.0_60.jdk/Contents/Home/jre
Default locale: en_US, platform encoding: UTF-8
OS name: ""mac os x"", version: ""10.10.3"", arch: ""x86_64"", family: ""mac""

Let me nuke it and reinstall maven.

Cheers
<k/>


"
Patrick Wendell <pwendell@gmail.com>,"Fri, 3 Jul 2015 16:13:27 -0700",Re: [VOTE] Release Apache Spark 1.4.1 (RC2),Krishna Sankar <ksankar42@gmail.com>,"Thanks - it appears this is just a legitimate issue with the build,
affecting all versions of Maven.


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Fri, 3 Jul 2015 16:30:50 -0700",Re: [VOTE] Release Apache Spark 1.4.1 (RC2),Krishna Sankar <ksankar42@gmail.com>,"Let's continue the disucssion on the other thread relating to the master build.


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Fri, 3 Jul 2015 16:36:30 -0700",Re: Can not build master,Tarek Auel <tarek.auel@gmail.com>,"Okay I did some forensics with Sean Owen. Some things about this bug:

1. The underlying cause is that we added some code to make the tests
of sub modules depend on the core tests. For unknown reasons this
causes Spark to hit MSHADE-148 for *some* combinations of build
profiles.

2. MSHADE-148 can be worked around by disabling building of
""dependency reduced poms"" because then the buggy code path is
circumvented. Andrew Or did this in a patch on the 1.4 branch.
However, that is not a tenable option for us because our *published*
pom files require dependency reduction to substitute in the scala
version correctly for the poms published to maven central.

3. As a result, Andrew Or reverted his patch recently, causing some
package builds to start failing again (but publishing works now).

4. The reason this is not detected in our test harness or release
build is that it is sensitive to the profiles enabled. The combination
of profiles we enable in the test harness and release builds do not
trigger this bug.

The best path I see forward right now is to do the following:

1. Disable creation of dependency reduced poms by default (this
doesn't matter for people doing a package build) so typical users
won't have this bug.

2. Add a profile that re-enables that setting.

3. Use the above profile when publishing release artifacts to maven central.

4. Hope that we don't hit this bug for publishing.

- Patrick


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Fri, 3 Jul 2015 16:38:44 -0700",Re: Can not build master,Tarek Auel <tarek.auel@gmail.com>,"Patch that added test-jar dependencies:
https://github.com/apache/spark/commit/bfe74b34

Patch that originally disabled dependency reduced poms:
https://github.com/apache/spark/commit/984ad60147c933f2d5a2040c87ae687c14eb1724

Patch that reverted the disabling of dependency reduced poms:
https://github.com/apache/spark/commit/bc51bcaea734fe64a90d007559e76f5ceebfea9e


---------------------------------------------------------------------


"
Krishna Sankar <ksankar42@gmail.com>,"Fri, 3 Jul 2015 17:35:54 -0700",Re: Can not build master,Patrick Wendell <pwendell@gmail.com>,"Patrick,
   I assume an RC3 will be out for folks like me to test the distribution.
As usual, I will run the tests when you have a new distribution.
Cheers
<k/>


"
Andrew Or <andrew@databricks.com>,"Fri, 3 Jul 2015 18:05:21 -0700",Re: Can not build master,Krishna Sankar <ksankar42@gmail.com>,"@Tarek and Ted, what maven versions are you using?

2015-07-03 17:35 GMT-07:00 Krishna Sankar <ksankar42@gmail.com>:

"
Ted Yu <yuzhihong@gmail.com>,"Fri, 3 Jul 2015 18:32:21 -0700",Re: Can not build master,Andrew Or <andrew@databricks.com>,"Here is mine:

Apache Maven 3.3.1 (cab6659f9874fa96462afef40fcf6bc033d58c1c;
2015-03-13T13:10:27-07:00)
Maven home: /home/hbase/apache-maven-3.3.1
Java version: 1.8.0_45, vendor: Oracle Corporation
Java home: /home/hbase/jdk1.8.0_45/jre
Default locale: en_US, platform encoding: UTF-8
OS name: ""linux"", version: ""2.6.32-504.el6.x86_64"", arch: ""amd64"", family:
""unix""


"
Tarek Auel <tarek.auel@gmail.com>,"Sat, 04 Jul 2015 01:51:08 +0000",Re: Can not build master,"Ted Yu <yuzhihong@gmail.com>, Andrew Or <andrew@databricks.com>","That's mine

Apache Maven 3.3.3 (7994120775791599e205a5524ec3e0dfe41d4a06;
2015-04-22T04:57:37-07:00)

Maven home: /usr/local/Cellar/maven/3.3.3/libexec

Java version: 1.8.0_45, vendor: Oracle Corporation

Java home:
/Library/Java/JavaVirtualMachines/jdk1.8.0_45.jdk/Contents/Home/jre

Default locale: en_US, platform encoding: UTF-8

OS name: ""mac os x"", version: ""10.10.3"", arch: ""x86_64"", family: ""mac""


"
Andrew Or <andrew@databricks.com>,"Fri, 3 Jul 2015 20:28:30 -0700",Re: Can not build master,Tarek Auel <tarek.auel@gmail.com>,"Thanks, I just tried it with 3.3.3 and I was able to reproduce it as well.

2015-07-03 18:51 GMT-07:00 Tarek Auel <tarek.auel@gmail.com>:

"
tomo cocoa <cocoatomo77@gmail.com>,"Sat, 4 Jul 2015 23:00:30 +0900",Re: Can not build master,Andrew Or <andrew@databricks.com>,"Hi all,

I have a same error and it seems depending on Maven versions.

I tried building Spark using Maven with several versions on Jenkins.

+ Output of
""/Users/tomohiko/.jenkins/tools/hudson.tasks.Maven_MavenInstallation/mvn-3.3.3/bin/mvn
-version"":

Apache Maven 3.3.3 (7994120775791599e205a5524ec3e0dfe41d4a06;
2015-04-22T20:57:37+09:00)
Maven home:
/Users/tomohiko/.jenkins/tools/hudson.tasks.Maven_MavenInstallation/mvn-3.3.3
Java version: 1.8.0, vendor: Oracle Corporation
Java home: /Library/Java/JavaVirtualMachines/jdk1.8.0.jdk/Contents/Home/jre
Default locale: en_US, platform encoding: UTF-8
OS name: ""mac os x"", version: ""10.10.3"", arch: ""x86_64"", family: ""mac""

+ Jenkins Configuration:
Jenkins project type: Maven Project
Goals and options: -Phadoop-2.6 -DskipTests clean package

+ Maven versions and results:
3.3.3 -> infinite loop
3.3.1 -> infinite loop
3.2.5 -> SUCCESS


So do we prefer to build Spark with Maven 3.2.5?





-- 
class Cocoatomo:
    name = 'cocoatomo'
    email_address = 'cocoatomo77@gmail.com'
    twitter_id = '@cocoatomo'
"
Patrick Wendell <pwendell@gmail.com>,"Sat, 4 Jul 2015 08:31:17 -0700",Re: Can not build master,tomo cocoa <cocoatomo77@gmail.com>,"Hi Tomo,

For now you can do that as a work around. We are working on a fix for
this in the master branch but it may take a couple of days since the
issue is fairly complicated.

- Patrick


---------------------------------------------------------------------


"
Niranda Perera <niranda.perera@gmail.com>,"Sun, 5 Jul 2015 17:08:22 +0530","Re: Error in invoking a custom StandaloneRecoveryModeFactory in java
 env (Spark v1.3.0)",Josh Rosen <rosenville@gmail.com>,"Hi Josh,

I tried using the spark 1.4.0 upgrade.

here is the class I'm trying to use

package org.wso2.carbon.analytics.spark.core.util.master

import akka.serialization.Serialization
import org.apache.spark.SparkConf
import org.apache.spark.deploy.master._

class AnalyticsRecoveryModeFactoryScala(conf: SparkConf, serializer:
Serialization)
  extends StandaloneRecoveryModeFactory(conf, serializer) {

  override def createPersistenceEngine(): PersistenceEngine = new
      AnalyticsPersistenceEngine(conf, serializer)

  override def createLeaderElectionAgent(master: LeaderElectable):
LeaderElectionAgent = new
      AnalyticsLeaderElectionAgent(master)
}

object AnalyticsRecoveryModeFactoryScala {

}

when I invoke this factory from the master, I get a similar error as before

 org.wso2.carbon.analytics.spark.core.util.master.AnalyticsRecoveryModeFactory.<init>(org.apache.spark.SparkConf,
akka.serialization.Serialization)
akka.actor.ActorInitializationException: exception during creation
at akka.actor.ActorInitializationException$.apply(Actor.scala:164)
at akka.actor.ActorCell.create(ActorCell.scala:596)
at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:456)
at akka.actor.ActorCell.systemInvoke(ActorCell.scala:478)
at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:263)
at akka.dispatch.Mailbox.run(Mailbox.scala:219)
at
akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)
at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
at
scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
at
scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: java.lang.NoSuchMethodException:
org.wso2.carbon.analytics.spark.core.util.master.AnalyticsRecoveryModeFactory.<init>(org.apache.spark.SparkConf,
akka.serialization.Serialization)
at java.lang.Class.getConstructor0(Class.java:2810)
at java.lang.Class.getConstructor(Class.java:1718)
at org.apache.spark.deploy.master.Master.preStart(Master.scala:168)
at akka.actor.Actor$class.aroundPreStart(Actor.scala:470)
at org.apache.spark.deploy.master.Master.aroundPreStart(Master.scala:52)
at akka.actor.ActorCell.create(ActorCell.scala:580)
... 9 more


what could be the reason for this?

rgds


:
ala,
o
overyModeFactory.<init>(org.apache.spark.SparkConf,
$
coveryModeFactory.<init>(org.apache.spark.SparkConf,
ctDispatcher.scala:393)
ava:1339)
)
.java:107)
overyModeFactory.<init>(org.apache.spark.SparkConf,
)
sistance/components/analytics-processors/org.wso2.carbon.analytics.spark.core/src/main/java/org/wso2/carbon/analytics/spark/core/util/master/AnalyticsStandaloneRecoveryModeFactory.java
rsistance/components/analytics-processors/org.wso2.carbon.analytics.spark.core/src/main/java/org/wso2/carbon/analytics/spark/core/util/master/AnalyticsStandaloneRecoveryModeFactory.java>



-- 
Niranda
@n1r44 <https://twitter.com/N1R44>
https://pythagoreanscript.wordpress.com/
"
Niranda Perera <niranda.perera@gmail.com>,"Mon, 6 Jul 2015 02:05:03 +0530","Re: Error in invoking a custom StandaloneRecoveryModeFactory in java
 env (Spark v1.3.0)",Josh Rosen <rosenville@gmail.com>,"Hi,

Sorry this was a class loading issue at my side. Sorted it out.

Sorry if I caused any inconvenience

Rgds

Niranda Perera
+94 71 554 8430

ctory.<init>(org.apache.spark.SparkConf,
Dispatcher.scala:393)
a:1339)
9)
ava:107)
tory.<init>(org.apache.spark.SparkConf,
m
cala,
s's
coveryModeFactory.<init>(org.apache.spark.SparkConf,
ecoveryModeFactory.<init>(org.apache.spark.SparkConf,
actDispatcher.scala:393)
)
java:1339)
9)
d.java:107)
coveryModeFactory.<init>(org.apache.spark.SparkConf,
2)
rsistance/components/analytics-processors/org.wso2.carbon.analytics.spark.core/src/main/java/org/wso2/carbon/analytics/spark/core/util/master/AnalyticsStandaloneRecoveryModeFactory.java
ersistance/components/analytics-processors/org.wso2.carbon.analytics.spark.core/src/main/java/org/wso2/carbon/analytics/spark/core/util/master/AnalyticsStandaloneRecoveryModeFactory.java>
"
"""Huang, Jie"" <jie.huang@intel.com>","Mon, 6 Jul 2015 01:01:07 +0000",[SparkScore]Performance portal for Apache Spark - WW27,"""user@spark.apache.org"" <user@spark.apache.org>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","Performance Portal for Apache Spark
Description
________________________________
Each data point represents each workload runtime percent compared with the previous week. Different lines represents different workloads running on spark yarn-client mode.
Hardware
________________________________
CPU type: Intel(r) Xeon(r) CPU E5-2697 v2 @ 2.70GHz
Memory: 128GB
NIC: 10GbE
Disk(s): 8 x 1TB SATA HDD
Software
________________________________
JAVA version: 1.8.0_25
Hadoop version: 2.5.0-CDH5.3.2
HiBench version: 4.0
Spark on yarn-client mode
Cluster
________________________________
1 node for Master
10 nodes for Slave
Regular
Summary
The lower percent the better performance.
________________________________
Group

ww19

ww20

ww22

ww23

ww24

ww25

ww26

ww27

ww28

HiBench

9.1%

6.6%

6.0%

7.9%

-6.5%

-3.1%

-2.1%

-6.4%

-2.7%

spark-perf

4.1%

4.4%

-1.8%

4.1%

-4.7%

-4.6%

-5.4%

-4.6%

-12.8%

[http://01org.github.io/sparkscore/image/plaf1.time/overall.png]
Y-Axis: normalized completion time; X-Axis: Work Week.
The commit number can be found in the result table.
The performance score for each workload is normalized based on the elapsed time for 1.2 release. The lower the better.
Detail
________________________________
HiBench
________________________________
JOB

ww19

ww20

ww22

ww23

ww24

ww25

ww26

ww27

ww28

commit

489700c8

8e3822a0

530efe3e

90c60692

db81b9d8

4eb48ed1

32e3cdaa

ec784381

2b820f2a

sleep

%

%

-2.1%

-2.9%

-4.1%

12.8%

-5.1%

-4.5%

-3.1%

wordcount

17.6%

11.4%

8.0%

8.3%

-18.6%

-10.9%

6.9%

-12.9%

-10.0%

kmeans

92.1%

61.5%

72.1%

92.9%

86.9%

95.8%

123.3%

99.3%

127.9%

scan

-4.9%

-7.2%

%

-1.1%

-25.5%

-21.0%

-12.4%

-19.8%

-19.7%

bayes

-24.3%

-20.1%

-18.3%

-11.1%

-29.7%

-31.3%

-30.9%

-31.1%

-31.0%

aggregation

5.6%

10.5%

%

9.2%

-15.3%

-15.0%

-37.6%

-37.0%

-37.3%

join

4.5%

1.2%

%

1.0%

-12.7%

-13.9%

-16.4%

-17.8%

-14.8%

sort

-3.3%

-0.5%

-11.9%

-12.5%

-17.5%

-17.3%

-20.7%

-17.7%

-13.9%

pagerank

2.2%

3.2%

4.0%

2.9%

-11.4%

-13.0%

-11.4%

-10.1%

-12.0%

terasort

-7.1%

-0.2%

-9.5%

-7.3%

-16.7%

-17.0%

-16.3%

-11.9%

-13.1%

Comments: null means no such workload running or workload failed in this time.
[http://01org.github.io/sparkscore/image/plaf1.time/HiBench_workloads.png]
Y-Axis: normalized completion time; X-Axis: Work Week.
The commit number can be found in the result table.
The performance score for each workload is normalized based on the elapsed time for 1.2 release. The lower the better.
spark-perf
________________________________
JOB

ww19

ww20

ww22

ww23

ww24

ww25

ww26

ww27

ww28

commit

489700c8

8e3822a0

530efe3e

90c60692

db81b9d8

4eb48ed1

32e3cdaa

ec784381

2b820f2a

agg

13.2%

7.0%

%

18.3%

5.2%

2.5%

1.1%

3.0%

-18.8%

agg-int

16.4%

21.2%

%

9.6%

4.0%

8.2%

7.0%

7.5%

6.2%

agg-naive

4.3%

-2.4%

%

-0.8%

-6.7%

-6.8%

-8.5%

-6.9%

-15.5%

scheduling

-6.1%

-8.9%

-14.5%

-2.1%

-6.4%

-6.5%

-5.7%

-1.8%

-6.0%

count-filter

4.1%

1.0%

6.6%

6.8%

-10.2%

-10.4%

-9.8%

-10.4%

-18.0%

count

4.8%

4.6%

6.7%

8.0%

-7.3%

-7.0%

-8.0%

-7.4%

-15.1%

sort

-8.1%

-2.5%

-6.2%

-7.0%

-14.6%

-14.4%

-13.9%

-15.9%

-24.0%

sort-int

4.5%

15.3%

-1.6%

-0.1%

-1.5%

-2.2%

-5.3%

-5.0%

-11.3%

Comments: null means no such workload running or workload failed in this time.
[http://01org.github.io/sparkscore/image/plaf1.time/spark-perf_workloads.png]
Y-Axis: normalized completion time; X-Axis: Work Week.
The commit number can be found in the result table.
The performance score for each workload is normalized based on the elapsed time for 1.2 release. The lower the better.


"
Akhil Das <akhil@sigmoidanalytics.com>,"Mon, 6 Jul 2015 12:26:12 +0530",Re: [SparkScore]Performance portal for Apache Spark - WW27,"""Huang, Jie"" <jie.huang@intel.com>","Cool http://01org.github.io/sparkscore/plaf1.html

Thanks
Best Regards


e
r
r
ng]
r
"
"""Huang, Jie"" <jie.huang@intel.com>","Mon, 6 Jul 2015 07:45:32 +0000",RE: [SparkScore]Performance portal for Apache Spark - WW27,Akhil Das <akhil@sigmoidanalytics.com>,"Thanks. Thatâ€™s right.  You can find more intros from our web portal.

Thank you && Best Regards,
Grace ï¼ˆHuang Jie)

From: Akhil Das [mailto:akhil@sigmoidanalytics.com]
Sent: Monday, July 6, 2015 2:56 PM
To: Huang, Jie
Cc: user@spark.apache.org; dev@spark.apache.org
Subject: Re: [SparkScore]Performance portal for Apache Spark - WW27

Cool http://01org.github.io/sparkscore/plaf1.html

Thanks
Best Regards

On Mon, Jul 6, 2015 at 6:31 AM, Huang, Jie <jie.huang@intel.com<mailto:jie.huang@intel.com>> wrote:
Performance Portal for Apache Spark
Description
________________________________
Each data point represents each workload runtime percent compared with the previous week. Different lines represents different workloads running on spark yarn-client mode.
Hardware
________________________________
CPU type: IntelÂ® XeonÂ® CPU E5-2697 v2 @ 2.70GHz
Memory: 128GB
NIC: 10GbE
Disk(s): 8 x 1TB SATA HDD
Software
________________________________
JAVA version: 1.8.0_25
Hadoop version: 2.5.0-CDH5.3.2
HiBench version: 4.0
Spark on yarn-client mode
Cluster
________________________________
1 node for Master
10 nodes for Slave
Regular
Summary
The lower percent the better performance.
________________________________
Group

ww19

ww20

ww22

ww23

ww24

ww25

ww26

ww27

ww28

HiBench

9.1%

6.6%

6.0%

7.9%

-6.5%

-3.1%

-2.1%

-6.4%

-2.7%

spark-perf

4.1%

4.4%

-1.8%

4.1%

-4.7%

-4.6%

-5.4%

-4.6%

-12.8%

[http://01org.github.io/sparkscore/image/plaf1.time/overall.png]
Y-Axis: normalized completion time; X-Axis: Work Week.
The commit number can be found in the result table.
The performance score for each workload is normalized based on the elapsed time for 1.2 release. The lower the better.
Detail
________________________________
HiBench
________________________________
JOB

ww19

ww20

ww22

ww23

ww24

ww25

ww26

ww27

ww28

commit

489700c8

8e3822a0

530efe3e

90c60692

db81b9d8

4eb48ed1

32e3cdaa

ec784381

2b820f2a

sleep

%

%

-2.1%

-2.9%

-4.1%

12.8%

-5.1%

-4.5%

-3.1%

wordcount

17.6%

11.4%

8.0%

8.3%

-18.6%

-10.9%

6.9%

-12.9%

-10.0%

kmeans

92.1%

61.5%

72.1%

92.9%

86.9%

95.8%

123.3%

99.3%

127.9%

scan

-4.9%

-7.2%

%

-1.1%

-25.5%

-21.0%

-12.4%

-19.8%

-19.7%

bayes

-24.3%

-20.1%

-18.3%

-11.1%

-29.7%

-31.3%

-30.9%

-31.1%

-31.0%

aggregation

5.6%

10.5%

%

9.2%

-15.3%

-15.0%

-37.6%

-37.0%

-37.3%

join

4.5%

1.2%

%

1.0%

-12.7%

-13.9%

-16.4%

-17.8%

-14.8%

sort

-3.3%

-0.5%

-11.9%

-12.5%

-17.5%

-17.3%

-20.7%

-17.7%

-13.9%

pagerank

2.2%

3.2%

4.0%

2.9%

-11.4%

-13.0%

-11.4%

-10.1%

-12.0%

terasort

-7.1%

-0.2%

-9.5%

-7.3%

-16.7%

-17.0%

-16.3%

-11.9%

-13.1%

Comments: null means no such workload running or workload failed in this time.
[http://01org.github.io/sparkscore/image/plaf1.time/HiBench_workloads.png]
Y-Axis: normalized completion time; X-Axis: Work Week.
The commit number can be found in the result table.
The performance score for each workload is normalized based on the elapsed time for 1.2 release. The lower the better.
spark-perf
________________________________
JOB

ww19

ww20

ww22

ww23

ww24

ww25

ww26

ww27

ww28

commit

489700c8

8e3822a0

530efe3e

90c60692

db81b9d8

4eb48ed1

32e3cdaa

ec784381

2b820f2a

agg

13.2%

7.0%

%

18.3%

5.2%

2.5%

1.1%

3.0%

-18.8%

agg-int

16.4%

21.2%

%

9.6%

4.0%

8.2%

7.0%

7.5%

6.2%

agg-naive

4.3%

-2.4%

%

-0.8%

-6.7%

-6.8%

-8.5%

-6.9%

-15.5%

scheduling

-6.1%

-8.9%

-14.5%

-2.1%

-6.4%

-6.5%

-5.7%

-1.8%

-6.0%

count-filter

4.1%

1.0%

6.6%

6.8%

-10.2%

-10.4%

-9.8%

-10.4%

-18.0%

count

4.8%

4.6%

6.7%

8.0%

-7.3%

-7.0%

-8.0%

-7.4%

-15.1%

sort

-8.1%

-2.5%

-6.2%

-7.0%

-14.6%

-14.4%

-13.9%

-15.9%

-24.0%

sort-int

4.5%

15.3%

-1.6%

-0.1%

-1.5%

-2.2%

-5.3%

-5.0%

-11.3%

Comments: null means no such workload running or workload failed in this time.
[http://01org.github.io/sparkscore/image/plaf1.time/spark-perf_workloads.png]
Y-Axis: normalized completion time; X-Axis: Work Week.
The commit number can be found in the result table.
The performance score for each workload is normalized based on the elapsed time for 1.2 release. The lower the better.



"
Reynold Xin <rxin@databricks.com>,"Mon, 6 Jul 2015 13:27:47 -0700",Re: asf git merge currently not working,"""dev@spark.apache.org"" <dev@spark.apache.org>","Looks like it is back up now.



"
Michael Armbrust <michael@databricks.com>,"Mon, 6 Jul 2015 12:04:37 -0700","Re: [SparkSQL 1.4.0]The result of SUM(xxx) in SparkSQL is 0.0 but not
 null when the column xxx is all null",StanZhai <mail@zhaishidan.cn>,"This was a change that was made to match a wrong answer coming from older
versions of Hive.  Unfortunately I think its too late to fix this in the
1.4 branch (as I'd like to avoid changing answers at all in point
releases), but in Spark 1.5 we revert to the correct behavior.

https://issues.apache.org/jira/browse/SPARK-8828


"
Reynold Xin <rxin@databricks.com>,"Mon, 6 Jul 2015 13:06:56 -0700",asf git merge currently not working,"""dev@spark.apache.org"" <dev@spark.apache.org>","FYI there are some problems with ASF's git or ldap infra. As a result, we
cannot merge anything into Spark right now.

An infra ticket has been created:
https://issues.apache.org/jira/browse/INFRA-9932

Please watch/vote on that ticket for progress. Thanks.
"
Sean Owen <sowen@cloudera.com>,"Tue, 7 Jul 2015 11:05:22 +0100",Re: Unable to add to roles in JIRA,dev <dev@spark.apache.org>,"PS the resolution on this is just that we've hit a JIRA limit, since
the Contributor role is so big now.

We have a currently-unused Developer role that barely has different
permissions. I propose to move people that I recognize as regular
Contributors into the Developer group to make room. Practically
speaking, there's no difference. Just a heads up in case you see
changes here.

But for reference, new contributors should go to Contributors by default.


---------------------------------------------------------------------


"
Gil Vernik <GILV@il.ibm.com>,"Tue, 7 Jul 2015 13:12:24 +0300",TableScan vs PrunedScan,dev@spark.apache.org,"Hi All,

I wanted to experiment a little bit with TableScan and PrunedScan.
My first test was to print columns from various SQL queries. 
To make this test easier, i just took spark-csv and i replaced TableScan 
with PrunedScan. 
I then changed buildScan method of CsvRelation from 

def BuildScan = { 

to 

def buildScan(requiredColumns: Array[String]) = {?

This was the only modification i did to CsvRelation.scala.  And I added 
print of requiredColums to log.

I then took the same CSV file and run very simple SELECT query on it.
I noticed that when CsvRelation used TableScan - all worked correctly.
But when i used PrunedScan - it didn?t worked and returned empty columns / 
or columns in wrong order. 

Why is this happens? Is it some bug? Because I thought that PrunedScan 
suppose to work exactly the same as TableScan and i can modify freely 
TableScan to PrunedScan. I thought that the only difference is that 
buildScan of PrunedScan has requiredColumns as parameter.

Can someone explain me the behavior i saw?

I am using Spark 1.5 from trunk.
Thanks a lot
Gil."
Ram Sriharsha <sriharsha.ram@gmail.com>,"Tue, 7 Jul 2015 06:50:13 -0700",Re: TableScan vs PrunedScan,Gil Vernik <GILV@il.ibm.com>,"Hi Gil

You would need to prune the resulting Row as well based on the requested columns.

Ram

Sent from my iPhone

ith PrunedScan. 
int of requiredColums to log. 
olumns / or columns in wrong order.  
pose to work exactly the same as TableScan and i can modify freely TableScan to PrunedScan. I thought that the only difference is that buildScan of PrunedScan has requiredColumns as parameter. 
"
swetha <swethakasireddy@gmail.com>,"Tue, 7 Jul 2015 09:47:02 -0700 (MST)",Regarding master node failure,dev@spark.apache.org,"Hi,

What happens if a master node fails in the case of Spark Streaming? Would
the data be lost in that case?

Thanks,
Swetha



--

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Tue, 7 Jul 2015 11:59:07 -0700",[RESULT] [VOTE] Release Apache Spark 1.4.1 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey All,

This vote is cancelled in favor of RC3.

- Patrick


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Tue, 7 Jul 2015 12:06:58 -0700",[VOTE] Release Apache Spark 1.4.1 (RC3),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version 1.4.1!

This release fixes a handful of known issues in Spark 1.4.0, listed here:
http://s.apache.org/spark-1.4.1

The tag to be voted on is v1.4.1-rc3 (commit 3e8ae38):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=
3e8ae38944f13895daf328555c1ad22cd590b089

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.4.1-rc3-bin/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
[published as version: 1.4.1]
https://repository.apache.org/content/repositories/orgapachespark-1123/
[published as version: 1.4.1-rc3]
https://repository.apache.org/content/repositories/orgapachespark-1124/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.4.1-rc3-docs/

Please vote on releasing this package as Apache Spark 1.4.1!

The vote is open until Friday, July 10, at 20:00 UTC and passes
if a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.4.1
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

---------------------------------------------------------------------


"
swetha <swethakasireddy@gmail.com>,"Tue, 7 Jul 2015 12:35:55 -0700 (MST)",Data interaction between various RDDs in Spark Streaming,dev@spark.apache.org,"Hi,

Suppose I want the data to be grouped by and Id named ""12345"" and I have
certain amount of data coming out from one batch for ""12345"" and I have data
related to ""12345"" coming after 5 hours, how do I group by ""12345"" and have
a single RDD of list?

Thanks,
Swetha



--

---------------------------------------------------------------------


"
Andrew Or <andrew@databricks.com>,"Tue, 7 Jul 2015 12:38:24 -0700",Re: [VOTE] Release Apache Spark 1.4.1 (RC3),Patrick Wendell <pwendell@gmail.com>,"+1

Verified that the previous blockers SPARK-8781 and SPARK-8819 are now
resolved.

2015-07-07 12:06 GMT-07:00 Patrick Wendell <pwendell@gmail.com>:

"
Akhil Das <akhil@sigmoidanalytics.com>,"Wed, 8 Jul 2015 01:18:29 +0530",Re: Data interaction between various RDDs in Spark Streaming,swetha <swethakasireddy@gmail.com>,"UpdatestateByKey?

Thanks
Best Regards


"
Reynold Xin <rxin@databricks.com>,"Tue, 7 Jul 2015 13:39:51 -0700",Re: Unable to add to roles in JIRA,Sean Owen <sowen@cloudera.com>,"I've been adding people to the developer role to get around the jira limit.



"
Sean Owen <sowen@cloudera.com>,"Tue, 7 Jul 2015 21:42:42 +0100",Re: Unable to add to roles in JIRA,Reynold Xin <rxin@databricks.com>,"Yeah, I've just realized a problem, that the permission for Developer
are not the same as Contributor. It includes the ability to Assign,
but doesn't seem to include other more basic permission.

I cleared room in Contributor the meantime (no point in having
Committers there; Committer permission is a superset), and I think we
can actually fix this long-term by just removing barely-active people
from Contributor since it won't matter (they only need to be in the
group to be Assigned usually).

I've also pinged the ticket to get more control over JIRA permissions
so we can rectify more of this.


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Tue, 7 Jul 2015 13:46:26 -0700",Re: Unable to add to roles in JIRA,Sean Owen <sowen@cloudera.com>,"BTW Infra has the ability to create multiple groups. Maybe that's a better
solution.

Have contributor1, contributor2, contributor3 ...


"
Krishna Sankar <ksankar42@gmail.com>,"Tue, 7 Jul 2015 15:10:22 -0700",Re: [VOTE] Release Apache Spark 1.4.1 (RC3),Patrick Wendell <pwendell@gmail.com>,"+1 (non-binding, of course)

1. Compiled OSX 10.10 (Yosemite) OK Total time: 27:24 min
     mvn clean package -Pyarn -Phadoop-2.6 -DskipTests
2. Tested pyspark, mllib
2.1. statistics (min,max,mean,Pearson,Spearman) OK
2.2. Linear/Ridge/Laso Regression OK
"
spark user <spark_user@yahoo.com.INVALID>,"Tue, 7 Jul 2015 22:57:34 +0000 (UTC)",spark - redshift !!!,"User <user@spark.apache.org>, Didi <didist@gmail.com>, 
	Akhil Das <akhil@sigmoidanalytics.com>, 
	Koert Kuipers <koert@tresata.com>, 
	""deepujain@gmail.com"" <deepujain@gmail.com>, 
	Raghavendra Pandey <raghavendra.pandey@gmail.com>, 
	Ted Yu <yuzhihong@gmail.com>, Ashish Soni <asoni.learn@gmail.com>, 
	Dev <dev@spark.apache.org>","HiÂ Can you help me how to load data from s3 bucket to Â redshift , if you gave sample code can you pls send meÂ 
ThanksÂ su"
Judy Nash <judynash@exchange.microsoft.com>,"Wed, 8 Jul 2015 03:53:20 +0000",thrift server reliability issue,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi everyone,

Found a thrift server reliability issue on spark 1.3.1 that causes thrift to fail.

When thrift server has too little memory allocated to the driver to process the request, its Spark SQL session exits with OutOfMemory exception, causing thrift server to stop working.

Is this a known issue?

Thanks,
Judy

------------------
Full stacktrace of out of memory exception:
2015-07-08 03:30:18,011 ERROR actor.ActorSystemImpl (Slf4jLogger.scala:apply$mcV$sp(66)) - Uncaught fatal error from thread [sparkDriver-akka.remote.default-remote-dispatcher-6] shutting down ActorSystem [sparkDriver]
java.lang.OutOfMemoryError: Java heap space
                at org.spark_project.protobuf.ByteString.toByteArray(ByteString.java:515)
                at akka.remote.serialization.MessageContainerSerializer.fromBinary(MessageContainerSerializer.scala:64)
                at akka.serialization.Serialization$$anonfun$deserialize$1.apply(Serialization.scala:104)
                at scala.util.Try$.apply(Try.scala:161)
                at akka.serialization.Serialization.deserialize(Serialization.scala:98)
                at akka.remote.MessageSerializer$.deserialize(MessageSerializer.scala:23)
                at akka.remote.DefaultMessageDispatcher.payload$lzycompute$1(Endpoint.scala:58)
                at akka.remote.DefaultMessageDispatcher.payload$1(Endpoint.scala:58)
                at akka.remote.DefaultMessageDispatcher.dispatch(Endpoint.scala:76)
                at akka.remote.EndpointReader$$anonfun$receive$2.applyOrElse(Endpoint.scala:937)
                at akka.actor.Actor$class.aroundReceive(Actor.scala:465)
                at akka.remote.EndpointActor.aroundReceive(Endpoint.scala:415)
                at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)
                at akka.actor.ActorCell.invoke(ActorCell.scala:487)
                at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)
                at akka.dispatch.Mailbox.run(Mailbox.scala:220)
                at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)
                at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
                at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
                at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
                at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
"
Pankaj Arora <Pankaj.Arora@guavus.com>,"Wed, 8 Jul 2015 05:25:57 +0000",Spark job hangs when History server events are written to hdfs ,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

I am running long running application over yarn using spark and I am facing issues while using spark’s history server when the events are written to hdfs. It seems to work fine for some time and in between I see following exception.


2015-06-01 00:00:03,247 [SparkListenerBus] ERROR org.apache.spark.scheduler.LiveListenerBus - Listener EventLoggingListener threw an exception

java.lang.reflect.InvocationTargetException

        at sun.reflect.GeneratedMethodAccessor69.invoke(Unknown Source)

        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)

        at java.lang.reflect.Method.invoke(Unknown Source)

        at org.apache.spark.util.FileLogger$$anonfun$flush$2.apply(FileLogger.scala:203)

        at org.apache.spark.util.FileLogger$$anonfun$flush$2.apply(FileLogger.scala:203)

        at scala.Option.foreach(Option.scala:236)

        at org.apache.spark.util.FileLogger.flush(FileLogger.scala:203)

        at org.apache.spark.scheduler.EventLoggingListener.logEvent(EventLoggingListener.scala:90)

        at org.apache.spark.scheduler.EventLoggingListener.onUnpersistRDD(EventLoggingListener.scala:121)

        at org.apache.spark.scheduler.SparkListenerBus$$anonfun$postToAll$11.apply(SparkListenerBus.scala:66)

        at org.apache.spark.scheduler.SparkListenerBus$$anonfun$postToAll$11.apply(SparkListenerBus.scala:66)

        at org.apache.spark.scheduler.SparkListenerBus$$anonfun$foreachListener$1.apply(SparkListenerBus.scala:83)

        at org.apache.spark.scheduler.SparkListenerBus$$anonfun$foreachListener$1.apply(SparkListenerBus.scala:81)

        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)

        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)

        at org.apache.spark.scheduler.SparkListenerBus$class.foreachListener(SparkListenerBus.scala:81)

        at org.apache.spark.scheduler.SparkListenerBus$class.postToAll(SparkListenerBus.scala:66)

        at org.apache.spark.scheduler.LiveListenerBus.postToAll(LiveListenerBus.scala:32)

        at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(LiveListenerBus.scala:56)

        at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(LiveListenerBus.scala:56)

        at scala.Option.foreach(Option.scala:236)

        at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(LiveListenerBus.scala:56)

        at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply(LiveListenerBus.scala:47)

        at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply(LiveListenerBus.scala:47)

        at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1545)

        at org.apache.spark.scheduler.LiveListenerBus$$anon$1.run(LiveListenerBus.scala:46)

Caused by: java.io.IOException: All datanodes 192.168.162.54:50010 are bad. Aborting...

        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1128)

        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:924)

        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:486)



And after that this error continue to come and spark reaches into unstable stage where no job is able to progress.

FYI.
HDFS was up and running before and after this error and on restarting application it runs fine for some hours and again same error comes.
Enough disk space was available on each data node.

Any suggestion or help would be appreciated.

Regards
Pankaj

"
"""Cheng, Hao"" <hao.cheng@intel.com>","Wed, 8 Jul 2015 05:50:21 +0000",RE: thrift server reliability issue,"Judy Nash <judynash@exchange.microsoft.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","Yes, it's a known issue, either set a bigger heap size for driver, or you can try to set the ` spark.sql.thriftServer.incrementalCollect=true` , it's work around for the query returns a huge result set.

From: Judy Nash [mailto:judynash@exchange.microsoft.com]
Sent: Wednesday, July 8, 2015 11:53 AM
To: dev@spark.apache.org
Subject: thrift server reliability issue

Hi everyone,

Found a thrift server reliability issue on spark 1.3.1 that causes thrift to fail.

When thrift server has too little memory allocated to the driver to process the request, its Spark SQL session exits with OutOfMemory exception, causing thrift server to stop working.

Is this a known issue?

Thanks,
Judy

------------------
Full stacktrace of out of memory exception:
2015-07-08 03:30:18,011 ERROR actor.ActorSystemImpl (Slf4jLogger.scala:apply$mcV$sp(66)) - Uncaught fatal error from thread [sparkDriver-akka.remote.default-remote-dispatcher-6] shutting down ActorSystem [sparkDriver]
java.lang.OutOfMemoryError: Java heap space
                at org.spark_project.protobuf.ByteString.toByteArray(ByteString.java:515)
                at akka.remote.serialization.MessageContainerSerializer.fromBinary(MessageContainerSerializer.scala:64)
                at akka.serialization.Serialization$$anonfun$deserialize$1.apply(Serialization.scala:104)
                at scala.util.Try$.apply(Try.scala:161)
                at akka.serialization.Serialization.deserialize(Serialization.scala:98)
                at akka.remote.MessageSerializer$.deserialize(MessageSerializer.scala:23)
                at akka.remote.DefaultMessageDispatcher.payload$lzycompute$1(Endpoint.scala:58)
                at akka.remote.DefaultMessageDispatcher.payload$1(Endpoint.scala:58)
                at akka.remote.DefaultMessageDispatcher.dispatch(Endpoint.scala:76)
                at akka.remote.EndpointReader$$anonfun$receive$2.applyOrElse(Endpoint.scala:937)
                at akka.actor.Actor$class.aroundReceive(Actor.scala:465)
                at akka.remote.EndpointActor.aroundReceive(Endpoint.scala:415)
                at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)
                at akka.actor.ActorCell.invoke(ActorCell.scala:487)
                at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)
                at akka.dispatch.Mailbox.run(Mailbox.scala:220)
                at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)
                at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
                at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
                at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
                at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
"
Akhil Das <akhil@sigmoidanalytics.com>,"Wed, 8 Jul 2015 13:10:06 +0530",Re: Spark job hangs when History server events are written to hdfs,Pankaj Arora <Pankaj.Arora@guavus.com>,"Can you look in the datanode logs and see whats going on? Most likely, you
are hitting the ulimit on open file handles.

Thanks
Best Regards


are
r
)
203)
203)
ener.scala:90)
ngListener.scala:121)
parkListenerBus.scala:66)
parkListenerBus.scala:66)
ply(SparkListenerBus.scala:83)
ply(SparkListenerBus.scala:81)
a:59)
stenerBus.scala:81)
Bus.scala:66)
a:32)
n$apply$mcV$sp$1.apply(LiveListenerBus.scala:56)
n$apply$mcV$sp$1.apply(LiveListenerBus.scala:56)
cV$sp(LiveListenerBus.scala:56)
iveListenerBus.scala:47)
iveListenerBus.scala:47)
ala:46)
dOrRecovery(DFSOutputStream.java:1128)
DFSOutputStream.java:924)
ava:486)
"
spark user <spark_user@yahoo.com.INVALID>,"Wed, 8 Jul 2015 07:52:49 +0000 (UTC)",Re: spark - redshift !!!,shahab <shahab.mokari@gmail.com>,"Hi 'I am looking how to load data in redshift .ThanksÂ  


   

 Hi,
I did some experiment with loading data from s3 into spark. I loaded data from s3 using sc.textFile(....). Have a look at the following code snippet:
val csv = sc.textFile(""s3n://mybucket/myfile.csv"")Â Â     val rdd = csv.map(line => line.split("","").map(elem => elem.trim)) Â // my data format is in CSV format, comma separated    .map (r =>Â  MyIbject(r(3), r(4).toLong, r(5).toLong, r(6))) Â //just map it to the target object format
hope this helps,best,/Shahab


HiÂ Can you help me how to load data from s3 bucket to Â redshift , if you gave sample code can you pls send meÂ 
ThanksÂ su



  "
Archit Thakur <archit279thakur@gmail.com>,"Wed, 8 Jul 2015 16:36:23 +0530",Re: Spark job hangs when History server events are written to hdfs,Akhil Das <akhil@sigmoidanalytics.com>,"As such we do not open any files by ourselves. EventLoggingListener opens
the file to write down the events in json format for history server. But it
uses the same writer(PrintWriter object) and eventually the same output
stream (which boils down to DFSOutputStream for us). It
seems DFSOutputStream maintains an instance variable hasError in case of an
error and even if hdfs comes back up, it throws exceptions.


u
 are
e
er
e)
:203)
:203)
tener.scala:90)
ingListener.scala:121)
SparkListenerBus.scala:66)
SparkListenerBus.scala:66)
pply(SparkListenerBus.scala:83)
pply(SparkListenerBus.scala:81)
la:59)
istenerBus.scala:81)
rBus.scala:66)
la:32)
un$apply$mcV$sp$1.apply(LiveListenerBus.scala:56)
un$apply$mcV$sp$1.apply(LiveListenerBus.scala:56)
mcV$sp(LiveListenerBus.scala:56)
LiveListenerBus.scala:47)
LiveListenerBus.scala:47)
cala:46)
ndOrRecovery(DFSOutputStream.java:1128)
(DFSOutputStream.java:924)
java:486)
"
Sean Owen <sowen@cloudera.com>,"Wed, 8 Jul 2015 12:18:48 +0100",Re: [VOTE] Release Apache Spark 1.4.1 (RC3),Patrick Wendell <pwendell@gmail.com>,"The POM issue is resolved and the build succeeds. The license and sigs
still work. The tests pass for me with ""-Pyarn -Phadoop-2.6"", with the
following two exceptions. Is anyone else seeing these? this is
consistent on Ubuntu 14 with Java 7/8:

DataFrameStatSuite:
...
- special crosstab elements (., '', null, ``) *** FAILED ***
  java.lang.NullPointerException:
  at org.apache.spark.sql.execution.stat.StatFunctions$$anonfun$4.apply(StatFunctions.scala:131)
  at org.apache.spark.sql.execution.stat.StatFunctions$$anonfun$4.apply(StatFunctions.scala:121)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
  at scala.collection.immutable.Map$Map4.foreach(Map.scala:181)
  at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
  at scala.collection.AbstractTraversable.map(Traversable.scala:105)
  at org.apache.spark.sql.execution.stat.StatFunctions$.crossTabulate(StatFunctions.scala:121)
  at org.apache.spark.sql.DataFrameStatFunctions.crosstab(DataFrameStatFunctions.scala:94)
  at org.apache.spark.sql.DataFrameStatSuite$$anonfun$5.apply$mcV$sp(DataFrameStatSuite.scala:97)
  ...

HiveSparkSubmitSuite:
- SPARK-8368: includes jars passed in through --jars *** FAILED ***
  Process returned with exit code 1. See the log4j logs for more
detail. (HiveSparkSubmitSuite.scala:92)
- SPARK-8020: set sql conf in spark conf *** FAILED ***
  Process returned with exit code 1. See the log4j logs for more
detail. (HiveSparkSubmitSuite.scala:92)
- SPARK-8489: MissingRequirementError during reflection *** FAILED ***
  Process returned with exit code 1. See the log4j logs for more
detail. (HiveSparkSubmitSuite.scala:92)


---------------------------------------------------------------------


"
Pankaj Arora <Pankaj.Arora@guavus.com>,"Wed, 8 Jul 2015 11:42:59 +0000",Re: Spark job hangs when History server events are written to hdfs,"Archit Thakur <archit279thakur@gmail.com>, Akhil Das
	<akhil@sigmoidanalytics.com>","I will reproduce this and get the datanode logs but I remember there was some exception in data node logs.
Also this is reproducible if you restart hdfs in between and this doesn’t recover after hdfs comes back again. Shouldn’t there be a way to recover from these type of errors.

Thanks and Regards
Pankaj

From: Archit Thakur <archit279thakur@gmail.com<mailto:archit279thakur@gmail.com>>
Date: Wednesday, 8 July 2015 4:36 pm
To: Akhil Das <akhil@sigmoidanalytics.com<mailto:akhil@sigmoidanalytics.comCc: Pankaj Arora <pankaj.arora@guavus.com<mailto:pankaj.arora@guavus.com>>, ""dev@spark.apache.org<mailto:dev@spark.apache.org>"" <dev@spark.apache.org<mailto:dev@spark.apache.org>>
Subject: Re: Spark job hangs when History server events are written to hdfs

As such we do not open any files by ourselves. EventLoggingListener opens the file to write down the events in json format for history server. But it uses the same writer(PrintWriter object) and eventually the same output stream (which boils down to DFSOutputStream for us). It seems DFSOutputStream maintains an instance variable hasError in case of an error and even if hdfs comes back up, it throws exceptions.

Can you look in the datanode logs and see whats going on? Most likely, you are hitting the ulimit on open file handles.

Thanks
Best Regards

Hi,

I am running long running application over yarn using spark and I am facing issues while using spark’s history server when the events are written to hdfs. It seems to work fine for some time and in between I see following exception.


2015-06-01 00:00:03,247 [SparkListenerBus] ERROR org.apache.spark.scheduler.LiveListenerBus - Listener EventLoggingListener threw an exception

java.lang.reflect.InvocationTargetException

        at sun.reflect.GeneratedMethodAccessor69.invoke(Unknown Source)

        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)

        at java.lang.reflect.Method.invoke(Unknown Source)

        at org.apache.spark.util.FileLogger$$anonfun$flush$2.apply(FileLogger.scala:203)

        at org.apache.spark.util.FileLogger$$anonfun$flush$2.apply(FileLogger.scala:203)

        at scala.Option.foreach(Option.scala:236)

        at org.apache.spark.util.FileLogger.flush(FileLogger.scala:203)

        at org.apache.spark.scheduler.EventLoggingListener.logEvent(EventLoggingListener.scala:90)

        at org.apache.spark.scheduler.EventLoggingListener.onUnpersistRDD(EventLoggingListener.scala:121)

        at org.apache.spark.scheduler.SparkListenerBus$$anonfun$postToAll$11.apply(SparkListenerBus.scala:66)

        at org.apache.spark.scheduler.SparkListenerBus$$anonfun$postToAll$11.apply(SparkListenerBus.scala:66)

        at org.apache.spark.scheduler.SparkListenerBus$$anonfun$foreachListener$1.apply(SparkListenerBus.scala:83)

        at org.apache.spark.scheduler.SparkListenerBus$$anonfun$foreachListener$1.apply(SparkListenerBus.scala:81)

        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)

        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)

        at org.apache.spark.scheduler.SparkListenerBus$class.foreachListener(SparkListenerBus.scala:81)

        at org.apache.spark.scheduler.SparkListenerBus$class.postToAll(SparkListenerBus.scala:66)

        at org.apache.spark.scheduler.LiveListenerBus.postToAll(LiveListenerBus.scala:32)

        at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(LiveListenerBus.scala:56)

        at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(LiveListenerBus.scala:56)

        at scala.Option.foreach(Option.scala:236)

        at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(LiveListenerBus.scala:56)

        at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply(LiveListenerBus.scala:47)

        at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply(LiveListenerBus.scala:47)

        at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1545)

        at org.apache.spark.scheduler.LiveListenerBus$$anon$1.run(LiveListenerBus.scala:46)

Caused by: java.io.IOException: All datanodes 192.168.162.54:50010<http://192.168.162.54:50010> are bad. Aborting...

        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1128)

        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:924)

        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:486)



And after that this error continue to come and spark reaches into unstable stage where no job is able to progress.

FYI.
HDFS was up and running before and after this error and on restarting application it runs fine for some hours and again same error comes.
Enough disk space was available on each data node.

Any suggestion or help would be appreciated.

Regards
Pankaj



"
Pradeep Bashyal <pradeep@bashyal.com>,"Wed, 8 Jul 2015 11:35:45 -0500",Re: [VOTE] Release Apache Spark 1.4.1 (RC3),,"Here's one thing I ran into:

The SparkR documentation example in
http://people.apache.org/~pwendell/spark-releases/latest/sparkr.html is
incorrect.

    sc <- sparkR.init(packages=""com.databricks:spark-csv_2.11:1.0.3"")

should be

    sc <- sparkR.init(sparkPackages=""com.databricks:spark-csv_2.11:1.0.3"")


Thanks
Pradeep



"
Sean Owen <sowen@cloudera.com>,"Wed, 8 Jul 2015 17:38:09 +0100",Re: [VOTE] Release Apache Spark 1.4.1 (RC3),Pradeep Bashyal <pradeep@bashyal.com>,"Although that should be fixed if it's incorrect, it's not something
that would nearly block a release. The question here is whether this
artifact can be released as 1.4.1, or whether it has a blocking
regression from 1.4.0.


---------------------------------------------------------------------


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Wed, 8 Jul 2015 09:40:00 -0700",Re: [VOTE] Release Apache Spark 1.4.1 (RC3),Pradeep Bashyal <pradeep@bashyal.com>,"Hi Pradeep

Thanks for the catch -- Lets open a JIRA and PR for it. I don't think
documentation changes affect the release though Patrick can confirm that.

Thanks
Shivaram


"
Mark Hamstra <mark@clearstorydata.com>,"Wed, 8 Jul 2015 10:03:26 -0700",Re: [VOTE] Release Apache Spark 1.4.1 (RC3),Sean Owen <sowen@cloudera.com>,"HiveSparkSubmitSuite is fine for me, but I do see the same issue with
DataFrameStatSuite
-- OSX 10.10.4, java

1.7.0_75, -Phive -Phive-thriftserver -Phadoop-2.4 -Pyarn


"
Patrick Wendell <pwendell@gmail.com>,"Wed, 8 Jul 2015 10:21:29 -0700",Re: [VOTE] Release Apache Spark 1.4.1 (RC3),Mark Hamstra <mark@clearstorydata.com>,"Yeah - we can fix the docs separately from the release.

- Patrick


---------------------------------------------------------------------


"
Pradeep Bashyal <pradeep@bashyal.com>,"Wed, 8 Jul 2015 12:43:40 -0500",Re: [VOTE] Release Apache Spark 1.4.1 (RC3),shivaram@eecs.berkeley.edu,"Hi Shivaram,

I created a Jira Issue for the documentation error.
 https://issues.apache.org/jira/browse/SPARK-8901

Thanks
Pradeep


"
Andrew Or <andrew@databricks.com>,"Wed, 8 Jul 2015 11:13:53 -0700",Re: [VOTE] Release Apache Spark 1.4.1 (RC3),Pradeep Bashyal <pradeep@bashyal.com>,"@Sean You actually need to run HiveSparkSubmitSuite with `-Phive` and
`-Phive-thriftserver`. The MissingRequirementsError is just complaining
that it can't find the right classes. The other one (DataFrameStatSuite) is
a little more concerning.

2015-07-08"
Sean Owen <sowen@cloudera.com>,"Wed, 8 Jul 2015 19:30:01 +0100",Re: [VOTE] Release Apache Spark 1.4.1 (RC3),Andrew Or <andrew@databricks.com>,"I see, but shouldn't this test not be run when Hive isn't in the build?


---------------------------------------------------------------------


"
Chandrashekhar Kotekar <shekhar.kotekar@gmail.com>,"Thu, 9 Jul 2015 00:10:37 +0530",What steps to take to work on [Spark-8899] issue?,dev@spark.apache.org,"Hi,

Although I have 7+ years experience in Java development, I am new to open
source contribution. To understand which steps one needs to take to work on
some issue and upload those changes, I have decided to work on this
[Spark-8899] issue which is marked as 'trivial'.

So far I have done following steps :

1. Opened http://github.com/apache/spark in chrome
2. Clicked on 'Fork' button which is on upper right hand side of the page
3. Cloned the project using github shell app.

Now what should I do next to work on this issue? Can anyone please help?

Thanks,
Chandrash3khar Kotekar
Mobile - +91 8600011455
"
Michael Armbrust <michael@databricks.com>,"Wed, 8 Jul 2015 11:44:55 -0700",Re: What steps to take to work on [Spark-8899] issue?,Chandrashekhar Kotekar <shekhar.kotekar@gmail.com>,"There is a lot of info here:
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark

In this particular case I'd start by looking at the JIRA (which already has
a pull request posted to it).


"
Chandrashekhar Kotekar <shekhar.kotekar@gmail.com>,"Thu, 9 Jul 2015 00:17:24 +0530",Re: What steps to take to work on [Spark-8899] issue?,Michael Armbrust <michael@databricks.com>,"Maybe it is stupid question but 'pull request posted to it' means this bug
is already fixed?


Regards,
Chandrash3khar Kotekar
Mobile - +91 8600011455


"
Holden Karau <holden@pigscanfly.ca>,"Wed, 8 Jul 2015 11:51:30 -0700",Re: What steps to take to work on [Spark-8899] issue?,Chandrashekhar Kotekar <shekhar.kotekar@gmail.com>,"Not exactly but it means someone has come up with what they think a
solution to the problem is and that they've submitted some code for
consideration/review.



-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
Linked In: https://www.linkedin.com/in/holdenkarau
"
Josh Rosen <rosenville@gmail.com>,"Wed, 8 Jul 2015 12:00:33 -0700",Re: [VOTE] Release Apache Spark 1.4.1 (RC3),Sean Owen <sowen@cloudera.com>,"I've filed https://issues.apache.org/jira/browse/SPARK-8903 to fix the
DataFrameStatSuite test failure. The problem turned out to be caused by a
mistake made while resolving a merge-conflict when backporting that patch
to branch-1.4.

I've submitted https://github.com/apache/spark/pull/7295 to fix this issue.


"
Josh Rosen <joshrosen@databricks.com>,"Wed, 8 Jul 2015 15:57:38 -0700",Helpful IntelliJ shortcuts for working with Javadoc / Scaladoc,spark-dev@apache.org,"Here's a helpful IntelliJ feature for writing and browsing Scala/Javadoc:

You can press Ctrl-j (or View menu -> Quick Documentation) to bring up a
popup which displays the Scaladoc / Javadoc for the currently-selected
symbol:

[image: Inline image 2]

This window has a few neat features to make documentation writing easier.
If you press the pin button in the upper right-hand corner, the popup
window will be replaced by a window which stays on screen as you navigate
around the code:

[image: Inline image 3]

There are options to anchor this as a floating window or to pin it into the
side context bar:

[image: Inline image 4]

In this persistent window, there is one useful option to make documentation
easier to work with.  The ""Auto Update from Source"" option causes the
documentation to automatically refresh when you select a different symbol
or when you edit the documentation itself.  This allows you to have a live
preview when writing documentation.
[image: Inline image 6]

This feature has some limitations (namely, line breaks and lists in
Scaladoc aren't rendered the same way that they would be in the displayed
Scaladoc on the website), but most Scaladoc features work (including the
{{{ syntax for code examples) and it works perfectly for Javadoc.

Anyhow, just wanted to share this feature because it's a big productivity
improver when writing docs.

- Josh
"
Eugene Morozov <fathersson@list.ru>,"Thu, 9 Jul 2015 02:13:14 +0300",Code movements from Driver to Workers,dev@spark.apache.org,"Hi, 

I have a question regarding code movements. It’s not clear of how exactly my code is being moved onto Worker nodes to be completed.

My assumption was that by submitting jar file through spark-submit, Spark copies this jar file to Worker nodes and adds this jar to their classpath. My experiments with custom kryo registrators gives me another experience. Without adding jar to SparkConf, Worker nodes simply says that they cannot find class of my CustomKryoRegistrator. 

Which makes me think that the process is the following. Jar file in spark-submit is not copied to Worker nodes and is not added into their classpath, but instead each function (transformation / action) is serialized and moved to Workers along with all required classes to be actually completed. Since my CustomKryoRegistrator is not part of any function, then it never end up being on a Workers nodes, thus class not found. 

Is my “new” understanding correct? Could you, please, explain in couple of words how code being moved from Driver to Workers? Could you give me a hint of where to find this in sources?

Thanks in advance.
--
Eugene Morozov
fathersson@list.ru




"
Burak Yavuz <brkyvz@gmail.com>,"Wed, 8 Jul 2015 16:29:23 -0700",Re: Helpful IntelliJ shortcuts for working with Javadoc / Scaladoc,Josh Rosen <joshrosen@databricks.com>,"This is awesome Josh! Very helpful. Thanks for sharing!

Burak


"
Patrick Wendell <pwendell@gmail.com>,"Wed, 8 Jul 2015 18:17:59 -0700",Re: [VOTE] Release Apache Spark 1.4.1 (RC3),Josh Rosen <rosenville@gmail.com>,"Hey All,

The issue that Josh pointed out is not just a test failure, it's an
issue with an important bug fix that was not correctly back-ported
into the 1.4 branch. Unfortunately the overall state of the 1.4 branch
tests on Jenkins was not in great shape so this was missed earlier on.

Given that this is fixed now, I have prepared another RC and am
leaning towards restarting the vote. If anyone feels strongly one way
or the other let me know, otherwise I'll restart it in a few hours. I
figured since this will likely finalize over the weekend anyways, it's
not so bad to wait 1 additional day in order to get that fix.

- Patrick


---------------------------------------------------------------------


"
ankits <ankitsoni9@gmail.com>,"Wed, 8 Jul 2015 19:14:01 -0700 (MST)",Why are all spark deps not shaded to avoid dependency hell?,dev@spark.apache.org,"I frequently encounter problems building Spark as a dependency in java
projects because of version conflicts with other dependencies. Usually there
will be two different versions of a library and we'll see an
AbstractMethodError or invalid signature etc.

So far, I've seen it happen with jackson, slf4j, netty, jetty, javax.servlet
and maybe a few others. Even the spark-jobserver project excludes the netty
dependencies when including spark. These errors are always a pain to debug
and workaround - in one particularly nasty conflict with jackson 1.x on
spark and our codebase, we ended up just migrating to jackson 2.

I see shading is already setup for jetty and guava in the spark POM so
perhaps shading the others wouldn't be too difficult. Is there any reason
all of these dependencies aren't shaded in the spark POM? Seems like a good
idea to me.



--

---------------------------------------------------------------------


"
Meethu Mathew <meethu.mathew@flytxt.com>,"Thu, 9 Jul 2015 09:44:36 +0530",Re: Helpful IntelliJ shortcuts for working with Javadoc / Scaladoc,Burak Yavuz <brkyvz@gmail.com>,"This is really helpful. I think its Ctrl+Q in Intellij 14.1.

Regards,

Meethu Mathew



"
Chandrashekhar Kotekar <shekhar.kotekar@gmail.com>,"Thu, 9 Jul 2015 10:08:37 +0530",Re: What steps to take to work on [Spark-8899] issue?,Holden Karau <holden@pigscanfly.ca>,"Ohk. Thanks, I will choose some other issue then.


Regards,
Chandrash3khar Kotekar
Mobile - +91 8600011455


"
Patrick Wendell <pwendell@gmail.com>,"Wed, 8 Jul 2015 22:53:19 -0700",[RESULT] [VOTE] Release Apache Spark 1.4.1 (RC3),"""dev@spark.apache.org"" <dev@spark.apache.org>","This vote is cancelled in favor of RC4.

- Patrick


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Wed, 8 Jul 2015 22:55:16 -0700",[VOTE] Release Apache Spark 1.4.1 (RC4),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version 1.4.1!

This release fixes a handful of known issues in Spark 1.4.0, listed here:
http://s.apache.org/spark-1.4.1

The tag to be voted on is v1.4.1-rc4 (commit dbaa5c2):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=
dbaa5c294eb565f84d7032e387e4b8c1a56e4cd2

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.4.1-rc4-bin/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
[published as version: 1.4.1]
https://repository.apache.org/content/repositories/orgapachespark-1125/
[published as version: 1.4.1-rc4]
https://repository.apache.org/content/repositories/orgapachespark-1126/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.4.1-rc4-docs/

Please vote on releasing this package as Apache Spark 1.4.1!

The vote is open until Sunday, July 12, at 06:55 UTC and passes
if a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.4.1
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

---------------------------------------------------------------------


"
"""Vasili I. Galchin"" <vigalchin@gmail.com>","Wed, 8 Jul 2015 23:19:36 -0700",Spark and Haskell support,dev@spark.apache.org,"Hello,

      1) I have been rereading kind email responses to my Spark queries. Thx.

      2) I have also been reading ""R"" code:

         1) RDD.R

         2) DataFrame.R

      3) All following API's =>
https://cwiki.apache.org/confluence/display/SPARK/Spark+Internals

      4) Python ... https://spark.apache.org/docs/latest/programming-guide.html


           Based on the above points, when I see in e.g. DataFrame.R,
calls to a function ""callJMethod"" ... two questions ...

            - is is callJMethod calling into the JVM or better yet the
Spark Java API? Please answer in a more mathematical way without
ambiguities ... this will save too many back and forth queries ....

            - if the above is true, how is the JVM ""imported"" into the
DataFrame.R code.

             I would appreciate if you answers would be interleaved
with my questions to avoid ambiguities .. Thx,


Thx,

Vasili

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Wed, 8 Jul 2015 23:58:58 -0700",Re: [VOTE] Release Apache Spark 1.4.1 (RC4),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Thu, 9 Jul 2015 00:02:48 -0700",Re: [VOTE] Release Apache Spark 1.4.1 (RC4),Patrick Wendell <pwendell@gmail.com>,1
=?UTF-8?B?54mb5YWG5o23?= <nzjemail@gmail.com>,"Thu, 9 Jul 2015 16:19:26 +0800",Questions about Fault tolerance of Spark,"""dev@spark.apache.org"" <dev@spark.apache.org>, ""user@spark.apache.org"" <user@spark.apache.org>","Hi All:

We already know that Spark utilizes the lineage to recompute the RDDs when
failure occurs.
I want to study the performance of this fault-tolerant approach and have
some questions about it.

1) Is there any benchmark (or standard failure model) to test the fault
tolerance of these kinds of in-memory data processing systems?

2) How do you emulate the failures in testing spark?  (e.g., kill a
computation task? or kill the computation nodes?)

Thanks!!!

-- 
*Regards,*
*Zhaojie*
"
Niranda Perera <niranda.perera@gmail.com>,"Thu, 9 Jul 2015 17:39:47 +0530",databases currently supported by Spark SQL JDBC,"""user@spark.apache.org"" <user@spark.apache.org>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

I'm planning to use Spark SQL JDBC datasource provider in various RDBMS
databases.

what are the databases currently supported by Spark JDBC relation provider?

rgds

-- 
Niranda
@n1r44 <https://twitter.com/N1R44>
https://pythagoreanscript.wordpress.com/
"
Yijie Shen <henry.yijieshen@gmail.com>,"Thu, 9 Jul 2015 22:24:59 +0800",The latest master branch didn't compile with -Phive?,Spark-dev <dev@spark.apache.org>,"Hi,

I use the clean version just clone from the master branch, build with:

build/mvn -Phive -Phadoop-2.4 -DskipTests package
And BUILD FAILURE at last, due to:

[error]      while compiling: /Users/yijie/spark/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMetastoreCatalog.scala
[error]         during phase: typer
[error]      library version: version 2.10.4
[error]     compiler version: version 2.10.4
...
[error]  
[error]   last tree to typer: Ident(Warehouse)
[error]               symbol: <none> (flags: )
[error]    symbol definition: <none>
[error]        symbol owners:  
[error]       context owners: lazy value hiveWarehouse -> class HiveMetastoreCatalog -> package hive
[error]  
[error] == Enclosing template or block ==
[error]  
[error] Template( // val <local HiveMetastoreCatalog>: <notype> in class HiveMetastoreCatalog
[error]   ""Catalog"", ""Logging"" // parents
[error]   ValDef(
[error]     private
[error]     ""_""
[error]     <tpt>
[error]     <empty>
[error]   )
[error]   // 24 statements
[error]   ValDef( // private[this] val client: org.apache.spark.sql.hive.client.ClientInterface in class HiveMetastoreCatalog
[error]     private <local> <paramaccessor>
[error]     ""client""
[error]     ""ClientInterface""
[error]     <empty>
â€¦

https://gist.github.com/yijieshen/e0925e2227a312ae4c64#file-build_failure

Did I make a silly mistake?

Thanks, Yijie"
Stephen Boesch <javadba@gmail.com>,"Thu, 9 Jul 2015 07:28:57 -0700",Re: The latest master branch didn't compile with -Phive?,Yijie Shen <henry.yijieshen@gmail.com>,"Please do a *clean* package and reply back if you still encounter issues.

2015-07-09 7:24 GMT-07:00 Yijie Shen <henry.yijieshen@gmail.com>:

org/apache/spark/sql/hive/HiveMetastoreCatalog.scala
toreCatalog -> package hive
HiveMetastoreCatalog
client.ClientInterface in class HiveMetastoreCatalog
"
Sean Owen <sowen@cloudera.com>,"Thu, 9 Jul 2015 15:38:52 +0100",Re: [VOTE] Release Apache Spark 1.4.1 (RC4),Patrick Wendell <pwendell@gmail.com>,"+1 nonbinding. All previous RC issues appear resolved. All tests pass
with the ""-Pyarn -Phadoop-2.6 -Phive -Phive-thriftserver"" invocation.
Signatures et al are OK.


---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Thu, 9 Jul 2015 07:45:28 -0700",Re: The latest master branch didn't compile with -Phive?,Yijie Shen <henry.yijieshen@gmail.com>,"Looking at
https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-Maven-with-YARN/HADOOP_PROFILE=hadoop-2.4,label=centos/2875/consoleFull
:

[error]
[error]      while compiling:
/home/jenkins/workspace/Spark-Master-Maven-with-YARN/HADOOP_PROFILE/hadoop-2.4/label/centos/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMetastoreCatalog.scala
[error]         during phase: typer
[error]      library version: version 2.10.4
[error]     compiler version: version 2.10.4


I traced back to build #2869 and the error was there - didn't go back further.


FYI



org/apache/spark/sql/hive/HiveMetastoreCatalog.scala
toreCatalog -> package hive
HiveMetastoreCatalog
client.ClientInterface in class HiveMetastoreCatalog
"
Sean Owen <sowen@cloudera.com>,"Thu, 9 Jul 2015 15:51:06 +0100",Re: The latest master branch didn't compile with -Phive?,Ted Yu <yuzhihong@gmail.com>,"This is an error from scalac and not Spark. I find it happens
frequently for me but goes away on a clean build. *shrug*

ADOOP_PROFILE=hadoop-2.4,label=centos/2875/consoleFull
p-2.4/label/centos/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMetastoreCatalog.scala
eMetastoreCatalog.scala
e

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Thu, 9 Jul 2015 07:58:55 -0700",Re: The latest master branch didn't compile with -Phive?,Sean Owen <sowen@cloudera.com>,"From
https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-Maven-with-YARN/HADOOP_PROFILE=hadoop-2.4,label=centos/2875/consoleFull
:

+ build/mvn -DzincPort=3439 -DskipTests -Phadoop-2.4 -Pyarn -Phive
-Phive-thriftserver -Pkinesis-asl clean package


FYI



ADOOP_PROFILE=hadoop-2.4,label=centos/2875/consoleFull
p-2.4/label/centos/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMetastoreCatalog.scala
MetastoreCatalog.scala
ss
"
Ted Yu <yuzhihong@gmail.com>,"Thu, 9 Jul 2015 08:48:46 -0700",Re: The latest master branch didn't compile with -Phive?,Sean Owen <sowen@cloudera.com>,"I guess the compilation issue didn't surface in QA run because sbt was used:

[info] Building Spark (w/Hive 0.13.1) using SBT with these arguments:
-Pyarn -Phadoop-2.3 -Dhadoop.version=2.3.0 -Pkinesis-asl
-Phive-thriftserver -Phive package assembly/assembly
streaming-kafka-assembly/assembly streaming-flume-assembly/assembly


Cheers



ADOOP_PROFILE=hadoop-2.4,label=centos/2875/consoleFull
ve-thriftserver -Pkinesis-asl clean package
HADOOP_PROFILE=hadoop-2.4,label=centos/2875/consoleFull
op-2.4/label/centos/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMetastoreCatalog.scala
:
eMetastoreCatalog.scala
e
"
Josh Rosen <rosenville@gmail.com>,"Thu, 9 Jul 2015 08:50:32 -0700",Re: The latest master branch didn't compile with -Phive?,Ted Yu <yuzhihong@gmail.com>,"Jenkins runs compile-only builds for Maven as an early warning system for
this type of issue; you can see from
https://amplab.cs.berkeley.edu/jenkins/view/Spark-QA-Compile/ that the
Maven compilation is now broken in master.


yarn -Phadoop-2.3 -Dhadoop.version=2.3.0 -Pkinesis-asl -Phive-thriftserver -Phive package assembly/assembly streaming-kafka-assembly/assembly streaming-flume-assembly/assembly
HADOOP_PROFILE=hadoop-2.4,label=centos/2875/consoleFull
ive-thriftserver -Pkinesis-asl clean package
/HADOOP_PROFILE=hadoop-2.4,label=centos/2875/consoleFull
oop-2.4/label/centos/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMetastoreCatalog.scala
h:
veMetastoreCatalog.scala
re
"
Pradeep Bashyal <pradeep@bashyal.com>,"Thu, 9 Jul 2015 11:21:06 -0500",spark-ec2 Fails installing ganglia properly in 1.4,dev@spark.apache.org,"Hello,

The ec2/spark-ec2 fails installing ganglia properly in 1.4. The issue seems
to be an older version of httpd(2.2) but the /etc/httpd/conf/httpd.conf is
for 2.4

[ec2-user@ip-172-30-0-123 ~]$ httpd -v
Server version: Apache/2.2.29 (Unix)
Server built:   Mar 12 2015 03:50:17

There is already an Jira issue describing the problem and seems to be fixed
upstream.
https://issues.apache.org/jira/browse/SPARK-8338

Is this a reasonable fix for the 1.4.1 release?

Thanks
Pradeep
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Thu, 9 Jul 2015 09:33:42 -0700",Re: spark-ec2 Fails installing ganglia properly in 1.4,Pradeep Bashyal <pradeep@bashyal.com>,"There are a couple of PRs open for it (linked from the JIRA) and I am
reviewing https://github.com/mesos/spark-ec2/pull/121-- Also the EC2 fixes
can be out of band from the release itself, so the fix will make to 1.4.1
once the above PR is merged.

Thanks
Shivaram


"
Pradeep Bashyal <pradeep@bashyal.com>,"Thu, 9 Jul 2015 11:38:52 -0500",Re: spark-ec2 Fails installing ganglia properly in 1.4,shivaram@eecs.berkeley.edu,"Awesome! Thank you.


"
Burak Yavuz <brkyvz@gmail.com>,"Thu, 9 Jul 2015 09:58:22 -0700",Re: [VOTE] Release Apache Spark 1.4.1 (RC4),Sean Owen <sowen@cloudera.com>,"+1 nonbinding.


"
Mark Hamstra <mark@clearstorydata.com>,"Thu, 9 Jul 2015 10:07:05 -0700",Re: [VOTE] Release Apache Spark 1.4.1 (RC4),Patrick Wendell <pwendell@gmail.com>,1
Michael Armbrust <michael@databricks.com>,"Thu, 9 Jul 2015 10:26:06 -0700",Re: [VOTE] Release Apache Spark 1.4.1 (RC4),Mark Hamstra <mark@clearstorydata.com>,1
Andrew Or <andrew@databricks.com>,"Thu, 9 Jul 2015 11:31:19 -0700",Re: [VOTE] Release Apache Spark 1.4.1 (RC4),Michael Armbrust <michael@databricks.com>,"+1

2015-07-09 10:26 GMT-07:00 Michael Armbrust <michael@databricks.com>:

"
"""York, Brennon"" <Brennon.York@capitalone.com>","Thu, 9 Jul 2015 14:51:18 -0400",Re: [VOTE] Release Apache Spark 1.4.1 (RC4),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1 (non-binding)

* ran spark-on-YARN MLLib ALS recommendation pipeline (success)
  * no regression / performance issues
* ran spark-on-YARN GraphX pipeline (success)
  * no regression / performance issues





____________________________________________"
Krishna Sankar <ksankar42@gmail.com>,"Thu, 9 Jul 2015 12:16:55 -0700",Re: [VOTE] Release Apache Spark 1.4.1 (RC4),Patrick Wendell <pwendell@gmail.com>,"+1

1. Compiled OSX 10.10 (Yosemite) OK Total time: 38:11 min
     mvn clean package -Pyarn -Phadoop-2.6 -DskipTests
2. Tested pyspark, mllib
2.1. statistics (min,max,mean,Pearson,Spearman) OK
2.2. Linear/Ridge/Laso Regression OK
2.3. Decision Tree, Naive"
Luciano Resende <luckbr1975@gmail.com>,"Thu, 9 Jul 2015 12:55:03 -0700",Re: [VOTE] Release Apache Spark 1.4.1 (RC4),Patrick Wendell <pwendell@gmail.com>,"+1 (non-binding) mostly looking in the legal aspects of the release.




-- 
Luciano Resende
http://people.apache.org/~lresende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
emrehan <emrehan.tuzun@gmail.com>,"Thu, 9 Jul 2015 13:04:34 -0700 (MST)",Are These Issues Suitable for our Senior Project?,dev@spark.apache.org,"Hi all,

We could contribute to a feature to Spark MLlib by May 2016 and make it
count as our undergraduate senior project. The following list of issues seem
interesting to us:

*  https://issues.apache.org/jira/browse/SPARK-2273
rning
algorithms: Passive Aggressive
*  https://issues.apache.org/jira/browse/SPARK-2335
<https://issues.apache.org/jira/browse/SPARK-2335>    â€“  K-Nearest Neighbor
classification and regression for MLLib
*  https://issues.apache.org/jira/browse/SPARK-2401
<https://issues.apache.org/jira/browse/SPARK-2401>    â€“  AdaBoost.MH, a
multi-class multi-label classifier
*  https://issues.apache.org/jira/browse/SPARK-4251
<https://issues.apache.org/jira/browse/SPARK-4251>    â€“  Add Restricted
Boltzmann machine(RBM) algorithm to MLlib
*  https://issues.apache.org/jira/browse/SPARK-4752
<https://issues.apache.org/jira/browse/SPARK-4752>    â€“  Classifier based on
artificial neural network
*  https://issues.apache.org/jira/browse/SPARK-5575
<https://issues.apache.org/jira/browse/SPARK-5575>    â€“  Artificial neural
networks for MLlib deep learning
*  https://issues.apache.org/jira/browse/SPARK-5992
<https://issues.apache.org/jira/browse/SPARK-5992>    â€“  Locality Sensitive
Hashing (LSH) for MLlib
*  https://issues.apache.org/jira/browse/SPARK-6425
<https://issues.apache.org/jira/browse/SPARK-6425>    â€“  Add parallel
Q-learning algorithm to MLLib
*  https://issues.apache.org/jira/browse/SPARK-6442
<https://issues.apache.org/jira/browse/SPARK-6442>    â€“  Local Linear
Algebra Package
*  https://issues.apache.org/jira/browse/SPARK-8499
<https://issues.apache.org/jira/browse/SPARK-8499>    â€“  NaiveBayes
implementation for MLPipeline

All of these tickets are marked unassigned but have some work done on them.
Are any of these issues are unsuitable for us as a senior project?

Kind regards,
Can Giracoglu, Emrehan Tuzun, Remzi Can Aksoy, Saygin Dogu




--
3.nabble.com/Are-These-Issues-Suitable-for-our-Senior-Project-tp13119.html
om.

---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Thu, 9 Jul 2015 13:07:39 -0700","jenkins downtime 7/13/15, 7am PDT","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","i'll be taking jenkins down for system and jenkins app updates.  this
should be pretty quick and i'm expecting to have everything back up
and building by 9am.

i will send a reminder email this weekend, and again when i start the
maintenance.

if there's any reason for me to delay this, please let me know ASAP.
this downtime isn't critical, and can be delayed if needed.

thanks!

your friendly devops guy,

shane

---------------------------------------------------------------------


"
Feynman Liang <fliang@databricks.com>,"Thu, 9 Jul 2015 13:20:52 -0700",Re: Are These Issues Suitable for our Senior Project?,emrehan <emrehan.tuzun@gmail.com>,"Exciting, thanks for the contribution! I'm currently aware of:

   - SPARK-8499 is currently in progress (in a duplicate issue); I updated
   the JIRA to reflect that.
   - SPARK-5992 has a spark package
   <http://spark-packages.org/package/mrsqueeze/spark-hash> linked but I'm
   unclear on whether there is any progress there.

Feynman


earning
t
.MH, a
ricted
er based
al neural
llel
near
es
m.
s-Suitable-for-our-Senior-Project-tp13119.html
"
Holden Karau <holden@pigscanfly.ca>,"Thu, 9 Jul 2015 13:27:10 -0700",Re: [VOTE] Release Apache Spark 1.4.1 (RC4),Patrick Wendell <pwendell@gmail.com>,"+1 - compiled on ubuntu & centos, spark-perf run against yarn in client
mode on a small cluster comparing 1.4.0 & 1.4.1 (for core) doesn't have any
huge jumps (albeit with a small scaling factor).




-- 
Cell : 425-233-8271
Twitter: https://twitter.com/h"
Joseph Bradley <joseph@databricks.com>,"Thu, 9 Jul 2015 16:17:07 -0700",Re: Are These Issues Suitable for our Senior Project?,Feynman Liang <fliang@databricks.com>,"It would be great to get more contributions!  If you're new to
contributing, it will be good to start with some small contributions and
check out:
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark

But if those build up to a larger contribution, the top ones I'd pick out
are:

SPARK-6442 (local linear algebra): This could be done incrementally, and
should be coordinated on that JIRA since I believe others may be working on
it.

SPARK-3703 (Ensemble algorithms): It would be great to get a generic
boosting algorithm under the Pipelines API (probably AdaBoost).

SPARK-5992 (LSH): I believe there is active work on this, so it would be
important to coordinate via JIRA on that.

The other JIRAs which Feynman & I did not comment on either have some
active work or are likely lower priority.  However, if you're interested in
one of those algorithms, you could publish it as a Spark package:
http://spark-packages.org/

Good luck!
Joseph

:

learning
st
t.MH, a
tricted
ier
ial
y
allel
inear
yes
es-Suitable-for-our-Senior-Project-tp13119.html
"
Xiangrui Meng <mengxr@gmail.com>,"Thu, 9 Jul 2015 16:21:13 -0700",Re: Are These Issues Suitable for our Senior Project?,Feynman Liang <fliang@databricks.com>,"Hi Emrehan,

Thanks for asking! There are actually many TODOs for MLlib. I would
recommend starting with small tasks before picking a topic for your
senior project. Please check
https://issues.apache.org/jira/browse/SPARK-8445 for the 1.5 roadmap
and see whether there are ones you are interested in. Thanks!

Best,
Xiangrui

:
learning
st
t.MH, a
tricted
ier based
ial neural
y
allel
inear
yes
es-Suitable-for-our-Senior-Project-tp13119.html

---------------------------------------------------------------------


"
"""Vasili I. Galchin"" <vigalchin@gmail.com>","Thu, 9 Jul 2015 16:28:06 -0700",callJMethod?,dev@spark.apache.org,"Hello,

       I am reading R code, e.g. RDD.R, DataFrame.R, etc. I see that
callJMethod is repeatedly call. Is callJMethod part of the Spark Java API?
Thx.

Vasili
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Thu, 9 Jul 2015 16:41:11 -0700",Re: callJMethod?,"""Vasili I. Galchin"" <vigalchin@gmail.com>","callJMethod is a private R function that is defined in
https://github.com/apache/spark/blob/a0cc3e5aa3fcfd0fce6813c520152657d327aaf2/R/pkg/R/backend.R#L31

callJMethod serializes the function names, arguments and sends them over a
socket to the JVM. This is the socket-based R to JVM bridge described in
the Spark Summit talk
https://spark-summit.org/2015/events/sparkr-the-past-the-present-and-the-future/

Thanks
Shivaram


"
"""Vasili I. Galchin"" <vigalchin@gmail.com>","Thu, 9 Jul 2015 17:54:04 -0700",Re: callJMethod?,shivaram@eecs.berkeley.edu,"very nice explanation. Thx


---------------------------------------------------------------------


"
"""Vasili I. Galchin"" <vigalchin@gmail.com>","Thu, 9 Jul 2015 18:01:50 -0700",Re: callJMethod?,shivaram@eecs.berkeley.edu,"Now I want to look at the PySpark side for comparison. I assume same
mechanism to do remote function call!!

Maybe in the slides .. I assume there are multiple JVMs for load
balancing and fault tolerance, yes??


How can I get one pdf with all slides together and not slide show?

Vasili


---------------------------------------------------------------------


"
JaeSung Jun <jaesjun@gmail.com>,"Fri, 10 Jul 2015 11:17:51 +1000",Re: databases currently supported by Spark SQL JDBC,Niranda Perera <niranda.perera@gmail.com>,"As long as JDBC driver is provided, any database can be used in JDBC
datasource provider.
you can provide driver class in options field like followings :

CREATE TEMPORARY TABLE jdbcTable
USING org.apache.spark.sql.jdbc
OPTIOS(
url ""jdbc:oracle:thin:@myhost:1521:orcl""
driver ""oracle.jdbc.driver.OracleDriver""
dbtable ""users""
)

thx
jason


"
gogototo <wangbin83@gmail.com>,"Thu, 9 Jul 2015 21:57:11 -0700 (MST)","Re: Spark ThriftServer encounter
 java.lang.IllegalArgumentException: Unknown auth type: null Allowed values
 are: [auth-int, auth-conf, auth]",dev@spark.apache.org,"I think it's the hive 0.13.1 issue, which fixed in hive 0.14.
https://issues.apache.org/jira/browse/HIVE-6741
shall you please release some artifact of org.spark-project.hive 0.14 above
?
Thx very much!



--

---------------------------------------------------------------------


"
"""Vasili I. Galchin"" <vigalchin@gmail.com>","Thu, 9 Jul 2015 22:40:53 -0700",PySpark vs R,dev@spark.apache.org,"Hello,

    Just trying to get up to speed ( a week .. pls be patient with me).

    I have been reading several docs .. plus ...

reading PySpark vs R code. I don't see an invariant between the Python
and R implementations. ??

   Probably I should read native Scala code, yes?

Kind thx,

Vasili

---------------------------------------------------------------------


"
Tom Graves <tgraves_cs@yahoo.com.INVALID>,"Fri, 10 Jul 2015 13:09:22 +0000 (UTC)",Re: [VOTE] Release Apache Spark 1.4.1 (RC4),"Patrick Wendell <pwendell@gmail.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","+1
Tom 


   

 Please vote on releasing the following candidate as Apache Spark version 1.4.1!

This release fixes a handful of known issues in Spark 1.4.0, listed here:
http://s.apache.org/spark-1.4.1

The tag to be voted on is v1.4.1-rc4 (commit dbaa5c"
Ted Yu <yuzhihong@gmail.com>,"Fri, 10 Jul 2015 06:16:11 -0700",Re: The latest master branch didn't compile with -Phive?,Josh Rosen <rosenville@gmail.com>,"Compilation on master branch has been fixed.

Thanks to Cheng Lian.


Pyarn -Phadoop-2.3 -Dhadoop.version=2.3.0 -Pkinesis-asl -Phive-thriftserver -Phive package assembly/assembly streaming-kafka-assembly/assembly streaming-flume-assembly/assembly
/HADOOP_PROFILE=hadoop-2.4,label=centos/2875/consoleFull
hive-thriftserver -Pkinesis-asl clean package
N/HADOOP_PROFILE=hadoop-2.4,label=centos/2875/consoleFull
doop-2.4/label/centos/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMetastoreCatalog.scala
k
m
iveMetastoreCatalog.scala
ure
"
MIKE HYNES <91mbbh@gmail.com>,"Fri, 10 Jul 2015 10:05:25 -0400",Re: Questions about Fault tolerance of Spark,"=?UTF-8?B?54mb5YWG5o23?= <nzjemail@gmail.com>, ""dev@spark.apache.org""
 <dev@spark.apache.org>","Gentle bump on this topic; how to test the fault tolerance and previous benchmark results are both things we are interested in as well.Â 
Mike

<div>-------- Original message --------</div><div>From: ç‰›å…†æ· <nzjemail@gmail.com> </div><div>Date:07-09-2015  04:19  (GMT-05:00) </div><div>To: dev@spark.apache.org, user@spark.apache.org </div><div>Subject: Questions about Fault tolerance of Spark </div><div>
</div>Hi All:

We already know that Spark utilizes the lineage to recompute the RDDs when failure occurs.
I want to study the performance of this fault-tolerant approach and have some questions about it.

1) Is there any benchmark (or standard failure model) to test the fault tolerance of these kinds of in-memory data processing systems? 

2) How do you emulate the failures in testing spark?  (e.g., kill a computation task? or kill the computation nodes?)

Thanks!!!

-- 
Regards,
Zhaojie

"
Denny Lee <denny.g.lee@gmail.com>,"Fri, 10 Jul 2015 15:21:21 +0000",Re: [VOTE] Release Apache Spark 1.4.1 (RC4),"Tom Graves <tgraves_cs@yahoo.com>, Patrick Wendell <pwendell@gmail.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","+1 (non binding)


"
Sean McNamara <Sean.McNamara@Webtrends.com>,"Fri, 10 Jul 2015 15:30:02 +0000",Re: [VOTE] Release Apache Spark 1.4.1 (RC4),Patrick Wendell <pwendell@gmail.com>,"+1

Sean

1.4.1!


---------------------------------------------------------------------


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Fri, 10 Jul 2015 09:07:29 -0700",Re: PySpark vs R,"""Vasili I. Galchin"" <vigalchin@gmail.com>","The R and Python implementations differ in how they communicate with the
JVM so there is no invariant there per-se.

Thanks
Shivaram


"
"""Vasili I. Galchin"" <vigalchin@gmail.com>","Fri, 10 Jul 2015 19:00:17 -0700",language-independent RDD Spark core code?,dev@spark.apache.org,"I am looking at R side, but curious what the RDD core side looks like.
Not sure which directory to look inside. ??

Thanks,

Vasili

---------------------------------------------------------------------


"
"""Vasili I. Galchin"" <vigalchin@gmail.com>","Fri, 10 Jul 2015 19:25:01 -0700",Re: language-independent RDD Spark core code?,dev@spark.apache.org,"think I found this RDD code


---------------------------------------------------------------------


"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Sat, 11 Jul 2015 02:44:32 +0000",Model parallelism with RDD,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

I am interested how scalable can be the model parallelism within Spark. Suppose, the model contains N weights of type Double and N is so large that does not fit into the memory of a single node. So, we can store the model in RDD[Double] within several nodes. To train the model, one needs to perform K iterations that update all the weights and check the convergence. Then we also need to exchange some weights between the nodes to synchronize the model or update the global state. I've sketched the code that does iterative updates with RDD (without global update yet). Surprisingly, each iteration takes more time than previous as shown below (time in seconds). Could you suggest what is the reason for that? I've checked GC, it does something within few milliseconds.

Configuration: Spark 1.4, 1 master and 5 worker nodes, 5 executors, Intel Xeon 2.2, 16GB RAM each
Iteration 0 time:1.127990986
Iteration 1 time:1.391120414
Iteration 2 time:1.6429691381000002
Iteration 3 time:1.9344402954
Iteration 4 time:2.2075294246999997
Iteration 5 time:2.6328659593
Iteration 6 time:2.7911690492999996
Iteration 7 time:3.0850374104
Iteration 8 time:3.4031050061
Iteration 9 time:3.8826580919

Code:
val modelSize = 1000000000
val numIterations = 10
val parallelizm = 5
var oldRDD = sc.parallelize(1 to modelSize, parallelizm).map(x => 0.1)
var newRDD = sc.parallelize(1 to 1, parallelizm).map(x => 0.1)
var i = 0
while (i < numIterations) {
  val t = System.nanoTime()
  // updating the weights
  val newRDD = oldRDD.map(x => x * x)
  oldRDD.unpersist(true)
  // ""checking"" convergence
  newRDD.mean
  println(""Iteration "" + i + "" time:"" + (System.nanoTime() - t) / 1e9 / numIterations)
  oldRDD = newRDD
  i += 1
}


Best regards, Alexander
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Fri, 10 Jul 2015 21:23:20 -0700",Re: Model parallelism with RDD,"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","I think you need to do `newRDD.cache()` and `newRDD.count` before you do
oldRDD.unpersist(true) -- Otherwise it might be recomputing all the
previous iterations each time.

Thanks
Shivaram


at
e.
ze
 does
h
t does
)
"
Sean Busbey <busbey@cloudera.com>,"Fri, 10 Jul 2015 23:34:49 -0500",Foundation policy on releases and Spark nightly builds,dev@spark.apache.org,"Hi Folks!

I noticed that Spark website's download page lists nightly builds and
instructions for accessing SNAPSHOT maven artifacts[1]. The ASF policy on
releases expressly forbids this kind of publishing outside of the dev@spark
community[2].

If you'd like to discuss having the policy updated (including expanding the
definition of ""in the development community""), please contribute to the
discussion on general@incubator[3] after removing the offending items.

[1]: http://spark.apache.org/downloads.html#nightly-packages-and-artifacts
[2]: http://www.apache.org/dev/release.html#what
[3]: http://s.apache.org/XFP

-- 
Sean
"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Sat, 11 Jul 2015 04:49:36 +0000",Re: Model parallelism with RDD,"""<shivaram@eecs.berkeley.edu>"" <shivaram@eecs.berkeley.edu>","Hi Shivaram,

Thank you for suggestion! If I do .cache and .count, each iteration take much more time, which is spent in GC. Is it normal?

10 èþëÿ 2015 ã., â 21:23, Shivaram Venkataraman <shivaram@eecs.berkeley.edu<mailto:shivaram@eecs.berkeley.edu>> íàïèñàë(à):

I think you need to do `newRDD.cache()` and `newRDD.count` before you do oldRDD.unpersist(true) -- Otherwise it might be recomputing all the previous iterations each time.

Thanks
Shivaram

Hi,

I am interested how scalable can be the model parallelism within Spark. Suppose, the model contains N weights of type Double and N is so large that does not fit into the memory of a single node. So, we can store the model in RDD[Double] within several nodes. To train the model, one needs to perform K iterations that update all the weights and check the convergence. Then we also need to exchange some weights between the nodes to synchronize the model or update the global state. I’ve sketched the code that does iterative updates with RDD (without global update yet). Surprisingly, each iteration takes more time than previous as shown below (time in seconds). Could you suggest what is the reason for that? I’ve checked GC, it does something within few milliseconds.

Configuration: Spark 1.4, 1 master and 5 worker nodes, 5 executors, Intel Xeon 2.2, 16GB RAM each
Iteration 0 time:1.127990986
Iteration 1 time:1.391120414
Iteration 2 time:1.6429691381000002
Iteration 3 time:1.9344402954
Iteration 4 time:2.2075294246999997
Iteration 5 time:2.6328659593
Iteration 6 time:2.7911690492999996
Iteration 7 time:3.0850374104
Iteration 8 time:3.4031050061
Iteration 9 time:3.8826580919

Code:
val modelSize = 1000000000
val numIterations = 10
val parallelizm = 5
var oldRDD = sc.parallelize(1 to modelSize, parallelizm).map(x => 0.1)
var newRDD = sc.parallelize(1 to 1, parallelizm).map(x => 0.1)
var i = 0
while (i < numIterations) {
  val t = System.nanoTime()
  // updating the weights
  val newRDD = oldRDD.map(x => x * x)
  oldRDD.unpersist(true)
  // “checking” convergence
  newRDD.mean
  println(""Iteration "" + i + "" time:"" + (System.nanoTime() - t) / 1e9 / numIterations)
  oldRDD = newRDD
  i += 1
}


Best regards, Alexander


---------------------------------------------------------------------


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Fri, 10 Jul 2015 22:03:36 -0700",Re: Model parallelism with RDD,"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Yeah I can see that being the case -- caching implies creating objects that
will be stored in memory. So there is a trade-off between storing data in
memory but having to garbage collect it later vs. recomputing the data.

Shivaram


aman <
°Ð¿Ð¸ÑÐ°Ð»(Ð°):
at
e.
ze
 does
h
t does
)
"
Sean Owen <sowen@cloudera.com>,"Sat, 11 Jul 2015 12:17:55 +0100",Re: Foundation policy on releases and Spark nightly builds,Sean Busbey <busbey@cloudera.com>,"I suggest we move this info to the developer wiki, to keep it out from
the place all and users look for downloads. What do you think about
that Sean B?


---------------------------------------------------------------------


"
=?UTF-8?B?54mb5YWG5o23?= <nzjemail@gmail.com>,"Sat, 11 Jul 2015 22:34:02 +0800",Re: Questions about Fault tolerance of Spark,MIKE HYNES <91mbbh@gmail.com>,"I am in the beginning, would you like to share something on this area?

2015-07-10 22:05 GMT+08:00 MIKE HYNES <91mbbh@gmail.com>:

n


-- 
*Regards,*
*Zhaojie*
"
Sean Busbey <busbey@cloudera.com>,"Sat, 11 Jul 2015 09:44:12 -0500",Re: Foundation policy on releases and Spark nightly builds,Sean Owen <sowen@cloudera.com>,"That would be great.

A note on that page that it's meant for the use of folks working on the
project with a link to your ""get involved"" howto would be nice additional
context.

-- 
Sean

"
Reynold Xin <rxin@databricks.com>,"Sat, 11 Jul 2015 08:29:10 -0700",Re: Foundation policy on releases and Spark nightly builds,Sean Busbey <busbey@cloudera.com>,"I don't get this rule. It is arbitrary, and does not seem like something
that should be enforced at the foundation level. By this reasoning, are we
not allowed to list ""source code management"" on the project public page as
well?

The download page clearly states the nightly builds are ""bleeding-edge"".

Note that technically we did not violate any rules, since the ones we
showed were not ""nightly builds"" by the foundation's definition: ""Nightly
Builds are simply built from the Subversion trunk, usually once a day."".
Spark nightly artifacts were built from git, not svn trunk. :)  (joking).




"
Sean Owen <sowen@cloudera.com>,"Sat, 11 Jul 2015 16:53:17 +0100",Re: Foundation policy on releases and Spark nightly builds,Reynold Xin <rxin@databricks.com>,"nightly builds should be hidden from non-developer end users. In an
age of Github, what on earth is the problem with distributing the
content of master? However I do understand why this exists.

To the extent the ASF provides any value, it is at least a legal
framework for defining what it means for you and I to give software to
a bunch of other people. Software artifacts released according to an
ASF process becomes something the ASF can take responsibility for as
an entity. Nightly builds are not. It might matter to the committers
if, say, somebody commits a serious data loss bug. You don't want to
be on the hook individually for putting that into end-user hands.

More practically, I think this exists to prevent some projects from
lazily depending on unofficial nightly builds as pseudo-releases for
long periods of time. End users may come to perceive them as official
sanctioned releases when they aren't. That's not the case here of
course.

I think nightlies aren't for end-users anyway, and I think developers
who care would know how to get nightlies anyway. There's little cost
to moving this info to the wiki, so I'd do it.


---------------------------------------------------------------------


"
Matt Goodman <meawoppl@gmail.com>,"Sat, 11 Jul 2015 11:07:12 -0700",Re: Should spark-ec2 get its own repo?,dev@spark.apache.org,"I wanted to revive the conversation about the spark-ec2 tools, as it seems
to have been lost in the 1.4.1 release voting spree.

I think that splitting it into its own repository is a really good move,
and I would also be happy to help with this transition, as well as help
maintain the resulting repository.  Here is my justification for why we
ought to do this split.

User Facing:

   - The spark-ec2 launcher dosen't use anything in the parent spark
   repository
   - spark-ec2 version is disjoint from the parent repo.  I consider it
   confusing that the spark-ec2 script dosen't launch the version of spark it
   is checked-out with.
   - Someone interested in setting up spark-ec2 with anything but the
   default configuration will have to clone at least 2 repositories at
   present, and probably fork and push changes to 1.
   - spark-ec2 has mismatched dependencies wrt. to spark itself.  This
   includes a confusing shim in the spark-ec2 script to install boto, which
   frankly should just be a dependency of the script

Developer Facing:

   - Support across 2 repos will be worse than across 1.  Its unclear where
   to file issues/PRs, and requires extra communications for even fairly
   trivial stuff.
   - Spark-ec2 also depends on a number binary blobs being in the right
   place, currently the responsibility for these is decentralized, and likely
   prone to various flavors of dumb.
   - The current flow of booting a spark-ec2 cluster is _complicated_ I
   spent the better part of a couple days figuring out how to integrate our
   custom tools into this stack.  This is very hard to fix when commits/PR's
   need to span groups/repositories/buckets-o-binary, I am sure there are
   several other problems that are languishing under similar roadblocks
   - It makes testing possible.  The spark-ec2 script is a great case for
   CI given the number of permutations of launch criteria there are.  I
   suspect AWS would be happy to foot the bill on spark-ec2 testing (probably
   ~20 bucks a month based on some envelope sketches), as it is a piece of
   software that directly impacts other people giving them money.  I have some
   contacts there, and I am pretty sure this would be an easy conversation,
   particularly if the repo directly concerned with ec2.  Think also being
   able to assemble the binary blobs into s3 bucket dedicated to spark-ec2

Any other thoughts/voices appreciated here.  spark-ec2 is a super-power
tool and deserves a fair bit of attention!
--Matthew Goodman

=====================
Check Out My Website: http://craneium.net
Find me on LinkedIn: http://tinyurl.com/d6wlch
"
"""Vasili I. Galchin"" <vigalchin@gmail.com>","Sat, 11 Jul 2015 20:57:53 -0700",Spark application examples,dev@spark.apache.org,"Hello,

      Reading slides entitled ""DATABRICKS"" written by Holden Karau, et. al.

      I am also reading Spark application examples under
../spark/examples/src/main/*.

      Let's assume examples Driver data-manipulation.R and
dataframe.R. Question: where in these Drivers are the worker ""bees""
spawned??

Vasili

---------------------------------------------------------------------


"
Jerry Lam <chilinglam@gmail.com>,"Sat, 11 Jul 2015 22:32:37 -0700 (MST)",Re: [PySpark DataFrame] When a Row is not a Row,dev@spark.apache.org,"Hi guys,

I just hit the same problem. It is very confusing when Row is not the same
Row type at runtime. The worst thing is that when I use Spark in local mode,
the Row is the same Row type! so it passes the test cases but it fails when
I deploy the application. 

Can someone suggest a workaround?

Best Regards,

Jerry



--

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Sun, 12 Jul 2015 09:34:31 +0100",Re: Should spark-ec2 get its own repo?,Matt Goodman <meawoppl@gmail.com>,"I agree with these points. The ec2 support is substantially a separate
project, and would likely be better managed as one. People can much
more rapidly iterate on it and release it.

I suggest:

1. Pick a new repo location. amplab/spark-ec2 ? spark-ec2/spark-ec2 ?
2. Add interested parties as owners/contributors
3. Reassemble a working clone of the current code from spark/ec2 and
mesos/spark-ec2 and check it in
4. Announce the new location on user@, dev@
5. Triage open JIRAs to the new repo's issue tracker and close them elsewhere
6. Remove the old copies of the code and leave a pointer to the new
location in their place

I'd also like to hear a few more nods before pulling the trigger though.


---------------------------------------------------------------------


"
Gil Vernik <GILV@il.ibm.com>,"Sun, 12 Jul 2015 13:05:57 +0300",question related partitions of the DataFrame,Dev <dev@spark.apache.org>,"Hi,

DataFrame extends RDDApi, that provides RDD like methods.
My question is, does DataFrame is sort of  stand alone RDD with it?s own 
partitions or it depends on the underlying RDD that was used to load the 
data into its partitions? It's written that DataFrame has ability to scale 
from kilobytes of data on a single laptop to petabytes on a large cluster, 
but i don't understand if the partitions of data frame are independent of 
the partitions of the data source that was used to load the data.

So assume theoretically that i used external DataSource API and wrote code 
 that load 1GB of data into single partition. Then I map this DataSource 
to DataFrame and perform some SQL that returns all the records. Will this 
DataFrame also has one partition in memory or Spark somehow will divide 
this DataFrame into various partitions? If so, how it will be divide it 
into partitions? By size? (can someone point me to the code to see some 
example)? 


Thanks,
Gil."
=?UTF-8?Q?Ren=C3=A9_Treffer?= <rtreffer@gmail.com>,"Sun, 12 Jul 2015 12:49:21 +0200",Spark master broken?,dev@spark.apache.org,"Hi *,

I'm currently trying to build master but it fails with

 [error] Picked up JAVA_TOOL_OPTIONS:
 I've tried to build for 2.11 and 2.10 without success. Is there a known
issue on master?

Regards,
  Rene Treffer
"
Ted Yu <yuzhihong@gmail.com>,"Sun, 12 Jul 2015 07:32:05 -0700",Re: Spark master broken?,=?UTF-8?Q?Ren=C3=A9_Treffer?= <rtreffer@gmail.com>,"Jenkins shows green builds.

What Java version did you use ?

Cheers

te:

park/sql/execution/UnsafeExternalRowSorter.java:135:
t
"
=?UTF-8?Q?Ren=C3=A9_Treffer?= <rtreffer@gmail.com>,"Sun, 12 Jul 2015 17:22:52 +0200",Re: Spark master broken?,Ted Yu <yuzhihong@gmail.com>,"Java 8, make-distribution

Jenkins does show the same error, though:
https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-Maven-Snapshots/325/console


rote:
spark/sql/execution/UnsafeExternalRowSorter.java:135:
ct
-
"
Josh Rosen <rosenville@gmail.com>,"Sun, 12 Jul 2015 08:36:36 -0700",Re: Spark master broken?,=?utf-8?Q?Ren=C3=A9_Treffer?= <rtreffer@gmail.com>,"I think it is just broken for 2.11 since pull requests are building properly.

Sent from my phone


enkins/job/Spark-Master-Maven-Snapshots/325/console
rote:
aag.jar 
/apache/spark/sql/execution/UnsafeExternalRowSorter.java:135: 
r$1> is not abstract and does not override abstract method <B>
,Ordering<B>) 
-------- 
94s] 
2.035s] 
506s] 
076s] 
5.520s] 
41s] 
41s] 
98s] 
154s] 
048s]
 issue on master?
"
Olivier Delalleau <shish@keba.be>,"Sun, 12 Jul 2015 12:37:15 -0400",Spark development under Windows,dev@spark.apache.org,"Hi,

New to Spark here, trying to look into issue
https://issues.apache.org/jira/browse/SPARK-8976

Ideallly I'd rather develop under Windows rather than a Linux VM, since
this is the environment I want to test Spark in. My first step before
changing any code would be to run the test suite (dev/run-tests). However
it's not working ""out of the box"" in Git's MinGW shell.

Thus two questions:
1. Are there instructions somewhere on how to run tests under Windows?
2. If not, is there interest among Spark developers that I help make it
work, or should I just switch to a Linux VM for development and forget
about running the test-suite under Windows?

Thanks,

-=- Olivier
"
Patrick Wendell <pwendell@gmail.com>,"Sun, 12 Jul 2015 14:50:23 -0400",Re: Foundation policy on releases and Spark nightly builds,Sean Owen <sowen@cloudera.com>,"Hey Sean B.,

Thanks for bringing this to our attention. I think putting them on the
developer wiki would substantially decrease visibility in a way that
is not beneficial to the project - this feature was specifically
requested by developers from other projects that integrate with Spark.

If the concern underlying that policy is that snapshot builds could be
misconstrued as formal releases, I think it would work to put a very
clear disclaimer explaining the difference directly adjacent to the
link. That's arguably more explicit than just moving the same text to
a different page.

The formal policy asks us not to include links ""that encourage
non-developers to download"" the builds. Stating clearly that the
audience for those links is developers, in my interpretation that
would satisfy the letter and spirit of this policy.

- Patrick


---------------------------------------------------------------------


"
Davies Liu <davies@databricks.com>,"Sun, 12 Jul 2015 12:21:46 -0700",Re: [PySpark DataFrame] When a Row is not a Row,Jerry Lam <chilinglam@gmail.com>,"We finally fix this in 1.5 (next release), see
https://github.com/apache/spark/pull/7301


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Sun, 12 Jul 2015 20:39:39 +0100",Re: Foundation policy on releases and Spark nightly builds,Patrick Wendell <pwendell@gmail.com>,"(This sounds pretty good to me. Mark it developers-only, not formally
tested by the community, etc.)


---------------------------------------------------------------------


"
Cheolsoo Park <piaozhexiu@gmail.com>,"Sun, 12 Jul 2015 16:33:42 -0400",pyspark.sql.tests: is test_time_with_timezone a flaky test?,Dev <dev@spark.apache.org>,"Hi devs,

For some reason, I keep getting this test failure (3 out of 4 builds) in my
PR <https://github.com/apache/spark/pull/7216>-

======================================================================
FAIL: test_time_with_timezone (__main__.SQLTests)
----------------------------------------------------------------------
Traceback (most recent call last):
  File
""/home/jenkins/workspace/SparkPullRequestBuilder/python/pyspark/sql/tests.py"",
line 718, in test_time_with_timezone
    *self.assertEqual(now, now1)*
AssertionError: datetime.datetime(2015, 7, 12, 13, 18, 46, *504366*) !=
datetime.datetime(2015, 7, 12, 13, 18, 46, *504365*)

Jenkins builds-
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/37100/console
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/37092/console
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/37081/console

I am aware that there was a hot fix for this test case, and I already have
it in the commit log-

commit 05ac023dc8d9004a27c2f06ee875b0ff3743ccdd

Author: Davies Liu <davies@databricks.com>
Date:   Fri Jul 10 13:05:23 2015 -0700
    [HOTFIX] fix flaky test in PySpark SQL

I looked at the test code, and it seems that precision in microseconds is
lost somewhere in a round trip from Python to DataFrame. Can someone please
help me debug this error?

Thanks!
Cheolsoo
"
Patrick Wendell <pwendell@gmail.com>,"Sun, 12 Jul 2015 17:26:59 -0400",Re: Foundation policy on releases and Spark nightly builds,Sean Owen <sowen@cloudera.com>,"Thanks Sean O. I was thinking something like ""NOTE: Nightly builds are
meant for development and testing purposes. They do not go through
Apache's release auditing process and are not official releases.""

- Patrick


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Sun, 12 Jul 2015 18:02:33 -0400",Re: [VOTE] Release Apache Spark 1.4.1 (RC4),Sean McNamara <Sean.McNamara@webtrends.com>,"I think we can close this vote soon. Any addition votes/testing would
be much appreciated!


---------------------------------------------------------------------


"
emrehan <emrehan.tuzun@gmail.com>,"Sun, 12 Jul 2015 16:12:12 -0700 (MST)",Re: Are These Issues Suitable for our Senior Project?,dev@spark.apache.org,"Thanks for all the kind responses!

I'd love to experience the contribution flow with a small task first, but I
couldn't find any unassigned interesting tickets for 1.5. I'm hoping to get
assigned to a small ticket by the end of the summer though.

If nobody would suggest anything else it seems like our best bets are
SPARK-6442 (Local Linear Algebra) and SPARK-3703 (Ensemble Learning). Could
these tickets need to be resolved before May 2016?

Out of these two, I'm more inclined towards SPARK-6442, not because it's
more interesting but it's an isolated feature.

â€‹Best regards,
Emrehanâ€‹



--
3.nabble.com/Are-These-Issues-Suitable-for-our-Senior-Project-tp13119p13167.html
om.

---------------------------------------------------------------------


"
Davies Liu <davies@databricks.com>,"Sun, 12 Jul 2015 19:45:39 -0700",Re: pyspark.sql.tests: is test_time_with_timezone a flaky test?,Cheolsoo Park <piaozhexiu@gmail.com>,"Thanks for reporting this, I'm working on it. It turned out that it's
a bug in when run with Python3.4, will sending out a fix soon.


---------------------------------------------------------------------


"
Sean Busbey <busbey@cloudera.com>,"Sun, 12 Jul 2015 21:52:34 -0500",Re: Foundation policy on releases and Spark nightly builds,Patrick Wendell <pwendell@gmail.com>,"Please note that when the policy refers to ""developers"" it means the
developers of the project at hand, that is participants on the dev@spark
mailing list.

As I stated in my original email, you're welcome to continue the discussion
on the policy including the definition of developers on general@incubator.
But please comply with foundation policy before seeking to have it changed.

Just to set expectations, ""this feature was specifically requested by
developers from other projects that integrate with Spark"" sounds like
exactly the kind of thing the policy seeks to prevent. The standing
guidance is ""release more often"" if downstream projects need to integrate
with features faster.





-- 
Sean
"
Davies Liu <davies@databricks.com>,"Sun, 12 Jul 2015 19:59:45 -0700",Re: pyspark.sql.tests: is test_time_with_timezone a flaky test?,Cheolsoo Park <piaozhexiu@gmail.com>,"Will be fixed by https://github.com/apache/spark/pull/7363


---------------------------------------------------------------------


"
Xiaoyu Ma <hzmaxiaoyu@corp.netease.com>,"Mon, 13 Jul 2015 11:26:28 +0800",./dev/run-tests fail on master,dev@spark.apache.org,"Hi guys,
I was trying to rerun test using run-tests on master but I got below errors. I was able to build using maven though. Any advice?

[error]                        ^
[error] /Users/ilovesoup1/workspace/eclipseWS/spark/network/common/src/main/java/org/apache/spark/network/server/TransportRequestHandler.java:24: error: package org.slf4j does not exist
[error] import org.slf4j.Logger;
[error]                 ^
[error] /Users/ilovesoup1/workspace/eclipseWS/spark/network/common/src/main/java/org/apache/spark/network/server/TransportRequestHandler.java:25: error: package org.slf4j does not exist
[error] import org.slf4j.LoggerFactory;
[error]                 ^
[error] /Users/ilovesoup1/workspace/eclipseWS/spark/network/common/src/main/java/org/apache/spark/network/server/TransportChannelHandler.java:75: error: cannot find symbol
[error]   public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception {
[error]                               ^
[error]   symbol:   class ChannelHandlerContext
[error]   location: class TransportChannelHandler
[error] 100 errors
[info] Done updating.
[info] Done updating.
[warn] There may be incompatibilities among your library dependencies.
[warn] Here are some of the libraries that were evicted:
[warn] 	* com.google.code.findbugs:jsr305:1.3.9 -> 2.0.1
[warn] 	* com.google.guava:guava:11.0.2 -> 14.0.1
[warn] 	* io.netty:netty-all:4.0.23.Final -> 4.0.28.Final
[warn] 	* commons-net:commons-net:2.2 -> 3.1
[warn] Run 'evicted' to see detailed eviction warnings
[info] Updating {file:/Users/ilovesoup1/workspace/eclipseWS/spark/}graphx...
[info] Updating {file:/Users/ilovesoup1/workspace/eclipseWS/spark/}catalyst...
[info] Updating {file:/Users/ilovesoup1/workspace/eclipseWS/spark/}bagel...
[info] Updating {file:/Users/ilovesoup1/workspace/eclipseWS/spark/}yarn...
[info] Updating {file:/Users/ilovesoup1/workspace/eclipseWS/spark/}streaming...
[info] Done updating.
[warn] There may be incompatibilities among your library dependencies.
[warn] Here are some of the libraries that were evicted:
[warn] 	* com.google.guava:guava:11.0.2 -> 14.0.1
[warn] Run 'evicted' to see detailed eviction warnings
[info] Done updating.
[warn] There may be incompatibilities among your library dependencies.
[warn] Here are some of the libraries that were evicted:
[warn] 	* com.google.guava:guava:11.0.2 -> 14.0.1
[warn] Run 'evicted' to see detailed eviction warnings
[info] Updating {file:/Users/ilovesoup1/workspace/eclipseWS/spark/}streaming-twitter...
[info] Updating {file:/Users/ilovesoup1/workspace/eclipseWS/spark/}streaming-kafka...
[info] Updating {file:/Users/ilovesoup1/workspace/eclipseWS/spark/}streaming-flume...
[info] Updating {file:/Users/ilovesoup1/workspace/eclipseWS/spark/}streaming-zeromq...
[info] Updating {file:/Users/ilovesoup1/workspace/eclipseWS/spark/}kinesis-asl...
[info] Done updating.
[info] Updating {file:/Users/ilovesoup1/workspace/eclipseWS/spark/}streaming-mqtt...
[info] Done updating.
[info] Updating {file:/Users/ilovesoup1/workspace/eclipseWS/spark/}tools...
[info] Done updating.
[warn] There may be incompatibilities among your library dependencies.
[warn] Here are some of the libraries that were evicted:
[warn] 	* com.google.guava:guava:11.0.2 -> 14.0.1
[warn] Run 'evicted' to see detailed eviction warnings
[info] Updating {file:/Users/ilovesoup1/workspace/eclipseWS/spark/}sql...
[info] Done updating.
[info] Done updating.
[info] Done updating.
[info] Done updating.
[info] Done updating.
[info] Updating {file:/Users/ilovesoup1/workspace/eclipseWS/spark/}streaming-flume-assembly...
[info] Done updating.
[info] Updating {file:/Users/ilovesoup1/workspace/eclipseWS/spark/}streaming-kafka-assembly...
[info] Done updating.
[info] Done updating.
[info] Done updating.
[warn] There may be incompatibilities among your library dependencies.
[warn] Here are some of the libraries that were evicted:
[warn] 	* commons-net:commons-net:2.2 -> 3.1
[warn] Run 'evicted' to see detailed eviction warnings
[info] Done updating.
[info] Updating {file:/Users/ilovesoup1/workspace/eclipseWS/spark/}mllib...
[info] Updating {file:/Users/ilovesoup1/workspace/eclipseWS/spark/}hive...
[info] Done updating.
[info] Updating {file:/Users/ilovesoup1/workspace/eclipseWS/spark/}repl...
[info] Done updating.
[info] Updating {file:/Users/ilovesoup1/workspace/eclipseWS/spark/}hive-thriftserver...
[info] Updating {file:/Users/ilovesoup1/workspace/eclipseWS/spark/}examples...
[info] Done updating.
[info] Done updating.
[info] Done updating.
[warn] There may be incompatibilities among your library dependencies.
[warn] Here are some of the libraries that were evicted:
[warn] 	* com.google.guava:guava:11.0.2 -> 14.0.1
[warn] Run 'evicted' to see detailed eviction warnings
[info] Updating {file:/Users/ilovesoup1/workspace/eclipseWS/spark/}assembly...
[info] Done updating.
[error] (streaming-flume-sink/compile:compile) Compilation failed
[error] (unsafe/compile:compile) javac returned nonzero exit code
[error] (network-common/compile:compile) javac returned nonzero exit code
[error] Total time: 376 s, completed Jul 13, 2015 11:17:33 AM

é©¬æ™“å®‡ / Xiaoyu Ma
hzmaxiaoyu@corp.netease.com




"
Patrick Wendell <pwendell@gmail.com>,"Sun, 12 Jul 2015 20:32:11 -0700",Re: Foundation policy on releases and Spark nightly builds,Sean Busbey <busbey@cloudera.com>,"Hey Sean B,

Would you mind outlining for me how we go about changing this policy -
I think it's outdated and doesn't make much sense. Ideally I'd like to
propose a vote to modify the text slightly such that our current
behavior is seen as complaint. Specifically:

- What concrete steps can I take to change the policy?
- Who has the authority to change this document?
- You keep mentioning the incubator@ list, why is this the place for
such policy to be discussed or decided on?
- What is the reasonable amount of time frame in which the policy
change is likely to be decided?

We've had a few times people from the various parts of the ASF come
and say we are in violation of a policy. And sometimes other ASF
people come and then get in a fight on our mailing list, and there is
back and fourth, and it turns out there isn't so much a widely
followed policy as a doc somewhere that is really old and not actually
universally followed. It's difficult for us in such situations to now
how to proceed and how much autonomy we as a PMC have to make
decisions about our own project.

- Patrick


---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Sun, 12 Jul 2015 20:34:02 -0700",Re: ./dev/run-tests fail on master,Xiaoyu Ma <hzmaxiaoyu@corp.netease.com>,"When I ran dev/run-tests , I got :

File ""./dev/run-tests.py"", line 68, in
__main__.identify_changed_files_from_git_commits
Failed example:
    'root' in [x.name for x in determine_modules_for_files(
 identify_changed_files_from_git_commits(""50a0496a43"",
target_ref=""6765ef9""))]
Exception raised:
    Traceback (most recent call last):
      File ""/usr/lib64/python2.6/doctest.py"", line 1253, in __run
        compileflags, 1) in test.globs
      File ""<doctest __main__.identify_changed_files_from_git_commits[1]>"",
line 1, in <module>
        'root' in [x.name for x in determine_modules_for_files(
 identify_changed_files_from_git_commits(""50a0496a43"",
target_ref=""6765ef9""))]
      File ""./dev/run-tests.py"", line 82, in
identify_changed_files_from_git_commits
        raw_output = subprocess.check_output(['git', 'diff', '--name-only',
patch_sha, diff_target],
    AttributeError: 'module' object has no attribute 'check_output'

I was using python 2.6.6

Xiaoyu:
In the interim, can you use maven to run test suite ?

Cheers


org/apache/spark/network/server/TransportRequestHandler.java:24:
org/apache/spark/network/server/TransportRequestHandler.java:25:
org/apache/spark/network/server/TransportChannelHandler.java:75:
e
..
.
..
ly...
ly...
..
.
.
"
shane knapp <sknapp@berkeley.edu>,"Sun, 12 Jul 2015 20:49:47 -0700","Re: jenkins downtime 7/13/15, 7am PDT","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","reminder:  this is happening tomorrow morning!


---------------------------------------------------------------------


"
Xiaoyu Ma <hzmaxiaoyu@corp.netease.com>,"Mon, 13 Jul 2015 14:11:48 +0800",Re: ./dev/run-tests fail on master,Ted Yu <yuzhihong@gmail.com>,"Hi Ted,
Seems maven build/test part works fine for me. Thanks!

Forget to provide more info:
Iâ€™m using python 2.7.6, MacOS 10.10.3, JDK 1.7.0_79, Maven 3.3.1

é©¬æ™“å®‡ / Xiaoyu Ma
hzmaxiaoyu@corp.netease.com




__main__.identify_changed_files_from_git_commits
determine_modules_for_files(          identify_changed_files_from_git_commits(""50a0496a43"", target_ref=""6765ef9""))]
__main__.identify_changed_files_from_git_commits[1]>"", line 1, in <module>
determine_modules_for_files(          identify_changed_files_from_git_commits(""50a0496a43"", target_ref=""6765ef9""))]
identify_changed_files_from_git_commits
'--name-only', patch_sha, diff_target],
<hzmaxiaoyu@corp.netease.com <mailto:hzmaxiaoyu@corp.netease.com>> errors. I was able to build using maven though. Any advice?
/Users/ilovesoup1/workspace/eclipseWS/spark/network/common/src/main/java/org/apache/spark/network/server/TransportRequestHandler.java:24: error: package org.slf4j does not exist
/Users/ilovesoup1/workspace/eclipseWS/spark/network/common/src/main/java/org/apache/spark/network/server/TransportRequestHandler.java:25: error: package org.slf4j does not exist
/Users/ilovesoup1/workspace/eclipseWS/spark/network/common/src/main/java/org/apache/spark/network/server/TransportChannelHandler.java:75: error: cannot find symbol
Throwable cause) throws Exception {
{file:/Users/ilovesoup1/workspace/eclipseWS/spark/}graphx...
{file:/Users/ilovesoup1/workspace/eclipseWS/spark/}catalyst...
{file:/Users/ilovesoup1/workspace/eclipseWS/spark/}bagel...
{file:/Users/ilovesoup1/workspace/eclipseWS/spark/}yarn...
{file:/Users/ilovesoup1/workspace/eclipseWS/spark/}streaming...
{file:/Users/ilovesoup1/workspace/eclipseWS/spark/}streaming-twitter...
{file:/Users/ilovesoup1/workspace/eclipseWS/spark/}streaming-kafka...
{file:/Users/ilovesoup1/workspace/eclipseWS/spark/}streaming-flume...
{file:/Users/ilovesoup1/workspace/eclipseWS/spark/}streaming-zeromq...
{file:/Users/ilovesoup1/workspace/eclipseWS/spark/}kinesis-asl...
{file:/Users/ilovesoup1/workspace/eclipseWS/spark/}streaming-mqtt...
{file:/Users/ilovesoup1/workspace/eclipseWS/spark/}tools...
{file:/Users/ilovesoup1/workspace/eclipseWS/spark/}sql...
{file:/Users/ilovesoup1/workspace/eclipseWS/spark/}streaming-flume-assembly...
{file:/Users/ilovesoup1/workspace/eclipseWS/spark/}streaming-kafka-assembly...
{file:/Users/ilovesoup1/workspace/eclipseWS/spark/}mllib...
{file:/Users/ilovesoup1/workspace/eclipseWS/spark/}hive...
{file:/Users/ilovesoup1/workspace/eclipseWS/spark/}repl...
{file:/Users/ilovesoup1/workspace/eclipseWS/spark/}hive-thriftserver...
{file:/Users/ilovesoup1/workspace/eclipseWS/spark/}examples...
{file:/Users/ilovesoup1/workspace/eclipseWS/spark/}assembly...
code

"
Patrick Wendell <pwendell@gmail.com>,"Sun, 12 Jul 2015 23:18:58 -0700",[RESULT] [VOTE] Release Apache Spark 1.4.1 (RC4),"""dev@spark.apache.org"" <dev@spark.apache.org>","This vote passes with 14 +1 (7 binding) votes and no 0 or -1 votes.

+1 (14):
Patrick Wendell
Reynold Xin
Sean Owen
Burak Yavuz
Mark Hamstra
Michael Armbrust
Andrew Or
York, Brennon
Krishna Sankar
Luciano Resende
Holden Karau
Tom Graves
Denny Lee
Sean McNamara

- Patrick


---------------------------------------------------------------------


"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Mon, 13 Jul 2015 09:16:29 +0000",RandomForest evaluator for grid search,dev <dev@spark.apache.org>,"Hi everyone,
Using spark-ml there seems to be only BinaryClassificationEvaluator and
RegressionEvaluator, is there any way or plan to provide a ROC-based or
PR-based or F-Measure based for multi-class, I would be interested
especially in evaluating and doing a grid search for a RandomForest model.

Regards,

Olivier.
"
Cheolsoo Park <piaozhexiu@gmail.com>,"Mon, 13 Jul 2015 06:05:55 -0400",Re: pyspark.sql.tests: is test_time_with_timezone a flaky test?,Davies Liu <davies@databricks.com>,"Thank you!


"
srinivasraghavansr71 <sreenivas.raghavan7@gmail.com>,"Mon, 13 Jul 2015 04:59:53 -0700 (MST)",Contributiona nd choice of langauge,dev@spark.apache.org,"Hello everyone,
                   I am interested to contribute to apache spark. I am more
inclined towards algorithms and computational methods for matrices,etc. I
took one course in edx where spark was taught through python interface. So
My doubts are as follows:

1. Place from where I can start working.
2. Language for coding - Is using python okay, or Is there any fixed
language I should Use? 



--

---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Mon, 13 Jul 2015 07:09:14 -0700","Re: jenkins downtime 7/13/15, 7am PDT","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","this is happening now.


---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Mon, 13 Jul 2015 08:13:22 -0700","Re: jenkins downtime 7/13/15, 7am PDT","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","this is still ongoing...  30 minutes after applying the system updates
and running init 6, the jenkins master hasn't yet recovered from the
reboot.  i've opened a ticket w/our sysadmin group and am crossing my
fingers that this doesn't mean a trip down to the datacenter.

more updates as they come.


---------------------------------------------------------------------


"
spark user <spark_user@yahoo.com.INVALID>,"Mon, 13 Jul 2015 16:22:13 +0000 (UTC)",How to Read Excel file in Spark 1.4,"Spark User <spark_user@yahoo.com>, Dev <dev@spark.apache.org>","HiÂ 
I need your help to save excel data in hive .
   
   - how to read excel file in spark using spark 1.4Â 
   - How to save using data frameÂ 
If you have some sample code pls sendÂ 
ThanksÂ 
su"
shane knapp <sknapp@berkeley.edu>,"Mon, 13 Jul 2015 09:39:07 -0700","Re: jenkins downtime 7/13/15, 7am PDT","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>, 
	Josh Rosen <joshrosen@databricks.com>, Patrick Wendell <patrick@databricks.com>","(or it's a kernel issue, but whatevs, i'll fix it)


"
shane knapp <sknapp@berkeley.edu>,"Mon, 13 Jul 2015 09:37:18 -0700","Re: jenkins downtime 7/13/15, 7am PDT","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>, 
	Josh Rosen <joshrosen@databricks.com>, Patrick Wendell <patrick@databricks.com>","looks like we lost our boot drive...  i'll be fixing this today, but expect
the downtime to last for a while as we sort this out.




"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Mon, 13 Jul 2015 09:47:40 -0700",Re: Should spark-ec2 get its own repo?,Sean Owen <sowen@cloudera.com>,"I think moving the repo-location and re-organizing the python code to
handle dependencies, testing etc. sounds good to me. However, I think there
are a couple of things which I am not sure about

1. I strongly believe that we should preserve existing command-line in
ec2/spark-ec2 (i.e. the shell script not the python file). This could be a
thin wrapper script that just checks out the or downloads something
(similar to say build/mvn). Mainly, I see no reason to break the workflow
that users are used to right now.

2. I am also not sure about that moving the issue tracker is necessarily a
good idea. I don't think we get a large number of issues due to the EC2
stuff  and if we do have a workflow for launching EC2 clusters, the Spark
JIRA would still be the natural place to report issues related to this.

At a high level I see the spark-ec2 scripts as an effort to provide a
reference implementation for launching EC2 clusters with Apache Spark --
Given this view I am not sure it makes sense to completely decouple this
from the Apache project.

Thanks
Shivaram


"
Sandy Ryza <sandy.ryza@cloudera.com>,"Mon, 13 Jul 2015 10:16:04 -0700",Re: How to Read Excel file in Spark 1.4,spark user <spark_user@yahoo.com>,"Hi Su,

Spark can't read excel files directly.  Your best best is probably to
export the contents as a CSV and use the ""csvFile"" API.

-Sandy


"
shane knapp <sknapp@berkeley.edu>,"Mon, 13 Jul 2015 10:20:38 -0700","Re: jenkins downtime 7/13/15, 7am PDT","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>, 
	Josh Rosen <joshrosen@databricks.com>, Patrick Wendell <patrick@databricks.com>","ok, we're up.  looks like my yum update, which got interrupted by AT&T for
2 minutes, messed up the kernel and ganglia updates.  after reinstalling
the kernel packages and whack-a-moling the user:group settings on some
ganglia directories w/our sysadmin, things seem to be working fine.

i'll be keeping an eye on this today to see if i missed anything.

lessons (re)learned:  use a screen session while doing this, unless you
trust your telco.  :)


"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Mon, 13 Jul 2015 18:15:14 +0000",RE: Model parallelism with RDD,"""shivaram@eecs.berkeley.edu"" <shivaram@eecs.berkeley.edu>","Below are the average timings for one iteration of model update with RDD  (with cache, as Shivaram suggested):
Model size, RDD[Double].count / time, s
10M 0.585336926
100M 1.767947506
1B 125.6078817

There is a ~100x increase in time while 10x increase in model size (from 100 million to 1 billion of Double). More than half of the time is spent in GC, and this time varies heavily. Two questions:
1)Can I use project Tungstenâ€™s unsafe? Actually, can I reduce the GC time if I use DataFrame instead of RDD and set the Tungsten key: spark.sql.unsafe.enabled=true ?
2) RDD[Double] of one billion elements is 26.1GB persisted (as Spark UI shows). It is around 26 bytes per element. How many bytes is RDD overhead?

The code:
val modelSize = 1000000000
val numIterations = 10
val parallelism = 5
var oldRDD = sc.parallelize(1 to modelSize, parallelism).map(x => 0.1).cache
var newRDD = sc.parallelize(1 to 1, parallelism).map(x => 0.1)
var i = 0
var avgTime = 0.0
while (i < numIterations) {
  val t = System.nanoTime()
  val newRDD = oldRDD.map(x => x * x)
  newRDD.cache
  newRDD.count()
  oldRDD.unpersist(true)
  newRDD.mean
  avgTime += (System.nanoTime() - t) / 1e9
  oldRDD = newRDD
  i += 1
}
println(""Avg iteration time:"" + avgTime / numIterations)

Best regards, Alexanderkeley.edu]
Sent: Friday, July 10, 2015 10:04 PM
To: Ulanov, Alexander
Cc: <shivaram@eecs.berkeley.edu>; dev@spark.apache.org
Subject: Re: Model parallelism with RDD

Yeah I can see that being the case -- caching implies creating objects that will be stored in memory. So there is a trade-off between storing data in memory but having to garbage collect it later vs. recomputing the data.

Shivaram

On Fri, Jul 10, 2015 at 9:49 PM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
Hi Shivaram,

Thank you for suggestion! If I do .cache and .count, each iteration take much more time, which is spent in GC. Is it normal?

10 Ð¸ÑŽÐ»Ñ 2015 Ð³., Ð² 21:23, Shivaram Venkataraman <shivaram@eecs.berkeley.edu<mailto:shivaram@eecs.berkeley.edu><mailto:shivaram@eecs.berkeley.edu<mailto:shivaram@eecs.berkeley.edu>>> Ð½Ð°Ð¿Ð¸ÑÐ°Ð»(Ð°):

I think you need to do `newRDD.cache()` and `newRDD.count` before you do oldRDD.unpersist(true) -- Otherwise it might be recomputing all the previous iterations each time.

Thanks
Shivaram
On Fri, Jul 10, 2015 at 7:44 PM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
Hi,

I am interested how scalable can be the model parallelism within Spark. Suppose, the model contains N weights of type Double and N is so large that does not fit into the memory of a single node. So, we can store the model in RDD[Double] within several nodes. To train the model, one needs to perform K iterations that update all the weights and check the convergence. Then we also need to exchange some weights between the nodes to synchronize the model or update the global state. Iâ€™ve sketched the code that does iterative updates with RDD (without global update yet). Surprisingly, each iteration takes more time than previous as shown below (time in seconds). Could you suggest what is the reason for that? Iâ€™ve checked GC, it does something within few milliseconds.

Configuration: Spark 1.4, 1 master and 5 worker nodes, 5 executors, Intel Xeon 2.2, 16GB RAM each
Iteration 0 time:1.127990986
Iteration 1 time:1.391120414
Iteration 2 time:1.6429691381000002
Iteration 3 time:1.9344402954
Iteration 4 time:2.2075294246999997
Iteration 5 time:2.6328659593
Iteration 6 time:2.7911690492999996
Iteration 7 time:3.0850374104
Iteration 8 time:3.4031050061
Iteration 9 time:3.8826580919

Code:
val modelSize = 1000000000
val numIterations = 10
val parallelizm = 5
var oldRDD = sc.parallelize(1 to modelSize, parallelizm).map(x => 0.1)
var newRDD = sc.parallelize(1 to 1, parallelizm).map(x => 0.1)
var i = 0
while (i < numIterations) {
  val t = System.nanoTime()
  // updating the weights
  val newRDD = oldRDD.map(x => x * x)
  oldRDD.unpersist(true)
  // â€œcheckingâ€ convergence
  newRDD.mean
  println(""Iteration "" + i + "" time:"" + (System.nanoTime() - t) / 1e9 / numIterations)
  oldRDD = newRDD
  i += 1
}


Best regards, Alexander

"
Reynold Xin <rxin@databricks.com>,"Mon, 13 Jul 2015 11:18:22 -0700",Re: How to Read Excel file in Spark 1.4,Sandy Ryza <sandy.ryza@cloudera.com>,"What Sandy meant was there was no out-of-the-box support in Spark for
reading excel files. However, you can still read excel:

If you are using Python, you can use Pandas to load an excel file and then
convert it into a Spark DataFrame.

If you are using the JVM, you can find any excel library for Java/Scala to
read excel files either in the driver, or read them in parallel on workers
if you have lots of excel files.


Note that this question does not really belong in the dev list. It should
be sent to the user list or asked on stackoverflow.



"
Feynman Liang <fliang@databricks.com>,"Mon, 13 Jul 2015 12:12:35 -0700",Re: RandomForest evaluator for grid search,Olivier Girardot <o.girardot@lateral-thoughts.com>,"There is MulticlassMetrics in MLlib; unfortunately a pipelined version
hasn't yet been made for spark-ml. SPARK-7690
<https://issues.apache.org/jira/browse/SPARK-7690> is tracking work on this
if you are interested in following the development.


"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Mon, 13 Jul 2015 20:16:16 +0000",Re: RandomForest evaluator for grid search,"Feynman Liang <fliang@databricks.com>, 
	Olivier Girardot <o.girardot@lateral-thoughts.com>","thx for the info.

I'd be interested in getting the full predict_proba like in scikit learn (
http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.predict_proba)
for the random forest model.
There doesn't seem to be a way to get the details, is there any reason for
that ?

Regards,

Olivier.

Le lun. 13 juil. 2015 Ã  21:12, Feynman Liang <fliang@databricks.com> a
Ã©crit :

l.
"
Feynman Liang <fliang@databricks.com>,"Mon, 13 Jul 2015 13:39:23 -0700",Re: RandomForest evaluator for grid search,Olivier Girardot <o.girardot@lateral-thoughts.com>,"That is currently tracked by SPARK-3727
<https://issues.apache.org/jira/browse/SPARK-3727>.


(
orestClassifier.html#sklearn.ensemble.RandomForestClassifier.predict_proba)
r
 a
el.
"
Feynman Liang <fliang@databricks.com>,"Mon, 13 Jul 2015 14:08:41 -0700",Re: RandomForest evaluator for grid search,Olivier Girardot <o.girardot@lateral-thoughts.com>,"Joseph may be already working on it; I would continue the discussion on
JIRA and first ask if anyone is working on it before starting your own PR.


n
mForestClassifier.html#sklearn.ensemble.RandomForestClassifier.predict_proba)
m> a
sed or
odel.
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 13 Jul 2015 21:46:55 +0000",Re: Should spark-ec2 get its own repo?,"shivaram@eecs.berkeley.edu, Sean Owen <sowen@cloudera.com>","reference implementation for launching EC2 clusters with Apache Spark

project that does something similar: reference implementation.

Nick
2015ë…„ 7ì›” 13ì¼ (ì›”) ì˜¤í›„ 1:27, Shivaram Venkataraman <shivaram@eecs.berkeley.edu>ë‹˜ì´
ìž‘ì„±:

re
a
a
:
o
lt
nd
r
I
r
"
Animesh Tripathy <a.tripathy101@gmail.com>,"Mon, 13 Jul 2015 18:58:17 -0400",Joining Apache Spark,dev@spark.apache.org,"I would like to join the Apache Spark Development Team in order to
contribute code for further improvement of Apache Spark. I was referred
here from EECS professor Anthony after the completion of Big Data with
Apache Spark.

Sincerely,
Animesh Tripathy
"
Marcelo Vanzin <vanzin@cloudera.com>,"Mon, 13 Jul 2015 16:08:48 -0700",Re: Joining Apache Spark,Animesh Tripathy <a.tripathy101@gmail.com>,"Hello, welcome, and please start by going through the web site (
http://spark.apache.org/), especially the ""Contributors"" section at the
bottom.





-- 
Marcelo
"
Josh Rosen <rosenville@gmail.com>,"Mon, 13 Jul 2015 16:10:04 -0700",Re: Joining Apache Spark,Marcelo Vanzin <vanzin@cloudera.com>,"Also, check out
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark


"
Animesh Tripathy <a.tripathy101@gmail.com>,"Mon, 13 Jul 2015 19:33:49 -0400",Re: Joining Apache Spark,Josh Rosen <rosenville@gmail.com>,"Awesome thank you for the swift reply! I have been thinking of this for the
longest time while using Apache Spark. Do you think the future holds any
hope for Swift, C#, or Ruby users to be able to integrate Apache Spark? I
am really excited of the newest 1.4 release finally Apache Spark supports
Python 3!!!

Thanks,
Animesh


"
Patrick Wendell <pwendell@gmail.com>,"Mon, 13 Jul 2015 16:35:01 -0700","Re: jenkins downtime 7/13/15, 7am PDT",amp-infra@googlegroups.com,"Thanks shane for the updates and helping get this up and running.

- Patrick


"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Tue, 14 Jul 2015 01:01:17 +0000",BlockMatrix multiplication,"""dev@spark.apache.org"" <dev@spark.apache.org>","Dear Spark developers,

I am trying to perform BlockMatrix multiplication in Spark. My test is as follows: 1)create a matrix of N blocks, so that each row of block matrix contains only 1 block and each block resides in separate partition on separate node, 2)transpose the block matrix and 3)multiply the transposed matrix by the original non-transposed one. This should preserve the data locality, so there should be no need for shuffle. However, I observe huge shuffle with the block matrix size of 50000x10000 and one block 10000x10000, 5 blocks per matrix. Could you suggest what is wrong?

My setup is Spark 1.4, one master and 5 worker nodes, each is Xeon 2.2 16 GB RAM.
Below is the test code:

import org.apache.spark.mllib.linalg.Matrices
import org.apache.spark.mllib.linalg.distributed.BlockMatrix
val parallelism = 5
val blockSize = 10000
val rows = parallelism * blockSize
val columns = blockSize
val size = rows * columns
assert(rows % blockSize == 0)
assert(columns % blockSize == 0)
val rowBlocks = rows / blockSize
val columnBlocks = columns / blockSize
val rdd = sc.parallelize( {
                for(i <- 0 until rowBlocks; j <- 0 until columnBlocks) yield (i, j)
                }, parallelism).map( coord => (coord, Matrices.rand(blockSize, blockSize, util.Random.self)))
val bm = new BlockMatrix(rdd, blockSize, blockSize).cache()
bm.validate()
val mb = bm.transpose.cache()
mb.validate()
val t = System.nanoTime()
val ata = mb.multiply(bm)
ata.validate()
println(rows + ""x"" + columns + "", block:"" + blockSize + ""\t"" + (System.nanoTime() - t) / 1e9)


Best regards, Alexander
"
"""Huang, Jie"" <jie.huang@intel.com>","Tue, 14 Jul 2015 00:45:34 +0000",[SparkScore] Performance portal for Apache Spark - WW28,"user <user@spark.apache.org>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","Performance Portal for Apache Spark
Description
________________________________
Each data point represents each workload runtime percent compared with the previous week. Different lines represents different workloads running on spark yarn-client mode.
Hardware
________________________________
CPU type: Intel(r) Xeon(r) CPU E5-2697 v2 @ 2.70GHz
Memory: 128GB
NIC: 10GbE
Disk(s): 8 x 1TB SATA HDD
Software
________________________________
JAVA version: 1.8.0_25
Hadoop version: 2.5.0-CDH5.3.2
HiBench version: 4.0
Spark on yarn-client mode
Cluster
________________________________
1 node for Master
10 nodes for Slave
Regular
Summary
The lower percent the better performance.
________________________________
Group

ww22

ww23

ww24

ww25

ww26

ww27

ww28

ww29

HiBench

6.0%

7.9%

-6.5%

-3.1%

-2.1%

-6.4%

-2.7%

-0.7%

spark-perf

-1.8%

4.1%

-4.7%

-4.6%

-5.4%

-4.6%

-12.8%

-12.5%

[http://01org.github.io/sparkscore/image/plaf1.time/overall.png]
Y-Axis: normalized completion time; X-Axis: Work Week.
The commit number can be found in the result table.
The performance score for each workload is normalized based on the elapsed time for 1.2 release. The lower the better.
Detail
________________________________
HiBench
________________________________
JOB

ww22

ww23

ww24

ww25

ww26

ww27

ww28

ww29

commit

530efe3e

90c60692

db81b9d8

4eb48ed1

32e3cdaa

ec784381

2b820f2a

c472eb17

sleep

-2.1%

-2.9%

-4.1%

12.8%

-5.1%

-4.5%

-3.1%

-0.7%

wordcount

8.0%

8.3%

-18.6%

-10.9%

6.9%

-12.9%

-10.0%

-9.2%

kmeans

72.1%

92.9%

86.9%

95.8%

123.3%

99.3%

127.9%

102.6%

scan

%

-1.1%

-25.5%

-21.0%

-12.4%

-19.8%

-19.7%

-20.5%

bayes

-18.3%

-11.1%

-29.7%

-31.3%

-30.9%

-31.1%

-31.0%

-30.1%

aggregation

%

9.2%

-15.3%

-15.0%

-37.6%

-37.0%

-37.3%

7.6%

join

%

1.0%

-12.7%

-13.9%

-16.4%

-17.8%

-14.8%

-13.2%

sort

-11.9%

-12.5%

-17.5%

-17.3%

-20.7%

-17.7%

-13.9%

-15.6%

pagerank

4.0%

2.9%

-11.4%

-13.0%

-11.4%

-10.1%

-12.0%

-11.7%

terasort

-9.5%

-7.3%

-16.7%

-17.0%

-16.3%

-11.9%

-13.1%

-15.7%

Comments: null means no such workload running or workload failed in this time.
[http://01org.github.io/sparkscore/image/plaf1.time/HiBench_workloads.png]
Y-Axis: normalized completion time; X-Axis: Work Week.
The commit number can be found in the result table.
The performance score for each workload is normalized based on the elapsed time for 1.2 release. The lower the better.
spark-perf
________________________________
JOB

ww22

ww23

ww24

ww25

ww26

ww27

ww28

ww29

commit

530efe3e

90c60692

db81b9d8

4eb48ed1

32e3cdaa

ec784381

2b820f2a

c472eb17

agg

%

18.3%

5.2%

2.5%

1.1%

3.0%

-18.8%

-19.0%

agg-int

%

9.6%

4.0%

8.2%

7.0%

7.5%

6.2%

11.4%

agg-naive

%

-0.8%

-6.7%

-6.8%

-8.5%

-6.9%

-15.5%

-18.0%

scheduling

-14.5%

-2.1%

-6.4%

-6.5%

-5.7%

-1.8%

-6.0%

-9.7%

count-filter

6.6%

6.8%

-10.2%

-10.4%

-9.8%

-10.4%

-18.0%

-17.4%

count

6.7%

8.0%

-7.3%

-7.0%

-8.0%

-7.4%

-15.1%

-14.3%

sort

-6.2%

-7.0%

-14.6%

-14.4%

-13.9%

-15.9%

-24.0%

-23.2%

sort-int

-1.6%

-0.1%

-1.5%

-2.2%

-5.3%

-5.0%

-11.3%

-9.6%

Comments: null means no such workload running or workload failed in this time.
[http://01org.github.io/sparkscore/image/plaf1.time/spark-perf_workloads.png]
Y-Axis: normalized completion time; X-Axis: Work Week.
The commit number can be found in the result table.
The performance score for each workload is normalized based on the elapsed time for 1.2 release. The lower the better.


"
"""Vasili I. Galchin"" <vigalchin@gmail.com>","Mon, 13 Jul 2015 20:51:12 -0700","Spark Core and ways of ""talking"" to it for enhancing application
 language support",dev@spark.apache.org,"Hello,

     So far I think there are at two ways (maybe more) to interact
from various programming languages with the Spark Core: PySpark API
and R API. From reading code it seems that PySpark approach and R
approach are very disparate ... with the latter using the R-Java
bridge. Vis-a-vis/regarding I am trying to decide Haskell which way to
go. I realize that like any open software effort that approaches
varied based on history. Is there an intent to adopt one approach as
standard?(Not trying to start a war :-) :-(.

Vasili

BTW I guess Java and Scala APIs are simple given the nature of both
languages vis-a-vis the JVM??

---------------------------------------------------------------------


"
=?UTF-8?B?54mb5YWG5o23?= <nzjemail@gmail.com>,"Tue, 14 Jul 2015 13:35:39 +0800",RDD checkpoint,"""dev@spark.apache.org"" <dev@spark.apache.org>, ""user@spark.apache.org"" <user@spark.apache.org>","The checkpointed RDD  computed twice, why not do the checkpoint for the RDD
once it is computed?

Is there any special reason for this?

-- 
*Regards,*
*Zhaojie*
"
Akhil Das <akhil@sigmoidanalytics.com>,"Tue, 14 Jul 2015 13:21:41 +0530",Re: Contributiona nd choice of langauge,srinivasraghavansr71 <sreenivas.raghavan7@gmail.com>,"This will get you started
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark

Thanks
Best Regards


"
Gil Vernik <GILV@il.ibm.com>,"Tue, 14 Jul 2015 11:59:22 +0300",Re: question related partitions of the DataFrame,Gil Vernik <GILV@il.ibm.com>,"I see that most recent code doesn't has RDDApi anymore.
But i still would like to understand the logic of partitions of DataFrame. 
Does DataFrame has it's own partitions and is sort of RDD by itself, or it 
depends on the partitions of the underline RDD that was used to load the 
data?

For example, if i create DataFrame from HadoopRDD - does it means that 
DataFrame has the same partitions as HadoopRDD?

Thanks
Gil.




From:   Gil Vernik/Haifa/IBM@IBMIL
To:     Dev <dev@spark.apache.org>
Date:   12/07/2015 13:06
Subject:        question related partitions of the DataFrame



Hi, 

DataFrame extends RDDApi, that provides RDD like methods. 
My question is, does DataFrame is sort of  stand alone RDD with it?s own 
partitions or it depends on the underlying RDD that was used to load the 
data into its partitions? It's written that DataFrame has ability to scale 
from kilobytes of data on a single laptop to petabytes on a large cluster, 
but i don't understand if the partitions of data frame are independent of 
the partitions of the data source that was used to load the data. 

So assume theoretically that i used external DataSource API and wrote code 
 that load 1GB of data into single partition. Then I map this DataSource 
to DataFrame and perform some SQL that returns all the records. Will this 
DataFrame also has one partition in memory or Spark somehow will divide 
this DataFrame into various partitions? If so, how it will be divide it 
into partitions? By size? (can someone point me to the code to see some 
example)? 


Thanks, 
Gil.

"
Gil Vernik <GILV@il.ibm.com>,"Tue, 14 Jul 2015 12:23:15 +0300",problems with build of latest the master,Dev <dev@spark.apache.org>,"I just did checkout of the master and tried to build it with 

mvn -Dhadoop.version=2.6.0 -DskipTests clean package

Got:

[ERROR] 
/Users/gilv/Dev/Spark/spark/core/src/test/java/org/apache/spark/shuffle/unsafe/UnsafeShuffleWriterSuite.java:117: 
error: cannot find symbol
[ERROR] 
when(shuffleMemoryManager.tryToAcquire(anyLong())).then(returnsFirstArg());
[ERROR]                                                       ^
[ERROR]   symbol:   method then(Answer<Object>)
[ERROR] 
/Users/gilv/Dev/Spark/spark/core/src/test/java/org/apache/spark/shuffle/unsafe/UnsafeShuffleWriterSuite.java:408: 
error: cannot find symbol
[ERROR]       .then(returnsFirstArg()) // Allocate initial sort buffer
[ERROR]       ^
[ERROR]   symbol:   method then(Answer<Object>)
[ERROR] 
/Users/gilv/Dev/Spark/spark/core/src/test/java/org/apache/spark/shuffle/unsafe/UnsafeShuffleWriterSuite.java:435: 
error: cannot find symbol
[ERROR]       .then(returnsFirstArg()) // Allocate initial sort buffer
[ERROR]       ^
[ERROR]   symbol:   method then(Answer<Object>)
[ERROR] 
/Users/gilv/Dev/Spark/spark/core/src/test/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorterSuite.java:98: 
error: cannot find symbol
[ERROR] 
when(shuffleMemoryManager.tryToAcquire(anyLong())).then(returnsFirstArg());
[ERROR]                                                       ^
[ERROR]   symbol:   method then(Answer<Object>)
[ERROR] 
/Users/gilv/Dev/Spark/spark/core/src/test/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorterSuite.java:130: 
error: cannot find symbol
[ERROR]       .then(returnsSecondArg());
[ERROR]       ^
[ERROR]   symbol:   method then(Answer<Object>)
[ERROR] Note: 
/Users/gilv/Dev/Spark/spark/core/src/test/java/org/apache/spark/JavaAPISuite.java 
uses or overrides a deprecated API.
[ERROR] Note: Recompile with -Xlint:deprecation for details.
[ERROR] 5 errors
[INFO] 
------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Spark Project Parent POM ........................... SUCCESS [ 
3.183 s]
[INFO] Spark Project Launcher ............................. SUCCESS [ 
7.681 s]
[INFO] Spark Project Networking ........................... SUCCESS [ 
7.178 s]
[INFO] Spark Project Shuffle Streaming Service ............ SUCCESS [ 
4.125 s]
[INFO] Spark Project Unsafe ............................... SUCCESS [ 
3.734 s]
[INFO] Spark Project Core ................................. FAILURE [02:16 
min]
[INFO] Spark Project Bagel ................................ SKIPPED"
srinivasraghavansr71 <sreenivas.raghavan7@gmail.com>,"Tue, 14 Jul 2015 03:40:05 -0700 (MST)",Re: Contributiona nd choice of langauge,dev@spark.apache.org,"I saw the contribution sections. As a new contibutor, should I try to build
patches or can I add some new algorithm to MLlib. I am comfortable with
python and R. Are they enough to contribute for spark?  



--

---------------------------------------------------------------------


"
Akhil Das <akhil@sigmoidanalytics.com>,"Tue, 14 Jul 2015 16:21:58 +0530",Re: Contributiona nd choice of langauge,srinivasraghavansr71 <sreenivas.raghavan7@gmail.com>,"You can try to resolve some Jira issues, to start with try out some newbie
JIRA's.

Thanks
Best Regards


"
Gil Vernik <GILV@il.ibm.com>,"Tue, 14 Jul 2015 14:22:01 +0300",Re: problems with build of latest the master,Dev <dev@spark.apache.org>,"I figured it out.
I tried to build Spark configured to access OpenStack Swift and 
hadoop-openstack.jar has the same issue as were described here 
https://github.com/apache/spark/pull/7090/commits

So for those who wants to build Spark 1.5.- master with OpenStack Swift 
support, just remove mockito-all dependency from the core/pom.xml:
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-openstack</artifactId>
     <exclusions>
        <exclusion>
          <groupId>javax.servlet</groupId>
          <artifactId>servlet-api</artifactId>
        </exclusion>
        <exclusion>
          <groupId>org.codehaus.jackson</groupId>
          <artifactId>jackson-mapper-asl</artifactId>
        </exclusion>
         <exclusion>
            <groupId>org.mockito</groupId>
            <artifactId>mockito-all</artifactId>
          </exclusion>
      </exclusions>
    </dependency>
 
I guess this is needed for Hadoop version 2.6.0, but perhaps latest Hadoop 
versions has the same mockito versions as Spark uses.

Gil Vernik. 



From:   Gil Vernik/Haifa/IBM@IBMIL
To:     Dev <dev@spark.apache.org>
Date:   14/07/2015 12:23
Subject:        problems with build of latest the master



I just did checkout of the master and tried to build it with 

mvn -Dhadoop.version=2.6.0 -DskipTests clean package 

Got: 

[ERROR] 
/Users/gilv/Dev/Spark/spark/core/src/test/java/org/apache/spark/shuffle/unsafe/UnsafeShuffleWriterSuite.java:117: 
error: cannot find symbol 
[ERROR] 
when(shuffleMemoryManager.tryToAcquire(anyLong())).then(returnsFirstArg()); 

[ERROR]                                                       ^ 
[ERROR]   symbol:   method then(Answer<Object>) 
[ERROR] 
/Users/gilv/Dev/Spark/spark/core/src/test/java/org/apache/spark/shuffle/unsafe/UnsafeShuffleWriterSuite.java:408: 
error: cannot find symbol 
[ERROR]       .then(returnsFirstArg()) // Allocate initial sort buffer 
[ERROR]       ^ 
[ERROR]   symbol:   method then(Answer<Object>) 
[ERROR] 
/Users/gilv/Dev/Spark/spark/core/src/test/java/org/apache/spark/shuffle/unsafe/UnsafeShuffleWriterSuite.java:435: 
error: cannot find symbol 
[ERROR]       .then(returnsFirstArg()) // Allocate initial sort buffer 
[ERROR]       ^ 
[ERROR]   symbol:   method then(Answer<Object>) 
[ERROR] 
/Users/gilv/Dev/Spark/spark/core/src/test/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorterSuite.java:98: 
error: cannot find symbol 
[ERROR] 
when(shuffleMemoryManager.tryToAcquire(anyLong())).then(returnsFirstArg()); 

[ERROR]                                                       ^ 
[ERROR]   symbol:   method then(Answer<Object>) 
[ERROR] 
/Users/gilv/Dev/Spark/spark/core/src/test/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorterSuite.java:130: 
error: cannot find symbol 
[ERROR]       .then(returnsSecondArg()); 
[ERROR]       ^ 
[ERROR]   symbol:   method then(Answer<Object>) 
[ERROR] Note: 
/Users/gilv/Dev/Spark/spark/core/src/test/java/org/apache/spark/JavaAPISuite.java 
uses or overrides a deprecated API. 
[ERROR] Note: Recompile with -Xlint:deprecation for details. 
[ERROR] 5 errors 
[INFO] 
------------------------------------------------------------------------ 
[INFO] Reactor Summary: 
[INFO] 
[INFO] Spark Project Parent POM ........................... SUCCESS [ 
3.183 s] 
[INFO] Spark Project Launcher ............................. SUCCESS [ 
7.681 s] 
[INFO] Spark Project Networking ........................... SUCCESS [ 
7.178 s] 
[INFO] Spark Project Shuffle Streaming Service ............ SUCCESS [ 
4.125 s] 
[INFO] Spark Project Unsafe ............................... SUCCESS [ 
3.734 s] 
[INFO] Spark Project Core ................................. FAILURE [02:16 
min] 
[INFO] Spark Project Bagel ................................ SKIPPED

"
Ted Yu <yuzhihong@gmail.com>,"Tue, 14 Jul 2015 04:22:23 -0700",Re: problems with build of latest the master,Gil Vernik <GILV@il.ibm.com>,"Looking at Jenkins, master branch compiles.

Can you try the following command ?

mvn -Phive -Phadoop-2.6 -DskipTests clean package

What version of Java are you using ?

Cheers


"
Rakesh Chalasani <vnit.rakesh@gmail.com>,"Tue, 14 Jul 2015 13:46:59 +0000",Re: Contributiona nd choice of langauge,"Akhil Das <akhil@sigmoidanalytics.com>, 
	srinivasraghavansr71 <sreenivas.raghavan7@gmail.com>","Here is a more specific MLlib related Umbrella for 1.5 that can help you
get started
https://issues.apache.org/jira/browse/SPARK-8445?jql=text%20~%20%22mllib%201.5%22

Rakesh


"
Rakesh Chalasani <vnit.rakesh@gmail.com>,"Tue, 14 Jul 2015 13:58:55 +0000",Re: BlockMatrix multiplication,"""Ulanov, Alexander"" <alexander.ulanov@hp.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Block matrix stores the data as key->Matrix pairs and multiply does a
reduceByKey operations, aggregating matrices per key. Since you said each
block is residing in a separate partition, reduceByKey might be effectively
shuffling all of the data. A better way to go about this is to allow
multiple blocks within each partition so that reduceByKey does a local
reduce before aggregating across nodes.

Rakesh


"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Tue, 14 Jul 2015 14:58:15 +0000",Re: BlockMatrix multiplication,Rakesh Chalasani <vnit.rakesh@gmail.com>,"Hi Rakesh,

Thanks for suggestion. Each block of original matrix is in separate partition. Each block of transposed matrix is also in a separate partition. The partition numbers are the same for the blocks that undergo multiplication. Each partition is on a separate worker. Basically, I want to force each worker to multiply only 2 blocks. This should be the optimal configuration for multiplication, as far as I understand. Having several blocks in each partition as you suggested is not optimal, is it?

Best regards, Alexander



Block matrix stores the data as key->Matrix pairs and multiply does a reduceByKey operations, aggregating matrices per key. Since you said each block is residing in a separate partition, reduceByKey might be effectively shuffling all of the data. A better way to go about this is to allow multiple blocks within each partition so that reduceByKey does a local reduce before aggregating across nodes.

Rakesh

Dear Spark developers,

I am trying to perform BlockMatrix multiplication in Spark. My test is as follows: 1)create a matrix of N blocks, so that each row of block matrix contains only 1 block and each block resides in separate partition on separate node, 2)transpose the block matrix and 3)multiply the transposed matrix by the original non-transposed one. This should preserve the data locality, so there should be no need for shuffle. However, I observe huge shuffle with the block matrix size of 50000x10000 and one block 10000x10000, 5 blocks per matrix. Could you suggest what is wrong?

My setup is Spark 1.4, one master and 5 worker nodes, each is Xeon 2.2 16 GB RAM.
Below is the test code:

import org.apache.spark.mllib.linalg.Matrices
import org.apache.spark.mllib.linalg.distributed.BlockMatrix
val parallelism = 5
val blockSize = 10000
val rows = parallelism * blockSize
val columns = blockSize
val size = rows * columns
assert(rows % blockSize == 0)
assert(columns % blockSize == 0)
val rowBlocks = rows / blockSize
val columnBlocks = columns / blockSize
val rdd = sc.parallelize( {
                for(i <- 0 until rowBlocks; j <- 0 until columnBlocks) yield (i, j)
                }, parallelism).map( coord => (coord, Matrices.rand(blockSize, blockSize, util.Random.self)))
val bm = new BlockMatrix(rdd, blockSize, blockSize).cache()
bm.validate()
val mb = bm.transpose.cache()
mb.validate()
val t = System.nanoTime()
val ata = mb.multiply(bm)
ata.validate()
println(rows + ""x"" + columns + "", block:"" + blockSize + ""\t"" + (System.nanoTime() - t) / 1e9)


Best regards, Alexander

---------------------------------------------------------------------


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Tue, 14 Jul 2015 08:17:41 -0700","Re: Spark Core and ways of ""talking"" to it for enhancing application
 language support","""Vasili I. Galchin"" <vigalchin@gmail.com>","Both SparkR and the PySpark API call into the JVM Spark API (i.e.
JavaSparkContext, JavaRDD etc.). They use different methods (Py4J vs. the
R-Java bridge) to call into the JVM based on libraries available / features
supported in each language. So for Haskell, one would need to see what is
the best way to call the underlying Java API functions from Haskell and get
results back.

Thanks
Shivaram


"
Rakesh Chalasani <vnit.rakesh@gmail.com>,"Tue, 14 Jul 2015 16:04:42 +0000",Re: BlockMatrix multiplication,"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Hi Alexander:

Aw, I missed the 'cogroup' on BlockMatrix multiply! I stand corrected. Check
https://github.com/apache/spark/blob/3c0156899dc1ec1f7dfe6d7c8af47fa6dc7d00bf/mllib/src/main/scala/org/apache/spark/mllib/linalg/distributed/BlockMatrix.scala#L361

BlockMatrix multiply uses a custom partitioner called GridPartitioner, that
might be causing the shuffle; which, in your special case need not happen.
But, from what I understood from your code, I don't think this is an issue
since your special case can be handled using computeGramMatrix on
RowMatrix. Is there a reason you did not use that?

Rakesh



"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Tue, 14 Jul 2015 16:37:32 +0000",RE: BlockMatrix multiplication,Rakesh Chalasani <vnit.rakesh@gmail.com>,"Hi Rakesh,

I am not interested in a particular case of A^T*A. This case is a handy setup so I donâ€™t need to create another matrix and force the blocks to co-locate. Basically, I am trying to understand the effectiveness of BlockMatrix for multiplication of distributed matrices. It seems that I am missing something or using it wrong.

Best regards, Alexander

From: Rakesh Chalasani [mailto:vnit.rakesh@gmail.com]
Sent: Tuesday, July 14, 2015 9:05 AM
To: Ulanov, Alexander
Cc: dev@spark.apache.org
Subject: Re: BlockMatrix multiplication

Hi Alexander:

Aw, I missed the 'cogroup' on BlockMatrix multiply! I stand corrected. Check
https://github.com/apache/spark/blob/3c0156899dc1ec1f7dfe6d7c8af47fa6dc7d00bf/mllib/src/main/scala/org/apache/spark/mllib/linalg/distributed/BlockMatrix.scala#L361

BlockMatrix multiply uses a custom partitioner called GridPartitioner, that might be causing the shuffle; which, in your special case need not happen. But, from what I understood from your code, I don't think this is an issue since your special case can be handled using computeGramMatrix on RowMatrix. Is there a reason you did not use that?

Rakesh


On Tue, Jul 14, 2015 at 11:03 AM Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
Hi Rakesh,

Thanks for suggestion. Each block of original matrix is in separate partition. Each block of transposed matrix is also in a separate partition. The partition numbers are the same for the blocks that undergo multiplication. Each partition is on a separate worker. Basically, I want to force each worker to multiply only 2 blocks. This should be the optimal configuration for multiplication, as far as I understand. Having several blocks in each partition as you suggested is not optimal, is it?

Best regards, Alexander


Block matrix stores the data as key->Matrix pairs and multiply does a reduceByKey operations, aggregating matrices per key. Since you said each block is residing in a separate partition, reduceByKey might be effectively shuffling all of the data. A better way to go about this is to allow multiple blocks within each partition so that reduceByKey does a local reduce before aggregating across nodes.

Rakesh

On Mon, Jul 13, 2015 at 9:24 PM Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
Dear Spark developers,

I am trying to perform BlockMatrix multiplication in Spark. My test is as follows: 1)create a matrix of N blocks, so that each row of block matrix contains only 1 block and each block resides in separate partition on separate node, 2)transpose the block matrix and 3)multiply the transposed matrix by the original non-transposed one. This should preserve the data locality, so there should be no need for shuffle. However, I observe huge shuffle with the block matrix size of 50000x10000 and one block 10000x10000, 5 blocks per matrix. Could you suggest what is wrong?

My setup is Spark 1.4, one master and 5 worker nodes, each is Xeon 2.2 16 GB RAM.
Below is the test code:

import org.apache.spark.mllib.linalg.Matrices
import org.apache.spark.mllib.linalg.distributed.BlockMatrix
val parallelism = 5
val blockSize = 10000
val rows = parallelism * blockSize
val columns = blockSize
val size = rows * columns
assert(rows % blockSize == 0)
assert(columns % blockSize == 0)
val rowBlocks = rows / blockSize
val columnBlocks = columns / blockSize
val rdd = sc.parallelize( {
                for(i <- 0 until rowBlocks; j <- 0 until columnBlocks) yield (i, j)
                }, parallelism).map( coord => (coord, Matrices.rand(blockSize, blockSize, util.Random.self)))
val bm = new BlockMatrix(rdd, blockSize, blockSize).cache()
bm.validate()
val mb = bm.transpose.cache()
mb.validate()
val t = System.nanoTime()
val ata = mb.multiply(bm)
ata.validate()
println(rows + ""x"" + columns + "", block:"" + blockSize + ""\t"" + (System.nanoTime() - t) / 1e9)


Best regards, Alexander
"
Sean Busbey <busbey@cloudera.com>,"Tue, 14 Jul 2015 12:09:42 -0500",Re: Foundation policy on releases and Spark nightly builds,dev <dev@spark.apache.org>,"Responses inline, with some liberties on ordering.






It's foundation level policy, so I'd presume the board needs to. Since it's
part of our legal position, it might be owned by the legal affairs
committee[1]. That would mean they could update it without a board
resolution. (legal-discuss@ could tell you for sure).



The Legal Affairs Committee is reachable either through their mailing
list[2] or their issue tracker[3].

Please be sure to read the entire original document, it explains the
rationale that has gone into it. You'll need to address the matters raised
there.





It can't be decided on the general@incubator list, but there are already
several relevant parties discussing the matter there. You certainly don't
*need* to join that conversation, but the participants there have overlap
with the folks who can ultimately decide the issue. Thus, it may help avoid
having to repeat things.



I am neither a participant on legal affairs nor the board, so I have no
idea.




Please keep in mind that you are also ""ASF people,"" as is the entire Spark
community (users and all)[4]. Phrasing things in terms of ""us and them"" by
drawing a distinction on ""[they] get in a fight on our mailing list"" is not
helpful.



Understanding and abiding by ASF legal obligations and policies is the job
of each project PMC as a part of their formation by the board[5]. If anyone
in your community has questions about what the project can or can not do
then it's the job of the PMC find out proactively (rather than take a ""ask
for forgiveness"" approach). Where the existing documentation is unclear or
where you think it might be out of date, you can often get guidance from
general@incubator (since it contains a large number of members and folks
from across foundation projects) or comdev[6] (since their charter includes
explaining ASF policy). If those resources prove insufficient matters can
be brought up with either legal-discuss@ or board@.

If you find out of date documentation that is not ASF policy, you can have
it removed by notifying the appropriate group (i.e. legal-discuss, comdev,
or whomever is hosting it).


[1]: http://apache.org/legal/
[2]: http://www.apache.org/foundation/mailinglists.html#foundation-legal
[3]: https://issues.apache.org/jira/browse/LEGAL/
[4]: http://www.apache.org/foundation/how-it-works.html#roles
[5]: http://apache.org/foundation/how-it-works.html#pmc
[6]: https://community.apache.org/

-- 
Sean
"
Burak Yavuz <brkyvz@gmail.com>,"Tue, 14 Jul 2015 10:14:04 -0700",Re: BlockMatrix multiplication,"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Hi Alexander,

and 5 rows. When you perform an A^T^A multiplication, you will generate a
separate GridPartitioner with 5 columns and 5 rows. Therefore you are
observing a huge shuffle. If you would generate a diagonal-block matrix as
an example (5x5), you should not observe any shuffle.

Basically, your example causes the worst kind of shuffle. We can implement
RowBasedPartitioning, and ColumnBasedPartitioning for optimization, but we
didn't initially see it necessary to expose the partitioners to users, and
didn't add them (you can find the old implementations here
<https://github.com/brkyvz/spark/commit/9ae85aa1ebabdc099d7f655bc1d9021d34d2910f>
).

Hope that helps!

Best,
Burak


ks to
m
00bf/mllib/src/main/scala/org/apache/spark/mllib/linalg/distributed/BlockMatrix.scala#L361
n
n.
l
ly
m>
"
Mark Hamstra <mark@clearstorydata.com>,"Tue, 14 Jul 2015 10:28:25 -0700",Re: Foundation policy on releases and Spark nightly builds,Sean Busbey <busbey@cloudera.com>,"
<whine>But they started it!</whine>

A bit more seriously, my perspective is that the Spark community and
development process works very well and quite smoothly.  The only
significant strains that I have witnessed during the time in which Spark
has been Apache Spark have been when ""ASF people"" who otherwise have
neither contributed to Spark development nor participated in the Spark
community parachute in to tell us that we are doing things wrong and that
we must change our practice in order to conform to their expectations of
""The Apache Way"".  Sometimes those admonitions are based on actual Apache
bylaws and/or legal requirements, and we need to take them seriously.
Other times they have seemed more subjective and have felt more like
meddling or stirring up trouble in the community and with a process that is
actually working very well.


"
Feynman Liang <fliang@databricks.com>,"Tue, 14 Jul 2015 10:31:44 -0700",Re: Contributiona nd choice of langauge,Rakesh Chalasani <vnit.rakesh@gmail.com>,"I would suggest starting with some starter tasks
<https://issues.apache.org/jira/browse/SPARK-8925?jql=project%20%3D%20SPARK%20AND%20status%20in%20(Open%2C%20%22In%20Progress%22%2C%20Reopened)%20AND%20component%20in%20(ML%2C%20MLlib)%20AND%20labels%20%3D%20starter%20AND%20%22Target%20Version%2Fs%22%20%3D%201.5.0>
to
get familiar with the codebase.


b%201.5%22
-nd-choice-of-langauge-tp13179p13209.html
"
Sean Busbey <busbey@cloudera.com>,"Tue, 14 Jul 2015 13:39:12 -0500",Re: Foundation policy on releases and Spark nightly builds,Mark Hamstra <mark@clearstorydata.com>,"Point well taken. Allow me to walk back a little and move us in a more
productive direction.

I can personally empathize with the desire to have nightly builds. I'm a
passionate advocate for tight feedback cycles between a project and its
downstream users. I am personally involved in several projects that would
benefit from nightly builds and would love to see change in this policy, if
only to get an example of an implementation that's not buried on a project
wiki.

My interest here in Spark is two-fold. First, protecting the foundation via
its established policies. To this end I periodically look for projects that
show up as non-compliant. Second, it seems like the Spark community has
some friction with larger ASF processes and I'd like to smooth that out
where I can help do so. (I guess this is my way of saying that for better
or worse this isn't a drive by ;) )

like-minded PPMCs and it is a known location for other PMCs to easily join
in. Since you are a PMC you'll have enough standing with legal-discuss to
not need someone else on your side of the request, but more PMCs helps to
show the demand.

We could go directly to legal-discuss with just the question of labeling
the nightly section of our download page as ""developer only."" I'm skeptical
that this will be accepted given the tone of the guidance document.

They have two download pages, one for thr general public and one for
project QA and localization volunteers.

http://ooo-site.apache.org/download/devbuilds.html

I didn't suggest this alternative earlier because I haven't verified yet
that it meets the standard of the guidance, and I am reasonably certain
that the dev wiki page does.

How about we reach out to the OO PMC and see if they've proactively
checked? If not we can go as a group to legal-discuss.

-- 
Sean

"
Eugene Morozov <fathersson@list.ru>,"Wed, 15 Jul 2015 00:29:10 +0300",Re: question related partitions of the DataFrame,Gil Vernik <GILV@il.ibm.com>,"Gil, 

I’d say that DataFrame is a result of transformation of any other RDD. Your input RDD might contains strings and numbers. But as a result of transformation you end up with RDD that contains GenericRowWithSchema, which is what DataFrame actually is. So, I’d say that DataFrame is just sort of wrapper around simple RDD, which provides some additional and pretty useful stuff.

Hope, this helps.
 

DataFrame. Does DataFrame has it's own partitions and is sort of RDD by itself, or it depends on the partitions of the underline RDD that was used to load the data? 
DataFrame has the same partitions as HadoopRDD? 
own partitions or it depends on the underlying RDD that was used to load the data into its partitions? It's written that DataFrame has ability to scale from kilobytes of data on a single laptop to petabytes on a large cluster, but i don't understand if the partitions of data frame are independent of the partitions of the data source that was used to load the data. 
code  that load 1GB of data into single partition. Then I map this DataSource to DataFrame and perform some SQL that returns all the records. Will this  DataFrame also has one partition in memory or Spark somehow will divide this DataFrame into various partitions? If so, how it will be divide it into partitions? By size? (can someone point me to the code to see some example)? 

Eugene Morozov
fathersson@list.ru




"
"""Vasili I. Galchin"" <vigalchin@gmail.com>","Tue, 14 Jul 2015 15:10:34 -0700","Re: Spark Core and ways of ""talking"" to it for enhancing application
 language support","""shivaram@eecs.berkeley.edu"" <shivaram@eecs.berkeley.edu>","thanks.


"
swetha <swethakasireddy@gmail.com>,"Tue, 14 Jul 2015 15:32:55 -0700 (MST)",Regarding sessionization with updateStateByKey,dev@spark.apache.org,"Hi,

I have a question regarding sessionization using updateStateByKey. If near
real time state needs to be maintained in a Streaming application, what
happens when the number of RDDs to maintain the state becomes very large?
Does it automatically get saved to HDFS and reload when needed?

Thanks,
Swetha



--

---------------------------------------------------------------------


"
swetha <swethakasireddy@gmail.com>,"Tue, 14 Jul 2015 16:11:34 -0700 (MST)",Re: Does RDD checkpointing store the entire state in HDFS?,dev@spark.apache.org,"
Hi TD, 

I have a question regarding sessionization using updateStateByKey. If near
real time state needs to be maintained in a Streaming application, what
happens when the number of RDDs to maintain the state becomes very large?
Does it automatically get saved to HDFS and reload when needed or do I have
to use any code like ssc.checkpoint(checkpointDir)?  Also, how is the
performance if I use both DStream Checkpointing for maintaining the state
and use Kafka Direct approach for exactly once semantics?


Thanks, 
Swetha



--

---------------------------------------------------------------------


"
Joel Zambrano <joelz@microsoft.com>,"Tue, 14 Jul 2015 23:21:39 +0000",RestSubmissionClient Basic Auth,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi! We have a gateway with basic auth that relays calls to the head node in our cluster. Is adding support for basic auth the wrong approach? Should we use a relay proxy? I've seen the code and it would probably require adding a few configs and appending the header on the get and post request of the REST submission client.

A best strategy would be greatly appreciated.

Thanks,
Joel
"
Tathagata Das <tdas@databricks.com>,"Tue, 14 Jul 2015 16:23:41 -0700",Re: Does RDD checkpointing store the entire state in HDFS?,swetha <swethakasireddy@gmail.com>,"1. When you set ssc.checkpoint(checkpointDir), the spark streaming
periodically saves the state RDD (which is a snapshot of all the state
data) to HDFS using RDD checkpointing. In fact, a streaming app with
updateStateByKey will not start until you set checkpoint directory.

2. The updateStateByKey performance is sort of independent of the what is
the source that is being use - receiver based or direct Kafka. The
absolutely performance obvious depends on a LOT of variables, size of the
cluster, parallelization, etc. The key things is that you must ensure
sufficient parallelization at every stage - receiving, shuffles
(updateStateByKey included), and output.

Some more discussion in my talk -
https://www.youtube.com/watch?v=d5UJonrruHk



"
Tathagata Das <tdas@databricks.com>,"Tue, 14 Jul 2015 16:24:34 -0700",Re: Does RDD checkpointing store the entire state in HDFS?,swetha <swethakasireddy@gmail.com>,"BTW, this is more like a user-list kind of mail, than a dev-list. The
dev-list is for Spark developers.


"
swetha <swethakasireddy@gmail.com>,"Tue, 14 Jul 2015 16:43:39 -0700 (MST)",Re: Does RDD checkpointing store the entire state in HDFS?,dev@spark.apache.org,"OK. Thanks a lot TD.



--

---------------------------------------------------------------------


"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Wed, 15 Jul 2015 01:23:40 +0000",RE: BlockMatrix multiplication,Burak Yavuz <brkyvz@gmail.com>,"Hi Burak,

Thank you for explanation! I will try to make a diagonal block matrix and report you the results.

Column- or row based partitioner make sense to me, because it is a direct analogy from column or row-based data storage in matrices, which is used in BLAS.

Best regards, Alexander

From: Burak Yavuz [mailto:brkyvz@gmail.com]
Sent: Tuesday, July 14, 2015 10:14 AM
To: Ulanov, Alexander
Cc: Rakesh Chalasani; dev@spark.apache.org
Subject: Re: BlockMatrix multiplication

Hi Alexander,

From your example code, using the GridPartitioner, you will have 1 column, and 5 rows. When you perform an A^T^A multiplication, you will generate a separate GridPartitioner with 5 columns and 5 rows. Therefore you are observing a huge shuffle. If you would generate a diagonal-block matrix as an example (5x5), you should not observe any shuffle.

Basically, your example causes the worst kind of shuffle. We can implement RowBasedPartitioning, and ColumnBasedPartitioning for optimization, but we didn't initially see it necessary to expose the partitioners to users, and didn't add them (you can find the old implementations here<https://github.com/brkyvz/spark/commit/9ae85aa1ebabdc099d7f655bc1d9021d34d2910f>).

Hope that helps!

Best,
Burak

On Tue, Jul 14, 2015 at 9:37 AM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
Hi Rakesh,

I am not interested in a particular case of A^T*A. This case is a handy setup so I donâ€™t need to create another matrix and force the blocks to co-locate. Basically, I am trying to understand the effectiveness of BlockMatrix for multiplication of distributed matrices. It seems that I am missing something or using it wrong.

Best regards, Alexander

From: Rakeshsh@gmail.com>]
Sent: Tuesday, July 14, 2015 9:05 AM
To: Ulanov, Alexander
Cc: dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: BlockMatrix multiplication

Hi Alexander:

Aw, I missed the 'cogroup' on BlockMatrix multiply! I stand corrected. Check
https://github.com/apache/spark/blob/3c0156899dc1ec1f7dfe6d7c8af47fa6dc7d00bf/mllib/src/main/scala/org/apache/spark/mllib/linalg/distributed/BlockMatrix.scala#L361

BlockMatrix multiply uses a custom partitioner called GridPartitioner, that might be causing the shuffle; which, in your special case need not happen. But, from what I understood from your code, I don't think this is an issue since your special case can be handled using computeGramMatrix on RowMatrix. Is there a reason you did not use that?

Rakesh


On Tue, Jul 14, 2015 at 11:03 AM Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
Hi Rakesh,

Thanks for suggestion. Each block of original matrix is in separate partition. Each block of transposed matrix is also in a separate partition. The partition numbers are the same for the blocks that undergo multiplication. Each partition is on a separate worker. Basically, I want to force each worker to multiply only 2 blocks. This should be the optimal configuration for multiplication, as far as I understand. Having several blocks in each partition as you suggested is not optimal, is it?

Best regards, Alexander

Block matrix stores the data as key->Matrix pairs and multiply does a reduceByKey operations, aggregating matrices per key. Since you said each block is residing in a separate partition, reduceByKey might be effectively shuffling all of the data. A better way to go about this is to allow multiple blocks within each partition so that reduceByKey does a local reduce before aggregating across nodes.

Rakesh

On Mon, Jul 13, 2015 at 9:24 PM Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
Dear Spark developers,

I am trying to perform BlockMatrix multiplication in Spark. My test is as follows: 1)create a matrix of N blocks, so that each row of block matrix contains only 1 block and each block resides in separate partition on separate node, 2)transpose the block matrix and 3)multiply the transposed matrix by the original non-transposed one. This should preserve the data locality, so there should be no need for shuffle. However, I observe huge shuffle with the block matrix size of 50000x10000 and one block 10000x10000, 5 blocks per matrix. Could you suggest what is wrong?

My setup is Spark 1.4, one master and 5 worker nodes, each is Xeon 2.2 16 GB RAM.
Below is the test code:

import org.apache.spark.mllib.linalg.Matrices
import org.apache.spark.mllib.linalg.distributed.BlockMatrix
val parallelism = 5
val blockSize = 10000
val rows = parallelism * blockSize
val columns = blockSize
val size = rows * columns
assert(rows % blockSize == 0)
assert(columns % blockSize == 0)
val rowBlocks = rows / blockSize
val columnBlocks = columns / blockSize
val rdd = sc.parallelize( {
                for(i <- 0 until rowBlocks; j <- 0 until columnBlocks) yield (i, j)
                }, parallelism).map( coord => (coord, Matrices.rand(blockSize, blockSize, util.Random.self)))
val bm = new BlockMatrix(rdd, blockSize, blockSize).cache()
bm.validate()
val mb = bm.transpose.cache()
mb.validate()
val t = System.nanoTime()
val ata = mb.multiply(bm)
ata.validate()
println(rows + ""x"" + columns + "", block:"" + blockSize + ""\t"" + (System.nanoTime() - t) / 1e9)


Best regards, Alexander

"
Matt Cheah <mcheah@palantir.com>,"Wed, 15 Jul 2015 02:11:56 +0000",PySpark GroupByKey implementation question,"""dev@spark.apache.org"" <dev@spark.apache.org>, Josh Rosen
	<joshrosen@databricks.com>","Hi everyone,

I was examining the Pyspark implementation of groupByKey in rdd.py. I would
like to submit a patch improving Scala RDD¹s groupByKey that has a similar
robustness against large groups, as Pyspark¹s implementation has logic to
spill part of a single group to disk along the way.

Its implementation appears to do the following:
1. Combine and group-by-key per partition locally, potentially spilling
individual groups to disk
2. Shuffle the data explicitly using partitionBy
3. After the shuffle, do another local groupByKey to get the final result,
again potentially spilling individual groups to disk
My question is: what does the explicit map-side-combine step (#1)
specifically benefit here? I was under the impression that map-side-combine
for groupByKey was not optimal and is turned off in the Scala implementation
­ Scala PairRDDFunctions.groupByKey calls to combineByKey with
map-side-combine set to false. Is it something specific to how Pyspark can
potentially spill the individual groups to disk?

Thanks,

-Matt Cheah

P.S. Relevant Links:

https://issues.apache.org/jira/browse/SPARK-3074
https://github.com/apache/spark/pull/1977



"
Matt Goodman <meawoppl@gmail.com>,"Tue, 14 Jul 2015 21:35:04 -0700",Re: Should spark-ec2 get its own repo?,Nicholas Chammas <nicholas.chammas@gmail.com>,"I concur with the things Sean said about keeping the same JIRA.  Frankly,
its a pretty small part of spark, and as mentioned by Nicholas, a reference
implementation of getting Spark running in ec2.

I can see wanting to grow it to a little more general tool that implements
launchers for other compute platforms.  Porting this over to
Google/M$/rackspace offerings would be not too far out of reach.

--Matthew Goodman

=====================
Check Out My Website: http://craneium.net
Find me on LinkedIn: http://tinyurl.com/d6wlch


 Shivaram Venkataraman <
ere
 a
w
2
k
.
e
er
CI
0
e
s
"
Takeshi Yamamuro <linguin.m.s@gmail.com>,"Wed, 15 Jul 2015 16:47:34 +0900",Expression.resolved unmatched with the correct values in catalyst?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi, devs

I found that the case of 'Expression.resolved !=
(Expression.childrenResolved && checkInputDataTypes().isSuccess)'
occurs in the output of Analyzer.
That is, some tests in o.a.s.sql.* fail if the codes below are added in
CheckAnalysis:

https://github.com/maropu/spark/commit/a488eee8351f5ec49854eef0266e4445269d5867

Is this a correct behaviour in catalyst?
If correct, anyone explains the case if this happens?

Thanks,
takeshi

-- 
---
Takeshi Yamamuro (maropu)
"
Sean Owen <sowen@cloudera.com>,"Wed, 15 Jul 2015 09:08:04 +0100",Re: Should spark-ec2 get its own repo?,Matt Goodman <meawoppl@gmail.com>,"The code can continue to be a good reference implementation, no matter
where it lives. In fact, it can be a better more complete one, and
easier to update.

I agree that ec2/ needs to retain some kind of pointer to the new
location. Yes, maybe a script as well that does the checkout as you
say. We have to be careful that the effect here isn't to make people
think this code is still part of the blessed bits of a Spark release,
since it isn't. But I suppose the point is that it isn't quite now
either (isn't tested, isn't fully contained in apache/spark) and
that's what we're fixing.

I still don't like the idea of using the ASF JIRA for Spark to track
issues in a separate project, as these kinds of splits are what we're
trying to get rid of. I think it's a plus to be able to only bother
with the Github PR/issue system, and not parallel JIRAs as well. I
also worry that this blurs the line between code that is formally
tested and blessed in a Spark release, and that which is not. You fix
an issue in this separate repo and marked it ""fixed in Spark 1.5"" --
what does that imply?

I think the issue is people don't like the sense this is getting
argue it hasn't really properly been part of Spark -- that's why we
need this change to happen. But, I also think this is easy to resolve
other ways: spark-packages.org, the pointer in the repo, prominent
notes in the wiki, etc.

I suggest Shivaram owns this, and that amplab/spark-ec2 is used to
host? I'm not qualified to help make the new copy or repo admin but
would be happy to help with the rest, like triaging, if you can give
me rights to open issues.


ce
s
, Shivaram Venkataraman
here
e a
milar
sers
y
C2
rk
-
s
h.
y
re
m
20
re
ts

---------------------------------------------------------------------


"
Steve Loughran <stevel@hortonworks.com>,"Wed, 15 Jul 2015 09:06:22 +0000",Re: problems with build of latest the master,"Ted Yu <yuzhihong@gmail.com>, Gil Vernik <GILV@il.ibm.com>","

Looking at Jenkins, master branch compiles.

Can you try the following command ?

mvn -Phive -Phadoop-2.6 -DskipTests clean package

What version of Java are you using ?

Ted, Giles has stuck in hadoop-openstack, it's that which is creating the problem

Giles, I don't know why hadoop-openstack has a mockito dependency as  it should be test time only

Looking at the POM it's tag

in hadoop-2.7 tis scoped to compile, which
    <dependency>
      <groupId>org.mockito</groupId>
      <artifactId>mockito-all</artifactId>
      <scope>compile</scope>
    </dependency>

it should be ""provided"", shouldn't it?

Created https://issues.apache.org/jira/browse/HADOOP-12235 : if someone supplies a patch I'll get it in.

-steve
"
Akhil Das <akhil@sigmoidanalytics.com>,"Wed, 15 Jul 2015 14:37:08 +0530",Re: RestSubmissionClient Basic Auth,Joel Zambrano <joelz@microsoft.com>,"Either way is fine. Relay proxy would be much easier, adding authentication
to the REST client would require you to rebuild and test the piece of code
that you wrote for authentication.

Thanks
Best Regards


ld
quire
"
"""daniel.mescheder"" <daniel.mescheder@realimpactanalytics.com>","Wed, 15 Jul 2015 05:49:19 -0700 (MST)","Spark-SQL parameters like shuffle.partitions should be stored in
 the lineage",dev@spark.apache.org,"Hey everyone,

Consider the following use of spark.sql.shuffle.partitions:

case class Data(A:String = f""${(math.random*1e8).toLong}%09.0f"", B: String
= f""${(math.random*1e8).toLong}%09.0f"")
val dataFrame = (1 to 1000).map(_ => Data()).toDF
dataFrame.registerTempTable(""data"")

sqlContext.setConf( ""spark.sql.shuffle.partitions"", ""10"")
val a = sqlContext.sql(""SELECT * FROM data CLUSTER BY A"")

sqlContext.setConf( ""spark.sql.shuffle.partitions"", ""20"")
val b = sqlContext.sql(""SELECT * FROM data CLUSTER BY A"")

a.rdd.partitions.size
b.rdd.partitions.size

Expected result:
10
20
Actual result:
20
20

The bad thing about this is that the way the DataFrame is evaluated
currently depends on the global state. Multiple actions on the same
DataFrame may hence behave nondeterministically.
Furthermore, imagine a user has a job that includes multiple shuffles each
of which might require different settings. It is very hard to determine
(due to lazyness) at which place the configuration is read and hence to
place the setConf call.

Instead I propose to ensure that all relevant configuration parameters are
stored ""in the lineage"" at the place where the DataFrame is created so that
every call to that DataFrame at a later point will deterministically use
the context that the DataFrame was initially created in.

Best regards,
Daniel




--"
Joel Zambrano <joelz@microsoft.com>,"Wed, 15 Jul 2015 14:00:31 +0000",Re: RestSubmissionClient Basic Auth,Akhil Das <akhil@sigmoidanalytics.com>,"Thanks Akhil! For the one where I change the rest client, how likely would it be that a change like that goes thru? Would it be rejected as an uncommon scenario? I really don't want to have this as a separate form of the branch.

Thanks,
Joel
________________________________
From: Akhil Das <akhil@sigmoidanalytics.com>
Sent: Wednesday, July 15, 2015 2:07:08 AM
To: Joel Zambrano
Cc: dev@spark.apache.org
Subject: Re: RestSubmissionClient Basic Auth

Either way is fine. Relay proxy would be much easier, adding authentication to the REST client would require you to rebuild and test the piece of code that you wrote for authentication.

Thanks
Best Regards

Hi! We have a gateway with basic auth that relays calls to the head node in our cluster. Is adding support for basic auth the wrong approach? Should we use a relay proxy? I've seen the code and it would probably require adding a few configs and appending the header on the get and post request of the REST submission client.

A best strategy would be greatly appreciated.

Thanks,
Joel

"
Ted Yu <yuzhihong@gmail.com>,"Wed, 15 Jul 2015 07:10:12 -0700",Re: problems with build of latest the master,Steve Loughran <stevel@hortonworks.com>,"I attached a patch for HADOOP-12235

BTW openstack was not mentioned in the first email from Gil.
My email and Gil's second email were sent around the same moment.

Cheers


"
Josh Rosen <joshrosen@databricks.com>,"Wed, 15 Jul 2015 08:21:37 -0700",Re: problems with build of latest the master,Ted Yu <yuzhihong@gmail.com>,"We may be able to fix this from the Spark side by adding appropriate
exclusions in our Hadoop dependencies, right?  If possible, I think that we
should do this.


"
Davies Liu <davies@databricks.com>,"Wed, 15 Jul 2015 08:21:47 -0700",Re: PySpark GroupByKey implementation question,Matt Cheah <mcheah@palantir.com>,"If the map-side-combine is not that necessary, given the fact that it cannot
reduce the size of data for shuffling much (do need to serialized the key for
each value), but can reduce the number of key-value pairs, and potential reduce
the number of operations later (repartition and groupby).

ld
a similar
logic to
ne
ion
n

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Wed, 15 Jul 2015 08:28:46 -0700",Re: problems with build of latest the master,Josh Rosen <joshrosen@databricks.com>,"If I understand correctly, hadoop-openstack is not currently dependence in Spark. 



sions in our Hadoop dependencies, right?  If possible, I think that we should do this.
e problem
 should be test time only 
upplies a patch I'll get it in.
"
Matt Cheah <mcheah@palantir.com>,"Wed, 15 Jul 2015 16:52:33 +0000",Re: PySpark GroupByKey implementation question,Davies Liu <davies@databricks.com>,"Should we actually enable map-side-combine for groupByKey in Scala RDD as
well, then? If we implement external-group-by should we implement it with
the map-side-combine semantics that Pyspark does?

-Matt Cheah


"
Animesh Tripathy <a.tripathy101@gmail.com>,"Wed, 15 Jul 2015 12:55:21 -0400",Re: Joining Apache Spark,,"I know Django users would love to use Apache Spark with Python 3 since 1.4
release is out. However, I was thinking of making a PHP library for Spark.
Anyone know what can help me get started. I am kinda new to developing
libraries I have only done library designing for Amazon MWS.
"
Davies Liu <davies@databricks.com>,"Wed, 15 Jul 2015 10:01:13 -0700",Re: PySpark GroupByKey implementation question,Matt Cheah <mcheah@palantir.com>,"I think we should start without map-side-combine for Scala, because
it's easy to OOM
in JVM than in Python (we don't have hard limit in Python yet).


ogic
_ji
Fb6oO
VX3ZI
e=
_sp
&r=hz
jAC

---------------------------------------------------------------------


"
RJ Nowling <rnowling@gmail.com>,"Wed, 15 Jul 2015 12:31:28 -0500",Record metadata with RDDs and DataFrames,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,

I'm working on an ETL task with Spark.  As part of this work, I'd like to
mark records with some info such as:

1. Whether the record is good or bad (e.g, Either)
2. Originating file and lines

Part of my motivation is to prevent errors with individual records from
stopping the entire pipeline.  I'd also like to filter out and log bad
records at various stages.

I could use RDD[Either[T]] for everything but that won't work for
DataFrames.  I was wondering if anyone has had a similar situation and if
they found elegant ways to handle this?

Thanks,
RJ
"
Reynold Xin <rxin@databricks.com>,"Wed, 15 Jul 2015 10:36:35 -0700",Re: Record metadata with RDDs and DataFrames,RJ Nowling <rnowling@gmail.com>,"How about just using two fields, one boolean field to mark good/bad, and
another to get the source file?



"
RJ Nowling <rnowling@gmail.com>,"Wed, 15 Jul 2015 12:53:06 -0500",Re: Record metadata with RDDs and DataFrames,Reynold Xin <rxin@databricks.com>,"I'm considering a few approaches -- one of which is to provide new
functions like mapLeft, mapRight, filterLeft, etc.

But this all falls shorts with DataFrames.  RDDs can easily be extended
from RDD[T] to RDD[Record[T]].  I guess with DataFrames, I could add
special columns?


"
Reynold Xin <rxin@databricks.com>,"Wed, 15 Jul 2015 11:01:14 -0700",Re: Record metadata with RDDs and DataFrames,RJ Nowling <rnowling@gmail.com>,"Yea - I'd just add a bunch of columns. Doesn't seem like that big of a deal.



"
Gil Vernik <GILV@il.ibm.com>,"Wed, 15 Jul 2015 21:33:22 +0300",Re: problems with build of latest the master,Ted Yu <yuzhihong@gmail.com>,"Right, it's not currently dependence in Spark.
If we already mention it, is it possible to make it part of current 
dependence, but only for Hadoop profiles 2.4 and up?
This will solve a lot of headache to those who use Spark + OpenStack Swift 
and need every time to manually edit pom.xml to add dependence of it.



From:   Ted Yu <yuzhihong@gmail.com>
To:     Josh Rosen <joshrosen@databricks.com>
Cc:     Steve Loughran <stevel@hortonworks.com>, Gil 
Vernik/Haifa/IBM@IBMIL, Dev <dev@spark.apache.org>
Date:   15/07/2015 18:28
Subject:        Re: problems with build of latest the master



If I understand correctly, hadoop-openstack is not currently dependence in 
Spark. 




We may be able to fix this from the Spark side by adding appropriate 
exclusions in our Hadoop dependencies, right?  If possible, I think that 
we should do this.

I attached a patch for HADOOP-12235

BTW openstack was not mentioned in the first email from Gil.
My email and Gil's second email were sent around the same moment.

Cheers



Looking at Jenkins, master branch compiles. 

Can you try the following command ?

mvn -Phive -Phadoop-2.6 -DskipTests clean package

What version of Java are you using ?

Ted, Giles has stuck in hadoop-openstack, it's that which is creating the 
problem

Giles, I don't know why hadoop-openstack has a mockito dependency as  it 
should be test time only 

Looking at the POM it's tag

in hadoop-2.7 tis scoped to compile, which 
    <dependency>
      <groupId>org.mockito</groupId>
      <artifactId>mockito-all</artifactId>
      <scope>compile</scope>
    </dependency>

it should be ""provided"", shouldn't it?

Created https://issues.apache.org/jira/browse/HADOOP-12235 : if someone 
supplies a patch I'll get it in.

-steve



"
Bob Beauchemin <bobb@sqlskills.com>,"Wed, 15 Jul 2015 11:29:11 -0700 (MST)",Use of non-standard LIMIT keyword in JDBC tableExists code,dev@spark.apache.org,"tableExists in 
spark/sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcUtils.scala uses
non-standard SQL (specifically, the LIMIT keyword) to determine whether a
table exists in a JDBC data source. This will cause an exception in
many/most JDBC databases that doesn't support LIMIT keyword. See
http://stackoverflow.com/questions/1528604/how-universal-is-the-limit-statement-in-sql

To check for table existence or an exception, it could be recrafted around
""select 1 from $table where 0 = 1"" which isn't the same (it returns an empty
resultset rather than the value '1'), but would support more data sources
and also support empty tables. The standard way to check for existence would
be to use information_schema.tables which is a SQL standard but may not work
for other JDBC data sources that support SQL, but not the
information_schema.

As an aside, I tried to submit a bug through JIRA, but using Create
Bug-Quick or Bug-Detailed through the Spark issue-tracker webpage appears to
submit the bug as a bug against the Atlas project, rather than the Spark
project.

Cheers, Bob



--

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Wed, 15 Jul 2015 19:40:55 +0100",Re: problems with build of latest the master,Gil Vernik <GILV@il.ibm.com>,"You shouldn't get dependencies you need from Spark, right? you declare
direct dependencies. Are we talking about re-scoping or excluding this
dep from Hadoop transitively?


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Wed, 15 Jul 2015 11:47:08 -0700",Re: Use of non-standard LIMIT keyword in JDBC tableExists code,Bob Beauchemin <bobb@sqlskills.com>,"Hi Bob,

Thanks for the email. You can select Spark as the project when you file a
JIRA ticket at https://issues.apache.org/jira/browse/SPARK



For ""select 1 from $table where 0=1"" -- if the database's optimizer doesn't
do constant folding and short-circuit execution, could the query end up
scanning all the data?



"
Marcelo Vanzin <vanzin@cloudera.com>,"Wed, 15 Jul 2015 11:47:51 -0700",Slight API incompatibility caused by SPARK-4072,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey all,

Just noticed this when some of our tests started to fail. SPARK-4072 added
a new method to the ""SparkListener"" trait, and even though it has a default
implementation, it doesn't seem like that applies retroactively.

Namely, if you have an existing, compiled app that has an implementation of
SparkListener, that app won't work on 1.5 without a recompile. You'll get
something like this:

java.lang.AbstractMethodError
	at org.apache.spark.scheduler.SparkListenerBus$class.onPostEvent(SparkListenerBus.scala:62)
	at org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31)
	at org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31)
	at org.apache.spark.util.ListenerBus$class.postToAll(ListenerBus.scala:56)
	at org.apache.spark.util.AsynchronousListenerBus.postToAll(AsynchronousListenerBus.scala:37)
	at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(AsynchronousListenerBus.scala:79)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1235)
	at org.apache.spark.util.AsynchronousListenerBus$$anon$1.run(AsynchronousListenerBus.scala:63)


Now I know that ""SparkListener"" is marked as @DeveloperApi, but is this
something we should care about? Seems like adding methods to traits is just
as backwards-incompatible as adding new methods to Java interfaces.


-- 
Marcelo
"
Gil Vernik <GILV@il.ibm.com>,"Wed, 15 Jul 2015 21:52:15 +0300",Re: problems with build of latest the master,Sean Owen <sowen@cloudera.com>,"I mean currently users that wish to use Spark and configure Spark to use 
OpenStack Swift need to manually edit pom.xml of Spark ( main, core, yarn 
) and add hadoop-openstack.jar to it and then compile Spark. 
My question is why not to include this dependency in Spark for Hadoop 
profiles 2.4 and up? ( hadoop-openstack.jar exists for 2.4 and upper 
versions )

I think when we first integrated Spark  + OpenStack Swift there were no 
profiles in Spark and so it was problematic to include this dependency 
there. But now it seems to be easy to achieve, since we have hadoop 
profiles in the poms.



From:   Sean Owen <sowen@cloudera.com>
To:     Gil Vernik/Haifa/IBM@IBMIL
Cc:     Ted Yu <yuzhihong@gmail.com>, Dev <dev@spark.apache.org>, Josh 
Rosen <joshrosen@databricks.com>, Steve Loughran <stevel@hortonworks.com>
Date:   15/07/2015 21:41
Subject:        Re: problems with build of latest the master



You shouldn't get dependencies you need from Spark, right? you declare
direct dependencies. Are we talking about re-scoping or excluding this
dep from Hadoop transitively?

Swift
in
we
the



"
Sean Owen <sowen@cloudera.com>,"Wed, 15 Jul 2015 20:07:55 +0100",Re: problems with build of latest the master,Gil Vernik <GILV@il.ibm.com>,"Why does Spark need to depend on it? I'm missing that bit. If an
openstack artifact is needed for openstack, shouldn't openstack add
it? otherwise everybody gets it in their build.


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Wed, 15 Jul 2015 12:14:58 -0700",Re: Slight API incompatibility caused by SPARK-4072,Marcelo Vanzin <vanzin@cloudera.com>,"an abstract class - in the doc it says that it exists more or less to
allow for binary compatibility (it says it's for Java users, but
really Scala could use this also):

https://github.com/apache/spark/blob/master/core/src/main/java/org/apache/spark/JavaSparkListener.java#L23

I think it might be reasonable that the Scala trait provides only
source compatibitly and the Java class provides binary compatibility.

- Patrick


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Wed, 15 Jul 2015 12:15:12 -0700",Re: Slight API incompatibility caused by SPARK-4072,Marcelo Vanzin <vanzin@cloudera.com>,"Actually the java one is a concrete class.


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Wed, 15 Jul 2015 12:19:15 -0700",Re: Slight API incompatibility caused by SPARK-4072,Patrick Wendell <pwendell@gmail.com>,"It's bad that expose a trait - even though we want to mixin stuff. We
should really audit all of these and expose only abstract classes for
anything beyond an extremely simple interface. That itself however would
break binary compatibility.



"
Marcelo Vanzin <vanzin@cloudera.com>,"Wed, 15 Jul 2015 12:20:19 -0700",Re: Slight API incompatibility caused by SPARK-4072,Patrick Wendell <pwendell@gmail.com>,"Hmm, the Java listener was added in 1.3, so I think it will work for my
needs.

Might be worth it to make it clear in the SparkListener documentation that
people should avoid using it directly. Or follow Reynold's suggestion.





-- 
Marcelo
"
Marcelo Vanzin <vanzin@cloudera.com>,"Wed, 15 Jul 2015 12:21:58 -0700",Re: Slight API incompatibility caused by SPARK-4072,Patrick Wendell <pwendell@gmail.com>,"Or, alternatively, the bus could catch that error and ignore / log it,
instead of stopping the context...





-- 
Marcelo
"
Bob Beauchemin <bobb@sqlskills.com>,"Wed, 15 Jul 2015 13:38:09 -0700 (MST)",Re: Use of non-standard LIMIT keyword in JDBC tableExists code,dev@spark.apache.org,"Granted the 1=0 thing is ugly and assumes constant-folding support or reads
way too much data.

Submitted JIRA SPARK-9078 (thanks for pointers) and expounded on possible
solutions a little bit more there.

Cheers, and thanks, Bob 



--

---------------------------------------------------------------------


"
Joseph Bradley <joseph@databricks.com>,"Wed, 15 Jul 2015 14:18:31 -0700",Re: Are These Issues Suitable for our Senior Project?,emrehan <emrehan.tuzun@gmail.com>,"Per recent comments on SPARK-6442, I'd recommend not working on that one
for now.  Instead, even if tasks are not that interesting to you, you
should try some small tasks at first to get used to contributing.  I am
quite sure we'll want to solve SPARK-3703 by May 2016; that's pretty far in
the future!  That one will be solved piecemeal, prioritizing common
algorithms.


 I
et
ld
s-Suitable-for-our-Senior-Project-tp13119p13167.html
"
"""Kelly, Jonathan"" <jonathak@amazon.com>","Wed, 15 Jul 2015 21:22:38 +0000","Re: Unable to use dynamicAllocation if spark.executor.instances is
 set in spark-defaults.conf","""dev@spark.apache.org"" <dev@spark.apache.org>","I haven't gotten a response on user@ yet for these questions, but these are probably better questions for dev@ anyway, aren't they? Could somebody on dev@ please respond?

Thanks,
Jonathan

From: Jonathan Kelly <jonathak@amazon.com<mailto:jonathak@amazon.com>>
Date: Wednesday, July 15, 2015 at 12:18 PM
To: ""user@spark.apache.org<mailto:user@spark.apache.org>"" <user@spark.apache.org<mailto:user@spark.apache.org>>
Subject: Re: Unable to use dynamicAllocation if spark.executor.instances is set in spark-defaults.conf

Would there be any problem in having spark.executor.instances (or --num-executors) be completely ignored (i.e., even for non-zero values) if spark.dynamicAllocation.enabled is true (i.e., rather than throwing an exception)?

I can see how the exception would be helpful if, say, you tried to pass both ""-c spark.executor.instances"" (or --num-executors) *and* ""-c spark.dynamicAllocation.enabled=true"" to spark-submit on the command line (as opposed to having one of them in spark-defaults.conf and one of them in the spark-submit args), but currently there doesn't seem to be any way to distinguish between arguments that were actually passed to spark-submit and settings that simply came from spark-defaults.conf.

If there were a way to distinguish them, I think the ideal situation would be for the validation exception to be thrown only if spark.executor.instances and spark.dynamicAllocation.enabled=true were both passed via spark-submit args or were both present in spark-defaults.conf, but passing spark.dynamicAllocation.enabled=true to spark-submit would take precedence over spark.executor.instances configured in spark-defaults.conf, and vice versa.

Thanks,
Jonathan

From: Jonathan Kelly <jonathak@amazon.com<mailto:jonathak@amazon.com>>
Date: Tuesday, July 14, 2015 at 4:23 PM
To: ""user@spark.apache.org<mailto:user@spark.apache.org>"" <user@spark.apache.org<mailto:user@spark.apache.org>>
Subject: Unable to use dynamicAllocation if spark.executor.instances is set in spark-defaults.conf

I've set up my cluster with a pre-calcualted value for spark.executor.instances in spark-defaults.conf such that I can run a job and have it maximize the utilization of the cluster resources by default. However, if I want to run a job with dynamicAllocation (by passing -c spark.dynamicAllocation.enabled=true to spark-submit), I get this exception:

Exception in thread ""main"" java.lang.IllegalArgumentException: Explicitly setting the number of executors is not compatible with spark.dynamicAllocation.enabled!
at org.apache.spark.deploy.yarn.ClientArguments.parseArgs(ClientArguments.scala:192)
at org.apache.spark.deploy.yarn.ClientArguments.<init>(ClientArguments.scala:59)
at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:54)
...

The exception makes sense, of course, but ideally I would like it to ignore what I've put in spark-defaults.conf for spark.executor.instances if I've enabled dynamicAllocation. The most annoying thing about this is that if I have spark.executor.instances present in spark-defaults.conf, I cannot figure out any way to spark-submit a job with spark.dynamicAllocation.enabled=true without getting this error. That is, even if I pass ""-c spark.executor.instances=0 -c spark.dynamicAllocation.enabled=true"", I still get this error because the validation in ClientArguments.parseArgs() that's checking for this condition simply checks for the presence of spark.executor.instances rather than whether or not its value is > 0.

Should the check be changed to allow spark.executor.instances to be set to 0 if spark.dynamicAllocation.enabled is true? That would be an OK compromise, but I'd really prefer to be able to enable dynamicAllocation simply by setting spark.dynamicAllocation.enabled=true rather than by also having to set spark.executor.instances to 0.

Thanks,
Jonathan
"
Patrick Wendell <pwendell@gmail.com>,"Wed, 15 Jul 2015 14:48:28 -0700",Announcing Spark 1.4.1!,"""dev@spark.apache.org"" <dev@spark.apache.org>, ""user@spark.apache.org"" <user@spark.apache.org>","Hi All,

I'm happy to announce the Spark 1.4.1 maintenance release.
We recommend all users on the 1.4 branch upgrade to
this release, which contain several important bug fixes.

Download Spark 1.4.1 - http://spark.apache.org/downloads.html
Release notes - http://spark.apache.org/releases/spark-release-1-4-1.html
Comprehensive list of fixes - http://s.apache.org/spark-1.4.1

Thanks to the 85 developers who worked on this release!

Please contact me directly for errata in the release notes.

- Patrick

---------------------------------------------------------------------


"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Wed, 15 Jul 2015 22:04:16 +0000",RE: BlockMatrix multiplication,Burak Yavuz <brkyvz@gmail.com>,"Hi Burak,

Iâ€™ve modified my code as you suggested, however it still leads to shuffling. Could you suggest whatâ€™s wrong with my code or provide an example code with block matrices multiplication that preserves data locality and does not cause shuffling?


Modified code:
import org.apache.spark.mllib.linalg.Matrices
import org.apache.spark.mllib.linalg.distributed.BlockMatrix
val parallelism = 5
val blockSize = 10000
val rows = parallelism * blockSize
val columns = blockSize
val size = rows * columns
assert(rows % blockSize == 0)
assert(columns % blockSize == 0)
val rowBlocks = rows / blockSize
val columnBlocks = columns / blockSize
// make block-diagonal matrix
val rdd = sc.parallelize( {
                for(i <- 0 until rowBlocks; j <- 0 until columnBlocks) yield (i, i)
                }, parallelism).map( coord => (coord, Matrices.rand(blockSize, blockSize, util.Random.self)))
val bm = new BlockMatrix(rdd, blockSize, blockSize).cache()
bm.validate()
val t = System.nanoTime()
// multiply matrix with itself
val aa = bm.multiply(bm)
aa.validate()
println(rows + ""x"" + columns + "", block:"" + blockSize + ""\t"" + (System.nanoTime() - t) / 1e9)


Best regards, Alexander

From: Ulanov, Alexander
Sent: Tuesday, July 14, 2015 6:24 PM
To: 'Burak Yavuz'
Cc: Rakesh Chalasani; dev@spark.apache.org
Subject: RE: BlockMatrix multiplication

Hi Burak,

Thank you for explanation! I will try to make a diagonal block matrix and report you the results.

Column- or row based partitioner make sense to me, because it is a direct analogy from column or row-based data storage in matrices, which is used in BLAS.

Best regards, Alexander

From: Burak Yavuz [mailto:brkyvz@gmail.com]
Sent: Tuesday, July 14, 2015 10:14 AM
To: Ulanov, Alexander
Cc: Rakesh Chalasani; dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: BlockMatrix multiplication

Hi Alexander,

From your example code, using the GridPartitioner, you will have 1 column, and 5 rows. When you perform an A^T^A multiplication, you will generate a separate GridPartitioner with 5 columns and 5 rows. Therefore you are observing a huge shuffle. If you would generate a diagonal-block matrix as an example (5x5), you should not observe any shuffle.

Basically, your example causes the worst kind of shuffle. We can implement RowBasedPartitioning, and ColumnBasedPartitioning for optimization, but we didn't initially see it necessary to expose the partitioners to users, and didn't add them (you can find the old implementations here<https://github.com/brkyvz/spark/commit/9ae85aa1ebabdc099d7f655bc1d9021d34d2910f>).

Hope that helps!

Best,
Burak

On Tue, Jul 14, 2015 at 9:37 AM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
Hi Rakesh,

I am not interested in a particular case of A^T*A. This case is a handy setup so I donâ€™t need to create another matrix and force the blocks to co-locate. Basically, I am trying to understand the effectiveness of BlockMatrix for multiplication of distributed matrices. It seems that I am missing something or using it wrong.

Best regards, Alexander

From: Rakesh Chalasancom>]
Sent: Tuesday, July 14, 2015 9:05 AM
To: Ulanov, Alexander
Cc: dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: BlockMatrix multiplication

Hi Alexander:

Aw, I missed the 'cogroup' on BlockMatrix multiply! I stand corrected. Check
https://github.com/apache/spark/blob/3c0156899dc1ec1f7dfe6d7c8af47fa6dc7d00bf/mllib/src/main/scala/org/apache/spark/mllib/linalg/distributed/BlockMatrix.scala#L361

BlockMatrix multiply uses a custom partitioner called GridPartitioner, that might be causing the shuffle; which, in your special case need not happen. But, from what I understood from your code, I don't think this is an issue since your special case can be handled using computeGramMatrix on RowMatrix. Is there a reason you did not use that?

Rakesh


On Tue, Jul 14, 2015 at 11:03 AM Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
Hi Rakesh,

Thanks for suggestion. Each block of original matrix is in separate partition. Each block of transposed matrix is also in a separate partition. The partition numbers are the same for the blocks that undergo multiplication. Each partition is on a separate worker. Basically, I want to force each worker to multiply only 2 blocks. This should be the optimal configuration for multiplication, as far as I understand. Having several blocks in each partition as you suggested is not optimal, is it?

Best regards, Alexander

Block matrix stores the data as key->Matrix pairs and multiply does a reduceByKey operations, aggregating matrices per key. Since you said each block is residing in a separate partition, reduceByKey might be effectively shuffling all of the data. A better way to go about this is to allow multiple blocks within each partition so that reduceByKey does a local reduce before aggregating across nodes.

Rakesh

On Mon, Jul 13, 2015 at 9:24 PM Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
Dear Spark developers,

I am trying to perform BlockMatrix multiplication in Spark. My test is as follows: 1)create a matrix of N blocks, so that each row of block matrix contains only 1 block and each block resides in separate partition on separate node, 2)transpose the block matrix and 3)multiply the transposed matrix by the original non-transposed one. This should preserve the data locality, so there should be no need for shuffle. However, I observe huge shuffle with the block matrix size of 50000x10000 and one block 10000x10000, 5 blocks per matrix. Could you suggest what is wrong?

My setup is Spark 1.4, one master and 5 worker nodes, each is Xeon 2.2 16 GB RAM.
Below is the test code:

import org.apache.spark.mllib.linalg.Matrices
import org.apache.spark.mllib.linalg.distributed.BlockMatrix
val parallelism = 5
val blockSize = 10000
val rows = parallelism * blockSize
val columns = blockSize
val size = rows * columns
assert(rows % blockSize == 0)
assert(columns % blockSize == 0)
val rowBlocks = rows / blockSize
val columnBlocks = columns / blockSize
val rdd = sc.parallelize( {
                for(i <- 0 until rowBlocks; j <- 0 until columnBlocks) yield (i, j)
                }, parallelism).map( coord => (coord, Matrices.rand(blockSize, blockSize, util.Random.self)))
val bm = new BlockMatrix(rdd, blockSize, blockSize).cache()
bm.validate()
val mb = bm.transpose.cache()
mb.validate()
val t = System.nanoTime()
val ata = mb.multiply(bm)
ata.validate()
println(rows + ""x"" + columns + "", block:"" + blockSize + ""\t"" + (System.nanoTime() - t) / 1e9)


Best regards, Alexander

"
Burak Yavuz <brkyvz@gmail.com>,"Wed, 15 Jul 2015 15:29:00 -0700",Re: BlockMatrix multiplication,"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Hi Alexander,

I just noticed the error in my logic. There will always be a shuffle due to
the `cogroup`. `join` also uses cogroup, therefore a shuffle is inevitable.
However, the reduceByKey will not cause a shuffle. I forgot about how
cogroup will try to match things, even if they don't exist.

An optimization we wanted to perform was that in a grid partitioned
setting, send a partition a single copy of the block and match blocks
within the partition. Right now we send a partition multiple copies,
because we cogroup on (i, j, k).

Unfortunately in the current setting, I don't think there is a way to
reduce the shuffle. Could you observe what the shuffle is if you change:
val bm = new BlockMatrix(rdd, 2 * blockSize, 2 * blockSize).cache()

My hypothesis is that the shuffle should decrease then.

Best,
Burak


 an
in
,
s
t
e
d
4d2910f>
ks to
m
00bf/mllib/src/main/scala/org/apache/spark/mllib/linalg/distributed/BlockMatrix.scala#L361
n
n.
l
ly
m>
"
Sean Owen <sowen@cloudera.com>,"Thu, 16 Jul 2015 08:21:09 +0100",Re: Foundation policy on releases and Spark nightly builds,Sean Busbey <busbey@cloudera.com>,"To move this forward, I think one of two things needs to happen:

1. Move this guidance to the wiki. Seems that people gathered here
believe that resolves the issue. Done.

2. Put disclaimers on the current downloads page. This may resolve the
issue, but then we bring it up on the right mailing list for
discussion. It may end up at #1, or may end in a tweak to the policy.

I can drive either one. Votes on how to proceed?


---------------------------------------------------------------------


"
Akhil Das <akhil@sigmoidanalytics.com>,"Thu, 16 Jul 2015 12:52:15 +0530",Re: RestSubmissionClient Basic Auth,Joel Zambrano <joelz@microsoft.com>,"You can possibly raise a JIRA ticket for feature and start working on it,
once done you can send a pull request with the code changes.

Thanks
Best Regards


he
?
bably
t
"
Steve Loughran <stevel@hortonworks.com>,"Thu, 16 Jul 2015 09:14:38 +0000",Re: problems with build of latest the master,Sean Owen <sowen@cloudera.com>,"Patching hadoop's build will fix this long term, but not until Hadoop-2.7.2


I think just adding the openstack JAR to the spark classpath should be enough to pick this up, which the --jars command can do with ease

azure and s3a support in), would be the spark shell scripts to auto-add everything
in SPARK_HOME/lib to the CP (or at least having the option to do this). Because spark-class doesn't do that, run-example has to have its own code to add
the examples jar.

Has this been discussed before? Even having an env var that spark-class, pyspark & others could pick up would be enough


n )
iles
h
ift
 in
e:
t
he
t


---------------------------------------------------------------------


"
Hao Ren <invkrh@gmail.com>,"Thu, 16 Jul 2015 11:39:49 +0200",S3 Read / Write makes executors deadlocked,"user <user@spark.apache.org>, dev@spark.apache.org","Given the following code which just reads from s3, then saves files to s3

----------------------------

val inputFileName: String = ""s3n://input/file/path""
val outputFileName: String = ""s3n://output/file/path""
val conf = new SparkConf().setAppName(this.getClass.getName).setMaster(""local[4]"")

val sparkContext = new SparkContext(conf)

// Problems here: executors thread locked
sparkContext.textFile(inputFileName).saveAsTextFile(outputFileName)
// But this one works
sparkContext.textFile(inputFileName).count()
----------------------------

It blocks without showing any exceptions or errors. jstack shows that
all executors are locked. The thread dump is in end of this post.

I am using spark-1.4.0 on my PC which has 4 CPU cores.
There are 21 parquet files in the input directory, 500KB / file.

In addition, if we change the last action to a non IO bounded one, for
example, count(). It works.
It seems that S3 read and write in the same stage makes executors deadlocked.

I encountered the same problem when using DataFrame load/save
operations, jira created:
https://issues.apache.org/jira/browse/SPARK-8869

""Executor task launch worker-3"" #69 daemon prio=5 os_prio=0
tid=0x00007f7bd4036800 nid=0x1296 in Object.wait()
[0x00007f7c1099a000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	at org.apache.commons.httpclient.MultiThreadedHttpConnectionManager.doGetConnection(MultiThreadedHttpConnectionManager.java:518)
	- *locked* <0x00000000e56745b8> (a
org.apache.commons.httpclient.MultiThreadedHttpConnectionManager$ConnectionPool)
	at org.apache.commons.httpclient.MultiThreadedHttpConnectionManager.getConnectionWithTimeout(MultiThreadedHttpConnectionManager.java:416)
	at org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:153)
	at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:397)
	at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:323)
	at org.jets3t.service.impl.rest.httpclient.RestS3Service.performRequest(RestS3Service.java:342)
	at org.jets3t.service.impl.rest.httpclient.RestS3Service.performRestHead(RestS3Service.java:718)
	at org.jets3t.service.impl.rest.httpclient.RestS3Service.getObjectImpl(RestS3Service.java:1599)
	at org.jets3t.service.impl.rest.httpclient.RestS3Service.getObjectDetailsImpl(RestS3Service.java:1535)
	at org.jets3t.service.S3Service.getObjectDetails(S3Service.java:1987)
	at org.jets3t.service.S3Service.getObjectDetails(S3Service.java:1332)
	at org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.retrieveMetadata(Jets3tNativeFileSystemStore.java:111)
	at sun.reflect.GeneratedMethodAccessor1.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at org.apache.hadoop.fs.s3native.$Proxy15.retrieveMetadata(Unknown Source)
	at org.apache.hadoop.fs.s3native.NativeS3FileSystem.getFileStatus(NativeS3FileSystem.java:414)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1397)
	at org.apache.hadoop.fs.s3native.NativeS3FileSystem.create(NativeS3FileSystem.java:341)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:905)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:798)
	at org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:123)
	at org.apache.spark.SparkHadoopWriter.open(SparkHadoopWriter.scala:90)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1104)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1095)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)
	at org.apache.spark.scheduler.Task.run(Task.scala:70)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)


-- 
Hao Ren

Data Engineer @ leboncoin

Paris, France
"
Steve Loughran <stevel@hortonworks.com>,"Thu, 16 Jul 2015 12:44:05 +0000",why doesn't jenkins like me?,Dev <dev@spark.apache.org>,"

https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/37491/testReport/junit/org.apache.spark/DistributedSuite/_It_is_not_a_test_/

This isn't the only pull request that's failing, and I've merged in the master branch to make sure its not something fixed in the source.

is there some intermittent race condition or similar?

-steve
"
Eugene Morozov <fathersson@list.ru>,"Thu, 16 Jul 2015 17:57:24 +0300",KryoSerializer gives class cast exception,Dev <dev@spark.apache.org>,"Hi, some time ago we’ve found that it’s better use Kryo serializer instead of Java one.
So, we turned it on and use it everywhere.

I have pretty complex objects, which I can’t change. Previously my algo was building such an objects and then storing them into external storage. It was not required to reshuffle partitions. Now, it seems I have to reshuffle them, but I’m stuck with ClassCastException. I investigated it a little and it seems to me that KryoSerializer does not clear it’s state at some point, so it tries to use StringSerializer for my non String object. My objects are pretty complex, it’d be pretty hard to make them serializable.

Caused by: java.lang.ClassCastException: com.company.metadata.model.cleanse.CleanseInfoSequence cannot be cast to java.lang.String
	at com.esotericsoftware.kryo.serializers.DefaultSerializers$StringSerializer.write(DefaultSerializers.java:146)
	at com.esotericsoftware.kryo.Kryo.writeObjectOrNull(Kryo.java:549)
	at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:68)
	at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:18)
	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:501)
	at com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.write(FieldSerializer.java:564)
	... 71 more

I’ve found this state issue in Kryo jira and that it’s been fixed after 2.21 (current kryo version in spark). But spark cannot update, because of chill and chill cannot be updated because of some dependencies on their side. So, spark sort of stuck with kryo version 2.21.

My own thoughts how I could workaround this
1. Rewrite algo, so that my objects shouldn’t be reshuffled. But at some point it’be required.
2. Make my objects implement Serializable and be stuck with java serialization forever.
3. My object inside of kryo looks like ArrayList with my object, so I’m not sure it’s possible to register my class with custom serializer in kryo.

Any advice would be highly appreciated. 
Thanks.
--
Eugene Morozov
fathersson@list.ru




"
nipun <ibnipun10@gmail.com>,"Thu, 16 Jul 2015 05:31:03 -0700 (MST)",Apache gives exception when running groupby on df temp table,dev@spark.apache.org,"I have a dataframe. I register it as a temp table and run a spark sql query
on it to get another dataframe. Now when I run groupBy on it, it gives me
this exception

e: Lost task 1.3 in stage 21.0 (TID 579, 172.28.0.162):
java.lang.ClassCastException: java.lang.String cannot be cast to
org.apache.spark.sql.types.UTF8String
        at
org.apache.spark.sql.execution.SparkSqlSerializer2$$anonfun$createSerializationFunction$1.apply(SparkSqlSerializer2.scala:319)
        at
org.apache.spark.sql.execution.SparkSqlSerializer2$$anonfun$createSerializationFunction$1.apply(SparkSqlSerializer2.scala:212)
        at
org.apache.spark.sql.execution.Serializer2SerializationStream.writeKey(SparkSqlSerializer2.scala:65)
        at
org.apache.spark.storage.DiskBlockObjectWriter.write(BlockObjectWriter.scala:206)
        at
org.apache.spark.util.collection.WritablePartitionedIterator$$anon$3.writeNext(WritablePartitionedPairCollection.scala:104)
        at
org.apache.spark.util.collection.ExternalSorter.spillToPartitionFiles(ExternalSorter.scala:375)
        at
org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:208)
        at
org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:62)
        at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:70)
        at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
        at org.apache.spark.scheduler.Task.run(Task.scala:70)
        at
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
        at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)



--

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Thu, 16 Jul 2015 08:51:26 -0700",Re: Apache gives exception when running groupby on df temp table,nipun <ibnipun10@gmail.com>,"Can you provide a bit more information such as:

release of Spark you use
snippet of your SparkSQL query

Thanks


"
Hao Ren <invkrh@gmail.com>,"Thu, 16 Jul 2015 23:29:06 +0200",Re: S3 Read / Write makes executors deadlocked,"user <user@spark.apache.org>, dev@spark.apache.org","I have tested on another pc which has 8 CPU cores.
But it hangs when defaultParallelismLevel > 4, e.g.
sparkConf.setMaster(""local[*]"")
local[1] ~ local[3] work well.

4 is the mysterious boundary.

It seems that I am not the only one encountered this problem:
https://issues.apache.org/jira/browse/SPARK-8898

Here is Sean's answer for the jira above:
this is a jets3t problem. You will have to manage to update it in your
build or get EC2 + Hadoop 2 to work, which I think can be done. At least,
this is just a subset of ""EC2 should support Hadoop 2"" and/or that the EC2
support should move out of Spark anyway. I don't know there's another
action to take in Spark.

But I just use sbt the get the published spark 1.4, and it does not work on
my local PC, not EC2.
Seriously, I do think something should be done for Spark, because s3
read/write is quite a common use case.

Any help on this issue is highly appreciated.
If you need more info, checkout the jira I created:
https://issues.apache.org/jira/browse/SPARK-8869





-- 
Hao Ren

Data Engineer @ leboncoin

Paris, France
"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Thu, 16 Jul 2015 23:20:03 +0000",RE: BlockMatrix multiplication,Burak Yavuz <brkyvz@gmail.com>,"Hi Burak,

If I change the code as you suggested then it fails with (given that blockSize is 10000):
â€œorg.apache.spark.SparkException: The MatrixBlock at (3, 3) has dimensions different than rowsPerBlock: 20000, and colsPerBlock: 20000. Blocks on the right and bottom edges can have smaller dimensions. You may use the repartition method to fix this issue.â€

Should I submit a JIRA Issue related to the problem of block matrix shuffling given the blocks co-location?

Best regards, Alexander

From: Burak Yavuz [mailto:brkyvz@gmail.com]
Sent: Wednesday, July 15, 2015 3:29 PM
To: Ulanov, Alexander
Cc: Rakesh Chalasani; dev@spark.apache.org
Subject: Re: BlockMatrix multiplication

Hi Alexander,

I just noticed the error in my logic. There will always be a shuffle due to the `cogroup`. `join` also uses cogroup, therefore a shuffle is inevitable. However, the reduceByKey will not cause a shuffle. I forgot about how cogroup will try to match things, even if they don't exist.

An optimization we wanted to perform was that in a grid partitioned setting, send a partition a single copy of the block and match blocks within the partition. Right now we send a partition multiple copies, because we cogroup on (i, j, k).

Unfortunately in the current setting, I don't think there is a way to reduce the shuffle. Could you observe what the shuffle is if you change:
val bm = new BlockMatrix(rdd, 2 * blockSize, 2 * blockSize).cache()

My hypothesis is that the shuffle should decrease then.

Best,
Burak

On Wed, Jul 15, 2015 at 3:04 PM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
Hi Burak,

Iâ€™ve modified my code as you suggested, however it still leads to shuffling. Could you suggest whatâ€™s wrong with my code or provide an example code with block matrices multiplication that preserves data locality and does not cause shuffling?


Modified code:
import org.apache.spark.mllib.linalg.Matrices
import org.apache.spark.mllib.linalg.distributed.BlockMatrix
val parallelism = 5
val blockSize = 10000
val rows = parallelism * blockSize
val columns = blockSize
val size = rows * columns
assert(rows % blockSize == 0)
assert(columns % blockSize == 0)
val rowBlocks = rows / blockSize
val columnBlocks = columns / blockSize
// make block-diagonal matrix
val rdd = sc.parallelize( {
                for(i <- 0 until rowBlocks; j <- 0 until columnBlocks) yield (i, i)
                }, parallelism).map( coord => (coord, Matrices.rand(blockSize, blockSize, util.Random.self)))
val bm = new BlockMatrix(rdd, blockSize, blockSize).cache()
bm.validate()
val t = System.nanoTime()
// multiply matrix with itself
val aa = bm.multiply(bm)
aa.validate()
println(rows + ""x"" + columns + "", block:"" + blockSize + ""\t"" + (System.nanoTime() - t) / 1e9)


Best regards, Alexander

From: Ulanov, Alexander
Sent: Tuesday, July 14, 2015 6:24 PM
To: 'Burak Yavuz'
Cc: Rakesh Chalasani; dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: RE: BlockMatrix multiplication

Hi Burak,

Thank you for explanation! I will try to make a diagonal block matrix and report you the results.

Column- or row based partitioner make sense to me, because it is a direct analogy from column or row-based data storage in matrices, which is used in BLAS.

Best regards, Alexander

From: Burak Yavuz [mailto:brkyvz@gmail.com]
Sent: Tuesday, July 14, 2015 10:14 AM
To: Ulanov, Alexander
Cc: Rakesh Chalasani; dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: BlockMatrix multiplication

Hi Alexander,

From your example code, using the GridPartitioner, you will have 1 column, and 5 rows. When you perform an A^T^A multiplication, you will generate a separate GridPartitioner with 5 columns and 5 rows. Therefore you are observing a huge shuffle. If you would generate a diagonal-block matrix as an example (5x5), you should not observe any shuffle.

Basically, your example causes the worst kind of shuffle. We can implement RowBasedPartitioning, and ColumnBasedPartitioning for optimization, but we didn't initially see it necessary to expose the partitioners to users, and didn't add them (you can find the old implementations here<https://github.com/brkyvz/spark/commit/9ae85aa1ebabdc099d7f655bc1d9021d34d2910f>).

Hope that helps!

Best,
Burak

On Tue, Jul 14, 2015 at 9:37 AM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
Hi Rakesh,

I am not interested in a particular case of A^T*A. This case is a handy setup so I donâ€™t need to create another matrix and force the blocks to co-locate. Basically, I am trying to understand the effectiveness of BlockMatrix for multiplication of distributed matrices. It seems that I am missing something or using it wrong.

Best regards, Alexander

From: Rakesh Chalasani [mailto:vnit.rakesh@gmail.com<mailto:vnit.rakesh@gmail.com>]
Sent: Tuesday, July 14, 2015 9:05 AM
To: Ulanov, Alexander
Cc: dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: BlockMatrix multiplication

Hi Alexander:

Aw, I missed the 'cogroup' on BlockMatrix multiply! I stand corrected. Check
https://github.com/apache/spark/blob/3c0156899dc1ec1f7dfe6d7c8af47fa6dc7d00bf/mllib/src/main/scala/org/apache/spark/mllib/linalg/distributed/BlockMatrix.scala#L361

BlockMatrix multiply uses a custom partitioner called GridPartitioner, that might be causing the shuffle; which, in your special case need not happen. But, from what I understood from your code, I don't think this is an issue since your special case can be handled using computeGramMatrix on RowMatrix. Is there a reason you did not use that?

Rakesh


On Tue, Jul 14, 2015 at 11:03 AM Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
Hi Rakesh,

Thanks for suggestion. Each block of original matrix is in separate partition. Each block of transposed matrix is also in a separate partition. The partition numbers are the same for the blocks that undergo multiplication. Each partition is on a separate worker. Basically, I want to force each worker to multiply only 2 blocks. This should be the optimal configuration for multiplication, as far as I understand. Having several blocks in each partition as you suggested is not optimal, is it?

Best regards, Alexander

Block matrix stores the data as key->Matrix pairs and multiply does a reduceByKey operations, aggregating matrices per key. Since you said each block is residing in a separate partition, reduceByKey might be effectively shuffling all of the data. A better way to go about this is to allow multiple blocks within each partition so that reduceByKey does a local reduce before aggregating across nodes.

Rakesh

On Mon, Jul 13, 2015 at 9:24 PM Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
Dear Spark developers,

I am trying to perform BlockMatrix multiplication in Spark. My test is as follows: 1)create a matrix of N blocks, so that each row of block matrix contains only 1 block and each block resides in separate partition on separate node, 2)transpose the block matrix and 3)multiply the transposed matrix by the original non-transposed one. This should preserve the data locality, so there should be no need for shuffle. However, I observe huge shuffle with the block matrix size of 50000x10000 and one block 10000x10000, 5 blocks per matrix. Could you suggest what is wrong?

My setup is Spark 1.4, one master and 5 worker nodes, each is Xeon 2.2 16 GB RAM.
Below is the test code:

import org.apache.spark.mllib.linalg.Matrices
import org.apache.spark.mllib.linalg.distributed.BlockMatrix
val parallelism = 5
val blockSize = 10000
val rows = parallelism * blockSize
val columns = blockSize
val size = rows * columns
assert(rows % blockSize == 0)
assert(columns % blockSize == 0)
val rowBlocks = rows / blockSize
val columnBlocks = columns / blockSize
val rdd = sc.parallelize( {
                for(i <- 0 until rowBlocks; j <- 0 until columnBlocks) yield (i, j)
                }, parallelism).map( coord => (coord, Matrices.rand(blockSize, blockSize, util.Random.self)))
val bm = new BlockMatrix(rdd, blockSize, blockSize).cache()
bm.validate()
val mb = bm.transpose.cache()
mb.validate()
val t = System.nanoTime()
val ata = mb.multiply(bm)
ata.validate()
println(rows + ""x"" + columns + "", block:"" + blockSize + ""\t"" + (System.nanoTime() - t) / 1e9)


Best regards, Alexander


"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Thu, 16 Jul 2015 23:59:37 +0000",RE: Model parallelism with RDD,"""shivaram@eecs.berkeley.edu"" <shivaram@eecs.berkeley.edu>","Dear Spark developers,

What happens if RDD does not fit into memory and cache would not work in the code below? Will all previous iterations repeated each new iteration within iterative RDD update (as described below)?

Also, could you clarify regarding DataFrame and GC overhead: does setting spark.sql.unsafe.enabled=true removes the GC when persisting/unpersisting the DataFrame?

Best regards, Alexander

From: Ulanov, Alexander
Sent: Monday, July 13, 2015 11:15 AM
To: shivaram@eecs.berkeley.edu
Cc: dev@spark.apache.org
Subject: RE: Model parallelism with RDD

Below are the average timings for one iteration of model update with RDD  (with cache, as Shivaram suggested):
Model size, RDD[Double].count / time, s
10M 0.585336926
100M 1.767947506
1B 125.6078817

There is a ~100x increase in time while 10x increase in model size (from 100 million to 1 billion of Double). More than half of the time is spent in GC, and this time varies heavily. Two questions:
1)Can I use project Tungstenâ€™s unsafe? Actually, can I reduce the GC time if I use DataFrame instead of RDD and set the Tungsten key: spark.sql.unsafe.enabled=true ?
2) RDD[Double] of one billion elements is 26.1GB persisted (as Spark UI shows). It is around 26 bytes per element. How many bytes is RDD overhead?

The code:
val modelSize = 1000000000
val numIterations = 10
val parallelism = 5
var oldRDD = sc.parallelize(1 to modelSize, parallelism).map(x => 0.1).cache
var newRDD = sc.parallelize(1 to 1, parallelism).map(x => 0.1)
var i = 0
var avgTime = 0.0
while (i < numIterations) {
  val t = System.nanoTime()
  val newRDD = oldRDD.map(x => x * x)
  newRDD.cache
  newRDD.count()
  oldRDD.unpersist(true)
  newRDD.mean
  avgTime += (System.nanoTime() - t) / 1e9
  oldRDD = newRDD
  i += 1
}
println(""Avg iteration time:"" + avgTime / numIterations)

Best regardram@eecs.berkeley.edu]
Sent: Friday, July 10, 2015 10:04 PM
To: Ulanov, Alexander
Cc: <shivaram@eecs.berkeley.eorg<mailto:dev@spark.apache.org>
Subject: Re: Model parallelism with RDD

Yeah I can see that being the case -- caching implies creating objects that will be stored in memory. So there is a trade-off between storing data in memory but having to garbage collect it later vs. recomputing the data.

Shivaram

On Fri, Jul 10, 2015 at 9:49 PM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
Hi Shivaram,

Thank you for suggestion! If I do .cache and .count, each iteration take much more time, which is spent in GC. Is it normal?

10 Ð¸ÑŽÐ»Ñ 2015 Ð³., Ð² 21:23, Shivaram Venkatarama@eecs.berkeley.edu>>> Ð½Ð°Ð¿Ð¸ÑÐ°Ð»(Ð°):

I think you need to do `newRDD.cache()` and `newRDD.count` before you do oldRDD.unpersist(true) -- Otherwise it might be recomputing all the previous iterations each time.

Thanks
Shivaram
On Fri, Jul 10, 2015 at 7:44 PM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
Hi,

I am interested how scalable can be the model parallelism within Spark. Suppose, the model contains N weights of type Double and N is so large that does not fit into the memory of a single node. So, we can store the model in RDD[Double] within several nodes. To train the model, one needs to perform K iterations that update all the weights and check the convergence. Then we also need to exchange some weights between the nodes to synchronize the model or update the global state. Iâ€™ve sketched the code that does iterative updates with RDD (without global update yet). Surprisingly, each iteration takes more time than previous as shown below (time in seconds). Could you suggest what is the reason for that? Iâ€™ve checked GC, it does something within few milliseconds.

Configuration: Spark 1.4, 1 master and 5 worker nodes, 5 executors, Intel Xeon 2.2, 16GB RAM each
Iteration 0 time:1.127990986
Iteration 1 time:1.391120414
Iteration 2 time:1.6429691381000002
Iteration 3 time:1.9344402954
Iteration 4 time:2.2075294246999997
Iteration 5 time:2.6328659593
Iteration 6 time:2.7911690492999996
Iteration 7 time:3.0850374104
Iteration 8 time:3.4031050061
Iteration 9 time:3.8826580919

Code:
val modelSize = 1000000000
val numIterations = 10
val parallelizm = 5
var oldRDD = sc.parallelize(1 to modelSize, parallelizm).map(x => 0.1)
var newRDD = sc.parallelize(1 to 1, parallelizm).map(x => 0.1)
var i = 0
while (i < numIterations) {
  val t = System.nanoTime()
  // updating the weights
  val newRDD = oldRDD.map(x => x * x)
  oldRDD.unpersist(true)
  // â€œcheckingâ€ convergence
  newRDD.mean
  println(""Iteration "" + i + "" time:"" + (System.nanoTime() - t) / 1e9 / numIterations)
  oldRDD = newRDD
  i += 1
}


Best regards, Alexander

"
Xiaoyu Ma <hzmaxiaoyu@corp.netease.com>,"Fri, 17 Jul 2015 18:26:06 +0800",Hive Table with large number of partitions,dev@spark.apache.org,"Hi guys,
I saw when Hive Table object created it tries to load all existing partitions. 

@transient val hiveQlPartitions: Seq[Partition] = table.getAllPartitions.map { p =>
  val tPartition = new org.apache.hadoop.hive.metastore.api.Partition
  tPartition.setDbName(databaseName)
  tPartition.setTableName(tableName)
  tPartition.setValues(p.values)
Above code for getAllPartitions costs minutes, tons of memory and failed almost always on table with large partitions. This made it useless in our use case. 
I think this should be kind of lazy loading or can be postpone till partition pruning stage. Not sure why we need full partition info at this stage.
Any existing ticket to solve this? 

é©¬æ™“å®‡ / Xiaoyu Ma
hzmaxiaoyu@corp.netease.com




"
Michael Armbrust <michael@databricks.com>,"Fri, 17 Jul 2015 06:59:22 -0700",Re: Hive Table with large number of partitions,Xiaoyu Ma <hzmaxiaoyu@corp.netease.com>,"https://github.com/apache/spark/pull/7421


s.map { p =>
"
nipun <ibnipun10@gmail.com>,"Fri, 17 Jul 2015 07:52:37 -0700 (MST)",Re: Apache gives exception when running groupby on df temp table,dev@spark.apache.org,"spark version 1.4

import com.datastax.spark.connector._
import  org.apache.spark._
import org.apache.spark.sql.cassandra.CassandraSQLContext
import org.apache.spark.SparkConf
//import com.microsoft.sqlserver.jdbc.SQLServerDriver
import java.sql.Connection
import java.sql.DriverManager
import java.io.IOException
import org.apache.spark.sql.DataFrame

 def populateEvents() : Unit = {

                var query = ""SELECT brandname, appname, packname, eventname,
client, timezone  FROM sams.events WHERE eventtime > '"" + _from + ""' AND
eventtime < '"" + _to + ""'""
                // read data from cassandra table
                val rdd = runCassandraQuery(query)

                rdd.registerTempTable(""newdf"")

                query = ""Select brandname, appname, packname, eventname,
client.OSName as platform, timezone from newdf""
                val dfCol = runCassandraQuery(query)

                val grprdd = dfCol.groupBy(""brandname"", ""appname"",
""packname"", ""eventname"", ""platform"", ""timezone"").count()

Do let me know if you need any more information



--

---------------------------------------------------------------------


"
Josh Rosen <rosenville@gmail.com>,"Fri, 17 Jul 2015 08:15:02 -0700",Re: KryoSerializer gives class cast exception,Eugene Morozov <fathersson@list.ru>,"We've run into other problems caused by our old Kryo versions. I agree that
the Chill dependency is one of the main blockers to upgrading Kryo, but I
don't think that it's insurmountable: if necessary, we could just publish
our own forked version of Chill under our own namespace, similar to what we
used to do with Pyrolite.

A bigger concern, perhaps, is dependency conflicts with user-specified Kryo
versions.

See https://github.com/apache/spark/pull/6361 and
https://issues.apache.org/jira/browse/SPARK-7708 for some more previous
discussions RE: Kryo upgrade.

Anyhow, I'm not sure what the right solution is yet, but just wanted to
link to some previous context / discussions.

- Josh


erializer instead
y algo
ated it a
 state at
.write(DefaultSerializers.java:146)
onSerializer.java:68)
onSerializer.java:18)
ieldSerializer.java:564)
en fixed after
at some
€™m
r in kryo.
"
Josh Rosen <rosenville@gmail.com>,"Fri, 17 Jul 2015 08:17:14 -0700",Re: why doesn't jenkins like me?,Steve Loughran <stevel@hortonworks.com>,"The ""It is not a test"" failed test message means that something went wrong
in a suite-wide setup or teardown method.  This could be some sort of race
or flakiness.  If this problem persists, we should file a JIRA and label it
with ""flaky-test"" so that we can find it later.


"
Yana Kadiyska <yana.kadiyska@gmail.com>,"Fri, 17 Jul 2015 11:19:17 -0400",Re: Apache gives exception when running groupby on df temp table,nipun <ibnipun10@gmail.com>,"I think that might be a connector issue. You say you are using Spark 1.4,
are you also using 1.4 version of the Spark-cassandra-connector? The do
have some bugs around this, e.g.
https://datastax-oss.atlassian.net/browse/SPARKC-195. Also, I see that you
import org.apache.spark.sql.cassandra.CassandraSQLContext and I've seen
some odd things using that class. Things work out a lot better for me if I
create a dataframe like this:


val cassDF = sqlContext.read.format(""org.apache.spark.sql.cassandra"").options(Map(
""table"" -> ""some_table"", ""keyspace"" -> ""myks"")).load

â€‹


,
ception-when-running-groupby-on-df-temp-table-tp13275p13285.html
"
nipun <ibnipun10@gmail.com>,"Fri, 17 Jul 2015 09:57:46 -0700 (MST)",Re: Apache gives exception when running groupby on df temp table,dev@spark.apache.org,"You are right. There are some odd things about this connector. Earlier I got
OOM exception with this connector just because there was a bug in the
connector which transferred only 64 bytes before closing the connection and
now this one
Strangely I copied the data into another data frame and it worked on the new
dataframe 
val dfCol1 = dfCol.limit(dfCol.count.toInt) and now groupby worked 

Can you tel me whats the difference between using cassandrasqlcontext and
the sqlcontext with loading the cassandra driver. I think internally
cassandra is also loading the same driver right?



--

---------------------------------------------------------------------


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Fri, 17 Jul 2015 10:10:28 -0700",Re: Model parallelism with RDD,"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","You can also use checkpoint to truncate the lineage and the data can be
persisted to HDFS. Fundamentally the state of the RDD needs to be saved to
memory or disk if you don't want to repeat the computation.

Thanks
Shivaram


 spark.sql.unsafe.enabled=true
in
 GC time
?
ta
.
aman <
°Ð¿Ð¸ÑÐ°Ð»(Ð°):
at
e.
ze
 does
h
t does
)
"
Burak Yavuz <brkyvz@gmail.com>,"Fri, 17 Jul 2015 10:34:46 -0700",Re: BlockMatrix multiplication,"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Hi Alexander,

Feel free to submit an ""improvement"" JIRA.

Best,
Burak


imensions
e
 an
in
,
s
t
e
d
4d2910f>
ks to
m
00bf/mllib/src/main/scala/org/apache/spark/mllib/linalg/distributed/BlockMatrix.scala#L361
n
n.
l
ly
m>
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Fri, 17 Jul 2015 10:58:59 -0700",Re: Should spark-ec2 get its own repo?,Sean Owen <sowen@cloudera.com>,"Some replies inline


artifacts that are packaged and released together. I agree that marking a
fix version as 1.5 for a change in another repo doesn't make a lot of
sense, but we could just not use fix versions for the EC2 issues ?


have had EC2 scripts be a part of the Spark distribution from a very early
stage (from version 0.5.0 if my git history reading is correct).  So users
will assume that any error with EC2 scripts belong to the Spark project. In
addition almost all the contributions to the EC2 scripts come from Spark
developers and so keeping the issues in the same mailing list / JIRA seems
natural. This I guess again relates to the question of managing issues for
code that isn't part of the Spark release artifact.

I suggest Shivaram owns this, and that amplab/spark-ec2 is used to
there are more comments on this thread. This will at least alleviate some
of the naming confusion over using a repository in mesos and I'll give
Sean, Nick, Matthew commit access to it. I am still not convinced about
moving the issues over though.

Thanks
Shivaram
"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Fri, 17 Jul 2015 17:57:18 +0000",RE: Model parallelism with RDD,"""shivaram@eecs.berkeley.edu"" <shivaram@eecs.berkeley.edu>","Hi Shivaram,

Thank you for the explanation. Is there a direct way to check the length of the lineage i.e. that the computation is repeated?

Best regards, Alexander

From: Shivaram Venkataraman [mailto:shivaram@eecs.berkeley.edu]
Sent: Friday, July 17, 2015 10:10 AM
To: Ulanov, Alexander
Cc: shivaram@eecs.berkeley.edu; dev@spark.apache.org
Subject: Re: Model parallelism with RDD

You can also use checkpoint to truncate the lineage and the data can be persisted to HDFS. Fundamentally the state of the RDD needs to be saved to memory or disk if you don't want to repeat the computation.

Thanks
Shivaram

On Thu, Jul 16, 2015 at 4:59 PM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
Dear Spark developers,

What happens if RDD does not fit into memory and cache would not work in the code below? Will all previous iterations repeated each new iteration within iterative RDD update (as described below)?

Also, could you clarify regarding DataFrame and GC overhead: does setting spark.sql.unsafe.enabled=true removes the GC when persisting/unpersisting the DataFrame?

Best regards, Alexander

From: Ulanov, Alexander
Sent: Monday, July 13, 2015 11:15 AM
To: shivaram@eecs.berkeley.edu<mailto:shivaram@eecs.berkeley.edu>
Cc: dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: RE: Model parallelism with RDD

Below are the average timings for one iteration of model update with RDD  (with cache, as Shivaram suggested):
Model size, RDD[Double].count / time, s
10M 0.585336926
100M 1.767947506
1B 125.6078817

There is a ~100x increase in time while 10x increase in model size (from 100 million to 1 billion of Double). More than half of the time is spent in GC, and this time varies heavily. Two questions:
1)Can I use project Tungstenâ€™s unsafe? Actually, can I reduce the GC time if I use DataFrame instead of RDD and set the Tungsten key: spark.sql.unsafe.enabled=true ?
2) RDD[Double] of one billion elements is 26.1GB persisted (as Spark UI shows). It is around 26 bytes per element. How many bytes is RDD overhead?

The code:
val modelSize = 1000000000
val numIterations = 10
val parallelism = 5
var oldRDD = sc.parallelize(1 to modelSize, parallelism).map(x => 0.1).cache
var newRDD = sc.parallelize(1 to 1, parallelism).map(x => 0.1)
var i = 0
var avgTime = 0.0
while (i < numIterations) {
  val t = System.nanoTime()
  val newRDD = oldRDD.map(x => x * x)
  newRDD.cache
  newRDD.count()
  oldRDD.unpersist(true)
  newRDD.mean
  avgTime += (System.nanoTime() - t) / 1e9
  oldRDD = newRDD
  i += 1
}
println(""Avg iteration time:"" + avgTime / numIterations)

Best regards, Alexander

From: Shivaram Venkataraman [mai015 10:04 PM
To: Ulanov, Alexander
Cc: <shivaram@eecs.bk.apache.org<mailto:dev@spark.apache.org>
Subject: Re: Model parallelism with RDD

Yeah I can see that being the case -- caching implies creating objects that will be stored in memory. So there is a trade-off between storing data in memory but having to garbage collect it later vs. recomputing the data.

Shivaram

On Fri, Jul 10, 2015 at 9:49 PM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
Hi Shivaram,

Thank you for suggestion! If I do .cache and .count, each iteration take much more time, which is spent in GC. Is it normal?

10 Ð¸ÑŽÐ»Ñ 2015 Ð³., Ð² 21:23, Shivaram Vecs.berkeley.edu><mailto:shivaram@eecs.berkeley.edu<mailtohink you need to do `newRDD.cache()` and `newRDD.count` before you do oldRDD.unpersist(true) -- Otherwise it might be recomputing all the previous iterations each time.

Thanks
Shivaram
On Fri, Jul 10, 2015 at 7:44 PM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
Hi,

I am interested how scalable can be the model parallelism within Spark. Suppose, the model contains N weights of type Double and N is so large that does not fit into the memory of a single node. So, we can store the model in RDD[Double] within several nodes. To train the model, one needs to perform K iterations that update all the weights and check the convergence. Then we also need to exchange some weights between the nodes to synchronize the model or update the global state. Iâ€™ve sketched the code that does iterative updates with RDD (without global update yet). Surprisingly, each iteration takes more time than previous as shown below (time in seconds). Could you suggest what is the reason for that? Iâ€™ve checked GC, it does something within few milliseconds.

Configuration: Spark 1.4, 1 master and 5 worker nodes, 5 executors, Intel Xeon 2.2, 16GB RAM each
Iteration 0 time:1.127990986
Iteration 1 time:1.391120414
Iteration 2 time:1.6429691381000002
Iteration 3 time:1.9344402954
Iteration 4 time:2.2075294246999997
Iteration 5 time:2.6328659593
Iteration 6 time:2.7911690492999996
Iteration 7 time:3.0850374104
Iteration 8 time:3.4031050061
Iteration 9 time:3.8826580919

Code:
val modelSize = 1000000000
val numIterations = 10
val parallelizm = 5
var oldRDD = sc.parallelize(1 to modelSize, parallelizm).map(x => 0.1)
var newRDD = sc.parallelize(1 to 1, parallelizm).map(x => 0.1)
var i = 0
while (i < numIterations) {
  val t = System.nanoTime()
  // updating the weights
  val newRDD = oldRDD.map(x => x * x)
  oldRDD.unpersist(true)
  // â€œcheckingâ€ convergence
  newRDD.mean
  println(""Iteration "" + i + "" time:"" + (System.nanoTime() - t) / 1e9 / numIterations)
  oldRDD = newRDD
  i += 1
}


Best regards, Alexander


"
Sean Owen <sowen@cloudera.com>,"Fri, 17 Jul 2015 23:00:47 +0100",Re: Should spark-ec2 get its own repo?,Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"
*shrug* it just seems harder and less natural to use ASF JIRA. What's
the benefit? I agree it's not a big deal either way but it's a small
part of the problem we're solving in the first place. I suspect that
one way or the other, there would be issues filed both places, so this
probably isn't worth debating.



Yeah good question -- Github doesn't give you a mailing list. I think
dev@ would still be where it's discussed which is ... again 'part of
the problem' but as you say, probably beneficial. It's a pretty low
traffic topic anyway.



I won't move the issues. Maybe time tells whether one approach is
better, or that it just doesn't matter.

However it'd be a great opportunity to review and clear stale EC2 issues.

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Sat, 18 Jul 2015 00:48:52 -0700",[discuss] Removing individual commit messages from the squash commit message,"""dev@spark.apache.org"" <dev@spark.apache.org>","I took a look at the commit messages in git log -- it looks like the
individual commit messages are not that useful to include, but do make the
commit messages more verbose. They are usually just a bunch of extremely
concise descriptions of ""bug fixes"", ""merges"", etc:

    cb3f12d [xxx] add whitespace
    6d874a6 [xxx] support pyspark for yarn-client

    89b01f5 [yyy] Update the unit test to add more cases
    275d252 [yyy] Address the comments
    7cc146d [yyy] Address the comments
    2624723 [yyy] Fix rebase conflict
    45befaa [yyy] Update the unit test
    bbc1c9c [yyy] Fix checkpointing doesn't retain driver port issue


Anybody against removing those from the merge script so the log looks
cleaner? If nobody feels strongly about this, we can just create a JIRA to
remove them, and only keep the author names.
"
Sean Owen <sowen@cloudera.com>,"Sat, 18 Jul 2015 09:35:29 +0100","Re: [discuss] Removing individual commit messages from the squash
 commit message",Reynold Xin <rxin@databricks.com>,"+1 to removing them. Sometimes there are 50+ commits because people
have been merging from master into their branch rather than rebasing.


---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Sat, 18 Jul 2015 03:32:22 -0700",Re: [discuss] Removing individual commit messages from the squash commit message,Sean Owen <sowen@cloudera.com>,"+1 to removing commit messages. 




---------------------------------------------------------------------


"
Silas Davis <silas@silasdavis.net>,"Sat, 18 Jul 2015 16:24:45 +0000",Writing to multiple outputs in Spark,dev@spark.apache.org,"*tl;dr hadoop and cascading* *provide ways of writing tuples to multiple
output files based on key, but the plain RDD interface doesn't seem to and
it should.*

I have been looking into ways to write to multiple outputs in Spark. It
seems like a feature that is somewhat missing from Spark.

The idea is to partition output and write the elements of an RDD to
different locations depending based on the key. For example in a pair RDD
your key may be (language, date, userId) and you would like to write
separate files to $someBasePath/$language/$date. Then there would be  a
version of saveAsHadoopDataset that would be able to multiple location
based on key using the underlying OutputFormat. Perahps it would take a
pair RDD with keys ($partitionKey, $realKey), so for example ((language,
date), userId).

The prior art I have found on this is the following.

Using SparkSQL:
The 'partitionBy' method of DataFrameWriter:
https://spark.apache.org/docs/1.4.0/api/scala/index.html#org.apache.spark.sql.DataFrameWriter

This only works for parquet at the moment.

Using Spark/Hadoop:
This pull request (with the hadoop1 API,) :
https://github.com/apache/spark/pull/4895/files.

This uses MultipleTextOutputFormat (which in turn uses
MultipleOutputFormat) which is part of the old hadoop1 API. It only works
for text but could be generalised for any underlying OutputFormat by using
MultipleOutputFormat (but only for hadoop1 - which doesn't support
ParquetAvroOutputFormat for example)

This gist (With the hadoop2 API):
https://gist.github.com/mlehman/df9546f6be2e362bbad2

This uses MultipleOutputs (available for both the old and new hadoop APIs)
and extends saveAsNewHadoopDataset to support multiple outputs. Should work
for any underlying OutputFormat. Probably better implemented by extending
saveAs[NewAPI]HadoopDataset.

In Cascading:
Cascading provides PartititionTap:
http://docs.cascading.org/cascading/2.5/javadoc/cascading/tap/local/PartitionTap.html
to do this

So my questions are: is there a reason why Spark doesn't provide this? Does
Spark provide similar functionality through some other mechanism? How would
it be best implemented?

Since I started composing this message I've had a go at writing an wrapper
OutputFormat that writes multiple outputs using hadoop MultipleOutputs but
doesn't require modification of the PairRDDFunctions. The principle is
similar however. Again it feels slightly hacky to use dummy fields for the
ReduceContextImpl, but some of this may be a part of the impedance mismatch
between Spark and plain Hadoop... Here is my attempt:
https://gist.github.com/silasdavis/d1d1f1f7ab78249af462

I'd like to see this functionality in Spark somehow but invite suggestion
of how best to achieve it.

Thanks,
Silas
"
Mridul Muralidharan <mridul@gmail.com>,"Sat, 18 Jul 2015 10:25:10 -0700","If gmail, check sparm",dev@spark.apache.org,"https://plus.google.com/+LinusTorvalds/posts/DiG9qANf5PA

I have noticed a bunch of mails from dev@ and github going to spam -
including spark maliing list.
Might be a good idea for dev, committers to check if they are missing
things in their spam folder if on gmail.

Regards,
Mridul

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Sat, 18 Jul 2015 10:40:09 -0700",Re: Expression.resolved unmatched with the correct values in catalyst?,Takeshi Yamamuro <linguin.m.s@gmail.com>,"What if you move your addition to before line 64 (in master branch there is
case for if e.checkInputDataTypes().isFailure):

          case c: Cast if !c.resolved =>

Cheers


"
Patrick Wendell <pwendell@gmail.com>,"Sat, 18 Jul 2015 14:44:40 -0700","Re: [discuss] Removing individual commit messages from the squash
 commit message",Ted Yu <yuzhihong@gmail.com>,"+1 from me too


---------------------------------------------------------------------


"
Ram Sriharsha <sriharsha.ram@gmail.com>,"Sat, 18 Jul 2015 14:54:14 -0700",Re: [discuss] Removing individual commit messages from the squash commit message,Patrick Wendell <pwendell@gmail.com>,"+1 

Sent from my iPhone

e:
he
y
 to

---------------------------------------------------------------------


"
Mridul Muralidharan <mridul@gmail.com>,"Sat, 18 Jul 2015 15:45:53 -0700","Re: [discuss] Removing individual commit messages from the squash
 commit message",Reynold Xin <rxin@databricks.com>,"Just to clarify, the proposal is to have a single commit msg giving the
jira and pr id?
That sounds like a good change to have.

Regards
Mridul


"
Reynold Xin <rxin@databricks.com>,"Sat, 18 Jul 2015 15:48:09 -0700","Re: [discuss] Removing individual commit messages from the squash
 commit message",Mridul Muralidharan <mridul@gmail.com>,"A single commit message consisting of:

1. Pull request title (which includes JIRA number and component, e.g.
[SPARK-1234][MLlib])

2. Pull request description

3. List of authors contributing to the patch

The main thing that changes is 3: we used to also include the individual
commits to the pull request branch that are squashed.



"
Mridul Muralidharan <mridul@gmail.com>,"Sat, 18 Jul 2015 16:00:32 -0700","Re: [discuss] Removing individual commit messages from the squash
 commit message",Reynold Xin <rxin@databricks.com>,"Thanks for detailing, definitely sounds better.
+1

Regards
Mridul


"
Ted Yu <yuzhihong@gmail.com>,"Sat, 18 Jul 2015 16:28:50 -0700","Re: If gmail, check sparm",Mridul Muralidharan <mridul@gmail.com>,"Interesting read. 

I did find a lot of Spark mails in Spam folder. 

Thanks Mridul 




---------------------------------------------------------------------


"
Dogtail Ray <spark.rui92@gmail.com>,"Sat, 18 Jul 2015 21:47:18 -0400",Dynamic resource allocation in Standalone mode,dev@spark.apache.org,"Hi all,

I am planning to dynamically increase or decrease the number of executors
allocated to an application during runtime, and it is similar to dynamic
resource allocation, which is only feasible in Spark on Yarn mode. Any
suggestions on how to implement this feature in Standalone mode?

My current problem is: I want to send a ADD_EXECUTOR command from scheduler
module (in CoarseGrainedSchedulerBackend.scala) to deploy module (in
Master.scala), but don't know how to communicate between the two
modules.... Great thanks for any suggestions!
"
Sean Owen <sowen@cloudera.com>,"Sun, 19 Jul 2015 10:32:36 +0100",Re: Foundation policy on releases and Spark nightly builds,dev <dev@spark.apache.org>,"I am going to make an edit to the download page on the web site to
start, as that much seems uncontroversial. Proposed change:

Reorder sections to put developer-oriented sections at the bottom,
including the info on nightly builds:
  Download Spark
  Link with Spark
  All Releases
  Spark Source Code Management
  Nightly Builds

Change text to emphasize the audience:

Packages are built regularly off of Sparkâ€™s master branch and release
branches. These provide *Spark developers* access to the bleeding-edge
of Spark master or the most recent fixes not yet incorporated into a
maintenance release. *They should not be used by anyone except Spark
developers, and may be unstable or have serious bugs. End users should
only use official releases above. Please subscribe to
dev@spark.apache.org if you are a Spark developer to be aware of
issues in nightly builds.* Spark nightly packages are available at:


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Sun, 19 Jul 2015 09:26:29 -0700",Re: Foundation policy on releases and Spark nightly builds,Sean Owen <sowen@cloudera.com>,"Hey Sean,

nightly builds to the wiki and just have header called ""Nightly
builds"" at the end of the downloads page that says ""For developers,
Spark maintains nightly builds. More information is available on the
[Spark developer Wiki](link)"". I think this would preserve
discoverability while also placing the information on the wiki, which
seems to be the main ask of the policy.

- Patrick

ease

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Sun, 19 Jul 2015 09:51:37 -0700",Re: Foundation policy on releases and Spark nightly builds,Sean Busbey <busbey@cloudera.com>,"Sean B.,

Thank you for giving a thorough reply. I will work with Sean O. and
see what we can change to make us more in line with the stated policy.

I did some research and it appears that some time between October [1]
and December [2] 2006, this page was modified to include stricter
policy surrounding nightly builds. Actually, the original version of
the policy page encouraged projects to post nightly builds for the
benefit of all developers, just as we have been doing.

If you detect frustration from the Spark community, it's because this
type of situation occurs with some regularity. In this case:

(a) A policy exists from ~10 years ago, presumably because some
project back then had problematic release management practices and so
a ""policy"" needed to be created to solve a problem.
(b) The policy is outdated now, and no one is 100% sure why it was
created (likely many of the people are no longer involved in the ASF
who helped craft it).
(c) The steps for how to change it are unclear and there isn't clear
ownership of the policy document.

I think it's unavoidable given the decentralized organization
structure of the ASF, but I just want to be up front about our
perspective and why you might sense some frustration.

[1] https://web.archive.org/web/20061020220358/http://www.apache.org/dev/release.html
[2] https://web.archive.org/web/20061231050046/http://www.apache.org/dev/release.html

- Patrick


---------------------------------------------------------------------


"
Sandy Ryza <sandy.ryza@cloudera.com>,"Sun, 19 Jul 2015 10:24:33 -0700","Re: [discuss] Removing individual commit messages from the squash
 commit message",Mridul Muralidharan <mridul@gmail.com>,1
=?UTF-8?B?0KHQtdGA0LPQtdC5INCb0LjRhdC+0LzQsNC9?= <serglihoman@gmail.com>,"Sun, 19 Jul 2015 20:40:23 +0300",Compact RDD representation,dev@spark.apache.org,"Hi,

I am looking for suitable issue for Master Degree project(it sounds like
scalability problems and improvements for spark streaming) and seems like
introduction of grouped RDD(for example: don't store
""Spark"", ""Spark"", ""Spark"", instead store (""Spark"", 3)) can:

1. Reduce memory needed for RDD (roughly, used memory will be:  % of uniq
messages)
2. Improve performance(no need to apply function several times for the same
message).

Can I create ticket and introduce API for grouped RDDs? Is it make sense?
Also I will be very appreciated for critic and ideas
"
Sandy Ryza <sandy.ryza@cloudera.com>,"Sun, 19 Jul 2015 11:02:58 -0700",Re: Compact RDD representation,=?UTF-8?B?0KHQtdGA0LPQtdC5INCb0LjRhdC+0LzQsNC9?= <serglihoman@gmail.com>,"This functionality already basically exists in Spark.  To create the
""grouped RDD"", one can run:

  val groupedRdd = rdd.reduceByKey(_ + _)

To get it back into the original form:

  groupedRdd.flatMap(x => List.fill(x._1)(x._2))

-Sandy

-Sandy

›Ð¸Ñ…Ð¾Ð¼Ð°Ð½ <serglihoman@gmail.com>

"
Sandy Ryza <sandy.ryza@cloudera.com>,"Sun, 19 Jul 2015 11:04:22 -0700",Re: Compact RDD representation,=?UTF-8?B?0KHQtdGA0LPQtdC5INCb0LjRhdC+0LzQsNC9?= <serglihoman@gmail.com>,"Edit: the first line should read:

  val groupedRdd = rdd.map((_, 1)).reduceByKey(_ + _)


Ð›Ð¸Ñ…Ð¾Ð¼Ð°Ð½ <serglihoman@gmail.com>
e
q
?
"
=?UTF-8?B?0KHQtdGA0LPQtdC5INCb0LjRhdC+0LzQsNC9?= <serglihoman@gmail.com>,"Sun, 19 Jul 2015 21:09:45 +0300",Re: Compact RDD representation,Sandy Ryza <sandy.ryza@cloudera.com>,"Thanks for answer! Could you please answer for one more question? Will we
have in memory original rdd and grouped rdd in the same time?

2015-07-19 21:04 GMT+03:00 Sandy Ryza <sandy.ryza@cloudera.com>:

Ð›Ð¸Ñ…Ð¾Ð¼Ð°Ð½ <serglihoman@gmail.com>
e
ke
"
Sandy Ryza <sandy.ryza@cloudera.com>,"Sun, 19 Jul 2015 11:26:40 -0700",Re: Compact RDD representation,=?UTF-8?B?0KHQtdGA0LPQtdC5INCb0LjRhdC+0LzQsNC9?= <serglihoman@gmail.com>,"The user gets to choose what they want to reside in memory.  If they call
rdd.cache() on the original RDD, it will be in memory.  If they call
rdd.cache() on the compact RDD, it will be in memory.  If cache() is called
on both, they'll both be in memory.

-Sandy

›Ð¸Ñ…Ð¾Ð¼Ð°Ð½ <serglihoman@gmail.com>

Ð›Ð¸Ñ…Ð¾Ð¼Ð°Ð½ <serglihoman@gmail.com>
ems
"
=?UTF-8?B?0KHQtdGA0LPQtdC5INCb0LjRhdC+0LzQsNC9?= <serglihoman@gmail.com>,"Sun, 19 Jul 2015 21:41:01 +0300",Re: Compact RDD representation,Sandy Ryza <sandy.ryza@cloudera.com>,"Sorry, maybe I am saying something completely wrong...  we have a stream,
we digitize it to created rdd. rdd in this case will be just array of any.
than we apply transformation to create new grouped rdd and GC should remove
original rdd from memory(if we won't persist it). Will we have GC step in  val
groupedRdd = rdd.map((_, 1)).reduceByKey(_ + _) ? my suggestion is to
remove creation and reclaiming of unneeded rdd and create already grouped
one

2015-07-19 21:26 GMT+03:00 Sandy Ryza <sandy.ryza@cloudera.com>:

ed
Ð›Ð¸Ñ…Ð¾Ð¼Ð°Ð½ <serglihoman@gmail.com>
 Ð›Ð¸Ñ…Ð¾Ð¼Ð°Ð½ <serglihoman@gmail.com
eems
e
"
Sandy Ryza <sandy.ryza@cloudera.com>,"Sun, 19 Jul 2015 11:46:18 -0700",Re: Compact RDD representation,=?UTF-8?B?0KHQtdGA0LPQtdC5INCb0LjRhdC+0LzQsNC9?= <serglihoman@gmail.com>,"In the Spark model, constructing an RDD does not mean storing all its
contents in memory.  Rather, an RDD is a description of a dataset that
enables iterating over its contents, record by record (in parallel).  The
only time the full contents of an RDD are stored in memory is when a user
explicitly calls ""cache"" or ""persist"" on it.

-Sandy

›Ð¸Ñ…Ð¾Ð¼Ð°Ð½ <serglihoman@gmail.com>

.
ve
  val
l
led
Ð›Ð¸Ñ…Ð¾Ð¼Ð°Ð½ <serglihoman@gmail.com>
¹ Ð›Ð¸Ñ…Ð¾Ð¼Ð°Ð½ <
seems
"
=?UTF-8?B?SnVhbiBSb2Ryw61ndWV6IEhvcnRhbMOh?= <juan.rodriguez.hortala@gmail.com>,"Sun, 19 Jul 2015 20:56:05 +0200",Re: Compact RDD representation,Sandy Ryza <sandy.ryza@cloudera.com>,"Hi,

My two cents is that that could be interesting if all RDD and pair
RDD operations would be lifted to work on groupedRDD. For example as
suggested a map on grouped RDDs would be more efficient if the original RDD
had lots of duplicate entries, but for RDDs with little repetitions I guess
you in fact lose efficiency. The same applies to filter, sortBy, count,
max, ... but for example I guess there is no gain for reduce and other
operations. Also note the order is lost when passing to grouped RDD, so the
semantics is not exactly the same, but would be good enough for
many applications. Also I would look for suitable use cases where RDD with
many repetitions arise naturally, and the transformations with performance
gain like map are used often, and I would do some experiments to compare
performance between a computation with grouped RDD and the same computation
without grouping, for different input sizes


El domingo, 19 de julio de 2015, Sandy Ryza <sandy.ryza@cloudera.com>
escribiÃ³:

Ð›Ð¸Ñ…Ð¾Ð¼Ð°Ð½ <serglihoman@gmail.com
e
q
?
"
=?UTF-8?B?0KHQtdGA0LPQtdC5INCb0LjRhdC+0LzQsNC9?= <serglihoman@gmail.com>,"Sun, 19 Jul 2015 22:49:10 +0300",Re: Compact RDD representation,=?UTF-8?B?SnVhbiBSb2Ryw61ndWV6IEhvcnRhbMOh?= <juan.rodriguez.hortala@gmail.com>,"Hi Juan,

It's exactly what I meant. if we will have high load with many repetitions it
can significantly reduce rdd size and improve performance. in real use
cases application frequently need to enrich data from cache or external
system, so we will save time on each repetition.
I will also do some experiments.  About little repetitions: in what use
cases we will lose efficiency? it will also test it.
What I need to do this commitment? Just create ticket in Jira?



2015-07-19 21:56 GMT+03:00 Juan RodrÃ­guez HortalÃ¡ <
juan.rodriguez.hortala@gmail.com>:

DD
ss
he
h
e
on
Ð›Ð¸Ñ…Ð¾Ð¼Ð°Ð½ <serglihoman@gmail.com>
e
ke
"
Ted Yu <yuzhihong@gmail.com>,"Sun, 19 Jul 2015 17:32:40 -0700",KinesisStreamSuite failing in master branch,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,
I noticed that KinesisStreamSuite fails for both hadoop profiles in master
Jenkins builds.

From
https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-Maven-with-YARN/HADOOP_PROFILE=hadoop-2.4,label=centos/3011/console
:

KinesisStreamSuite:*** RUN ABORTED ***  java.lang.AssertionError:
assertion failed: Kinesis test not enabled, should not attempt to get
AWS credentials  at scala.Predef$.assert(Predef.scala:179)  at
org.apache.spark.streaming.kinesis.KinesisTestUtils$.getAWSCredentials(KinesisTestUtils.scala:189)
 at org.apache.spark.streaming.kinesis.KinesisTestUtils.org$apache$spark$streaming$kinesis$KinesisTestUtils$$kinesisClient$lzycompute(KinesisTestUtils.scala:59)
 at org.apache.spark.streaming.kinesis.KinesisTestUtils.org$apache$spark$streaming$kinesis$KinesisTestUtils$$kinesisClient(KinesisTestUtils.scala:58)
 at org.apache.spark.streaming.kinesis.KinesisTestUtils.describeStream(KinesisTestUtils.scala:121)
 at org.apache.spark.streaming.kinesis.KinesisTestUtils.findNonExistentStreamName(KinesisTestUtils.scala:157)
 at org.apache.spark.streaming.kinesis.KinesisTestUtils.createStream(KinesisTestUtils.scala:78)
 at org.apache.spark.streaming.kinesis.KinesisStreamSuite.beforeAll(KinesisStreamSuite.scala:45)
 at org.scalatest.BeforeAndAfterAll$class.beforeAll(BeforeAndAfterAll.scala:187)
 at org.apache.spark.streaming.kinesis.KinesisStreamSuite.beforeAll(KinesisStreamSuite.scala:33)


FYI
"
"""Huang, Jie"" <jie.huang@intel.com>","Mon, 20 Jul 2015 00:51:29 +0000",[SparkScore] Performance portal for Apache Spark - WW29,"user <user@spark.apache.org>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","Performance Portal for Apache Spark
Description
________________________________
Each data point represents each workload runtime percent compared with the previous week. Different lines represents different workloads running on spark yarn-client mode.
Hardware
________________________________
CPU type: Intel(r) Xeon(r) CPU E5-2697 v2 @ 2.70GHz
Memory: 128GB
NIC: 10GbE
Disk(s): 8 x 1TB SATA HDD
Software
________________________________
JAVA version: 1.8.0_25
Hadoop version: 2.5.0-CDH5.3.2
HiBench version: 4.0
Spark on yarn-client mode
Cluster
________________________________
1 node for Master
10 nodes for Slave
Regular
Summary
The lower percent the better performance.
________________________________
Group

ww22

ww23

ww24

ww25

ww26

ww27

ww28

ww29

HiBench

6.0%

7.9%

-6.5%

-3.1%

-2.1%

-6.4%

-2.7%

-0.7%

spark-perf

-1.8%

4.1%

-4.7%

-4.6%

-5.4%

-4.6%

-12.8%

-12.5%

[http://01org.github.io/sparkscore/image/plaf1.time/overall.png]
Y-Axis: normalized completion time; X-Axis: Work Week.
The commit number can be found in the result table.
The performance score for each workload is normalized based on the elapsed time for 1.2 release. The lower the better.
Detail
________________________________
HiBench
________________________________
JOB

ww22

ww23

ww24

ww25

ww26

ww27

ww28

ww29

commit

530efe3e

90c60692

db81b9d8

4eb48ed1

32e3cdaa

ec784381

2b820f2a

c472eb17

sleep

-2.1%

-2.9%

-4.1%

12.8%

-5.1%

-4.5%

-3.1%

-0.7%

wordcount

8.0%

8.3%

-18.6%

-10.9%

6.9%

-12.9%

-10.0%

-9.2%

kmeans

72.1%

92.9%

86.9%

95.8%

123.3%

99.3%

127.9%

102.6%

scan

%

-1.1%

-25.5%

-21.0%

-12.4%

-19.8%

-19.7%

-20.5%

bayes

-18.3%

-11.1%

-29.7%

-31.3%

-30.9%

-31.1%

-31.0%

-30.1%

aggregation

%

9.2%

-15.3%

-15.0%

-37.6%

-37.0%

-37.3%

7.6%

join

%

1.0%

-12.7%

-13.9%

-16.4%

-17.8%

-14.8%

-13.2%

sort

-11.9%

-12.5%

-17.5%

-17.3%

-20.7%

-17.7%

-13.9%

-15.6%

pagerank

4.0%

2.9%

-11.4%

-13.0%

-11.4%

-10.1%

-12.0%

-11.7%

terasort

-9.5%

-7.3%

-16.7%

-17.0%

-16.3%

-11.9%

-13.1%

-15.7%

Comments: null means no such workload running or workload failed in this time.
[http://01org.github.io/sparkscore/image/plaf1.time/HiBench_workloads.png]
Y-Axis: normalized completion time; X-Axis: Work Week.
The commit number can be found in the result table.
The performance score for each workload is normalized based on the elapsed time for 1.2 release. The lower the better.
spark-perf
________________________________
JOB

ww22

ww23

ww24

ww25

ww26

ww27

ww28

ww29

commit

530efe3e

90c60692

db81b9d8

4eb48ed1

32e3cdaa

ec784381

2b820f2a

c472eb17

agg

%

18.3%

5.2%

2.5%

1.1%

3.0%

-18.8%

-19.0%

agg-int

%

9.6%

4.0%

8.2%

7.0%

7.5%

6.2%

11.4%

agg-naive

%

-0.8%

-6.7%

-6.8%

-8.5%

-6.9%

-15.5%

-18.0%

scheduling

-14.5%

-2.1%

-6.4%

-6.5%

-5.7%

-1.8%

-6.0%

-9.7%

count-filter

6.6%

6.8%

-10.2%

-10.4%

-9.8%

-10.4%

-18.0%

-17.4%

count

6.7%

8.0%

-7.3%

-7.0%

-8.0%

-7.4%

-15.1%

-14.3%

sort

-6.2%

-7.0%

-14.6%

-14.4%

-13.9%

-15.9%

-24.0%

-23.2%

sort-int

-1.6%

-0.1%

-1.5%

-2.2%

-5.3%

-5.0%

-11.3%

-9.6%

Comments: null means no such workload running or workload failed in this time.
[http://01org.github.io/sparkscore/image/plaf1.time/spark-perf_workloads.png]
Y-Axis: normalized completion time; X-Axis: Work Week.
The commit number can be found in the result table.
The performance score for each workload is normalized based on the elapsed time for 1.2 release. The lower the better.
"
Josh Rosen <rosenville@gmail.com>,"Sun, 19 Jul 2015 18:03:53 -0700",Re: KinesisStreamSuite failing in master branch,Ted Yu <yuzhihong@gmail.com>,"Yep, I emailed TD about it; I think that we may need to make a change to
the pull request builder to fix this.  Pending that, we could just revert
the commit that added this.


r
ADOOP_PROFILE=hadoop-2.4,label=centos/3011/console
on failed: Kinesis test not enabled, should not attempt to get AWS credentials  at scala.Predef$.assert(Predef.scala:179)  at org.apache.spark.streaming.kinesis.KinesisTestUtils$.getAWSCredentials(KinesisTestUtils.scala:189)  at org.apache.spark.streaming.kinesis.KinesisTestUtils.org$apache$spark$streaming$kinesis$KinesisTestUtils$$kinesisClient$lzycompute(KinesisTestUtils.scala:59)  at org.apache.spark.streaming.kinesis.KinesisTestUtils.org$apache$spark$streaming$kinesis$KinesisTestUtils$$kinesisClient(KinesisTestUtils.scala:58)  at org.apache.spark.streaming.kinesis.KinesisTestUtils.describeStream(KinesisTestUtils.scala:121)  at org.apache.spark.streaming.kinesis.KinesisTestUtils.findNonExistentStreamName(KinesisTestUtils.scala:157)  at org.apache.spark.streaming.kinesis.KinesisTestUtils.createStream(KinesisTestUtils.scala:78)  at org.apache.spark.streaming.kinesis.KinesisStreamSuite.beforeAll(KinesisStreamSuite.scala:45)  at org.scalatest.BeforeAndAfterAll$class.beforeAll(BeforeAndAfterAll.scala:187)  at org.apache.spark.streaming.kinesis.KinesisStreamSuite.beforeAll(KinesisStreamSuite.scala:33)
"
Patrick Wendell <pwendell@gmail.com>,"Sun, 19 Jul 2015 18:08:49 -0700",Re: KinesisStreamSuite failing in master branch,Josh Rosen <rosenville@gmail.com>,"I think we should just revert this patch on all affected branches. No
reason to leave the builds broken until a fix is in place.

- Patrick


---------------------------------------------------------------------


"
Tathagata Das <tdas@databricks.com>,"Sun, 19 Jul 2015 18:41:34 -0700",Re: KinesisStreamSuite failing in master branch,Patrick Wendell <pwendell@gmail.com>,"I am taking care of this right now.


"
Tathagata Das <tdas@databricks.com>,"Sun, 19 Jul 2015 19:21:44 -0700",Re: KinesisStreamSuite failing in master branch,Patrick Wendell <pwendell@gmail.com>,"The PR to fix this is out.
https://github.com/apache/spark/pull/7519



"
Jerry Lam <chilinglam@gmail.com>,"Sun, 19 Jul 2015 22:57:05 -0400",Re: Spark Mesos Dispatcher,"""Jahagirdar, Madhu"" <madhu.jahagirdar@philips.com>","Yes. 

Sent from my iPhone

 example we can run drivers with Spark 1.3 and Spark 1.4 at the same time ?
rotected under applicable law. The message is intended solely for the addressee(s). If you are not the intended recipient, you are hereby notified that any use, forwarding, dissemination, or reproduction of this message is strictly prohibited and may be unlawful. If you are not the intended recipient, please contact the sender by return e-mail and destroy all copies of the original message.
"
Jerry Lam <chilinglam@gmail.com>,"Mon, 20 Jul 2015 00:23:43 -0400",Re: Spark Mesos Dispatcher,"""Jahagirdar, Madhu"" <madhu.jahagirdar@philips.com>","I only used client mode both 1.3 and 1.4 versions on mesos.
I skimmed through
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/mesos/MesosClusterDispatcher.scala.
I would actually backport the Cluster Mode feature. Sorry, I don't have an
answer for this.



"
Sean Owen <sowen@cloudera.com>,"Mon, 20 Jul 2015 08:22:51 +0100",Re: Foundation policy on releases and Spark nightly builds,Patrick Wendell <pwendell@gmail.com>,"This is done, and yes I believe that resolves the issue as far all here know.

http://spark.apache.org/downloads.html
->
https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools#UsefulDeveloperTools-NightlyBuilds


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Mon, 20 Jul 2015 00:24:56 -0700",Re: Foundation policy on releases and Spark nightly builds,Sean Owen <sowen@cloudera.com>,"Thanks, Sean.



"
Manoj Kumar <manojkumarsivaraj334@gmail.com>,"Mon, 20 Jul 2015 12:58:51 +0530","Re: [discuss] Removing individual commit messages from the squash
 commit message",dev <dev@spark.apache.org>,"+1

Sounds like a great idea.




-- 
Godspeed,
Manoj Kumar,
http://manojbits.wordpress.com
<http://goog_1017110195>
http://github.com/MechCoder
"
Andrew Or <andrew@databricks.com>,"Mon, 20 Jul 2015 01:31:44 -0700",Re: Dynamic resource allocation in Standalone mode,Dogtail Ray <spark.rui92@gmail.com>,"Hi Ray,

In standalone mode, you have this thing called the
SparkDeploySchedulerBackend, which has this thing called the AppClient.
This is the thing on the driver side that already talks to the Master to
register the application.

As for dynamic allocation in standalone mode, I literally *just* created a
patch on Github: https://github.com/apache/spark/pull/7532. Feel free to
have a look if you're interested. :)

-Andrew

2015-07-18 18:47 GMT-07:00 Dogtail Ray <spark.rui92@gmail.com>:

"
=?UTF-8?B?SnVhbiBSb2Ryw61ndWV6IEhvcnRhbMOh?= <juan.rodriguez.hortala@gmail.com>,"Mon, 20 Jul 2015 10:31:37 +0200",Re: Compact RDD representation,=?UTF-8?B?0KHQtdGA0LPQtdC5INCb0LjRhdC+0LzQsNC9?= <serglihoman@gmail.com>,"Hi,

I'm not an authority in the Spark community, but what I would do is adding
the project to spark packages http://spark-packages.org/. In fact I think
this case is similar to IndexedRDD, which is also in spark packages
http://spark-packages.org/package/amplab/spark-indexedrdd

2015-07-19 21:49 GMT+02:00 Ð¡ÐµÑ€Ð³ÐµÐ¹ Ð›Ð¸Ñ…Ð¾Ð¼Ð°Ð½ <serglihoman@gmail.com>:

s it
RDD
ess
the
th
ce
ion
Ð›Ð¸Ñ…Ð¾Ð¼Ð°Ð½ <serglihoman@gmail.com>
ems
"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Mon, 20 Jul 2015 11:28:49 +0200",countByValue on dataframe with multiple columns,dev <dev@spark.apache.org>,"Hi,
Is there any plan to add the countByValue function to Spark SQL Dataframe ?
Even
https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/feature/StringIndexer.scala#L78
is using the RDD part right now, but for ML purposes, being able to get the
most frequent categorical value on multiple columns would be very useful.


Regards,


-- 
*Olivier Girardot* | AssociÃ©
o.girardot@lateral-thoughts.com
+33 6 24 09 17 94
"
Eugene Morozov <fathersson@list.ru>,"Mon, 20 Jul 2015 13:53:57 +0300",Re: KryoSerializer gives class cast exception,Josh Rosen <rosenville@gmail.com>,"Josh, thanks for the reply.

So, it looks like despite the progress there is no other way as to fork and fix the chill itself. It indeed doesn’t compile with kryo 2.24.0, but it wasn’t that hard to fix (looks like I’ve just guessed the right code), although there are test failures now.


that the Chill dependency is one of the main blockers to upgrading Kryo, but I don't think that it's insurmountable: if necessary, we could just publish our own forked version of Chill under our own namespace, similar to what we used to do with Pyrolite.
Kryo versions.
https://issues.apache.org/jira/browse/SPARK-7708 for some more previous discussions RE: Kryo upgrade.
to link to some previous context / discussions.
instead of Java one.
algo was building such an objects and then storing them into external storage. It was not required to reshuffle partitions. Now, it seems I have to reshuffle them, but I’m stuck with ClassCastException. I investigated it a little and it seems to me that KryoSerializer does not clear it’s state at some point, so it tries to use StringSerializer for my non String object. My objects are pretty complex, it’d be pretty hard to make them serializable.
com.company.metadata.model.cleanse.CleanseInfoSequence cannot be cast to java.lang.String
com.esotericsoftware.kryo.serializers.DefaultSerializers$StringSerializer.write(DefaultSerializers.java:146)
com.esotericsoftware.kryo.Kryo.writeObjectOrNull(Kryo.java:549)
com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:68)
com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:18)
com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.write(FieldSerializer.java:564)
after 2.21 (current kryo version in spark). But spark cannot update, because of chill and chill cannot be updated because of some dependencies on their side. So, spark sort of stuck with kryo version 2.21.
some point it’be required.
serialization forever.
I’m not sure it’s possible to register my class with custom serializer in kryo.

Eugene Morozov
fathersson@list.ru




"
Jonathan Winandy <jonathan.winandy@gmail.com>,"Mon, 20 Jul 2015 14:18:01 +0200",Re: countByValue on dataframe with multiple columns,Olivier Girardot <o.girardot@lateral-thoughts.com>,"Ahoy !

Maybe you can get countByValue by using sql.GroupedData :

// some DFval df: DataFrame =
sqlContext.createDataFrame(sc.parallelize(List(""A"",""B"", ""B"",
""A"")).map(Row.apply(_)), StructType(List(StructField(""n"",
StringType))))


df.groupBy(""n"").count().show()


// generic
def countByValueDf(df:DataFrame) = {

  val (h :: r) = df.columns.toList

  df.groupBy(h, r:_*).count()
}

countByValueDf(df).show()


Cheers,
Jon


 ?
he/spark/ml/feature/StringIndexer.scala#L78
he
"
Ted Yu <yuzhihong@gmail.com>,"Mon, 20 Jul 2015 07:28:56 -0700",Re: KinesisStreamSuite failing in master branch,Tathagata Das <tdas@databricks.com>,"TD:
Thanks for getting the builds back to green.


"
Richard Marscher <rmarscher@localytics.com>,"Mon, 20 Jul 2015 12:09:55 -0400","Re: If gmail, check sparm","""dev@spark.apache.org"" <dev@spark.apache.org>","I've setup filters in my gmail to avoid this. If you have a filter matching
the mailing list(s) you can set a flag in Gmail to never send it to spam.




-- 
*Richard Marscher*
Software Engineer
Localytics
Localytics.com <http://localytics.com/> | Our Blog
<http://localytics.com/blog> | Twitter <http://twitter.com/localytics> |
Facebook <http://facebook.com/localytics> | LinkedIn
<http://www.linkedin.com/company/1148792?trk=tyah>
"
Richard Marscher <rmarscher@localytics.com>,"Mon, 20 Jul 2015 12:56:41 -0400",Worker memory leaks?,dev@spark.apache.org,"Hi,

we have been experiencing issues in production over the past couple weeks
with Spark Standalone Worker JVMs seeming to have memory leaks. They
accumulate Old Gen until it reaches max and then reach a failed state that
starts critically failing some applications running against the cluster.

I've done some exploration of the Spark code base related to Worker in
search of potential sources of this problem and am looking for some
commentary on a couple theories I have:

Observation 1: The `finishedExecutors` HashMap seem to only accumulate new
entries over time unbounded. It only seems to be appended and never
periodically purged or cleaned of older executors in line with something
like the worker cleanup scheduler.
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala#L473

I feel somewhat confident that over time this will exhibit a ""leak"". I
quote it just because it may be intentional to hold these references to
support functionality versus a true leak where you just accidentally hold
onto memory.

Observation 2: I feel much less certain about this, but it seemed like if
the Worker is messaged with `KillExecutor` then it only kills the `
ExecutorRunner` but does not clean it up from the executor map.
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala#L492

I haven't been able to sort out whether I'm missing something indirect
where it before/after cleans that executor from the map. However, if it
does not, then it may be leaking references on this map.

codebase itself. We used to periodically see that our completed
applications had the status of ""Killed"" instead of ""Exited"" for all the
executors. However, now we see every completed application has a final
state of ""Killed"" for all the executors. I might speculatively correlate
this to Observation 2 as a potential reason we have started seeing this
issue more recently.

We also have a larger and increasing workload over the past few weeks and
possibly code changes to the application description that could be
exacerbating these potential underlying issues. We run a lot of smaller
applications per day, something in the range of hundreds to maybe 1000
applications per day with 16 executors per application.

Thanks
-- 
*Richard Marscher*
Software Engineer
Localytics
Localytics.com <http://localytics.com/> | Our Blog
<http://localytics.com/blog> | Twitter <http://twitter.com/localytics> |
Facebook <http://facebook.com/localytics> | LinkedIn
<http://www.linkedin.com/company/1148792?trk=tyah>
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Mon, 20 Jul 2015 12:01:26 -0700",Re: Should spark-ec2 get its own repo?,Sean Owen <sowen@cloudera.com>,"I've created https://github.com/amplab/spark-ec2 and added an initial set
of committers. Note that this is not a fork of the existing
github.com/mesos/spark-ec2 and users will need to fork from here. This is
mostly to avoid the base-fork in pull requests being set incorrectly etc.

I'll be migrating some PRs / closing them in the old repo and will also
update the README in that repo.

Thanks
Shivaram


"
Reynold Xin <rxin@databricks.com>,"Mon, 20 Jul 2015 12:03:20 -0700",Re: Should spark-ec2 get its own repo?,Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Is amplab the right owner, given its ending next year? Maybe we should
create spark-ec2, or spark-project instead?



"
Josh Rosen <joshrosen@databricks.com>,"Mon, 20 Jul 2015 12:20:23 -0700",Re: Worker memory leaks?,Richard Marscher <rmarscher@localytics.com>,"Hi Richard,

Thanks for your detailed investigation of this issue.  I agree with your
observation that the finishedExecutors hashmap is a source of memory leaks
for very-long-lived clusters.  It looks like the finishedExecutors map is
only read when rendering the Worker Web UI and in constructing REST API
responses.  I think that we could address this leak by adding a
configuration to cap the maximum number of retained executors,
applications, etc.  We already have similar caps in the driver UI.  If we
add this configuration, I think that we should pick some sensible default
value rather than an unlimited one.  This is technically a user-facing
behavior change but I think it's okay since the current behavior is to
crash / OOM.

Regarding `KillExecutor`, I think that there might be some asynchrony and
indirection masking the cleanup here.  Based on a quick glance through the
code, it looks like ExecutorRunner's thread will end an
ExecutorStateChanged RPC back to the Worker after the executor is killed,
so I think that the cleanup will be triggered by that RPC.  Since this
isn't clear from reading the code, though, it would be great to add some
comments to the code to explain this, plus a unit test to make sure that
this indirect cleanup mechanism isn't broken in the future.

I'm not sure what's causing the Killed vs Exited issue, but I have one
theory: does the behavior vary based on whether your application cleanly
shuts down the SparkContext via SparkContext.stop()? It's possible that
omitting the stop() could lead to a ""Killed"" exit status, but I don't know
for sure.  (This could probably also be clarified with a unit test).

To my knowledge, the spark-perf suite does not contain the sort of
scale-testing workload that would expose these types of memory leaks; we
have some tests for very long-lived individual applications, but not tests
for long-lived clusters that run thousands of applications between
restarts.  I'm going to create some tickets to add such tests.

I've filed https://issues.apache.org/jira/browse/SPARK-9202 to follow up on
the finishedExecutors leak.

- Josh


"
Michael Segel <msegel_hadoop@hotmail.com>,"Mon, 20 Jul 2015 12:26:40 -0700",Silly question about building Spark 1.4.1,dev@spark.apache.org,"Hi, 

Iâ€™m looking at the online docs for building spark 1.4.1 â€¦ 

http://spark.apache.org/docs/latest/building-spark.html <http://spark.apache.org/docs/latest/building-spark.html> 

I was interested in building spark for Scala 2.11 (latest scala) and also for Hive and JDBC support. 

The docs say:
â€œ
To produce a Spark package compiled with Scala 2.11, use the -Dscala-2.11 property:
dev/change-version-to-2.11.sh
mvn -Pyarn -Phadoop-2.4 -Dscala-2.11 -DskipTests clean package
â€œ 
Soâ€¦ 
Is there a reason I shouldnâ€™t build against hadoop-2.6 ? 

If I want to add the Thirft and Hive support, is it possible? 
Looking at the Scala build, it looks like hive support is being built? 
(Looking at the stdout messagesâ€¦)
Should the docs be updated? Am I missing something? 
(Dean W. can confirm, I am completely brain dead. ;-) 

Thx

-Mike
PS. Yes I can probably download a prebuilt image, but Iâ€™m a glutton for punishment. ;-) 

"
Richard Marscher <rmarscher@localytics.com>,"Mon, 20 Jul 2015 15:37:05 -0400",Re: Worker memory leaks?,Josh Rosen <joshrosen@databricks.com>,"Hi,

thanks for the follow up. You are right regarding the invalidation of
observation #2. I later realized the Worker UI page directly displays the
entries in the executors map and can see in our production UI it's in a
proper state.

As for the Killed vs Exited, it's less relevant now since the theory about
the executors map is invalid. However to answer your question, the current
setup is that the SparkContext lifecycle encapsulates exactly one
application. That is we create a single context per application submitted
and close it upon success/failure completion of the application.

Thanks,




-- 
*Richard Marscher*
Software Engineer
Localytics
Localytics.com <http://localytics.com/> | Our Blog
<http://localytics.com/blog> | Twitter <http://twitter.com/localytics> |
Facebook <http://facebook.com/localytics> | LinkedIn
<http://www.linkedin.com/company/1148792?trk=tyah>
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Mon, 20 Jul 2015 12:44:17 -0700",Re: Should spark-ec2 get its own repo?,Reynold Xin <rxin@databricks.com>,"Technically I think the project ends in 2017 and I think we will figure out
a transition for AMPLab repositories when the project ends. I think it
should be pretty simple to transfer ownership to a new organization if /
when the time comes around.

Thanks
Shivaram


"
Mridul Muralidharan <mridul@gmail.com>,"Mon, 20 Jul 2015 12:55:08 -0700",Re: Should spark-ec2 get its own repo?,shivaram@eecs.berkeley.edu,"Might be a good idea to get the PMC's of both projects to sign off to
prevent future issues with apache.

Regards,
Mridul


---------------------------------------------------------------------


"
Alexey Goncharuk <alexey.goncharuk@gmail.com>,"Mon, 20 Jul 2015 14:16:56 -0700",Make off-heap store pluggable,dev@spark.apache.org,"Hello Spark community,

I was looking through the code in order to understand better how RDD is
persisted to Tachyon off-heap filesystem. It looks like that the Tachyon
filesystem is hard-coded and there is no way to switch to another in-memory
filesystem. I think it would be great if the implementation of the
BlockManager and BlockStore would be able to plug in another filesystem.

For example, Apache Ignite also has an implementation of in-memory
filesystem which can store data in on-heap and off-heap formats. It would
be great if it could integrate with Spark.

I have filed a ticket in Jira:
https://issues.apache.org/jira/browse/SPARK-9203

If it makes sense, I will be happy to contribute to it.

Thoughts?

-Alexey (Apache Ignite PMC)
"
Prashant Sharma <scrapcodes@gmail.com>,"Tue, 21 Jul 2015 10:02:55 +0530",Re: Make off-heap store pluggable,Alexey Goncharuk <alexey.goncharuk@gmail.com>,"+1 Looks like a nice idea(I do not see any harm). Would you like to work on
the patch to support it ?

Prashant Sharma




"
Reynold Xin <rxin@databricks.com>,"Mon, 20 Jul 2015 21:34:34 -0700",Re: Make off-heap store pluggable,Prashant Sharma <scrapcodes@gmail.com>,"They are already pluggable.



"
Reynold Xin <rxin@databricks.com>,"Mon, 20 Jul 2015 21:40:34 -0700",Re: Make off-heap store pluggable,Prashant Sharma <scrapcodes@gmail.com>,"I sent it prematurely.

They are already pluggable, or at least in the process to be more
pluggable. In 1.4, instead of calling the external system's API directly,
we added an API for that.  There is a patch to add support for HDFS
in-memory cache.

Somewhat orthogonal to this, longer term, I am not sure whether it makes
sense to have the current off heap API, because there is no namespacing and
the benefit to end users is actually not very substantial (at least I can
think of much simpler ways to achieve exactly the same gains), and yet it
introduces quite a bit of complexity to the codebase.





"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 20 Jul 2015 23:29:32 -0700",Re: Make off-heap store pluggable,Reynold Xin <rxin@databricks.com>,"I agree with this -- basically, to build on Reynold's point, you should be able to get almost the same performance by implementing either the Hadoop FileSystem API or the Spark Data Source API over Ignite in the right way. This would let people save data persistently in Ignite in addition to using it for caching, and it would provide a global namespace, optionally a schema, etc. You can still provide data locality, short-circuit reads, etc with these APIs.

Matei

pluggable. In 1.4, instead of calling the external system's API directly, we added an API for that.  There is a patch to add support for HDFS in-memory cache. 
makes sense to have the current off heap API, because there is no namespacing and the benefit to end users is actually not very substantial (at least I can think of much simpler ways to achieve exactly the same gains), and yet it introduces quite a bit of complexity to the codebase.
work on the patch to support it ?
is persisted to Tachyon off-heap filesystem. It looks like that the Tachyon filesystem is hard-coded and there is no way to switch to another in-memory filesystem. I think it would be great if the implementation of the BlockManager and BlockStore would be able to plug in another filesystem.
filesystem which can store data in on-heap and off-heap formats. It would be great if it could integrate with Spark.
<https://issues.apache.org/jira/browse/SPARK-9203>

"
Sean Owen <sowen@cloudera.com>,"Tue, 21 Jul 2015 07:52:16 +0100",Re: Make off-heap store pluggable,Reynold Xin <rxin@databricks.com>,"(Related, not important comment: it would also be nice to separate out the
Tachyon dependency from core, as it's conceptually pluggable but is still
hard-coded into several places in the code, and a lot of the comments/docs
in the code.)


"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Tue, 21 Jul 2015 13:24:35 +0200",Re: countByValue on dataframe with multiple columns,Jonathan Winandy <jonathan.winandy@gmail.com>,"Yop,
actually the generic part does not work, the countByValue on one column
gives you the count for each value seen in the column.
I would like a generic (multi-column) countByValue to give me the same kind
of output for each column, not considering each n-uples of each column
value as the key (which is what the groupBy is doing by default).

Regards,

Olivier

2015-07-20 14:18 GMT+02:00 Jonathan Winandy <jonathan.winandy@gmail.com>:

(List(""A"",""B"", ""B"", ""A"")).map(Row.apply(_)), StructType(List(StructField(""n"", StringType))))
e
che/spark/ml/feature/StringIndexer.scala#L78
the
.


-- 
*Olivier Girardot* | AssociÃ©
o.girardot@lateral-thoughts.com
+33 6 24 09 17 94
"
Ted Malaska <ted.malaska@cloudera.com>,"Tue, 21 Jul 2015 07:40:02 -0400",Re: countByValue on dataframe with multiple columns,Olivier Girardot <o.girardot@lateral-thoughts.com>,"I'm guessing you want something like what I put in this blog post.

http://blog.cloudera.com/blog/2015/07/how-to-do-data-quality-checks-using-apache-spark-dataframes/

This is a very common use case.  If there is a +1 I would love to add it to
dataframes.

Let me know
Ted Malaska


mn
e(List(""A"",""B"", ""B"", ""A"")).map(Row.apply(_)), StructType(List(StructField(""n"", StringType))))
ache/spark/ml/feature/StringIndexer.scala#L78
 the
l.
"
Jonathan Winandy <jonathan.winandy@gmail.com>,"Tue, 21 Jul 2015 15:08:20 +0200",Re: countByValue on dataframe with multiple columns,Ted Malaska <ted.malaska@cloudera.com>,"Ha ok !

Then generic part would have that signature :

def countColsByValue(df:Dataframe):Map[String /* colname */,Dataframe]


+1 for more work (blog / api) for data quality checks.

Cheers,
Jonathan


TopCMSParams and some other monoids from Algebird are really cool for that :
https://github.com/twitter/algebird/blob/develop/algebird-core/src/main/scala/com/twitter/algebird/CountMinSketch.scala#L590



-apache-spark-dataframes/
umn
:
ze(List(""A"",""B"", ""B"", ""A"")).map(Row.apply(_)), StructType(List(StructField(""n"", StringType))))
pache/spark/ml/feature/StringIndexer.scala#L78
t the
ul.
"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Tue, 21 Jul 2015 16:19:06 +0200",Re: countByValue on dataframe with multiple columns,Jonathan Winandy <jonathan.winandy@gmail.com>,"Hi Ted,
The TopNList would be great to see directly in the Dataframe API and my
wish would be to be able to apply it on multiple columns at the same time
and get all these statistics.
the .describe() function is close to what we want to achieve, maybe we
could try to enrich its output.
Anyway, even as a spark-package, if you could package your code for
Dataframes, that would be great.

Regards,

Olivier.

2015-07-21 15:08 GMT+02:00 Jonathan Winandy <jonathan.winandy@gmail.com>:

t
cala/com/twitter/algebird/CountMinSketch.scala#L590
g-apache-spark-dataframes/
lumn
ize(List(""A"",""B"", ""B"", ""A"")).map(Row.apply(_)), StructType(List(StructField(""n"", StringType))))
apache/spark/ml/feature/StringIndexer.scala#L78
et the
ful.


-- 
*Olivier Girardot* | AssociÃ©
o.girardot@lateral-thoughts.com
+33 6 24 09 17 94
"
Ted Malaska <ted.malaska@cloudera.com>,"Tue, 21 Jul 2015 10:39:13 -0400",Re: countByValue on dataframe with multiple columns,Olivier Girardot <o.girardot@lateral-thoughts.com>,"100% I would love to do it.  Who a good person to review the design with.
All I need is a quick chat about the design and approach and I'll create
the jira and push a patch.

Ted Malaska


scala/com/twitter/algebird/CountMinSketch.scala#L590
ng-apache-spark-dataframes/
t
n
olumn
m
lize(List(""A"",""B"", ""B"", ""A"")).map(Row.apply(_)), StructType(List(StructField(""n"", StringType))))
/apache/spark/ml/feature/StringIndexer.scala#L78
get the
eful.
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Tue, 21 Jul 2015 09:22:11 -0700",Re: Should spark-ec2 get its own repo?,Mridul Muralidharan <mridul@gmail.com>,"There is technically no PMC for the spark-ec2 project (I guess we are kind
of establishing one right now). I haven't heard anything from the Spark PMC
on the dev list that might suggest a need for a vote so far. I will send
another round of email notification to the dev list when we have a JIRA /
PR that actually moves the scripts (right now the only thing that changed
is the location of some scripts in mesos/ to amplab/).

Thanks
Shivaram


"
Reynold Xin <rxin@databricks.com>,"Tue, 21 Jul 2015 10:19:30 -0700",Re: countByValue on dataframe with multiple columns,Ted Malaska <ted.malaska@cloudera.com>,"Is this just frequent items?

https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/DataFrameStatFunctions.scala#L97




e
:
/scala/com/twitter/algebird/CountMinSketch.scala#L590
ing-apache-spark-dataframes/
e
column
elize(List(""A"",""B"", ""B"", ""A"")).map(Row.apply(_)), StructType(List(StructField(""n"", StringType))))
g/apache/spark/ml/feature/StringIndexer.scala#L78
 get the
seful.
"
Alexey Goncharuk <alexey.goncharuk@gmail.com>,"Tue, 21 Jul 2015 10:48:40 -0700",Re: Make off-heap store pluggable,Reynold Xin <rxin@databricks.com>,"2015-07-20 21:40 GMT-07:00 Reynold Xin <rxin@databricks.com>:


Is there a ticket or branch you can show me so I can take a look at the
ongoing work?

--Alexey
"
Alexey Goncharuk <alexey.goncharuk@gmail.com>,"Tue, 21 Jul 2015 10:50:12 -0700",Re: Make off-heap store pluggable,Prashant Sharma <scrapcodes@gmail.com>,"2015-07-20 21:32 GMT-07:00 Prashant Sharma <scrapcodes@gmail.com>:

Yes, I would like to contribute to it once we clarify the appropriate path.

--Alexey


"
Alexey Goncharuk <alexey.goncharuk@gmail.com>,"Tue, 21 Jul 2015 10:56:47 -0700",Re: Make off-heap store pluggable,Matei Zaharia <matei.zaharia@gmail.com>,"2015-07-20 23:29 GMT-07:00 Matei Zaharia <matei.zaharia@gmail.com>:


Absolutely agree.

In fact, Ignite already provides a shared RDD implementation which is
essentially a view of Ignite cache data. This implementation adheres to the
Spark DataFrame API. More information can be found here:
http://ignite.incubator.apache.org/features/igniterdd.html

Also, Ignite in-memory filesystem is compliant with Hadoop filesystem API
and can transparently replace HDFS if needed. Plugging it into Spark should
be fairly easy. More information can be found here:
http://ignite.incubator.apache.org/features/igfs.html

--Alexey
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Tue, 21 Jul 2015 11:09:52 -0700",Re: Should spark-ec2 get its own repo?,Mridul Muralidharan <mridul@gmail.com>,"Thats part of the confusion we are trying to fix here -- the repository
used to live in the mesos github account but was never a part of the Apache
Mesos project. It was a remnant part of Spark from when Spark used to live
at github.com/mesos/spark.

Shivaram


"
Zhan Zhang <zzhang@hortonworks.com>,"Tue, 21 Jul 2015 18:11:45 +0000",Re: Make off-heap store pluggable,Alexey Goncharuk <alexey.goncharuk@gmail.com>,"Hi Alexey,

SPARK-6479<https://issues.apache.org/jira/browse/SPARK-6479> is for the plugin API, and SPARK-6112<https://issues.apache.org/jira/browse/SPARK-6112> is for hdfs plugin.


Thanks.

Zhan Zhang



2015-07-20 23:29 GMT-07:00 Matei Zaharia <matei.zaharia@gmail.com<mailto:matei.zaharia@gmail.com>>:
I agree with this -- basically, to build on Reynold's point, you should be able to get almost the same performance by implementing either the Hadoop FileSystem API or the Spark Data Source API over Ignite in the right way. This would let people save data persistently in Ignite in addition to using it for caching, and it would provide a global namespace, optionally a schema, etc. You can still provide data locality, short-circuit reads, etc with these APIs.

Absolutely agree.

In fact, Ignite already provides a shared RDD implementation which is essentially a view of Ignite cache data. This implementation adheres to the Spark DataFrame API. More information can be found here: http://ignite.incubator.apache.org/features/igniterdd.html

Also, Ignite in-memory filesystem is compliant with Hadoop filesystem API and can transparently replace HDFS if needed. Plugging it into Spark should be fairly easy. More information can be found here: http://ignite.incubator.apache.org/features/igfs.html

--Alexey


"
Mridul Muralidharan <mridul@gmail.com>,"Tue, 21 Jul 2015 11:03:35 -0700",Re: Should spark-ec2 get its own repo?,shivaram@eecs.berkeley.edu,"If I am not wrong, since the code was hosted within mesos project
repo, I assume (atleast part of it) is owned by mesos project and so
its PMC ?

- Mridul


---------------------------------------------------------------------


"
Ted Malaska <ted.malaska@cloudera.com>,"Tue, 21 Jul 2015 14:30:16 -0400",Re: countByValue on dataframe with multiple columns,Reynold Xin <rxin@databricks.com>,"Look at the implementation for frequently items.  It is a different from
true count.

pache/spark/sql/DataFrameStatFunctions.scala#L97
me
n/scala/com/twitter/algebird/CountMinSketch.scala#L590
:
sing-apache-spark-dataframes/
 each
lt).
lelize(List(""A"",""B"", ""B"", ""A"")).map(Row.apply(_)), StructType(List(StructField(""n"", StringType))))
rg/apache/spark/ml/feature/StringIndexer.scala#L78
o get the
useful.
"
Sean Owen <sowen@cloudera.com>,"Tue, 21 Jul 2015 19:47:38 +0100",Re: Should spark-ec2 get its own repo?,Mridul Muralidharan <mridul@gmail.com>,"I agree it's worth informing Mesos devs and checking that there are no
big objections. I presume Shivaram is plugged in enough to Mesos that
there won't be any surprises there, and that the project would also
agree with moving this Spark-specific bit out. they may also want to
leave a pointer to the new location in the mesos repo of course.

I don't think it is something that requires a formal vote. It's not a
question of ownership -- neither Apache nor the project PMC owns the
code. I don't think it's different from retiring or removing any other
code.






---------------------------------------------------------------------


"
Mridul Muralidharan <mridul@gmail.com>,"Tue, 21 Jul 2015 11:53:00 -0700",Re: Should spark-ec2 get its own repo?,shivaram@eecs.berkeley.edu,"That sounds good. Thanks for clarifying !


Regards,
Mridul


---------------------------------------------------------------------


"
Sean Busbey <busbey@cloudera.com>,"Tue, 21 Jul 2015 16:14:03 -0500",Re: Foundation policy on releases and Spark nightly builds,dev <dev@spark.apache.org>,"Looks good to me. Thanks for helping find a common ground everyone, and
Sean for handling the implementation.




-- 
Sean
"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Tue, 21 Jul 2015 23:15:18 +0200",Re: countByValue on dataframe with multiple columns,Ted Malaska <ted.malaska@cloudera.com>,"yes and freqItems does not give you an ordered count (right ?) + the
threshold makes it difficult to calibrate it + we noticed some strange
behaviour when testing it on small datasets.

2015-07-21 20:30 GMT+02:00 Ted Malaska <ted.malaska@cloudera.com>:

apache/spark/sql/DataFrameStatFunctions.scala#L97
l
y
ime
m
]
in/scala/com/twitter/algebird/CountMinSketch.scala#L590
using-apache-spark-dataframes/
d
f each
ult).
llelize(List(""A"",""B"", ""B"", ""A"")).map(Row.apply(_)), StructType(List(StructField(""n"", StringType))))
org/apache/spark/ml/feature/StringIndexer.scala#L78
to get the
 useful.


-- 
*Olivier Girardot* | AssociÃ©
o.girardot@lateral-thoughts.com
+33 6 24 09 17 94
"
Ted Malaska <ted.malaska@cloudera.com>,"Tue, 21 Jul 2015 19:53:12 -0400",Re: countByValue on dataframe with multiple columns,Olivier Girardot <o.girardot@lateral-thoughts.com>,"Cool I will make a jira after I check in to my hotel.  And try to get a
patch early next week.
m>

/apache/spark/sql/DataFrameStatFunctions.scala#L97
ll
me
e
e]
r
ain/scala/com/twitter/algebird/CountMinSketch.scala#L590
-using-apache-spark-dataframes/
of each
ault).
allelize(List(""A"",""B"", ""B"", ""A"")).map(Row.apply(_)), StructType(List(StructField(""n"", StringType))))
/org/apache/spark/ml/feature/StringIndexer.scala#L78
 to get the
y useful.
"
Aaron <aarongmldt@gmail.com>,"Tue, 21 Jul 2015 20:27:46 -0400",#NAME?,dev@spark.apache.org,"I compile/make a distribution, with either the 1.4 branch or  master,
using the -Phive-thriftserver, and attempt a JDBC connection to a
mysql DB..using latest connector (5.1.36) jar.

When I setup the pyspark shell doing:

bin/pyspark --jars mysql-connection...jar --driver-class-path
mysql-connector..jar

when I make a data frame from sqlContext.read.jdbc(""jdbc://...)

and perhaps I do df.show()

 things seem to work;  but if I compile with out that
-Phive-thriftserver, the same python, in the same settings (spark-env,
spark-defaults), just hangs...never to return.

I am curious, how the hive-thriftserver module plays into this type of
interaction.

Thanks in advance.

Cheers,
Aaron

---------------------------------------------------------------------


"
Ted Malaska <ted.malaska@cloudera.com>,"Tue, 21 Jul 2015 22:04:42 -0400",Re: countByValue on dataframe with multiple columns,Olivier Girardot <o.girardot@lateral-thoughts.com>,"I added the following jira

https://issues.apache.org/jira/browse/SPARK-9237

Please help me get it assigned to myself thanks.

Ted Malaska


m
g/apache/spark/sql/DataFrameStatFunctions.scala#L97
'll
ame
main/scala/com/twitter/algebird/CountMinSketch.scala#L590
s-using-apache-spark-dataframes/
 of each
fault).
rallelize(List(""A"",""B"", ""B"", ""A"")).map(Row.apply(_)), StructType(List(StructField(""n"", StringType))))
a/org/apache/spark/ml/feature/StringIndexer.scala#L78
e to get the
ry useful.
"
Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"Tue, 21 Jul 2015 22:47:50 -0700 (MST)","What is the difference between SlowSparkPullRequestBuilder and
 SparkPullRequestBuilder?",dev@spark.apache.org,"Hi all, 

When we send a PR, it seems that two requests to run tests are thrown to the
Jenkins sometimes. 
What is the difference between SparkPullRequestBuilder and
SlowSparkPullRequestBuilder?

Thanks,
Yu



-----
-- Yu Ishikawa
--

---------------------------------------------------------------------


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Tue, 21 Jul 2015 22:59:39 -0700",Re: Should spark-ec2 get its own repo?,Sean Owen <sowen@cloudera.com>,"Yeah I'll send a note to the mesos dev list just to make sure they are
informed.

Shivaram


"
Andrew Or <andrew@databricks.com>,"Wed, 22 Jul 2015 00:11:28 -0700",Re: What is the difference between SlowSparkPullRequestBuilder and SparkPullRequestBuilder?,Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"Hi Yu,

As it stands today, they are identical except for trigger mechanism. When
you say ""test this please"" or push a commit, SparkPullRequestBuilder is the
one that's running the tests. SlowSparkPullRequestBuilder, however, is not
used by default, but only triggered when you say ""slow test please"".
Functionally there is currently no difference; the latter came about
recently in an ongoing experiment to make unit tests run faster.

-Andrew

2015-07-21 22:47 GMT-07:00 Yu Ishikawa <yuu.ishikawa+spark@gmail.com>:

"
Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"Wed, 22 Jul 2015 00:52:59 -0700 (MST)","Re: What is the difference between SlowSparkPullRequestBuilder and
 SparkPullRequestBuilder?",dev@spark.apache.org,"Hi Andrew,

I understand that there is no difference currently.

Thanks,
Yu



-----
-- Yu Ishikawa
--

---------------------------------------------------------------------


"
Cheng Lian <lian@databricks.com>,"Wed, 22 Jul 2015 16:35:22 +0800",Deleted unreleased version 1.6.0 from JIRA by mistake,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,

The unreleased version 1.6.0 has was removed from JIRA due to my 
misoperation. I've added it back, but JIRA tickets that once targeted to 
1.6.0 now have empty target version/s. If you found tickets that should 
have targeted to 1.6.0, please help marking the target version/s field 
back to 1.6.0.

Thanks in advance and sorry for all the trouble!

Best,
Cheng

---------------------------------------------------------------------


"
=?UTF-8?Q?Sergio_Ram=c3=adrez?= <sramirezga@ugr.es>,"Wed, 22 Jul 2015 11:37:18 +0200",Fixed number of partitions in RangePartitioner,dev <dev@spark.apache.org>,"
Hi all:

I am developing an algorithm that needs to put together elements with 
the same key as much as possible but with always using a fixed number of 
partitions. To do that, this algorithm sorts by key the elements. The 
problem is that the number of distinct keys influences in the number of 
final partitions. For example, if I define 200 distinct keys and 800 
partitions in the /sortByKey/ function, the resulting number of 
partitions is equal to 202.

I have took a look to the code and I have found this:

Note that the actual number of partitions created by the 
RangePartitioner might not be the same
as the `partitions` parameter, in the case where the number of sampled 
records is less than the value of `partitions`.

I have tried with /repartition/ with /RangePartitioner/ with the same 
result (obvious).

Â¿Is there any function that can solve my problem, like 
/repartitionAndSortWithinPartitions/? Â¿Is there any sequence of 
instructions that can help me? If not, I think it can become a real 
problem to sort cases in which the number of  rows is huge and the 
number of distinct keys is small.

Thanks in advance,

Sergio R.


"
Takeshi Yamamuro <linguin.m.s@gmail.com>,"Wed, 22 Jul 2015 22:26:32 +0900",Re: Expression.resolved unmatched with the correct values in catalyst?,Ted Yu <yuzhihong@gmail.com>,"The move prevents some errors though, all the errors cannot be gone.
For example, in o.a.s.sql.catalyst.analysis.*Suite,

The case '
https://github.com/maropu/spark/commit/961b5e99e2136167f175598ed36585987cc1e236
'
causes 3 errors.
AnalysisSuite:
- analyze project *** FAILED ***
AnalysisErrorSuite:
- unresolved attributes *** FAILED ***
- unresolved window function *** FAILED ***

The case ''
https://github.com/maropu/spark/commit/da0e1c4da9960150770c1c819cc6c092a9849cb7
""
causes a 1 error.
AnalysisErrorSuite:
- unresolved window function *** FAILED ***

I'm currently looking into this tough, not sure about culprits.
Thanks,

// maropu




-- 
---
Takeshi Yamamuro
"
Justin Uang <justin.uang@gmail.com>,"Wed, 22 Jul 2015 16:49:21 +0000",Re: PySpark on PyPi,"Olivier Girardot <o.girardot@lateral-thoughts.com>, jey@cs.berkeley.edu, 
	Josh Rosen <rosenville@gmail.com>, Davies Liu <davies@databricks.com>, 
	Punyashloka Biswal <punya.biswal@gmail.com>","// + *Davies* for his comments
// + Punya for SA

For development and CI, like Olivier mentioned, I think it would be hugely
beneficial to publish pyspark (only code in the python/ dir) on PyPI. If
anyone wants to develop against PySpark APIs, they need to download the
distribution and do a lot of PYTHONPATH munging for all the tools (pylint,
pytest, IDE code completion). Right now that involves adding python/ and
python/lib/py4j-0.8.2.1-src.zip. In case pyspark ever wants to add more
dependencies, we would have to manually mirror all the PYTHONPATH munging
in the ./pyspark script. With a proper pyspark setup.py which declares its
dependencies, and a published distribution, depending on pyspark will just
be adding pyspark to my setup.py dependencies.

Of course, if we actually want to run parts of pyspark that is backed by
Py4J calls, then we need the full spark distribution with either ./pyspark
or ./spark-submit, but for things like linting and development, the
PYTHONPATH munging is very annoying.

I don't think the version-mismatch issues are a compelling reason to not go
ahead with PyPI publishing. At runtime, we should definitely enforce that
the version has to be exact, which means there is no backcompat nightmare
as suggested by Davies in https://issues.apache.org/jira/browse/SPARK-1267.
This would mean that even if the user got his pip installed pyspark to
somehow get loaded before the spark distribution provided pyspark, then the
user would be alerted immediately.

*Davies*, if you buy this, should me or someone on my team pick up
https://issues.apache.org/jira/browse/SPARK-1267 and
https://github.com/apache/spark/pull/464?


e
y
,
en
Ã©crit :
ow,
e,
much
.zip:$PYTHONPATH
:
f
he
art
ct
ly.
"
Takeshi Yamamuro <linguin.m.s@gmail.com>,"Thu, 23 Jul 2015 08:00:32 +0900",Re: Expression.resolved unmatched with the correct values in catalyst?,Ted Yu <yuzhihong@gmail.com>,"Ok, thanks,
I understood why this happened.

best regards,

// maropu





-- 
---
Takeshi Yamamuro
"
Punyashloka Biswal <punya.biswal@gmail.com>,"Wed, 22 Jul 2015 23:41:01 +0000",Re: PySpark on PyPi,"Justin Uang <justin.uang@gmail.com>, 
	Olivier Girardot <o.girardot@lateral-thoughts.com>, jey@cs.berkeley.edu, 
	Josh Rosen <rosenville@gmail.com>, Davies Liu <davies@databricks.com>","I agree with everything Justin just said. An additional advantage of
publishing PySpark's Python code in a standards-compliant way is the fact
that we'll be able to declare transitive dependencies (Pandas, Py4J) in a
way that pip can use. Contrast this with the current situation, where
df.toPandas() exists in the Spark API but doesn't actually work until you
install Pandas.

Punya

y
,
s
t
k
re
hen
Ã©crit :
s
now,
le,
 much
c.zip:$PYTHONPATH
 bet
the
part
ect
rly.
"
"""Bing Xiao (Bing)"" <bing.xiao@huawei.com>","Wed, 22 Jul 2015 23:53:28 +0000","Package Release Annoucement: Spark SQL on HBase ""Astro""","""dev@spark.apache.org"" <dev@spark.apache.org>,
        ""user@spark.apache.org""
	<user@spark.apache.org>","We are happy to announce the availability of the Spark SQL on HBase 1.0.0 release.  http://spark-packages.org/package/Huawei-Spark/Spark-SQL-on-HBase
The main features in this package, dubbed ""Astro"", include:

*         Systematic and powerful handling of data pruning and intelligent scan, based on partial evaluation technique

*         HBase pushdown capabilities like custom filters and coprocessor to support ultra low latency processing

*         SQL, Data Frame support

*         More SQL capabilities made possible (Secondary index, bloom filter, Primary Key, Bulk load, Update)

*         Joins with data from other sources

*         Python/Java/Scala support

*         Support latest Spark 1.4.0 release


The tests by Huawei team and community contributors covered the areas: bulk load; projection pruning; partition pruning; partial evaluation; code generation; coprocessor; customer filtering; DML; complex filtering on keys and non-keys; Join/union with non-Hbase data; Data Frame; multi-column family test.  We will post the test results including performance tests the middle of August.
You are very welcomed to try out or deploy the package, and help improve the integration tests with various combinations of the settings, extensive Data Frame tests, complex join/union test and extensive performance tests.  Please use the ""Issues"" ""Pull Requests"" links at this package homepage, if you want to report bugs, improvement or feature requests.
Special thanks to project owner and technical leader Yan Zhou, Huawei global team, community contributors and Databricks.   Databricks has been providing great assistance from the design to the release.
""Astro"", the Spark SQL on HBase package will be useful for ultra low latency query and analytics of large scale data sets in vertical enterprises. We will continue to work with the community to develop new features and improve code base.  Your comments and suggestions are greatly appreciated.

Yan Zhou / Bing Xiao
Huawei Big Data team

"
Pedro Rodriguez <ski.rodriguez@gmail.com>,"Wed, 22 Jul 2015 17:22:17 -0700",PySpark addPyFile for directories,dev@spark.apache.org,"I am running into an inconvenience while developing which I think could be
fixed by extending addPyFile. I am working on a pyspark project which has a
primary entry point plus several modules. The effect of this is that unless
the code is copied to the cluster in the PYTHONPATH or zipped and shipped
with addPyFile, it will return an error.

This would be easy if it were a single file or zip file with addPyFile, but
it is inconvenient while developing since it would mean reziping every time
a change was made.

Would it be a good idea to allow addPyFile to support adding local
directories? Does this seem like a good idea?

The implementation would be to have python zip the given directory into a
tmp directory, then ship that to the cluster.

-- 
Pedro Rodriguez
CU Boulder Phd Student
UCBerkeley 2014 | Computer Science
"
Xiaoyu Ma <hzmaxiaoyu@corp.netease.com>,"Thu, 23 Jul 2015 09:57:33 +0800",Where to find Spark-project-hive,dev@spark.apache.org,"Hi guys,
Iâ€™m trying to patch hive thrift server part related to HIVE-7620. I saw in spark is pulling a private fork of hive under spark-project hive name.
Any idea where I can find the source code of it?

Thanks~

é©¬æ™“å®‡ / Xiaoyu Ma
hzmaxiaoyu@corp.netease.com




"
Debasish Das <debasish.das83@gmail.com>,"Wed, 22 Jul 2015 20:35:45 -0700","Re: Package Release Annoucement: Spark SQL on HBase ""Astro""","""Bing Xiao (Bing)"" <bing.xiao@huawei.com>","Does it also support insert operations ?

e:
essor
m
e
ys
he
e
.
inks at this package homepage, if
r ultra low
"
"""Yan Zhou.sc"" <Yan.Zhou.sc@huawei.com>","Thu, 23 Jul 2015 04:03:11 +0000","RE: Package Release Annoucement: Spark SQL on HBase ""Astro""","Debasish Das <debasish.das83@gmail.com>,
        ""Bing Xiao (Bing)""
	<bing.xiao@huawei.com>","Yes, but not all SQL-standard insert variants .

From: Debasish Das [mailto:debasish.das83@gmail.com]
Sent: Wednesday, July 22, 2015 7:36 PM
To: Bing Xiao (Bing)
Cc: user; dev; Yan Zhou.sc
Subject: Re: Package Release Annoucement: Spark SQL on HBase ""Astro""


Does it also support insert operations ?
On Jul 22, 2015 4:53 PM, ""Bing Xiao (Bing)"" <bing.xiao@huawei.com<mailto:bing.xiao@huawei.com>> wrote:
We are happy to announce the availability of the Spark SQL on HBase 1.0.0 release.  http://spark-packages.org/package/Huawei-Spark/Spark-SQL-on-HBase
The main features in this package, dubbed â€œAstroâ€, include:

â€¢         Systematic and powerful handling of data pruning and intelligent scan, based on partial evaluation technique

â€¢         HBase pushdown capabilities like custom filters and coprocessor to support ultra low latency processing

â€¢         SQL, Data Frame support

â€¢         More SQL capabilities made possible (Secondary index, bloom filter, Primary Key, Bulk load, Update)

â€¢         Joins with data from other sources

â€¢         Python/Java/Scala support

â€¢         Support latest Spark 1.4.0 release


The tests by Huawei team and community contributors covered the areas: bulk load; projection pruning; partition pruning; partial evaluation; code generation; coprocessor; customer filtering; DML; complex filtering on keys and non-keys; Join/union with non-Hbase data; Data Frame; multi-column family test.  We will post the test results including performance tests the middle of August.
You are very welcomed to try out or deploy the package, and help improve the integration tests with various combinations of the settings, extensive Data Frame tests, complex join/union test and extensive performance tests.  Please use the â€œIssuesâ€ â€œPull Requestsâ€ links at this package homepage, if you want to report bugs, improvement or feature requests.
Special thanks to project owner and technical leader Yan Zhou, Huawei global team, community contributors and Databricks.   Databricks has been providing great assistance from the design to the release.
â€œAstroâ€, the Spark SQL on HBase package will be useful for ultra low latency query and analytics of large scale data sets in vertical enterprises. We will continue to work with the community to develop new features and improve code base.  Your comments and suggestions are greatly appreciated.

Yan Zhou / Bing Xiao
Huawei Big Data team

"
Reynold Xin <rxin@databricks.com>,"Wed, 22 Jul 2015 21:08:13 -0700",non-deprecation compiler warnings are upgraded to build errors now,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,

FYI, we just merged a patch that fails a build if there is a scala compiler
warning (if it is not deprecation warning).

In the past, many compiler warnings are actually caused by legitimate bugs
that we need to address. However, if we don't fail the build with warnings,
people don't pay attention at all to the warnings (it is also tough to pay
attention since there are a lot of deprecated warnings due to unit tests
testing deprecated APIs and reliance on Hadoop on deprecated APIs).

Note that ideally we should be able to mark deprecation warnings as errors
as well. However, due to the lack of ability to suppress individual warning
messages in the Scala compiler, we cannot do that (since we do need to
access deprecated APIs in Hadoop).
"
Andrew Vykhodtsev <yozhyg@gmail.com>,"Thu, 23 Jul 2015 12:52:05 +0200","Shouldn't SparseVector constructor give error when declared number of
 elements less than array lenght?",dev@spark.apache.org,"Dear Developers,

I found that one can create SparseVector inconsistently and it will lead to
an Java error in runtime, for example when training
LogisticRegressionWithSGD.

Here is the test case:


In [2]:
sc.version
Out[2]:
u'1.3.1'
In [13]:
from pyspark.mllib.linalg import SparseVector
from pyspark.mllib.regression import LabeledPoint
from pyspark.mllib.classification import LogisticRegressionWithSGD
In [3]:
x =  SparseVector(2, {1:1, 2:2, 3:3, 4:4, 5:5})
In [10]:
l = LabeledPoint(0, x)
In [12]:
r = sc.parallelize([l])
In [14]:
m = LogisticRegressionWithSGD.train(r)

Error:


Py4JJavaError: An error occurred while calling
o86.trainLogisticRegressionModelWithSGD.
: org.apache.spark.SparkException: Job aborted due to stage failure:
Task 7 in stage 11.0 failed 1 times, most recent failure: Lost task
7.0 in stage 11.0 (TID 47, localhost):
*java.lang.ArrayIndexOutOfBoundsException: 2*



Attached is the notebook with the scenario and the full message:



Should I raise a JIRA for this (forgive me if there is such a JIRA and
I did not notice it)

---------------------------------------------------------------------"
Manoj Kumar <manojkumarsivaraj334@gmail.com>,"Thu, 23 Jul 2015 16:32:46 +0530","Re: Shouldn't SparseVector constructor give error when declared
 number of elements less than array lenght?",Andrew Vykhodtsev <yozhyg@gmail.com>,"Hi,

I think this should raise an error both in the scala code and python API.

Please open a JIRA.





-- 
Godspeed,
Manoj Kumar,
http://manojbits.wordpress.com
<http://goog_1017110195>
http://github.com/MechCoder
"
Andrew Vykhodtsev <yozhyg@gmail.com>,"Thu, 23 Jul 2015 14:49:03 +0200","Re: Shouldn't SparseVector constructor give error when declared
 number of elements less than array lenght?",Manoj Kumar <manojkumarsivaraj334@gmail.com>,"Hi Manoj,

Done.

https://issues.apache.org/jira/browse/SPARK-9277


"
Steve Loughran <stevel@hortonworks.com>,"Thu, 23 Jul 2015 20:09:28 +0000",Re: Where to find Spark-project-hive,Xiaoyu Ma <hzmaxiaoyu@corp.netease.com>,"
On 22 Jul 2015, at 18:57, Xiaoyu Ma <hzmaxiaoyu@corp.netease.com<mailto:hzmaxiaoyu@corp.netease.com>> wrote:

Hi guys,
Iâ€™m trying to patch hive thrift server part related to HIVE-7620. I saw in spark is pulling a private fork of hive under spark-project hive name.
Any idea where I can find the source code of it?

Thanks~

é©¬æ™“å®‡ / Xiaoyu Ma
hzmaxiaoyu@corp.netease.com<mailto:hzmaxiaoyu@corp.netease.com>





The JIRA related to this issue is https://issues.apache.org/jira/browse/SPARK-5111
"
Steve Loughran <stevel@hortonworks.com>,"Thu, 23 Jul 2015 20:10:20 +0000",Re: Where to find Spark-project-hive,Xiaoyu Ma <hzmaxiaoyu@corp.netease.com>,"Xiaoyo,

In SPARK-8064 I've been working on getting spark & hive working against Hive 1.2.1; targeting spark 1.5

that should pick up the Hive fix automatically â€”though we need to get the sql/hive tests all working first, and there aren't any tests backed by minikdc to verify secure operation



On 22 Jul 2015, at 18:57, Xiaoyu Ma <hzmaxiaoyu@corp.netease.com<mailto:hzmaxiaoyu@corp.netease.com>> wrote:

Hi guys,
Iâ€™m trying to patch hive thrift server part related to HIVE-7620. I saw in spark is pulling a private fork of hive under spark-project hive name.
Any idea where I can find the source code of it?

Thanks~

é©¬æ™“å®‡ / Xiaoyu Ma
hzmaxiaoyu@corp.netease.com<mailto:hzmaxiaoyu@corp.netease.com>





"
"""Mattmann, Chris A (3980)"" <chris.a.mattmann@jpl.nasa.gov>","Thu, 23 Jul 2015 20:43:05 +0000",Fwd: posts are not accepted,"""dev@spark.apache.org"" <dev@spark.apache.org>","

Sent from my iPhone

Begin forwarded message:

From: Rob Sargent <rob.sargent@utah.edu<mailto:rob.sargent@utah.edu>>
Date: July 23, 2015 at 1:14:04 PM PDT
To: <user-owner@spark.apache.org<mailto:user-owner@spark.apache.org>>
Subject: posts are not accepted

Hello,

my user name is iceback and my email is rob.sargent@utah.edu<mailto:rob.sargent@utah.edu>.

There seems to be a problem with my account as my posts are never accepted.

Any information would be appreciated,

rjs

"
Bharath Ravi Kumar <reachbach@gmail.com>,"Fri, 24 Jul 2015 13:21:41 +0530",Re: [ANNOUNCE] Nightly maven and package builds for Spark,Patrick Wendell <pwendell@gmail.com>,"I noticed the last (1.5) build has a timestamp of 16th July. Have nightly
builds been discontinued since then?

Thanks,
Bharath


"
Eugen Cepoi <cepoi.eugen@gmail.com>,"Fri, 24 Jul 2015 13:03:44 +0200",review SPARK-8730,dev@spark.apache.org,"Hey,

I've opened a PR to fix ser/de issue of primitives classes in the java
serializer.
I already encountered this problem in different scenarios, so I am bringing
it up.
Would be great if someone wants to have a look at it! :)

https://issues.apache.org/jira/browse/SPARK-8730
https://github.com/apache/spark/pull/7122


Thanks,
Eugen
"
Sean Owen <sowen@cloudera.com>,"Fri, 24 Jul 2015 13:12:00 +0100",Re: review SPARK-8730,Eugen Cepoi <cepoi.eugen@gmail.com>,"It looks like you have a number of review comments on the PR that you
have not replied to. The PR does not merge at the moment either.


---------------------------------------------------------------------


"
Eugen Cepoi <cepoi.eugen@gmail.com>,"Fri, 24 Jul 2015 14:15:11 +0200",Re: review SPARK-8730,Sean Owen <sowen@cloudera.com>,"I just got those comments and it doesn't merge since few time, the code
evolved since I opened the pr

2015-07-24 14:12 GMT+02:00 Sean Owen <sowen@cloudera.com>:

"
Punyashloka Biswal <punya.biswal@gmail.com>,"Fri, 24 Jul 2015 14:43:39 +0000",Re: non-deprecation compiler warnings are upgraded to build errors now,"Reynold Xin <rxin@databricks.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Would it make sense to isolate the use of deprecated warnings to a subset
of projects? That way we could turn on more stringent checks for the other
ones.

Punya


"
Patrick Wendell <pwendell@gmail.com>,"Fri, 24 Jul 2015 08:08:45 -0700",Re: [ANNOUNCE] Nightly maven and package builds for Spark,Bharath Ravi Kumar <reachbach@gmail.com>,"Hey Bharath,

There was actually an incompatible change to the build process that
broke several of the Jenkins builds. This should be patched up in the
next day or two and nightly builds will resume.

- Patrick


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Fri, 24 Jul 2015 09:40:25 -0700",Re: non-deprecation compiler warnings are upgraded to build errors now,Punyashloka Biswal <punya.biswal@gmail.com>,"You can give it a shot, but we will have to revert it for a project as soon
as a project uses a deprecated API somewhere.



"
=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"Fri, 24 Jul 2015 19:24:43 +0200",Re: non-deprecation compiler warnings are upgraded to build errors now,Reynold Xin <rxin@databricks.com>,"
Hi all,
Iâ€™m a bit confused, since I see quite a lot of warnings in semi-legitimate
code.

For instance, @transient (plenty of instances like this in spark-streaming)
might generate warnings like:

abstract class ReceiverInputDStream[T: ClassTag](@transient ssc_ :
StreamingContext)
  extends InputDStream[T](ssc_) {

// and the warning is:
no valid targets for annotation on value ssc_ - it is discarded
unused. You may specify targets with meta-annotations, e.g.
@(transient @param)

At least thatâ€™s what happens if I build with Scala 2.11, not sure if this
setting is only for 2.10, or something really weird is happening on my
machine that doesnâ€™t happen on others.

iulian


s
s,
y
s
ng
-- 

--
Iulian Dragos

------
Reactive Apps on the JVM
www.typesafe.com
"
Reynold Xin <rxin@databricks.com>,"Fri, 24 Jul 2015 11:19:38 -0700",Re: non-deprecation compiler warnings are upgraded to build errors now,=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"Jenkins only run Scala 2.10. I'm actually not sure what the behavior is
with 2.11 for that patch.

iulian - can you take a look into it and see if it is working as expected?


e.com>

egitimate
ingContext)
ou may specify targets with meta-annotations, e.g. @(transient @param)
 if this
to
al
eed
"
Patrick Wendell <pwendell@gmail.com>,"Fri, 24 Jul 2015 12:57:28 -0700",Policy around backporting bug fixes,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi All,

A few times I've been asked about backporting and when to backport and
not backport fix patches. Since I have managed this for many of the
past releases, I wanted to point out the way I have been thinking
about it. If we have some consensus I can put it on the wiki.

The trade off when backporting is you get to deliver the fix to people
running older versions (great!), but you risk introducing new or even
worse bugs in maintenance releases (bad!). The decision point is when
you have a bug fix and it's not clear whether it is worth backporting.

I think the following facets are important to consider:
(a) Backports are an extremely valuable service to the community and
should be considered for any bug fix.
(b) Introducing a new bug in a maintenance release must be avoided at
all costs. It over time would erode confidence in our release process.
(c) Distributions or advanced users can always backport risky patches
on their own, if they see fit.

For me, the consequence of these is that we should backport in the
following situations:
- Both the bug and the fix are well understood and isolated. Code
being modified is well tested.
- The bug being addressed is high priority to the community.
- The backported fix does not vary widely from the master branch fix.

We tend to avoid backports in the converse situations:
- The bug or fix are not well understood. For instance, it relates to
interactions between complex components or third party libraries (e.g.
Hadoop libraries). The code is not well tested outside of the
immediate bug being fixed.
- The bug is not clearly a high priority for the community.
- The backported fix is widely different from the master branch fix.

These are clearly subjective criteria, but ones worth considering. I
am always happy to help advise people on specific patches if they want
a soundingboard to understand whether it makes sense to backport.

- Patrick

---------------------------------------------------------------------


"
Jeremy Freeman <freeman.jeremy@gmail.com>,"Fri, 24 Jul 2015 16:50:12 -0400",Re: PySpark on PyPi,Punyashloka Biswal <punya.biswal@gmail.com>,"Hey all, great discussion, just wanted to +1 that I see a lot of value in steps that make it easier to use PySpark as an ordinary python library.

You might want to check out this (https://github.com/minrk/findspark <https://github.com/minrk/findspark>), started by Jupyter project devs, that offers one way to facilitate this stuff. Iâ€™ve also cced them here to join the conversation.

Also, @Jey, I can also confirm that at least in some scenarios (Iâ€™ve done it in an EC2 cluster in standalone mode) itâ€™s possible to run PySpark jobs just using `from pyspark import SparkContext; sc = SparkContext(master=â€œXâ€)` so long as the environmental variables (PYTHONPATH and PYSPARK_PYTHON) are set correctly on *both* workers and driver. That said, thereâ€™s definitely additional configuration / functionality that would require going through the proper submit scripts.

publishing PySpark's Python code in a standards-compliant way is the fact that we'll be able to declare transitive dependencies (Pandas, Py4J) in a way that pip can use. Contrast this with the current situation, where df.toPandas() exists in the Spark API but doesn't actually work until you install Pandas.
hugely beneficial to publish pyspark (only code in the python/ dir) on PyPI. If anyone wants to develop against PySpark APIs, they need to download the distribution and do a lot of PYTHONPATH munging for all the tools (pylint, pytest, IDE code completion). Right now that involves adding python/ and python/lib/py4j-0.8.2.1-src.zip. In case pyspark ever wants to add more dependencies, we would have to manually mirror all the PYTHONPATH munging in the ./pyspark script. With a proper pyspark setup.py which declares its dependencies, and a published distribution, depending on pyspark will just be adding pyspark to my setup.py dependencies.
by Py4J calls, then we need the full spark distribution with either ./pyspark or ./spark-submit, but for things like linting and development, the PYTHONPATH munging is very annoying.
not go ahead with PyPI publishing. At runtime, we should definitely enforce that the version has to be exact, which means there is no backcompat nightmare as suggested by Davies in https://issues.apache.org/jira/browse/SPARK-1267 <https://issues.apache.org/jira/browse/SPARK-1267>. This would mean that even if the user got his pip installed pyspark to somehow get loaded before the spark distribution provided pyspark, then the user would be alerted immediately.
https://issues.apache.org/jira/browse/SPARK-1267 <https://issues.apache.org/jira/browse/SPARK-1267> and https://github.com/apache/spark/pull/464 <https://github.com/apache/spark/pull/464>?
<o.girardot@lateral-thoughts.com because right now if I want to set-up a CI env for PySpark, I have to :
every agent
site-packages/ directory
project), I have to (except if I'm mistaken) 
on every agent
additional classpath using the conf/spark-default file 
python-based dependency, and some to the way SparkContext are launched when using pyspark.
spark-shell is downloading such dependencies automatically, I think if nothing's done yet it will (I guess ?).
enough, I'm not exactly advocating to distribute a full 300Mb spark distribution in PyPi, maybe there's a better compromise ?
<mailto:jey@cs.berkeley.edu>> a Ã©crit :
as a shim to an existing Spark installation? Or it could even download the latest Spark binary if SPARK_HOME isn't set during installation. Right now, Spark doesn't play very well with the usual Python ecosystem. For example, why do I need to use a strange incantation when booting up IPython if I want to use PySpark in a notebook with MASTER=""local[4]""? It would be much nicer to just type `from pyspark import SparkContext; sc = SparkContext(""local[4]"")` in my notebook.
SPARK_HOME is set and Py4J is on the PYTHONPATH:
PYTHONPATH=$SPARK_HOME/python/:$SPARK_HOME/python/lib/py4j-0.8.2.1-src.zip:$PYTHONPATH python $SPARK_HOME/python/pyspark/rdd.py
https://issues.apache.org/jira/browse/SPARK-1267 <https://issues.apache.org/jira/browse/SPARK-1267>
of PySpark than just requiring SPARK_HOME to be set; if we did this, I bet we'd run into tons of issues when users try to run a newer version of the Python half of PySpark against an older set of Java components or vice-versa.
<o.girardot@lateral-thoughts.com defined anyway, I think it would be interesting to deploy the Python part of Spark on PyPi in order to handle the dependencies in a Python project needing PySpark via pip.
site-packages/ in order for PyCharm or other lint tools to work properly.

"
Steve Loughran <stevel@hortonworks.com>,"Fri, 24 Jul 2015 23:05:21 +0000",jenkins failing on Kinesis shard limits,"""dev@spark.apache.org"" <dev@spark.apache.org>","
Looks like Jenkins is hitting some AWS limits

https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/38396/testReport/org.apache.spark.streaming.kinesis/KinesisBackedBlockRDDSuite/_It_is_not_a_test_/
"
Calvin Jia <jia.calvin@gmail.com>,"Fri, 24 Jul 2015 16:43:13 -0700",Jenkins HiveCompatibilitySuite Test Failures,dev@spark.apache.org,"Hi,

I've been seeing errors with
org.apache.spark.sql.hive.execution.HiveCompatibilitySuite
from the Jenkins tests in a PR I proposed as well as in PRs from other
members of the community. Is this test stable at the moment?

Thanks,
Calvin
"
Patrick Wendell <pwendell@gmail.com>,"Fri, 24 Jul 2015 22:57:36 -0700",Re: jenkins failing on Kinesis shard limits,Steve Loughran <stevel@hortonworks.com>,"I've disabled the test and filed a JIRA:

https://issues.apache.org/jira/browse/SPARK-9335


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Fri, 24 Jul 2015 22:59:52 -0700",Protocol for build breaks,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi All,

If there is a build break (i.e. a compile issue or consistently
failing test) that somehow makes it into master, the best protocol is:

1. Revert the offending patch.
2. File a JIRA and assign it to the committer of the offending patch.
The JIRA should contain links to broken builds.

It's not worth waiting any time to try and figure out how to fix it,
or blocking on tracking down the commit author. This is because every
hour that we have the PRB broken is a major cost in terms of developer
productivity.

- Patrick

---------------------------------------------------------------------


"
"""Prabeesh K."" <prabsmails@gmail.com>","Sat, 25 Jul 2015 10:46:03 +0400",Re: jenkins failing on Kinesis shard limits,Patrick Wendell <pwendell@gmail.com>,"For me

https://amplab.cs.berkeley.edu/jenkins/job/SlowSparkPullRequestBuilder/97/console

https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/38417/console


"
anshu shukla <anshushukla0@gmail.com>,"Sat, 25 Jul 2015 15:29:08 +0530","ReceiverStream SPARK not able to cope up with 20,000 events /sec .","user <user@spark.apache.org>, dev@spark.apache.org","My eventGen is emitting 20,000  events/sec ,and I am using store(s1)
in receive()  method to push data to receiverStream .

But this logic is working fine for upto 4000 events/sec and no batch
are seen emitting for larger rate .

*CODE:TOPOLOGY -*


*JavaDStream<String> sourcestream = ssc.receiverStream(        new
TetcCustomEventReceiver(datafilename,spoutlog,argumentClass.getScalingFactor(),datasetType));*

*CODE:TetcCustomEventReceiver -*

public void receive(List<String> event) {
StringBuffer tuple=new StringBuffer();
msgId++;
for(String s:event)
        {
            tuple.append(s).append("","");
        }
String s1=MsgIdAddandRemove.addMessageId(tuple.toString(),msgId);
store(s1);
    }




-- 
Thanks & Regards,
Anshu Shukla
"
Shixiong Zhu <zsxwing@gmail.com>,"Sat, 25 Jul 2015 23:44:48 +0800",Re: jenkins failing on Kinesis shard limits,Patrick Wendell <pwendell@gmail.com>,"The issue is in KinesisBackedBlockRDDSuite

I have sent https://github.com/apache/spark/pull/7661 to remove the streams
created by KinesisBackedBlockRDDSuite and
https://github.com/apache/spark/pull/7663 to fix the issue.

Best Regards,
Shixiong Zhu

2015-07-25 14:46 GMT+08:00 Prabeesh K. <prabsmails@gmail.com>:

"
=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"Sat, 25 Jul 2015 17:52:29 +0200",Re: non-deprecation compiler warnings are upgraded to build errors now,Reynold Xin <rxin@databricks.com>,"
Jenkins only run Scala 2.10. I'm actually not sure what the behavior is
?
It is, in the sense that warnings fail the build. Unfortunately there are
warnings in 2.11 that were not there in 2.10, and that fail the build. For
instance:

[error] /Users/dragos/workspace/git/spark/core/src/main/scala/org/apache/spark/rdd/BinaryFileRDD.scala:31:
no valid targets for annotation on value conf - it is discarded
unused. You may specify targets with meta-annotations, e.g.
@(transient @param)
[error]     @transient conf: Configuration,
[error]

Currently the 2.11 build is broken. I donâ€™t think fixing these is too hard,
but it requires these parameters to become vals. I havenâ€™t looked at all
warnings, but I think this is the most common one (if not the only one).

iulian


:
mingContext)
You may specify targets with meta-annotations, e.g. @(transient @param)
e if this
 to
ual
need
-- 

--
Iulian Dragos

------
Reactive Apps on the JVM
www.typesafe.com
"
anshu shukla <anshushukla0@gmail.com>,"Sat, 25 Jul 2015 23:13:26 +0530",Parallelism of Custom receiver in spark,"user <user@spark.apache.org>, dev@spark.apache.org","1 - How to increase the level of *parallelism in spark streaming  custom
 RECEIVER* .

2 -  Will  ssc.receiverstream(/**anything //)   will *delete the data
stored in spark memory using store(s) * logic .

-- 
Thanks & Regards,
Anshu Shukla
"
anshu shukla <anshushukla0@gmail.com>,"Sun, 26 Jul 2015 04:04:57 +0530",Log For in[put rate value in streaming statistics,"user <user@spark.apache.org>, dev@spark.apache.org, 
	Tathagata Das <tdas@databricks.com>","Hello ,

Is  there  any logging  every sec for the *input rate  parameter  shown in
the Streaming Statistics *of SPARK 1.4 .
(To see the final pattern i need the full set of values , or can we enable
logging for the parameter . )

*- Attached sample for statistics*

-- 
Thanks & Regards,
Anshu Shukla

---------------------------------------------------------------------"
Debasish Das <debasish.das83@gmail.com>,"Sat, 25 Jul 2015 22:45:17 -0700",Confidence in implicit factorization,dev <dev@spark.apache.org>,"Hi,

Implicit factorization is important for us since it drives recommendation
when modeling user click/no-click and also topic modeling to handle 0
counts in document x word matrices through NMF and Sparse Coding.

I am a bit confused on this code:

val c1 = alpha * math.abs(rating)
if (rating > 0) ls.add(srcFactor, (c1 + 1.0)/c1, c1)

When the alpha = 1.0 (high confidence) and rating is > 0 (true for word
counts), why this formula does not become same as explicit formula:

ls.add(srcFactor, rating, 1.0)

For modeling document, I believe implicit Y'Y needs to stay but we need
explicit ls.add(srcFactor, rating, 1.0)

I am understanding confidence code further. Please let me know if the idea
of mapping implicit to handle 0 counts in document word matrix makes sense.

Thanks.
Deb
"
Tathagata Das <tdas@databricks.com>,"Sat, 25 Jul 2015 23:21:45 -0700",Re: jenkins failing on Kinesis shard limits,Shixiong Zhu <zsxwing@gmail.com>,"I have remove the flag in the PullRequestBuilder that enabled Kinesis
tests. All the Kinesis tests should be ignored now. In the mean time we can
fix the tests.


"
Sean Owen <sowen@cloudera.com>,"Sun, 26 Jul 2015 08:34:57 +0100",Re: Confidence in implicit factorization,Debasish Das <debasish.das83@gmail.com>,"confidence = 1 + alpha * |rating| here (so, c1 means confidence - 1),
so alpha = 1 doesn't specially mean high confidence. The loss function
is computed over the whole input matrix, including all missing ""0""
entries. These have a minimal confidence of 1 according to this
formula. alpha controls how much more confident you are in what the
entries that do exist in the input mean. So alpha = 1 is low-ish and
means you don't think the existence of ratings means a lot more than
their absence.

I think the explicit case is similar, but not identical -- here. The
cost function for the explicit case is not the same, which is the more
substantial difference between the two. There, ratings aren't inputs
to a confidence value that becomes a weight in the loss function,
during this factorization of a 0/1 matrix. Instead the rating matrix
is the thing being factorized directly.


---------------------------------------------------------------------


"
Debasish Das <debasish.das83@gmail.com>,"Sun, 26 Jul 2015 01:19:03 -0700",Re: Confidence in implicit factorization,Sean Owen <sowen@cloudera.com>,"Yeah, I think the idea of confidence is a bit different than what I am
looking for using implicit factorization to do document clustering.

I basically need (r_ij - w_ih_j)^2 for all observed ratings and (0 -
w_ih_j)^2 for all the unobserved ratings...Think about the document x word
matrix where r_ij is the count that's observed, 0 are the word counts that
are not in particular document.

The broadcasted value of gram matrix w_i'wi or h_j'h_j will also count the
r_ij those are observed...So I might be fine using the broadcasted gram
matrix and use the linear term as \sum (-r_ijw_i) or \sum (-rijh_j)...

I will think further but in the current implicit formulation with
confidence, looks like I am really factorizing a 0/1 matrix with weights 1
+ alpha*rating for  . It's a bit different from LSA model.






"
Debasish Das <debasish.das83@gmail.com>,"Sun, 26 Jul 2015 01:20:57 -0700",Re: Confidence in implicit factorization,Sean Owen <sowen@cloudera.com>,"I will think further but in the current implicit formulation with
confidence, looks like I am factorizing a 0/1 matrix with weights 1 +
alpha*rating for observed (1) values and 1 for unobserved (0) values. It's
a bit different from LSA model.


"
Sean Owen <sowen@cloudera.com>,"Sun, 26 Jul 2015 09:23:06 +0100",Re: Confidence in implicit factorization,Debasish Das <debasish.das83@gmail.com>,"It sounds like you're describing the explicit case, or any matrix
decomposition. Are you sure that's best for count-like data? ""It
depends,"" but my experience is that the implicit formulation is
better. In a way, the difference between 10,000 and 1,000 count is
less significant than the difference between 1 and 10. However if your
loss function penalizes the square of the error, then the former case
not only matters more for the same relative error, it matters 10x more
than the latter. It's very heavily skewed to pay attention to the
high-count instances.



---------------------------------------------------------------------


"
Debasish Das <debasish.das83@gmail.com>,"Sun, 26 Jul 2015 01:30:38 -0700",Re: Confidence in implicit factorization,Sean Owen <sowen@cloudera.com>,"We got good clustering results from Implicit factorization using alpha =
1.0 since I thought to have a confidence of 1 + rating to observed entries
and 1 to unobserved entries. I used positivity / sparse coding basically to
force sparsity on document / topic matrix...But then I got confused because
I am modifying the real counts from dataset (does not matter much for in
practical sense since we really don't have true documents)

I mean gram matrix is the key here but then how much weight to give on real
counts also matters...I have not yet started looking into perplexity but
that will give me further insights...


"
Debasish Das <debasish.das83@gmail.com>,"Sun, 26 Jul 2015 01:35:33 -0700",Re: Confidence in implicit factorization,Sean Owen <sowen@cloudera.com>,"In your experience with using implicit factorization for document
clustering, how did you tune alpha ? Using perplexity measures or just
something simple like 1 + rating since the ratings are always positive in
this case....


"
Sean Owen <sowen@cloudera.com>,"Sun, 26 Jul 2015 10:11:56 +0100",Re: Confidence in implicit factorization,Debasish Das <debasish.das83@gmail.com>,"You can tune alpha like any other hyperparam, and measuring whatever
metric makes most sense -- AUC, etc. I don't think there's a general
guidelines that's more specific than that. I also have not applied
this to document retrieval / recommendation before

I don't think you need to modify counts or ratings, and shouldn't,
since the formulation is already trying to take care of translating
counts into weights as 1 + alpha * r.


---------------------------------------------------------------------


"
"""=?utf-8?B?U2Vh?="" <261810726@qq.com>","Mon, 27 Jul 2015 00:57:54 +0800","=?utf-8?B?5Zue5aSN77yaIEFza2VkIHRvIHJlbW92ZSBub24t?=
 =?utf-8?B?ZXhpc3RlbnQgZXhlY3V0b3IgZXhjZXB0aW9u?=","""=?utf-8?B?VGVkIFl1?="" <yuzhihong@gmail.com>, ""=?utf-8?B?UGEgUsO2?="" <paul.roewer1990@googlemail.com>","This exception is so ugly!!!  The screen is full of these information when the program runs a long time,  and they will not fail the job. 
 
I comment it in the source code. I think this information is useless because the executor is already removed and I don't know what does the executor id mean.
 
Should we remove this information forever?
 
 
 
 15/07/23 13:26:41 ERROR SparkDeploySchedulerBackend: Asked to remove non-existent executor 2...
 
15/07/23 13:26:41 ERROR SparkDeploySchedulerBackend: Asked to remove non-existent executor 2...






  

 

 ------------------ åŽŸå§‹é‚®ä»¶ ------------------
  å‘ä»¶äºº: ""Ted Yu"";<yuzhihong@gmail.com>;
 å‘é€æ—¶é—´: 2015å¹´7æœˆ26æ—¥(æ˜ŸæœŸå¤©) æ™šä¸Š10:51
 æ”¶ä»¶äºº: ""Pa RÃ¶""<paul.roewer1990@googlemail.com>; 
 æŠ„é€: ""user""<user@spark.apache.org>; 
 ä¸»é¢˜: Re: Asked to remove non-existent executor exception

 

 You can list the files in tmpfs in reverse chronological order and remove the oldest until you have enough space.  

 Cheers

 
 On Sun, Jul 26, 2015 at 12:43 AM, Pa RÃ¶ <paul.roewer1990@googlemail.com> wrote:
  i has seen that the ""tempfs"" is full, how i can clear this?
   
 2015-07-23 13:41 GMT+02:00 Pa RÃ¶ <paul.roewer1990@googlemail.com>:
     hello spark community,


i have build an application with geomesa, accumulo and spark.

if it run on spark local mode, it is working, but on spark

cluster not. in short it says: No space left on device. Asked to remove non-existent executor XY. 
IÂ´m confused, because there were many GBÂ´s of free space. do i need to change my configuration or what else can i do? thanks in advance.

here is the complete exception:

og4j:WARN No appenders could be found for logger (org.apache.accumulo.fate.zookeeper.ZooSession).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
15/07/23 13:26:39 INFO SparkContext: Running Spark version 1.3.0
15/07/23 13:26:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
15/07/23 13:26:39 INFO SecurityManager: Changing view acls to: marcel
15/07/23 13:26:39 INFO SecurityManager: Changing modify acls to: marcel
15/07/23 13:26:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(marcel); users with modify permissions: Set(marcel)
15/07/23 13:26:39 INFO Slf4jLogger: Slf4jLogger started
15/07/23 13:26:40 INFO Remoting: Starting remoting
15/07/23 13:26:40 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@node1-scads02:52478]
15/07/23 13:26:40 INFO Utils: Successfully started service 'sparkDriver' on port 52478.
15/07/23 13:26:40 INFO SparkEnv: Registering MapOutputTracker
15/07/23 13:26:40 INFO SparkEnv: Registering BlockManagerMaster
15/07/23 13:26:40 INFO DiskBlockManager: Created local directory at /tmp/spark-ca9319d4-68a2-4add-a21a-48b13ae9cf81/blockmgr-cbf8af23-e113-4732-8c2c-7413ad237b3b
15/07/23 13:26:40 INFO MemoryStore: MemoryStore started with capacity 1916.2 MB
15/07/23 13:26:40 INFO HttpFileServer: HTTP File server directory is /tmp/spark-9d4a04d5-3535-49e0-a859-d278a0cc7bf8/httpd-1882aafc-45fe-4490-803d-c04fc67510a2
15/07/23 13:26:40 INFO HttpServer: Starting HTTP Server
15/07/23 13:26:40 INFO Server: jetty-8.y.z-SNAPSHOT
15/07/23 13:26:40 INFO AbstractConnector: Started SocketConnector@0.0.0.0:56499
15/07/23 13:26:40 INFO Utils: Successfully started service 'HTTP file server' on port 56499.
15/07/23 13:26:40 INFO SparkEnv: Registering OutputCommitCoordinator
15/07/23 13:26:40 INFO Server: jetty-8.y.z-SNAPSHOT
15/07/23 13:26:40 INFO AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040
15/07/23 13:26:40 INFO Utils: Successfully started service 'SparkUI' on port 4040.
15/07/23 13:26:40 INFO SparkUI: Started SparkUI at http://node1-scads02:4040
15/07/23 13:26:40 INFO AppClient$ClientActor: Connecting to master akka.tcp://sparkMaster@node1-scads02:7077/user/Master...
15/07/23 13:26:40 INFO SparkDeploySchedulerBackend: Connected to Spark cluster with app ID app-20150723132640-0000
15/07/23 13:26:40 INFO AppClient$ClientActor: Executor added: app-20150723132640-0000/0 on worker-20150723132524-node3-scads06-7078 (node3-scads06:7078) with 8 cores
15/07/23 13:26:40 INFO SparkDeploySchedulerBackend: Granted executor ID app-20150723132640-0000/0 on hostPort node3-scads06:7078 with 8 cores, 512.0 MB RAM
15/07/23 13:26:40 INFO AppClient$ClientActor: Executor added: app-20150723132640-0000/1 on worker-20150723132513-node2-scads05-7078 (node2-scads05:7078) with 8 cores
15/07/23 13:26:40 INFO SparkDeploySchedulerBackend: Granted executor ID app-20150723132640-0000/1 on hostPort node2-scads05:7078 with 8 cores, 512.0 MB RAM
15/07/23 13:26:40 INFO AppClient$ClientActor: Executor updated: app-20150723132640-0000/0 is now RUNNING
15/07/23 13:26:40 INFO AppClient$ClientActor: Executor updated: app-20150723132640-0000/1 is now RUNNING
15/07/23 13:26:40 INFO AppClient$ClientActor: Executor updated: app-20150723132640-0000/0 is now LOADING
15/07/23 13:26:40 INFO AppClient$ClientActor: Executor updated: app-20150723132640-0000/1 is now LOADING
15/07/23 13:26:40 INFO NettyBlockTransferService: Server created on 45786
15/07/23 13:26:40 INFO BlockManagerMaster: Trying to register BlockManager
15/07/23 13:26:40 INFO BlockManagerMasterActor: Registering block manager node1-scads02:45786 with 1916.2 MB RAM, BlockManagerId(<driver>, node1-scads02, 45786)
15/07/23 13:26:40 INFO BlockManagerMaster: Registered BlockManager
15/07/23 13:26:40 INFO AppClient$ClientActor: Executor updated: app-20150723132640-0000/0 is now FAILED (java.io.IOException: No space left on device)
15/07/23 13:26:40 INFO SparkDeploySchedulerBackend: Executor app-20150723132640-0000/0 removed: java.io.IOException: No space left on device
15/07/23 13:26:40 ERROR SparkDeploySchedulerBackend: Asked to remove non-existent executor 0
15/07/23 13:26:40 INFO AppClient$ClientActor: Executor added: app-20150723132640-0000/2 on worker-20150723132524-node3-scads06-7078 (node3-scads06:7078) with 8 cores
15/07/23 13:26:40 INFO SparkDeploySchedulerBackend: Granted executor ID app-20150723132640-0000/2 on hostPort node3-scads06:7078 with 8 cores, 512.0 MB RAM
15/07/23 13:26:40 INFO AppClient$ClientActor: Executor updated: app-20150723132640-0000/1 is now FAILED (java.io.IOException: No space left on device)
15/07/23 13:26:40 INFO SparkDeploySchedulerBackend: Executor app-20150723132640-0000/1 removed: java.io.IOException: No space left on device
15/07/23 13:26:40 ERROR SparkDeploySchedulerBackend: Asked to remove non-existent executor 1
15/07/23 13:26:40 INFO AppClient$ClientActor: Executor added: app-20150723132640-0000/3 on worker-20150723132513-node2-scads05-7078 (node2-scads05:7078) with 8 cores
15/07/23 13:26:40 INFO SparkDeploySchedulerBackend: Granted executor ID app-20150723132640-0000/3 on hostPort node2-scads05:7078 with 8 cores, 512.0 MB RAM
15/07/23 13:26:40 INFO AppClient$ClientActor: Executor updated: app-20150723132640-0000/2 is now LOADING
15/07/23 13:26:40 INFO AppClient$ClientActor: Executor updated: app-20150723132640-0000/3 is now LOADING
15/07/23 13:26:40 INFO AppClient$ClientActor: Executor updated: app-20150723132640-0000/2 is now RUNNING
15/07/23 13:26:40 INFO SparkDeploySchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
15/07/23 13:26:40 INFO AppClient$ClientActor: Executor updated: app-20150723132640-0000/3 is now RUNNING
15/07/23 13:26:41 INFO AppClient$ClientActor: Executor updated: app-20150723132640-0000/2 is now FAILED (java.io.IOException: No space left on device)
15/07/23 13:26:41 INFO SparkDeploySchedulerBackend: Executor app-20150723132640-0000/2 removed: java.io.IOException: No space left on device
15/07/23 13:26:41 ERROR SparkDeploySchedulerBackend: Asked to remove non-existent executor 2..."
Priya Ch <learnings.chitturi@gmail.com>,"Sun, 26 Jul 2015 23:49:54 +0530",Writing streaming data to cassandra creates duplicates,"dev@spark.apache.org, ""user@spark.apache.org"" <user@spark.apache.org>","Hi All,

 I have a problem when writing streaming data to cassandra. Or existing
product is on Oracle DB in which while wrtiting data, locks are maintained
such that duplicates in the DB are avoided.

But as spark has parallel processing architecture, if more than 1 thread is
trying to write same data i.e with same primary key, is there as any scope
to created duplicates? If yes, how to address this problem either from
spark or from cassandra side ?

Thanks,
Padma Ch
"
Josh Rosen <rosenville@gmail.com>,"Sun, 26 Jul 2015 11:29:42 -0700",Re: non-deprecation compiler warnings are upgraded to build errors now,=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"Given that 2.11 may be more stringent with respect to warnings, we might
consider building with 2.11 instead of 2.10 in the pull request builder.
This would also have some secondary benefits in terms of letting us use
tools like Scapegoat or SCoverage highlighting.

.com>

d?
r
spark/rdd/BinaryFileRDD.scala:31: no valid targets for annotation on value conf - it is discarded unused. You may specify targets with meta-annotations, e.g. @(transient @param)
 too
looked
amingContext)
 You may specify targets with meta-annotations, e.g. @(transient @param)
re if
n my
o
e to
d
dual
 need
"
Mridul Muralidharan <mridul@gmail.com>,"Sun, 26 Jul 2015 15:28:19 -0700",Re: Asked to remove non-existent executor exception,Sea <261810726@qq.com>,"Simply customize your log4j confit instead of modifying code if you don't
want messages from that class.


Regards
Mridul


n
-
¥(æ˜ŸæœŸå¤©) æ™šä¸Š10:51
m
com
i need to
e
sers
'
4732-8c2c-7413ad237b3b
0-803d-c04fc67510a2
86
,
left
n
left
n
o:
left
n
"
Ted Yu <yuzhihong@gmail.com>,"Sun, 26 Jul 2015 15:36:49 -0700",Re: Asked to remove non-existent executor exception,Mridul Muralidharan <mridul@gmail.com>,"If I read the code correctly, that error message came
from CoarseGrainedSchedulerBackend.

There may be existing / future error messages, other than the one cited
below, which are useful. Maybe change the log level of this message  to
DEBUG ?

Cheers


--
¥(æ˜ŸæœŸå¤©) æ™šä¸Š10:51
om>;
n
e
.com>
e
 i need to
le
l
n
users
-4732-8c2c-7413ad237b3b
90-803d-c04fc67510a2
n
D
D
 left
on
D
 left
on
D
s
io:
 left
on
"
Bharath Ravi Kumar <reachbach@gmail.com>,"Mon, 27 Jul 2015 08:18:47 +0530",Re: [ANNOUNCE] Nightly maven and package builds for Spark,Patrick Wendell <pwendell@gmail.com>,"Thanks Patrick. I'll await resumption of the master tree's nightly builds.

-Bharath


"
"""Huang, Jie"" <jie.huang@intel.com>","Mon, 27 Jul 2015 03:10:27 +0000",[SparkScore]Performance portal for Apache Spark - WW30,"user <user@spark.apache.org>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","Performance Portal for Apache Spark
Description
________________________________
Each data point represents each workload runtime percent compared with the previous week. Different lines represents different workloads running on spark yarn-client mode.
Hardware
________________________________
CPU type: Intel(r) Xeon(r) CPU E5-2697 v2 @ 2.70GHz
Memory: 128GB
NIC: 10GbE
Disk(s): 8 x 1TB SATA HDD
Software
________________________________
JAVA version: 1.8.0_25
Hadoop version: 2.5.0-CDH5.3.2
HiBench version: 4.0
Spark on yarn-client mode
Cluster
________________________________
1 node for Master
10 nodes for Slave
Regular
Summary
The lower percent the better performance.
________________________________
Group

ww24

ww25

ww26

ww27

ww28

ww29

ww30

HiBench

-6.5%

-3.1%

-2.1%

-6.4%

-2.7%

-0.7%

2.2%

spark-perf

-4.7%

-4.6%

-5.4%

-4.6%

-12.8%

-12.5%

-12.3%

[http://01org.github.io/sparkscore/image/plaf1.time/overall.png]
Y-Axis: normalized completion time; X-Axis: Work Week.
The commit number can be found in the result table.
The performance score for each workload is normalized based on the elapsed time for 1.2 release. The lower the better.
Detail
________________________________
HiBench
________________________________
JOB

ww24

ww25

ww26

ww27

ww28

ww29

ww30

commit

db81b9d8

4eb48ed1

32e3cdaa

ec784381

2b820f2a

c472eb17

a803ac3e

sleep

-4.1%

12.8%

-5.1%

-4.5%

-3.1%

-0.7%

-0.6%

wordcount

-18.6%

-10.9%

6.9%

-12.9%

-10.0%

-9.2%

-8.0%

kmeans

86.9%

95.8%

123.3%

99.3%

127.9%

102.6%

95.3%

scan

-25.5%

-21.0%

-12.4%

-19.8%

-19.7%

-20.5%

8.6%

bayes

-29.7%

-31.3%

-30.9%

-31.1%

-31.0%

-30.1%

-27.6%

aggregation

-15.3%

-15.0%

-37.6%

-37.0%

-37.3%

7.6%

12.9%

join

-12.7%

-13.9%

-16.4%

-17.8%

-14.8%

-13.2%

-15.6%

sort

-17.5%

-17.3%

-20.7%

-17.7%

-13.9%

-15.6%

-16.9%

pagerank

-11.4%

-13.0%

-11.4%

-10.1%

-12.0%

-11.7%

-11.0%

terasort

-16.7%

-17.0%

-16.3%

-11.9%

-13.1%

-15.7%

-14.5%

Comments: null means no such workload running or workload failed in this time.
[http://01org.github.io/sparkscore/image/plaf1.time/HiBench_workloads.png]
Y-Axis: normalized completion time; X-Axis: Work Week.
The commit number can be found in the result table.
The performance score for each workload is normalized based on the elapsed time for 1.2 release. The lower the better.
spark-perf
________________________________
JOB

ww24

ww25

ww26

ww27

ww28

ww29

ww30

commit

db81b9d8

4eb48ed1

32e3cdaa

ec784381

2b820f2a

c472eb17

a803ac3e

agg

5.2%

2.5%

1.1%

3.0%

-18.8%

-19.0%

-18.4%

agg-int

4.0%

8.2%

7.0%

7.5%

6.2%

11.4%

5.5%

agg-naive

-6.7%

-6.8%

-8.5%

-6.9%

-15.5%

-18.0%

-20.6%

scheduling

-6.4%

-6.5%

-5.7%

-1.8%

-6.0%

-9.7%

-1.0%

count-filter

-10.2%

-10.4%

-9.8%

-10.4%

-18.0%

-17.4%

-19.6%

count

-7.3%

-7.0%

-8.0%

-7.4%

-15.1%

-14.3%

-16.5%

sort

-14.6%

-14.4%

-13.9%

-15.9%

-24.0%

-23.2%

-21.7%

sort-int

-1.5%

-2.2%

-5.3%

-5.0%

-11.3%

-9.6%

-6.4%

Comments: null means no such workload running or workload failed in this time.
[http://01org.github.io/sparkscore/image/plaf1.time/spark-perf_workloads.png]
Y-Axis: normalized completion time; X-Axis: Work Week.
The commit number can be found in the result table.
The performance score for each workload is normalized based on the elapsed time for 1.2 release. The lower the better.
"
Sean Owen <sowen@cloudera.com>,"Mon, 27 Jul 2015 12:34:20 +0100",Re: Policy around backporting bug fixes,Patrick Wendell <pwendell@gmail.com>,"I took the liberty of adding this to the wiki, where it can change
further if needed.

https://cwiki.apache.org/confluence/display/SPARK/Committers#Committers-PolicyonBackportingBugFixes


---------------------------------------------------------------------


"
Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"Mon, 27 Jul 2015 06:29:06 -0700 (MST)",Is `dev/lint-python` broken?,dev@spark.apache.org,"Hi all,

When I run `dev/lint-python` at the latest master branch, I got an error
message as follows.
Is the lint script broken? Or is there any problems with my environment?

```
$> ./dev/lint-python
./dev/lint-python: line 64: syntax error near unexpected token `>'
./dev/lint-python: line 64: `    easy_install -d ""$PYLINT_HOME""
pylint==1.4.4 &>> ""$PYLINT_INSTALL_INFO""'
```

If the redirect is a syntax error, I'll send a PR to fix.

Thanks,
Yu



-----
-- Yu Ishikawa
--

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Mon, 27 Jul 2015 14:39:17 +0100",Re: Is `dev/lint-python` broken?,Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"I do not see any problem like that on master. The syntax looks valid.
Do you have an old version bash?


---------------------------------------------------------------------


"
Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"Mon, 27 Jul 2015 06:50:55 -0700 (MST)",Re: Is `dev/lint-python` broken?,dev@spark.apache.org,"Hi Sean,

Thank you for answering my question.
It seems that I used an old version bash which is the default Mac bash.

```
$> bash --version
GNU bash, version 3.2.57(1)-release (x86_64-apple-darwin14)
Copyright (C) 2007 Free Software Foundation, Inc.
share_history
```

Thanks,
Yu



-----
-- Yu Ishikawa
--

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Mon, 27 Jul 2015 14:58:53 +0100",Re: Is `dev/lint-python` broken?,Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"I'm on 4.3.39, though that is probably newer than what comes with Macs
in general as I use brew to get newer versions of lots of things (this
may be a good option for you in general if you're a developer). What
OS X are you on -- is it also old? or is this likely to be a more
widespread problem?


---------------------------------------------------------------------


"
Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"Mon, 27 Jul 2015 07:10:30 -0700 (MST)",Re: Is `dev/lint-python` broken?,dev@spark.apache.org,"I'm using 10.10.4. And Xcode is version 6.4. Maybe, it isn't old.
I guess the old bash version causes the problem. I'll try to install another
bash with brew.




-----
-- Yu Ishikawa
--

---------------------------------------------------------------------


"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Mon, 27 Jul 2015 15:42:58 +0000",Two joins in GraphX Pregel implementation,"""dev@spark.apache.org"" <dev@spark.apache.org>","Dear Spark developers,

Below is the GraphX Pregel code snippet from https://spark.apache.org/docs/latest/graphx-programming-guide.html#pregel-api:
(it does not contain caching step):

while (activeMessages > 0 && i < maxIterations) {
     // Receive the messages: -----------------------------------------------------------------------
      // (1st join) Run the vertex program on all vertices that receive messages
      val newVerts = g.vertices.innerJoin(messages)(vprog).cache()
      // (2nd join) Merge the new vertex values back into the graph
      g = g.outerJoinVertices(newVerts) { (vid, old, newOpt) => newOpt.getOrElse(old) }.cache()
      // Send Messages: ------------------------------------------------------------------------------
      // Vertices that didn't receive a message above don't appear in newVerts and therefore don't
      // get to send messages.  More precisely the map phase of mapReduceTriplets is only invoked
      // on edges in the activeDir of vertices in newVerts
      messages = g.mapReduceTriplets(sendMsg, mergeMsg, Some((newVerts, activeDir))).cache()
      activeMessages = messages.count()
      i += 1
    }

It seems that the mentioned two joins can be rewritten as one outer join as follows:
g = g.outerJoinVertices(messages) { (vid, old, mess) => mess match {
  case Some(mess) => vprog(vid, old, mess)
  case None => old }
}
This code passes PregelSuite (after removing newVerts). Could you elaborate why two joins are used instead of one and why do you need intermediate variable `newVerts`? Are there some performance considerations?

Best regards, Alexander
"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Mon, 27 Jul 2015 16:01:47 +0000",RE: Two joins in GraphX Pregel implementation,Robin East <robin.east@xense.co.uk>,"Thank you, your explanation does make sense to me. Do you think that one join will work if `mapReduceTriplets` is replaced by the new `aggregateMessages`? The latter does not return the vertices that did not receive a message.
onday, July 27, 2015 8:56 AM
To: Ulanov, Alexander
Cc: dev@spark.apache.org
Subject: Re: Two joins in GraphX Pregel implementation

What happens to this line of code:

messages = g.mapReduceTriplets(sendMsg, mergeMsg, Some((newVerts, activeDir))).cache()

Part of the Pregel â€˜contractâ€™ is that vertices that donâ€™t receive messages from the previous superstep donâ€™t get to send messages this superstep. Not sure if there is a test for that but there ought to be.

Robin


On 27 Jul 2015, at 16:42, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:

Dear Spark developers,

Below is the GraphX Pregel code snippet from https://spark.apache.org/docs/latest/graphx-programming-guide.html#pregel-api:
(it does not contain caching step):

while (activeMessages > 0 && i < maxIterations) {
     // Receive the messages: -----------------------------------------------------------------------
      // (1st join) Run the vertex program on all vertices that receive messages
      val newVerts = g.vertices.innerJoin(messages)(vprog).cache()
      // (2nd join) Merge the new vertex values back into the graph
      g = g.outerJoinVertices(newVerts) { (vid, old, newOpt) => newOpt.getOrElse(old) }.cache()
      // Send Messages: ------------------------------------------------------------------------------
      // Vertices that didn't receive a message above don't appear in newVerts and therefore don't
      // get to send messages.  More precisely the map phase of mapReduceTriplets is only invoked
      // on edges in the activeDir of vertices in newVerts
      messages = g.mapReduceTriplets(sendMsg, mergeMsg, Some((newVerts, activeDir))).cache()
      activeMessages = messages.count()
      i += 1
    }

It seems that the mentioned two joins can be rewritten as one outer join as follows:
g = g.outerJoinVertices(messages) { (vid, old, mess) => mess match {
  case Some(mess) => vprog(vid, old, mess)
  case None => old }
}
This code passes PregelSuite (after removing newVerts). Could you elaborate why two joins are used instead of one and why do you need intermediate variable `newVerts`? Are there some performance considerations?

Best regards, Alexander

"
Nitin Goyal <nitin2goyal@gmail.com>,"Mon, 27 Jul 2015 09:08:49 -0700 (MST)",Ever increasing physical memory for a Spark Application in YARN,dev@spark.apache.org,"I am running a spark application in YARN having 2 executors with Xms/Xmx as
32 Gigs and spark.yarn.excutor.memoryOverhead as 6 gigs.

I am seeing that the app's physical memory is ever increasing and finally
gets killed by node manager

2015-07-25 15:07:05,354 WARN
org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl:
Container [pid=10508,containerID=container_1437828324746_0002_01_000003] is
running beyond physical memory limits. Current usage: 38.0 GB of 38 GB
physical memory used; 39.5 GB of 152 GB virtual memory used. Killing
container.
Dump of the process-tree for container_1437828324746_0002_01_000003 :
    |- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS)
SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE
    |- 10508 9563 10508 10508 (bash) 0 0 9433088 314 /bin/bash -c
-Xms32768m -Xmx32768m  -Dlog4j.configuration=log4j-executor.properties
-XX:MetaspaceSize=512m -XX:+UseG1GC -XX:+PrintGCTimeStamps
-XX:+PrintGCDateStamps -XX:+PrintGCDetails -Xloggc:gc.log
-XX:AdaptiveSizePolicyOutputInterval=1  -XX:+UseGCLogFileRotation
-XX:GCLogFileSize=500M -XX:NumberOfGCLogFiles=1
-XX:MaxDirectMemorySize=3500M -XX:NewRatio=3 -Dcom.sun.management.jmxremote
-Dcom.sun.management.jmxremote.port=36082
-Dcom.sun.management.jmxremote.authenticate=false
-Dcom.sun.management.jmxremote.ssl=false -XX:NativeMemoryTracking=detail
-XX:ReservedCodeCacheSize=100M -XX:MaxMetaspaceSize=512m
-XX:CompressedClassSpaceSize=256m
-Djava.io.tmpdir=/data/yarn/datanode/nm-local-dir/usercache/admin/appcache/application_1437828324746_0002/container_1437828324746_0002_01_000003/tmp
'-Dspark.driver.port=43354'
-Dspark.yarn.app.container.log.dir=/opt/hadoop/logs/userlogs/application_1437828324746_0002/container_1437828324746_0002_01_000003
org.apache.spark.executor.CoarseGrainedExecutorBackend
akka.tcp://sparkDriver@nn1:43354/user/CoarseGrainedScheduler 1 dn3 6
application_1437828324746_0002 1>
/opt/hadoop/logs/userlogs/application_1437828324746_0002/container_1437828324746_0002_01_000003/stdout
2>
/opt/hadoop/logs/userlogs/application_1437828324746_0002/container_1437828324746_0002_01_000003/stderr


I diabled YARN's parameter ""yarn.nodemanager.pmem-check-enabled"" and noticed
that physical memory usage went till 40 gigs

I checked the total RSS in /proc/pid/smaps and it was same value as physical
memory reported by Yarn and seen in top command.

I checked that its not a problem with the heap but something is increasing
in off heap/ native memory. I used tools like Visual VM but didn't find
anything that's increasing there. MaxDirectMmeory also didn't exceed 600MB.
Peak number of active threads was 70-80 and thread stack size didn't exceed
100MB. MetaspaceSize was around 60-70MB.

FYI, I am on Spark 1.2 and Hadoop 2.4.0 and my spark application is based on
Spark SQL and it's an HDFS read/write intensive application and caches data
in Spark SQL's in-memory caching

Any help would be highly appreciated. Or any hint that where should I look
to debug memory leak or if any tool already there. Let me know if any other
information is needed.



--

---------------------------------------------------------------------


"
Josh Rosen <rosenville@gmail.com>,"Mon, 27 Jul 2015 10:16:17 -0700",Re: Worker memory leaks?,Richard Marscher <rmarscher@localytics.com>,"I have promoted https://issues.apache.org/jira/browse/SPARK-9202 to a
blocker to ensure that we get a fix for it before 1.5.0  I'm pretty swamped
with other tasks for the next few days, but I'd be happy to shepherd a
bugfix patch for this (this should be pretty straightforward and the JIRA
ticket contains a sketch of how I'd do it).


"
Vyacheslav Baranov <slavik.baranov@gmail.com>,"Mon, 27 Jul 2015 21:23:12 +0300",Converting DataFrame to RDD of case class,dev@spark.apache.org,"Hi all,

For now it's possible to convert RDD of case class to DataFrame:

case class Person(name: String, age: Int)

val people: RDD[Person] = ...
val df = sqlContext.createDataFrame(people)

but backward conversion is not possible with existing API, so currently 
code looks like this (example from documentation):

teenagers.map(t => ""Name: "" + t.getAs[String](""name""))

whereas it would be much more convenient to use RDD of case class:

teenagers.rdd[Person].map(""Name: "" + _.name)


I've implemented proof of concept library that allows to convert 
DataFrame to typed RDD with ""Pimp my library"" pattern. It adds some 
typesafety (conversion fails before running distributed operation if 
some fields have incompatible types) and it's much more convenient when 
working with nested rows, for example:

case class Room(number: Int, visitors: Seq[Person])

roomsDf.explode[Seq[Row], Person](""visitors"", ""visitor"")(_.map(rowToPerson))

Would the community be interested in having this functionality in core?

Regards,
Vyacheslav

"
Reynold Xin <rxin@databricks.com>,"Mon, 27 Jul 2015 11:51:17 -0700",Re: Converting DataFrame to RDD of case class,Vyacheslav Baranov <slavik.baranov@gmail.com>,"There is this pull request: https://github.com/apache/spark/pull/5713

We mean to merge it for 1.5. Maybe you can help review it too?


"
Imran Rashid <irashid@cloudera.com>,"Mon, 27 Jul 2015 14:49:13 -0500",Re: non-deprecation compiler warnings are upgraded to build errors now,Josh Rosen <rosenville@gmail.com>,"Does scoverage work with the spark build in 2.11?  That sounds like a big
win


fe.com
:
e
or
/spark/rdd/BinaryFileRDD.scala:31: no valid targets for annotation on value conf - it is discarded unused. You may specify targets with meta-annotations, e.g. @(transient @param)
s too
 looked
y
eamingContext)
. You may specify targets with meta-annotations, e.g. @(transient @param)
ure if
on my
h
so
ue to
ed
idual
o need
"
Pedro Rodriguez <ski.rodriguez@gmail.com>,"Mon, 27 Jul 2015 13:09:52 -0700",Re: Is `dev/lint-python` broken?,Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"I am having the same issue, but the python style checks are failing on the
Jenkins build server. Is anyone else having this problem? Failed build is
here:
https://amplab.cs.berkeley.edu/jenkins/job/SlowSparkPullRequestBuilder/121/console

Pedro Rodriguez




-- 
Pedro Rodriguez
UCBerkeley 2014 | Computer Science
SnowGeek <http://SnowGeek.org>
pedro-rodriguez.com
ski.rodriguez@gmail.com
208-340-1703
"
Reynold Xin <rxin@databricks.com>,"Mon, 27 Jul 2015 13:11:50 -0700",Re: Is `dev/lint-python` broken?,Pedro Rodriguez <ski.rodriguez@gmail.com>,"I just pushed a hotfix to disable Pylint.



"
Pedro Rodriguez <ski.rodriguez@gmail.com>,"Mon, 27 Jul 2015 13:14:17 -0700",Re: Is `dev/lint-python` broken?,Reynold Xin <rxin@databricks.com>,"Should there be any delay in Jenkins using that? I rebased/pushed code to
most recent master after the hotfix commit (and double checked just now),
but the build still fails.




-- 
Pedro Rodriguez
UCBerkeley 2014 | Computer Science
SnowGeek <http://SnowGeek.org>
pedro-rodriguez.com
ski.rodriguez@gmail.com
208-340-1703
"
Jonathan Winandy <jonathan.winandy@gmail.com>,"Mon, 27 Jul 2015 20:30:38 +0000",Re: Converting DataFrame to RDD of case class,"Reynold Xin <rxin@databricks.com>, Vyacheslav Baranov <slavik.baranov@gmail.com>","Hello !

Can both methods be compare in term of performance ? Tried the pull request
and it felt slow compare to manual mapping.

Cheers,
Jonathan


"
Ryan Williams <ryan.blake.williams@gmail.com>,"Mon, 27 Jul 2015 17:59:31 -0400","""Spree"": Live-updating web UI for Spark","""dev@spark.apache.org"" <dev@spark.apache.org>","Hi dev@spark, I wanted to quickly ping about Spree
<http://www.hammerlab.org/2015/07/25/spree-58-a-live-updating-web-ui-for-spark/>,
a live-updating web UI for Spark that I released on Friday (along with some
supporting infrastructure), and mention a couple things that came up while
I worked on it that are relevant to this list.

This blog post
<http://www.hammerlab.org/2015/07/25/spree-58-a-live-updating-web-ui-for-spark/>
and github <https://github.com/hammerlab/spree/> have lots of info about
functionality, implementation details, and installation instructions, but
the tl;dr is:

   - You register a SparkListener called JsonRelay
   <https://github.com/hammerlab/spark-json-relay> via the
   spark.extraListeners conf (thanks @JoshRosen!).
   - That listener ships SparkListenerEvents to a server called slim
   <https://github.com/hammerlab/slim> that stores them in Mongo.
      - Really what it stores are a bunch of stats similar to those
      maintained by JobProgressListener.
    - A Meteor <https://www.meteor.com/> app displays live-updating views
   of whatâ€™s in Mongo.

Feel free to read about it / try it! but the rest of this email is just
questions about Spark APIs and plans.
JsonProtocol scoping

The most awkward thing about Spree is that JsonRelay declares itself to be
in org.apache.spark
<https://github.com/hammerlab/spark-json-relay/blob/1.0.0/src/main/scala/org/apache/spark/JsonRelay.scala#L1>
so that it can use JsonProtocol.

Will JsonProtocol be private[spark] forever, on purpose, or is it just not
considered stable enough yet, so you want to discourage direct use? Iâ€™m
relatively impartial at this point since Iâ€™ve done the hacky thing and it
works for my purposes, but thought Iâ€™d ask in case there are interesting
perspectives on the ideal scope for it going forward.
@DeveloperApi trait SparkListener

Another set of tea leaves I wasnâ€™t sure how to read was the @DeveloperApi-ness
of SparkListener
<https://github.com/apache/spark/blob/v1.4.1/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala#L131-L132>.
I assumed I was doing something frowny by having JsonRelay implement the
SparkListener interface. However, I just noticed that Iâ€™m actually
extending SparkFirehoseListener
<https://github.com/apache/spark/blob/v1.4.1/core/src/main/java/org/apache/spark/SparkFirehoseListener.java>,
which is *not* @DeveloperApi afaict, so maybe Iâ€™m ok there after all?

Are there other SparkListener implementations of note in the wild (seems
like â€œnoâ€)? Is that an API that people can and should use externally (seems
like â€œyesâ€ to me)? I saw @vanzin recently imply on this list that the
answers may be â€œnoâ€ and â€œnoâ€
<http://apache-spark-developers-list.1001551.n3.nabble.com/Slight-API-incompatibility-caused-by-SPARK-4072-tp13257.html>
.
Augmenting JsonProtocol

JsonRelay does two things that JsonProtocol does not:

   - adds an appId field to all events; this makes it possible/easy for
   downstream things (slim, in this case) to handle information about
   multiple Spark applications.
   - JSON-serializes SparkListenerExecutorMetricsUpdate events. This was
   added to JsonProtocol in SPARK-9036
   <https://issues.apache.org/jira/browse/SPARK-9036> (though itâ€™s unused
   in the Spark repo currently), but Iâ€™ll have to leave my version in as long
   as I want to support Spark <= 1.4.1.
      - From one perspective, JobProgressListener was sort of â€œcheatingâ€ by
      using these events that were previously not accessible via
      JsonProtocol.

It seems like making an effort to let external tools get the same kinds of
data as the internal listeners is a good principle to try to maintain,
which is also relevant to the scoping questions about JsonProtocol above.

Should JsonProtocol add appIds to all events itself? Should Spark make it
easier for downstream things to to process events from multiple Spark
applications? JsonRelay currently pulls the app ID out of the SparkConf
that it is instantiated with
<https://github.com/hammerlab/spark-json-relay/blob/1.0.0/src/main/scala/org/apache/spark/JsonRelay.scala#L16>;
it works, but also feels hacky and like maybe Iâ€™m doing things Iâ€™m not
supposed to.
Thrift SparkListenerEvent Implementation?

A few months ago I built a first version of this project involving a
SparkListener called Spear <https://github.com/hammerlab/spear> that
aggregated stats from SparkListenerEvents *and* wrote those stats to Mongo,
combining JsonRelay and slim from above.

Spear used a couple of libraries (Rogue
<https://github.com/foursquare/rogue> and Spindle
<https://github.com/foursquare/spindle>) to define schemas in thrift,
generate Scala for those classes, and do all the Mongo querying in a nice,
type-safe way.

Unfortunately for me, all of the Mongo queries were synchronous in that
implementation, which led to events being dropped
<https://github.com/apache/spark/blob/v1.4.1/core/src/main/scala/org/apache/spark/scheduler/LiveListenerBus.scala#L40>
when I tested it on large jobs (thanks a lot to @squito for helping debug
that). I got spooked and decided all possible work had to be removed from
the Spark driver, hence JsonRelay doing the most minimal possible thing and
sending the events to a network address.

I also decided to rewrite my JobProgressListener-equivalent piece in Node
rather than Scala because I was finding the type-safety a little
constraining and Rogue and Spindle are no longer maintained.

Anyways, a couple of things I want to call out, given this back-story:

   - There is a bunch of boilerplate related to implementing my Mongo
   schemas sitting abandoned in Spear, in thrift
   <https://github.com/hammerlab/spear/blob/5f2affe80fae833a120eb63d51154c3c00ee57b0/src/main/thrift/spark.thrift>
   and case classes
   <https://github.com/hammerlab/spear/blob/6f9efa9aab0ce00fe229083ded237199aafd9b74/src/main/scala/org/hammerlab/spear/SparkIDL.scala>
   .
      SparkListenerEvents themselves in thrift and/or case classes.
      - They seem like things youâ€™d want to be able to be able to send in
      and out of various services/languages easily.
      - Writing the raw events to Mongo is a related goal for slim, covered
      by slim#35 <https://github.com/hammerlab/slim/issues/35>.
    - Itâ€™s probably not super difficult to take something like Spear and
   make it use non-blocking Mongo queries.
      - This might make it quick enough to not have to drop events.
      - It would simplify the infrastructure story by folding slim into the
      listener that is registered with the driver.

Possible Upstreaming

I donâ€™t know what the interest level for getting any of this functionality
(e.g. live-updating web UI, event-stats persistence to Mongo) upstreamed is
and donâ€™t have time to work on that, so I wonâ€™t go into much detail here
but if anyoneâ€™s interested I have some ideas about how some of it could
happen (e.g. DDP <https://www.meteor.com/ddp> server implementation in
java/scala serving the web UI; Spree currently involves two JS servers so
some rewriting of things would probably have to happen), why it might be
good to do, and why it might be not good or not worth it (e.g. Spark should
make sure itâ€™s possible and easy to do sophisticated things like this
outside of the Spark repo, putting more work in the driver process is a bad
idea, etc.).

OK, thatâ€™s my brain dump, Iâ€™d love to hear peoplesâ€™ thoughts on any/all of
this, otherwise thanks for the APIs and sorry for having to cheat them a
bit! :)

-Ryan
â€‹
"
Pedro Rodriguez <ski.rodriguez@gmail.com>,"Mon, 27 Jul 2015 16:17:55 -0700","Re: ""Spree"": Live-updating web UI for Spark",Ryan Williams <ryan.blake.williams@gmail.com>,"+1 to awesome work. I saw it this morning and it solves a annoyance/problem
I have had for a while, and even thought about maybe contributing something
for. I am excited to give it a try.

Pedro


spark/>,
me
e
spark/>
org/apache/spark/JsonRelay.scala#L1>"
Niranda Perera <niranda.perera@gmail.com>,"Tue, 28 Jul 2015 10:31:06 +0530",dynamically update the master list of a worker or a spark context,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,

I have been developing a custom recovery implementation for spark masters
and workers using hazlecast clustering.

in the Spark worker code [1], we see that a list of masters needs to be
provided at the worker start up, in order to achieve high availability.
this effectively means that one should know the urls of possible masters,
before spawning a worker. same applies to the spark context (Pls correct me
if I'm wrong)

In our implementation we are planning to add masters dynamically. for an
example, say the elected leader goes down, then we would spawn another
master in the cluster and all the workers connected to the previous master
should connect to the newly spawned master. to do this, we need provision
to dynamically update the master list of an already spawned worker.

can this be achieved as of the current spark implementation?

rgds

[1]
https://github.com/apache/spark/blob/v1.4.0/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala#L538

-- 
Niranda
@n1r44 <https://twitter.com/N1R44>
https://pythagoreanscript.wordpress.com/
"
Debasish Das <debasish.das83@gmail.com>,"Mon, 27 Jul 2015 22:01:53 -0700","RE: Package Release Annoucement: Spark SQL on HBase ""Astro""","""Yan Zhou. sc"" <Yan.Zhou.sc@huawei.com>","Hi Yan,

Is it possible to access the hbase table through spark sql jdbc layer ?

Thanks.
Deb

e:
essor
m
e
ys
he
e
.
inks at this package homepage, if
r ultra low
"
"""Yan Zhou.sc"" <Yan.Zhou.sc@huawei.com>","Tue, 28 Jul 2015 05:12:12 +0000","RE: Package Release Annoucement: Spark SQL on HBase ""Astro""",Debasish Das <debasish.das83@gmail.com>,"HBase in this case is no different from any other Spark SQL data sources, so yes you should be able to access HBase data through Astro from Spark SQLâ€™s JDBC interface.

Graphically, the access path is as follows:

Spark SQL JDBC Interface -> Spark SQL Parser/Analyzer/Optimizer->Astro Optimizer-> HBase Scans/Gets -> â€¦ -> HBase Region server


Regards,

Yan

From: Debasish Das [mailto:debasish.das83@gmail.com]
Sent: Monday, July 27, 2015 10:02 PM
To: Yan Zhou.sc
Cc: Bing Xiao (Bing); dev; user
Subject: RE: Package Release Annoucement: Spark SQL on HBase ""Astro""


Hi Yan,

Is it possible to access the hbase table through spark sql jdbc layer ?

Thanks.
Deb
On Jul 22, 2015 9:03 PM, ""Yan Zhou.sc"" <Yan.Zhou.sc@huawei.com<mailto:Yan.Zhou.sc@huawei.com>> wrote:
Yes, but not all SQL-standard insert variants .

From: Debasish Das [mailto:debasish.das83@gmail.com<mailto:debasish.das83@gmail.com>]
Sent: Wednesday, July 22, 2015 7:36 PM
To: Bing Xiao (Bing)
Cc: user; dev; Yan Zhou.sc
Subject: Re: Package Release Annoucement: Spark SQL on HBase ""Astro""


Does it also support insert operations ?
On Jul 22, 2015 4:53 PM, ""Bing Xiao (Bing)"" <bing.xiao@huawei.com<mailto:bing.xiao@huawei.com>> wrote:
We are happy to announce the availability of the Spark SQL on HBase 1.0.0 release.  http://spark-packages.org/package/Huawei-Spark/Spark-SQL-on-HBase
The main features in this package, dubbed â€œAstroâ€, include:

â€¢         Systematic and powerful handling of data pruning and intelligent scan, based on partial evaluation technique

â€¢         HBase pushdown capabilities like custom filters and coprocessor to support ultra low latency processing

â€¢         SQL, Data Frame support

â€¢         More SQL capabilities made possible (Secondary index, bloom filter, Primary Key, Bulk load, Update)

â€¢         Joins with data from other sources

â€¢         Python/Java/Scala support

â€¢         Support latest Spark 1.4.0 release


The tests by Huawei team and community contributors covered the areas: bulk load; projection pruning; partition pruning; partial evaluation; code generation; coprocessor; customer filtering; DML; complex filtering on keys and non-keys; Join/union with non-Hbase data; Data Frame; multi-column family test.  We will post the test results including performance tests the middle of August.
You are very welcomed to try out or deploy the package, and help improve the integration tests with various combinations of the settings, extensive Data Frame tests, complex join/union test and extensive performance tests.  Please use the â€œIssuesâ€ â€œPull Requestsâ€ links at this package homepage, if you want to report bugs, improvement or feature requests.
Special thanks to project owner and technical leader Yan Zhou, Huawei global team, community contributors and Databricks.   Databricks has been providing great assistance from the design to the release.
â€œAstroâ€, the Spark SQL on HBase package will be useful for ultra low latency query and analytics of large scale data sets in vertical enterprises. We will continue to work with the community to develop new features and improve code base.  Your comments and suggestions are greatly appreciated.

Yan Zhou / Bing Xiao
Huawei Big Data team

"
Tathagata Das <tdas@databricks.com>,"Mon, 27 Jul 2015 23:42:22 -0700",Re: Writing streaming data to cassandra creates duplicates,Priya Ch <learnings.chitturi@gmail.com>,"You have to partition that data on the Spark Streaming by the primary key,
and then make sure insert data into Cassandra atomically per key, or per
set of keys in the partition. You can use the combination of the (batch
time, and partition Id) of the RDD inside foreachRDD as the unique id for
the data you are inserting. This will guard against multiple attempts to
run the task that inserts into Cassandra.

See
http://spark.apache.org/docs/latest/streaming-programming-guide.html#semantics-of-output-operations

TD


"
Debasish Das <debasish.das83@gmail.com>,"Tue, 28 Jul 2015 00:13:29 -0700","Re: Package Release Annoucement: Spark SQL on HBase ""Astro""","""Yan Zhou.sc"" <Yan.Zhou.sc@huawei.com>","That's awesome Yan. I was considering Phoenix for SQL calls to HBase since
Cassandra supports CQL but HBase QL support was lacking. I will get back to
you as I start using it on our loads.

I am assuming the latencies won't be much different from accessing HBase
through tsdb asynchbase as that's one more option I am looking into.


m
e:
essor
m
e
ys
he
e
.
inks at this package homepage, if
r ultra low
"
Akhil Das <akhil@sigmoidanalytics.com>,"Tue, 28 Jul 2015 13:21:18 +0530","Re: ReceiverStream SPARK not able to cope up with 20,000 events /sec .",anshu shukla <anshushukla0@gmail.com>,"You need to find the bottleneck here, it could your network (if the data is
huge) or your producer code isn't pushing at 20k/s, If you are able to
produce at 20k/s then make sure you are able to receive at that rate (try
it without spark).

Thanks
Best Regards


"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Tue, 28 Jul 2015 08:15:34 +0000",RE: Two joins in GraphX Pregel implementation,Robin East <robin.east@xense.co.uk>,"Iâ€™ve found two PRs (almost identical) for replacing mapReduceTriplets with aggregateMessages:
https://github.com/apache/spark/pull/3782
https://github.com/apache/spark/pull/3883
First is closed by Daveâ€™s suggestion, second is stale.
Also there is a PR for the new Pregel API, which is also closed.

Do you know the reason why this improvement is not pushed?

CCâ€™ing Dave

From: Robin East [mailto:robin.east@xense.co.uk]
Sent: Monday, July 27, 2015 9:11 AM
To: Ulanov, Alexander
Cc: dev@spark.apache.org
Subject: Re: Two joins in GraphX Pregel implementation

Quite possibly - there is a JIRA open for replacing mapReduceTriplets with aggregateMessages (donâ€™t recall the number off the top of my head)

Robin
On 27 Jul 2015, at 17:01, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:

Thank you, your explanation does make sense to me. Do you think that one join will work if `mapReduceTriplets` is replaced by the new `aggregateMessages`? The latter does not return the vertices that did not receive a message.
From: Robin East [mailto:robin.east@xense.co.uk]
Sent: Monday, July 27, 2015 8:56 AM
To: Ulanov, Alexander
Cc: dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: Two joins in GraphX Pregel implementation

What happens to this line of code:

messages = g.mapReduceTriplets(sendMsg, mergeMsg, Some((newVerts, activeDir))).cache()

Part of the Pregel â€˜contractâ€™ is that vertices that donâ€™t receive messages from the previous superstep donâ€™t get to send messages this superstep. Not sure if there is a test for that but there ought to be.

Robin


On 27 Jul 2015, at 16:42, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:

Dear Spark developers,

Below is the GraphX Pregel code snippet from https://spark.apache.org/docs/latest/graphx-programming-guide.html#pregel-api:
(it does not contain caching step):

while (activeMessages > 0 && i < maxIterations) {
     // Receive the messages: -----------------------------------------------------------------------
      // (1st join) Run the vertex program on all vertices that receive messages
      val newVerts = g.vertices.innerJoin(messages)(vprog).cache()
      // (2nd join) Merge the new vertex values back into the graph
      g = g.outerJoinVertices(newVerts) { (vid, old, newOpt) => newOpt.getOrElse(old) }.cache()
      // Send Messages: ------------------------------------------------------------------------------
      // Vertices that didn't receive a message above don't appear in newVerts and therefore don't
      // get to send messages.  More precisely the map phase of mapReduceTriplets is only invoked
      // on edges in the activeDir of vertices in newVerts
      messages = g.mapReduceTriplets(sendMsg, mergeMsg, Some((newVerts, activeDir))).cache()
      activeMessages = messages.count()
      i += 1
    }

It seems that the mentioned two joins can be rewritten as one outer join as follows:
g = g.outerJoinVertices(messages) { (vid, old, mess) => mess match {
  case Some(mess) => vprog(vid, old, mess)
  case None => old }
}
This code passes PregelSuite (after removing newVerts). Could you elaborate why two joins are used instead of one and why do you need intermediate variable `newVerts`? Are there some performance considerations?

Best regards, Alexander

"
Sachith Withana <swsachith@gmail.com>,"Tue, 28 Jul 2015 14:45:19 +0530",Custom UDFs with zero parameters support,dev@spark.apache.org,"Hi all,

Currently I need to support custom UDFs with sparkSQL queries which have no
parameters.

ex: now() : which returns the current time in milliseconds.

Spark currently have support for UDFs having 1 or more parameters but does
not contain a UDF0 Adaptor. Is there a way to implement this?

Or is there a way to support custom keywords such as ""now"" which would act
as an custom UDF with no parameters.


-- 
Thanks,
Sachith Withana
"
Justin Uang <justin.uang@gmail.com>,"Tue, 28 Jul 2015 09:36:35 +0000","DataFrame#rdd doesn't respect DataFrame#cache, slowing down CrossValidator","""dev@spark.apache.org"" <dev@spark.apache.org>","Hey guys,

I'm running into some pretty bad performance issues when it comes to using
a CrossValidator, because of caching behavior of DataFrames.

The root of the problem is that while I have cached my DataFrame
representing the features and labels, it is caching at the DataFrame level,
while CrossValidator/LogisticRegression both drop down to the dataset.rdd
level, which ignores the caching that I have previously done. This is
worsened by the fact that for each combination of a fold and a param set
from the grid, it recomputes my entire input dataset because the caching
was lost.

My current solution is to force the input DataFrame to be based off of a
cached RDD, which I did with this horrible hack (had to drop down to java
from the pyspark because of something to do with vectors not be inferred
correctly):

def checkpoint_dataframe_caching(df):
    return
DataFrame(sqlContext._ssql_ctx.createDataFrame(df._jdf.rdd().cache(),
train_data._jdf.schema()), sqlContext)

before I pass it into the CrossValidator.fit(). If I do this, I still have
to cache the underlying rdd once more than necessary (in addition to
DataFrame#cache()), but at least in cross validation, it doesn't recompute
the RDD graph anymore.

Note, that input_df.rdd.cache() doesn't work because the python
CrossValidator implementation applies some more dataframe transformations
like filter, which then causes filtered_df.rdd to return a completely
different rdd that recomputes the entire graph.

Is it the intention of Spark SQL that calling DataFrame#rdd removes any
caching that was done for the query? Is the fix as simple as getting the
DataFrame#rdd to reference the cached query, or is there something more
subtle going on.

Best,

Justin
"
StanZhai <mail@zhaishidan.cn>,"Tue, 28 Jul 2015 05:32:53 -0700 (MST)","[Spark SQL]Could not read parquet table after recreating it with
 the same table name",dev@spark.apache.org,"Hi all,

I'am using SparkSQL in Spark 1.4.1. I encounter an error when using parquet
table after recreating it, we can reproduce the error as following:

```scala
// hc is an instance of HiveContext
hc.sql(""select * from b"").show()         // this is ok and b is a parquet
table
val df = hc.sql(""select * from a"")
df.write.mode(SaveMode.Overwrite).saveAsTable(""b"")
hc.sql(""select * from b"").show()         // got error
```

The error is: 

java.io.FileNotFoundException: File does not exist:
/user/hive/warehouse/test.db/b/part-r-00004-3abcbb07-e20a-4b5e-a6e5-59356c3d3149.gz.parquet
	at
org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:65)
	at
org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:55)
	at
org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsUpdateTimes(FSNamesystem.java:1716)
	at
org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1659)
	at
org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1639)
	at
org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1613)
	at
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:497)
	at
org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:322)
	at
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at
org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at
org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at
sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at
sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at
org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
	at
org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)
	at
org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1144)
	at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1132)
	at org.apache.hadoop.hdfs.DFSClient.getBlockLocations(DFSClient.java:1182)
	at
org.apache.hadoop.hdfs.DistributedFileSystem$1.doCall(DistributedFileSystem.java:218)
	at
org.apache.hadoop.hdfs.DistributedFileSystem$1.doCall(DistributedFileSystem.java:214)
	at
org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at
org.apache.hadoop.hdfs.DistributedFileSystem.getFileBlockLocations(DistributedFileSystem.java:214)
	at
org.apache.hadoop.hdfs.DistributedFileSystem.getFileBlockLocations(DistributedFileSystem.java:206)
	at
org.apache.spark.sql.parquet.FilteringParquetRowInputFormat$$anonfun$getTaskSideSplits$1.apply(ParquetTableOperations.scala:625)
	at
org.apache.spark.sql.parquet.FilteringParquetRowInputFormat$$anonfun$getTaskSideSplits$1.apply(ParquetTableOperations.scala:621)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at
org.apache.spark.sql.parquet.FilteringParquetRowInputFormat.getTaskSideSplits(ParquetTableOperations.scala:621)
	at
org.apache.spark.sql.parquet.FilteringParquetRowInputFormat.getSplits(ParquetTableOperations.scala:511)
	at parquet.hadoop.ParquetInputFormat.getSplits(ParquetInputFormat.java:245)
	at
org.apache.spark.sql.parquet.FilteringParquetRowInputFormat.getSplits(ParquetTableOperations.scala:464)
	at
org.apache.spark.sql.parquet.ParquetRelation2$$anonfun$buildScan$1$$anon$1.getPartitions(newParquet.scala:305)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:219)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:217)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:217)
	at
org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:32)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:219)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:217)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:217)
	at
org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:32)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:219)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:217)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:217)
	at
org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:32)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:219)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:217)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:217)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1781)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:885)
	at
org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at
org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:286)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:884)
	at
cn.zhaishidan.test.service.SparkHiveService.formatDF(SparkHiveService.scala:64)
	at
cn.zhaishidan.test.service.SparkHiveService.query(SparkHiveService.scala:78)
	at
cn.zhaishidan.test.api.DatabaseApi$$anonfun$query$1.apply(DatabaseApi.scala:41)
	at
cn.zhaishidan.test.api.DatabaseApi$$anonfun$query$1.apply(DatabaseApi.scala:32)
	at cn.zhaishidan.test.web.JettyUtils$$anon$1.getOrPost(JettyUtils.scala:81)
	at cn.zhaishidan.test.web.JettyUtils$$anon$1.doGet(JettyUtils.scala:115)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:735)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:848)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)
	at
org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:501)
	at
org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)
	at
org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:428)
	at
org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)
	at
org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
	at org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:52)
	at
org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
	at org.eclipse.jetty.server.Server.handle(Server.java:370)
	at
org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)
	at
org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:971)
	at
org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1033)
	at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)
	at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)
	at
org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
	at
org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:667)
	at
org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)
	at
org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at
org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:745)
Caused by:
org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File
does not exist:
/user/hive/warehouse/test.db/b/part-r-00004-3abcbb07-e20a-4b5e-a6e5-59356c3d3149.gz.parquet
	at
org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:65)
	at
org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:55)
	at
org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsUpdateTimes(FSNamesystem.java:1716)
	at
org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1659)
	at
org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1639)
	at
org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1613)
	at
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:497)
	at
org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:322)
	at
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at
org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at
org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at
org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy23.getBlockLocations(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor22.invoke(Unknown Source)
	at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at
org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at
org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy23.getBlockLocations(Unknown Source)
	at
org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:219)
	at
org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1142)
	... 71 more

I've set spark.sql.parquet.cacheMetadata=false, so the error is confusing
for me.

I also found that, we can avoid the error by executing ""refresh table
test.`b`"" after recreating the table 'b'. 

Currently, To avoid the error, I've modified
""spark/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMetastoreCatalog.scala""
at line of 323, and changed the code:

```scala
val paths = Seq(metastoreRelation.hiveQlTable.getDataLocation.toString)

val cached = getCached(tableIdentifier, paths, metastoreSchema, None)
val parquetRelation = cached.getOrElse {
  val created = LogicalRelation(
    new ParquetRelation2(paths.toArray, None, None, parquetOptions)(hive))
  cachedDataSourceTables.put(tableIdentifier, created)
  created
}

parquetRelation
```

to

```scala
val paths = Seq(metastoreRelation.hiveQlTable.getDataLocation.toString)

val cached = getCached(tableIdentifier, paths, metastoreSchema, None)
val parquetRelation = LogicalRelation(
    new ParquetRelation2(paths.toArray, None, None, parquetOptions)(hive))
  cachedDataSourceTables.put(tableIdentifier, created)

parquetRelation
```

It's working fine for me, after rebuilding spark.

Is this a bug of caching parquet relation? Any suggestion? 

Best, Stan.



--

---------------------------------------------------------------------


"
Justin Uang <justin.uang@gmail.com>,"Tue, 28 Jul 2015 12:51:20 +0000",Re: PySpark on PyPi,"Jeremy Freeman <freeman.jeremy@gmail.com>, Punyashloka Biswal <punya.biswal@gmail.com>","// ping

do we have any signoff from the pyspark devs to submit a PR to publish to
PyPI?


™ve done
park jobs
variables
ing
to
TH
rk
ore
n
when
 the
 now,
ple,
I
e much
rc.zip:$PYTHONPATH
I bet
 the
 part
ject
erly.
"
Ted Yu <yuzhihong@gmail.com>,"Tue, 28 Jul 2015 08:25:17 -0700",ReceiverTrackerSuite failing in master build,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,
I noticed that ReceiverTrackerSuite is failing in master Jenkins build for
both hadoop profiles.

The failure seems to start with:
https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-Maven-with-YARN/3104/

FYI
"
Michal Haris <michal.haris@visualdna.com>,"Tue, 28 Jul 2015 16:59:45 +0100",Generalised Spark-HBase integration,"user@spark.apache.org, dev <dev@spark.apache.org>","Hi all, last couple of months I've been working on a large graph analytics
and along the way have written from scratch a HBase-Spark integration as
none of the ones out there worked either in terms of scale or in the way
they integrated with the RDD interface. This week I have generalised it
into an (almost) spark module, which works with the latest spark and the
new hbase api, so... sharing! :
https://github.com/michal-harish/spark-on-hbase


-- 
Michal Haris
Technical Architect
direct line: +44 (0) 207 749 0229
www.visualdna.com | t: +44 (0) 207 734 7033
31 Old Nichol Street
London
E2 7HR
"
Jules Damji <jdamji@hortonworks.com>,"Tue, 28 Jul 2015 16:02:28 +0000",Re: Generalised Spark-HBase integration,"Michal Haris <michal.haris@visualdna.com>, ""user@spark.apache.org""
	<user@spark.apache.org>, dev <dev@spark.apache.org>","
Brilliant! Will check it out.

Cheers
Jules

--
The Best Ideas Are Simple
Jules Damji
Developer Relations & Community Outreach
jdamji@hortonworks.com
http://hortonworks.com


Hi all, last couple of months I've been working on a large graph analytics and along the way have written from scratch a HBase-Spark integration as none of the ones out there worked either in terms of scale or in the way they integrated with the RDD interface. This week I have generalised it into an (almost) spark module, which works with the latest spark and the new hbase api, so... sharing! :  https://github.com/michal-harish/spark-on-hbase


--
Michal Haris
Technical Architect
direct line: +44 (0) 207 749 0229
www.visualdna.com<http://www.visualdna.com> | t: +44 (0) 207 734 7033
31 Old Nichol Street
London
E2 7HR
"
Ted Malaska <ted.malaska@cloudera.com>,"Tue, 28 Jul 2015 12:06:26 -0400",Re: Generalised Spark-HBase integration,Michal Haris <michal.haris@visualdna.com>,"Thanks Michal,

Just to share what I'm working on in a related topic.  So a long time ago I
http://blog.cloudera.com/blog/2014/12/new-in-cloudera-labs-sparkonhbase/

Also recently I am working on getting this into HBase core.  It will
hopefully be in HBase core with in the next couple of weeks.

https://issues.apache.org/jira/browse/HBASE-13992

Then I'm planing on adding dataframe and bulk load support through

https://issues.apache.org/jira/browse/HBASE-14149
https://issues.apache.org/jira/browse/HBASE-14150

Also if you are interested this is running today a at least a half a dozen
companies with Spark Streaming.  Here is one blog post of successful
implementation

http://blog.cloudera.com/blog/2015/03/how-edmunds-com-used-spark-streaming-to-build-a-near-real-time-dashboard/

Also here is an additional example blog I also put together

http://blog.cloudera.com/blog/2014/11/how-to-do-near-real-time-sessionization-with-spark-streaming-and-apache-hadoop/

Let me know if you have any questions, also let me know if you want to
connect to join efforts.

Ted Malaska


"
Ted Yu <yuzhihong@gmail.com>,"Tue, 28 Jul 2015 09:11:24 -0700",Re: Generalised Spark-HBase integration,Michal Haris <michal.haris@visualdna.com>,"I got a compilation error:

[INFO] /home/hbase/s-on-hbase/src/main/scala:-1: info: compiling
[INFO] Compiling 18 source files to /home/hbase/s-on-hbase/target/classes
at 1438099569598
[ERROR]
/home/hbase/s-on-hbase/src/main/scala/org/apache/spark/hbase/examples/simple/HBaseTableSimple.scala:36:
error: type mismatch;
[INFO]  found   : Int
[INFO]  required: Short
[INFO]       while (scanner.advance) numCells += 1
[INFO]                                        ^
[ERROR] one error found

FYI


"
Michal Haris <michal.haris@visualdna.com>,"Tue, 28 Jul 2015 17:12:38 +0100",Re: Generalised Spark-HBase integration,Ted Malaska <ted.malaska@cloudera.com>,"Hi Ted, yes, cloudera blog and your code was my starting point - but I
needed something more spark-centric rather than on hbase. Basically doing a
lot of ad-hoc transformations with RDDs that were based on HBase tables and
then mutating them after series of iterative (bsp-like) steps.




-- 
Michal Haris
Technical Architect
direct line: +44 (0) 207 749 0229
www.visualdna.com | t: +44 (0) 207 734 7033
31 Old Nichol Street
London
E2 7HR
"
Michal Haris <michal.haris@visualdna.com>,"Tue, 28 Jul 2015 17:14:09 +0100",Re: Generalised Spark-HBase integration,Ted Yu <yuzhihong@gmail.com>,"Oops, yes, I'm still messing with the repo on a daily basis.. fixed




-- 
Michal Haris
Technical Architect
direct line: +44 (0) 207 749 0229
www.visualdna.com | t: +44 (0) 207 734 7033
31 Old Nichol Street
London
E2 7HR
"
Ted Malaska <ted.malaska@cloudera.com>,"Tue, 28 Jul 2015 12:14:45 -0400",Re: Generalised Spark-HBase integration,Michal Haris <michal.haris@visualdna.com>,"Yup you should be able to do that with the APIs that are going into HBase.

Let me know if you need to chat about the problem and how to implement it
with the HBase apis.

We have tried to cover any possible way to use HBase with Spark.  Let us
know if we missed anything if we did we will add it.


"
Michal Haris <michal.haris@visualdna.com>,"Tue, 28 Jul 2015 17:17:06 +0100",Re: Generalised Spark-HBase integration,Ted Malaska <ted.malaska@cloudera.com>,"Cool, will revisit, is your latest code visible publicly somewhere ?




-- 
Michal Haris
Technical Architect
direct line: +44 (0) 207 749 0229
www.visualdna.com | t: +44 (0) 207 734 7033
31 Old Nichol Street
London
E2 7HR
"
Patrick Wendell <pwendell@gmail.com>,"Tue, 28 Jul 2015 09:23:00 -0700",Re: ReceiverTrackerSuite failing in master build,"Ted Yu <yuzhihong@gmail.com>, Shixiong Zhu <zsxwing@gmail.com>, 
	Tathagata Das <tathagata.das1565@gmail.com>","Thanks ted for pointing this out. CC to Ryan and TD


---------------------------------------------------------------------


"
Ted Malaska <ted.malaska@cloudera.com>,"Tue, 28 Jul 2015 12:23:14 -0400",Re: Generalised Spark-HBase integration,Michal Haris <michal.haris@visualdna.com>,"Stuff that people are using is here.


The stuff going into HBase is here
https://issues.apache.org/jira/browse/HBASE-13992

If you want to add things to the hbase ticket lets do it in another jira.
Like these jira

https://issues.apache.org/jira/browse/HBASE-14149
https://issues.apache.org/jira/browse/HBASE-14150

This first jira is mainly getting the Spark dependancies and separate
module set up so we can start making additional jiras to add additional
functionality.

The goal is to have the following in HBase by end of summer:

RDD and DStream Functions
1. BulkPut
2. BulkGet
3. BulkDelete
4. Foreach with connection
5. Map with connection
6. Distributed Scan
7. BulkLoad

DataFrame Functions
1. BulkPut
2. BulkGet
6. Distributed Scan
7. BulkLoad

If you think there should be more let me know

Ted Malaska



"
Ted Malaska <ted.malaska@cloudera.com>,"Tue, 28 Jul 2015 12:24:28 -0400",Re: Generalised Spark-HBase integration,Michal Haris <michal.haris@visualdna.com>,"Sorry this is more correct

RDD and DStream Functions
1. BulkPut
2. BulkGet
3. BulkDelete
4. Foreach with connection
5. Map with connection
6. Distributed Scan
7. BulkLoad

DataFrame Functions
1. BulkPut
2. BulkGet
3. Foreach with connection
4. Map with connection
5. Distributed Scan
6. BulkLoad



"
Reynold Xin <rxin@databricks.com>,"Tue, 28 Jul 2015 09:48:58 -0700",Re: Custom UDFs with zero parameters support,Sachith Withana <swsachith@gmail.com>,"I think we do support 0 arg UDFs:
https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/functions.scala#L2165

How are you using UDFs?



"
Mike Hynes <91mbbh@gmail.com>,"Tue, 28 Jul 2015 13:37:09 -0400",Broadcast variable of size 1 GB fails with negative memory exception,dev@spark.apache.org,"Hello Devs,

I am investigating how matrix vector multiplication can scale for an
IndexedRowMatrix in mllib.linalg.distributed.

Currently, I am broadcasting the vector to be multiplied on the right.
The IndexedRowMatrix is stored across a cluster with up to 16 nodes,
each with >200 GB of memory. The spark driver is on an identical node,
having more than 200 Gb of memory.

In scaling n, the size of the vector to be broadcast, I find that the
maximum size of n that I can use is 2^26. For 2^27, the broadcast will
fail. The array being broadcast is of type Array[Double], so the
contents have size 2^30 bytes, which is approximately 1 (metric) GB.

I have read in PR  [SPARK-3721] [PySpark] ""broadcast objects larger
than 2G"" that this should be supported (I assume this works for scala,
as well?). However, when I increase n to 2^27 or above, the program
invariably crashes at the broadcast.

The problem stems from the size of the result block to be sent in
BlockInfo.scala; the size is reportedly negative. An example error log
is shown below.

If anyone has more experience or knowledge of why this broadcast is
failing, I'd appreciate the input.
-- 
Thanks,
Mike

55584:INFO:MemoryStore:ensureFreeSpace(-2147480008) called with
curMem=0, maxMem=92610625536:
55584:INFO:MemoryStore:Block broadcast-2 stored as values in memory
(estimated size -2147480008.0 B, free 88.3 GB):
Exception in thread ""main"" java.lang.IllegalArgumentException:
requirement failed: sizeInBytes was negative: -2147480008
        at scala.Predef$.require(Predef.scala:233)
        at org.apache.spark.storage.BlockInfo.markReady(BlockInfo.scala:55)
        at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:815)
        at org.apache.spark.storage.BlockManager.putIterator(BlockManager.scala:638)
        at org.apache.spark.storage.BlockManager.putSingle(BlockManager.scala:996)
        at org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:99)
        at org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:85)
        at org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34)
        at org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:63)
        at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1297)
        at org.apache.spark.mllib.linalg.distributed.IndexedRowMatrix.multiply(IndexedRowMatrix.scala:184)
        at himrod.linalg.KrylovTests$.main(KrylovTests.scala:172)
        at himrod.linalg.KrylovTests.main(KrylovTests.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:666)
        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:178)
        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:118)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Tue, 28 Jul 2015 11:51:56 -0700",update on git timeouts for jenkins builds,"amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","hey all, i'm just back in from my wedding weekend (woot!) and am
working on figuring out what's happening w/the git timeouts for pull
request builds.

TL;DR:  if your build fails due to a timeout, please retrigger your
builds.  i know this isn't the BEST solution, but until we get some
stuff implemented (traffic shaping, git cache for the workers) it's
the only thing i can recommend.

here's a snapshot of the state of the union:
$ get_timeouts.sh 5
timeouts by date:
2015-07-23 -- 3
2015-07-24 -- 1
2015-07-26 -- 7
2015-07-27 -- 18
2015-07-28 -- 9

timeouts by project:
     35 SparkPullRequestBuilder
      3 Tachyon-Pull-Request-Builder
total builds (excepting aborted by a user):
1908

total percentage of builds timing out:
01%

nothing has changed on our end AFAIK, our traffic graphs look totally
fine, but starting sunday, we started seeing a spike in timeouts, with
yesterday being the worst.  today is also not looking good either.

github is looking OK, but not ""great"":
https://status.github.com/

as a solution, we'll be setting up some traffic shaping on our end, as
well as implementing a git cache on the workers so that we'll
(hopefully) minimize how many hits we make against github.  i was
planning on doing the git cache months ago, but the timeout issue
pretty much went away and i back-burnered that idea until today.

other than that, i'll be posting updates as we get them.

shane

---------------------------------------------------------------------


"
Ankur Dave <ankurdave@gmail.com>,"Tue, 28 Jul 2015 12:05:27 -0700",Re: Two joins in GraphX Pregel implementation,"""Ulanov, Alexander"" <alexander.ulanov@hp.com>",":



You're right. In fact, the outer join can be streamlined further using a
method from GraphOps:

g = g.joinVertices(messages)(vprog).cache()

Then, instead of passing newVerts as the active set for mapReduceTriplets,
we could pass `messages`.

If you're interested in proposing a PR for this, I've attached a patch with
these changes and updates to the comments.


lets with

[...]


There isn't any performance benefit to switching Pregel to use
aggregateMessages while preserving its current interface, because the
interface uses Iterators and would require us to wrap and unwrap them
anyway. The semantics of aggregateMessagesWithActiveSet are otherwise the
same as mapReduceTriplets, so there isn't any functionality we are missing
out on. And this change seems too small to justify introducing a new
version of Pregel, though it would be worthwhile when combined with other
improvements <https://github.com/apache/spark/pull/1217>.

Ankur <http://www.ankurdave.com/>

---------------------------------------------------------------------"
=?UTF-8?Q?F=C3=A9lix=2DAntoine_Fortin?= <felix-antoine.fortin@calculquebec.ca>,"Tue, 28 Jul 2015 15:13:52 -0400",Opinion on spark-class script simplification and posix compliance,dev@spark.apache.org,"Hi,

Out of curiosity, I have tried to replace the dependence on bash by sh
in the different scripts to launch Spark daemons and jobs. So far,
most scripts work with sh, except ""bin/spark-class"". The culprit is
the while loop that compose the final command by parsing the output of
launcher library.

After a bit of fiddling, I found that the following loop:
CMD=()
while IFS= read -d '' -r ARG; do
CMD+=(""$ARG"")
done < <(""$RUNNER"" -cp ""$LAUNCH_CLASSPATH"" org.apache.spark.launcher.Main ""$@"")

could be replaced by a single line:
CMD=$(""$RUNNER"" -cp ""$LAUNCH_CLASSPATH"" org.apache.spark.launcher.Main
""$@"" | xargs -0)

The while loop cannot be executed with sh, while the single line can
be. Since on my system, sh is simply a link on bash, with some options
activated, I guess this simply means that the while loop syntax is not
posix compliant. Which spawns two questions:

1. Would it be useful to make sure Spark scripts are POSIX compliant?
2. Is the simplification of spark-class enough to consider making a
pull-request?

Thanks,
Felix-Antoine Fortin

---------------------------------------------------------------------


"
Marcelo Vanzin <vanzin@cloudera.com>,"Tue, 28 Jul 2015 12:18:49 -0700",Re: Opinion on spark-class script simplification and posix compliance,=?UTF-8?Q?F=C3=A9lix=2DAntoine_Fortin?= <felix-antoine.fortin@calculquebec.ca>,"

I guess if people are trying to run Spark on systems where bash is not
super common it could help. Not sure how common that use case is.
Otherwise, it seems like it would be more noise than actually useful...

Also note that bash, when executed as ""/bin/sh"", still allows syntax that
other shells do not necessarily understand. A better test would be to try
to run the script with dash or something else that is not bash.


-- 
Marcelo
"
shane knapp <sknapp@berkeley.edu>,"Tue, 28 Jul 2015 12:28:23 -0700",Re: update on git timeouts for jenkins builds,"amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>, 
	Josh Rosen <joshrosen@databricks.com>","++joshrosen

ok, i found out some of what's going on.  some builds were failing as such:
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/38749/console

note that it's unable to remove the target/ directory during the
build...  this is c"
shane knapp <sknapp@berkeley.edu>,"Tue, 28 Jul 2015 12:49:54 -0700",Re: update on git timeouts for jenkins builds,"amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>, 
	Josh Rosen <joshrosen@databricks.com>","btw, the directory perm issue was only happening on
amp-jenkins-worker-04 and -05.  both of the broken dirs were
clobbered, so we won't be seeing any more of these again.


---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Tue, 28 Jul 2015 13:30:21 -0700",Re: update on git timeouts for jenkins builds,"amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>, 
	Josh Rosen <joshrosen@databricks.com>","git caches are set up on all workers for the pull request builder, and
builds are building w/the cache...  however in the build logs it
doesn't seem to be actually *hitting* the cache, so i guess i'll be
doing some more poking and prodding to see wtf is going on.



---------------------------------------------------------------------


"
Meihua Wu <rotationsymmetry14@gmail.com>,"Tue, 28 Jul 2015 13:46:48 -0700",Rebase and Squash Commits to Revise PR?,dev@spark.apache.org,"I am planning to update my PR to incorporate comments from reviewers.
Do I need to rebase/squash the commits into a single one?

Thanks!

-MW

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Tue, 28 Jul 2015 21:49:16 +0100",Re: Rebase and Squash Commits to Revise PR?,Meihua Wu <rotationsymmetry14@gmail.com>,"You only need to rebase if your branch/PR now conflicts with master.
you don't need to squash since the merge script will do that in the
end for you. You can squash commits and force-push if you think it
would help clean up your intent, but, often it's clearer to leave the
review and commit history of your branch since the review comments go
along with it.


---------------------------------------------------------------------


"
Meihua Wu <rotationsymmetry14@gmail.com>,"Tue, 28 Jul 2015 14:08:19 -0700",Re: Rebase and Squash Commits to Revise PR?,Sean Owen <sowen@cloudera.com>,"Thanks Sean. Very helpful!


---------------------------------------------------------------------


"
Imran Rashid <irashid@cloudera.com>,"Tue, 28 Jul 2015 16:14:26 -0500",Re: Broadcast variable of size 1 GB fails with negative memory exception,Mike Hynes <91mbbh@gmail.com>,"Hi Mike,

are you sure there the size isn't off 2x somehow?  I just tried to
reproduce with a simple test in BlockManagerSuite:

test(""large block"") {
  store = makeBlockManager(4e9.toLong)
  val arr = new Array[Double](1 << 28)
  println(arr.size)
  val blockId = BlockId(""rdd_3_10"")
  val result = store.putIterator(blockId, Iterator(arr),
StorageLevel.MEMORY_AND_DISK)
  result.foreach{println}
}

it fails at 1 << 28 with nearly the same message, but its fine for (1 <<
28) - 1 with a reported block size of 2147483680.  Not exactly the same as
what you did, but I expect it to be close enough to exhibit the same error.


org.apache.spark.storage.BlockInfo.markReady(BlockInfo.scala:55)
org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:815)
org.apache.spark.storage.BlockManager.putIterator(BlockManager.scala:638)
org.apache.spark.storage.BlockManager.putSingle(BlockManager.scala:996)
org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:99)
org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:85)
org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34)
org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:63)
org.apache.spark.SparkContext.broadcast(SparkContext.scala:1297)
org.apache.spark.mllib.linalg.distributed.IndexedRowMatrix.multiply(IndexedRowMatrix.scala:184)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$runMain(SparkSubmit.scala:666)
org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:178)
org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:203)
org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:118)
"
Joseph Bradley <joseph@databricks.com>,"Tue, 28 Jul 2015 14:33:37 -0700","Re: DataFrame#rdd doesn't respect DataFrame#cache, slowing down CrossValidator",Justin Uang <justin.uang@gmail.com>,"Thanks for bringing this up!  I talked with Michael Armbrust, and it sounds
like this is a from a bug in DataFrame caching:
https://issues.apache.org/jira/browse/SPARK-9141
It's marked as a blocker for 1.5.
Joseph


"
Justin Uang <justin.uang@gmail.com>,"Tue, 28 Jul 2015 21:48:15 +0000","Re: DataFrame#rdd doesn't respect DataFrame#cache, slowing down CrossValidator",Joseph Bradley <joseph@databricks.com>,"Sweet! Does this cover DataFrame#rdd also using the cached query from
DataFrame#cache? I think the ticket 9141 is mainly concerned with whether a
derived DataFrame (B) of a cached DataFrame (A) uses the cached query of A,
not whether the rdd from A.rdd or B.rdd uses the cached query of A.

"
Michael Armbrust <michael@databricks.com>,"Tue, 28 Jul 2015 15:20:57 -0700","Re: DataFrame#rdd doesn't respect DataFrame#cache, slowing down CrossValidator",Justin Uang <justin.uang@gmail.com>,"Can you add your description of the problem as a comment to that ticket and
we'll make sure to test both cases and break it out if the root cause ends
up being different.


"
Priya Ch <learnings.chitturi@gmail.com>,"Wed, 29 Jul 2015 08:29:52 +0530",Fwd: Writing streaming data to cassandra creates duplicates,"""user@spark.apache.org"" <user@spark.apache.org>, dev@spark.apache.org","Hi TD,

 Thanks for the info. I have the scenario like this.

 I am reading the data from kafka topic. Let's say kafka has 3 partitions
for the topic. In my streaming application, I would configure 3 receivers
with 1 thread each such that they would receive 3 dstreams (from 3
partitions of kafka topic) and also I implement partitioner. Now there is a
possibility of receiving messages with same primary key twice or more, one
is at the time message is created and other times if there is an update to
any fields for same message.

If two messages M1 and M2 with same primary key are read by 2 receivers
then even the partitioner in spark would still end up in parallel
processing as there are altogether in different dstreams. How do we address
in this situation ?

Thanks,
Padma Ch


"
Mike Hynes <91mbbh@gmail.com>,"Tue, 28 Jul 2015 23:56:35 -0400",Re: Broadcast variable of size 1 GB fails with negative memory exception,Imran Rashid <irashid@cloudera.com>,"Hi Imran,

Thanks for your reply. I have double-checked the code I ran to
generate an nxn matrix and nx1 vector for n = 2^27. There was
unfortunately a bug in it, where instead of having typed 134,217,728
for n = 2^27, I included a third '7' by mistake, making the size 10x
larger.

However, even after having corrected this, my question about
broadcasting is still whether or not a variable >= 2G in size may be
transferred? In this case, for n >= 2^28, the broadcast variable
crashes, and an array of size MAX_INT cannot be broadcast.

Looking at Chowdhury's ""Performance and Scalability of Broadcast in
Spark"" technical report, I realize that the results are reported only
for broadcast variables up to 1 GB in physical size. I was hoping,
however, that an Array of size MAX_INT would be transferrable via a
broadcast (since the previous PR I mentioned seems to have added
support for > 2GB variables) such that the matrix-vector
multiplication would scale to MAX_INT x MAX_INT matrices with a
broadcast variable.

Would you or anyone on the dev list be able to comment on whether this
is possible? Since the (corrected) overflow I'm seeing is for > 2^31
physical bytes being transferred, I am guessing that there is still a
physical limitation on how many bytes may be sent via broadcasting, at
least for a primitive Array[Double]?

Thanks,
Mike

19176&INFO&IndexedRowMatrix&Broadcasting vecArray with size 268435456&
19177&INFO&MemoryStore&ensureFreeSpace(-2147483592) called with
curMem=6888, maxMem=92610625536&
19177&INFO&MemoryStore&Block broadcast_2 stored as values in memory
(estimated size -2147483592.0 B, free 88.3 GB)&
Exception in thread ""main"" java.lang.IllegalArgumentException:
requirement failed: sizeInBytes was negative: -2147483592



-- 
Thanks,
Mike

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Tue, 28 Jul 2015 22:46:43 -0700",Reminder about Spark 1.5.0 code freeze deadline of Aug 1st,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey All,

Just a friendly reminder that Aug 1st is the feature freeze for Spark
1.5, meaning major outstanding changes will need to land in the this
week.

After May 1st we'll package a release for testing and then go into the
normal triage process where bugs are prioritized and some smaller
features are allowed on a case by case basis (if they are very low risk/
additive/feature flagged/etc).

As always, I'll invite the community to help participate in code
review of patches in the this week, since review bandwidth is the
single biggest determinant of how many features will get in. Please
also keep in mind that most active committers are working overtime
(nights/weekends) during this period and will try their best to help
usher in as many patches as possible, along with their own code.

As a reminder, release window dates are always maintained on the wiki
and are updated after each release according to our 3 month release
cadence:

https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage

Thanks - and happy coding!

- Reynold
"
Sachith Withana <swsachith@gmail.com>,"Wed, 29 Jul 2015 11:29:33 +0530",Re: Custom UDFs with zero parameters support,Reynold Xin <rxin@databricks.com>,"Hi Reynold,

I'm implementing the interfaces given here (
https://github.com/apache/spark/tree/master/sql/core/src/main/java/org/apache/spark/sql/api/java
).
But currently there is no UDF0 adapter.

Any suggestions? I'm new to Spark and any help would be appreciated.

-- 
Thanks,
Sachith Withana


"
Reynold Xin <rxin@databricks.com>,"Tue, 28 Jul 2015 23:16:17 -0700",Re: Custom UDFs with zero parameters support,Sachith Withana <swsachith@gmail.com>,"We should add UDF0 to it.

For now, can you just create an one-arg UDF and don't use the argument?



"
Sachith Withana <sachith@wso2.com>,"Wed, 29 Jul 2015 12:06:06 +0530",Re: Custom UDFs with zero parameters support,Reynold Xin <rxin@databricks.com>,"That's what I'm doing right now.
I'm implementing UDF1 for the now() UDF and in the UDF registration I'm
registering UDFs with zero parameters as a UDF1s.

For the above example, although I add the now() UDF as is, since it's
registered as an UDF1, I need to provide an empty parameter in the query
such as timestamp < now(' ') or it won't work.







-- 
Sachith Withana
Software Engineer; WSO2 Inc.; http://wso2.com
E-mail: sachith AT wso2.com
M: +94715518127
Linked-In: <http://goog_416592669>https://lk.linkedin.com/in/sachithwithana
"
Reynold Xin <rxin@databricks.com>,"Tue, 28 Jul 2015 23:38:57 -0700",Re: Custom UDFs with zero parameters support,Sachith Withana <sachith@wso2.com>,"BTW for 1.5, there is already a now like function being added, so it should
work out of the box in 1.5.0, to be released end of Aug/early Sep.



"
Sean Owen <sowen@cloudera.com>,"Wed, 29 Jul 2015 07:39:28 +0100",Re: Reminder about Spark 1.5.0 code freeze deadline of Aug 1st,"""dev@spark.apache.org"" <dev@spark.apache.org>","Right now, 603 issues have been resolved for 1.5.0. 424 are still
targeted for 1.5.0, of which 33 are marked Blocker and 60 Critical.
This count is not supposed to be 0 at this point, but must
conceptually get to 0 at the time of 1.5.0's release. Most will simply
be un-targeted or pushed down the road.

If the plan is to begin meaningful testing on Aug 1 (great) and
release by Aug 15, this seems to be far too large. Yes, it just means
some prioritization has to happen. Target Version and Priority still
seem like the right tools to communicate this.

Let me put up a straw-man: untarget any JIRA targeted to 1.5.0 that
isn't Blocker or Critical on Aug 1. (JIRAs can be explicitly
retargeted in the following week.) This still leaves 93 issues, which
seems unrealistic to address in 2 weeks.

What are additional or alternative steps to handle this?
- Untarget a lot of the remaining 93?
- Push out 1.5 by X weeks to address more items?
- Argue there's another way to manage this?


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Tue, 28 Jul 2015 23:38:20 -0700",Re: Custom UDFs with zero parameters support,Sachith Withana <sachith@wso2.com>,"Yup - would you be willing to submit a patch to add UDF0?

Should be pretty easy (really just add a new Java class, and then add a new
function to registerUDF)



"
Sachith Withana <swsachith@gmail.com>,"Wed, 29 Jul 2015 12:12:47 +0530",Re: Custom UDFs with zero parameters support,Reynold Xin <rxin@databricks.com>,"Sure. Will do.

Thanks a lot for the help.




-- 
Thanks,
Sachith Withana
"
prakhar jauhari <prak840@gmail.com>,"Wed, 29 Jul 2015 00:55:23 -0700 (MST)","Spark (1.2) yarn allocator does not remove container request for
 allocated container, resulting in a bloated ask[] of containers and
 inefficient resource utilization of cluster resources.",dev@spark.apache.org,"This is because Yarn's AM client does not remove fulfilled container request
from its MAP until the application's AM specifically calls
removeContainerRequest for fulfilled container requests.

Spark-1.2 : Spark's yarn AM does not call removeContainerRequest for
fulfilled container request.

Spark-1.3 : yarn AM calls removeContainerRequest for the container requests
it can map to be fulfilled. Tried the same test case of killing one executor
with spark-1.3 and the ask[] in this case was for 1 container.

As long as the cluster size is large enough to allocate the bloated
container requests, containers are sent to spark yarn allocator in allocate
response, spark yarn allocator uses missing number of container to launch
new executors and release the extra allocated containers.

The problem magnifies in case of long running jobs with large executor
memory requirements. In this case when ever a executor gets killed, the next
ask to yarn Resource manager (RM) is of n+1 containers (n being count of
already requested containers), which might be served by the RM if it still
has enough resources, else RM starts reserving cluster resources for a
containers which are not even required by spark in the first place. 

This causes resource crunch for other applications, and inefficient resource
utilization of cluster resources.

I was not able to find a thread talking specifically about this. Is there
any known use case due to which removeContainerRequest is not done in spark
1.2?  





--

---------------------------------------------------------------------


"
Bharath Ravi Kumar <reachbach@gmail.com>,"Wed, 29 Jul 2015 18:35:43 +0530",Re: [ANNOUNCE] Nightly maven and package builds for Spark,Patrick Wendell <pwendell@gmail.com>,"Hey Patrick,

Any update on this front please?

Thanks,
Bharath


"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Wed, 29 Jul 2015 13:37:44 +0000",RE: Two joins in GraphX Pregel implementation,Ankur Dave <ankurdave@gmail.com>,"Hi Ankur,

Thank you! This looks like a nice simplification. There should be some performance improvement since newVerts are not chached now.
Iâ€™ve added your patch:
https://issues.apache.org/jira/browse/SPARK-9436

Best regards, Alexander

From: Ankur Dave [mailto:ankurdave@gmail.com]
Sent: Tuesday, July 28, 2015 12:05 PM
To: Ulanov, Alexander
Cc: Robin East; dev@spark.apache.org
Subject: Re: Two joins in GraphX Pregel implementation

On 27 Jul 2015, at 16:42, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
It seems that the mentioned two joins can be rewritten as one outer join

You're right. In fact, the outer join can be streamlined further using a method from GraphOps:

g = g.joinVertices(messages)(vprog).cache()

Then, instead of passing newVerts as the active set for mapReduceTriplets, we could pass `messages`.

If you're interested in proposing a PR for this, I've attached a patch with these changes and updates to the comments.

On Tue, Jul 28, 2015 at 1:15 AM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
Iâ€™ve found two PRs (almost identical) for replacing mapReduceTriplets with aggregateMessages
[...]
Do you know the reason why this improvement is not pushed?

There isn't any performance benefit to switching Pregel to use aggregateMessages while preserving its current interface, because the interface uses Iterators and would require us to wrap and unwrap them anyway. The semantics of aggregateMessagesWithActiveSet are otherwise the same as mapReduceTriplets, so there isn't any functionality we are missing out on. And this change seems too small to justify introducing a new version of Pregel, though it would be worthwhile when combined with other improvements<https://github.com/apache/spark/pull/1217>.

Ankur<http://www.ankurdave.com/>
"
mkhaitman <mark.khaitman@chango.com>,"Wed, 29 Jul 2015 06:52:50 -0700 (MST)","Re: ""Spree"": Live-updating web UI for Spark",dev@spark.apache.org,"We tested this out on our dev cluster (Hadoop 2.7.1 + Spark 1.4.0), and it
looks great! I might also be interested in contributing to it when I get a
chance! Keep up the awesome work! :)

Mark.



--

---------------------------------------------------------------------


"
Imran Rashid <irashid@cloudera.com>,"Wed, 29 Jul 2015 11:26:08 -0500",Re: Broadcast variable of size 1 GB fails with negative memory exception,Mike Hynes <91mbbh@gmail.com>,"Hi Mike,

I dug into this a little more, and it turns out in this case there is a
pretty trivial fix -- the problem you are seeing is just from integer
overflow before casting to a long in SizeEstimator.  I've opened
https://issues.apache.org/jira/browse/SPARK-9437 for this.

For now, I think your workaround is to break into multiple arrays, and
broadcast them each separately.  If its any consolation, you would have to
do this anyway if you tried to broadcast more than Int.MAX_VALUE doubles in
any case.  The JVM only allows arrays up to that length.  So even if spark
didn't have this limitation, you could go up to an array of 2^31 doubles,
which would be 16GB, before having to break into multiple arrays.

There *are* a number of places in spark where things are limited to 2GB,
and there are a handful of open issues to deal with it.  However I think
this one is pretty easy to fix (I must have looked at this a half-dozen
times before and never realized the fix was so simple before ...).  However
it could be we'll run into something else after this particular issue w/
SizeEstimator is fixed.  This certainly won't work with HttpBroadcast, but
I think it might just work as long as you stick with TorrentBroadcast.

imran


"
shane knapp <sknapp@berkeley.edu>,"Wed, 29 Jul 2015 12:20:09 -0700",Re: update on git timeouts for jenkins builds,"amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","ok, i think i found the problem and solution to the git timeouts:

https://stackoverflow.com/questions/12236415/git-clone-return-result-18-code-200-on-a-specific-repository

so, on each worker i've run ""git config --global http.postBuffer
524288000"" as the jenkins user and we'll see if this makes a
difference.


---------------------------------------------------------------------


"
Mike Hynes <91mbbh@gmail.com>,"Wed, 29 Jul 2015 16:21:00 -0400",Re: Broadcast variable of size 1 GB fails with negative memory exception,Imran Rashid <irashid@cloudera.com>,"Hi Imran,
Thanks to you and Shivaram for looking into this, and opening the
JIRA/PR. I will update you once the PR is merged if there are any
other problems that arise from the broadcast.
Mike



-- 
Thanks,
Mike

---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Wed, 29 Jul 2015 15:06:39 -0700",Re: update on git timeouts for jenkins builds,"amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","newp.  still happening, and i'm still looking in to it:

https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/38880/console


---------------------------------------------------------------------


"
JaeSung Jun <jaesjun@gmail.com>,"Thu, 30 Jul 2015 12:02:20 +1000",unit test failure for hive query,dev@spark.apache.org,"Hi,
I'm working on custom sql processing on top of Spark-SQL, and i'm upgrading
it along with spark 1.4.1.
I've got an error regarding multiple test suites access hive meta store at
the same time like :

 Cause: org.apache.derby.impl.jdbc.EmbedSQLException: Another instance of
Derby may have already booted the database /Users/~~~/metastore_db.

  at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown
Source)

  at
org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown
Source)

  at
org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown
Source)

  at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)

  at org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(Unknown Source)

  at org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)

  at org.apache.derby.impl.jdbc.EmbedConnection40.<init>(Unknown Source)

  at org.apache.derby.jdbc.Driver40.getNewEmbedConnection(Unknown Source)

  at org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)

  at org.apache.derby.jdbc.Driver20.connect(Unknown Source)


It was okay with spark 1.3.0.

Any idea to fist this?


thanks in advance.

Jason
"
Michael Armbrust <michael@databricks.com>,"Wed, 29 Jul 2015 19:09:55 -0700",Re: unit test failure for hive query,JaeSung Jun <jaesjun@gmail.com>,"I'd suggest using org.apache.spark.sql.hive.test.TestHive as the context in
unit tests.  It takes care of creating separate directories for each
invocation automatically.


"
prakhar jauhari <prak840@gmail.com>,"Wed, 29 Jul 2015 23:25:55 -0700 (MST)","Re: Spark (1.2) yarn allocator does not remove container request
 for allocated container, resulting in a bloated ask[] of containers and
 inefficient resource utilization of cluster resources.",dev@spark.apache.org,"hey all,

Thanks in advance.
I am facing this issue in production, where due to increased container
request RM is reserving memory and hampering cluster utilization. Thus the
fix needs to be patched on spark 1.2. 

Has any one looked in the removeContainerRequest part for allocated
containers in spark 1.2?

Regards,
Prakhar. 





--

---------------------------------------------------------------------


"
Priya Ch <learnings.chitturi@gmail.com>,"Thu, 30 Jul 2015 14:20:38 +0530",Re: Writing streaming data to cassandra creates duplicates,"""user@spark.apache.org"" <user@spark.apache.org>, dev@spark.apache.org","Hi All,

 Can someone throw insights on this ?


"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Thu, 30 Jul 2015 08:58:02 +0000",Machine learning unit tests guidelines,"""dev@spark.apache.org"" <dev@spark.apache.org>","Dear Spark developers,

Are there any best practices or guidelines for machine learning unit tests in Spark? After taking a brief look at the unit tests in ML and MLlib, I have found that each algorithm is tested in a different way. There are few kinds of tests:
1)Partial check of internal algorithm correctness. This can be anything.
2)Generate test data with distribution specific to the algorithm, do machine learning and check the outcomes. This is also very specific.
3)Compare the parameters (weights) of machine learning model with parameters from existing implementations, such as R or SciPy. This looks more like a useful test, so that you are sure you will get the same result from the algorithm as other people get using other software.

After googling a bit, I've found the following guidelines rather relevant:
http://blog.mpacula.com/2011/02/17/unit-testing-statistical-software/

I am wondering, should we come up with specific guidelines for machine learning, such as that the user is guaranteed to get the expected result? This also might be considered as additional benefit for Spark - to be standardized ML.

Best regards, Alexander
"
Sachith Withana <swsachith@gmail.com>,"Thu, 30 Jul 2015 15:30:46 +0530",UDF Method overloading,dev@spark.apache.org,"Hi all,

Does spark support UDF Method overloading?

ex: I want to have an UDF with varying number of arguments

multiply(a,b)
multiply(a,b,c)

Any suggestions?

-- 
Thanks,
Sachith Withana
"
Joe Halliwell <joe.halliwell@gmail.com>,"Thu, 30 Jul 2015 11:52:03 +0100",Re: UDF Method overloading,Sachith Withana <swsachith@gmail.com>,"Hi Sachith,

Yes, that's possible, you just need to implement
https://hive.apache.org/javadocs/r0.10.0/api/org/apache/hadoop/hive/ql/udf/generic/GenericUDF.html

Note the class documentation:
""A Generic User-defined function (GenericUDF) for the use with Hive.
New GenericUDF classes need to inherit from this GenericUDF class. The
GenericUDF are superior to normal UDFs in the following ways: 1. It
can accept arguments of complex types, and return complex types. 2. It
can accept variable length of arguments. 3. It can accept an infinite
number of function signature - for example, it's easy to write a
GenericUDF that accepts array, array> and so on (arbitrary levels of
nesting). 4. It can do short-circuit evaluations using DeferedObject.""

PS I think this question may have been more suited to the ""Spark
Users"" mailing list

Cheers,
Joe




-- 
Best regards,
Joe

---------------------------------------------------------------------


"
Joseph Batchik <josephbatchik@gmail.com>,"Thu, 30 Jul 2015 16:44:37 +0000",Data source aliasing,dev@spark.apache.org,"Hi all,

There are now starting to be a lot of data source packages for Spark. A
annoyance I see is that I have to type in the full class name like:

sqlContext.read.format(""com.databricks.spark.avro"").load(path).

Spark internally has formats such as ""parquet"" and ""jdbc"" registered and it
would be nice to be able just to type in ""avro"", ""redshift"", etc. as well.
Would it be a good idea to use something like a service loader to allow
data sources defined in other packages to register themselves with Spark? I
think that this would make it easier for end users. I would be interested
in adding this, please let me know what you guys think.

- Joe
"
Patrick Wendell <pwendell@gmail.com>,"Thu, 30 Jul 2015 11:18:09 -0700",Re: Data source aliasing,Joseph Batchik <josephbatchik@gmail.com>,"Yeah this could make sense - allowing data sources to register a short
name. What mechanism did you have in mind? To use the jar service loader?

The only issue is that there could be conflicts since many of these are
third party packages. If the same name were registered twice I'm not sure
what the best behavior would be. Ideally in my mind if the same shortname
were registered twice we'd force the user to use a fully qualified name and
say the short name is ambiguous.

Patrick

"
Michael Armbrust <michael@databricks.com>,"Thu, 30 Jul 2015 11:20:31 -0700",Re: Data source aliasing,Patrick Wendell <pwendell@gmail.com>,1
Joseph Batchik <josephbatchik@gmail.com>,"Thu, 30 Jul 2015 18:33:03 +0000",Re: Data source aliasing,"Michael Armbrust <michael@databricks.com>, Patrick Wendell <pwendell@gmail.com>","Yep I was looking into using the jar service loader.

I pushed a rough draft to my fork of Spark:
https://github.com/JDrit/spark/commit/946186e3f17ddcc54acf2be1a34aebf246b06d2f

Right now it will use the first alias it finds, but I can change that to
check them all and report an error if it finds duplicate aliases. I tested
this locally with the spark-avro package and it allows me to use ""avro"" as
the format specified. It defaults to using the class name, just like you
would if you did not alias anything.



"
Yucheng <yl2695@nyu.edu>,"Thu, 30 Jul 2015 13:26:49 -0700 (MST)",FrequentItems in spark-sql-execution-stat,dev@spark.apache.org,"Hi all,

I'm reading the code in spark-sql-execution-stat-FrequentItems.scala, and
I'm a little confused about the ""add"" method in the FreqItemCounter class.
Please see the link here:
https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/stat/FrequentItems.scala

My question is when the baseMap does not contain the key, and the size of
the baseMap is not less than size, why should we just keep the key/value
pairs whose value is greater than count?

Just like this example:
Now the baseMap is Map(1 -> 3, 2 -> 3, 3 -> 4), and the size is 3. I want to
add Map(4 -> 25) into this baseMap, so it will retain the key/values whose
value is greater than 25, and in that way, the baseMap will be null.
However, I think we should at least add 4 -> 25 into the baseMap. Could
anybody help me with this problem?

Best,
Yucheng



--

---------------------------------------------------------------------


"
Mridul Muralidharan <mridul@gmail.com>,"Thu, 30 Jul 2015 14:00:55 -0700",Re: Data source aliasing,Joseph Batchik <josephbatchik@gmail.com>,"Would be a good idea to generalize this for spark core - and allow for
its use in serde, compression, etc.

Regards,
Mridul


---------------------------------------------------------------------


"
satyajit vegesna <satyajit.apasprk@gmail.com>,"Thu, 30 Jul 2015 15:26:51 -0700",Parquet SaveMode.Append Trouble.,"user@spark.apache.org, dev@spark.apache.org","Hi,

I am new to using Spark and Parquet files,

Below is what i am trying to do, on Spark-shell,

val df =
sqlContext.parquetFile(""/data/LM/Parquet/Segment/pages/part-m-00000.gz.parquet"")
Have also tried below command,

val
df=sqlContext.read.format(""parquet"").load(""/data/LM/Parquet/Segment/pages/part-m-00000.gz.parquet"")

Now i have an other existing parquet file to which i want to append this
Parquet file data of df.

so i use,

df.save(""/data/LM/Parquet/Segment/pages2/part-m-00000.gz.parquet"",""parquet"",
SaveMode.Append )

also tried below command,

df.save(""/data/LM/Parquet/Segment/pages2/part-m-00000.gz.parquet"",
SaveMode.Append )


and it throws me below error,

<console>:26: error: not found: value SaveMode

df.save(""/data/LM/Parquet/Segment/pages2/part-m-00000.gz.parquet"",""parquet"",
SaveMode.Append )

Please help me, in case i am doing something wrong here.

Regards,
Satyajit.
"
Christophe Schmitz <cofcof.oz@gmail.com>,"Fri, 31 Jul 2015 12:41:04 +1000",High availability with zookeeper: worker discovery,dev@spark.apache.org,"Hi there,

I am trying to run a 3 node spark cluster where each nodes contains a spark
worker and a spark maser. Election of the master happens via zookeeper.

The way I am configuring it is by (on each node) giving the IP:PORT of the
local master to the local worker, and I wish the worker could autodiscover
the elected master automatically.

But unfortunatly, only the local worker of the elected master registered to
the elected master. Why aren't the other worker getting to connect to the
elected master?

The interessing thing is that if I kill the elected master and wait a bit,
then the new elected master sees all the workers!

I am wondering if I am missing something to make this happens without
having to kill the elected master.

Thanks!


PS: I am on spark 1.2.2
"
StanZhai <mail@zhaishidan.cn>,"Thu, 30 Jul 2015 20:52:16 -0700 (MST)",Re: Parquet SaveMode.Append Trouble.,dev@spark.apache.org,"You should import org.apache.spark.sql.SaveMode



--

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Thu, 30 Jul 2015 21:18:31 -0700",Re: High availability with zookeeper: worker discovery,Christophe Schmitz <cofcof.oz@gmail.com>,"zookeeper is not a direct dependency of Spark.

Can you give a bit more detail on how the election / discovery of master
works ?

Cheers


"
Sachin Aggarwal <different.sachin@gmail.com>,"Fri, 31 Jul 2015 09:53:16 +0530",add to user list,dev@spark.incubator.apache.org,"-- 

Thanks & Regards

Sachin Aggarwal
7760502772
"
Ted Yu <yuzhihong@gmail.com>,"Thu, 30 Jul 2015 21:26:58 -0700",Re: add to user list,Sachin Aggarwal <different.sachin@gmail.com>,"Please take a look at the first section of:
https://spark.apache.org/community


"
Christophe Schmitz <cofcof.oz@gmail.com>,"Fri, 31 Jul 2015 14:34:59 +1000",Re: High availability with zookeeper: worker discovery,Ted Yu <yuzhihong@gmail.com>,"Hi Ted,

Thanks for your reply. I think zookeeper is an optional dependency of
Spark. To enable it, I essentially use this flags on all my spark-env.sh:

SPARK_DAEMON_JAVA_OPTS=""-Dspark.deploy.recoveryMode=ZOOKEEPER
-Dspark.deploy.zookeeper.url=my-zoo-ip:2181""

and of course, I have my zookeeper runing on my-zoo-ip:2181 (just 1
zookeeper node at this stage)

spark master election seems to work as only 1 master is active, and the
remaining one are in standby. I can't tell you how the election / discovery
of master works, I assume the zookeeper module of spark is somehow using
zookeeper primitives to do the election. The thing that is blocking me is
that the worker don't seem to discover the first-ever elected-master.

Cheers,

Christophe



"
burakkk <burak.isikli@gmail.com>,"Fri, 31 Jul 2015 09:38:00 +0300",Spark CBO,dev@spark.apache.org,"Hi everyone,
I'm wondering that is there any plan to implement cost-based optimizer for
Spark SQL?

Best regards...

-- 

*BURAK ISIKLI* | *http://burakisikli.wordpress.com
<http://burakisikli.wordpress.com>*
"
james <yiazhou@gmail.com>,"Fri, 31 Jul 2015 00:49:42 -0700 (MST)",Came across Spark SQL hang issue with Spark 1.5 Tungsten feature,dev@spark.apache.org,"I try to enable Tungsten with Spark SQL and set below 3 parameters, but i
found the Spark SQL always hang below point. So could you please point me
what's the potential cause ? I'd appreciate any input.
spark.shuffle.manager=tungsten-sort
spark.sql.codegen=true
spark.sql.unsafe.enabled=true

15/07/31 15:19:46 INFO scheduler.TaskSetManager: Starting task 110.0 in
stage 131.0 (TID 280, bignode3, PROCESS_LOCAL, 1446 bytes)
15/07/31 15:19:46 INFO scheduler.TaskSetManager: Starting task 111.0 in
stage 131.0 (TID 281, bignode2, PROCESS_LOCAL, 1446 bytes)
15/07/31 15:19:46 INFO storage.BlockManagerInfo: Added broadcast_132_piece0
in memory on bignode3:38948 (size: 7.4 KB, free: 1766.4 MB)
15/07/31 15:19:46 INFO storage.BlockManagerInfo: Added broadcast_132_piece0
in memory on bignode3:57341 (size: 7.4 KB, free: 1766.4 MB)
15/07/31 15:19:46 INFO storage.BlockManagerInfo: Added broadcast_132_piece0
in memory on bignode1:33229 (size: 7.4 KB, free: 1766.4 MB)
15/07/31 15:19:46 INFO storage.BlockManagerInfo: Added broadcast_132_piece0
in memory on bignode1:42261 (size: 7.4 KB, free: 1766.4 MB)
15/07/31 15:19:46 INFO storage.BlockManagerInfo: Added broadcast_132_piece0
in memory on bignode2:44033 (size: 7.4 KB, free: 1766.4 MB)
15/07/31 15:19:46 INFO storage.BlockManagerInfo: Added broadcast_132_piece0
in memory on bignode2:42863 (size: 7.4 KB, free: 1766.4 MB)
15/07/31 15:19:46 INFO storage.BlockManagerInfo: Added broadcast_132_piece0
in memory on bignode4:58639 (size: 7.4 KB, free: 1766.4 MB)
15/07/31 15:19:46 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send
map output locations for shuffle 3 to bignode3:46462
15/07/31 15:19:46 INFO spark.MapOutputTrackerMaster: Size of output statuses
for shuffle 3 is 71847 bytes
15/07/31 15:19:46 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send
map output locations for shuffle 3 to bignode3:38803
15/07/31 15:19:46 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send
map output locations for shuffle 3 to bignode1:35241
15/07/31 15:19:46 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send
map output locations for shuffle 3 to bignode1:48323
15/07/31 15:19:46 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send
map output locations for shuffle 3 to bignode2:56697
15/07/31 15:19:46 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send
map output locations for shuffle 3 to bignode4:55810
15/07/31 15:19:46 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send
map output locations for shuffle 3 to bignode2:37386




--

---------------------------------------------------------------------


"
james <yiazhou@gmail.com>,"Fri, 31 Jul 2015 01:31:30 -0700 (MST)","Re: Came across Spark SQL hang/Error issue with Spark 1.5 Tungsten
 feature",dev@spark.apache.org,"Another errorï¼š
15/07/31 16:15:28 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send
map output locations for shuffle 3 to bignode1:40443
15/07/31 16:15:28 INFO spark.MapOutputTrackerMaster: Size of output statuses
for shuffle 3 is 583 bytes
15/07/31 16:15:28 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send
map output locations for shuffle 3 to bignode1:40474
15/07/31 16:15:28 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send
map output locations for shuffle 3 to bignode2:34052
15/07/31 16:15:28 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send
map output locations for shuffle 3 to bignode3:46929
15/07/31 16:15:28 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send
map output locations for shuffle 3 to bignode3:50890
15/07/31 16:15:28 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send
map output locations for shuffle 3 to bignode2:47795
15/07/31 16:15:28 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send
map output locations for shuffle 3 to bignode4:35120
15/07/31 16:15:28 INFO scheduler.TaskSetManager: Finished task 32.0 in stage
151.0 (TID 1203) in 155 ms on bignode3 (1/50)
15/07/31 16:15:28 INFO scheduler.TaskSetManager: Finished task 35.0 in stage
151.0 (TID 1204) in 157 ms on bignode2 (2/50)
15/07/31 16:15:28 INFO scheduler.TaskSetManager: Finished task 8.0 in stage
151.0 (TID 1196) in 168 ms on bignode3 (3/50)
15/07/31 16:15:28 WARN scheduler.TaskSetManager: Lost task 46.0 in stage
151.0 (TID 1184, bignode1): java.lang.NegativeArraySizeException
        at
org.apache.spark.sql.catalyst.expressions.UnsafeRow.getBinary(UnsafeRow.java:314)
        at
org.apache.spark.sql.catalyst.expressions.UnsafeRow.getUTF8String(UnsafeRow.java:297)
        at SC$SpecificProjection.apply(Unknown Source)
        at
org.apache.spark.sql.catalyst.expressions.FromUnsafeProjection.apply(Projection.scala:152)
        at
org.apache.spark.sql.catalyst.expressions.FromUnsafeProjection.apply(Projection.scala:140)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        at scala.collection.Iterator$$anon$10.next(Iterator.scala:312)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        at
org.apache.spark.shuffle.unsafe.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:148)
        at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:71)
        at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
        at org.apache.spark.scheduler.Task.run(Task.scala:86)
        at
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
        at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)





--
3.nabble.com/Came-across-Spark-SQL-hang-Error-issue-with-Spark-1-5-Tungsten-feature-tp13537p13538.html
om.

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Fri, 31 Jul 2015 01:39:57 -0700",Re: Came across Spark SQL hang/Error issue with Spark 1.5 Tungsten feature,james <yiazhou@gmail.com>,"Is this deterministically reproducible? Can you try this on the latest
master branch?

Would be great to turn debug logging and and dump the generated code. Also
would be great to dump the array size at your line 314 in UnsafeRow (and
whatever master branch's appropriate line is).


d
d
d
d
d
d
d
ge
ava:314)
ow.java:297)
ection.scala:152)
ection.scala:140)
iter.java:148)
)
)
:1145)
a:615)
rk-SQL-hang-Error-issue-with-Spark-1-5-Tungsten-feature-tp13537p13538.html
"
Sandeep Giri <sandeep@knowbigdata.com>,"Fri, 31 Jul 2015 14:41:51 +0530",New Feature Request,dev@spark.apache.org,"Dear Spark Dev Community,

I am wondering if there is already a function to solve my problem. If not,
then should I work on this?

Say you just want to check if a word exists in a huge text file. I could
not find better ways than those mentioned here
<http://www.knowbigdata.com/blog/interview-questions-apache-spark-part-2#q6>
.

So, I was proposing if we have a function called *exists *in RDD with the
following signature:

#returns the true if n elements exist which qualify our criteria.
#qualifying function would receive the element and its index and return
true or false.
def *exists*(qualifying_function, n):
     ....


Regards,
Sandeep Giri,
+1 347 781 4573 (US)
+91-953-899-8962 (IN)

www.KnowBigData.com. <http://KnowBigData.com.>
Phone: +1-253-397-1945 (Office)

[image: linkedin icon] <https://linkedin.com/company/knowbigdata> [image:
other site icon] <http://knowbigdata.com>  [image: facebook icon]
<https://facebook.com/knowbigdata> [image: twitter icon]
<https://twitter.com/IKnowBigData> <https://twitter.com/IKnowBigData>
"
Carsten Schnober <schnober@ukp.informatik.tu-darmstadt.de>,"Fri, 31 Jul 2015 11:29:01 +0200",Re: New Feature Request,<dev@spark.apache.org>,"Hi,
the RDD class does not have an exist()-method (in the Scala API), but
the functionality you need seems easy to resemble with the existing methods:

val containsNMatchingElements =
data.filter(qualifying_function).take(n).count() >= n

Note: I am not sure whether the intermediate take(n) really increases
performance, but the idea is to arbitrarily reduce the number of
elements in the RDD before counting because we are not interested in the
full count.

If you need to check specifically whether there is at least one matching
occurrence, it is probably preferable to use isEmpty() instead of
count() and check whether the result is false:

val contains1MatchingElement = !(data.filter(qualifying_function).isEmpty())

Best,
Carsten



Am 31.07.2015 um 11:11 schrieb Sandeep Giri:

-- 
Carsten Schnober
Doctoral Researcher
Ubiquitous Knowledge Processing (UKP) Lab
FB 20 / Computer Science Department
Technische UniversitÃ¤t Darmstadt
Hochschulstr. 10, D-64289 Darmstadt, Germany
phone [+49] (0)6151 16-6227, fax -5455, room S2/02/B111
schnober@ukp.informatik.tu-darmstadt.de
www.ukp.tu-darmstadt.de

Web Research at TU Darmstadt (WeRC): www.werc.tu-darmstadt.de
GRK 1994: Adaptive Preparation of Information from Heterogeneous Sources
(AIPHES): www.aiphes.tu-darmstadt.de
PhD program: Knowledge Discovery in Scientific Literature (KDSL)
www.kdsl.tu-darmstadt.de

---------------------------------------------------------------------


"
Jonathan Winandy <jonathan.winandy@gmail.com>,"Fri, 31 Jul 2015 12:07:03 +0200",Re: New Feature Request,Carsten Schnober <schnober@ukp.informatik.tu-darmstadt.de>,"Hello !

You could try something like that :

def exists[T](rdd:RDD[T])(f:T=>Boolean, n:Int):Boolean = {
  rdd.filter(f).countApprox(timeout = 10000).getFinalValue().low > n
}

If would work for large datasets and large value of n.

Have a nice day,

Jonathan




d
6
n
"
Sean Owen <sowen@cloudera.com>,"Fri, 31 Jul 2015 11:08:26 +0100",Re: Should spark-ec2 get its own repo?,Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"PS is this still in progress? it feels like something that would be
good to do before 1.5.0, if it's going to happen soon.


---------------------------------------------------------------------


"
Justin Uang <justin.uang@gmail.com>,"Fri, 31 Jul 2015 16:31:39 +0000","Re: DataFrame#rdd doesn't respect DataFrame#cache, slowing down CrossValidator",Michael Armbrust <michael@databricks.com>,"Sweet! It's here:
https://issues.apache.org/jira/browse/SPARK-9141?focusedCommentId=14649437&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14649437


"
Josh Rosen <joshrosen@databricks.com>,"Fri, 31 Jul 2015 09:45:39 -0700",Re: Came across Spark SQL hang/Error issue with Spark 1.5 Tungsten feature,Reynold Xin <rxin@databricks.com>,"It would also be great to test this with codegen and unsafe enabled but
while continuing to use sort shuffle manager instead of the new
tungsten-sort one.


o
nd
nd
nd
nd
nd
nd
nd
java:314)
Row.java:297)
jection.scala:152)
jection.scala:140)
riter.java:148)
1)
1)
a:1145)
va:615)
ark-SQL-hang-Error-issue-with-Spark-1-5-Tungsten-feature-tp13537p13538.html
"
Koert Kuipers <koert@tresata.com>,"Fri, 31 Jul 2015 14:44:26 -0400",Re: FrequentItems in spark-sql-execution-stat,Yucheng <yl2695@nyu.edu>,"this looks like a mistake in FrequentItems to me. if the map is full
(map.size==size) then it should still add the new item (after removing
items from the map and decrementing counts).

if its not a mistake then at least it looks to me like the algo is
different than described in the paper. is this maybe on purpose?


"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Fri, 31 Jul 2015 22:28:48 +0200",Re: Spark CBO,burakkk <burak.isikli@gmail.com>,"Hi,
there is one cost-based analyzer implemented in Spark SQL, if I'm not
mistaken, regarding the Join operations,
If the join operation is done with a small dataset then Spark SQL's
strategy will be to broadcast automatically the small dataset instead of
shuffling.

I guess you have something else on your mind ?

Regards,

Olivier.

2015-07-31 8:38 GMT+02:00 burakkk <burak.isikli@gmail.com>:

r


-- 
*Olivier Girardot* | AssociÃ©
o.girardot@lateral-thoughts.com
+33 6 24 09 17 94
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Fri, 31 Jul 2015 14:13:24 -0700",Re: Should spark-ec2 get its own repo?,Sean Owen <sowen@cloudera.com>,"Yes - It is still in progress, but I have just not gotten time to get to
this. I think getting the repo moved from mesos to amplab in the codebase
by 1.5 should be possible.

Thanks
Shivaram


"
