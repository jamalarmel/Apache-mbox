Holden Karau <holden@pigscanfly.ca>,"Sat, 30 Apr 2016 18:10:14 -0700",Re: persist versus checkpoint,Renyi Xiong <renyixiong0@gmail.com>,"They are different, also this might be better suited for the user list.
Persist by default will cache in memory on one machine, although you can
specify a different storage level. Checkpoint on the other hand will write
out to a persistent store and get rid of the dependency graph used to
compute the RDD (so it is often seen in iterative algorithms which may
build very large or complex dependency graphs over time).



-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
Renyi Xiong <renyixiong0@gmail.com>,"Sun, 1 May 2016 12:13:33 -0700","MetadataFetchFailedException if executorLost when spark.speculation
 enabled ?",dev <dev@spark.apache.org>,"Hi,

We observed MetadataFetchFailedException during executorLost if
spark.speculation enabled.

looks like something out of sync between original task on failed executor
and its speculative counterpart task?

is it a known issue?

please let me know if you need more details.

Thanks,
Renyi.
"
Reynold Xin <rxin@databricks.com>,"Sun, 1 May 2016 15:59:52 -0700",[ANNOUNCE] Spark branch-2.0,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi devs,

Three weeks ago I mentioned on the dev list creating branch-2.0
(effectively ""feature freeze"") in 2 - 3 weeks. I've just created Spark's
branch-2.0 to form the basis of the 2.0 release. We have closed ~ 1700
issues. That's huge progress, and we should celebrate that.

Compared with past releases when we cut the release branch, we have way
fewer open issues. In the past we usually have 200 - 400 open issues when
we cut the release branch. As of today we have less than 100 open issues
for 2.0.0, and among these 14 critical and 2 blocker (Jersey dependency
upgrade and some remaining issues in separating out local linear algebra
library).

What does this mean for committers?

0. For patches that should go into Spark 2.0.0, make sure you also merge
them into not just master, but also branch-2.0.

1. In the next couple of days, sheppard some of the more important,
straggler pull requests in.

2. Switch the focus from new feature development to bug fixes, stability
improvements, finalizing API tweaks, and documentation.

3. Experimental features (e.g. R, structured streaming) can continue to be
developed, provided that the changes don't impact the non-experimental
features.

4. We should become increasingly conservative as time goes on, even for
experimental features.

5. Please un-target or re-target issues if they don't make sense for 2.0.
We should burn # issues down to ~ 0 by the time we have a release candidate.

7. If possible, reach out to users and start testing branch-2.0 to find
bugs. The more testing we can do on real workloads before the release, the
less bugs we will find in the actual Spark 2.0 release.
"
Rahul Tanwani <tanwanirahul@gmail.com>,"Mon, 2 May 2016 00:05:19 -0700 (MST)",Cross Validator to work with K-Fold value of 1?,dev@spark.apache.org,"Hi,

In certain cases (mostly due to time constraints), we need some model to run
without cross validation. In such a case, since k-fold value for cross
validator cannot be one, we have to maintain two different code paths to
achieve both the scenarios (with and without cross validation).

Would it be an okay idea to generalize the cross validator so it can work
with k-fold value of 1? The only purpose for this is to avoid maintaining
two different code paths and in functionality it should be similar to as if
the cross validation is not present.





--

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Mon, 2 May 2016 03:48:51 -0700",Re: spark 2 segfault,"Koert Kuipers <koert@tresata.com>, dev@spark.apache.org","I tried the same statement using Spark 1.6.1
There was no error with default memory setting. 

Suggest logging a bug. 

howed earlier.
_67 , I got:
b"", ""c"")), (Map(""2"" -> ""b""), List(""d"", ""e"")))).toDF
, _2: array<string>]
9)
1)
eProjection.apply(Unknown Source)
ressionEncoder.scala:241)
execute$1$1$$anonfun$apply$13.apply(Dataset.scala:2121)
execute$1$1$$anonfun$apply$13.apply(Dataset.scala:2121)
e.scala:234)
e.scala:234)
ed.scala:33)
)
execute$1$1.apply(Dataset.scala:2121)
xecution.scala:54)
(Dataset.scala:2120)
ataset.scala:2127)
1)
0)

n. i see the error for all data sources (json, parquet, etc.).
"")), (Map(""2"" -> ""b""), List(""d"", ""e"")))).toDF 
ativeArraySizeException
lse staticinvoke(class org.apache.spark.sql.catalyst.util.ArrayBasedMapData$, ObjectType(interface scala.collection.Map), toScalaMap, staticinvoke(class scala.collection.mutable.WrappedArray$, ObjectType(interface scala.collection.Seq), make, mapobjects(lambdavariable(MapObjects_loopValue16, MapObjects_loopIsNull17, StringType), lambdavariable(MapObjects_loopValue16, MapObjects_loopIsNull17, StringType).toString, input[0, map<string,string>].keyArray).array, true), staticinvoke(class scala.collection.mutable.WrappedArray$, ObjectType(interface scala.collection.Seq), make, mapobjects(lambdavariable(MapObjects_loopValue18, MapObjects_loopIsNull19, StringType), lambdavariable(MapObjects_loopValue18, MapObjects_loopIsNull19, StringType).toString, input[0, map<string,string>].valueArray).array, true), true), if (isnull(input[1, array<string>])) null else staticinvoke(class scala.collection.mutable.WrappedArray$, ObjectType(interface scala.collection.Seq), make, mapobjects(lambdavariable(MapObjects_loopValue20, MapObjects_loopIsNull21, StringType), lambdavariable(MapObjects_loopValue20, MapObjects_loopIsNull21, StringType).toString, input[1, array<string>]).array, true), StructField(_1,MapType(StringType,StringType,true),true), StructField(_2,ArrayType(StringType,true),true))
ke(class org.apache.spark.sql.catalyst.util.ArrayBasedMapData$, ObjectType(interface scala.collection.Map), toScalaMap, staticinvoke(class scala.collection.mutable.WrappedArray$, ObjectType(interface scala.collection.Seq), make, mapobjects(lambdavariable(MapObjects_loopValue16, MapObjects_loopIsNull17, StringType), lambdavariable(MapObjects_loopValue16, MapObjects_loopIsNull17, StringType).toString, input[0, map<string,string>].keyArray).array, true), staticinvoke(class scala.collection.mutable.WrappedArray$, ObjectType(interface scala.collection.Seq), make, mapobjects(lambdavariable(MapObjects_loopValue18, MapObjects_loopIsNull19, StringType), lambdavariable(MapObjects_loopValue18, MapObjects_loopIsNull19, StringType).toString, input[0, map<string,string>].valueArray).array, true), true)
BasedMapData$, ObjectType(interface scala.collection.Map), toScalaMap, staticinvoke(class scala.collection.mutable.WrappedArray$, ObjectType(interface scala.collection.Seq), make, mapobjects(lambdavariable(MapObjects_loopValue16, MapObjects_loopIsNull17, StringType), lambdavariable(MapObjects_loopValue16, MapObjects_loopIsNull17, StringType).toString, input[0, map<string,stringedArray$, ObjectType(interface scala.collection.Seq), make, mapobjects(lambdavariable(MapObjects_loopValue18, MapObjects_loopIsNull19, StringType), lambdavariable(MapObjects_loopValue18, MapObjects_loopIsNull19, StringType).toString, input[0, map<string,string>].valueArray).array, true), true)
$, ObjectType(interface scala.collection.Seq), make, mapobjects(lambdavariable(MapObjects_loopValue16, MapObjects_loopIsNull17, StringType), lambdavariable(MapObjects_loopValue16, MapObjects_loopIsNull17, StringType).toString, input[0, map<string,string>].keyArray).array, true)
Objects_loopIsNull17, StringType), lambdavariable(MapObjects_loopValue16, MapObjects_loopIsNull17, StringType).toString, input[0, map<string,string>].keyArray).array
apObjects_loopIsNull17, StringType), lambdavariable(MapObjects_loopValue16, MapObjects_loopIsNull17, StringType).toString, input[0, map<string,string>].keyArray)
ts_loopIsNull17, StringType).toString
jects_loopIsNull17, StringType)
$, ObjectType(interface scala.collection.Seq), make, mapobjects(lambdavariable(MapObjects_loopValue18, MapObjects_loopIsNull19, StringType), lambdavariable(MapObjects_loopValue18, MapObjects_loopIsNull19, StringType).toString, input[0, map<string,string>].valueArray).array, true)
Objects_loopIsNull19, StringType), lambdavariable(MapObjects_loopValue18, MapObjects_loopIsNull19, StringType).toString, input[0, map<string,string>].valueArray).array
apObjects_loopIsNull19, StringType), lambdavariable(MapObjects_loopValue18, MapObjects_loopIsNull19, StringType).toString, input[0, map<string,string>].valueArray)
ts_loopIsNull19, StringType).toString
jects_loopIsNull19, StringType)
ass scala.collection.mutable.WrappedArray$, ObjectType(interface scala.collection.Seq), make, mapobjects(lambdavariable(MapObjects_loopValue20, MapObjects_loopIsNull21, StringType), lambdavariable(MapObjects_loopValue20, MapObjects_loopIsNull21, StringType).toString, input[1, array<string>]).array, true)
bjectType(interface scala.collection.Seq), make, mapobjects(lambdavariable(MapObjects_loopValue20, MapObjects_loopIsNull21, StringType), lambdavariable(MapObjects_loopValue20, MapObjects_loopIsNull21, StringType).toString, input[1, array<string>]).array, true)
ects_loopIsNull21, StringType), lambdavariable(MapObjects_loopValue20, MapObjects_loopIsNull21, StringType).toString, input[1, array<string>]).array
Objects_loopIsNull21, StringType), lambdavariable(MapObjects_loopValue20, MapObjects_loopIsNull21, StringType).toString, input[1, array<string>])
loopIsNull21, StringType).toString
ts_loopIsNull21, StringType)
mRow(ExpressionEncoder.scala:244)
ataset$$execute$1$1$$anonfun$apply$13.apply(Dataset.scala:2121)
ataset$$execute$1$1$$anonfun$apply$13.apply(Dataset.scala:2121)
sableLike.scala:245)
sableLike.scala:245)
qOptimized.scala:33)
la:186)
cala:245)
86)
ataset$$execute$1$1.apply(Dataset.scala:2121)
nId(SQLExecution.scala:54)
a:2408)
xecute$1(Dataset.scala:2120)
ataset$$collect$1.apply(Dataset.scala:2125)
ataset$$collect$1.apply(Dataset.scala:2125)
)
ollect(Dataset.scala:2125)


 2, and i get the error below.
011417856
.0_75-b13)
ux-amd64 compressed oops)
a8,  free space=1010k
 C=native code)
V+0
a/lang/Object;JJ)V+34
g;+5
feProjection.apply(Ljava/lang/Object;)Ljava/lang/Object;+876
rg/apache/spark/sql/catalyst/InternalRow;)Ljava/lang/Object;+5
$execute$1$1$$anonfun$apply$13.apply(Lorg/apache/spark/sql/catalyst/InternalRow;)Ljava/lang/Object;+11
$execute$1$1$$anonfun$apply$13.apply(Ljava/lang/Object;)Ljava/lang/Object;+5
1;Lscala/collection/generic/CanBuildFrom;)Ljava/lang/Object; (7 bytes) @ 0x00007f7c25eeae08 [0x00007f7c25eead40+0xc8]
$execute$1$1.apply()Ljava/lang/Object;+43
g/apache/spark/sql/SparkSession;Lorg/apache/spark/sql/execution/QueryExecution;Lscala/Function0;)Ljava/lang/Object;+106
java/lang/Object;+12
1()Ljava/lang/Object;+9
$collect$1.apply(Lorg/apache/spark/sql/Dataset;)Ljava/lang/Object;+4
$collect$1.apply(Ljava/lang/Object;)Ljava/lang/Object;+5
ache/spark/sql/Dataset;Lscala/Function1;)Ljava/lang/Object;+25
Z)Ljava/lang/Object;+20
"
Pete Robbins <robbinspg@gmail.com>,"Mon, 02 May 2016 13:54:44 +0000",Re: [ANNOUNCE] Spark branch-2.0,"Reynold Xin <rxin@databricks.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","https://issues.apache.org/jira/browse/SPARK-13745

is really a defect and a blocker unless it is the decision to drop support
for Big Endian platforms. The PR has been reviewed and tested and I
strongly believe this needs to be targeted for 2.0.


"
Koert Kuipers <koert@tresata.com>,"Mon, 2 May 2016 10:15:12 -0400",Re: spark 2 segfault,Ted Yu <yuzhihong@gmail.com>,"Created issue:
https://issues.apache.org/jira/browse/SPARK-15062


"
Arun Allamsetty <arun.allamsetty@gmail.com>,"Mon, 2 May 2016 09:24:37 -0600",Re: Requesting feedback for PR for SPARK-11962,dev@spark.apache.org,"Hi,

Since the 2.0.0 branch has been created and is now nearing feature freeze,
can SPARK-11962 get some love please. If we can decide if this should go
into 2.0.0 or 2.1.0, that would be great. Personally, I feel it can totally
go into 2.0.0 as the code is pretty much ready (except for the one bug that
I need your help with).

Thanks,
Arun


"
shane knapp <sknapp@berkeley.edu>,"Mon, 2 May 2016 08:26:49 -0700","Re: [build system] short downtime monday morning (5-2-16), 7-9am PDT","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","this is happening now.


---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Mon, 2 May 2016 08:44:12 -0700","Re: [build system] short downtime monday morning (5-2-16), 7-9am PDT","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","hey everyone!

looks like two of the workers didn't survive a reboot, so i will need
to head to the colo and console in to see what's going on.

sadly, one of the workers that didn't come back is -01, which runs the
doc builds.

anyways, i will post another update within the hour with the status of
these two machines.  i'm also unpausing builds.


---------------------------------------------------------------------


"
Daniel Darabos <daniel.darabos@lynxanalytics.com>,"Mon, 2 May 2016 17:45:10 +0200",Re: Ever increasing physical memory for a Spark Application in YARN,Nitin Goyal <nitin2goyal@gmail.com>,"Hi Nitin,
Sorry for waking up this ancient thread. That's a fantastic set of JVM
flags! We just hit the same problem, but we haven't even discovered all
those flags for limiting memory growth. I wanted to ask if you ever
discovered anything further?

I see you also set -XX:NewRatio=3. This is a very important flag since
Spark 1.6.0. With unified memory management with the default
spark.memory.fraction=0.75 the cache will fill up 75% of the heap. The
default NewRatio is 2, so the cache will not fit in the old generation
pool, constantly triggering full GCs. With NewRatio=3 the old generation
pool is 75% of the heap, so it (just) fits the cache. We find this makes a
very significant performance difference in practice.

Perhaps this should be documented somewhere. Or the default
spark.memory.fraction should be 0.66, so that it works out with the default
JVM flags.


"
Julio Antonio Soto de Vicente <julio@esbet.es>,"Mon, 2 May 2016 17:45:26 +0200",Re: Cross Validator to work with K-Fold value of 1?,Rahul Tanwani <tanwanirahul@gmail.com>,"Hi,

Same goes for the PolynomialExpansion in org.apache.spark.ml.feature. It would be dice to cross-validate with degree 1 polynomial expansion (this is, with no expansion at all) vs other degree polynomial expansions. Unfortunately, degree is forced to be >= 2.

--
Julio

³:
un
f
n3.nabble.com/Cross-Validator-to-work-with-K-Fold-value-of-1-tp17404.html
com.

---------------------------------------------------------------------


"
Nick Pentreath <nick.pentreath@gmail.com>,"Mon, 02 May 2016 16:20:19 +0000",Re: Cross Validator to work with K-Fold value of 1?,"Julio Antonio Soto de Vicente <julio@esbet.es>, Rahul Tanwani <tanwanirahul@gmail.com>","There is a JIRA and PR around for supporting polynomial expansion with
degree 1. Offhand I can't recall if it's been merged

o
o
rk
ng
s
-to-work-with-K-Fold-value-of-1-tp17404.html
"
Reynold Xin <rxin@databricks.com>,"Mon, 2 May 2016 09:42:39 -0700",Re: spark 2 segfault,Koert Kuipers <koert@tresata.com>,"Definitely looks like a bug.

Ted - are you looking at this?



"
Ted Yu <yuzhihong@gmail.com>,"Mon, 2 May 2016 10:00:35 -0700",Re: spark 2 segfault,Reynold Xin <rxin@databricks.com>,"I plan to.

I am not that familiar with all the parts involved though :-)


"
shane knapp <sknapp@berkeley.edu>,"Mon, 2 May 2016 10:48:52 -0700","Re: [build system] short downtime monday morning (5-2-16), 7-9am PDT","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","workers -01 and -04 are back up, is is -06 (as i hit the wrong power
button by accident).  :)

-01 and -04 got hung on shutdown, so i'll investigate them and see
what exactly happened.  regardless, we should be building happily!


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Mon, 2 May 2016 10:49:48 -0700","Re: [build system] short downtime monday morning (5-2-16), 7-9am PDT",shane knapp <sknapp@berkeley.edu>,"Thanks, Shane!


"
Renyi Xiong <renyixiong0@gmail.com>,"Mon, 2 May 2016 13:48:11 -0700",Re: Spark streaming Kafka receiver WriteAheadLog question,"Mario Ds Briggs <mario.briggs@in.ibm.com>, Cody Koeninger <cody@koeninger.org>, 
	dev <dev@spark.apache.org>","sorry, I removed others by mistake

thanks a lot, Mario, for explaining. Appreciate it.


"
Koert Kuipers <koert@tresata.com>,"Tue, 3 May 2016 12:16:59 -0400","SQLContext and ""stable identifier required""","""dev@spark.apache.org"" <dev@spark.apache.org>","with the introduction of SparkSession SQLContext changed from being a lazy
val to a def.
however this is troublesome if you want to do:

import someDataset.sqlContext.implicits._

because it is no longer a stable identifier, i think? i get:
stable identifier required, but someDataset.sqlContext.implicits found.

anyone else seen this?
"
Ted Yu <yuzhihong@gmail.com>,"Tue, 3 May 2016 09:56:54 -0700","Re: SQLContext and ""stable identifier required""",Koert Kuipers <koert@tresata.com>,"Have you tried the following ?

scala> import spark.implicits._
import spark.implicits._

scala> spark
res0: org.apache.spark.sql.SparkSession =
org.apache.spark.sql.SparkSession@323d1fa2

Cheers


"
Koert Kuipers <koert@tresata.com>,"Tue, 3 May 2016 13:21:18 -0400","Re: SQLContext and ""stable identifier required""",Ted Yu <yuzhihong@gmail.com>,"yes it works fine if i switch to using the implicits on the SparkSession
(which is a val)

but do we want to break the old way of doing the import?


"
Reynold Xin <rxin@databricks.com>,"Tue, 3 May 2016 10:21:46 -0700","Re: SQLContext and ""stable identifier required""",Koert Kuipers <koert@tresata.com>,"Probably not. Want to submit a pull request?


"
Koert Kuipers <koert@tresata.com>,"Tue, 3 May 2016 13:22:13 -0400","Re: SQLContext and ""stable identifier required""",Reynold Xin <rxin@databricks.com>,"sure i can do that


"
kyle <chaii.tsai@gmail.com>,"Tue, 3 May 2016 16:56:56 -0700",Unable To Find Proto Buffer Class Error With RDD,dev@spark.apache.org,"Hi,

I ran into an issue when using proto buffer in spark RDD.
I googled this and found it seems to be a known compatible issue.
Have anyone ran into the same issue before and found any solutions?


The detailed description could be found in this link.
https://qnalist.com/questions/5156782/unable-to-find-proto-buffer-class-error-with-rdd-protobuf


Thanks,
Kyle
"
Nitin Goyal <nitin2goyal@gmail.com>,"Wed, 4 May 2016 08:33:23 +0530",Re: Ever increasing physical memory for a Spark Application in YARN,Daniel Darabos <daniel.darabos@lynxanalytics.com>,"Hi Daniel,

I could indeed discover the problem in my case and it turned out to be a
bug at parquet side and I had raised and contributed to the following issue
:-

https://issues.apache.org/jira/browse/PARQUET-353

Hope this helps!

Thanks
-Nitin





-- 
Regards
Nitin Goyal
"
Yanbo Liang <ybliang8@gmail.com>,"Tue, 3 May 2016 23:52:23 -0700",Re: Cross Validator to work with K-Fold value of 1?,Nick Pentreath <nick.pentreath@gmail.com>,"Here is the JIRA and PR for supporting PolynomialExpansion with degree 1,
and it has been merged.

https://issues.apache.org/jira/browse/SPARK-13338
https://github.com/apache/spark/pull/11216

2016-05-02 9:20 GMT-07:00 Nick Pentreath <nick.pentreath@gmail.com>:

s>
to
r-to-work-with-K-Fold-value-of-1-tp17404.html
"
Adam Roberts <AROBERTS@uk.ibm.com>,"Wed, 4 May 2016 18:01:37 +0100",Caching behaviour and deserialized size,dev@spark.apache.org,"Hi, 

Given a very simple test that uses a bigger version of the pom.xml file in 
our Spark home directory (cat with a bash for loop into itself so it 
becomes 100 MB), I've noticed with larger heap sizes it looks like we have 
more RDDs reported as being cached, is this intended behaviour? What 
exactly are we looking at, replicas perhaps (the resiliency in RDD) or 
partitions for the same RDD?

With a 512 MB heap (max and initial size), regardless of JDK vendor:

Looking for mybiggerpom.xml in the directory you're running this 
application from
Added broadcast_0_piece0 in memory on 10.0.2.15:35762 (size: 15.8 KB, 
free: 159.0 MB)
caching in memory
Added broadcast_1_piece0 in memory on 10.0.2.15:35762 (size: 1789.0 B, 
free: 159.0 MB)
Added rdd_1_0 in memory on 10.0.2.15:35762 (size: 110.7 MB, free: 48.3 MB)
lines.count(): 2790700

Yet if I increase it to 1024 MB (again max and initial size), I see this:

Looking for mybiggerpom.xml in the directory you're running this 
application from
Added broadcast_0_piece0 in memory on 10.0.2.15:39739 (size: 15.8 KB, 
free: 543.0 MB)
caching in memory
Added broadcast_1_piece0 in memory on 10.0.2.15:39739 (size: 1789.0 B, 
free: 543.0 MB)
Added rdd_1_0 in memory on 10.0.2.15:39739 (size: 110.7 MB, free: 432.3 
MB)
Added rdd_1_1 in memory on 10.0.2.15:39739 (size: 107.3 MB, free: 325.0 
MB)
Added rdd_1_2 in memory on 10.0.2.15:39739 (size: 107.0 MB, free: 218.1 
MB)
lines.count(): 2790700

My simple test case:
//scalastyle:off

import java.io.File
import org.apache.spark._
import org.apache.spark.rdd._

object Trimmed {

  def main(args: Array[String]) {
    val sc = new SparkContext(new SparkConf().setAppName(""Adam RDD cached 
size experiment"")
      .setMaster(""local[1]""))

    var fileName = ""mybiggerpom.xml""
    if (args != null && args.length > 0) {
     fileName = args(0)
    }
    println(""Looking for "" + fileName + "" in the directory you're running 
this application from"")
    val lines = sc.textFile(fileName)
    println(""caching in memory"")
    lines.cache()
    println(""lines.count(): "" + lines.count())
  }
}

I also want to figure out where the cached RDD size value comes from and I 
noticed deserializedSize is used (in BlockManagerMasterEndpoint.scala), 
where does this value come from? I understand SizeEstimator plays a big 
role but it's unclear who's responsible for figuring out deserializedSize 
in the first place despite my best efforts with Intellij and a lot of 
grepping.

I'm using recent Spark 2.0 code, any guidance here will be appreciated, 
cheers




Unless stated otherwise above:
IBM United Kingdom Limited - Registered in England and Wales with number 
741598. 
Registered office: PO Box 41, North Harbour, Portsmouth, Hampshire PO6 3AU
"
shane knapp <sknapp@berkeley.edu>,"Wed, 4 May 2016 11:38:34 -0700","[build system] short downtime next thursday morning, 5-12-16 @ 8am PDT","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","there's a security update coming out for jenkins next week, and i'm
going to install the update first thing thursday morning.

i'll send out another reminder early next week.

thanks!

shane

---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Fri, 6 May 2016 09:36:24 +0200","TaskSchedulerImpl#initialize - why is rootPool initialized here not
 while TaskSchedulerImpl is created?",dev <dev@spark.apache.org>,"Hi,

While reviewing TaskSchedulerImpl I've noticed that rootPool is
created and initialized in TaskSchedulerImpl#initialize [1], but seems
legit to do it as part of TaskSchedulerImpl's instantiation.

What is the reason for creating and initializing rootPool late in
TaskSchedulerImpl's lifecycle?

[1] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala#L131-L142

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Hyukjin Kwon <gurwls223@gmail.com>,"Sat, 7 May 2016 00:45:36 +0900",Proposal of closing some PRs and maybe some PRs abandoned by its author,dev <dev@spark.apache.org>,"Hi all,


This was similar with the proposal of closing PRs before I asked.

I think the PRs suggested to be closed below are closable but not very sure
of PRs apparently abandoned by its author at least for a month.

I remember the discussion about auto-closing PR before. So, I included the
PRs as below anyway.

I looked though the open every PR at this time and could make a list as
below:


1. Suggested to be closed.


https://github.com/apache/spark/pull/7739  <- not sure

https://github.com/apache/spark/pull/9354

https://github.com/apache/spark/pull/9451

https://github.com/apache/spark/pull/10507

https://github.com/apache/spark/pull/10486

https://github.com/apache/spark/pull/10460

https://github.com/apache/spark/pull/10967

https://github.com/apache/spark/pull/10945

https://github.com/apache/spark/pull/10701

https://github.com/apache/spark/pull/10681

https://github.com/apache/spark/pull/11766



2. Author not answering at least for a month.


https://github.com/apache/spark/pull/9907

https://github.com/apache/spark/pull/9920

https://github.com/apache/spark/pull/9936

https://github.com/apache/spark/pull/10052

https://github.com/apache/spark/pull/10125

https://github.com/apache/spark/pull/10209

https://github.com/apache/spark/pull/10572 <- not sure

https://github.com/apache/spark/pull/10326

https://github.com/apache/spark/pull/10379

https://github.com/apache/spark/pull/10403

https://github.com/apache/spark/pull/10466

https://github.com/apache/spark/pull/10572 <- not sure

https://github.com/apache/spark/pull/10995

https://github.com/apache/spark/pull/10887

https://github.com/apache/spark/pull/10842

https://github.com/apache/spark/pull/11005

https://github.com/apache/spark/pull/11036

https://github.com/apache/spark/pull/11129

https://github.com/apache/spark/pull/11610

https://github.com/apache/spark/pull/11729

https://github.com/apache/spark/pull/11980

https://github.com/apache/spark/pull/12075


Thanks.
"
Ted Yu <yuzhihong@gmail.com>,"Fri, 6 May 2016 09:15:20 -0700",Re: Proposal of closing some PRs and maybe some PRs abandoned by its author,Hyukjin Kwon <gurwls223@gmail.com>,"PR #10572 was listed twice.

In the future, is it possible to include the contributor's handle beside
the PR number so that people can easily recognize their own PR ?

Thanks


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 06 May 2016 17:00:17 +0000",Re: Proposal of closing some PRs and maybe some PRs abandoned by its author,"Ted Yu <yuzhihong@gmail.com>, Hyukjin Kwon <gurwls223@gmail.com>","Alex has built tooling for this btw:
https://github.com/databricks/spark-pr-dashboard/pull/71


"
Sean Owen <sowen@cloudera.com>,"Fri, 6 May 2016 18:47:32 +0100",Re: Proposal of closing some PRs and maybe some PRs abandoned by its author,Hyukjin Kwon <gurwls223@gmail.com>,"Skimmed these as a one-off exercise, and I suggest ...

Leave open for now

7739 mgrover
10701 nraychaudhuri
9920 jliwork
9936 Lewuathe
10052 dereksabryfb
10125 kevinyu98
10466 hhbyyh
10995 tedyu
10887 blbradley
11005 huaxingao
11129 AtkinsChang
11610 iyounus
11729 mbaddar1
11980 koertkuipers
12075 zhuoliu
10572 navis
10945 dmarcous

Close

9354 jacek-lewandowski
9451 vidma
10507 JerryLead
10486 wilson888888888
10460 huaxingao
10967 kevinyu98
10681 nikit-os
11766 s4weng
9907 Lewuathe
10209 nongli
10379 yanakad
10403 naveenminchu
10842 rajeshbalamohan
11036 mbautin


---------------------------------------------------------------------


"
Hyukjin Kwon <gurwls223@gmail.com>,"Sat, 7 May 2016 14:54:41 +0900",Re: Proposal of closing some PRs and maybe some PRs abandoned by its author,Sean Owen <sowen@cloudera.com>,"Sorry, here are two more PRs I guess should be closed.

10052  dereksabryfb   - This one is suggested by Sean and Herman to close
10356  somideshmukh   - This has not been confirmed by a committer yet but
I tested this and it seems not an issue.

Thanks.

2016-05-07 2:47 GMT+09:00 Sean Owen <sowen@cloudera.com>:

"
VHPC 16 <vhpc.dist@gmail.com>,"Sat, 7 May 2016 18:09:43 +0300","CfP 11th Workshop on Virtualization in High-Performance Cloud
 Computing (VHPC '16) (deadline extended May 20th)",dev@spark.apache.org,"CfP 11th Workshop on Virtualization in High-Performance Cloud
Computing (VHPC '16)


====================================================================

CALL FOR PAPERS



11th Workshop on Virtualization in HighÂ­-Performance Cloud Computing
(VHPC '16) held in conjunction with the International Supercomputing
Conference - High Performance (ISC), June 19-23, 2016, Frankfurt,
Germany.


====================================================================


Date: June 23, 2016

Workshop URL: http://vhpc.org


Paper Submission Deadline: May 20th (extended)



Call for Papers


Virtualization technologies constitute a key enabling factor for
flexible resource management in modern data centers, and particularly
in cloud environments.  Cloud providers need to manage complex
infrastructures in a seamless fashion to support the highly dynamic
and heterogeneous workloads and hosted applications customers deploy.
Similarly, HPC environments have been increasingly adopting techniques
that enable flexible management of vast computing and networking
resources, close to marginal provisioning cost, which is unprecedented
in the history of scientific and commercial computing.


Various virtualization technologies contribute to the overall picture
in different ways: machine virtualization, with its capability to
enable consolidation of multiple underÂ­utilized servers with
heterogeneous software and operating systems (OSes), and its
capability to liveÂ­-migrate a fully operating virtual machine (VM)
with a very short downtime, enables novel and dynamic ways to manage
physical servers; OS-Â­level virtualization (i.e., containerization),
with its capability to isolate multiple userÂ­-space environments and
to allow for their coÂ­existence within the same OS kernel, promises to
provide many of the advantages of machine virtualization with high
levels of responsiveness and performance; I/O Virtualization allows
physical NICs/HBAs to take traffic from multiple VMs or containers;
network virtualization, with its capability to create logical network
overlays that are independent of the underlying physical topology and
IP addressing, provides the fundamental ground on top of which evolved
network services can be realized with an unprecedented level of
dynamicity and flexibility; the increasingly adopted paradigm of
Software-Â­Defined Networking (SDN) promises to extend this flexibility
to the control and data planes of network paths.



Topics of Interest


The VHPC program committee solicits original, high-quality submissions
related to virtualization across the entire software stack with a
special focus on the intersection of HPC and the cloud. Topics
include, but are not limited to:


- Virtualization in supercomputing environments, HPC clusters, cloud

  HPC and grids

- OS-level virtualization including container runtimes (Docker, rkt et

  al.)

- Lightweight compute node operating systems/VMMs

- Optimizations of virtual machine monitor platforms, hypervisors

- QoS and SLA in hypervisors and network virtualization

- Cloud based network and system management for SDN and NFV

- Management, deployment and monitoring of virtualized environments

- Virtual per job / on-demand clusters and cloud bursting

- Performance measurement, modelling and monitoring of

  virtualized/cloud workloads

- Programming models for virtualized environments

- Virtualization in data intensive computing and Big Data processing

- Cloud reliability, fault-tolerance, high-availability and security

- Heterogeneous virtualized environments, virtualized accelerators,

  GPUs and co-processors

- Optimized communication libraries/protocols in the cloud and for HPC

  in the cloud

- Topology management and optimization for distributed virtualized applications

- Adaptation of emerging HPC technologies (high performance networks,

  RDMA, etc..)

- I/O and storage virtualization, virtualization aware file systems

- Job scheduling/control/policy in virtualized environments

- Checkpointing and migration of VM-based large compute jobs

- Cloud frameworks and APIs

- Energy-efficient / power-aware virtualization



The Workshop on Virtualization in HighÂ­-Performance Cloud Computing
(VHPC) aims to bring together researchers and industrial practitioners
facing the challenges posed by virtualization in order to foster
discussion, collaboration, mutual exchange of knowledge and
experience, enabling research to ultimately provide novel solutions
for virtualized computing systems of tomorrow.


The workshop will be one day in length, composed of 20 min paper
presentations, each followed by 10 min discussion sections, plus
lightning talks that are limited to 5 minutes.  Presentations may be
accompanied by interactive demonstrations.


Important Dates


May 20, 2016 - Paper submission deadline

May 30, 2016 Acceptance notification

June 23, 2016 - Workshop Day

July 25, 2016 - Camera-ready version due



Chair


Michael Alexander (chair), TU Wien, Austria

Anastassios Nanos (co-Â­chair), NTUA, Greece

Balazs Gerofi (co-Â­chair), RIKEN Advanced Institute for Computational

Science, Japan



Program committee


Stergios Anastasiadis, University of Ioannina, Greece

Costas Bekas, IBM Research, Switzerland

Jakob Blomer, CERN

Ron Brightwell, Sandia National Laboratories, USA

Roberto Canonico, University of Napoli Federico II, Italy


Stephen Crago, USC ISI, USA

Christoffer Dall, Columbia University, USA

Patrick Dreher, MIT, USA

Robert Futrick, Cycle Computing, USA

Robert Gardner, University of Chicago, USA

William Gardner, University of Guelph, Canada

Wolfgang Gentzsch, UberCloud, USA

Kyle Hale, Northwestern University, USA

Marcus Hardt, Karlsruhe Institute of Technology, Germany

Krishna Kant, Templte University, USA

Romeo Kinzler, IBM, Switzerland

Brian Kocoloski, University of Pittsburgh, USA

Kornilios Kourtis, IBM Research, Switzerland

Nectarios Koziris, National Technical University of Athens, Greece

John Lange, University of Pittsburgh, USA

Nikos Parlavantzas, IRISA, France

Kevin Pendretti, Sandia National Laboratories, USA

Che-Rung Roger Lee, National Tsing Hua University, Taiwan

Giuseppe Lettieri, University of Pisa, Italy

Qing Liu, Oak Ridge National Laboratory, USA

Paul Mundt, Adaptant, Germany

Amer Qouneh, University of Florida, USA

Carlos ReaÃ±o, Technical University of Valencia, Spain

Seetharami Seelam, IBM Research, USA

Josh Simons, VMWare, USA

Borja Sotomayor, University of Chicago, USA

Dieter Suess, TU Wien, Austria

Craig Stewart, Indiana University, USA

Anata Tiwari, San Diego Supercomputer Center, USA

Kurt Tutschku, Blekinge Institute of Technology, Sweden

Amit Vadudevan, Carnegie Mellon University, USA

Yasuhiro Watashiba, Osaka University, Japan

Nicholas Wright, Lawrence Berkeley National Laboratory, USA

Chao-Tung Yang, Tunghai University, Taiwan

Gianluigi Zanetti, CRS4, Italy



Paper Submission-Publication


Papers submitted to the workshop will be reviewed by at least two
members of the program committee and external reviewers. Submissions
should include abstract, key words, the e-mail address of the
corresponding author, and must not exceed 10 pages, including tables
and figures at a main font size no smaller than 11 point. Submission
of a paper should be regarded as a commitment that, should the paper
be accepted, at least one of the authors will register and attend the
conference to present the work.


The format must be according to the Springer LNCS Style. Initial
submissions are in PDF; authors of accepted papers will be requested
to provide source files.


Format Guidelines:

ftp://ftp.springer.de/pub/tex/latex/llncs/latex2e/llncs2e.zip


Abstract, Paper Submission Link:

https://edas.info/newPaper.php?c=21801



Lightning Talks


Lightning Talks are non-paper track, synoptical in nature and are
strictly limited to 5 minutes.  They can be used to gain early
feedback on ongoing research, for demonstrations, to present research
results, early research ideas, perspectives and positions of interest
to the community. Submit abstract via the main submission link.



General Information


The workshop is one day in length and will be held in conjunction with
the International Supercomputing Conference - High Performance (ISC)
2016, June 19-23, Frankfurt, Germany.

---------------------------------------------------------------------


"
Ali Tootoonchian <ali@levyx.com>,"Sun, 8 May 2016 17:17:53 -0700 (MST)",Re: Cache Shuffle Based Operation Before Sort,dev@spark.apache.org,"Thanks for your comment. 
Which image or chart are you pointing?



--

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Sun, 8 May 2016 22:21:31 -0700",Re: Cache Shuffle Based Operation Before Sort,Ali Tootoonchian <ali@levyx.com>,"I assume there were supposed to be images following this line (which I
don't see in the email thread):

bq. Letâ€™s look at details of execution for 10 and 100 scale factor input

Consider using 3rd party image site.


ased-Operation-Before-Sort-tp17331p17438.html
"
"""Jesse F Chen"" <jfchen@us.ibm.com>","Mon, 9 May 2016 13:24:46 -0700",spark 2.0 issue with yarn?,"""spark users"" <user@spark.apache.org>, dev <dev@spark.apache.org>","

I had been running fine until builds around 05/07/2016....

If I used the ""--master yarn"" in builds after 05/07, I got the following
error...sounds like something jars are missing.

I am using YARN 2.7.2 and Hive 1.2.1.

Do I need something new to deploy related to YARN?

bin/spark-sql -driver-memory 10g --verbose --master yarn --packages
com.databricks:spark-csv_2.10:1.3.0 --executor-memory 4g --num-executors
20 --executor-cores 2

16/05/09 13:15:21 INFO server.Server: jetty-8.y.z-SNAPSHOT
16/05/09 13:15:21 INFO server.AbstractConnector: Started
SelectChannelConnector@0.0.0.0:4041
16/05/09 13:15:21 INFO util.Utils: Successfully started service 'SparkUI'
on port 4041.
16/05/09 13:15:21 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at
http://bigaperf116.svl.ibm.com:4041
Exception in thread ""main"" java.lang.NoClassDefFoundError:
com/sun/jersey/api/client/config/ClientConfig
	at
org.apache.hadoop.yarn.client.api.TimelineClient.createTimelineClient
(TimelineClient.java:45)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.serviceInit
(YarnClientImpl.java:163)
	at org.apache.hadoop.service.AbstractService.init
(AbstractService.java:163)
	at org.apache.spark.deploy.yarn.Client.submitApplication
(Client.scala:150)
	at
org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start
(YarnClientSchedulerBackend.scala:56)
	at org.apache.spark.scheduler.TaskSchedulerImpl.start
(TaskSchedulerImpl.scala:148)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:502)
	at org.apache.spark.SparkContext$.getOrCreate
(SparkContext.scala:2246)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate
(SparkSession.scala:762)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLEnv$.init
(SparkSQLEnv.scala:57)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.<init>
(SparkSQLCLIDriver.scala:281)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main
(SparkSQLCLIDriver.scala:138)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main
(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke
(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke
(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy
$SparkSubmit$$runMain(SparkSubmit.scala:727)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1
(SparkSubmit.scala:183)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:208)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:122)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.ClassNotFoundException:
com.sun.jersey.api.client.config.ClientConfig
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	... 22 more
16/05/09 13:15:21 INFO storage.DiskBlockManager: Shutdown hook called
16/05/09 13:15:21 INFO util.ShutdownHookManager: Shutdown hook called
16/05/09 13:15:21 INFO util.ShutdownHookManager: Deleting
directory /tmp/spark-ac33b501-b9c3-47a3-93c8-fa02720bf4bb
16/05/09 13:15:21 INFO util.ShutdownHookManager: Deleting
directory /tmp/spark-65cb43d9-c122-4106-a0a8-ae7d92d9e19c
16/05/09 13:15:21 INFO util.ShutdownHookManager: Deleting
directory /tmp/spark-65cb43d9-c122-4106-a0a8-ae7d92d9e19c/userFiles-46dde536-29e5-46b3-a530-e5ad6640f8b2




                                                                                                                                              
                                                                                                                                              
                                                                                                                                              
                                                                                                                                              
                                                                                                                                              
                                                                                                                                              
                                   JESSE CHEN                                                                                                 
                                   Big Data Performance | IBM Analytics                                                                       
                                                                                                                                              
                                   Office:  408 463 2296                                                                                      
                                   Mobile: 408 828 9068                                                                                       
                                   Email:   jfchen@us.ibm.com                                                                                 
                                                                                                                                              
                                                                                                                                              


"
Sean Owen <sowen@cloudera.com>,"Mon, 9 May 2016 22:19:21 +0100",Re: spark 2.0 issue with yarn?,Jesse F Chen <jfchen@us.ibm.com>,"Hm, this may be related to updating to Jersey 2, which happened 4 days ago:
https://issues.apache.org/jira/browse/SPARK-12154

That is a Jersey 1 class that's missing. How are you building and running
Spark?

I think the theory was that Jersey 1 would still be supplied at runtime. We
may have to revise the exclusions.


"
"""Jesse F Chen"" <jfchen@us.ibm.com>","Mon, 9 May 2016 14:52:04 -0700",Re: spark 2.0 issue with yarn?,Sean Owen <sowen@cloudera.com>,"
Sean - thanks. definitely related to SPARK-12154.

Is there a way to continue use Jersey 1 for existing working environment?

Or, what's the best way to patch up existing Jersey 1 environment to Jersey
2?

This does break all of our Spark jobs running Spark 2.0 on YARN.

                                                                                                                                              
                                                                                                                                              
                                                                                                                                              
                                                                                                                                              
                                                                                                                                              
                                                                                                                                              
                                   JESSE CHEN                                                                                                 
                                   Big Data Performance | IBM Analytics                                                                       
                                                                                                                                              
                                   Office:  408 463 2296                                                                                      
                                   Mobile: 408 828 9068                                                                                       
                                   Email:   jfchen@us.ibm.com                                                                                 
                                                                                                                                              
                                                                                                                                              






From:	Sean Owen <sowen@cloudera.com>
To:	Jesse F Chen/San Francisco/IBM@IBMUS
Cc:	spark users <user@spark.apache.org>, dev
            <dev@spark.apache.org>, Roy Cecil <roy.cecil@ie.ibm.com>, Matt
            Cheah <mcheah@palantir.com>
Date:	05/09/2016 02:19 PM
Subject:	Re: spark 2.0 issue with yarn?



Hm, this may be related to updating to Jersey 2, which happened 4 days
ago: https://issues.apache.org/jira/browse/SPARK-12154

That is a Jersey 1 class that's missing. How are you building and running
Spark?

I think the theory was that Jersey 1 would still be supplied at runtime. We
may have to revise the exclusions.

  I had been running fine until builds around 05/07/2016....

  If I used the ""--master yarn"" in builds after 05/07, I got the following
  error...sounds like something jars are missing.

  I am using YARN 2.7.2 and Hive 1.2.1.

  Do I need something new to deploy related to YARN?

  bin/spark-sql -driver-memory 10g --verbose --master yarn --packages
  com.databricks:spark-csv_2.10:1.3.0 --executor-memory 4g --num-executors
  20 --executor-cores 2

  16/05/09 13:15:21 INFO server.Server: jetty-8.y.z-SNAPSHOT
  16/05/09 13:15:21 INFO server.AbstractConnector: Started
  SelectChannelConnector@0.0.0.0:4041
  16/05/09 13:15:21 INFO util.Utils: Successfully started service 'SparkUI'
  on port 4041.
  16/05/09 13:15:21 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started
  at http://bigaperf116.svl.ibm.com:4041
  Exception in thread ""main"" java.lang.NoClassDefFoundError:
  com/sun/jersey/api/client/config/ClientConfig
  at org.apache.hadoop.yarn.client.api.TimelineClient.createTimelineClient
  (TimelineClient.java:45)
  at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.serviceInit
  (YarnClientImpl.java:163)
  at org.apache.hadoop.service.AbstractService.init
  (AbstractService.java:163)
  at org.apache.spark.deploy.yarn.Client.submitApplication
  (Client.scala:150)
  at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start
  (YarnClientSchedulerBackend.scala:56)
  at org.apache.spark.scheduler.TaskSchedulerImpl.start
  (TaskSchedulerImpl.scala:148)
  at org.apache.spark.SparkContext.<init>(SparkContext.scala:502)
  at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2246)
  at org.apache.spark.sql.SparkSession$Builder.getOrCreate
  (SparkSession.scala:762)
  at org.apache.spark.sql.hive.thriftserver.SparkSQLEnv$.init
  (SparkSQLEnv.scala:57)
  at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.<init>
  (SparkSQLCLIDriver.scala:281)
  at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main
  (SparkSQLCLIDriver.scala:138)
  at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main
  (SparkSQLCLIDriver.scala)
  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  at sun.reflect.NativeMethodAccessorImpl.invoke
  (NativeMethodAccessorImpl.java:62)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke
  (DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:497)
  at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy
  $SparkSubmit$$runMain(SparkSubmit.scala:727)
  at org.apache.spark.deploy.SparkSubmit$.doRunMain$1
  (SparkSubmit.scala:183)
  at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:208)
  at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:122)
  at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
  Caused by: java.lang.ClassNotFoundException:
  com.sun.jersey.api.client.config.ClientConfig
  at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
  at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
  ... 22 more
  16/05/09 13:15:21 INFO storage.DiskBlockManager: Shutdown hook called
  16/05/09 13:15:21 INFO util.ShutdownHookManager: Shutdown hook called
  16/05/09 13:15:21 INFO util.ShutdownHookManager: Deleting
  directory /tmp/spark-ac33b501-b9c3-47a3-93c8-fa02720bf4bb
  16/05/09 13:15:21 INFO util.ShutdownHookManager: Deleting
  directory /tmp/spark-65cb43d9-c122-4106-a0a8-ae7d92d9e19c
  16/05/09 13:15:21 INFO util.ShutdownHookManager: Deleting
  directory /tmp/spark-65cb43d9-c122-4106-a0a8-ae7d92d9e19c/userFiles-46dde536-29e5-46b3-a530-e5ad6640f8b2








                                                                                                                                            
                                                                                                                                            
                                                                                                                                            
                                                                                                                                            
                             JESSE CHEN                                                                                                     
                             Big Data Performance | IBM Analytics                                                                           
                                                                                                                                            
                             Office: 408 463 2296                                                                                           
                             Mobile: 408 828 9068                                                                                           
                             Email: jfchen@us.ibm.com                                                                                       
                                                                                                                                            
                                                                                                                                            












"
Sean Owen <sowen@cloudera.com>,"Mon, 9 May 2016 22:54:55 +0100",Re: spark 2.0 issue with yarn?,Jesse F Chen <jfchen@us.ibm.com>,"The reason I ask how you're running is that YARN itself should have the
classes that YARN needs, and should be on your classpath. That's why it's
not in Spark.

There still could be a subtler problem. The best way to trouble shoot, if
you want to act on it now, is to have a look at the exclusions for Jersey 1
artifacts put in place in that PR, and try removing ones related to YARN,
and see if that remedies it. If so we may need to undo that part.


"
Marcelo Vanzin <vanzin@cloudera.com>,"Mon, 9 May 2016 15:10:54 -0700",Re: spark 2.0 issue with yarn?,Jesse F Chen <jfchen@us.ibm.com>,"Hi Jesse,


The error you're getting is because of a third-party extension that
tries to talk to the YARN ATS; that's not part of upstream Spark,
although I believe it's part of HDP. So you may have to talk to
Hortonworks about that, or disable that extension in Spark 2.0 for the
moment.


-- 
Marcelo

---------------------------------------------------------------------


"
Matt Cheah <mcheah@palantir.com>,"Mon, 9 May 2016 22:34:00 +0000",Re: spark 2.0 issue with yarn?,"Marcelo Vanzin <vanzin@cloudera.com>, Jesse F Chen <jfchen@us.ibm.com>","@Marcelo: Interesting - why would this manifest on the YARN-client side
though (as Spark is the client to YARN in this case)? Spark as a client
shouldnâ€™t care about what auxiliary services are on the YARN cluster.

@Jesse: The change I wrote exclude"
Marcelo Vanzin <vanzin@cloudera.com>,"Mon, 9 May 2016 15:37:58 -0700",Re: spark 2.0 issue with yarn?,Matt Cheah <mcheah@palantir.com>,"ter.

The ATS client is based on jersey 1.

-- 
Marcelo

---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Mon, 9 May 2016 16:17:41 -0700","Re: [build system] short downtime next thursday morning, 5-12-16 @
 8am PDT","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","reminder:  this is happening thursday morning.


---------------------------------------------------------------------


"
Steve Loughran <stevel@hortonworks.com>,"Tue, 10 May 2016 12:48:16 +0000",Re: spark 2.0 issue with yarn?,Jesse F Chen <jfchen@us.ibm.com>,"


I had been running fine until builds around 05/07/2016....

If I used the ""--master yarn"" in builds after 05/07, I got the following error...sounds like something jars are missing.

I am using YARN 2.7.2 and Hive 1.2.1.

Do I need something new to deploy related to YARN?

bin/spark-sql -driver-memory 10g --verbose --master yarn --packages com.databricks:spark-csv_2.10:1.3.0 --executor-memory 4g --num-executors 20 --executor-cores 2

16/05/09 13:15:21 INFO server.Server: jetty-8.y.z-SNAPSHOT
16/05/09 13:15:21 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0<mailto:SelectChannelConnector@0.0.0.0>:4041
16/05/09 13:15:21 INFO util.Utils: Successfully started service 'SparkUI' on port 4041.
16/05/09 13:15:21 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://bigaperf116.svl.ibm.com:4041<http://bigaperf116.svl.ibm.com:4041/>
Exception in thread ""main"" java.lang.NoClassDefFoundError: com/sun/jersey/api/client/config/ClientConfig
at org.apache.hadoop.yarn.client.api.TimelineClient.createTimelineClient(TimelineClient.java:45)
at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.serviceInit(YarnClientImpl.java:163)
at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:150)
at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:56)
at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:148)


Looks like Jersey client isn't on the classpath.

1. Consider filing a JIRA
2. set  spark.hadoop.yarn.timeline-service.enabled false to turn off ATS


at org.apache.spark.SparkContext.<init>(SparkContext.scala:502)
at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2246)
at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:762)
at org.apache.spark.sql.hive.thriftserver.SparkSQLEnv$.init(SparkSQLEnv.scala:57)
at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.<init>(SparkSQLCLIDriver.scala:281)
at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:138)
at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:497)
at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:727)
at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:183)
at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:208)
at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:122)
at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.ClassNotFoundException: com.sun.jersey.api.client.config.ClientConfig
at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
... 22 more
16/05/09 13:15:21 INFO storage.DiskBlockManager: Shutdown hook called
16/05/09 13:15:21 INFO util.ShutdownHookManager: Shutdown hook called
16/05/09 13:15:21 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-ac33b501-b9c3-47a3-93c8-fa02720bf4bb
16/05/09 13:15:21 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-65cb43d9-c122-4106-a0a8-ae7d92d9e19c
16/05/09 13:15:21 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-65cb43d9-c122-4106-a0a8-ae7d92d9e19c/userFiles-46dde536-29e5-46b3-a530-e5ad6640f8b2





<07983638.gif>  JESSE CHEN
Big Data Performance | IBM Analytics

Office: 408 463 2296
Mobile: 408 828 9068
Email: jfchen@us.ibm.com<mailto:jfchen@us.ibm.com>




"
Michael Gummelt <mgummelt@mesosphere.io>,"Tue, 10 May 2016 17:52:46 -0700",Remote JAR download in client mode,dev <dev@spark.apache.org>,"Client mode doesn't seem to support remote JAR downloading, as reported
here: https://issues.apache.org/jira/browse/SPARK-10643

The docs here:
http://spark.apache.org/docs/latest/submitting-applications.html don't
mention any client mode restriction to local files, so I'm assuming this is
a bug.  Can someone confirm that this is in fact a bug?  If so, I'm happy
to submit a PR.

-- 
Michael Gummelt
Software Engineer
Mesosphere
"
Timothy Chen <tnachen@gmail.com>,"Tue, 10 May 2016 21:09:53 -0700",Re: Remote JAR download in client mode,Michael Gummelt <mgummelt@mesosphere.io>,"I think it's just not implemented, +1 for adding it.

Tim


e:
re: https://issues.apache.org/jira/browse/SPARK-10643
.html don't mention any client mode restriction to local files, so I'm assuming this is a bug.  Can someone confirm that this is in fact a bug?  If so, I'm happy to submit a PR.
"
Ofir Manor <ofir.manor@equalum.io>,"Wed, 11 May 2016 11:47:55 +0300",Structured Streaming with Kafka source/sink,dev@spark.apache.org,"Hi,
I'm trying out Structured Streaming from current 2.0 branch.
Does the branch currently support Kafka as either source or sink? I
couldn't find a specific JIRA or design doc for that in SPARK-8360 or in
the examples... Is it still targeted for 2.0?

Also, I naively assume it will look similar to hdfs or JDBC *stream(""path"")*,
where the path will be some sort of Kafka URI (maybe protocol + broker list
+ topic list). Is that the current thinking?

Thanks,

Ofir Manor

Co-Founder & CTO | Equalum

Mobile: +972-54-7801286 | Email: ofir.manor@equalum.io
"
Ted Yu <yuzhihong@gmail.com>,"Wed, 11 May 2016 04:37:05 -0700",Re: Structured Streaming with Kafka source/sink,Ofir Manor <ofir.manor@equalum.io>,"Please see this thread:

http://search-hadoop.com/m/q3RTt9XAz651PiG/Adhoc+queries+spark+streaming&subj=Re+Adhoc+queries+on+Spark+2+0+with+Structured+Streaming

't find a specific JIRA or design doc for that in SPARK-8360 or in the examples... Is it still targeted for 2.0?
, where the path will be some sort of Kafka URI (maybe protocol + broker list + topic list). Is that the current thinking?
"
Tony Jin <linbojin203@gmail.com>,"Wed, 11 May 2016 21:55:08 +0800","dataframe udf functioin will be executed twice when filter on new
 column created by withColumn","spark users <user@spark.apache.org>, dev <dev@spark.apache.org>","Hi guys,

I have a problem about spark DataFrame. My spark version is 1.6.1.
Basically, i used udf and df.withColumn to create a ""new"" column, and then
i filter the values on this new columns and call show(action). I see the
udf function (which is used to by withColumn to create the new column) is
called twice(duplicated). And if filter on ""old"" column, udf only run once
which is expected. I attached the example codes, line 30~38 shows the
problem.

 Anyone knows the internal reason? Can you give me any advices? Thank you
very much.


1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47

scala> import org.apache.spark.sql.functions._
import org.apache.spark.sql.functions._

scala> val df = sc.parallelize(Seq((""a"", ""b""), (""a1"", ""b1""))).toDF(""old"",""old1"")
df: org.apache.spark.sql.DataFrame = [old: string, old1: string]

scala> val udfFunc = udf((s: String) => {println(s""running udf($s)""); s })
udfFunc: org.apache.spark.sql.UserDefinedFunction =
UserDefinedFunction(<function1>,StringType,List(StringType))

scala> val newDF = df.withColumn(""new"", udfFunc(df(""old"")))
newDF: org.apache.spark.sql.DataFrame = [old: string, old1: string, new: string]

scala> newDF.show
running udf(a)
running udf(a1)
+---+----+---+
|old|old1|new|
+---+----+---+
|  a|   b|  a|
| a1|  b1| a1|
+---+----+---+


old1: string, new: string]

old1: string, new: string]

running udf(a)
running udf(a)
running udf(a1)
+---+----+---+
|old|old1|new|
+---+----+---+
|  a|   b|  a|
+---+----+---+


running udf(a)
+---+----+---+
|old|old1|new|
+---+----+---+
|  a|   b|  a|
+---+----+---+



Best wishes.
By Linbo
"
Ted Yu <yuzhihong@gmail.com>,"Wed, 11 May 2016 07:49:18 -0700","Re: dataframe udf functioin will be executed twice when filter on new
 column created by withColumn",Tony Jin <linbojin203@gmail.com>,"In master branch, behavior is the same.

Suggest opening a JIRA if you haven't done so.


"
James Hammerton <james@gluru.co>,"Wed, 11 May 2016 17:49:41 +0100","Re: dataframe udf functioin will be executed twice when filter on new
 column created by withColumn",Ted Yu <yuzhihong@gmail.com>,"This may be related to: https://issues.apache.org/jira/browse/SPARK-13773

Regards,

James


"
Brian Cho <chobrian@gmail.com>,"Wed, 11 May 2016 12:01:32 -0700",Adding HDFS read-time metrics per task (RE: SPARK-1683),dev@spark.apache.org,"Hi,

I'm interested in adding read-time (from HDFS) to Task Metrics. The
motivation is to help debug performance issues. After some digging, its
briefly mentioned in SPARK-1683 that this feature didn't make it due to
metric collection causing a performance regression [1].

I'd like to try tackling this, but would be very grateful if those with
experience can give some more information on what was attempted previously,
and why this didn't work previously. Or if there are philosophical
objections to these metrics. If you feel this is a dead-end please help me
from myself.

Thank you,
Brian

[1] https://github.com/apache/spark/pull/962
"
"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Wed, 11 May 2016 19:46:29 +0000",Shrinking the DataFrame lineage ,"""dev@spark.apache.org"" <dev@spark.apache.org>","Dear Spark developers,

Recently, I was trying to switch my code from RDDs to DataFrames in order to compare the performance. The code computes RDD in a loop. I use RDD.persist followed by RDD.count to force Spark compute the RDD and cache it, so that it does not need to re-compute it on each iteration. However, it does not seem to work for DataFrame:

import scala.util.Random
val rdd = sc.parallelize(1 to 10, 2).map(x => (Random(5), Random(5))
val edges = sqlContext.createDataFrame(rdd).toDF(""from"", ""to"")
val vertices = edges.select(""from"").unionAll(edges.select(""to"")).distinct().cache()
vertices.count
[Stage 34:=================>                                     (65 + 4) / 200]
[Stage 34:========================>                              (90 + 5) / 200]
[Stage 34:==============================>                       (114 + 4) / 200]
[Stage 34:====================================>                 (137 + 4) / 200]
[Stage 34:==========================================>           (157 + 4) / 200]
[Stage 34:=================================================>    (182 + 4) / 200]

res25: Long = 5
If I run count again, it recomputes it again instead of using the cached result:
scala> vertices.count
[Stage 37:=============>                                         (49 + 4) / 200]
[Stage 37:==================>                                    (66 + 4) / 200]
[Stage 37:========================>                              (90 + 4) / 200]
[Stage 37:=============================>                        (110 + 4) / 200]
[Stage 37:===================================>                  (133 + 4) / 200]
[Stage 37:==========================================>           (157 + 4) / 200]
[Stage 37:================================================>     (178 + 5) / 200]
res26: Long = 5

Could you suggest how to schrink the DataFrame lineage ?

Best regards, Alexander
"
Reynold Xin <rxin@databricks.com>,"Wed, 11 May 2016 14:01:39 -0700",Re: Adding HDFS read-time metrics per task (RE: SPARK-1683),Brian Cho <chobrian@gmail.com>,"Adding Kay



"
shane knapp <sknapp@berkeley.edu>,"Wed, 11 May 2016 16:42:43 -0700","Re: [build system] short downtime next thursday morning, 5-12-16 @
 8am PDT","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","reminder:  this is happening tomorrow morning!

7am PDT:  builds paused
8am PDT:  master reboot, upgrade happens
9am PDT:  builds restarted


---------------------------------------------------------------------


"
Brian Cho <chobrian@gmail.com>,"Wed, 11 May 2016 20:44:00 -0700",Re: Adding HDFS read-time metrics per task (RE: SPARK-1683),Kay Ousterhout <keo@eecs.berkeley.edu>,"Hi Kay,

Thank you for the detailed explanation.

If I understand correctly, I *could* time each record processing time by
measuring the time in reader.next, but this would add overhead for every
single record. And this is the method that was abandoned because of
performance regressions.

The other possibility is changing HDFS first. This method looks promising
even if it takes some time. I'll play around with it a bit for now. Thanks
again!

-Brian


"
Senthil Kumar <senthilec566@gmail.com>,"Thu, 12 May 2016 16:23:49 +0530",Spark Exposing RDD as WebService ?,dev@spark.apache.org,"Hi All , I have a requirement to Process huge file ( 75 GB ) ..

          Here is the sample data :
          <InodeSection>
           <inode>
             <id>100</id>
             <name>spark.conf</name>
              .
              .
              .
           </inode>
          </InodeSection>

           <INodeDirectorySection>

<directory><parent>99</parent><inode>98</inode><inode>97</inode><inode>96</inode></directory>
          </INodeDirectorySection>


          Steps :
            1)    Load complete <InodeSection>
            2)    Load INodeDirectorySection
            3)    Iterate each INode and Query InodeSection as well as
InodeDirectory Section to know the Parents ( till ROOT directory )


          Currently i have done this , as below
          1) Load Inodes to Redis
          2) Load InodeDirectorySection to Redis
          3) For each Inode Query Redis and compute the Parents

           The number of Inodes close to 200 Million so the Job is not
completing within SLA.. I have max SLA as 2-2.5 Hours for this Operation.

           How do i use Spark here and Expose RDD as Service for my
requirement ??  Can this be done with Other methodologies ? ..

--Senthil
"
james <yiazhou@gmail.com>,"Thu, 12 May 2016 07:36:41 -0700 (MST)","How Spark SQL correctly connect hive metastore database with Spark
 2.0 ?",dev@spark.apache.org,"Hi Spark guys,
I am try to run Spark SQL using bin/spark-sql with Spark 2.0 master
code(commit ba181c0c7a32b0e81bbcdbe5eed94fc97b58c83e) but ran across an
issue that it always connect local derby database and can't connect my
existing hive metastore database. Could you help me to check what's the root
cause ? What's specific configuration for integration with hive metastore in
Spark 2.0 ? BTW, this case is OK in Spark 1.6.

Build package command:
./dev/make-distribution.sh --tgz -Pyarn -Phadoop-2.6
-Dhadoop.version=2.6.0-cdh5.5.1 -Phive -Phive-thriftserver -DskipTests

Key configurations in spark-defaults.conf:
spark.sql.hive.metastore.version=1.1.0
spark.sql.hive.metastore.jars=/usr/lib/hive/lib/*:/usr/lib/hadoop/client/*
spark.executor.extraClassPath=/etc/hive/conf
spark.driver.extraClassPath=/etc/hive/conf
spark.yarn.jars=local:/usr/lib/spark/jars/*

There is existing hive metastore database named by ""test_sparksql"". I always
got error ""metastore.ObjectStore: Failed to get database test_sparksql,
returning NoSuchObjectException"" after issuing 'use test_sparksql'. Please
see below steps for details.
 
$ /usr/lib/spark/bin/spark-sql --master yarn --deploy-mode client

SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in
[jar:file:/usr/lib/spark/jars/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in
[jar:file:/usr/lib/avro/avro-tools-1.7.6-cdh5.5.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in
[jar:file:/usr/lib/zookeeper/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an
explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
16/05/12 22:23:28 WARN conf.HiveConf: HiveConf of name
hive.enable.spark.execution.engine does not exist
16/05/12 22:23:30 INFO metastore.HiveMetaStore: 0: Opening raw store with
implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
16/05/12 22:23:30 INFO metastore.ObjectStore: ObjectStore, initialize called
16/05/12 22:23:30 WARN DataNucleus.General: Plugin (Bundle)
""org.datanucleus.store.rdbms"" is already registered. Ensure you dont have
multiple JAR versions of the same plugin in the classpath. The URL
""file:/usr/lib/hive/lib/datanucleus-rdbms-3.2.9.jar"" is already registered,
and you are trying to register an identical plugin located at URL
""file:/usr/lib/spark/jars/datanucleus-rdbms-3.2.9.jar.""
16/05/12 22:23:30 WARN DataNucleus.General: Plugin (Bundle)
""org.datanucleus"" is already registered. Ensure you dont have multiple JAR
versions of the same plugin in the classpath. The URL
""file:/usr/lib/hive/lib/datanucleus-core-3.2.10.jar"" is already registered,
and you are trying to register an identical plugin located at URL
""file:/usr/lib/spark/jars/datanucleus-core-3.2.10.jar.""
16/05/12 22:23:30 WARN DataNucleus.General: Plugin (Bundle)
""org.datanucleus.api.jdo"" is already registered. Ensure you dont have
multiple JAR versions of the same plugin in the classpath. The URL
""file:/usr/lib/spark/jars/datanucleus-api-jdo-3.2.6.jar"" is already
registered, and you are trying to register an identical plugin located at
URL ""file:/usr/lib/hive/lib/datanucleus-api-jdo-3.2.6.jar.""
16/05/12 22:23:30 INFO DataNucleus.Persistence: Property
datanucleus.cache.level2 unknown - will be ignored
16/05/12 22:23:30 INFO DataNucleus.Persistence: Property
hive.metastore.integral.jdo.pushdown unknown - will be ignored
16/05/12 22:23:31 WARN conf.HiveConf: HiveConf of name
hive.enable.spark.execution.engine does not exist
16/05/12 22:23:31 INFO metastore.ObjectStore: Setting MetaStore object pin
classes with
hive.metastore.cache.pinobjtypes=""Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order""
16/05/12 22:23:32 INFO DataNucleus.Datastore: The class
""org.apache.hadoop.hive.metastore.model.MFieldSchema"" is tagged as
""embedded-only"" so does not have its own datastore table.
16/05/12 22:23:32 INFO DataNucleus.Datastore: The class
""org.apache.hadoop.hive.metastore.model.MOrder"" is tagged as ""embedded-only""
so does not have its own datastore table.
16/05/12 22:23:33 INFO DataNucleus.Datastore: The class
""org.apache.hadoop.hive.metastore.model.MFieldSchema"" is tagged as
""embedded-only"" so does not have its own datastore table.
16/05/12 22:23:33 INFO DataNucleus.Datastore: The class
""org.apache.hadoop.hive.metastore.model.MOrder"" is tagged as ""embedded-only""
so does not have its own datastore table.
16/05/12 22:23:33 INFO metastore.MetaStoreDirectSql: Using direct SQL,
underlying DB is DERBY
16/05/12 22:23:33 INFO metastore.ObjectStore: Initialized ObjectStore
16/05/12 22:23:33 WARN metastore.ObjectStore: Version information not found
in metastore. hive.metastore.schema.verification is not enabled so recording
the schema version 1.2.0
16/05/12 22:23:33 WARN metastore.ObjectStore: Failed to get database
default, returning NoSuchObjectException
16/05/12 22:23:34 INFO metastore.HiveMetaStore: Added admin role in
metastore
16/05/12 22:23:34 INFO metastore.HiveMetaStore: Added public role in
metastore
16/05/12 22:23:34 INFO metastore.HiveMetaStore: No user is added in admin
role, since config is empty
16/05/12 22:23:34 INFO metastore.HiveMetaStore: 0: get_all_databases
16/05/12 22:23:34 INFO HiveMetaStore.audit: ugi=root    ip=unknown-ip-addr     
cmd=get_all_databases
16/05/12 22:23:34 INFO metastore.HiveMetaStore: 0: get_functions: db=default
pat=*
16/05/12 22:23:34 INFO HiveMetaStore.audit: ugi=root    ip=unknown-ip-addr     
cmd=get_functions: db=default pat=*
16/05/12 22:23:34 INFO DataNucleus.Datastore: The class
""org.apache.hadoop.hive.metastore.model.MResourceUri"" is tagged as
""embedded-only"" so does not have its own datastore table.
16/05/12 22:23:34 INFO session.SessionState: Created local directory:
/tmp/4e7ccc40-e10b-455c-b51d-ed225be85ffe_resources
16/05/12 22:23:34 INFO session.SessionState: Created HDFS directory:
/tmp/hive/root/4e7ccc40-e10b-455c-b51d-ed225be85ffe
16/05/12 22:23:34 INFO session.SessionState: Created local directory:
/tmp/root/4e7ccc40-e10b-455c-b51d-ed225be85ffe
16/05/12 22:23:34 INFO session.SessionState: Created HDFS directory:
/tmp/hive/root/4e7ccc40-e10b-455c-b51d-ed225be85ffe/_tmp_space.db
16/05/12 22:23:34 INFO spark.SparkContext: Running Spark version
2.0.0-SNAPSHOT
16/05/12 22:23:34 INFO spark.SecurityManager: Changing view acls to: root
16/05/12 22:23:34 INFO spark.SecurityManager: Changing modify acls to: root
16/05/12 22:23:34 INFO spark.SecurityManager: Changing view acls groups to:
16/05/12 22:23:34 INFO spark.SecurityManager: Changing modify acls groups
to:
16/05/12 22:23:34 INFO spark.SecurityManager: SecurityManager:
authentication disabled; ui acls disabled; users  with view permissions:
Set(root); groups with view permissions: Set(); users  with modify
permissions: Set(root); groups with modify permissions: Set()
16/05/12 22:23:35 INFO util.Utils: Successfully started service
'sparkDriver' on port 37223.
16/05/12 22:23:35 INFO spark.SparkEnv: Registering MapOutputTracker
16/05/12 22:23:35 INFO spark.SparkEnv: Registering BlockManagerMaster
16/05/12 22:23:35 INFO storage.DiskBlockManager: Created local directory at
/tmp/blockmgr-5a30adbe-4f9a-4b34-b52f-b61671f8b06d
16/05/12 22:23:35 INFO memory.MemoryStore: MemoryStore started with capacity
511.1 MB
16/05/12 22:23:35 INFO spark.SparkEnv: Registering OutputCommitCoordinator
16/05/12 22:23:35 INFO server.Server: jetty-8.y.z-SNAPSHOT
16/05/12 22:23:35 INFO server.AbstractConnector: Started
SelectChannelConnector@0.0.0.0:4040
16/05/12 22:23:35 INFO util.Utils: Successfully started service 'SparkUI' on
port 4040.
16/05/12 22:23:35 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at
http://192.168.3.11:4040
16/05/12 22:23:35 INFO client.RMProxy: Connecting to ResourceManager at
hw-node2/192.168.3.12:8032
16/05/12 22:23:35 INFO yarn.Client: Requesting a new application from
cluster with 4 NodeManagers
16/05/12 22:23:35 INFO yarn.Client: Verifying our application has not
requested more than the maximum memory capability of the cluster (196608 MB
per container)
16/05/12 22:23:35 INFO yarn.Client: Will allocate AM container, with 896 MB
memory including 384 MB overhead
16/05/12 22:23:35 INFO yarn.Client: Setting up container launch context for
our AM
16/05/12 22:23:35 INFO yarn.Client: Setting up the launch environment for
our AM container
16/05/12 22:23:35 INFO yarn.Client: Preparing resources for our AM container
16/05/12 22:23:35 INFO yarn.Client: Uploading resource
file:/tmp/spark-a712ffb6-a0d0-48db-99b4-ee6a41b3f132/__spark_conf__7597761027449817951.zip
->
hdfs://hw-node2:8020/user/root/.sparkStaging/application_1463053929123_0006/__spark_conf__.zip
16/05/12 22:23:36 INFO yarn.Client: Uploading resource
file:/tmp/spark-a712ffb6-a0d0-48db-99b4-ee6a41b3f132/__spark_conf__9093112552235548615.zip
->
hdfs://hw-node2:8020/user/root/.sparkStaging/application_1463053929123_0006/__spark_conf__9093112552235548615.zip
16/05/12 22:23:36 INFO spark.SecurityManager: Changing view acls to: root
16/05/12 22:23:36 INFO spark.SecurityManager: Changing modify acls to: root
16/05/12 22:23:36 INFO spark.SecurityManager: Changing view acls groups to:
16/05/12 22:23:36 INFO spark.SecurityManager: Changing modify acls groups
to:
16/05/12 22:23:36 INFO spark.SecurityManager: SecurityManager:
authentication disabled; ui acls disabled; users  with view permissions:
Set(root); groups with view permissions: Set(); users  with modify
permissions: Set(root); groups with modify permissions: Set()
16/05/12 22:23:36 INFO yarn.Client: Submitting application
application_1463053929123_0006 to ResourceManager
16/05/12 22:23:36 INFO impl.YarnClientImpl: Submitted application
application_1463053929123_0006
16/05/12 22:23:36 INFO cluster.SchedulerExtensionServices: Starting Yarn
extension services with app application_1463053929123_0006 and attemptId
None
16/05/12 22:23:37 INFO yarn.Client: Application report for
application_1463053929123_0006 (state: ACCEPTED)
16/05/12 22:23:37 INFO yarn.Client:
         client token: N/A
         diagnostics: N/A
         ApplicationMaster host: N/A
         ApplicationMaster RPC port: -1
         queue: root.root
         start time: 1463063016173
         final status: UNDEFINED
         tracking URL:
http://hw-node2:8088/proxy/application_1463053929123_0006/
         user: root
16/05/12 22:23:38 INFO yarn.Client: Application report for
application_1463053929123_0006 (state: ACCEPTED)
16/05/12 22:23:38 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint:
ApplicationMaster registered as NettyRpcEndpointRef(null)
16/05/12 22:23:38 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter.
org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS
-> hw-node2, PROXY_URI_BASES ->
http://hw-node2:8088/proxy/application_1463053929123_0006),
/proxy/application_1463053929123_0006
16/05/12 22:23:38 INFO ui.JettyUtils: Adding filter:
org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
16/05/12 22:23:39 INFO yarn.Client: Application report for
application_1463053929123_0006 (state: RUNNING)
16/05/12 22:23:39 INFO yarn.Client:
         client token: N/A
         diagnostics: N/A
         ApplicationMaster host: 192.168.3.16
         ApplicationMaster RPC port: 0
         queue: root.root
         start time: 1463063016173
         final status: UNDEFINED
         tracking URL:
http://hw-node2:8088/proxy/application_1463053929123_0006/
         user: root
16/05/12 22:23:39 INFO cluster.YarnClientSchedulerBackend: Application
application_1463053929123_0006 has started running.
16/05/12 22:23:39 INFO util.Utils: Successfully started service
'org.apache.spark.network.netty.NettyBlockTransferService' on port 45022.
16/05/12 22:23:39 INFO netty.NettyBlockTransferService: Server created on
192.168.3.11:45022
16/05/12 22:23:39 INFO storage.BlockManager: external shuffle service port =
7337
16/05/12 22:23:39 INFO storage.BlockManagerMaster: Trying to register
BlockManager
16/05/12 22:23:39 INFO storage.BlockManagerMasterEndpoint: Registering block
manager 192.168.3.11:45022 with 511.1 MB RAM, BlockManagerId(driver,
192.168.3.11, 45022)
16/05/12 22:23:39 INFO storage.BlockManagerMaster: Registered BlockManager
16/05/12 22:23:39 INFO scheduler.EventLoggingListener: Logging events to
hdfs://hw-node2:8020/user/spark/applicationHistory/application_1463053929123_0006
16/05/12 22:23:39 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend
is ready for scheduling beginning after reached minRegisteredResourcesRatio:
0.8
16/05/12 22:23:39 INFO hive.HiveSharedState: Setting Hive metastore
warehouse path to '/root/spark-warehouse'
16/05/12 22:23:39 INFO hive.HiveUtils: Initializing HiveMetastoreConnection
version 1.1.0 using
file:/usr/lib/hive/lib/httpcore-4.2.5.jar:file:/usr/lib/hive/lib/hive-contrib.jar:file:/usr/lib/hive/lib/oro-2.0.8.jar:file:/usr/lib/hive/lib/accumulo-start-1.6.0.jar:file:/usr/lib/hive/lib/groovy-all-2.4.4.jar:file:/usr/lib/hive/lib/hive-metastore.jar:file:/usr/lib/hive/lib/hive-beeline.jar:file:/usr/lib/hive/lib/datanucleus-core-3.2.10.jar:file:/usr/lib/hive/lib/jackson-core-2.2.2.jar:file:/usr/lib/hive/lib/velocity-1.5.jar:file:/usr/lib/hive/lib/hive-serde.jar:file:/usr/lib/hive/lib/hive-metastore-1.1.0-cdh5.5.1.jar:file:/usr/lib/hive/lib/commons-beanutils-core-1.8.0.jar:file:/usr/lib/hive/lib/hamcrest-core-1.1.jar:file:/usr/lib/hive/lib/jta-1.1.jar:file:/usr/lib/hive/lib/hive-shims-0.23-1.1.0-cdh5.5.1.jar:file:/usr/lib/hive/lib/antlr-2.7.7.jar:file:/usr/lib/hive/lib/hive-exec-1.1.0-cdh5.5.1.jar:file:/usr/lib/hive/lib/geronimo-jta_1.1_spec-1.1.1.jar:file:/usr/lib/hive/lib/accumulo-fate-1.6.0.jar:file:/usr/lib/hive/lib/hive-accumulo-handler.jar:file:/usr/lib/hive/lib/snappy-java-1.0.4.1.jar:file:/usr/lib/hive/lib/tempus-fugit-1.1.jar:file:/usr/lib/hive/lib/maven-scm-provider-svn-commons-1.4.jar:file:/usr/lib/hive/lib/libfb303-0.9.2.jar:file:/usr/lib/hive/lib/datanucleus-rdbms-3.2.9.jar:file:/usr/lib/hive/lib/xz-1.0.jar:file:/usr/lib/hive/lib/hbase-common.jar:file:/usr/lib/hive/lib/activation-1.1.jar:file:/usr/lib/hive/lib/hive-ant-1.1.0-cdh5.5.1.jar:file:/usr/lib/hive/lib/accumulo-trace-1.6.0.jar:file:/usr/lib/hive/lib/hive-serde-1.1.0-cdh5.5.1.jar:file:/usr/lib/hive/lib/commons-compress-1.4.1.jar:file:/usr/lib/hive/lib/hbase-hadoop2-compat.jar:file:/usr/lib/hive/lib/commons-configuration-1.6.jar:file:/usr/lib/hive/lib/servlet-api-2.5.jar:file:/usr/lib/hive/lib/libthrift-0.9.2.jar:file:/usr/lib/hive/lib/stax-api-1.0.1.jar:file:/usr/lib/hive/lib/hive-testutils.jar:file:/usr/lib/hive/lib/hive-shims-scheduler-1.1.0-cdh5.5.1.jar:file:/usr/lib/hive/lib/hive-testutils-1.1.0-cdh5.5.1.jar:file:/usr/lib/hive/lib/junit-4.11.jar:file:/usr/lib/hive/lib/jackson-annotations-2.2.2.jar:file:/usr/lib/hive/lib/stringtemplate-3.2.1.jar:file:/usr/lib/hive/lib/super-csv-2.2.0.jar:file:/usr/lib/hive/lib/hive-hwi-1.1.0-cdh5.5.1.jar:file:/usr/lib/hive/lib/log4j-1.2.16.jar:file:/usr/lib/hive/lib/geronimo-jaspic_1.0_spec-1.0.jar:file:/usr/lib/hive/lib/accumulo-core-1.6.0.jar:file:/usr/lib/hive/lib/hive-hbase-handler.jar:file:/usr/lib/hive/lib/high-scale-lib-1.1.1.jar:file:/usr/lib/hive/lib/hbase-protocol.jar:file:/usr/lib/hive/lib/hive-common-1.1.0-cdh5.5.1.jar:file:/usr/lib/hive/lib/hive-jdbc.jar:file:/usr/lib/hive/lib/commons-logging-1.1.3.jar:file:/usr/lib/hive/lib/derby-10.11.1.1.jar:file:/usr/lib/hive/lib/hive-jdbc-1.1.0-cdh5.5.1.jar:file:/usr/lib/hive/lib/hive-shims-scheduler.jar:file:/usr/lib/hive/lib/asm-commons-3.1.jar:file:/usr/lib/hive/lib/hive-jdbc-standalone.jar:file:/usr/lib/hive/lib/maven-scm-api-1.4.jar:file:/usr/lib/hive/lib/janino-2.7.6.jar:file:/usr/lib/hive/lib/hive-cli.jar:file:/usr/lib/hive/lib/maven-scm-provider-svnexe-1.4.jar:file:/usr/lib/hive/lib/bonecp-0.8.0.RELEASE.jar:file:/usr/lib/hive/lib/zookeeper.jar:file:/usr/lib/hive/lib/jline-2.12.jar:file:/usr/lib/hive/lib/asm-3.2.jar:file:/usr/lib/hive/lib/logredactor-1.0.3.jar:file:/usr/lib/hive/lib/hive-ant.jar:file:/usr/lib/hive/lib/hive-shims-1.1.0-cdh5.5.1.jar:file:/usr/lib/hive/lib/ant-launcher-1.9.1.jar:file:/usr/lib/hive/lib/hive-cli-1.1.0-cdh5.5.1.jar:file:/usr/lib/hive/lib/gson-2.2.4.jar:file:/usr/lib/hive/lib/avro.jar:file:/usr/lib/hive/lib/parquet-hadoop-bundle.jar:file:/usr/lib/hive/lib/commons-beanutils-1.7.0.jar:file:/usr/lib/hive/lib/commons-digester-1.8.jar:file:/usr/lib/hive/lib/apache-log4j-extras-1.2.17.jar:file:/usr/lib/hive/lib/calcite-core-1.0.0-incubating.jar:file:/usr/lib/hive/lib/metrics-json-3.0.2.jar:file:/usr/lib/hive/lib/hive-jdbc-1.1.0-cdh5.5.1-standalone.jar:file:/usr/lib/hive/lib/jackson-databind-2.2.2.jar:file:/usr/lib/hive/lib/hive-exec.jar:file:/usr/lib/hive/lib/jersey-server-1.14.jar:file:/usr/lib/hive/lib/asm-tree-3.1.jar:file:/usr/lib/hive/lib/jdo-api-3.0.1.jar:file:/usr/lib/hive/lib/geronimo-annotation_1.0_spec-1.1.1.jar:file:/usr/lib/hive/lib/metrics-core-3.0.2.jar:file:/usr/lib/hive/lib/commons-dbcp-1.4.jar:file:/usr/lib/hive/lib/mail-1.4.1.jar:file:/usr/lib/hive/lib/metrics-jvm-3.0.2.jar:file:/usr/lib/hive/lib/paranamer-2.3.jar:file:/usr/lib/hive/lib/commons-lang-2.6.jar:file:/usr/lib/hive/lib/commons-compiler-2.7.6.jar:file:/usr/lib/hive/lib/commons-codec-1.4.jar:file:/usr/lib/hive/lib/guava-14.0.1.jar:file:/usr/lib/hive/lib/hive-service-1.1.0-cdh5.5.1.jar:file:/usr/lib/hive/lib/jersey-servlet-1.14.jar:file:/usr/lib/hive/lib/regexp-1.3.jar:file:/usr/lib/hive/lib/jpam-1.1.jar:file:/usr/lib/hive/lib/calcite-linq4j-1.0.0-incubating.jar:file:/usr/lib/hive/lib/hive-accumulo-handler-1.1.0-cdh5.5.1.jar:file:/usr/lib/hive/lib/hbase-server.jar:file:/usr/lib/hive/lib/eigenbase-properties-1.1.4.jar:file:/usr/lib/hive/lib/commons-pool-1.5.4.jar:file:/usr/lib/hive/lib/commons-vfs2-2.0.jar:file:/usr/lib/hive/lib/jackson-jaxrs-1.9.2.jar:file:/usr/lib/hive/lib/hive-hbase-handler-1.1.0-cdh5.5.1.jar:file:/usr/lib/hive/lib/commons-math-2.1.jar:file:/usr/lib/hive/lib/commons-cli-1.2.jar:file:/usr/lib/hive/lib/commons-io-2.4.jar:file:/usr/lib/hive/lib/ant-1.9.1.jar:file:/usr/lib/hive/lib/ST4-4.0.4.jar:file:/usr/lib/hive/lib/hive-shims-common-1.1.0-cdh5.5.1.jar:file:/usr/lib/hive/lib/hive-common.jar:file:/usr/lib/hive/lib/jetty-all-server-7.6.0.v20120127.jar:file:/usr/lib/hive/lib/hive-service.jar:file:/usr/lib/hive/lib/hbase-hadoop-compat.jar:file:/usr/lib/hive/lib/hive-shims-0.23.jar:file:/usr/lib/hive/lib/hive-contrib-1.1.0-cdh5.5.1.jar:file:/usr/lib/hive/lib/curator-client-2.6.0.jar:file:/usr/lib/hive/lib/commons-httpclient-3.0.1.jar:file:/usr/lib/hive/lib/plexus-utils-1.5.6.jar:file:/usr/lib/hive/lib/pentaho-aggdesigner-algorithm-5.1.5-jhyde.jar:file:/usr/lib/hive/lib/jetty-all-7.6.0.v20120127.jar:file:/usr/lib/hive/lib/hive-shims.jar:file:/usr/lib/hive/lib/datanucleus-api-jdo-3.2.6.jar:file:/usr/lib/hive/lib/htrace-core.jar:file:/usr/lib/hive/lib/httpclient-4.2.5.jar:file:/usr/lib/hive/lib/jcommander-1.32.jar:file:/usr/lib/hive/lib/antlr-runtime-3.4.jar:file:/usr/lib/hive/lib/opencsv-2.3.jar:file:/usr/lib/hive/lib/jsr305-3.0.0.jar:file:/usr/lib/hive/lib/jackson-xc-1.9.2.jar:file:/usr/lib/hive/lib/hive-shims-common.jar:file:/usr/lib/hive/lib/curator-framework-2.6.0.jar:file:/usr/lib/hive/lib/calcite-avatica-1.0.0-incubating.jar:file:/usr/lib/hive/lib/hive-beeline-1.1.0-cdh5.5.1.jar:file:/usr/lib/hive/lib/hive-hwi.jar:file:/usr/lib/hive/lib/hbase-client.jar:file:/usr/lib/hadoop/client/httpcore-4.2.5.jar:file:/usr/lib/hadoop/client/hadoop-hdfs.jar:file:/usr/lib/hadoop/client/apacheds-i18n-2.0.0-M15.jar:file:/usr/lib/hadoop/client/apacheds-kerberos-codec.jar:file:/usr/lib/hadoop/client/slf4j-api-1.7.5.jar:file:/usr/lib/hadoop/client/commons-net.jar:file:/usr/lib/hadoop/client/commons-beanutils-core-1.8.0.jar:file:/usr/lib/hadoop/client/jackson-annotations-2.2.3.jar:file:/usr/lib/hadoop/client/commons-logging.jar:file:/usr/lib/hadoop/client/curator-recipes-2.7.1.jar:file:/usr/lib/hadoop/client/hadoop-aws-2.6.0-cdh5.5.1.jar:file:/usr/lib/hadoop/client/snappy-java.jar:file:/usr/lib/hadoop/client/leveldbjni-all.jar:file:/usr/lib/hadoop/client/jackson-databind.jar:file:/usr/lib/hadoop/client/commons-lang.jar:file:/usr/lib/hadoop/client/xmlenc-0.52.jar:file:/usr/lib/hadoop/client/snappy-java-1.0.4.1.jar:file:/usr/lib/hadoop/client/commons-httpclient.jar:file:/usr/lib/hadoop/client/hadoop-yarn-server-common.jar:file:/usr/lib/hadoop/client/jackson-databind-2.2.3.jar:file:/usr/lib/hadoop/client/guava-11.0.2.jar:file:/usr/lib/hadoop/client/xz-1.0.jar:file:/usr/lib/hadoop/client/hadoop-yarn-api-2.6.0-cdh5.5.1.jar:file:/usr/lib/hadoop/client/hadoop-mapreduce-client-jobclient.jar:file:/usr/lib/hadoop/client/hadoop-mapreduce-client-app.jar:file:/usr/lib/hadoop/client/activation-1.1.jar:file:/usr/lib/hadoop/client/jaxb-api.jar:file:/usr/lib/hadoop/client/commons-compress-1.4.1.jar:file:/usr/lib/hadoop/client/commons-configuration-1.6.jar:file:/usr/lib/hadoop/client/jackson-xc.jar:file:/usr/lib/hadoop/client/servlet-api-2.5.jar:file:/usr/lib/hadoop/client/xmlenc.jar:file:/usr/lib/hadoop/client/jackson-jaxrs.jar:file:/usr/lib/hadoop/client/jackson-xc-1.8.8.jar:file:/usr/lib/hadoop/client/hadoop-common-2.6.0-cdh5.5.1.jar:file:/usr/lib/hadoop/client/apacheds-kerberos-codec-2.0.0-M15.jar:file:/usr/lib/hadoop/client/commons-cli.jar:file:/usr/lib/hadoop/client/hadoop-mapreduce-client-app-2.6.0-cdh5.5.1.jar:file:/usr/lib/hadoop/client/aws-java-sdk-1.7.4.jar:file:/usr/lib/hadoop/client/netty.jar:file:/usr/lib/hadoop/client/protobuf-java.jar:file:/usr/lib/hadoop/client/jaxb-api-2.2.2.jar:file:/usr/lib/hadoop/client/commons-logging-1.1.3.jar:file:/usr/lib/hadoop/client/commons-net-3.1.jar:file:/usr/lib/hadoop/client/hadoop-annotations.jar:file:/usr/lib/hadoop/client/hadoop-hdfs-2.6.0-cdh5.5.1.jar:file:/usr/lib/hadoop/client/jersey-core.jar:file:/usr/lib/hadoop/client/hadoop-yarn-client-2.6.0-cdh5.5.1.jar:file:/usr/lib/hadoop/client/hadoop-auth-2.6.0-cdh5.5.1.jar:file:/usr/lib/hadoop/client/zookeeper.jar:file:/usr/lib/hadoop/client/commons-collections-3.2.2.jar:file:/usr/lib/hadoop/client/servlet-api.jar:file:/usr/lib/hadoop/client/guava.jar:file:/usr/lib/hadoop/client/hadoop-yarn-api.jar:file:/usr/lib/hadoop/client/commons-math3.jar:file:/usr/lib/hadoop/client/slf4j-api.jar:file:/usr/lib/hadoop/client/stax-api.jar:file:/usr/lib/hadoop/client/hadoop-auth.jar:file:/usr/lib/hadoop/client/commons-io.jar:file:/usr/lib/hadoop/client/commons-digester.jar:file:/usr/lib/hadoop/client/hadoop-annotations-2.6.0-cdh5.5.1.jar:file:/usr/lib/hadoop/client/gson-2.2.4.jar:file:/usr/lib/hadoop/client/hadoop-mapreduce-client-core-2.6.0-cdh5.5.1.jar:file:/usr/lib/hadoop/client/avro.jar:file:/usr/lib/hadoop/client/activation.jar:file:/usr/lib/hadoop/client/apacheds-i18n.jar:file:/usr/lib/hadoop/client/hadoop-yarn-common.jar:file:/usr/lib/hadoop/client/commons-beanutils-1.7.0.jar:file:/usr/lib/hadoop/client/hadoop-mapreduce-client-common.jar:file:/usr/lib/hadoop/client/commons-digester-1.8.jar:file:/usr/lib/hadoop/client/jetty-util.jar:file:/usr/lib/hadoop/client/jackson-core-asl-1.8.8.jar:file:/usr/lib/hadoop/client/jetty-util-6.1.26.cloudera.4.jar:file:/usr/lib/hadoop/client/httpcore.jar:file:/usr/lib/hadoop/client/curator-client.jar:file:/usr/lib/hadoop/client/netty-3.6.2.Final.jar:file:/usr/lib/hadoop/client/jackson-mapper-asl.jar:file:/usr/lib/hadoop/client/commons-beanutils-core.jar:file:/usr/lib/hadoop/client/jackson-jaxrs-1.8.8.jar:file:/usr/lib/hadoop/client/xz.jar:file:/usr/lib/hadoop/client/paranamer-2.3.jar:file:/usr/lib/hadoop/client/commons-lang-2.6.jar:file:/usr/lib/hadoop/client/jackson-annotations.jar:file:/usr/lib/hadoop/client/commons-codec-1.4.jar:file:/usr/lib/hadoop/client/jersey-core-1.9.jar:file:/usr/lib/hadoop/client/api-asn1-api-1.0.0-M20.jar:file:/usr/lib/hadoop/client/commons-collections.jar:file:/usr/lib/hadoop/client/hadoop-mapreduce-client-jobclient-2.6.0-cdh5.5.1.jar:file:/usr/lib/hadoop/client/api-util.jar:file:/usr/lib/hadoop/client/jsr305.jar:file:/usr/lib/hadoop/client/httpclient.jar:file:/usr/lib/hadoop/client/xml-apis-1.3.04.jar:file:/usr/lib/hadoop/client/hadoop-mapreduce-client-shuffle.jar:file:/usr/lib/hadoop/client/hadoop-mapreduce-client-core.jar:file:/usr/lib/hadoop/client/curator-client-2.7.1.jar:file:/usr/lib/hadoop/client/commons-httpclient-3.1.jar:file:/usr/lib/hadoop/client/hadoop-yarn-common-2.6.0-cdh5.5.1.jar:file:/usr/lib/hadoop/client/commons-cli-1.2.jar:file:/usr/lib/hadoop/client/commons-io-2.4.jar:file:/usr/lib/hadoop/client/curator-framework.jar:file:/usr/lib/hadoop/client/stax-api-1.0-2.jar:file:/usr/lib/hadoop/client/htrace-core4.jar:file:/usr/lib/hadoop/client/jackson-core-2.2.3.jar:file:/usr/lib/hadoop/client/jackson-core-asl.jar:file:/usr/lib/hadoop/client/commons-configuration.jar:file:/usr/lib/hadoop/client/commons-compress.jar:file:/usr/lib/hadoop/client/hadoop-mapreduce-client-common-2.6.0-cdh5.5.1.jar:file:/usr/lib/hadoop/client/xercesImpl-2.9.1.jar:file:/usr/lib/hadoop/client/jersey-client-1.9.jar:file:/usr/lib/hadoop/client/log4j.jar:file:/usr/lib/hadoop/client/jackson-mapper-asl-1.8.8.jar:file:/usr/lib/hadoop/client/leveldbjni-all-1.8.jar:file:/usr/lib/hadoop/client/api-util-1.0.0-M20.jar:file:/usr/lib/hadoop/client/curator-framework-2.7.1.jar:file:/usr/lib/hadoop/client/commons-codec.jar:file:/usr/lib/hadoop/client/xml-apis.jar:file:/usr/lib/hadoop/client/jersey-client.jar:file:/usr/lib/hadoop/client/hadoop-yarn-client.jar:file:/usr/lib/hadoop/client/aws-java-sdk.jar:file:/usr/lib/hadoop/client/paranamer.jar:file:/usr/lib/hadoop/client/hadoop-aws.jar:file:/usr/lib/hadoop/client/hadoop-yarn-server-common-2.6.0-cdh5.5.1.jar:file:/usr/lib/hadoop/client/commons-math3-3.1.1.jar:file:/usr/lib/hadoop/client/hadoop-mapreduce-client-shuffle-2.6.0-cdh5.5.1.jar:file:/usr/lib/hadoop/client/httpclient-4.2.5.jar:file:/usr/lib/hadoop/client/commons-beanutils.jar:file:/usr/lib/hadoop/client/hadoop-common.jar:file:/usr/lib/hadoop/client/api-asn1-api.jar:file:/usr/lib/hadoop/client/htrace-core4-4.0.1-incubating.jar:file:/usr/lib/hadoop/client/log4j-1.2.17.jar:file:/usr/lib/hadoop/client/jsr305-3.0.0.jar:file:/usr/lib/hadoop/client/curator-recipes.jar:file:/usr/lib/hadoop/client/slf4j-log4j12.jar:file:/usr/lib/hadoop/client/jackson-core.jar:file:/usr/lib/hadoop/client/protobuf-java-2.5.0.jar:file:/usr/lib/hadoop/client/xercesImpl.jar:file:/usr/lib/hadoop/client/gson.jar
16/05/12 22:23:40 INFO metastore.HiveMetaStore: 0: Opening raw store with
implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
16/05/12 22:23:40 INFO metastore.ObjectStore: ObjectStore, initialize called
16/05/12 22:23:40 INFO DataNucleus.Persistence: Property
datanucleus.cache.level2 unknown - will be ignored
16/05/12 22:23:40 INFO DataNucleus.Persistence: Property
hive.metastore.integral.jdo.pushdown unknown - will be ignored
16/05/12 22:23:41 INFO metastore.ObjectStore: Setting MetaStore object pin
classes with
hive.metastore.cache.pinobjtypes=""Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order""
16/05/12 22:23:42 INFO DataNucleus.Datastore: The class
""org.apache.hadoop.hive.metastore.model.MFieldSchema"" is tagged as
""embedded-only"" so does not have its own datastore table.
16/05/12 22:23:42 INFO DataNucleus.Datastore: The class
""org.apache.hadoop.hive.metastore.model.MOrder"" is tagged as ""embedded-only""
so does not have its own datastore table.
16/05/12 22:23:42 INFO DataNucleus.Datastore: The class
""org.apache.hadoop.hive.metastore.model.MFieldSchema"" is tagged as
""embedded-only"" so does not have its own datastore table.
16/05/12 22:23:42 INFO DataNucleus.Datastore: The class
""org.apache.hadoop.hive.metastore.model.MOrder"" is tagged as ""embedded-only""
so does not have its own datastore table.
16/05/12 22:23:42 INFO DataNucleus.Query: Reading in results for query
""org.datanucleus.store.rdbms.query.SQLQuery@0"" since the connection used is
closing
16/05/12 22:23:42 INFO metastore.MetaStoreDirectSql: Using direct SQL,
underlying DB is DERBY
16/05/12 22:23:42 INFO metastore.ObjectStore: Initialized ObjectStore
16/05/12 22:23:42 INFO metastore.HiveMetaStore: Added admin role in
metastore
16/05/12 22:23:42 INFO metastore.HiveMetaStore: Added public role in
metastore
16/05/12 22:23:42 INFO metastore.HiveMetaStore: No user is added in admin
role, since config is empty
16/05/12 22:23:42 INFO log.PerfLogger: <PERFLOG method=get_all_functions
from=org.apache.hadoop.hive.metastore.RetryingHMSHandler>
16/05/12 22:23:42 INFO metastore.HiveMetaStore: 0: get_all_functions
16/05/12 22:23:42 INFO HiveMetaStore.audit: ugi=root    ip=unknown-ip-addr     
cmd=get_all_functions
16/05/12 22:23:42 INFO DataNucleus.Datastore: The class
""org.apache.hadoop.hive.metastore.model.MResourceUri"" is tagged as
""embedded-only"" so does not have its own datastore table.
16/05/12 22:23:42 INFO log.PerfLogger: </PERFLOG method=get_all_functions
start=1463063022896 end=1463063022941 duration=45
from=org.apache.hadoop.hive.metastore.RetryingHMSHandler threadId=0
retryCount=0 error=false>
16/05/12 22:23:42 INFO session.SessionState: Created local directory:
/tmp/f1ff20d6-3eac-4df0-adbd-64f7e73f35e8_resources
16/05/12 22:23:42 INFO session.SessionState: Created HDFS directory:
/tmp/hive/root/f1ff20d6-3eac-4df0-adbd-64f7e73f35e8
16/05/12 22:23:42 INFO session.SessionState: Created local directory:
/tmp/root/f1ff20d6-3eac-4df0-adbd-64f7e73f35e8
16/05/12 22:23:42 INFO session.SessionState: Created HDFS directory:
/tmp/hive/root/f1ff20d6-3eac-4df0-adbd-64f7e73f35e8/_tmp_space.db
16/05/12 22:23:42 INFO session.SessionState: No Tez session required at this
point. hive.execution.engine=mr.
16/05/12 22:23:42 INFO client.HiveClientImpl: Warehouse location for Hive
client (version 1.1.0) is /root/spark-warehouse
16/05/12 22:23:43 INFO session.SessionState: Created local directory:
/tmp/4f466b18-e85b-4fa5-9b3a-2a1a67118851_resources
16/05/12 22:23:43 INFO session.SessionState: Created HDFS directory:
/tmp/hive/root/4f466b18-e85b-4fa5-9b3a-2a1a67118851
16/05/12 22:23:43 INFO session.SessionState: Created local directory:
/tmp/root/4f466b18-e85b-4fa5-9b3a-2a1a67118851
16/05/12 22:23:43 INFO session.SessionState: Created HDFS directory:
/tmp/hive/root/4f466b18-e85b-4fa5-9b3a-2a1a67118851/_tmp_space.db
16/05/12 22:23:43 INFO session.SessionState: No Tez session required at this
point. hive.execution.engine=mr.
16/05/12 22:23:43 INFO client.HiveClientImpl: Warehouse location for Hive
client (version 1.1.0) is /root/spark-warehouse
spark-sql> use test_sparksql;
16/05/12 22:25:06 INFO execution.SparkSqlParser: Parsing command: use
test_sparksql
16/05/12 22:25:06 INFO log.PerfLogger: <PERFLOG method=create_database
from=org.apache.hadoop.hive.metastore.RetryingHMSHandler>
16/05/12 22:25:06 INFO metastore.HiveMetaStore: 0: create_database:
Database(name:default, description:default database,
locationUri:hdfs://hw-node2:8020/root/spark-warehouse, parameters:{})
16/05/12 22:25:06 INFO HiveMetaStore.audit: ugi=root    ip=unknown-ip-addr     
cmd=create_database: Database(name:default, description:default database,
locationUri:hdfs://hw-node2:8020/root/spark-warehouse, parameters:{})
16/05/12 22:25:06 ERROR metastore.RetryingHMSHandler:
AlreadyExistsException(message:Database default already exists)
        at
org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database(HiveMetaStore.java:898)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at
org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:133)
        at
org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:99)
        at com.sun.proxy.$Proxy34.create_database(Unknown Source)
        at
org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createDatabase(HiveMetaStoreClient.java:645)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at
org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:91)
        at com.sun.proxy.$Proxy35.createDatabase(Unknown Source)
        at
org.apache.hadoop.hive.ql.metadata.Hive.createDatabase(Hive.java:341)
        at
org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply$mcV$sp(HiveClientImpl.scala:292)
        at
org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply(HiveClientImpl.s"
"
shane knapp <sknapp@berkeley.edu>,Thu"," 12 May 2016 08:00:04 -0700""","Re: [build system] short downtime next thursday morning, 5-12-16 @
 8am PDT","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","this is happening now.


---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Thu, 12 May 2016 08:41:05 -0700","Re: [build system] short downtime next thursday morning, 5-12-16 @
 8am PDT","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","things are looking good -- i'm backing up the entire jenkins
installation right now (just in case), so that's taking a while to
finish.

i'm doing the backup as LTS has finally surpassed the version we're
on, so i'm taking this opportunity to move this installation to LTS.

shane


---------------------------------------------------------------------


"
Takeshi Yamamuro <linguin.m.s@gmail.com>,"Fri, 13 May 2016 00:47:19 +0900",Re: Cache Shuffle Based Operation Before Sort,Ali Tootoonchian <ali@levyx.com>,"Hi,

Look interesting.
This optimisation also seems effective in case of simply loading and
sorting df;
val df = sqlCtx.read.load(path)
df.cache.sort(""some colum"")

How big does this optimisation have effects on actual performance?
If big, it'd be better to open JIRA.

// maropu


r input
Based-Operation-Before-Sort-tp17331p17438.html


-- 
---
Takeshi Yamamuro
"
shane knapp <sknapp@berkeley.edu>,"Thu, 12 May 2016 09:27:28 -0700","Re: [build system] short downtime next thursday morning, 5-12-16 @
 8am PDT","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","ok, i've decided to roll back the upgrade and do this again early next
week.  some of the new features/security fixes break the pull request
builder, so i will need to revisit my plan.

sorry for the downtime -- we're back up and running now.


---------------------------------------------------------------------


"
Steve Loughran <stevel@hortonworks.com>,"Thu, 12 May 2016 19:12:50 +0000",Re: Adding HDFS read-time metrics per task (RE: SPARK-1683),Brian Cho <chobrian@gmail.com>,"

Hi Kay,

Thank you for the detailed explanation.

If I understand correctly, I *could* time each record processing time by measuring the time in reader.next, but this would add overhead for every single record. And this is the method that was abandoned because of performance regressions.

The other possibility is changing HDFS first. This method looks promising even if it takes some time. I'll play around with it a bit for now. Thanks again!

-Brian

Hi Brian,

Unfortunately it's not possible to do this in Spark for two reasons.  First, we read records from Spark one at a time (e.g., if you're reading a HDFS file and performing some map function, one record will be read from HDFS, then the map function will be applied, then the next record will be read, etc.). The relevant code is here<https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala#L209>: we create an iterator that's then passed on to other downstream RDDs.  As a result, we'd need to time each record's processing, which adds too much overhead.

The other potential issue is that we use the RecordReader interface, which means that we get deserialized and decompressed records, so any time we measured would include time to read the data from disk and decompress/deserialize it (not sure if you're trying to isolate the disk time).

Measuring decompression overhead alone is interesting. Indeed, with encryption at rest and erasure coding in hadoop, you'd think about isolating work there too, to see where the bottlenecks move to after a switch to SSDs.


It *is* possible to do this instrumentation for disk read time in HDFS, because HDFS reads larger blocks from disk (and then passes them to Spark one by one), and I did that (in a hacky way) in the most recent commits in this Hadoop branch<https://github.com/kayousterhout/hadoop-common/commits/2.0.2-instrumented>.  I filed a Hadoop JIRA <https://issues.apache.org/jira/browse/HADOOP-11873> to add this (in a less hacky way, using FileSystem.Statistics) but haven't submitted a patch for it.  If there's sufficient interest, I could properly implement the metrics and see if it could be merged into Hadoop, at which point Spark could start reading those metrics (unfortunately, the delay for this would be pretty significant because we'd need to wait for a new Hadoop version and then a new Spark version, and it would only be available in newer versions of Hadoop).

The metrics API changed 19 hours ago into something more sophisticated, though it doesn't measure timings.

https://issues.apache.org/jira/browse/HADOOP-13065

it's designed to be more extensible; you'll ask for a metric by name, not compile-time field...this will let different filesystems add different values

A few minutes ago, https://issues.apache.org/jira/browse/HADOOP-13028 went in to do some metric work for spark, and there the stats can be printed in logs, because the filesystem and inputStream toString() operators return the metrics. That's for people: not machines; the text may break without warning. But you can at least dump the metrics in your logs to see what's going on. That stuff can be seen in downstream tests, but not directly published as metrics. The aggregate stats are also collected as metrics2 stats, which should somehow be convertible to Coda Hale metrics, and hence with the rest of Spark's monitoring.


A more straightforward action might just be for spark itself to subclass FilterFileSystem and implement operation timing there, both for operations and any input/output streams returned in create & open.

"
Brian Cho <chobrian@gmail.com>,"Thu, 12 May 2016 13:59:24 -0700",Re: Adding HDFS read-time metrics per task (RE: SPARK-1683),Steve Loughran <stevel@hortonworks.com>,"Kay -- we would like to add the read metrics (in a compatible way) into our
internal DFS at Facebook and then call that method from Spark. In parallel
if you can finish up HADOOP-11873 :) , then we could add hooks to those
metrics in Spark. What do you think? Does this look like a feasible plan to
getting the metrics in?

Thanks,
Brian


"
Alexander Pivovarov <apivovarov@gmail.com>,"Thu, 12 May 2016 14:16:30 -0700",Spark uses disk instead of memory to store RDD blocks,dev <dev@spark.apache.org>,"Hello Everyone

I use Spark 1.6.0 on YARN  (EMR-4.3.0)

I use MEMORY_AND_DISK_SER StorageLevel for my RDD. And I use Kryo Serializer

I noticed that Spark uses Disk to store some RDD blocks even if Executors
have lots memory available. See the screenshot
http://postimg.org/image/gxpsw1fk1/

Any ideas why it might happen?

Thank you
Alex
"
Reynold Xin <rxin@databricks.com>,"Thu, 12 May 2016 14:29:24 -0700","[discuss] separate API annotation into two components:
 InterfaceAudience & InterfaceStability","""dev@spark.apache.org"" <dev@spark.apache.org>","We currently have three levels of interface annotation:

- unannotated: stable public API
- DeveloperApi: A lower-level, unstable API intended for developers.
- Experimental: An experimental user-facing API.


After using this annotation for ~ 2 years, I would like to propose the
following changes:

1. Require explicitly annotation for public APIs. This reduces the chance
of us accidentally exposing private APIs.

2. Separate interface annotation into two components: one that describes
intended audience, and the other that describes stability, similar to what
Hadoop does. This allows us to define ""low level"" APIs that are stable,
e.g. the data source API (I'd argue this is the API that should be more
stable than end-user-facing APIs).

InterfaceAudience: Public, Developer

InterfaceStability: Stable, Experimental


What do you think?
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Thu, 12 May 2016 15:34:13 -0700","Re: [discuss] separate API annotation into two components:
 InterfaceAudience & InterfaceStability",Reynold Xin <rxin@databricks.com>,"+1

I'm not very sure about this. What advantage do we get from Public vs.
Developer ? Also somebody needs to take a judgement call on that which
might not always be easy to do

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Thu, 12 May 2016 15:35:26 -0700","Re: [discuss] separate API annotation into two components:
 InterfaceAudience & InterfaceStability",Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"That's true. I think I want to differentiate end-user vs developer. Public
isn't the best word. Maybe EndUser?


"
Takeshi Yamamuro <linguin.m.s@gmail.com>,"Fri, 13 May 2016 09:12:31 +0900",Re: Spark uses disk instead of memory to store RDD blocks,Alexander Pivovarov <apivovarov@gmail.com>,"Hi,

Not sure this is a correct answer though, seems `UnifiedMemoryManager`
spills
some blocks of RDDs into disk when execution memory runs short.

// maropu





-- 
---
Takeshi Yamamuro
"
Alexander Pivovarov <apivovarov@gmail.com>,"Thu, 12 May 2016 17:35:25 -0700",Re: Spark uses disk instead of memory to store RDD blocks,Takeshi Yamamuro <linguin.m.s@gmail.com>,"Each executor on the screenshot has 25GB memory remaining . What was the
reason to store 170-500 MB to disk if executor has 25GB memory available?


"
Takeshi Yamamuro <linguin.m.s@gmail.com>,"Fri, 13 May 2016 11:23:14 +0900",Re: Spark uses disk instead of memory to store RDD blocks,Alexander Pivovarov <apivovarov@gmail.com>,"If you invoked the shuffling that eats a large amount of execution memory,
it possibly swept away
cached RDD blocks because the memory for the shuffling run short.
Please see:
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/memory/UnifiedMemoryManager.scala#L32

// maropu




-- 
---
Takeshi Yamamuro
"
Sean Busbey <busbey@cloudera.com>,"Thu, 12 May 2016 20:32:13 -0700","Re: [discuss] separate API annotation into two components:
 InterfaceAudience & InterfaceStability",Reynold Xin <rxin@databricks.com>,"We could switch to the Audience Annotation from Apache Yetus[1], and
then rely on Public for end-users and LimitedPrivate for those things
we intend as lower-level things with particular non-end-user
audiences.

[1]: http://yetus.apache.org/documentation/in-progress/#yetus-audience-annotations




-- 
busbey

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Thu, 12 May 2016 21:30:59 -0700","Re: [discuss] separate API annotation into two components:
 InterfaceAudience & InterfaceStability",Sean Busbey <busbey@cloudera.com>,"Hm LimitedPrivate is not the intention. Those APIs (e.g. data source) are
by no means private. They are just lower level APIs whose intended audience
is library developers, not end users.


"
=?UTF-8?B?5q6155+z55+z?= <burness1990@gmail.com>,"Fri, 13 May 2016 14:41:48 +0800",code change for adding takeSample of DataFrame,dev@spark.apache.org,"Hi, all:


I have add takeSample in the DataFrame, which sampling with specify num of
the rows. It has a similary version in RDD, but not supported in DataFrame
now. And now  local test is done, Is it ok to make a pr ?



Thanks
"
Reynold Xin <rxin@databricks.com>,"Thu, 12 May 2016 23:43:49 -0700",Re: code change for adding takeSample of DataFrame,=?UTF-8?B?5q6155+z55+z?= <burness1990@gmail.com>,"Sure go for it. Thanks.



f
e
"
=?gb2312?B?zfTR8w==?= <tiandiwoxin@icloud.com>,"Fri, 13 May 2016 17:47:28 +0800",HiveContext.refreshTable() missing in spark 2.0,dev@spark.apache.org,"Hi all,

I notice that HiveContext used to have a refreshTable() method, but it doesn¡¯t in branch-2.0. 

Do we drop that intentionally? If yes, how do we achieve similar functionality?

Thanks.

Yang"
Steve Loughran <stevel@hortonworks.com>,"Fri, 13 May 2016 10:24:43 +0000","Re: [discuss] separate API annotation into two components:
 InterfaceAudience & InterfaceStability",Reynold Xin <rxin@databricks.com>,"
llowing changes:
 of us accidentally exposing private APIs.

+1

intended audience, and the other that describes stability, similar to what Hadoop does. This allows us to define ""low level"" APIs that are stable, e.g. the data source API (I'd argue this is the API that should be more stable than end-user-facing APIs).


you should know there's a bit of a ""discussion"" in Hadoop right now about what ""LimitedPrivate"" means, that is: things marked ""LimitedPrivate(MapReduce)"" are pretty much universally used in YARN apps, and other things tagged as private (UGI) are so universal that its meaningless. That is: even if you tag up something as Developer, it may end up being used so widely that it becomes public. The hard part then becomes recognising which classes and methods have such a use, which ends up needing an IDE with everything loaded in.

Java 9 is going to open up a lot more in terms of modularization, though i don't know what that will mean for scala. For Java projects, it may allow isolation to be more explicit

---------------------------------------------------------------------


"
Tom Graves <tgraves_cs@yahoo.com.INVALID>,"Fri, 13 May 2016 13:37:21 +0000 (UTC)","Re: [discuss] separate API annotation into two components:
 InterfaceAudience & InterfaceStability","Reynold Xin <rxin@databricks.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","So we definitely need to be careful here. Â I know you didn't mention it but it mentioned by others so I would not recommend using LimitedPrivate. Â I had started a discussion on Hadoop about some of this due to the way Spark needed to use some of the Api's.https://issues.apache.org/jira/browse/HADOOP-10506

Overall it seems like a good idea, but we definitely need definitions with these and make sure they are clear to the end user looking at the code or docs.
I assume Developer really means to be used only within Spark? Developer is a pretty broad term which could mean end user developer or spark internal developer, etc. Â Hadoop uses Private for this I think from an end user point of view PRIVATE is more obvious that they shouldn't be using it. So perhaps something other then Developer. Â (INTERNAL, PROJECT_PRIVATE, etc.)
Tom
 

ote:
 

 We currently have three levels of interface annotation:
- unannotated: stable public API- DeveloperApi:Â A lower-level, unstable API intended for developers.- Experimental:Â An experimental user-facing API.

After using this annotation for ~ 2 years, I would like to propose the following changes:
1. Require explicitly annotation for public APIs. This reduces the chance of us accidentally exposing private APIs.
2. Separate interface annotation into two components: one that describes intended audience, and the other that describes stability, similar to what Hadoop does. This allows us to define ""low level"" APIs that are stable, e.g. the data source API (I'd argue this is the API that should be more stable than end-user-facing APIs).
InterfaceAudience: Public, Developer
InterfaceStability: Stable, Experimental

What do you think?

  "
Sean Busbey <busbey@cloudera.com>,"Fri, 13 May 2016 10:18:29 -0700","Re: [discuss] separate API annotation into two components:
 InterfaceAudience & InterfaceStability",Tom Graves <tgraves_cs@yahoo.com>,"

I think LimitedPrivate gets a bad rap due to the way it is misused in
Hadoop. The use case here -- ""we offer this to developers of
intermediate layers; those willing to update their software as we
update ours"" -- is a perfectly acceptable distinction from the ""this
is just for us"" and ""this is something folks can rely on enough to
contract out their software development"". Essentially,
LimitedPrivate(LIBRARY) or LimitedPrivate(PORCELAIN) (to borrow from
git's distinction on interfaces for tool makers vs end users).



-- 
busbey

---------------------------------------------------------------------


"
Marcelo Vanzin <vanzin@cloudera.com>,"Fri, 13 May 2016 11:15:41 -0700","Re: [discuss] separate API annotation into two components:
 InterfaceAudience & InterfaceStability",Sean Busbey <busbey@cloudera.com>,"
I think ""LimitedPrivate"" is a rather confusing name for that. I think
Reynold's first e-mail better matches that use case: this would be
""InterfaceAudience(Developer)"" and ""InterfaceStability(Experimental)"".

But I don't really like ""Developer"" as a name here, because it's
ambiguous. Developer of what? Theoretically everybody writing Spark or
on top of its APIs is a developer. In that sense, I prefer using
something like ""Library"" and ""Application"" instead of ""Developer"" and
""Public"".

Personally, in fact, I don't see a lot of gain in differentiating
between the target users of an interface... knowing whether it's a
stable interface or not is a lot more useful. If you're equating a
""developer API"" with ""it's not really stable"", then you don't really
need two annotations for that - just say it's not stable.

-- 
Marcelo

---------------------------------------------------------------------


"
Joseph Bradley <joseph@databricks.com>,"Fri, 13 May 2016 12:38:20 -0700",Re: Shrinking the DataFrame lineage,"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Here's a JIRA for it: https://issues.apache.org/jira/browse/SPARK-13346

I don't have a great method currently, but hacks can get around it: convert
the DataFrame to an RDD and back to truncate the query plan lineage.

Joseph


"
Michael Armbrust <michael@databricks.com>,"Fri, 13 May 2016 13:18:22 -0700","Re: [discuss] separate API annotation into two components:
 InterfaceAudience & InterfaceStability",Marcelo Vanzin <vanzin@cloudera.com>,"+1 to the general structure of Reynold's proposal.  I've found what we do
currently a little confusing.  In particular, it doesn't make much sense
that @DeveloperApi things are always labeled as possibly changing.  For
example the Data Source API should a"
"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Fri, 13 May 2016 21:29:35 +0000",RE: Shrinking the DataFrame lineage,Joseph Bradley <joseph@databricks.com>,"Hi Joseph,

Thank you for the link! Two follow up questions
1)Suppose I have the original DataFrame in Tungsen, i.e. catalyst types and cached in off-heap store. It might be quite useful for iterative workloads due to lower GC overhead. Then I convert it to RDD and then backto DF. Will the resulting DF remain off-heap or it will be on heap as regular RDD?
2)How is the mentioned problem handled in GraphFrames? Suppose, I want to use aggregateMessages in the iterative loop, for implementing PageRank.

Best regards, Alexander

From: Joseph Bradley [mailto:joseph@databricks.com]
Sent: Friday, May 13, 2016 12:38 PM
To: Ulanov, Alexander <alexander.ulanov@hpe.com>
Cc: dev@spark.apache.org
Subject: Re: Shrinking the DataFrame lineage

Here's a JIRA for it: https://issues.apache.org/jira/browse/SPARK-13346

I don't have a great method currently, but hacks can get around it: convert the DataFrame to an RDD and back to truncate the query plan lineage.

Joseph

On Wed, May 11, 2016 at 12:46 PM, Ulanov, Alexander <alexander.ulanov@hpe.com<mailto:alexander.ulanov@hpe.com>> wrote:
Dear Spark developers,

Recently, I was trying to switch my code from RDDs to DataFrames in order to compare the performance. The code computes RDD in a loop. I use RDD.persist followed by RDD.count to force Spark compute the RDD and cache it, so that it does not need to re-compute it on each iteration. However, it does not seem to work for DataFrame:

import scala.util.Random
val rdd = sc.parallelize(1 to 10, 2).map(x => (Random(5), Random(5))
val edges = sqlContext.createDataFrame(rdd).toDF(""from"", ""to"")
val vertices = edges.select(""from"").unionAll(edges.select(""to"")).distinct().cache()
vertices.count
[Stage 34:=================>                                     (65 + 4) / 200]
[Stage 34:========================>                              (90 + 5) / 200]
[Stage 34:==============================>                       (114 + 4) / 200]
[Stage 34:====================================>                 (137 + 4) / 200]
[Stage 34:==========================================>           (157 + 4) / 200]
[Stage 34:=================================================>    (182 + 4) / 200]

res25: Long = 5
If I run count again, it recomputes it again instead of using the cached result:
scala> vertices.count
[Stage 37:=============>                                         (49 + 4) / 200]
[Stage 37:==================>                                    (66 + 4) / 200]
[Stage 37:========================>                              (90 + 4) / 200]
[Stage 37:=============================>                        (110 + 4) / 200]
[Stage 37:===================================>                  (133 + 4) / 200]
[Stage 37:==========================================>           (157 + 4) / 200]
[Stage 37:================================================>     (178 + 5) / 200]
res26: Long = 5

Could you suggest how to schrink the DataFrame lineage ?

Best regards, Alexander

"
Jonathan Gray <jonny.gray@gmail.com>,"Sat, 14 May 2016 10:29:41 +0100",Nested/Chained case statements generate codegen over 64k exception,dev@spark.apache.org,"Hi,

I've raised JIRA SPARK-15258 (with code attached to re-produce problem) and
would like to have a go at fixing it but don't really know where to start.
Could anyone provide some pointers?

I've looked at the code associated with SPARK-13242 but was hoping to find
a way to avoid the codegen fallback.  Is this something that is possible?

Thanks,
Jon
"
Reynold Xin <rxin@databricks.com>,"Sat, 14 May 2016 19:01:46 -0700",Re: Nested/Chained case statements generate codegen over 64k exception,Jonathan Gray <jonny.gray@gmail.com>,"It might be best to fix this with fallback first, and then figure out how
we can do it more intelligently.




"
Alexander Pivovarov <apivovarov@gmail.com>,"Sat, 14 May 2016 20:13:44 -0700",combitedTextFile and CombineTextInputFormat,dev <dev@spark.apache.org>,"Hello Everyone

Do you think it would be useful to add combinedTextFile method (which uses
CombineTextInputFormat) to SparkContext?

It allows one task to read data from multiple text files and control number
of RDD partitions by setting
mapreduce.input.fileinputformat.split.maxsize


  def combinedTextFile(sc: SparkContext)(path: String): RDD[String] = {
    val conf = sc.hadoopConfiguration
    sc.newAPIHadoopFile(path, classOf[CombineTextInputFormat],
classOf[LongWritable], classOf[Text], conf).
      map(pair => pair._2.toString).setName(path)
  }


Alex
"
Hamel Kothari <hamelkothari@gmail.com>,"Sun, 15 May 2016 14:15:55 +0000",Re: Shrinking the DataFrame lineage,"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>, Joseph Bradley <joseph@databricks.com>","I don't know about the second one but for question #1:
When you convert from a cached DF to an RDD (via a map function or the
""rdd"" value) the types are converted from the off-heap types to on-heap
types. If your rows are fairly large/complex this can have a pretty big
performance impact so I would watch out for that.


"
Renyi Xiong <renyixiong0@gmail.com>,"Sun, 15 May 2016 11:46:33 -0700",Spark shuffling OutOfMemoryError Java heap space,dev <dev@spark.apache.org>,"Hi

I am consistently observing driver OutOfMemoryError (Java heap space)
during shuffling operation indicated by the log:

â€¦â€¦â€¦â€¦

16/05/14 21:57:03 INFO MapOutputTrackerMaster: Size of output statuses for
shuffle 2 is 36060250 bytes Ã  shuffle metadata size is big and the full
metadata will be sent to all workers?

16/05/14 21:57:06 INFO MapOutputTrackerMasterEndpoint: Asked to send map
output locations for shuffle 2 to <host1>:45757

16/05/14 21:57:06 INFO MapOutputTrackerMasterEndpoint: Asked to send map
output locations for shuffle 2 to <host2>:20300

16/05/14 21:57:06 INFO MapOutputTrackerMasterEndpoint: Asked to send map
output locations for shuffle 2 to <host3>:12389

16/05/14 21:57:06 INFO MapOutputTrackerMasterEndpoint: Asked to send map
output locations for shuffle 2 to <host4>:32197

â€¦â€¦â€¦â€¦

Exception in thread ""dispatcher-event-loop-17"" Exception in thread
""dispatcher-event-loop-3"" Exception in thread ""dispatcher-event-loop-6""
16/05/14 21:59:04 INFO MapOutputTrackerMasterEndpoint: Asked to send map
output locations for shuffle 2 to <host5>:19639

Exception in thread ""dispatcher-event-loop-21"" 16/05/14 21:59:08 INFO
MapOutputTrackerMasterEndpoint: Asked to send map output locations for
shuffle 2 to <host6>:58461

Exception in thread ""dispatcher-event-loop-20"" Exception in thread
""dispatcher-event-loop-13"" Exception in thread
""dispatcher-event-loop-9"" java.lang.OutOfMemoryError:
Java heap space

java.lang.OutOfMemoryError: Java heap space

                at java.util.Arrays.copyOf(Arrays.java:2271)

                at
java.io.ByteArrayOutputStream.toByteArray(ByteArrayOutputStream.java:178)

                at org.apache.spark.serializer.
JavaSerializerInstance.serialize(JavaSerializer.scala:103) Ã  shuffle
metadata duplicated (?) when sending to each executor?

                at
org.apache.spark.rpc.netty.NettyRpcEnv.serialize(NettyRpcEnv.scala:252)

                at
org.apache.spark.rpc.netty.RemoteNettyRpcCallContext.send(NettyRpcCallContext.scala:64)

                at
org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)

                at
org.apache.spark.MapOutputTrackerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(MapOutputTracker.scala:62)

                at
org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:104)

                at
org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:204)

                at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)

                at
org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:215)

                at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)

                at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
                at java.lang.Thread.run(Thread.java:724)

I enable memory dump and used jhat to analyze. In heap histogram, I found
146 byte array objects with exact same size of 36,060,293 bytes.

I wonder if the 146 big objects are actually duplicates of the same shuffle
metadata, *can experts please help understand if it's true?*

(8G driver memory was specified for the above run, should be sufficient for
the 36M shuffle metadata. but probably not for 146 duplicates)

thanks,
Renyi.
"
Holden Karau <holden@pigscanfly.ca>,"Sun, 15 May 2016 14:40:51 -0700",PySpark mixed with Jython,"""dev@spark.apache.org"" <dev@spark.apache.org>","I've been doing some looking at EclairJS (Spark + Javascript) which takes a
really interesting approach. The driver program is run in node and the
workers are run in nashorn. I was wondering if anyone has given much though
to optionally exposing an interface for PySpark in a similar fashion. For
some UDFs and UDAFs we could keep the data entirely in the JVM, and still
go back to our old PipelinedRDD based interface for operations which
require native libraries or otherwise aren't supported in Jython. Have I
had too much coffee and this is actually a bad idea or is this something
people think would be worth investigating some?

-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
Hyukjin Kwon <gurwls223@gmail.com>,"Mon, 16 May 2016 10:50:55 +0900",Question about enabling some of missing rules.,dev <dev@spark.apache.org>,"Hi all,

Lately, I made a list of rules currently not applied on Spark from
http://www.scalastyle.org/rules-dev.html and then I tried to test them.

I found two rules that I think might be helpful but I am not too sure.
Could I ask both can be added?


*RedundantIfChecker *(See
http://www.scalastyle.org/rules-dev.html#org_scalastyle_scalariform_RedundantIfChecker
)
It seems there are two usage of this. This simply checks if (cond) true
else false or if (cond) false else true,which can be just cond or !cond


*ProcedureDeclarationChecker *(See
http://www.scalastyle.org/rules-dev.html#org_scalastyle_scalariform_ProcedureDeclarationChecker
)
â€‹

It seems this simply checks if functions has the return type `= :Unit`
explicitly. This one seems right because it is written in
https://cwiki.apache.org/confluence/display/SPARK/Spark+Code+Style+Guide#SparkCodeStyleGuide-ReturnTypes
â€‹

However, it seems the number of occurrence is super a lot. (It seems
roughly more than 800 times). It seems this will cause a lot of conflicts.



Here is a list of rules not mentioned in scalastyle-config.xml just in case
someone wants to know.

*IndentationChecker*

<check enabled=""true"" class=""org.scalastyle.file.IndentationChecker""
level=""warning"">

 <parameters>

  <parameter name=""tabSize"">2</parameter>

  <parameter name=""methodParamIndentSize"">2</parameter>

 </parameters>

</check>


*BlockImportChecker*

<check enabled=""true"" class=""org.scalastyle.scalariform.BlockImportChecker""
level=""warning""/>


*DeprecatedJavaChecker*

<check enabled=""true""
class=""org.scalastyle.scalariform.DeprecatedJavaChecker"" level=""warning""/>


*EmptyClassChecker*

<check enabled=""true"" class=""org.scalastyle.scalariform.EmptyClassChecker""
level=""warning""/>


*ForBraceChecker*

<check enabled=""true"" class=""org.scalastyle.scalariform.ForBraceChecker""
level=""warning""/>


*LowercasePatternMatchChecker*

<check enabled=""true""
class=""org.scalastyle.scalariform.LowercasePatternMatchChecker""
level=""warning""/>


*MultipleStringLiteralsChecker*

<check enabled=""true""
class=""org.scalastyle.scalariform.MultipleStringLiteralsChecker""
level=""warning"">

 <parameters>

  <parameter name=""allowed"">1</parameter>

  <parameter name=""ignoreRegex"">^\&quot;\&quot;$</parameter>

 </parameters>

</check>


*PatternMatchAlignChecker*

<check enabled=""true""
class=""org.scalastyle.scalariform.PatternMatchAlignChecker""
level=""warning""/>


*ProcedureDeclarationChecker*

<check enabled=""true""
class=""org.scalastyle.scalariform.ProcedureDeclarationChecker""
level=""warning""/>


*RedundantIfChecker*

<check enabled=""true"" class=""org.scalastyle.scalariform.RedundantIfChecker""
level=""warning""/>


*ScalaDocChecker*

<check enabled=""true"" class=""org.scalastyle.scalariform.ScalaDocChecker""
level=""warning"">

 <parameters>

  <parameter name=""ignoreRegex"">(.*Spec$)|(.*SpecIT$)</parameter>

 </parameters>

</check>


*TodoCommentChecker*

<checker enabled=""true""
class=""org.scalastyle.scalariform.TodoCommentChecker"" level=""warning"">

 <parameters>

  <parameter default=""TODO|FIXME"" type=""string"" name=""words""/>

 </parameters>

</checker>


*VarFieldChecker*

<check enabled=""true"" class=""org.scalastyle.scalariform.VarFieldChecker""
level=""warning""/>


*VarLocalChecker*

<check enabled=""true"" class=""org.scalastyle.scalariform.VarLocalChecker""
level=""warning""/>


*WhileChecker*

<check enabled=""true"" class=""org.scalastyle.scalariform.WhileChecker""
level=""warning""/>


*XmlLiteralChecker*

<check enabled=""true"" class=""org.scalastyle.scalariform.XmlLiteralChecker""
level=""warning""/>



Thank you very much!!
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 16 May 2016 03:05:04 +0000",Re: Question about enabling some of missing rules.,"Hyukjin Kwon <gurwls223@gmail.com>, dev <dev@spark.apache.org>","Relevant discussion from some time ago:
https://issues.apache.org/jira/browse/SPARK-3849?focusedCommentId=14168961&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14168961

In short, if enabling a new style rule requires sweeping changes throughout
the code base, then it should not be enabled.

We've talked in the past about developing some way of enforcing new style
rules only on changed lines in a PR, allowing the project's style to
gradually improve over time without a sudden, sweeping change that breaks
everybody's workflow. So far nobody's been able to put such a system
together, as far as I know.

Nick


dantIfChecker
dureDeclarationChecker
SparkCodeStyleGuide-ReturnTypes
.
/>
ng""/>
cker""
er""
/>
er""
er""
er""
cker""
"
Hyukjin Kwon <gurwls223@gmail.com>,"Mon, 16 May 2016 12:16:56 +0900",Re: Question about enabling some of missing rules.,Nicholas Chammas <nicholas.chammas@gmail.com>,"Thank you so much for detailed explanation and the history.


I understood and it seems *ProcedureDeclarationChecker* should not be
enabled.


However, it seems *RedundantIfChecker* okay because there are only two
errors for this across the code base.


I have seen some rules have been added time to time, which do not change
super a lot through code base.


For example,

https://github.com/apache/spark/commit/b0f5497e9520575e5082fa8ce8be5569f43abe74
https://github.com/apache/spark/commit/d717ae1fd74d125a9df21350a70e7c2b2d2b4786
https://github.com/apache/spark/commit/947b9020b0d621bc97661a0a056297e6889936d3


Thanks!

2016-05-16 12:05 GMT+09:00 Nicholas Chammas <nicholas.chammas@gmail.com>:

961&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14168961
ndantIfChecker
edureDeclarationChecker
#SparkCodeStyleGuide-ReturnTypes
s.
""/>
ing""/>
/>
ker""
""/>
ker""
"">
ker""
ker""
""
/>
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 16 May 2016 03:56:29 +0000",Re: Question about enabling some of missing rules.,Hyukjin Kwon <gurwls223@gmail.com>,"Ah I see, good references. Perhaps it's really then a committer judgement
call on how many changes become ""too many"" for a single PR.
2016ë…„ 5ì›” 15ì¼ (ì¼) ì˜¤í›„ 11:16, Hyukjin Kwon <gurwls223@gmail.com>ë‹˜ì´ ìž‘ì„±:

3abe74
2b4786
9936d3
8961&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14168961
e
s
:
undantIfChecker
cedureDeclarationChecker
`
e#SparkCodeStyleGuide-ReturnTypes
ts.
""
g""/>
ning""/>
""/>
cker""
g""/>
cker""
g"">
cker""
cker""
r""
""/>
"
Yin Huai <yhuai@databricks.com>,"Tue, 17 May 2016 11:57:13 -0700",Re: HiveContext.refreshTable() missing in spark 2.0,=?UTF-8?B?5rGq5rSL?= <tiandiwoxin@icloud.com>,"Hi Yang,

I think it's deleted accidentally while we were working on the API
migration. We will add it back (
https://issues.apache.org/jira/browse/SPARK-15367).

Thanks,

Yin


"
dhruve ashar <dhruveashar@gmail.com>,"Tue, 17 May 2016 13:58:30 -0500",SBT doesn't pick resource file after clean,dev@spark.apache.org,"We are trying to pick the spark version automatically from pom instead of
manually modifying the files. This also includes richer pieces of
information like last commit, version, user who built the code etc to
better identify the framework running.

The setup is as follows :
- A shell script generates this piece of information and dumps it into a
properties file under *core/target/extra-resources - *we don't want to
pollute the source directory and hence we are generating this under target
as its dealing with build information.
- The shell script is invoked in both mvn and sbt.

The issue is that sbt doesn't pick up the generated properties file after
doing a clean. But it does pick it up in subsequent runs. Note, the
properties file is created before the classes are generated.

The code for this is available in the PR :
https://github.com/apache/spark/pull/13061

Does anybody have an idea about how we can achieve this in sbt?

Thanks,
Dhruve
"
Marcelo Vanzin <vanzin@cloudera.com>,"Tue, 17 May 2016 12:00:32 -0700",Re: SBT doesn't pick resource file after clean,dhruve ashar <dhruveashar@gmail.com>,"Perhaps you need to make the ""compile"" task of the appropriate module
depend on the task that generates the resource file?

Sorry but my knowledge of sbt doesn't really go too far.




-- 
Marcelo

---------------------------------------------------------------------


"
Michael Segel <msegel_hadoop@hotmail.com>,"Tue, 17 May 2016 12:48:10 -0700",Indexing of RDDs and DF in 2.0? ,dev@spark.apache.org,"Hi, 

I saw a replay of a talk about whatâ€™s coming in Spark 2.0 and the speed performancesâ€¦ 

I am curious about indexing of data sets. 
In HBase/MapRDB you can create ordered sets of indexes through an inverted table. 
Here, you can take the intersection of the indexes to find the result set of rows.  
(Or intersect/null if you have left outer joinsâ€¦) 

AFAIK, there was a project on an indexedRDD, but not sure how far that had gone? 

I realize that some of the improvements are based on using hashed joins, which would make indexing a bit harderâ€¦ or am I missing something? 

Thx



---------------------------------------------------------------------


"
Koert Kuipers <koert@tresata.com>,"Tue, 17 May 2016 18:29:52 -0400",CompileException for spark-sql generated code in 2.0.0-SNAPSHOT,"""dev@spark.apache.org"" <dev@spark.apache.org>","hello all, we are slowly expanding our test coverage for spark
2.0.0-SNAPSHOT to more in-house projects. today i ran into this issue...

this runs fine:
val df = sc.parallelize(List((""1"", ""2""), (""3"", ""4""))).toDF(""a"", ""b"")
df
  .map(row => row)(RowEncoder(df.schema))
  .select(""a"", ""b"")
  .show

however this fails:
val df = sc.parallelize(List((""1"", ""2""), (""3"", ""4""))).toDF(""a"", ""b"")
df
  .map(row => row)(RowEncoder(df.schema))
  .select(""b"", ""a"")
  .show

the error is:
java.lang.Exception: failed to compile:
org.codehaus.commons.compiler.CompileException: File 'generated.java', Line
94, Column 57: Expression ""mapelements_isNull"" is not an rvalue
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIterator(references);
/* 003 */ }
/* 004 */
/* 005 */ /** Codegened pipeline for:
/* 006 */ * Project [b#11,a#10]
/* 007 */ +- SerializeFromObject [if (input[0,
org.apache.spark.sql.Row].isNullAt) null else staticinvoke(class org.ap...
/* 008 */   */
/* 009 */ final class GeneratedIterator extends
org.apache.spark.sql.execution.BufferedRowIterator {
/* 010 */   private Object[] references;
/* 011 */   private scala.collection.Iterator inputadapter_input;
/* 012 */   private UnsafeRow project_result;
/* 013 */   private
org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder
project_holder;
/* 014 */   private
org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter
project_rowWriter;
/* 015 */   private Object[] deserializetoobject_values;
/* 016 */   private org.apache.spark.sql.types.StructType
deserializetoobject_schema;
/* 017 */   private UnsafeRow deserializetoobject_result;
/* 018 */   private
org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder
deserializetoobject_holder;
/* 019 */   private
org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter
deserializetoobject_rowWriter;
/* 020 */   private UnsafeRow mapelements_result;
/* 021 */   private
org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder
mapelements_holder;
/* 022 */   private
org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter
mapelements_rowWriter;
/* 023 */   private UnsafeRow serializefromobject_result;
/* 024 */   private
org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder
serializefromobject_holder;
/* 025 */   private
org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter
serializefromobject_rowWriter;
/* 026 */   private UnsafeRow project_result1;
/* 027 */   private
org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder
project_holder1;
/* 028 */   private
org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter
project_rowWriter1;
/* 029 */
/* 030 */   public GeneratedIterator(Object[] references) {
/* 031 */     this.references = references;
/* 032 */   }
/* 033 */
/* 034 */   public void init(int index, scala.collection.Iterator inputs[])
{
/* 035 */     partitionIndex = index;
/* 036 */     inputadapter_input = inputs[0];
/* 037 */     project_result = new UnsafeRow(2);
/* 038 */     this.project_holder = new
org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(project_result,
64);
/* 039 */     this.project_rowWriter = new
org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(project_holder,
2);
/* 040 */
/* 041 */     this.deserializetoobject_schema =
(org.apache.spark.sql.types.StructType) references[0];
/* 042 */     deserializetoobject_result = new UnsafeRow(1);
/* 043 */     this.deserializetoobject_holder = new
org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(deserializetoobject_result,
32);
/* 044 */     this.deserializetoobject_rowWriter = new
org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(deserializetoobject_holder,
1);
/* 045 */     mapelements_result = new UnsafeRow(1);
/* 046 */     this.mapelements_holder = new
org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(mapelements_result,
32);
/* 047 */     this.mapelements_rowWriter = new
org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(mapelements_holder,
1);
/* 048 */     serializefromobject_result = new UnsafeRow(2);
/* 049 */     this.serializefromobject_holder = new
org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(serializefromobject_result,
64);
/* 050 */     this.serializefromobject_rowWriter = new
org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(serializefromobject_holder,
2);
/* 051 */     project_result1 = new UnsafeRow(2);
/* 052 */     this.project_holder1 = new
org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(project_result1,
64);
/* 053 */     this.project_rowWriter1 = new
org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(project_holder1,
2);
/* 054 */   }
/* 055 */
/* 056 */   protected void processNext() throws java.io.IOException {
/* 057 */     /*** PRODUCE: Project [b#11,a#10] */
/* 058 */
/* 059 */     /*** PRODUCE: SerializeFromObject [if (input[0,
org.apache.spark.sql.Row].isNullAt) null else staticinvoke(class
org.apache.spark.unsafe.types... */
/* 060 */
/* 061 */     /*** PRODUCE: MapElements <function1>, obj#9:
org.apache.spark.sql.Row */
/* 062 */
/* 063 */     /*** PRODUCE: DeserializeToObject createexternalrow(if
(isnull(a#5)) null else a#5.toString, if (isnull(b#6)) null else
b#6.toString, StructFi... */
/* 064 */
/* 065 */     /*** PRODUCE: Project [_1#2 AS a#5,_2#3 AS b#6] */
/* 066 */
/* 067 */     /*** PRODUCE: INPUT */
/* 068 */
/* 069 */     while (inputadapter_input.hasNext()) {
/* 070 */       InternalRow inputadapter_row = (InternalRow)
inputadapter_input.next();
/* 071 */       /*** CONSUME: Project [_1#2 AS a#5,_2#3 AS b#6] */
/* 072 */
/* 073 */       /*** CONSUME: DeserializeToObject createexternalrow(if
(isnull(a#5)) null else a#5.toString, if (isnull(b#6)) null else
b#6.toString, StructFi... */
/* 074 */       /* input[0, string] */
/* 075 */       /* input[0, string] */
/* 076 */       boolean inputadapter_isNull = inputadapter_row.isNullAt(0);
/* 077 */       UTF8String inputadapter_value = inputadapter_isNull ? null
: (inputadapter_row.getUTF8String(0));
/* 078 */       /* input[1, string] */
/* 079 */       /* input[1, string] */
/* 080 */       boolean inputadapter_isNull1 = inputadapter_row.isNullAt(1);
/* 081 */       UTF8String inputadapter_value1 = inputadapter_isNull1 ?
null : (inputadapter_row.getUTF8String(1));
/* 082 */
/* 083 */       /*** CONSUME: MapElements <function1>, obj#9:
org.apache.spark.sql.Row */
/* 084 */
/* 085 */       /*** CONSUME: SerializeFromObject [if (input[0,
org.apache.spark.sql.Row].isNullAt) null else staticinvoke(class
org.apache.spark.unsafe.types... */
/* 086 */
/* 087 */       /*** CONSUME: Project [b#11,a#10] */
/* 088 */
/* 089 */       /*** CONSUME: WholeStageCodegen */
/* 090 */
/* 091 */       /* input[1, string] */
/* 092 */       /* if (input[0, org.apache.spark.sql.Row].isNullAt) null
else staticinvoke(class org.apache.spark.unsafe.types.UTF8String,
StringTy... */
/* 093 */       /* input[0, org.apache.spark.sql.Row].isNullAt */
/* 094 */       boolean serializefromobject_isNull9 = mapelements_isNull ||
false;
/* 095 */       final boolean serializefromobject_value9 =
serializefromobject_isNull9 ? false : mapelements_value.isNullAt(1);
/* 096 */       boolean serializefromobject_isNull8 = false;
/* 097 */       UTF8String serializefromobject_value8 = null;
/* 098 */       if (!serializefromobject_isNull9 &&
serializefromobject_value9) {
/* 099 */         /* null */
/* 100 */         final UTF8String serializefromobject_value12 = null;
/* 101 */         serializefromobject_isNull8 = true;
/* 102 */         serializefromobject_value8 = serializefromobject_value12;
/* 103 */       } else {
/* 104 */         /* staticinvoke(class
org.apache.spark.unsafe.types.UTF8String, StringType, fromString,
getexternalrowfield(input[0, org.apache.spa... */
/* 105 */         /* getexternalrowfield(input[0,
org.apache.spark.sql.Row], 1, ObjectType(class java.lang.String)) */
/* 106 */         if (mapelements_isNull) {
/* 107 */           throw new RuntimeException(""The input external row
cannot be null."");
/* 108 */         }
/* 109 */
/* 110 */         if (mapelements_value.isNullAt(1)) {
/* 111 */           throw new RuntimeException(""The 1th field of input row
cannot be null."");
/* 112 */         }
/* 113 */
/* 114 */         final java.lang.String serializefromobject_value14 =
(java.lang.String) mapelements_value.get(1);
/* 115 */         boolean serializefromobject_isNull13 = false;
/* 116 */         final UTF8String serializefromobject_value13 =
serializefromobject_isNull13 ? null :
org.apache.spark.unsafe.types.UTF8String.fromString(serializefromobject_value14);
/* 117 */         serializefromobject_isNull13 =
serializefromobject_value13 == null;
/* 118 */         serializefromobject_isNull8 =
serializefromobject_isNull13;
/* 119 */         serializefromobject_value8 = serializefromobject_value13;
/* 120 */       }
/* 121 */       /* input[0, string] */
/* 122 */       /* if (input[0, org.apache.spark.sql.Row].isNullAt) null
else staticinvoke(class org.apache.spark.unsafe.types.UTF8String,
StringTy... */
/* 123 */       /* input[0, org.apache.spark.sql.Row].isNullAt */
/* 124 */       /* input[0, org.apache.spark.sql.Row] */
/* 125 */       /* <function1>.apply */
/* 126 */       /* <function1> */
/* 127 */       /* expression: <function1> */
/* 128 */       Object mapelements_obj = ((Expression)
references[1]).eval(null);
/* 129 */       scala.Function1 mapelements_value1 = (scala.Function1)
mapelements_obj;
/* 130 */       /* input[0, org.apache.spark.sql.Row] */
/* 131 */       /* createexternalrow(if (isnull(input[0, string])) null
else input[0, string].toString, if (isnull(input[1, string])) null else
inp... */
/* 132 */       deserializetoobject_values = new Object[2];
/* 133 */       /* if (isnull(input[0, string])) null else input[0,
string].toString */
/* 134 */       boolean deserializetoobject_isNull1 = false;
/* 135 */       java.lang.String deserializetoobject_value1 = null;
/* 136 */       if (!false && inputadapter_isNull) {
/* 137 */         /* null */
/* 138 */         final java.lang.String deserializetoobject_value4 = null;
/* 139 */         deserializetoobject_isNull1 = true;
/* 140 */         deserializetoobject_value1 = deserializetoobject_value4;
/* 141 */       } else {
/* 142 */         /* input[0, string].toString */
/* 143 */         boolean deserializetoobject_isNull5 = inputadapter_isNull;
/* 144 */         final java.lang.String deserializetoobject_value5 =
deserializetoobject_isNull5 ? null : (java.lang.String)
inputadapter_value.toString();
/* 145 */         deserializetoobject_isNull5 = deserializetoobject_value5
== null;
/* 146 */         deserializetoobject_isNull1 = deserializetoobject_isNull5;
/* 147 */         deserializetoobject_value1 = deserializetoobject_value5;
/* 148 */       }
/* 149 */       if (deserializetoobject_isNull1) {
/* 150 */         deserializetoobject_values[0] = null;
/* 151 */       } else {
/* 152 */         deserializetoobject_values[0] =
deserializetoobject_value1;
/* 153 */       }
/* 154 */       /* if (isnull(input[1, string])) null else input[1,
string].toString */
/* 155 */       boolean deserializetoobject_isNull7 = false;
/* 156 */       java.lang.String deserializetoobject_value7 = null;
/* 157 */       if (!false && inputadapter_isNull1) {
/* 158 */         /* null */
/* 159 */         final java.lang.String deserializetoobject_value10 = null;
/* 160 */         deserializetoobject_isNull7 = true;
/* 161 */         deserializetoobject_value7 = deserializetoobject_value10;
/* 162 */       } else {
/* 163 */         /* input[1, string].toString */
/* 164 */         boolean deserializetoobject_isNull11 =
inputadapter_isNull1;
/* 165 */         final java.lang.String deserializetoobject_value11 =
deserializetoobject_isNull11 ? null : (java.lang.String)
inputadapter_value1.toString();
/* 166 */         deserializetoobject_isNull11 =
deserializetoobject_value11 == null;
/* 167 */         deserializetoobject_isNull7 =
deserializetoobject_isNull11;
/* 168 */         deserializetoobject_value7 = deserializetoobject_value11;
/* 169 */       }
/* 170 */       if (deserializetoobject_isNull7) {
/* 171 */         deserializetoobject_values[1] = null;
/* 172 */       } else {
/* 173 */         deserializetoobject_values[1] =
deserializetoobject_value7;
/* 174 */       }
/* 175 */
/* 176 */       final org.apache.spark.sql.Row deserializetoobject_value =
new
org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(deserializetoobject_values,
this.deserializetoobject_schema);
/* 177 */       boolean mapelements_isNull = false || false;
/* 178 */       final org.apache.spark.sql.Row mapelements_value =
mapelements_isNull ? null : (org.apache.spark.sql.Row)
mapelements_value1.apply(deserializetoobject_value);
/* 179 */       mapelements_isNull = mapelements_value == null;
/* 180 */
/* 181 */       boolean serializefromobject_isNull1 = mapelements_isNull ||
false;
/* 182 */       final boolean serializefromobject_value1 =
serializefromobject_isNull1 ? false : mapelements_value.isNullAt(0);
/* 183 */       boolean serializefromobject_isNull = false;
/* 184 */       UTF8String serializefromobject_value = null;
/* 185 */       if (!serializefromobject_isNull1 &&
serializefromobject_value1) {
/* 186 */         /* null */
/* 187 */         final UTF8String serializefromobject_value4 = null;
/* 188 */         serializefromobject_isNull = true;
/* 189 */         serializefromobject_value = serializefromobject_value4;
/* 190 */       } else {
/* 191 */         /* staticinvoke(class
org.apache.spark.unsafe.types.UTF8String, StringType, fromString,
getexternalrowfield(input[0, org.apache.spa... */
/* 192 */         /* getexternalrowfield(input[0,
org.apache.spark.sql.Row], 0, ObjectType(class java.lang.String)) */
/* 193 */         if (mapelements_isNull) {
/* 194 */           throw new RuntimeException(""The input external row
cannot be null."");
/* 195 */         }
/* 196 */
/* 197 */         if (mapelements_value.isNullAt(0)) {
/* 198 */           throw new RuntimeException(""The 0th field of input row
cannot be null."");
/* 199 */         }
/* 200 */
/* 201 */         final java.lang.String serializefromobject_value6 =
(java.lang.String) mapelements_value.get(0);
/* 202 */         boolean serializefromobject_isNull5 = false;
/* 203 */         final UTF8String serializefromobject_value5 =
serializefromobject_isNull5 ? null :
org.apache.spark.unsafe.types.UTF8String.fromString(serializefromobject_value6);
/* 204 */         serializefromobject_isNull5 = serializefromobject_value5
== null;
/* 205 */         serializefromobject_isNull = serializefromobject_isNull5;
/* 206 */         serializefromobject_value = serializefromobject_value5;
/* 207 */       }
/* 208 */       project_holder1.reset();
/* 209 */
/* 210 */       project_rowWriter1.zeroOutNullBytes();
/* 211 */
/* 212 */       if (serializefromobject_isNull8) {
/* 213 */         project_rowWriter1.setNullAt(0);
/* 214 */       } else {
/* 215 */         project_rowWriter1.write(0, serializefromobject_value8);
/* 216 */       }
/* 217 */
/* 218 */       if (serializefromobject_isNull) {
/* 219 */         project_rowWriter1.setNullAt(1);
/* 220 */       } else {
/* 221 */         project_rowWriter1.write(1, serializefromobject_value);
/* 222 */       }
/* 223 */       project_result1.setTotalSize(project_holder1.totalSize());
/* 224 */       append(project_result1);
/* 225 */       if (shouldStop()) return;
/* 226 */     }
/* 227 */   }
/* 228 */ }
"
Reynold Xin <rxin@databricks.com>,"Tue, 17 May 2016 16:55:32 -0700",Re: CompileException for spark-sql generated code in 2.0.0-SNAPSHOT,Koert Kuipers <koert@tresata.com>,"It seems like the problem here is that we are not using unique names
for mapelements_isNull?




"
Michael Armbrust <michael@databricks.com>,"Tue, 17 May 2016 18:06:42 -0700",Re: CompileException for spark-sql generated code in 2.0.0-SNAPSHOT,Reynold Xin <rxin@databricks.com>,"Yeah, can you open a JIRA with that reproduction please?  You can ping me
on it.


"
Hyukjin Kwon <gurwls223@gmail.com>,"Wed, 18 May 2016 14:14:17 +0900",Re: Question about enabling some of missing rules.,Nicholas Chammas <nicholas.chammas@gmail.com>,"Considering the references It seems *RedundantIfChecker *is okay then.
Could I try to add this one?


*RedundantIfChecker *(See
http://www.scalastyle.org/rules-dev.html#org_scalastyle_scalariform_RedundantIfChecker
)
It seems there are two usage of this. This simply checks if (cond) true
else false or if (cond) false else true,which can be just cond or !cond



Thanks.


2016-05-16 12:56 GMT+09:00 Nicholas Chammas <nicholas.chammas@gmail.com>:

, Hyukjin Kwon <gurwls223@gmail.com>ë‹˜ì´ ìž‘ì„±:
43abe74
d2b4786
89936d3
:
68961&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14168961
 to
ks
.
dundantIfChecker
ocedureDeclarationChecker
t`
de#SparkCodeStyleGuide-ReturnTypes
cts.
r""
ng""/>
rning""/>
g""/>
/>
ng""/>
ng"">
/>
/>
er""
g""/>
"
Reynold Xin <rxin@apache.org>,"Tue, 17 May 2016 22:40:51 -0700",[vote] Apache Spark 2.0.0-preview release (rc1),"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

In the past the Apache Spark community have created preview packages (not
official releases) and used those as opportunities to ask community members
to test the upcoming versions of Apache Spark. Several people in the Apache
community have suggested we conduct votes for these preview packages and
turn them into formal releases by the Apache foundation's standard. Preview
releases are not meant to be functional, i.e. they can and highly likely
will contain critical bugs or documentation errors, but we will be able to
post them to the project's website to get wider feedback. They should
satisfy the legal requirements of Apache's release policy (
http://www.apache.org/dev/release.html) such as having proper licenses.


Please vote on releasing the following candidate as Apache Spark version
2.0.0-preview. The vote is open until Friday, May 20, 2015 at 11:00 PM PDT
and passes if a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 2.0.0-preview
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see http://spark.apache.org/

The tag to be voted on is 2.0.0-preview
(8f5a04b6299e3a47aca13cbb40e72344c0114860)

The release files, including signatures, digests, etc. can be found at:
http://home.apache.org/~pwendell/spark-releases/spark-2.0.0-preview-bin/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The documentation corresponding to this release can be found at:
http://home.apache.org/~pwendell/spark-releases/spark-2.0.0-preview-docs/

The list of resolved issues are:
https://issues.apache.org/jira/browse/SPARK-15351?jql=project%20%3D%20SPARK%20AND%20fixVersion%20%3D%202.0.0


If you are a Spark user, you can help us test this release by taking an
existing Apache Spark workload and running on this candidate, then
reporting any regressions.
"
JaeSung Jun <jaesjun@gmail.com>,"Wed, 18 May 2016 23:12:50 +1000",Query parsing error for the join query between different database,dev@spark.apache.org,"Hi,

I'm working on custom data source provider, and i'm using fully qualified
table name in FROM clause like following :

SELECT user. uid, dept.name
FROM userdb.user user, deptdb.dept
WHERE user.dept_id = dept.id

and i've got the following error :

MismatchedTokenException(279!=26)
at
org.antlr.runtime.BaseRecognizer.recoverFromMismatchedToken(BaseRecognizer.java:617)
at org.antlr.runtime.BaseRecognizer.match(BaseRecognizer.java:115)
at
org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.tableSource(HiveParser_FromClauseParser.java:4608)
at
org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.fromSource(HiveParser_FromClauseParser.java:3729)
at
org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.joinSource(HiveParser_FromClauseParser.java:1873)
at
org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.fromClause(HiveParser_FromClauseParser.java:1518)
at
org.apache.hadoop.hive.ql.parse.HiveParser.fromClause(HiveParser.java:45861)
at
org.apache.hadoop.hive.ql.parse.HiveParser.selectStatement(HiveParser.java:41516)
at
org.apache.hadoop.hive.ql.parse.HiveParser.regularBody(HiveParser.java:41402)
at
org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpressionBody(HiveParser.java:40413)
at
org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpression(HiveParser.java:40283)
at
org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1590)
at
org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1109)
at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:202)
at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
at org.apache.spark.sql.hive.HiveQl$.getAst(HiveQl.scala:276)
at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:303)
at
org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
at
org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
at
scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
at
scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
at
scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
at
scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
at
scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
at
scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
at
scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
at
scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
at
scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
at
org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:295)

Any idea?

Thanks
Jason
"
Ted Yu <yuzhihong@gmail.com>,"Wed, 18 May 2016 06:31:03 -0700",Re: Query parsing error for the join query between different database,JaeSung Jun <jaesjun@gmail.com>,"Which release of Spark / Hive are you using ?

Cheers

able name in FROM clause like following :
Recognizer.java:617)
eSource(HiveParser_FromClauseParser.java:4608)
Source(HiveParser_FromClauseParser.java:3729)
Source(HiveParser_FromClauseParser.java:1873)
Clause(HiveParser_FromClauseParser.java:1518)
.java:45861)
arser.java:41516)
r.java:41402)
ionBody(HiveParser.java:40413)
ion(HiveParser.java:40283)
ser.java:1590)
java:1109)
va:202)
va:166)
.apply(ExtendedHiveQlParser.scala:41)
.apply(ExtendedHiveQlParser.scala:40)
136)
135)
y(Parsers.scala:242)
y(Parsers.scala:242)
a:222)
anonfun$apply$2.apply(Parsers.scala:254)
anonfun$apply$2.apply(Parsers.scala:254)
la:202)
pply(Parsers.scala:254)
pply(Parsers.scala:254)
a:222)
apply(Parsers.scala:891)
apply(Parsers.scala:891)
a:890)
atParsers.scala:110)
actSparkSQLParser.scala:34)
"
Nick White <nwhite@palantir.com>,"Wed, 18 May 2016 14:03:43 +0000",PR for In-App Scheduling,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi ­ I raised a PR here: https://github.com/apache/spark/pull/12951 add a
mechanism that prevents starvation when scheduling work within a single
application. Could a committer take a look? Thanks -

Nick


"
Koert Kuipers <koert@tresata.com>,"Wed, 18 May 2016 10:10:50 -0400",Re: CompileException for spark-sql generated code in 2.0.0-SNAPSHOT,Michael Armbrust <michael@databricks.com>,"https://issues.apache.org/jira/browse/SPARK-15384


"
Sean Owen <sowen@cloudera.com>,"Wed, 18 May 2016 15:28:58 +0100",Re: [vote] Apache Spark 2.0.0-preview release (rc1),Reynold Xin <rxin@apache.org>,"I think it's a good idea. Although releases have been preceded before
by release candidates for developers, it would be good to get a formal
preview/beta release ratified for public consumption ahead of a new
major release. Better to have a little more testing in the wild to
identify problems before 2.0.0 is finalized.

8, compilation and tests succeed for ""-Pyarn -Phive
-Phive-thriftserver -Phadoop-2.6"".


---------------------------------------------------------------------


"
Ovidiu-Cristian MARCU <ovidiu-cristian.marcu@inria.fr>,"Wed, 18 May 2016 18:00:15 +0200",Re: [vote] Apache Spark 2.0.0-preview release (rc1),Sean Owen <sowen@cloudera.com>,"+1 Great, I see the list of resolved issues, do you have a list of known issue you plan to stay with this release?

with
build/mvn -Pyarn -Phadoop-2.6 -Dhadoop.version=2.7.1 -Phive -Phive-thriftserver -DskipTests clean package

mvn -version
Apache Maven 3"
Reynold Xin <rxin@databricks.com>,"Wed, 18 May 2016 09:19:06 -0700",Re: [vote] Apache Spark 2.0.0-preview release (rc1),Ovidiu-Cristian MARCU <ovidiu-cristian.marcu@inria.fr>,"You can find that by changing the filter to target version = 2.0.0. Cheers.


œmac""
0
5
rs
he
ew
o
T
PARK%20AND%20fixVersion%20%3D%202.0.0
"
Reynold Xin <rxin@databricks.com>,"Wed, 18 May 2016 09:55:23 -0700",Re: [vote] Apache Spark 2.0.0-preview release (rc1),Michael F Ringenburg <miker@uw.edu>,"Michael,

You can comment on the JIRA ticket and tag some of the more active
contributors to Mesos/SparkR. That said, committers are focusing on bug
fixes and stability fixes at the moment for 2.0, and it's unlikely at this
point for new features to get in. It can of course target the next release.





"
Krishna Sankar <ksankar42@gmail.com>,"Wed, 18 May 2016 10:06:08 -0700",Re: [vote] Apache Spark 2.0.0-preview release (rc1),Sean Owen <sowen@cloudera.com>,"+1. Looks Good.
The mllib results are in line with 1.6.1. Deprecation messages. I will
convert to ml and test later in the day.
Also will try GraphX exercises for our Strata London Tutorial

Quick Notes:

   1. pyspark env variables need to be changed
   "
Michael Armbrust <michael@databricks.com>,"Wed, 18 May 2016 10:28:22 -0700",Re: [vote] Apache Spark 2.0.0-preview release (rc1),Krishna Sankar <ksankar42@gmail.com>,"+1, excited for 2.0!


"
Ovidiu-Cristian MARCU <ovidiu-cristian.marcu@inria.fr>,"Wed, 18 May 2016 19:43:38 +0200",Re: [vote] Apache Spark 2.0.0-preview release (rc1),Reynold Xin <rxin@databricks.com>,"Yes, I can filter..
Did that and for example:
https://issues.apache.org/jira/browse/SPARK-15370?jql=project%20%3D%20SPARK%20AND%20resolution%20%3D%20Unresolved%20AND%20affectedVersion%20%3D%202.0.0 <https://issues.apache.org/jira/browse/SPARK-15370?jql=project%20=%20SPARK%20AND%20resolution%20=%20Unresolved%20AND%20affectedVersion%20=%202.0.0>

To rephrase: for 2.0 do you have specific issues that are not a priority and will released maybe with 2.1 for example?

Keep up the good work!

Cheers.
<ovidiu-cristian.marcu@inria.fr <mailto:ovidiu-cristian.marcu@inria.fr>> known issue you plan to stay with this release?
-Phive-thriftserver -DskipTests clean package
2015-11-10T17:41:47+01:00)
/Library/Java/JavaVirtualMachines/jdk1.7.0_80.jdk/Contents/Home/jre
â€œmac""
2.635 s]
1.896 s]
2.560 s]
6.533 s]
4.176 s]
4.809 s]
6.242 s]
[01:20 min]
9.148 s]
22.760 s]
50.783 s]
[01:05 min]
4.281 s]
54.537 s]
0.747 s]
33.032 s]
3.198 s]
3.573 s]
4.617 s]
7.321 s]
16.496 s]
2.300 s]
4.219 s]
6.987 s]
1.465 s]
6.891 s]
13.465 s]
2.815 s]
------------------------------------------------------------------------
------------------------------------------------------------------------
------------------------------------------------------------------------
formal
(not
members
Apache
and
Preview
likely
able to
should
<http://www.apache.org/dev/release.html>) such as having proper licenses.
version
PM PDT
http://spark.apache.org/ <http://spark.apache.org/>
at:
http://home.apache.org/~pwendell/spark-releases/spark-2.0.0-preview-bin/ <http://home.apache.org/~pwendell/spark-releases/spark-2.0.0-preview-bin/>
<https://people.apache.org/keys/committer/pwendell.asc>
http://home.apache.org/~pwendell/spark-releases/spark-2.0.0-preview-docs/ <http://home.apache.org/~pwendell/spark-releases/spark-2.0.0-preview-docs/https://issues.apache.org/jira/browse/SPARK-15351?jql=project%20%3D%20SPARK%20AND%20fixVersion%20%3D%202.0.0 <https://issues.apache.org/jira/browse/SPARK-15351?jql=project%20%3D%20SPARK%20AND%20fixVersion%20%3D%202.0.0>
an
reporting
<mailto:dev-unsubscribe@spark.apache.org>
<mailto:dev-help@spark.apache.org>

"
Reynold Xin <rxin@databricks.com>,"Wed, 18 May 2016 10:49:13 -0700",Re: [vote] Apache Spark 2.0.0-preview release (rc1),Ovidiu-Cristian MARCU <ovidiu-cristian.marcu@inria.fr>,"Hi Ovidiu-Cristian ,

The best source of truth is change the filter with target version to 2.1.0.
Not a lot of tickets have been targeted yet, but I'd imagine as we get
closer to 2.0 release, more will be retargeted at 2.1.0.




PARK%20AND%20resolution%20%3D%20Unresolved%20AND%20affectedVersion%20%3D%202.0.0
SPARK%20AND%20resolution%20=%20Unresolved%20AND%20affectedVersion%20=%202.0.0>
ers.
œmac""
t
to
DT
/
SPARK%20AND%20fixVersion%20%3D%202.0.0
"
JaeSung Jun <jaesjun@gmail.com>,"Thu, 19 May 2016 07:51:53 +1000",Re: Query parsing error for the join query between different database,Ted Yu <yuzhihong@gmail.com>,"It's spark 1.6.1 and hive 1.2.1 (spark-sql saying ""SET
spark.sql.hive.version=1.2.1"").

Thanks


"
=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@questtec.nl>,"Thu, 19 May 2016 00:02:42 +0200",Re: Query parsing error for the join query between different database,JaeSung Jun <jaesjun@gmail.com>,"'User' is a SQL2003 keyword. This is normally not a problem, except when
you use it as a table alias (which you are doing). Change the alias or
place it between backticks and you should be fine.


2016-05-18 23:51 GMT+02:00 JaeSung Jun <jaesjun@gmail.com>:

"
JaeSung Jun <jaesjun@gmail.com>,"Thu, 19 May 2016 10:02:59 +1000",Re: Query parsing error for the join query between different database,=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@questtec.nl>,"Thanks Herman,
I didn't recognise the ""user"" is reserved word. it works now.


zer.java:617)
(HiveParser_FromClauseParser.java:4608)
HiveParser_FromClauseParser.java:3729)
HiveParser_FromClauseParser.java:1873)
HiveParser_FromClauseParser.java:1518)
5861)
ava:41516)
41402)
(HiveParser.java:40413)
eParser.java:40283)
a:1590)
09)
ExtendedHiveQlParser.scala:41)
ExtendedHiveQlParser.scala:40)
rs.scala:242)
rs.scala:242)
2)
$apply$2.apply(Parsers.scala:254)
$apply$2.apply(Parsers.scala:254)
rsers.scala:254)
rsers.scala:254)
2)
arsers.scala:891)
arsers.scala:891)
0)
rs.scala:110)
kSQLParser.scala:34)
"
Gayathri Murali <gayathri.m.softie@gmail.com>,"Wed, 18 May 2016 17:12:50 -0700",SparkR dataframe error,dev@spark.apache.org,"I am trying to run a basic example on Interactive R shell and run into the
following error. Also note that head(df) does not display any rows. Can
someone please help if I am missing something?

[image: Inline image 2]

 Thanks
Gayathri
"
Sun Rui <sunrise_win@163.com>,"Thu, 19 May 2016 08:27:52 +0800",Re: SparkR dataframe error,Gayathri Murali <gayathri.m.softie@gmail.com>,"Itâ€™s wrong behaviour that head(df) outputs no row
Could you send a screenshot displaying whole error message?
the following error. Also note that head(df) does not display any rows. Can someone please help if I am missing something?
é‚®ä»¶å¸¦æœ‰é™„ä»¶é¢„è§ˆé“¾æŽ¥ï¼Œè‹¥æ‚¨è½¬å‘æˆ–å›žå¤æ­¤é‚®ä»¶æ—¶ä¸å¸Œæœ›å¯¹æ–¹é¢„è§ˆé™„ä»¶ï¼Œå»ºè®®æ‚¨æ‰‹åŠ¨åˆ é™¤é“¾æŽ¥ã€‚
<http://preview.mail.163.com/xdownload?filename=Screen+Shot+2016-05-18+at+5.09.29+PM.png&mid=xtbB0QpumlUL%2BmgE3wAAs4&part=3&sign=de2b9113bb74f7c3ed7f83a1243fb575&time=1463616821&uid=sunrise_win%40163.com> åœ¨çº¿é¢„è§ˆ <http://preview.mail.163.com/preview?mid=xtbB0QpumlUL%2BmgE3wAAs4&part=3&sign=de2b9113bb74f7c3ed7f83a1243fb575&time=1463616821&uid=sunrise_win%40163.com>
"
Gayathri Murali <gayathri.m.softie@gmail.com>,"Wed, 18 May 2016 17:37:18 -0700",Re: SparkR dataframe error,Sun Rui <sunrise_win@163.com>,"There was a screenshot attached to my original email. If you did not get
it, attaching here again.


e
æœ‰é™„ä»¶é¢„è§ˆé“¾æŽ¥ï¼Œè‹¥æ‚¨è½¬å‘æˆ–å›žå¤æ­¤é‚®ä»¶æ—¶ä¸å¸Œæœ›å¯¹æ–¹é¢„è§ˆé™„ä»¶ï¼Œå»ºè®®æ‚¨æ‰‹åŠ¨åˆ é™¤é“¾æŽ¥ã€‚
è½½
at+5.09.29+PM.png&mid=xtbB0QpumlUL%2BmgE3wAAs4&part=3&sign=de2b9113bb74f7c3ed7f83a1243fb575&time=1463616821&uid=sunrise_win%40163.com>
=3&sign=de2b9113bb74f7c3ed7f83a1243fb575&time=1463616821&uid=sunrise_win%40163.com>

---------------------------------------------------------------------"
Sun Rui <sunrise_win@163.com>,"Thu, 19 May 2016 09:57:17 +0800",Re: SparkR dataframe error,Gayathri Murali <gayathri.m.softie@gmail.com>,"I saw it, but I canâ€™t see the complete error message on it.
I mean the part after â€œerror in invokingJava(â€¦)â€

get it, attaching here again.
<gayathri.m.softie@gmail.com <mailto:gayathri.m.softie@gmail.com>> into the following error. Also note that head(df) does not display any rows. Can someone please help if I am missing something?
é‚®ä»¶å¸¦æœ‰é™„ä»¶é¢„è§ˆé“¾æŽ¥ï¼Œè‹¥æ‚¨è½¬å‘æˆ–å›žå¤æ­¤é‚®ä»¶æ—¶ä¸å¸Œæœ›å¯¹æ–¹é¢„è§ˆé™„ä»¶ï¼Œå»ºè®®æ‚¨æ‰‹åŠ¨åˆ é™¤é“¾æŽ¥ã€‚
<http://preview.mail.163.com/xdownload?filename=Screen+Shot+2016-05-18+at+5.09.29+PM.png&mid=xtbB0QpumlUL%2BmgE3wAAs4&part=3&sign=de2b9113bb74f7c3ed7f83a1243fb575&time=1463616821&uid=sunrise_win%40163.com> åœ¨çº¿é¢„è§ˆ <http://preview.mail.163.com/preview?mid=xtbB0QpumlUL%2BmgE3wAAs4&part=3&sign=de2b9113bb74f7c3ed7f83a1243fb575&time=1463616821&uid=sunrise_win%40163.com>
é‚®ä»¶å¸¦æœ‰é™„ä»¶é¢„è§ˆé“¾æŽ¥ï¼Œè‹¥æ‚¨è½¬å‘æˆ–å›žå¤æ­¤é‚®ä»¶æ—¶ä¸å¸Œæœ›å¯¹æ–¹é¢„è§ˆé™„ä»¶ï¼Œå»ºè®®æ‚¨æ‰‹åŠ¨åˆ é™¤é“¾æŽ¥ã€‚
<http://preview.mail.163.com/xdownload?filename=Screen+Shot+2016-05-18+at+5.09.29+PM.png&mid=1tbiPhRumlXlgWp0SwAAs6&part=3&sign=e0cf0eb619175e79cfa81bad7c1d26c9&time=1463622289&uid=sunrise_win%40163.com> åœ¨çº¿é¢„è§ˆ <http://preview.mail.163.com/preview?mid=1tbiPhRumlXlgWp0SwAAs6&part=3&sign=e0cf0eb619175e79cfa81bad7c1d26c9&time=1463622289&uid=sunrise_win%40163.com><Screen Shot 2016-05-18 at 5.09.29 PM.png>

"
Jonathan Gray <jonny.gray@gmail.com>,"Thu, 19 May 2016 09:25:10 +0100",Re: Nested/Chained case statements generate codegen over 64k exception,Reynold Xin <rxin@databricks.com>,"That makes sense, I will take a look there first. That will at least give a
clearer understanding of the problem space to determine when to fallback.

"
Reynold Xin <rxin@databricks.com>,"Thu, 19 May 2016 02:43:28 -0700",Re: combitedTextFile and CombineTextInputFormat,Alexander Pivovarov <apivovarov@gmail.com>,"Users would be able to run this already with the 3 lines of code you
supplied right? In general there are a lot of methods already on
SparkContext and we lean towards the more conservative side in introducing
new API variants.

Note that this is something we are doing automatically in Spark SQL for
file sources (Dataset/DataFrame).



"
ScoRp <nik.scrp@gmail.com>,"Thu, 19 May 2016 04:09:12 -0700 (MST)",Spark Security. Generating SSL keystore for each job,dev@spark.apache.org,"Hello,

I have a question about Spark Security.

keystore for SSL encryption should be generated for each job and may be
distributed using spark.yarn.dist.files option. But there is no
implementation for this is Spark Source Code, meaning that end user needs to
make changes himself in order to generate keystore for each job
automatically.

What does community think about this? Should this feature be implemented in
Spark Source Codes or there are some objections about having it
out-of-the-box? Or is Spark going to lose support of SSL encryption and use
some other encryption protocol in near future?

Best Regards,
Nikita



--

---------------------------------------------------------------------


"
Matei Zaharia <matei.zaharia@gmail.com>,"Thu, 19 May 2016 11:34:59 -0400",[DISCUSS] Removing or changing maintainer process,dev <dev@spark.apache.org>,"Hi folks,

Around 1.5 years ago, Spark added a maintainer process for reviewing API and architectural changes (https://cwiki.apache.org/confluence/display/SPARK/Committers#Committers-ReviewProcessandMaintainers) to make sure these are seen by people who spent a lot of time on that component. At the time, the worry was that changes might go unnoticed as the project grows, but there were also concerns that this approach makes the project harder to contribute to and less welcoming. Since implementing the model, I think that a good number of developers concluded it doesn't make a huge difference, so because of these concerns, it may be useful to remove it. I've also heard that we should try to keep some other instructions for contributors to find the ""right"" reviewers, so it would be great to see suggestions on that. For my part, I'd personally prefer something ""automatic"", such as easily tracking who reviewed each patch and having people look at the commit history of the module they want to work on, instead of a list that needs to be maintained separately.

Matei
---------------------------------------------------------------------


"
Mridul Muralidharan <mridul@gmail.com>,"Thu, 19 May 2016 08:38:11 -0700",Re: [DISCUSS] Removing or changing maintainer process,Matei Zaharia <matei.zaharia@gmail.com>,"+1 (binding) on removing maintainer process.
I agree with your opinion of ""automatic "" instead of a manual list.


Regards
Mridul


"
Xiangrui Meng <mengxr@gmail.com>,"Thu, 19 May 2016 15:41:54 +0000",Re: combitedTextFile and CombineTextInputFormat,"Reynold Xin <rxin@databricks.com>, Alexander Pivovarov <apivovarov@gmail.com>","This was implemented as sc.wholeTextFiles.


"
Tom Graves <tgraves_cs@yahoo.com.INVALID>,"Thu, 19 May 2016 15:39:56 +0000 (UTC)",Re: [DISCUSS] Removing or changing maintainer process,"Matei Zaharia <matei.zaharia@gmail.com>, dev <dev@spark.apache.org>","+1 (binding)
Tom 

 

 Hi folks,

Around 1.5 years ago, Spark added a maintainer process for reviewing API and architectural changes (https://cwiki.apache.org/confluence/display/SPARK/Committers#Committers-ReviewProcessandMaintainers) to make sure these a"
Reynold Xin <rxin@databricks.com>,"Thu, 19 May 2016 08:43:14 -0700",Re: combitedTextFile and CombineTextInputFormat,Xiangrui Meng <mengxr@gmail.com>,"It is different isn't it. Whole text files returns one element per file,
whereas combined inout format is similar to coalescing partitions to bin
pack into a certain size.


"
Xiangrui Meng <mengxr@gmail.com>,"Thu, 19 May 2016 15:44:36 +0000",Re: combitedTextFile and CombineTextInputFormat,"Reynold Xin <rxin@databricks.com>, Alexander Pivovarov <apivovarov@gmail.com>","Not exacly the same as the one you suggested but you can chain it with
flatMap to get what you want, if each file is not huge.


"
Xiangrui Meng <mengxr@gmail.com>,"Thu, 19 May 2016 15:46:54 +0000",Re: SparkR dataframe error,"Sun Rui <sunrise_win@163.com>, Gayathri Murali <gayathri.m.softie@gmail.com>","Is it on 1.6.x?


Can
¦æœ‰é™„ä»¶é¢„è§ˆé“¾æŽ¥ï¼Œè‹¥æ‚¨è½¬å‘æˆ–å›žå¤æ­¤é‚®ä»¶æ—¶ä¸å¸Œæœ›å¯¹æ–¹é¢„è§ˆé™„ä»¶ï¼Œå»ºè®®æ‚¨æ‰‹åŠ¨åˆ é™¤é“¾æŽ¥ã€‚
‹è½½
+at+5.09.29+PM.png&mid=xtbB0QpumlUL%2BmgE3wAAs4&part=3&sign=de2b9113bb74f7c3ed7f83a1243fb575&time=1463616821&uid=sunrise_win%40163.com>
=3&sign=de2b9113bb74f7c3ed7f83a1243fb575&time=1463616821&uid=sunrise_win%40163.com>
æœ‰é™„ä»¶é¢„è§ˆé“¾æŽ¥ï¼Œè‹¥æ‚¨è½¬å‘æˆ–å›žå¤æ­¤é‚®ä»¶æ—¶ä¸å¸Œæœ›å¯¹æ–¹é¢„è§ˆé™„ä»¶ï¼Œå»ºè®®æ‚¨æ‰‹åŠ¨åˆ é™¤é“¾æŽ¥ã€‚
è½½
at+5.09.29+PM.png&mid=1tbiPhRumlXlgWp0SwAAs6&part=3&sign=e0cf0eb619175e79cfa81bad7c1d26c9&time=1463622289&uid=sunrise_win%40163.com>
3&sign=e0cf0eb619175e79cfa81bad7c1d26c9&time=1463622289&uid=sunrise_win%40163.com>
"
Andres Perez <andres@tresata.com>,"Thu, 19 May 2016 11:48:04 -0400",right outer joins on Datasets,dev@spark.apache.org,"Hi all, I'm getting some odd behavior when using the joinWith functionality
for Datasets. Here is a small test case:

  val left = List((""a"", 1), (""a"", 2), (""b"", 3), (""c"", 4)).toDS()
  val right = List((""a"", ""x""), (""b"", ""y""), (""d"", ""z"")).toDS()

  val joined = left.toDF(""k"", ""v"").as[(String, Int)].alias(""left"")
    .joinWith(right.toDF(""k"", ""u"").as[(String, String)].alias(""right""),
functions.col(""left.k"") === functions.col(""right.k""), ""right_outer"")
    .as[((String, Int), (String, String))]
    .map { case ((k, v), (_, u)) => (k, (v, u)) }.as[(String, (Int,
String))]

I would expect the result of this right-join to be:

  (a,(1,x))
  (a,(2,x))
  (b,(3,y))
  (d,(null,z))

but instead I'm getting:

  (a,(1,x))
  (a,(2,x))
  (b,(3,y))
  (null,(-1,z))

Not that the key for the final tuple is null instead of ""d"". (Also, is
there a reason the value for the left-side of the last tuple is -1 and not
null?)

-Andy
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 19 May 2016 15:48:00 +0000",Re: [DISCUSS] Removing or changing maintainer process,"Tom Graves <tgraves_cs@yahoo.com>, Matei Zaharia <matei.zaharia@gmail.com>, 
	dev <dev@spark.apache.org>","Iâ€™ve also heard that we should try to keep some other instructions for
contributors to find the â€œrightâ€ reviewers, so it would be great to see
suggestions on that. For my part, Iâ€™d personally prefer something
â€œautomaticâ€, such as easily tracking who reviewed each patch and having
people look at the commit history of the module they want to work on,
instead of a list that needs to be maintained separately.

Some code review and management tools like Phabricator have a system for
this <http://phacility.com/phabricator/herald/>, where you can configure
alerts to automatically ping certain people if a file matching some rule
(e.g. has this extension, is in this folder, etc.) is modified by a PR.

I think short of deploying Phabricator somehow, probably the most realistic
option for us to get automatic alerts like this is to have someone add that
as a feature to the Spark PR Dashboard <https://spark-prs.appspot.com/>.

I created an issue for this some time ago if anyone wants to take a crack
at it: https://github.com/databricks/spark-pr-dashboard/issues/47

Nick
â€‹


m>
eviewProcessandMaintainers)
g
be
d
"
Imran Rashid <irashid@cloudera.com>,"Thu, 19 May 2016 11:03:24 -0500",Re: [DISCUSS] Removing or changing maintainer process,dev <dev@spark.apache.org>,"+1 (binding) on removal of maintainers

I dont' have a strong opinion yet on how to have a system for finding the
right reviewers.  I agree it would be nice to have something to help you
find reviewers, though I'm a little skeptical of anything automatic."
Yin Huai <yhuai@databricks.com>,"Thu, 19 May 2016 09:12:29 -0700",Re: [vote] Apache Spark 2.0.0-preview release (rc1),Reynold Xin <rxin@databricks.com>,"+1


SPARK%20AND%20resolution%20%3D%20Unresolved%20AND%20affectedVersion%20%3D%202.0.0
0SPARK%20AND%20resolution%20=%20Unresolved%20AND%20affectedVersion%20=%202.0.0>
n
œmac""
-
-
-
ot
d
y
.
n
/
s/
0SPARK%20AND%20fixVersion%20%3D%202.0.0
"
Joseph Bradley <joseph@databricks.com>,"Thu, 19 May 2016 09:18:03 -0700",Re: [vote] Apache Spark 2.0.0-preview release (rc1),Reynold Xin <rxin@databricks.com>,"+1


SPARK%20AND%20resolution%20%3D%20Unresolved%20AND%20affectedVersion%20%3D%202.0.0
0SPARK%20AND%20resolution%20=%20Unresolved%20AND%20affectedVersion%20=%202.0.0>
n
œmac""
-
-
-
ot
d
y
.
n
/
s/
0SPARK%20AND%20fixVersion%20%3D%202.0.0
"
Xiangrui Meng <meng@databricks.com>,"Thu, 19 May 2016 16:20:57 +0000",Re: [vote] Apache Spark 2.0.0-preview release (rc1),"Joseph Bradley <joseph@databricks.com>, Reynold Xin <rxin@databricks.com>","+1


:
e
0SPARK%20AND%20resolution%20%3D%20Unresolved%20AND%20affectedVersion%20%3D%202.0.0
20SPARK%20AND%20resolution%20=%20Unresolved%20AND%20affectedVersion%20=%202.0.0>
y
€œmac""
--
--
--
nd
ly
e
on
:
n/
cs/
20SPARK%20AND%20fixVersion%20%3D%202.0.0
n
"
Alexander Pivovarov <apivovarov@gmail.com>,"Thu, 19 May 2016 09:59:36 -0700",Re: combitedTextFile and CombineTextInputFormat,Reynold Xin <rxin@databricks.com>,"Spark users might not know about CombineTextInputFormat. They probably
think that sc.textFile already implements the best way to read text files.

I think CombineTextInputFormat can replace regular TextInputFormat in most
of the cases.
Maybe Spark 2.0 can use CombineTextInputFormat in sc.textFile ?

"
Andres Perez <andres@tresata.com>,"Thu, 19 May 2016 14:12:31 -0400",Dataset reduceByKey,dev@spark.apache.org,"Hi all,

We were in the process of porting an RDD program to one which uses
Datasets. Most things were easy to transition, but one hole in
functionality we found was the ability to reduce a Dataset by key,
something akin to PairRDDFunctions.reduceByKey. Our first attempt of adding
the functionality ourselves involved creating a KeyValueGroupedDataset and
calling reduceGroups to get the reduced Dataset.

  class RichPairDataset[K, V: ClassTag](val ds: Dataset[(K, V)]) {
    def reduceByKey(func: (V, V) => V)(implicit e1: Encoder[K], e2:
Encoder[V], e3: Encoder[(K, V)]): Dataset[(K, V)] =
      ds.groupByKey(_._1).reduceGroups { (tup1, tup2) => (tup1._1,
func(tup1._2, tup2._2)) }.map { case (k, (_, v)) => (k, v) }
  }

Note that the functions passed into .reduceGroups takes in the key-value
pair. It'd be nicer to pass in a function that maps just the values, i.e.
reduceGroups(func). This would require the ability to modify the values of
the KeyValueGroupedDataset (which is returned by the .groupByKey call on a
Dataset). Such a function (e.g., KeyValuedGroupedDataset.mapValues(func: V
=> U)) does not currently exist.

The more important issue, however, is the inefficiency of .reduceGroups.
The function does not support partial aggregation (reducing map-side), and
as a result requires shuffling all the data in the Dataset. A more
efficient alternative that that we explored involved creating a Dataset
from the KeyValueGroupedDataset by creating an Aggregator and passing it as
a TypedColumn to KeyValueGroupedDataset's .agg function. Unfortunately, the
Aggregator necessitated the creation of a zero to create a valid monoid.
However, the zero is dependent on the reduce function. The zero for a
function such as addition on Ints would be different from the zero for
taking the minimum over Ints, for example. The Aggregator requires that we
not break the rule of reduce(a, zero) == a. To do this we had to create an
Aggregator with a buffer type that stores the value along with a null flag
(using Scala's nice Option syntax yielded some mysterious errors that I
haven't worked through yet, unfortunately), used by the zero element to
signal that it should not participate in the reduce function.

-Andy
"
Andrew Or <andrew@databricks.com>,"Thu, 19 May 2016 11:27:58 -0700",Re: [DISCUSS] Removing or changing maintainer process,dev <dev@spark.apache.org>,"+1, some maintainers are hard to find

2016-05-19 9:03 GMT-07:00 Imran Rashid <irashid@cloudera.com>:

"
Gayathri Murali <gayathri.m.softie@gmail.com>,"Thu, 19 May 2016 11:42:09 -0700",Re: SparkR dataframe error,Xiangrui Meng <mengxr@gmail.com>,"This is on Spark 2.0. I see the following on the unit-tests.log when I run
the R/run-tests.sh. This on a single MAC laptop, on the recently rebased
master. R version is 3.3.0.

16/05/19 11:28:13.863 Executor task launch worker-1 ERROR Executor:
Exception in task 0.0 in stage 5186.0 (TID 10370)
1384595 org.apache.spark.SparkException: R computation failed with
1384596
1384597 Execution halted
1384598
1384599 Execution halted
1384600
1384601 Execution halted
1384602     at org.apache.spark.api.r.RRunner.compute(RRunner.scala:107)
1384603     at org.apache.spark.api.r.BaseRRDD.compute(RRDD.scala:49)
1384604     at
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:318)
1384605     at org.apache.spark.rdd.RDD.iterator(RDD.scala:282)
1384606     at
org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
1384607     at org.apache.spark.scheduler.Task.run(Task.scala:85)
1384608     at
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
1384609     at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
1384610     at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
1384611     at java.lang.Thread.run(Thread.java:745)
1384612 16/05/19 11:28:13.864 Thread-1 INFO ContextHandler: Stopped
o.s.j.s.ServletContextHandler@22f76fa8{/jobs/json,null,UNAVAILABLE}
1384613 16/05/19 11:28:13.869 Thread-1 INFO ContextHandler: Stopped
o.s.j.s.ServletContextHandler@afe0d9f{/jobs,null,UNAVAILABLE}
1384614 16/05/19 11:28:13.869 Thread-1 INFO SparkUI: Stopped Spark web UI
at http://localhost:4040
1384615 16/05/19 11:28:13.871 Executor task launch worker-4 ERROR Executor:
Exception in task 1.0 in stage 5186.0 (TID 10371)
1384616 org.apache.spark.SparkException: R computation failed with
1384617
1384618 Execution halted
1384619
1384620 Execution halted
1384621
1384622 Execution halted
1384623     at org.apache.spark.api.r.RRunner.compute(RRunner.scala:107)
1384624     at org.apache.spark.api.r.BaseRRDD.compute(RRDD.scala:49)
1384625     at
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:318)
1384626     at org.apache.spark.rdd.RDD.iterator(RDD.scala:282)
1384627     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.
t org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
1384630     at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
1384631     at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
1384632     at java.lang.Thread.run(Thread.java:745)
1384633 16/05/19 11:28:13.874 nioEventLoopGroup-2-1 INFO DAGScheduler: Job
5183 failed: collect at null:-1, took 0.211674 s
1384634 16/05/19 11:28:13.875 nioEventLoopGroup-2-1 ERROR RBackendHandler:
collect on 26345 failed
1384635 16/05/19 11:28:13.876 Thread-1 INFO DAGScheduler: ResultStage 5186
(collect at null:-1) failed in 0.210 s
1384636 16/05/19 11:28:13.877 Thread-1 ERROR LiveListenerBus:
SparkListenerBus has already stopped! Dropping event
SparkListenerStageCompleted(org.apache.spark.scheduler.StageIn
 fo@413da307)
1384637 16/05/19 11:28:13.878 Thread-1 ERROR LiveListenerBus:
SparkListenerBus has already stopped! Dropping event
SparkListenerJobEnd(5183,1463682493877,JobFailed(org.apache.sp
 ark.SparkException: Job 5183 cancelled because SparkContext was shut down))
1384638 16/05/19 11:28:13.880 dispatcher-event-loop-1 INFO
MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
1384639 16/05/19 11:28:13.904 Thread-1 INFO MemoryStore: MemoryStore cleared
1384640 16/05/19 11:28:13.904 Thread-1 INFO BlockManager: BlockManager
stopped
1384641 16/05/19 11:28:13.904 Thread-1 INFO BlockManagerMaster:
BlockManagerMaster stopped
1384642 16/05/19 11:28:13.905 dispatcher-event-loop-0 INFO
OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:
OutputCommitCoordinator stopped!
1384643 16/05/19 11:28:13.909 Thread-1 INFO SparkContext: Successfully
stopped SparkContext
1384644 16/05/19 11:28:13.910 Thread-1 INFO ShutdownHookManager: Shutdown
hook called
1384645 16/05/19 11:28:13.911 Thread-1 INFO ShutdownHookManager: Deleting
directory
/private/var/folders/xy/qc35m0y55vq83dsqzg066_c40000gn/T/spark-dfafdddc-fd25-4eb4-bb1d-565915
       1c8231



 Can
¦æœ‰é™„ä»¶é¢„è§ˆé“¾æŽ¥ï¼Œè‹¥æ‚¨è½¬å‘æˆ–å›žå¤æ­¤é‚®ä»¶æ—¶ä¸å¸Œæœ›å¯¹æ–¹é¢„è§ˆé™„ä»¶ï¼Œå»ºè®®æ‚¨æ‰‹åŠ¨åˆ é™¤é“¾æŽ¥ã€‚
‹è½½
8+at+5.09.29+PM.png&mid=xtbB0QpumlUL%2BmgE3wAAs4&part=3&sign=de2b9113bb74f7c3ed7f83a1243fb575&time=1463616821&uid=sunrise_win%40163.com>
t=3&sign=de2b9113bb74f7c3ed7f83a1243fb575&time=1463616821&uid=sunrise_win%40163.com>
¦æœ‰é™„ä»¶é¢„è§ˆé“¾æŽ¥ï¼Œè‹¥æ‚¨è½¬å‘æˆ–å›žå¤æ­¤é‚®ä»¶æ—¶ä¸å¸Œæœ›å¯¹æ–¹é¢„è§ˆé™„ä»¶ï¼Œå»ºè®®æ‚¨æ‰‹åŠ¨åˆ é™¤é“¾æŽ¥ã€‚
‹è½½
+at+5.09.29+PM.png&mid=1tbiPhRumlXlgWp0SwAAs6&part=3&sign=e0cf0eb619175e79cfa81bad7c1d26c9&time=1463622289&uid=sunrise_win%40163.com>
=3&sign=e0cf0eb619175e79cfa81bad7c1d26c9&time=1463622289&uid=sunrise_win%40163.com>
"
=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@questtec.nl>,"Thu, 19 May 2016 21:49:48 +0200",Re: [vote] Apache Spark 2.0.0-preview release (rc1),Xiangrui Meng <meng@databricks.com>,"+1


2016-05-19 18:20 GMT+02:00 Xiangrui Meng <meng@databricks.com>:

we
20SPARK%20AND%20resolution%20%3D%20Unresolved%20AND%20affectedVersion%20%3D%202.0.0
%20SPARK%20AND%20resolution%20=%20Unresolved%20AND%20affectedVersion%20=%202.0.0>
€œmac""
---
---
-"
Xiao Li <gatorsmile@gmail.com>,"Thu, 19 May 2016 13:20:14 -0700",Re: [vote] Apache Spark 2.0.0-preview release (rc1),=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@questtec.nl>,"-1

Unable to use Hive meta-store in pyspark shell. Tried both HiveContext and
SparkSession. Both failed. It always uses in-memory catalog. Anybody else
hit the same issue?


Method 1: SparkSession





DataFrame[]

INTO TABLE src"")

Traceback (most recen"
Reynold Xin <rxin@databricks.com>,"Thu, 19 May 2016 13:26:53 -0700",Re: [vote] Apache Spark 2.0.0-preview release (rc1),Xiao Li <gatorsmile@gmail.com>,"Xiao thanks for posting. Please file a bug in JIRA. Again as I said in the
email this is not meant to be a functional release and will contain bugs.


d
py4j/java_gateway.py"",
py4j/protocol.py"",
atalog.scala:297)
alog.scala:280)
lt$lzycompute(commands.scala:57)
lt(commands.scala:55)
ands.scala:69)
an.scala:115)
an.scala:115)
arkPlan.scala:136)
:151)
)
tion.scala:85)
85)
:57)
mpl.java:43)
py4j/java_gateway.py"",
py4j/protocol.py"",
atalog.scala:297)
alog.scala:280)
lt$lzycompute(commands.scala:57)
lt(commands.scala:55)
ands.scala:69)
an.scala:115)
an.scala:115)
arkPlan.scala:136)
:151)
)
tion.scala:85)
85)
:57)
mpl.java:43)
s we
D%20SPARK%20AND%20resolution%20%3D%20Unresolved%20AND%20affectedVersion%20%3D%202.0.0
=%20SPARK%20AND%20resolution%20=%20Unresolved%20AND%20affectedVersion%20=%202.0.0>
.
-----
-----
-----
re
a
s
e
s
ld
-bin/
-docs/
3D%20SPARK%20AND%20fixVersion%20%3D%202.0.0
g
--
"
Xiao Li <gatorsmile@gmail.com>,"Thu, 19 May 2016 13:28:54 -0700",Re: [vote] Apache Spark 2.0.0-preview release (rc1),Reynold Xin <rxin@databricks.com>,"Will do. Thanks!

2016-05-19 13:26 GMT-07:00 Reynold Xin <rxin@databricks.com>:

e
,
/py4j/java_gateway.py"",
/py4j/protocol.py"",
Catalog.scala:297)
talog.scala:280)
ult$lzycompute(commands.scala:57)
ult(commands.scala:55)
mands.scala:69)
lan.scala:115)
lan.scala:115)
parkPlan.scala:136)
a:151)
3)
ution.scala:85)
:85)
a:57)
Impl.java:43)
,
,
/py4j/java_gateway.py"",
/py4j/protocol.py"",
Catalog.scala:297)
talog.scala:280)
ult$lzycompute(commands.scala:57)
ult(commands.scala:55)
mands.scala:69)
lan.scala:115)
lan.scala:115)
parkPlan.scala:136)
a:151)
3)
ution.scala:85)
:85)
a:57)
Impl.java:43)
as we
3D%20SPARK%20AND%20resolution%20%3D%20Unresolved%20AND%20affectedVersion%20%3D%202.0.0
=%20SPARK%20AND%20resolution%20=%20Unresolved%20AND%20affectedVersion%20=%202.0.0>
0.
e
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
------
------
------
w
va
y
.
0
d
w-bin/
w-docs/
%3D%20SPARK%20AND%20fixVersion%20%3D%202.0.0
---
"
vishnu prasad <vishnu667@gmail.com>,"Fri, 20 May 2016 02:24:44 +0530",Re: [vote] Apache Spark 2.0.0-preview release (rc1),=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@questtec.nl>,"+1


 we
%20SPARK%20AND%20resolution%20%3D%20Unresolved%20AND%20affectedVersion%20%3D%202.0.0
=%20SPARK%20AND%20resolution%20=%20Unresolved%20AND%20affectedVersion%20=%202.0.0>
€œmac""
[
[
[
[
[
[
----
----
----
e
al
:
d
/
bin/
docs/
D%20SPARK%20AND%20fixV"
Doug Balog <doug.sparkdev@dugos.com>,"Thu, 19 May 2016 16:56:08 -0400",Possible Hive problem with Spark 2.0.0 preview. ,dev@spark.apache.org,"I havenâ€™t had time to really look into this problem, but I want to mention it. 
I downloaded http://people.apache.org/~pwendell/spark-releases/spark-2.0.0-preview-bin/spark-2.0.0-preview-bin-hadoop2.7.tgz
and tried to run it against our Secure Hadoop cluster and access a Hive table.

1. â€œval sqlContext = new org.apache.spark.sql.hive.HiveContext(sc)â€  doesnâ€™t work because â€œHiveContext not a member of org.apache.spark.sql.hiveâ€  I checked the documentation, and it looks like it should still work for spark-2.0.0-preview-bin-hadoop2.7.tgz

2. I also tried the new spark session, â€˜spark.table(â€œdb.tableâ€)â€™, it fails with a HDFS permission denied canâ€™t write to â€œ/user/hive/warehouseâ€

Is there a new config option that I missed ? 

I tried a  SNAPSHOT version, downloaded from Patricks apacheâ€™s dir  from Apr 26th,  that worked the way I expected.
Iâ€™m going to go through the commits and see which one broke the change, but my builds are not running (no such method ConcurrentHashMap.keySet()) so I have to fix that problem first.

Thanks for any hints. 

Doug



---------------------------------------------------------------------


"
Jakob Odersky <jakob@odersky.com>,"Thu, 19 May 2016 14:17:17 -0700",Re: SBT doesn't pick resource file after clean,Marcelo Vanzin <vanzin@cloudera.com>,"To echo my comment on the PR: I think the ""sbt way"" to add extra,
generated resources to the classpath is by adding a new task to the
`resourceGenerators` setting. Also, the task should output any files
into the directory specified by the `resourceManaged` setting. See
http://www.scala-sbt.org/0.13/docs/Howto-Generating-Files.html. There
shouldn't by any issues with clean if you follow the above
conventions.


---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Thu, 19 May 2016 14:44:38 -0700",Re: Possible Hive problem with Spark 2.0.0 preview.,Doug Balog <doug.sparkdev@dugos.com>,"sc)â€
ooks like

HiveContext has been deprecated and moved to a 1.x compatibility package,
which you'll need to include explicitly.  Docs have not been updated yet.


ableâ€)â€™, it fails
warehouseâ€

Where are the HDFS configurations located?  We might not be propagating
them correctly any more.
"
Shankar Venkataraman <shankarvenkataraman666@gmail.com>,"Thu, 19 May 2016 22:16:19 +0000",Spark driver and yarn behavior,dev@spark.apache.org,"Hi!

We are running into an interesting behavior with the Spark driver. We Spark
running under Yarn. The spark driver seems to be sending work to a dead
executor for 3 hours before it recognizes it. The workload seems to have
been processed by other executors just fine and we see no loss in overall
through put. This Jira - https://issues.apache.org/jira/browse/SPARK-10586 -
seems to indicate a similar behavior.

The yarn resource manager log indicates the following:

2016-05-02 21:36:40,081 INFO  util.AbstractLivelinessMonitor
(AbstractLivelinessMonitor.java:run(127)) -
Expired:dn-a01.example.org:45454 Timed out after 600 secs
2016-05-02 21:36:40,082 INFO  rmnode.RMNodeImpl
(RMNodeImpl.java:transition(746)) - Deactivating Node
dn-a01.example.org:45454 as it is now LOST

The Executor is not reachable for 10 minutes according to this log message
but the Excutor's log shows plenty of RDD processing during that time frame.
This seems like a pretty big issue because the orphan executor seems to
cause a memory leak in the Driver and the Driver becomes non-respondent due
to heavy Full GC.

Has anyone else run into a similar situation?

Thanks for any and all feedback / suggestions.

Shankar
"
Arun Allamsetty <arun.allamsetty@gmail.com>,"Thu, 19 May 2016 16:51:10 -0600",Re: Possible Hive problem with Spark 2.0.0 preview.,Michael Armbrust <michael@databricks.com>,"Hi Doug,

If you look at the API docs here:
http://home.apache.org/~pwendell/spark-releases/spark-2.0.0-preview-docs/api/scala/index.html#org.apache.spark.sql.hive.HiveContext,
you'll see
Deprecate* (Since version 2.0.0)* Use
SparkSession.builder.enableHiveSupport instead
So you probably need to use that.

Arun


sc)â€
looks like
tableâ€)â€™, it
r/hive/warehouseâ€
"
Reynold Xin <rxin@databricks.com>,"Thu, 19 May 2016 15:52:20 -0700",Re: Possible Hive problem with Spark 2.0.0 preview.,Arun Allamsetty <arun.allamsetty@gmail.com>,"The old one is deprecated but should still work though.



api/scala/index.html#org.apache.spark.sql.hive.HiveContext,
(sc)â€
 looks like
,
.
.tableâ€)â€™, it
er/hive/warehouseâ€
"
Xiangrui Meng <mengxr@gmail.com>,"Thu, 19 May 2016 23:17:46 +0000",Re: SparkR dataframe error,Gayathri Murali <gayathri.m.softie@gmail.com>,"We no longer have `SparkRWrappers` in Spark 2.0. So if you are testing the
latest branch-2.0, there could be an issue with your SparkR installation.
Did you try `R/install-dev.sh`?


n
:1142)
a:617)
:1142)
a:617)
b
:
6
n))
d25-4eb4-bb1d-565915

t
m>
. Can
¦æœ‰é™„ä»¶é¢„è§ˆé“¾æŽ¥ï¼Œè‹¥æ‚¨è½¬å‘æˆ–å›žå¤æ­¤é‚®ä»¶æ—¶ä¸å¸Œæœ›å¯¹æ–¹é¢„è§ˆé™„ä»¶ï¼Œå»ºè®®æ‚¨æ‰‹åŠ¨åˆ é™¤é“¾æŽ¥ã€‚
‹è½½
18+at+5.09.29+PM.png&mid=xtbB0QpumlUL%2BmgE3wAAs4&part=3&sign=de2b9113bb74f7c3ed7f83a1243fb575&time=1463616821&uid=sunrise_win%40163.com>
rt=3&sign=de2b9113bb74f7c3ed7f83a1243fb575&time=1463616821&uid=sunrise_win%40163.com>
¦æœ‰é™„ä»¶é¢„è§ˆé“¾æŽ¥ï¼Œè‹¥æ‚¨è½¬å‘æˆ–å›žå¤æ­¤é‚®ä»¶æ—¶ä¸å¸Œæœ›å¯¹æ–¹é¢„è§ˆé™„ä»¶ï¼Œå»ºè®®æ‚¨æ‰‹åŠ¨åˆ é™¤é“¾æŽ¥ã€‚
‹è½½
8+at+5.09.29+PM.png&mid=1tbiPhRumlXlgWp0SwAAs6&part=3&sign=e0cf0eb619175e79cfa81bad7c1d26c9&time=1463622289&uid=sunrise_win%40163.com>
=3&sign=e0cf0eb619175e79cfa81bad7c1d26c9&time=1463622289&uid=sunrise_win%40163.com>
"
Luciano Resende <luckbr1975@gmail.com>,"Thu, 19 May 2016 16:20:49 -0700",Re: Spark driver and yarn behavior,Shankar Venkataraman <shankarvenkataraman666@gmail.com>,"
I am not sure if this is exactly the same issue, but while we were doing
heavy processing of large history of tweet data via streaming, we were
having similar issues due to the load on the executors, and we bumped some
configurations to avoid loosing some of these executors (even though there
were alive, but busy to heart beat or something)

Some of these are described at
https://github.com/SparkTC/redrock/blob/master/twitter-decahose/src/main/scala/com/decahose/ApplicationContext.scala



-- 
Luciano Resende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
Shankar Venkataraman <shankarvenkataraman666@gmail.com>,"Thu, 19 May 2016 23:34:14 +0000",Re: Spark driver and yarn behavior,Luciano Resende <luckbr1975@gmail.com>,"Thanks Luciano. The case we are seeing is different - the yarn resource
manager is shutting down the container in which the executor is running
since there does not seem to be a response and it is deeming it dead. It
started another container but the driver seems to be oblivious for nearly 2
hours. Am wondering if there is a condition where the driver is not seeing
the notification from the Yarn RM about the executor container going away.
We will try some of the settings you pointed to, and see if alleviates the
issue.

Shankar


"
Jeff Zhang <zjffdu@gmail.com>,"Fri, 20 May 2016 07:41:42 +0800",Re: [vote] Apache Spark 2.0.0-preview release (rc1),Xiao Li <gatorsmile@gmail.com>,"@Xiao,

It is tracked in SPARK-15345
<https://issues.apache.org/jira/browse/SPARK-15345>


d
py4j/java_gateway.py"",
py4j/protocol.py"",
atalog.scala:297)
alog.scala:280)
lt$lzycompute(commands.scala:57)
lt(commands.scala:55)
ands.scala:69)
an.scala:115)
an"
Gayathri Murali <gayathri.m.softie@gmail.com>,"Thu, 19 May 2016 17:09:46 -0700",Re: SparkR dataframe error,Xiangrui Meng <mengxr@gmail.com>,"That helped! Thanks. I am building from source code and I am not sure what
caused the issue with SparkR.


e
a:1142)
va:617)
I
a:1142)
va:617)
wn))
n
g
fd25-4eb4-bb1d-565915

m>
o
s. Can
¸¦æœ‰é™„ä»¶é¢„è§ˆé“¾æŽ¥ï¼Œè‹¥æ‚¨è½¬å‘æˆ–å›žå¤æ­¤é‚®ä»¶æ—¶ä¸å¸Œæœ›å¯¹æ–¹é¢„è§ˆé™„ä»¶ï¼Œå»ºè®®æ‚¨æ‰‹åŠ¨åˆ é™¤é“¾æŽ¥ã€‚
¸‹è½½
-18+at+5.09.29+PM.png&mid=xtbB0QpumlUL%2BmgE3wAAs4&part=3&sign=de2b9113bb74f7c3ed7f83a1243fb575&time=1463616821&uid=sunrise_win%40163.com>
art=3&sign=de2b9113bb74f7c3ed7f83a1243fb575&time=1463616821&uid=sunrise_win%40163.com>
¦æœ‰é™„ä»¶é¢„è§ˆé“¾æŽ¥ï¼Œè‹¥æ‚¨è½¬å‘æˆ–å›žå¤æ­¤é‚®ä»¶æ—¶ä¸å¸Œæœ›å¯¹æ–¹é¢„è§ˆé™„ä»¶ï¼Œå»ºè®®æ‚¨æ‰‹åŠ¨åˆ é™¤é“¾æŽ¥ã€‚
‹è½½
18+at+5.09.29+PM.png&mid=1tbiPhRumlXlgWp0SwAAs6&part=3&sign=e0cf0eb619175e79cfa81bad7c1d26c9&time=1463622289&uid=sunrise_win%40163.com>
=3&sign=e0cf0eb619175e79cfa81bad7c1d26c9&time=1463622289&uid=sunrise_win%40163.com>
"
Sun Rui <sunrise_win@163.com>,"Fri, 20 May 2016 09:59:09 +0800",Re: SparkR dataframe error,Gayathri Murali <gayathri.m.softie@gmail.com>,"You must specify -Psparkr when building from source.
what caused the issue with SparkR.
the latest branch-2.0, there could be an issue with your SparkR installation. Did you try `R/install-dev.sh`?
<gayathri.m.softie@gmail.com <mailto:gayathri.m.softie@gmail.com>> run the R/run-tests.sh. This on a single MAC laptop, on the recently rebased master. R version is 3.3.0.
Exception in task 0.0 in stage 5186.0 (TID 10370)
org.apache.spark.api.r.RRunner.compute(RRunner.scala:107)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:318)
org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
o.s.j.s.ServletContextHandler@22f76fa8{/jobs/json,null,UNAVAILABLE}
o.s.j.s.ServletContextHandler@afe0d9f{/jobs,null,UNAVAILABLE}
UI at http://localhost:4040 <http://localhost:4040/>
Executor: Exception in task 1.0 in stage 5186.0 (TID 10371)
org.apache.spark.api.r.RRunner.compute(RRunner.scala:107)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:318)
org.apache.spark.scheduler.ResultTask.runTask(ResultTask.
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
Job 5183 failed: collect at null:-1, took 0.211674 s
RBackendHandler: collect on 26345 failed
5186 (collect at null:-1) failed in 0.210 s
SparkListenerBus has already stopped! Dropping event SparkListenerStageCompleted(org.apache.spark.scheduler.StageIn        fo@413da307)
SparkListenerBus has already stopped! Dropping event SparkListenerJobEnd(5183,1463682493877,JobFailed(org.apache.sp        ark.SparkException: Job 5183 cancelled because SparkContext was shut down))
MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
cleared
stopped
BlockManagerMaster stopped
OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
stopped SparkContext
Shutdown hook called
Deleting directory /private/var/folders/xy/qc35m0y55vq83dsqzg066_c40000gn/T/spark-dfafdddc-fd25-4eb4-bb1d-565915        1c8231

<gayathri.m.softie@gmail.com <mailto:gayathri.m.softie@gmail.com>> get it, attaching here again.
<gayathri.m.softie@gmail.com <mailto:gayathri.m.softie@gmail.com>> into the following error. Also note that head(df) does not display any rows. Can someone please help if I am missing something?
é‚®ä»¶å¸¦æœ‰é™„ä»¶é¢„è§ˆé“¾æŽ¥ï¼Œè‹¥æ‚¨è½¬å‘æˆ–å›žå¤æ­¤é‚®ä»¶æ—¶ä¸å¸Œæœ›å¯¹æ–¹é¢„è§ˆé™„ä»¶ï¼Œå»ºè®®æ‚¨æ‰‹åŠ¨åˆ é™¤é“¾æŽ¥ã€‚
<http://preview.mail.163.com/xdownload?filename=Screen+Shot+2016-05-18+at+5.09.29+PM.png&mid=xtbB0QpumlUL%2BmgE3wAAs4&part=3&sign=de2b9113bb74f7c3ed7f83a1243fb575&time=1463616821&uid=sunrise_win%40163.com> åœ¨çº¿é¢„è§ˆ <http://preview.mail.163.com/preview?mid=xtbB0QpumlUL%2BmgE3wAAs4&part=3&sign=de2b9113bb74f7c3ed7f83a1243fb575&time=1463616821&uid=sunrise_win%40163.com>
é‚®ä»¶å¸¦æœ‰é™„ä»¶é¢„è§ˆé“¾æŽ¥ï¼Œè‹¥æ‚¨è½¬å‘æˆ–å›žå¤æ­¤é‚®ä»¶æ—¶ä¸å¸Œæœ›å¯¹æ–¹é¢„è§ˆé™„ä»¶ï¼Œå»ºè®®æ‚¨æ‰‹åŠ¨åˆ é™¤é“¾æŽ¥ã€‚
<http://preview.mail.163.com/xdownload?filename=Screen+Shot+2016-05-18+at+5.09.29+PM.png&mid=1tbiPhRumlXlgWp0SwAAs6&part=3&sign=e0cf0eb619175e79cfa81bad7c1d26c9&time=1463622289&uid=sunrise_win%40163.com> åœ¨çº¿é¢„è§ˆ <http://preview.mail.163.com/preview?mid=1tbiPhRumlXlgWp0SwAAs6&part=3&sign=e0cf0eb619175e79cfa81bad7c1d26c9&time=1463622289&uid=sunrise_win%40163.com><Screen Shot 2016-05-18 at 5.09.29 PM.png>

"
dhruve ashar <dhruveashar@gmail.com>,"Thu, 19 May 2016 21:21:48 -0500",Re: SBT doesn't pick resource file after clean,Jakob Odersky <jakob@odersky.com>,"Based on the conversation on PR, the intent was not to pollute the source
directory and hence we are placing the generated file outside it in the
target/extra-resources directory. I agree that the ""sbt way"" is to add the
generated resources under the resourceManaged setting which was essentially
the earlier approach implemented.

However, even on generating the  file under the default resourceDirectory
=> core/src/resources doesn't pick the file in jar after doing a clean. So
this seems to be a different issue.









-- 
-Dhruve Ashar
"
Takuya UESHIN <ueshin@happy-camper.st>,"Fri, 20 May 2016 12:05:02 +0900",Re: [vote] Apache Spark 2.0.0-preview release (rc1),Reynold Xin <rxin@apache.org>,"-1 (non-binding)

I filed 2 major bugs of Spark SQL:

SPARK-15308 <https://issues.apache.org/jira/browse/SPARK-15308>: RowEncoder
should preserve nested column name.
SPARK-15313 <https://issues.apache.org/jira/browse/SPARK-15313>:
EmbedSerializerInFilter
"
Kai Jiang <jiangkai@gmail.com>,"Thu, 19 May 2016 20:13:47 -0700",Re: SparkR dataframe error,Sun Rui <sunrise_win@163.com>,"I was trying to build SparkR this week. hmm~ But, I encountered problem
with SparkR unit testing. That is probably similar as Gayathri encountered
with.
I tried many times with running ./R/run-tests.sh script. It seems like
every time the test will be failed.

Here are some environments when I was building:
java 7
R 3.30        (sudo apt-get install r-base-dev        under   ubuntu 15.04)
set SPARK_HOME=/path

R -e 'install.packages(""testthat"", repos=""http://cran.us.r-project.org"")'
build with:       build/mvn -Pyarn -Phadoop-2.4 -Dhadoop.version=2.4.0
-Psparkr -DskipTests -T 1C clean package

./R/install-dev.sh
./R/run-tests.sh

Here is the error message I got:
https://gist.github.com/vectorijk/71f4ff34e3d34a628b8a3013f0ca2aa2

I guess this issue related to permission. It seems I used `sudo
./R/run-tests.sh` and it worked sometimes. Without permission, maybe we
couldn't access /tmp directory.  However, the SparkR unit testing is
brittle.

Could someone give any hints of how to solve this?

Best,
Kai.


t
)
va:1142)
ava:617)
)
.
va:1142)
ava:617)
own))
-fd25-4eb4-bb1d-565915
:

ny
¸¦æœ‰é™„ä»¶é¢„è§ˆé“¾æŽ¥ï¼Œè‹¥æ‚¨è½¬å‘æˆ–å›žå¤æ­¤é‚®ä»¶æ—¶ä¸å¸Œæœ›å¯¹æ–¹é¢„è§ˆé™„ä»¶ï¼Œå»ºè®®æ‚¨æ‰‹åŠ¨åˆ é™¤é“¾æŽ¥ã€‚
¸‹è½½
5-18+at+5.09.29+PM.png&mid=xtbB0QpumlUL%2BmgE3wAAs4&part=3&sign=de2b9113bb74f7c3ed7f83a1243fb575&time=1463616821&uid=sunrise_win%40163.com>
part=3&sign=de2b9113bb74f7c3ed7f83a1243fb575&time=1463616821&uid=sunrise_win%40163.com>
¸¦æœ‰é™„ä»¶é¢„è§ˆé“¾æŽ¥ï¼Œè‹¥æ‚¨è½¬å‘æˆ–å›žå¤æ­¤é‚®ä»¶æ—¶ä¸å¸Œæœ›å¯¹æ–¹é¢„è§ˆé™„ä»¶ï¼Œå»ºè®®æ‚¨æ‰‹åŠ¨åˆ é™¤é“¾æŽ¥ã€‚
¸‹è½½
-18+at+5.09.29+PM.png&mid=1tbiPhRumlXlgWp0SwAAs6&part=3&sign=e0cf0eb619175e79cfa81bad7c1d26c9&time=1463622289&uid=sunrise_win%40163.com>
t=3&sign=e0cf0eb619175e79cfa81bad7c1d26c9&time=1463622289&uid=sunrise_win%40163.com>
"
Saisai Shao <sai.sai.shao@gmail.com>,"Fri, 20 May 2016 11:30:40 +0800",Re: combitedTextFile and CombineTextInputFormat,Alexander Pivovarov <apivovarov@gmail.com>,"enough for you to support any InputFormat you wanted. IMO it is not so
necessary to add a new API for this.


"
Takuya UESHIN <ueshin@happy-camper.st>,"Fri, 20 May 2016 13:44:16 +0900",Re: [vote] Apache Spark 2.0.0-preview release (rc1),Reynold Xin <rxin@apache.org>,"Hi all,

I'm sorry, I misunderstood the purpose of this vote.

I change to +1.

Thanks.




2016-05-20 12:05 GMT+09:00 Takuya UESHIN <ueshin@happy-camper.st>:




-- 
Takuya UESHIN
Tokyo, Japan

http://twitter.com/ueshin
"
Sun Rui <sunrise_win@163.com>,"Fri, 20 May 2016 12:54:42 +0800",Re: SparkR dataframe error,Kai Jiang <jiangkai@gmail.com>,"Yes. I also met this issue. It is likely related to recent R versions.
Could you help to submit a JIRA issue? I will take a look at it
problem with SparkR unit testing. That is probably similar as Gayathri encountered with.
every time the test will be failed.
15.04)
 <http://cran.us.r-project.org/>"")'
 -Psparkr -DskipTests -T 1C clean package
https://gist.github.com/vectorijk/71f4ff34e3d34a628b8a3013f0ca2aa2 <https://gist.github.com/vectorijk/71f4ff34e3d34a628b8a3013f0ca2aa2>
./R/run-tests.sh` and it worked sometimes. Without permission, maybe we couldn't access /tmp directory.  However, the SparkR unit testing is brittle.
<gayathri.m.softie@gmail.com <mailto:gayathri.m.softie@gmail.com>> what caused the issue with SparkR.
testing the latest branch-2.0, there could be an issue with your SparkR installation. Did you try `R/install-dev.sh`?
<gayathri.m.softie@gmail.com <mailto:gayathri.m.softie@gmail.com>> I run the R/run-tests.sh. This on a single MAC laptop, on the recently rebased master. R version is 3.3.0.
Exception in task 0.0 in stage 5186.0 (TID 10370)
org.apache.spark.api.r.RRunner.compute(RRunner.scala:107)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:318)
org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
o.s.j.s.ServletContextHandler@22f76fa8{/jobs/json,null,UNAVAILABLE}
o.s.j.s.ServletContextHandler@afe0d9f{/jobs,null,UNAVAILABLE}
web UI at http://localhost:4040 <http://localhost:4040/>
Executor: Exception in task 1.0 in stage 5186.0 (TID 10371)
org.apache.spark.api.r.RRunner.compute(RRunner.scala:107)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:318)
org.apache.spark.scheduler.ResultTask.runTask(ResultTask.
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
DAGScheduler: Job 5183 failed: collect at null:-1, took 0.211674 s
RBackendHandler: collect on 26345 failed
5186 (collect at null:-1) failed in 0.210 s
SparkListenerBus has already stopped! Dropping event SparkListenerStageCompleted(org.apache.spark.scheduler.StageIn        fo@413da307)
SparkListenerBus has already stopped! Dropping event SparkListenerJobEnd(5183,1463682493877,JobFailed(org.apache.sp        ark.SparkException: Job 5183 cancelled because SparkContext was shut down))
MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
cleared
BlockManager stopped
BlockManagerMaster stopped
OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
Successfully stopped SparkContext
Shutdown hook called
Deleting directory /private/var/folders/xy/qc35m0y55vq83dsqzg066_c40000gn/T/spark-dfafdddc-fd25-4eb4-bb1d-565915        1c8231

<gayathri.m.softie@gmail.com <mailto:gayathri.m.softie@gmail.com>> get it, attaching here again.
<gayathri.m.softie@gmail.com <mailto:gayathri.m.softie@gmail.com>> into the following error. Also note that head(df) does not display any rows. Can someone please help if I am missing something?
é‚®ä»¶å¸¦æœ‰é™„ä»¶é¢„è§ˆé“¾æŽ¥ï¼Œè‹¥æ‚¨è½¬å‘æˆ–å›žå¤æ­¤é‚®ä»¶æ—¶ä¸å¸Œæœ›å¯¹æ–¹é¢„è§ˆé™„ä»¶ï¼Œå»ºè®®æ‚¨æ‰‹åŠ¨åˆ é™¤é“¾æŽ¥ã€‚
<http://preview.mail.163.com/xdownload?filename=Screen+Shot+2016-05-18+at+5.09.29+PM.png&mid=xtbB0QpumlUL%2BmgE3wAAs4&part=3&sign=de2b9113bb74f7c3ed7f83a1243fb575&time=1463616821&uid=sunrise_win%40163.com> åœ¨çº¿é¢„è§ˆ <http://preview.mail.163.com/preview?mid=xtbB0QpumlUL%2BmgE3wAAs4&part=3&sign=de2b9113bb74f7c3ed7f83a1243fb575&time=1463616821&uid=sunrise_win%40163.com>
é‚®ä»¶å¸¦æœ‰é™„ä»¶é¢„è§ˆé“¾æŽ¥ï¼Œè‹¥æ‚¨è½¬å‘æˆ–å›žå¤æ­¤é‚®ä»¶æ—¶ä¸å¸Œæœ›å¯¹æ–¹é¢„è§ˆé™„ä»¶ï¼Œå»ºè®®æ‚¨æ‰‹åŠ¨åˆ é™¤é“¾æŽ¥ã€‚
<http://preview.mail.163.com/xdownload?filename=Screen+Shot+2016-05-18+at+5.09.29+PM.png&mid=1tbiPhRumlXlgWp0SwAAs6&part=3&sign=e0cf0eb619175e79cfa81bad7c1d26c9&time=1463622289&uid=sunrise_win%40163.com> åœ¨çº¿é¢„è§ˆ <http://preview.mail.163.com/preview?mid=1tbiPhRumlXlgWp0SwAAs6&part=3&sign=e0cf0eb619175e79cfa81bad7c1d26c9&time=1463622289&uid=sunrise_win%40163.com><Screen Shot 2016-05-18 at 5.09.29 PM.png>

"
Hyukjin Kwon <gurwls223@gmail.com>,"Fri, 20 May 2016 13:55:09 +0900",Re: [vote] Apache Spark 2.0.0-preview release (rc1),Takuya UESHIN <ueshin@happy-camper.st>,"I happened to test SparkR in Windows 7 (32 bits) and it seems some tests
are failed.

Could this be a reason to downvote?

For more details of the tests, please see
https://github.com/apache/spark/pull/13165#issuecomment-220515182



2016-05-20 13:44 GMT+09:00 Takuya UESHIN <ueshin@happy-camper.st>:

"
Sun Rui <sunrise_win@163.com>,"Fri, 20 May 2016 12:55:58 +0800",Re: SparkR dataframe error,Kai Jiang <jiangkai@gmail.com>,"Kai,
You can simply ignore this test failure before it is fixed
problem with SparkR unit testing. That is probably similar as Gayathri encountered with.
like every time the test will be failed.
15.04)
repos=""http://cran.us.r-project.org <http://cran.us.r-project.org/>"")'
-Dhadoop.version=2.4.0 -Psparkr -DskipTests -T 1C clean package
https://gist.github.com/vectorijk/71f4ff34e3d34a628b8a3013f0ca2aa2 <https://gist.github.com/vectorijk/71f4ff34e3d34a628b8a3013f0ca2aa2>
./R/run-tests.sh` and it worked sometimes. Without permission, maybe we couldn't access /tmp directory.  However, the SparkR unit testing is brittle.
<gayathri.m.softie@gmail.com <mailto:gayathri.m.softie@gmail.com>> sure what caused the issue with SparkR.
testing the latest branch-2.0, there could be an issue with your SparkR installation. Did you try `R/install-dev.sh`?
<gayathri.m.softie@gmail.com <mailto:gayathri.m.softie@gmail.com>> I run the R/run-tests.sh. This on a single MAC laptop, on the recently rebased master. R version is 3.3.0.
Exception in task 0.0 in stage 5186.0 (TID 10370)
org.apache.spark.api.r.RRunner.compute(RRunner.scala:107)
org.apache.spark.api.r.BaseRRDD.compute(RRDD.scala:49)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:318)
org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
o.s.j.s.ServletContextHandler@22f76fa8{/jobs/json,null,UNAVAILABLE}
o.s.j.s.ServletContextHandler@afe0d9f{/jobs,null,UNAVAILABLE}
web UI at http://localhost:4040 <http://localhost:4040/>
Executor: Exception in task 1.0 in stage 5186.0 (TID 10371)
org.apache.spark.api.r.RRunner.compute(RRunner.scala:107)
org.apache.spark.api.r.BaseRRDD.compute(RRDD.scala:49)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:318)
org.apache.spark.scheduler.ResultTask.runTask(ResultTask.
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
DAGScheduler: Job 5183 failed: collect at null:-1, took 0.211674 s
RBackendHandler: collect on 26345 failed
ResultStage 5186 (collect at null:-1) failed in 0.210 s
SparkListenerBus has already stopped! Dropping event SparkListenerStageCompleted(org.apache.spark.scheduler.StageIn        fo@413da307)
SparkListenerBus has already stopped! Dropping event SparkListenerJobEnd(5183,1463682493877,JobFailed(org.apache.sp        ark.SparkException: Job 5183 cancelled because SparkContext was shut down))
MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
cleared
BlockManager stopped
BlockManagerMaster stopped
OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
Successfully stopped SparkContext
Shutdown hook called
Deleting directory /private/var/folders/xy/qc35m0y55vq83dsqzg066_c40000gn/T/spark-dfafdddc-fd25-4eb4-bb1d-565915        1c8231

<gayathri.m.softie@gmail.com <mailto:gayathri.m.softie@gmail.com>> not get it, attaching here again.
<gayathri.m.softie@gmail.com <mailto:gayathri.m.softie@gmail.com>> into the following error. Also note that head(df) does not display any rows. Can someone please help if I am missing something?
é‚®ä»¶å¸¦æœ‰é™„ä»¶é¢„è§ˆé“¾æŽ¥ï¼Œè‹¥æ‚¨è½¬å‘æˆ–å›žå¤æ­¤é‚®ä»¶æ—¶ä¸å¸Œæœ›å¯¹æ–¹é¢„è§ˆé™„ä»¶ï¼Œå»ºè®®æ‚¨æ‰‹åŠ¨åˆ é™¤é“¾æŽ¥ã€‚
<http://preview.mail.163.com/xdownload?filename=Screen+Shot+2016-05-18+at+5.09.29+PM.png&mid=xtbB0QpumlUL%2BmgE3wAAs4&part=3&sign=de2b9113bb74f7c3ed7f83a1243fb575&time=1463616821&uid=sunrise_win%40163.com> åœ¨çº¿é¢„è§ˆ <http://preview.mail.163.com/preview?mid=xtbB0QpumlUL%2BmgE3wAAs4&part=3&sign=de2b9113bb74f7c3ed7f83a1243fb575&time=1463616821&uid=sunrise_win%40163.com>
é‚®ä»¶å¸¦æœ‰é™„ä»¶é¢„è§ˆé“¾æŽ¥ï¼Œè‹¥æ‚¨è½¬å‘æˆ–å›žå¤æ­¤é‚®ä»¶æ—¶ä¸å¸Œæœ›å¯¹æ–¹é¢„è§ˆé™„ä»¶ï¼Œå»ºè®®æ‚¨æ‰‹åŠ¨åˆ é™¤é“¾æŽ¥ã€‚
<http://preview.mail.163.com/xdownload?filename=Screen+Shot+2016-05-18+at+5.09.29+PM.png&mid=1tbiPhRumlXlgWp0SwAAs6&part=3&sign=e0cf0eb619175e79cfa81bad7c1d26c9&time=1463622289&uid=sunrise_win%40163.com> åœ¨çº¿é¢„è§ˆ <http://preview.mail.163.com/preview?mid=1tbiPhRumlXlgWp0SwAAs6&part=3&sign=e0cf0eb619175e79cfa81bad7c1d26c9&time=1463622289&uid=sunrise_win%40163.com><Screen Shot 2016-05-18 at 5.09.29 PM.png>

"
Xiao Li <gatorsmile@gmail.com>,"Thu, 19 May 2016 21:57:44 -0700",Re: [vote] Apache Spark 2.0.0-preview release (rc1),Reynold Xin <rxin@databricks.com>,"Changed my vote to +1. Thanks!

2016-05-19 13:28 GMT-07:00 Xiao Li <gatorsmile@gmail.com>:

y
"",
p/py4j/java_gateway.py"",
p/py4j/protocol.py"",
yCatalog.scala:297)
atalog.scala:280)
)
sult$lzycompute(commands.scala:57)
sult(commands.scala:55)
mmands.scala:69)
Plan.scala:115)
Plan.scala:115)
SparkPlan.scala:136)
la:151)
33)
)
cution.scala:85)
a:85)
va:57)
rImpl.java:43)
"",
"",
p/py4j/java_gateway.py"",
p/py4j/protocol.py"",
yCatalog.scala:297)
atalog.scala:280)
)
sult$lzycompute(commands.scala:57)
sult(commands.scala:55)
mmands.scala:69)
Plan.scala:115)
Plan.scala:115)
SparkPlan.scala:136)
la:151)
33)
)
cution.scala:85)
a:85)
va:57)
rImpl.java:43)
o
 as we
%3D%20SPARK%20AND%20resolution%20%3D%20Unresolved%20AND%20affectedVersion%20%3D%202.0.0
0=%20SPARK%20AND%20resolution%20=%20Unresolved%20AND%20affectedVersion%20=%202.0.0>
.0.
f
re
-------
-------
-------
ew
o
e
ew-bin/
ew-docs/
0%3D%20SPARK%20AND%20fixVersion%20%3D%202.0.0
n
----
"
Alexander Pivovarov <apivovarov@gmail.com>,"Thu, 19 May 2016 22:15:27 -0700",Re: combitedTextFile and CombineTextInputFormat,Saisai Shao <sai.sai.shao@gmail.com>,"Saisai, Reynold,

Thank you for your replies.
I also think that many variation of textFile() methods might be confusing
for users. Better to have just one good textFile() implementation.

Do you think sc.textFile() should use CombineTextInputFormat instead
of TextInputFormat?

CombineTextInputFormat allows users to control number of partitions in RDD
(control split size)
It's useful for real workloads (e.g. 100 folders, 200,000 files, all files
are different size, e.g. 100KB - 500MB, total 4TB)

if we use current implementation of sc.textFile() it will generate RDD with
250,000+ partitions (one partition for each small file, several partitions
for big files).

Using CombineTextInputFormat allows us to control number of partitions and
split size by settign mapreduce.input.fileinputformat.split.maxsize
property. e.g. if we set it to 256MB spark will generate RDD with ~20,000
partitions.

It's better to have RDD with 20,000 partitions by 256MB than RDD with
250,000+ partition all different sizes from 100KB to 128MB

So, I see only advantages if sc.textFile() starts using CombineTextInputFormat
instead of TextInputFormat

Alex


"
Saisai Shao <sai.sai.shao@gmail.com>,"Fri, 20 May 2016 13:24:17 +0800",Re: combitedTextFile and CombineTextInputFormat,Alexander Pivovarov <apivovarov@gmail.com>,"Hi Alex,

APIs to Dataset/DataFrame based ones, so for me it is not so necessary to
add a new RDD based API as I mentioned before. Also for the problem of so
many partitions, I think there're many other solutions to handle it.

Of course it is just my own thought.

Thanks
Saisai


"
Kai Jiang <jiangkai@gmail.com>,"Thu, 19 May 2016 23:01:22 -0700",Re: SparkR dataframe error,Sun Rui <sunrise_win@163.com>,"Cool. I will open a JIRA issue to track this.

Thanks,
Kai.


d
4)
)'
7)
ava:1142)
java:617)
7)
k.
)
ava:1142)
java:617)
down))
!
c-fd25-4eb4-bb1d-565915
€
:
any
¸¦æœ‰é™„ä»¶é¢„è§ˆé“¾æŽ¥ï¼Œè‹¥æ‚¨è½¬å‘æˆ–å›žå¤æ­¤é‚®ä»¶æ—¶ä¸å¸Œæœ›å¯¹æ–¹é¢„è§ˆé™„ä»¶ï¼Œå»ºè®®æ‚¨æ‰‹åŠ¨åˆ é™¤é“¾æŽ¥ã€‚
¸‹è½½
05-18+at+5.09.29+PM.png&mid=xtbB0QpumlUL%2BmgE3wAAs4&part=3&sign=de2b9113bb74f7c3ed7f83a1243fb575&time=1463616821&uid=sunrise_win%40163.com>
&part=3&sign=de2b9113bb74f7c3ed7f83a1243fb575&time=1463616821&uid=sunrise_win%40163.com>
¸¦æœ‰é™„ä»¶é¢„è§ˆé“¾æŽ¥ï¼Œè‹¥æ‚¨è½¬å‘æˆ–å›žå¤æ­¤é‚®ä»¶æ—¶ä¸å¸Œæœ›å¯¹æ–¹é¢„è§ˆé™„ä»¶ï¼Œå»ºè®®æ‚¨æ‰‹åŠ¨åˆ é™¤é“¾æŽ¥ã€‚
¸‹è½½
5-18+at+5.09.29+PM.png&mid=1tbiPhRumlXlgWp0SwAAs6&part=3&sign=e0cf0eb619175e79cfa81bad7c1d26c9&time=1463622289&uid=sunrise_win%40163.com>
rt=3&sign=e0cf0eb619175e79cfa81bad7c1d26c9&time=1463622289&uid=sunrise_win%40163.com>
"
Reynold Xin <rxin@databricks.com>,"Fri, 20 May 2016 00:11:06 -0700",Re: right outer joins on Datasets,Andres Perez <andres@tresata.com>,"I filed https://issues.apache.org/jira/browse/SPARK-15441




"
Reynold Xin <rxin@databricks.com>,"Fri, 20 May 2016 00:35:40 -0700",Re: Dataset reduceByKey,Andres Perez <andres@tresata.com>,"Andres - this is great feedback. Let me think about it a little bit more
and reply later.



"
Ross Lawley <ross.lawley@gmail.com>,"Fri, 20 May 2016 11:40:12 +0000",Re: [vote] Apache Spark 2.0.0-preview release (rc1),dev@spark.apache.org,"+1 Having an rc1 would help me get stable feedback on using my library with
Spark, compared to relying on 2.0.0-SNAPSHOT.


dy
)
y"",
ip/py4j/java_gateway.py"",
,
ip/py4j/protocol.py"",
d
ryCatalog.scala:297)
Catalog.scala:280)
3)
esult$lzycompute(commands.s"
Steve Loughran <stevel@hortonworks.com>,"Fri, 20 May 2016 13:31:49 +0000",Re: Spark driver and yarn behavior,,"
On 20 May 2016, at 00:34, Shankar Venkataraman <shankarvenkataraman666@gmail.com<mailto:shankarvenkataraman666@gmail.com>> wrote:

Thanks Luciano. The case we are seeing is different - the yarn resource manager is shutting down the container in which the executor is running since there does not seem to be a response and it is deeming it dead. It started another container but the driver seems to be oblivious for nearly 2 hours. Am wondering if there is a condition where the driver is not seeing the notification from the Yarn RM about the executor container going away. We will try some of the settings you pointed to, and see if alleviates the issue.

Shankar



the YARN RM doesn't (AFAIK) do any liveness checks on executors.

1. The AM regularly heartbeats with the RM;
2. if that stops the AM is killed (and unless its requested container preservation), all its containers. The AM is then restarted (if retries < yarn.am.retry.count (?"").
3. Node Managers, one per server, heartbeat to the RM.
4. If they stop checking in, AM assumes node and all running containers are dead, reports failures to the AM, leaves it to deal with. (Special case: Work preserving NM restart).
5. If the process running in  a container fails, the NM picks it up and relays that to the AM via the RM.

some details: http://www.slideshare.net/steve_l/yarn-services


Have a look in the NM logs to see what it thinks is happening â€”but i think it may well be some driver/executor communication problem.


"
Doug Balog <doug.sparkdev@dugos.com>,"Fri, 20 May 2016 10:01:56 -0400",Re: Possible Hive problem with Spark 2.0.0 preview.,Reynold Xin <rxin@databricks.com>,"Some more info Iâ€™m still digging.
Iâ€™m just trying to do  `spark.table(â€œdb.tableâ€).count`from a spark-shell
â€œdb.tableâ€ is just a hive table.

At commit b67668b this worked just fine and it returned the number of rows in db.table.
Starting at ca99171  ""[SPARK-15073][SQL] Hide SparkSession constructor from the publicâ€ it fails with 

org.apache.spark.sql.AnalysisException: Database â€˜db' does not exist;
  at org.apache.spark.sql.catalyst.catalog.ExternalCatalog.requireDbExists(ExternalCatalog.scala:37)
  at org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.tableExists(InMemoryCatalog.scala:195)
  at org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.requireTableExists(InMemoryCatalog.scala:63)
  at org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.getTable(InMemoryCatalog.scala:186)
  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupRelation(SessionCatalog.scala:337)
  at org.apache.spark.sql.SparkSession.table(SparkSession.scala:524)
  at org.apache.spark.sql.SparkSession.table(SparkSession.scala:520)
  ... 48 elided

If I run ""org.apache.spark.sql.SparkSession.builder.enableHiveSupport.getOrCreate.catalog.listDatabases.show(False)â€

+-------------------------------------------------------------------------------------------------------------------------------------------------+-----------+-----------+
|name                                                                                                                                             |description|locationUri|
+-------------------------------------------------------------------------------------------------------------------------------------------------+-----------+-----------+
|Database[name='default', description='default database', path='hdfs://ns/{CWD}/spark-warehouse']|
+-------------------------------------------------------------------------------------------------------------------------------------------------+-----------+â€”â€”â€”â€”â€”+


 Where CWD is the current working directory of where I started my spark-shell.

It looks like this commit causes spark.catalog to be the internal one instead of the Hive one. 

Michael, I dont this this is related to the HDFS configurations, they are in /etc/hadoop/conf on each of the nodes in the cluster. 

Arun, I was referring to these docs, http://home.apache.org/~pwendell/spark-releases/spark-2.0.0-preview-docs/sql-programming-guide.html they need to be updated to no refer to HiveContext. 

I donâ€™t think HiveContext should be marked as private[Hive], it should be public. 
 
Iâ€™ll keep digging.

Doug


http://home.apache.org/~pwendell/spark-releases/spark-2.0.0-preview-docs/api/scala/index.html#org.apache.spark.sql.hive.HiveContext, you'll see
SparkSession.builder.enableHiveSupport instead
org.apache.spark.sql.hive.HiveContext(sc)â€  doesnâ€™t work because â€œHiveContext not a member of org.apache.spark.sql.hiveâ€  I checked the documentation, and it looks like it should still work for spark-2.0.0-preview-bin-hadoop2.7.tgz
package, which you'll need to include explicitly.  Docs have not been updated yet.
â€˜spark.table(â€œdb.tableâ€)â€™, it fails with a HDFS permission denied canâ€™t write to â€œ/user/hive/warehouseâ€
propagating them correctly any more. 


---------------------------------------------------------------------


"
Mark Hamstra <mark@clearstorydata.com>,"Fri, 20 May 2016 09:33:21 -0700",Re: [vote] Apache Spark 2.0.0-preview release (rc1),Ross Lawley <ross.lawley@gmail.com>,"This is isn't yet a release candidate since, as Reynold mention in his
opening post, preview releases are ""not meant to be functional, i.e. they
can and highly likely will contain critical bugs or documentation errors.""
errors, then the release candidates will start.


n
t
ody
"")
py"",
)
zip/py4j/java_gateway.py"",
"",
zip/py4j/protocol.py"",
ed
oryCatalog.scala:297)
nCatalog.scala:280)
Result$lzycompute(commands.scala:57)
Result(commands.scala:55)
commands.scala:69)
rkPlan.scala:115)
rkPlan.scala:115)
y(SparkPlan.scala:136)
cala:151)
:133)
xecution.scala:85)
ala:85)
java:57)
sorImpl.java:43)
8)
py"",
py"",
)
zip/py4j/java_gateway.py"",
"",
zip/py4j/protocol.py"",
ed
oryCatalog.scala:297)
nCatalog.scala:280)
Result$lzycompute(commands.scala:57)
Result(commands.scala:55)
commands.scala:69)
rkPlan.scala:115)
rkPlan.scala:115)
y(SparkPlan.scala:136)
cala:151)
:133)
xecution.scala:85)
ala:85)
java:57)
sorImpl.java:43)
8)
agine as
20%3D%20SPARK%20AND%20resolution%20%3D%20Unresolved%20AND%20affectedVersion%20%3D%202.0.0
%20=%20SPARK%20AND%20resolution%20=%20Unresolved%20AND%20affectedVersion%20=%202.0.0>
/jre
:
---------
---------
---------
n
k
view-bin/
:
view-docs/
%20%3D%20SPARK%20AND%20fixVersion%20%3D%202.0.0
------
"
Jakob Odersky <jakob@odersky.com>,"Fri, 20 May 2016 12:48:07 -0700",Re: SBT doesn't pick resource file after clean,dhruve ashar <dhruveashar@gmail.com>,"Ah, I think I see the issue. resourceManaged and core/src/resources
aren't included in the classpath; to achieve that, you need to scope
the setting to either ""compile"" or ""test"" (probably compile in your
case). So, the simplest way to add the extra settings would be
something like:

resourceGenerators in Compile += {
  val file = //generate properties file
  IO.copy(file, (resourceManaged in Compile).value  / ""foo.properties"")


---------------------------------------------------------------------


"
Ricardo Almeida <ricardo.almeida@actnowib.com>,"Sat, 21 May 2016 01:02:37 +0200",Re: [vote] Apache Spark 2.0.0-preview release (rc1),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1

----
Ricardo Almeida


""
n
in
:
.py"",
d)
.zip/py4j/java_gateway.py"",
y"",
.zip/py4j/protocol.py"",
.
moryCatalog.scala:297)
onCatalog.scala:280)
)
tResult$lzycompute(commands.scala:57)
tResult(commands.scala:55)
(commands.scala:69)
arkPlan.scala:115)
ar"
Yash Sharma <yash360@gmail.com>,"Sat, 21 May 2016 10:54:14 +1000",Quick question on spark performance,dev@spark.apache.org,"Hi All,
I am here to get some expert advice on a use case I am working on.

Cluster & job details below -

Data - 6 Tb
Cluster - EMR - 15 Nodes C3-8xLarge (shared by other MR apps)

Parameters-
--executor-memory 10G \
--executor-cores 6 \
--conf spark.dynamicAllocation.enabled=true \
--conf spark.dynamicAllocation.initialExecutors=15 \

Runtime : 3 Hrs

(since I don't have lot of groupings)

Reducing to --executor-memory 3G, Runtime reduced to: 2 Hrs

Question:
anything I can tune/change/experiment with to make the job faster.

Workload: Mostly reduceBy's and scans.

Would appreciate any insights and thoughts. Best Regards
"
Reynold Xin <rxin@databricks.com>,"Fri, 20 May 2016 17:59:12 -0700",Re: Quick question on spark performance,Yash Sharma <yash360@gmail.com>,"It's probably due to GC.


"
Ted Yu <yuzhihong@gmail.com>,"Fri, 20 May 2016 18:03:11 -0700",Re: Quick question on spark performance,Reynold Xin <rxin@databricks.com>,"Yash:
Can you share the JVM parameters you used ?

How many partitions are there in your data set ?

Thanks


"
Yash Sharma <yash360@gmail.com>,"Sat, 21 May 2016 11:07:07 +1000",Re: Quick question on spark performance,Reynold Xin <rxin@databricks.com>,"The median GC time is 1.3 mins for a median duration of 41 mins. What
parameters can I tune for controlling GC.

Other details, median Peak execution memory of 13 G and input records of
2.3 gigs.
180-200 executors launched.

- Thanks, via mobile,  excuse brevity.

"
Yash Sharma <yash360@gmail.com>,"Sat, 21 May 2016 11:10:21 +1000",Re: Quick question on spark performance,Reynold Xin <rxin@databricks.com>,"Am going with the default java opts for emr-
-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps
-XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70
-XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled

The data is not partitioned. Its 6Tb data of around 400 Megs gz files. The
workload is a scan/filter/reduceBy which needs to scan the entire data.




"
dhruve ashar <dhruveashar@gmail.com>,"Fri, 20 May 2016 20:47:31 -0500",Re: SBT doesn't pick resource file after clean,Jakob Odersky <jakob@odersky.com>,"The issue is fixed.

Here's an explanation which interested people can read through:

For the earlier mail, the default resourceDirectory =>
core/src/main/resources didn't yield the expected result. By default all
the static resources placed under this directory are picked up and included
in the jar. But not the dynamic ones.

inferred that internally sbt prepares a list of files to be copied before
creating the jar and these files have to exist at the time of list
creation.  Dynamically created files do not make it to the list and hence
are not copied.

So here's what was happening:

1. Perform clean - file doesn't exist
2. SBT prepares a list, file doesn't make it to the list.
3. Script invoked which generates the properties file.
4. jar created, but props file missing.

1. props file exists from previous run.
2. SBT prepares a list, this time it appears in the list.
3. Script overwrites it with latest data.
4. jar created, with props file containing latest data.

So the whole idea was to inform sbt to add the dynamically generated file
in its list of files to be copied. So here's what we did:
resourceGenerators in Compile += {
  1. Invoke shell script - this creates the props file.
  2. Obtain a handle to the file
  3. Return the file handle so that it gets included in the list and gets
picked up the first time as well.
}

There is no need of providing unmanagedResourceDirectories as it picks up
only static files. For dynamically generated files, you must return the
file handle to be included. It doesn't pick it up from the listed
directory.

Note: It was easy to miss this as the shell script was generating the file.

The behavior was also verified using a simple sbt project.

-Dhruve










-- 
-Dhruve Ashar
"
Steve Loughran <stevel@hortonworks.com>,"Sat, 21 May 2016 13:11:12 +0000",Re: [DISCUSS] Removing or changing maintainer process,Matei Zaharia <matei.zaharia@gmail.com>,"
> On 19 May 2016, at 16:34, Matei Zaharia <matei.zaharia@gmail.com> wrote:
> 
> Hi folks,
> 
> Around 1.5 years ago, Spark added a maintainer process for reviewing API and architectural changes (https://cwiki.apache.org/confluence/display/SPARK/Committers#Committers-ReviewProcessandMaintainers) to make sure these are seen by people who spent a lot of time on that component. At the time, the worry was that changes might go unnoticed as the project grows, but there were also concerns that this approach makes the project harder to contribute to and less welcoming. Since implementing the model, I think that a good number of developers concluded it doesn't make a huge difference, so because of these concerns, it may be useful to remove it. I've also heard that we should try to keep some other instructions for contributors to find the ""right"" reviewers, so it would be great to see suggestions on that. For my part, I'd personally prefer something ""automatic"", such as easily tracking who reviewed each patch and having people look at the commit history of the module they want to work on, instead of a list that needs to be maintained separately.
> 


Putting on my ASF hat, I'm supportive of this.  An Apache software project is a large, shared, body of code â€”and committers are expected to be trusted to be allowed to contribute through, and not to break things. That's in their own patches â€”and in their review of other's work.

Having a nominated owner of a module is marking out bits of the code as territories, which can not only keep others out of it, it can actually put extra responsibility on the maintainer, who, because they are an effective choke point for patches for a module, have to try to keep up with an ever growing patch queue.

Managing code review is a scale problem; any large OSS project, especially those on a RTC process, comes up against it. I'd love to seem some good Software Engineering research/papers in this area

There is one forthcoming soon, ""Why Should Architecture-Savvy Developers Write Code?"". This looks at some of the development practises of various ASF projects (Hadoop, Hive, Pig, OFBiz, Camel, OpenJPA) and tries to see if there is a correlation in code quality between Linked-In proclaimed architects, code contributions and the (as inferred from JIRA) quality of those classes. Having seen the draft, I think everyone should find it interesting â€”even if I have a few issues with some of its classifications, at least based on the analysis of my bits.

http://blog.ieeesoftware.org/2016/02/why-should-software-architects-write.html

What I've not seen is anything looking at what makes both a good process for succouring quality contributions, but for doing so in a way which preserves quality, maintainability and an overall coherent architecture of a large system. If there are any, I'd love a copy.


-Steve


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org
"
Gurvinder Singh <gurvinder.singh@uninett.no>,"Sat, 21 May 2016 18:30:08 +0200",spark on kubernetes,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

I am currently working on deploying Spark on kuberentes (K8s) and it is
working fine. I am running Spark with standalone mode and checkpointing
the state to shared system. So if master fails K8s starts it and from
checkpoint it recover the earlier state and things just works fine. I
have an issue with the Spark master Web UI to access the worker and
application UI links. In brief, kubernetes service model allows me to
expose the master service to internet, but accessing the
application/workers UI is not possible as then I have to expose them too
individually and given I can have multiple application it becomes hard
to manage.

information/state/logs from application/workers. As it has the
information about their endpoint when application/worker register with
master, so when a user initiate a request to access the information,
master can proxy the request to corresponding endpoint.

So I am wondering if someone has already done work in this direction
then it would be great to know. If not then would the community will be
interesting in such feature. If yes then how and where I should get
started as it would be helpful for me to have some guidance to start
working on this.

Kind Regards,
Gurvinder

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Sat, 21 May 2016 12:18:36 -0700",Re: [vote] Apache Spark 2.0.0-preview release (rc1),Reynold Xin <rxin@apache.org>,"This vote passes with 14 +1s (5 binding*) and no 0 or -1!  Thanks to
everyone who voted.  I'll start work on publishing the release.

+1:
Reynold Xin*
Sean Owen*
Ovidiu-Cristian MARCU
Krishna Sankar
Michael Armbrust*
Yin Huai
Joseph Bradley*
Xiangrui Meng*
Herman van HÃ¶vell tot Westerflier
Vishnu Prasad
Takuya UESHIN
Xiao Li
Ross Lawley
Ricardo Almeida

0: (none)

-1: (none)



rs
he
ew
o
T
PARK%20AND%20fixVersion%20%3D%202.0.0
"
Sun Rui <sunrise_win@163.com>,"Sun, 22 May 2016 14:30:11 +0800",Re: spark on kubernetes,Gurvinder Singh <gurvinder.singh@uninett.no>,"I think â€œreverse proxyâ€ is beneficial  to monitoring a cluster in a secure way. This feature is not only desired for Spark on standalone, but also Spark on YARN, and also projects other than spark.

Maybe Apache Knox can help you. Not sure how Knox can integrate with Spark.

"
Reynold Xin <rxin@databricks.com>,"Sat, 21 May 2016 23:32:22 -0700",Re: spark on kubernetes,Gurvinder Singh <gurvinder.singh@uninett.no>,"Kubernetes itself already has facilities for http proxy, doesn't it?



"
Gurvinder Singh <gurvinder.singh@uninett.no>,"Sun, 22 May 2016 08:53:47 +0200",Re: spark on kubernetes,dev@spark.apache.org,"I think to secure the Spark you can use any reverse proxy out there e.g
Knox or light weight as nginx/node-http-proxy or pick your favorite
language. There are even oauth2-proxy
(https://github.com/bitly/oauth2_proxy) too which can secure for example
Spark UI using github/google accounts.

But the issue here is that in the Spark master UI page has links to
information about workers which points to their internal IP addresses,
so you need to have either VPN or on the same network to get the worker
information e.g logs, etc. Same goes for application UI as driver is
inside the spark cluster network.

So the idea is that the Spark master UI can act as a reverse proxy to
get these information. for example

Worker with ID worker1 running at IP address 10.2.3.4:8081 in current
situation a user accessing the master UI and want to see information
from worker1, user needs to either connect VPN or have 10.2.3.4
accessible from his/her machine. So the proposal is to have a
functionality in spark master UI where to access the worker with ID
worker1 the link will be like spark-master.com/worker1 when user access
this link, master will proxy this to 10.2.3.4:8081 and back. So user
does not need to be on the same network.

This will really simplify the spark ui access in general case too where
you will need to expose only one IP to the public.

I have done preliminary study of the code and it seems Spark is using
Jetty for it and Jetty has ProxyServlet which can serve this purpose. So
would be good to know if community is interested in having such a
feature and get together to add it then :)

- Gurvinder


---------------------------------------------------------------------


"
Gurvinder Singh <gurvinder.singh@uninett.no>,"Sun, 22 May 2016 08:55:50 +0200",Re: spark on kubernetes,Reynold Xin <rxin@databricks.com>,"Yeah kubernetes has ingress controller which can act the L7 load
balancer and router traffic to Spark UI in this case. But I am referring
to link present in UI to worker and application UI. Replied in the
detail to Sun Rui's mail where I gave example of possible scenario.

- Gurvinder


---------------------------------------------------------------------


"
Sun Rui <sunrise_win@163.com>,"Sun, 22 May 2016 16:23:14 +0800",Re: spark on kubernetes,Gurvinder Singh <gurvinder.singh@uninett.no>,"If it is possible to rewrite URL in outbound responses in Knox or other reverse proxy, would that solve your issue?
referring
it is
checkpointing
from
fine. I
and
to
them too
hard
access
with
information,
direction
will be
get
start
---------------------------------------------------------------------



---------------------------------------------------------------------


"
Gurvinder Singh <gurvinder.singh@uninett.no>,"Sun, 22 May 2016 12:49:27 +0200",Re: spark on kubernetes,dev@spark.apache.org,"Any process which can keep track of workers and application drivers IP
addresses and route traffic to those will work. Considering Spark Master
does exactly this due to all workers and application has to register to
the master, therefore I propose master to be the place to add such a
functionality.

I am not aware with Knox capabilities but Nginx or any other normal
reverse proxy will not be able to this on its own due to dynamic nature
of application drivers and to some extent workers too.

- Gurvinder


---------------------------------------------------------------------


"
"""=?gb18030?B?s8nHvw==?="" <cq365423762@qq.com>","Sun, 22 May 2016 20:04:12 +0800",,"""=?gb18030?B?ZGV2?="" <dev@spark.apache.org>",I would like to contribute to spark. I am working on spark-15429. Please give permission to contribute.
Sun Rui <sunrise_win@163.com>,"Sun, 22 May 2016 20:33:07 +0800",Re: ,=?utf-8?B?5oiQ5by6?= <cq365423762@qq.com>,"No permission is required. Just send your PR:)

"
Dongjoon Hyun <dongjoon@apache.org>,"Sun, 22 May 2016 13:25:04 -0700",Using Travis for JDK7/8 compilation and lint-java.,dev <dev@spark.apache.org>,"Hi, All.

I want to propose the followings.

- Turn on Travis CI for Apache Spark PR queue.
- Recommend this for contributors, too

Currently, Spark provides Travis CI configuration file to help contributors
check Scala/Java style conformance and JDK7/8 compilation easily during
their preparing pull requests. Please note that it's only about static
analysis.

- For Oracle JDK7, mvn -DskipTests install and run `dev/lint-java`.
- For Oracle JDK7, mvn -DskipTests install and run `dev/lint-java`.
Scalastyle is included in the step 'mvn install', too.

Yep, if you turn on your Travis CI configuration, you can already see the
results on your branches before making PR. I wrote this email to prevent
more failures proactively and community-widely.

For stability, I have been monitoring that for two weeks. It detects the
failures or recovery on JDK7 builds or Java linter on Spark master branch
correctly. The only exceptional case I observed rarely is `timeout` failure
due to hangs of maven. But, as we know, it's happen in our Jenkins
SparkPullRequestBuilder, too. I think we can ignore that.

I'm sure that this will save much more community's efforts on the static
errors by preventing them at the very early stage. But, there might be
another reason not to do this. I'm wondering about your thoughts.

I can make a Apache INFRA Jira issue for this if there is some consensus.

Warmly,
Dongjoon.
"
Ted Yu <yuzhihong@gmail.com>,"Sun, 22 May 2016 13:29:39 -0700",Re: Using Travis for JDK7/8 compilation and lint-java.,Dongjoon Hyun <dongjoon@apache.org>,"The following line was repeated twice:

- For Oracle JDK7, mvn -DskipTests install and run `dev/lint-java`.

Did you intend to cover JDK 8 ?

Cheers


"
Dongjoon Hyun <dongjoon@apache.org>,"Sun, 22 May 2016 13:30:32 -0700",Re: Using Travis for JDK7/8 compilation and lint-java.,Ted Yu <yuzhihong@gmail.com>,"Oh, Sure. My bad!

- For Oracle JDK7, mvn -DskipTests install and run `dev/lint-java`.
- For Oracle JDK8, mvn -DskipTests install and run `dev/lint-java`.

Thank you, Ted.

Dongjoon.


"
Matei Zaharia <matei.zaharia@gmail.com>,"Sun, 22 May 2016 20:34:01 -0400",[VOTE] Removing module maintainer process,dev <dev@spark.apache.org>,"It looks like the discussion thread on this has only had positive replies, so I'm going to call a VOTE. The proposal is to remove the maintainer process in https://cwiki.apache.org/confluence/display/SPARK/Committers#Committers-ReviewProcessandMaintainers <https://cwiki.apache.org/confluence/display/SPARK/Committers#Committers-ReviewProcessandMaintainers> given that it doesn't seem to have had a huge impact on the project, and it can unnecessarily create friction in contributing. We already have +1s from Mridul, Tom, Andrew Or and Imran on that thread.

I'll leave the VOTE open for 48 hours, until 9 PM EST on May 24, 2016.

Matei
---------------------------------------------------------------------


"
Luciano Resende <luckbr1975@gmail.com>,"Sun, 22 May 2016 18:54:00 -0700",Re: [VOTE] Removing module maintainer process,Matei Zaharia <matei.zaharia@gmail.com>,"
Thanks Matei, please note that a formal vote should generally be permitted
to run for at least 72 hours to provide an opportunity for all concerned
persons to participate regardless of their geographic locations.


http://www.apache.org/foundation/voting.html

Thank you
-- Luciano





-- 
Sent from my Mobile device
"
Sean Owen <sowen@cloudera.com>,"Sun, 22 May 2016 21:17:23 -0500",Re: [VOTE] Removing module maintainer process,Matei Zaharia <matei.zaharia@gmail.com>,"+1 (binding)


"
Matei Zaharia <matei.zaharia@gmail.com>,"Sun, 22 May 2016 22:18:57 -0400",Re: [VOTE] Removing module maintainer process,dev <dev@spark.apache.org>,"Correction, let's run this for 72 hours, so until 9 PM EST May 25th.

replies, so I'm going to call a VOTE. The proposal is to remove the maintainer process in https://cwiki.apache.org/confluence/display/SPARK/Committers#Committers-ReviewProcessandMaintainers <https://cwiki.apache.org/confluence/display/SPARK/Committers#Committers-ReviewProcessandMaintainers> given that it doesn't seem to have had a huge impact on the project, and it can unnecessarily create friction in contributing. We already have +1s from Mridul, Tom, Andrew Or and Imran on that thread.


---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Sun, 22 May 2016 20:32:31 -0700",Re: Using Travis for JDK7/8 compilation and lint-java.,Dongjoon Hyun <dongjoon@apache.org>,"Without Zinc, 'mvn -DskipTests clean install' takes ~30 minutes.

Maybe not everyone is willing to wait that long.


"
Dongjoon Hyun <dongjoon@apache.org>,"Sun, 22 May 2016 21:21:29 -0700",Re: Using Travis for JDK7/8 compilation and lint-java.,Ted Yu <yuzhihong@gmail.com>,"Thank you for feedback. Sure, correctly, that's the reason why the current
SparkPullRequestBuilder do not run `lint-java`. :-)

In addition, that's the same reason why contributors are reluctant to run
`lint-java` and causes breaking on JDK7 builds.

Such a tedious and time-consuming job should be done by CI without human
interventions.

By the way, why do you think we need to wait for that? We should not wait
for any CIs, we should continue our own work.

My proposal isn't for making you wait to watch the result. There are two
use cases I want for us to focus here.

Case 1: When you make a PR to Spark PR queue.

    Travis CI will finish before SparkPullRequestBuilder.
    We will run the followings in parallel mode.
         1. Current SparkPullRequestBuilder: JDK8 + sbt build + (no Java
Linter)
         2. Travis: JDK7 + mvn build + Java Linter
         3. Travis: JDK8 + mvn build + Java Linter
     As we know, 1 is the longest time-consuming one which have lots of
works (except maven building or lint-  java). You don't need to wait more
in many cases. Yes, in many cases, not all the cases.


Case 2: When you prepare a PR on your branch.

    If you are at the final commit (maybe already-squashed), just go to
case 1.

    However, usually, we makes lots of commits locally while making
preparing our PR.
    And, finally we squashed them into one and send a PR to Spark.
    I mean you can use Travis CI during preparing your PRs.
    Again, don't wait for Travis CI. Just push it sometime or at every
commit, and continue your work.

    At the final stage when you finish your coding, squash your commits
into one,
    and amend your commit title or messages, see the Travis CI.
    Or, you can monitor Travis CI result on status menu bar.
    If it shows green icon, you have nothing to do.

       https://docs.travis-ci.com/user/apps/

To sum up, I think we don't need to wait for any CIs. It's like an email.
`Send and back to work.`

Dongjoon.



"
Nick Pentreath <nick.pentreath@gmail.com>,"Mon, 23 May 2016 06:02:42 +0000",Re: [VOTE] Removing module maintainer process,"Matei Zaharia <matei.zaharia@gmail.com>, dev <dev@spark.apache.org>","+1 (binding)

"
Gurvinder Singh <gurvinder.singh@uninett.no>,"Mon, 23 May 2016 08:53:04 +0200",Re: spark on kubernetes,"dev@spark.apache.org, Reynold Xin <rxin@databricks.com>","Hi Reynold,

So if that's OK with you, can I go ahead and create JIRA for this. As it
seems this feature is missing currently and can benefit not just for
kubernetes users but in general Spark standalone mode users too.

- Gurvinder


---------------------------------------------------------------------


"
Ovidiu-Cristian MARCU <ovidiu-cristian.marcu@inria.fr>,"Mon, 23 May 2016 11:16:56 +0200",Building spark master failed,dev@spark.apache.org,"Hi

I have the following issue when trying to build the latest spark source code on master:

/spark/common/network-common/src/main/java/org/apache/spark/network/util/JavaUtils.java:147: error: cannot find symbol
[error]       if (process != null && process.isAlive()) {
[error]                                     ^
[error]   symbol:   method isAlive()
[error]   location: variable process of type Process
[error] 1 error
[error] Compile failed at May 23, 2016 11:13:58 AM [1.319s]

related to [INFO] Spark Project Networking ........................... FAILURE [  1.495 s]

Am I missing some fix?

Thanks

Best,
Ovidiu"
"""=?gb18030?B?wu3zVA==?="" <mabiaocsu@qq.com>","Mon, 23 May 2016 17:27:24 +0800",I will fix SPARK-15477 ,"""=?gb18030?B?ZGV2?="" <dev@spark.apache.org>","Hi
I will fix Spark 15477
Can you please assign the contributor permission to me?
I've start using spark since two years ago, and familiar with spark source code and coding style.


Warmest regards~
From: Biao Ma"
Dongjoon Hyun <dongjoon@apache.org>,"Mon, 23 May 2016 02:35:44 -0700",Re: Building spark master failed,Ovidiu-Cristian MARCU <ovidiu-cristian.marcu@inria.fr>,"Hi,

That is not the latest.

The bug was fixed 5 days ago.

Regards,
Dongjoon.



"
Ovidiu-Cristian MARCU <ovidiu-cristian.marcu@inria.fr>,"Mon, 23 May 2016 11:39:36 +0200",Re: Building spark master failed,Dongjoon Hyun <dongjoon@apache.org>,"Youâ€™re right, I tought latest will only compile against Java8.
Thanks
 
<ovidiu-cristian.marcu@inria.fr <mailto:ovidiu-cristian.marcu@inria.fr>> source code on master:
/spark/common/network-common/src/main/java/org/apache/spark/network/util/JavaUtils.java:147: error: cannot find symbol
FAILURE [  1.495 s]

"
Steve Loughran <stevel@hortonworks.com>,"Mon, 23 May 2016 09:54:34 +0000",Re: Using Travis for JDK7/8 compilation and lint-java.,Dongjoon Hyun <dongjoon@apache.org>,"

Thank you for feedback. Sure, correctly, that's the reason why the current SparkPullRequestBuilder do not run `lint-java`. :-)

In addition, that's the same reason why contributors are reluctant to run `lint-java` and causes breaking on JDK7 builds.

Such a tedious and time-consuming job should be done by CI without human interventions.

By the way, why do you think we need to wait for that? We should not wait for any CIs, we should continue our own work.


+1

Any time you spend waiting for tests to complete is time you could be doing useful things. I've had VMs running jenkins watching git branches do this in the past: every time I push



My proposal isn't for making you wait to watch the result. There are two use cases I want for us to focus here.

Case 1: When you make a PR to Spark PR queue.

    Travis CI will finish before SparkPullRequestBuilder.
    We will run the followings in parallel mode.
         1. Current SparkPullRequestBuilder: JDK8 + sbt build + (no Java Linter)
         2. Travis: JDK7 + mvn build + Java Linter
         3. Travis: JDK8 + mvn build + Java Linter
     As we know, 1 is the longest time-consuming one which have lots of works (except maven building or lint-  java). You don't need to wait more in many cases. Yes, in many cases, not all the cases.


Case 2: When you prepare a PR on your branch.

    If you are at the final commit (maybe already-squashed), just go to case 1.

    However, usually, we makes lots of commits locally while making preparing our PR.
    And, finally we squashed them into one and send a PR to Spark.
    I mean you can use Travis CI during preparing your PRs.
    Again, don't wait for Travis CI. Just push it sometime or at every commit, and continue your work.

    At the final stage when you finish your coding, squash your commits into one,
    and amend your commit title or messages, see the Travis CI.
    Or, you can monitor Travis CI result on status menu bar.
    If it shows green icon, you have nothing to do.

       https://docs.travis-ci.com/user/apps/

To sum up, I think we don't need to wait for any CIs. It's like an email. `Send and back to work.`



I'd add another, which is ""do a build and test of this patch while I get on with something else, that is, things which aren't ready for review, just the work you've done in the past hour or two which you'd like tested out
"
Hyukjin Kwon <gurwls223@gmail.com>,"Mon, 23 May 2016 19:35:15 +0900",Re: Using Travis for JDK7/8 compilation and lint-java.,Steve Loughran <stevel@hortonworks.com>,"+1 -  I wouldn't be bothered if a build becomes longer if I can write
cleaner codes without manually running it.

I have just looked though the related PRs and JIRAs and it looks generally
okay and reasonable to me.

2016-05-23 18:54 GMT+09:00 Steve Lough"
Sean Owen <sowen@cloudera.com>,"Mon, 23 May 2016 11:39:30 +0000",Re: I will fix SPARK-15477,"=?UTF-8?B?6ams6aqJ?= <mabiaocsu@qq.com>, dev <dev@spark.apache.org>","You don't need it assigned but I do not know if this is a valid change. If
in doubt try to get confirmation from someone familiar with the
implications that it is correct


e
"
Tom Graves <tgraves_cs@yahoo.com.INVALID>,"Mon, 23 May 2016 13:13:17 +0000 (UTC)",Re: [VOTE] Removing module maintainer process,"Matei Zaharia <matei.zaharia@gmail.com>, dev <dev@spark.apache.org>","+1 (binding)
Tom 

 

 It looks like the discussion thread on this has only had positive replies, so I'm going to call a VOTE. The proposal is to remove the maintainer process in https://cwiki.apache.org/confluence/display/SPARK/Committers#Committers-Revi"
Ted Yu <yuzhihong@gmail.com>,"Mon, 23 May 2016 06:32:22 -0700",Re: Using Travis for JDK7/8 compilation and lint-java.,Dongjoon Hyun <dongjoon@apache.org>,"Do you know if more than one PR would be verified on the same machine ?

I wonder whether the 'mvn install' from two simultaneous PR builds may have
conflict.


"
Imran Rashid <irashid@cloudera.com>,"Mon, 23 May 2016 10:22:50 -0500",Re: [VOTE] Removing module maintainer process,Tom Graves <tgraves_cs@yahoo.com>,"+1 (binding)


"
Ovidiu-Cristian MARCU <ovidiu-cristian.marcu@inria.fr>,"Mon, 23 May 2016 17:58:13 +0200",Running TPCDSQueryBenchmark results in java.lang.OutOfMemoryError,dev <dev@spark.apache.org>,"Hi

1) Using latest spark 2.0 I've managed to run TPCDSQueryBenchmark first 9 queries and then it ends in the OutOfMemoryError [1].

What was the configuration used for running this benchmark? Can you explain the meaning of 4 shuffle partitions? Thanks!

./bin/spark-submit --class org.apache.spark.sql.execution.benchmark.TPCDSQueryBenchmark --master local[4] jars/spark-sql_2.11-2.0.0-SNAPSHOT-tests.jar
configured with:
      .set(""spark.sql.parquet.compression.codec"", ""snappy"")
      .set(""spark.sql.shuffle.partitions"", ""4"")
      .set(""spark.driver.memory"", ""3g"")
      .set(""spark.executor.memory"", ""3g"")
      .set(""spark.sql.autoBroadcastJoinThreshold"", (20 * 1024 * 1024).toString)

Scale factor of TPCDS is 5, data generated using notes from https://github.com/databricks/spark-sql-perf <https://github.com/databricks/spark-sql-perf>.

2) Running spark-sql-perf with: val experiment = tpcds.runExperiment(tpcds.runnable) on the same dataset reveals some exceptions:

Running execution q9-v1.4 iteration: 1, StandardRun=true
java.lang.NullPointerException
	at org.apache.spark.sql.execution.ScalarSubquery.dataType(subquery.scala:45)
	at org.apache.spark.sql.catalyst.expressions.CaseWhenBase.dataType(conditionalExpressions.scala:103)
	at org.apache.spark.sql.catalyst.expressions.Alias.toAttribute(namedExpressions.scala:165)
	at org.apache.spark.sql.execution.ProjectExec$$anonfun$output$1.apply(basicPhysicalOperators.scala:33)
...	at org.apache.spark.sql.execution.ProjectExec.output(basicPhysicalOperators.scala:33)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.output(WholeStageCodegenExec.scala:289)
	at org.apache.spark.sql.execution.DeserializeToObject$$anonfun$2.apply(objects.scala:61)
	at org.apache.spark.sql.execution.DeserializeToObject$$anonfun$2.apply(objects.scala:60)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$23.apply(RDD.scala:774)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$23.apply(RDD.scala:774)

or

Running execution q25-v1.4 iteration: 1, StandardRun=true
java.lang.IllegalStateException: Task -1024 has already locked broadcast_755_piece0 for writing
	at org.apache.spark.storage.BlockInfoManager.lockForWriting(BlockInfoManager.scala:232)
	at org.apache.spark.storage.BlockManager.removeBlock(BlockManager.scala:1296)

Best,
Ovidiu

[1]
Exception in thread ""broadcast-exchange-164"" java.lang.OutOfMemoryError: Java heap space
	at org.apache.spark.sql.execution.joins.LongToUnsafeRowMap.append(HashedRelation.scala:539)
	at org.apache.spark.sql.execution.joins.LongHashedRelation$.apply(HashedRelation.scala:803)
	at org.apache.spark.sql.execution.joins.HashedRelation$.apply(HashedRelation.scala:105)
	at org.apache.spark.sql.execution.joins.HashedRelationBroadcastMode.transform(HashedRelation.scala:816)
	at org.apache.spark.sql.execution.joins.HashedRelationBroadcastMode.transform(HashedRelation.scala:812)
	at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:89)
	at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:71)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionId(SQLExecution.scala:94)
	at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:71)
	at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:71)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)"
Ted Yu <yuzhihong@gmail.com>,"Mon, 23 May 2016 09:16:30 -0700",Re: Running TPCDSQueryBenchmark results in java.lang.OutOfMemoryError,Ovidiu-Cristian MARCU <ovidiu-cristian.marcu@inria.fr>,"Can you tell us the commit hash using which the test was run ?

For #2, if you can give full stack trace, that would be nice.

Thanks


"
Dongjoon Hyun <dongjoon@apache.org>,"Mon, 23 May 2016 09:43:02 -0700",Re: Using Travis for JDK7/8 compilation and lint-java.,Ted Yu <yuzhihong@gmail.com>,"Thank you, Steve and Hyukjin.

And, don't worry, Ted.

Travis launches new VMs for every PR.

Apache Spark repository uses the following setting.

VM: Google Compute Engine
OS: Ubuntu 14.04.3 LTS Server Edition 64bit
CPU: ~2 CORE
RAM: 7.5GB

FYI, you can find more information about this here.

https://docs.travis-ci.com/user/ci-environment/#Virtualization-environments

Dongjoon.




"
Deepak Sharma <deepakmca05@gmail.com>,"Mon, 23 May 2016 22:35:42 +0530",How to map values read from test file to 2 different RDDs,dev@spark.apache.org,"Hi
I am reading a text file with 16 fields.
All the place holders for the values of this text file has been defined in
say 2 different case classes:
Case1 and Case2

How do i map values read from text file , so my function in scala should be
able to return 2 different RDDs , with each each RDD of these 2 different
cse class type?
E.g first 11 fields mapped to Case1 while rest 6 fields mapped to Case2
Any pointer here or code snippet would be really helpful.


-- 
Thanks
Deepak
"
Timothy Chen <tnachen@gmail.com>,"Mon, 23 May 2016 10:14:39 -0700",Re: spark on kubernetes,Gurvinder Singh <gurvinder.singh@uninett.no>,"This will also simplify Mesos users as well, DCOS has to work around
this with our own proxying.

Tim


---------------------------------------------------------------------


"
Ovidiu-Cristian MARCU <ovidiu-cristian.marcu@inria.fr>,"Mon, 23 May 2016 19:16:02 +0200",Re: Running TPCDSQueryBenchmark results in java.lang.OutOfMemoryError,Ted Yu <yuzhihong@gmail.com>,"Yes,

git log
commit dafcb05c2ef8e09f45edfb7eabf58116c23975a0
Author: Sameer Agarwal <sameer@databricks.com>
Date:   Sun May 22 23:32:39 2016 -0700

for #2 see my comments in https://issues.apache.org/jira/browse/SPARK-15078 <https://issues.apache.org/jira/browse/SPARK-15078>

<ovidiu-cristian.marcu@inria.fr <mailto:ovidiu-cristian.marcu@inria.fr>> first 9 queries and then it ends in the OutOfMemoryError [1].
explain the meaning of 4 shuffle partitions? Thanks!
org.apache.spark.sql.execution.benchmark.TPCDSQueryBenchmark --master local[4] jars/spark-sql_2.11-2.0.0-SNAPSHOT-tests.jar
1024).toString)
https://github.com/databricks/spark-sql-perf <https://github.com/databricks/spark-sql-perf>.
tpcds.runExperiment(tpcds.runnable) on the same dataset reveals some exceptions:
org.apache.spark.sql.execution.ScalarSubquery.dataType(subquery.scala:45)
org.apache.spark.sql.catalyst.expressions.CaseWhenBase.dataType(conditionalExpressions.scala:103)
org.apache.spark.sql.catalyst.expressions.Alias.toAttribute(namedExpressions.scala:165)
org.apache.spark.sql.execution.ProjectExec$$anonfun$output$1.apply(basicPhysicalOperators.scala:33)
org.apache.spark.sql.execution.ProjectExec.output(basicPhysicalOperators.scala:33)
org.apache.spark.sql.execution.WholeStageCodegenExec.output(WholeStageCodegenExec.scala:289)
org.apache.spark.sql.execution.DeserializeToObject$$anonfun$2.apply(objects.scala:61)
org.apache.spark.sql.execution.DeserializeToObject$$anonfun$2.apply(objects.scala:60)
org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$23.apply(RDD.scala:774)
org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$23.apply(RDD.scala:774)
broadcast_755_piece0 for writing
org.apache.spark.storage.BlockInfoManager.lockForWriting(BlockInfoManager.scala:232)
org.apache.spark.storage.BlockManager.removeBlock(BlockManager.scala:1296)
java.lang.OutOfMemoryError: Java heap space
org.apache.spark.sql.execution.joins.LongToUnsafeRowMap.append(HashedRelation.scala:539)
org.apache.spark.sql.execution.joins.LongHashedRelation$.apply(HashedRelation.scala:803)
org.apache.spark.sql.execution.joins.HashedRelation$.apply(HashedRelation.scala:105)
org.apache.spark.sql.execution.joins.HashedRelationBroadcastMode.transform(HashedRelation.scala:816)
org.apache.spark.sql.execution.joins.HashedRelationBroadcastMode.transform(HashedRelation.scala:812)
org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:89)
org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:71)
org.apache.spark.sql.execution.SQLExecution$.withExecutionId(SQLExecution.scala:94)
org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:71)
org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:71)
scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

"
Radoslaw Gruchalski <radek@gruchalski.com>,"Mon, 23 May 2016 19:18:30 +0200",Re: spark on kubernetes,"Gurvinder Singh <gurvinder.singh@uninett.no>, Timothy Chen
 <tnachen@gmail.com>","Sounds surprisingly close to this:
https://github.com/apache/spark/pull/9608

I can ressurect the work on the bridge mode for Spark 2. The reason why the work on the old one was suspended was because Spark was going through so many changes at that time that a lot of work done, was wiped out by the changes towards 2.0.

I know that Lightbend was also interested in having bridge mode.
â€“  
Best regards,â€¨
Radek Gruchalski
â€¨radek@gruchalski.com
de.linkedin.com/in/radgruchalski

Confidentiality:
This communication is intended for the above-named person and may be confidential and/or legally privileged.
If it has come to you in error you must take no action based on it, nor must you copy or show it to anyone; please delete/destroy and inform the sender immediately.


This will also simplify Mesos users as well, DCOS has to work around  
this with our own proxying.  

Tim  

t  
er reverse proxy, would that solve your issue?  
  
er  
o  
e  
?  
ring  
 
t is  
ting  
om  
 I  
  
to  
m too  
ard  
ess  
ith  
,  
n  
l be  
  
t  
--  
-  
  
 

---------------------------------------------------------------------  
For additional commands, e-mail: dev-help@spark.apache.org  

"
Gurvinder Singh <gurvinder.singh@uninett.no>,"Mon, 23 May 2016 19:55:39 +0200",Re: spark on kubernetes,"Radoslaw Gruchalski <radek@gruchalski.com>,
 Timothy Chen <tnachen@gmail.com>","I might have overlooked it but bridge mode work appears to make Spark
work with docker containers and able to communicate with them when
running on more than one machines.

Here I am trying to enable getting information from Spark UI
irrespective of Spark running in containers or not. Spark UI's link to
workers and application drivers are pointing to internal/protected
network. So to get this information from user's machine, he/she has to
connect to VPN. Therefore the proposal is to make Spark master UI
reverse proxy this information back to user. So only Spark master UI
needs to be opened up to internet and there is no need to change
anything else how Spark runs internally either in Standalone mode, Mesos
or in containers on kubernetes.

- Gurvinder
> Best regards,â€¨
> â€¨radek@gruchalski.com <mailto:radek@gruchalski.com>


---------------------------------------------------------------------


"
Holden Karau <holden@pigscanfly.ca>,"Mon, 23 May 2016 11:18:31 -0700",Re: [VOTE] Removing module maintainer process,Imran Rashid <irashid@cloudera.com>,"+1 non-binding (as a contributor anything which speed things up is worth a
try, and git blame is a good enough substitute for the list when figuring
out who to ping on a PR).



-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
Gurvinder Singh <gurvinder.singh@uninett.no>,"Mon, 23 May 2016 20:36:00 +0200",Re: spark on kubernetes,"Radoslaw Gruchalski <radek@gruchalski.com>,
 Timothy Chen <tnachen@gmail.com>","OK created this issue https://issues.apache.org/jira/browse/SPARK-15487
please comment on this and also let me know if anyone want to
collaborate on implementing it. Its my first contribution to Spark so
will be exciting.

- Gurvinder
>> Best regards,â€¨
>> â€¨radek@gruchalski.com <mailto:radek@gruchalski.com>


---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Mon, 23 May 2016 12:20:50 -0700",Re: Using Travis for JDK7/8 compilation and lint-java.,Dongjoon Hyun <dongjoon@apache.org>,"We did turn on travis a few years ago, but ended up turning it off because
it was failing (I believe because of insufficient resources) which was
confusing for developers.  I wouldn't be opposed to turning it on if it
provides more/faster signal, but its not obvious to me that it would.  In
particular, do we know that given the rate PRs are created if we will hit
rate limits?

Really my main feedback is, if the java linter is important we should
probably have it as part of the canonical build process.  I worry about
having more than one set of CI infrastructure to maintain.


"
shane knapp <sknapp@berkeley.edu>,"Mon, 23 May 2016 13:16:59 -0700",Re: Using Travis for JDK7/8 compilation and lint-java.,Michael Armbrust <michael@databricks.com>,"chiming in, as i'm the one who currently maintains the CI infrastructure...  :)

+1 on not having more than one CI system...  there's no way i can
commit to keeping an eye on anything else other than jenkins.

and i agree wholeheartedly w/michael:  if it's this important, let's
add it to the jenkins builds.


---------------------------------------------------------------------


"
Dongjoon Hyun <dongjoon@apache.org>,"Mon, 23 May 2016 13:24:16 -0700",Re: Using Travis for JDK7/8 compilation and lint-java.,Michael Armbrust <michael@databricks.com>," Thank you for your opinion!

Sure. I know that history and totally agree with all your concerns.
I indeed has hesitated about sending this kind of suggestion for a while.

If Travis CI cannot handle those simple jobs at this time again,
we must turn off from Spark PR queue.
We can see the result quickly in one or two days.
To turn on/off, Spark have nothing to do. INFRA team will do that.

In fact, the goal is not about using another CI (like Travis), it is about
preventing the followings.

1. JDK7 compilation errors. (Recently, 2 days ago and 5 days ago)
2. Java static errors. (Not critical but more frequently.)
3. Maven installation errors. (A month ago, it's reported in this mailing
list.)

Scala 2.10 compilation errors are fixed nearly instantly. But, 1~3 were not.
If SparkPullRequestBuilder can do the above 1~3, that's the best for us.
Do you think it is possible in some ways?

By the way, as of today, Spark has 724 Java files and 96762 lines (without
comment/blank).
It's about 1/3 of Scala code. It's not small.
--------------------------------------------------------------------------
Language                  files          blank        comment         code
--------------------------------------------------------------------------
Scala                      2368          63578         124904       322518
Java                        724          18569          23445
96762

Dongjoon.




"
Dongjoon Hyun <dongjoon@apache.org>,"Mon, 23 May 2016 13:26:14 -0700",Re: Using Travis for JDK7/8 compilation and lint-java.,Michael Armbrust <michael@databricks.com>,"Thank you, Shane!

I really hope that SparkPullRequestBuilder handle them if possible.

Dongjoon.


"
Dongjoon Hyun <dongjoon@apache.org>,"Mon, 23 May 2016 13:43:51 -0700",Re: Using Travis for JDK7/8 compilation and lint-java.,Michael Armbrust <michael@databricks.com>,"I want to clarify something here.

For Travis CI, it's free for open source projects and there is only one
management point, `.travis.xml`, for Spark community.

It's not some like physical Jenkins cluster farm. It's just a cloud service
like Github.

PS.
I'm also not an employee of Travis(or Github). :-)
If Spark uses Travis CI freely, they might dislike me for the heavy traffic.



"
Ted Yu <yuzhihong@gmail.com>,"Mon, 23 May 2016 13:57:52 -0700",Re: Using Travis for JDK7/8 compilation and lint-java.,Dongjoon Hyun <dongjoon@apache.org>,"For #1 below, currently Jenkins uses Java 8:

JAVA_HOME=/usr/java/jdk1.8.0_60


How about switching to Java 7 ?


My two cents.



"
Sean Owen <sowen@cloudera.com>,"Mon, 23 May 2016 21:09:36 +0000",Re: Using Travis for JDK7/8 compilation and lint-java.,"Ted Yu <yuzhihong@gmail.com>, Dongjoon Hyun <dongjoon@apache.org>","No, because then none of the Java 8 support can build. Marcelo has a JIRA
for handling that the right way with bootstrap class path config.

Ideally it can be rolled into Jenkins though there are possibly historical
reasons it was not enabled before. Best to fix those if possible but if not
I'd rather have some automated checking than none. Checking lint is
reasonably important.


"
Dongjoon Hyun <dongjoon@apache.org>,"Mon, 23 May 2016 15:12:21 -0700",Re: Using Travis for JDK7/8 compilation and lint-java.,Sean Owen <sowen@cloudera.com>,"Thank you, Sean!


"
Sachin Janani <sjanani@snappydata.io>,"Tue, 24 May 2016 12:12:59 +0530",Issue with Spark Streaming UI,dev@spark.apache.org,"Hi,
I'm trying to run a simple spark streaming application with File Streaming
and its working properly but when I try to monitor the number of events in
the Streaming Ui it shows that as 0.Is this a issue and are there any plans
to fix this.Attached is the screenshot of what it shows on the UI.



Regards,
Sachin Janani

---------------------------------------------------------------------"
Saisai Shao <sai.sai.shao@gmail.com>,"Tue, 24 May 2016 14:58:06 +0800",Re: Issue with Spark Streaming UI,Sachin Janani <sjanani@snappydata.io>,"I think it is by design FileInputDStream doesn't support report info,
because FileInputDStream doesn't have event/record concept (it is file
based), so it is hard to define how to correctly report the input info.

Current input info reporting can be supported for all receiver based
InputDStream and DirectKafkaInputDStream.


"
Zhan Zhang <zhazhan@gmail.com>,"Tue, 24 May 2016 00:10:16 -0700 (MST)",Re: right outer joins on Datasets,dev@spark.apache.org,"The reason for ""-1"" is that the default value for Integer is -1 if the value
is null

  def defaultValue(jt: String): String = jt match {
    ...
    case JAVA_INT => ""-1""
    ...   
 }



--

---------------------------------------------------------------------


"
Ovidiu-Cristian MARCU <ovidiu-cristian.marcu@inria.fr>,"Tue, 24 May 2016 12:12:28 +0200",Re: Running TPCDSQueryBenchmark results in java.lang.OutOfMemoryError,Ted Yu <yuzhihong@gmail.com>,"Do you need more information?


<mailto:sameer@databricks.com>>
https://issues.apache.org/jira/browse/SPARK-15078 <https://issues.apache.org/jira/browse/SPARK-15078>
<ovidiu-cristian.marcu@inria.fr <mailto:ovidiu-cristian.marcu@inria.fr>> first 9 queries and then it ends in the OutOfMemoryError [1].
explain the meaning of 4 shuffle partitions? Thanks!
org.apache.spark.sql.execution.benchmark.TPCDSQueryBenchmark --master local[4] jars/spark-sql_2.11-2.0.0-SNAPSHOT-tests.jar
1024).toString)
https://github.com/databricks/spark-sql-perf <https://github.com/databricks/spark-sql-perf>.
tpcds.runExperiment(tpcds.runnable) on the same dataset reveals some exceptions:
org.apache.spark.sql.execution.ScalarSubquery.dataType(subquery.scala:45)
org.apache.spark.sql.catalyst.expressions.CaseWhenBase.dataType(conditionalExpressions.scala:103)
org.apache.spark.sql.catalyst.expressions.Alias.toAttribute(namedExpressions.scala:165)
org.apache.spark.sql.execution.ProjectExec$$anonfun$output$1.apply(basicPhysicalOperators.scala:33)
org.apache.spark.sql.execution.ProjectExec.output(basicPhysicalOperators.scala:33)
org.apache.spark.sql.execution.WholeStageCodegenExec.output(WholeStageCodegenExec.scala:289)
org.apache.spark.sql.execution.DeserializeToObject$$anonfun$2.apply(objects.scala:61)
org.apache.spark.sql.execution.DeserializeToObject$$anonfun$2.apply(objects.scala:60)
org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$23.apply(RDD.scala:774)
org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$23.apply(RDD.scala:774)
broadcast_755_piece0 for writing
org.apache.spark.storage.BlockInfoManager.lockForWriting(BlockInfoManager.scala:232)
org.apache.spark.storage.BlockManager.removeBlock(BlockManager.scala:1296)
java.lang.OutOfMemoryError: Java heap space
org.apache.spark.sql.execution.joins.LongToUnsafeRowMap.append(HashedRelation.scala:539)
org.apache.spark.sql.execution.joins.LongHashedRelation$.apply(HashedRelation.scala:803)
org.apache.spark.sql.execution.joins.HashedRelation$.apply(HashedRelation.scala:105)
org.apache.spark.sql.execution.joins.HashedRelationBroadcastMode.transform(HashedRelation.scala:816)
org.apache.spark.sql.execution.joins.HashedRelationBroadcastMode.transform(HashedRelation.scala:812)
org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:89)
org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:71)
org.apache.spark.sql.execution.SQLExecution$.withExecutionId(SQLExecution.scala:94)
org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:71)
org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:71)
scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

"
Koert Kuipers <koert@tresata.com>,"Tue, 24 May 2016 10:31:29 -0400",Re: right outer joins on Datasets,Zhan Zhang <zhazhan@gmail.com>,"got it, but i assume thats an internal implementation detail, and it should
show null not -1?


"
Koert Kuipers <koert@tresata.com>,"Tue, 24 May 2016 11:33:29 -0400",ClassCastException: SomeCaseClass cannot be cast to org.apache.spark.sql.Row,"""dev@spark.apache.org"" <dev@spark.apache.org>","hello,
as we continue to test spark 2.0 SNAPSHOT in-house we ran into the
following trying to port an existing application from spark 1.6.1 to spark
2.0.0-SNAPSHOT.

given this code:

case class Test(a: Int, b: String)
val rdd = sc.parallelize(List(Row(List(Test(5, ""ha""), Test(6, ""ba"")))))
val schema = StructType(Seq(
  StructField(""x"", ArrayType(
    StructType(Seq(
      StructField(""a"", IntegerType, false),
      StructField(""b"", StringType, true)
    )),
    true)
  , true)
  ))
val df = sqlc.createDataFrame(rdd, schema)
df.show

this works fine in spark 1.6.1 and gives:

+----------------+
|               x|
+----------------+
|[[5,ha], [6,ba]]|
+----------------+

but in spark 2.0.0-SNAPSHOT i get:

org.apache.spark.SparkException: Job aborted due to stage failure: Task 0
in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage
0.0 (TID 0, localhost): java.lang.RuntimeException: Error while encoding:
java.lang.ClassCastException: Test cannot be cast to
org.apache.spark.sql.Row
[info] getexternalrowfield(input[0, org.apache.spark.sql.Row, false], 0, x,
IntegerType) AS x#0
[info] +- getexternalrowfield(input[0, org.apache.spark.sql.Row, false], 0,
x, IntegerType)
[info]    +- input[0, org.apache.spark.sql.Row, false]
"
Zhan <zhazhan@gmail.com>,"Tue, 24 May 2016 08:40:24 -0700",Re: right outer joins on Datasets,Koert Kuipers <koert@tresata.com>,"The first item as a whole should be null please refer to the jira.


Sent from my iPhone

d show null not -1?
lue
.n3.nabble.com/right-outer-joins-on-Datasets-tp17542p17651.html
.com.
"
Ted Yu <yuzhihong@gmail.com>,"Tue, 24 May 2016 09:21:40 -0700",Re: ClassCastException: SomeCaseClass cannot be cast to org.apache.spark.sql.Row,Koert Kuipers <koert@tresata.com>,"Please log a JIRA.

Thanks


"
"""Owen O'Malley"" <omalley@apache.org>","Tue, 24 May 2016 09:27:33 -0700",Re: [VOTE] Removing module maintainer process,dev <dev@spark.apache.org>,"+1 (non-binding)

I think this is an important step to improve Spark as an Apache project.

.. Owen


"
Koert Kuipers <koert@tresata.com>,"Tue, 24 May 2016 12:27:49 -0400",Re: ClassCastException: SomeCaseClass cannot be cast to org.apache.spark.sql.Row,Ted Yu <yuzhihong@gmail.com>,"https://issues.apache.org/jira/browse/SPARK-15507


"
Reynold Xin <rxin@databricks.com>,"Tue, 24 May 2016 10:15:35 -0700",Re: ClassCastException: SomeCaseClass cannot be cast to org.apache.spark.sql.Row,Koert Kuipers <koert@tresata.com>,"Thanks, Koert. This is great. Please keep them coming.



"
Dongjoon Hyun <dongjoon@apache.org>,"Tue, 24 May 2016 10:25:52 -0700",Re: Using Travis for JDK7/8 compilation and lint-java.,Sean Owen <sowen@cloudera.com>,"Hi, All.

As Sean said, Vanzin made a PR for JDK7 compilation. We can ignore the
issue of JDK7 compilation.

The remaining issues are the java-linter and maven installation test.

To: Michael
For the rate limit, Apache Foundation seems to use 30 concurrent according
to the INFRA blog.

https://blogs.apache.org/infra/

However, this does not include the Travis CI queue in personal users.
We have over 800 github registered contributors.
They can run Travis CI on their branch without this limitation like me.

To: Shane
For Travis CI, I volunteer for turning on/off and taking care about
complains during that test period.

I hope these two helps you make a positive decision on Travis CI test drive.
Any other potential issues we should consider?

Dongjoon.
"
shane knapp <sknapp@berkeley.edu>,"Tue, 24 May 2016 12:15:48 -0700",Re: Using Travis for JDK7/8 compilation and lint-java.,Dongjoon Hyun <dongjoon@apache.org>,"vanzin and i are working together on this right now...  we currently
have java 7u79 installed on all of the workers.  if some random test
failures keep happening during his tests, i will roll out 7u80 (which
is known to be 'good') and set that as the default java on all of the
workers (which is currently 7-79).

if this is the route we decide to go, could you please update the qa
infra wiki entry and add a relevant section for the travis setup?

https://cwiki.apache.org/confluence/display/SPARK/Spark+QA+Infrastructure

thanks,

shane

---------------------------------------------------------------------


"
Dongjoon Hyun <dongjoon@apache.org>,"Tue, 24 May 2016 15:02:48 -0700",Re: Using Travis for JDK7/8 compilation and lint-java.,shane knapp <sknapp@berkeley.edu>,"Thank you, Shane.

Sure, could you give me the permission for Spark Jira?

Although we haven't decided yet, I can add Travis related section
(summarizing current configurations and expected VM HW, etc).

That will be helpful for further discussions.

It's just a Wiki, you can delete the Travis Section anytime.

Dongjoon.



"
shane knapp <sknapp@berkeley.edu>,"Tue, 24 May 2016 15:45:44 -0700",Re: Using Travis for JDK7/8 compilation and lint-java.,Dongjoon Hyun <dongjoon@apache.org>,"i can't give you permissions -- that has to be (most likely) through
someone @ databricks, like michael.

let's hold off on adding a section until we actually decide that this
is critical and something that cannot be done currently w/jenkins.

ayup.

---------------------------------------------------------------------


"
Dongjoon Hyun <dongjoon@apache.org>,"Tue, 24 May 2016 15:53:20 -0700",Re: Using Travis for JDK7/8 compilation and lint-java.,shane knapp <sknapp@berkeley.edu>,"Yep. Let's hold on. :)


"
Michael Armbrust <michael@databricks.com>,"Tue, 24 May 2016 17:46:44 -0700",Re: Using Travis for JDK7/8 compilation and lint-java.,shane knapp <sknapp@berkeley.edu>,"
Another clarification: not databricks, but the Apache Spark PMC grants
access to the JIRA / wiki.  That said... I'm not actually sure how its done.
"
shane knapp <sknapp@berkeley.edu>,"Tue, 24 May 2016 19:23:28 -0700",Re: Using Travis for JDK7/8 compilation and lint-java.,Michael Armbrust <michael@databricks.com>,"
word.  i'll make the changes if we need to.

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Tue, 24 May 2016 23:44:36 -0700",[ANNOUNCE] Apache Spark 2.0.0-preview release,"""dev@spark.apache.org"" <dev@spark.apache.org>","In the past the Spark community have created preview packages (not official
releases) and used those as opportunities to ask community members to test
the upcoming versions of Apache Spark. Several people in the Apache
community have suggested we conduct votes for these preview packages and
turn them into formal releases by the Apache foundation's standard. This is
a result of that.

Note that this preview release should contain almost all the new features
that will be in Apache Spark 2.0.0. However, it is not meant to be
functional, i.e. the preview release contain critical bugs and
documentation errors. To download, please see the bottom of this web page:
http://spark.apache.org/downloads.html

For the list of known issues, please see
https://issues.apache.org/jira/browse/SPARK-15520?jql=project%20%3D%20SPARK%20AND%20status%20in%20(Open%2C%20%22In%20Progress%22%2C%20Reopened)%20AND%20%22Target%20Version%2Fs%22%20%3D%202.0.0


all the files are propagated to all mirrors, I will update the link to link
to the mirror selector instead.

Note 2: This is the first time we are publishing official, voted preview
releases. Would love to hear feedback.
"
Priya Ch <learnings.chitturi@gmail.com>,"Wed, 25 May 2016 12:35:12 +0530",Cartesian join on RDDs taking too much time,"""user@spark.apache.org"" <user@spark.apache.org>, dev@spark.apache.org","Hi All,

  I have two RDDs A and B where in A is of size 30 MB and B is of size 7
MB, A.cartesian(B) is taking too much time. Is there any bottleneck in
cartesian operation ?

I am using spark 1.6.0 version

Regards,
Padma Ch
"
Daniel Darabos <daniel.darabos@lynxanalytics.com>,"Wed, 25 May 2016 12:13:32 +0200",Re: [ANNOUNCE] Apache Spark 2.0.0-preview release,Reynold Xin <rxin@databricks.com>,"Awesome, thanks! It's very helpful for preparing for the migration. Do you
plan to push 2.0.0-preview to Maven too? (I for one would appreciate the
convenience.)


"
Yiannis Gkoufas <johngouf85@gmail.com>,"Wed, 25 May 2016 14:12:37 +0100",Cannot build master with sbt,dev@spark.apache.org,"Hi there,

I have cloned the latest version from github.
I am using scala 2.10.x
When I invoke

build/sbt clean package

I get the exceptions because for the sbt-antlr library:

[warn]     module not found: com.simplytyped#sbt-antlr4;0.7.10
[warn] ==== typesafe-ivy-releases: tried
[warn]
https://repo.typesafe.com/typesafe/ivy-releases/com.simplytyped/sbt-antlr4/scala_2.10/sbt_0.13/0.7.10/ivys/ivy.xml
[warn] ==== sbt-plugin-releases: tried
[warn]
https://repo.scala-sbt.org/scalasbt/sbt-plugin-releases/com.simplytyped/sbt-antlr4/scala_2.10/sbt_0.13/0.7.10/ivys/ivy.xml
[warn] ==== local: tried
[warn]
/home/johngouf/.ivy2/local/com.simplytyped/sbt-antlr4/scala_2.10/sbt_0.13/0.7.10/ivys/ivy.xml
[warn] ==== public: tried
[warn]
https://repo1.maven.org/maven2/com/simplytyped/sbt-antlr4_2.10_0.13/0.7.10/sbt-antlr4-0.7.10.pom
[warn] ==== simplytyped: tried
[warn]
http://simplytyped.github.io/repo/releases/com/simplytyped/sbt-antlr4_2.10_0.13/0.7.10/sbt-antlr4-0.7.10.pom
[info] Resolving org.fusesource.jansi#jansi;1.4 ...
[warn]     ::::::::::::::::::::::::::::::::::::::::::::::
[warn]     ::          UNRESOLVED DEPENDENCIES         ::
[warn]     ::::::::::::::::::::::::::::::::::::::::::::::
[warn]     :: com.simplytyped#sbt-antlr4;0.7.10: not found
[warn]     ::::::::::::::::::::::::::::::::::::::::::::::
[warn]
[warn]     Note: Some unresolved dependencies have extra attributes.  Check
that these dependencies exist with the requested attributes.
[warn]         com.simplytyped:sbt-antlr4:0.7.10 (scalaVersion=2.10,
sbtVersion=0.13)
[warn]
[warn]     Note: Unresolved dependencies path:
[warn]         com.simplytyped:sbt-antlr4:0.7.10 (scalaVersion=2.10,
sbtVersion=0.13) (/home/johngouf/IOT/spark/project/plugins.sbt#L26-27)
[warn]           +- plugins:plugins:0.1-SNAPSHOT (scalaVersion=2.10,
sbtVersion=0.13)
sbt.ResolveException: unresolved dependency:
com.simplytyped#sbt-antlr4;0.7.10: not found

Any idea what is the problem here?

Thanks!
"
Nick Pentreath <nick.pentreath@gmail.com>,"Wed, 25 May 2016 13:17:11 +0000",Re: Cannot build master with sbt,"Yiannis Gkoufas <johngouf85@gmail.com>, dev@spark.apache.org","I've filed https://issues.apache.org/jira/browse/SPARK-15525

For now, you would have to check out sbt-antlr4 at
https://github.com/ihji/sbt-antlr4/commit/23eab68b392681a7a09f6766850785afe8dfa53d
(since
I don't see any branches or tags in the github repo for different
versions), and sbt publishLocal to get the dependency locally.


"
Max Sperlich <max.sperlich@gmail.com>,"Wed, 25 May 2016 09:51:33 -0400",Re: Cartesian join on RDDs taking too much time,Priya Ch <learnings.chitturi@gmail.com>,"Cartesian joins tend to give a huge result size, and are inherently slow.
If RDD B has N records then your result size will be at least N * 30 MB,
since you have to replicate all the rows of A for a single record in B.

Assuming RDD B has 10,000 records then you can see that your cartesian join
will give an RDD that takes at least 300 GB, presumably more than the RAM
on your system...


"
Marcin Tustin <mtustin@handybook.com>,"Wed, 25 May 2016 09:53:28 -0400",Re: [ANNOUNCE] Apache Spark 2.0.0-preview release,Reynold Xin <rxin@databricks.com>,"Would it be useful to start baking docker images? Would anyone find that a
boon to their testing?



-- 
Want to work at Handy? Check out our culture deck and open roles 
<http://www.handy.com/careers>
Latest news <http://www.handy.com/press> at Handy
Handy just raised $50m 
<http://venturebeat.com/2015/11/02/on-demand-home-service-handy-raises-50m-in-round-led-by-fidelity/> led 
by Fidelity

"
Yiannis Gkoufas <johngouf85@gmail.com>,"Wed, 25 May 2016 16:26:15 +0100",Re: Cannot build master with sbt,Nick Pentreath <nick.pentreath@gmail.com>,"Thanks so much for the workaround!


"
Reynold Xin <rxin@databricks.com>,"Wed, 25 May 2016 08:30:53 -0700",Re: [ANNOUNCE] Apache Spark 2.0.0-preview release,Daniel Darabos <daniel.darabos@lynxanalytics.com>,"Yup I have published it to maven. Will post the link in a bit.

snapshot because that one probably has fewer bugs than the preview one.


"
Scott walent <scottwalent@gmail.com>,"Wed, 25 May 2016 18:18:09 +0000",The 7th and Largest Spark Summit is less than 2 weeks away!,"""user@spark.apache.org"" <user@spark.apache.org>, ""dev@spark.apache.org"" <dev@spark.apache.org>","*With every Spark Summit, an Apache Spark Community event, increasing
numbers of users and developers attend. This is the seventh Summit, and
whether you believe that â€œSevenâ€ is the worldâ€™s most popular number, we are
offering a special promo code* for all Apache Spark users and developers on
this list: SparkSummit7*


*Register (http://spark-summit.org/2016 <http://spark-summit.org/2016>)
between now and May 31st with this promo code and get 25% off.**V*alid only
for new registrations.*
"
Reynold Xin <rxin@databricks.com>,"Wed, 25 May 2016 12:08:49 -0700",Re: [ANNOUNCE] Apache Spark 2.0.0-preview release,Daniel Darabos <daniel.darabos@lynxanalytics.com>,"The maven artifacts can be found at

https://repository.apache.org/content/repositories/orgapachespark-1182/

But really for people on this list, it might be better to go straight to
the nightly snapshots.
https://repository.apache.org/content/groups/snapshots/org/apache/spark/


"
Jacek Laskowski <jacek@japila.pl>,"Wed, 25 May 2016 21:59:35 +0200",LiveListenerBus with started and stopped flags? Why both?,dev <dev@spark.apache.org>,"Hi,

I'm wondering why LiveListenerBus has two AtomicBoolean flags [1]?
Could it not have just one, say started? Why does Spark have to check
the stopped state?

[1] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/LiveListenerBus.scala#L49-L51

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Luciano Resende <luckbr1975@gmail.com>,"Wed, 25 May 2016 13:12:56 -0700",Re: [ANNOUNCE] Apache Spark 2.0.0-preview release,Marcin Tustin <mtustin@handybook.com>,"
+1, I had done one (still based on 1.6) for some SystemML experiments, I
could easily get it based on a nightly build.

https://github.com/lresende/docker-spark

every week ? I could see if I can automate the build + publish in a CI job
at one of our Jenkins servers (Apache or something)...



-- 
Luciano Resende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
Reynold Xin <rxin@databricks.com>,"Wed, 25 May 2016 13:20:21 -0700",Re: feedback on dataset api explode,Cheng Lian <lian@databricks.com>,"Created JIRA ticket: https://issues.apache.org/jira/browse/SPARK-15533

you send api feedbacks to the dev@ list instead of user@?




"
Koert Kuipers <koert@tresata.com>,"Wed, 25 May 2016 16:25:06 -0400",Re: feedback on dataset api explode,Reynold Xin <rxin@databricks.com>,"oh yes, this was by accident, it should have gone to dev


"
Marcin Tustin <mtustin@handybook.com>,"Wed, 25 May 2016 16:41:16 -0400",Re: [ANNOUNCE] Apache Spark 2.0.0-preview release,Luciano Resende <luckbr1975@gmail.com>,"Ah very nice. Would it be possible to have this blessed into an official
image?



-- 
Want to work at Handy? Check out our culture deck and open roles 
<http://www.handy.com/careers>
Latest news <http://www.handy.com/press> at Handy
Handy just raised $50m 
<http://venturebeat.com/2015/11/02/on-demand-home-service-handy-raises-50m-in-round-led-by-fidelity/> led 
by Fidelity

"
Luciano Resende <luckbr1975@gmail.com>,"Wed, 25 May 2016 13:48:00 -0700",Labeling Jiras,dev <dev@spark.apache.org>,"I recently used labels to mark couple jiras that me and my team have some
interest on them, so it's easier to share a query and check the status on
them. But I noticed that these labels were removed.

Are there any issues with labeling jiras ? Any other suggestions ?



-- 
Luciano Resende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
Sean Owen <sowen@cloudera.com>,"Wed, 25 May 2016 16:34:25 -0500",Re: [ANNOUNCE] Apache Spark 2.0.0-preview release,Marcin Tustin <mtustin@handybook.com>,"I don't think the project would bless anything but the standard
release artifacts since only those are voted on. People are free to
maintain whatever they like and even share it, as long as it's clear
it's not from the Apache project.


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Wed, 25 May 2016 16:33:18 -0500",Re: Labeling Jiras,Luciano Resende <luckbr1975@gmail.com>,"I don't think we generally use labels at all except ""starter"". I
sometimes remove labels when I'm editing a JIRA otherwise, perhaps to
make that point. I don't recall doing this recently.

However I'd say they should not be used to tag JIRAs for your internal
purposes. Have you looked at things like JIRA Client from Almworks?
It's free and I highly recommend it, and IIRC it lets you manage some
private labels locally.


---------------------------------------------------------------------


"
Marcin Tustin <mtustin@handybook.com>,"Wed, 25 May 2016 17:44:57 -0400",Spark docker image - does that sound useful?,"Sean Owen <sowen@cloudera.com>, user@spark.apache.org","Makes sense, but then let me ask a different question: if there's demand,
should the project brew up its own release version in docker format?

I've copied this to the user list to see if there's any demand.



-- 
Want to work at Handy? Check out our culture deck and open roles 
<http://www.handy.com/careers>
Latest news <http://www.handy.com/press> at Handy
Handy just raised $50m 
<http://venturebeat.com/2015/11/02/on-demand-home-service-handy-raises-50m-in-round-led-by-fidelity/> led 
by Fidelity

"
Luciano Resende <luckbr1975@gmail.com>,"Wed, 25 May 2016 14:58:58 -0700",Re: Labeling Jiras,Sean Owen <sowen@cloudera.com>,"

We have used for other things in the past, like to identify the big-endian
related issues
https://issues.apache.org/jira/browse/SPARK-15154?jql=labels%20%3D%20big-endian


The issue with maintaining anything locally is that then it's not easily
sharable (e.g. I can't just send a link to a query)



The question is more like, what issues can be caused by using labels ?


-- 
Luciano Resende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
Luciano Resende <luckbr1975@gmail.com>,"Wed, 25 May 2016 15:05:13 -0700",Re: [ANNOUNCE] Apache Spark 2.0.0-preview release,Sean Owen <sowen@cloudera.com>,"
+1


-- 
Luciano Resende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
Sean Owen <sowen@cloudera.com>,"Wed, 25 May 2016 17:08:50 -0500",Re: Labeling Jiras,Luciano Resende <luckbr1975@gmail.com>,"Yeah I think using labels is fine -- just not if they're for someone's
internal purpose. I don't have a problem with using meaningful labels
if they're meaningful to everyone. In fact, I'd rather be using labels
rather than ""umbrella"" JIRAs.

Labels I have removed as unuseful are ones like ""patch"" or ""important""
or ""bug"". ""big-endian"" sounds useful. The only downside is that,
inevitably, a label won't be consistently applied. But such is life.


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Wed, 25 May 2016 15:45:07 -0700",Re: Labeling Jiras,Luciano Resende <luckbr1975@gmail.com>,"I think the risk is everybody starts following this, then this will be
unmanageable, given the size of the number of organizations involved.

The two main labels that we actually use are starter + releasenotes.


"
Luciano Resende <luckbr1975@gmail.com>,"Wed, 25 May 2016 16:02:52 -0700",Re: Labeling Jiras,Reynold Xin <rxin@databricks.com>,"
Well, if we consider the worst case scenario, and we have a jira, let's say
with a few labels, what harm does it make ?


-- 
Luciano Resende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
Matei Zaharia <matei.zaharia@gmail.com>,"Wed, 25 May 2016 20:38:47 -0400",Re: [ANNOUNCE] Apache Spark 2.0.0-preview release,Luciano Resende <luckbr1975@gmail.com>,"Just wondering, what is the main use case for the Docker images -- to develop apps locally or to deploy a cluster? If the image is really just a script to download a certain package name from a mirror, it may be okay to create an official one, though it does seem tricky to make it properly use the right mirror.

Matei

"
Marcin Tustin <mtustin@handybook.com>,"Wed, 25 May 2016 21:37:49 -0400",Re: [ANNOUNCE] Apache Spark 2.0.0-preview release,Matei Zaharia <matei.zaharia@gmail.com>,"The use case of docker images in general is that you can deploy and develop
with exactly the same binary environment - same java 8, same scala, same
spark. This makes things repeatable.



-- 
Want to work at Handy? Check out our culture deck and open roles 
<http://www.handy.com/careers>
Latest news <http://www.handy.com/press> at Handy
Handy just raised $50m 
<http://venturebeat.com/2015/11/02/on-demand-home-service-handy-raises-50m-in-round-led-by-fidelity/> led 
by Fidelity

"
Gurvinder Singh <gurvinder.singh@uninett.no>,"Thu, 26 May 2016 09:52:15 +0200",Re: [ANNOUNCE] Apache Spark 2.0.0-preview release,Matei Zaharia <matei.zaharia@gmail.com>,"I use docker images for both development and deploying on production
cluster. As it makes sure I have the correct version of Java and Spark.
If the image is really just
I don't think that's an issue, as you will publish a docker image which
will already have spark baked from which ever mirror you choose. The
mirror issue will be only when people want to build their own image from
published Dockerfile, then they can change the mirror if they prefer.

Here is the link to current Spark Dockerfile
(https://gist.github.com/gurvindersingh/8308d46995a58303b90e4bc2fc46e343)
I use as base, then I can start master and worker from it as I like.

- Gurvinder


---------------------------------------------------------------------


"
Steve Loughran <stevel@hortonworks.com>,"Thu, 26 May 2016 09:32:48 +0000",Re: Labeling Jiras,Sean Owen <sowen@cloudera.com>,"

labels are good in JIRA for things that span components, even transient tagging for events like ""hackathon"". Don't scale to personal/team use in the ASF; that's what google spreadsheets are better for

Now, what would be nice there would be for some spreadsheet plugin to pull JIRA status into a spreadsheet

---------------------------------------------------------------------


"
dvlpr <nandolaprasiddh@gmail.com>,"Thu, 26 May 2016 03:04:31 -0700 (MST)",Merging two datafiles,dev@spark.apache.org,"Hi everyone,
I am doing some research in spark. I have one doubt: Can we merge or combine
two datafiles and two indexfiles of different jobs (on same rdd) ?
Please give me some ideas.

Thank you!



--

---------------------------------------------------------------------


"
Priya Ch <learnings.chitturi@gmail.com>,"Thu, 26 May 2016 20:10:52 +0530",Spark Job Execution halts during shuffle...,"""user@spark.apache.org"" <user@spark.apache.org>, dev@spark.apache.org","Hello Team,


 I am trying to perform join 2 rdds where one is of size 800 MB and the
other is 190 MB. During the join step, my job halts and I don't see
progress in the execution.

This is the message I see on console -

INFO spark.MapOutputTrackerMasterEndPoint: Asked to send map output
locations for shuffle 0 to <hostname1>:40000
INFO spark.MapOutputTrackerMasterEndPoint: Asked to send map output
locations for shuffle 1 to <hostname2>:40000

After these messages, I dont see any progress. I am using Spark 1.6.0
version and yarn scheduler (running in YARN client mode). My cluster
configurations is - 3 node cluster (1 master and 2 slaves). Each slave has
1 TB hard disk space, 300GB memory and 32 cores.

HDFS block size is 128 MB.

Thanks,
Padma Ch
"
Matei Zaharia <matei.zaharia@gmail.com>,"Thu, 26 May 2016 14:18:45 -0400",[RESULT][VOTE] Removing module maintainer process,dev <dev@spark.apache.org>,"Thanks everyone for voting. With only +1 votes, the vote passes, so I'll update the contributor wiki appropriately.

+1 votes:

Matei Zaharia (binding)
Mridul Muralidharan (binding)
Andrew Or (binding)
Sean Owen (binding)
Nick Pentreath (binding)
Tom Graves (binding)
Imran Rashid (binding)
Holden Karau
Owen O'Malley

No 0 or -1 votes.

Matei


project.
worth a try, and git blame is a good enough substitute for the list when figuring out who to ping on a PR).
replies, so I'm going to call a VOTE. The proposal is to remove the maintainer process in https://cwiki.apache.org/confluence/display/SPARK/Committers#Committers-ReviewProcessandMaintainers  <https://cwiki.apache.org/confluence/display/SPARK/Committers#Committers-ReviewProcessandMaintainers><https://cwiki.apache.org/confluence/display/SPARK/Committers#Committers-ReviewProcessandMaintainers <https://cwiki.apache.org/confluence/display/SPARK/Committers#Committers-ReviewProcessandMaintainers>> given that it doesn't seem to have had a huge impact on the project, and it can unnecessarily create friction in contributing. We already have +1s from Mridul, Tom, Andrew Or and Imran on that thread.
<https://twitter.com/holdenkarau>

"
"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Thu, 26 May 2016 12:36:57 -0700",Re: LiveListenerBus with started and stopped flags? Why both?,Jacek Laskowski <jacek@japila.pl>,"Just to prevent from restarting LiveListenerBus. The internal Thread cannot
be restarted.


"
"""jpivarski@gmail.com"" <jpivarski@gmail.com>","Thu, 26 May 2016 14:46:46 -0700 (MST)","How to access the off-heap representation of cached data in Spark
 2.0",dev@spark.apache.org,"Following up on an  earlier thread
<http://apache-spark-developers-list.1001551.n3.nabble.com/Tungsten-off-heap-memory-access-for-C-libraries-td13898.html> 
, I would like to access the off-heap representation of cached data in Spark
2.0 in order to see how Spark might be linked to physics software written in
C and C++.
I'm willing to do exploration on my own, but could somebody point me to a
place to start? I have downloaded the 2.0 preview and created a persisted
Dataset:
import scala.util.Randomcase class Muon(px: Double, py: Double) {  def pt =
Math.sqrt(px*px + py*py)}val rdd = sc.parallelize(0 until 10000 map {x => 
Muon(Random.nextGaussian, Random.nextGaussian)  }, 10)val df = rdd.toDFval
ds = df.as[Muon]ds.persist()
So I have a Dataset in memory, and if I understand the  blog articles
<https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html>  
correctly, it's in off-heap memory (sun.misc.Unsafe). Is there any way I
could get a pointer to that data that I could explore with BridJ? Any hints
on how it's stored? Like, could I get started through some Djinni calls or
something?
Thanks!
-- Jim




--"
Mohammed Guller <mohammed@glassbeam.com>,"Thu, 26 May 2016 22:09:23 +0000",RE: JDBC Dialect for saving DataFrame into Vertica Table,"Aaron Ilovici <ailovici@wayfair.com>, ""user@spark.apache.org""
	<user@spark.apache.org>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Vertica also provides a Spark connector. It was not GA the last time I looked at it, but available on the Vertica community site. Have you tried using the Vertica Spark connector instead of the JDBC driver?

Mohammed
Author: Big Data Analytics with Spark<http://www.amazon.com/Big-Data-Analytics-Spark-Practitioners/dp/1484209656/>

From: Aaron Ilovici [mailto:ailovici@wayfair.com]
Sent: Thursday, May 26, 2016 8:08 AM
To: user@spark.apache.org; dev@spark.apache.org
Subject: JDBC Dialect for saving DataFrame into Vertica Table

I am attempting to write a DataFrame of Rows to Vertica via DataFrameWriter's jdbc function in the following manner:

dataframe.write().mode(SaveMode.Append).jdbc(url, table, properties);

This works when there are no NULL values in any of the Rows in my DataFrame. However, when there are rows, I get the following error:

ERROR Executor: Exception in task 0.0 in stage 3.0 (TID 24)
java.sql.SQLFeatureNotSupportedException: [Vertica][JDBC](10220) Driver not capable.
    at com.vertica.exceptions.ExceptionConverter.toSQLException(Unknown Source)
    at com.vertica.jdbc.common.SPreparedStatement.checkTypeSupported(Unknown Source)
    at com.vertica.jdbc.common.SPreparedStatement.setNull(Unknown Source)

This appears to be Spark's attempt to set a null value in a PreparedStatement, but Vertica does not understand the type upon executing the transaction. I see in JdbcDialects.scala that there are dialects for MySQL, Postgres, DB2, MsSQLServer, Derby, and Oracle.

1 - Would writing a dialect for Vertica eleviate the issue, by setting a 'NULL' in a type that Vertica would understand?
2 - What would be the best way to do this without a Spark patch? Scala, Java, make a jar and call 'JdbcDialects.registerDialect(VerticaDialect)' once created?
3 - Where would one find the proper mapping between Spark DataTypes and Vertica DataTypes? I don't see 'NULL' handling for any of the dialects, only the base case 'case _ => None' - is None mapped to the proper NULL type elsewhere?

My environment: Spark 1.6, Vertica Driver 7.2.2, Java 1.7

I would be happy to create a Jira and submit a pull request with the VerticaDialect once I figure this out.

Thank you for any insight on this,

AARON ILOVICI
Software Engineer
Marketing Engineering

[cid:image001.png@01D1B760.973BD800]

WAYFAIR
4 Copley Place
Boston, MA 02116
(617) 532-6100 x1231
ailovici@wayfair.com<mailto:ailovici@wayfair.com>


"
Reynold Xin <rxin@databricks.com>,"Thu, 26 May 2016 15:11:40 -0700",Re: JDBC Dialect for saving DataFrame into Vertica Table,Mohammed Guller <mohammed@glassbeam.com>,"It's probably a good idea to have the vertica dialect too, since it doesn't
seem like it'd be too difficult to maintain. It is not going to be as
performant as the native Vertica data source, but is going to be much
lighter weight.



"
Koert Kuipers <koert@tresata.com>,"Thu, 26 May 2016 18:35:56 -0400",changed behavior for csv datasource and quoting in spark 2.0.0-SNAPSHOT,"""dev@spark.apache.org"" <dev@spark.apache.org>","in spark 1.6.1 we used:
 sqlContext.read
      .format(""com.databricks.spark.csv"")
      .delimiter(""~"")
      .option(""quote"", null)

this effectively turned off quoting, which is a necessity for certain data
formats where quoting is not supported and ""\"""" is a valid character itself
in the data.

in spark 2.0.0-SNAPSHOT we did same thing:
 sqlContext.read
      .format(""csv"")
      .delimiter(""~"")
      .option(""quote"", null)

but this did not work, we got weird blowups where spark was trying to parse
thousands of lines as if it is one record. the reason was that a (valid)
quote character (""\"""") was present in the data. for example
a~b""c~d

as it turns out setting quote to null does not turn of quoting anymore.
instead it means to use the default quote character.

does anyone know how to turn off quoting now?

our current workaround is:
 sqlContext.read
      .format(""csv"")
      .delimiter(""~"")
      .option(""quote"", ""â˜ƒ"")

(we assume there are no unicode snowman's in our data...)
"
Reynold Xin <rxin@databricks.com>,"Thu, 26 May 2016 16:07:05 -0700",Re: changed behavior for csv datasource and quoting in spark 2.0.0-SNAPSHOT,Koert Kuipers <koert@tresata.com>,"This is unfortunately due to the way we set handle default values in
Python. I agree it doesn't follow the principle of least astonishment.

Maybe the best thing to do here is to put the actual default values in the
Python API for csv (and json, parquet, etc), rather than using None in
Python. This would require us to duplicate default values twice (once in
data source options, and another in the Python API), but that's probably OK
given they shouldn't change all the time.

Ticket https://issues.apache.org/jira/browse/SPARK-15585





a
lf
"
Koert Kuipers <koert@tresata.com>,"Thu, 26 May 2016 19:17:44 -0400",Re: changed behavior for csv datasource and quoting in spark 2.0.0-SNAPSHOT,Reynold Xin <rxin@databricks.com>,"ok, thanks for creating ticket.

just to be clear: my example was in scala


e
OK
r
"
Reynold Xin <rxin@databricks.com>,"Thu, 26 May 2016 16:18:17 -0700",Re: changed behavior for csv datasource and quoting in spark 2.0.0-SNAPSHOT,Koert Kuipers <koert@tresata.com>,"Yup - but the reason we did the null handling that way was for Python,
which also affects Scala.



in
 OK
:
er
"
Reynold Xin <rxin@databricks.com>,"Thu, 26 May 2016 18:49:23 -0700",Re: Dataset reduceByKey,Andres Perez <andres@tresata.com>,"Here's a ticket: https://issues.apache.org/jira/browse/SPARK-15598




"
Benjii519 <benjii.man.smith@gmail.com>,"Thu, 26 May 2016 18:54:37 -0700 (MST)",Creation of SparkML Estimators in Java broken?,dev@spark.apache.org,"Hello, 

Let me preface this with the fact that I am completely new to Spark and
Scala, so I may be missing something basic. 

I have been looking at implementing a clustering algorithm on top of SparkML
using Java, and ran into immediate problems. As a sanity check, I went to
the Java API example, but encountered the same behavior: I am unable to set
parameters on a Java defined Estimator 

Focusing on the JavaDeveloperApiExample, as I trust that more than my code,
I encounter the exception pasted at end of post. 

Digging around the Spark code, it looks like adding parameters through Java
is broken because the Scala params implementation is using reflection to
determine valid parameters. This works fine in the Scala Estimators as they
appear to use implementation specific params as a trait. In the Java case,
the params are a generic base class and reflection on params won't find
anything to populate (all defined on the Estimator class). Therefore, when I
try to set a parameter on the estimator, the validation fails as an unknown
parameter. 

Any feedback / suggestions? Is this a known issue? 

Thanks! 

Exception in thread ""main"" java.lang.IllegalArgumentException: requirement
failed: Param myJavaLogReg_d3e770dacdc9__maxIter does not belong to
myJavaLogReg_d3e770dacdc9. 
        at scala.Predef$.require(Predef.scala:233) 
        at
org.apache.spark.ml.param.Params$class.shouldOwn(params.scala:740) 
        at org.apache.spark.ml.param.Params$class.set(params.scala:618) 
        at org.apache.spark.ml.PipelineStage.set(Pipeline.scala:43) 
        at org.apache.spark.ml.param.Params$class.set(params.scala:604) 
        at org.apache.spark.ml.PipelineStage.set(Pipeline.scala:43) 
        at
org.apache.spark.examples.ml.MyJavaLogisticRegression.setMaxIter(JavaDeveloperApiExample.java:144) 
        at
org.apache.spark.examples.ml.MyJavaLogisticRegression.init(JavaDeveloperApiExample.java:139) 
        at
org.apache.spark.examples.ml.MyJavaLogisticRegression.<init>(JavaDeveloperApiExample.java:111) 
        at
org.apache.spark.examples.ml.JavaDeveloperApiExample.main(JavaDeveloperApiExample.java:68)



--

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Thu, 26 May 2016 22:40:09 -0500",Re: [ANNOUNCE] Apache Spark 2.0.0-preview release,Reynold Xin <rxin@databricks.com>,"I still don't see any artifacts in maven -- did it publish?

http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22org.apache.spark%22


---------------------------------------------------------------------


"
Yanbo Liang <ybliang8@gmail.com>,"Fri, 27 May 2016 01:02:09 -0700",Re: Creation of SparkML Estimators in Java broken?,Benjii519 <benjii.man.smith@gmail.com>,"This is because we do not have excellent coverage for Java-friendly
wrappers.
I found we only implement JavaParams who is the wrappers of Scala Params.
We still need Java-friendly wrappers for other traits who extends from
Scala Params.

For example, in Scala we have:
    trait HasLabelCol extends Params
We should have the Java-friendly wrappers as follows:
    class JavaHasLabelCol extends JavaParams

Then each params of the Estimator will be generated correctly and param
validation will success.
I think this should be further defined.







2016-05-26 18:54 GMT-07:00 Benjii519 <benjii.man.smith@gmail.com>:

"
Yanbo Liang <ybliang8@gmail.com>,"Fri, 27 May 2016 01:39:39 -0700",Re: Creation of SparkML Estimators in Java broken?,Benjii519 <benjii.man.smith@gmail.com>,"Create JIRA https://issues.apache.org/jira/browse/SPARK-15605 .

2016-05-27 1:02 GMT-07:00 Yanbo Liang <ybliang8@gmail.com>:

"
Koert Kuipers <koert@tresata.com>,"Fri, 27 May 2016 16:00:17 -0400",NegativeArraySizeException / segfault,"""dev@spark.apache.org"" <dev@spark.apache.org>","hello all,
after getting our unit tests to pass on spark 2.0.0-SNAPSHOT we are now
trying to run some algorithms at scale on our cluster.
unfortunately this means that when i see errors i am having a harder time
boiling it down to a small reproducible example.

today we are running an iterative algo using the dataset api and we are
seeing tasks fail with errors which seem to related to unsafe operations.
the same tasks succeed without issues in our unit tests.

i see either:

16/05/27 12:54:46 ERROR executor.Executor: Exception in task 31.0 in stage
21.0 (TID 1073)
java.lang.NegativeArraySizeException
        at
org.apache.spark.unsafe.types.UTF8String.getBytes(UTF8String.java:229)
        at
org.apache.spark.unsafe.types.UTF8String.toString(UTF8String.java:821)
        at
org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown
Source)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
        at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
        at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
        at
org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown
Source)
        at
org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown
Source)
        at
org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
        at
org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$7$$anon$1.hasNext(WholeStageCodegenExec.scala:359)
        at
org.apache.spark.sql.execution.aggregate.SortBasedAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortBasedAggregateExec.scala:74)
        at
org.apache.spark.sql.execution.aggregate.SortBasedAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortBasedAggregateExec.scala:71)
        at
org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:775)
        at
org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:775)
        at
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:318)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:282)
        at
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:318)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:282)
        at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
        at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
        at org.apache.spark.scheduler.Task.run(Task.scala:85)
        at
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
        at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

or alternatively:

# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007fe571041cba, pid=2450, tid=140622965913344
#
# JRE version: Java(TM) SE Runtime Environment (7.0_75-b13) (build
1.7.0_75-b13)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (24.75-b04 mixed mode
linux-amd64 compressed oops)
# Problematic frame:
# v  ~StubRoutines::jbyte_disjoint_arraycopy

i assume the best thing would be to try to get it to print out the
generated code that is causing this?
what switch do i need to use again to do so?
thanks,
koert
"
Reynold Xin <rxin@databricks.com>,"Fri, 27 May 2016 13:25:41 -0700",Re: NegativeArraySizeException / segfault,Koert Kuipers <koert@tresata.com>,"They should get printed if you turn on debug level logging.


"
Koert Kuipers <koert@tresata.com>,"Fri, 27 May 2016 19:34:24 -0400",Re: NegativeArraySizeException / segfault,Reynold Xin <rxin@databricks.com>,"it seemed to be related to an Aggregator, so for tests we replaced it with
an ordinary Dataset.reduce operation, and now we got:

java.lang.NegativeArraySizeException
        at
org.apache.spark.unsafe.types.UTF8String.getBytes(UTF8String.java:229)
        at
org.apache.spark.unsafe.types.UTF8String.toString(UTF8String.java:821)
        at
org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown
Source)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
        at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
        at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
        at
org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:147)
        at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
        at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
        at org.apache.spark.scheduler.Task.run(Task.scala:85)
        at
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
        at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)

i did get the generated code, but its like 17 subtrees and its not a test
but a company real program so i cannot just send it over.

i will try to create a small test program to reproduce it.


"
"""jpivarski@gmail.com"" <jpivarski@gmail.com>","Sat, 28 May 2016 08:29:33 -0700 (MST)","Re: How to access the off-heap representation of cached data in
 Spark 2.0",dev@spark.apache.org,"Is this not the place to ask such questions? Where can I get a hint as to how
to access the new off-heap cache, or C++ API, if it exists? I'm willing to
do my own research, but I have to have a place to start. (In fact, this is
the first step in that research.)

Thanks,
-- Jim




--

---------------------------------------------------------------------


"
Ricardo Almeida <ricardo.almeida@actnowib.com>,"Sat, 28 May 2016 17:37:33 +0200",Spark Streaming - Twitter on Python current status,dev@spark.apache.org,"As far as I could understand...
1. Using Python (PySpark), the use of Twitter Streaming (TwitterUtils
<http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.twitter.TwitterUtils$>)
as well as Customer Receivers is restricted to Scala and Java APIs on Spark
1.6.1;
2. Maven linking Twitter/spark-streaming-twitter_2.10 is being removed from
Spark Streaming core Scala/Java API (Maven Linking
<http://home.apache.org/~pwendell/spark-releases/spark-2.0.0-preview-docs/streaming-programming-guide.html#linking>
)
3. There are no plans to support Twitter Streaming Python API (pyspark) on
Spark 2.0 or later
4. Twitter Streaming API usage over Python (Spark pyspark) is and will
continue being restricted to Twitter REST API


Could you please confirm if these 4 my assumption are correct or is there
any alternative way for using Twitter Streaming API on PySpark from Spark
2.0?
"
Jacek Laskowski <jacek@japila.pl>,"Sun, 29 May 2016 01:17:20 +0200",Re: How to access the off-heap representation of cached data in Spark 2.0,"""jpivarski@gmail.com"" <jpivarski@gmail.com>","Hi Jim,

There's no C++ API in Spark to access the off-heap data. Moreover, I
also think ""off-heap"" has an overloaded meaning in Spark - for
tungsten and to persist your data off-heap (it's all about memory but
for different purposes and with client- and internal API).

That's my limited understanding of the things (and I'm not even sure
how trustworthy it is). Use with extreme caution.

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
"""Kazuaki Ishizaki"" <ISHIZAKI@jp.ibm.com>","Sun, 29 May 2016 09:29:39 +0900",Re: How to access the off-heap representation of cached data in Spark 2.0,dev <dev@spark.apache.org>,"Hi,
According to my understanding, contents in df.cache() is currently on Java 
heap as a set of Byte arrays in 
https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec.scala#L58
. Data is accessed by using sun.misc.unsafe APIs. Data maybe compressed 
sometime.
CachedBatch is private, and this representation may be changed in the 
future.

In general, It is not easy to access this data by using C/C++ API.

Regards,
Kazuaki Ishizaki



From:   Jacek Laskowski <jacek@japila.pl>
To:     ""jpivarski@gmail.com"" <jpivarski@gmail.com>
Cc:     dev <dev@spark.apache.org>
Date:   2016/05/29 08:18
Subject:        Re: How to access the off-heap representation of cached 
data in Spark 2.0



Hi Jim,

There's no C++ API in Spark to access the off-heap data. Moreover, I
also think ""off-heap"" has an overloaded meaning in Spark - for
tungsten and to persist your data off-heap (it's all about memory but
for different purposes and with client- and internal API).

That's my limited understanding of the things (and I'm not even sure
how trustworthy it is). Use with extreme caution.

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski


to how
to
is
http://apache-spark-developers-list.1001551.n3.nabble.com/How-to-access-the-off-heap-representation-of-cached-data-in-Spark-2-0-tp17701p17717.html

Nabble.com.

---------------------------------------------------------------------




"
"""jpivarski@gmail.com"" <jpivarski@gmail.com>","Sun, 29 May 2016 08:30:42 -0700 (MST)","Re: How to access the off-heap representation of cached data in
 Spark 2.0",dev@spark.apache.org,"Thanks Jacek and Kazuaki,

I guess got the wrong impression about the C++ API from somewhere--- I think
I read it in a JIRA wish list. If the byte array is accessed through
sun.misc.Unsafe, that's what I mean by off-heap. I found the Platform class,
which provides access to the bytes in Unsafe in a uniform way.

Thanks for pointing me to CachedBatch (private). If I find a way to provide
access by modifying Spark source, can I just submit a pull request, or do I
need to be a recognized Spark developer? If so, is there a process for
becoming one?

-- Jim




--

---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Sun, 29 May 2016 19:08:32 +0200",Re: How to access the off-heap representation of cached data in Spark 2.0,"""jpivarski@gmail.com"" <jpivarski@gmail.com>","

Start a discussion here (or user@spark) and *iff* your idea attract
enough Spark committers to support it, you'd file an JIRA issue and
send a pull request afterwards.

Jacek

---------------------------------------------------------------------


"
"""jpivarski@gmail.com"" <jpivarski@gmail.com>","Sun, 29 May 2016 16:21:31 -0700 (MST)","Re: How to access the off-heap representation of cached data in
 Spark 2.0",dev@spark.apache.org,"Okay, that makes sense.



--

---------------------------------------------------------------------


"
=?UTF-8?B?SmnFmcOtIFN5cm92w70=?= <syrovy.jiri@gmail.com>,"Mon, 30 May 2016 10:38:06 +0200",Re: NegativeArraySizeException / segfault,Koert Kuipers <koert@tresata.com>,"I think I saw this one already as the first indication that something is
wrong and it was related to
https://issues.apache.org/jira/browse/SPARK-13516

2016-05-28 1:34 GMT+02:00 Koert Kuipers <koert@tresata.com>:

"
"""Debusmann, Ralph"" <ralph.debusmann@sap.com>","Mon, 30 May 2016 11:12:29 +0000",NLP & Constraint Programming,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

I am still a Spark newbie who'd like to contribute.

There are two topics which I am most interested in:

1)      Deep NLP (Syntactic/Semantic analysis)

2)      Constraint Programming

For both, I see no built-in support in Spark yet. Or is there?

Cheers,
Ralph

"
Michael Segel <msegel_hadoop@hotmail.com>,"Mon, 30 May 2016 09:08:20 -0700",Secondary Indexing?,"dev@spark.apache.org,
 User <user@spark.apache.org>","Iâ€™m not sure where to post this since its a bit of a philosophical question in terms of design and vision for spark. 

If we look at SparkSQL and performanceâ€¦ where does Secondary indexing fit in? 

The reason this is a bit awkward is that if you view Spark as querying RDDs which are temporary, indexing doesnâ€™t make sense until you consider your use case and how long is â€˜temporaryâ€™.
Then if you consider your RDD result set could be based on querying tablesâ€¦ and you could end up with an inverted table as an indexâ€¦ then indexing could make sense. 

Does it make sense to discuss this in user or dev email lists? Has anyone given this any thought in the past? 

Thx

-Mike


---------------------------------------------------------------------


"
Michael Segel <msegel_hadoop@hotmail.com>,"Mon, 30 May 2016 12:06:01 -0700",Re: Secondary Indexing?,Mich Talebzadeh <mich.talebzadeh@gmail.com>,"I have to clarify somethingâ€¦ 
In SparkSQL, we can query against both immutable existing RDDs, and Hive/HBase/MapRDB/<insert data source>  which are mutable.  
So we have to keep this in mind while we are talking about secondary indexing. (Its not just RDDs)

I think the only advantage to being immutable is that once you generate and index the RDD, its not going to change, so the â€˜correctnessâ€™ or RI is implicit. 
Here, the issue becomes how long will the RDD live. There is a cost to generate the index, which has to be weighed against its usefulness and the longevity of the underlying RDD. Since the RDD is typically associated to a single spark context, building indexes may be cost prohibitive. 

At the same timeâ€¦ if you are dealing with a large enough set of dataâ€¦ you will have I/O. Both in terms of networking and Physical. This is true of both Spark and in-memory RDBMs.  This is due to the footprint of data along with the need to persist the data. 

But I digress. 

So in one scenario, weâ€™re building our RDDs from a system that has indexing available.  Is it safe to assume that SparkSQL will take advantage of the indexing in the underlying system? (Imagine sourcing data from an Oracle or DB2 database in order to build RDDs.) If so, then we donâ€™t have to work about indexing. 

In another scenario, weâ€™re joining an RDD against a table in an RDBMS. Is it safe to assume that Spark will select data from the database in to an RDD prior to attempting to do the join?  Here, the RDBMs table will use its index when you execute the query? (Again its an assumptionâ€¦)  Then you have two data sets that then need to be joined, which leads to the third scenarioâ€¦

Joining two spark RDDs. 
Going from memory, its a hash join. Here the RDD is used to create a hash table which would imply an index   of the hash key.  So for joins, you wouldnâ€™t need a secondary index? 
They wouldnâ€™t provide any value due to the hash table being created. (And you would probably apply the filter while you inserted a row in to the hash table before the join. ) 

Did I just answer my own question? 



conventional IMDB like Oracle TimesTen meaning concurrency is not an issue for certain indexes.
reducing memory footprint and CPU demands and using indexes may help for full key lookups. if I recall correctly in-memory databases support hash-indexes and T-tree indexes which are pretty common in these situations. But there is an overhead in creating indexes on RDDS and I presume parallelize those indexes.
Hive into a temp table, then depending on the size of that temp table, one can debate an index on that temp table.
https://www.linkedin.com/profile/view?id=AAEAAAAWh2gBxianrbJd6zP6AcPCCdOABUrV8Pw <https://www.linkedin.com/profile/view?id=AAEAAAAWh2gBxianrbJd6zP6AcPCCdOABUrV8Pw>
<http://talebzadehmich.wordpress.com/>
philosophical question in terms of design and vision for spark.
indexing fit in?
RDDs which are temporary, indexing doesnâ€™t make sense until you consider your use case and how long is â€˜temporaryâ€™.
tablesâ€¦ and you could end up with an inverted table as an indexâ€¦ then indexing could make sense.
anyone given this any thought in the past?
<mailto:user-unsubscribe@spark.apache.org>
<mailto:user-help@spark.apache.org>

"
Marcin Tustin <mtustin@handybook.com>,"Mon, 30 May 2016 15:27:18 -0400",Re: NLP & Constraint Programming,"""Debusmann, Ralph"" <ralph.debusmann@sap.com>","Hi Ralph,

You could look at https://spark-packages.org/ and see if there's anything
you want on there, and if not release your packages there.

Constraint programming might benefit from integration into Spark, though.

Marcin



-- 
Want to work at Handy? Check out our culture deck and open roles 
<http://www.handy.com/careers>
Latest news <http://www.handy.com/press> at Handy
Handy just raised $50m 
<http://venturebeat.com/2015/11/02/on-demand-home-service-handy-raises-50m-in-round-led-by-fidelity/> led 
by Fidelity

"
Reynold Xin <rxin@databricks.com>,"Mon, 30 May 2016 23:14:45 -0700",Re: Spark Streaming - Twitter on Python current status,Ricardo Almeida <ricardo.almeida@actnowib.com>,"I think your understanding is correct. There will be external libraries
that allow you to use the twitter streaming dstream API even in 2.0 though.



"
