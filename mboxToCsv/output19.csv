Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 01 Dec 2014 00:24:31 +0000","Re: Spurious test failures, testing best practices","Patrick Wendell <pwendell@gmail.com>, Ryan Williams <ryan.blake.williams@gmail.com>","   - currently the docs only contain information about building with maven,
   and even then don’t cover many important cases

 All other points aside, I just want to point out that the docs document
both how to use Maven and SBT and clearly state
<https://github.com/apache/spark/blob/master/docs/building-spark.md#building-with-sbt>
that Maven is the “build of reference” while SBT may be preferable for
day-to-day development.

I believe the main reason most people miss this documentation is that,
though it’s up-to-date on GitHub, it has’t been published yet to the docs
site. It should go out with the 1.2 release.

Improvements to the documentation on building Spark belong here:
https://github.com/apache/spark/blob/master/docs/building-spark.md

If there are clear recommendations that come out of this thread but are not
in that doc, they should be added in there. Other, less important details
may possibly be better suited for the Contributing to Spark
<https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark>
guide.

Nick
​

:

w
,
e
h
n,
g
t
e,
rs
r
g
o
it
.
ss
e
d
n
y
an,
ut
n
al
 a
my
,
s`
g
ew
f
ys
s?
en
g
s
ho
r
do
e
t
d
is
g
y
t,
&
E
"
Mark Hamstra <mark@clearstorydata.com>,"Sun, 30 Nov 2014 17:36:15 -0800","Re: Spurious test failures, testing best practices","Matei Zaharia <matei.zaharia@gmail.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","s


The equivalent using Maven:

- Start zinc
- Build your assembly using the mvn ""package"" or ""install"" target
(""install"" is actually the equivalent of SBT's ""publishLocal"") -- this step
is the first step in
http://spark.apache.org/docs/latest/building-with-maven.html#spark-tests-in-maven
- Run all the tests in one module: mvn -pl core test
- Run a specific suite: mvn -pl core
-DwildcardSuites=org.apache.spark.rdd.RDDSuite test (the -pl option isn't
strictly necessary if you don't mind waiting for Maven to scan through all
the other sub-projects only to do nothing; and, of course, it needs to be
something other than ""core"" if the test you want to run is in another
sub-project.)

You also typically want to carry along in each subsequent step any relevant
command line options you added in the ""package""/""install"" step.


ep
s
ch
ck
n
ed
o
e
t
s
t
uite
`
is
s
l
en
y
re
t
ly
nd
n`
s
to
is
s
e
en
d
t
or
ng
e
ob
lt
re
s
/484c2fb8bc0efa0e39d142087eefa9c3d5292ea3/dev%20run-tests:%20fail
/ce264e469be3641f061eabd10beb1d71ac243991/mvn%20test:%20fail
/6bc76c67aeef9c57ddd9fb2ba260fb4189dbb927/mvn%20test%20case:%20pass%20test,%20fail%20subsequent%20compile
=2&ved=0CCUQFjAB&url=http%3A%2F%2Fapache-spark-user-list.1001560.n3.nabble.com%2Fscalac-crash-when-compiling-DataTypeConversions-scala-td17083.html&ei=aRF6VJrpNKr-iAKDgYGYBQ&usg=AFQjCNHjM9m__Hrumh-ecOsSE00-JkjKBQ&sig2=zDeSqOgs02AXJXj78w5I9g&bvm=bv.80642063,d.cGE&cad=rja
/4ab0bd6e76d9fc5745eb4b45cdf13195d10efaa2/mvn%20test,%20post%20clean,%20need%20dependencies%20built
/f4c7e6fc8c301f869b00598c7b541dac243fb51e/dev%20run-tests,%20post%20clean
ests-failure-too-many-files-open-then-hang-L5260
-txt-L853
-txt-L852
-txt-L854
les-quot-exception-on-reduceByKey-td2462.html
too-many-open-files
-in-maven
eUNhuCr41B7KRPTEwMn4cga_2TNpZrWqQB8REekokxzg@mail.gmail.com%3E
"
Ryan Williams <ryan.blake.williams@gmail.com>,"Mon, 01 Dec 2014 01:53:57 +0000","Re: Spurious test failures, testing best practices","Nicholas Chammas <nicholas.chammas@gmail.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","Thanks Nicholas, glad to hear that some of this info will be pushed to the
main site soon, but this brings up yet another point of confusion that I've
struggled with, namely whether the documentation on github or that on
spark.apache.org should be considered the primary reference for people
seeking to learn about best practices for developing Spark.

Trying to read docs starting from
https://github.com/apache/spark/blob/master/docs/index.md right now, I find
that all of the links to other parts of the documentation are broken: they
point to relative paths that end in "".html"", which will work when published
on the docs-site, but that would have to end in "".md"" if a person was to be
able to navigate them on github.

So expecting people to use the up-to-date docs on github (where all
internal URLs 404 and the main github README suggests that the ""latest
Spark documentation"" can be found on the actually-months-old docs-site
<https://github.com/apache/spark#online-documentation>) is not a good
problematic, as this thread and your last email have borne out.  The result
is that there is no good place on the internet to learn about the most
up-to-date best practices for using/developing Spark.

Why not build http://spark.apache.org/docs/latest/ nightly (or every
commit) off of what's in github, rather than having that URL point to the
last release's docs (up to ~3 months old)? This way, casual users who want
the docs for the released version they happen to be using (which is already
frequently != ""/latest"" today, for many Spark users) can (still) find them
at http://spark.apache.org/docs/X.Y.Z, and the github README can safely
point people to a site (/latest) that actually has up-to-date docs that
reflect ToT and whose links work.

If there are concerns about existing semantics around ""/latest"" URLs being
broken, some new URL could be used, like
http://spark.apache.org/docs/snapshot/, but given that everything under
http://spark.apache.org/docs/latest/ is in a state of
planned-backwards-incompatible-changes every ~3mos, that doesn't sound like
that serious an issue to me; anyone sending around permanent links to
things under /latest is already going to have those links break / not make
sense in the near future.



ing-with-sbt>
referable for
 yet to the docs
n,
re
th
ng
st
m
or
n
o
y
n
r
a
ve
nd
en
ly
ean,
on
n
.
e
e
o
s
),
d
,
e
g
of
g
t
d
?
he
nd
e
.
ng
ny
b&
)
l
"
Patrick Wendell <pwendell@gmail.com>,"Sun, 30 Nov 2014 18:15:05 -0800","Re: Spurious test failures, testing best practices",Ryan Williams <ryan.blake.williams@gmail.com>,"Hey Ryan,

The existing JIRA also covers publishing nightly docs:
https://issues.apache.org/jira/browse/SPARK-1517

- Patrick


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Sun, 30 Nov 2014 18:19:32 -0800","Re: Spurious test failures, testing best practices",Ryan Williams <ryan.blake.williams@gmail.com>,"Btw - the documnetation on github represents the source code of our
docs, which is versioned with each release. Unfortunately github will
always try to render "".md"" files so it could look to a passerby like
this is supposed to represent published docs. This is a feature
limitation of github, AFAIK we cannot disable it.

The official published docs are associated with each release and
available on the apache.org website. I think ""/latest"" is a common
convention for referring to the latest *published release* docs, so
probably we can't change that (the audience for /latest is orders of
magnitude larger than for snapshot docs). However we could just add
/snapshot and publish docs there.

- Patrick


---------------------------------------------------------------------


"
Ryan Williams <ryan.blake.williams@gmail.com>,"Mon, 01 Dec 2014 02:31:43 +0000","Re: Spurious test failures, testing best practices","Nicholas Chammas <nicholas.chammas@gmail.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","Thanks Mark, most of those commands are things I've been using and used in
my original post except for ""Start zinc"". I now see the section about it on
the ""unpublished"" building-spark
<https://github.com/apache/spark/blob/master/docs/building-spark.md#speeding-up-compilation-with-zinc>
page and will try using it.

Even so, finding those commands took a nontrivial amount of trial and
error, I've not seen them very-well-documented outside of this list (your
and Matei's emails (and previous emails to this list) each have more info
about building/testing with Maven and SBT (resp.) than building-spark
<https://github.com/apache/spark/blob/master/docs/building-spark.md#spark-tests-in-maven>
does),
the per-suite invocation is still subject to requiring assembly in some
cases (""without warning"" from my perspective, having not read up on the
names of all Spark integration tests), spurious failures still abound,
there's no good way to run only the things that a given change actually
could have broken, etc.

Anyway, hopefully zinc brings me to the world of ~minute iteration times
that have been reported on this thread.



e
ve
:
on
so
lt
t
dy
hem
g
ke
e
ding-with-sbt>
preferable for
d yet to the docs
t
to
ly
y
f
t
6
].
to
n
ns
ed
I
n
he
ng
t
R
o
h
e
).
eb&
"
Matei Zaharia <matei.zaharia@gmail.com>,"Sun, 30 Nov 2014 18:39:11 -0800",Re: [RESULT] [VOTE] Designating maintainers for some Spark components,dev <dev@spark.apache.org>,"An update on this: After adding the initial maintainer list, we got feedback to add more maintainers for some components, so we added four others (Josh Rosen for core API, Mark Hamstra for scheduler, Shivaram Venkataraman for MLlib and Xiangrui Meng for Python). We also decided to lower the ""timeout"" for waiting for a maintainer to a week. Hopefully this will provide more options for reviewing in these components.

The complete list is available at https://cwiki.apache.org/confluence/display/SPARK/Committers.

Matei

for, the vote passes, but there were some concerns that I wanted to address for everyone who brought them up, as well as in the wording we will use for this policy.
process (http://www.apache.org/foundation/voting.html), wherein all code changes are done by consensus. This means that any PMC member can block a code change on technical grounds, and thus that there is consensus when something goes in. It's absolutely true that every PMC member is responsible for the whole codebase, as Greg said (not least due to legal reasons, e.g. making sure it complies to licensing rules), and this idea will not change that. To make this clear, I will include that in the wording on the project page, to make sure new committers and other community members are all aware of it.
process, by having a required review from some people on some types of code changes (assuming those people respond in time). Projects can have their own diverse review processes (e.g. some do commit-then-review and others do review-then-commit, some point people to specific reviewers, etc). This kind of process seems useful to try (and to refine) as the project grows. We will of course evaluate how it goes and respond to any problems.
and vote on, every code change. In fact all community members are welcome to do this, and lots are doing it.
consensus as described at http://www.apache.org/foundation/voting.html)
architectural and API changes by the maintainers before merging.
a hot-well of discord ;), and even in the case of discord, the point of the ASF voting process is to create consensus. The goal is just to have a better structure for reviewing and minimize the chance of errors.
the website.


---------------------------------------------------------------------


"
Ryan Williams <ryan.blake.williams@gmail.com>,"Mon, 01 Dec 2014 02:41:52 +0000","Re: Spurious test failures, testing best practices",Patrick Wendell <pwendell@gmail.com>,"Thanks Patrick, great to hear that docs-snapshots-via-jenkins is already
JIRA'd; you can interpret some of this thread as a gigantic +1 from me on
prioritizing that, which it looks like you are doing :)

I do understand the limitations of the ""github vs. official site"" status
quo; I was mostly responding to a perceived implication that I should have
been getting building/testing-spark advice from the github .md files
instead of from /latest. I agree that neither one works very well
currently, and that docs-snapshots-via-jenkins is the right solution. Per
my other email, leaving /latest as-is sounds reasonable, as long as jenkins
is putting the latest docs *somewhere*.


"
"""Ganelin, Ilya"" <Ilya.Ganelin@capitalone.com>","Sun, 30 Nov 2014 23:57:14 -0500","Re: Spurious test failures, testing best practices","Patrick Wendell <pwendell@gmail.com>, Ryan Williams
	<ryan.blake.williams@gmail.com>","Hi, Patrick - with regards to testing on Jenkins, is the process for this
to submit a pull request for the branch or is there another interface we
can use to submit a build to Jenkins for testing?


n,

________________________________________________________

The information contained in this e-mail is confidential and/or proprietary is intended only for use by the individual or entity to which it is addressed.  If the reader of this message is not the intended recipient, you are hereby notified that any review, retransmission, dissemination, distribution, copying or other use of, or taking of any action in reliance upon this information is strictly prohibited. If you have received this communication in error, please contact the sender and delete the material from your computer.


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Sun, 30 Nov 2014 21:01:37 -0800","Re: Spurious test failures, testing best practices","""Ganelin, Ilya"" <Ilya.Ganelin@capitalone.com>","Hi Ilya - you can just submit a pull request and the way we test them
is to run it through jenkins. You don't need to do anything special.

,
e
h
g
t
r
e
d
n
y
an,
n
,
f
e
d
g
y
&
E
th is intended only for use by the individual or entity to which it is addressed.  If the reader of this message is not the intended recipient, you are hereby notified that any review, retransmission, dissemination, distribution, copying or other use of, or taking of any action in reliance upon this information is strictly prohibited. If you have received this communication in error, please contact the sender and delete the material from your computer.

---------------------------------------------------------------------


"
Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"Sun, 30 Nov 2014 22:32:35 -0700 (MST)",Re: [mllib] Which is the correct package to add a new algorithm?,dev@spark.incubator.apache.org,"Hi Joseph, 

Thank you for your nice work and telling us the draft!


I understand that we should contribute new algorithms to spark.mllib.
thanks, 
Yu



-----
-- Yu Ishikawa
--

---------------------------------------------------------------------


"
Sandy Ryza <sandy.ryza@cloudera.com>,"Mon, 1 Dec 2014 00:17:07 -0800",Re: [VOTE] Release Apache Spark 1.2.0 (RC1),GuoQiang Li <witgo@qq.com>,"+1 (non-binding)

built from source
fired up a spark-shell against YARN cluster
ran some jobs using parallelize
ran some jobs that read files
clicked around the web UI



e9ec13203d0c51564265e94d77a054498fdb
"".
"
Niranda Perera <niranda@wso2.com>,"Mon, 1 Dec 2014 15:04:55 +0530",Re: Creating a SchemaRDD from an existing API,Michael Armbrust <michael@databricks.com>,"Hi Michael,

About this new data source API, what type of data sources would it support?
Does it have to be RDBMS necessarily?

Cheers




-- 
*Niranda Perera*
Software Engineer, WSO2 Inc.
Mobile: +94-71-554-8430
Twitter: @n1r44 <https://twitter.com/N1R44>
"
Lochana Menikarachchi <lochanac@gmail.com>,"Mon, 01 Dec 2014 16:28:50 +0530",packaging spark run time with osgi service,dev@spark.apache.org,"I have spark core and mllib as dependencies for a spark based osgi 
service. When I call the model building method through a unit test 
(without osgi) it works OK. When I call it through the osgi service, 
nothing happens. I tried adding spark assembly jar. Now it throws 
following error..

An error occurred while building supervised machine learning model: No 
configuration setting found for key 'akka.version'
com.typesafe.config.ConfigException$Missing: No configuration setting 
found for key 'akka.version'
     at com.typesafe.config.impl.SimpleConfig.findKey(SimpleConfig.java:115)
     at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:136)
     at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:142)
     at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:150)
     at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:155)
     at 
com.typesafe.config.impl.SimpleConfig.getString(SimpleConfig.java:197)

What is the correct way to include spark runtime dependencies to osgi 
service.. Thanks.

Lochana

---------------------------------------------------------------------


"
Scott walent <scottwalent@gmail.com>,"Mon, 1 Dec 2014 09:52:48 -0800",Spark Summit East CFP - 5 days until deadline,"dev@spark.apache.org, user@spark.apache.org","The inaugural Spark Summit East (spark-summit.org/east), an event to bring
the Apache Spark community together, will be in New York City on March 18,
2015. The call for submissions is currently open, but will close this
Friday December 5, at 11:59pm PST.   The summit is looking for talks that
will cover topics including applications, development, research, and data
science.

At the Summit you can look forward to hearing from committers, developers,
CEOs, and companies who are solving real-world big data challenges with
Spark.

All submissions will be reviewed by a Program Committee that is made up of
the creators, top committers and individuals who have heavily contributed
to the Spark project. No speaker slots are being sold to sponsors in an
effort to to keep the Summit a community driven event.

To submit your abstracts please visit: spark-summit.org/east/2015/cfp

Looking forward to seeing you there!

Best,
Scott & The Spark Summit Organizers
"
Josh Rosen <rosenville@gmail.com>,"Mon, 1 Dec 2014 11:18:16 -0800",Re: [VOTE] Release Apache Spark 1.2.0 (RC1),"""=?utf-8?Q?dev=40spark.apache.org?="" <dev@spark.apache.org>","Hi everyone,

There’s an open bug report related to Spark standalone which could be a potential release-blocker (pending investigation / a bug fix): https://issues.apache.org/jira/browse/SPARK-4498.  This issue seems non-deterministc and only affects long-running Spark standalone deployments, so it may be hard to reproduce.  I’m going to work on a patch to add additional logging in order to help with debugging.

I just wanted to give an early head’s up about this issue and to get more eyes on it in case anyone else has run into it or wants to help with debugging.

- Josh


Please vote on releasing the following candidate as Apache Spark version 1.2.0!  

The tag to be voted on is v1.2.0-rc1 (commit 1056e9ec1):  
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=1056e9ec13203d0c51564265e94d77a054498fdb  

The release files, including signatures, digests, etc. can be found at:  
http://people.apache.org/~pwendell/spark-1.2.0-rc1/  

Release artifacts are signed with the following key:  
https://people.apache.org/keys/committer/pwendell.asc  

The staging repository for this release can be found at:  
https://repository.apache.org/content/repositories/orgapachespark-1048/  

The documentation corresponding to this release can be found at:  
http://people.apache.org/~pwendell/spark-1.2.0-rc1-docs/  

Please vote on releasing this package as Apache Spark 1.2.0!  

The vote is open until Tuesday, December 02, at 05:15 UTC and passes  
if a majority of at least 3 +1 PMC votes are cast.  

[ ] +1 Release this package as Apache Spark 1.1.0  
[ ] -1 Do not release this package because ...  

To learn more about Apache Spark, please see  
http://spark.apache.org/  

== What justifies a -1 vote for this release? ==  
This vote is happening very late into the QA period compared with  
previous votes, so -1 votes should only occur for significant  
regressions from 1.0.2. Bugs already present in 1.1.X, minor  
regressions, or bugs related to new features will not block this  
release.  

== What default changes should I be aware of? ==  
1. The default value of ""spark.shuffle.blockTransferService"" has been  
changed to ""netty""  
--> Old behavior can be restored by switching to ""nio""  

2. The default value of ""spark.shuffle.manager"" has been changed to ""sort"".  
--> Old behavior can be restored by setting ""spark.shuffle.manager"" to ""hash"".  

== Other notes ==  
Because this vote is occurring over a weekend, I will likely extend  
the vote if this RC survives until the end of the vote period.  

- Patrick  

---------------------------------------------------------------------  
For additional commands, e-mail: dev-help@spark.apache.org  

"
Michael Armbrust <michael@databricks.com>,"Mon, 1 Dec 2014 11:20:35 -0800",Re: Creating a SchemaRDD from an existing API,Niranda Perera <niranda@wso2.com>,"No, it should support any data source that has a schema and can produce
rows.


"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 1 Dec 2014 15:30:13 -0800",Re: [VOTE] Release Apache Spark 1.2.0 (RC1),Josh Rosen <rosenville@gmail.com>,"+0.9 from me. Tested it on Mac and Windows (someone has to do it) and while things work, I noticed a few recent scripts don't have Windows equivalents, namely https://issues.apache.org/jira/browse/SPARK-4683 and https://issues.apache.org/jira/browse/SPARK"
shane knapp <sknapp@berkeley.edu>,"Mon, 1 Dec 2014 17:10:15 -0800","jenkins downtime: 730-930am, 12/12/14","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","i'll send out a reminder next week, but i wanted to give a heads up:  i'll
be bringing down the entire jenkins infrastructure for reboots and system
updates.

please let me know if there are any conflicts with this, thanks!

shane
"
Stephen Boesch <javadba@gmail.com>,"Mon, 1 Dec 2014 18:42:25 -0800",Required file not found in building,"""dev@spark.apache.org"" <dev@spark.apache.org>","It seems there were some additional settings required to build spark now .
This should be a snap for most of you ot there about what I am missing.
Here is the command line I have traditionally used:

   mvn -Pyarn -Phadoop-2.3 -Phive install compile package -DskipTests

That command line is however failing with the lastest from HEAD:

INFO] --- scala-maven-plugin:3.2.0:compile (scala-compile-first) @
spark-network-common_2.10 ---
[INFO] Using zinc server for incremental compilation
[INFO] compiler plugin:
BasicArtifact(org.scalamacros,paradise_2.10.4,2.0.1,null)

*[error] Required file not found: scala-compiler-2.10.4.jar*

*[error] See zinc -help for information about locating necessary files*

[INFO]
------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO]
[INFO] Spark Project Parent POM .......................... SUCCESS [4.077s]
[INFO] Spark Project Networking .......................... FAILURE [0.445s]


OK let's try ""zinc -help"":

18:38:00/spark2 $*zinc -help*
Nailgun server running with 1 cached compiler

Version = 0.3.5.1

Zinc compiler cache limit = 5
Resident scalac cache limit = 0
Analysis cache limit = 5

Compiler(Scala 2.10.4) [74ff364f]
Setup = {
*   scala compiler =
/Users/steve/.m2/repository/org/scala-lang/scala-compiler/2.10.4/scala-compiler-2.10.4.jar*
   scala library =
/Users/steve/.m2/repository/org/scala-lang/scala-library/2.10.4/scala-library-2.10.4.jar
   scala extra = {

/Users/steve/.m2/repository/org/scala-lang/scala-reflect/2.10.4/scala-reflect-2.10.4.jar
      /shared/zinc-0.3.5.1/lib/scala-reflect.jar
   }
   sbt interface = /shared/zinc-0.3.5.1/lib/sbt-interface.jar
   compiler interface sources =
/shared/zinc-0.3.5.1/lib/compiler-interface-sources.jar
   java home =
   fork java = false
   cache directory = /Users/steve/.zinc/0.3.5.1
}

Does that compiler jar exist?  Yes!

18:39:34/spark2 $ll
/Users/steve/.m2/repository/org/scala-lang/scala-compiler/2.10.4/scala-compiler-2.10.4.jar
-rw-r--r--  1 steve  staff  14445780 Apr  9  2014
/Users/steve/.m2/repository/org/scala-lang/scala-compiler/2.10.4/scala-compiler-2.10.4.jar
"
"""Dinesh J. Weerakkody"" <dineshjweerakkody@gmail.com>","Tue, 2 Dec 2014 08:17:32 +0530",Re: packaging spark run time with osgi service,Lochana Menikarachchi <lochanac@gmail.com>,"Hi Lochana,

can you please go through this mail thread [1]. I haven't tried but can be
useful.

[1]
http://apache-spark-user-list.1001560.n3.nabble.com/Packaging-a-spark-job-using-maven-td5615.html




-- 
Thanks & Best Regards,

*Dinesh J. Weerakkody*
*www.dineshjweerakkody.com <http://www.dineshjweerakkody.com>*
"
Lochana Menikarachchi <lochanac@gmail.com>,"Tue, 02 Dec 2014 08:21:44 +0530",Re: packaging spark run time with osgi service,"""Dinesh J. Weerakkody"" <dineshjweerakkody@gmail.com>","Already tried the solutions they provided.. Did not workout..

"
Ted Yu <yuzhihong@gmail.com>,"Mon, 1 Dec 2014 18:55:54 -0800",Re: Required file not found in building,Stephen Boesch <javadba@gmail.com>,"I tried the same command on MacBook and didn't experience the same error.

Which OS are you using ?

Cheers


"
Stephen Boesch <javadba@gmail.com>,"Mon, 1 Dec 2014 19:02:30 -0800",Re: Required file not found in building,Ted Yu <yuzhihong@gmail.com>,"Mac as well.  Just found the problem:  I had created an alias to zinc a
couple of months back. Apparently that is not happy with the build anymore.
No problem now that the issue has been isolated - just need to fix my zinc
alias.

2014-12-01 18:55 GMT-08:00 Ted Yu <yuzhihong@gmail.com>:

"
Stephen Boesch <javadba@gmail.com>,"Mon, 1 Dec 2014 20:00:31 -0800",Re: Required file not found in building,Ted Yu <yuzhihong@gmail.com>,"Anyone maybe can assist on how to run zinc with the latest maven build?

I am starting zinc as follows:

/shared/zinc-0.3.5.3/dist/target/zinc-0.3.5.3/bin/zinc -scala-home
$SCALA_HOME -nailed -start

The pertinent env vars are:


19:58:11/lib $echo $SCALA_HOME
/shared/scala
19:58:14/lib $which scala
/shared/scala/bin/scala
19:58:16/lib $scala -version
Scala code runner version 2.10.4 -- Copyright 2002-2013, LAMP/EPFL


When I do *not *start zinc then the maven build works .. but v slowly since
no incremental compiler available.

When zinc is started as shown above then the error occurs on all of the
modules except parent:


[INFO] Using zinc server for incremental compilation
[INFO] compiler plugin:
BasicArtifact(org.scalamacros,paradise_2.10.4,2.0.1,null)
[error] Required file not found: scala-compiler-2.10.4.jar
[error] See zinc -help for information about locating necessary files

2014-12-01 19:02 GMT-08:00 Stephen Boesch <javadba@gmail.com>:

"
Ted Yu <yuzhihong@gmail.com>,"Mon, 1 Dec 2014 20:12:08 -0800",Re: Required file not found in building,Stephen Boesch <javadba@gmail.com>,"I use zinc 0.2.0 and started zinc with the same command shown below.

I don't observe such error.

How did you install zinc-0.3.5.3 ?

Cheers


"
Stephen Boesch <javadba@gmail.com>,"Mon, 1 Dec 2014 20:18:50 -0800",Re: Required file not found in building,Ted Yu <yuzhihong@gmail.com>,"The zinc src zip for  0.3.5.3 was  downloaded  and exploded. Then I  ran
sbt dist/create .  zinc is being launched from
dist/target/zinc-0.3.5.3/bin/zinc

2014-12-01 20:12 GMT-08:00 Ted Yu <yuzhihong@gmail.com>:

"
Ted Yu <yuzhihong@gmail.com>,"Mon, 1 Dec 2014 21:16:51 -0800",Re: Required file not found in building,Stephen Boesch <javadba@gmail.com>,"I used the following for brew:
http://repo.typesafe.com/typesafe/zinc/com/typesafe/zinc/dist/0.3.0/zinc-0.3.0.tgz

After starting zinc, I issued the same mvn command but didn't encounter the
error you saw.

FYI


"
Isca Harmatz <pop1998@gmail.com>,"Tue, 2 Dec 2014 07:18:05 +0200",Monitoring Spark,dev@spark.apache.org,"hello,

im running spark on a cluster and i want to monitor how many nodes/ cores
are active in different (specific) points of the program.

is there any way to do this?

thanks,
  Isca
"
Niranda Perera <niranda@wso2.com>,"Tue, 2 Dec 2014 10:52:45 +0530","Can the Scala classes in the spark source code, be inherited in Java classes?","""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

Can the Scala classes in the spark source code, be inherited (and other OOP
concepts) in Java classes?

I want to customize some part of the code, but I would like to do it in a
Java environment.

Rgds

-- 
*Niranda Perera*
Software Engineer, WSO2 Inc.
Mobile: +94-71-554-8430
Twitter: @n1r44 <https://twitter.com/N1R44>
"
Sean Owen <sowen@cloudera.com>,"Tue, 2 Dec 2014 06:39:18 +0000",Re: Required file not found in building,Stephen Boesch <javadba@gmail.com>,"I'm having no problems with the build or zinc on my Mac. I use zinc
from ""brew install zinc"".


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Tue, 2 Dec 2014 06:38:45 +0000","Re: Can the Scala classes in the spark source code, be inherited in
 Java classes?",Niranda Perera <niranda@wso2.com>,"Yes, they are compiled to classes in JVM bytecode just the same. You
may find the generated code from Scala looks a bit strange and uses
Scala-specific classes, but it's certainly possible to treat them like
other Java classes.


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Mon, 1 Dec 2014 22:48:00 -0800","Re: Can the Scala classes in the spark source code, be inherited in
 Java classes?",Niranda Perera <niranda@wso2.com>,"Oops my previous response wasn't sent properly to the dev list. Here you go
for archiving.


Yes you can. Scala classes are compiled down to classes in bytecode. Take a
look at this: https://twitter.github.io/scala_school/java.html

Note that questions like this are not exactly what this dev list is meant
for  ...


"
Patrick Wendell <pwendell@gmail.com>,"Mon, 1 Dec 2014 23:07:28 -0800",Re: [VOTE] Release Apache Spark 1.2.0 (RC1),Matei Zaharia <matei.zaharia@gmail.com>,"Hey All,

Just an update. Josh, Andrew, and others are working to reproduce
SPARK-4498 and fix it. Other than that issue no serious regressions
have been reported so far. If we are able to get a fix in for that
soon, we'll likely cut another RC with the patch.

Continued testing of RC1 is definitely appreciated!

I'll leave this vote open to allow folks to continue posting comments.
It's fine to still give ""+1"" from your own testing... i.e. you can
assume at this point SPARK-4498 will be fixed before releasing.

- Patrick

te:
le things work, I noticed a few recent scripts don't have Windows equivalents, namely https://issues.apache.org/jira/browse/SPARK-4683 and https://issues.apache.org/jira/browse/SPARK-4684. The first one at least would be good to fix if we do another RC. Not blocking the release but useful to fix in docs is https://issues.apache.org/jira/browse/SPARK-4685.
potential release-blocker (pending investigation / a bug fix): https://issues.apache.org/jira/browse/SPARK-4498.  This issue seems non-deterministc and only affects long-running Spark standalone deployments, so it may be hard to reproduce.  I'm going to work on a patch to add additional logging in order to help with debugging.
e eyes on it in case anyone else has run into it or wants to help with debugging.
 1.2.0!
6e9ec13203d0c51564265e94d77a054498fdb
t"".
hash"".

---------------------------------------------------------------------


"
Denny Lee <denny.g.lee@gmail.com>,"Tue, 02 Dec 2014 08:22:59 +0000",Re: [VOTE] Release Apache Spark 1.2.0 (RC1),"Patrick Wendell <pwendell@gmail.com>, Matei Zaharia <matei.zaharia@gmail.com>","+1 (non-binding)

Verified on OSX 10.10.2, built from source,
spark-shell / spark-submit jobs
ran various simple Spark / Scala queries
ran various SparkSQL queries (including HiveContext)
ran ThriftServer service and connected via beeline
ran SparkSVD



"
Niranda Perera <niranda@wso2.com>,"Tue, 2 Dec 2014 14:18:49 +0530","Re: Can the Scala classes in the spark source code, be inherited in
 Java classes?",Reynold Xin <rxin@databricks.com>,"Thanks.

And @Reynold, sorry my bad, Guess I should have used something like
Stackoverflow!




-- 
*Niranda Perera*
Software Engineer, WSO2 Inc.
Mobile: +94-71-554-8430
Twitter: @n1r44 <https://twitter.com/N1R44>
"
Stephen Boesch <javadba@gmail.com>,"Tue, 2 Dec 2014 08:28:36 -0800",Re: Required file not found in building,Sean Owen <sowen@cloudera.com>,"Thanks Sean, I followed suit (brew install zinc) and that is working.

2014-12-01 22:39 GMT-08:00 Sean Owen <sowen@cloudera.com>:

"
Yana Kadiyska <yana.kadiyska@gmail.com>,"Tue, 2 Dec 2014 12:31:45 -0500","Fwd: [Thrift,1.2 RC] what happened to parquet.hive.serde.ParquetHiveSerDe","""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Apologies if people get this more than once -- I sent mail to dev@spark
last night and don't see it in the archives. Trying the incubator list
now...wanted to make sure it doesn't get lost in case it's a bug...

---------- Forwarded message ----------
From: Yana Kadiyska <yana.kadiyska@gmail.com>
Date: Mon, Dec 1, 2014 at 8:10 PM
Subject: [Thrift,1.2 RC] what happened to
parquet.hive.serde.ParquetHiveSerDe
To: dev@spark.apache.org


Hi all, apologies if this is not a question for the dev list -- figured
User list might not be appropriate since I'm having trouble with the RC tag.

I just tried deploying the RC and running ThriftServer. I see the following
error:

14/12/01 21:31:42 ERROR UserGroupInformation: PriviledgedActionException
as:anonymous (auth:SIMPLE)
cause:org.apache.hive.service.cli.HiveSQLException:
java.lang.RuntimeException:
MetaException(message:java.lang.ClassNotFoundException Class
parquet.hive.serde.ParquetHiveSerDe not found)
14/12/01 21:31:42 WARN ThriftCLIService: Error executing statement:
org.apache.hive.service.cli.HiveSQLException: java.lang.RuntimeException:
MetaException(message:java.lang.ClassNotFoundException Class
parquet.hive.serde.ParquetHiveSerDe not found)
at
org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.run(Shim13.scala:192)
at
org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:231)
at
org.apache.hive.service.cli.session.HiveSessionImpl.executeStatement(HiveSessionImpl.java:212)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at
org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:79)
at
org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:37)
at
org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:64)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:415)
​


I looked at a working installation that I have(build master a few weeks
ago) and this class used to be included in spark-assembly:

ls *.jar|xargs grep parquet.hive.serde.ParquetHiveSerDe
Binary file spark-assembly-1.2.0-SNAPSHOT-hadoop2.0.0-mr1-cdh4.2.0.jar
matches

but with the RC build it's not there?

I tried both the prebuilt CDH drop and later manually built the tag with
the following command:

 ./make-distribution.sh --tgz -Phive -Dhadoop.version=2.0.0-mr1-cdh4.2.0
-Phive-thriftserver
$JAVA_HOME/bin/jar -tvf spark-assembly-1.2.0-hadoop2.0.0-mr1-cdh4.2.0.jar
|grep parquet.hive.serde.ParquetHiveSerDe

comes back empty...
"
Jeremy Freeman <freeman.jeremy@gmail.com>,"Tue, 2 Dec 2014 13:16:02 -0500",Re: [VOTE] Release Apache Spark 1.2.0 (RC1),Denny Lee <denny.g.lee@gmail.com>,"+1 (non-binding)

Installed version pre-built for Hadoop on a private HPC
ran PySpark shell w/ iPython
loaded data using custom Hadoop input formats
ran MLlib routines in PySpark
ran custom workflows in PySpark
browsed the web UI

Noticeable improvements "
Kay Ousterhout <kayousterhout@gmail.com>,"Tue, 2 Dec 2014 12:23:09 -0800",keeping PR titles / descriptions up to date,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,

I've noticed a bunch of times lately where a pull request changes to be
pretty different from the original pull request, and the title /
description never get updated.  Because the pull request title and
description are used as the commit message, the incorrect description lives
on forever, making it harder to understand the reason behind a particular
commit without going back and reading the entire conversation on the pull
request.  If folks could try to keep these up to date (and committers, try
to remember to verify that the title and description are correct before
making merging pull requests), that would be awesome.

-Kay
"
Mridul Muralidharan <mridul@gmail.com>,"Wed, 3 Dec 2014 02:20:24 +0530",Re: keeping PR titles / descriptions up to date,Kay Ousterhout <kayousterhout@gmail.com>,"I second that !
Would also be great if the JIRA was updated accordingly too.

Regards,
Mridul



---------------------------------------------------------------------


"
Andrew Or <andrew@databricks.com>,"Tue, 2 Dec 2014 12:58:27 -0800",Re: [VOTE] Release Apache Spark 1.2.0 (RC1),Jeremy Freeman <freeman.jeremy@gmail.com>,"+1. I also tested on Windows just in case, with jars referring other jars
and python files referring other python files. Path resolution still works.

2014-12-02 10:16 GMT-08:00 Jeremy Freeman <freeman.jeremy@gmail.com>:

s
tFound /
r.
ew
:
e
=
n
to
-
"
Patrick Wendell <pwendell@gmail.com>,"Tue, 2 Dec 2014 13:05:28 -0800",Re: keeping PR titles / descriptions up to date,Mridul Muralidharan <mridul@gmail.com>,"Also a note on this for committers - it's possible to re-word the
title during merging, by just running ""git commit -a --amend"" before
you push the PR.

- Patrick


---------------------------------------------------------------------


"
Andrew Or <andrew@databricks.com>,"Tue, 2 Dec 2014 13:36:47 -0800",Announcing Spark 1.1.1!,"""dev@spark.apache.org"" <dev@spark.apache.org>, ""user@spark.apache.org"" <user@spark.apache.org>","I am happy to announce the availability of Spark 1.1.1! This is a
maintenance release with many bug fixes, most of which are concentrated in
the core. This list includes various fixes to sort-based shuffle, memory
leak, and spilling issues. Contributions from this release came from 55
developers.

Visit the release notes [1] to read about the new features, or
download [2] the release today.

[1] http://spark.apache.org/releases/spark-release-1-1-1.html
[2] http://spark.apache.org/downloads.html

Please e-mail me directly for any typo's in the release notes or name
listing.

Thanks for everyone who contributed, and congratulations!
-Andrew
"
Tom Graves <tgraves_cs@yahoo.com.INVALID>,"Tue, 2 Dec 2014 22:04:53 +0000 (UTC)",Re: [VOTE] Release Apache Spark 1.2.0 (RC1),"Patrick Wendell <pwendell@gmail.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","+1 tested on yarn.
Tom 

   

 Please vote on releasing the following candidate as Apache Spark version 1.2.0!

The tag to be voted on is v1.2.0-rc1 (commit 1056e9ec1):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=1056e9ec13203d0c5156426"
Ryan Williams <ryan.blake.williams@gmail.com>,"Tue, 02 Dec 2014 22:40:07 +0000","Re: Spurious test failures, testing best practices","Mark Hamstra <mark@clearstorydata.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Following on Mark's Maven examples, here is another related issue I'm
having:

I'd like to compile just the `core` module after a `mvn clean`, without
building an assembly JAR first. Is this possible?

Attempting to do it myself, the steps I performed were:

- `mvn compile -pl core`: fails because `core` depends on `network/common`
and `network/shuffle`, neither of which is installed in my local maven
cache (and which don't exist in central Maven repositories, I guess? I
thought Spark is publishing snapshot releases?)

- `network/shuffle` also depends on `network/common`, so I'll `mvn install`
the latter first: `mvn install -DskipTests -pl network/common`. That
succeeds, and I see a newly built 1.3.0-SNAPSHOT jar in my local maven
repository.

- However, `mvn install -DskipTests -pl network/shuffle` subsequently
fails, seemingly due to not finding network/core. Here's
<https://gist.github.com/ryan-williams/1711189e7d0af558738d> a sample full
output from running `mvn install -X -U -DskipTests -pl network/shuffle`
from such a state (the -U was to get around a previous failure based on
having cached a failed lookup of network-common-1.3.0-SNAPSHOT).

- Thinking maven might be special-casing ""-SNAPSHOT"" versions, I tried
replacing ""1.3.0-SNAPSHOT"" with ""1.3.0.1"" globally and repeating these
steps, but the error seems to be the same
<https://gist.github.com/ryan-williams/37fcdd14dd92fa562dbe>.

Any ideas?

Thanks,

-Ryan


"
Marcelo Vanzin <vanzin@cloudera.com>,"Tue, 2 Dec 2014 14:45:47 -0800","Re: Spurious test failures, testing best practices",Ryan Williams <ryan.blake.williams@gmail.com>,"
Out of curiosity, may I ask why? What's the problem with running ""mvn
install -DskipTests"" first (or ""package"" instead of ""install"",
although I generally do the latter)?

You can probably do what you want if you manually build / install all
the needed dependencies first; you found two, but it seems you're also
missing the ""spark-parent"" project (which is the top-level pom). That
sounds like a lot of trouble though, for not any gains that I can
see... after the first build you should be able to do what you want
easily.

-- 
Marcelo

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Tue, 2 Dec 2014 14:46:09 -0800","Re: Spurious test failures, testing best practices",Ryan Williams <ryan.blake.williams@gmail.com>,"Hey Ryan,

What if you run a single ""mvn install"" to install all libraries
locally - then can you ""mvn compile -pl core""? I think this may be the
only way to make it work.

- Patrick


---------------------------------------------------------------------


"
Ryan Williams <ryan.blake.williams@gmail.com>,"Tue, 02 Dec 2014 23:39:40 +0000","Re: Spurious test failures, testing best practices",Marcelo Vanzin <vanzin@cloudera.com>,"Marcelo: by my count, there are 19 maven modules in the codebase. I am
typically only concerned with ""core"" (and therefore its two dependencies as
well, `network/{shuffle,common}`).

The `mvn package` workflow (and its sbt equivalent) that most people
apparently use involves (for me) compiling+packaging 16 other modules that
I don't care about; I pay this cost whenever I rebase off of master or
encounter the sbt-compiler-crash bug, among other possible scenarios.

Compiling one module (after building/installing its dependencies) seems
like the sort of thing that should be possible, and I don't see why my
previously-documented attempt is failing.

re: Marcelo's comment about ""missing the 'spark-parent' project"", I saw
that error message too and tried to ascertain what it could mean. Why would
`network/shuffle` need something from the parent project? AFAICT
`network/common` has the same references to the parent project as
`network/shuffle` (namely just a <parent> block in its POM), and yet I can
difference is that `network/shuffle` has a dependency on another module,
while `network/common` does not.

Does Maven not let you build modules that depend on *any* other modules
without building *all* modules, or is there a way to do this that we've not
found yet?

Patrick: per my response to Marcelo above, I am trying to avoid having to
compile and package a bunch of stuff I am not using, which both `mvn
package` and `mvn install` on the parent project do.






"
Marcelo Vanzin <vanzin@cloudera.com>,"Tue, 2 Dec 2014 15:46:19 -0800","Re: Spurious test failures, testing best practices",Ryan Williams <ryan.blake.williams@gmail.com>,"
the baseline, you can just compile / test ""core"" to your heart's
desire. Core tests won't even run until you build the assembly anyway,
since some of them require the assembly to be present.

Also, even if you work in core - I'd say especially if you work in
core - you should still, at some point, compile and test everything
else that depends on it.

So, do this ONCE:

  mvn install -DskipTests

Then do this as many times as you want:

  mvn -pl spark-core_2.10 something

That doesn't seem too bad to me. (Be aware of the ""assembly"" comment
above, since testing spark-core means you may have to rebuild the
assembly from time to time, if your changes affect those tests.)


The ""spark-parent"" project is the main pom that defines dependencies
and their version, along with lots of build plugins and
configurations. It's needed by all modules to compile correctly.

-- 
Marcelo

---------------------------------------------------------------------


"
Ryan Williams <ryan.blake.williams@gmail.com>,"Wed, 03 Dec 2014 00:40:20 +0000","Re: Spurious test failures, testing best practices",Marcelo Vanzin <vanzin@cloudera.com>,"


once... every time I rebase off master, or am obliged to `mvn clean` by
some other build-correctness bug, as I said before. In my experience this
works out to a few times per week.




I understand that this is a workflow that does what I want as a side effect
of doing 3-5x more work (depending whether you count [number of modules
built] or [lines of scala/java compiled]), none of the extra work being
useful to me (more on that below).




The tests you refer to are exactly the ones that I'd like to let Jenkins
run from here on out, per advice I was given elsewhere in this thread and
due to the myriad unpleasantries I've encountered in trying to run them
myself.



Last response applies.



again, s/ONCE/several times a week/, in my experience.



(Be aware of the ""assembly"" comment

- I understand the parent POM has that information.

- I don't understand why Maven would feel that it is unable to compile the
`network/shuffle` module without having first compiled, packaged, and
installed 17 modules (19 minus `network/shuffle` and its dependency
`network/common`) that are not transitive dependencies of `network/shuffle`.

- I am trying to understand whether my failure to get Maven to compile
`network/shuffle` stems from my not knowing the correct incantation to feed
to Maven or from Maven's having a different (and seemingly worse) model for
how it handles module dependencies than I expected.



"
Marcelo Vanzin <vanzin@cloudera.com>,"Tue, 2 Dec 2014 16:49:56 -0800","Re: Spurious test failures, testing best practices",Ryan Williams <ryan.blake.williams@gmail.com>,"
No, you only need to do it something upstream from core changed (i.e.,
spark-parent, network/common or network/shuffle) in an incompatible
way. Otherwise, you can rebase and just recompile / retest core,
without having to install everything else. I do this kind of thing all
the time. If you have to do ""mvn clean"" often you're probably doing
something wrong somewhere else.

I understand where you're coming from, but the way you're thinking is
just not how maven works. I too find annoying that maven requires lots
of things to be ""installed"" before you can use them, when they're all
part of the same project. But well, that's the way things are.

-- 
Marcelo

---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Tue, 2 Dec 2014 17:37:31 -0800","Re: [Thrift,1.2 RC] what happened to parquet.hive.serde.ParquetHiveSerDe",Yana Kadiyska <yana.kadiyska@gmail.com>," In Hive 13 (which is the default for Spark 1.2), parquet is included and
thus we no longer include the Hive parquet bundle. You can now use the
included
ParquetSerDe: org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe

If you want to compile Spark 1.2 with Hive 12 instead you can pass
-Phive-0.12.0 and  parquet.hive.serde.ParquetHiveSerDe will be included as
before.

Michael


ng
(Shim13.scala:192)
nal(HiveSessionImpl.java:231)
SessionImpl.java:212)
:57)
mpl.java:43)
oxy.java:79)
onProxy.java:37)
xy.java:64)
0
"
flyson <m_qiu@msn.com>,"Tue, 2 Dec 2014 20:22:32 -0700 (MST)",object xxx is not a member of package com,dev@spark.incubator.apache.org,"Hello everyone,

Could anybody tell me how to import and call the 3rd party java classes from
inside spark?
Here's my case:
I have a jar file (the directory layout is com.xxx.yyy.zzz) which contains
some java classes, and I need to call some of them in spark code.
I used the statement ""import com.xxx.yyy.zzz._"" on top of the impacted spark
file and set the location of the jar file in the CLASSPATH environment, and
use "".sbt/sbt assembly"" to build the project. As a result, I got an error
saying ""object xxx is not a member of package com"".

I thought that could be related to the library dependencies, but couldn't
figure it out. Any suggestion/solution from you would be appreciated!

By the way in the scala console, if the :cp is used to point to the jar
file, I can import the classes from the jar file.

Thanks! 



--

---------------------------------------------------------------------


"
"""Wang, Daoyuan"" <daoyuan.wang@intel.com>","Wed, 3 Dec 2014 04:29:21 +0000",RE: object xxx is not a member of package com,"flyson <m_qiu@msn.com>, ""dev@spark.incubator.apache.org""
	<dev@spark.incubator.apache.org>","I think you can place the jar in lib/ in SPARK_HOME, and then compile without any change to your class path. This could be a temporary way to include your jar. You can also put them in your pom.xml.

Thanks,
Daoyuan


Could anybody tell me how to import and call the 3rd party java classes from inside spark?
Here's my case:
I have a jar file (the directory layout is com.xxx.yyy.zzz) which contains some java classes, and I need to call some of them in spark code.
I used the statement ""import com.xxx.yyy.zzz._"" on top of the impacted spark file and set the location of the jar file in the CLASSPATH environment, and use "".sbt/sbt assembly"" to build the project. As a result, I got an error saying ""object xxx is not a member of package com"".

I thought that could be related to the library dependencies, but couldn't figure it out. Any suggestion/solution from you would be appreciated!

By the way in the scala console, if the :cp is used to point to the jar file, I can import the classes from the jar file.

Thanks! 



--
3.nabble.com/object-xxx-is-not-a-member-of-package-com-tp9619.html
om.

---------------------------------------------------------------------
mands, e-mail: dev-help@spark.apache.org


---------------------------------------------------------------------


"
Yana Kadiyska <yana.kadiyska@gmail.com>,"Wed, 3 Dec 2014 10:09:19 -0500","Re: [Thrift,1.2 RC] what happened to parquet.hive.serde.ParquetHiveSerDe",Michael Armbrust <michael@databricks.com>,"Thanks Michael, you are correct.

I also opened https://issues.apache.org/jira/browse/SPARK-4702 -- if
someone can comment on why this might be happening that would be great.
This would be a blocker to me using 1.2 and it used to work so I'm a bit
puzzled. I was hoping that it's again a result of the default profile
switch but it didn't seem to be the case

(ps. please advise if this is more user-list appropriate. I'm posting to
dev as it's an RC)


s
:
n(Shim13.scala:192)
rnal(HiveSessionImpl.java:231)
eSessionImpl.java:212)
a:57)
Impl.java:43)
roxy.java:79)
ionProxy.java:37)
oxy.java:64)
.0
r
"
"""York, Brennon"" <Brennon.York@capitalone.com>","Wed, 3 Dec 2014 10:24:35 -0500",Re: object xxx is not a member of package com,"""Wang, Daoyuan"" <daoyuan.wang@intel.com>, flyson <m_qiu@msn.com>,
	""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","For reference here is the relevant `sbt` documentation that states
Daoyuans solution as well as a few other options to try.

http://www.scala-sbt.org/0.13/tutorial/Library-Dependencies.html





________________________________________________________

The information contained in this e-mail is confidential and/or proprietary is intended only for use by the individual or entity to which it is addressed.  If the reader of this message is not the intended recipient, you are hereby notified that any review, retransmission, dissemination, distribution, copying or other use of, or taking of any action in reliance upon this information is strictly prohibited. If you have received this communication in error, please contact the sender and delete the material from your computer.


---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 03 Dec 2014 21:44:25 +0000",zinc invocation examples,dev <dev@spark.apache.org>,"https://github.com/apache/spark/blob/master/docs/building-spark.md#speeding-up-compilation-with-zinc

Could someone summarize how they invoke zinc as part of a regular
build-test-etc. cycle?

I'll add it in to the aforelinked page if appropriate.

Nick
"
Michael Armbrust <michael@databricks.com>,"Wed, 3 Dec 2014 11:05:18 -0800","Re: [Thrift,1.2 RC] what happened to parquet.hive.serde.ParquetHiveSerDe",Yana Kadiyska <yana.kadiyska@gmail.com>,"Thanks for reporting. As a workaround you should be able to SET
spark.sql.hive.convertMetastoreParquet=false, but I'm going to try to fix
this before the next RC.


e
as
n
n:
un(Shim13.scala:192)
ernal(HiveSessionImpl.java:231)
veSessionImpl.java:212)
va:57)
rImpl.java:43)
Proxy.java:79)
sionProxy.java:37)
roxy.java:64)
h
2.0
ar
"
Xiangrui Meng <mengxr@gmail.com>,"Thu, 4 Dec 2014 08:19:47 +0800",Re: [VOTE] Release Apache Spark 1.2.0 (RC1),Krishna Sankar <ksankar42@gmail.com>,"Krishna, could you send me some code snippets for the issues you saw
in naive Bayes and k-means? -Xiangrui


---------------------------------------------------------------------


"
flyson <m_qiu@msn.com>,"Wed, 3 Dec 2014 10:49:00 -0700 (MST)",RE: object xxx is not a member of package com,dev@spark.incubator.apache.org,"Hi Daoyuan,

Actually I had already tried the way as you mentioned, but it didn't work
for my case. I still got the same compilation errors.

Anyone can tell me how to resolve the library dependency on the 3rd party
jar in sbt?

Thanks!
Min



--

---------------------------------------------------------------------


"
Takeshi Yamamuro <linguin.m.s@gmail.com>,"Thu, 4 Dec 2014 10:38:30 +0900",Re: [VOTE] Release Apache Spark 1.2.0 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1 (non-binding)

Checked on CentOS 6.5, compiled from the source.
Ran various examples in stand-alone master and three slaves, and
browsed the web UI.


"
RJ Nowling <rnowling@gmail.com>,"Wed, 3 Dec 2014 13:00:16 -0500","RDDs for ""dimensional"" (time series, spatial) data","""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,

I created a JIRA to discuss adding RDDs for ""dimensional"" (not sure what
else to call it) data like time series and spatial data.  Spark could be a
better time series and/or spatial ""database"" than existing approaches out
there.

https://issues.apache.org/jira/browse/SPARK-4727

I saw that MLlib supports some operations for time series in 1.2.0-rc1, but
I think that specialized RDDs could optimize the partitioning and
algorithms better than a regular RDD.  Or, for example, spatial data could
be partitioned into a grid.

Any feedback would be great!

Thanks,
RJ Nowling

-- 
em rnowling@gmail.com
"
Sean Owen <sowen@cloudera.com>,"Thu, 4 Dec 2014 06:47:12 -0600",Re: zinc invocation examples,Nicholas Chammas <nicholas.chammas@gmail.com>,"You just run it once with ""zinc -start"" and leave it running as a
background process on your build machine. You don't have to do
anything for each build.


---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Wed, 3 Dec 2014 11:25:25 -0800","Re: [Thrift,1.2 RC] what happened to parquet.hive.serde.ParquetHiveSerDe",Yana Kadiyska <yana.kadiyska@gmail.com>,"Here's a fix: https://github.com/apache/spark/pull/3586


ix
d
De
 as
k
d
C
on
run(Shim13.scala:192)
ternal(HiveSessionImpl.java:231)
iveSessionImpl.java:212)
ava:57)
orImpl.java:43)
nProxy.java:79)
ssionProxy.java:37)
Proxy.java:64)
s
th
.2.0
"
Lochana Menikarachchi <lochanac@gmail.com>,"Thu, 04 Dec 2014 08:20:44 +0530",Re: packaging spark run time with osgi service,"""Dinesh J. Weerakkody"" <dineshjweerakkody@gmail.com>","I think the problem has to do with akka not picking up the 
reference.conf file in the assembly.jar

We managed to make akka pick the conf file by temporary switching the 
class loaders.

Thread.currentThread().setContextClassLoader(JavaSparkContext.class.getClassLoader());

The model gets build but execution fails during some later stage with a snappy error..

14/12/04 08:07:44 ERROR Executor: Exception in task 0.0 in stage 105.0 (TID 104)
java.lang.UnsatisfiedLinkError: org.xerial.snappy.SnappyNative.maxCompressedLength(I)I
	at org.xerial.snappy.SnappyNative.maxCompressedLength(Native Method)
	at org.xerial.snappy.Snappy.maxCompressedLength(Snappy.java:320)
	at org.xerial.snappy.SnappyOutputStream.<init>(SnappyOutputStream.java:79)
	at org.apache.spark.io.SnappyCompressionCodec.compressedOutputStream(CompressionCodec.scala:125)

According to akka documentation a conf file can be parsed with -Dconfig.file= but, we couldn't get it to work..

Any ideas how to do this?

Lochana





"
Jun Feng Liu <liujunf@cn.ibm.com>,"Thu, 4 Dec 2014 13:02:43 +0800",Ooyala Spark JobServer,dev@spark.apache.org,"Hi, I am wondering the status of the Ooyala Spark Jobserver, any plan to 
get it into the spark release?
 
Best Regards
 
Jun Feng Liu
IBM China Systems & Technology Laboratory in Beijing



Phone: 86-10-82452683 
E-mail: liujunf@cn.ibm.com


BLD 28,ZGC Software Park 
No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193 
China 
 

 "
invkrh <invkrh@gmail.com>,"Wed, 3 Dec 2014 09:50:23 -0700 (MST)",scala.MatchError on SparkSQL when creating ArrayType of StructType,dev@spark.incubator.apache.org,"Hi,

I am using SparkSQL on 1.1.0 branch.

The following code leads to a scala.MatchError 
at
org.apache.spark.sql.catalyst.expressions.Cast.cast$lzycompute(Cast.scala:247)

val scm = StructType(*inputRDD*.schema.fields.init :+
      StructField(""list"",
        ArrayType(
          StructType(
            Seq(StructField(""*date*"", StringType, nullable = *false*),
              StructField(""*nbPurchase*"", IntegerType, nullable =
*false*)))),
        nullable = false))

// *purchaseRDD* is RDD[sql.ROW] whose schema is corresponding to scm. It is
transformed from *inputRDD*
val schemaRDD = hiveContext.applySchema(purchaseRDD, scm)
schemaRDD.registerTempTable(""t_purchase"")

Here's the stackTrace:
scala.MatchError: ArrayType(StructType(List(StructField(date,StringType,
*true* ), StructField(n_reachat,IntegerType, *true* ))),true) (of class
org.apache.spark.sql.catalyst.types.ArrayType)
	at
org.apache.spark.sql.catalyst.expressions.Cast.cast$lzycompute(Cast.scala:247)
	at org.apache.spark.sql.catalyst.expressions.Cast.cast(Cast.scala:247)
	at org.apache.spark.sql.catalyst.expressions.Cast.eval(Cast.scala:263)
	at
org.apache.spark.sql.catalyst.expressions.Alias.eval(namedExpressions.scala:84)
	at
org.apache.spark.sql.catalyst.expressions.InterpretedMutableProjection.apply(Projection.scala:66)
	at
org.apache.spark.sql.catalyst.expressions.InterpretedMutableProjection.apply(Projection.scala:50)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at
org.apache.spark.sql.hive.execution.InsertIntoHiveTable.org$apache$spark$sql$hive$execution$InsertIntoHiveTable$$writeToFile$1(InsertIntoHiveTable.scala:149)
	at
org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$saveAsHiveFile$1.apply(InsertIntoHiveTable.scala:158)
	at
org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$saveAsHiveFile$1.apply(InsertIntoHiveTable.scala:158)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
	at org.apache.spark.scheduler.Task.run(Task.scala:54)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
	at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

The strange thing is that *nullable* of *date* and *nbPurchase* field are
set to true while it were false in the code. If I set both to *true*, it
works. But, in fact, they should not be nullable.

Here's what I find at Cast.scala:247 on 1.1.0 branch

  private[this] lazy val cast: Any => Any = dataType match {
    case StringType => castToString
    case BinaryType => castToBinary
    case DecimalType => castToDecimal
    case TimestampType => castToTimestamp
    case BooleanType => castToBoolean
    case ByteType => castToByte
    case ShortType => castToShort
    case IntegerType => castToInt
    case FloatType => castToFloat
    case LongType => castToLong
    case DoubleType => castToDouble
  }

Any idea? Thank you.

Hao



--

---------------------------------------------------------------------


"
Krishna Sankar <ksankar42@gmail.com>,"Wed, 3 Dec 2014 17:59:17 -0800",Re: [VOTE] Release Apache Spark 1.2.0 (RC1),Xiangrui Meng <mengxr@gmail.com>,"Will do. Am on the road - will annotate an iPython notebook with what works
& what didn't work ...
Cheers
<k/>


"
Lochana Menikarachchi <lochanac@gmail.com>,"Thu, 04 Dec 2014 11:34:31 +0530",spark osgi class loading issue,"""dev@spark.apache.org"" <dev@spark.apache.org>","We are trying to call spark through an osgi service (with osgifyed 
version of assembly.jar). Spark does not work (due to the way spark 
reads akka reference.conf) unless we switch the class loader as follows.

Thread.currentThread().setContextClassLoader(JavaSparkContext.class.getClassLoader());

The problem is there is no way to switch between class loaders and get the information generated by spark operations.
Is there a way to run spark through an osgi service. I think if we can parse the reference.conf some other way this might work. Can somebody shed some light on this..

Thanks.

Lochana



"
Ryan Williams <ryan.blake.williams@gmail.com>,"Wed, 03 Dec 2014 23:49:50 +0000","Re: Spurious test failures, testing best practices",Marcelo Vanzin <vanzin@cloudera.com>,"Thanks Marcelo, ""this is just how Maven works (unfortunately)"" answers my
question.

Another related question: I tried to use `mvn scala:cc` and discovered that
it only seems to work scan src/main and src/test directories (according to its
docs <http://scala-tools.org/mvnsites/maven-scala-plugin/usage_cc.html>),
and so can only be run from within submodules, not from the root directory.

I'll add a note about this to building-spark.html unless there is a way to
do it for all modules / from the root directory that I've missed. Let me
know!





"
Andrew Or <andrew@databricks.com>,"Wed, 3 Dec 2014 13:34:14 -0800",Re: keeping PR titles / descriptions up to date,Patrick Wendell <pwendell@gmail.com>,"I realize we're not voting, but +1 to this proposal since commit messages
can't be changed whereas JIRA issues can always be updated after the fact.

2014-12-02 13:05 GMT-08:00 Patrick Wendell <pwendell@gmail.com>:

"
nivdul <ludwine.probst@gmail.com>,"Thu, 4 Dec 2014 07:31:41 -0700 (MST)","Dependent on multiple versions of servlet-api jars lead to throw an
 SecurityException when Spark built for hadoop 2.5.0",dev@spark.incubator.apache.org,"Hi !

I have the same issue that this  one
<https://issues.apache.org/jira/browse/SPARK-1693>   but using the version
2.5 of Hadoop.

The fix bug is  here
<https://github.com/witgo/spark/commit/dc63905908cb7c84c741bb5fdc4ad7d4abdcb0b2>  
for Hadoop 2.4 and 2.3.

For now I just changed my version of Hadoop but I would like to use the 2.5
one.

Any suggestion ?

Ludwine





--

---------------------------------------------------------------------


"
"""jinkui.sjk"" <jinkui.sjk@alibaba-inc.com>","Thu, 4 Dec 2014 23:11:41 +0800",a question of  Graph build api ,dev@spark.apache.org,"hi, all

where build a graph from edge tuples with api  Graph.fromEdgeTuples, 
the edges object type is RDD[Edge], inside of  EdgeRDD.fromEdge,  EdgePartitionBuilder.add func’s param is better to be Edge object.
no need to create a new Edge object again.



  def fromEdgeTuples[VD: ClassTag](
      rawEdges: RDD[(VertexId, VertexId)],
      defaultValue: VD,
      uniqueEdges: Option[PartitionStrategy] = None,
      edgeStorageLevel: StorageLevel = StorageLevel.MEMORY_ONLY,
      vertexStorageLevel: StorageLevel = StorageLevel.MEMORY_ONLY): Graph[VD, Int] =
  {
    val edges = rawEdges.map(p => Edge(p._1, p._2, 1))
    val graph = GraphImpl(edges, defaultValue, edgeStorageLevel, vertexStorageLevel)
    uniqueEdges match {
      case Some(p) => graph.partitionBy(p).groupEdges((a, b) => a + b)
      case None => graph
    }
  }




  object GraphImpl {

  /** Create a graph from edges, setting referenced vertices to `defaultVertexAttr`. */
  def apply[VD: ClassTag, ED: ClassTag](
      edges: RDD[Edge[ED]],
      defaultVertexAttr: VD,
      edgeStorageLevel: StorageLevel,
      vertexStorageLevel: StorageLevel): GraphImpl[VD, ED] = {
    fromEdgeRDD(EdgeRDD.fromEdges(edges), defaultVertexAttr, edgeStorageLevel, vertexStorageLevel)
  }



  object EdgeRDD {
  /**
   * Creates an EdgeRDD from a set of edges.
   *
   * @tparam ED the edge attribute type
   * @tparam VD the type of the vertex attributes that may be joined with the returned EdgeRDD
   */
  def fromEdges[ED: ClassTag, VD: ClassTag](edges: RDD[Edge[ED]]): EdgeRDD[ED, VD] = {
    val edgePartitions = edges.mapPartitionsWithIndex { (pid, iter) =>
      val builder = new EdgePartitionBuilder[ED, VD]
      iter.foreach { e =>
        builder.add(e.srcId, e.dstId, e.attr)
      }
      Iterator((pid, builder.toEdgePartition))
    }
    EdgeRDD.fromEdgePartitions(edgePartitions)
  }


"
jinkui.sjk <jinkui.sjk@alibaba-inc.com>,"Thu, 4 Dec 2014 23:12:37 +0800",a question of  Graph build api ,dev@spark.apache.org,"hi, all

where build a graph from edge tuples with api  Graph.fromEdgeTuples, 
the edges object type is RDD[Edge], inside of  EdgeRDD.fromEdge,  EdgePartitionBuilder.add func’s param is better to be Edge object.
no need to create a new Edge object again.



  def fromEdgeTuples[VD: ClassTag](
      rawEdges: RDD[(VertexId, VertexId)],
      defaultValue: VD,
      uniqueEdges: Option[PartitionStrategy] = None,
      edgeStorageLevel: StorageLevel = StorageLevel.MEMORY_ONLY,
      vertexStorageLevel: StorageLevel = StorageLevel.MEMORY_ONLY): Graph[VD, Int] =
  {
    val edges = rawEdges.map(p => Edge(p._1, p._2, 1))
    val graph = GraphImpl(edges, defaultValue, edgeStorageLevel, vertexStorageLevel)
    uniqueEdges match {
      case Some(p) => graph.partitionBy(p).groupEdges((a, b) => a + b)
      case None => graph
    }
  }




  object GraphImpl {

  /** Create a graph from edges, setting referenced vertices to `defaultVertexAttr`. */
  def apply[VD: ClassTag, ED: ClassTag](
      edges: RDD[Edge[ED]],
      defaultVertexAttr: VD,
      edgeStorageLevel: StorageLevel,
      vertexStorageLevel: StorageLevel): GraphImpl[VD, ED] = {
    fromEdgeRDD(EdgeRDD.fromEdges(edges), defaultVertexAttr, edgeStorageLevel, vertexStorageLevel)
  }



  object EdgeRDD {
  /**
   * Creates an EdgeRDD from a set of edges.
   *
   * @tparam ED the edge attribute type
   * @tparam VD the type of the vertex attributes that may be joined with the returned EdgeRDD
   */
  def fromEdges[ED: ClassTag, VD: ClassTag](edges: RDD[Edge[ED]]): EdgeRDD[ED, VD] = {
    val edgePartitions = edges.mapPartitionsWithIndex { (pid, iter) =>
      val builder = new EdgePartitionBuilder[ED, VD]
      iter.foreach { e =>
        builder.add(e.srcId, e.dstId, e.attr)
      }
      Iterator((pid, builder.toEdgePartition))
    }
    EdgeRDD.fromEdgePartitions(edgePartitions)
  }


"
Imran Rashid <imran@therashids.com>,"Thu, 4 Dec 2014 09:49:46 -0800","Re: Spurious test failures, testing best practices",Matei Zaharia <matei.zaharia@gmail.com>,"I agree we should separate out the integration tests so it's easy for dev
to just run the other fast tests locally.  I opened a jira for it

https://issues.apache.org/jira/plugins/servlet/mobile#issue/SPARK-4746

ep
s
ch
ck
n
ed
o
e
t
s
t
uite
`
is
s
l
en
y
re
t
ly
nd
n`
s
to
is
s
e
en
d
t
or
ng
e
ob
lt
re
s
/484c2fb8bc0efa0e39d142087eefa9c3d5292ea3/dev%20run-tests:%20fail
/ce264e469be3641f061eabd10beb1d71ac243991/mvn%20test:%20fail
/6bc76c67aeef9c57ddd9fb2ba260fb4189dbb927/mvn%20test%20case:%20pass%20test,%20fail%20subsequent%20compile
=2&ved=0CCUQFjAB&url=http%3A%2F%2Fapache-spark-user-list.1001560.n3.nabble.com%2Fscalac-crash-when-compiling-DataTypeConversions-scala-td17083.html&ei=aRF6VJrpNKr-iAKDgYGYBQ&usg=AFQjCNHjM9m__Hrumh-ecOsSE00-JkjKBQ&sig2=zDeSqOgs02AXJXj78w5I9g&bvm=bv.80642063,d.cGE&cad=rja
/4ab0bd6e76d9fc5745eb4b45cdf13195d10efaa2/mvn%20test,%20post%20clean,%20need%20dependencies%20built
/f4c7e6fc8c301f869b00598c7b541dac243fb51e/dev%20run-tests,%20post%20clean
ests-failure-too-many-files-open-then-hang-L5260
-txt-L853
-txt-L852
-txt-L854
les-quot-exception-on-reduceByKey-td2462.html
too-many-open-files
-in-maven
eUNhuCr41B7KRPTEwMn4cga_2TNpZrWqQB8REekokxzg@mail.gmail.com%3E
"
Patrick Wendell <pwendell@gmail.com>,"Thu, 4 Dec 2014 10:26:42 -0800",Re: Ooyala Spark JobServer,Jun Feng Liu <liujunf@cn.ibm.com>,"Hey Jun,

The Ooyala server is being maintained by it's original author (Evan Chan)
here:

https://github.com/spark-jobserver/spark-jobserver

This is likely to stay as a standalone project for now, since it builds
directly on Spark's public API's.

- Patrick


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 04 Dec 2014 23:22:48 +0000",Re: zinc invocation examples,Sean Owen <sowen@cloudera.com>,"Oh, derp. I just assumed from looking at all the options that there was
something to it. Thanks Sean.


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 05 Dec 2014 00:05:26 +0000",Re: Unit tests in < 5 minutes,Reynold Xin <rxin@databricks.com>,"fwiw, when we did this work in HBase, we categorized the tests. Then some
tests can share a single jvm, while some others need to be isolated in
their own jvm. Nevertheless surefire can still run them in parallel by
starting/stopping several jvm.

I think we need to do this as well. Perhaps the test naming hierarchy can
be used to group non-parallelizable tests in the same JVM.

For example, here are some Hive tests from our project:

org.apache.spark.sql.hive.StatisticsSuite
org.apache.spark.sql.hive.execution.HiveQuerySuite
org.apache.spark.sql.QueryTest
org.apache.spark.sql.parquet.HiveParquetSuite

If we group tests by the first 5 parts of their name (e.g.
org.apache.spark.sql.hive), then we’d have the first 2 tests run in the
same JVM, and the next 2 tests each run in their own JVM.

I’m new to this stuff so I’m not sure if I’m going about this in the right
way, but you can see my attempt with this approach on GitHub
<https://github.com/nchammas/spark/blob/ab127b798dbfa9399833d546e627f9651b060918/project/SparkBuild.scala#L388-L397>,
as well as the related discussion on JIRA
<https://issues.apache.org/jira/browse/SPARK-3431>.

If anyone has more feedback on this, I’d love to hear it (either on this
thread or in the JIRA issue).

Nick
​


:
"
Ted Yu <yuzhihong@gmail.com>,"Thu, 4 Dec 2014 16:33:22 -0800",Re: Unit tests in < 5 minutes,Nicholas Chammas <nicholas.chammas@gmail.com>,"Have you seen this thread http://search-hadoop.com/m/JW1q5xxSAa2 ?

Test categorization in HBase is done through maven-surefire-plugin

Cheers

m

in the
g about this in the right
b060918/project/SparkBuild.scala#L388-L397>,
on this
"
Jianshi Huang <jianshi.huang@gmail.com>,"Fri, 5 Dec 2014 11:37:50 +0800",Exception adding resource files in latest Spark,"""dev@spark.apache.org"" <dev@spark.apache.org>, user <user@spark.apache.org>","I got the following error during Spark startup (Yarn-client mode):

14/12/04 19:33:58 INFO Client: Uploading resource
file:/x/home/jianshuang/spark/spark-latest/lib/datanucleus-api-jdo-3.2.6.jar
->
hdfs://stampy/user/jianshuang/.sparkStaging/application_1404410683830_531767/datanucleus-api-jdo-3.2.6.jar
java.lang.IllegalArgumentException: Wrong FS:
hdfs://stampy/user/jianshuang/.sparkStaging/application_1404410683830_531767/datanucleus-api-jdo-3.2.6.jar,
expected: file:///
        at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:643)
        at
org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:79)
        at
org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:506)
        at
org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:724)
        at
org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:501)
        at
org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:397)
        at
org.apache.spark.deploy.yarn.ClientDistributedCacheManager.addResource(ClientDistributedCacheManager.scala:67)
        at
org.apache.spark.deploy.yarn.ClientBase$$anonfun$prepareLocalResources$5.apply(ClientBase.scala:257)
        at
org.apache.spark.deploy.yarn.ClientBase$$anonfun$prepareLocalResources$5.apply(ClientBase.scala:242)
        at scala.Option.foreach(Option.scala:236)
        at
org.apache.spark.deploy.yarn.ClientBase$class.prepareLocalResources(ClientBase.scala:242)
        at
org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:35)
        at
org.apache.spark.deploy.yarn.ClientBase$class.createContainerLaunchContext(ClientBase.scala:350)
        at
org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:35)
        at
org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:80)
        at
org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57)
        at
org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:140)
        at org.apache.spark.SparkContext.<init>(SparkContext.scala:335)
        at
org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:986)
        at $iwC$$iwC.<init>(<console>:9)
        at $iwC.<init>(<console>:18)
        at <init>(<console>:20)
        at .<init>(<console>:24)

I'm using latest Spark built from master HEAD yesterday. Is this a bug?

-- 
Jianshi Huang

LinkedIn: jianshi
Twitter: @jshuang
Github & Blog: http://huangjs.github.com/
"
Jianshi Huang <jianshi.huang@gmail.com>,"Fri, 5 Dec 2014 11:54:50 +0800",Re: Exception adding resource files in latest Spark,"""dev@spark.apache.org"" <dev@spark.apache.org>, user <user@spark.apache.org>","Looks like somehow Spark failed to find the core-site.xml in /et/hadoop/conf

I've already set the following env variables:

export YARN_CONF_DIR=/etc/hadoop/conf
export HADOOP_CONF_DIR=/etc/hadoop/conf
export HBASE_CONF_DIR=/etc/hbase/conf

Should I put $HADOOP_CONF_DIR/* to HADOOP_CLASSPATH?

Jianshi





-- 
Jianshi Huang

LinkedIn: jianshi
Twitter: @jshuang
Github & Blog: http://huangjs.github.com/
"
Jianshi Huang <jianshi.huang@gmail.com>,"Fri, 5 Dec 2014 12:02:11 +0800",Re: Exception adding resource files in latest Spark,"""dev@spark.apache.org"" <dev@spark.apache.org>, user <user@spark.apache.org>","Actually my HADOOP_CLASSPATH has already been set to include
/etc/hadoop/conf/*

export
HADOOP_CLASSPATH=/etc/hbase/conf/hbase-site.xml:/usr/lib/hbase/lib/hbase-protocol.jar:$(hbase
classpath)

Jianshi





-- 
Jianshi Huang

LinkedIn: jianshi
Twitter: @jshuang
Github & Blog: http://huangjs.github.com/
"
Jianshi Huang <jianshi.huang@gmail.com>,"Fri, 5 Dec 2014 12:45:51 +0800",Re: Exception adding resource files in latest Spark,"""dev@spark.apache.org"" <dev@spark.apache.org>, user <user@spark.apache.org>","Looks like the datanucleus*.jar shouldn't appear in the hdfs path in
Yarn-client mode.

Maybe this patch broke yarn-client.

https://github.com/apache/spark/commit/a975dc32799bb8a14f9e1c76defaaa7cfbaf8b53

Jianshi





-- 
Jianshi Huang

LinkedIn: jianshi
Twitter: @jshuang
Github & Blog: http://huangjs.github.com/
"
Jianshi Huang <jianshi.huang@gmail.com>,"Fri, 5 Dec 2014 13:31:52 +0800",Re: Exception adding resource files in latest Spark,"""dev@spark.apache.org"" <dev@spark.apache.org>, user <user@spark.apache.org>","Correction:

According to Liancheng, this hotfix might be the root cause:


https://github.com/apache/spark/commit/38cb2c3a36a5c9ead4494cbc3dde008c2f0698ce

Jianshi





-- 
Jianshi Huang

LinkedIn: jianshi
Twitter: @jshuang
Github & Blog: http://huangjs.github.com/
"
Jianshi Huang <jianshi.huang@gmail.com>,"Fri, 5 Dec 2014 13:51:51 +0800",Re: Exception adding resource files in latest Spark,"""dev@spark.apache.org"" <dev@spark.apache.org>, user <user@spark.apache.org>","I created a ticket for this:

  https://issues.apache.org/jira/browse/SPARK-4757


Jianshi





-- 
Jianshi Huang

LinkedIn: jianshi
Twitter: @jshuang
Github & Blog: http://huangjs.github.com/
"
Patrick Wendell <pwendell@gmail.com>,"Thu, 4 Dec 2014 22:12:51 -0800",Re: Exception adding resource files in latest Spark,Jianshi Huang <jianshi.huang@gmail.com>,"Thanks for flagging this. I reverted the relevant YARN fix in Spark
1.2 release. We can try to debug this in master.


---------------------------------------------------------------------


"
Jianshi Huang <jianshi.huang@gmail.com>,"Fri, 5 Dec 2014 15:30:08 +0800",Re: Auto BroadcastJoin optimization failed in latest Spark,"""Cheng, Hao"" <hao.cheng@intel.com>","Sorry for the late of follow-up.

I used Hao's DESC EXTENDED command and found some clue:

new (broadcast broken Spark build):
parameters:{numFiles=0, EXTERNAL=TRUE, transient_lastDdlTime=1417763892,
COLUMN_STATS_ACCURATE=false, totalSize=0, numRows=-1, rawDataSize=-1}

old (broadcast working Spark build):
parameters:{EXTERNAL=TRUE, transient_lastDdlTime=1417763591,
totalSize=56166}

Looks like the table size computation failed in the latest version.

I've run the analyze command:

  ANALYZE TABLE $table COMPUTE STATISTICS noscan

And the tables are created from Parquet files:

e.g.
CREATE EXTERNAL TABLE table1 (
  code int,
  desc string
)
STORED AS PARQUET
LOCATION '/user/jianshuang/data/dim_tables/table1.parquet'


Anyone knows what went wrong?


Thanks,
Jianshi




t the
:
d
e
obably is what
adcast
s



-- 
Jianshi Huang

LinkedIn: jianshi
Twitter: @jshuang
Github & Blog: http://huangjs.github.com/
"
Jianshi Huang <jianshi.huang@gmail.com>,"Fri, 5 Dec 2014 15:31:56 +0800",drop table if exists throws exception,"user <user@spark.apache.org>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

I got exception saying Hive: NoSuchObjectException(message:<table> table
not found)

when running ""DROP TABLE IF EXISTS <table>""

Looks like a new regression in Hive module.

Anyone can confirm this?

Thanks,
-- 
Jianshi Huang

LinkedIn: jianshi
Twitter: @jshuang
Github & Blog: http://huangjs.github.com/
"
Jianshi Huang <jianshi.huang@gmail.com>,"Fri, 5 Dec 2014 15:33:17 +0800",Re: Auto BroadcastJoin optimization failed in latest Spark,"""Cheng, Hao"" <hao.cheng@intel.com>","If I run ANALYZE without NOSCAN, then Hive can successfully get the size:

parameters:{numFiles=0, EXTERNAL=TRUE, transient_lastDdlTime=1417764589,
COLUMN_STATS_ACCURATE=true, totalSize=0, numRows=1156, rawDataSize=76296}

Is Hive's PARQUET support broken?

Jianshi



3892,
=-1}
et the
):
ld
s
robably is what
oadcast
ins
.



-- 
Jianshi Huang

LinkedIn: jianshi
Twitter: @jshuang
Github & Blog: http://huangjs.github.com/
"
Jianshi Huang <jianshi.huang@gmail.com>,"Fri, 5 Dec 2014 15:50:30 +0800",Re: Auto BroadcastJoin optimization failed in latest Spark,"""Cheng, Hao"" <hao.cheng@intel.com>","With Liancheng's suggestion, I've tried setting

 spark.sql.hive.convertMetastoreParquet  false

but still analyze noscan return -1 in rawDataSize

Jianshi



4589,
=76296}
63892,
=-1}
get the
sure
:
probably is what
roadcast
oins
2.



-- 
Jianshi Huang

LinkedIn: jianshi
Twitter: @jshuang
Github & Blog: http://huangjs.github.com/
"
Ryan Williams <ryan.blake.williams@gmail.com>,"Fri, 05 Dec 2014 19:05:15 +0000",Re: zinc invocation examples,"Nicholas Chammas <nicholas.chammas@gmail.com>, Sean Owen <sowen@cloudera.com>","fwiw I've been using `zinc -scala-home $SCALA_HOME -nailed -start` which:

- starts a nailgun server as well,
- uses my installed scala 2.{10,11}, as opposed to zinc's default 2.9.2
<https://github.com/typesafehub/zinc#scala>: ""If no options are passed to
locate a version of Scala then Scala 2.9.2 is used by default (which is
bundled with zinc).""

The latter seems like it might be especially important.



"
Andrew Or <andrew@databricks.com>,"Fri, 5 Dec 2014 11:05:28 -0800",Re: Unit tests in < 5 minutes,Ted Yu <yuzhihong@gmail.com>,"@Patrick and Josh actually we went even further than that. We simply
disable the UI for most tests and these used to be the single largest
source of port conflict.
"
Patrick Wendell <pwendell@gmail.com>,"Fri, 5 Dec 2014 11:10:58 -0800",Re: zinc invocation examples,Ryan Williams <ryan.blake.williams@gmail.com>,"script to ""sbt/sbt"" that transparently downloads Zinc, Scala, and
Maven in a subdirectory of Spark and sets it up correctly. I.e.
""build/mvn"".

Outside of brew for MacOS there aren't good Zinc packages, and it's a
pain to figure out how to set it up.

https://issues.apache.org/jira/browse/SPARK-4501

Prashant Sharma looked at this for a bit but I don't think he's
working on it actively any more, so if someone wanted to do this, I'd
be extremely grateful.

- Patrick


---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Fri, 5 Dec 2014 11:42:51 -0800",Re: drop table if exists throws exception,Jianshi Huang <jianshi.huang@gmail.com>,"The command run fine for me on master.  Note that Hive does print an
exception in the logs, but that exception does not propogate to user code.


"
Mark Hamstra <mark@clearstorydata.com>,"Fri, 5 Dec 2014 11:45:04 -0800",Re: drop table if exists throws exception,Michael Armbrust <michael@databricks.com>,"And that is no different from how Hive has worked for a long time.


"
kb <kendb15@hotmail.com>,"Fri, 5 Dec 2014 13:03:19 -0700 (MST)",CREATE TABLE AS SELECT does not work with temp tables in 1.2.0,dev@spark.incubator.apache.org,"I am having trouble getting ""create table as select"" or saveAsTable from a
hiveContext to work with temp tables in spark 1.2.  No issues in 1.1.0 or
1.1.1

Simple modification to test case in the hive SQLQuerySuite.scala:

test(""double nested data"") {
    sparkContext.parallelize(Nested1(Nested2(Nested3(1))) ::
Nil).registerTempTable(""nested"")
    checkAnswer(
      sql(""SELECT f1.f2.f3 FROM nested""),
      1)
    checkAnswer(sql(""CREATE TABLE test_ctas_1234 AS SELECT * from nested""),
Seq.empty[Row])
    checkAnswer(
      sql(""SELECT * FROM test_ctas_1234""),
      sql(""SELECT * FROM nested"").collect().toSeq)
  }


output:

11:57:15.974 ERROR org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:
org.apache.hadoop.hive.ql.parse.SemanticException: Line 1:45 Table not found
'nested'
	at
org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getMetaData(SemanticAnalyzer.java:1243)
	at
org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getMetaData(SemanticAnalyzer.java:1192)
	at
org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:9209)
	at
org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:327)
	at
org.apache.spark.sql.hive.execution.CreateTableAsSelect.metastoreRelation$lzycompute(CreateTableAsSelect.scala:59)
	at
org.apache.spark.sql.hive.execution.CreateTableAsSelect.metastoreRelation(CreateTableAsSelect.scala:55)
	at
org.apache.spark.sql.hive.execution.CreateTableAsSelect.sideEffectResult$lzycompute(CreateTableAsSelect.scala:82)
	at
org.apache.spark.sql.hive.execution.CreateTableAsSelect.sideEffectResult(CreateTableAsSelect.scala:70)
	at
org.apache.spark.sql.hive.execution.CreateTableAsSelect.execute(CreateTableAsSelect.scala:89)
	at
org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:425)
	at
org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:425)
	at org.apache.spark.sql.SchemaRDDLike$class.$init$(SchemaRDDLike.scala:58)
	at org.apache.spark.sql.SchemaRDD.<init>(SchemaRDD.scala:105)
	at org.apache.spark.sql.hive.HiveContext.sql(HiveContext.scala:103)
	at
org.apache.spark.sql.hive.execution.SQLQuerySuite$$anonfun$4.apply$mcV$sp(SQLQuerySuite.scala:122)
	at
org.apache.spark.sql.hive.execution.SQLQuerySuite$$anonfun$4.apply(SQLQuerySuite.scala:117)
	at
org.apache.spark.sql.hive.execution.SQLQuerySuite$$anonfun$4.apply(SQLQuerySuite.scala:117)
	at
org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.scalatest.Suite$class.withFixture(Suite.scala:1122)
	at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)
	at
org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at
org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at
org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1555)
	at
org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at
org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at
org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at
org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at
org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at
org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.scalatest.FunSuite.run(FunSuite.scala:1555)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
	at
org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
	at
org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
	at
org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
	at
org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
	at
org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
	at
org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
	at org.scalatest.tools.Runner$.run(Runner.scala:883)
	at org.scalatest.tools.Runner.run(Runner.scala)
	at
org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.runScalaTest2(ScalaTestRunner.java:141)
	at
org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.main(ScalaTestRunner.java:32)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:134)





--

---------------------------------------------------------------------


"
<spark.dubovsky.jakub@seznam.cz>,"Fri, 05 Dec 2014 21:51:32 +0100 (CET)",Protobuf version in mvn vs sbt,<dev@spark.apache.org>,"Hi devs,

  I play with your amazing Spark here in Prague for some time. I have 
stumbled on a thing which I like to ask about. I create assembly jars from 
source and then use it to run simple jobs on our 2.3.0-cdh5.1.3 cluster 
using yarn. Example of my usage [1]. Formerly I had started to use sbt for 
creating assemblies like this [2] which runs just fine. Then reading those 
maven-prefered stories here on dev list I found make-distribution.sh script 
in root of codebase and wanted to give it a try. I used it to create 
assembly by both [3] and [4].

  But I am not able to use assemblies created by make-distribution because 
it refuses to be submited to cluster. Here is what happens:
- run [3] or [4]
- recompile app agains new assembly
- submit job using new assembly by [1] like command
- submit fails with important parts of stack trace being [5]

  My guess is that it is due to improper version of protobuf included in 
assembly jar. My questions are:
- Can you confirm this hypothesis?
- What is the difference between sbt and mvn way of creating assembly? I 
mean sbt works and mvn not...
- What additional option I need to pass to make-distribution to make it 
work?

  Any help/explanation here would be appreciated

  Jakub
----------------------
[1] ./bin/spark-submit --num-executors 200 --master yarn-cluster --conf 
spark.yarn.jar=assembly/target/scala-2.10/spark-assembly-1.2.1-SNAPSHOT-
hadoop2.3.0-cdh5.1.3.jar --class org.apache.spark.mllib.
CreateGuidDomainDictionary root-0.1.jar ${args}

[2] ./sbt/sbt -Dhadoop.version=2.3.0-cdh5.1.3 -Pyarn -Phive assembly/
assembly

[3] ./make-distribution.sh -Dhadoop.version=2.3.0-cdh5.1.3 -Pyarn -Phive -
DskipTests

[4] ./make-distribution.sh -Dyarn.version=2.3.0 -Dhadoop.version=2.3.0-cdh
5.1.3 -Pyarn -Phive -DskipTests

[5]Exception in thread ""main"" org.apache.hadoop.yarn.exceptions.
YarnRuntimeException: java.lang.reflect.InvocationTargetException
        at org.apache.hadoop.yarn.factories.impl.pb.RpcClientFactoryPBImpl.
getClient(RpcClientFactoryPBImpl.java:79)
        at org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC.getProxy
(HadoopYarnProtoRPC.java:48)
        at org.apache.hadoop.yarn.client.RMProxy$1.run(RMProxy.java:134)
...
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native 
Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance
(NativeConstructorAccessorImpl.java:39)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance
(DelegatingConstructorAccessorImpl.java:27)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
        at org.apache.hadoop.yarn.factories.impl.pb.RpcClientFactoryPBImpl.
getClient(RpcClientFactoryPBImpl.java:76)
... 27 more
Caused by: java.lang.VerifyError: class org.apache.hadoop.yarn.proto.
YarnServiceProtos$SubmitApplicationRequestProto overrides final method 
getUnknownFields.()Lcom/google/protobuf/UnknownFieldSet;
        at java.lang.ClassLoader.defineClass1(Native Method)
        at java.lang.ClassLoader.defineClassCond(ClassLoader.java:631)

"
Michael Armbrust <michael@databricks.com>,"Fri, 5 Dec 2014 12:51:21 -0800",Re: CREATE TABLE AS SELECT does not work with temp tables in 1.2.0,kb <kendb15@hotmail.com>,"Thanks for reporting.  This looks like a regression related to:
https://github.com/apache/spark/pull/2570

I've filed it here: https://issues.apache.org/jira/browse/SPARK-4769


"
Marcelo Vanzin <vanzin@cloudera.com>,"Fri, 5 Dec 2014 12:54:40 -0800",Re: Protobuf version in mvn vs sbt,spark.dubovsky.jakub@seznam.cz,"When building against Hadoop 2.x, you need to enable the appropriate
profile, aside from just specifying the version. e.g. ""-Phadoop-2.3""
for Hadoop 2.3.




-- 
Marcelo

---------------------------------------------------------------------


"
DB Tsai <dbtsai@dbtsai.com>,"Fri, 5 Dec 2014 13:29:10 -0800",Re: Protobuf version in mvn vs sbt,Marcelo Vanzin <vanzin@cloudera.com>,"As Marcelo said, CDH5.3 is based on hadoop 2.3, so please try

./make-distribution.sh -Pyarn -Phive -Phadoop-2.3
-Dhadoop.version=2.3.0-cdh5.1.3 -DskipTests

See the detail of how to change the profile at
https://spark.apache.org/docs/latest/building-with-maven.html

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Fri, 5 Dec 2014 15:38:00 -0600",Re: Protobuf version in mvn vs sbt,DB Tsai <dbtsai@dbtsai.com>,"(Nit: CDH *5.1.x*, including 5.1.3, is derived from Hadoop 2.3.x. 5.3
is based on 2.5.x)


---------------------------------------------------------------------


"
DB Tsai <dbtsai@dbtsai.com>,"Fri, 5 Dec 2014 13:48:29 -0800",Re: Protobuf version in mvn vs sbt,Sean Owen <sowen@cloudera.com>,"oh, I meant to say cdh5.1.3 used by Jakub's company is based on 2.3. You
can see it from the first part of the Cloudera's version number - ""2.3.0-cdh
5.1.3"".


Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai


"
Judy Nash <judynash@exchange.microsoft.com>,"Fri, 5 Dec 2014 21:51:54 +0000",build in IntelliJ IDEA,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi everyone,

Have a newbie question on using IntelliJ to build and debug.

I followed this wiki to setup IntelliJ:
https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools#UsefulDeveloperTools-BuildingSparkinIntelliJIDEA

Afterward I tried to build via Toolbar (Build > Rebuild Project).
The action fails with the error message:
Cannot start compiler: the SDK is not specified.

What SDK do I need to specify to get the build working?

Thanks,
Judy
"
Patrick Wendell <pwendell@gmail.com>,"Fri, 5 Dec 2014 14:02:49 -0800",Re: [VOTE] Release Apache Spark 1.2.0 (RC1),Takeshi Yamamuro <linguin.m.s@gmail.com>,"Hey All,

Thanks all for the continued testing!

The issue I mentioned earlier SPARK-4498 was fixed earlier this week
(hat tip to Mark Hamstra who contributed to fix).

In the interim a few smaller blocker-level issues with Spark SQL were
found and fixed (SPARK-4753, SPARK-4552, SPARK-4761).

There is currently an outstanding issue (SPARK-4740[1]) in Spark core
that needs to be fixed.

I want to thank in particular Shopify and Intel China who have
identified and helped test blocker issues with the release. This type
of workload testing around releases is really helpful for us.


- Patrick


---------------------------------------------------------------------


"
Josh Rosen <rosenville@gmail.com>,"Fri, 5 Dec 2014 15:21:14 -0800",Re: build in IntelliJ IDEA,"Judy Nash <judynash@exchange.microsoft.com>, 
 ""=?utf-8?Q?dev=40spark.apache.org?="" <dev@spark.apache.org>","If you go to “File -> Project Structure” and click on “Project” under the “Project settings” heading, do you see an entry for “Project SDK?”  If not, you should click “New…” and configure a JDK; by default, I think IntelliJ should figure out a correct path to your system JDK, so you should just be able to hit “Ok” then rebuild your project.   For reference, here’s a screenshot showing what my version of that window looks like: http://i.imgur.com/hRfQjIi.png


Hi everyone,  

Have a newbie question on using IntelliJ to build and debug.  

I followed this wiki to setup IntelliJ:  
https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools#UsefulDeveloperTools-BuildingSparkinIntelliJIDEA  

Afterward I tried to build via Toolbar (Build > Rebuild Project).  
The action fails with the error message:  
Cannot start compiler: the SDK is not specified.  

What SDK do I need to specify to get the build working?  

Thanks,  
Judy  
"
Jianshi Huang <jianshi.huang@gmail.com>,"Sat, 6 Dec 2014 08:29:29 +0800",Re: drop table if exists throws exception,Mark Hamstra <mark@clearstorydata.com>,"I see. The resulting SchemaRDD is returned so like Michael said, the
exception does not propogate to user code.

However printing out the following log is confusing :)

scala> sql(""drop table if exists abc"")
14/12/05 16:27:02 INFO ParseDriver: Parsing command: drop table if exists
abc
14/12/05 16:27:02 INFO ParseDriver: Parse Completed
14/12/05 16:27:02 INFO PerfLogger: <PERFLOG method=Driver.run
from=org.apache.hadoop.hive.ql.Driver>
14/12/05 16:27:02 INFO PerfLogger: <PERFLOG method=TimeToSubmit
from=org.apache.hadoop.hive.ql.Driver>
14/12/05 16:27:02 INFO Driver: Concurrency mode is disabled, not creating a
lock manager
14/12/05 16:27:02 INFO PerfLogger: <PERFLOG method=compile
from=org.apache.hadoop.hive.ql.Driver>
14/12/05 16:27:02 INFO PerfLogger: <PERFLOG method=parse
from=org.apache.hadoop.hive.ql.Driver>
14/12/05 16:27:02 INFO ParseDriver: Parsing command: DROP TABLE IF EXISTS
abc
14/12/05 16:27:02 INFO ParseDriver: Parse Completed
14/12/05 16:27:02 INFO PerfLogger: </PERFLOG method=parse
start=1417825622650 end=1417825622650 duration=0
from=org.apache.hadoop.hive.ql.Driver>
14/12/05 16:27:02 INFO PerfLogger: <PERFLOG method=semanticAnalyze
from=org.apache.hadoop.hive.ql.Driver>
14/12/05 16:27:02 INFO HiveMetaStore: 0: get_table : db=default tbl=abc
14/12/05 16:27:02 INFO audit: ugi=jianshuang    ip=unknown-ip-addr
 cmd=get_table : db=default tbl=abc
14/12/05 16:27:02 INFO Driver: Semantic Analysis Completed
14/12/05 16:27:02 INFO PerfLogger: </PERFLOG method=semanticAnalyze
start=1417825622650 end=1417825622653 duration=3
from=org.apache.hadoop.hive.ql.Driver>
14/12/05 16:27:02 INFO Driver: Returning Hive schema:
Schema(fieldSchemas:null, properties:null)
14/12/05 16:27:02 INFO PerfLogger: </PERFLOG method=compile
start=1417825622650 end=1417825622654 duration=4
from=org.apache.hadoop.hive.ql.Driver>
14/12/05 16:27:02 INFO PerfLogger: <PERFLOG method=Driver.execute
from=org.apache.hadoop.hive.ql.Driver>
14/12/05 16:27:02 INFO Driver: Starting command: DROP TABLE IF EXISTS abc
14/12/05 16:27:02 INFO PerfLogger: </PERFLOG method=TimeToSubmit
start=1417825622650 end=1417825622654 duration=4
from=org.apache.hadoop.hive.ql.Driver>
14/12/05 16:27:02 INFO PerfLogger: <PERFLOG method=runTasks
from=org.apache.hadoop.hive.ql.Driver>
14/12/05 16:27:02 INFO PerfLogger: <PERFLOG method=task.DDL.Stage-0
from=org.apache.hadoop.hive.ql.Driver>
14/12/05 16:27:02 INFO HiveMetaStore: 0: get_table : db=default tbl=abc
14/12/05 16:27:02 INFO audit: ugi=jianshuang    ip=unknown-ip-addr
 cmd=get_table : db=default tbl=abc
14/12/05 16:27:02 ERROR Hive: NoSuchObjectException(message:default.abc
table not found)
        at
org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_table(HiveMetaStore.java:1560)
        at sun.reflect.GeneratedMethodAccessor57.invoke(Unknown Source)
        at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at
org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:105)




-- 
Jianshi Huang

LinkedIn: jianshi
Twitter: @jshuang
Github & Blog: http://huangjs.github.com/
"
Takeshi Yamamuro <linguin.m.s@gmail.com>,"Sat, 6 Dec 2014 19:40:42 +0900",Re: a question of Graph build api,"""jinkui.sjk"" <jinkui.sjk@alibaba-inc.com>","Hi,

Yes, I think so.
However, EdgePartitionBuilder might have edge lists as other types instead
of Edge in a future.
This is because the change can make fast graph construction.
See also SPARK-1987.
https://issues.apache.org/jira/browse/SPARK-1987

Thanks,
takeshi



t.
)
"
<spark.dubovsky.jakub@seznam.cz>,"Sat, 06 Dec 2014 14:46:15 +0100 (CET)",Re: Protobuf version in mvn vs sbt,<dbtsai@dbtsai.com>,"Hi,

  I have created assembly with additional hadoop-2.3 profile and submit is 
smooth now.

  Thank you for quick reply!

  Now I can move to another maybe dev related problem. Posted to user 
mailinglist under ""Including data nucleus tools"".

  Jakub


---------- Původní zpráva ----------
Od: DB Tsai <dbtsai@dbtsai.com>
Komu: Marcelo Vanzin <vanzin@cloudera.com>
Datum: 5. 12. 2014 22:31:13
Předmět: Re: Protobuf version in mvn vs sbt

""As Marcelo said, CDH5.3 is based on hadoop 2.3, so please try

./make-distribution.sh -Pyarn -Phive -Phadoop-2.3
-Dhadoop.version=2.3.0-cdh5.1.3 -DskipTests

See the detail of how to change the profile at
https://spark.apache.org/docs/latest/building-with-maven.html

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai


e:





from


for

those

script

e

I


OT-

ive 
-
3.0-
cdh







---------------------------------------------------------------------
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sat, 06 Dec 2014 18:54:29 +0000",Re: Unit tests in < 5 minutes,"Andrew Or <andrew@databricks.com>, Ted Yu <yuzhihong@gmail.com>","Ted,

I posted some updates
<https://issues.apache.org/jira/browse/SPARK-3431?focusedCommentId=14236540&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14236540>
on
JIRA on my progress (or lack thereof) getting SBT to parallelize test
suites properly. I'm currently stuck with SBT / ScalaTest, so I may move on
to trying Maven.

Andrew,

next step will probably be to use containers (i.e. Docker) to allow more
parallelization, especially for those tests that, for example, contend for
ports.

Nick


"
Ted Yu <yuzhihong@gmail.com>,"Sat, 6 Dec 2014 10:57:09 -0800",Re: Unit tests in < 5 minutes,Nicholas Chammas <nicholas.chammas@gmail.com>,"bq. I may move on to trying Maven.

Maven is my favorite :-)


"
"""Cheng, Hao"" <hao.cheng@intel.com>","Sun, 7 Dec 2014 04:28:07 +0000",RE: CREATE TABLE AS SELECT does not work with temp tables in 1.2.0,"Michael Armbrust <michael@databricks.com>, kb <kendb15@hotmail.com>","I've created(reused) the PR https://github.com/apache/spark/pull/3336, hopefully we can fix this regression.

Thanks for the reporting.

Cheng Hao

-----Original Message-----
From: Michael Armbrust [mailto:michael@databricks.com] 
Sent: Saturday, December 6, 2014 4:51 AM
To: kb
Cc: dev@spark.incubator.apache.org; Cheng Hao
Subject: Re: CREATE TABLE AS SELECT does not work with temp tables in 1.2.0

Thanks for reporting.  This looks like a regression related to:
https://github.com/apache/spark/pull/2570

I've filed it here: https://issues.apache.org/jira/browse/SPARK-4769

On Fri, Dec 5, 2014 at 12:03 PM, kb <kendb15@hotmail.com> wrote:

> I am having trouble getting ""create table as select"" or saveAsTable 
> from a hiveContext to work with temp tables in spark 1.2.  No issues 
> in 1.1.0 or
> 1.1.1
>
> Simple modification to test case in the hive SQLQuerySuite.scala:
>
> test(""double nested data"") {
>     sparkContext.parallelize(Nested1(Nested2(Nested3(1))) ::
> Nil).registerTempTable(""nested"")
>     checkAnswer(
>       sql(""SELECT f1.f2.f3 FROM nested""),
>       1)
>     checkAnswer(sql(""CREATE TABLE test_ctas_1234 AS SELECT * from 
> nested""),
> Seq.empty[Row])
>     checkAnswer(
>       sql(""SELECT * FROM test_ctas_1234""),
>       sql(""SELECT * FROM nested"").collect().toSeq)
>   }
>
>
> output:
>
> 11:57:15.974 ERROR org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:
> org.apache.hadoop.hive.ql.parse.SemanticException: Line 1:45 Table not 
> found 'nested'
>         at
>
> org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getMetaData(SemanticAnalyzer.java:1243)
>         at
>
> org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getMetaData(SemanticAnalyzer.java:1192)
>         at
>
> org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:9209)
>         at
>
> org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:327)
>         at
>
> org.apache.spark.sql.hive.execution.CreateTableAsSelect.metastoreRelation$lzycompute(CreateTableAsSelect.scala:59)
>         at
>
> org.apache.spark.sql.hive.execution.CreateTableAsSelect.metastoreRelation(CreateTableAsSelect.scala:55)
>         at
>
> org.apache.spark.sql.hive.execution.CreateTableAsSelect.sideEffectResult$lzycompute(CreateTableAsSelect.scala:82)
>         at
>
> org.apache.spark.sql.hive.execution.CreateTableAsSelect.sideEffectResult(CreateTableAsSelect.scala:70)
>         at
>
> org.apache.spark.sql.hive.execution.CreateTableAsSelect.execute(CreateTableAsSelect.scala:89)
>         at
>
> org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:425)
>         at
> org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:425)
>         at
> org.apache.spark.sql.SchemaRDDLike$class.$init$(SchemaRDDLike.scala:58)
>         at org.apache.spark.sql.SchemaRDD.<init>(SchemaRDD.scala:105)
>         at org.apache.spark.sql.hive.HiveContext.sql(HiveContext.scala:103)
>         at
>
> org.apache.spark.sql.hive.execution.SQLQuerySuite$$anonfun$4.apply$mcV$sp(SQLQuerySuite.scala:122)
>         at
>
> org.apache.spark.sql.hive.execution.SQLQuerySuite$$anonfun$4.apply(SQLQuerySuite.scala:117)
>         at
>
> org.apache.spark.sql.hive.execution.SQLQuerySuite$$anonfun$4.apply(SQLQuerySuite.scala:117)
>         at
>
> org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
>         at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
>         at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
>         at org.scalatest.Transformer.apply(Transformer.scala:22)
>         at org.scalatest.Transformer.apply(Transformer.scala:20)
>         at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
>         at org.scalatest.Suite$class.withFixture(Suite.scala:1122)
>         at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)
>         at
>
> org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
>         at
> org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
>         at
> org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
>         at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
>         at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
>         at org.scalatest.FunSuite.runTest(FunSuite.scala:1555)
>         at
>
> org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
>         at
>
> org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
>         at
>
> org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
>         at
>
> org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
>         at scala.collection.immutable.List.foreach(List.scala:318)
>         at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
>         at
> org.scalatest.SuperEngine.org
> $scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
>         at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
>         at
> org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
>         at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
>         at org.scalatest.Suite$class.run(Suite.scala:1424)
>         at
> org.scalatest.FunSuite.org
> $scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
>         at
> org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
>         at
> org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
>         at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
>         at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
>         at org.scalatest.FunSuite.run(FunSuite.scala:1555)
>         at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
>         at
>
> org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
>         at
>
> org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
>         at scala.collection.immutable.List.foreach(List.scala:318)
>         at
> org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
>         at
>
> org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
>         at
>
> org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
>         at
>
> org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
>         at
>
> org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
>         at org.scalatest.tools.Runner$.run(Runner.scala:883)
>         at org.scalatest.tools.Runner.run(Runner.scala)
>         at
>
> org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.runScalaTest2(ScalaTestRunner.java:141)
>         at
>
> org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.main(ScalaTestRunner.java:32)
>         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
>         at
>
> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
>         at
>
> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
>         at java.lang.reflect.Method.invoke(Method.java:606)
>         at
> com.intellij.rt.execution.application.AppMain.main(AppMain.java:134)
>
>
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/CREATE-TABLE
> -AS-SELECT-does-not-work-with-temp-tables-in-1-2-0-tp9662.html
> Sent from the Apache Spark Developers List mailing list archive at 
> Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For 
> additional commands, e-mail: dev-help@spark.apache.org
>
>
"
Jianshi Huang <jianshi.huang@gmail.com>,"Sun, 7 Dec 2014 12:28:48 +0800","Re: Hive Problem in Pig generated Parquet file schema in CREATE
 EXTERNAL TABLE (e.g. bag::col1)","user <user@spark.apache.org>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Ok, found another possible bug in Hive.

My current solution is to use ALTER TABLE CHANGE to rename the column names.

The problem is after renaming the column names, the value of the columns
became all NULL.

Before renaming:
scala> sql(""select `sorted::cre_ts` from pmt limit 1"").collect
res12: Array[org.apache.spark.sql.Row] = Array([12/02/2014 07:38:54])

Execute renaming:
scala> sql(""alter table pmt change `sorted::cre_ts` cre_ts string"")
res13: org.apache.spark.sql.SchemaRDD =
SchemaRDD[972] at RDD at SchemaRDD.scala:108
== Query Plan ==
<Native command: executed by Hive>

After renaming:
scala> sql(""select cre_ts from pmt limit 1"").collect
res16: Array[org.apache.spark.sql.Row] = Array([null])

I created a JIRA for it:

  https://issues.apache.org/jira/browse/SPARK-4781


Jianshi





-- 
Jianshi Huang

LinkedIn: jianshi
Twitter: @jshuang
Github & Blog: http://huangjs.github.com/
"
Judy Nash <judynash@exchange.microsoft.com>,"Mon, 8 Dec 2014 05:07:10 +0000",RE: build in IntelliJ IDEA,"Josh Rosen <rosenville@gmail.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","Thanks Josh. That was the issue.

From: Josh Rosen [mai 3:21 PM
To: Judy Nash; dev@spark.apache.org
Subject: Re: build in IntelliJ IDEA

If you go to “File -> Project Structure” and click on “Project” under the “Project settings” heading, do you see an entry for “Project SDK?”  If not, you should click “New…” and configure a JDK; by default, I think IntelliJ should figure out a correct path to your system JDK, so you should just be able to hit “Ok” then rebuild your project.   For reference, here’s a screenshot showing what my version of that window looks like: http://i.imgur.com/hRfQjIi.png


On December 5, 2014 at 1:52:35 PM, Judy Nash (judynash@exchange.microsoft.com<mailto:judynash@exchange.microsoft.com>) wrote:
Hi everyone,

Have a newbie question on using IntelliJ to build and debug.

I followed this wiki to setup IntelliJ:
https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools#UsefulDeveloperTools-BuildingSparkinIntelliJIDEA

Afterward I tried to build via Toolbar (Build > Rebuild Project).
The action fails with the error message:
Cannot start compiler: the SDK is not specified.

What SDK do I need to specify to get the build working?

Thanks,
Judy
"
Gerard Maas <gerard.maas@gmail.com>,"Mon, 8 Dec 2014 16:38:59 +0100",Understanding reported times on the Spark UI [+ Streaming],"spark users <user@spark.apache.org>, dev@spark.apache.org","Hi,

I'm confused about the Stage times reported on the Spark-UI (Spark 1.1.0)
for an Spark-Streaming job.  I'm hoping somebody can shine some light on it:

Let's do this with an example:

232runJob at RDDFunctions.scala:23
<http://localhost:24040/stages/stage?id=232&attempt=0>+details

2014/12/08 15:06:2518 s
12/12
When I click on it for details, I see: [1]

Total time across all tasks = 42s

Aggregated metrics by executor:
Executor1 19s
Executor2 24s

Summing all tasks is actually: 40,009s

What is the time reported on the overview page? (18s?)

What is relation between the reported time on the overview and the detail
page?

My Spark Streaming job is reported to be taking 3m24s, and (I think)
there's only 1 stage in my job. How does the timing per stage relate to the
Spark Streaming reported in the 'streaming' page ? (e.g. 'last batch') ?

Is there a way to relate a streaming batch to the stages executed  to
complete that batch?
The numbers as they are at the moment don't seem to add up.

Thanks,

Gerard.


[1] https://drive.google.com/file/d/0BznIWnuWhoLlMkZubzY2dTdOWDQ
"
Michael Armbrust <michael@databricks.com>,"Mon, 8 Dec 2014 11:34:47 -0800","Re: Hive Problem in Pig generated Parquet file schema in CREATE
 EXTERNAL TABLE (e.g. bag::col1)",Jianshi Huang <jianshi.huang@gmail.com>,"This is by hive's design.  From the Hive documentation:

The column change command will only modify Hive's metadata, and will not




"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 08 Dec 2014 19:58:08 +0000",Re: Handling stale PRs,Patrick Wendell <pwendell@gmail.com>,"I recently came across this blog post, which reminded me of this thread.

How to Discourage Open Source Contributions
<http://danluu.com/discourage-oss/>

We are currently at 320+ open PRs, many of which haven't been updated in
over a month. We have quite a few PRs that haven't been touched in 3-5
months.

*If you have the time and interest, please hop on over to the Spark PR
Dashboard <https://spark-prs.appspot.com/>, sort the PRs by
least-recently-updated, and update them where you can.*

I share the blog author's opinion that letting PRs go stale discourages
contributions, especially from first-time contributors, and especially more
so when the PR author is waiting on feedback from a committer or
contributor.

I've been thinking about simple ways to make it easier for all of us to
chip in on controlling stale PRs in an incremental way. For starters, would
it help if an automated email went out to the dev list once a week that a)
reported the number of stale PRs, and b) directly linked to the 5 least
recently updated PRs?

Nick


"
"""Ganelin, Ilya"" <Ilya.Ganelin@capitalone.com>","Mon, 8 Dec 2014 15:07:59 -0500",Re: Handling stale PRs,"Nicholas Chammas <nicholas.chammas@gmail.com>, Patrick Wendell
	<pwendell@gmail.com>","Thank you for pointing this out, Nick. I know that for myself and my
colleague who are starting to contribute to Spark, its definitely
discouraging to have fixes sitting in the pipeline. Could you recommend
any other ways that we can facilitate getting these PRs accepted? Clean,
well-tested code is an obvious one but Id like to know if there are some
non-obvious things we (as contributors) could do to make the committers
lives easier? Thanks!

-Ilya 



________________________________________________________

The information contained in this e-mail is confidential and/or proprietary is intended only for use by the individual or entity to which it is addressed.  If the reader of this message is not the intended recipient, you are hereby notified that any review, retransmission, dissemination, distribution, copying or other use of, or taking of any action in reliance upon this information is strictly prohibited. If you have received this communication in error, please contact the sender and delete the material from your computer.


---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 08 Dec 2014 20:27:07 +0000",Re: Handling stale PRs,"""Ganelin, Ilya"" <Ilya.Ganelin@capitalone.com>, Patrick Wendell <pwendell@gmail.com>","Things that help:

   - Be persistent. People are busy, so just ping them if there’s been no
   response for a couple of weeks. Hopefully, as the project continues to
   develop, this will become less necessary.
   sure all the tests are clear before reaching out, unless you need help
   understanding why a test is failing.
   - Whenever possible, keep PRs small, small, small.
   - Get buy-in on the dev list before working on something, especially
   larger features, to make sure you are making something that people
   understand and that is in accordance with Spark’s design.

I’m just speaking as a random contributor here, so don’t take this advice
as gospel.

Nick
​


 some
¹
a)
"
Yin Huai <huaiyin.thu@gmail.com>,"Mon, 8 Dec 2014 15:58:53 -0500",Re: scala.MatchError on SparkSQL when creating ArrayType of StructType,invkrh <invkrh@gmail.com>,"Seems you hit https://issues.apache.org/jira/browse/SPARK-4245. It was
fixed in 1.2.

Thanks,

Yin


"
Michael Armbrust <michael@databricks.com>,"Mon, 8 Dec 2014 17:44:53 -0800",Re: CREATE TABLE AS SELECT does not work with temp tables in 1.2.0,"""Cheng, Hao"" <hao.cheng@intel.com>","This is merged now and should be fixed in the next 1.2 RC.


"
Jianshi Huang <jianshi.huang@gmail.com>,"Tue, 9 Dec 2014 13:31:24 +0800","Re: Hive Problem in Pig generated Parquet file schema in CREATE
 EXTERNAL TABLE (e.g. bag::col1)",Michael Armbrust <michael@databricks.com>,"Ah... I see. Thanks for pointing it out.

Then it means we cannot mount external table using customized column names.
hmm...

Then the only option left is to use a subquery to add a bunch of column
alias. I'll try it later.

Thanks,
Jianshi




-- 
Jianshi Huang

LinkedIn: jianshi
Twitter: @jshuang
Github & Blog: http://huangjs.github.com/
"
shane knapp <sknapp@berkeley.edu>,"Tue, 9 Dec 2014 10:49:00 -0800",adding new jenkins worker nodes to eventually replace existing ones,"amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","i just turned up a new jenkins slave (amp-jenkins-worker-01) to ensure it
builds properly.  these machines have half the ram, same number of
processors and more disk, which will hopefully help us achieve more than
the ~15-20% system utilization we're getting on the current
amp-jenkins-slave-{01..05} nodes.

instead of 5 super beefy slaves w/16 workers each, we're planning on 8 less
beefy slaves w/12 workers each.  this should definitely cut down on the
build queue, and not impact build times in a negative way at all.

i'll keep a close eye on amp-jenkins-worker-01 before i start releasing the
other seven in to the wild.

there should be a minimal user impact, but if i happen to miss something,
please don't hesitate to let me know!

thanks,

shane
"
Michael Armbrust <michael@databricks.com>,"Tue, 9 Dec 2014 11:37:35 -0800","Re: Hive Problem in Pig generated Parquet file schema in CREATE
 EXTERNAL TABLE (e.g. bag::col1)",Jianshi Huang <jianshi.huang@gmail.com>,"You might also try out the recently added support for views.


"
"""York, Brennon"" <Brennon.York@capitalone.com>","Tue, 9 Dec 2014 15:05:58 -0500",Re: zinc invocation examples,"Patrick Wendell <pwendell@gmail.com>, Ryan Williams
	<ryan.blake.williams@gmail.com>","Patrick, Ive nearly completed a basic build out for the SPARK-4501 issue
(at https://github.com/brennonyork/spark/tree/SPARK-4501) and would be
great to get your initial read on it. Per this thread I need to add in the
-scala-home call to zinc, but its close to ready for a PR.



________________________________________________________

The information contained in this e-mail is confidential and/or proprietary is intended only for use by the individual or entity to which it is addressed.  If the reader of this message is not the intended recipient, you are hereby notified that any review, retransmission, dissemination, distribution, copying or other use of, or taking of any action in reliance upon this information is strictly prohibited. If you have received this communication in error, please contact the sender and delete the material from your computer.


---------------------------------------------------------------------


"
"""Ganelin, Ilya"" <Ilya.Ganelin@capitalone.com>","Tue, 9 Dec 2014 15:38:52 -0500",Adding RDD function to segment an RDD (like substring),dev <dev@spark.apache.org>,"Hi all  a utility that Ive found useful several times now when working with RDDs is to be able to reason about segments of the RDD.

For example, if I have two large RDDs and I want to combine them in a way that would be intractable in terms of memory or disk storage (e.g. A cartesian) but a piece-wise approach is tractable, there are not many methods that enable this.

Existing relevant methods are :
takeSample  Which allows me to take a number of random values from an RDD but does not let me process the entire RDD
randomSplit  Which segments the RDD into an array of smaller RDDs  with this, however, Ive seen numerous memory issues during execution on larger datasets or when splitting into many RDDs
forEach or collect  Which require the RDD to fit into memory  which is not possible for larger datasets

What I am proposing is the addition of a simple method to operate on an RDD which would operate equivalently to a substring operation in String:

For an RDD[T]:

def sample(startIdx : Int, endIdx : Int) : RDD[T] = {
val zipped = this.zipWithIndex()
val sampled = filter(idx => (idx >= startIdx) && (idx < endIdx))
sampled.map(_._1)
}

I would appreciate any feedback  thank you!


________________________________________________________

The information contained in this e-mail is confidential and/or proprietary is intended only for use by the individual or entity to which it is addressed.  If the reader of this message is not the intended recipient, you are hereby notified that any review, retransmission, dissemination, distribution, copying or other use of, or taking of any action in reliance upon this information is strictly prohibited. If you have received this communication in error, please contact the sender and delete the material from your computer.
"
Mark Hamstra <mark@clearstorydata.com>,"Tue, 9 Dec 2014 13:16:51 -0800",Re: Adding RDD function to segment an RDD (like substring),"""Ganelin, Ilya"" <Ilya.Ganelin@capitalone.com>","`zipWithIndex` is both compute intensive and breaks Spark's
""transformations are lazy"" model, so it is probably not appropriate to add
this to the public RDD API.  If `zipWithIndex` weren't already what I
consider to be broken, I'd be much friendlier to building something more on
top of it, but I really don't like `zipWithIndex` or this `sample` idea of
yours as they stand.  Aside from those concerns, the name `sample` doesn't
really work since it is too easy to assume that it means some kind of
random sampling.

None of that is to say that you can't make effective use of `zipWithIndex`
and your `sample` in your own code; but I'm not a fan of extending the
Spark public API in this way.


ow when working
om an RDD
s – with
on larger
 which is
"
Sean Owen <sowen@cloudera.com>,"Tue, 9 Dec 2014 21:53:08 +0000",Is this a little bug in BlockTransferMessage ?,"aaron@databricks.com, dev <dev@spark.apache.org>","https://github.com/apache/spark/blob/master/network/shuffle/src/main/java/org/apache/spark/network/shuffle/protocol/BlockTransferMessage.java#L70

public byte[] toByteArray() {
  ByteBuf buf = Unpooled.buffer(encodedLength());
  buf.writeByte(type().id);
  encode(buf);
  assert buf.writableBytes() == 0 : ""Writable bytes remain: "" +
buf.writableBytes();
  return buf.array();
}

Running the Java tests at last might have turned up a little bug here,
but wanted to check. This makes a buffer to hold enough bytes to
encode the message. But it writes 1 byte, plus the message. This makes
the buffer expand, and then does have nonzero capacity afterwards, so
the assert fails.

So just needs a ""+ 1"" in the size?

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Tue, 9 Dec 2014 22:33:34 +0000",Re: Is this a little bug in BlockTransferMessage ?,Aaron Davidson <aaron@databricks.com>,"Yep, will do. The test does catch it -- it's just not being executed.
I think I have a reasonable start on re-enabling surefire + Java tests
for SPARK-4159.


---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Tue, 9 Dec 2014 15:04:11 -0800",Re: adding new jenkins worker nodes to eventually replace existing ones,"amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","forgot to install git on this node.  /headdesk

i retirggered the failed spark prb jobs.


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 09 Dec 2014 23:30:46 +0000",Re: Is this a little bug in BlockTransferMessage ?,"Sean Owen <sowen@cloudera.com>, Aaron Davidson <aaron@databricks.com>","So all this time the tests that Jenkins has been running via Jenkins and
SBT + ScalaTest... those haven't been running any of the Java unit tests?

SPARK-4159 <https://issues.apache.org/jira/browse/SPARK-4159> only mentions
Maven as a problem, but I'm wondering how these tests got through Jenkins
OK.


"
Sean Owen <sowen@cloudera.com>,"Tue, 9 Dec 2014 23:32:32 +0000",Re: Is this a little bug in BlockTransferMessage ?,Nicholas Chammas <nicholas.chammas@gmail.com>,"I'm not so sure about SBT, but I'm looking at the output now and do
not see things like JavaAPISuite being run. I see them compiled. That
I'm not as sure how to fix. I think I have a solution for Maven on
SPARK-4159.


---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 09 Dec 2014 23:35:54 +0000",Re: Is this a little bug in BlockTransferMessage ?,Sean Owen <sowen@cloudera.com>,"OK. That's concerning. Hopefully that's the only bug we'll dig up once we
run all the Java tests but who knows.

Patrick,

Shouldn't this be a release blocking bug for 1.2 (mostly just because it
has already been covered by a unit test)? Well, that, as well as any other
bugs that come up as we run these Java tests.

Nick


"
Patrick Wendell <pwendell@gmail.com>,"Tue, 9 Dec 2014 22:07:45 -0800",Re: Is this a little bug in BlockTransferMessage ?,Nicholas Chammas <nicholas.chammas@gmail.com>,"Hey Nick,

Thanks for bringing this up. I believe these Java tests are running in
the sbt build right now, the issue is that this particular bug was
flagged by the triggering of a runtime Java ""assert"" (not a normal
Junit test assertion) and those are not enabled in our sbt tests. It
would be good to fix it so that assertions run when we do the sbt
tests, for some reason I think the sbt tests disable them by default.

I think the original issue is fixed now (that Sean found and
reported). It would be good to get assertions running in our tests,
but I'm not sure I'd block the release on it. The normal JUnit
assertions are running correctly.

- Patrick


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Wed, 10 Dec 2014 07:22:36 +0000",Re: Is this a little bug in BlockTransferMessage ?,Patrick Wendell <pwendell@gmail.com>,"Oops, yes I see Java tests run with SBT now. You're right, it must be
because of the assertion. I can try to add '-ea' to the SBT build as a
closely-related change for SPARK-4159.

FWIW this error is the only one I saw once the Maven tests ran the Java tests.


---------------------------------------------------------------------


"
Jun Feng Liu <liujunf@cn.ibm.com>,"Wed, 10 Dec 2014 16:25:37 +0800",HA support for Spark,dev@spark.apache.org,"Do we have any high availability support in Spark driver level? For 
example, if we want spark drive can move to another node continue 
execution when failure happen. I can see the RDD checkpoint can help to 
serialization the status of RDD. I can image to load the check point from 
another node when error happen, but seems like will lost track all tasks 
status or even executor information that maintain in spark context. I am 
not sure if there is any existing stuff I can leverage to do that. thanks 
for any suggests
 
Best Regards
 
Jun Feng Liu
IBM China Systems & Technology Laboratory in Beijing



Phone: 86-10-82452683 
E-mail: liujunf@cn.ibm.com


BLD 28,ZGC Software Park 
No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193 
China 
 

 "
Reynold Xin <rxin@databricks.com>,"Wed, 10 Dec 2014 00:30:55 -0800",Re: HA support for Spark,Jun Feng Liu <liujunf@cn.ibm.com>,"This would be plausible for specific purposes such as Spark streaming or
Spark SQL, but I don't think it is doable for general Spark driver since it
is just a normal JVM process with arbitrary program state.


"
Guillaume Pitel <guillaume.pitel@exensa.com>,"Wed, 10 Dec 2014 11:42:36 +0100",Maven profile in MLLib netlib-lgpl not working (1.1.1),"user@spark.apache.org, dev@spark.apache.org","Hi

Issue created https://issues.apache.org/jira/browse/SPARK-4816

Probably a maven-related question for profiles in child modules

I couldn't find a clean solution, just a workaround : modify pom.xml in 
mllib module to force activation of netlib-lgpl module.

Hope a maven expert will help.

Guillaume


-- 
eXenSa

	
*Guillaume PITEL, Président*
+33(0)626 222 431

eXenSa S.A.S. <http://www.exensa.com/>
41, rue Périer - 92120 Montrouge - FRANCE
Tel +33(0)184 163 677 / Fax +33(0)972 283 705

"
Jun Feng Liu <liujunf@cn.ibm.com>,"Wed, 10 Dec 2014 21:30:24 +0800",Re: HA support for Spark,Reynold Xin <rxin@databricks.com>,"Well, it should not be mission impossible thinking there are so many HA 
solution existing today. I would interest to know if there is any specific 
difficult.
 
Best Regards
 
Jun Feng Liu
IBM China Systems & Technology Laboratory in Beijing



Phone: 86-10-82452683 
E-mail: liujunf@cn.ibm.com


BLD 28,ZGC Software Park 
No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193 
China 
 

 



Reynold Xin <rxin@databricks.com> 
2014/12/10 16:30

To
Jun Feng Liu/China/IBM@IBMCN, 
cc
""dev@spark.apache.org"" <dev@spark.apache.org>
Subject
Re: HA support for Spark






This would be plausible for specific purposes such as Spark streaming or
Spark SQL, but I don't think it is doable for general Spark driver since 
it
is just a normal JVM process with arbitrary program state.


execution
serialization
even
there
*86-10-82452683

"
Jun Feng Liu <liujunf@cn.ibm.com>,"Wed, 10 Dec 2014 21:47:12 +0800",Tachyon in Spark,"""dev@spark.apache.org"" <dev@spark.apache.org>","Dose Spark today really leverage Tachyon linage to process data? It seems 
like the application should call createDependency function in TachyonFS to 
create a new linage node. But I did not find any place call that in Spark 
code. Did I missed anything?
Best Regards
 
Jun Feng Liu
IBM China Systems & Technology Laboratory in Beijing



Phone: 86-10-82452683 
E-mail: liujunf@cn.ibm.com


BLD 28,ZGC Software Park 
No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193 
China 
 

 "
Sandy Ryza <sandy.ryza@cloudera.com>,"Wed, 10 Dec 2014 09:34:26 -0800",Re: HA support for Spark,Jun Feng Liu <liujunf@cn.ibm.com>,"I think that if we were able to maintain the full set of created RDDs as
well as some scheduler and block manager state, it would be enough for most
apps to recover.


"
Andrew Lee <alee526@hotmail.com>,"Wed, 10 Dec 2014 09:44:18 -0800","Build Spark 1.2.0-rc1 encounter exceptions when running HiveContext
 - Caused by: java.lang.ClassNotFoundException:
 com.esotericsoftware.shaded.org.objenesis.strategy.InstantiatorStrategy","""dev@spark.apache.org"" <dev@spark.apache.org>","Hi All,
I tried to include necessary libraries in SPARK_CLASSPATH in spark-env.sh to include auxiliaries JARs and datanucleus*.jars from Hive, however, when I run HiveContext, it gives me the following error:
Caused by: java.lang.ClassNotFoundException: com.esotericsoftware.shaded.org.objenesis.strategy.InstantiatorStrategy
I have checked the JARs with (jar tf), looks like this is already included (shaded) in the assembly JAR (spark-assembly-1.2.0-hadoop2.4.1.jar) which is configured in the System classpath already. I couldn't figure out what is going on with the shading on the esotericsoftware JARs here. Any help is appreciated.
How to reproduce the problem?Run the following 3 statements in spark-shell ( This is how I launched my spark-shell. cd /opt/spark; ./bin/spark-shell --master yarn --deploy-mode client --queue research --driver-memory 1024M)
import org.apache.spark.SparkContextval hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)hiveContext.hql(""CREATE TABLE IF NOT EXISTS spark_hive_test_table (key INT, value STRING)"")

A reference of my environment.Apache Hadoop 2.4.1Apache Hive 0.13.1Apache Spark branch-1.2 (installed under /opt/spark/, and config under /etc/spark/)Maven build command:







mvn -U -X -Phadoop-2.4 -Pyarn -Phive -Phive-0.13.1 -Dhadoop.version=2.4.1 -Dyarn.version=2.4.1 -Dhive.version=0.13.1 -DskipTests install
Source Code commit label: eb4d457a870f7a281dc0267db72715cd00245e82
















My spark-env.sh have the following contents when I executed spark-shell:







HADOOP_HOME=/opt/hadoop/
HIVE_HOME=/opt/hive/
HADOOP_CONF_DIR=/etc/hadoop/
YARN_CONF_DIR=/etc/hadoop/
HIVE_CONF_DIR=/etc/hive/
HADOOP_SNAPPY_JAR=$(find $HADOOP_HOME/share/hadoop/common/lib/ -type f -name ""snappy-java-*.jar"")
HADOOP_LZO_JAR=$(find $HADOOP_HOME/share/hadoop/common/lib/ -type f -name ""hadoop-lzo-*.jar"")








SPARK_YARN_DIST_FILES=/user/spark/libs/spark-assembly-1.2.0-hadoop2.4.1.jar
export JAVA_LIBRARY_PATH=$JAVA_LIBRARY_PATH:$HADOOP_HOME/lib/native
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HADOOP_HOME/lib/native
export SPARK_LIBRARY_PATH=$SPARK_LIBRARY_PATH:$HADOOP_HOME/lib/native
export SPARK_CLASSPATH=$SPARK_CLASSPATH:$HADOOP_SNAPPY_JAR:$HADOOP_LZO_JAR:$HIVE_CONF_DIR:/opt/hive/lib/datanucleus-api-jdo-3.2.6.jar:/opt/hive/lib/datanucleus-core-3.2.10.jar:/opt/hive/lib/datanucleus-rdbms-3.2.9.jar
Here's what I see from my stack trace.
warning: there were 1 deprecation warning(s); re-run with -deprecation for details
Hive history file=/home/hive/log/alti-test-01/hive_job_log_b5db9539-4736-44b3-a601-04fa77cb6730_1220828461.txt
java.lang.NoClassDefFoundError: com/esotericsoftware/shaded/org/objenesis/strategy/InstantiatorStrategy
	at org.apache.hadoop.hive.ql.exec.Utilities.<clinit>(Utilities.java:925)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.validate(SemanticAnalyzer.java:9718)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.validate(SemanticAnalyzer.java:9712)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:434)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:322)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:975)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1040)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:911)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:901)
	at org.apache.spark.sql.hive.HiveContext.runHive(HiveContext.scala:305)
	at org.apache.spark.sql.hive.HiveContext.runSqlHive(HiveContext.scala:276)
	at org.apache.spark.sql.hive.execution.NativeCommand.sideEffectResult$lzycompute(NativeCommand.scala:35)
	at org.apache.spark.sql.hive.execution.NativeCommand.sideEffectResult(NativeCommand.scala:35)
	at org.apache.spark.sql.execution.Command$class.execute(commands.scala:46)
	at org.apache.spark.sql.hive.execution.NativeCommand.execute(NativeCommand.scala:30)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:425)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:425)
	at org.apache.spark.sql.SchemaRDDLike$class.$init$(SchemaRDDLike.scala:58)
	at org.apache.spark.sql.SchemaRDD.<init>(SchemaRDD.scala:108)
	at org.apache.spark.sql.hive.HiveContext.hiveql(HiveContext.scala:102)
	at org.apache.spark.sql.hive.HiveContext.hql(HiveContext.scala:106)
	at $iwC$$iwC$$iwC$$iwC.<init>(<console>:16)
	at $iwC$$iwC$$iwC.<init>(<console>:21)
	at $iwC$$iwC.<init>(<console>:23)
	at $iwC.<init>(<console>:25)
	at <init>(<console>:27)
	at .<init>(<console>:31)
	at .<clinit>(<console>)
	at .<init>(<console>:7)
	at .<clinit>(<console>)
	at $print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:852)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1125)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:674)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:705)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:669)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:828)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:873)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:785)
	at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:628)
	at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:636)
	at org.apache.spark.repl.SparkILoop.loop(SparkILoop.scala:641)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:968)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1011)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:353)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.ClassNotFoundException: com.esotericsoftware.shaded.org.objenesis.strategy.InstantiatorStrategy
	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
	... 61 more

 		 	   		  "
Andrew Lee <alee526@hotmail.com>,"Wed, 10 Dec 2014 09:48:26 -0800","RE: Build Spark 1.2.0-rc1 encounter exceptions when running
 HiveContext - Caused by: java.lang.ClassNotFoundException:
 com.esotericsoftware.shaded.org.objenesis.strategy.InstantiatorStrategy","""dev@spark.apache.org"" <dev@spark.apache.org>","Apologize for the format, somehow it got messed up and linefeed were removed. Here's a reformatted version.
Hi All,
I tried to include necessary libraries in SPARK_CLASSPATH in spark-env.sh to include auxiliaries JARs and datanucleus*.jars from Hive, however, when I run HiveContext, it gives me the following error:

Caused by: java.lang.ClassNotFoundException: com.esotericsoftware.shaded.org.objenesis.strategy.InstantiatorStrategy

I have checked the JARs with (jar tf), looks like this is already included (shaded) in the assembly JAR (spark-assembly-1.2.0-hadoop2.4.1.jar) which is configured in the System classpath already. I couldn't figure out what is going on with the shading on the esotericsoftware JARs here.  Any help is appreciated.


How to reproduce the problem?
Run the following 3 statements in spark-shell ( This is how I launched my spark-shell. cd /opt/spark; ./bin/spark-shell --master yarn --deploy-mode client --queue research --driver-memory 1024M)

import org.apache.spark.SparkContext
val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)
hiveContext.hql(""CREATE TABLE IF NOT EXISTS spark_hive_test_table (key INT, value STRING)"")



A reference of my environment.
Apache Hadoop 2.4.1
Apache Hive 0.13.1
Apache Spark branch-1.2 (installed under /opt/spark/, and config under /etc/spark/)
Maven build command:

mvn -U -X -Phadoop-2.4 -Pyarn -Phive -Phive-0.13.1 -Dhadoop.version=2.4.1 -Dyarn.version=2.4.1 -Dhive.version=0.13.1 -DskipTests install

Source Code commit label: eb4d457a870f7a281dc0267db72715cd00245e82

My spark-env.sh have the following contents when I executed spark-shell:
-name ""snappy-java-*.jar"")
me ""hadoop-lzo-*.jar"")
.jar
JAR:$HIVE_CONF_DIR:/opt/hive/lib/datanucleus-api-jdo-3.2.6.jar:/opt/hive/lib/datanucleus-core-3.2.10.jar:/opt/hive/lib/datanucleus-rdbms-3.2.9.jar


for details
6-44b3-a601-04fa77cb6730_1220828461.txt
/strategy/InstantiatorStrategy
lyzer.java:9718)
lyzer.java:9712)
6)
ycompute(NativeCommand.scala:35)
tiveCommand.scala:35)
6)
nd.scala:30)
ntext.scala:425)
:425)
8)
java:57)
sorImpl.java:43)
852)
1125)
)
28)
la:873)
kILoop.scala:968)
scala:916)
scala:916)
Loader.scala:135)
java:57)
sorImpl.java:43)
org.objenesis.strategy.InstantiatorStrategy
 		 	   		  "
shane knapp <sknapp@berkeley.edu>,"Wed, 10 Dec 2014 11:28:23 -0800","Re: jenkins downtime: 730-930am, 12/12/14","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","reminder -- this is happening friday morning @ 730am!


"
Debasish Das <debasish.das83@gmail.com>,"Wed, 10 Dec 2014 11:44:58 -0800",Row Similarity,dev <dev@spark.apache.org>,"Hi,

It seems there are multiple places where we would like to compute row
similarity (accurate or approximate similarities)

Basically through RowMatrix columnSimilarities we can compute column
similarities of a tall skinny matrix

Similarly we should have an API in RowMatrix called rowSimilarities where
we can compute similar rows in a map-reduce fashion. It will be useful for
following use-cases:

1. Generate topK users for each user from matrix factorization model
2. Generate topK products for each product from matrix factorization model
3. Generate kernel matrix for use in spectral clustering
4. Generate kernel matrix for use in kernel regression/classification

I am not sure if there are already good implementation for map-reduce row
similarity that we can use (ideas like fastfood and kitchen sink felt more
like for classification use-case but for recommendation also user
similarities show up which is unsupervised)...

Is there a JIRA tracking it ? If not I can open one and we can discuss
further on it.

Thanks.
Deb
"
Patrick Wendell <pwendell@gmail.com>,"Wed, 10 Dec 2014 12:23:08 -0800","Re: Build Spark 1.2.0-rc1 encounter exceptions when running
 HiveContext - Caused by: java.lang.ClassNotFoundException: com.esotericsoftware.shaded.org.objenesis.strategy.InstantiatorStrategy",Andrew Lee <alee526@hotmail.com>,"Hi Andrew,

It looks like somehow you are including jars from the upstream Apache
Hive 0.13 project on your classpath. For Spark 1.2 Hive 0.13 support,
we had to modify Hive to use a different version of Kryo that was
compatible with Spark's Kryo version.

https://github.com/pwendell/hive/commit/5b582f242946312e353cfce92fc3f3fa472aedf3

I would look through the actual classpath and make sure you aren't
including your own hive-exec jar somehow.

- Patrick

ved. Here's a reformatted version.
 to include auxiliaries JARs and datanucleus*.jars from Hive, however, when I run HiveContext, it gives me the following error:
org.objenesis.strategy.InstantiatorStrategy
d (shaded) in the assembly JAR (spark-assembly-1.2.0-hadoop2.4.1.jar) which is configured in the System classpath already. I couldn't figure out what is going on with the shading on the esotericsoftware JARs here.  Any help is appreciated.
 spark-shell. cd /opt/spark; ./bin/spark-shell --master yarn --deploy-mode client --queue research --driver-memory 1024M)
T, value STRING)"")
tc/spark/)
.1 -Dyarn.version=2.4.1 -Dhive.version=0.13.1 -DskipTests install
 -name ""snappy-java-*.jar"")
ame ""hadoop-lzo-*.jar"")
1.jar
_JAR:$HIVE_CONF_DIR:/opt/hive/lib/datanucleus-api-jdo-3.2.6.jar:/opt/hive/lib/datanucleus-core-3.2.10.jar:/opt/hive/lib/datanucleus-rdbms-3.2.9.jar
or details
36-44b3-a601-04fa77cb6730_1220828461.txt
s/strategy/InstantiatorStrategy
a:925)
ticAnalyzer.java:9718)
ticAnalyzer.java:9712)
5)
:305)
ala:276)
ult$lzycompute(NativeCommand.scala:35)
ult(NativeCommand.scala:35)
cala:46)
eCommand.scala:30)
(SQLContext.scala:425)
.scala:425)
cala:58)
102)
)
rImpl.java:57)
dAccessorImpl.java:43)
scala:852)
scala:1125)
la:674)
)
)
cala:828)
op.scala:873)
:628)
36)
p(SparkILoop.scala:968)
ILoop.scala:916)
ILoop.scala:916)
aClassLoader.scala:135)
rImpl.java:57)
dAccessorImpl.java:43)
53)
.org.objenesis.strategy.InstantiatorStrategy

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Wed, 10 Dec 2014 13:07:14 -0800",[RESULT] [VOTE] Release Apache Spark 1.2.0 (RC1),Takeshi Yamamuro <linguin.m.s@gmail.com>,"This vote is closed in favor of RC2.


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Wed, 10 Dec 2014 13:08:44 -0800",[VOTE] Release Apache Spark 1.2.0 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version 1.2.0!

The tag to be voted on is v1.2.0-rc2 (commit a428c446e2):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=a428c446e23e628b746e0626cc02b7b3cadf588e

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-1.2.0-rc2/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1055/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-1.2.0-rc2-docs/

Please vote on releasing this package as Apache Spark 1.2.0!

The vote is open until Saturday, December 13, at 21:00 UTC and passes
if a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.2.0
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

== What justifies a -1 vote for this release? ==
This vote is happening relatively late into the QA period, so
-1 votes should only occur for significant regressions from
1.0.2. Bugs already present in 1.1.X, minor
regressions, or bugs related to new features will not block this
release.

== What default changes should I be aware of? ==
1. The default value of ""spark.shuffle.blockTransferService"" has been
changed to ""netty""
--> Old behavior can be restored by switching to ""nio""

2. The default value of ""spark.shuffle.manager"" has been changed to ""sort"".
--> Old behavior can be restored by setting ""spark.shuffle.manager"" to ""hash"".

== How does this differ from RC1 ==
This has fixes for a handful of issues identified - some of the
notable fixes are:

[Core]
SPARK-4498: Standalone Master can fail to recognize completed/failed
applications

[SQL]
SPARK-4552: Query for empty parquet table in spark sql hive get
IllegalArgumentException
SPARK-4753: Parquet2 does not prune based on OR filters on partition columns
SPARK-4761: With JDBC server, set Kryo as default serializer and
disable reference tracking
SPARK-4785: When called with arguments referring column fields, PMOD throws NPE

- Patrick

---------------------------------------------------------------------


"
Reza Zadeh <reza@databricks.com>,"Wed, 10 Dec 2014 17:30:50 -0500",Re: Row Similarity,Debasish Das <debasish.das83@gmail.com>,"It's not so cheap to compute row similarities when there are many rows, as
it amounts to computing the outer product of a matrix A (i.e. computing
AA^T, which is expensive).

There is a JIRA to track handling (1) and (2) more efficiently than
computing all pairs: https://issues.apache.org/jira/browse/SPARK-3066




"
Matei Zaharia <matei.zaharia@gmail.com>,"Wed, 10 Dec 2014 15:05:41 -0800",Re: [VOTE] Release Apache Spark 1.2.0 (RC2),Patrick Wendell <pwendell@gmail.com>,"+1

Tested on Mac OS X.

Matei

version 1.2.0!
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=a428c446e23e628b746e0626cc02b7b3cadf588e
at:
https://repository.apache.org/content/repositories/orgapachespark-1055/
""sort"".
""hash"".
columns
thro"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 11 Dec 2014 01:24:50 +0000",Re: Is Apache JIRA down?,dev <dev@spark.apache.org>,"Nevermind, seems to be back up now.


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 11 Dec 2014 00:46:30 +0000",Is Apache JIRA down?,dev <dev@spark.apache.org>,"For example: https://issues.apache.org/jira/browse/SPARK-3431

Where do we report/track issues with JIRA itself being down?

Nick
"
Patrick Wendell <pwendell@gmail.com>,"Wed, 10 Dec 2014 17:29:53 -0800",Re: Is Apache JIRA down?,Nicholas Chammas <nicholas.chammas@gmail.com>,"I believe many apache services are/were down due to an outage.


---------------------------------------------------------------------


"
Debasish Das <debasish.das83@gmail.com>,"Wed, 10 Dec 2014 18:01:02 -0800",Re: Row Similarity,Reza Zadeh <reza@databricks.com>,"I added code to compute topK products for each user and topK user for each
product in SPARK-3066..

That is different than row similarity calculation as we need both user and
product factors to calculate the topK recommendations..

For (1) and (2) we are trying to answer similarUsers to given a user and
similarProducts to a given product....

similarProducts to a given product is straightforward to compute through
columnSimilarities/dimsum when products are skinny...

similarUser to a given user will need a map-reduce implementation of row
similarity since the matrix is tall...

I don't see a JIRA for that yet...Are there any good reference for map
reduce implementation of row similarity ?


"
Reza Zadeh <reza@databricks.com>,"Wed, 10 Dec 2014 21:15:48 -0500",Re: Row Similarity,Debasish Das <debasish.das83@gmail.com>,"Here we go: https://issues.apache.org/jira/browse/SPARK-4823


"
Alessandro Baretta <alexbaretta@gmail.com>,"Wed, 10 Dec 2014 18:19:03 -0800",SparkSQL not honoring schema,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hello,

I defined a SchemaRDD by applying a hand-crafted StructType to an RDD. Some
of the Rows in the RDD are malformed--that is, they do not conform to the
schema defined by the StructType. When running a select statement on this
SchemaRDD I would expect SparkSQL to either reject the malformed rows or
fail. Instead, it returns whatever data it finds, even if malformed. Is
this the desired behavior? Is there no method in SparkSQL to check for
validity with respect to the schema?

Thanks.

Alex
"
Michael Armbrust <michael@databricks.com>,"Wed, 10 Dec 2014 18:27:58 -0800",Re: SparkSQL not honoring schema,Alessandro Baretta <alexbaretta@gmail.com>,"As the scala doc for applySchema says, ""It is important to make sure that
the structure of every [[Row]] of the provided RDD matches the provided
schema. Otherwise, there will be runtime exceptions.""  We don't check as
doing runtime reflection on all of the data would be very expensive.  You
will only get errors if you try to manipulate the data, but otherwise it
will pass it though.

I have written some debugging code (developer API, not guaranteed to be
stable) though that you can use.

import org.apache.spark.sql.execution.debug._
schemaRDD.typeCheck()


"
Alessandro Baretta <alexbaretta@gmail.com>,"Wed, 10 Dec 2014 18:45:07 -0800",Re: SparkSQL not honoring schema,Michael Armbrust <michael@databricks.com>,"Hey Michael,

Thanks for the clarification. I was actually assuming the query would fail.
Ok, so this means I will have to do the validation in an RDD transformation
feeding into the SchemaRDD.


"
Jun Feng Liu <liujunf@cn.ibm.com>,"Thu, 11 Dec 2014 11:34:06 +0800",Re: HA support for Spark,Sandy Ryza <sandy.ryza@cloudera.com>,"
Right, perhaps also need preserve some DAG information? I am wondering if
there is any work around this.



                                                                           
             Sandy Ryza                                                    
             <sandy.ryza@cloud                                             
             era.com>                                                   To 
                                       Jun Feng Liu/China/IBM@IBMCN,       
             2014-12-11 01:34                                           cc 
                                       Reynold Xin <rxin@databricks.com                                       ""dev@spark.apache.org""              
                                       <dev@spark.apache.org>              
                                                                   Subject 
                                       Re: HA support for Spark            
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           




I think that if we were able to maintain the full set of created RDDs as
well as some scheduler and block manager state, it would be enough for most
apps to recover.

e:

HA
specific
*86-10-82452683
 or
nce
it


node
or
if
suggests
"
Krishna Sankar <ksankar42@gmail.com>,"Wed, 10 Dec 2014 21:55:14 -0800",Re: [VOTE] Release Apache Spark 1.2.0 (RC1),Xiangrui Meng <mengxr@gmail.com>,"   - K-Means iPython notebook & data attached.
   - It is the zip that gives the error ; while one of the RDDs is from the
   prediction, most probably there is no problem with the K-Means.
   - Lines 34,35 & 36 essentially are the same. But only 36 works with
   1.2.0.
   - Interestingly, lines 34,35 & 36 work with 1.1.1 (Checked just now)
      - The plot thickens!
      - In 1.1.1, freq_cluster_map.take(5) prints normally for 34 & 35, but
      in exponential form for 36. So there is some difference even in 1.1.1.
      - #34,#35 [(array([28143, 0, 174, 1, 0, 0, 7000]), 1),

       (array([19244,     0,   215,     2,     0,     0,  6968]), 1),
       (array([41354,     0,  4123,     4,     0,     0,  7034]), 1),
       (array([14776,     0,   500,     1,     0,     0,  6952]), 1),
       (array([97752,     0, 43300,    26,  2077,     4,  6935]), 0)]

      -

      #36 [(array([  2.81430000e+04,   0.00000000e+00,   1.74000000e+02,

                 1.00000000e+00,   0.00000000e+00,   0.00000000e+00,
                 7.00000000e+03]), 1),
       (array([  1.92440000e+04,   0.00000000e+00,   2.15000000e+02,
                 2.00000000e+00,   0.00000000e+00,   0.00000000e+00,
                 6.96800000e+03]), 1),
       (array([  4.13540000e+04,   0.00000000e+00,   4.12300000e+03,
                 4.00000000e+00,   0.00000000e+00,   0.00000000e+00,
                 7.03400000e+03]), 1),
       (array([  1.47760000e+04,   0.00000000e+00,   5.00000000e+02,
                 1.00000000e+00,   0.00000000e+00,   0.00000000e+00,
                 6.95200000e+03]), 1),
       (array([  9.77520000e+04,   0.00000000e+00,   4.33000000e+04,
                 2.60000000e+01,   2.07700000e+03,   4.00000000e+00,
                 6.93500000e+03]), 0)]

      - I had overwritten the naive bayes example. Will chase the older
   versions down

Cheers
<k/>



---------------------------------------------------------------------"
Tathagata Das <tathagata.das1565@gmail.com>,"Thu, 11 Dec 2014 04:20:49 -0800",Re: HA support for Spark,Jun Feng Liu <liujunf@cn.ibm.com>,"Spark Streaming essentially does this by saving the DAG of DStreams, which
can deterministically regenerate the DAG of RDDs upon recovery from
failure. Along with that the progress information (which batches have
finished, which batches are queued, etc.) is also saved, so that upon
recovery the system can restart from where it was before failure. This was
conceptually easy to do because the RDDs are very deterministically
generated in every batch. Extending this to a very general Spark program
with arbitrary RDD computations is definitely conceptually possible but not
that easy to do.


"
Madhu <madhu@madhu.com>,"Thu, 11 Dec 2014 07:29:30 -0700 (MST)",Re: [VOTE] Release Apache Spark 1.2.0 (RC2),dev@spark.incubator.apache.org,"+1 (non-binding)

Built and tested on Windows 7:

cd apache-spark
git fetch
git checkout v1.2.0-rc2
sbt assembly
[warn]
...
[warn]
[success] Total time: 720 s, completed Dec 11, 2014 8:57:36 AM

dir assembly\target\scala-2.10\spark-assembly-1.2.0-hadoop1."
Jun Feng Liu <liujunf@cn.ibm.com>,"Thu, 11 Dec 2014 22:59:59 +0800",Re: HA support for Spark,Tathagata Das <tathagata.das1565@gmail.com>,"Interesting, you saying StreamContext checkpoint can regenerate DAG stuff? 

 
Best Regards
 
Jun Feng Liu
IBM China Systems & Technology Laboratory in Beijing



Phone: 86-10-82452683 
E-mail: liujunf@cn.ibm.com


BLD 28,ZGC Software Park 
No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193 
China 
 

 



Tathagata Das <tathagata.das1565@gmail.com> 
2014/12/11 20:20

To
Jun Feng Liu/China/IBM@IBMCN, 
cc
Sandy Ryza <sandy.ryza@cloudera.com>, ""dev@spark.apache.org"" 
<dev@spark.apache.org>, Reynold Xin <rxin@databricks.com>
Subject
Re: HA support for Spark






Spark Streaming essentially does this by saving the DAG of DStreams, which 
can deterministically regenerate the DAG of RDDs upon recovery from 
failure. Along with that the progress information (which batches have 
finished, which batches are queued, etc.) is also saved, so that upon 
recovery the system can restart from where it was before failure. This was 
conceptually easy to do because the RDDs are very deterministically 
generated in every batch. Extending this to a very general Spark program 
with arbitrary RDD computations is definitely conceptually possible but 
not that easy to do.

Right, perhaps also need preserve some DAG information? I am wondering if 
there is any work around this.


Sandy Ryza ---2014-12-11 01:36:35---Sandy Ryza <sandy.ryza@cloudera.com>


Sandy Ryza <sandy.ryza@cloudera.com>  
2014-12-11 01:34



To

Jun Feng Liu/China/IBM@IBMCN, 

cc

Reynold Xin <rxin@databricks.com>, ""dev@spark.apache.org"" <
dev@spark.apache.org>

Subject

Re: HA support for Spark





I think that if we were able to maintain the full set of created RDDs as
well as some scheduler and block manager state, it would be enough for 
most
apps to recover.


specific
*86-10-82452683
it
node
suggests


"
Sean Owen <sowen@cloudera.com>,"Thu, 11 Dec 2014 16:58:29 +0000",Re: [VOTE] Release Apache Spark 1.2.0 (RC2),Patrick Wendell <pwendell@gmail.com>,"Signatures and checksums are OK. License and notice still looks fine.
The plain-vanilla source release compiles with Maven 3.2.1 and passes
tests, on OS X 10.10 + Java 8.


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Thu, 11 Dec 2014 09:38:36 -0800",Re: [VOTE] Release Apache Spark 1.2.0 (RC2),Patrick Wendell <pwendell@gmail.com>,"+1

Tested on OS X.


"
Sandy Ryza <sandy.ryza@cloudera.com>,"Thu, 11 Dec 2014 09:51:27 -0800",Re: [VOTE] Release Apache Spark 1.2.0 (RC2),Reynold Xin <rxin@databricks.com>,"+1 (non-binding).  Tested on Ubuntu against YARN.


"
kidynamit <paul.mwanjohi@gmail.com>,"Thu, 11 Dec 2014 05:41:09 -0700 (MST)",Evaluation Metrics for Spark's MLlib,dev@spark.incubator.apache.org,"Hi, 

I would like to contribute to Spark's Machine Learning library by adding
evaluation metrics that would be used to gauge the accuracy of a model given
a certain features' set. In particular, I seek to contribute the k-fold
validation metrics, f-beta metric among others on top of the current MLlib
framework available.

Please assist in steps I could take to contribute in this manner. 

Regards, 
kidynamit



--

---------------------------------------------------------------------


"
Alessandro Baretta <alexbaretta@gmail.com>,"Thu, 11 Dec 2014 14:37:25 -0800",Where are the docs for the SparkSQL DataTypes?,"Michael Armbrust <michael@databricks.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Michael & other Spark SQL junkies,

As I read through the Spark API docs, in particular those for the
org.apache.spark.sql package, I can't seem to find details about the Scala
classes representing the various SparkSQL DataTypes, for instance
DecimalType. I find DataType classes in org.apache.spark.sql.api.java, but
they don't seem to match the similarly named scala classes. For instance,
DecimalType is documented as having a nullary constructor, but if I try to
construct an instance of org.apache.spark.sql.DecimalType without any
parameters, the compiler complains about the lack of a precisionInfo field,
which I have discovered can be passed in as None. Where is all this stuff
documented?

Alex
"
shane knapp <sknapp@berkeley.edu>,"Thu, 11 Dec 2014 15:08:34 -0800","Re: jenkins downtime: 730-930am, 12/12/14","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","here's the plan...  reboots, of course, come last.  :)

pause build queue at 7am, kill off (and eventually retrigger) any
stragglers at 8am.  then begin maintenance:

all systems:
* yum update all servers (amp-jekins-master, amp-jenkins-slave-{01..05},
amp-jenkins-worker-{01..08})
* reboots

jenkins slaves:
* install python2.7 (along side 2.6, which would remain the default)
* install numpy 1.9.1 (currently on 1.4, breaking some spark branch builds)
* add new slaves to the master, remove old ones (keep them around just in
case)

there will be no jenkins system or plugin upgrades at this time.  things
there seems to be working just fine!

i'm expecting to be up and building by 9am at the latest.  i'll update this
thread w/any new time estimates.

word.

shane, your rained-in devops guy :)


"
Joseph Bradley <joseph@databricks.com>,"Thu, 11 Dec 2014 15:23:50 -0800",Re: Evaluation Metrics for Spark's MLlib,kidynamit <paul.mwanjohi@gmail.com>,"Hi, I'd recommend starting by checking out the existing helper
functionality for these tasks.  There are helper methods to do K-fold
cross-validation in MLUtils:
https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/util/MLUtils.scala

The experimental spark.ml API in the Spark 1.2 release (in branch-1.2 and
master) has a CrossValidator class which does this more automatically:
https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/tuning/CrossValidator.scala

There are also a few evaluation metrics implemented:
https://github.com/apache/spark/tree/master/mllib/src/main/scala/org/apache/spark/mllib/evaluation

There definitely could be more metrics and/or better APIs to make it easier
to evaluate models on RDDs.  If you spot such cases, I'd recommend opening
up JIRAs for the new features or improvements to get some feedback before
sending PRs:
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark

Hope this helps & looking forward to the contributions!
Joseph


"
Andrew Ash <andrew@andrewash.com>,"Thu, 11 Dec 2014 15:51:12 -0800",Re: Tachyon in Spark,Jun Feng Liu <liujunf@cn.ibm.com>,"is supposed to realize performance gains without sacrificing durability is
by storing the lineage of data rather than full copies of it (similar to
Spark).  But if Spark isn't sending lineage information into Tachyon, then
I'm not sure how this isn't a durability concern.


"
Reynold Xin <rxin@databricks.com>,"Thu, 11 Dec 2014 15:54:11 -0800",Re: Tachyon in Spark,Andrew Ash <andrew@andrewash.com>,"I don't think the lineage thing is even turned on in Tachyon - it was
mostly a research prototype, so I don't think it'd make sense for us to use
that.



"
Tim Harsch <tharsch@cray.com>,"Fri, 12 Dec 2014 00:06:20 +0000",running the Terasort example,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,
I just joined the list, so I dont have a message history that would allow
me to reply to this post:
http://apache-spark-developers-list.1001551.n3.nabble.com/Terasort-example-
td9284.html

I am interested in running the terasort example.  I cloned the repo
https://github.com/ehiggs/spark and did checkout of the terasort branch.
In the above referenced post Ewan gives the example

# Generate 1M 100 byte records:
  ./bin/run-example terasort.TeraGen 100M ~/data/terasort_in


I dont see a run-example in that repo.  Im sure I am missing something
basic, or less likely, maybe some changes werent pushed?

Thanks for any help,
Tim


---------------------------------------------------------------------


"
"""Cheng, Hao"" <hao.cheng@intel.com>","Fri, 12 Dec 2014 00:35:33 +0000",RE: Where are the docs for the SparkSQL DataTypes?,"Alessandro Baretta <alexbaretta@gmail.com>, Michael Armbrust
	<michael@databricks.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Part of it can be found at:
https://github.com/apache/spark/pull/3429/files#diff-f88c3e731fcb17b1323b778807c35b38R34
 
Sorry it's a TO BE reviewed PR, but still should be informative.

Cheng Hao

-----Original Message-----
From: Alessandro Baretta [mailto:alexbaretta@gmail.com] 
Sent: Friday, December 12, 2014 6:37 AM
To: Michael Armbrust; dev@spark.apache.org
Subject: Where are the docs for the SparkSQL DataTypes?

Michael & other Spark SQL junkies,

As I read through the Spark API docs, in particular those for the org.apache.spark.sql package, I can't seem to find details about the Scala classes representing the various SparkSQL DataTypes, for instance DecimalType. I find DataType classes in org.apache.spark.sql.api.java, but they don't seem to match the similarly named scala classes. For instance, DecimalType is documented as having a nullary constructor, but if I try to construct an instance of org.apache.spark.sql.DecimalType without any parameters, the compiler complains about the lack of a precisionInfo field, which I have discovered can be passed in as None. Where is all this stuff documented?

Alex

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org
"
Yi Tian <tianyi.asiainfo@gmail.com>,"Fri, 12 Dec 2014 09:47:10 +0800",Is there any document to explain how to build the hive jars for spark?,dev@spark.apache.org,"Hi, all

We found some bugs in hive-0.12, but we could not wait for hive 
community fixing them.

We want to fix these bugs in our lab and build a new release which could 
be recognized by spark.

As we know, spark depends on a special release of hive, like:

|<dependency>
   <groupId>org.spark-project.hive</groupId>
   <artifactId>hive-metastore</artifactId>
   <version>${hive.version}</version>
</dependency>
|

The different between |org.spark-project.hive| and |org.apache.hive| was 
described by Patrick:

|There are two differences:

1. We publish hive with a shaded protobuf dependency to avoid
conflicts with some Hadoop versions.
2. We publish a proper hive-exec jar that only includes hive packages.
The upstream version of hive-exec bundles a bunch of other random
dependencies in it which makes it really hard for third-party projects
to use it.
|

Is there any document to guide us how to build the hive jars for spark?

Any help would be greatly appreciated.

​
"
Reynold Xin <rxin@databricks.com>,"Thu, 11 Dec 2014 18:22:46 -0800",Re: Tachyon in Spark,Andrew Ash <andrew@andrewash.com>,"Actually HY emailed me offline about this and this is supported in the
latest version of Tachyon. It is a hard problem to push this into storage;
need to think about how to handle isolation, resource allocation, etc.

https://github.com/amplab/tachyon/blob/master/core/src/main/java/tachyon/master/Dependency.java


"
Alessandro Baretta <alexbaretta@gmail.com>,"Thu, 11 Dec 2014 18:45:39 -0800",Re: Where are the docs for the SparkSQL DataTypes?,"""Cheng, Hao"" <hao.cheng@intel.com>","Thanks. This is useful.

Alex

"
Michael Armbrust <michael@databricks.com>,"Thu, 11 Dec 2014 23:00:43 -0800",Re: Where are the docs for the SparkSQL DataTypes?,Alessandro Baretta <alexbaretta@gmail.com>,"I'd suggest looking at the reference in the programming guide:
http://spark.apache.org/docs/latest/sql-programming-guide.html#spark-sql-datatype-reference



"
Ohad Assulin <mrohad@gmail.com>,"Fri, 12 Dec 2014 11:19:01 +0200",JavaScript run-time contribution to Spark,dev@spark.apache.org,"Hello there.
I am running the Internet Technologies Lab at <a href=""
http://new.huji.ac.il/en"">HUJI</a>.
A team of my students would like to contribute a JavaScript run-time (
node.js/v8 based) to Spark.

I wonder
(1) What do you think about the necessity of such a project?
(2) Where should we get started? We have only experience as Spark users.
What are the right docs? Who should we talk to? What architecture guidance
we better follow? etc.

Thanks!
Ohad Assulin
"
Jun Feng Liu <liujunf@cn.ibm.com>,"Fri, 12 Dec 2014 21:06:11 +0800",Re: Tachyon in Spark,Reynold Xin <rxin@databricks.com>,"I think the linage is the key feature of tachyon to reproduce the RDD when 
any error happen. Otherwise, there have to be some data replica among 
tachyon nodes to ensure the data redundancy for fault tolerant - I think 
tachyon is avoiding to go to this path. Dose it mean the off-heap solution 
is not ready yet if tachyon linage dose not work right now? 
 
Best Regards
 
Jun Feng Liu
IBM China Systems & Technology Laboratory in Beijing



Phone: 86-10-82452683 
E-mail: liujunf@cn.ibm.com


BLD 28,ZGC Software Park 
No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193 
China 
 

 



Reynold Xin <rxin@databricks.com> 
2014/12/12 10:22

To
Andrew Ash <andrew@andrewash.com>, 
cc
Jun Feng Liu/China/IBM@IBMCN, ""dev@spark.apache.org"" 
<dev@spark.apache.org>
Subject
Re: Tachyon in Spark






Actually HY emailed me offline about this and this is supported in the
latest version of Tachyon. It is a hard problem to push this into storage;
need to think about how to handle isolation, resource allocation, etc.

https://github.com/amplab/tachyon/blob/master/core/src/main/java/tachyon/master/Dependency.java



use
is
to
then
TachyonFS
in

"
shane knapp <sknapp@berkeley.edu>,"Fri, 12 Dec 2014 07:26:45 -0800","Re: jenkins downtime: 730-930am, 12/12/14","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","reminder:  jenkins is going down NOW.


"
Haoyuan Li <haoyuan.li@gmail.com>,"Fri, 12 Dec 2014 08:17:40 -0800",Re: Tachyon in Spark,Jun Feng Liu <liujunf@cn.ibm.com>,"Junfeng, by off the heap solution, did you mean ""rdd.persist(OFF_HEAP)""?
That feature is different from the lineage feature. You can use this
feature (rdd.persist(OFF_HEAP)) now for any Spark version later than 1.0.0
with Tachyon without a problem.

Regarding Reynold's last email, those are good points. Tachyon had provided
this a while ago. We are working on enhancing this feature and the
integration part with Spark.

Thanks,

Haoyuan


-- 
Haoyuan Li
AMPLab, EECS, UC Berkeley
http://www.cs.berkeley.edu/~haoyuan/
"
shane knapp <sknapp@berkeley.edu>,"Fri, 12 Dec 2014 08:47:06 -0800","Re: jenkins downtime: 730-930am, 12/12/14","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","downtime is extended to 10am PST so that i can finish testing the numpy
upgrade...  besides that, everything looks good and the system updates and
reboots went off w/o a hitch.

shane


"
shane knapp <sknapp@berkeley.edu>,"Fri, 12 Dec 2014 09:47:35 -0800","Re: jenkins downtime: 730-930am, 12/12/14","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","ok, we're back up w/all new jenkins workers.  i'll be keeping an eye on
these pretty closely today for any build failures caused by the new
systems, and if things look bleak, i'll switch back to the original five.

thanks for your patience!


"
Patrick Wendell <pwendell@gmail.com>,"Fri, 12 Dec 2014 10:31:58 -0800",Re: zinc invocation examples,"""York, Brennon"" <Brennon.York@capitalone.com>","Hey York - I'm sending some feedback off-list, feel free to open a PR as well.


ue
e
s
th is intended only for use by the individual or entity to which it is addressed.  If the reader of this message is not the intended recipient, you are hereby notified that any review, retransmission, dissemination, distribution, copying or other use of, or taking of any action in reliance upon this information is strictly prohibited. If you have received this communication in error, please contact the sender and delete the material from your computer.

---------------------------------------------------------------------


"
DB Tsai <dbtsai@dbtsai.com>,"Fri, 12 Dec 2014 11:37:05 -0800",CrossValidator API in new spark.ml package,"Xiangrui Meng <mengxr@gmail.com>, joseph@databricks.com, dev@spark.apache.org","Hi Xiangrui,

It seems that it's stateless so will be hard to implement
regularization path. Any suggestion to extend it? Thanks.

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai

---------------------------------------------------------------------


"
"""Ganelin, Ilya"" <Ilya.Ganelin@capitalone.com>","Fri, 12 Dec 2014 16:41:51 -0500",Newest ML-Lib on Spark 1.1,dev <dev@spark.apache.org>,"Hi all  were running CDH 5.2 and would be interested in having the latest and greatest ML Lib version on our cluster (with YARN). Could anyone help me out in terms of figuring out what build profiles to use to get this to play well? Will I be able to update ML-Lib independently of updating the rest of spark to 1.2 and beyond? I ran into numerous issues trying to build 1.2 against CDHs Hadoop deployment. Alternately, if anyone has managed to get the trunk successfully built and tested against Clouderas YARN and Hadoop for 5.2 I would love some help. Thanks!
________________________________________________________

The information contained in this e-mail is confidential and/or proprietary is intended only for use by the individual or entity to which it is addressed.  If the reader of this message is not the intended recipient, you are hereby notified that any review, retransmission, dissemination, distribution, copying or other use of, or taking of any action in reliance upon this information is strictly prohibited. If you have received this communication in error, please contact the sender and delete the material from your computer.
"
Debasish Das <debasish.das83@gmail.com>,"Fri, 12 Dec 2014 13:50:30 -0800",Re: Newest ML-Lib on Spark 1.1,"""Ganelin, Ilya"" <Ilya.Ganelin@capitalone.com>","For CDH this works well for me...tested till 5.1...

./make-distribution -Dhadoop.version=2.3.0-cdh5.1.0 -Phadoop-2.3 -Pyarn
-Phive -DskipTests

To build with hive thriftserver support for spark-sql

 having the
ne
is
he
ld
anaged to
RN and
"
Sean Owen <sowen@cloudera.com>,"Fri, 12 Dec 2014 21:54:09 +0000",Re: Newest ML-Lib on Spark 1.1,"""Ganelin, Ilya"" <Ilya.Ganelin@capitalone.com>","Could you specify what problems you're seeing? there is nothing
special about the CDH distribution at all.

The latest and greatest is 1.1, and that is what is in CDH 5.2. You
can certainly compile even master for CDH and get it to work though.

The safest build flags should be ""-Phadoop-2.4 -Dhadoop.version=2.5.0-cdh5.2.1"".

5.3 is just around the corner, and includes 1.2, which is also just
around the corner.

 having the latest and greatest ML Lib version on our cluster (with YARN). Could anyone help me out in terms of figuring out what build profiles to use to get this to play well? Will I be able to update ML-Lib independently of updating the rest of spark to 1.2 and beyond? I ran into numerous issues trying to build 1.2 against CDH’s Hadoop deployment. Alternately, if anyone has managed to get the trunk successfully built and tested against Cloudera’s YARN and Hadoop for 5.2 I would love some help. Thanks!
th is intended only for use by the individual or entity to which it is addressed.  If the reader of this message is not the intended recipient, you are hereby notified that any review, retransmission, dissemination, distribution, copying or other use of, or taking of any action in reliance upon this information is strictly prohibited. If you have received this communication in error, please contact the sender and delete the material from your computer.

---------------------------------------------------------------------


"
Robert C Senkbeil <rcsenkbe@us.ibm.com>,"Fri, 12 Dec 2014 16:16:45 -0600",IBM open-sources Spark Kernel,"dev@spark.apache.org, user@spark.apache.org","


We are happy to announce a developer preview of the Spark Kernel which
enables remote applications to dynamically interact with Spark. You can
think of the Spark Kernel as a remote Spark Shell that uses the IPython
notebook interface to provide a common entrypoint for any application. The
Spark Kernel obviates the need to submit jars using spark-submit, and can
replace the existing Spark Shell.

You can try out the Spark Kernel today by installing it from our github
repo athttps://github.com/ibm-et/spark-kernel. To help you get a demo
environment up and running quickly, the repository also includes a
Dockerfile and a Vagrantfile to build a Spark Kernel container and connect
to it from an IPython notebook.

We have included a number of documents with the project to help explain it
and provide how-to information:

* A high-level overview of the Spark Kernel and its client library (
https://issues.apache.org/jira/secure/attachment/12683624/Kernel%20Architecture.pdf
).

* README (https://github.com/ibm-et/spark-kernel/blob/master/README.md) -
building and testing the kernel, and deployment options including building
the Docker container and packaging the kernel.

* IPython instructions (
https://github.com/ibm-et/spark-kernel/blob/master/docs/IPYTHON.md) -
setting up the development version of IPython and connecting a Spark
Kernel.

* Client library tutorial (
https://github.com/ibm-et/spark-kernel/blob/master/docs/CLIENT.md) -
building and using the client library to connect to a Spark Kernel.

* Magics documentation (
https://github.com/ibm-et/spark-kernel/blob/master/docs/MAGICS.md) - the
magics in the kernel and how to write your own.

We think the Spark Kernel will be useful for developing applications for
Spark, and we are making it available with the intention of improving these
capabilities within the context of the Spark community (
https://issues.apache.org/jira/browse/SPARK-4605). We will continue to
develop the codebase and welcome your comments and suggestions.


Signed,

Chip Senkbeil
IBM Emerging Technology Software Engineer"
"""Ganelin, Ilya"" <Ilya.Ganelin@capitalone.com>","Fri, 12 Dec 2014 17:32:46 -0500",RE: Newest ML-Lib on Spark 1.1,'Sean Owen' <sowen@cloudera.com>,"Hi Sean - I should clarify : I was able to build the master but when running I hit really random looking protobuf errors (just starting up a spark shell), I can try doing a build later today and give the exact stack trace.

I know that 5.2 is running 1.1 but I believe the latest and greatest Ml Lib is much fresher than the one in 1.1 and specifically includes fixed for ALS to help it scale better.

I had built with the exact flags you suggested below. After doing so I tried to run the test suite and run a spark she'll without success. Might you have any other suggestions? Thanks!



Sent with Good (www.good.com)


-----Original Message-----
From: Sean Owen [sowen@cloudera.com<mailto:sowen@cloudera.com>]
Sent: Friday, December 12, 2014 04:54 PM Eastern Standard Time
To: Ganelin, Ilya
Cc: dev
Subject: Re: Newest ML-Lib on Spark 1.1


Could you specify what problems you're seeing? there is nothing
special about the CDH distribution at all.

The latest and greatest is 1.1, and that is what is in CDH 5.2. You
can certainly compile even master for CDH and get it to work though.

The safest build flags should be ""-Phadoop-2.4 -Dhadoop.version=2.5.0-cdh5.2.1"".

5.3 is just around the corner, and includes 1.2, which is also just
around the corner.

On Fri, Dec 12, 2014 at 9:41 PM, Ganelin, Ilya
<Ilya.Ganelin@capitalone.com> wrote:
> Hi all – we’re running CDH 5.2 and would be interested in having the latest and greatest ML Lib version on our cluster (with YARN). Could anyone help me out in terms of figuring out what build profiles to use to get this to play well? Will I be able to update ML-Lib independently of updating the rest of spark to 1.2 and beyond? I ran into numerous issues trying to build 1.2 against CDH’s Hadoop deployment. Alternately, if anyone has managed to get the trunk successfully built and tested against Cloudera’s YARN and Hadoop for 5.2 I would love some help. Thanks!
> ________________________________________________________
>
> The information contained in this e-mail is confidential and/or proprietary to Capital One and/or its affiliates. The information transmitted herewith is intended only for use by the individual or entity to which it is addressed.  If the reader of this message is not the intended recipient, you are hereby notified that any review, retransmission, dissemination, distribution, copying or other use of, or taking of any action in reliance upon this information is strictly prohibited. If you have received this communication in error, please contact the sender and delete the material from your computer.
________________________________________________________

The information contained in this e-mail is confidential and/or proprietary to Capital One and/or its affiliates. The information transmitted herewith is intended only for use by the individual or entity to which it is addressed.  If the reader of this message is not the intended recipient, you are hereby notified that any review, retransmission, dissemination, distribution, copying or other use of, or taking of any action in reliance upon this information is strictly prohibited. If you have received this communication in error, please contact the sender and delete the material from your computer.
"
Sean Owen <sowen@cloudera.com>,"Fri, 12 Dec 2014 22:34:44 +0000",Re: Newest ML-Lib on Spark 1.1,"""Ganelin, Ilya"" <Ilya.Ganelin@capitalone.com>","What errors do you see? protobuf errors usually mean you didn't build
for the right version of Hadoop, but if you are using -Phadoop-2.3 or
better -Phadoop-2.4 that should be fine. Yes, a stack trace would be
good. I'm still not sure what error you are seeing.

ing
e.
ib
LS
ied
ave
n having the
one
his
the
ild
managed to
ARN and
y to
d
have
te
ry
h
you
e

---------------------------------------------------------------------


"
Debasish Das <debasish.das83@gmail.com>,"Fri, 12 Dec 2014 14:37:40 -0800",Re: Newest ML-Lib on Spark 1.1,Sean Owen <sowen@cloudera.com>,"protobuf comes from missing -Phadoop2.3

 in having the
g
s managed
 YARN and
u
,
al
"
DB Tsai <dbtsai@dbtsai.com>,"Fri, 12 Dec 2014 14:53:41 -0800",Re: CrossValidator API in new spark.ml package,"Xiangrui Meng <mengxr@gmail.com>, joseph@databricks.com, dev@spark.apache.org","Okay, I got it. In Estimator, fit(dataset: SchemaRDD, paramMaps:
Array[ParamMap]): Seq[M] can be overwritten to implement
regularization path. Correct me if I'm wrong.

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



---------------------------------------------------------------------


"
Robert C Senkbeil <rcsenkbe@us.ibm.com>,"Fri, 12 Dec 2014 17:29:15 -0600",Re: IBM open-sources Spark Kernel,Sam Bessalah <samkiller.oss@gmail.com>,"
Hi Sam,

We developed the Spark Kernel with a focus on the newest version of the
IPython message protocol (5.0) for the upcoming IPython 3.0 release.

We are building around Apache Spark's REPL, which is used in the current
Spark Shell implementation.

The Spark Kernel was designed to be extensible through magics (
https://github.com/ibm-et/spark-kernel/blob/master/docs/MAGICS.md),
providing functionality that might be needed outside the Scala interpreter.

Finally, a big part of our focus is on application development. Because of
this, we are providing a client library for applications to connect to the
Spark Kernel without needing to implement the ZeroMQ protocol.

Signed,
Chip Senkbeil



From:	Sam Bessalah <samkiller.oss@gmail.com>
To:	Robert C Senkbeil/Austin/IBM@IBMUS
Date:	12/12/2014 04:20 PM
Subject:	Re: IBM open-sources Spark Kernel



Wow. Thanks. Can't wait to try this out.
Great job.
How Is it different from Iscala or Ispark?


te:



  We are happy to announce a developer preview of the Spark Kernel which
  enables remote applications to dynamically interact with Spark. You can
  think of the Spark Kernel as a remote Spark Shell that uses the IPython
  notebook interface to provide a common entrypoint for any application.
  The
  Spark Kernel obviates the need to submit jars using spark-submit, and can
  replace the existing Spark Shell.

  You can try out the Spark Kernel today by installing it from our github
  repo athttps://github.com/ibm-et/spark-kernel. To help you get a demo
  environment up and running quickly, the repository also includes a
  Dockerfile and a Vagrantfile to build a Spark Kernel container and
  connect
  to it from an IPython notebook.

  We have included a number of documents with the project to help explain
  it
  and provide how-to information:

  * A high-level overview of the Spark Kernel and its client library (
  https://issues.apache.org/jira/secure/attachment/12683624/Kernel%20Architecture.pdf

  ).

  * README (https://github.com/ibm-et/spark-kernel/blob/master/README.md) -
  building and testing the kernel, and deployment options including
  building
  the Docker container and packaging the kernel.

  * IPython instructions (
  https://github.com/ibm-et/spark-kernel/blob/master/docs/IPYTHON.md) -
  setting up the development version of IPython and connecting a Spark
  Kernel.

  * Client library tutorial (
  https://github.com/ibm-et/spark-kernel/blob/master/docs/CLIENT.md) -
  building and using the client library to connect to a Spark Kernel.

  * Magics documentation (
  https://github.com/ibm-et/spark-kernel/blob/master/docs/MAGICS.md) - the
  magics in the kernel and how to write your own.

  We think the Spark Kernel will be useful for developing applications for
  Spark, and we are making it available with the intention of improving
  these
  capabilities within the context of the Spark community (
  https://issues.apache.org/jira/browse/SPARK-4605). We will continue to
  develop the codebase and welcome your comments and suggestions.


  Signed,

  Chip Senkbeil
  IBM Emerging Technology Software Engineer"
Lochana Menikarachchi <lochanac@gmail.com>,"Sat, 13 Dec 2014 07:46:01 +0530",one hot encoding,"""dev@spark.apache.org"" <dev@spark.apache.org>","Do we have one-hot encoding in spark MLLib 1.1.1 or 1.2.0 ? It wasn't 
available in 1.1.0.
Thanks.

---------------------------------------------------------------------


"
Josh Rosen <rosenville@gmail.com>,"Fri, 12 Dec 2014 20:00:38 -0800",Re: [VOTE] Release Apache Spark 1.2.0 (RC2),"Sandy Ryza <sandy.ryza@cloudera.com>, Reynold Xin
 <rxin@databricks.com>","+1.  Tested using spark-perf and the Spark EC2 scripts.  I didn’t notice any performance regressions that could not be attributed to changes of default configurations.  To be more specific, when running Spark 1.2.0 with the Spark 1.1.0 settings of s"
Mark Hamstra <mark@clearstorydata.com>,"Fri, 12 Dec 2014 20:55:17 -0800",Re: [VOTE] Release Apache Spark 1.2.0 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1

 notice
th
sion
 I
 so
s,
.0
c446e23e628b746e0626cc02b7b3cadf588e
t:
o
"
Denny Lee <denny.g.lee@gmail.com>,"Sat, 13 Dec 2014 06:47:05 +0000",Re: [VOTE] Release Apache Spark 1.2.0 (RC2),"Mark Hamstra <mark@clearstorydata.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","+1 Tested on OSX

Tested Scala 2.10.3, SparkSQL with Hive 0.12 / Hadoop 2.5, Thrift Server,
MLLib SVD



t notice
rt
s,
f
t,
al
2.
)
es
d
n
D
"
Sandy Ryza <sandy.ryza@cloudera.com>,"Sat, 13 Dec 2014 02:18:51 -0800",Re: one hot encoding,Lochana Menikarachchi <lochanac@gmail.com>,"Hi Lochana,

We haven't yet added this in 1.2.
https://issues.apache.org/jira/browse/SPARK-4081 tracks adding categorical
feature indexing, which one-hot encoding can be built on.
https://issues.apache.org/jira/browse/SPARK-1216 also tracks a version of
this prior to the ML pipelines work.

-Sandy

"
Tom Graves <tgraves_cs@yahoo.com.INVALID>,"Sat, 13 Dec 2014 12:58:25 +0000 (UTC)",Re: [VOTE] Release Apache Spark 1.2.0 (RC2),"Denny Lee <denny.g.lee@gmail.com>, 
	Mark Hamstra <mark@clearstorydata.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","+1 built and tested on Yarn on Hadoop 2.x cluster.
Tom 

   

 +1 Tested on OSX

Tested Scala 2.10.3, SparkSQL with Hive 0.12 / Hadoop 2.5, Thrift Server,
MLLib SVD



dn’t notice
.2.0
rt
ttings,
f
e
t,
des
icro
al
2.
)
es
d
n
D

   "
"""=?gb18030?B?R3VvUWlhbmcgTGk=?="" <witgo@qq.com>","Sat, 13 Dec 2014 21:11:37 +0800",Re: [VOTE] Release Apache Spark 1.2.0 (RC2),"""=?gb18030?B?UGF0cmljayBXZW5kZWxs?="" <pwendell@gmail.com>","+1 (non-binding).  Tested on CentOS 6.4


------------------ Original ------------------
From:  ""Patrick Wendell"";<pwendell@gmail.com>;
Date:  Thu, Dec 11, 2014 05:08 AM
To:  ""dev@spark.apache.org""<dev@spark.apache.org>;


Subject:  [VOTE] Re"
"""Nick Pentreath"" <nick.pentreath@gmail.com>","Sat, 13 Dec 2014 06:47:20 -0800 (PST)",Re: [VOTE] Release Apache Spark 1.2.0 (RC2),dev@spark.apache.org,"+1

—
Sent from Mailbox


1.2.0!
git;a=commit;h=a428c446e23e628b746e0626cc02b7b3cadf588e
been
""sort"".
to ""hash"".
columns
throws NPE
org"
slcclimber <anant.asty@gmail.com>,"Sat, 13 Dec 2014 11:32:52 -0700 (MST)",Re: [VOTE] Release Apache Spark 1.2.0 (RC2),dev@spark.incubator.apache.org,"I am building and testing using sbt.
I get a lot of 
""Job aborted due to stage failure: Master removed our application: FAILED""
did not contain ""cancelled"", and ""Job aborted due to stage failure: Master
removed our application: FAILED"" did not contain ""killed""
errors trying to run tests.  (JobCancellationSuite.scala:236)
I have never experienced this before so it is concerning.

I  was able to successfully run all the python examples for spark and Mllib
successfully.




--

---------------------------------------------------------------------


"
Sean McNamara <Sean.McNamara@Webtrends.com>,"Sat, 13 Dec 2014 20:47:11 +0000",Re: [VOTE] Release Apache Spark 1.2.0 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1 tested on OS X and deployed+tested our apps via YARN into our staging cluster.

Sean


te:
8c446e23e628b746e0626cc02b7b3cadf588e
t"".


---------------------------------------------------------------------


"
Josh Rosen <rosenville@gmail.com>,"Sat, 13 Dec 2014 14:05:56 -0800","Nabble mailing list mirror errors: ""This post has NOT been accepted
 by the mailing list yet""","""user@spark.apache.org"" <user@spark.apache.org>, dev <dev@spark.apache.org>","I've noticed that several users are attempting to post messages to Spark's
user / dev mailing lists using the Nabble web UI (
http://apache-spark-user-list.1001560.n3.nabble.com/).  However, there are
many posts in Nabble that are not posted to the Apache lists and are
flagged with ""This post has NOT been accepted by the mailing list yet.""
errors.

I suspect that the issue is that users are not completing the sign-up
confirmation process (
http://apache-spark-user-list.1001560.n3.nabble.com/mailing_list/MailingListOptions.jtp?forum=1),
which is preventing their emails from being accepted by the mailing list.

I wanted to mention this issue to the Spark community to see whether there
are any good solutions to address this.  I have spoken to users who think
that our mailing list is unresponsive / inactive because their un-posted
messages haven't received any replies.

- Josh
"
Yana Kadiyska <yana.kadiyska@gmail.com>,"Sat, 13 Dec 2014 21:27:27 -0500","Re: Nabble mailing list mirror errors: ""This post has NOT been
 accepted by the mailing list yet""",Josh Rosen <rosenville@gmail.com>,"Since you mentioned this, I had a related quandry recently -- it also says
that the forum archives ""*user@spark.incubator.apache.org
<user@spark.incubator.apache.org>""/* *dev@spark.incubator.apache.org
<dev@spark.incubator.apache.org> *respectively, yet the ""Community page""
clearly says to email the @spark.apache.org list (but the nabble archive is
linked right there too). IMO even putting a clear explanation at the top

""Posting here requires that you create an account via the UI. Your message
will be sent to both spark.incubator.apache.org and spark.apache.org (if
that is the case, i'm not sure which alias nabble posts get sent to)"" would
make things a lot more clear.

"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sun, 14 Dec 2014 07:31:50 +0000",Spark JIRA Report,dev <dev@spark.apache.org>,"What do y’all think of a report like this emailed out to the dev list on a
monthly basis?

The goal would be to increase visibility into our open issues and encourage
developers to tend to our issue tracker more frequently.

Nick

There are 1,236 unresolved issues
<https://issues.apache.org/jira/issues/?jql=project+%3D+SPARK+AND+resolution+%3D+Unresolved+ORDER+BY+updated+DESC>
in the Spark project on JIRA.
Recently Updated Issues
<https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20resolution%20%3D%20Unresolved%20ORDER%20BY%20updated%20DESC>
Type Key Priority Summary Last Updated   Bug SPARK-4841
<https://issues.apache.org/jira/browse/SPARK-4841> Major Batch serializer
bug in PySpark’s RDD.zip Dec 14, 2014  Question SPARK-4810
<https://issues.apache.org/jira/browse/SPARK-4810> Major Failed to run
collect Dec 14, 2014  Bug SPARK-785
<https://issues.apache.org/jira/browse/SPARK-785> Major ClosureCleaner not
invoked on most PairRDDFunctions Dec 14, 2014  New Feature SPARK-3405
<https://issues.apache.org/jira/browse/SPARK-3405> Minor EC2 cluster
creation on VPC Dec 13, 2014  Improvement SPARK-1555
<https://issues.apache.org/jira/browse/SPARK-1555> Minor enable
ec2/spark_ec2.py to stop/delete cluster non-interactively Dec 13, 2014   Stale
Issues
<https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20resolution%20%3D%20Unresolved%20AND%20updated%20%3C%3D%20-90d%20ORDER%20BY%20updated%20ASC>
Type Key Priority Summary Last Updated   Bug SPARK-560
<https://issues.apache.org/jira/browse/SPARK-560> None Specialize RDDs /
iterators Oct 22, 2012  New Feature SPARK-540
<https://issues.apache.org/jira/browse/SPARK-540> None Add API to customize
in-memory representation of RDDs Oct 22, 2012  Improvement SPARK-573
<https://issues.apache.org/jira/browse/SPARK-573> None Clarify semantics of
the parallelized closures Oct 22, 2012  New Feature SPARK-609
<https://issues.apache.org/jira/browse/SPARK-609> Minor Add instructions
for enabling Akka debug logging Nov 06, 2012  New Feature SPARK-636
<https://issues.apache.org/jira/browse/SPARK-636> Major Add mechanism to
run system management/configuration tasks on all workers Dec 17, 2012   Most
Watched Issues
<https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20resolution%20%3D%20Unresolved%20ORDER%20BY%20watchers%20DESC>
Type Key Priority Summary Watchers   New Feature SPARK-3561
<https://issues.apache.org/jira/browse/SPARK-3561> Major Allow for
pluggable execution contexts in Spark 75  New Feature SPARK-2365
<https://issues.apache.org/jira/browse/SPARK-2365> Major Add IndexedRDD, an
efficient updatable key-value store 33  Improvement SPARK-2044
<https://issues.apache.org/jira/browse/SPARK-2044> Major Pluggable
interface for shuffles 30  New Feature SPARK-1405
<https://issues.apache.org/jira/browse/SPARK-1405> Critical parallel Latent
Dirichlet Allocation (LDA) atop of spark in MLlib 26  New Feature SPARK-1406
<https://issues.apache.org/jira/browse/SPARK-1406> Major PMML model
evaluation support via MLib 21   Most Voted Issues
<https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20resolution%20%3D%20Unresolved%20ORDER%20BY%20votes%20DESC>
Type Key Priority Summary Votes   Bug SPARK-2541
<https://issues.apache.org/jira/browse/SPARK-2541> Major Standalone mode
can’t access secure HDFS anymore 12  New Feature SPARK-2365
<https://issues.apache.org/jira/browse/SPARK-2365> Major Add IndexedRDD, an
efficient updatable key-value store 9  Improvement SPARK-3533
<https://issues.apache.org/jira/browse/SPARK-3533> Major Add
saveAsTextFileByKey() method to RDDs 8  Bug SPARK-2883
<https://issues.apache.org/jira/browse/SPARK-2883> Blocker Spark Support
for ORCFile format 6  New Feature SPARK-1442
<https://issues.apache.org/jira/browse/SPARK-1442> Major Add Window
function support 6
​
"
Andrew Ash <andrew@andrewash.com>,"Sat, 13 Dec 2014 23:43:27 -0800",Governance of the Jenkins whitelist,dev <dev@spark.apache.org>,"Jenkins is a really valuable tool for increasing quality of incoming
patches to Spark, but I've noticed that there are often a lot of patches
waiting for testing because they haven't been approved for testing.

Certain users can instruct Jenkins to run on a PR, or add other users to a
whitelist. How does governance work for that list of admins?  Meaning who
is currently on it, and what are the requirements to be on that list?

Can I be permissioned to allow Jenkins to run on certain PRs?  I've often
come across well-intentioned PRs that are languishing because Jenkins has
yet to run on them.

Andrew
"
Andrew Ash <andrew@andrewash.com>,"Sat, 13 Dec 2014 23:48:38 -0800",Re: Spark JIRA Report,Nicholas Chammas <nicholas.chammas@gmail.com>,"The goal of increasing visibility on open issues is a good one.  How is
this different from just a link to Jira though?  Some might say this adds
noise to the mailing list and doesn't contain any information not already
available in Jira.

The idea seems good but the formatting leaves a little to be desired.  If
you aren't opposed to using HTML, I might suggest this more compact format:

SPARK-2044 <https://issues.apache.org/jira/browse/SPARK-2044>
Pluggable interface
for shuffles
SPARK-2365 <https://issues.apache.org/jira/browse/SPARK-2365> Add
IndexedRDD, an efficient updatable key-value
SPARK-3561 <https://issues.apache.org/jira/browse/SPARK-3561> Allow
for pluggable
execution contexts in Spark

Andrew


list on a
ge
tion+%3D+Unresolved+ORDER+BY+updated+DESC
20resolution%20%3D%20Unresolved%20ORDER%20BY%20updated%20DESC
t
20resolution%20%3D%20Unresolved%20AND%20updated%20%3C%3D%20-90d%20ORDER%20BY%20updated%20ASC
20resolution%20%3D%20Unresolved%20ORDER%20BY%20watchers%20DESC
20resolution%20%3D%20Unresolved%20ORDER%20BY%20votes%20DESC
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sun, 14 Dec 2014 08:20:54 +0000",Re: Spark JIRA Report,Andrew Ash <andrew@andrewash.com>,"I formatted this report using Markdown; I'm open to changing the structure
or formatting or reducing the amount of information to make the report more
easily consumable.

Regarding just sending links or whether this would just be mailing list
noise, those are a good questions.

I've sent out links before, but I feel from a UX perspective having the
information right in the email itself makes it frictionless for people to
act on the information. For me, that difference is enough to hook me into
spending a few minutes on JIRA vs. just glossing over an email with a link.

I wonder if that's also the case for others on this list.

If you already spend a good amount of time cleaning up on JIRA, then this
report won't be that relevant to you. But given the number and growth of
open issues on our tracker, I suspect we could do with quite a few more
people chipping in and cleaning up where they can.

That's the real problem that this report is intended to help with.

Nick



t:
nterface
luggable
 list on a
t
"
WangTaoTheTonic <barneystinson@aliyun.com>,"Sun, 14 Dec 2014 09:21:57 -0700 (MST)","Re: jenkins downtime: 730-930am, 12/12/14",dev@spark.incubator.apache.org,"Jenkins is still not available now as some unit tests(about streaming) failed
all the time. Does it have something to do with this update?



--

---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Sun, 14 Dec 2014 11:36:26 -0800",Re: Is there any document to explain how to build the hive jars for spark?,Yi Tian <tianyi.asiainfo@gmail.com>,"The modified version of hive can be found here:
https://github.com/pwendell/hive

"
shane knapp <sknapp@berkeley.edu>,"Sun, 14 Dec 2014 11:38:40 -0800","Re: jenkins downtime: 730-930am, 12/12/14",WangTaoTheTonic <barneystinson@aliyun.com>,"josh rosen has this PR open to address the streaming test failures:

https://github.com/apache/spark/pull/3687


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sun, 14 Dec 2014 20:28:45 +0000",Re: Spark JIRA Report,Andrew Ash <andrew@andrewash.com>,"Taking after Andrew’s suggestion, perhaps the report can just focus on
Stale issues (no updates in > 90 days), since those are probably the
easiest to act on.

For example:
Stale Issues
<https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20resolution%20%3D%20Unresolved%20AND%20updated%20%3C%3D%20-90d%20ORDER%20BY%20updated%20ASC>

   - [Oct 22, 2012] SPARK-560
   <https://issues.apache.org/jira/browse/SPARK-560>: Specialize RDDs /
   iterators
   - [Oct 22, 2012] SPARK-540
   <https://issues.apache.org/jira/browse/SPARK-540>: Add API to customize
   in-memory representation of RDDs
   - [Oct 22, 2012] SPARK-573
   <https://issues.apache.org/jira/browse/SPARK-573>: Clarify semantics of
   the parallelized closures
   - [Nov 06, 2012] SPARK-609
   <https://issues.apache.org/jira/browse/SPARK-609>: Add instructions for
   enabling Akka debug logging
   - [Dec 17, 2012] SPARK-636
   <https://issues.apache.org/jira/browse/SPARK-636>: Add mechanism to run
   system management/configuration tasks on all workers

Andrew,

Does that seem more useful?

Nick
​


e
re
k.
s
y
f
at:
interface
pluggable
v list on
r
"
Koert Kuipers <koert@tresata.com>,"Sun, 14 Dec 2014 15:41:58 -0500",spark kafka batch integration,"""dev@spark.apache.org"" <dev@spark.apache.org>, ""user@spark.apache.org"" <user@spark.apache.org>","hello all,
we at tresata wrote a library to provide for batch integration between
spark and kafka (distributed write of rdd to kafa, distributed read of rdd
from kafka). our main use cases are (in lambda architecture jargon):
* period appends to the immutable master dataset on hdfs from kafka using
spark
* make non-streaming data available in kafka with periodic data drops from
hdfs using spark. this is to facilitate merging the speed and batch layer
in spark-streaming
* distributed writes from spark-streaming

see here:
https://github.com/tresata/spark-kafka

best,
koert
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 15 Dec 2014 09:33:41 +0000",Archiving XML test reports for analysis,dev <dev@spark.apache.org>,"Every time we run a test cycle on our Jenkins cluster, we generate hundreds
of XML reports covering all the tests we have (e.g.
`streaming/target/test-reports/org.apache.spark.streaming.util.WriteAheadLogSuite.xml`).

These reports contain interesting information about whether tests succeeded
or failed, and how long they took to complete. There is also detailed
information about the environment they ran in.

It might be valuable to have a window into all these reports across all
Jenkins builds and across all time, and use that to track basic statistics
about our tests. That could give us basic insight into what tests are flaky
or slow, and perhaps drive other improvements to our testing infrastructure
that we can't see just yet.

Do people think that would be valuable? Do we already have something like
this?

I'm thinking for starters it might be cool if we automatically uploaded all
the XML test reports from the Master and the Pull Request builders to an S3
bucket and just opened it up for the dev community to analyze.

Nick
"
Cody Koeninger <cody@koeninger.org>,"Mon, 15 Dec 2014 09:51:18 -0600",Re: spark kafka batch integration,Koert Kuipers <koert@tresata.com>,"For an alternative take on a similar idea, see

https://github.com/koeninger/spark-1/tree/kafkaRdd/external/kafka/src/main/scala/org/apache/spark/rdd/kafka

An advantage of the approach I'm taking is that the lower and upper offsets
of the RDD are known in advance, so it's deterministic.

I haven't had a need to write to kafka from spark yet, so that's an obvious
advantage of your library.

I think the existing kafka dstream is inadequate for a number of use cases,
and would really like to see some combination of these approaches make it
into the spark codebase.


"
Pei Chen <chenpei@apache.org>,"Mon, 15 Dec 2014 11:26:13 -0500",Spark Web Site,dev@spark.apache.org,"Hi Spark Dev,
The cTAKES community was looking at revamping their web site and really
liked the clean look of the Spark one.  Is it just using Bootstrap and
static html pages checked into SVN?  Or are you using the Apache CMS
somehow?  Mind if we borrow the layout?

--Pei
"
shane knapp <sknapp@berkeley.edu>,"Mon, 15 Dec 2014 08:48:23 -0800",Re: Archiving XML test reports for analysis,Nicholas Chammas <nicholas.chammas@gmail.com>,"right now, the following logs are archived on to the master:

  local log_files=$(
    find .\
      -name ""unit-tests.log"" -o\
      -path ""./sql/hive/target/HiveCompatibilitySuite.failed"" -o\
      -path ""./sql/hive/target/HiveCompatibilitySuite.hiveFailed"" -o\
      -path ""./sql/hive/target/HiveCompatibilitySuite.wrong""
  )

regarding dumping stuff to S3 -- thankfully, since we're not looking at a
lot of disk usage, i don't see a problem w/this.  we could tar/zip up the
XML for each build and just dump it there.

what builds are we thinking about?  spark pull request builder?  what
others?

"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 15 Dec 2014 09:34:52 -0800",Re: Spark Web Site,Pei Chen <chenpei@apache.org>,"It's just Bootstrap checked into SVN and built using Jekyll. You can check out the raw source files from SVN from https://svn.apache.org/repos/asf/spark. IMO it's fine if you guys use the layout, but just make sure it doesn't look exactly the same because otherwise both sites will look like they're copied from some standard template. We used a slightly customized Bootstrap theme for it.

Matei

really


---------------------------------------------------------------------


"
Andrew Ash <andrew@andrewash.com>,"Mon, 15 Dec 2014 09:54:44 -0800",Re: Spark JIRA Report,Nicholas Chammas <nicholas.chammas@gmail.com>,"Nick,

Putting the N most stale issues into a report like your latest one does
seem like a good way to tackle the wall of text effect that I'm worried
about.


us on
%20resolution%20%3D%20Unresolved%20AND%20updated%20%3C%3D%20-90d%20ORDER%20BY%20updated%20ASC>
e
o
o
nk.
s
:
ds
dy
 interface
 pluggable
ev list
/
s
o
e
t
"
Patrick Wendell <pwendell@gmail.com>,"Mon, 15 Dec 2014 10:51:07 -0800",Test failures after Jenkins upgrade,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey All,

It appears that a single test suite is failing after the jenkins
upgrade: ""org.apache.spark.streaming.rdd.WriteAheadLogBackedBlockRDDSuite"".
My guess is the suite is not resilient in some way to differences in
the environment (JVM, OS version, or something else).

I'm going to disable the suite to get the build passing. This should
be done in the next 30 minutes or so.

- Patrick

---------------------------------------------------------------------


"
Josh Rosen <rosenville@gmail.com>,"Mon, 15 Dec 2014 10:54:33 -0800",Re: Test failures after Jenkins upgrade,"""=?utf-8?Q?dev=40spark.apache.org?="" <dev@spark.apache.org>, Patrick
 Wendell <pwendell@gmail.com>","There’s a JIRA for this: https://issues.apache.org/jira/browse/SPARK-4826

And two open PRs:

https://github.com/apache/spark/pull/3695
https://github.com/apache/spark/pull/3701

We might be close to fixing this via one of those PRs, so maybe we should try using one of those instead?


Hey All,  

It appears that a single test suite is failing after the jenkins  
upgrade: ""org.apache.spark.streaming.rdd.WriteAheadLogBackedBlockRDDSuite"".  
My guess is the suite is not resilient in some way to differences in  
the environment (JVM, OS version, or something else).  

I'm going to disable the suite to get the build passing. This should  
be done in the next 30 minutes or so.  

- Patrick  

---------------------------------------------------------------------  
For additional commands, e-mail: dev-help@spark.apache.org  

"
Patrick Wendell <pwendell@gmail.com>,"Mon, 15 Dec 2014 10:59:00 -0800",Re: Test failures after Jenkins upgrade,Josh Rosen <rosenville@gmail.com>,"Ah cool Josh - I think for some reason we are hitting this every time
now. Since this is holding up a bunch of other patches, I just pushed
something ignoring the tests as a hotfix. Even waiting for a couple
hours is really expensive productivity-wise given the frequency with
which we run tests. We should just re-enable them when we merge the
appropriate fix.


---------------------------------------------------------------------


"
Josh Rosen <rosenville@gmail.com>,"Mon, 15 Dec 2014 11:30:31 -0800",Re: Test failures after Jenkins upgrade,Patrick Wendell <pwendell@gmail.com>,"Opened one more PR for this: https://github.com/apache/spark/pull/3704



Ah cool Josh - I think for some reason we are hitting this every time  
now. Since this is holding up a bunch of other patches, I just pushed  
something ignoring the tests as a hotfix. Even waiting for a couple  
hours is really expensive productivity-wise given the frequency with  
which we run tests. We should just re-enable them when we merge the  
appropriate fix.  

te:  
26  
ld  
com)  
uite"".  
"
Xiangrui Meng <mengxr@gmail.com>,"Tue, 16 Dec 2014 03:42:07 +0800",Re: CrossValidator API in new spark.ml package,DB Tsai <dbtsai@dbtsai.com>,"Yes, regularization path could be viewed as training multiple models
at once. -Xiangrui


---------------------------------------------------------------------


"
Xiangrui Meng <mengxr@gmail.com>,"Tue, 16 Dec 2014 03:40:41 +0800",Re: [VOTE] Release Apache Spark 1.2.0 (RC1),Krishna Sankar <ksankar42@gmail.com>,"Hi Krishna,

Thanks for providing the notebook! I tried and found that the problem
is with PySpark's zip. I created a JIRA to track the issue:
https://issues.apache.org/jira/browse/SPARK-4841

-Xiangrui


---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 15 Dec 2014 22:35:33 +0000",Re: Spark JIRA Report,Andrew Ash <andrew@andrewash.com>,"OK, that's good.

Another approach we can take to controlling the number of stale JIRA issues
is writing a bot that simply closes issues after N days of inactivity and
prompts people to reopen the issue if it's still valid. I believe Sean Owen
proposed that at one point (?).

I wonder if that might be better since I feel that even a slimmed down
email might not be enough to get already-busy people to spend time on JIRA
management.

Nick


cus on
D%20resolution%20%3D%20Unresolved%20AND%20updated%20%3C%3D%20-90d%20ORDER%20BY%20updated%20ASC>
he
to
to
ink.
wth
ore
s
dds
ady
dev list
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 15 Dec 2014 22:39:21 +0000",Re: Archiving XML test reports for analysis,shane knapp <sknapp@berkeley.edu>,"How about all of them <https://amplab.cs.berkeley.edu/jenkins/view/Spark/>? How
much data per day would it roughly be if we uploaded all the logs for all
these builds?

Also, would Databricks be willing to offer up an S3 bucket for this purpose?

Nick


"
shane knapp <sknapp@berkeley.edu>,"Mon, 15 Dec 2014 15:00:01 -0800",Re: Archiving XML test reports for analysis,Nicholas Chammas <nicholas.chammas@gmail.com>,"i have no problem w/storing all of the logs.  :)

i also have no problem w/donated S3 buckets.  :)

"
Jun Feng Liu <liujunf@cn.ibm.com>,"Tue, 16 Dec 2014 09:23:03 +0800",Re: Tachyon in Spark,Haoyuan Li <haoyuan.li@gmail.com>,"
Thanks  the response. I got the point - sounds like todays Spark linage
dose not push to Tachyon linage.  Would be good to see how it works.

Jun Feng Liu.



                                                                           
             Haoyuan Li                                                    
             <haoyuan.li@gmail                                             
             .com>                                                      To 
                                       Jun Feng Liu/China/IBM@IBMCN,       
             2014-12-13 00:17                                           cc 
                                       Reynold Xin <rxin@databricks.com                                       Andrew Ash <andrew@andrewash.com                                       ""dev@spark.apache.org""              
                                       <dev@spark.apache.org>              
                                                                   Subject 
                                       Re: Tachyon in Spark                
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           




Junfeng, by off the heap solution, did you mean ""rdd.persist(OFF_HEAP)""?
That feature is different from the lineage feature. You can use this
feature (rdd.persist(OFF_HEAP)) now for any Spark version later than 1.0.0
with Tachyon without a problem.

Regarding Reynold's last email, those are good points. Tachyon had provided
this a while ago. We are working on enhancing this feature and the
integration part with Spark.

Thanks,

Haoyuan

e:

when

ink
solution
*86-10-82452683
<dev@spark.apache.org
e
storage;
.
https://github.com/amplab/tachyon/blob/master/core/src/main/java/tachyon/master/Dependency.java

ote:
as
s to
s
durability
lar
to
on,

 It
hat
in

--
Haoyuan Li
AMPLab, EECS, UC Berkeley
http://www.cs.berkeley.edu/~haoyuan/
"
Patrick Wendell <pwendell@gmail.com>,"Mon, 15 Dec 2014 18:23:00 -0800",Re: Governance of the Jenkins whitelist,Andrew Ash <andrew@andrewash.com>,"Hey Andrew,

The list of admins is maintained by the Amplab as part of their
donation of this infrastructure. The reason why we need to have admins
is that the pull request builder will fetch and then execute arbitrary
user code, so we need to do a security audit before we can approve
testing new patches. Over time when we get to know users we usually
whitelist them so they can test whatever they want.

I can see offline if the Amplab would be open to adding you as an
admin. I think we've added people over time who are very involved in
the community. Just wanted to send this e-mail so people understand
how it works.

- Patrick


---------------------------------------------------------------------


"
Ewan Higgs <ewan.higgs@ugent.be>,"Tue, 16 Dec 2014 09:38:19 +0100",Re: running the Terasort example,"Tim Harsch <tharsch@cray.com>, 
 ""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Tim,
run-example is here:
https://github.com/ehiggs/spark/blob/terasort/bin/run-example

It should be in the repository that you cloned. So if you were at the 
top level of the checkout, run-example would be run as ./bin/run-example.

Yours,
Ewan Higgs



---------------------------------------------------------------------


"
Jeniba Johnson <Jeniba.Johnson@lntinfotech.com>,"Tue, 16 Dec 2014 16:02:21 +0530",Data Loss - Spark streaming,Hari Shreedharan <hshreedharan@cloudera.com>,"Hi,

I need a clarification, while running streaming examples, suppose the batch interval is set to 5 minutes, after collecting the data from the input source(FLUME) and  processing till 5 minutes.
What will happen to the data which is flowing continuously from the input source to spark streaming ? Will that data be stored somewhere or else the data will be lost ?
Or else what is the solution to capture each and every data without any loss in Spark streaming.

Awaiting for your kind reply.


Regards,
Jeniba Johnson


________________________________
The contents of this e-mail and any attachment(s) may contain confidential or privileged information for the intended recipient(s). Unintended recipients are prohibited from taking action on the basis of information in this e-mail and using or disseminating the information, and must notify the sender and delete it from their system. L&T Infotech will not accept responsibility or liability for the accuracy or completeness of, or the presence of any virus or disabling code in this e-mail""
"
Madhu <madhu@madhu.com>,"Tue, 16 Dec 2014 10:09:54 -0700 (MST)",RDD data flow,dev@spark.incubator.apache.org,"I was looking at some of the Partition implementations in core/rdd and
getOrCompute(...) in CacheManager.
It appears that getOrCompute(...) returns an InterruptibleIterator, which
delegates to a wrapped Iterator.
That would imply that Partitions should extend Iterator, but that is not
always the case.
For example, these Partitions for these RDDs do not extend Iterator:

https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/PartitionwiseSampledRDD.scala
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/CoalescedRDD.scala

Why is that? Shouldn't all Partitions be Iterators? Clearly I'm missing
something.

more detail. The code is not hard to follow, but it's nice to have a simple
picture with the major components and some explanation of the flow.  The
declaration of Partition is throwing me off.

Thanks!



-----
--
Madhu
https://www.linkedin.com/in/msiddalingaiah
--

---------------------------------------------------------------------


"
Tim Harsch <tharsch@cray.com>,"Tue, 16 Dec 2014 18:27:42 +0000",Re: running the Terasort example,"Ewan Higgs <ewan.higgs@ugent.be>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","Hi Ewan,
Thanks, I think I was just a bit confused at the time, I was looking at
the spark-perf repo when there was the problem (uh.. ok)

I notice now with a pull down just minutes back that I still get a compile
problem. 
[ERROR] 
/Users/tharsch/git/ehiggs/spark/examples/src/main/scala/org/apache/spark/ex
amples/terasort/TeraInputFormat.scala:40: object task is not a member of
package org.apache.hadoop.mapreduce
[ERROR] import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
[ERROR]                                    ^
[ERROR] 
/Users/tharsch/git/ehiggs/spark/examples/src/main/scala/org/apache/spark/ex
amples/terasort/TeraInputFormat.scala:132: not found: type
TaskAttemptContextImpl
[ERROR]             val context = new TaskAttemptContextImpl(
[ERROR]                               ^
[ERROR] 
/Users/tharsch/git/ehiggs/spark/examples/src/main/scala/org/apache/spark/ex
amples/terasort/TeraOutputFormat.scala:76: value hsync is not a member of
org.apache.hadoop.fs.FSDataOutputStream
[ERROR]         out.hsync();
[ERROR]             ^




I can get past this by setting hadoop.version to 2.5.0 in the parent pom.

Thanks,
Tim


On 12/16/14, 12:38 AM, ""Ewan Higgs"" <ewan.higgs@ugent.be> wrote:

>Hi Tim,
>run-example is here:
>https://github.com/ehiggs/spark/blob/terasort/bin/run-example
>
>It should be in the repository that you cloned. So if you were at the
>top level of the checkout, run-example would be run as ./bin/run-example.
>
>Yours,
>Ewan Higgs
>
>On 12/12/14 01:06, Tim Harsch wrote:
>> Hi all,
>> I just joined the list, so I dont have a message history that would
>>allow
>> me to reply to this post:
>> 
>>http://apache-spark-developers-list.1001551.n3.nabble.com/Terasort-exampl
>>e-
>> td9284.html
>>
>> I am interested in running the terasort example.  I cloned the repo
>> https://github.com/ehiggs/spark and did checkout of the terasort branch.
>> In the above referenced post Ewan gives the example
>>
>> # Generate 1M 100 byte records:
>>    ./bin/run-example terasort.TeraGen 100M ~/data/terasort_in
>>
>>
>> I dont see a run-example in that repo.  Im sure I am missing
>>something
>> basic, or less likely, maybe some changes werent pushed?
>>
>> Thanks for any help,
>> Tim
>>
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>
>

"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 17 Dec 2014 02:02:03 +0000",Re: Scala's Jenkins setup looks neat,Reynold Xin <rxin@databricks.com>,"News flash!

<https://developer.github.com/v3/repos/statuses/>:

Note that the repo:status OAuth scope
<https://developer.github.com/v3/oauth/#scopes> grants targeted access to
Statuses *without* also granting access to repository code, while the repo
scope grants permission to code as well as statuses.

As I understand it, ASF Infra has said no in the past to granting access to
statuses because it also granted push access.

If so, this no longer appears to be the case.

1) Did I understand correctly and 2) should I open a new request with ASF
Infra to give us OAuth keys with repo:status access?

Nick


Aww, that's a bummer...
s
w
l
r
"
Reynold Xin <rxin@databricks.com>,"Tue, 16 Dec 2014 18:06:47 -0800",Re: Scala's Jenkins setup looks neat,Nicholas Chammas <nicholas.chammas@gmail.com>,"It's worth trying :)


o
or
"
Patrick Wendell <pwendell@gmail.com>,"Tue, 16 Dec 2014 18:10:59 -0800",Re: Scala's Jenkins setup looks neat,Reynold Xin <rxin@databricks.com>,"Yeah you can do it - just make sure they understand it is a new
feature so we're asking them to revisit it. They looked at it in the
past and they concluded they couldn't give us access without giving us
push access.

- Patrick


---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 17 Dec 2014 02:23:39 +0000",Re: Scala's Jenkins setup looks neat,"Patrick Wendell <pwendell@gmail.com>, Reynold Xin <rxin@databricks.com>","Actually, reading through the existing issue opened for this
<https://issues.apache.org/jira/browse/INFRA-7367> back in February, I
don’t see any explanation from ASF Infra as to why they won’t grant
permission against the Status API. They just recommended transitioning to
the Apache Jenkins instance.

Reynold/Patrick, was there any discussion elsewhere about this, or should I
just go ahead and try reopening the issue with the appropriate explanation?

Nick
​

:

ss
ra
a
e
d
"
Reynold Xin <rxin@databricks.com>,"Tue, 16 Dec 2014 18:37:43 -0800",Re: Scala's Jenkins setup looks neat,Nicholas Chammas <nicholas.chammas@gmail.com>,"This was the ticket: https://issues.apache.org/jira/browse/INFRA-7918

t grant
:
t
e
he
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 17 Dec 2014 02:41:39 +0000",Re: Scala's Jenkins setup looks neat,Reynold Xin <rxin@databricks.com>,"I see. That’s a separate discussion about closing PRs vs. just updating the
CI status on individual commits.

I’ll comment on INFRA-7367
<https://issues.apache.org/jira/browse/INFRA-7367>.

Nick
​


t grant
o
d
e
h
ut
he
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 17 Dec 2014 03:27:51 +0000",Re: Scala's Jenkins setup looks neat,Reynold Xin <rxin@databricks.com>,"Shot down again.
<https://issues.apache.org/jira/browse/INFRA-7367?focusedCommentId=14249382&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14249382>
​


dating
t grant
to
s
"
GregBowyer <gbowyer@fastmail.co.uk>,"Tue, 16 Dec 2014 21:22:39 -0700 (MST)",Re: Interested in contributing to GraphX in Python,dev@spark.incubator.apache.org,"I have been thinking about this for a little while and I wonder if it makes
sense to look at forcing off heap mmap storage what can be shared with
python.

The idea would be that java makes a DirectByteBuffer (or similar) with
python doing memoryview over that buffer.

Then for all except for real objects everything could be written in a numpy
friendly format into the mmap ?



--

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Tue, 16 Dec 2014 21:01:59 -0800",Re: RDD data flow,Madhu <madhu@madhu.com>,"
The Partition itself doesn't need to be an iterator - the iterator
comes from the result of compute(partition). The Partition is just an
identifier for that partition, not the data itself. Take a look at the
signature for compute() in the RDD class.

https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/RDD.scala#L97


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Tue, 16 Dec 2014 21:20:29 -0800",[RESULT] [VOTE] Release Apache Spark 1.2.0 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","This vote has PASSED with 12 +1 votes (8 binding) and no 0 or -1 votes:

+1:
Matei Zaharia*
Madhu Siddalingaiah
Reynold Xin*
Sandy Ryza
Josh Rozen*
Mark Hamstra*
Denny Lee
Tom Graves*
GuiQiang Li
Nick Pentreath*
Sean McNamara*
Patrick Wendell*

0:

-1:

I'll finalize and package this release in the next 48 hours. Thanks to
everyone who contributed.

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Tue, 16 Dec 2014 21:20:13 -0800",Re: [VOTE] Release Apache Spark 1.2.0 (RC2),Sean McNamara <Sean.McNamara@webtrends.com>,"I'm closing this vote now, will send results in a new thread.


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Tue, 16 Dec 2014 21:28:05 -0800",[ANNOUNCE] Requiring JIRA for inclusion in release credits,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey All,

Due to the very high volume of contributions, we're switching to an
automated process for generating release credits. This process relies
on JIRA for categorizing contributions, so it's not possible for us to
provide credits in the case where users submit pull requests with no
associated JIRA.

This needed to be automated because, with more than 1000 commits per
release, finding proper names for every commit and summarizing
contributions was taking on the order of days of time.

For 1.2.0 there were around 100 commits that did not have JIRA's. I'll
try to manually merge these into the credits, but please e-mail me
directly if you are not credited once the release notes are posted.
The notes should be posted within 48 hours of right now.

We already ask that users include a JIRA for pull requests, but now it
will be required for proper attribution. I've updated the contributing
guide on the wiki to reflect this.

- Patrick

---------------------------------------------------------------------


"
Ewan Higgs <ewan.higgs@ugent.be>,"Wed, 17 Dec 2014 08:42:07 +0100",Re: running the Terasort example,Tim Harsch <tharsch@cray.com>,"Hi Tim,

at
The PR that I am working on is indeed for spark-perf. 

pom.
I wasn’t sure how to get this working across all the Hadoop versions so I made it work with 2.4.0 and above. If you have advice on back porting this then I’m happy to implement it.

NB, TeraValidate may not be functioning appropriately. If you have trouble with it, I recommend using the Hadoop version.

Yours,
Ewan

./bin/run-example.
would
http://apache-spark-developers-list.1001551.n3.nabble.com/Terasort-exampl
branch.
sure I am missing
---------------------------------------------------------------------


---------------------------------------------------------------------


"
Xuelin Cao <xuelincao@yahoo.com.INVALID>,"Wed, 17 Dec 2014 10:25:14 +0000 (UTC)",When will Spark SQL support building DB index natively?,"Dev <dev@spark.apache.org>, User <user@spark.apache.org>","
Hi, 
     In Spark SQL help document, it says ""Some of these (such as indexes) are less important due to Spark SQL’s in-memory  computational model. Others are slotted for future releases of Spark SQL.   
   - Block level bitmap indexes and virtual columns (used to build indexes)""

     For our use cases, DB index is quite important. I have about 300G data in our database, and we always use ""customer id"" as a predicate for DB look up.  Without DB index, we will have to scan all 300G data, and it will take > 1 minute for a simple DB look up, while MySQL only takes 10 seconds. We tried to create an independent table for each ""customer id"", the result is pretty good, but the logic will be very complex. 
     I'm wondering when will Spark SQL supports DB index, and before that, is there an alternative way to support DB index function?
Thanks
"
Madhu <madhu@madhu.com>,"Wed, 17 Dec 2014 07:45:51 -0700 (MST)",Re: RDD data flow,dev@spark.incubator.apache.org,"Patrick Wendell wrote

OK, that makes sense. The docs for Partition are a bit vague on this point.
Maybe I'll add this to the docs.

Thanks Patrick!



-----
--
Madhu
https://www.linkedin.com/in/msiddalingaiah
--

---------------------------------------------------------------------


"
Tim Harsch <tharsch@cray.com>,"Wed, 17 Dec 2014 17:42:37 +0000",Re: running the Terasort example,Ewan Higgs <ewan.higgs@ugent.be>,"
On 12/16/14, 11:42 PM, ""Ewan Higgs"" <ewan.higgs@ugent.be> wrote:

>Hi Tim,
>
>> On 16 Dec 2014, at 19:27, Tim Harsch <tharsch@cray.com> wrote:
>> 
>> Hi Ewan,
>> Thanks, I think I was just a bit confused at the time, I was looking at
>> the spark-perf repo when there was the problem (uh.. ok)
>> 
>The PR that I am working on is indeed for spark-perf.
Yes but the example usage you gave, is for the code in ehiggs/spark (which
is where I got myself confused)

? git remote show origin
* remote origin
  Fetch URL: git@github.com:ehiggs/spark.git
  Push  URL: git@github.com:ehiggs/spark.git


? ll bin/run-example
-rwxr-xr-x  1 tharsch  513   2.1K Dec 11 21:02 bin/run-example


run-example is not in spark-perf, What is the expected usage, for the code
that is in spark-perf?  Im hoping Ill have time to run it later today,
so hopefully I will figure it out on my own.



> 
>
>> snip...
>> 
>> 
>> I can get past this by setting hadoop.version to 2.5.0 in the parent
>>pom.
>> 
>I wasnt sure how to get this working across all the Hadoop versions so I
>made it work with 2.4.0 and above. If you have advice on back porting
>this then Im happy to implement it.

I would like to try, hopefully I can find the time.

>
>NB, TeraValidate may not be functioning appropriately. If you have
>trouble with it, I recommend using the Hadoop version.

Thanks for the warning, I bet I could have banged my head on that for
hours.

>
>Yours,
>Ewan
>
>> Thanks,
>> Tim
>> 
>> 
>> On 12/16/14, 12:38 AM, ""Ewan Higgs"" <ewan.higgs@ugent.be> wrote:
>> 
>>> Hi Tim,
>>> run-example is here:
>>> https://github.com/ehiggs/spark/blob/terasort/bin/run-example
>>> 
>>> It should be in the repository that you cloned. So if you were at the
>>> top level of the checkout, run-example would be run as
>>>./bin/run-example.
>>> 
>>> Yours,
>>> Ewan Higgs
>>> 
>>> On 12/12/14 01:06, Tim Harsch wrote:
>>>> Hi all,
>>>> I just joined the list, so I dont have a message history that would
>>>> allow
>>>> me to reply to this post:
>>>> 
>>>> 
>>>>http://apache-spark-developers-list.1001551.n3.nabble.com/Terasort-exam
>>>>pl
>>>> e-
>>>> td9284.html
>>>> 
>>>> I am interested in running the terasort example.  I cloned the repo
>>>> https://github.com/ehiggs/spark and did checkout of the terasort
>>>>branch.
>>>> In the above referenced post Ewan gives the example
>>>> 
>>>> # Generate 1M 100 byte records:
>>>>   ./bin/run-example terasort.TeraGen 100M ~/data/terasort_in
>>>> 
>>>> 
>>>> I dont see a run-example in that repo.  Im sure I am missing
>>>> something
>>>> basic, or less likely, maybe some changes werent pushed?
>>>> 
>>>> Thanks for any help,
>>>> Tim
>>>> 
>>>> 
>>>> ---------------------------------------------------------------------
>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>> 
>>> 
>> 
>

"
Josh Rosen <rosenville@gmail.com>,"Wed, 17 Dec 2014 13:09:06 -0800","Re: Nabble mailing list mirror errors: ""This post has NOT been
 accepted by the mailing list yet""",yana.kadiyska@gmail.com,"Yeah, it looks like messages that are successfully posted via Nabble end up
on the Apache mailing list, but messages posted directly to Apache aren't
mirrored to Nabble anymore because it's based off the incubator mailing
list.  We should fix this so that Nabble posts to / archives the
non-incubator list.

"
Krishna Sankar <ksankar42@gmail.com>,"Wed, 17 Dec 2014 15:25:30 -0800",Fwd: [VOTE] Release Apache Spark 1.2.0 (RC2),"Patrick Wendell <pwendell@gmail.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Forgot Reply To All ;o(
---------- Forwarded message ----------
From: Krishna Sankar <ksankar42@gmail.com>
Date: Wed, Dec 10, 2014 at 9:16 PM
Subject: Re: [VOTE] Release Apache Spark 1.2.0 (RC2)
To: Matei Zaharia <matei.zaharia@gmail.com>

+1
Works same as RC1
1. Compiled OSX 10.10 (Yosemite) mvn -Pyarn -Phadoop-2.4
-Dhadoop.version=2.4.0 -DskipTests clean package 13:07 min
2. Tested pyspark, mlib - running as well as compare results with 1.1.x
2.1. statistics OK
2.2. Linear/Ridge/Laso Regression OK
       Slight difference in the print method (vs. 1.1.x) of the model
object - with a label & more details. This is good.
2.3. Decision Tree, Naive Bayes OK
       Changes in print(model) - now print (model.ToDebugString()) - OK
       Some changes in NaiveBayes. Different from my 1.1.x code - had to
flatten list structures, zip required same number in partitions
       After code changes ran fine.
2.4. KMeans OK
       Center And Scale OK
       zip occasionally fails with error ""localhost):
org.apache.spark.SparkException: Can only zip RDDs with same number of
elements in each partition""
Has https://issues.apache.org/jira/browse/SPARK-2251 reappeared ?
Made it work by doing a different transformation ie reusing an original
rdd.
(Xiangrui, I will end you the iPython Notebook & the dataset by a separate
e-mail)
2.5. rdd operations OK
       State of the Union Texts - MapReduce, Filter,sortByKey (word count)
2.6. recommendation OK
2.7. Good work ! In 1.x.x, had a map distinct over the movielens medium
dataset which never worked. Works fine in 1.2.0 !
3. Scala Mlib - subset of examples as in #2 above, with Scala
3.1. statistics OK
3.2. Linear Regression OK
3.3. Decision Tree OK
3.4. KMeans OK
Cheers
<k/>


"
Alessandro Baretta <alexbaretta@gmail.com>,"Wed, 17 Dec 2014 23:38:16 -0800",Re: Spark Shell slowness on Google Cloud,"Denny Lee <denny.g.lee@gmail.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Here's another data point: the slow part of my code is the construction of
an RDD as the union of the textFile RDDs representing data from several
distinct google storage directories. So the question becomes the following:
what computation happens when calling the union method on two RDDs?

"
Alessandro Baretta <alexbaretta@gmail.com>,"Thu, 18 Dec 2014 01:04:06 -0800",What RDD transformations trigger computations?,"""dev@spark.apache.org"" <dev@spark.apache.org>","All,

I noticed that while some operations that return RDDs are very cheap, such
as map and flatMap, some are quite expensive, such as union and groupByKey.
I'm referring here to the cost of constructing the RDD scala value, not the
cost of collecting the values contained in the RDD. This does not match my
understanding that RDD transformations only set up a computation without
actually running it. Oh, Spark developers, can you please provide some
clarity?

Alex
"
"""smyee@yahoo.com.INVALID"" <smyee@yahoo.com.INVALID>","Thu, 18 Dec 2014 17:54:10 +0800",Re: one hot encoding,Sandy Ryza <sandy.ryza@cloudera.com>,"Sandy, will it be available to pyspark use too?



---------------------------------------------------------------------


"
Cody Koeninger <cody@koeninger.org>,"Thu, 18 Dec 2014 09:07:05 -0600",Which committers care about Kafka?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Now that 1.2 is finalized...  who are the go-to people to get some
long-standing Kafka related issues resolved?

The existing api is not sufficiently safe nor flexible for our production
use.  I don't think we're alone in this viewpoint, because I've seen
several different patches and libraries to fix the same things we've been
running into.

Regarding flexibility

https://issues.apache.org/jira/browse/SPARK-3146

has been outstanding since August, and IMHO an equivalent of this is
absolutely necessary.  We wrote a similar patch ourselves, then found that
PR and have been running it in production.  We wouldn't be able to get our
jobs done without it.  It also allows users to solve a whole class of
problems for themselves (e.g. SPARK-2388, arbitrary delay of messages, etc).

Regarding safety, I understand the motivation behind WriteAheadLog as a
general solution for streaming unreliable sources, but Kafka already is a
reliable source.  I think there's a need for an api that treats it as
such.  Even aside from the performance issues of duplicating the
write-ahead log in kafka into another write-ahead log in hdfs, I need
exactly-once semantics in the face of failure (I've had failures that
prevented reloading a spark streaming checkpoint, for instance).

I've got an implementation i've been using

https://github.com/koeninger/spark-1/tree/kafkaRdd/external/kafka
/src/main/scala/org/apache/spark/rdd/kafka

Tresata has something similar at https://github.com/tresata/spark-kafka,
and I know there were earlier attempts based on Storm code.

Trying to distribute these kinds of fixes as libraries rather than patches
to Spark is problematic, because large portions of the implementation are
private[spark].

 I'd like to help, but i need to know whose attention to get.
"
francois.garillot@typesafe.com,"Thu, 18 Dec 2014 11:04:58 -0800 (PST)",Spark Streaming Data flow graph,dev@spark.apache.org,"I’ve been trying to produce an updated box diagram to refresh :
http://www.slideshare.net/spark-project/deep-divewithsparkstreaming-tathagatadassparkmeetup20130617/26


… after the SPARK-3129, and other switches (a surprising number of comments still mention NetworkReceiver).


Here’s what I have so far:
https://www.dropbox.com/s/q79taoce2ywdmf1/SparkStreaming.pdf?dl=0


This is not supposed to respect any particular convention (ER, ORM, …). Data flow up to right before RDD creation is in bold arrows, metadata flow is in normal width arrows.


This diagram is still very much a WIP (see below : todo), but I wanted to share it to ask:
- what’s wrong ?
- what are the glaring omissions ?
- how can I make this better (i.e. what should I add first to the Todo-list below) ?


I’ll be happy to share this (including sources) with whoever asks for it. 


Todo :
- mark private/public classes
- mark queues in Receiver, ReceivedBlockHandler, BlockManager
- mark type of info on transport : e.g. Actor message, ReceivedBlockInfo 



—
François Garillot"
Patrick Wendell <pwendell@gmail.com>,"Thu, 18 Dec 2014 11:13:02 -0800",Re: Which committers care about Kafka?,Cody Koeninger <cody@koeninger.org>,"Hey Cody,

Thanks for reaching out with this. The lead on streaming is TD - he is
traveling this week though so I can respond a bit. To the high level
point of whether Kafka is important - it definitely is. Something like
80% of Spark Streaming deployments (anecdotally) ingest data from
Kafka. Also, good support for Kafka is something we generally want in
Spark and not a library. In some cases IIRC there were user libraries
that used unstable Kafka API's and we were somewhat waiting on Kafka
to stabilize them to merge things upstream. Otherwise users wouldn't
be able to use newer Kakfa versions. This is a high level impression
only though, I haven't talked to TD about this recently so it's worth
revisiting given the developments in Kafka.

Please do bring things up like this on the dev list if there are
blockers for your usage - thanks for pinging it.

- Patrick


---------------------------------------------------------------------


"
"""Hari Shreedharan"" <hshreedharan@cloudera.com>","Thu, 18 Dec 2014 11:44:19 -0800 (PST)",Re: Which committers care about Kafka?,"""Patrick Wendell"" <pwendell@gmail.com>","Hi Cody,




I am an absolute +1 on SPARK-3146. I think we can implement something pretty simple and lightweight for that one.




For the Kafka DStream skipping the WAL implementation - this is something I discussed with TD a few weeks ago. Though it is a good idea to implement this to avoid unnecessary HDFS writes, it is an optimization. For that reason, we must be careful in implementation. There are a couple of issues that we need to ensure works properly - specifically ordering. To ensure we pull messages from different topics and partitions in the same order after failure, we’d still have to persist the metadata to HDFS (or some other system) - this metadata must contain the order of messages consumed, so we know how to re-read the messages. I am planning to explore this once I have some time (probably in Jan). In addition, we must also ensure bucketing functions work fine as well. I will file a placeholder jira for this one. 




I also wrote an API to write data back to Kafka a while back - https://github.com/apache/spark/pull/2994 . I am hoping that this will get pulled in soon, as this is something I know people want. I am open to feedback on that - anything that I can do to make it better.




Thanks, Hari


production
been
that
our
etc).
a

patches
are
org"
Josh Rosen <rosenville@gmail.com>,"Thu, 18 Dec 2014 11:52:46 -0800",Re: What RDD transformations trigger computations?,"Alessandro Baretta <alexbaretta@gmail.com>, 
 ""=?utf-8?Q?dev=40spark.apache.org?="" <dev@spark.apache.org>","Could you provide an example?  These operations are lazy, in the sense that they don’t trigger Spark jobs:


scala> val a = sc.parallelize(1 to 10000, 1).mapPartitions{ x => println(""computed a!""); x}
a: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[14] at mapPartitions at <console>:18

scala> a.union(a)
res4: org.apache.spark.rdd.RDD[Int] = UnionRDD[15] at union at <console>:22

scala> a.map(x => (x, x)).groupByKey()
res5: org.apache.spark.rdd.RDD[(Int, Iterable[Int])] = ShuffledRDD[17] at groupByKey at <console>:22

scala> a.map(x => (x, x)).groupByKey().count()
computed a!
res6: Long = 10000



All,  

I noticed that while some operations that return RDDs are very cheap, such  
as map and flatMap, some are quite expensive, such as union and groupByKey.  
I'm referring here to the cost of constructing the RDD scala value, not the  
cost of collecting the values contained in the RDD. This does not match my  
understanding that RDD transformations only set up a computation without  
actually running it. Oh, Spark developers, can you please provide some  
clarity?  

Alex  
"
Josh Rosen <rosenville@gmail.com>,"Thu, 18 Dec 2014 12:07:43 -0800",Re: Spark JIRA Report,"Nicholas Chammas <nicholas.chammas@gmail.com>, Andrew Ash
 <andrew@andrewash.com>","I don’t think that it makes sense to just close inactive JIRA issue without any human review.  There are many legitimate feature requests / bug reports that might be inactive for a long time because they’re low priorities to fix or because nobody has had time to deal with them yet.


OK, that's good.  

Another approach we can take to controlling the number of stale JIRA issues  
is writing a bot that simply closes issues after N days of inactivity and  
prompts people to reopen the issue if it's still valid. I believe Sean Owen  
proposed that at one point (?).  

I wonder if that might be better since I feel that even a slimmed down  
email might not be enough to get already-busy people to spend time on JIRA  
management.  

Nick  

te:  

  
  
focus on  
20AND%20resolution%20%3D%20Unresolved%20AND%20updated%20%3C%3D%20-90d%20ORDER%20BY%20updated%20ASC>  
 the  
st  
he  
e to  
e into  
 link.  
 
rowth  
 more  
 
is  
s adds  
ready  
.  
ct  
 
e dev list  
 run  
er  
-3405  
  
  
Ds  
  
6  
m  
12  
  
 
 
 
365  
 
"
Reynold Xin <rxin@databricks.com>,"Thu, 18 Dec 2014 12:14:11 -0800",Re: What RDD transformations trigger computations?,Josh Rosen <rosenville@gmail.com>,"Alessandro was probably referring to some transformations whose
implementations depend on some actions. For example: sortByKey requires
sampling the data to get the histogram.

There is a ticket tracking this:
https://issues.apache.org/jira/browse/SPARK-2992






s
at
h
y.
he
y
"
Cody Koeninger <cody@koeninger.org>,"Thu, 18 Dec 2014 14:26:08 -0600",Re: Which committers care about Kafka?,Hari Shreedharan <hshreedharan@cloudera.com>,"Thanks for the replies.

Regarding skipping WAL, it's not just about optimization.  If you actually
want exactly-once semantics, you need control of kafka offsets as well,
including the ability to not use zookeeper as the system of record for
offsets.  Kafka already is a reliable system that has strong ordering
guarantees (within a partition) and does not mandate the use of zookeeper
to store offsets.  I think there should be a spark api that acts as a very
simple intermediary between Kafka and the user's choice of downstream store.

Take a look at the links I posted - if there's already been 2 independent
implementations of the idea, chances are it's something people need.

m
t
s
we
r
 other
e
ve
a
s
a,
"
Mark Hamstra <mark@clearstorydata.com>,"Thu, 18 Dec 2014 12:29:16 -0800",Re: What RDD transformations trigger computations?,Reynold Xin <rxin@databricks.com>,"SPARK-2992 is a good start, but it's not exhaustive.  For example,
zipWithIndex is also an eager transformation, and we occasionally see PRs
suggesting additional eager transformations.

:
ons
] at
t
"
"""Hari Shreedharan"" <hshreedharan@cloudera.com>","Thu, 18 Dec 2014 13:27:00 -0800 (PST)",Re: Which committers care about Kafka?,"""Cody Koeninger"" <cody@koeninger.org>","I get what you are saying. But getting exactly once right is an extremely hard problem - especially in presence of failure. The issue is failures can happen in a bunch of places. For example, before the notification of downstream store being successful reaches the receiver that updates the offsets, the node fails. The store was successful, but duplicates came in either way. This is something worth discussing by itself - but without uuids etc this might not really be solved even when you think it is.




Anyway, I will look at the links. Even I am interested in all of the features you mentioned - no HDFS WAL for Kafka and once-only delivery, but I doubt the latter is really possible to guarantee - though I really would love to have that!




Thanks, Hari


actually
zookeeper
very
store.
independent
com
something
implement
issues
 we
after
some other
we
have
.
to
get
,
 a
is
need
that
com/tresata/spark-kafka,
implementation"
Cody Koeninger <cody@koeninger.org>,"Thu, 18 Dec 2014 15:51:54 -0600",Re: Which committers care about Kafka?,Hari Shreedharan <hshreedharan@cloudera.com>,"If the downstream store for the output data is idempotent or transactional,
and that downstream store also is the system of record for kafka offsets,
then you have exactly-once semantics.  Commit offsets with / after the data

Yes, this approach is biased towards the etl-like use cases rather than
near-realtime-analytics use cases.

m
an
t
d
as
g
r
ry
ore.
t
 to
 For
f
o
e
 HDFS (or
ore
to
t
s
d
t
n
"
=?UTF-8?Q?Luis_=C3=81ngel_Vicente_S=C3=A1nchez?= <langel.groups@gmail.com>,"Thu, 18 Dec 2014 21:56:35 +0000",Re: Which committers care about Kafka?,Cody Koeninger <cody@koeninger.org>,"But idempotency is not that easy t achieve sometimes. A strong only once
semantic through a proper API would  be superuseful; but I'm not implying
this is easy to achieve.

l,
ta
ly
in
s
rd
to HDFS
so
er
l
n
is
ke
n
s
h
n
ve
is
nd
of
dy
as
-
"
Sean Owen <sowen@cloudera.com>,"Thu, 18 Dec 2014 22:00:41 +0000",Fwd: Spark JIRA Report,dev <dev@spark.apache.org>,"In practice, most issues with no activity for, say, 6+ months are
dead. There's down-side in believing they will eventually get done by
somebody, since they almost always don't.

Most is clutter, but if there are important bugs among them, then the
fact they're idling is a different problem: too much demand / not
enough supply of attention, not saying 'no' to enough, fast enough,
and so on.

Sure you can prompt people to at least ping an issue they care about
once every 6 months to keep it alive. Which is essentially the same
as: Resolve and invite anyone who cares to Reopen. If nobody bothers,
can it be important? If the problem is, well, nobody would really be
paying attention to the prompts, that's this different problem again.

So: I think the auto-Resolve idea, or an email blast, is at best a
forcing mechanism to pay attention to a more fundamental issue. I
myself am less interested in that than working on the causes of
long-lived important stuff in a JIRA backlog.

You can see regular process progress like auto-closing PRs,
spark-prs.appspot.com, some big passes at closing stale issues. It's
still my impression that the bulk of existing JIRA does not get
reviewed, so there's more to do. For example, from a recent tour
through the JIRA list, there were ~50 that were even definitively
resolved, and not marked as such. It's not for lack of excellent
effort. The pace of good change outstrips any other project I've seen
by a wide margin, dwarfed only by unprecedented inbound load.

I'd rather the conversation be about more attacks on the supply/demand
problem, like adding committers to offload resolution of the easy and
clear changes more rapidly, docs or tools to help contributors make
better PRs/JIRAs in the first place, stating what is in and out of
scope upfront to direct efforts, and so on. That's a different
discussion from this one though.


ue without any human review.  There are many legitimate feature requests / bug reports that might be inactive for a long time because they’re low priorities to fix or because nobody has had time to deal with them yet.
es
en
A

---------------------------------------------------------------------


"
Alessandro Baretta <alexbaretta@gmail.com>,"Thu, 18 Dec 2014 14:01:54 -0800",Re: What RDD transformations trigger computations?,Reynold Xin <rxin@databricks.com>,"Reynold,

Yes, this exactly what I was referring to. I specifically noted this
unexpected behavior with sortByKey. I also noted that union is unexpectedly
very slow, taking several minutes to define the RDD: although it does not
seem to trigger a spark computation per se, it seems to cause the input
files to read by the Hadoop subsystem, which to the console such messages
as these:

14/12/18 05:52:49 INFO mapred.FileInputFormat: Total input paths to process
: 9
14/12/18 05:54:15 INFO mapred.FileInputFormat: Total input paths to process
: 759
14/12/18 05:54:40 INFO mapred.FileInputFormat: Total input paths to process
: 228
14/12/18 06:00:11 INFO mapred.FileInputFormat: Total input paths to process
: 3076
14/12/18 06:02:02 INFO mapred.FileInputFormat: Total input paths to process
: 1013
14/12/18 06:02:21 INFO mapred.FileInputFormat: Total input paths to process
: 156

More generally, it would be important for the documentation to clearly
point out what RDD transformations are eager, otherwise it is easy to
introduce horrible performance bugs by constructing unneeded RDDs, assuming
this is a lazy operation. I would venture to suggest introducing one or
more traits to collect all the eager RDD-to-RDD transformations, so that
the type system can be used to enforce that no eager transformation is used
where a lazy one is intended to be used.

Alex

:
ns
 at
ch
my
"
Josh Rosen <rosenville@gmail.com>,"Thu, 18 Dec 2014 15:37:27 -0800",Re: Fwd: Spark JIRA Report,"Sean Owen <sowen@cloudera.com>, dev <dev@spark.apache.org>","Slightly off-topic, but or helping to clear the PR review backlog, I have a proposal to add some “PR lifecycle” tools to spark-prs.appspot.com to make it easier to track which PRs are blocked on reviewers vs. authors: https://github.com/databricks/spark-pr-dashboard/pull/39


e:

In practice, most issues with no activity for, say, 6+ months are  
dead. There's down-side in believing they will eventually get done by  
somebody, since they almost always don't.  

Most is clutter, but if there are important bugs among them, then the  
fact they're idling is a different problem: too much demand / not  
enough supply of attention, not saying 'no' to enough, fast enough,  
and so on.  

Sure you can prompt people to at least ping an issue they care about  
once every 6 months to keep it alive. Which is essentially the same  
as: Resolve and invite anyone who cares to Reopen. If nobody bothers,  
can it be important? If the problem is, well, nobody would really be  
paying attention to the prompts, that's this different problem again.  

So: I think the auto-Resolve idea, or an email blast, is at best a  
forcing mechanism to pay attention to a more fundamental issue. I  
myself am less interested in that than working on the causes of  
long-lived important stuff in a JIRA backlog.  

You can see regular process progress like auto-closing PRs,  
spark-prs.appspot.com, some big passes at closing stale issues. It's  
still my impression that the bulk of existing JIRA does not get  
reviewed, so there's more to do. For example, from a recent tour  
through the JIRA list, there were ~50 that were even definitively  
resolved, and not marked as such. It's not for lack of excellent  
effort. The pace of good change outstrips any other project I've seen  
by a wide margin, dwarfed only by unprecedented inbound load.  

I'd rather the conversation be about more attacks on the supply/demand  
problem, like adding committers to offload resolution of the easy and  
clear changes more rapidly, docs or tools to help contributors make  
better PRs/JIRAs in the first place, stating what is in and out of  
scope upfront to direct efforts, and so on. That's a different  
discussion from this one though.  


e:  
ssue without any human review. There are many legitimate feature requests / bug reports that might be inactive for a long time because they’re low priorities to fix or because nobody has had time to deal with them yet.  
sues  
nd  
Owen  
 
IRA  

---------------------------------------------------------------------  
For additional commands, e-mail: dev-help@spark.apache.org  

"
andy <andykonwinski@gmail.com>,"Thu, 18 Dec 2014 17:38:17 -0700 (MST)","Re: Nabble mailing list mirror errors: ""This post has NOT been
 accepted by the mailing list yet""",dev@spark.apache.org,"I just changed the domain name in the mailing list archive settings to remove
"".incubator"" so maybe it'll work now.



--

---------------------------------------------------------------------


"
andy <andykonwinski@gmail.com>,"Thu, 18 Dec 2014 17:39:41 -0700 (MST)","Re: Nabble mailing list mirror errors: ""This post has NOT been
 accepted by the mailing list yet""",dev@spark.apache.org,"I just changed the domain name in the mailing list archive settings to remove
"".incubator"" so maybe it'll work now.

Andy



--

---------------------------------------------------------------------


"
"""Shao, Saisai"" <saisai.shao@intel.com>","Fri, 19 Dec 2014 02:03:30 +0000",RE: Which committers care about Kafka?,"=?utf-8?B?THVpcyDDgW5nZWwgVmljZW50ZSBTw6FuY2hleg==?=
	<langel.groups@gmail.com>, Cody Koeninger <cody@koeninger.org>","Hi all,

I agree with Hari that Strong exact-once semantics is very hard to guarantee, especially in the failure situation. From my understanding even current implementation of ReliableKafkaReceiver cannot fully guarantee the exact once semantics once failed, first is the ordering of data replaying from last checkpoint, this is hard to guarantee when multiple partitions are injected in; second is the design complexity of achieving this, you can refer to the Kafka Spout in Trident, we have to dig into the very details of Kafka metadata management system to achieve this, not to say rebalance and fault-tolerance. 

Thanks
Jerry

-----Original Message-----
From: Luis Ángel Vicente Sánchez [mailto:langel.groups@gmail.com] 
Sent: Friday, December 19, 2014 5:57 AM
To: Cody Koeninger
Cc: Hari Shreedharan; Patrick Wendell; dev@spark.apache.org
Subject: Re: Which committers care about Kafka?

But idempotency is not that easy t achieve sometimes. A strong only once semantic through a proper API would  be superuseful; but I'm not implying this is easy to achieve.
On 18 Dec 2014 21:52, ""Cody Koeninger"" <cody@koeninger.org> wrote:

> If the downstream store for the output data is idempotent or 
> transactional, and that downstream store also is the system of record 
> for kafka offsets, then you have exactly-once semantics.  Commit 
> offsets with / after the data is stored.  On any failure, restart from the last committed offsets.
>
> Yes, this approach is biased towards the etl-like use cases rather 
> than near-realtime-analytics use cases.
>
> On Thu, Dec 18, 2014 at 3:27 PM, Hari Shreedharan < 
> hshreedharan@cloudera.com
> > wrote:
> >
> > I get what you are saying. But getting exactly once right is an 
> > extremely hard problem - especially in presence of failure. The 
> > issue is failures
> can
> > happen in a bunch of places. For example, before the notification of 
> > downstream store being successful reaches the receiver that updates 
> > the offsets, the node fails. The store was successful, but 
> > duplicates came in either way. This is something worth discussing by 
> > itself - but without uuids etc this might not really be solved even when you think it is.
> >
> > Anyway, I will look at the links. Even I am interested in all of the 
> > features you mentioned - no HDFS WAL for Kafka and once-only 
> > delivery,
> but
> > I doubt the latter is really possible to guarantee - though I really
> would
> > love to have that!
> >
> > Thanks,
> > Hari
> >
> >
> > On Thu, Dec 18, 2014 at 12:26 PM, Cody Koeninger 
> > <cody@koeninger.org>
> > wrote:
> >
> >> Thanks for the replies.
> >>
> >> Regarding skipping WAL, it's not just about optimization.  If you 
> >> actually want exactly-once semantics, you need control of kafka 
> >> offsets
> as
> >> well, including the ability to not use zookeeper as the system of 
> >> record for offsets.  Kafka already is a reliable system that has 
> >> strong
> ordering
> >> guarantees (within a partition) and does not mandate the use of
> zookeeper
> >> to store offsets.  I think there should be a spark api that acts as 
> >> a
> very
> >> simple intermediary between Kafka and the user's choice of 
> >> downstream
> store.
> >>
> >> Take a look at the links I posted - if there's already been 2
> independent
> >> implementations of the idea, chances are it's something people need.
> >>
> >> On Thu, Dec 18, 2014 at 1:44 PM, Hari Shreedharan < 
> >> hshreedharan@cloudera.com> wrote:
> >>>
> >>> Hi Cody,
> >>>
> >>> I am an absolute +1 on SPARK-3146. I think we can implement 
> >>> something pretty simple and lightweight for that one.
> >>>
> >>> For the Kafka DStream skipping the WAL implementation - this is 
> >>> something I discussed with TD a few weeks ago. Though it is a good
> idea to
> >>> implement this to avoid unnecessary HDFS writes, it is an
> optimization. For
> >>> that reason, we must be careful in implementation. There are a 
> >>> couple
> of
> >>> issues that we need to ensure works properly - specifically ordering.
> To
> >>> ensure we pull messages from different topics and partitions in 
> >>> the
> same
> >>> order after failure, we’d still have to persist the metadata to 
> >>> HDFS
> (or
> >>> some other system) - this metadata must contain the order of 
> >>> messages consumed, so we know how to re-read the messages. I am 
> >>> planning to
> explore
> >>> this once I have some time (probably in Jan). In addition, we must 
> >>> also ensure bucketing functions work fine as well. I will file a 
> >>> placeholder jira for this one.
> >>>
> >>> I also wrote an API to write data back to Kafka a while back -
> >>> https://github.com/apache/spark/pull/2994 . I am hoping that this 
> >>> will get pulled in soon, as this is something I know people want. 
> >>> I am open
> to
> >>> feedback on that - anything that I can do to make it better.
> >>>
> >>> Thanks,
> >>> Hari
> >>>
> >>>
> >>> On Thu, Dec 18, 2014 at 11:14 AM, Patrick Wendell 
> >>> <pwendell@gmail.com>
> >>> wrote:
> >>>
> >>>>  Hey Cody,
> >>>>
> >>>> Thanks for reaching out with this. The lead on streaming is TD - 
> >>>> he is traveling this week though so I can respond a bit. To the 
> >>>> high level point of whether Kafka is important - it definitely 
> >>>> is. Something like 80% of Spark Streaming deployments 
> >>>> (anecdotally) ingest data from Kafka. Also, good support for 
> >>>> Kafka is something we generally want in Spark and not a library. 
> >>>> In some cases IIRC there were user libraries that used unstable 
> >>>> Kafka API's and we were somewhat waiting on Kafka to stabilize 
> >>>> them to merge things upstream. Otherwise users wouldn't be able 
> >>>> to use newer Kakfa versions. This is a high level impression only 
> >>>> though, I haven't talked to TD about this recently so it's worth revisiting given the developments in Kafka.
> >>>>
> >>>> Please do bring things up like this on the dev list if there are 
> >>>> blockers for your usage - thanks for pinging it.
> >>>>
> >>>> - Patrick
> >>>>
> >>>> On Thu, Dec 18, 2014 at 7:07 AM, Cody Koeninger 
> >>>> <cody@koeninger.org>
> >>>> wrote:
> >>>> > Now that 1.2 is finalized... who are the go-to people to get 
> >>>> > some long-standing Kafka related issues resolved?
> >>>> >
> >>>> > The existing api is not sufficiently safe nor flexible for our
> >>>> production
> >>>> > use. I don't think we're alone in this viewpoint, because I've 
> >>>> > seen several different patches and libraries to fix the same 
> >>>> > things we've
> >>>> been
> >>>> > running into.
> >>>> >
> >>>> > Regarding flexibility
> >>>> >
> >>>> > https://issues.apache.org/jira/browse/SPARK-3146
> >>>> >
> >>>> > has been outstanding since August, and IMHO an equivalent of 
> >>>> > this is absolutely necessary. We wrote a similar patch 
> >>>> > ourselves, then found
> >>>> that
> >>>> > PR and have been running it in production. We wouldn't be able 
> >>>> > to
> get
> >>>> our
> >>>> > jobs done without it. It also allows users to solve a whole 
> >>>> > class of problems for themselves (e.g. SPARK-2388, arbitrary 
> >>>> > delay of
> >>>> messages, etc).
> >>>> >
> >>>> > Regarding safety, I understand the motivation behind 
> >>>> > WriteAheadLog
> as
> >>>> a
> >>>> > general solution for streaming unreliable sources, but Kafka 
> >>>> > already
> >>>> is a
> >>>> > reliable source. I think there's a need for an api that treats 
> >>>> > it as such. Even aside from the performance issues of 
> >>>> > duplicating the write-ahead log in kafka into another 
> >>>> > write-ahead log in hdfs, I
> need
> >>>> > exactly-once semantics in the face of failure (I've had 
> >>>> > failures
> that
> >>>> > prevented reloading a spark streaming checkpoint, for instance).
> >>>> >
> >>>> > I've got an implementation i've been using
> >>>> >
> >>>> > https://github.com/koeninger/spark-1/tree/kafkaRdd/external/kaf
> >>>> > ka /src/main/scala/org/apache/spark/rdd/kafka
> >>>> >
> >>>> > Tresata has something similar at
> >>>> https://github.com/tresata/spark-kafka,
> >>>> > and I know there were earlier attempts based on Storm code.
> >>>> >
> >>>> > Trying to distribute these kinds of fixes as libraries rather 
> >>>> > than
> >>>> patches
> >>>> > to Spark is problematic, because large portions of the
> implementation
> >>>> are
> >>>> > private[spark].
> >>>> >
> >>>> > I'd like to help, but i need to know whose attention to get.
> >>>>
> >>>> -----------------------------------------------------------------
> >>>> ---- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For 
> >>>> additional commands, e-mail: dev-help@spark.apache.org
> >>>>
> >>>>
> >>>
> >
>
"
Patrick Wendell <pwendell@gmail.com>,"Thu, 18 Dec 2014 18:23:00 -0800",Re: [RESULT] [VOTE] Release Apache Spark 1.2.0 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","Update: An Apache infrastructure issue prevented me from pushing this
last night. The issue was resolved today and I should be able to push
the final release artifacts tonight.


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Fri, 19 Dec 2014 00:52:19 -0800",Announcing Spark 1.2!,"""user@spark.apache.org"" <user@spark.apache.org>, ""dev@spark.apache.org"" <dev@spark.apache.org>","I'm happy to announce the availability of Spark 1.2.0! Spark 1.2.0 is
the third release on the API-compatible 1.X line. It is Spark's
largest release ever, with contributions from 172 developers and more
than 1,000 commits!

This release brings operational and performance improvements in Spark
core including a new network transport subsytem designed for very
large shuffles. Spark SQL introduces an API for external data sources
along with Hive 13 support, dynamic partitioning, and the
fixed-precision decimal type. MLlib adds a new pipeline-oriented
package (spark.ml) for composing multiple algorithms. Spark Streaming
adds a Python API and a write ahead log for fault tolerance. Finally,
GraphX has graduated from alpha and introduces a stable API along with
performance improvements.

Visit the release notes [1] to read about the new features, or
download [2] the release today.

For errata in the contributions or release notes, please e-mail me
*directly* (not on-list).

Thanks to everyone involved in creating, testing, and documenting this release!

[1] http://spark.apache.org/releases/spark-release-1-2-0.html
[2] http://spark.apache.org/downloads.html

---------------------------------------------------------------------


"
Dibyendu Bhattacharya <dibyendu.bhattachary@gmail.com>,"Fri, 19 Dec 2014 15:14:56 +0530",Re: Which committers care about Kafka?,"""Shao, Saisai"" <saisai.shao@intel.com>","Hi,

Thanks to Jerry for mentioning the Kafka Spout for Trident. The Storm
Trident has done the exact-once guarantee by processing the tuple in a
batch  and assigning same transaction-id for a given batch . The replay for
a given batch with a transaction-id will have exact same set of tuples and
replay of batches happen in exact same order before the failure.

Having this paradigm, if downstream system process data for a given batch
for having a given transaction-id , and if during failure if same batch is
again emitted , you can check if same transaction-id is already processed
or not and hence can guarantee exact once semantics.

And this can only be achieved in Spark if we use Low Level Kafka consumer
API to process the offsets. This low level Kafka Consumer (
https://github.com/dibbhatt/kafka-spark-consumer) has implemented the Spark
Kafka consumer which uses Kafka Low Level APIs . All of the Kafka related
logic has been taken from Storm-Kafka spout and which manages all Kafka
re-balance and fault tolerant aspects and Kafka metadata managements.

Presently this Consumer maintains that during Receiver failure, it will
re-emit the exact same Block with same set of messages . Every message have
the details of its partition, offset and topic related details which can
tackle the SPARK-3146.

As this Low Level consumer has complete control over the Kafka Offsets , we
can implement Trident like feature on top of it like having implement a
transaction-id for a given block , and re-emit the same block with same set
of message during Driver failure.

Regards,
Dibyendu


:
n
e
an
m]
g.
a to
"
Shixiong Zhu <zsxwing@gmail.com>,"Fri, 19 Dec 2014 17:50:08 +0800",Re: Announcing Spark 1.2!,Patrick Wendell <pwendell@gmail.com>,"Congrats!

A little question about this release: Which commit is this release based
on? v1.2.0 and v1.2.0-rc2 are pointed to different commits in
https://github.com/apache/spark/releases

Best Regards,
Shixiong Zhu

2014-12-19 16:52 GMT+08:00 Patrick Wendell <pwendell@gmail.com>:
"
Sean Owen <sowen@cloudera.com>,"Fri, 19 Dec 2014 09:55:29 +0000",Re: Announcing Spark 1.2!,Shixiong Zhu <zsxwing@gmail.com>,"Tag 1.2.0 is older than 1.2.0-rc2. I wonder if it just didn't get
updated. I assume it's going to be 1.2.0-rc2 plus a few commits
related to the release process.


---------------------------------------------------------------------


"
"""wyphao.2007"" <wyphao.2007@163.com>","Fri, 19 Dec 2014 19:16:57 +0800 (CST)",Re:Re: Announcing Spark 1.2!,dev@spark.apache.org,"

In the http://spark.apache.org/downloads.html page,We cann't download the newest Spark release.  






At 2014-12-19 17:55:29,""Sean Owen"" <sowen@cloudera.com> wrote:
>Tag 1.2.0 is older than 1.2.0-rc2. I wonder if it just didn't get
>updated. I assume it's going to be 1.2.0-rc2 plus a few commits
>related to the release process.
>
>On Fri, Dec 19, 2014 at 9:50 AM, Shixiong Zhu <zsxwing@gmail.com> wrote:
>> Congrats!
>>
>> A little question about this release: Which commit is this release based on?
>> v1.2.0 and v1.2.0-rc2 are pointed to different commits in
>> https://github.com/apache/spark/releases
>>
>> Best Regards,
>>
>> Shixiong Zhu
>>
>> 2014-12-19 16:52 GMT+08:00 Patrick Wendell <pwendell@gmail.com>:
>>>
>>> I'm happy to announce the availability of Spark 1.2.0! Spark 1.2.0 is
>>> the third release on the API-compatible 1.X line. It is Spark's
>>> largest release ever, with contributions from 172 developers and more
>>> than 1,000 commits!
>>>
>>> This release brings operational and performance improvements in Spark
>>> core including a new network transport subsytem designed for very
>>> large shuffles. Spark SQL introduces an API for external data sources
>>> along with Hive 13 support, dynamic partitioning, and the
>>> fixed-precision decimal type. MLlib adds a new pipeline-oriented
>>> package (spark.ml) for composing multiple algorithms. Spark Streaming
>>> adds a Python API and a write ahead log for fault tolerance. Finally,
>>> GraphX has graduated from alpha and introduces a stable API along with
>>> performance improvements.
>>>
>>> Visit the release notes [1] to read about the new features, or
>>> download [2] the release today.
>>>
>>> For errata in the contributions or release notes, please e-mail me
>>> *directly* (not on-list).
>>>
>>> Thanks to everyone involved in creating, testing, and documenting this
>>> release!
>>>
>>> [1] http://spark.apache.org/releases/spark-release-1-2-0.html
>>> [2] http://spark.apache.org/downloads.html
>>>
>>> ---------------------------------------------------------------------
>>> To unsubscribe, e-mail: user-unsubscribe@spark.apache.org
>>> For additional commands, e-mail: user-help@spark.apache.org
>>>
>>
>
>---------------------------------------------------------------------
>To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>For additional commands, e-mail: dev-help@spark.apache.org
>
>
>
>
"
Sean Owen <sowen@cloudera.com>,"Fri, 19 Dec 2014 11:19:32 +0000",Re: Re: Announcing Spark 1.2!,"""wyphao.2007"" <wyphao.2007@163.com>","I can download it. Make sure you refresh the page, maybe, so that it
shows the 1.2.0 download as an option.


---------------------------------------------------------------------


"
David McWhorter <mcwhorter@ccri.com>,"Fri, 19 Dec 2014 12:09:45 -0500",spark-yarn_2.10 1.2.0 artifacts,dev@spark.apache.org,"Hi all,

Thanks for your work on spark!  I am trying to locate spark-yarn jars 
for the new 1.2.0 release.  The jars for spark-core, etc, are on maven 
central, but the spark-yarn jars are missing.

Confusingly and perhaps relatedly, I also can't seem to get the 
spark-yarn artifact to install on my local computer when I run 'mvn 
-Pyarn -Phadoop-2.2 -Dhadoop.version=2.2.0 -DskipTests clean install'.  
At the install plugin stage, maven reports:

[INFO] --- maven-install-plugin:2.5.1:install (default-install) @ 
spark-yarn_2.10 ---
[INFO] Skipping artifact installation

Any help or insights into how to use spark-yarn_2.10 1.2.0 in a maven 
build would be appreciated.

David

-- 

David McWhorter
Software Engineer
Commonwealth Computer Research, Inc.
1422 Sachem Place, Unit #1
Charlottesville, VA 22901
mcwhorter@ccri.com | 434.299.0090x204


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Fri, 19 Dec 2014 17:13:18 +0000",Re: spark-yarn_2.10 1.2.0 artifacts,David McWhorter <mcwhorter@ccri.com>,"I believe spark-yarn does not exist from 1.2 onwards. Have a look at
spark-network-yarn for where some of that went, I believe.


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Fri, 19 Dec 2014 10:50:50 -0800",Re: Announcing Spark 1.2!,Sean Owen <sowen@cloudera.com>,"Thanks for pointing out the tag issue. I've updated all links to point
to the correct tag (from the vote thread):

a428c446e23e628b746e0626cc02b7b3cadf588e


---------------------------------------------------------------------


"
thlee <tiong@ooyala.com>,"Fri, 19 Dec 2014 11:51:53 -0700 (MST)","Re: Confirming race condition in DagScheduler
 (NoSuchElementException)",dev@spark.apache.org,"any comments?



--

---------------------------------------------------------------------


"
Mingyu Kim <mkim@palantir.com>,"Fri, 19 Dec 2014 18:57:41 +0000","Spark master OOMs with exception stack trace stored in
 JobProgressListener (SPARK-4906)","""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

I just filed a bug SPARK-4906<https://issues.apache.org/jira/browse/SPARK-4906>, regarding Spark master OOMs. If I understand correctly, the UI states for all running applications are kept in memory retained by JobProgressListener, and when there are a lot of exception stack traces, this UI states can take up a significant amount of heap. This seems very bad especially for long-running applications.

Can you correct me if Im misunderstanding anything? If my understanding is correct, is there any work being done to make sure the UI states dont grow indefinitely over time? Would it make sense to spill some states to disk or work with what spark.eventLog is doing so Spark master doesnt need to keep things in memory?

Thanks,
Mingyu
"
Hari Shreedharan <hshreedharan@cloudera.com>,"Fri, 19 Dec 2014 11:06:09 -0800",Re: Which committers care about Kafka?,Dibyendu Bhattacharya <dibyendu.bhattachary@gmail.com>,"Hi Dibyendu,

Thanks for the details on the implementation. But I still do not believe
that it is no duplicates - what they achieve is that the same batch is
processed exactly the same way every time (but see it may be processed more
than once) - so it depends on the operation being idempotent. I believe
Trident uses ZK to keep track of the transactions - a batch can be
processed multiple times in failure scenarios (for example, the transaction
is processed but before ZK is updated the machine fails, causing a ""new""
node to process it again).

I don't think it is impossible to do this in Spark Streaming as well and
I'd be really interested in working on it at some point in the near future.


or
d
s
s.
ve
a
et
en
he
g
can
s
e
om]
g
.
ta to
.
"
Harikrishna Kamepalli <harikrishna.kamepalli@gmail.com>,"Sat, 20 Dec 2014 01:13:03 +0530",Spark Dev,dev@spark.apache.org,"i am interested to contribute to spark
"
Sandy Ryza <sandy.ryza@cloudera.com>,"Fri, 19 Dec 2014 14:54:58 -0500",Re: Spark Dev,Harikrishna Kamepalli <harikrishna.kamepalli@gmail.com>,"Hi Harikrishna,

A good place to start is taking a look at the wiki page on contributing:
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark

-Sandy

"
Sean McNamara <Sean.McNamara@Webtrends.com>,"Fri, 19 Dec 2014 20:27:44 +0000",Re: Which committers care about Kafka?,Hari Shreedharan <hshreedharan@cloudera.com>,"Please feel free to correct me if I’m wrong, but I think the exactly once spark streaming semantics can easily be solved using updateStateByKey. Make the key going into updateStateByKey be a hash of the event, or pluck off some uuid from the message.  The updateFunc would only emit the message if the key did not exist, and the user has complete control over the window of time / state lifecycle for detecting duplicates.  It also makes it really easy to detect and take action (alert?) when you DO see a duplicate, or make memory tradeoffs within an error bound using a sketch algorithm.  The kafka simple consumer is insanely complex, if possible I think it would be better (and vastly more flexible) to get reliability using the primitives that spark so elegantly provides.

Cheers,

Sean


> On Dec 19, 2014, at 12:06 PM, Hari Shreedharan <hshreedharan@cloudera.com> wrote:
> 
> Hi Dibyendu,
> 
> Thanks for the details on the implementation. But I still do not believe
> that it is no duplicates - what they achieve is that the same batch is
> processed exactly the same way every time (but see it may be processed more
> than once) - so it depends on the operation being idempotent. I believe
> Trident uses ZK to keep track of the transactions - a batch can be
> processed multiple times in failure scenarios (for example, the transaction
> is processed but before ZK is updated the machine fails, causing a ""new""
> node to process it again).
> 
> I don't think it is impossible to do this in Spark Streaming as well and
> I'd be really interested in working on it at some point in the near future.
> 
> On Fri, Dec 19, 2014 at 1:44 AM, Dibyendu Bhattacharya <
> dibyendu.bhattachary@gmail.com> wrote:
> 
>> Hi,
>> 
>> Thanks to Jerry for mentioning the Kafka Spout for Trident. The Storm
>> Trident has done the exact-once guarantee by processing the tuple in a
>> batch  and assigning same transaction-id for a given batch . The replay for
>> a given batch with a transaction-id will have exact same set of tuples and
>> replay of batches happen in exact same order before the failure.
>> 
>> Having this paradigm, if downstream system process data for a given batch
>> for having a given transaction-id , and if during failure if same batch is
>> again emitted , you can check if same transaction-id is already processed
>> or not and hence can guarantee exact once semantics.
>> 
>> And this can only be achieved in Spark if we use Low Level Kafka consumer
>> API to process the offsets. This low level Kafka Consumer (
>> https://github.com/dibbhatt/kafka-spark-consumer) has implemented the
>> Spark Kafka consumer which uses Kafka Low Level APIs . All of the Kafka
>> related logic has been taken from Storm-Kafka spout and which manages all
>> Kafka re-balance and fault tolerant aspects and Kafka metadata managements.
>> 
>> Presently this Consumer maintains that during Receiver failure, it will
>> re-emit the exact same Block with same set of messages . Every message have
>> the details of its partition, offset and topic related details which can
>> tackle the SPARK-3146.
>> 
>> As this Low Level consumer has complete control over the Kafka Offsets ,
>> we can implement Trident like feature on top of it like having implement a
>> transaction-id for a given block , and re-emit the same block with same set
>> of message during Driver failure.
>> 
>> Regards,
>> Dibyendu
>> 
>> 
>> On Fri, Dec 19, 2014 at 7:33 AM, Shao, Saisai <saisai.shao@intel.com>
>> wrote:
>>> 
>>> Hi all,
>>> 
>>> I agree with Hari that Strong exact-once semantics is very hard to
>>> guarantee, especially in the failure situation. From my understanding even
>>> current implementation of ReliableKafkaReceiver cannot fully guarantee the
>>> exact once semantics once failed, first is the ordering of data replaying
>>> from last checkpoint, this is hard to guarantee when multiple partitions
>>> are injected in; second is the design complexity of achieving this, you can
>>> refer to the Kafka Spout in Trident, we have to dig into the very details
>>> of Kafka metadata management system to achieve this, not to say rebalance
>>> and fault-tolerance.
>>> 
>>> Thanks
>>> Jerry
>>> 
>>> -----Original Message-----
>>> From: Luis Ángel Vicente Sánchez [mailto:langel.groups@gmail.com]
>>> Sent: Friday, December 19, 2014 5:57 AM
>>> To: Cody Koeninger
>>> Cc: Hari Shreedharan; Patrick Wendell; dev@spark.apache.org
>>> Subject: Re: Which committers care about Kafka?
>>> 
>>> But idempotency is not that easy t achieve sometimes. A strong only once
>>> semantic through a proper API would  be superuseful; but I'm not implying
>>> this is easy to achieve.
>>> On 18 Dec 2014 21:52, ""Cody Koeninger"" <cody@koeninger.org> wrote:
>>> 
>>>> If the downstream store for the output data is idempotent or
>>>> transactional, and that downstream store also is the system of record
>>>> for kafka offsets, then you have exactly-once semantics.  Commit
>>>> offsets with / after the data is stored.  On any failure, restart from
>>> the last committed offsets.
>>>> 
>>>> Yes, this approach is biased towards the etl-like use cases rather
>>>> than near-realtime-analytics use cases.
>>>> 
>>>> On Thu, Dec 18, 2014 at 3:27 PM, Hari Shreedharan <
>>>> hshreedharan@cloudera.com
>>>>> wrote:
>>>>> 
>>>>> I get what you are saying. But getting exactly once right is an
>>>>> extremely hard problem - especially in presence of failure. The
>>>>> issue is failures
>>>> can
>>>>> happen in a bunch of places. For example, before the notification of
>>>>> downstream store being successful reaches the receiver that updates
>>>>> the offsets, the node fails. The store was successful, but
>>>>> duplicates came in either way. This is something worth discussing by
>>>>> itself - but without uuids etc this might not really be solved even
>>> when you think it is.
>>>>> 
>>>>> Anyway, I will look at the links. Even I am interested in all of the
>>>>> features you mentioned - no HDFS WAL for Kafka and once-only
>>>>> delivery,
>>>> but
>>>>> I doubt the latter is really possible to guarantee - though I really
>>>> would
>>>>> love to have that!
>>>>> 
>>>>> Thanks,
>>>>> Hari
>>>>> 
>>>>> 
>>>>> On Thu, Dec 18, 2014 at 12:26 PM, Cody Koeninger
>>>>> <cody@koeninger.org>
>>>>> wrote:
>>>>> 
>>>>>> Thanks for the replies.
>>>>>> 
>>>>>> Regarding skipping WAL, it's not just about optimization.  If you
>>>>>> actually want exactly-once semantics, you need control of kafka
>>>>>> offsets
>>>> as
>>>>>> well, including the ability to not use zookeeper as the system of
>>>>>> record for offsets.  Kafka already is a reliable system that has
>>>>>> strong
>>>> ordering
>>>>>> guarantees (within a partition) and does not mandate the use of
>>>> zookeeper
>>>>>> to store offsets.  I think there should be a spark api that acts as
>>>>>> a
>>>> very
>>>>>> simple intermediary between Kafka and the user's choice of
>>>>>> downstream
>>>> store.
>>>>>> 
>>>>>> Take a look at the links I posted - if there's already been 2
>>>> independent
>>>>>> implementations of the idea, chances are it's something people need.
>>>>>> 
>>>>>> On Thu, Dec 18, 2014 at 1:44 PM, Hari Shreedharan <
>>>>>> hshreedharan@cloudera.com> wrote:
>>>>>>> 
>>>>>>> Hi Cody,
>>>>>>> 
>>>>>>> I am an absolute +1 on SPARK-3146. I think we can implement
>>>>>>> something pretty simple and lightweight for that one.
>>>>>>> 
>>>>>>> For the Kafka DStream skipping the WAL implementation - this is
>>>>>>> something I discussed with TD a few weeks ago. Though it is a good
>>>> idea to
>>>>>>> implement this to avoid unnecessary HDFS writes, it is an
>>>> optimization. For
>>>>>>> that reason, we must be careful in implementation. There are a
>>>>>>> couple
>>>> of
>>>>>>> issues that we need to ensure works properly - specifically
>>> ordering.
>>>> To
>>>>>>> ensure we pull messages from different topics and partitions in
>>>>>>> the
>>>> same
>>>>>>> order after failure, we’d still have to persist the metadata to
>>>>>>> HDFS
>>>> (or
>>>>>>> some other system) - this metadata must contain the order of
>>>>>>> messages consumed, so we know how to re-read the messages. I am
>>>>>>> planning to
>>>> explore
>>>>>>> this once I have some time (probably in Jan). In addition, we must
>>>>>>> also ensure bucketing functions work fine as well. I will file a
>>>>>>> placeholder jira for this one.
>>>>>>> 
>>>>>>> I also wrote an API to write data back to Kafka a while back -
>>>>>>> https://github.com/apache/spark/pull/2994 . I am hoping that this
>>>>>>> will get pulled in soon, as this is something I know people want.
>>>>>>> I am open
>>>> to
>>>>>>> feedback on that - anything that I can do to make it better.
>>>>>>> 
>>>>>>> Thanks,
>>>>>>> Hari
>>>>>>> 
>>>>>>> 
>>>>>>> On Thu, Dec 18, 2014 at 11:14 AM, Patrick Wendell
>>>>>>> <pwendell@gmail.com>
>>>>>>> wrote:
>>>>>>> 
>>>>>>>> Hey Cody,
>>>>>>>> 
>>>>>>>> Thanks for reaching out with this. The lead on streaming is TD -
>>>>>>>> he is traveling this week though so I can respond a bit. To the
>>>>>>>> high level point of whether Kafka is important - it definitely
>>>>>>>> is. Something like 80% of Spark Streaming deployments
>>>>>>>> (anecdotally) ingest data from Kafka. Also, good support for
>>>>>>>> Kafka is something we generally want in Spark and not a library.
>>>>>>>> In some cases IIRC there were user libraries that used unstable
>>>>>>>> Kafka API's and we were somewhat waiting on Kafka to stabilize
>>>>>>>> them to merge things upstream. Otherwise users wouldn't be able
>>>>>>>> to use newer Kakfa versions. This is a high level impression only
>>>>>>>> though, I haven't talked to TD about this recently so it's worth
>>> revisiting given the developments in Kafka.
>>>>>>>> 
>>>>>>>> Please do bring things up like this on the dev list if there are
>>>>>>>> blockers for your usage - thanks for pinging it.
>>>>>>>> 
>>>>>>>> - Patrick
>>>>>>>> 
>>>>>>>> On Thu, Dec 18, 2014 at 7:07 AM, Cody Koeninger
>>>>>>>> <cody@koeninger.org>
>>>>>>>> wrote:
>>>>>>>>> Now that 1.2 is finalized... who are the go-to people to get
>>>>>>>>> some long-standing Kafka related issues resolved?
>>>>>>>>> 
>>>>>>>>> The existing api is not sufficiently safe nor flexible for our
>>>>>>>> production
>>>>>>>>> use. I don't think we're alone in this viewpoint, because I've
>>>>>>>>> seen several different patches and libraries to fix the same
>>>>>>>>> things we've
>>>>>>>> been
>>>>>>>>> running into.
>>>>>>>>> 
>>>>>>>>> Regarding flexibility
>>>>>>>>> 
>>>>>>>>> https://issues.apache.org/jira/browse/SPARK-3146
>>>>>>>>> 
>>>>>>>>> has been outstanding since August, and IMHO an equivalent of
>>>>>>>>> this is absolutely necessary. We wrote a similar patch
>>>>>>>>> ourselves, then found
>>>>>>>> that
>>>>>>>>> PR and have been running it in production. We wouldn't be able
>>>>>>>>> to
>>>> get
>>>>>>>> our
>>>>>>>>> jobs done without it. It also allows users to solve a whole
>>>>>>>>> class of problems for themselves (e.g. SPARK-2388, arbitrary
>>>>>>>>> delay of
>>>>>>>> messages, etc).
>>>>>>>>> 
>>>>>>>>> Regarding safety, I understand the motivation behind
>>>>>>>>> WriteAheadLog
>>>> as
>>>>>>>> a
>>>>>>>>> general solution for streaming unreliable sources, but Kafka
>>>>>>>>> already
>>>>>>>> is a
>>>>>>>>> reliable source. I think there's a need for an api that treats
>>>>>>>>> it as such. Even aside from the performance issues of
>>>>>>>>> duplicating the write-ahead log in kafka into another
>>>>>>>>> write-ahead log in hdfs, I
>>>> need
>>>>>>>>> exactly-once semantics in the face of failure (I've had
>>>>>>>>> failures
>>>> that
>>>>>>>>> prevented reloading a spark streaming checkpoint, for instance).
>>>>>>>>> 
>>>>>>>>> I've got an implementation i've been using
>>>>>>>>> 
>>>>>>>>> https://github.com/koeninger/spark-1/tree/kafkaRdd/external/kaf
>>>>>>>>> ka /src/main/scala/org/apache/spark/rdd/kafka
>>>>>>>>> 
>>>>>>>>> Tresata has something similar at
>>>>>>>> https://github.com/tresata/spark-kafka,
>>>>>>>>> and I know there were earlier attempts based on Storm code.
>>>>>>>>> 
>>>>>>>>> Trying to distribute these kinds of fixes as libraries rather
>>>>>>>>> than
>>>>>>>> patches
>>>>>>>>> to Spark is problematic, because large portions of the
>>>> implementation
>>>>>>>> are
>>>>>>>>> private[spark].
>>>>>>>>> 
>>>>>>>>> I'd like to help, but i need to know whose attention to get.
>>>>>>>> 
>>>>>>>> -----------------------------------------------------------------
>>>>>>>> ---- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For
>>>>>>>> additional commands, e-mail: dev-help@spark.apache.org
>>>>>>>> 
>>>>>>>> 
>>>>>>> 
>>>>> 
>>>> 
>>> 
>> 

"
Andy Konwinski <andykonwinski@gmail.com>,"Fri, 19 Dec 2014 12:51:07 -0800","Re: Nabble mailing list mirror errors: ""This post has NOT been
 accepted by the mailing list yet""",Josh Rosen <rosenville@gmail.com>,"Yesterday, I changed the domain name in the mailing list archive settings
to remove "".incubator"" so maybe it'll work now.

However, I also sent two emails about this through the nabble interface (in
this same thread) yesterday and they don't appear to have made it through
so not sure if it actually worked after all.

Andy

"
Ted Yu <yuzhihong@gmail.com>,"Fri, 19 Dec 2014 13:36:56 -0800","Re: Nabble mailing list mirror errors: ""This post has NOT been
 accepted by the mailing list yet""",Andy Konwinski <andykonwinski@gmail.com>,"Andy:
I saw two emails from you from yesterday.

See this thread: http://search-hadoop.com/m/JW1q5opRsY1

Cheers


"
Cody Koeninger <cody@koeninger.org>,"Fri, 19 Dec 2014 15:48:29 -0600",Re: Which committers care about Kafka?,Sean McNamara <Sean.McNamara@webtrends.com>,"The problems you guys are discussing come from trying to store state in
spark, so don't do that.  Spark isn't a distributed database.

Just map kafka partitions directly to rdds, llet user code specify the
range of offsets explicitly, and let them be in charge of committing
offsets.

Using the simple consumer isn't that bad, I'm already using this in
production with the code I linked to, and tresata apparently has been as
well.  Again, for everyone saying this is impossible, have you read either
of those implementations and looked at the approach?




tly once
ke
f
of
e
e
e
""
d
y
h
a
l
an
 ,
e
e
l.com]
d
om
f
y
e
y
s
d.
d
ata to
t
y
.
-
r
"
"""Hari Shreedharan"" <hshreedharan@cloudera.com>","Fri, 19 Dec 2014 13:57:04 -0800 (PST)",Re: Which committers care about Kafka?,"""Cody Koeninger"" <cody@koeninger.org>","Can you explain your basic algorithm for the once-only-delivery? It is quite a bit of very Kafka-specific code, that would take more time to read than I can currently afford? If you can explain your algorithm a bit, it might help.




Thanks, Hari


either
com>
exactly once
Make
off
if
 of
really
or
The
be
primitives
believe
is
processed
believe
""new""
and
Storm
a
replay
tuples
batch
the
Kafka
manages
will
message
can
Offsets ,
same
com>
understanding
guarantee
groups@gmail.com]
record
from
of
updates
by
even
the
really
you
of
as
need.
good
metadata to
must
a
this
.
-
the
.
unstable
able
only
worth
are
our
I've
able
treats
instance).
f
--
For"
Cody Koeninger <cody@koeninger.org>,"Fri, 19 Dec 2014 16:32:29 -0600",Re: Which committers care about Kafka?,Hari Shreedharan <hshreedharan@cloudera.com>,"That KafkaRDD code is dead simple.

Given a user specified map

(topic1, partition0) -> (startingOffset, endingOffset)
(topic1, partition1) -> (startingOffset, endingOffset)
...
turn each one of those entries into a partition of an rdd, using the simple
consumer.
That's it.  No recovery logic, no state, nothing - for any failures, bail
on the rdd and let it retry.
Spark stays out of the business of being a distributed database.

The client code does any transformation it wants, then stores the data and
offsets.  There are two ways of doing this, either based on idempotence or
a transactional data store.

For idempotent stores:

1.manipulate data
2.save data to store
3.save ending offsets to the same store

If you fail between 2 and 3, the offsets haven't been stored, you start
again at the same beginning offsets, do the same calculations in the same
order, overwrite the same data, all is good.


For transactional stores:

1. manipulate data
2. begin transaction
3. save data to the store
4. save offsets
5. commit transaction

If you fail before 5, the transaction rolls back.  To make this less
heavyweight, you can write the data outside the transaction and then update
a pointer to the current data inside the transaction.


Again, spark has nothing much to do with guaranteeing exactly once.  In
fact, the current streaming api actively impedes my ability to do the
above.  I'm just suggesting providing an api that doesn't get in the way of
exactly-once.





m

d
er
actly
Key.
ck
sage
ndow
ate,
.
ould
s
d
ve
m
 a
e
s
ail.com]
es
en
u
f
adata to
a
t.
 -
e
y.
e
e
th
re
r
e
e
s
"
Koert Kuipers <koert@tresata.com>,"Fri, 19 Dec 2014 17:36:34 -0500",Re: Which committers care about Kafka?,Cody Koeninger <cody@koeninger.org>,"yup, we at tresata do the idempotent store the same way. very simple
approach.

le
d
r
te
of
t
n
as
exactly
l
n
ch
o
s,
gmail.com]
ly
:
t
er
on
ng
f
a
as
f
ts
is
a
in
etadata to
am
e
-
ly
ze
t
e
f
y
a
er
.
g
"
jay vyas <jayunit100.apache@gmail.com>,"Fri, 19 Dec 2014 19:21:06 -0500",EndpointWriter : Dropping message failure ReliableDeliverySupervisor errors...,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi spark.   Im trying to understand the akka debug messages when networking
doesnt work properly.  any hints would be great on this.

SIMPLE TESTS I RAN

- i tried a ping, works.
- i tried a telnet to the 7077 port of master, from slave, also works.

LOGS


ReliableDeliverySupervisor: Association with remote system
[akka.tcp://sparkWorker@s2.docker:45477] has failed, address is now gated
for [500] ms  Reason is: [Disassociated].

2) I also see a periodic, repeated ERROR message :

 ERROR EndpointWriter: dropping message [class
akka.actor.ActorSelectionMessage] for non-local recipient [Actor[akka.tcp://
sparkMaster@172.17.0.12:7077


Any idea what these folks mean?   From what i can tel, i can telnet from
s2.docker to my master server.

Any thoughts for more debugging of this would be appreciated! im out of
ideas for the time being ....

-- 
jay vyas
"
sreenivas putta <putta.sreenivas@gmail.com>,"Sat, 20 Dec 2014 18:00:20 +0530",Contribution in java,dev@spark.apache.org,"Hi,

I want to contribute for spark in java. Does it support java? please let me
know.

Thanks,
Sreenivas
"
Koert Kuipers <koert@tresata.com>,"Sat, 20 Dec 2014 11:31:53 -0500",Re: Contribution in java,sreenivas putta <putta.sreenivas@gmail.com>,"yes it does. although the core of spark is written in scala it also
maintains java and python apis, and there is plenty of work for those to
contribute to.

"
vaquar khan <vaquar.khan@gmail.com>,"Sun, 21 Dec 2014 00:04:08 +0530",Re: Contribution in java,sreenivas putta <putta.sreenivas@gmail.com>,"Hi Sreenivas,

Please read Spark doc first, everything mention in doc , without reading
doc how can you contribute ?

regards,
vaquar khan





-- 
Regards,
Vaquar Khan
+91 830-851-1500
"
jay vyas <jayunit100.apache@gmail.com>,"Sat, 20 Dec 2014 19:26:10 -0500","Re: EndpointWriter : Dropping message failure ReliableDeliverySupervisor
 errors...","""dev@spark.apache.org"" <dev@spark.apache.org>","Hi folks.

In the end, I found that the problem was that I was using IP Addresses
instead of hostnames.

I guess, maybe,  reverse dns is a requirement for spark slave -> master
communications...  ?







-- 
jay vyas
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sun, 21 Dec 2014 08:41:50 +0000",Re: Handling stale PRs,Patrick Wendell <pwendell@gmail.com>,"Shout-out to Michael and other Spark SQL contributors for really trimming
down the number of open/stale Spark SQL PRs
<https://spark-prs.appspot.com/#sql>.

As of right now, the least recently updated open Spark SQL PR goes back
only 11 days.

Nice work!

Nick



"
slcclimber <anant.asty@gmail.com>,"Sun, 21 Dec 2014 09:02:27 -0700 (MST)",Re: Announcing Spark 1.2!,dev@spark.apache.org,"Congratulations. This is quite exciting.



--

---------------------------------------------------------------------


"
Venkata ramana gollamudi <ramana.gollamudi@huawei.com>,"Mon, 22 Dec 2014 02:43:25 +0000",Data source interface for making multiple tables available for query,dev <dev@spark.apache.org>,"Hi,

Data source ddl.scala, CREATE TEMPORARY TABLE makes one table at time available to temp tables, how about the case if multiple/all tables from some data source needs to be available for query, just like hive tables. I think we also need that interface to connect such data sources. Please comment.

Regards,
Ramana
"
"""wyphao.2007"" <wyphao.2007@163.com>","Mon, 22 Dec 2014 12:10:18 +0800 (CST)",Use mvn to build Spark 1.2.0  failed,dev@spark.apache.org,"Hi all, Today download Spark source from http://spark.apache.org/downloads.html page, and I use


 ./make-distribution.sh --tgz -Phadoop-2.2 -Pyarn -DskipTests -Dhadoop.version=2.2.0 -Phive


to build the release, but I encountered an exception as follow:


[INFO] --- build-helper-maven-plugin:1.8:add-source (add-scala-sources) @ spark-parent ---
[INFO] Source directory: /home/q/spark/spark-1.2.0/src/main/scala added.
[INFO] 
[INFO] --- maven-remote-reINFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Spark Project Parent POM .......................... FAILURE [1.015s]
[INFO] Spark Project Networking .......................... SKIPPED
[INFO] Spark Project Shuffle Streaming Service ........... SKIPPED
[INFO] Spark Project Core ................................ SKIPPED
[INFO] Spark Project Bagel ............................... SKIPPED
[INFO] Spark Project GraphX .............................. SKIPPED
[INFO] Spark Project Streaming ........................... SKIPPED
[INFO] Spark Project Catalyst ............................ SKIPPED
[INFO] Spark Project SQL ................................. SKIPPED
[INFO] Spark Project ML Library .......................... SKIPPED
[INFO] Spark Project Tools ............................... SKIPPED
[INFO] Spark Project Hive ................................ SKIPPED
[INFO] Spark Project REPL ................................ SKIPPED
[INFO] Spark Project YARN Parent POM ..................... SKIPPED
[INFO] Spark Project YARN Stable API ..................... SKIPPED
[INFO] Spark Project Assembly ............................ SKIPPED
[INFO] Spark Project External Twitter .................... SKIPPED
[INFO] Spark Project External Flume Sink ................. SKIPPED
[INFO] Spark Project External Flume ...................... SKIPPED
[INFO] Spark Project External MQTT ....................... SKIPPED
[INFO] Spark Project External ZeroMQ ..................... SKIPPED
[INFO] Spark Project External Kafka ...................... SKIPPED
[INFO] Spark Project Examples ............................ SKIPPED
[INFO] Spark Project YARN Shuffle Service ................ SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 1.644s
[INFO] Finished at: Mon Dec 22 10:56:35 CST 2014
[INFO] Final Memory: 21M/481M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-remote-resources-plugin:1.5:process (default) on project spark-parent: Error finding remote resources manifests: /home/q/spark/spark-1.2.0/target/maven-shared-archive-resources/META-INF/NOTICE (No such file or directory) -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException


but the NOTICE file is in the download spark release:


[wyp@spark  /home/q/spark/spark-1.2.0]$ ll
total 248
drwxrwxr-x 3 1000 1000  4096 Dec 10 18:02 assembly
drwxrwxr-x 3 1000 1000  4096 Dec 10 18:02 bagel
drwxrwxr-x 2 1000 1000  4096 Dec 10 18:02 bin
drwxrwxr-x 2 1000 1000  4096 Dec 10 18:02 conf
-rw-rw-r-- 1 1000 1000   663 Dec 10 18:02 CONTRIBUTING.md
drwxrwxr-x 3 1000 1000  4096 Dec 10 18:02 core
drwxrwxr-x 3 1000 1000  4096 Dec 10 18:02 data
drwxrwxr-x 4 1000 1000  4096 Dec 10 18:02 dev
drwxrwxr-x 3 1000 1000  4096 Dec 10 18:02 docker
drwxrwxr-x 7 1000 1000  4096 Dec 10 18:02 docs
drwxrwxr-x 4 1000 1000  4096 Dec 10 18:02 ec2
drwxrwxr-x 4 1000 1000  4096 Dec 10 18:02 examples
drwxrwxr-x 8 1000 1000  4096 Dec 10 18:02 external
drwxrwxr-x 5 1000 1000  4096 Dec 10 18:02 extras
drwxrwxr-x 4 1000 1000  4096 Dec 10 18:02 graphx
-rw-rw-r-- 1 1000 1000 45242 Dec 10 18:02 LICENSE
-rwxrwxr-x 1 1000 1000  7941 Dec 10 18:02 make-distribution.sh
drwxrwxr-x 3 1000 1000  4096 Dec 10 18:02 mllib
drwxrwxr-x 5 1000 1000  4096 Dec 10 18:02 network
-rw-rw-r-- 1 1000 1000 22559 Dec 10 18:02 NOTICE
-rw-rw-r-- 1 1000 1000 49002 Dec 10 18:02 pom.xml
drwxrwxr-x 4 1000 1000  4096 Dec 10 18:02 project
drwxrwxr-x 6 1000 1000  4096 Dec 10 18:02 python
-rw-rw-r-- 1 1000 1000  3645 Dec 10 18:02 README.md
drwxrwxr-x 5 1000 1000  4096 Dec 10 18:02 repl
drwxrwxr-x 2 1000 1000  4096 Dec 10 18:02 sbin
drwxrwxr-x 2 1000 1000  4096 Dec 10 18:02 sbt
-rw-rw-r-- 1 1000 1000  7804 Dec 10 18:02 scalastyle-config.xml
drwxrwxr-x 6 1000 1000  4096 Dec 10 18:02 sql
drwxrwxr-x 3 1000 1000  4096 Dec 10 18:02 streaming
drwxrwxr-x 3 1000 1000  4096 Dec 10 18:02 tools
-rw-rw-r-- 1 1000 1000   838 Dec 10 18:02 tox.ini
drwxrwxr-x 5 1000 1000  4096 Dec 10 18:02 yarn



"
Sean Owen <sowen@cloudera.com>,"Mon, 22 Dec 2014 10:24:24 +0000",Re: Use mvn to build Spark 1.2.0 failed,"""wyphao.2007"" <wyphao.2007@163.com>","I just tried the exact same command and do not see any error. Maybe
you can make sure you're starting from a clean extraction of the
distro, and check your environment. I'm on OSX, Maven 3.2, Java 8 but
I don't know that any of those would be relevant.

s.html page, and I use
rsion=2.2.0 -Phive
 spark-parent ---
rent ---
------
s]
------
------
------
urces-plugin:1.5:process (default) on project spark-parent: Error finding remote resources manifests: /home/q/spark/spark-1.2.0/target/maven-shared-archive-resources/META-INF/NOTICE (No such file or directory) -> [Help 1]
-e switch.
ase read the following articles:
cutionException

---------------------------------------------------------------------


"
Priya Ch <learnings.chitturi@gmail.com>,"Mon, 22 Dec 2014 19:15:28 +0530",Spark exception when sending message to akka actor,"user@spark.apache.org, dev@spark.apache.org","Hi All,

I have akka remote actors running on 2 nodes. I submitted spark application
from node1. In the spark code, in one of the rdd, i am sending message to
actor running on node1. My Spark code is as follows:




class ActorClient extends Actor with Serializable
{
  import context._

  val currentActor: ActorSelection =
context.system.actorSelection(""akka.tcp://
ActorSystem@192.168.145.183:2551/user/MasterActor"")
  implicit val timeout = Timeout(10 seconds)


  def receive =
  {
  case msg:String => { if(msg.contains(""Spark""))
                       { currentActor ! msg
                         sender ! ""Local""
                       }
                       else
                       {
                        println(""Received..""+msg)
                        val future=currentActor ? msg
                        val result = Await.result(future,
timeout.duration).asInstanceOf[String]
                        if(result.contains(""ACK""))
                              sender ! ""OK""
                       }
                     }
  case PoisonPill => context.stop(self)
  }
}

object SparkExec extends Serializable
{

  implicit val timeout = Timeout(10 seconds)
   val actorSystem=ActorSystem(""ClientActorSystem"")
   val
actor=actorSystem.actorOf(Props(classOf[ActorClient]),name=""ClientActor"")

 def main(args:Array[String]) =
  {

     val conf = new SparkConf().setAppName(""DeepLearningSpark"")

     val sc=new SparkContext(conf)

    val
textrdd=sc.textFile(""hdfs://IMPETUS-DSRV02:9000/deeplearning/sample24k.csv"")
    val rdd1=textrddmap{ line => println(""In Map..."")

                                   val future = actor ? ""Hello..Spark""
                                   val result =
Await.result(future,timeout.duration).asInstanceOf[String]
                                   if(result.contains(""Local"")){
                                     println(""Recieved in map....""+result)
                                      //actorSystem.shutdown
                                      }
                                      (10)
                                     }


     val rdd2=rdd1.map{ x =>
                             val future=actor ? ""Done""
                             val result = Await.result(future,
timeout.duration).asInstanceOf[String]
                              if(result.contains(""OK""))
                              {
                               actorSystem.stop(remoteActor)
                               actorSystem.shutdown
                              }
                             (2) }
     rdd2.saveAsTextFile(""/home/padma/SparkAkkaOut"")
}

}

In my ActorClientActor, through actorSelection, identifying the remote
receiving ack from remote actor, i am killing the actor ActorClient and
shutting down the ActorSystem.

The above code is throwing the following exception:




14/12/22 19:04:36 WARN scheduler.TaskSetManager: Lost task 1.0 in stage 0.0
(TID 1, IMPETUS-DSRV05.impetus.co.in):
java.lang.ExceptionInInitializerError:
        com.impetus.spark.SparkExec$$anonfun$2.apply(SparkExec.scala:166)
        com.impetus.spark.SparkExec$$anonfun$2.apply(SparkExec.scala:159)
        scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        scala.collection.Iterator$$anon$11.next(Iterator.scala:328)

org.apache.spark.rdd.PairRDDFunctions$$anonfun$13.apply(PairRDDFunctions.scala:984)

org.apache.spark.rdd.PairRDDFunctions$$anonfun$13.apply(PairRDDFunctions.scala:974)
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
        org.apache.spark.scheduler.Task.run(Task.scala:54)

org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)

java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)

java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        java.lang.Thread.run(Thread.java:722)
14/12/22 19:04:36 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 0.0
(TID 0, IMPETUS-DSRV05.impetus.co.in): java.lang.NoClassDefFoundError:
Could not initialize class com.impetus.spark.SparkExec$
        com.impetus.spark.SparkExec$$anonfun$2.apply(SparkExec.scala:166)
        com.impetus.spark.SparkExec$$anonfun$2.apply(SparkExec.scala:159)
        scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        scala.collection.Iterator$$anon$11.next(Iterator.scala:328)

org.apache.spark.rdd.PairRDDFunctions$$anonfun$13.apply(PairRDDFunctions.scala:984)

org.apache.spark.rdd.PairRDDFunctions$$anonfun$13.apply(PairRDDFunctions.scala:974)
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
        org.apache.spark.scheduler.Task.run(Task.scala:54)

org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)

java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)

java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        java.lang.Thread.run(Thread.java:722)


Please anyone could help me on this ? My concern is i want to send message
to an actor within a spark rdd and after sending the messages the
actorsystem need to be shutdown.

Thanks,
Padma Ch
"
Gerard Maas <gerard.maas@gmail.com>,"Mon, 22 Dec 2014 17:33:17 +0100",Tuning Spark Streaming jobs,"spark users <user@spark.apache.org>, dev@spark.apache.org, 
	Tathagata Das <tathagata.das1565@gmail.com>","Hi,

After facing issues with the performance of some of our Spark Streaming
 jobs, we invested quite some effort figuring out the factors that affect
the performance characteristics of a Streaming job. We  defined an
empirical model that helps us reason about Streaming jobs and applied it to
tune the jobs in order to maximize throughput.

We have summarized our findings in a blog post with the intention of
collecting feedback and hoping that it is useful to other Spark Streaming
users facing similar issues.

 http://www.virdata.com/tuning-spark/

Your feedback is welcome.

With kind regards,

Gerard.
Data Processing Team Lead
Virdata.com
@maasg
"
Timothy Chen <tnachen@gmail.com>,"Mon, 22 Dec 2014 09:01:02 -0800",Re: Tuning Spark Streaming jobs,Gerard Maas <gerard.maas@gmail.com>,"Hi Gerard,

Really nice guide!

I'm particularly interested in the Mesos scheduling side to more evenly distribute cores across cluster.

I wonder if you are using coarse grain mode or fine grain mode? 

I'm making changes to the spark mesos scheduler and I think we can propose a best way to achieve what you mentioned.

Tim

Sent from my iPhone

o

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Mon, 22 Dec 2014 09:46:30 -0800",Re: Use mvn to build Spark 1.2.0 failed,Sean Owen <sowen@cloudera.com>,"I also couldn't reproduce this issued.

ds.html page, and I use
ersion=2.2.0 -Phive
@ spark-parent ---
arent ---
-------
5s]
-------
-------
-------
ources-plugin:1.5:process (default) on project spark-parent: Error finding remote resources manifests: /home/q/spark/spark-1.2.0/target/maven-shared-archive-resources/META-INF/NOTICE (No such file or directory) -> [Help 1]
 -e switch.
ease read the following articles:
ecutionException

---------------------------------------------------------------------


"
Cody Koeninger <cody@koeninger.org>,"Mon, 22 Dec 2014 12:19:42 -0600",cleaning up cache files left by SPARK-2713,"""dev@spark.apache.org"" <dev@spark.apache.org>","Is there a reason not to go ahead and move the _cache and _lock files
created by Utils.fetchFiles into the work directory, so they can be cleaned
up more easily?  I saw comments to that effect in the discussion of the PR
for 2713, but it doesn't look like it got done.

And no, I didn't just have a machine fill up the /tmp directory, why do you
ask?  :)
"
Marcelo Vanzin <vanzin@cloudera.com>,"Mon, 22 Dec 2014 10:39:22 -0800",Re: cleaning up cache files left by SPARK-2713,Cody Koeninger <cody@koeninger.org>,"https://github.com/apache/spark/pull/3705




-- 
Marcelo

---------------------------------------------------------------------


"
David McWhorter <mcwhorter@ccri.com>,"Mon, 22 Dec 2014 14:08:02 -0500",Re: spark-yarn_2.10 1.2.0 artifacts,Sean Owen <sowen@cloudera.com>,"Thank you, Sean, using spark-network-yarn seems to do the trick.


-- 

David McWhorter
Software Engineer
Commonwealth Computer Research, Inc.
1422 Sachem Place, Unit #1
Charlottesville, VA 22901
mcwhorter@ccri.com | 434.299.0090x204


---------------------------------------------------------------------


"
Gerard Maas <gerard.maas@gmail.com>,"Mon, 22 Dec 2014 20:16:25 +0100",Re: Tuning Spark Streaming jobs,Timothy Chen <tnachen@gmail.com>,"Hi Tim,

That would be awesome. We have seen some really disparate Mesos allocations
for our Spark Streaming jobs. (like (7,4,1) over 3 executors for 4 kafka
consumer instead of the ideal (3,3,3,3))
For network dependent consumers, achieving an even deployment would
 provide a reliable and reproducible streaming job execution from the
performance point of view.
We're deploying in coarse grain mode. Not sure Spark Streaming would work
well in fine-grained given the added latency to acquire a worker.

You mention that you're changing the Mesos scheduler. Is there a Jira where
this job is taking place?

-kr, Gerard.



"
Michael Armbrust <michael@databricks.com>,"Mon, 22 Dec 2014 11:33:38 -0800",Re: Data source interface for making multiple tables available for query,Venkata ramana gollamudi <ramana.gollamudi@huawei.com>,"I agree and this is something that we have discussed in the past.
Essentially I think instead of creating a RelationProvider that returns a
single table, we'll have something like an external catalog that can return
multiple base relations.


"
Xiangrui Meng <mengxr@gmail.com>,"Mon, 22 Dec 2014 12:37:43 -0800",Announcing Spark Packages,"""user@spark.apache.org"" <user@spark.apache.org>, dev <dev@spark.apache.org>","Dear Spark users and developers,

I’m happy to announce Spark Packages (http://spark-packages.org), a
community package index to track the growing number of open source
packages and libraries that work with Apache Spark. Spark Packages
makes it easy for users to find, discuss, rate, and install packages
for any version of Spark, and makes it easy for developers to
contribute packages.

Spark Packages will feature integrations with various data sources,
management tools, higher level domain-specific libraries, machine
learning algorithms, code samples, and other Spark content. Thanks to
the package authors, the initial listing of packages includes
scientific computing libraries, a job execution server, a connector
for importing Avro data, tools for launching Spark on Google Compute
Engine, and many others.

I’d like to invite you to contribute and use Spark Packages and
provide feedback! As a disclaimer: Spark Packages is a community index
maintained by Databricks and (by design) will include packages outside
of the ASF Spark project. We are excited to help showcase and support
all of the great work going on in the broader Spark community!

Cheers,
Xiangrui

---------------------------------------------------------------------


"
Alessandro Baretta <alexbaretta@gmail.com>,"Mon, 22 Dec 2014 13:32:56 -0800",More general submitJob API,"""dev@spark.apache.org"" <dev@spark.apache.org>","Fellow Sparkers,

I'm rather puzzled at the submitJob API. I can't quite figure out how it is
supposed to be used. Is there any more documentation about it?

Also, is there any simpler way to multiplex jobs on the cluster, such as
starting multiple computations in as many threads in the driver and reaping
all the results when they are available?

Thanks,

Alex
"
Andrew Ash <andrew@andrewash.com>,"Mon, 22 Dec 2014 15:14:02 -0800",Re: Announcing Spark Packages,Xiangrui Meng <mengxr@gmail.com>,"Hi Xiangrui,

That link is currently returning a 503 Over Quota error message.  Would you
mind pinging back out when the page is back up?

Thanks!
Andrew


 a
"
Patrick Wendell <pwendell@gmail.com>,"Mon, 22 Dec 2014 15:38:49 -0800",Re: Announcing Spark Packages,peng <pc175@uowmail.edu.au>,"Xiangrui asked me to report that it's back and running :)


---------------------------------------------------------------------


"
Hitesh Shah <hitesh@apache.org>,"Mon, 22 Dec 2014 15:40:00 -0800",Re: Announcing Spark Packages,Xiangrui Meng <mengxr@gmail.com>,"Hello Xiangrui, 

If you have not already done so, you should look at http://www.apache.org/foundation/marks/#domains for the policy on use of ASF trademarked terms in domain names. 

thanks
 Hitesh




---------------------------------------------------------------------


"
Andrew Ash <andrew@andrewash.com>,"Mon, 22 Dec 2014 16:04:05 -0800",Re: More general submitJob API,Alessandro Baretta <alexbaretta@gmail.com>,"Hi Alex,

SparkContext.submitJob() is marked as experimental -- most client programs
shouldn't be using it.  What are you looking to do?

For multiplexing jobs, one thing you can do is have multiple threads in
your client JVM each submit jobs on your SparkContext job.  This is
described here in the docs:
http://spark.apache.org/docs/latest/job-scheduling.html#scheduling-within-an-application

Andrew


"
Alessandro Baretta <alexbaretta@gmail.com>,"Mon, 22 Dec 2014 16:15:45 -0800",Re: More general submitJob API,Andrew Ash <andrew@andrewash.com>,"Andrew,

Thanks, yes, this is what I wanted: basically just to start multiple jobs
concurrently in threads.

Alex

"
Patrick Wendell <pwendell@gmail.com>,"Mon, 22 Dec 2014 16:18:27 -0800",Re: More general submitJob API,Alessandro Baretta <alexbaretta@gmail.com>,"A SparkContext is thread safe, so you can just have different threads
that create their own RDD's and do actions, etc.

- Patrick


---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 23 Dec 2014 01:29:08 +0000",Re: Announcing Spark Packages,"Hitesh Shah <hitesh@apache.org>, Xiangrui Meng <mengxr@gmail.com>","Hitesh,


You may not use ASF trademarks such as “Apache” or “ApacheFoo” or “Foo” in
your own domain names if that use would be likely to confuse a relevant
consumer about the source of software or services provided through your
website, without written approval of the VP, Apache Brand Management or
designee.

The title on the packages website is “A community index of packages for
Apache Spark.” Furthermore, the footnote of the website reads “Spark
Packages is a community site hosting modules that are not part of Apache
Spark.”

I think there’s nothing on there that would “confuse a relevant consumer
about the source of software”. It’s pretty clear that the Spark Packages
name is well within the ASF’s guidelines.

Have I misunderstood the ASF’s policy?

Nick
​


/
), a
"
Patrick Wendell <pwendell@gmail.com>,"Mon, 22 Dec 2014 17:33:53 -0800",Re: Announcing Spark Packages,Nicholas Chammas <nicholas.chammas@gmail.com>,"Hey Nick,

I think Hitesh was just trying to be helpful and point out the policy
- not necessarily saying there was an issue. We've taken a close look
at this and I think we're in good shape her vis-a-vis this policy.

- Patrick


---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 23 Dec 2014 01:36:55 +0000",Re: Announcing Spark Packages,Patrick Wendell <pwendell@gmail.com>,"Okie doke! (I just assumed there was an issue since the policy was brought
up.)


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 23 Dec 2014 02:17:14 +0000",Re: [ANNOUNCE] Requiring JIRA for inclusion in release credits,"Patrick Wendell <pwendell@gmail.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Does this include contributions made against the spark-ec2
<https://github.com/mesos/spark-ec2> repo?


"
Patrick Wendell <pwendell@gmail.com>,"Mon, 22 Dec 2014 22:52:14 -0800",Re: [ANNOUNCE] Requiring JIRA for inclusion in release credits,Nicholas Chammas <nicholas.chammas@gmail.com>,"Hey Josh,

We don't explicitly track contributions to spark-ec2 in the Apache
Spark release notes. The main reason is that usually updates to
spark-ec2 include a corresponding update to spark so we get it there.
This may not always be the case though, so let me know if you think
there is something missing we should add.

- Patrick


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Mon, 22 Dec 2014 22:52:25 -0800",Re: [ANNOUNCE] Requiring JIRA for inclusion in release credits,Nicholas Chammas <nicholas.chammas@gmail.com>,"s/Josh/Nick/ - sorry!


---------------------------------------------------------------------


"
Timothy Chen <tnachen@gmail.com>,"Tue, 23 Dec 2014 11:02:30 -0800",Re: Tuning Spark Streaming jobs,Gerard Maas <gerard.maas@gmail.com>,"Hi Gerard,

SPARK-4286 is the ticket I am working on, which besides supporting shuffle
service it also supports the executor scaling callbacks (kill/request
total) for coarse grain mode.

I created SPARK-4940 to discuss more about the distribution problem, and
let's bring our discussions there.

Tim




Hi Tim,

That would be awesome. We have seen some really disparate Mesos allocations
for our Spark Streaming jobs. (like (7,4,1) over 3 executors for 4 kafka
consumer instead of the ideal (3,3,3,3))
For network dependent consumers, achieving an even deployment would
 provide a reliable and reproducible streaming job execution from the
performance point of view.
We're deploying in coarse grain mode. Not sure Spark Streaming would work
well in fine-grained given the added latency to acquire a worker.

You mention that you're changing the Mesos scheduler. Is there a Jira where
this job is taking place?

-kr, Gerard.



"
Jianshi Huang <jianshi.huang@gmail.com>,"Wed, 24 Dec 2014 15:12:17 +0800","Re: Hive Problem in Pig generated Parquet file schema in CREATE
 EXTERNAL TABLE (e.g. bag::col1)",Michael Armbrust <michael@databricks.com>,"FYI,

Latest hive 0.14/parquet will have column renaming support.

Jianshi




-- 
Jianshi Huang

LinkedIn: jianshi
Twitter: @jshuang
Github & Blog: http://huangjs.github.com/
"
tanejagagan <tanejagagan@yahoo.com>,"Wed, 24 Dec 2014 01:48:30 -0700 (MST)",Re: Support for Hive buckets,dev@spark.apache.org,"(for example, you might be 
able to avoid a shuffle when doing joins on tables that are already 
bucketed by exposing more metastore information to the planner). 

Can you provide more input on how to implement this functionality so that i
can speed up join between 2 hive tables, both with few billion rows 



--

---------------------------------------------------------------------


"
Josh Rosen <rosenville@gmail.com>,"Wed, 24 Dec 2014 13:13:55 -0800",Re: cleaning up cache files left by SPARK-2713,"Cody Koeninger <cody@koeninger.org>, Marcelo Vanzin
 <vanzin@cloudera.com>","I reviewed and merged that PR, in case you want to try out the fix.

- Josh


https://github.com/apache/spark/pull/3705 




-- 
Marcelo 

--------------------------------------------------------------------- 

"
Josh Rosen <rosenville@gmail.com>,"Wed, 24 Dec 2014 13:18:57 -0800","Re: Confirming race condition in DagScheduler
 (NoSuchElementException)","thlee <tiong@ooyala.com>, dev@spark.apache.org","I’m investigating this issue and left some comments on the proposed fix: https://github.com/apache/spark/pull/3345#issuecomment-68014353

To summarize, I agree with your description of the problem but think that the right fix may be a bit more involved than what’s proposed in that PR (that PR’s fix shouldn’t actually work, as far as I can tell).

- Josh


any comments?  



--  
.n3.nabble.com/Confirming-race-condition-in-DagScheduler-NoSuchElementException-tp9798p9855.html  
.com.  

---------------------------------------------------------------------  
For additional commands, e-mail: dev-help@spark.apache.org  

"
Naveen Madhire <vmadhire@umail.iu.edu>,"Wed, 24 Dec 2014 23:59:26 -0500",Starting with Spark,dev@spark.apache.org,"Hi All,

I am starting to use Spark. I am having trouble getting the latest code
from git.
I am using Intellij as suggested in the below link,

https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-StarterTasks


The below link isn't working as well,

http://spark.apache.org/building-spark.html


Does anyone know any useful links to get spark running on local laptop

Please help.


Thanks
"
Timothy Chen <tnachen@gmail.com>,"Wed, 24 Dec 2014 21:08:20 -0800",Re: Starting with Spark,Naveen Madhire <vmadhire@umail.iu.edu>,"What error are you getting?

Tim

Sent from my iPhone


ntributingtoSpark-StarterTasks

---------------------------------------------------------------------


"
Will Yang <era.yeung@gmail.com>,"Thu, 25 Dec 2014 13:34:40 +0800",Problems with large dataset using collect() and broadcast(),dev@spark.apache.org,"Hi all,
In my occasion, I have a huge HashMap[(Int, Long), (Double, Double,
Double)], say several GB to tens of GB, after each iteration, I need to
collect() this HashMap and perform some calculation, and then broadcast()
it to every node. Now I have 20GB for each executor and after it
performances collect(), it gets stuck at ""Added rdd_xx_xx"", no further
respond showed on the Application UI.

I've tried to lower the spark.shuffle.memoryFraction and
spark.storage.memoryFraction, but it seems that it can only deal with as
much as 2GB HashMap. What should I optimize for such conditions.

(ps: sorry for my bad English & Grammar)


Thanks
"
Patrick Wendell <pwendell@gmail.com>,"Wed, 24 Dec 2014 22:42:16 -0800",Re: Problems with large dataset using collect() and broadcast(),Will Yang <era.yeung@gmail.com>,"Hi Will,

When you call collect() the item you are collecting needs to fit in
memory on the driver. Is it possible your driver program does not have
enough memory?

- Patrick


---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 25 Dec 2014 06:49:00 +0000",Re: Starting with Spark,"Naveen Madhire <vmadhire@umail.iu.edu>, dev@spark.apache.org","The correct docs link is:
https://spark.apache.org/docs/1.2.0/building-spark.html

Where did you get that bad link from?

Nick



"
"""Shao, Saisai"" <saisai.shao@intel.com>","Thu, 25 Dec 2014 06:52:41 +0000",Question on saveAsTextFile with overwrite option,"""user@spark.apache.org"" <user@spark.apache.org>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","Hi,

We have such requirements to save RDD output to HDFS with saveAsTextFile like API, but need to overwrite the data if existed. I'm not sure if current Spark support such kind of operations, or I need to check this manually?

There's a thread in mailing list discussed about this (http://apache-spark-user-list.1001560.n3.nabble.com/How-can-I-make-Spark-1-0-saveAsTextFile-to-overwrite-existing-file-td6696.html), I'm not sure this feature is enabled or not, or with some configurations?

Appreciate your suggestions.

Thanks a lot
Jerry
"
Patrick Wendell <pwendell@gmail.com>,"Wed, 24 Dec 2014 23:21:59 -0800",Re: Question on saveAsTextFile with overwrite option,"""Shao, Saisai"" <saisai.shao@intel.com>","Is it sufficient to set ""spark.hadoop.validateOutputSpecs"" to false?

http://spark.apache.org/docs/latest/configuration.html

- Patrick


---------------------------------------------------------------------


"
"""Cheng, Hao"" <hao.cheng@intel.com>","Thu, 25 Dec 2014 07:28:45 +0000",RE: Question on saveAsTextFile with overwrite option,"Patrick Wendell <pwendell@gmail.com>, ""Shao, Saisai""
	<saisai.shao@intel.com>","I am wondering if we can provide more friendly API, other than configuration for this purpose. What do you think Patrick?

Cheng Hao

Is it sufficient to set ""spark.hadoop.validateOutputSpecs"" to false?

http://spark.apache.org/docs/latest/configuration.html

- Patrick

e:
to check this manually?

---------------------------------------------------------------------
mmands, e-mail: user-help@spark.apache.org


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Wed, 24 Dec 2014 23:42:55 -0800",Re: Question on saveAsTextFile with overwrite option,"""Cheng, Hao"" <hao.cheng@intel.com>","So the behavior of overwriting existing directories IMO is something
we don't want to encourage. The reason why the Hadoop client has these
checks is that it's very easy for users to do unsafe things without
them. For instance, a user could overwrite an RDD that had 100
partitions with an RDD that has 10 partitions... and if they read back
the RDD they would get a corrupted RDD that has a combination of data
from the old and new RDD.

If users want to circumvent these safety checks, we need to make them
explicitly disable them. Given this, I think a config option is as
reasonable as any alternatives. This is already pretty easy IMO.

- Patrick


---------------------------------------------------------------------


"
Cody Koeninger <cody@koeninger.org>,"Thu, 25 Dec 2014 01:46:39 -0600",Re: Which committers care about Kafka?,Koert Kuipers <koert@tresata.com>,"After a long talk with Patrick and TD (thanks guys), I opened the following
jira

https://issues.apache.org/jira/browse/SPARK-4964

Sample PR has an impementation for the batch and the dstream case, and a
link to a project with example usage.


l
nd
or
e
it
in
e
 exactly
t
t
h
ll
r
en
t
h
to
y
@gmail.com]
t
rt
n
e
of
ka
m
of
e
a
 a
metadata
e
 -
r
n
e
et
me
of
e
ry
ka
.
t.
"
"""Hari Shreedharan"" <hshreedharan@cloudera.com>","Wed, 24 Dec 2014 23:54:37 -0800 (PST)",Re: Which committers care about Kafka?,"""Cody Koeninger"" <cody@koeninger.org>","In general such discussions happen or is posted on the dev lists. Could you please post a summary? Thanks.



Thanks, Hari


following
bail
and
 or
start
same
In
way
 is
to
bit, it
 in
the
committing
been
the exactly
or
the
the
it
it
batch
be
a
well
near
tuple
The

given
same
Kafka
implemented
the
it
with
 to
data
very
say
groups@gmail.com]
not
org>
of
Commit
restart
an
The
solved
 of
I
If
kafka
system
that
 of
2
people
implement
this
 a
are a
specifically
partitions
 metadata
of
I
we
back -
that
people
.
is
To
for
be
impression
it's
there
get
for
because
same
 of
be
whole
arbitrary
Kafka
code.
get."
"""Shao, Saisai"" <saisai.shao@intel.com>","Thu, 25 Dec 2014 07:58:22 +0000",RE: Question on saveAsTextFile with overwrite option,"Patrick Wendell <pwendell@gmail.com>, ""Cheng, Hao"" <hao.cheng@intel.com>","Thanks Patrick for your detailed explanation.

BR
Jerry

So the behavior of overwriting existing directories IMO is something we don't want to encourage. The reason why the Hadoop client has these checks is that it's very easy for users to do unsafe things without them. For instance, a user could overwrite an RDD that had 100 partitions with an RDD that has 10 partitions... and if they read back the RDD they would get a corrupted RDD that has a combination of data from the old and new RDD.

If users want to circumvent these safety checks, we need to make them explicitly disable them. Given this, I think a config option is as reasonable as any alternatives. This is already pretty easy IMO.

- Patrick

ion for this purpose. What do you think Patrick?
ote:
 to check this manually?
?

---------------------------------------------------------------------


"
Cody Koeninger <cody@koeninger.org>,"Thu, 25 Dec 2014 01:59:21 -0600",Re: Which committers care about Kafka?,Hari Shreedharan <hshreedharan@cloudera.com>,"The conversation was mostly getting TD up to speed on this thread since he
had just gotten back from his trip and hadn't seen it.

The jira has a summary of the requirements we discussed, I'm sure TD or
Patrick can add to the ticket if I missed something.

:
e
t
n
is
o
,
e
g
he exactly
r
e
be
 a
e
.
me
a
he
d
a
y
ot
f
l
I
t
e
 2
t
s
y
e
of
e
r.
e
s
e
t
e
"
"""Haopu Wang"" <HWang@qilinsoft.com>","Thu, 25 Dec 2014 18:42:01 +0800",Do you know any Spark modeling tool?,"""user"" <user@spark.apache.org>,
	<dev@spark.apache.org>","Hi, I think a modeling tool may be helpful because sometimes it's
hard/tricky to program Spark. I don't know if there is already such a
tool.

Thanks!

---------------------------------------------------------------------


"
ranamitabh <ranamitabh@gmail.com>,"Thu, 25 Dec 2014 05:34:08 -0700 (MST)","=?UTF-8?Q?MapR_distribution_Spark_throwing_=E2=80=9Cno_?=
 =?UTF-8?Q?MapRClient_in_java.library.path=E2=80=9D_error?=",dev@spark.apache.org,"I am a big data developer and for past some time I have been using apache
spark of cloudera distribution. I have successfully written all my codes on
Cloudera-Spark but when I am trying my hands on MapR distribution of Spark
then I am getting the following error.

===============================================
java.lang.reflect.InvocationTargetException
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
    at java.lang.reflect.Method.invoke(Unknown Source)
    at com.mapr.fs.ShimLoader.loadNativeLibrary(ShimLoader.java:308)
    at com.mapr.fs.ShimLoader.load(ShimLoader.java:197)
    at
org.apache.hadoop.conf.CoreDefaultProperties.<clinit>(CoreDefaultProperties.java:54)
    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Unknown Source)
    at
org.apache.hadoop.conf.Configuration.getClassByNameOrNull(Configuration.java:1822)
    at
org.apache.hadoop.conf.Configuration.getProperties(Configuration.java:2037)
    at
org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2238)
    at
org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2190)
    at
org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2107)
    at org.apache.hadoop.conf.Configuration.set(Configuration.java:967)
    at org.apache.hadoop.conf.Configuration.set(Configuration.java:941)
    at
org.apache.spark.deploy.SparkHadoopUtil.newConfiguration(SparkHadoopUtil.scala:102)
    at
org.apache.spark.deploy.SparkHadoopUtil.<init>(SparkHadoopUtil.scala:42)
    at
org.apache.spark.deploy.SparkHadoopUtil$.<init>(SparkHadoopUtil.scala:202)
    at
org.apache.spark.deploy.SparkHadoopUtil$.<clinit>(SparkHadoopUtil.scala)
    at org.apache.spark.util.Utils$.getSparkOrYarnConfig(Utils.scala:1784)
    at org.apache.spark.storage.BlockManager.<init>(BlockManager.scala:105)
    at org.apache.spark.storage.BlockManager.<init>(BlockManager.scala:180)
    at org.apache.spark.SparkEnv$.create(SparkEnv.scala:292)
    at org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:159)
    at org.apache.spark.SparkContext.<init>(SparkContext.scala:232)
    at org.apache.spark.SparkContext.<init>(SparkContext.scala:136)
    at org.apache.spark.SparkContext.<init>(SparkContext.scala:151)
    at
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:67)
    at word.count.WordCount.main(WordCount.java:28)
Caused by: java.lang.UnsatisfiedLinkError: no MapRClient in
java.library.path
    at java.lang.ClassLoader.loadLibrary(Unknown Source)
    at java.lang.Runtime.loadLibrary0(Unknown Source)
    at java.lang.System.loadLibrary(Unknown Source)
    at com.mapr.fs.shim.LibraryLoader.loadLibrary(LibraryLoader.java:41)
    ... 30 more
==========Unable to find library in jar due to exception. ==============
java.lang.RuntimeException: no native library is found for os.name=Windows
and os.arch=x86_64
    at com.mapr.fs.ShimLoader.findNativeLibrary(ShimLoader.java:496)
    at com.mapr.fs.ShimLoader.loadNativeLibrary(ShimLoader.java:318)
    at com.mapr.fs.ShimLoader.load(ShimLoader.java:197)
    at
org.apache.hadoop.conf.CoreDefaultProperties.<clinit>(CoreDefaultProperties.java:54)
    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Unknown Source)
    at
org.apache.hadoop.conf.Configuration.getClassByNameOrNull(Configuration.java:1822)
    at
org.apache.hadoop.conf.Configuration.getProperties(Configuration.java:2037)
    at
org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2238)
    at
org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2190)
    at
org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2107)
    at org.apache.hadoop.conf.Configuration.set(Configuration.java:967)
    at org.apache.hadoop.conf.Configuration.set(Configuration.java:941)
    at
org.apache.spark.deploy.SparkHadoopUtil.newConfiguration(SparkHadoopUtil.scala:102)
    at
org.apache.spark.deploy.SparkHadoopUtil.<init>(SparkHadoopUtil.scala:42)
    at
org.apache.spark.deploy.SparkHadoopUtil$.<init>(SparkHadoopUtil.scala:202)
    at
org.apache.spark.deploy.SparkHadoopUtil$.<clinit>(SparkHadoopUtil.scala)
    at org.apache.spark.util.Utils$.getSparkOrYarnConfig(Utils.scala:1784)
    at org.apache.spark.storage.BlockManager.<init>(BlockManager.scala:105)
    at org.apache.spark.storage.BlockManager.<init>(BlockManager.scala:180)
    at org.apache.spark.SparkEnv$.create(SparkEnv.scala:292)
    at org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:159)
    at org.apache.spark.SparkContext.<init>(SparkContext.scala:232)
    at org.apache.spark.SparkContext.<init>(SparkContext.scala:136)
    at org.apache.spark.SparkContext.<init>(SparkContext.scala:151)
    at
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:67)
    at word.count.WordCount.main(WordCount.java:28)
java.lang.reflect.InvocationTargetException
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
    at java.lang.reflect.Method.invoke(Unknown Source)
    at com.mapr.fs.ShimLoader.loadNativeLibrary(ShimLoader.java:308)
    at com.mapr.fs.ShimLoader.load(ShimLoader.java:197)
    at
org.apache.hadoop.conf.CoreDefaultProperties.<clinit>(CoreDefaultProperties.java:54)
    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Unknown Source)
    at
org.apache.hadoop.conf.Configuration.getClassByNameOrNull(Configuration.java:1822)
    at
org.apache.hadoop.conf.Configuration.getProperties(Configuration.java:2037)
    at
org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2238)
    at
org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2190)
    at
org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2107)
    at org.apache.hadoop.conf.Configuration.set(Configuration.java:967)
    at org.apache.hadoop.conf.Configuration.set(Configuration.java:941)
    at
org.apache.spark.deploy.SparkHadoopUtil.newConfiguration(SparkHadoopUtil.scala:102)
    at
org.apache.spark.deploy.SparkHadoopUtil.<init>(SparkHadoopUtil.scala:42)
    at
org.apache.spark.deploy.SparkHadoopUtil$.<init>(SparkHadoopUtil.scala:202)
    at
org.apache.spark.deploy.SparkHadoopUtil$.<clinit>(SparkHadoopUtil.scala)
    at org.apache.spark.util.Utils$.getSparkOrYarnConfig(Utils.scala:1784)
    at org.apache.spark.storage.BlockManager.<init>(BlockManager.scala:105)
    at org.apache.spark.storage.BlockManager.<init>(BlockManager.scala:180)
    at org.apache.spark.SparkEnv$.create(SparkEnv.scala:292)
    at org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:159)
    at org.apache.spark.SparkContext.<init>(SparkContext.scala:232)
    at org.apache.spark.SparkContext.<init>(SparkContext.scala:136)
    at org.apache.spark.SparkContext.<init>(SparkContext.scala:151)
    at
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:67)
    at word.count.WordCount.main(WordCount.java:28)
Caused by: java.lang.UnsatisfiedLinkError: no MapRClient in
java.library.path
    at java.lang.ClassLoader.loadLibrary(Unknown Source)
    at java.lang.Runtime.loadLibrary0(Unknown Source)
    at java.lang.System.loadLibrary(Unknown Source)
    at com.mapr.fs.shim.LibraryLoader.loadLibrary(LibraryLoader.java:41)
    ... 30 more
Exception in thread ""main"" java.lang.ExceptionInInitializerError
    at com.mapr.fs.ShimLoader.load(ShimLoader.java:214)
    at
org.apache.hadoop.conf.CoreDefaultProperties.<clinit>(CoreDefaultProperties.java:54)
    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Unknown Source)
    at
org.apache.hadoop.conf.Configuration.getClassByNameOrNull(Configuration.java:1822)
    at
org.apache.hadoop.conf.Configuration.getProperties(Configuration.java:2037)
    at
org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2238)
    at
org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2190)
    at
org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2107)
    at org.apache.hadoop.conf.Configuration.set(Configuration.java:967)
    at org.apache.hadoop.conf.Configuration.set(Configuration.java:941)
    at
org.apache.spark.deploy.SparkHadoopUtil.newConfiguration(SparkHadoopUtil.scala:102)
    at
org.apache.spark.deploy.SparkHadoopUtil.<init>(SparkHadoopUtil.scala:42)
    at
org.apache.spark.deploy.SparkHadoopUtil$.<init>(SparkHadoopUtil.scala:202)
    at
org.apache.spark.deploy.SparkHadoopUtil$.<clinit>(SparkHadoopUtil.scala)
    at org.apache.spark.util.Utils$.getSparkOrYarnConfig(Utils.scala:1784)
    at org.apache.spark.storage.BlockManager.<init>(BlockManager.scala:105)
    at org.apache.spark.storage.BlockManager.<init>(BlockManager.scala:180)
    at org.apache.spark.SparkEnv$.create(SparkEnv.scala:292)
    at org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:159)
    at org.apache.spark.SparkContext.<init>(SparkContext.scala:232)
    at org.apache.spark.SparkContext.<init>(SparkContext.scala:136)
    at org.apache.spark.SparkContext.<init>(SparkContext.scala:151)
    at
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:67)
    at word.count.WordCount.main(WordCount.java:28)
Caused by: java.lang.reflect.InvocationTargetException
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
    at java.lang.reflect.Method.invoke(Unknown Source)
    at com.mapr.fs.ShimLoader.loadNativeLibrary(ShimLoader.java:308)
    at com.mapr.fs.ShimLoader.load(ShimLoader.java:197)
    ... 24 more
Caused by: java.lang.UnsatisfiedLinkError: no MapRClient in
java.library.path
    at java.lang.ClassLoader.loadLibrary(Unknown Source)
    at java.lang.Runtime.loadLibrary0(Unknown Source)
    at java.lang.System.loadLibrary(Unknown Source)
    at com.mapr.fs.shim.LibraryLoader.loadLibrary(LibraryLoader.java:41)
    ... 30 more

====================================

I have written a simple word count code. *When I am removing MapR-Spark jar
and replacing it with Cloudera-Spark jar then the same code is executing
perfectly*. Kindly help.



--

---------------------------------------------------------------------


"
Naveen Madhire <vmadhire@umail.iu.edu>,"Thu, 25 Dec 2014 10:35:19 -0500",Re: Starting with Spark,Nicholas Chammas <nicholas.chammas@gmail.com>,"Thanks. I will work on this today and try to setup.

The bad link is present in the below github REAMME file,

https://github.com/apache/spark

Search with ""Build Spark with Maven""


"
slcclimber <anant.asty@gmail.com>,"Thu, 25 Dec 2014 11:08:40 -0700 (MST)",Re: Apache Spark (Data Aggregation) using Java API,dev@spark.apache.org,"You could convert your csv file to an rdd of vectors.
Then use stats from mllib.

Also this should be in the user list not the developer list.



--

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 25 Dec 2014 18:46:04 +0000",Re: Starting with Spark,Naveen Madhire <vmadhire@umail.iu.edu>,"Thanks for the pointer. This will be fixed in this PR
<https://github.com/apache/spark/pull/3802>.


"
Denny Lee <denny.g.lee@gmail.com>,"Thu, 25 Dec 2014 18:53:45 +0000",Re: Starting with Spark,"Nicholas Chammas <nicholas.chammas@gmail.com>, Naveen Madhire <vmadhire@umail.iu.edu>","Thanks for the catch Naveen!


"
Dirceu Semighini Filho <dirceu.semighini@gmail.com>,"Fri, 26 Dec 2014 12:08:08 -0200",Spark 1.2.0 Repl,,"Hello,
Is there any reason in not publishing spark repl in the version 1.2.0?
In repl/pom.xml the deploy and publish are been skipped.

Regards,
Dirceu
"
Sean Owen <sowen@cloudera.com>,"Fri, 26 Dec 2014 19:43:35 +0000",Re: Spark 1.2.0 Repl,Dirceu Semighini Filho <dirceu.semighini@gmail.com>,"It was not intended to be a public API but there is a request to keep
publishing it as a developer API:
https://issues.apache.org/jira/browse/SPARK-4923

"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 26 Dec 2014 21:18:25 +0000",SQL specification for reference during Spark SQL development,dev <dev@spark.apache.org>,"Do we have access to the SQL specification (say, SQL-92) for reference
during Spark SQL development? I know it's not freely available on the web.
Usually, you can only access drafts.

I know that, generally, we look to other systems (especially Hive) when
figuring out how something in Spark SQL should work, but it might be nice
to have the standard available for reference.

Nick
"
Alessandro Baretta <alexbaretta@gmail.com>,"Fri, 26 Dec 2014 16:11:05 -0800","SQLContext is Serializable, SparkContext is not","""dev@spark.apache.org"" <dev@spark.apache.org>","How, O how can this be? Doesn't the SQLContext hold a reference to the
SparkContext?

Alex
"
Alessandro Baretta <alexbaretta@gmail.com>,"Fri, 26 Dec 2014 18:15:45 -0800",Assembly jar file name does not match profile selection,"""dev@spark.apache.org"" <dev@spark.apache.org>","I am building spark with sbt off of branch 1.2. I'm using the following
command:

sbt/sbt -Pyarn -Phadoop-2.3 assembly

(http://spark.apache.org/docs/latest/building-spark.html#building-with-sbt)

Although the jar file I obtain does contain the proper version of the
hadoop libraries (v. 2.4), the assembly jar file name refers to hadoop
v.1.0.4:

./assembly/target/scala-2.10/spark-assembly-1.3.0-SNAPSHOT-hadoop1.0.4.jar

Any idea why?


Alex
"
Ted Yu <yuzhihong@gmail.com>,"Fri, 26 Dec 2014 20:41:33 -0800",Re: Assembly jar file name does not match profile selection,Alessandro Baretta <alexbaretta@gmail.com>,"Can you try this command ?

sbt/sbt -Pyarn -Phadoop-2.4 -Dhadoop.version=2.6.0 -Phive assembly


"
Cody Koeninger <cody@koeninger.org>,"Fri, 26 Dec 2014 23:08:39 -0600","Re: SQLContext is Serializable, SparkContext is not",Alessandro Baretta <alexbaretta@gmail.com>,"The spark context reference is transient.


"
Alessandro Baretta <alexbaretta@gmail.com>,"Fri, 26 Dec 2014 21:12:34 -0800",Re: Assembly jar file name does not match profile selection,Ted Yu <yuzhihong@gmail.com>,"Here's what I get:

./assembly/target/scala-2.10/spark-assembly-1.3.0-SNAPSHOT-hadoop2.6.0.jar

Alex


"
Alessandro Baretta <alexbaretta@gmail.com>,"Fri, 26 Dec 2014 22:47:20 -0800",Unsupported Catalyst types in Parquet,"""dev@spark.apache.org"" <dev@spark.apache.org>, Michael Armbrust <michael@databricks.com>","Michael,

I'm having trouble storing my SchemaRDDs in Parquet format with SparkSQL,
due to my RDDs having having DateType and DecimalType fields. What would it
take to add Parquet support for these Catalyst? Are there any other
Catalyst types for which there is no Catalyst support?

Alex
"
Kousuke Saruta <sarutak@oss.nttdata.co.jp>,"Sat, 27 Dec 2014 16:11:47 +0900",Re: Assembly jar file name does not match profile selection,"Alessandro Baretta <alexbaretta@gmail.com>, 
 ""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Alessandro,

It's fixed by SPARK-3787 and will be applied to 1.2.1 and 1.3.0.

https://issues.apache.org/jira/browse/SPARK-3787

- Kousuke



---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Sat, 27 Dec 2014 13:44:23 -0800",ANNOUNCE: New build script ./build/mvn,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi All,

A consistent piece of feedback from Spark developers has been that the
Maven build is very slow. Typesafe provides a tool called Zinc which
improves Scala complication speed substantially with Maven, but is
difficult to install and configure, especially for platforms other
than Mac OS.

I've just merged a patch (authored by Brennon York) that provides an
automatically configured Maven instance with Zinc embedded in Spark.
E.g.:

    ./build/mvn -Phive -Phive-thriftserver -Pyarn -Phadoop-2.3 package

It is hard to test changes like this across all environments, so
please give this a spin and report any issues on the Spark JIRA. It is
working correctly if you see the following message during compilation:

    [INFO] Using zinc server for incremental compilation

Note that developers preferring their own Maven installation are
unaffected by this and can just ignore this new feature.

Cheers,
- Patrick

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sat, 27 Dec 2014 21:51:43 +0000",Re: ANNOUNCE: New build script ./build/mvn,"Patrick Wendell <pwendell@gmail.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Linkies for the curious:

   - SPARK-4501 <https://issues.apache.org/jira/browse/SPARK-4501>: Create
   build/mvn to automatically download maven/zinc/scalac
   - https://github.com/apache/spark/pull/3707
   - New build folder (mvn and sbt):
   https://github.com/apache/spark/tree/master/build

Nick
​

:

"
Mark Hamstra <mark@clearstorydata.com>,"Sat, 27 Dec 2014 15:44:35 -0800",Re: ANNOUNCE: New build script ./build/mvn,Nicholas Chammas <nicholas.chammas@gmail.com>,"

Heh.  I like that.


e
"
Naveen Madhire <vmadhire@umail.iu.edu>,"Sun, 28 Dec 2014 18:10:23 -0500","Spark Error - Failed to locate the winutils binary in the hadoop
 binary path",dev@spark.apache.org,"Hi All,

I am getting the below error while running a simple spark application from
Eclipse.

I am using Eclipse, Maven, Java.

I've spark running locally on my Windows laptop. I copied the spark files
from the spark summit 2014 training
http://databricks.com/spark-training-resources#itas

I can run sample commands and small programs using the spark shell. But
getting the below error while running from Eclipse.

14/12/28 18:01:59 ERROR Shell: Failed to locate the winutils binary in the
hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in
the Hadoop binaries.
at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:278)
at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:300)
at org.apache.hadoop.util.Shell.<clinit>(Shell.java:293)
at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:76)
at
org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:362)
at org.apache.spark.SparkContext$$anonfun$26.apply(SparkContext.scala:696)
at org.apache.spark.SparkContext$$anonfun$26.apply(SparkContext.scala:696)
at
org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$6.apply(HadoopRDD.scala:170)
at
org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$6.apply(HadoopRDD.scala:170)
at scala.Option.map(Option.scala:145)
at org.apache.spark.rdd.HadoopRDD.getJobConf(HadoopRDD.scala:170)
at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:194)
at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:205)
at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:203)
at scala.Option.getOrElse(Option.scala:120)
at org.apache.spark.rdd.RDD.partitions(RDD.scala:203)
at org.apache.spark.rdd.MappedRDD.getPartitions(MappedRDD.scala:28)
at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:205)
at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:203)
at scala.Option.getOrElse(Option.scala:120)
at org.apache.spark.rdd.RDD.partitions(RDD.scala:203)
at org.apache.spark.rdd.FilteredRDD.getPartitions(FilteredRDD.scala:29)


Please suggest if I am doing something wrong.


Thanks for help

-Naveen
"
Naveen Madhire <vmadhire@umail.iu.edu>,"Sun, 28 Dec 2014 18:35:50 -0500","Re: Spark Error - Failed to locate the winutils binary in the hadoop
 binary path",dev@spark.apache.org,"Hi All,

Sorry, I should have checked the JIRA issue tracker before sending the
email.

I found this is an already existing issue,

https://issues.apache.org/jira/browse/SPARK-2356

And the solution is present in the below location,
http://qnalist.com/questions/4994960/run-spark-unit-test-on-windows-7


Now it is working fine.

Thanks all.



"
Naveen Madhire <vmadhire@umail.iu.edu>,"Sun, 28 Dec 2014 22:21:30 -0500",Spark 1.2.0 build error,dev@spark.apache.org,"Hi,

I am follow the below link for building Spark 1.2.0

https://spark.apache.org/docs/1.2.0/building-spark.html

I am getting the below error during the Maven build. I am using IntelliJ
IDE.

The build is failing in the scalatest plugin,

[INFO] Reactor Summary:
[INFO]
[INFO] Spark Project Parent POM .......................... SUCCESS [3.355s]
[INFO] Spark Project Networking .......................... SUCCESS [4.017s]
[INFO] Spark Project Shuffle Streaming Service ........... SUCCESS [2.914s]
[INFO] Spark Project Core ................................ FAILURE [9.678s]
[INFO] Spark Project Bagel ............................... SKIPPED
[INFO] Spark Project GraphX .............................. SKIPPED



[ERROR] Failed to execute goal
org.scalatest:scalatest-maven-plugin:1.0:test (test) on project
spark-core_2.10: There are test failures -> [Help 1]
org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute
goal org.scalatest:scalatest-maven-plugin:1.0:test (test) on project
spark-core_2.10: There are test failures
at
org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:212)
at
org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
at
org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
at
org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:84)
at
org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:59)
at
org.apache.maven.lifecycle.internal.LifecycleStarter.singleThreadedBuild(LifecycleStarter.java:183)
at
org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:161)
at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:317)
at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:152)
at org.apache.maven.cli.MavenCli.execute(MavenCli.java:555)
at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:214)


Is there something which I am missing during the build process. Please
suggest.

Using
IntelliJ 13.1.6
Maven 3.1.1
Scala 2.10.4
Spark 1.2.0


Thanks
Naveen
"
"""Wang, Daoyuan"" <daoyuan.wang@intel.com>","Mon, 29 Dec 2014 08:06:43 +0000",RE: Unsupported Catalyst types in Parquet,"Alessandro Baretta <alexbaretta@gmail.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>, Michael Armbrust <michael@databricks.com>","Hi Alex,

I'll create JIRA SPARK-4985 for date type support in parquet, and SPARK-4987 for timestamp type support. For decimal type, I think we only support decimals that fits in a long.

Thanks,
Daoyuan

-----Original Message-----
From: Alessandro Baretta [mailto:alexbaretta@gmail.com] 
Sent: Saturday, December 27, 2014 2:47 PM
To: dev@spark.apache.org; Michael Armbrust
Subject: Unsupported Catalyst types in Parquet

Michael,

I'm having trouble storing my SchemaRDDs in Parquet format with SparkSQL, due to my RDDs having having DateType and DecimalType fields. What would it take to add Parquet support for these Catalyst? Are there any other Catalyst types for which there is no Catalyst support?

Alex

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org
"
Sean Owen <sowen@cloudera.com>,"Mon, 29 Dec 2014 08:52:07 +0000",Re: Spark 1.2.0 build error,Naveen Madhire <vmadhire@umail.iu.edu>,"It means a test failed but you have not shown the test failure. This would
have been logged earlier. You would need to say how you ran tests too. The
tests for 1.2.0 pass for me on several common permutations.

"
"""Jakub Dubovsky"" <spark.dubovsky.jakub@seznam.cz>","Mon, 29 Dec 2014 16:59:20 +0100 (CET)",How to become spark developer in jira?,<dev@spark.apache.org>,"Hi devs,

  I'd like to ask what are the procedures/conditions for being assigned a 
role of a developer on spark jira? My motivation is to be able to assign 

scheme [1].

  regards
  Jakub

 [1] https://cwiki.apache.org/confluence/display/SPARK/Jira+Permissions+
Scheme
"
Alessandro Baretta <alexbaretta@gmail.com>,"Mon, 29 Dec 2014 08:13:53 -0800",RE: Unsupported Catalyst types in Parquet,"""Wang, Daoyuan"" <daoyuan.wang@intel.com>","Daoyuan,

Thanks for creating the jiras. I need these features by... last week, so
I'd be happy to take care of this myself, if only you or someone more
experienced than me in the SparkSQL codebase could provide some guidance.

Alex

"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 29 Dec 2014 10:30:34 -0800",Re: How to become spark developer in jira?,Jakub Dubovsky <spark.dubovsky.jakub@seznam.cz>,"Please ask someone else to assign them for now, and just comment on them that you're working on them. Over time if you contribute a bunch we'll add you to that list. The problem is that in the past, people would assign issues to themselves and never actually work on them, making it confusing for others.

Matei

assigned a 
assign 
permission 
https://cwiki.apache.org/confluence/display/SPARK/Jira+Permissions+


---------------------------------------------------------------------


"
"""Jakub Dubovsky"" <spark.dubovsky.jakub@seznam.cz>","Mon, 29 Dec 2014 19:46:13 +0100 (CET)",Re: How to become spark developer in jira?,"""Matei Zaharia"" <matei.zaharia@gmail.com>","Hi Matei,

  that makes sense. Thanks a lot!

  Jakub


---------- Původní zpráva ----------
Od: Matei Zaharia <matei.zaharia@gmail.com>
Komu: Jakub Dubovsky <spark.dubovsky.jakub@seznam.cz>
Datum: 29. 12. 2014 19:31:57
Předmět: Re: How to become spark developer in jira?

""Please ask someone else to assign them for now, and just comment on them 
that you're working on them. Over time if you contribute a bunch we'll add 
you to that list. The problem is that in the past, people would assign 
issues to themselves and never actually work on them, making it confusing 
for others.

Matei

.

 




---------------------------------------------------------------------
"
Tathagata Das <tathagata.das1565@gmail.com>,"Mon, 29 Dec 2014 14:33:18 -0800",Re: Which committers care about Kafka?,Cody Koeninger <cody@koeninger.org>,"Hey all,

Some wrap up thoughts on this thread.

Let me first reiterate what Patrick said, that Kafka is super super
important as it forms the largest fraction of Spark Streaming user
base. So we really want to improve the Kafka + Spark Streaming
integration. To this end, some of the things that needs to be
considered can be broadly classified into the following to sort
facilitate the discussion.

1. Data rate control
2. Receiver failure semantics - partially achieving this gives
at-least once, completely achieving this gives exactly-once
3. Driver failure semantics - partially achieving this gives at-least
once, completely achieving this gives exactly-once

Here is a run down of what is achieved by different implementations
(based on what I think).

1. Prior to WAL in Spark 1.2, the KafkaReceiver could handle 3, could
handle 1 partially (some duplicate data), and could NOT handle 2 (all
previously received data lost).

2. In Spark 1.2 with WAL enabled, the Saisai's ReliableKafkaReceiver
can handle 3, can almost completely handle 1 and 2 (except few corner
cases which prevents it from completely guaranteeing exactly-once).

3. I believe Dibyendu's solution (correct me if i am wrong) can handle
1 and 2 perfectly. And 3 can be partially solved with WAL, or possibly
completely solved by extending the solution further.

4. Cody's solution (again, correct me if I am wrong) does not use
receivers at all (so eliminates 2). It can handle 3 completely for
simple operations like map and filter, but not sure if it works
completely for stateful ops like windows and updateStateByKey. Also it
does not handle 1.

The real challenge for Kafka is in achieving 3 completely for stateful
operations while also handling 1.  (i.e., use receivers, but still get
driver failure guarantees). Solving this will give us our holy grail
solution, and this is what I want to achieve.

exactly-once semantics - https://github.com/apache/spark/pull/3798 . I
am reviewing it. Please follow the PR if you are interested.

TD

:
e
a
e:
a
ce
rt
In
 is
to
t,
te
ng
n
d
the exactly
or
he
s
k
t
 be
g a
e
he
f
e.
ame
y
ka
the
,
rd
ta
ay
g
not
of
s
ll
y
 I
at
se
t
n 2
nt
is
ly
he
 of
.
,
le
er.
be
's
o
se
nt
be

---------------------------------------------------------------------


"
Cody Koeninger <cody@koeninger.org>,"Mon, 29 Dec 2014 16:49:30 -0600",Re: Which committers care about Kafka?,Tathagata Das <tathagata.das1565@gmail.com>,"Can you give a little more clarification on exactly what is meant by

1. Data rate control

If someone wants to clamp the maximum number of messages per RDD partition
in my solution, it would be very straightforward to do so.

Regarding the holy grail, I'm pretty certain you can't have end-to-end
transactional semantics without the client code being in charge of offset
state.  That means the client code is going to also need to be in charge of
setting up an initial state for updateStateByKey that makes sense; as long
as they can do that, the job should be safe to restart from arbitrary
failures.


d
d
e
,
e
s
n
he
e
e
fy
k the
,
r
a
he
 I
he
as
e
a
f
h
ta
ry
ls
ka
ng
k
y
le
ng
e
m
or
m
s
e.
at
t
.
f
of
t
an
e
 the
ll
ng
t.
rt
a
t
e
he
h
t
t
at
ad
s
e
to
--
"
Michael Armbrust <michael@databricks.com>,"Mon, 29 Dec 2014 15:07:42 -0800",Re: Unsupported Catalyst types in Parquet,Alessandro Baretta <alexbaretta@gmail.com>,"I'd love to get both of these in.  There is some trickiness that I talk
about on the JIRA for timestamps since the SQL timestamp class can support
nano seconds and I don't think parquet has a type for this.  Other systems
(impala) seem to use INT96.  It would be great to maybe ask on the parquet
mailing list what the plan is there to make sure that whatever we do is
going to be compatible long term.

Michael


"
Naveen Madhire <vmadhire@umail.iu.edu>,"Mon, 29 Dec 2014 18:16:43 -0500",Re: Spark 1.2.0 build error,Sean Owen <sowen@cloudera.com>,"I am getting ""The command is too long"" error.

Is there anything which needs to be done.
However for the time being I followed the ""sbt"" way of buidling spark in
IntelliJ.


"
Andrew Lee <alee526@hotmail.com>,"Mon, 29 Dec 2014 16:09:03 -0800","RE: Build Spark 1.2.0-rc1 encounter exceptions when running
 HiveContext - Caused by: java.lang.ClassNotFoundException:
 com.esotericsoftware.shaded.org.objenesis.strategy.InstantiatorStrategy",Patrick Wendell <pwendell@gmail.com>,"Hi Patrick,
I manually hardcoded the hive version to 0.13.1a and it works. It turns out that for some reason, 0.13.1 is being picked up instead of the 0.13.1a version from maven.
So my solution was:hardcode the hive.version to 0.13.1a in my case since I am building it against hive 0.13 only, so the pom.xml was hardcoded with that version string, and the final JAR is working now with hive-exec 0.13.1a embed.
Possible Reason why it didn't work?I suspect our internal environment is picking up 0.13.1 since we do use our own maven repo as a proxy and caching.  0.13.1a did appear in our own repo and it got replicated from the maven central repo, but during the build process, maven picked up 0.13.1 instead of 0.13.1a.

Context - Caused by: java.lang.ClassNotFoundException: com.esotericsoftware.shaded.org.objenesis.strategy.InstantiatorStrategy
72aedf3
rote:
removed. Here's a reformatted version.
sh to include auxiliaries JARs and datanucleus*.jars from Hive, however, when I run HiveContext, it gives me the following error:
d.org.objenesis.strategy.InstantiatorStrategy
luded (shaded) in the assembly JAR (spark-assembly-1.2.0-hadoop2.4.1.jar) which is configured in the System classpath already. I couldn't figure out what is going on with the shading on the esotericsoftware JARs here.  Any help is appreciated.
my spark-shell. cd /opt/spark; ./bin/spark-shell --master yarn --deploy-mode client --queue research --driver-memory 1024M)
INT, value STRING)"")
r /etc/spark/)
.4.1 -Dyarn.version=2.4.1 -Dhive.version=0.13.1 -DskipTests install
:
 f -name ""snappy-java-*.jar"")
-name ""hadoop-lzo-*.jar"")
4.1.jar
e
ZO_JAR:$HIVE_CONF_DIR:/opt/hive/lib/datanucleus-api-jdo-3.2.6.jar:/opt/hive/lib/datanucleus-core-3.2.10.jar:/opt/hive/lib/datanucleus-rdbms-3.2.9.jar
on for details
4736-44b3-a601-04fa77cb6730_1220828461.txt
sis/strategy/InstantiatorStrategy
ava:925)
anticAnalyzer.java:9718)
anticAnalyzer.java:9712)
975)
)
la:305)
scala:276)
esult$lzycompute(NativeCommand.scala:35)
esult(NativeCommand.scala:35)
.scala:46)
iveCommand.scala:30)
te(SQLContext.scala:425)
xt.scala:425)
.scala:58)
a:102)
06)
sorImpl.java:57)
hodAccessorImpl.java:43)
n.scala:852)
n.scala:1125)
cala:674)
05)
69)
.scala:828)
Loop.scala:873)
)
la:628)
:636)
$sp(SparkILoop.scala:968)
rkILoop.scala:916)
rkILoop.scala:916)
alaClassLoader.scala:135)
)
1)
sorImpl.java:57)
hodAccessorImpl.java:43)
:353)
5)
ed.org.objenesis.strategy.InstantiatorStrategy
 		 	   		  "
Stephen Boesch <javadba@gmail.com>,"Mon, 29 Dec 2014 16:55:36 -0800",Adding third party jars to classpath used by pyspark,"""dev@spark.apache.org"" <dev@spark.apache.org>","What is the recommended way to do this?  We have some native database
client libraries for which we are adding pyspark bindings.

The pyspark invokes spark-submit.   Do we add our libraries to
the SPARK_SUBMIT_LIBRARY_PATH ?

This issue relates back to an error we have been seeing ""Py4jError: Trying
to call a package"" - the suspicion being that the third party libraries may
not be available on the jvm side.
"
Alessandro Baretta <alexbaretta@gmail.com>,"Mon, 29 Dec 2014 17:14:58 -0800",Re: Unsupported Catalyst types in Parquet,Michael Armbrust <michael@databricks.com>,"Michael,

Actually, Adrian Wang already created pull requests for these issues.

https://github.com/apache/spark/pull/3820
https://github.com/apache/spark/pull/3822

What do you think?

Alex


"
"""Shao, Saisai"" <saisai.shao@intel.com>","Tue, 30 Dec 2014 01:50:51 +0000",RE: Which committers care about Kafka?,"Cody Koeninger <cody@koeninger.org>, Tathagata Das
	<tathagata.das1565@gmail.com>","Hi Cody,

From my understanding rate control is an optional configuration in Spark Streaming and is disabled by default, so user can reach maximum throughput without any configuration.

The reason why rate control is so important in streaming processing is that Spark Streaming and other streaming frameworks are easily prone to unexpected behavior and failure situation due to network boost and other un-controllable inject rate.

Especially for Spark Streaming,  the large amount of processed data will delay the processing time, which will further delay the ongoing job, and finally lead to failure.

Thanks
Jerry

From: Cody Koeninger [mailto:cody@koeninger.org]
Sent: Tuesday, December 30, 2014 6:50 AM
To: Tathagata Das
Cc: Hari Shreedharan; Shao, Saisai; Sean McNamara; Patrick Wendell; Luis Ángel Vicente Sánchez; Dibyendu Bhattacharya; dev@spark.apache.org; Koert Kuipers
Subject: Re: Which committers care about Kafka?

Can you give a little more clarification on exactly what is meant by

1. Data rate control

If someone wants to clamp the maximum number of messages per RDD partition in my solution, it would be very straightforward to do so.

Regarding the holy grail, I'm pretty certain you can't have end-to-end transactional semantics without the client code being in charge of offset state.  That means the client code is going to also need to be in charge of setting up an initial state for updateStateByKey that makes sense; as long as they can do that, the job should be safe to restart from arbitrary failures.

On Mon, Dec 29, 2014 at 4:33 PM, Tathagata Das <tathagata.das1565@gmail.com<mailto:tathagata.das1565@gmail.com>> wrote:
Hey all,

Some wrap up thoughts on this thread.

Let me first reiterate what Patrick said, that Kafka is super super
important as it forms the largest fraction of Spark Streaming user
base. So we really want to improve the Kafka + Spark Streaming
integration. To this end, some of the things that needs to be
considered can be broadly classified into the following to sort
facilitate the discussion.

1. Data rate control
2. Receiver failure semantics - partially achieving this gives
at-least once, completely achieving this gives exactly-once
3. Driver failure semantics - partially achieving this gives at-least
once, completely achieving this gives exactly-once

Here is a run down of what is achieved by different implementations
(based on what I think).

1. Prior to WAL in Spark 1.2, the KafkaReceiver could handle 3, could
handle 1 partially (some duplicate data), and could NOT handle 2 (all
previously received data lost).

2. In Spark 1.2 with WAL enabled, the Saisai's ReliableKafkaReceiver
can handle 3, can almost completely handle 1 and 2 (except few corner
cases which prevents it from completely guaranteeing exactly-once).

3. I believe Dibyendu's solution (correct me if i am wrong) can handle
1 and 2 perfectly. And 3 can be partially solved with WAL, or possibly
completely solved by extending the solution further.

4. Cody's solution (again, correct me if I am wrong) does not use
receivers at all (so eliminates 2). It can handle 3 completely for
simple operations like map and filter, but not sure if it works
completely for stateful ops like windows and updateStateByKey. Also it
does not handle 1.

The real challenge for Kafka is in achieving 3 completely for stateful
operations while also handling 1.  (i.e., use receivers, but still get
driver failure guarantees). Solving this will give us our holy grail
solution, and this is what I want to achieve.

On that note, Cody submitted a PR on his style of achieving
exactly-once semantics - https://github.com/apache/spark/pull/3798 . I
am reviewing it. Please follow the PR if you are interested.

TD

On Wed, Dec 24, 2014 at 11:59 PM, Cody Koeninger <cody@koeninger.org<mailto:cody@koeninger.org>> wrote:
> The conversation was mostly getting TD up to speed on this thread since he
> had just gotten back from his trip and hadn't seen it.
>
> The jira has a summary of the requirements we discussed, I'm sure TD or
> Patrick can add to the ticket if I missed something.
> On Dec 25, 2014 1:54 AM, ""Hari Shreedharan"" <hshreedharan@cloudera.com<mailto:hshreedharan@cloudera.com>>
> wrote:
>
>> In general such discussions happen or is posted on the dev lists. Could
>> you please post a summary? Thanks.
>>
>> Thanks,
>> Hari
>>
>>
>> On Wed, Dec 24, 2014 at 11:46 PM, Cody Koeninger <cody@koeninger.org<mailto:cody@koeninger.org>>
>> wrote:
>>
>>>  After a long talk with Patrick and TD (thanks guys), I opened the
>>> following jira
>>>
>>> https://issues.apache.org/jira/browse/SPARK-4964
>>>
>>> Sample PR has an impementation for the batch and the dstream case, and a
>>> link to a project with example usage.
>>>
>>> On Fri, Dec 19, 2014 at 4:36 PM, Koert Kuipers <koert@tresata.com<mailto:koert@tresata.com>> wrote:
>>>
>>>> yup, we at tresata do the idempotent store the same way. very simple
>>>> approach.
>>>>
>>>> On Fri, Dec 19, 2014 at 5:32 PM, Cody Koeninger <cody@koeninger.org<mailto:cody@koeninger.org>>
>>>> wrote:
>>>>>
>>>>> That KafkaRDD code is dead simple.
>>>>>
>>>>> Given a user specified map
>>>>>
>>>>> (topic1, partition0) -> (startingOffset, endingOffset)
>>>>> (topic1, partition1) -> (startingOffset, endingOffset)
>>>>> ...
>>>>> turn each one of those entries into a partition of an rdd, using the
>>>>> simple
>>>>> consumer.
>>>>> That's it.  No recovery logic, no state, nothing - for any failures,
>>>>> bail
>>>>> on the rdd and let it retry.
>>>>> Spark stays out of the business of being a distributed database.
>>>>>
>>>>> The client code does any transformation it wants, then stores the data
>>>>> and
>>>>> offsets.  There are two ways of doing this, either based on idempotence
>>>>> or
>>>>> a transactional data store.
>>>>>
>>>>> For idempotent stores:
>>>>>
>>>>> 1.manipulate data
>>>>> 2.save data to store
>>>>> 3.save ending offsets to the same store
>>>>>
>>>>> If you fail between 2 and 3, the offsets haven't been stored, you start
>>>>> again at the same beginning offsets, do the same calculations in the
>>>>> same
>>>>> order, overwrite the same data, all is good.
>>>>>
>>>>>
>>>>> For transactional stores:
>>>>>
>>>>> 1. manipulate data
>>>>> 2. begin transaction
>>>>> 3. save data to the store
>>>>> 4. save offsets
>>>>> 5. commit transaction
>>>>>
>>>>> If you fail before 5, the transaction rolls back.  To make this less
>>>>> heavyweight, you can write the data outside the transaction and then
>>>>> update
>>>>> a pointer to the current data inside the transaction.
>>>>>
>>>>>
>>>>> Again, spark has nothing much to do with guaranteeing exactly once.  In
>>>>> fact, the current streaming api actively impedes my ability to do the
>>>>> above.  I'm just suggesting providing an api that doesn't get in the
>>>>> way of
>>>>> exactly-once.
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>> On Fri, Dec 19, 2014 at 3:57 PM, Hari Shreedharan <
>>>>> hshreedharan@cloudera.com<mailto:hshreedharan@cloudera.com>
>>>>> > wrote:
>>>>>
>>>>> > Can you explain your basic algorithm for the once-only-delivery? It is
>>>>> > quite a bit of very Kafka-specific code, that would take more time to
>>>>> read
>>>>> > than I can currently afford? If you can explain your algorithm a bit,
>>>>> it
>>>>> > might help.
>>>>> >
>>>>> > Thanks,
>>>>> > Hari
>>>>> >
>>>>> >
>>>>> > On Fri, Dec 19, 2014 at 1:48 PM, Cody Koeninger <cody@koeninger.org<mailto:cody@koeninger.org>>
>>>>> > wrote:
>>>>> >
>>>>> >>
>>>>> >> The problems you guys are discussing come from trying to store state
>>>>> in
>>>>> >> spark, so don't do that.  Spark isn't a distributed database.
>>>>> >>
>>>>> >> Just map kafka partitions directly to rdds, llet user code specify
>>>>> the
>>>>> >> range of offsets explicitly, and let them be in charge of committing
>>>>> >> offsets.
>>>>> >>
>>>>> >> Using the simple consumer isn't that bad, I'm already using this in
>>>>> >> production with the code I linked to, and tresata apparently has
>>>>> been as
>>>>> >> well.  Again, for everyone saying this is impossible, have you read
>>>>> either
>>>>> >> of those implementations and looked at the approach?
>>>>> >>
>>>>> >>
>>>>> >>
>>>>> >> On Fri, Dec 19, 2014 at 2:27 PM, Sean McNamara <
>>>>> >> Sean.McNamara@webtrends.com<mailto:Sean.McNamara@webtrends.com>> wrote:
>>>>> >>
>>>>> >>> Please feel free to correct me if I’m wrong, but I think the exactly
>>>>> >>> once spark streaming semantics can easily be solved using
>>>>> updateStateByKey.
>>>>> >>> Make the key going into updateStateByKey be a hash of the event, or
>>>>> pluck
>>>>> >>> off some uuid from the message.  The updateFunc would only emit the
>>>>> message
>>>>> >>> if the key did not exist, and the user has complete control over
>>>>> the window
>>>>> >>> of time / state lifecycle for detecting duplicates.  It also makes
>>>>> it
>>>>> >>> really easy to detect and take action (alert?) when you DO see a
>>>>> duplicate,
>>>>> >>> or make memory tradeoffs within an error bound using a sketch
>>>>> algorithm.
>>>>> >>> The kafka simple consumer is insanely complex, if possible I think
>>>>> it would
>>>>> >>> be better (and vastly more flexible) to get reliability using the
>>>>> >>> primitives that spark so elegantly provides.
>>>>> >>>
>>>>> >>> Cheers,
>>>>> >>>
>>>>> >>> Sean
>>>>> >>>
>>>>> >>>
>>>>> >>> > On Dec 19, 2014, at 12:06 PM, Hari Shreedharan <
>>>>> >>> hshreedharan@cloudera.com<mailto:hshreedharan@cloudera.com>> wrote:
>>>>> >>> >
>>>>> >>> > Hi Dibyendu,
>>>>> >>> >
>>>>> >>> > Thanks for the details on the implementation. But I still do not
>>>>> >>> believe
>>>>> >>> > that it is no duplicates - what they achieve is that the same
>>>>> batch is
>>>>> >>> > processed exactly the same way every time (but see it may be
>>>>> processed
>>>>> >>> more
>>>>> >>> > than once) - so it depends on the operation being idempotent. I
>>>>> believe
>>>>> >>> > Trident uses ZK to keep track of the transactions - a batch can be
>>>>> >>> > processed multiple times in failure scenarios (for example, the
>>>>> >>> transaction
>>>>> >>> > is processed but before ZK is updated the machine fails, causing a
>>>>> >>> ""new""
>>>>> >>> > node to process it again).
>>>>> >>> >
>>>>> >>> > I don't think it is impossible to do this in Spark Streaming as
>>>>> well
>>>>> >>> and
>>>>> >>> > I'd be really interested in working on it at some point in the
>>>>> near
>>>>> >>> future.
>>>>> >>> >
>>>>> >>> > On Fri, Dec 19, 2014 at 1:44 AM, Dibyendu Bhattacharya <
>>>>> >>> > dibyendu.bhattachary@gmail.com<mailto:dibyendu.bhattachary@gmail.com>> wrote:
>>>>> >>> >
>>>>> >>> >> Hi,
>>>>> >>> >>
>>>>> >>> >> Thanks to Jerry for mentioning the Kafka Spout for Trident. The
>>>>> Storm
>>>>> >>> >> Trident has done the exact-once guarantee by processing the
>>>>> tuple in a
>>>>> >>> >> batch  and assigning same transaction-id for a given batch . The
>>>>> >>> replay for
>>>>> >>> >> a given batch with a transaction-id will have exact same set of
>>>>> >>> tuples and
>>>>> >>> >> replay of batches happen in exact same order before the failure.
>>>>> >>> >>
>>>>> >>> >> Having this paradigm, if downstream system process data for a
>>>>> given
>>>>> >>> batch
>>>>> >>> >> for having a given transaction-id , and if during failure if same
>>>>> >>> batch is
>>>>> >>> >> again emitted , you can check if same transaction-id is already
>>>>> >>> processed
>>>>> >>> >> or not and hence can guarantee exact once semantics.
>>>>> >>> >>
>>>>> >>> >> And this can only be achieved in Spark if we use Low Level Kafka
>>>>> >>> consumer
>>>>> >>> >> API to process the offsets. This low level Kafka Consumer (
>>>>> >>> >> https://github.com/dibbhatt/kafka-spark-consumer) has
>>>>> implemented the
>>>>> >>> >> Spark Kafka consumer which uses Kafka Low Level APIs . All of the
>>>>> >>> Kafka
>>>>> >>> >> related logic has been taken from Storm-Kafka spout and which
>>>>> manages
>>>>> >>> all
>>>>> >>> >> Kafka re-balance and fault tolerant aspects and Kafka metadata
>>>>> >>> managements.
>>>>> >>> >>
>>>>> >>> >> Presently this Consumer maintains that during Receiver failure,
>>>>> it
>>>>> >>> will
>>>>> >>> >> re-emit the exact same Block with same set of messages . Every
>>>>> >>> message have
>>>>> >>> >> the details of its partition, offset and topic related details
>>>>> which
>>>>> >>> can
>>>>> >>> >> tackle the SPARK-3146.
>>>>> >>> >>
>>>>> >>> >> As this Low Level consumer has complete control over the Kafka
>>>>> >>> Offsets ,
>>>>> >>> >> we can implement Trident like feature on top of it like having
>>>>> >>> implement a
>>>>> >>> >> transaction-id for a given block , and re-emit the same block
>>>>> with
>>>>> >>> same set
>>>>> >>> >> of message during Driver failure.
>>>>> >>> >>
>>>>> >>> >> Regards,
>>>>> >>> >> Dibyendu
>>>>> >>> >>
>>>>> >>> >>
>>>>> >>> >> On Fri, Dec 19, 2014 at 7:33 AM, Shao, Saisa.com>>
>>>>> >>> >> wrote:
>>>>> >>> >>>
>>>>> >>> >>> Hi all,
>>>>> >>> >>>
>>>>> >>> >>> I agree with Hari that Strong exact-once semantics is very hard
>>>>> to
>>>>> >>> >>> guarantee, especially in the failure situation. From my
>>>>> >>> understanding even
>>>>> >>> >>> current implementation of ReliableKafkaReceiver cannot fully
>>>>> >>> guarantee the
>>>>> >>> >>> exact once semantics once failed, first is the ordering of data
>>>>> >>> replaying
>>>>> >>> >>> from last checkpoint, this is hard to guarantee when multiple
>>>>> >>> partitions
>>>>> >>> >>> are injected in; second is the design complexity of achieving
>>>>> this,
>>>>> >>> you can
>>>>> >>> >>> refer to the Kafka Spout in Trident, we have to dig into the
>>>>> very
>>>>> >>> details
>>>>> >>> >>> of Kafka metadata management system to achieve this, not to say
>>>>> >>> rebalance
>>>>> >>> >>> and fault-tolerance.
>>>>> >>> >>>
>>>>> >>> >>> Thanks
>>>>> >>> >>> Jerry
>>>>> >>> >>>
>>>>> >>> >>> -----Original Message-----
>>>>> >>> >>> From: Luis Ángel Vicente Sánchez [mailto:
>>>>> langel.groups@gmail.com<mailto:langel.groups@gmail.com>]
>>>>> >>> >>> Sent: Friday, December 19, 2014 5:57 AM
>>>>> >>> >>> To: Cody Koeninger
>>>>> >>> >>> Cc: Hari Shreedharan; Patrick Wendell; dev@spark.apache.org<mailto:dev@spark.apache.org>
>>>>> >>> >>> Subject: Re: Which committers care about Kafka?
>>>>> >>> >>>
>>>>> >>> >>> But idempotency is not that easy t achieve sometimes. A strong
>>>>> only
>>>>> >>> once
>>>>> >>> >>> semantic through a proper API would  be superuseful; but I'm not
>>>>> >>> implying
>>>>> >>> >>> this is easy to achieve.
>>>>> >>> >>> On 18 Dec 2014 21:52, ""Cody Koeninger"" <cody@koeninger.org<mailto:cody@koeninger.org>>
>>>>> wrote:
>>>>> >>> >>>
>>>>> >>> >>>> If the downstream store for the output data is idempotent or
>>>>> >>> >>>> transactional, and that downstream store also is the system of
>>>>> >>> record
>>>>> >>> >>>> for kafka offsets, then you have exactly-once semantics.
>>>>> Commit
>>>>> >>> >>>> offsets with / after the data is stored.  On any failure,
>>>>> restart
>>>>> >>> from
>>>>> >>> >>> the last committed offsets.
>>>>> >>> >>>>
>>>>> >>> >>>> Yes, this approach is biased towards the etl-like use cases
>>>>> rather
>>>>> >>> >>>> than near-realtime-analytics use cases.
>>>>> >>> >>>>
>>>>> >>> >>>> On Thu, Dec 18, 2014 at 3:27 PM, Hari Shreedharan <
>>>>> >>> >>>> hshreedharan@cloudera.com<mailto:hshreedharan@cloudera.com>
>>>>> >>> >>>>> wrote:
>>>>> >>> >>>>>
>>>>> >>> >>>>> I get what you are saying. But getting exactly once right is
>>>>> an
>>>>> >>> >>>>> extremely hard problem - especially in presence of failure.
>>>>> The
>>>>> >>> >>>>> issue is failures
>>>>> >>> >>>> can
>>>>> >>> >>>>> happen in a bunch of places. For example, before the
>>>>> notification
>>>>> >>> of
>>>>> >>> >>>>> downstream store being successful reaches the receiver that
>>>>> updates
>>>>> >>> >>>>> the offsets, the node fails. The store was successful, but
>>>>> >>> >>>>> duplicates came in either way. This is something worth
>>>>> discussing
>>>>> >>> by
>>>>> >>> >>>>> itself - but without uuids etc this might not really be
>>>>> solved even
>>>>> >>> >>> when you think it is.
>>>>> >>> >>>>>
>>>>> >>> >>>>> Anyway, I will look at the links. Even I am interested in all
>>>>> of
>>>>> >>> the
>>>>> >>> >>>>> features you mentioned - no HDFS WAL for Kafka and once-only
>>>>> >>> >>>>> delivery,
>>>>> >>> >>>> but
>>>>> >>> >>>>> I doubt the latter is really possible to guarantee - though I
>>>>> >>> really
>>>>> >>> >>>> would
>>>>> >>> >>>>> love to have that!
>>>>> >>> >>>>>
>>>>> >>> >>>>> Thanks,
>>>>> >>> >>>>> Hari
>>>>> >>> >>>>>
>>>>> >>> >>>>>
>>>>> >>> >>>>> On Thu, Dec 18, 2014 at 12:26 PM, Cody Koeninger
>>>>> >>> >>>>> <cody@koeninger.org<mailto:cody@koeninger.org>>
>>>>> >>> >>>>> wrote:
>>>>> >>> >>>>>
>>>>> >>> >>>>>> Thanks for the replies.
>>>>> >>> >>>>>>
>>>>> >>> >>>>>> Regarding skipping WAL, it's not just about optimization.
>>>>> If you
>>>>> >>> >>>>>> actually want exactly-once semantics, you need control of
>>>>> kafka
>>>>> >>> >>>>>> offsets
>>>>> >>> >>>> as
>>>>> >>> >>>>>> well, including the ability to not use zookeeper as the
>>>>> system of
>>>>> >>> >>>>>> record for offsets.  Kafka already is a reliable system that
>>>>> has
>>>>> >>> >>>>>> strong
>>>>> >>> >>>> ordering
>>>>> >>> >>>>>> guarantees (within a partition) and does not mandate the use
>>>>> of
>>>>> >>> >>>> zookeeper
>>>>> >>> >>>>>> to store offsets.  I think there should be a spark api that
>>>>> acts
>>>>> >>> as
>>>>> >>> >>>>>> a
>>>>> >>> >>>> very
>>>>> >>> >>>>>> simple intermediary between Kafka and the user's choice of
>>>>> >>> >>>>>> downstream
>>>>> >>> >>>> store.
>>>>> >>> >>>>>>
>>>>> >>> >>>>>> Take a look at the links I posted - if there's already been 2
>>>>> >>> >>>> independent
>>>>> >>> >>>>>> implementations of the idea, chances are it's something
>>>>> people
>>>>> >>> need.
>>>>> >>> >>>>>>
>>>>> >>> >>>>>> On Thu, Dec 18, 2014 at 1:44 PM, Hari Shreedharan <
>>>>> >>> >>>>>> hshreedharan@cloudera.com<mailto:hshreedharan@cloudera.com>> wrote:
>>>>> >>> >>>>>>>
>>>>> >>> >>>>>>> Hi Cody,
>>>>> >>> >>>>>>>
>>>>> >>> >>>>>>> I am an absolute +1 on SPARK-3146. I think we can implement
>>>>> >>> >>>>>>> something pretty simple and lightweight for that one.
>>>>> >>> >>>>>>>
>>>>> >>> >>>>>>> For the Kafka DStream skipping the WAL implementation -
>>>>> this is
>>>>> >>> >>>>>>> something I discussed with TD a few weeks ago. Though it is
>>>>> a
>>>>> >>> good
>>>>> >>> >>>> idea to
>>>>> >>> >>>>>>> implement this to avoid unnecessary HDFS writes, it is an
>>>>> >>> >>>> optimization. For
>>>>> >>> >>>>>>> that reason, we must be careful in implementation. There
>>>>> are a
>>>>> >>> >>>>>>> couple
>>>>> >>> >>>> of
>>>>> >>> >>>>>>> issues that we need to ensure works properly - specifically
>>>>> >>> >>> ordering.
>>>>> >>> >>>> To
>>>>> >>> >>>>>>> ensure we pull messages from different topics and
>>>>> partitions in
>>>>> >>> >>>>>>> the
>>>>> >>> >>>> same
>>>>> >>> >>>>>>> order after failure, we’d still have to persist the
>>>>> metadata to
>>>>> >>> >>>>>>> HDFS
>>>>> >>> >>>> (or
>>>>> >>> >>>>>>> some other system) - this metadata must contain the order of
>>>>> >>> >>>>>>> messages consumed, so we know how to re-read the messages.
>>>>> I am
>>>>> >>> >>>>>>> planning to
>>>>> >>> >>>> explore
>>>>> >>> >>>>>>> this once I have some time (probably in Jan). In addition,
>>>>> we
>>>>> >>> must
>>>>> >>> >>>>>>> also ensure bucketing functions work fine as well. I will
>>>>> file a
>>>>> >>> >>>>>>> placeholder jira for this one.
>>>>> >>> >>>>>>>
>>>>> >>> >>>>>>> I also wrote an API to write data back to Kafka a while
>>>>> back -
>>>>> >>> >>>>>>> https://github.com/apache/spark/pull/2994 . I am hoping
>>>>> that
>>>>> >>> this
>>>>> >>> >>>>>>> will get pulled in soon, as this is something I know people
>>>>> want.
>>>>> >>> >>>>>>> I am open
>>>>> >>> >>>> to
>>>>> >>> >>>>>>> feedback on that - anything that I can do to make it better.
>>>>> >>> >>>>>>>
>>>>> >>> >>>>>>> Thanks,
>>>>> >>> >>>>>>> Hari
>>>>> >>> >>>>>>>
>>>>> >>> >>>>>>>
>>>>> >>> >>>>>>> On Thu, Dec 18, 2014 at 11:14 AM, Patrick Wendell
>>>>> >>> >>>>>>> <pwendell@gmail.com<mailto:pwendell@gmail.com>>
>>>>> >>> >>>>>>> wrote:
>>>>> >>> >>>>>>>
>>>>> >>> >>>>>>>> Hey Cody,
>>>>> >>> >>>>>>>>
>>>>> >>> >>>>>>>> Thanks for reaching out with this. The lead on streaming
>>>>> is TD -
>>>>> >>> >>>>>>>> he is traveling this week though so I can respond a bit.
>>>>> To the
>>>>> >>> >>>>>>>> high level point of whether Kafka is important - it
>>>>> definitely
>>>>> >>> >>>>>>>> is. Something like 80% of Spark Streaming deployments
>>>>> >>> >>>>>>>> (anecdotally) ingest data from Kafka. Also, good support
>>>>> for
>>>>> >>> >>>>>>>> Kafka is something we generally want in Spark and not a
>>>>> library.
>>>>> >>> >>>>>>>> In some cases IIRC there were user libraries that used
>>>>> unstable
>>>>> >>> >>>>>>>> Kafka API's and we were somewhat waiting on Kafka to
>>>>> stabilize
>>>>> >>> >>>>>>>> them to merge things upstream. Otherwise users wouldn't be
>>>>> able
>>>>> >>> >>>>>>>> to use newer Kakfa versions. This is a high level
>>>>> impression
>>>>> >>> only
>>>>> >>> >>>>>>>> though, I haven't talked to TD about this recently so it's
>>>>> worth
>>>>> >>> >>> revisiting given the developments in Kafka.
>>>>> >>> >>>>>>>>
>>>>> >>> >>>>>>>> Please do bring things up like this on the dev list if
>>>>> there are
>>>>> >>> >>>>>>>> blockers for your usage - thanks for pinging it.
>>>>> >>> >>>>>>>>
>>>>> >>> >>>>>>>> - Patrick
>>>>> >>> >>>>>>>>
>>>>> >>> >>>>>>>> On Thu, Dec 18, 2014 at 7:07 AM, Cody Koeninger
>>>>> >>> >>>>>>>> <cody@koeninger.org<mailto:cody@koeninger.org>>
>>>>> >>> >>>>>>>> wrote:
>>>>> >>> >>>>>>>>> Now that 1.2 is finalized... who are the go-to people to
>>>>> get
>>>>> >>> >>>>>>>>> some long-standing Kafka related issues resolved?
>>>>> >>> >>>>>>>>>
>>>>> >>> >>>>>>>>> The existing api is not sufficiently safe nor flexible
>>>>> for our
>>>>> >>> >>>>>>>> production
>>>>> >>> >>>>>>>>> use. I don't think we're alone in this viewpoint, because
>>>>> I've
>>>>> >>> >>>>>>>>> seen several different patches and libraries to fix the
>>>>> same
>>>>> >>> >>>>>>>>> things we've
>>>>> >>> >>>>>>>> been
>>>>> >>> >>>>>>>>> running into.
>>>>> >>> >>>>>>>>>
>>>>> >>> >>>>>>>>> Regarding flexibility
>>>>> >>> >>>>>>>>>
>>>>> >>> >>>>>>>>> https://issues.apache.org/jira/browse/SPARK-3146
>>>>> >>> >>>>>>>>>
>>>>> >>> >>>>>>>>> has been outstanding since August, and IMHO an equivalent
>>>>> of
>>>>> >>> >>>>>>>>> this is absolutely necessary. We wrote a similar patch
>>>>> >>> >>>>>>>>> ourselves, then found
>>>>> >>> >>>>>>>> that
>>>>> >>> >>>>>>>>> PR and have been running it in production. We wouldn't be
>>>>> able
>>>>> >>> >>>>>>>>> to
>>>>> >>> >>>> get
>>>>> >>> >>>>>>>> our
>>>>> >>> >>>>>>>>> jobs done without it. It also allows users to solve a
>>>>> whole
>>>>> >>> >>>>>>>>> class of problems for themselves (e.g. SPARK-2388,
>>>>> arbitrary
>>>>> >>> >>>>>>>>> delay of
>>>>> >>> >>>>>>>> messages, etc).
>>>>> >>> >>>>>>>>>
>>>>> >>> >>>>>>>>> Regarding safety, I understand the motivation behind
>>>>> >>> >>>>>>>>> WriteAheadLog
>>>>> >>> >>>> as
>>>>> >>> >>>>>>>> a
>>>>> >>> >>>>>>>>> general solution for streaming unreliable sources, but
>>>>> Kafka
>>>>> >>> >>>>>>>>> already
>>>>> >>> >>>>>>>> is a
>>>>> >>> >>>>>>>>> reliable source. I think there's a need for an api that
>>>>> treats
>>>>> >>> >>>>>>>>> it as such. Even aside from the performance issues of
>>>>> >>> >>>>>>>>> duplicating the write-ahead log in kafka into another
>>>>> >>> >>>>>>>>> write-ahead log in hdfs, I
>>>>> >>> >>>> need
>>>>> >>> >>>>>>>>> exactly-once semantics in the face of failure (I've had
>>>>> >>> >>>>>>>>> failures
>>>>> >>> >>>> that
>>>>> >>> >>>>>>>>> prevented reloading a spark streaming checkpoint, for
>>>>> >>> instance).
>>>>> >>> >>>>>>>>>
>>>>> >>> >>>>>>>>> I've got an implementation i've been using
>>>>> >>> >>>>>>>>>
>>>>> >>> >>>>>>>>>
>>>>> >>> https://github.com/koeninger/spark-1/tree/kafkaRdd/external/kaf
>>>>> >>> >>>>>>>>> ka /src/main/scala/org/apache/spark/rdd/kafka
>>>>> >>> >>>>>>>>>
>>>>> >>> >>>>>>>>> Tresata has something similar at
>>>>> >>> >>>>>>>> https://github.com/tresata/spark-kafka,
>>>>> >>> >>>>>>>>> and I know there were earlier attempts based on Storm
>>>>> code.
>>>>> >>> >>>>>>>>>
>>>>> >>> >>>>>>>>> Trying to distribute these kinds of fixes as libraries
>>>>> rather
>>>>> >>> >>>>>>>>> than
>>>>> >>> >>>>>>>> patches
>>>>> >>> >>>>>>>>> to Spark is problematic, because large portions of the
>>>>> >>> >>>> implementation
>>>>> >>> >>>>>>>> are
>>>>> >>> >>>>>>>>> private[spark].
>>>>> >>> >>>>>>>>>
>>>>> >>> >>>>>>>>> I'd like to help, but i need to know whose attention to
>>>>> get.
>>>>> >>> >>>>>>>>
>>>>> >>> >>>>>>>>
>>>>> >>> -----------------------------------------------------------------
>>>>> >>> >>>>>>>> ---- To unsubscribe, e-mail:
>>>>> dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org>
>>>>> >>> For
>>>>> >>> >>>>>>>> additional commands, e-mail: dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>
>>>>> >>> >>>>>>>>
>>>>> >>> >>>>>>>>
>>>>> >>> >>>>>>>
>>>>> >>> >>>>>
>>>>> >>> >>>>
>>>>> >>> >>>
>>>>> >>> >>
>>>>> >>>
>>>>> >>>
>>>>> >>
>>>>> >
>>>>>
>>>>
>>>
>>

"
Michael Armbrust <michael@databricks.com>,"Mon, 29 Dec 2014 18:42:57 -0800",Re: Unsupported Catalyst types in Parquet,Alessandro Baretta <alexbaretta@gmail.com>,"Yeah, I saw those.  The problem is that #3822 truncates timestamps that
include nanoseconds.


"
Jeremy Freeman <freeman.jeremy@gmail.com>,"Mon, 29 Dec 2014 22:39:57 -0500",Re: Adding third party jars to classpath used by pyspark,Stephen Boesch <javadba@gmail.com>,"Hi Stephen, it should be enough to include 


in the command line call to either pyspark or spark-submit, as in


and you can check the bottom of the Web UIs Environment"" tab to make sure the jar gets on your classpath. Let me know if you still see errors related to this.

 Jeremy

-------------------------
jeremyfreeman.net
@thefreemanlab


Trying
libraries may

"
evil <qinggangwang7@gmail.com>,"Mon, 29 Dec 2014 20:29:54 -0700 (MST)",A question about using insert into in rdd foreach in spark 1.2,dev@spark.apache.org,"Hi All,
I have a  problem when I try to use insert into in loop, and this is my code  
def main(args: Array[String]) {
    //This is an empty table, schema is (Int,String)
   
sqlContext.parquetFile(""Data\\Test\\Parquet\\Temp"").registerTempTable(""temp"")
    //not empty table,  schema is (Int,String)
    val testData = sqlContext.parquetFile(""Data\\Test\\Parquet\\2"")
    testData.foreach{x=>
      sqlContext.sql(""INSERT INTO temp SELECT ""+x(0)+"" ,'""+x(1)+""'"")
    }
    sqlContext.sql(""select * from
temp"").collect().map(x=>(x(0),x(1))).foreach(println)
  }

when I run the code above in local mode, it will not stop and do not have
error log. The lastest log is as follows:
14/12/30 11:07:44 WARN ParquetRecordReader: Can not initialize counter due
to context is not a instance of TaskInputOutputContext, but is
org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
14/12/30 11:07:44 INFO InternalParquetRecordReader: RecordReader initialized
will read a total of 200 records.
14/12/30 11:07:44 INFO InternalParquetRecordReader: at row 0. reading next
block
14/12/30 11:07:44 INFO CodecPool: Got brand-new decompressor [.gz]
14/12/30 11:07:44 INFO InternalParquetRecordReader: block read in memory in
20 ms. row count = 200
14/12/30 11:07:45 INFO SparkContext: Starting job: runJob at
ParquetTableOperations.scala:325
14/12/30 11:07:45 INFO DAGScheduler: Got job 1 (runJob at
ParquetTableOperations.scala:325) with 1 output partitions
(allowLocal=false)
14/12/30 11:07:45 INFO DAGScheduler: Final stage: Stage 1(runJob at
ParquetTableOperations.scala:325)
14/12/30 11:07:45 INFO DAGScheduler: Parents of final stage: List()
14/12/30 11:07:45 INFO DAGScheduler: Missing parents: List()
14/12/30 11:07:45 INFO DAGScheduler: Submitting Stage 1 (MapPartitionsRDD[6]
at mapPartitions at basicOperators.scala:43), which has no missing parents
14/12/30 11:07:45 INFO MemoryStore: ensureFreeSpace(53328) called with
curMem=239241, maxMem=1013836677
14/12/30 11:07:45 INFO MemoryStore: Block broadcast_2 stored as values in
memory (estimated size 52.1 KB, free 966.6 MB)
14/12/30 11:07:45 INFO MemoryStore: ensureFreeSpace(31730) called with
curMem=292569, maxMem=1013836677
14/12/30 11:07:45 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes
in memory (estimated size 31.0 KB, free 966.6 MB)
14/12/30 11:07:45 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory
on localhost:52533 (size: 31.0 KB, free: 966.8 MB)
14/12/30 11:07:45 INFO BlockManagerMaster: Updated info of block
broadcast_2_piece0
14/12/30 11:07:45 INFO SparkContext: Created broadcast 2 from broadcast at
DAGScheduler.scala:838
14/12/30 11:07:45 INFO DAGScheduler: Submitting 1 missing tasks from Stage 1
(MapPartitionsRDD[6] at mapPartitions at basicOperators.scala:43)
14/12/30 11:07:45 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks

Can anyone give me a hand?

Thanks
evil



--

---------------------------------------------------------------------


"
guoxu1231 <guoxu1231@gmail.com>,"Mon, 29 Dec 2014 21:04:09 -0700 (MST)","Help,  pyspark.sql.List flatMap results become tuple",dev@spark.apache.org,"Hi pyspark guys, 

I have a json file, and its struct like below:

{""NAME"":""George"", ""AGE"":35, ""ADD_ID"":1212, ""POSTAL_AREA"":1,
""TIME_ZONE_ID"":1, ""INTEREST"":[{""INTEREST_NO"":1, ""INFO"":""x""},
{""INTEREST_NO"":2, ""INFO"":""y""}]}
{""NAME"":""John"", ""AGE"":45, ""ADD_ID"":1213, ""POSTAL_AREA"":1, ""TIME_ZONE_ID"":1,
""INTEREST"":[{""INTEREST_NO"":2, ""INFO"":""x""}, {""INTEREST_NO"":3, ""INFO"":""y""}]}

I'm using spark sql api to manipulate the json data in pyspark shell, 

*sqlContext = SQLContext(sc)
A400= sqlContext.jsonFile('jason_file_path')*
/Row(ADD_ID=1212, AGE=35, INTEREST=[Row(INFO=u'x', INTEREST_NO=1),
Row(INFO=u'y', INTEREST_NO=2)], NAME=u'George', POSTAL_AREA=1,
TIME_ZONE_ID=1)
Row(ADD_ID=1213, AGE=45, INTEREST=[Row(INFO=u'x', INTEREST_NO=2),
Row(INFO=u'y', INTEREST_NO=3)], NAME=u'John', POSTAL_AREA=1,
TIME_ZONE_ID=1)/
*X = A400.flatMap(lambda i: i.INTEREST)*
The flatMap results like below, each element in json array were flatten to
tuple, not my expected  pyspark.sql.Row. I can only access the flatten
results by index. but it supposed to be flatten to Row(namedTuple) and
support to access by name.
(u'x', 1)
(u'y', 2)
(u'x', 2)
(u'y', 3)

My spark version is 1.1.







--

---------------------------------------------------------------------


"
guoxu1231 <guoxu1231@gmail.com>,"Mon, 29 Dec 2014 22:41:28 -0700 (MST)","Re: Help,  pyspark.sql.List flatMap results become tuple",dev@spark.apache.org,"named tuple degenerate to tuple. 
*A400.map(lambda i: map(None,i.INTEREST))*
===============================
[(u'x', 1), (u'y', 2)]
[(u'x', 2), (u'y', 3)]



--

---------------------------------------------------------------------


"
Cody Koeninger <cody@koeninger.org>,"Mon, 29 Dec 2014 23:43:20 -0600",Re: Which committers care about Kafka?,"""Shao, Saisai"" <saisai.shao@intel.com>","Assuming you're talking about spark.streaming.receiver.maxRate, I just
updated my PR to configure rate limiting based on that setting.  So
hopefully that's issue 1 sorted.

Regarding issue 3, as far as I can tell regarding the odd semantics of
stateful or windowed operations in the face of failure, my solution is no
worse than existing classes such as FileStream that use inputdstream
directly rather than a receiver.  Can we get some specific cases that are a
concern?

Regarding the WAL solutions TD mentioned, one of the disadvantages of them
is that they rely on checkpointing, unlike my approach.  As I noted in this
thread and in the jira ticket, I need something that can recover even when
a checkpoint is lost, and I've already seen multiple situations in
production where a checkpoint cannot be recovered (e.g. because code needs
to be updated).

:

t
ache.org;
n
of
g
d
d
e
,
e
s
n
he
e
e
fy
k the
,
r
a
he
 I
he
as
e
a
f
h
ta
ry
ls
ka
ng
k
y
le
ng
e
m
or
m
s
e.
at
t
.
f
of
t
an
e
 the
ll
ng
t.
rt
a
t
e
he
h
t
t
at
ad
s
e
to
--
"
danqing0703 <danqing0703@berkeley.edu>,"Mon, 29 Dec 2014 23:12:47 -0700 (MST)","Problems concerning implementing machine learning algorithm from
 scratch based on Spark",dev@spark.apache.org,"Hi all,

I am trying to use some machine learning algorithms that are not included
in the Mllib. Like Mixture Model and LDA(Latent Dirichlet Allocation), and
I am using pyspark and Spark SQL.

My problem is: I have some scripts that implement these algorithms, but I
am not sure which part I shall change to make it fit into Big Data.

   - Like some very simple calculation may take much time if data is too
   big,but also constructing RDD or SQLContext table takes too much time. I am
   really not sure if I shall use map(), reduce() every time I need to make
   calculation.
   - Also, there are some matrix/array level calculation that can not be
   implemented easily merely using map(),reduce(), thus functions of the Numpy
   package shall be used. I am not sure when data is too big, and we simply
   use the numpy functions. Will it take too much time?

I have found some scripts that are not from Mllib and was created by other
developers(credits to Meethu Mathew from Flytxt, thanks for giving me
insights!:))

Many thanks and look forward to getting feedbacks!

Best, Danqing


GMMSpark.py (7K) <http://apache-spark-developers-list.1001551.n3.nabble.com/attachment/9964/0/GMMSpark.py>




--"
"""Wang, Daoyuan"" <daoyuan.wang@intel.com>","Tue, 30 Dec 2014 06:20:07 +0000",RE: Unsupported Catalyst types in Parquet,"Michael Armbrust <michael@databricks.com>, Alessandro Baretta
	<alexbaretta@gmail.com>","By adding a flag in SQLContext, I have modified #3822 to include nanoseconds now. Since passing too many flags is ugly, now I need the whole SQLContext, so that we can put more flags there.

Thanks,
Daoyuan

From: Michael Armbrust [mailto:michael@databricks.com]
Sent: Tuesday, December 30, 2014 10:43 AM
To: Alessandro Baretta
Cc: Wang, Daoyuan; dev@spark.apache.org
Subject: Re: Unsupported Catalyst types in Parquet

Yeah, I saw those.  The problem is that #3822 truncates timestamps that include nanoseconds.

On Mon, Dec 29, 2014 at 5:14 PM, Alessandro Baretta <alexbaretta@gmail.com<mailto:alexbaretta@gmail.com>> wrote:
Michael,

Actually, Adrian Wang already created pull requests for these issues.

https://github.com/apache/spark/pull/3820
https://github.com/apache/spark/pull/3822

What do you think?

Alex

On Mon, Dec 29, 2014 at 3:07 PM, Michael Armbrust <michael@databricks.com<mailto:michael@databricks.com>> wrote:
I'd love to get both of these in.  There is some trickiness that I talk about on the JIRA for timestamps since the SQL timestamp class can support nano seconds and I don't think parquet has a type for this.  Other systems (impala) seem to use INT96.  It would be great to maybe ask on the parquet mailing list what the plan is there to make sure that whatever we do is going to be compatible long term.

Michael

On Mon, Dec 29, 2014 at 8:13 AM, Alessandro Baretta <alexbaretta@gmail.com<mailto:alexbaretta@gmail.com>> wrote:

Daoyuan,

Thanks for creating the jiras. I need these features by... last week, so I'd be happy to take care of this myself, if only you or someone more experienced than me in the SparkSQL codebase could provide some guidance.

Alex
On Dec 29, 2014 12:06 AM, ""Wang, Daoyuan"" <daoyuan.wang@intel.com<mailto:daoyuan.wang@intel.com>> wrote:
Hi Alex,

I'll create JIRA SPARK-4985 for date type support in parquet, and SPARK-4987 for timestamp type support. For decimal type, I think we only support decimals that fits in a long.

Thanks,
Daoyuan

-----Original Message-----
From: Alessandro Baretta [mailto:alexbaretta@gmail.com<mailto:alexbaretta@gmail.com>]
Sent: Saturday, December 27, 2014 2:47 PM
To: dev@spark.apache.org<mailto:dev@spark.apache.org>; Michael Armbrust
Subject: Unsupported Catalyst types in Parquet

Michael,

I'm having trouble storing my SchemaRDDs in Parquet format with SparkSQL, due to my RDDs having having DateType and DecimalType fields. What would it take to add Parquet support for these Catalyst? Are there any other Catalyst types for which there is no Catalyst support?

Alex



"
MEETHU MATHEW <meethu2006@yahoo.co.in>,"Tue, 30 Dec 2014 10:41:06 +0000 (UTC)","Re: Problems concerning implementing machine learning algorithm
 from scratch based on Spark","danqing0703 <danqing0703@berkeley.edu>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,
The GMMSpark.py you mentioned is the old one.The new code is now added to spark-packages and is available at http://spark-packages.org/package/11 . Have a look at the new code.
We have used numpy functions in our code and didnt notice any slowdown because of this. Thanks & Regards,
Meethu M 

   

 Hi all,

I am trying to use some machine learning algorithms that are not included
in the Mllib. Like Mixture Model and LDA(Latent Dirichlet Allocation), and
I am using pyspark and Spark SQL.

My problem is: I have some scripts that implement these algorithms, but I
am not sure which part I shall change to make it fit into Big Data.

  - Like some very simple calculation may take much time if data is too
  big,but also constructing RDD or SQLContext table takes too much time. I am
  really not sure if I shall use map(), reduce() every time I need to make
  calculation.
  - Also, there are some matrix/array level calculation that can not be
  implemented easily merely using map(),reduce(), thus functions of the Numpy
  package shall be used. I am not sure when data is too big, and we simply
  use the numpy functions. Will it take too much time?

I have found some scripts that are not from Mllib and was created by other
developers(credits to Meethu Mathew from Flytxt, thanks for giving me
insights!:))

Many thanks and look forward to getting feedbacks!

Best, Danqing


GMMSpark.py (7K) <http://apache-spark-developers-list.1001551.n3.nabble.com/attachment/9964/0/GMMSpark.py>




--
3.nabble.com/Problems-concerning-implementing-machine-learning-algorithm-from-scratch-based-on-Spark-tp9964.html
om.

   "
Rahul Kavale <kavale.rahul@gmail.com>,"Tue, 30 Dec 2014 18:43:05 +0530",[Documentation] Adding external blog to documentation,dev@spark.apache.org,"Hi all,
I recently wrote a blog comparing MapReduce model with that of Apache Spark
trying to explain some important question I think a beginner might have
while exploring Spark.
The blog can be found here:
http://rahulkavale.github.io/blog/2014/11/16/scrap-your-map-reduce/

The blog received quite good audience and I think it might help people who
are just starting to explore Spark and might help them to get their
question cleared regarding comparison of MapReduce model with that of Spark.

I would like to reach it to a broader audience by including it in official
documentation at ""External blogs"" section at
https://spark.apache.org/documentation.html. I would really like to get
your thoughts on the same.

Thanks & Regards,
Rahul Kavale
"
eshioji <eshioji@gmail.com>,"Tue, 30 Dec 2014 07:32:43 -0700 (MST)",Re: Registering custom metrics,dev@spark.apache.org,"Hi,

Did you find a way to do this / working on this?
Am trying to find a way to do this as well, but haven't been able to find a
way.



--

---------------------------------------------------------------------


"
Cody Koeninger <cody@koeninger.org>,"Tue, 30 Dec 2014 14:24:45 -0600",Is there any way to tell if compute is being called from a retry?,"""dev@spark.apache.org"" <dev@spark.apache.org>","It looks like taskContext.attemptId doesn't mean what one thinks it might
mean, based on

http://apache-spark-developers-list.1001551.n3.nabble.com/Get-attempt-number-in-a-closure-td8853.html

and the unresolved

https://issues.apache.org/jira/browse/SPARK-4014



Is there any alternative way to tell if compute is being called from a
retry?  Barring that, does anyone have any tips on how it might be possible
to get the attempt count propagated to executors?

It would be extremely useful for the kafka rdd preferred location awareness.
"
Josh Rosen <rosenville@gmail.com>,"Tue, 30 Dec 2014 14:13:43 -0800",Re: Is there any way to tell if compute is being called from a retry?,Cody Koeninger <cody@koeninger.org>,"This is timely, since I just ran into this issue myself while trying to
write a test to reproduce a bug related to speculative execution (I wanted
to configure a job so that the first attempt to compute a partition would
run slow so that a second, fast speculative copy would be launched).

I've opened a PR with a proposed fix:
https://github.com/apache/spark/pull/3849




"
Alessandro Baretta <alexbaretta@gmail.com>,"Tue, 30 Dec 2014 16:39:00 -0800",Re: Unsupported Catalyst types in Parquet,"""Wang, Daoyuan"" <daoyuan.wang@intel.com>","Gents,

I tried #3820. It doesn't work. I'm still getting the following exceptions:

Exception in thread ""Thread-45"" java.lang.RuntimeException: Unsupported
datatype DateType
        at scala.sys.package$.error(package.scala:27)
        at
org.apache.spark.sql.parquet.ParquetTypesConverter$anonfun$fromDataType$2.apply(ParquetTypes.scala:343)
        at
org.apache.spark.sql.parquet.ParquetTypesConverter$anonfun$fromDataType$2.apply(ParquetTypes.scala:292)
        at scala.Option.getOrElse(Option.scala:120)
        at
org.apache.spark.sql.parquet.ParquetTypesConverter$.fromDataType(ParquetTypes.scala:291)
        at
org.apache.spark.sql.parquet.ParquetTypesConverter$anonfun$4.apply(ParquetTypes.scala:363)
        at
org.apache.spark.sql.parquet.ParquetTypesConverter$anonfun$4.apply(ParquetTypes.scala:362)

I would more than happy to fix this myself, but I would need some help
wading through the code. Could anyone explain to me what exactly is needed
to support a new data type in SparkSQL's Parquet storage engine?

Thanks.

Alex


"
Alessandro Baretta <alexbaretta@gmail.com>,"Tue, 30 Dec 2014 17:21:58 -0800",Re: Unsupported Catalyst types in Parquet,"""Wang, Daoyuan"" <daoyuan.wang@intel.com>","Sorry! My bad. I had stale spark jars sitting on the slave nodes...

Alex


"
Davies Liu <davies@databricks.com>,"Tue, 30 Dec 2014 17:39:36 -0800",Re: Adding third party jars to classpath used by pyspark,Jeremy Freeman <freeman.jeremy@gmail.com>,"
Unfortunately, you also need '--driver-class-path /path/to/file.jar'
to make it accessible in driver. (This may be fixed in 1.3).

"" tab to make sure the jar gets on your classpath. Let me know if you still see errors related to this.
ng
may

---------------------------------------------------------------------


"
Davies Liu <davies@databricks.com>,"Tue, 30 Dec 2014 17:40:14 -0800","Re: Help, pyspark.sql.List flatMap results become tuple",guoxu1231 <guoxu1231@gmail.com>,"This should be fixed in 1.2, could you try it?


---------------------------------------------------------------------


"
guoxu1231 <guoxu1231@gmail.com>,"Tue, 30 Dec 2014 20:24:42 -0700 (MST)","Re: Help, pyspark.sql.List flatMap results become tuple",dev@spark.apache.org,"Thanks Davies, it works in 1.2. 



--

---------------------------------------------------------------------


"
Shixiong Zhu <zsxwing@gmail.com>,"Wed, 31 Dec 2014 12:09:07 +0800",Why the major.minor version of the new hive-exec is 51.0?,"""dev@spark.apache.org"" <dev@spark.apache.org>","The major.minor version of the new org.spark-project.hive.hive-exec is
51.0, so it will require people use JDK7. Is it intentional?

<dependency>
<groupId>org.spark-project.hive</groupId>
<artifactId>hive-exec</artifactId>
<version>0.12.0-protobuf-2.5</version>
</dependency>

You can use the following steps to reproduce it (Need to use JDK6):

1. Create a Test.java file with the following content:

public class Test {

    public static void main(String[] args) throws Exception{
       Class.forName(""org.apache.hadoop.hive.conf.HiveConf"");
    }

}

2. javac Test.java
3. java -classpath
~/.m2/repository/org/spark-project/hive/hive-exec/0.12.0-protobuf-2.5/hive-exec-0.12.0-protobuf-2.5.jar:.
Test

Exception in thread ""main"" java.lang.UnsupportedClassVersionError:
org/apache/hadoop/hive/conf/HiveConf : Unsupported major.minor version 51.0
at java.lang.ClassLoader.defineClass1(Native Method)
at java.lang.ClassLoader.defineClassCond(ClassLoader.java:631)
at java.lang.ClassLoader.defineClass(ClassLoader.java:615)
at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:141)
at java.net.URLClassLoader.defineClass(URLClassLoader.java:283)
at java.net.URLClassLoader.access$000(URLClassLoader.java:58)
at java.net.URLClassLoader$1.run(URLClassLoader.java:197)
at java.security.AccessController.doPrivileged(Native Method)
at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
at java.lang.Class.forName0(Native Method)
at java.lang.Class.forName(Class.java:169)
at Test.main(Test.java:5)


Best Regards,
Shixiong Zhu
"
Naveen Madhire <vmadhire@umail.iu.edu>,"Tue, 30 Dec 2014 23:26:58 -0500",Sample Spark Program Error,dev <dev@spark.apache.org>,"Hi All,

I am trying to run a sample Spark program using Scala SBT,

Below is the program,

def main(args: Array[String]) {

      val logFile = ""E:/ApacheSpark/usb/usb/spark/bin/README.md"" // Should
be some file on your system
      val sc = new SparkContext(""local"", ""Simple App"",
""E:/ApacheSpark/usb/usb/spark/bin"",List(""target/scala-2.10/sbt2_2.10-1.0.jar""))
      val logData = sc.textFile(logFile, 2).cache()

      val numAs = logData.filter(line => line.contains(""a"")).count()
      val numBs = logData.filter(line => line.contains(""b"")).count()

      println(""Lines with a: %s, Lines with b: %s"".format(numAs, numBs))

    }


Below is the error log,


14/12/30 23:20:21 INFO rdd.HadoopRDD: Input split:
file:/E:/ApacheSpark/usb/usb/spark/bin/README.md:0+673
14/12/30 23:20:21 INFO storage.MemoryStore: ensureFreeSpace(2032) called
with curMem=34047, maxMem=280248975
14/12/30 23:20:21 INFO storage.MemoryStore: Block rdd_1_0 stored as values
in memory (estimated size 2032.0 B, free 267.2 MB)
14/12/30 23:20:21 INFO storage.BlockManagerInfo: Added rdd_1_0 in memory on
zealot:61452 (size: 2032.0 B, free: 267.3 MB)
14/12/30 23:20:21 INFO storage.BlockManagerMaster: Updated info of block
rdd_1_0
14/12/30 23:20:21 INFO executor.Executor: Finished task 0.0 in stage 0.0
(TID 0). 2300 bytes result sent to driver
14/12/30 23:20:21 INFO scheduler.TaskSetManager: Starting task 1.0 in stage
0.0 (TID 1, localhost, PROCESS_LOCAL, 1264 bytes)
14/12/30 23:20:21 INFO executor.Executor: Running task 1.0 in stage 0.0
(TID 1)
14/12/30 23:20:21 INFO spark.CacheManager: Partition rdd_1_1 not found,
computing it
14/12/30 23:20:21 INFO rdd.HadoopRDD: Input split:
file:/E:/ApacheSpark/usb/usb/spark/bin/README.md:673+673
14/12/30 23:20:21 INFO scheduler.TaskSetManager: Finished task 0.0 in stage
0.0 (TID 0) in 3507 ms on localhost (1/2)
14/12/30 23:20:21 INFO storage.MemoryStore: ensureFreeSpace(1912) called
with curMem=36079, maxMem=280248975
14/12/30 23:20:21 INFO storage.MemoryStore: Block rdd_1_1 stored as values
in memory (estimated size 1912.0 B, free 267.2 MB)
14/12/30 23:20:21 INFO storage.BlockManagerInfo: Added rdd_1_1 in memory on
zealot:61452 (size: 1912.0 B, free: 267.3 MB)
14/12/30 23:20:21 INFO storage.BlockManagerMaster: Updated info of block
rdd_1_1
14/12/30 23:20:21 INFO executor.Executor: Finished task 1.0 in stage 0.0
(TID 1). 2300 bytes result sent to driver
14/12/30 23:20:21 INFO scheduler.TaskSetManager: Finished task 1.0 in stage
0.0 (TID 1) in 261 ms on localhost (2/2)
14/12/30 23:20:21 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0,
whose tasks have all completed, from pool
14/12/30 23:20:21 INFO scheduler.DAGScheduler: Stage 0 (count at
Test1.scala:19) finished in 3.811 s
14/12/30 23:20:21 INFO spark.SparkContext: Job finished: count at
Test1.scala:19, took 3.997365232 s
14/12/30 23:20:21 INFO spark.SparkContext: Starting job: count at
Test1.scala:20
14/12/30 23:20:21 INFO scheduler.DAGScheduler: Got job 1 (count at
Test1.scala:20) with 2 output partitions (allowLocal=false)
14/12/30 23:20:21 INFO scheduler.DAGScheduler: Final stage: Stage 1(count
at Test1.scala:20)
14/12/30 23:20:21 INFO scheduler.DAGScheduler: Parents of final stage:
List()
14/12/30 23:20:21 INFO scheduler.DAGScheduler: Missing parents: List()
14/12/30 23:20:21 INFO scheduler.DAGScheduler: Submitting Stage 1
(FilteredRDD[3] at filter at Test1.scala:20), which has no missing parents
14/12/30 23:20:21 INFO storage.MemoryStore: ensureFreeSpace(2600) called
with curMem=37991, maxMem=280248975
14/12/30 23:20:21 INFO storage.MemoryStore: Block broadcast_2 stored as
values in memory (estimated size 2.5 KB, free 267.2 MB)
14/12/30 23:20:21 INFO scheduler.DAGScheduler: Submitting 2 missing tasks
from Stage 1 (FilteredRDD[3] at filter at Test1.scala:20)
14/12/30 23:20:21 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0
with 2 tasks
14/12/30 23:20:21 INFO scheduler.TaskSetManager: Starting task 0.0 in stage
1.0 (TID 2, localhost, ANY, 1264 bytes)
14/12/30 23:20:21 INFO executor.Executor: Running task 0.0 in stage 1.0
(TID 2)
14/12/30 23:20:21 INFO storage.BlockManager: Found block rdd_1_0 locally
14/12/30 23:20:21 INFO executor.Executor: Finished task 0.0 in stage 1.0
(TID 2). 1731 bytes result sent to driver
14/12/30 23:20:21 INFO scheduler.TaskSetManager: Starting task 1.0 in stage
1.0 (TID 3, localhost, ANY, 1264 bytes)
14/12/30 23:20:21 INFO executor.Executor: Running task 1.0 in stage 1.0
(TID 3)
14/12/30 23:20:21 INFO storage.BlockManager: Found block rdd_1_1 locally
14/12/30 23:20:21 INFO executor.Executor: Finished task 1.0 in stage 1.0
(TID 3). 1731 bytes result sent to driver
14/12/30 23:20:21 INFO scheduler.TaskSetManager: Finished task 1.0 in stage
1.0 (TID 3) in 7 ms on localhost (1/2)
14/12/30 23:20:21 INFO scheduler.TaskSetManager: Finished task 0.0 in stage
1.0 (TID 2) in 16 ms on localhost (2/2)
14/12/30 23:20:21 INFO scheduler.DAGScheduler: Stage 1 (count at
Test1.scala:20) finished in 0.016 s
14/12/30 23:20:21 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0,
whose tasks have all completed, from pool
14/12/30 23:20:21 INFO spark.SparkContext: Job finished: count at
Test1.scala:20, took 0.041709824 s
Lines with a: 24, Lines with b: 15
14/12/30 23:20:21 ERROR util.Utils: Uncaught exception in thread
SparkListenerBus
java.lang.InterruptedException
at
java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1301)
at java.util.concurrent.Semaphore.acquire(Semaphore.java:317)
at
org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(LiveListenerBus.scala:48)
at
org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply(LiveListenerBus.scala:47)
at
org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply(LiveListenerBus.scala:47)
at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1311)
[success] Total time: 12 s, completed Dec 30, 2014 11:20:21 PM
at
org.apache.spark.scheduler.LiveListenerBus$$anon$1.run(LiveListenerBus.scala:46)
14/12/30 23:20:21 ERROR spark.ContextCleaner: Error in cleaning thread
java.lang.InterruptedException
at java.lang.Object.wait(Native Method)
at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:135)
at
org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1.apply$mcV$sp(ContextCleaner.scala:136)
at
org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1.apply(ContextCleaner.scala:134)
at
org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1.apply(ContextCleaner.scala:134)
at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1311)
at org.apache.spark.ContextCleaner.org
$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:133)
at org.apache.spark.ContextCleaner$$anon$3.run(ContextCleaner.scala:65)
14/12/30 23:20:21 INFO network.ConnectionManager: Selector thread was
interrupted!
14/12/30 23:20:21 INFO storage.BlockManager: Removing broadcast 2
14/12/30 23:20:21 INFO storage.BlockManager: Removing block broadcast_2
14/12/30 23:20:21 INFO storage.MemoryStore: Block broadcast_2 of size 2600
dropped from memory (free 280210984)
14/12/30 23:20:21 INFO spark.ContextCleaner: Cleaned broadcast 2




Please let me know any pointers to debug the error.


Thanks a lot.
"
Ted Yu <yuzhihong@gmail.com>,"Tue, 30 Dec 2014 20:40:10 -0800",Re: Why the major.minor version of the new hive-exec is 51.0?,Shixiong Zhu <zsxwing@gmail.com>,"I extracted org/apache/hadoop/hive/common/CompressionUtils.class from the
jar and used hexdump to view the class file.
Bytes 6 and 7 are 00 and 33, respectively.

According to http://en.wikipedia.org/wiki/Java_class_file, the jar was
produced using Java 7.

FYI


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 31 Dec 2014 06:06:11 +0000",Re: Sample Spark Program Error,"Naveen Madhire <vmadhire@umail.iu.edu>, dev <dev@spark.apache.org>","You sent this to the dev list. Please send it instead to the user list.

We use the dev list to discuss development on Spark itself, new features,
fixes to known bugs, and so forth.

The user list is to discuss issues using Spark, which I believe is what you
are looking for.

Nick



"
Alessandro Baretta <alexbaretta@gmail.com>,"Tue, 30 Dec 2014 23:01:32 -0800",Re: Unsupported Catalyst types in Parquet,"""Wang, Daoyuan"" <daoyuan.wang@intel.com>","Here's a more meaningful exception:

java.lang.ClassCastException: org.apache.spark.sql.catalyst.types.DateType$
cannot be cast to org.apache.spark.sql.catalyst.types.PrimitiveType
        at
org.apache.spark.sql.parquet.RowWriteSupport.writeValue(ParquetTableSupport.scala:188)
        at
org.apache.spark.sql.parquet.RowWriteSupport.write(ParquetTableSupport.scala:167)
        at
org.apache.spark.sql.parquet.RowWriteSupport.write(ParquetTableSupport.scala:130)
        at
parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:120)
        at
parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:81)
        at
parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:37)
        at org.apache.spark.sql.parquet.InsertIntoParquetTable.org
$apache$spark$sql$parquet$InsertIntoParquetTable$writeShard$1(ParquetTableOperations.scala:309)
        at
org.apache.spark.sql.parquet.InsertIntoParquetTable$anonfun$saveAsHadoopFile$1.apply(ParquetTableOperations.scala:326)
        at
org.apache.spark.sql.parquet.InsertIntoParquetTable$anonfun$saveAsHadoopFile$1.apply(ParquetTableOperations.scala:326)
        at
org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
        at org.apache.spark.scheduler.Task.run(Task.scala:56)
        at
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)
        at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)


This is easy to fix even for a newbie like myself: it suffices to add the
PrimitiveType trait to the DateType object. You can find this change here:

https://github.com/alexbaretta/spark/compare/parquet-date-support

However, even this does not work. Here's the next blocker:

java.lang.RuntimeException: Unsupported datatype DateType, cannot write to
consumer
        at scala.sys.package$.error(package.scala:27)
        at
org.apache.spark.sql.parquet.MutableRowWriteSupport.consumeType(ParquetTableSupport.scala:361)
        at
org.apache.spark.sql.parquet.MutableRowWriteSupport.write(ParquetTableSupport.scala:329)
        at
org.apache.spark.sql.parquet.MutableRowWriteSupport.write(ParquetTableSupport.scala:315)
        at
parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:120)
        at
parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:81)
        at
parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:37)
        at org.apache.spark.sql.parquet.InsertIntoParquetTable.org
$apache$spark$sql$parquet$InsertIntoParquetTable$writeShard$1(ParquetTableOperations.scala:309)
        at
org.apache.spark.sql.parquet.InsertIntoParquetTable$anonfun$saveAsHadoopFile$1.apply(ParquetTableOperations.scala:326)
        at
org.apache.spark.sql.parquet.InsertIntoParquetTable$anonfun$saveAsHadoopFile$1.apply(ParquetTableOperations.scala:326)
        at
org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
        at org.apache.spark.scheduler.Task.run(Task.scala:56)
        at
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)
        at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)

Any input on how to address this issue would be welcome.

Alex


"
Alessandro Baretta <alexbaretta@gmail.com>,"Tue, 30 Dec 2014 23:32:17 -0800",Re: Unsupported Catalyst types in Parquet,"""Wang, Daoyuan"" <daoyuan.wang@intel.com>","I think I might have figure it out myself. Here's a pull request for you
guys to check out:

https://github.com/apache/spark/pull/3855

I successfully tested this code on my cluster.


"
Enno Shioji <eshioji@gmail.com>,"Wed, 31 Dec 2014 16:12:40 +0000","Big performance difference between ""client"" and ""cluster"" deployment
 mode; is this expected?",dev@spark.apache.org,"Hi,

I have a very, very simple streaming job. When I deploy this on the exact
same cluster, with the exact same parameters, I see big (40%) performance
difference between ""client"" and ""cluster"" deployment mode. This seems a bit
surprising.. Is this expected?

The streaming job is:

    val msgStream = kafkaStream
      .map { case (k, v) => v}
      .map(DatatypeConverter.printBase64Binary)
      .foreachRDD(save)
      .saveAsTextFile(""s3n://some.bucket/path"", classOf[LzoCodec])

I tried several times, but the job deployed with ""client"" mode can only
write at 60% throughput of the job deployed with ""cluster"" mode and this
happens consistently. I'm logging at INFO level, but my application code
doesn't log anything so it's only Spark logs. The logs I see in ""client""
mode doesn't seem like a crazy amount.

The setup is:
spark-ec2 [...] \
  --copy-aws-credentials \
  --instance-type=m3.2xlarge \
  -s 2 launch test_cluster

And all the deployment was done from the master machine.

ᐧ
"
Sean Owen <sowen@cloudera.com>,"Wed, 31 Dec 2014 18:21:40 +0000","Re: Big performance difference between ""client"" and ""cluster""
 deployment mode; is this expected?",Enno Shioji <eshioji@gmail.com>,"-dev, +user

A decent guess: Does your 'save' function entail collecting data back
to the driver? and are you running this from a machine that's not in
your Spark cluster? Then in client mode you're shipping data back to a
less-nearby machine, compared to"
Michael Armbrust <michael@databricks.com>,"Wed, 31 Dec 2014 12:16:48 -0800",Re: Why the major.minor version of the new hive-exec is 51.0?,Ted Yu <yuzhihong@gmail.com>,"This was not intended, can you open a JIRA?


"
