=?GB2312?B?zO/S4w==?= <tianyi@asiainfo.com>,"Fri, 1 Aug 2014 17:00:19 +0800",How to run specific sparkSQL test with maven,dev@spark.apache.org,"Hi everyone!

Could any one tell me how to run specific sparkSQL test with maven?

For example:

I want to test HiveCompatibilitySuite.

I ran °∞mvm test -Dtest=HiveCompatibilitySuite°±

It did not work. 

BTW, is there any information about how to build a test environment of sparkSQL?

I got this error when i ran the test.

It seems that the HiveCompatibilitySuite need a hadoop and hive environment, am I right?
 
""Relative path in absolute URI: file:$%7Bsystem:test.tmp.dir%7D/tmp_showcrt1°± 





"
"""=?utf-8?B?d2l0Z28=?="" <witgo@qq.com>","Fri, 1 Aug 2014 21:50:42 +0800",Re:How to run specific sparkSQL test with maven,"""=?utf-8?B?ZGV2?="" <dev@spark.apache.org>","You can try these commands‚Äç
./sbt/sbt assembly‚Äç./sbt/sbt ""test-only *.HiveCompatibilitySuite"" -Phive‚Äç

‚Äç





------------------ Original ------------------
From:  ""Áî∞ÊØÖ"";<tianyi@asiainfo.com>;
Date:  Fri, Aug 1, 2014 05:00 PM
To:  ""dev""<dev@spark.apache.org>; 

Subject:  How to run specific sparkSQL test with maven



Hi everyone!

Could any one tell me how to run specific sparkSQL test with maven?

For example:

I want to test HiveCompatibilitySuite.

I ran ‚Äúmvm test -Dtest=HiveCompatibilitySuite‚Äù

It did not work. 

BTW, is there any information about how to build a test environment of sparkSQL?

I got this error when i ran the test.

It seems that the HiveCompatibilitySuite need a hadoop and hive environment, am I right?
 
""Relative path in absolÄù"
Jeremy Freeman <freeman.jeremy@gmail.com>,"Fri, 1 Aug 2014 07:23:40 -0700 (PDT)",Re: Re:How to run specific sparkSQL test with maven,dev@spark.incubator.apache.org,"With maven you can run a particular test suite like this:

mvn -DwildcardSuites=org.apache.spark.sql.SQLQuerySuite test

see the note here (under ""Spark Tests in Maven""):

http://spark.apache.org/docs/latest/building-with-maven.html



--

"
andy petrella <andy.petrella@gmail.com>,"Fri, 1 Aug 2014 18:48:11 +0200","Re: [brainsotrming] Generalization of DStream, a ContinuousRDD ?",dev@spark.apache.org,"Heya,
Dunno if these ideas are still in the air or felt in the warp ^^.
However there is a paper on avocado
<http://www.cs.berkeley.edu/~kubitron/courses/cs262a-F13/projects/reports/project8_report.pdf>
that
mentions a way of working with their data (sequence's reads) in a windowed
manner without neither time nor timestamp field's value, but a kind-of
internal index as range delimiter -- thus defining their own exotic
continuum and break function.

greetz,

 a‚Ñïdy ‚Ñôetrella
about.me/noootsab
[image: a‚Ñïdy ‚Ñôetrella on about.me]

<http://about.me/noootsab>



I
ws
e
ge
y.
n
at
.
d
e,
gh
d
t,
e,
g
g
s
ct
ed
y,
s
che/spark/mllib/rdd/SlidingRDD.scala
sh
d.
he
s
,
at
y,
e
f
of
"
andy petrella <andy.petrella@gmail.com>,"Fri, 1 Aug 2014 18:48:11 +0200","Re: [brainsotrming] Generalization of DStream, a ContinuousRDD ?",dev@spark.apache.org,"Heya,
Dunno if these ideas are still in the air or felt in the warp ^^.
However there is a paper on avocado
<http://www.cs.berkeley.edu/~kubitron/courses/cs262a-F13/projects/reports/project8_report.pdf>
that
mentions a way of working with their data (sequence's reads) in a windowed
manner without neither time nor timestamp field's value, but a kind-of
internal index as range delimiter -- thus defining their own exotic
continuum and break function.

greetz,

 a‚Ñïdy ‚Ñôetrella
about.me/noootsab
[image: a‚Ñïdy ‚Ñôetrella on about.me]

<http://about.me/noootsab>



I
ws
e
ge
y.
n
at
.
d
e,
gh
d
t,
e,
g
g
s
ct
ed
y,
s
che/spark/mllib/rdd/SlidingRDD.scala
sh
d.
he
s
,
at
y,
e
f
of
"
Michael Armbrust <michael@databricks.com>,"Fri, 1 Aug 2014 10:36:59 -0700",Re: How to run specific sparkSQL test with maven,dev@spark.apache.org,"
You should only need Hadoop and Hive if you are creating new tests that we
need to compute the answers for.  Existing tests are run with cached
answers.  There are details about the configuration here:
https://github.com/apache/spark/tree/master/sql
"
Rajiv Abraham <rajiv.abraham@gmail.com>,"Fri, 1 Aug 2014 14:06:58 -0400",Interested in contributing to GraphX in Python,dev@spark.apache.org,"Hi,
I just saw Ankur's GraphX presentation and it looks very exciting! I would
like to contribute to a Python version of GraphX. I checked out JIRA and
Github but I did not find much info.

- Are there limitations currently to port GraphX in Python? (e.g. Maybe the
Python Spark RDD API is incomplete or not refactored for GraphX as compared
to the Scala version)
- If I had to start, could  I take inspiration from the Scala version and
try to emulate it in Python?
- Otherwise any suggestions of  starter tasks regarding GraphX in Python
would be appreciated



-- 
Take care,
Rajiv
"
"""Jin, Zhonghui"" <zhonghui.jin@intel.com>","Fri, 1 Aug 2014 08:14:02 +0000","My Spark application had huge performance refression after Spark
 git commit: 0441515f221146756800dc583b225bdec8a6c075","""dev@spark.apache.org"" <dev@spark.apache.org>","
I found huge performance regression ( 1/20 of original) of my application after Spark git commit: 0441515f221146756800dc583b225bdec8a6c075.

Apply the following patch, will fix my issue:

diff --git a/core/src/main/scala/org/apache/spark/executor/Executor.scala b/core/src/main/scala/org/apache/spark/executor/Executor.scala
index 214a8c8..ebec21d 100644
--- a/core/src/main/scala/org/apache/spark/executor/Executor.scala
+++ b/core/src/main/scala/org/apache/spark/executor/Executor.scala
@@ -145,7 +145,7 @@ private[spark] class Executor(
       }
     }
-    override def run() {
+    override def run() : Unit = SparkHadoopUtil.get.runAsSparkUser { () =>
       val startTime = System.currentTimeMillis()
       SparkEnv.set(env)
       Thread.currentThread.setContextClassLoader(replClassLoader)

In the runAsSparkUser will call the 'UserGroupInformation.doAs()' to execute the task and my application running OK;
if not through it, the performance was very poor. Application hotspot was JNIHandleBlock::alloc_handle (JVM code, very high CPI (cycles per instruction, < 1 is OK) > 10)

My application passed large array data (>80K length) to native C code through JNI.

Why the ""UserGroupInformation.doAs()"" great impacted the performance under this situation?


Thanks,
Zhonghui

"
Andrew Ash <andrew@andrewash.com>,"Fri, 1 Aug 2014 17:45:24 -0400","Re: Exception in Spark 1.0.1: com.esotericsoftware.kryo.KryoException:
 Buffer underflow",dev@spark.apache.org,"After several days of debugging, we think the issue is that we have
conflicting versions of Guava.  Our application was running with Guava 14
and the Spark services (Master, Workers, Executors) had Guava 16.  We had
custom Kryo serializers for Guava's ImmutableLists, and commenting out
those register calls did the trick.

Have people had issues with Guava version mismatches in the past?

I've found @srowen's Guava 14 -> 11 downgrade PR here
https://github.com/apache/spark/pull/1610 and some extended discussion on
https://issues.apache.org/jira/browse/SPARK-2420 for Hive compatibility



"
Patrick Wendell <pwendell@gmail.com>,"Fri, 1 Aug 2014 15:27:55 -0700",Re: Compiling Spark master (284771ef) with sbt/sbt assembly fails on EC2,user@spark.apache.org,"This is a Scala bug - I filed something upstream, hopefully they can fix it
soon and/or we can provide a work around:

https://issues.scala-lang.org/browse/SI-8772

- Patrick



"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Fri, 1 Aug 2014 15:30:13 -0700",Re: Compiling Spark master (284771ef) with sbt/sbt assembly fails on EC2,user@spark.apache.org,"Thanks Patrick -- It does look like some maven misconfiguration as

wget
http://repo1.maven.org/maven2/org/scala-lang/scala-library/2.10.2/scala-library-2.10.2.pom

works for me.

Shivaram




"
Mayur Rustagi <mayur.rustagi@gmail.com>,"Fri, 1 Aug 2014 15:42:36 -0700","Re: [brainsotrming] Generalization of DStream, a ContinuousRDD ?",dev <dev@spark.apache.org>,"Interesting, clickstream data would have its own window concept based on
session of User , I can imagine windows would change across streams but
wouldnt they large be domain specific in Nature?

Mayur Rustagi
Ph: +1 (760) 203 3257
http://www.sigmoidanalytics.com
@mayur_rustagi <https://twitter.com/mayur_rustagi>




project8_report.pdf
d
t
me
t
e
2,
s
ne
h
m
s
nd
h
of
l
e
,
he/spark/mllib/rdd/SlidingRDD.scala
a
al
.)
re
ve
wo
em
n
e
"
Mayur Rustagi <mayur.rustagi@gmail.com>,"Fri, 1 Aug 2014 15:42:36 -0700","Re: [brainsotrming] Generalization of DStream, a ContinuousRDD ?",dev <dev@spark.apache.org>,"Interesting, clickstream data would have its own window concept based on
session of User , I can imagine windows would change across streams but
wouldnt they large be domain specific in Nature?

Mayur Rustagi
Ph: +1 (760) 203 3257
http://www.sigmoidanalytics.com
@mayur_rustagi <https://twitter.com/mayur_rustagi>




project8_report.pdf
d
t
me
t
e
2,
s
ne
h
m
s
nd
h
of
l
e
,
he/spark/mllib/rdd/SlidingRDD.scala
a
al
.)
re
ve
wo
em
n
e
"
Colin McCabe <cmccabe@alumni.cmu.edu>,"Fri, 1 Aug 2014 16:15:28 -0700","Re: Exception in Spark 1.0.1: com.esotericsoftware.kryo.KryoException:
 Buffer underflow",dev@spark.apache.org,"
There's some discussion about dealing with Guava version issues in
Spark in SPARK-2420.

best,
Colin



"
andy petrella <andy.petrella@gmail.com>,"Sat, 2 Aug 2014 01:15:31 +0200","Re: [brainsotrming] Generalization of DStream, a ContinuousRDD ?",dev@spark.apache.org,"Actually for click stream, the users space wouldn't be a continuum, unless
the order of users is important or the fact that they are coming in a kind
of order can be used by the algo.
The purpose of the break or binning function is to package things in a
cluster for which we know the properties, but we don't know in advance
which or how many elements it will contain.
However,  this would need to extend the notion of continuum I thought of,
to, indeed,  include categorical space and thus allowing groupBy mapping to
RDDs.
And actually,  there would be a way to fallback to a continuum if the
breaks function would be dictated by a trained model that can cluster the
users,  and they were previously and accordingly shuffled to form a
sequence where they come in batch.
Just thinking (and hardly trying to use a tablet to write it, man... How
unfriendly is this keyboard and small screen ‚ò∫)
Cheers
Andy
"
andy petrella <andy.petrella@gmail.com>,"Sat, 2 Aug 2014 01:15:31 +0200","Re: [brainsotrming] Generalization of DStream, a ContinuousRDD ?",dev@spark.apache.org,"Actually for click stream, the users space wouldn't be a continuum, unless
the order of users is important or the fact that they are coming in a kind
of order can be used by the algo.
The purpose of the break or binning function is to package things in a
cluster for which we know the properties, but we don't know in advance
which or how many elements it will contain.
However,  this would need to extend the notion of continuum I thought of,
to, indeed,  include categorical space and thus allowing groupBy mapping to
RDDs.
And actually,  there would be a way to fallback to a continuum if the
breaks function would be dictated by a trained model that can cluster the
users,  and they were previously and accordingly shuffled to form a
sequence where they come in batch.
Just thinking (and hardly trying to use a tablet to write it, man... How
unfriendly is this keyboard and small screen ‚ò∫)
Cheers
Andy
"
Marcelo Vanzin <vanzin@cloudera.com>,"Fri, 1 Aug 2014 16:48:04 -0700",SparkContext.hadoopConfiguration vs. SparkHadoopUtil.newConfiguration(),dev@spark.apache.org,"Hi all,

While working on some seemingly unrelated code, I ran into this issue
where ""spark.hadoop.*"" configs were not making it to the Configuration
objects in some parts of the code. I was trying to do that to avoid
having to do dirty ticks with the classpath while running tests, but
that's a little besides the point.

Since I don't know the history of that code in SparkContext, does
anybody see any issue with moving it up a layer so that all code that
uses SparkHadoopUtil.newConfiguration() does the same thing?

This would also include some code (e.g. in the yarn module) that does
""new Configuration()"" directly instead of going through the wrapper.


-- 
Marcelo

"
Patrick Wendell <pwendell@gmail.com>,"Fri, 1 Aug 2014 20:13:27 -0700","Re: Exception in Spark 1.0.1: com.esotericsoftware.kryo.KryoException:
 Buffer underflow","""dev@spark.apache.org"" <dev@spark.apache.org>","Andrew - I think Spark is using Guava 14... are you using Guava 16 in your
user app (i.e. you inverted the versions in your earlier e-mail)?

- Patrick



"
Andrew Ash <andrew@andrewash.com>,"Sat, 2 Aug 2014 01:06:31 -0400","Re: Exception in Spark 1.0.1: com.esotericsoftware.kryo.KryoException:
 Buffer underflow",dev@spark.apache.org,"The original version numbers I reported were indeed what we had, so let me
clarify the situation.

Our application had Guava 14 because that's what Spark depends on.  But we
had added an in-house library to the Hadoop cluster and also the Spark
cluster to add a new FileSystem (think hdfs://, s3n://, etc) that was using
Guava 16.  So the Guava 16 from our additional FileSystem overrode the
Guava 11 jar from the CDH4.4.0 lib directory and the Guava 14 class files
that are bundled in the Spark assembly jar.  That mismatch between 16 on
the cluster and 14 on the driver caused us problems with ImmutableLists,
which must have changed in a way between 14 and 16 that aren't binary
compatible in Kryo serialization.

At least that's our current understanding of the bug we experienced.



"
Cheng Lian <lian.cs.zju@gmail.com>,"Sat, 2 Aug 2014 13:19:06 +0800",Re: How to run specific sparkSQL test with maven,dev@spark.apache.org,"It‚Äôs also useful to set hive.exec.mode.local.auto to true to accelerate the
test.
‚Äã



e
"
Patrick Wendell <pwendell@gmail.com>,"Fri, 1 Aug 2014 23:06:46 -0700",ASF JIRA is down for maintenance,"""dev@spark.apache.org"" <dev@spark.apache.org>","Please don't let this prevent you from merging patches, just keep a list
and we can update the JIRA later.

- Patrick
"
Patrick Wendell <pwendell@gmail.com>,"Sat, 2 Aug 2014 01:52:11 -0700",branch-1.1 of Spark has been cut,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey All,

I'm happy to announce branch-1.1 of Spark [1] - this branch will eventually
become the 1.1 release. Committers: new patches will need to be explicitly
back-ported into this branch in order to appear in the 1.1 release.

Thanks so much to all the committers and contributors who were extremely
active on review in the last week!

We'll now enter the standard triage process:
-> In the next 48 hours, smaller features that are very late in review
(e.g. loose ends) are okay to go in.
At that point we are solidly ""bug fixes only"" for most Spark components.
-> When deemed appropriate, we'll cut a release candidate for official
voting and start the process. As voting goes on, we'll eventually escalate
to ""regressions only"" - i.e. we hold the release only for fixes that
regress from earlier version.

I'll also try to document the release/triage process on the wiki in a bit
more detail. For those interested, here is a good example - the guidelines
for the Linux kernel release process:
https://www.kernel.org/doc/Documentation/development-process/2.Process

Thanks everyone, looking forward to a great 1.1 release!

[1]
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=shortlog;h=refs/heads/branch-1.1

- Patrick
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sat, 2 Aug 2014 10:24:45 -0400",Re: ASF JIRA is down for maintenance,Patrick Wendell <pwendell@gmail.com>,"Seems to be back up now.



"
Dibyendu Bhattacharya <dibyendu.bhattachary@gmail.com>,"Sat, 2 Aug 2014 22:39:42 +0530",Low Level Kafka Consumer for Spark,"dev@spark.apache.org, user@spark.apache.org","Hi,

I have implemented a Low Level Kafka Consumer for Spark Streaming using
Kafka Simple Consumer API. This API will give better control over the Kafka
offset management and recovery from failures. As the present Spark
KafkaUtils uses HighLevel Kafka Consumer API, I wanted to have a better
control over the offset management which is not possible in Kafka HighLevel
consumer.

This Project is available in below Repo :

https://github.com/dibbhatt/kafka-spark-consumer


I have implemented a Custom Receiver consumer.kafka.client.KafkaReceiver.
The KafkaReceiver uses low level Kafka Consumer API (implemented in
consumer.kafka packages) to fetch messages from Kafka and 'store' it in
Spark.

The logic will detect number of partitions for a topic and spawn that many
threads (Individual instances of Consumers). Kafka Consumer uses Zookeeper
for storing the latest offset for individual partitions, which will help to
recover in case of failure. The Kafka Consumer logic is tolerant to ZK
Failures, Kafka Leader of Partition changes, Kafka broker failures,
 recovery from offset errors and other fail-over aspects.

The consumer.kafka.client.Consumer is the sample Consumer which uses this
Kafka Receivers to generate DStreams from Kafka and apply a Output
operation for every messages of the RDD.

We are planning to use this Kafka Spark Consumer to perform Near Real Time
Indexing of Kafka Messages to target Search Cluster and also Near Real Time
Aggregation using target NoSQL storage.

Kindly let me know your view. Also if this looks good, can I contribute to
Spark Streaming project.

Regards,
Dibyendu
"
Debasish Das <debasish.das83@gmail.com>,"Sat, 2 Aug 2014 10:13:12 -0700",Using mllib-1.1.0-SNAPSHOT on Spark 1.0.1,dev@spark.apache.org,"Hi,

I have deployed spark stable 1.0.1 on the cluster but I have new code that
I added in mllib-1.1.0-SNAPSHOT.

I am trying to access the new code using spark-submit as follows:

spark-job --class com.verizon.bda.mllib.recommendation.ALSDriver
--executor-memory 16g --total-executor-cores 16 --jars
spark-mllib_2.10-1.1.0-SNAPSHOT.jar,scopt_2.10-3.2.0.jar
sag-core-0.0.1-SNAPSHOT.jar --rank 25 --numIterations 10 --lambda 1.0
--qpProblem 2 inputPath outputPath

I can see the jars are getting added to httpServer as expected:

14/08/02 12:50:04 INFO SparkContext: Added JAR
file:/vzhome/v606014/spark-glm/spark-mllib_2.10-1.1.0-SNAPSHOT.jar at
http://10.145.84.20:37798/jars/spark-mllib_2.10-1.1.0-SNAPSHOT.jar with
timestamp 1406998204236

14/08/02 12:50:04 INFO SparkContext: Added JAR
file:/vzhome/v606014/spark-glm/scopt_2.10-3.2.0.jar at
http://10.145.84.20:37798/jars/scopt_2.10-3.2.0.jar with timestamp
1406998204237

14/08/02 12:50:04 INFO SparkContext: Added JAR
file:/vzhome/v606014/spark-glm/sag-core-0.0.1-SNAPSHOT.jar at
http://10.145.84.20:37798/jars/sag-core-0.0.1-SNAPSHOT.jar with timestamp
1406998204238

But the job still can't access code form mllib-1.1.0 SNAPSHOT.jar...I think
it's picking up the mllib from cluster which is at 1.0.1...

Please help. I will ask for a PR tomorrow but internally we want to
generate results from the new code.

Thanks.

Deb
"
Xiangrui Meng <mengxr@gmail.com>,"Sat, 2 Aug 2014 10:46:37 -0700",Re: Using mllib-1.1.0-SNAPSHOT on Spark 1.0.1,Debasish Das <debasish.das83@gmail.com>,"You can try enabling ""spark.files.userClassPathFirst"". But I'm not
sure whether it could solve your problem. -Xiangrui


---------------------------------------------------------------------


"
Debasish Das <debasish.das83@gmail.com>,"Sat, 2 Aug 2014 10:54:02 -0700",Re: Using mllib-1.1.0-SNAPSHOT on Spark 1.0.1,Xiangrui Meng <mengxr@gmail.com>,"Let me try it...

Will this be fixed if I generate a assembly file with mllib-1.1.0 SNAPSHOT
jar and other dependencies with the rest of the application code ?




"
Xiangrui Meng <mengxr@gmail.com>,"Sat, 2 Aug 2014 11:12:53 -0700",Re: Using mllib-1.1.0-SNAPSHOT on Spark 1.0.1,Debasish Das <debasish.das83@gmail.com>,"Yes, that should work. spark-mllib-1.1.0 should be compatible with
spark-core-1.0.1.


---------------------------------------------------------------------


"
Anand Avati <avati@gluster.org>,"Sat, 2 Aug 2014 18:04:43 -0700",Scala 2.11 external dependencies,dev@spark.apache.org,"We are currently blocked on non availability of the following external
dependencies for porting Spark to Scala 2.11 [SPARK-1812 Jira]:

- akka-*_2.11 (2.3.4-shaded-protobuf from org.spark-project). The shaded
protobuf needs to be 2.5.0, and the shading is needed because Hadoop1
specifically needs protobuf 2.4. Issues arising because of this
incompatibility is already explained in SPARK-1812 Jira.

- chill_2.11 (0.4 from com.twitter) for core
- algebird_2.11 (0.7 from com.twitter) for examples
- kafka_2.11 (0.8 from org.apache) for external/kafka and examples
- akka-zeromq_2.11 (2.3.4 from com.typesafe, but probably not needed if a
shaded-protobuf version is released from org.spark-project)

First,
Who do I pester to get org.spark-project artifacts published for the akka
shaded-protobuf version?

Second,
In the past what has been the convention to request/pester external
projects to re-release artifacts in a new Scala version?

Thanks!
"
jun <kitaev@126.com>,"Sun, 3 Aug 2014 17:03:31 +0800 (CST)",Intellij IDEA can not recognize the MLlib package,dev-spark <dev@spark.apache.org>,"Hi,


I have started my spark exploration in intellij IDEA local model and want to focus on MLlib part.
but when I put some example codes in IDEA, It can not recognize mllib package, just loos like that:


>
> import org.apache.spark.SparkContext
>import org.apache.spark.mllib.recommendation.ALS
>


I hava configured the breeze in build.sbt file and also install the mingw gcc & gfortran lib. Here is my build.sbt:


>>>> build.sbt <<<<
name := ""SparkMLlibLocal"" version := ""1.0"" resolvers += ""Ooyala Bintray"" at ""http://dl.bintray.com/ooyala/maven"" resolvers += ""Akka Repository"" at ""http://repo.akka.io/releases/"" libraryDependencies += ""ooyala.cnd"" % ""job-server"" % ""0.3.1"" % ""provided"" libraryDependencies += ""com.github.fommil.netlib"" % ""all"" % ""1.1.2"" libraryDependencies += ""org.apache.spark"" %% ""spark-core"" % ""1.0.0"" libraryDependencies ++= Seq( ""org.scalanlp"" %% ""breeze"" % ""0.8.1"", ""org.scalanlp"" %% ""breeze-natives"" % ""0.8.1"" ) resolvers ++= Seq( ""Sonatype Snapshots"" at ""https://oss.sonatype.org/content/repositories/snapshots/"", ""Sonatype Releases"" at ""https://oss.sonatype.org/content/repositories/releases/"" ) scalaVersion := ""2.10.3""
>>>> End <<<<


Is there anything I missed?


BR
Kitaev"
jun  <kitaev@126.com>,"Sun, 3 Aug 2014 17:06:58 +0800 (CST)",Re:Intellij IDEA can not recognize the MLlib package,dev-spark <dev@spark.apache.org>,"Sorry the color is missing. the ""mllib"" is red word and ""import"" sentence is grey.>>import org.apache.spark.mllib.recommendation.ALS

At 2014-08-03 05:03:31, jun"" <kitaev@126.com> wrote: >Hi, > > >I have started my spark exploration in intellij IDEA local model and want to focus on MLlib part. >but when I put some example codes in IDEA, It can not recognize mllib package, just loos like that: > > >> >> import org.apache.spark.SparkContext >>import org.apache.spark.mllib.recommendation.ALS >> > > >I hava configured the breeze in build.sbt file and also install the mingw gcc & gfortran lib. Here is my build.sbt: > > >>>>> build.sbt <<<< >name := ""SparkMLlibLocal"" version := ""1.0"" resolvers += ""Ooyala Bintray"" at ""http://dl.bintray.com/ooyala/maven"" resolvers += ""Akka Repository"" at ""http://repo.akka.io/releases/"" libraryDependencies += ""ooyala.cnd"" % ""job-server"" % ""0.3.1"" % ""provided"" libraryDependencies += ""com.github.fommil.netlib"" % ""all"" % ""1.1.2"" libraryDependencies += ""org.apache.spark"" %% ""spark-core"" % ""1.0.0"" libraryDependencies ++= Seq( ""org.scalanlp"" %% ""breeze"" % ""0.8.1"", ""org.scalanlp"" %% ""breeze-natives"" % ""0.8.1"" ) resolvers ++= Seq( ""Sonatype Snapshots"" at ""https://oss.sonatype.org/content/repositories/snapshots/"", ""Sonatype Releases"" at ""https://oss.sonatype.org/content/repositories/releases/"" ) scalaVersion := ""2.10.3"" >>>>> End <<<< > > >Is there anything I missed? > > >BR >Kitaev"
Sean Owen <sowen@cloudera.com>,"Sun, 3 Aug 2014 10:06:57 +0100",Re: Intellij IDEA can not recognize the MLlib package,jun <kitaev@126.com>,"You missed the mllib artifact? that would certainly explain it! all I
see is core.

 to focus on MLlib part.
kage, just loos like that:
 gcc & gfortran lib. Here is my build.sbt:
tray"" at ""http://dl.bintray.com/ooyala/maven"" resolvers += ""Akka Repository"" at ""http://repo.akka.io/releases/"" libraryDependencies += ""ooyala.cnd"" % ""job-server"" % ""0.3.1"" % ""provided"" libraryDependencies += ""com.github.fommil.netlib"" % ""all"" % ""1.1.2"" libraryDependencies += ""org.apache.spark"" %% ""spark-core"" % ""1.0.0"" libraryDependencies ++= Seq( ""org.scalanlp"" %% ""breeze"" % ""0.8.1"", ""org.scalanlp"" %% ""breeze-natives"" % ""0.8.1"" ) resolvers ++= Seq( ""Sonatype Snapshots"" at ""https://oss.sonatype.org/content/repositories/snapshots/"", ""Sonatype Releases"" at ""https://oss.sonatype.org/content/repositories/releases/"" ) scalaVersion := ""2.10.3""

---------------------------------------------------------------------


"
jun  <kitaev@126.com>,"Sun, 3 Aug 2014 17:33:05 +0800 (CST)",Re:Re: Intellij IDEA can not recognize the MLlib package,"""Sean Owen"" <sowen@cloudera.com>","Got it and add spark-mllib libraryDependency:libraryDependencies += ""org.apache.spark"" %% ""spark-mllib"" % ""1.0.0""

It works and many thanks!


BR
Kitaev
At 2014-08-03 05:09:23, ""Sean Owen"" <sowen@cloudera.com> wrote:
>Yes, but it is nowhere in your project dependencies.
>
>On Sun, Aug 3, 2014 at 10:06 AM, jun <kitaev@126.com> wrote:
>> Sorry the color is missing. the ""mllib"" is red word and ""import"" sentence is grey.>>import org.apache.spark.mllib.recommendation.ALS
>>
>> At 2014-08-03 05:03:31, jun"" <kitaev@126.com> wrote: >Hi, > > >I have started my spark exploration in intellij IDEA local model and want to focus on MLlib part. >but when I put some example codes in IDEA, It can not recognize mllib package, just loos like that: > > >> >> import org.apache.spark.SparkContext >>import org.apache.spark.mllib.recommendation.ALS >> > > >I hava configured the breeze in build.sbt file and also install the mingw gcc & gfortran lib. Here is my build.sbt: > > >>>>> build.sbt <<<< >name := ""SparkMLlibLocal"" version := ""1.0"" resolvers += ""Ooyala Bintray"" at ""http://dl.bintray.com/ooyala/maven"" resolvers += ""Akka Repository"" at ""http://repo.akka.io/releases/"" libraryDependencies += ""ooyala.cnd"" % ""job-server"" % ""0.3.1"" % ""provided"" libraryDependencies += ""com.github.fommil.netlib"" % ""all"" % ""1.1.2"" libraryDependencies += ""org.apache.spark"" %% ""spark-core"" % ""1.0.0"" libraryDependencies ++= Seq( ""org.scalanlp"" %% ""breeze"" % ""0.8.1"", ""org.scalanlp"" %% ""breeze-natives"" % ""0.8.1"" ) resolvers ++= Seq( ""Sonatype Snapshots"" at ""https://oss.sonatype.org/content/repositories/snapshots/"", ""Sonatype Releases"" at ""https://oss.sonatype.org/content/repositories/releases/"" ) scalaVersion := ""2.10.3"" >>>>> End <<<< > > >Is there anything I missed? > > >BR >Kitaev
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sun, 3 Aug 2014 16:35:02 -0400",Re: -1s on pull requests?,dev <dev@spark.apache.org>,"

Some improvements to SparkQA <https://github.com/SparkQA> could help with
this. May I suggest:

   1. Include the commit hash in the ""tests have started/completed""
   messages, so that it's clear what code exactly is/has been tested for each
   test cycle.
   2. ""Pin"" a message to the start or end of the PR that is updated with
   the status of the PR. ""Testing not complete""; ""New commits since last
   test""; ""Tests failed""; etc. It should be easy for committers to get the
   status of the PR at a glance, without scrolling through the comment history.

Nick
"
pritish <pritish@nirvana-international.com>,"Sun, 3 Aug 2014 20:13:51 -0400 (EDT)",I would like to contribute,dev@spark.apache.org,"Hi

We would like to contribute to Spark but we are not sure how. We can offer
project management, release management to begin with. Please advice on how to
get engaged.

Thank you!!

Regards
Pritish
Nirvana International Inc.
Big Data, Hadoop, Oracle EBS and IT Solutions
VA - SWaM, MD - MBE Certified Company
pritish@nirvana-international.com
http://www.nirvana-international.com


---------------------------------------------------------------------


"
Josh Rosen <rosenville@gmail.com>,"Sun, 3 Aug 2014 17:23:49 -0700",Re: I would like to contribute,dev@spark.apache.org,"The Contributing to Spark guide on the Spark Wiki provides a good overview on how to start contributing:

https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark



Hi 

We would like to contribute to Spark but we are not sure how. We can offer 
project management, release management to begin with. Please advice on how to 
get engaged. 

Thank you!! 

Regards 
Pritish 
Nirvana International Inc. 
Big Data, Hadoop, Oracle EBS and IT Solutions 
VA - SWaM, MD - MBE Certified Company 
pritish@nirvana-international.com 
http://www.nirvana-international.com 


--------------------------------------------------------------------- 

"
Patrick Wendell <pwendell@gmail.com>,"Sun, 3 Aug 2014 20:29:13 -0700",Re: -1s on pull requests?,Nicholas Chammas <nicholas.chammas@gmail.com>,"
Great idea - I think this is easy to do given the current architecture. We
already have access to the commit ID in the same script that posts the
comments.

   2. ""Pin"" a message to the start or end of the PR that is updated with

This also is a good idea - I think this would be doable since the github
API allows us to edit comments, but it's a bit tricker. I think it would
require first making an API call to get the ""status comment"" ID and then
updating it.



Nick - Any interest in doing these? this is all doable from within the
spark repo itself because our QA harness scripts are in there:

https://github.com/apache/spark/blob/master/dev/run-tests-jenkins

If not, could you make a JIRA for them and put it under ""Project Infra"".

- Patrick
"
Larry Xiao <xiaodi@sjtu.edu.cn>,"Mon, 04 Aug 2014 11:48:52 +0800",Compiling Spark master (6ba6c3eb) with sbt/sbt assembly,dev@spark.apache.org,"assembly problem.

$ ./sbt/sbt assembly
Using /usr/lib/jvm/java-7-oracle as default JAVA_HOME.
Note, this will be overridden by -java-home if it is set.
[info] Loading project definition from ~/spark/project/project
[info] Loading project definition from 
~/.sbt/0.13/staging/ec3aa8f39111944cc5f2/sbt-pom-reader/project
[warn] Multiple resolvers having different access mechanism configured 
with same name 'sbt-plugin-releases'. To avoid conflict, Remove 
duplicate project resolvers (`resolvers`) or rename publishing resolv
er (`publishTo`).
[info] Loading project definition from ~/spark/project
[info] Set current project to spark-parent (in build file:~/spark/)
[info] Compiling 372 Scala sources and 35 Java sources to 
~/spark/core/target/scala-2.10/classes...
[error] 
~/spark/core/src/main/scala/org/apache/spark/ui/jobs/JobProgressListener.scala:116: 
type mismatch;
[error]  found   : org.apache.spark.ui.jobs.TaskUIData
[error]  required: org.apache.spark.ui.jobs.UIData.TaskUIData
[error]       stageData.taskData.put(taskInfo.taskId, new 
TaskUIData(taskInfo))
[error]                                               ^
[error] 
~/spark/core/src/main/scala/org/apache/spark/ui/jobs/JobProgressListener.scala:134: 
type mismatch;
[error]  found   : org.apache.spark.ui.jobs.ExecutorSummary
[error]  required: org.apache.spark.ui.jobs.UIData.ExecutorSummary
[error]       val execSummary = 
execSummaryMap.getOrElseUpdate(info.executorId, new ExecutorSummary)
[error] ^
[error] 
~/spark/core/src/main/scala/org/apache/spark/ui/jobs/JobProgressListener.scala:163: 
type mismatch;
[error]  found   : org.apache.spark.ui.jobs.TaskUIData
[error]  required: org.apache.spark.ui.jobs.UIData.TaskUIData
[error]       val taskData = 
stageData.taskData.getOrElseUpdate(info.taskId, new TaskUIData(info))
[error] ^
[error] 
~/spark/core/src/main/scala/org/apache/spark/ui/jobs/JobProgressListener.scala:180: 
type mismatch;
[error]  found   : org.apache.spark.ui.jobs.ExecutorSummary
[error]  required: org.apache.spark.ui.jobs.UIData.ExecutorSummary
[error]     val execSummary = 
stageData.executorSummary.getOrElseUpdate(execId, new ExecutorSummary)
[error] ^
[error] 
~/spark/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala:109: type 
mismatch;
[error]  found   : org.apache.spark.ui.jobs.TaskUIData => 
Seq[scala.xml.Node]
[error]  required: org.apache.spark.ui.jobs.UIData.TaskUIData => 
Seq[scala.xml.Node]
[error] Error occurred in an application involving default arguments.
[error]         taskHeaders, taskRow(hasInput, hasShuffleRead, 
hasShuffleWrite, hasBytesSpilled), tasks)
[error]                             ^
[error] 
~/spark/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala:119: constructor 
cannot be instantiated to expected type;
[error]  found   : org.apache.spark.ui.jobs.TaskUIData
[error]  required: org.apache.spark.ui.jobs.UIData.TaskUIData
[error]           val serializationTimes = validTasks.map { case 
TaskUIData(_, metrics, _) =>
[error]                                                          ^
[error] 
~/spark/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala:120: not 
found: value metrics
[error]             metrics.get.resultSerializationTime.toDouble
[error]             ^

I think the code doesn't make correct reference to the updated structure.

""core/src/main/scala/org/apache/spark/ui/jobs/UIData.scala"" is 
introduced in commit 72e9021eaf26f31a82120505f8b764b18fbe8d48

Larry

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Sun, 3 Aug 2014 20:59:03 -0700",Re: Low Level Kafka Consumer for Spark,Dibyendu Bhattacharya <dibyendu.bhattachary@gmail.com>,"I'll let TD chime on on this one, but I'm guessing this would be a welcome
addition. It's great to see community effort on adding new
streams/receivers, adding a Java API for receivers was something we did
specifically to allow this :)

- Patrick



"
Patrick Wendell <pwendell@gmail.com>,"Sun, 3 Aug 2014 21:09:58 -0700",Re: Scala 2.11 external dependencies,Anand Avati <avati@gluster.org>,"Hey Anand,

Thanks for looking into this - it's great to see momentum towards Scala
2.11 and I'd love if this land in Spark 1.2.

For the external dependencies, it would be good to create a sub-task of
SPARK-1812 to track our efforts encouraging other projects to upgrade. In
certain cases (e.g. Kafka) there is fairly late-stage work on this already,
so we can e.g. link to those JIRA's as well. A good starting point is to
just go to their dev list and ask what the status is, most Scala projects
have put at least some thought into this already. Another thing we can do
is submit patches ourselves to those projects to help get them upgraded.
The twitter libraries, e.g., tend to be pretty small and also open to
external contributions.

looking at this, so it might be good for you two to connect (probably off
list) and sync up. Prashant has contributed to many Scala projects, so he
might have cycles to go and help some of our dependencies get upgraded -
but I won't commit to that on his behalf :).

Regarding Akka - I shaded and published akka as a one-off thing:
https://github.com/pwendell/akka/tree/2.2.3-shaded-proto

Over time we've had to publish our own versions of a small number of
dependencies. It's somewhat high overhead, but it actually works quite well
in terms of avoiding some of the nastier dependency conflicts. At least
better than other alternatives I've seen such as using a shader build
plug-in.

Going forward, I'd actually like to track these in the Spark repo itself.
For instance, we have a bash script in the spark repo that can e.g. check
out akka, apply a few patches or regular expressions, and then you have a
fully shaded dependency that can be published to maven. If you wanted to
take a crack at something like that for akka 2.3.4, be my guest. I can help
with the actual publishing.

- Patrick



"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 4 Aug 2014 00:12:18 -0400",Re: -1s on pull requests?,Patrick Wendell <pwendell@gmail.com>,":

Nick - Any interest in doing these? this is all doable from within the
I‚Äôll make the JIRA and think about how to do this stuff. I‚Äôll have to
understand what that run-tests-jenkins script does and see how easy it is
to extend.

Nick
‚Äã
"
Patrick Wendell <pwendell@gmail.com>,"Sun, 3 Aug 2014 21:43:46 -0700",Re: -1s on pull requests?,Nicholas Chammas <nicholas.chammas@gmail.com>,"Sure thing - feel free to ping me off list if you need pointers. The script
just does string concatenation and a curl to post the comment... I think it
should be pretty accessible!

- Patrick



"
Pei-Lun Lee <pllee@appier.com>,"Mon, 4 Aug 2014 14:53:30 +0800",Re: Spark SQL 1.0.1 error on reading fixed length byte array,"user@spark.apache.org, dev@spark.apache.org","Hi,

We have a PR to support fixed length byte array in parquet file.

https://github.com/apache/spark/pull/1737

Can someone help verifying it?

Thanks.

2014-07-15 19:23 GMT+08:00 Pei-Lun Lee <pllee@appier.com>:

"
Patrick Wendell <pwendell@gmail.com>,"Mon, 4 Aug 2014 01:15:53 -0700",Re: Issues with HDP 2.4.0.2.1.3.0-563,"""Ron's Yahoo!"" <zlgonzalez@yahoo.com.invalid>","For hortonworks, I believe it should work to just link against the
corresponding upstream version. I.e. just set the Hadoop version to ""2.4.0""

Does that work?

- Patrick



"
Sean Owen <sowen@cloudera.com>,"Mon, 4 Aug 2014 09:25:04 +0100",Re: Issues with HDP 2.4.0.2.1.3.0-563,Patrick Wendell <pwendell@gmail.com>,"For any Hadoop 2.4 distro, yes, set hadoop.version but also set
-Phadoop-2.4. http://spark.apache.org/docs/latest/building-with-maven.html


---------------------------------------------------------------------


"
Larry Xiao <xiaodi@sjtu.edu.cn>,"Mon, 04 Aug 2014 16:44:14 +0800",Re: Compiling Spark master (6ba6c3eb) with sbt/sbt assembly,"dev@spark.apache.org, user@spark.apache.org","I guessed
     ./sbt/sbt clean
and it works fine now.



---------------------------------------------------------------------


"
Larry Xiao <xiaodi@sjtu.edu.cn>,"Mon, 04 Aug 2014 16:48:01 +0800",Re: Compiling Spark master (6ba6c3eb) with sbt/sbt assembly,"dev@spark.apache.org, user@spark.apache.org","Sorry I mean, I tried this command
     ./sbt/sbt clean
and now it works.

Is it because of cached components no recompiled?



---------------------------------------------------------------------


"
Steve Nunez <snunez@hortonworks.com>,"Mon, 04 Aug 2014 07:13:23 -0700",Re: Issues with HDP 2.4.0.2.1.3.0-563,Ron's Yahoo! <zlgonzalez@yahoo.com.invalid>,"Provided youπve got the HWX repo in your pom.xml, you can build with this
line:

mvn -Pyarn -Phive -Phadoop-2.4 -Dhadoop.version=2.4.0.2.1.1.0-385
-DskipTests clean package

I havenπt tried building a distro, but it should be similar.


	- SteveN





-- 
CONFIDENTIALITY NOTICE
NOTICE: This message is intended for the use of the individual or entity to 
which it is addressed and may contain information that is confidential, 
privileged and exempt from disclosure under applicable law. If the reader 
of this message is not the intended recipient, you are hereby notified that 
any printing, copying, dissemination, distribution, disclosure or 
forwarding of this communication is strictly prohibited. If you have 
received this communication in error, please contact the sender immediately 
and delete it from your system. Thank You.

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Mon, 4 Aug 2014 09:44:49 -0700",Re: Issues with HDP 2.4.0.2.1.3.0-563,"""Ron's Yahoo!"" <zlgonzalez@yahoo.com.invalid>","Can you try building without any of the special `hadoop.version` flags and
just building only with -Phadoop-2.4? In the past users have reported
issues trying to build random spot versions... I think HW is supposed to be
compatible with the normal 2.4.0 build.



d
is
l
to
at
ly
"
Patrick Wendell <pwendell@gmail.com>,"Mon, 4 Aug 2014 10:08:52 -0700",Re: Issues with HDP 2.4.0.2.1.3.0-563,"""Ron's Yahoo!"" <zlgonzalez@yahoo.com>","Ah I see, yeah you might need to set hadoop.version and yarn.version. I
thought he profile set this automatically.



d
be
d
nd
t
his
r
"
Steve Nunez <snunez@hortonworks.com>,"Mon, 04 Aug 2014 10:21:19 -0700",Re: Issues with HDP 2.4.0.2.1.3.0-563,"Patrick Wendell <pwendell@gmail.com>,
	Ron's Yahoo! <zlgonzalez@yahoo.com>","I don°Øt think there is an hwx profile, but there probably should be.

- Steve

From:  Patrick Wendell <pwendell@gmail.com>
Date:  Monday, August 4, 2014 at 10:08
To:  Ron's Yahoo! <zlgonzalez@yahoo.com>
Cc:  Ron's Yahoo! <zlgonzalez@yahoo.com.invalid>, Steve Nunez
<snunez@hortonworks.com>, <user@spark.apache.org>, ""dev@spark.apache.org""
<dev@spark.apache.org>
Subject:  Re: Issues with HDP 2.4.0.2.1.3.0-563

Ah I see, yeah you might need to set hadoop.version and yarn.version. I
thought he profile set this automatically.


1.0.4
and
ssues
lid>
r the
 find the
e versions
not
3
:
with this
ow.
me.
rted
e:
ted
oKeyI
:111)
62)
62)
62)
7)
r.jav
or.ja
-
ity
l,
ader
d




-- 
CONFIDENTIALITY NOTICE
NOTICE: This message is intended for the use of the individual or entity to 
which it is addressed and may contain information that is confidential, 
privileged and exempt from disclosure under applicable law. If the reader 
of this message is not the intended recipient, you are hereby notified that 
any printing, copying, dissemination, distribution, disclosure or 
forwarding of this communication is strictly prohibited. If you have 
received this communication in error, please contact the sender immediately 
and delete it from your system. Thank You.
"
Matt Forbes <matt@tellapart.com>,"Mon, 4 Aug 2014 10:23:00 -0700",Problems running modified spark version on ec2 cluster,dev@spark.apache.org,"I'm trying to run a forked version of mllib where I am experimenting with a
boosted trees implementation. Here is what I've tried, but can't seem to
get working properly:

*Directory layout:*

src/spark-dev  (spark github fork)
  pom.xml - I've tried changing the version to 1.2 arbitrarily in core and
mllib
src/forestry  (test driver)
  pom.xml - depends on spark-core and spark-mllib with version 1.2

*spark-defaults.conf:*

spark.master                    spark://
ec2-54-224-112-117.compute-1.amazonaws.com:7077
spark.verbose                   true
spark.files.userClassPathFirst  false  # I've tried both true and false here
spark.executor-memory           6G
spark.jars
 spark-mllib_2.10-1.2.0-SNAPSHOT.jar,spark-core_2.10-1.2.0-SNAPSHOT.jar,spark-streaming_2.10-1.2.0-SNAPSHOT.jar

*Build and run script:*

MASTER=root@ec2-54-224-112-117.compute-1.amazonaws.com
PRIMARY_JAR=forestry-main-1.0-SNAPSHOT-jar-with-dependencies.jar
FORESTRY_DIR=~/src/forestry-main
SPARK_DIR=~/src/spark-dev
cd $SPARK_DIR
mvn -T8 -DskipTests -pl core,mllib,streaming install
cd $FORESTRY_DIR
mvn -T8 -DskipTests package
rsync --progress
~/src/spark-dev/mllib/target/spark-mllib_2.10-1.2.0-SNAPSHOT.jar $MASTER:
rsync --progress
~/src/spark-dev/core/target/spark-core_2.10-1.2.0-SNAPSHOT.jar $MASTER:
rsync --progress
~/src/spark-dev/streaming/target/spark-streaming_2.10-1.2.0-SNAPSHOT.jar
$MASTER:
rsync --progress ~/src/forestry-main/target/$PRIMARY_JAR $MASTER:
rsync --progress ~/src/forestry-main/spark-defaults.conf $MASTER:spark/conf
ssh $MASTER ""spark/bin/spark-submit $PRIMARY_JAR --class forestry.TreeTest
--verbose""

In spark-dev/mllib I've added a new class, GradientBoostingTree, which I'm
referencing from TreeTest in my test driver. The driver pulls some data
from s3, converts to LabeledPoint, and then calls
GradientBoostingTree.train(...) identically to how DecisionTree works. This
is all fine until it we call examples.map { x => tree.predict(x.features) }
where tree is a DecisionTree that I've also modified in my fork. At this
point, the workers blow up because they can't find a new method I've added
to the tree.model.Node class. My suspicion is that maybe the workers have
deserialized the DecisionTreeModel into a different version of mllib that
doesn't have my changes?

Is my setup all wrong? I'm using an EC2 cluster because it is so easy to
startup and manage, maybe I need to fully distribute my new version of
spark to all the workers before starting the job? Is there an easy way to
do that?
"
Sean Owen <sowen@cloudera.com>,"Mon, 4 Aug 2014 18:35:52 +0100",Re: Issues with HDP 2.4.0.2.1.3.0-563,Steve Nunez <snunez@hortonworks.com>,"What would such a profile do though? In general building for a
specific vendor version means setting hadoop.verison and/or
yarn.version. Any hard-coded value is unlikely to match what a
particular user needs. Setting protobuf versions and so on is already
done by the generic profiles.

In a similar vein, I am not clear on why there's a mapr profile in the
build. Its versions are about to be out of date and won't work with
upcoming Hbase changes for example.

(Elsewhere in the build I think it wouldn't hurt to clear out
cloudera-specific profiles and releases too -- they're not in the pom
but are in the distribution script. It's the vendor's problem.)

This isn't any argument about being purist but just that I am not sure
these are things that the project can meaningfully bother with.

It makes sense to set vendor repos in the pom for convenience, and
makes sense to run smoke tests in Jenkins against particular versions.

$0.02
Sean

be.

---------------------------------------------------------------------


"
Steve Nunez <snunez@hortonworks.com>,"Mon, 04 Aug 2014 10:39:13 -0700",Re: Issues with HDP 2.4.0.2.1.3.0-563,Sean Owen <sowen@cloudera.com>,"Hmm. Fair enough. I hadnπt given that answer much thought and on
reflection think youπre right in that a profile would just be a bad hack.







-- 
CONFIDENTIALITY NOTICE
NOTICE: This message is intended for the use of the individual or entity to 
which it is addressed and may contain information that is confidential, 
privileged and exempt from disclosure under applicable law. If the reader 
of this message is not the intended recipient, you are hereby notified that 
any printing, copying, dissemination, distribution, disclosure or 
forwarding of this communication is strictly prohibited. If you have 
received this communication in error, please contact the sender immediately 
and delete it from your system. Thank You.

---------------------------------------------------------------------


"
Matt Forbes <matt@tellapart.com>,"Mon, 4 Aug 2014 12:14:14 -0700",Re: Problems running modified spark version on ec2 cluster,dev@spark.apache.org,"After rummaging through the worker instances I noticed they were using the
assembly jar (which I hadn't noticed before). Now instead of using the core
and mllib jars individually, I'm just overwriting the assembly jar in the
master and using spark-ec2/copy-dir. For posterity, my run script is:

MASTER=root@ec2-54-224-110-72.compute-1.amazonaws.com
PRIMARY_JAR=forestry-main-1.0-SNAPSHOT-jar-with-dependencies.jar
ASSEMBLY_SRC=spark-assembly-1.1.0-SNAPSHOT-hadoop1.0.4.jar
ASSEMBLY_DEST=spark-assembly-1.0.1-hadoop1.0.4.jar
FORESTRY_DIR=~/src/forestry-main
SPARK_DIR=~/src/spark-dev
cd $SPARK_DIR
mvn -T8 -DskipTests -pl core,mllib,assembly install
cd $FORESTRY_DIR
mvn -T8 -DskipTests package
rsync --progress ~/src/spark-dev/assembly/target/scala-2.10/$ASSEMBLY_SRC
$MASTER:spark/lib/$ASSEMBLY_DEST
rsync --progress ~/src/forestry-main/target/$PRIMARY_JAR $MASTER:
rsync --progress ~/src/forestry-main/spark-defaults.conf $MASTER:spark/conf
ssh $MASTER ""spark-ec2/copy-dir --delete /root/spark/lib""
ssh $MASTER ""spark/bin/spark-submit $PRIMARY_JAR --class
com.ttforbes.TreeTest --verbose""




"
Anand Avati <avati@gluster.org>,"Mon, 4 Aug 2014 13:01:15 -0700",Re: Scala 2.11 external dependencies,Patrick Wendell <pwendell@gmail.com>,"

Will give it a try, thanks!
"
Reynold Xin <rxin@databricks.com>,"Mon, 4 Aug 2014 15:03:50 -0700",Re: Interested in contributing to GraphX in Python,"""dev@spark.apache.org"" <dev@spark.apache.org>","Thanks for your interest.

I think the main challenge is if we have to call Python functions per
record, it can be pretty expensive to serialize/deserialize across
boundaries of the Python process and JVM process.  I don't know if there is
a good way to solve this problem yet.






"
Yan Fang <yanfang724@gmail.com>,"Mon, 4 Aug 2014 15:51:33 -0700",Re: Low Level Kafka Consumer for Spark,Dibyendu Bhattacharya <dibyendu.bhattachary@gmail.com>,"Another suggestion that may help is that, you can consider use Kafka to
store the latest offset instead of Zookeeper. There are at least two
benefits: 1) lower the workload of ZK 2) support replay from certain
offset. This is how Samza <http://samza.incubator.apache.org/> deals with
the Kafka offset, the doc is here
<http://samza.incubator.apache.org/learn/documentation/0.7.0/container/checkpointing.html>
.
Thank you.

Cheers,

Fang, Yan
yanfang724@gmail.com
+1 (206) 849-4108



"
Dmitriy Lyubimov <dlieu.7@gmail.com>,"Mon, 4 Aug 2014 17:01:58 -0700","""log"" overloaded in SparkContext/ Spark 1.0.x","""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","it would seem the code like

import o.a.spark.SparkContext._
import math._

....

a = log(b)

does not seem to compile anymore with Spark 1.0.x since SparkContext._ also
exposes a `log` function. Which happens a lot to a guy like me.

obvious workaround is to use something like

import o.a.spark.SparkContext.{log => sparkLog,  _}

but wouldn't it be easier just to avoid so expected clash in the first
place?

thank you.
-d
"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 4 Aug 2014 17:41:54 -0700","Re: ""log"" overloaded in SparkContext/ Spark 1.0.x","""=?utf-8?Q?dev=40spark.incubator.apache.org?=""
 <dev@spark.incubator.apache.org>, Dmitriy Lyubimov <dlieu.7@gmail.com>","Hah, weird. ""log"" should be protected actually (look at trait Logging). Is your class extending SparkContext or somehow being placed in the org.apache.spark package? Or maybe the Scala compiler looks at it anyway.. in that case we can rename it. Please open a JIRA for it if that's the case.


it would seem the code like 

import o.a.spark.SparkContext._ 
import math._ 

.... 

a = log(b) 

does not seem to compile anymore with Spark 1.0.x since SparkContext._ also 
exposes a `log` function. Which happens a lot to a guy like me. 

obvious workaround is to use something like 

import o.a.spark.SparkContext.{log => sparkLog, _} 

but wouldn't it be easier just to avoid so expected clash in the first 
place? 

thank you. 
-d 
"
Larry Xiao <xiaodi@sjtu.edu.cn>,"Tue, 05 Aug 2014 14:57:09 +0800",GraphX partitioning and threading details,"shijiaxin.cn@gmail.com, dev@spark.apache.org","Hi all,

about GraphX partitioning details and possible optimization.

  * Can you tell how are partitions distributed to nodes? And inside
    worker, how does partitions get allocated to threads?
      o Is it possible to make manual configuration, like partition A =>
        node 1, thread 1
  * How is memory organized among threads?
      o Can we exploit the shared memory to combine mirror cache on same
        node into one?
      o (our experiment shows that more partitions requires much more
        memory)

Larry

"
Dibyendu Bhattacharya <dibyendu.bhattachary@gmail.com>,"Tue, 5 Aug 2014 14:44:36 +0530",Re: Low Level Kafka Consumer for Spark,"Jonathan Hodges <hodgesz@gmail.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Thanks Jonathan,

Yes, till non-ZK based offset management is available in Kafka, I need to
maintain the offset in ZK. And yes, both cases explicit commit is
necessary. I modified the Low Level Kafka Spark Consumer little bit to have
Receiver spawns threads for every partition of the topic and perform the
'store' operation in multiple threads. It would be good if the
receiver.store methods are made thread safe..which is not now presently .

Waiting for TD's comment on this Kafka Spark Low Level consumer.


Regards,
Dibyendu




"
Mridul Muralidharan <mridul@gmail.com>,"Tue, 5 Aug 2014 18:40:28 +0530",Re: -1s on pull requests?,dev@spark.apache.org,"Just came across this mail, thanks for initiating this discussion Kay.
To add; another issue which recurs is very rapid commit's: before most
contributors have had a chance to even look at the changes proposed.
There is not much prior discussion on the jira or pr, and the time
between submitting the PR and committing it is < 12 hours.

Particularly relevant when contributors are not on US timezones and/or
colocated; I have raised this a few times before when the commit had
other side effects not considered.
little or no activity from committers side - making the contribution
stale; so too long a delay is also definitely not the direction to
take either !



Regards,
Mridul




---------------------------------------------------------------------


"
"""Shao, Saisai"" <saisai.shao@intel.com>","Tue, 5 Aug 2014 14:31:57 +0000",RE: Low Level Kafka Consumer for Spark,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

I think this is an awesome feature for Spark Streaming Kafka interface to offer user the controllability of partition offset, so user can have more applications based on this.

What I concern is that if we want to do offset management, fault tolerant related control and others, we have to take the role as current ZookeeperConsumerConnect did, that would be a big field we should take care of, for example when node is failed, how to pass current partition to another consumer and some others. I‚Äôm not sure what is your thought?

Thanks
Jerry

From: Dibyendu Bhattacharya [mailto:dibyendu.bhattachary@gmail.com]
Sent: Tuesday, August 05, 2014 5:15 PM
To: Jonathan Hodges; dev@spark.apache.org
Cc: user
Subject: Re: Low Level Kafka Consumer for Spark

Thanks Jonathan,

Yes, till non-ZK based offset management is available in Kafka, I need to maintain the offset in ZK. And yes, both cases explicit commit is necessary. I modified the Low Level Kafka Spark Consumer little bit to have Receiver spawns threads for every partition of the topic and perform the 'store' operation in multiple threads. It would be good if the receiver.store methods are made thread safe..which is not now presently .

Waiting for TD's comment on this Kafka Spark Low Level consumer.


Regards,
Dibyendu


On Tue, Aug 5, 2014 at 5:32 AM, Jonathan Hodges <hodgesz@gmail.com<mailto:hodgesz@gmail.com>> wrote:
Hi Yan,

That is a good suggestion.  I believe non-Zookeeper offset management will be a feature in the upcoming Kafka 0.8.2 release tentatively scheduled for September.

https://cwiki.apache.org/confluence/display/KAFKA/Inbuilt+Consumer+Offset+Management

That should make this fairly easy to implement, but it will still require explicit offset commits to avoid data loss which is different than the current KafkaUtils implementation.

Jonathan




On Mon, Aug 4, 2014 at 4:51 PM, Yan Fang <yanfang724@gmail.com<mailto:yanfang724@gmail.com>> wrote:
Another suggestion that may help is that, you can consider use Kafka to store the latest offset instead of Zookeeper. There are at least two benefits: 1) lower the workload of ZK 2) support replay from certain offset. This is how Samza<http://samza.incubator.apache.org/> deals with the Kafka offset, the doc is here<http://samza.incubator.apache.org/learn/documentation/0.7.0/container/checkpointing.html> . Thank you.

Cheers,

Fang, Yan
yanfang724@gmail.com<mailto:yanfang724@gmail.com>
+1 (206) 849-4108<tel:%2B1%20%28206%29%20849-4108>

On Sun, Aug 3, 2014 at 8:59 PM, Patrick Wendell <pwendell@gmail.com<mailto:pwendell@gmail.com>> wrote:
I'll let TD chime on on this one, but I'm guessing this would be a welcome addition. It's great to see community effort on adding new streams/receivers, adding a Java API for receivers was something we did specifically to allow this :)

- Patrick

On Sat, Aug 2, 2014 at 10:09 AM, Dibyendu Bhattacharya <dibyendu.bhattachary@gmail.com<mailto:dibyendu.bhattachary@gmail.com>> wrote:
Hi,

I have implemented a Low Level Kafka Consumer for Spark Streaming using Kafka Simple Consumer API. This API will give better control over the Kafka offset management and recovery from failures. As the present Spark KafkaUtils uses HighLevel Kafka Consumer API, I wanted to have a better control over the offset management which is not possible in Kafka HighLevel consumer.

This Project is available in below Repo :

https://github.com/dibbhatt/kafka-spark-consumer


I have implemented a Custom Receiver consumer.kafka.client.KafkaReceiver. The KafkaReceiver uses low level Kafka Consumer API (implemented in consumer.kafka packages) to fetch messages from Kafka and 'store' it in Spark.

The logic will detect number of partitions for a topic and spawn that many threads (Individual instances of Consumers). Kafka Consumer uses Zookeeper for storing the latest offset for individual partitions, which will help to recover in case of failure. The Kafka Consumer logic is tolerant to ZK Failures, Kafka Leader of Partition changes, Kafka broker failures,  recovery from offset errors and other fail-over aspects.

The consumer.kafka.client.Consumer is the sample Consumer which uses this Kafka Receivers to generate DStreams from Kafka and apply a Output operation for every messages of the RDD.

We are planning to use this Kafka Spark Consumer to perform Near Real Time Indexing of Kafka Messages to target Search Cluster and also Near Real Time Aggregation using target NoSQL storage.

Kindly let me know your view. Also if this looks good, can I contribute to Spark Streaming project.

Regards,
Dibyendu




"
Erik Erlandson <eje@redhat.com>,"Tue, 5 Aug 2014 11:22:21 -0400 (EDT)","any interest in something like rdd.parent[T](n)  (equivalent to
 firstParent[T] for n==0) ?",dev@spark.apache.org,"Not that  rdd.dependencies(n).rdd.asInstanceOf[RDD[T]]  is terrible, but rdd.parent[T](n) better captures the intent.

---------------------------------------------------------------------


"
Dibyendu Bhattacharya <dibyendu.bhattachary@gmail.com>,"Tue, 5 Aug 2014 21:08:51 +0530",Re: Low Level Kafka Consumer for Spark,"""Shao, Saisai"" <saisai.shao@intel.com>","Hi

This fault tolerant aspect already taken care in the Kafka-Spark Consumer
code , like if Leader of a partition changes etc.. in ZkCoordinator.java.
Basically it does a refresh of PartitionManagers every X seconds to make
sure Partition details is correct and consumer don't fail.

Dib



re
ht?
ve
:
l
r
+Management
ckpointing.html>
e
ka
el
y
r
to
e
me
o
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 5 Aug 2014 11:48:26 -0400",Re: -1s on pull requests?,Patrick Wendell <pwendell@gmail.com>,"
FYI: Looks like Xiangrui's already got a JIRA issue for this.

SPARK-2622: Add Jenkins build numbers to SparkQA messages
<https://issues.apache.org/jira/browse/SPARK-2622>

2. ""Pin"" a message to the start or end of the PR


Should new JIRA issues for this item fall under the following umbrella
issue?

SPARK-2230: Improvements to Jenkins QA Harness
<https://issues.apache.org/jira/browse/SPARK-2230>

Nick
"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Tue, 5 Aug 2014 16:04:20 +0000",Spark maven project with the latest Spark jars,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

I'm trying to create a maven project that references the latest build of Spark.
1)downloaded sources and compiled the latest version of Spark.
2)added new spark-core jar to the a new local maven repo
3)created Scala maven project with net.alchim31.maven (scala-archetype-simple v 1.5)
4)added dependency to the new spark-core inside the pom.xml
5)I create SparkContext in the code of this project: val sc = new SparkContext(""local"", ""test"")
6)When I run it, I get the error:
Error:scalac: bad symbolic reference. A signature in RDD.class refers to term io
in package org.apache.hadoop which is not available.
It may be completely missing from the current classpath, or the version on
the classpath might be incompatible with the version used when compiling RDD.class.

This problem doesn't occur if I reference the spark-core from the maven repo. What am I doing wrong?

Best regards, Alexander
"
Debasish Das <debasish.das83@gmail.com>,"Tue, 5 Aug 2014 09:10:22 -0700",Re: Using mllib-1.1.0-SNAPSHOT on Spark 1.0.1,Xiangrui Meng <mengxr@gmail.com>,"I created the assembly file but still it wants to pick the mllib from the
cluster:

jar tf ./target/ml-0.0.1-SNAPSHOT-jar-with-dependencies.jar | grep
QuadraticMinimizer

org/apache/spark/mllib/optimization/QuadraticMinimizer$$anon$1.class

/Users/v606014/dist-1.0.1/bin/spark-submit --master
spark://TUSCA09LMLVT00C.local:7077 --class ALSDriver
./target/ml-0.0.1-SNAPSHOT-jar-with-dependencies.jar inputPath outputPath

Exception in thread ""main"" java.lang.NoSuchMethodError:
org.apache.spark.mllib.recommendation.ALS.setLambdaL1(D)Lorg/apache/spark/mllib/recommendation/ALS;

Now if I force it to use the jar that I gave
using spark.files.userClassPathFirst, then it fails on some serialization
issues...
A simple solution is to cherry pick the files I need from spark branch to
the application branch but I am not sure that's the right thing to do...

The way userClassPathFirst is behaving, there might be bugs in it...

Any suggestions will be appreciated....

Thanks.
Deb



"
Xiangrui Meng <mengxr@gmail.com>,"Tue, 5 Aug 2014 10:32:34 -0700",Re: -1s on pull requests?,Nicholas Chammas <nicholas.chammas@gmail.com>,"I think the build number is included in the SparkQA message, for
example: https://github.com/apache/spark/pull/1788

The build number 17941 is in the URL
""https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/17941/consoleFull"".
Just need to be careful to match the number.

Another solution is to kill running Jenkins jobs if there is a code change.


---------------------------------------------------------------------


"
Xiangrui Meng <mengxr@gmail.com>,"Tue, 5 Aug 2014 10:37:01 -0700",Re: Using mllib-1.1.0-SNAPSHOT on Spark 1.0.1,Debasish Das <debasish.das83@gmail.com>,"If you cannot change the Spark jar deployed on the cluster, an easy
solution would be renaming ALS in your jar. If userClassPathFirst
doesn't work, could you create a JIRA and attach the log? Thanks!
-Xiangrui


---------------------------------------------------------------------


"
Stephen Boesch <javadba@gmail.com>,"Tue, 5 Aug 2014 12:14:14 -0700",Tiny curiosity question on closing the jdbc connection,dev@spark.apache.org,"Within its compute.close method, the JdbcRDD class has this interesting
logic for closing jdbc connection:


      try {
        if (null != conn && ! stmt.isClosed()) conn.close()
        logInfo(""closed connection"")
      } catch {
        case e: Exception => logWarning(""Exception closing connection"", e)
      }

Notice that the second check is on stmt  having been closed - not on the
connection.

I would wager this were not a simple oversight and there were some
motivation for this logic- curious if anyone would be able to shed some
light?   My particular interest is that I have written custom ORM's in jdbc
since late 90's  and never did it this way.
"
Reynold Xin <rxin@databricks.com>,"Tue, 5 Aug 2014 13:03:42 -0700",Re: Tiny curiosity question on closing the jdbc connection,Stephen Boesch <javadba@gmail.com>,"I'm pretty sure it is an oversight. Would you like to submit a pull request
to fix that?




"
Stephen Boesch <javadba@gmail.com>,"Tue, 5 Aug 2014 13:11:59 -0700",Re: Tiny curiosity question on closing the jdbc connection,Reynold Xin <rxin@databricks.com>,"Thanks Reynold, Ted Yu did mention offline and I put in a jira already.
Another small concern: there appears to be no exception handling from the
creation of the prepared statement (line 74) through to the executeQuery
(line 86).   In case of error/exception it would seem to be leaking
connections (/statements).  If that were the case then I would include a
small patch for the exception trapping in that section of code as well.
 BTW I was looking at this code for another reason, not intending to be a
bother ;)




2014-08-05 13:03 GMT-07:00 Reynold Xin <rxin@databricks.com>:

"
Reynold Xin <rxin@databricks.com>,"Tue, 5 Aug 2014 13:15:10 -0700",Re: Tiny curiosity question on closing the jdbc connection,Stephen Boesch <javadba@gmail.com>,"Thanks. Those are definitely great problems to fix!




"
Cody Koeninger <cody@koeninger.org>,"Tue, 5 Aug 2014 15:58:12 -0500",Re: Tiny curiosity question on closing the jdbc connection,,"The stmt.isClosed just looks like stupidity on my part, no secret
motivation :)  Thanks for noticing it.

As for the leaking in the case of malformed statements, isn't that
addressed by


or am I misunderstanding?



"
Stephen Boesch <javadba@gmail.com>,"Tue, 5 Aug 2014 14:01:00 -0700",Re: Tiny curiosity question on closing the jdbc connection,Cody Koeninger <cody@koeninger.org>,"Hi yes that callback takes care of it. thanks!


2014-08-05 13:58 GMT-07:00 Cody Koeninger <cody@koeninger.org>:

"
Reynold Xin <rxin@databricks.com>,"Tue, 5 Aug 2014 14:01:33 -0700",Re: Tiny curiosity question on closing the jdbc connection,Cody Koeninger <cody@koeninger.org>,"Yes it is. I actually commented on it:
https://github.com/apache/spark/pull/1792/files#r15840899




"
Stephen Boesch <javadba@gmail.com>,"Tue, 5 Aug 2014 14:26:36 -0700",Re: Tiny curiosity question on closing the jdbc connection,Reynold Xin <rxin@databricks.com>,"The existing callback does take care of it:  within the DAGScheduler  there
is a finally block to ensure the callbacks are executed.

      try {
        val result = job.func(taskContext, rdd.iterator(split, taskContext))
        job.listener.taskSucceeded(0, result)
      } finally {
      }

So I have removed that exception handling code from the  PR and updated the
JIRA.


2014-08-05 14:01 GMT-07:00 Reynold Xin <rxin@databricks.com>:

"
Gurumurthy Yeleswarapu <guruyvs@yahoo.com.INVALID>,"Tue, 5 Aug 2014 14:43:04 -0700",Hello All,"""dev@spark.apache.org"" <dev@spark.apache.org>","Im new to Spark community. Actively working on Hadoop eco system ( more specifically YARN). I'm very keen on getting my hands dirtily with Spark. Please let me know any pointers to start with.

Thanks in advance
Best regards
Guru Yeleswarapu"
Burak Yavuz <byavuz@stanford.edu>,"Tue, 5 Aug 2014 15:52:23 -0700 (PDT)",Re: Hello All,Gurumurthy Yeleswarapu <guruyvs@yahoo.com>,"Hi Guru,

Take a look at:
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark

It has all the information you need on how to contribute to Spark. Also take a look at:
https://issues.apache.org/jira/browse/SPARK/?selectedTab=com.atlassian.jira.jira-projects-plugin:summary-panel

where a list of issues exist that need fixing. You can also request or propose new additions to Spark.

Happy coding!
Burak


Thanks in advance
Best regards
Guru Yeleswarapu


---------------------------------------------------------------------


"
Dmitriy Lyubimov <dlieu.7@gmail.com>,"Tue, 5 Aug 2014 17:27:32 -0700",Unit test best practice for Spark-derived projects,dev@spark.apache.org,"Hello,

I 've been switching Mahout from Spark 0.9 to Spark 1.0.x [1] and noticed
that tests now run much slower compared to 0.9 with CPU running idle most
of the time. I had to conclude that most of that time is spent on tearing
down/resetting Spark context which apparently now takes significantly
longer time in local mode than before.

Q1 --- Is there a way to mitigate long session startup times with local
context?

Q2 -- Our unit tests are basically mixing in a rip-off of
LocalSparkContext, and we are using local[3]. Looking into 1.0.x code, i
 noticed that a lot of Spark unit test code has switched to
SharedSparkContext (i.e. no context reset between individual tests). Is
that now recommended practice to write Spark-based unit tests?

Q3 -- Any other reasons that i may have missed for degraded test
performance?


[1] https://github.com/apache/mahout/pull/40

thank you in advance.
-Dmitriy
"
Debasish Das <debasish.das83@gmail.com>,"Tue, 5 Aug 2014 17:59:48 -0700",Re: Using mllib-1.1.0-SNAPSHOT on Spark 1.0.1,Xiangrui Meng <mengxr@gmail.com>,"Hi Xiangrui,

I used your idea and kept a cherry picked version of ALS.scala in my
application and call it ALSQp.scala...this is a OK workaround for now till
a version adds up to master for example...

For the bug with userClassPathFirst, looks like Koert already found this
issue in the following JIRA:

https://issues.apache.org/jira/browse/SPARK-1863

By the way the userClassPathFirst feature is very useful since I am sure
the deployed version of spark on a production cluster will always be the
last stable (core at 1.0.1 in my case) and people would like to deploy
SNAPSHOT versions of libraries that build on top of spark core (mllib,
streaming etc)...

Another way is to have a build option that deploys only the core and not
the libraries built upon core...

Do we have an option like that in make-distribution script ?

Thanks.
Deb



"
Anand Avati <avati@gluster.org>,"Tue, 5 Aug 2014 23:42:49 -0700",Re: Scala 2.11 external dependencies,Patrick Wendell <pwendell@gmail.com>,"
Patrick, I have a set of scripts at https://github.com/avati/spark-shaded
which generate shaded artifacts for akka 2.3.4 and transitive dependencies
under org.spark-project group for both Scala 2.10 and 2.11 in your local
maven repo ($HOME/.m2/...) Publishing these artifacts is necessary for
version to 2.11 will be smooth w.r.t akka (verified it on my system)

If you think it will be useful, I can submit a PR of those scripts into
spark.git/extras or somewhere.

Thanks
"
Debasish Das <debasish.das83@gmail.com>,"Wed, 6 Aug 2014 08:28:48 -0700",Re: Using mllib-1.1.0-SNAPSHOT on Spark 1.0.1,Xiangrui Meng <mengxr@gmail.com>,"Hi Xiangrui,

Maintaining another file will be a pain later so I deployed spark 1.0.1
without mllib and then my application jar bundles mllib 1.1.0-SNAPSHOT
along with the code changes for quadratic optimization...

Later the plan is to patch the snapshot mllib with the deployed stable
mllib...

There are 5 variants that I am experimenting with around 400M ratings
(daily data, monthly data I will update in few days)...

1. LS
2. NNLS
3. Quadratic with bounds
4. Quadratic with L1
5. Quadratic with equality and positivity

Now the ALS 1.1.0 snapshot runs fine but after completion on this step
ALS.scala:311

// Materialize usersOut and productsOut.
usersOut.count()

I am getting from one of the executors: java.lang.ClassCastException:
scala.Tuple1 cannot be cast to scala.Product2

I am debugging it further but I was wondering if this is due to RDD
compatibility within 1.0.1 and 1.1.0-SNAPSHOT ?

I have built the jars on my Mac which has Java 1.7.0_55 but the deployed
cluster has Java 1.7.0_45.

The flow runs fine on my localhost spark 1.0.1 with 1 worker. Can that Java
version mismatch cause this ?

Stack traces are below

Thanks.
Deb


Executor stacktrace:

org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$4.apply(CoGroupedRDD.scala:156)


org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$4.apply(CoGroupedRDD.scala:154)


scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)

        scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)

        org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:154)

        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)

        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)


org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)

        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)

        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)


org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)

        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)

        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)


org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)

        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)

        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)


org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:126)


org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:123)


scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)


scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)

        scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)


scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)

        org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:123)

        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)

        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)


org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)

        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)

        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)


org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)

        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)

        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)

        org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)

        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)

        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)


org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:158)


org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)

        org.apache.spark.scheduler.Task.run(Task.scala:51)


org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)


java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)


java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

        java.lang.Thread.run(Thread.java:744)

Driver stacktrace:

at org.apache.spark.scheduler.DAGScheduler.org
$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1044)

at
org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1028)

at
org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1026)

at
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)

at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)

at
org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1026)

at
org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:634)

at
org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:634)

at scala.Option.foreach(Option.scala:236)

at
org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:634)

at
org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1229)

at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)

at akka.actor.ActorCell.invoke(ActorCell.scala:456)

at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)

at akka.dispatch.Mailbox.run(Mailbox.scala:219)

at
akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)

at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)

at
scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)

at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)

 at
scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)



"
Xiangrui Meng <mengxr@gmail.com>,"Wed, 6 Aug 2014 09:09:25 -0700",Re: Using mllib-1.1.0-SNAPSHOT on Spark 1.0.1,Debasish Das <debasish.das83@gmail.com>,"version of a Spark component on top of a older version of Spark core.
I don't remember any code change in MLlib that requires Spark v1.1 but
I might miss some PRs. There were changes to CoGroup, which may be
relevant:

https://github.com/apache/spark/commits/master/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala

Btw, for the constrained optimization, I'm really interested in how
they differ in the final recommendation? It would be great if you can
test prec@k or ndcg@k metrics.

Best,
Xiangrui


---------------------------------------------------------------------


"
Debasish Das <debasish.das83@gmail.com>,"Wed, 6 Aug 2014 09:33:49 -0700",Re: Using mllib-1.1.0-SNAPSHOT on Spark 1.0.1,Xiangrui Meng <mengxr@gmail.com>,"Ok...let me look into it a bit more and most likely I will deploy the Spark
v1.1 and then use mllib 1.1 SNAPSHOT jar with it so that we follow your
guideline of not running newer spark component on older version of spark
core...

That should solve this issue unless it is related to Java versions....

I am also keen to see the final recommendation within L1 and
Positivity....I will compute the metrics

Our plan is to use scalable matrix factorization as an engine to do
clustering, feature extraction, topic modeling and auto encoders (single
layer to start with). So these algorithms are not really constrained to
recommendation use-cases...




"
DB Tsai <dbtsai@dbtsai.com>,"Wed, 6 Aug 2014 09:45:26 -0700",Re: Using mllib-1.1.0-SNAPSHOT on Spark 1.0.1,Debasish Das <debasish.das83@gmail.com>,"use hadoop api directly)? Can I use mllib jar compile for one version of
hadoop and use it in another version of hadoop?

Sent from my Google Nexus 5

"
Debasish Das <debasish.das83@gmail.com>,"Wed, 6 Aug 2014 12:01:51 -0700",Re: Using mllib-1.1.0-SNAPSHOT on Spark 1.0.1,DB Tsai <dbtsai@dbtsai.com>,"I did not play with Hadoop settings...everything is compiled with
2.3.0CDH5.0.2 for me...

I did try to bump the version number of HBase from 0.94 to 0.96 or 0.98 but
there was no profile for CDH in the pom...but that's unrelated to this !



"
Ron Gonzalez <zlgonzalez@yahoo.com.INVALID>,"Wed, 6 Aug 2014 12:29:21 -0700",Buidling spark in Eclipse Kepler,Dev <dev@spark.apache.org>,"Hi,
† I'm trying to get the apache spark trunk compiling in my Eclipse, but I can't seem to get it going. In particular, I've tried sbt/sbt eclipse, but it doesn't seem to create the eclipse pieces for yarn and other projects. Doing mvn eclipse:eclipse on yarn seems to fail as well as sbt/sbt eclipse just for yarn fails. Is there some documentation available for eclipse? I've gone through the ones on the site, but to no avail.
† Any tips?

Thanks,
Ron"
Ted Yu <yuzhihong@gmail.com>,"Wed, 6 Aug 2014 13:17:27 -0700",compilation error in Catalyst module,"""dev@spark.apache.org"" <dev@spark.apache.org>","I refreshed my workspace.
I got the following error with this command:

mvn -Pyarn -Phive -Phadoop-2.4 -DskipTests install

[ERROR] bad symbolic reference. A signature in package.class refers to term
scalalogging
in package com.typesafe which is not available.
It may be completely missing from the current classpath, or the version on
the classpath might be incompatible with the version used when compiling
package.class.
[ERROR]
/homes/hortonzy/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/package.scala:36:
bad symbolic reference. A signature in package.class refers to term slf4j
in value com.typesafe.scalalogging which is not available.
It may be completely missing from the current classpath, or the version on
the classpath might be incompatible with the version used when compiling
package.class.
[ERROR] package object trees extends Logging {
[ERROR]                              ^
[ERROR] two errors found

Has anyone else seen the above ?

Thanks
"
Zongheng Yang <zongheng.y@gmail.com>,"Wed, 6 Aug 2014 13:36:44 -0700",Re: compilation error in Catalyst module,Ted Yu <yuzhihong@gmail.com>,"Hi Ted,

By refreshing do you mean you have done 'mvn clean'?


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Wed, 6 Aug 2014 21:43:00 +0100",Re: Buidling spark in Eclipse Kepler,Ron Gonzalez <zlgonzalez@yahoo.com>,"I think your best bet by far is to consume the Maven build as-is from
within Eclipse. I wouldn't try to export a project config from the
build as there is plenty to get lost in translation.

Certainly this works well with IntelliJ, and by the by, if you have a
choice, I would strongly recommend IntelliJ over Eclipse for working
with Maven and Scala.

 can't seem to get it going. In particular, I've tried sbt/sbt eclipse, but it doesn't seem to create the eclipse pieces for yarn and other projects. Doing mvn eclipse:eclipse on yarn seems to fail as well as sbt/sbt eclipse just for yarn fails. Is there some documentation available for eclipse? I've gone through the ones on the site, but to no avail.

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Wed, 6 Aug 2014 13:54:04 -0700",Re: compilation error in Catalyst module,Zongheng Yang <zongheng.y@gmail.com>,"Forgot to do that step.

Now compilation passes.



"
Tathagata Das <tathagata.das1565@gmail.com>,"Wed, 6 Aug 2014 16:16:24 -0700",Re: Low Level Kafka Consumer for Spark,Dibyendu Bhattacharya <dibyendu.bhattachary@gmail.com>,"Hi Dibyendu,

This is really awesome. I am still yet to go through the code to understand
the details, but I want to do it really soon. In particular, I want to
understand the improvements, over the existing Kafka receiver.

And its fantastic to see such contributions from the community. :)

TD


to
re
nt
ught?
to
e
 .
+Management
re
h
ckpointing.html
r.
p
is
"
Matt Forbes <matt@tellapart.com>,"Wed, 6 Aug 2014 18:11:31 -0700",Documentation confusing or incorrect for decision trees?,dev@spark.apache.org,"I found the section on ordering categorical features really interesting,
but the A, B, C example seemed inconsistent. Am I interpreting this passage
wrong, or are there typos? Aren't the split candidates A | C, B and A, C |
B ?

For example, for a binary classification problem with one categorical
feature with three categories A, B and C with corresponding proportion of
label 1 as 0.2, 0.6 and 0.4, the categorical features are ordered as A
followed by C followed B or A, B, C. The two split candidates are A | C, B
and A , B | C where | denotes the split.
"
Graham Dennis <graham.dennis@gmail.com>,"Thu, 7 Aug 2014 11:47:45 +1000",[SPARK-2878] Kryo serialisation with custom Kryo registrator failing,dev@spark.apache.org,"Hi Spark devs,

I‚Äôve posted an issue on JIRA (
https://issues.apache.org/jira/browse/SPARK-2878) which occurs when using
Kryo serialisation with a custom Kryo registrator to register custom
classes with Kryo.  This is an insidious issue that non-deterministically
causes Kryo to have different ID number => class name maps on different
nodes, which then causes weird exceptions (ClassCastException,
ClassNotFoundException, ArrayIndexOutOfBoundsException) at deserialisation
time.  I‚Äôve created a reliable reproduction for the issue here:
https://github.com/GrahamDennis/spark-kryo-serialisation

I‚Äôm happy to try and put a pull request together to try and address this,
but it‚Äôs not obvious to me the right way to solve this and I‚Äôd like to get
feedback / ideas on how to address this.

The root cause of the problem is a ""Failed to run spark.kryo.registrator‚Äù
error which non-deterministically occurs in some executor processes during
operation.  My custom Kryo registrator is in the application jar, and it is
accessible on the worker nodes.  This is demonstrated by the fact that most
of the time the custom kryo registrator is successfully run.

What‚Äôs happening is that Kryo serialisation/deserialisation is happening
most of the time on an ‚ÄúExecutor task launch worker‚Äù thread, which has the
thread's class loader set to contain the application jar.  This happens in
`org.apache.spark.executor.Executor.TaskRunner.run`, and from what I can
tell, it is only these threads that have access to the application jar
(that contains the custom Kryo registrator).  However, the
ConnectionManager threads sometimes need to serialise/deserialise objects
to satisfy ‚ÄúgetBlock‚Äù requests when the objects haven‚Äôt previously been
serialised.  As the ConnectionManager threads don‚Äôt have the application
jar available from their class loader, when it tries to look up the custom
Kryo registrator, this fails.  Spark then swallows this exception, which
results in a different ID number ‚Äî> class mapping for this kryo instance,
and this then causes deserialisation errors later on a different node.

A related issue to the issue reported in SPARK-2878 is that Spark probably
shouldn‚Äôt swallow the ClassNotFound exception for custom Kryo registrators.
 The user has explicitly specified this class, and if it deterministically
can‚Äôt be found, then it may cause problems at serialisation /
deserialisation time.  If only sometimes it can‚Äôt be found (as in this
case), then it leads to a data corruption issue later on.  Either way,
we‚Äôre better off dying due to the ClassNotFound exception earlier, than the
weirder errors later on.

I have some ideas on potential solutions to this issue, but I‚Äôm keen for
experienced eyes to critique these approaches:

1. The simplest approach to fixing this would be to just make the
application jar available to the connection manager threads, but I‚Äôm
guessing it‚Äôs a design decision to isolate the application jar to just the
executor task runner threads.  Also, I don‚Äôt know if there are any other
threads that might be interacting with kryo serialisation / deserialisation.
2. Before looking up the custom Kryo registrator, change the thread‚Äôs class
loader to include the application jar, then restore the class loader after
the kryo registrator has been run.  I don‚Äôt know if this would have any
other side-effects.
3. Always serialise / deserialise on the existing TaskRunner threads,
rather than delaying serialisation until later, when it can be done only if
needed.  This approach would probably have negative performance
consequences.
4. Create a new dedicated thread pool for lazy serialisation /
deserialisation that has the application jar on the class path.
 Serialisation / deserialisation would be the only thing these threads do,
and this would minimise conflicts / interactions between the application
jar and other jars.

#4 sounds like the best approach to me, but I think would require
considerable knowledge of Spark internals, which is beyond me at present.
 Does anyone have any better (and ideally simpler) ideas?

Cheers,

Graham
"
Reynold Xin <rxin@databricks.com>,"Wed, 6 Aug 2014 18:53:39 -0700",Re: [SPARK-2878] Kryo serialisation with custom Kryo registrator failing,Graham Dennis <graham.dennis@gmail.com>,"I don't think it was a conscious design decision to not include the
application classes in the connection manager serializer. We should fix
that. Where is it deserializing data in that thread?

4 might make sense in the long run, but it adds a lot of complexity to the
code base (whole separate code base, task queue, blocking/non-blocking
logic within task threads) that can be error prone, so I think it is best
to stay away from that right now.






n
ss this,
Äôd like to get
‚Äù
g
is
st
ppening
ad, which has the
n
Äôt previously been
ication
m
nstance,
y
istrators.
y
 this
, than the
een for
ôm
 just the
y other
ôs class
r
ve any
if
,
"
Graham Dennis <graham.dennis@gmail.com>,"Thu, 7 Aug 2014 12:01:53 +1000",Re: [SPARK-2878] Kryo serialisation with custom Kryo registrator failing,Reynold Xin <rxin@databricks.com>,"See my comment on https://issues.apache.org/jira/browse/SPARK-2878 for the
full stacktrace, but it's in the BlockManager/BlockManagerWorker where it's
trying to fulfil a ""getBlock"" request for another node.  The objects that
would be in the block haven't yet been serialised, and that then causes the
deserialisation to happen on that thread.  See MemoryStore.scala:102.



e
g
y
t
on
ess this,
Äôd like to get
‚Äù
ng
appening
ead, which has the
in
s
Äôt previously been
lication
om
instance,
ly
ly
n this
r, than
keen for
ôm
o just the
ny other
Äôs
er
ave any
o,
.
"
Ron Gonzalez <zlgonzalez@yahoo.com.INVALID>,"Wed, 6 Aug 2014 19:22:49 -0700",Re: Buidling spark in Eclipse Kepler,Sean Owen <sowen@cloudera.com>,"Ok I'll give it a little more time, and if I can't get it going, I'll switch. I am indeed a little disappointed in the Scala IDE plugin for Eclipse so I think switching to IntelliJ might be my best bet.

Thanks,
Ron

Sent from my iPad

an't seem to get it going. In particular, I've tried sbt/sbt eclipse, but it doesn't seem to create the eclipse pieces for yarn and other projects. Doing mvn eclipse:eclipse on yarn seems to fail as well as sbt/sbt eclipse just for yarn fails. Is there some documentation available for eclipse? I've gone through the ones on the site, but to no avail.

---------------------------------------------------------------------


"
Ron Gonzalez <zlgonzalez@yahoo.com.INVALID>,"Wed, 6 Aug 2014 20:53:57 -0700",Re: Buidling spark in Eclipse Kepler,"Ron Gonzalez <zlgonzalez@yahoo.com>, Dev <dev@spark.apache.org>","So I downloaded community edition of IntelliJ, and ran sbt/sbt gen-idea.
I then imported the pom.xml file.
I'm still getting all sorts of errors from IntelliJ about unresolved dependencies.
Any suggestions?

Thanks,pache spark trunk compiling in my Eclipse, but I can't seem to get it going. In particular, I've tried sbt/sbt eclipse, but it doesn't seem to create the eclipse pieces for yarn and other projects. Doing mvn eclipse:eclipse on yarn seems to fail as well as sbt/sbt eclipse just for yarn fails. Is there some documentation available for eclipse? I've gone through the ones on the site, but to no avail.
† Any tips?

Thanks,
Ron"
DB Tsai <dbtsai@dbtsai.com>,"Wed, 6 Aug 2014 21:26:51 -0700",Re: Buidling spark in Eclipse Kepler,Ron Gonzalez <zlgonzalez@yahoo.com>,"After sbt gen-idea , you can open the intellji project directly without
going through pom.xml

If u want to compile inside intellji, you have to remove one of the messo
jar. This is an open issue, and u can find the detail in JIRA.

Sent from my Google Nexus 5

"
Ron Gonzalez <zlgonzalez@yahoo.com.INVALID>,"Wed, 6 Aug 2014 21:38:47 -0700",Re: Buidling spark in Eclipse Kepler,DB Tsai <dbtsai@dbtsai.com>,"Thanks, will give that a try.

Sent from my iPad

ing through pom.xml
ar. This is an open issue, and u can find the detail in JIRA.
te:
ndencies.
 can't seem to get it going. In particular, I've tried sbt/sbt eclipse, but it doesn't seem to create the eclipse pieces for yarn and other projects. Doing mvn eclipse:eclipse on yarn seems to fail as well as sbt/sbt eclipse just for yarn fails. Is there some documentation available for eclipse? I've gone through the ones on the site, but to no avail.
"
Sean Owen <sowen@cloudera.com>,"Thu, 7 Aug 2014 07:15:05 +0100",Re: Buidling spark in Eclipse Kepler,Ron Gonzalez <zlgonzalez@yahoo.com>,"(Don't use gen-idea, just open it directly as a Maven project in IntelliJ.)

ndencies.
 can't seem to get it going. In particular, I've tried sbt/sbt eclipse, but it doesn't seem to create the eclipse pieces for yarn and other projects. Doing mvn eclipse:eclipse on yarn seems to fail as well as sbt/sbt eclipse just for yarn fails. Is there some documentation available for eclipse? I've gone through the ones on the site, but to no avail.

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Thu, 7 Aug 2014 07:19:10 +0100",Re: Documentation confusing or incorrect for decision trees?,Matt Forbes <matt@tellapart.com>,"It's definitely just a typo. The ordered categories are A, C, B so the
other split can't be A | B, C. Just open a PR.


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Wed, 6 Aug 2014 23:23:19 -0700",[SNAPSHOT] Snapshot1 of Spark 1.1.0 has been posted,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi All,

I've packaged and published a snapshot release of Spark 1.1 for testing.
This is being distributed to the community for QA and preview purposes. It
is not yet an official RC for voting. Going forward, we'll do preview
releases like this for testing ahead of official votes.

The tag of this release is v1.1.0-snapshot1 (commit d428d8):
*https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=d428d88418d385d1d04e1b0adcb6b068efe9c7b0
<https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=d428d88418d385d1d04e1b0adcb6b068efe9c7b0>*

The release files, including signatures, digests, etc can be found at:
*http://people.apache.org/~pwendell/spark-1.1.0-snapshot1/
<http://people.apache.org/~pwendell/spark-1.1.0-snapshot1/>*

Release artifacts are signed with the following key:
*https://people.apache.org/keys/committer/pwendell.asc
<https://people.apache.org/keys/committer/pwendell.asc>*

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1025/
<https://repository.apache.org/content/repositories/orgapachespark-1024/>

NOTE: Due to SPARK-2899, docs are not yet available for this release. Docs
will be posted ASAP.

To learn more about Apache Spark, please see
http://spark.apache.org/
"
Patrick Wendell <pwendell@gmail.com>,"Wed, 6 Aug 2014 23:24:46 -0700",Re: [SNAPSHOT] Snapshot1 of Spark 1.1.0 has been posted,"""dev@spark.apache.org"" <dev@spark.apache.org>","Minor correction: the encoded URL in the staging repo link was wrong.
The correct repo is:
https://repository.apache.org/content/repositories/orgapachespark-1025/


:
This is being distributed to the community for QA and preview purposes. It is not yet an official RC for voting. Going forward, we'll do preview releases like this for testing ahead of official votes.
d88418d385d1d04e1b0adcb6b068efe9c7b0
s will be posted ASAP.

---------------------------------------------------------------------


"
Madhu <madhu@madhu.com>,"Thu, 7 Aug 2014 07:07:59 -0700 (PDT)",Re: Buidling spark in Eclipse Kepler,dev@spark.incubator.apache.org,"Ron,

I was able to build core in Eclipse following these steps:

https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-Eclipse

I was working only on core, so I know that works in Eclipse Juno.
I haven't tried yarn or other Eclipse releases.
Are you able to build *core* in Eclipse Kepler?

In my view, tool independence is a good thing.
I'll do what I can to support Eclipse.



-----
--
Madhu
https://www.linkedin.com/in/msiddalingaiah
--

---------------------------------------------------------------------


"
Madhu <madhu@madhu.com>,"Thu, 7 Aug 2014 10:29:42 -0700 (PDT)",Re: Unit test best practice for Spark-derived projects,dev@spark.incubator.apache.org,"How long does it take to get a spark context?
I found that if you don't have a network connection (reverse DNS lookup most
likely), it can take up 30 seconds to start up locally. I think a hosts file
entry is sufficient.



-----
--
Madhu
https://www.linkedin.com/in/msiddalingaiah
--

---------------------------------------------------------------------


"
Dmitriy Lyubimov <dlieu.7@gmail.com>,"Thu, 7 Aug 2014 10:45:45 -0700",Re: Unit test best practice for Spark-derived projects,Madhu <madhu@madhu.com>,"Thanks.

let me check this hypothesis (i have dhcp connection on a private net but
consequently not sure if there's an inverse).



"
Cody Koeninger <cody@koeninger.org>,"Thu, 7 Aug 2014 16:35:56 -0500",Re: replacement for SPARK_JAVA_OPTS,"""dev@spark.apache.org"" <dev@spark.apache.org>","Just wanted to check in on this, see if I should file a bug report
regarding the mesos argument propagation.



"
Marcelo Vanzin <vanzin@cloudera.com>,"Thu, 7 Aug 2014 14:47:14 -0700",Re: replacement for SPARK_JAVA_OPTS,Cody Koeninger <cody@koeninger.org>,"Andrew has been working on a fix:
https://github.com/apache/spark/pull/1770




-- 
Marcelo

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Thu, 7 Aug 2014 16:29:00 -0700",Re: Unit test best practice for Spark-derived projects,Dmitriy Lyubimov <dlieu.7@gmail.com>,"In the past I've found if I do a jstack when running some tests, it
sits forever inside of a hostname resolution step or something. I
never narrowed it down, though.

- Patrick


---------------------------------------------------------------------


"
Ron Gonzalez <zlgonzalez@yahoo.com.INVALID>,"Thu, 7 Aug 2014 16:58:51 -0700",Re: Buidling spark in Eclipse Kepler,Sean Owen <sowen@cloudera.com>,"So I opened it as a maven project (I opened it using the top-level pom.xml file), but rebuilding the project ends up in all sorts of errors about unrehu, Aug 7, 2014 at 4:53 AM, Ron Gonzalez
<zlgonzalez@yahoo.com.invalid> wrote:
> So I downloaded community edition of IntelliJ, and ran sbt/sbt gen-idea.
> I then imported the pom.xml file.
> I'm still getting all sorts of errors from IntelliJ about unresolved dependencies.
> Any suggestio
>†  I'm trying to get the apache spark trunk compiling in my Eclipse, but I can't seem to get it going. In particular, I've tried sbt/sbt eclipse, but it doesn't seem to create the eclipse pieces for yarn and other projects. Doing mvn eclipse:eclipse on yarn seems to fail as well as sbt/sbt eclipse just for yarn fails. Is there some documentation available for eclipse? I've gone through the ones on the site, but to no avail.
>†  Any tips?
>
> Thanks,
> Ron

--------------------------------------------"
Gary Malouf <malouf.gary@gmail.com>,"Thu, 7 Aug 2014 20:30:58 -0400",Re: replacement for SPARK_JAVA_OPTS,Marcelo Vanzin <vanzin@cloudera.com>,"Can this be cherry-picked for 1.1 if everything works out?  In my opinion,
it could be qualified as a bug fix.



"
Andrew Or <andrew@databricks.com>,"Thu, 7 Aug 2014 19:42:35 -0700",Re: replacement for SPARK_JAVA_OPTS,Gary Malouf <malouf.gary@gmail.com>,"Thanks Marcelo, I have moved the changes to a new PR to describe the
problems more clearly: https://github.com/apache/spark/pull/1845

@Gary Yeah, the goal is to get this into 1.1 as a bug fix.


2014-08-07 17:30 GMT-07:00 Gary Malouf <malouf.gary@gmail.com>:

"
Jun Feng Liu <liujunf@cn.ibm.com>,"Fri, 8 Aug 2014 10:50:45 +0800",Re: Fine-Grained Scheduler on Yarn,"""dev@spark.apache.org"" <dev@spark.apache.org>","Any one know the answer?
 
Best Regards
 
Jun Feng Liu
IBM China Systems & Technology Laboratory in Beijing



Phone: 86-10-82452683 
E-mail: liujunf@cn.ibm.com


BLD 28,ZGC Software Park 
No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193 
China 
 

 



Jun Feng Liu/China/IBM 
2014/08/07 15:37

To
dev@spark.apache.org, 
cc

Subject
Fine-Grained Scheduler on Yarn





Hi, there

Just aware right now Spark only support fine grained scheduler on Mesos 
with MesosSchedulerBackend. The Yarn schedule sounds like only works on 
coarse-grained model. Is there any plan to implement fine-grained 
scheduler for YARN? Or there is any technical issue block us to do that.
 
Best Regards
 
Jun Feng Liu
IBM China Systems & Technology Laboratory in Beijing



Phone: 86-10-82452683 
E-mail: liujunf@cn.ibm.com


BLD 28,ZGC Software Park 
No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193 
China 
 

 
"
Andrew Or <andrew@databricks.com>,"Thu, 7 Aug 2014 19:55:28 -0700",Re: replacement for SPARK_JAVA_OPTS,Gary Malouf <malouf.gary@gmail.com>,"@Cody I took a quick glance at the Mesos code and it appears that we
currently do not even pass extra java options to executors except in coarse
grained mode, and even in this mode we do not pass them to executors
correctly. I have filed a related JIRA he"
Patrick Wendell <pwendell@gmail.com>,"Thu, 7 Aug 2014 21:42:39 -0700",Re: replacement for SPARK_JAVA_OPTS,Andrew Or <andrew@databricks.com>,"Andrew - I think your JIRA may duplicate existing work:
https://github.com/apache/spark/pull/1513



---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Thu, 7 Aug 2014 21:43:38 -0700",Re: Fine-Grained Scheduler on Yarn,Jun Feng Liu <liujunf@cn.ibm.com>,"The current YARN is equivalent to what is called ""fine grained"" mode in
Mesos. The scheduling of tasks happens totally inside of the Spark driver.



"
Patrick Wendell <pwendell@gmail.com>,"Thu, 7 Aug 2014 22:10:55 -0700",Re: Fine-Grained Scheduler on Yarn,Jun Feng Liu <liujunf@cn.ibm.com>,"Hey sorry about that - what I said was the opposite of what is true.

The current YARN mode is equivalent to ""coarse grained"" mesos. There is no
fine-grained scheduling on YARN at the moment. I'm not sure YARN supports
scheduling in units other than containers. Fine-grained scheduling requires
scheduling at the granularity of individual cores.



"
Andrew Or <andrew@databricks.com>,"Thu, 7 Aug 2014 22:19:00 -0700",Re: replacement for SPARK_JAVA_OPTS,Patrick Wendell <pwendell@gmail.com>,"Ah, great to know this is already being fixed. Thanks Patrick, I have
marked my JIRA as a duplicate.


2014-08-07 21:42 GMT-07:00 Patrick Wendell <pwendell@gmail.com>:

"
Jun Feng Liu <liujunf@cn.ibm.com>,"Fri, 8 Aug 2014 13:56:00 +0800",Re: Fine-Grained Scheduler on Yarn,Patrick Wendell <pwendell@gmail.com>,"Thanks for echo on this. Possible to adjust resource based on container 
numbers? e.g to allocate more container when driver need more resources 
and return some resource by delete some container when parts of container 
already have enough cores/memory 
 
Best Regards
 
Jun Feng Liu
IBM China Systems & Technology Laboratory in Beijing



Phone: 86-10-82452683 
E-mail: liujunf@cn.ibm.com


BLD 28,ZGC Software Park 
No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193 
China 
 

 



Patrick Wendell <pwendell@gmail.com> 
2014/08/08 13:10

To
Jun Feng Liu/China/IBM@IBMCN, 
cc
""dev@spark.apache.org"" <dev@spark.apache.org>
Subject
Re: Fine-Grained Scheduler on Yarn






Hey sorry about that - what I said was the opposite of what is true.

The current YARN mode is equivalent to ""coarse grained"" mesos. There is no 
fine-grained scheduling on YARN at the moment. I'm not sure YARN supports 
scheduling in units other than containers. Fine-grained scheduling 
requires scheduling at the granularity of individual cores.


The current YARN is equivalent to what is called ""fine grained"" mode in 
Mesos. The scheduling of tasks happens totally inside of the Spark driver.


Any one know the answer?
Best Regards 
  
Jun Feng Liu
IBM China Systems & Technology Laboratory in Beijing 



Phone: 86-10-82452683 
E-mail: liujunf@cn.ibm.com 


BLD 28,ZGC Software Park 
No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193 
China 
 

  



Jun Feng Liu/China/IBM 
2014/08/07 15:37 


To
dev@spark.apache.org, 
cc

Subject
Fine-Grained Scheduler on Yarn







Hi, there 

Just aware right now Spark only support fine grained scheduler on Mesos 
with MesosSchedulerBackend. The Yarn schedule sounds like only works on 
coarse-grained model. Is there any plan to implement fine-grained 
scheduler for YARN? Or there is any technical issue block us to do that.
Best Regards 
  
Jun Feng Liu
IBM China Systems & Technology Laboratory in Beijing 



Phone: 86-10-82452683 
E-mail: liujunf@cn.ibm.com 


BLD 28,ZGC Software Park 
No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193 
China 
 

  


"
Matei Zaharia <matei@databricks.com>,"Thu, 7 Aug 2014 21:57:30 -0700",Welcoming two new committers,"""=?utf-8?Q?dev=40spark.incubator.apache.org?=""
 <dev@spark.incubator.apache.org>","Hi everyone,

The PMC recently voted to add two new committers and PMC members: Joey Gonzalez and Andrew Or. Both have been huge contributors in the past year -- Joey on much of GraphX as well as quite a bit of the initial work in MLlib, and Andrew on Spark Core. Join me in welcoming them as committers!

Matei




---------------------------------------------------------------------


"
Joseph Gonzalez <jegonzal@eecs.berkeley.edu>,"Thu, 7 Aug 2014 22:39:20 -0700",Re: Welcoming two new committers,Matei Zaharia <matei@databricks.com>,"Hi Everyone,

Thank you for inviting me to be a committer.  I look forward to working
with everyone to ensure the continued success of the Spark project.

Thanks!
Joey





"
Sandy Ryza <sandy.ryza@cloudera.com>,"Fri, 8 Aug 2014 00:14:56 -0700",Re: Fine-Grained Scheduler on Yarn,Jun Feng Liu <liujunf@cn.ibm.com>,"Hi Jun,

Spark currently doesn't have that feature, i.e. it aims for a fixed number
of executors per application regardless of resource usage, but it's
definitely worth considering.  We could start more executors when we have a
large backlog of tasks and shut some down when we're underutilized.

The fine-grained task scheduling is blocked on work from YARN that will
allow changing the CPU allocation of a YARN container dynamically.  The
relevant JIRA for this dependency is YARN-1197, though YARN-1488 might
serve this purpose as well if it comes first.

-Sandy



"
Christopher Nguyen <ctn@adatao.com>,"Fri, 8 Aug 2014 00:14:55 -0700",Re: Welcoming two new committers,Joseph Gonzalez <jegonzal@eecs.berkeley.edu>,"+1 Joey & Andrew :)

--
Christopher T. Nguyen
Co-founder & CEO, Adatao <http://adatao.com> [ah-'DAY-tao]
linkedin.com/in/ctnguyen




"
Debasish Das <debasish.das83@gmail.com>,"Fri, 8 Aug 2014 00:18:48 -0700",Re: [SNAPSHOT] Snapshot1 of Spark 1.1.0 has been posted,Patrick Wendell <pwendell@gmail.com>,"Hi Patrick,

I am testing the 1.1 branch but I see lot of protobuf warnings while
building the jars:

[warn] Class com.google.protobuf.Parser not found - continuing with a stub.

[warn] Class com.google.protobuf.Parser not found - continuing with a stub.

[warn] Class com.google.protobuf.Parser not found - continuing with a stub.

[warn] Class com.google.protobuf.Parser not found - continuing with a stub.

I am compiling for 2.3.0cdh5.0.2...Later when running the jobs I am getting
a protobuf error:

Exception in thread ""main"" java.lang.VerifyError: class
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddBlockRequestProto
overrides final method
getUnknownFields.()Lcom/google/protobuf/UnknownFieldSet;

Is there a protobuf issue on this branch ?

Also on the branch at least I am noticing the following:

Welcome to

      ____              __

     / __/__  ___ _____/ /__

    _\ \/ _ \/ _ `/ __/  '_/

   /___/ .__/\_,_/_/ /_/\_\   version 1.0.0-SNAPSHOT

      /_/

Won't it be 1.1.0-SNAPSHOT ?

Thanks.

Deb



"
Prashant Sharma <scrapcodes@gmail.com>,"Fri, 8 Aug 2014 12:55:17 +0530",Re: [SNAPSHOT] Snapshot1 of Spark 1.1.0 has been posted,Debasish Das <debasish.das83@gmail.com>,"Yeah this should be changed. You can change the banner in the repl,
printWelcome function. Mind sending a PR ?

I think this should be a one place change in the future (Not sure how
feasible it is). Volunteers ?


Prashant Sharma





"
"""=?gb18030?B?d2l0Z28=?="" <witgo@qq.com>","Fri, 8 Aug 2014 15:27:49 +0800","=?gb18030?B?u9i4tKO6IFtTTkFQU0hPVF0gU25hcHNob3QxIG9m?=
 =?gb18030?B?IFNwYXJrIDEuMS4wIGhhcyBiZWVuIHBvc3RlZA==?=","""=?gb18030?B?RGViYXNpc2ggRGFz?="" <debasish.das83@gmail.com>, ""=?gb18030?B?UGF0cmljayBXZW5kZWxs?="" <pwendell@gmail.com>","Need a parameter ""-Phadoop-2.3""

eg:

./make-distribution.sh -Dhadoop.version=2.3.0-cdh5.0.2 -Dyarn.version=2.3.0-cdh5.0.2 -Phadoop-2.3 -Pyarn 
 







------------------ ‘≠ º” º˛ ------------------
∑¢º˛»À: ""Debasish Das""<debasish.das83@gmail.com>; 
∑¢ÀÕ ±º‰: 2014ƒÍ8‘¬8»’(–«∆⁄ŒÂ) œ¬ŒÁ3:18
 ’º˛»À: ""Patrick Wendell""<pwendell@gmail.com>; 
≥≠ÀÕ: ""dev""<dev@spark.apache.org>; 
÷˜Ã‚: Re: [SNAPSHOT] Snapshot1 of Spark 1.1.0 has been posted



Hi Patrick,

I am testing the 1.1 branch but I see lot of protobuf warnings while
building the jars:

[warn] Class com.google.protobuf.Parser not found - continuing with a stub.

[warn] Class com.google.protobuf.Parser not found - continuing with a stub.

[warn] Class com.google.protobuf.Parser not found - continuing with a stub.

[warn] Class com.google.protobuf.Parser not found - continuing with a stub.

I am compiling for 2.3.0cdh5.0.2...Later when running the jobs I am getting
a protobuf error:

Exception in thread ""main"" java.lang.VerifyError: class
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddBlockRequestProto
overrides final method
getUnknownFields.()Lcom/google/protobuf/UnknownFieldSet;

Is there a protobuf issue on this branch ?

Also on the branch at least I am noticing the following:

Welcome to

      ____              __

     / __/__  ___ _____/ /__

    _\ \/ _ \/ _ `/ __/  '_/

   /___/ .__/\_,_/_/ /_/\_\   version 1.0.0-SNAPSHOT

      /_/

Won't it be 1.1.0-SNAPSHOT ?

Thanks.

Deb


On Wed, Aug 6, 2014 at 11:24 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> Minor correction: the encoded URL in the staging repo link was wrong.
> The correct repo is:
> https://repository.apache.org/content/repositories/orgapachespark-1025/
>
>
> On Wed, Aug 6, 2014 at 11:23 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
> >
> > Hi All,
> >
> > I've packaged and published a snapshot release of Spark 1.1 for testing.
> This is being distributed to the community for QA and preview purposes. It
> is not yet an official RC for voting. Going forward, we'll do preview
> releases like this for testing ahead of official votes.
> >
> > The tag of this release is v1.1.0-snapshot1 (commit d428d8):
> >
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=d428d88418d385d1d04e1b0adcb6b068efe9c7b0
> >
> > The release files, including signatures, digests, etc can be found at:
> > http://people.apache.org/~pwendell/spark-1.1.0-snapshot1/
> >
> > Release artifacts are signed with the following key:
> > https://people.apache.org/keys/committer/pwendell.asc
> >
> > The staging repository for this release can be found at:
> > https://repository.apache.org/content/repositories/orgapachespark-1025/
> >
> > NOTE: Due to SPARK-2899, docs are not yet available for this release.
> Docs will be posted ASAP.
> >
> > To learn more about Apache Spark, please see
> > http://spark.apache.org/
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>"
Jun Feng Liu <liujunf@cn.ibm.com>,"Fri, 8 Aug 2014 15:37:12 +0800",Re: Fine-Grained Scheduler on Yarn,Sandy Ryza <sandy.ryza@cloudera.com>,"Yes, I think we need both level resource control (container numbers and 
dynamically change container resources), which can make the resource 
utilization much more effective, especially when we have more types work 
load share the same infrastructure. 

Is there anyway I can observe the tasks backlog in schedulerbackend? 
Sounds like scheduler backend be triggered during new taskset submitted. I 
did not figured if there is a way to check the whole backlog tasks inside 
it. I am interesting to implement some policy in schedulerbackend and test 
to see how useful it is going to be.
 
Best Regards
 
Jun Feng Liu
IBM China Systems & Technology Laboratory in Beijing



Phone: 86-10-82452683 
E-mail: liujunf@cn.ibm.com


BLD 28,ZGC Software Park 
No.8 Rd.Dong Bei Wang West, Dist.Haidian Beijing 100193 
China 
 

 



Sandy Ryza <sandy.ryza@cloudera.com> 
2014/08/08 15:14

To
Jun Feng Liu/China/IBM@IBMCN, 
cc
Patrick Wendell <pwendell@gmail.com>, ""dev@spark.apache.org"" 
<dev@spark.apache.org>
Subject
Re: Fine-Grained Scheduler on Yarn






Hi Jun,

Spark currently doesn't have that feature, i.e. it aims for a fixed number
of executors per application regardless of resource usage, but it's
definitely worth considering.  We could start more executors when we have 
a
large backlog of tasks and shut some down when we're underutilized.

The fine-grained task scheduling is blocked on work from YARN that will
allow changing the CPU allocation of a YARN container dynamically.  The
relevant JIRA for this dependency is YARN-1197, though YARN-1488 might
serve this purpose as well if it comes first.

-Sandy



and
no
supports
requires
driver.
scheduler

"
Sandy Ryza <sandy.ryza@cloudera.com>,"Fri, 8 Aug 2014 00:49:44 -0700",Re: Fine-Grained Scheduler on Yarn,Jun Feng Liu <liujunf@cn.ibm.com>,"I think that would be useful work.  I don't know the minute details of this
code, but in general TaskSchedulerImpl keeps track of pending tasks.  Tasks
are organized into TaskSets, each of which corresponds to a particular
stage.  Each TaskSet has a TaskSetManager, which directly tracks the
pending tasks for that stage.

-Sandy



"
Xiangrui Meng <mengxr@gmail.com>,"Fri, 8 Aug 2014 01:40:09 -0700",Re: Welcoming two new committers,Christopher Nguyen <ctn@adatao.com>,"Congrats, Joey & Andrew!!

-Xiangrui


---------------------------------------------------------------------


"
Prashant Sharma <scrapcodes@gmail.com>,"Fri, 8 Aug 2014 15:53:55 +0530",Re: Welcoming two new committers,Xiangrui Meng <mengxr@gmail.com>,"Congratulations Andrew and Joey.

Prashant Sharma





"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 8 Aug 2014 12:01:57 -0400",Unit tests in < 5 minutes,dev <dev@spark.apache.org>,"Howdy,

Do we think it's both feasible and worthwhile to invest in getting our unit
tests to finish in under 5 minutes (or something similarly brief) when run
by Jenkins?

Unit tests currently seem to take anywhere from 30 min to 2 hours. As
people add more tests, I imagine this time will only grow. I think it would
be better for both contributors and reviewers if they didn't have to wait
so long for test results; PR reviews would be shorter, if nothing else.

I don't know how how this is normally done, but maybe it wouldn't be too
much work to get a test cycle to feel lighter.

Most unit tests are independent and can be run concurrently, right? Would
it make sense to build a given patch on many servers at once and send
disjoint sets of unit tests to each?

I'd be interested in working on something like that if possible (and
sensible).

Nick
"
Sean Owen <sowen@cloudera.com>,"Fri, 8 Aug 2014 17:14:40 +0100",Re: Unit tests in < 5 minutes,Nicholas Chammas <nicholas.chammas@gmail.com>,"A common approach is to separate unit tests from integration tests.
Maven has support for this distinction. I'm not sure it helps a lot
though, since it only helps you to not run integration tests all the
time. But lots of Spark tests are integration-test-like and are
important to run to know a change works.

I haven't heard of a plugin to run different test suites remotely on
many machines, but I would not be surprised if it exists.

The Jenkins servers aren't CPU-bound as far as I can tell. It's that
the tests spend a lot of time waiting for bits to start up or
complete. That implies the existing tests could be sped up by just
running in parallel locally. I recall someone recently proposed this?

And I think the problem with that is simply that some of the tests
collide with each other, by opening up the same port at the same time
for example. I know that kind of problem is being attacked even right
now. But if all the tests were made parallel friendly, I imagine
parallelism could be enabled and speed up builds greatly without any
remote machines.



---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Fri, 8 Aug 2014 09:47:58 -0700",Re: Unit tests in < 5 minutes,Sean Owen <sowen@cloudera.com>,"How about using parallel execution feature of maven-surefire-plugin
(assuming all the tests were made parallel friendly) ?

http://maven.apache.org/surefire/maven-surefire-plugin/examples/fork-options-and-parallel-execution.html

Cheers



"
Reynold Xin <rxin@databricks.com>,"Fri, 8 Aug 2014 10:10:45 -0700",Re: Unit tests in < 5 minutes,Ted Yu <yuzhihong@gmail.com>,"ScalaTest actually has support for parallelization built-in. We can use
that.

The main challenge is to make sure all the test suites can work in parallel
when running along side each other.



"
Ron Gonzalez <zlgonzalez@yahoo.com.INVALID>,"Fri, 8 Aug 2014 10:12:09 -0700",1.1.0-SNAPSHOT possible regression,Dev <dev@spark.apache.org>,"Hi,
I have a running spark app against the released version of 1.0.1. I recently decided to try and upgrade to the trunk version. Interestingly enough, after building the 1.1.0-SNAPSHOT assembly, replacing it as my assembly in my app caused errors. In particular, it seems Kryo serialization isn't taking. Replacing it with 1.0.1 automatically gets it working again.

Any thoughts? Is this a known issue?

Thanks,
Ron

at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1180) at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347) at scala.collection.mutable.HashMap$$anonfun$writeObject$1.apply(HashMap.scala:137) at scala.collection.mutable.HashMap$$anonfun$writeObject$1.apply(HashMap.scala:135) at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226) at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39) at scala.collection.mutable.HashTable$class.serializeTo(HashTable.scala:124) at scala.collection.mutable.HashMap.serializeTo(HashMap.scala:39) at scala.collection.mutable.HashMap.writeObject(HashMap.scala:135) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at
 java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988) at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495) at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431) at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177) at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347) at org.apache.spark.util.Utils$.serialize(Utils.scala:64) at org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:232) at org.apache.spark.broadcast.TorrentBroadcast.sendBroadcast(TorrentBroadcast.scala:85) at org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:66) at org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:36) at org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:29) at
 org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:62) at org.apache.spark.SparkContext.broadcast(SparkContext.scala:809)"
Reynold Xin <rxin@databricks.com>,"Fri, 8 Aug 2014 10:12:21 -0700",Re: Unit tests in < 5 minutes,Ted Yu <yuzhihong@gmail.com>,"Nick,

Would you like to file a ticket to track this?

I think the first baby step is to log the amount of time each test cases
take. This is supposed to happen already (see the flag), but somehow the
time are not showing. If you have some time to figure that out, that'd be
great.

https://github.com/apache/spark/blob/master/project/SparkBuild.scala#L350





"
Reynold Xin <rxin@databricks.com>,"Fri, 8 Aug 2014 10:14:57 -0700",Re: 1.1.0-SNAPSHOT possible regression,Ron Gonzalez <zlgonzalez@yahoo.com>,"Pasting a better formatted trace:



at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1180)
at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
at
scala.collection.mutable.HashMap$$anonfun$writeObject$1.apply(HashMap.scala:137)
at
scala.collection.mutable.HashMap$$anonfun$writeObject$1.apply(HashMap.scala:135)
at
scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
at
scala.collection.mutable.HashTable$class.serializeTo(HashTable.scala:124)
at scala.collection.mutable.HashMap.serializeTo(HashMap.scala:39)
at scala.collection.mutable.HashMap.writeObject(HashMap.scala:135)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606) at
 java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
at org.apache.spark.util.Utils$.serialize(Utils.scala:64)
at
org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:232)
at
org.apache.spark.broadcast.TorrentBroadcast.sendBroadcast(TorrentBroadcast.scala:85)
at
org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:66)
at
org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:36)
at
org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:29)
at
 org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:62)
at org.apache.spark.SparkContext.broadcast(SparkContext.scala:809)



"
Reynold Xin <rxin@databricks.com>,"Fri, 8 Aug 2014 10:15:41 -0700",Re: 1.1.0-SNAPSHOT possible regression,Ron Gonzalez <zlgonzalez@yahoo.com>,"Looks like you didn't actually paste the exception message. Do you mind
doing that?




"
Nicolas Liochon <nkeywal@gmail.com>,"Fri, 8 Aug 2014 19:19:47 +0200",Re: Unit tests in < 5 minutes,Reynold Xin <rxin@databricks.com>,"fwiw, when we did this work in HBase, we categorized the tests. Then some
tests can share a single jvm, while some others need to be isolated in
their own jvm. Nevertheless surefire can still run them in parallel by
starting/stopping several jvm.

Nicolas



"
Ron Gonzalez <zlgonzalez@yahoo.com.INVALID>,"Fri, 8 Aug 2014 10:37:29 -0700",Re: 1.1.0-SNAPSHOT possible regression,Reynold Xin <rxin@databricks.com>,"Oops, exception is below.
For local, it works and that's the case since TorrentBroadcast has if !isLocal, then that's the only time the broadcast actually happens. It really seems as if the Kryo wrapper didn't kick in for some reason. Do we have a unit test that tests the Kryo serialization that I can give a try?
Thanks,
Ron

Exception in thread ""Driver"" java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:180)
Caused by: java.io.NotSerializableException: org.apache.avro.generic.GenericData$Record - custom writeObject daike you didn't actually paste the exception message. Do you mind
doing tht java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1180)
> at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
> at
> scala.collection.mutable.HashMap$$anonfun$writeObject$1.apply(HashMap.scala:137)
> at
> scala.collection.mutable.HashMap$$anonfun$writeObject$1.apply(HashMap.scala:135)
> at
> scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
> at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
> at
> scala.collection.mutable.HashTable$class.serializeTo(HashTable.scala:124)
> at scala.collection.mutable.HashMap.serializeTo(HashMap.scala:39)
> at scala.collection.mutable.HashMap.writeObject(HashMap.scala:135)
> at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
> at
> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
> at
> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
> at java.lang.reflect.Method.invoke(Method.java:606) at
>† java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)
> at
> java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)
> at
> java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
> at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
> at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
> at org.apache.spark.util.Utils$.serialize(Utils.scala:64)
> at
ast.scala:232)
> at
> org.apache.spark.broadcast.TorrentBroadcast.sendBroadcast(TorrentBroadcast.scala:85)
> at
> org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:66)
> at
> org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:36)
> at
> org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:29)
> at
>
>† org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:62)
> at org.apache.spark.SparkContext.broadcast(SparkContext.scala:809)
>
> released version of 1.0.1. I
>> recently decided to try and upgrade to the trunk version. Interestingly
>> enough, after building the 1.1.0-SNAPSHOT assembly, replacing it as my
>> assembly in my app caused errors. In particular, it seems Kryo
>> serialization isn't taking. Replacing it with 1.0.1 automatically gets it
>> working again.
>>
>> Any thoughts? Is this a known issue?
>>
>> Thanks,
>> Ron
>>
>> at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1180)
>> at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347) at
>> scala.collection.mutable.HashMap$$anonfun$writeObject$1.apply(HashMap.scala:137)
>hMap.scala:135)
>> at
>> scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
>> at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39) at
>> scala.collection.mutable.HashTable$class.serializeTo(HashTable.scala:124)
>> at scala.collection.mutable.HashMap.serializeTo(HashMap.scala:39) at
>> scala.collection.mutable.HashMap.writeObject(HashMap.scala:135) at
>> sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at
>> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
>> at
>> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
>> at java.lang.reflect.Method.invoke(Method.java:606) at
>>† java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)
>> at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)
>> at
>> java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
>> at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177) at
>> java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347) at
>.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:232)
>> at
>> org.apache.spark.broadcast.TorrentBroadcast.sendBroadcast(TorrentBroadcast.scala:85)
>> at
>> org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:66)
>> at
>> org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:36)
>> at
>> org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:29)
>> at
>>† org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:62)
>> at org.apache.spark.SparkContext.broadcast(SparkContext.scala:809)
>
>
>"
Reynold Xin <rxin@databricks.com>,"Fri, 8 Aug 2014 10:40:47 -0700",Re: 1.1.0-SNAPSHOT possible regression,Ron Gonzalez <zlgonzalez@yahoo.com>,"Yes, I'm pretty sure it doesn't actually use the right serializer in
TorrentBroadcast:
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala#L232

And TorrentBroadcast is turned on by default for 1.1 right now. Do you want
to submit a pull request to fix that? This would be a critical fix for 1.1
that's worth doing.




Local, then that's the only time the broadcast actually happens. It really seems as if the Kryo wrapper didn't kick in for some reason. Do we have a unit test that tests the Kryo serialization that I can give a try?
java:57)
sorImpl.java:43)
nMaster.scala:180)
ricData$Record
)
la:137)
la:135)
)
4)
:57)
mpl.java:43)
)
)
31)
)
ast.scala:232)
t.scala:85)
:66)
oadcastFactory.scala:36)
oadcastFactory.scala:29)
.scala:62)
y
0)
la:137)
la:135)
)
:57)
mpl.java:43)
8)
31)
ast.scala:232)
t.scala:85)
:66)
oadcastFactory.scala:36)
oadcastFactory.scala:29)
.scala:62)
"
Reynold Xin <rxin@databricks.com>,"Fri, 8 Aug 2014 10:42:50 -0700",Re: 1.1.0-SNAPSHOT possible regression,Ron Gonzalez <zlgonzalez@yahoo.com>,"I created a JIRA ticket to track this:
https://issues.apache.org/jira/browse/SPARK-2928

Let me know if you need help with it.




e/spark/broadcast/TorrentBroadcast.scala#L232
or
sLocal, then that's the only time the broadcast actually happens. It really seems as if the Kryo wrapper didn't kick in for some reason. Do we have a unit test that tests the Kryo serialization that I can give a try?
.java:57)
ssorImpl.java:43)
onMaster.scala:180)
ericData$Record
:
0)
ala:137)
ala:135)
6)
)
a:57)
Impl.java:43)
8)
5)
431)
7)
cast.scala:232)
st.scala:85)
a:66)
roadcastFactory.scala:36)
roadcastFactory.scala:29)
r.scala:62)
ly
y
s
)
ala:137)
ala:135)
6)
)
a:57)
Impl.java:43)
431)
t
cast.scala:232)
st.scala:85)
a:66)
roadcastFactory.scala:36)
roadcastFactory.scala:29)
r.scala:62)
"
Ron Gonzalez <zlgonzalez@yahoo.com.INVALID>,"Fri, 8 Aug 2014 10:50:22 -0700",Re: 1.1.0-SNAPSHOT possible regression,Reynold Xin <rxin@databricks.com>,"Sure let me give it a try. Any tips? I've only started looking at Spark code more closely recently.
I can compare Spark-1.0.1 code and see what's goto track this:†https://issues.apache.org/jira/browse/SPARK-2928

Let  doesn't actually use the right serializer in TorrentBroadcast:†https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala#L232
>
>
>And TorrentBroadcast is turned on by default for 1.1 right now. Do you want to submit a pull request to fix that? This would be a critical fix for 1.1 that's worth doing.
>
>nd that's the case since TorrentBroadcast has if !isLocal, then that's the only time the broadcast actually happens. It really seems as if the Kryo wrapper didn't kick in for some reason. Do we have a unit test that tests the Kryo serialization that I can give a try?
>>Thanks,
>>Ron
>>
>>
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:180)
Caused by: java.io.NotSerializableException: org.apache.avro.generic.GenericData$Record - custom writeObject data (class ""scala.collection.mutable.tually paste the exception message. Do you mind
>>doing that?
>>
>>1180)
>>> at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
>>> at
>>> scala.collection.mutable.HashMap$$anonfun$writeObject$1.apply(HashMap.scala:137)
>>> at
>>>
 scala.collection.mutable.HashMap$$anonfun$writeObject$1.apply(HashMap.scala:135)
>>> at
>>> scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
>>> at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
>>> at
>>> scala.collection.mutable.HashTable$class.serializeTo(HashTable.scala:124)
>>> at scala.collection.mutable.HashMap.serializeTo(HashMap.scala:39)
>>> at scala.collection.mutable.HashMap.writeObject(HashMap.scala:135)
>>> at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
>>> at
>>>
 sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
>>> at
>>> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
>>> at java.lang.reflect.Method.invoke(Method.java:606) at
>>>† java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)
>>> at
>>> java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)
>>> at
>>> java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
>>> at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
>>> at
 java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
ast.scala:232)
>>> at
>>> org.apache.spark.broadcast.TorrentBroadcast.sendBroadcast(TorrentBroadcast.scala:85)
>>> at
>>> org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:66)
>>> at
>>> org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:36)
>>> at
>>>
 org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:29)
>>> at
>>stManager.scala:62)
>>> at org.apache.spark.SparkContext.broadcast(SparkC I have a running spark app against the released
 version of 1.0.1. I
>
>>>> enough, after building the 1.1.0-SNAPSHOT assembly, replacing it as my
>>>> assembly in my app caused errors. In particular, it seems Kryo
>>>> serialization isn't taking. Replacing it with 1.0.1 automatically gets it
>>>> working again.
>>>>
>>>> Any thoughts? Is this a known issue?
>>>>
>>>> Thanks,
>>>> Ron
>>>>
>>>> at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1180)
>>>> at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347) at
>>>> scala.collection.mutable.HashMap$$anonfun$writeObject$1.apply(HashMap.scala:137)
>>>> at
>>>> scala.collection.mutable.HashMap$$anonfun$writeObject$1.apply(HashMap.scala:135)
>>>> at
>>>> scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
>>>> at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39) at
>>>> scala.collection.mutable.HashTable$class.serializeTo(HashTable.scala:124)
>>>> at scala.collection.mutable.HashMap.serializeTo(HashMap.scala:39) at
>>>>
 scala.collection.mutable.HashMap.writeObject(HashMap.scala:135) at
>>>> sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at
>>>> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
>>>> at
>>>> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
>>>> at java.lang.reflect.Method.invoke(Method.java:606) at
>>>>† java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)
>495)
>>>> at
>>>>
 java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
>>>> at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177) at
>>>> java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347) at
>>>> org.apache.spark.util.Utils$.serialize(Utils.scala:64) at
>>>> org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:232)
>>>> at
>>>> org.apache.spark.broadcast.TorrentBroadcast.sendBroadcast(TorrentBroadcast.scala:85)
>>>> at
>>>> org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:66)
>>>> at
>>>> org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:36)
>>>> at
>>roadcastFactory.scala:29)
>>>> at
>>>>† org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:62)
>>>> at org.apache.spark.SparkContext.broadcast(SparkContext.scala:809)
>>>
>>>
>>>
>"
Reynold Xin <rxin@databricks.com>,"Fri, 8 Aug 2014 10:50:53 -0700",Re: 1.1.0-SNAPSHOT possible regression,Ron Gonzalez <zlgonzalez@yahoo.com>,"Actually apparently there is a pull request for it. Thanks for reporting!

https://github.com/apache/spark/pull/1836




e/spark/broadcast/TorrentBroadcast.scala#L232
or
Local, then that's the only time the broadcast actually happens. It really seems as if the Kryo wrapper didn't kick in for some reason. Do we have a unit test that tests the Kryo serialization that I can give a try?
java:57)
sorImpl.java:43)
nMaster.scala:180)
ricData$Record
)
la:137)
la:135)
)
4)
:57)
mpl.java:43)
)
)
31)
)
ast.scala:232)
t.scala:85)
:66)
oadcastFactory.scala:36)
oadcastFactory.scala:29)
.scala:62)
y
0)
la:137)
la:135)
)
:57)
mpl.java:43)
8)
31)
ast.scala:232)
t.scala:85)
:66)
oadcastFactory.scala:36)
oadcastFactory.scala:29)
.scala:62)
"
Matei Zaharia <matei.zaharia@gmail.com>,"Fri, 8 Aug 2014 11:24:23 -0700",Re: Unit tests in < 5 minutes,"Nicolas Liochon <nkeywal@gmail.com>, Reynold Xin
 <rxin@databricks.com>","Just as a note, when you're developing stuff, you can use ""test-only"" in sbt, or the equivalent feature in Maven, to run just some of the tests. This is what I do, I don't wait for Jenkins to run things. 90% of the time if it passes the tests that I know could break stuff, it will pass all of Jenkins.

Jenkins should always be doing all the integration tests, so I don't think it will become *that* much shorter in the long run, though it can certainly be improved.

Matei


fwiw, when we did this work in HBase, we categorized the tests. Then some 
tests can share a single jvm, while some others need to be isolated in 
their own jvm. Nevertheless surefire can still run them in parallel by 
starting/stopping several jvm. 

Nicolas 



"
Patrick Wendell <pwendell@gmail.com>,"Fri, 8 Aug 2014 11:53:54 -0700",Re: Unit tests in < 5 minutes,Matei Zaharia <matei.zaharia@gmail.com>,"I dug around this a bit a while ago, I think if someone sat down and
profiled the tests it's likely we could find some things to optimize.
In particular, there may be overheads in starting up a local spark
context that could be minimized and speed up all the tests. Also,
there are some tests (especially in Streaming) that take really long,
like 60 seconds for a single test (see some of the new flume tests).
These could almost certainly be optimized.

I think 5 minutes might be out of reach, but something like a 2X
improvement might be possible and would be very valuable if
accomplished.

- Patrick

ote:
sbt, or the equivalent feature in Maven, to run just some of the tests. This is what I do, I don't wait for Jenkins to run things. 90% of the time if it passes the tests that I know could break stuff, it will pass all of Jenkins.
k it will become *that* much shorter in the long run, though it can certainly be improved.
te:
lel
ions-and-parallel-execution.html
?
e
t
 As
 it
to
be
?
end
nd
-

---------------------------------------------------------------------


"
Josh Rosen <rosenville@gmail.com>,"Fri, 8 Aug 2014 12:00:20 -0700",Re: Unit tests in < 5 minutes,"Matei Zaharia <matei.zaharia@gmail.com>, Patrick Wendell
 <pwendell@gmail.com>","ts that don‚Äôt need it. ¬†When running tests on my local machine while also running another Spark shell, I‚Äôve noticed that the test logs fill up with errors when the web UI attempts to bind to the default port, fails, and tries a higher one.

- Josh

I dug around this a bit a while ago, I think if someone sat down and  
profiled the tests it's likely we could find some things to optimize.  
In particular, there may be overheads in starting up a local spark  
context that could be minimized and speed up all the tests. Also,  
there are some tests (especially in Streaming) that take really long,  
like 60 seconds for a single test (see some of the new flume tests).  
These could almost certainly be optimized.  

I think 5 minutes might be out of reach, but something like a 2X  
improvement might be possible and would be very valuable if  
accomplished.  

- Patrick  

 in sbt, or the equivalent feature in Maven, to run just some of the tests. This is what I do, I don't wait for Jenkins to run things. 90% of the time if it passes the tests that I know could break stuff, it will pass all of Jenkins.  
ink it will become *that* much shorter in the long run, though it can certainly be improved.  
me  
 
 
rote:  
e  
allel  
:  
 
ptions-and-parallel-execution.html  
rote:  
.  
t  
he  
on  
at  
  
is?  
  
ime  
ght  
ny  
ng  
)  
s. As  
nk it  
e to  
g  
t be  
ht?  
 send  
(and  
---  

---------------------------------------------------------------------  
For additional commands, e-mail: dev-help@spark.apache.org  

"
Zhan Zhang <zhazhan@gmail.com>,"Fri, 8 Aug 2014 13:26:58 -0700 (PDT)",Re: Working Formula for Hive 0.13?,dev@spark.incubator.apache.org,"The API change seems not major. I have locally change it and compiled, but
not test yet. The major problem is still how to solve the hive-exec jar
dependency. I am willing to help on this issue. Is it better stick to the
same way as hive-0.12 until hive-exec is cleaned enough to switch back?



--

---------------------------------------------------------------------


"
Zhan Zhang <zhazhan@gmail.com>,"Fri, 8 Aug 2014 13:28:25 -0700 (PDT)",Re: Working Formula for Hive 0.13?,dev@spark.incubator.apache.org,"I can compile with no error, but my patch also includes other stuff. 



--

---------------------------------------------------------------------


"
Zhan Zhang <zhazhan@gmail.com>,"Fri, 8 Aug 2014 13:55:17 -0700 (PDT)",Re: Working Formula for Hive 0.13?,dev@spark.incubator.apache.org,"Here is the patch. Please ignore the pom.xml related change, which just for
compiling purpose. I need to further work on this one based on Wandou's
previous work.



--

---------------------------------------------------------------------


"
Zhan Zhang <zhazhan@gmail.com>,"Fri, 8 Aug 2014 13:57:04 -0700 (PDT)",Re: Working Formula for Hive 0.13?,dev@spark.incubator.apache.org,"Sorry, forget to upload files. I have never posted before :) hive.diff
<http://apache-spark-developers-list.1001551.n3.nabble.com/file/n7777/hive.diff>  



--

---------------------------------------------------------------------


"
chutium <teng.qiu@gmail.com>,"Fri, 8 Aug 2014 14:03:17 -0700 (PDT)",spark-shell is broken! (bad option: '--master'),dev@spark.incubator.apache.org,"git clone https://github.com/apache/spark.git
mvn -Pyarn -Phive -Phive-thriftserver -Dhadoop.version=2.3.0-cdh5.1.0
-DskipTests clean package

./bin/spark-shell --master local[2]

then i get

Spark assembly has been built with Hive, including Datanucleus jars on
classpath
bad option: '--master'



unbelievable...




--

---------------------------------------------------------------------


"
chutium <teng.qiu@gmail.com>,"Fri, 8 Aug 2014 14:14:30 -0700 (PDT)",Re: spark-shell is broken! (bad option: '--master'),dev@spark.incubator.apache.org,"maybe this commit is the reason?
https://github.com/apache/spark/commit/a6cd31108f0d73ce6823daafe8447677e03cfd13

i fand some discuss in its PR: https://github.com/apache/spark/pull/1801

important is what vanzin said:
https://github.com/apache/spark/pull/1801#issuecomment-51545117

i tried to use this:
bin/spark-class org.apache.spark.deploy.SparkSubmit --class
org.apache.spark.repl.Main spark-shell --master local[2]

also
bad option: '--master'


but this works

bin/spark-class org.apache.spark.deploy.SparkSubmit --master local[2]
--class org.apache.spark.repl.Main spark-shell





and some more try out:

~/tachyon/spark$ bin/spark-class org.apache.spark.deploy.SparkSubmit
--master local[2] --class org.apache.spark.repl.Main spark-shell --foo bar
Spark assembly has been built with Hive, including Datanucleus jars on
classpath
bad option: '--foo'

~/tachyon/spark$ bin/spark-class org.apache.spark.deploy.SparkSubmit
--master local[2] --foo bar --class org.apache.spark.repl.Main spark-shell
Spark assembly has been built with Hive, including Datanucleus jars on
classpath
Error: Unrecognized option '--foo'.
Run with --help for usage help or --verbose for debug output



god... no test suite for those shell scripts?

such a issue is really awful~




--

---------------------------------------------------------------------


"
chutium <teng.qiu@gmail.com>,"Fri, 8 Aug 2014 14:26:50 -0700 (PDT)",Re: spark-shell is broken! (bad option: '--master'),dev@spark.incubator.apache.org,"no one use spark-shell in master branch?

i created a PR as follow up commit of SPARK-2678 and PR #1801:

https://github.com/apache/spark/pull/1861



--

---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Fri, 8 Aug 2014 14:51:24 -0700",Re: Working Formula for Hive 0.13?,Zhan Zhang <zhazhan@gmail.com>,"Could you make a PR as described here:
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark



"
Zhan Zhang <zhazhan@gmail.com>,"Fri, 8 Aug 2014 15:43:34 -0700 (PDT)",Re: Working Formula for Hive 0.13?,dev@spark.incubator.apache.org,"Attached the diff the PR SPARK-2706. I am currently working on this problem.
If somebody are also working on this, we can share the load.



--

---------------------------------------------------------------------


"
Sandy Ryza <sandy.ryza@cloudera.com>,"Fri, 8 Aug 2014 15:46:59 -0700",Re: spark-shell is broken! (bad option: '--master'),chutium <teng.qiu@gmail.com>,"Hi Chutium,

This is currently being addressed in
https://github.com/apache/spark/pull/1825

-Sandy



"
Patrick Wendell <pwendell@gmail.com>,"Fri, 8 Aug 2014 15:47:43 -0700",Re: Unit tests in < 5 minutes,Josh Rosen <rosenville@gmail.com>,"Josh - that was actually fixed recently (we just bind to a random port
when running tests).


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Fri, 8 Aug 2014 15:48:32 -0700",Re: spark-shell is broken! (bad option: '--master'),Sandy Ryza <sandy.ryza@cloudera.com>,"Cheng Lian also has a fix for this. I've asked him to make a PR - he
is on China time so it probably won't come until tonight:

https://github.com/liancheng/spark/compare/apache:master...liancheng:spark-2894


---------------------------------------------------------------------


"
Cheng Lian <lian.cs.zju@gmail.com>,"Sat, 9 Aug 2014 09:14:52 +0800",Re: spark-shell is broken! (bad option: '--master'),Patrick Wendell <pwendell@gmail.com>,"Just opened a PR based on the branch Patrick mentioned for this issue
https://github.com/apache/spark/pull/1864



"
Andrew Or <andrew@databricks.com>,"Sat, 9 Aug 2014 00:43:15 -0700",Re: Welcoming two new committers,Prashant Sharma <scrapcodes@gmail.com>,"Thanks everyone. I look forward to continuing to work with all of you!


2014-08-08 3:23 GMT-07:00 Prashant Sharma <scrapcodes@gmail.com>:

"
Debasish Das <debasish.das83@gmail.com>,"Sat, 9 Aug 2014 00:46:41 -0700",Re: Using mllib-1.1.0-SNAPSHOT on Spark 1.0.1,DB Tsai <dbtsai@dbtsai.com>,"Hi Xiangrui,

Based on your suggestion I moved core and mllib both to 1.1.0-SNAPSHOT...I
am still getting class cast exception:

Exception in thread ""main"" org.apache.spark.SparkException: Job aborted due
to stage failure: Task 249 in stage 52.0 failed 4 times, most recent
failure: Lost task 249.3 in stage 52.0 (TID 10002,
tblpmidn06adv-hdp.tdc.vzwcorp.com): java.lang.ClassCastException:
scala.Tuple1 cannot be cast to scala.Product2

I am running ALS.scala merged with my changes. I will try the mllib jar
without my changes next...

Can this be due to the fact that my jars are compiled with Java 1.7_55 but
the cluster JRE is at 1.7_45.

Thanks.

Deb





"
Guru Medasani <gdmeda@outlook.com>,"Sat, 9 Aug 2014 04:00:43 -0500",RE: Welcoming two new committers,"Andrew Or <andrew@databricks.com>, Prashant Sharma <scrapcodes@gmail.com>","Congrats Joey and Andrew!

Sent from my Windows Phone
________________________________
From: Andrew Or<mailto:andrew@databricks.com>
Sent: ‚Äé8/‚Äé9/‚Äé2014 2:43 AM
To: Prashant Sharma<mailto:scrapcodes@gmail.com>
Cc: Xiangrui Meng<mailto:mengxr@gmail.com>; Christopher Nguyen<mailto:ctn@adatao.com>; Joseph Gonzalez<mailto:jegonzal@eecs.berkeley.edu>; Matei Zaharia<mailto:matei@databricks.com>; dev@spark.incubator.apache.org<mailto:dev@spark.incubator.apache.org>
Subject: Re: Welcoming two new committers

Thanks everyone. I look forward to continuing to work with all of you!


2014-08-08 3:23 GMT-07:00 Prashant Sharma <scrapcodes@gmail.com>:

ote:
m>
ng
.com>
t
k
"
Mridul Muralidharan <mridul@gmail.com>,"Sat, 9 Aug 2014 18:30:33 +0530",Re: Unit tests in < 5 minutes,Nicholas Chammas <nicholas.chammas@gmail.com>,"Issue with supporting this imo is the fact that scala-test uses the
same vm for all the tests (surefire plugin supports fork, but
scala-test ignores it iirc).
So different tests would initialize different spark context, and can
potentially step on each others toes.

Regards,
Mridul



---------------------------------------------------------------------


"
Debasish Das <debasish.das83@gmail.com>,"Sat, 9 Aug 2014 09:59:15 -0700",Re: Using mllib-1.1.0-SNAPSHOT on Spark 1.0.1,DB Tsai <dbtsai@dbtsai.com>,"I validated that I can reproduce this problem with master as well (without
adding any of my mllib changes)...

I separated mllib jar from assembly, deploy the assembly and then I supply
the mllib jar as --jars option to spark-submit...

I get this error:

14/08/09 12:49:32 INFO DAGScheduler: Failed to run count at ALS.scala:299

Exception in thread ""main"" org.apache.spark.SparkException: Job aborted due
to stage failure: Task 238 in stage 40.0 failed 4 times, most recent
failure: Lost task 238.3 in stage 40.0 (TID 10002,
tblpmidn05adv-hdp.tdc.vzwcorp.com): java.lang.ClassCastException:
scala.Tuple1 cannot be cast to scala.Product2


org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5$$anonfun$apply$4.apply(CoGroupedRDD.scala:159)

        scala.collection.Iterator$$anon$11.next(Iterator.scala:328)




org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)


org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:158)


scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)


scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)

        scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)


scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)

        org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:158)

        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)

        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)


org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)

        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)

        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)


org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)

        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)

        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)


org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)

        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)

        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)


org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:129)


org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:126)


scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)


scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)

        scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)


scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)

        org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:126)

        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)

        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)


org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)

        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)

        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)


org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)

        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)

        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)

        org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)

        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)

        org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:61)

        org.apache.spark.rdd.RDD.iterator(RDD.scala:227)

        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)

        org.apache.spark.scheduler.Task.run(Task.scala:54)


org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199)


java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)


java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

        java.lang.Thread.run(Thread.java:744)

Driver stacktrace:

at org.apache.spark.scheduler.DAGScheduler.org
$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1153)

at
org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1142)

at
org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1141)

at
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)

at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)

at
org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1141)

at
org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:682)

at
org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:682)

at scala.Option.foreach(Option.scala:236)

at
org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:682)

at
org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1359)

at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)

at akka.actor.ActorCell.invoke(ActorCell.scala:456)

at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)

at akka.dispatch.Mailbox.run(Mailbox.scala:219)

at
akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)

at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)

at
scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)

at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)

at
scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

I will try now with mllib inside the assembly....If that works then
something is weird here !



"
Matt Forbes <matt@tellapart.com>,"Sat, 9 Aug 2014 11:01:14 -0700",Re: Using mllib-1.1.0-SNAPSHOT on Spark 1.0.1,Debasish Das <debasish.das83@gmail.com>,"I was having this same problem early this week and had to include my
changes in the assembly.



"
Debasish Das <debasish.das83@gmail.com>,"Sat, 9 Aug 2014 11:12:51 -0700",Re: Using mllib-1.1.0-SNAPSHOT on Spark 1.0.1,Matt Forbes <matt@tellapart.com>,"Including mllib inside assembly worked fine...If I deploy only the core and
send mllib as --jars then this problem shows up...

Xiangrui could you please comment if it is a bug or expected behavior ? I
will create a JIRA if this needs to be tracked...



"
Debasish Das <debasish.das83@gmail.com>,"Sat, 9 Aug 2014 18:12:04 -0700",Re: Using mllib-1.1.0-SNAPSHOT on Spark 1.0.1,Matt Forbes <matt@tellapart.com>,"Actually nope it did not work fine...

With multiple ALS iteration, I am getting the same error (with or without
my mllib changes)....

Exception in thread ""main"" org.apache.spark.SparkException: Job aborted due
to stage failure: Task 206 in stage 52.0 failed 4 times, most recent
failure: Lost task 206.3 in stage 52.0 (TID 9999,
tblpmidn42adv-hdp.tdc.vzwcorp.com): java.lang.ClassCastException:
scala.Tuple1 cannot be cast to scala.Product2


org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5$$anonfun$apply$4.apply(CoGroupedRDD.scala:159)

        scala.collection.Iterator$$anon$11.next(Iterator.scala:328)




org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)


org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:158)


scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)


scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)

        scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)


scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)

        org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:158)

        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)

        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)


org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)

        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)

        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)


org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)

        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)

        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)


org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)

        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)

        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)


org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:129)


org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:126)


scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)


scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)

        scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)


scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)

        org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:126)

        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)

        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)


org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)

        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)

        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)


org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)

        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)

        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)

        org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)

        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)

        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)


org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)


org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)

        org.apache.spark.scheduler.Task.run(Task.scala:54)


org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199)


java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)


java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

        java.lang.Thread.run(Thread.java:744)

Driver stacktrace:

at org.apache.spark.scheduler.DAGScheduler.org
$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1153)

at
org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1142)

at
org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1141)

at
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)

at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)

at
org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1141)

at
org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:682)

at
org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:682)

at scala.Option.foreach(Option.scala:236)

at
org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:682)

at
org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1359)

at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)

at akka.actor.ActorCell.invoke(ActorCell.scala:456)

at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)

at akka.dispatch.Mailbox.run(Mailbox.scala:219)

at
akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)

at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)

at
scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)

at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)

at
scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

The behavior is consistent in standalone and yarn mode...

I am at the following checkin: commit
ec79063fad44751a6689f5e58d47886babeaecff

I also tested yarn deployment and I will use standalone mode to deploy
stable spark release (1.0.1 right now) and all the mllib changes I can test
on our datasets through yarn deployment...it works fine...

By the way, let me try if I can reproduce this issue on MovieLensALS
locally....Most likely it is a bug

Thanks.

Deb



"
amnonkhen <amnon.is@gmail.com>,"Sun, 10 Aug 2014 03:41:28 -0700 (PDT)","saveAsTextFile to s3 on spark does not work, just hangs",dev@spark.incubator.apache.org,"I am loading a csv text file from s3 into spark, filtering and mapping the
records and writing the result to s3.

I have tried several input sizes: 100k rows, 1M rows & 3.5M rows. The former
two finish successfully while the latter (3.5M rows) hangs in some weird
state in which the job stages monitor web app (the one in port 4040) stops ,
and the command line console gets stuck and does not even respond to ctrl-c.
The Master's web monitoring app still responds and shows the state as
FINISHED.

In s3, I see an empty directory with a single zero-sized entry
_temporary_$folder$. The s3 url is given using the s3n:// protocol.

I did not see any error in the logs in the web console. I also tried several
cluster sizes (1 master + 1 worker, 1 master + 5 workers) and got to the
same state.

Has anyone encountered such an issue? Any idea what's going on?

I also posted this question to Stack Overflow: 
http://stackoverflow.com/questions/25226419/saveastextfile-to-s3-on-spark-does-not-work-just-hangs
<http://stackoverflow.com/questions/25226419/saveastextfile-to-s3-on-spark-does-not-work-just-hangs>  



--

---------------------------------------------------------------------


"
=?UTF-8?B?5p2O5a6c6Iqz?= <xuite627@gmail.com>,"Sun, 10 Aug 2014 23:44:10 +0800",fair scheduler,dev@spark.apache.org,"Hi

I am trying to switch from FIFO to FAIR with standalone mode.

my environment:
hadoop 1.2.1
spark 0.8.0 using stanalone mode

and i modified the code..........

ClusterScheduler.scala  -> System.getProperty(""spark.scheduler.mode"",
""FAIR""))
SchedulerBuilder.scala  ->
val DEFAULT_SCHEDULING_MODE = SchedulingMode.FAIR

LocalScheduler.scala ->
System.getProperty(""spark.scheduler.mode"", ""FAIR)

spark-env.sh ->
export SPARK_JAVA_OPTS=""-Dspark.scheduler.mode=FAIR""
export SPARK_JAVA_OPTS="" -Dspark.scheduler.mode=FAIR"" ./run-example
org.apache.spark.examples.SparkPi spark://streaming1:7077


but it's not work
i want to switch from fifo to fair
how can i  do??

Regards
Crystal Lee
"
Matei Zaharia <matei.zaharia@gmail.com>,"Sun, 10 Aug 2014 15:49:16 -0700",Re: fair scheduler,"=?utf-8?Q?=E6=9D=8E=E5=AE=9C=E8=8A=B3?= <xuite627@gmail.com>, 
 dev@spark.apache.org","Hi Crystal,

The fair scheduler is only for jobs running concurrently within the same SparkContext (i.e. within an application), not for separate applications on the standalone cluster manager. It has no effect there. To run more of those concurrently, you need to set a cap on how many cores they each grab with spark.cores.max.

Matei


Hi  

I am trying to switch from FIFO to FAIR with standalone mode.  

my environment:  
hadoop 1.2.1  
spark 0.8.0 using stanalone mode  

and i modified the code..........  

ClusterScheduler.scala -> System.getProperty(""spark.scheduler.mode"",  
""FAIR""))  
SchedulerBuilder.scala ->  
val DEFAULT_SCHEDULING_MODE = SchedulingMode.FAIR  

LocalScheduler.scala ->  
System.getProperty(""spark.scheduler.mode"", ""FAIR)  

spark-env.sh ->  
export SPARK_JAVA_OPTS=""-Dspark.scheduler.mode=FAIR""  
export SPARK_JAVA_OPTS="" -Dspark.scheduler.mode=FAIR"" ./run-example  
org.apache.spark.examples.SparkPi spark://streaming1:7077  


but it's not work  
i want to switch from fifo to fair  
how can i do??  

Regards  
Crystal Lee  
"
chutium <teng.qiu@gmail.com>,"Mon, 11 Aug 2014 02:00:20 -0700 (PDT)",Re: spark-shell is broken! (bad option: '--master'),dev@spark.incubator.apache.org,"an issue 3 - 4 PR, spark dev community is really active :)

it seems currently spark-shell takes only some SUBMISSION_OPTS, but no
APPLICATION_OPTS

do you have plan to add some APPLICATION_OPTS or CLI_OPTS like
hive -e
hive -f
hive -hivevar

then we can use our scala code as scripts, run them direktly via
spark-shell, without compiling, building, packing and so on...

those APPLICATION_OPTS should be some augments for
org.apache.spark.repl.Main right?



--

---------------------------------------------------------------------


"
Devl Devel <devl.development@gmail.com>,"Mon, 11 Aug 2014 13:37:36 +0100",Spark Avro Generation,dev@spark.apache.org,"Hi

So far I've been managing to build Spark from source but since a change in
spark-streaming-flume I have no idea how to generate classes (e.g.
SparkFlumeProtocol) from the avro schema.

I have used sbt to run avro:generate (from the top level spark dir) but it
produces nothing - it just says:

[success] Total time: 0 s, completed Aug 11, 2014 12:26:49 PM.

Please can someone send me their build.sbt or just tell me how to build
spark so that all avro files get generated as well?

Sorry for the noob question but I really have tried by best on this one!
Cheers
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 11 Aug 2014 10:51:32 -0400",Re: Pull requests will be automatically linked to JIRA when submitted,dev <dev@spark.apache.org>,"It looks like this script doesn't catch PRs that are opened and *then* have
the JIRA issue ID added to the name. Would it be easy to somehow have the
script trigger on PR name changes as well as PR creates?

Alternately, is there a reason we can't or don't want to use the plugin
mentioned below? (I'm assuming it covers cases like this, but I'm not sure.)

Nick




t with
ira-bitbucket-connector-plugin
nd+GitHub+accounts+to+JIRA
ugins.jira-bitbucket-connector-plugin/86ff1a21-44fb-4227-aa4f-44c77aec2c97.png>
9232929
)
r
"
Hari Shreedharan <hshreedharan@cloudera.com>,"Mon, 11 Aug 2014 09:32:11 -0700",Re: Spark Avro Generation,Devl Devel <devl.development@gmail.com>,"Jay running sbt compile or assembly should generate the sources.


"
Patrick Wendell <pwendell@gmail.com>,"Mon, 11 Aug 2014 09:52:24 -0700",Re: Pull requests will be automatically linked to JIRA when submitted,Nicholas Chammas <nicholas.chammas@gmail.com>,"I spent some time on this and I'm not sure either of these is an option,
unfortunately.

We typically can't use custom JIRA plug-in's because this JIRA is
controlled by the ASF and we don't have rights to modify most things about
how it works (it's a large shared JIRA instance used by more than 50
projects). It's worth looking into whether they can do something. In
general we've tended to avoid going through ASF infra them whenever
possible, since they are generally overloaded and things move very slowly,
even if there are outages.

Here is the script we use to do the sync:
https://github.com/apache/spark/blob/master/dev/github_jira_sync.py

It might be possible to modify this to support post-hoc changes, but we'd
need to think about how to do so while minimizing function calls to the ASF
JIRA API, which I found are very slow.

- Patrick




"
Ron Gonzalez <zlgonzalez@yahoo.com.INVALID>,"Mon, 11 Aug 2014 09:09:07 -0800",Re: Spark Avro Generation,Hari Shreedharan <hshreedharan@cloudera.com>,"If you don't want to build the entire thing, you can also do

mvn generate-sources in externals/flume-sink

Thanks,
Ron

Sent from my iPhone

rote:
:
n
t

---------------------------------------------------------------------


"
Ron's Yahoo! <zlgonzalez@yahoo.com.INVALID>,"Mon, 11 Aug 2014 11:57:21 -0800",More productive compilation of spark code base,Dev <dev@spark.apache.org>,"Hi,
  Iíve been able to get things compiled on my environment, but Iím noticing that itís been quite difficult in IntelliJ. It always recompiles everything when I try to run one test like BroadcastTest, for example, despite having compiled make-distribution previously. In eclipse, I have no such recompilation issues. IntelliJ unfortunately does not support auto compilation for Scala. It also doesnít seem as if IntelliJ knows that that there are classes that have already been compiled since it always opts to recompile everything. Iím new to IntelliJ so it might really just be a lack of knowledge on my part.
  Can anyone share any tips on how they are productive compiling against the Spark code base using IntelliJ?

Thanks,
Ron
---------------------------------------------------------------------


"
Stephen Boesch <javadba@gmail.com>,"Mon, 11 Aug 2014 13:10:58 -0700",Re: More productive compilation of spark code base,"""Ron's Yahoo!"" <zlgonzalez@yahoo.com.invalid>","Hi Ron,
   A possible recommendation is to use maven for the entire process
(avoiding  the sbt artifacts/processing).  IJ is pretty solid in its maven
support.

a) mvn -DskipTests  -Pyarn -Phive -Phadoop-2.3 compile package
b)  Inside IJ:  Open the parent/root pom.xml as a new maven project
c)  Inside IJ: Build | Project
d) Enjoy running/debugging the individual scalatest classes
e) (No unnecessary recompilation pain after the initial build)

Also i found that when using maven there are no duplicate class directories
(whereas in sbt everything is doubled up under the project/target directory
for some reason).



2014-08-11 12:57 GMT-07:00 Ron's Yahoo! <zlgonzalez@yahoo.com.invalid>:

‚Äôm
compiles
no
that that
 be a lack
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 11 Aug 2014 16:10:43 -0400",Re: Pull requests will be automatically linked to JIRA when submitted,Patrick Wendell <pwendell@gmail.com>,"Thanks for looking into this. I think little tools like this are super
helpful.

Would it hurt to open a request with INFRA to install/configure the
JIRA-GitHub plugin while we continue to use the Python script we have? I
wouldn't mind opening that JIRA issue with them.

Nick



t
,
SF
e
s it with
a-bitbucket-connector-plugin
+GitHub+accounts+to+JIRA
ugins.jira-bitbucket-connector-plugin/86ff1a21-44fb-4227-aa4f-44c77aec2c97.png
9232929
y
r
e.
"
Zhan Zhang <zhazhan@gmail.com>,"Mon, 11 Aug 2014 16:17:17 -0700 (PDT)",Spark testsuite error for hive 0.13.,dev@spark.incubator.apache.org,"I am trying to change spark to support hive-0.13, but always met following
problem when running the test. My feeling is the test setup may need to
change, but don't know exactly. Who has the similar issue or is able to shed
light on it?

13:50:53.331 ERROR org.apache.hadoop.hive.ql.Driver: FAILED:
SemanticException [Error 10072]: Database does not exist: default
org.apache.hadoop.hive.ql.parse.SemanticException: Database does not exist:
default
        at
org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getDatabase(BaseSemanticAnalyzer.java:1302)
        at
org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getDatabase(BaseSemanticAnalyzer.java:1291)
        at
org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeCreateTable(SemanticAnalyzer.java:9944)
        at
org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:9180)
        at
org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:327)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:391)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:291)
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:944)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1009)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:880)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:870)
        at
org.apache.spark.sql.hive.HiveContext.runHive(HiveContext.scala:292)
        at
org.apache.spark.sql.hive.HiveContext.runSqlHive(HiveContext.scala:266)
        at
org.apache.spark.sql.hive.test.TestHiveContext.runSqlHive(TestHive.scala:83)
        at
org.apache.spark.sql.hive.execution.NativeCommand.sideEffectResult$lzycompute(NativeCommand.scala:35)
        at
org.apache.spark.sql.hive.execution.NativeCommand.sideEffectResult(NativeCommand.scala:35)
        at
org.apache.spark.sql.hive.HiveContext$QueryExecution.stringResult(HiveContext.scala:405)
        at
org.apache.spark.sql.hive.test.TestHiveContext$SqlCmd$$anonfun$cmd$1.apply$mcV$sp(TestHive.scala:164)
        at
org.apache.spark.sql.hive.test.TestHiveContext$$anonfun$loadTestTable$2.apply(TestHive.scala:282)
        at
org.apache.spark.sql.hive.test.TestHiveContext$$anonfun$loadTestTable$2.apply(TestHive.scala:282)
        at
scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
        at
scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
        at
org.apache.spark.sql.hive.test.TestHiveContext.loadTestTable(TestHive.scala:282)
        at
org.apache.spark.sql.hive.CachedTableSuite.<init>(CachedTableSuite.scala:28)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native
Method)
        at
sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
        at
sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
        at java.lang.Class.newInstance(Class.java:374)
        at
org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:621)
        at sbt.ForkMain$Run$2.call(ForkMain.java:294)
        at sbt.ForkMain$Run$2.call(ForkMain.java:284)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.hive.ql.parse.SemanticException: Database does
not exist: default
        at
org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getDatabase(BaseSemanticAnalyzer.java:1298)
        ... 35 more



--

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Tue, 12 Aug 2014 00:48:48 +0100",Re: More productive compilation of spark code base,Dev <dev@spark.apache.org>,"Try setting it to handle incremental compilation of Scala by itself
(IntelliJ) and to run its own compile server. This is in global
settings, under the Scala settings. It seems to compile incrementally
for me when I change a file or two.

‚Äôm noticing that it‚Äôs been quite difficult in IntelliJ. It always recompiles everything when I try to run one test like BroadcastTest, for example, despite having compiled make-distribution previously. In eclipse, I have no such recompilation issues. IntelliJ unfortunately does not support auto compilation for Scala. It also doesn‚Äôt seem as if IntelliJ knows that that there are classes that have already been compiled since it always opts to recompile everything. I‚Äôm new to IntelliJ so it might really just be a lack of knowledge on my part.
the Spark code base using IntelliJ?

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Tue, 12 Aug 2014 00:49:44 +0100",Re: Spark testsuite error for hive 0.13.,Zhan Zhang <zhazhan@gmail.com>,"I don't think this will work just by changing the version. Have a look
at: https://issues.apache.org/jira/browse/SPARK-2706


---------------------------------------------------------------------


"
Zhan Zhang <zhazhan@gmail.com>,"Mon, 11 Aug 2014 17:10:44 -0700 (PDT)",Re: Spark testsuite error for hive 0.13.,dev@spark.incubator.apache.org,"Thanks Sean,

I change both the API and version because there are some incompatibility
with hive-0.13, and actually can do some basic operation with the real hive
environment. But the test suite always complain with no default database
message. No clue yet.



--

---------------------------------------------------------------------


"
"""Shao, Saisai"" <saisai.shao@intel.com>","Tue, 12 Aug 2014 01:16:31 +0000",Spark SQL unit test failed when sort-based shuffle is enabled,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi folks,

I met several Spark SQL unit test failures when sort-based shuffle is enabled, seems Spark SQL uses GenericMutableRow which will make ExternalSorter's internal buffer all referred to the same object, I guess GenericMutableRow uses only one mutable object to represent different rows, this is OK for hash-based shuffle because the row is directly written to file; but will be failed in sort-based shuffle because it will store the object to sort them. I just opened a JIRA ticket for this, details can be seen in https://issues.apache.org/jira/browse/SPARK-2967.

Any suggestion?

Thanks
Jerry
"
crigano <chris.p.rigano@gmail.com>,"Mon, 11 Aug 2014 21:56:34 -0700 (PDT)",New to Open Source and Sparc Would Like to Contribute,dev@spark.incubator.apache.org,"I am new at contributing, How is the best way to start out?

Thanks!

Chris



--

---------------------------------------------------------------------


"
Josh Rosen <rosenville@gmail.com>,"Mon, 11 Aug 2014 22:37:06 -0700",Re: New to Open Source and Sparc Would Like to Contribute,dev@spark.incubator.apache.org,"The ‚ÄúContributing to Spark‚Äù guide is a good place to start:¬†https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark


rote:

I am new at contributing, How is the best way to start out?  

Thanks!  

Chris  



--  
.n3.nabble.com/New-to-Open-Source-and-Sparc-Would-Like-to-Contribute-tp7812.html  
.com.  

---------------------------------------------------------------------  
For additional commands, e-mail: dev-help@spark.apache.org  

"
Devl Devel <devl.development@gmail.com>,"Tue, 12 Aug 2014 08:12:59 +0100",Re: Spark Avro Generation,Ron Gonzalez <zlgonzalez@yahoo.com>,"Thanks very much that helps, not having to generate the entire build.



"
Devl Devel <devl.development@gmail.com>,"Tue, 12 Aug 2014 08:19:09 +0100",Compie error with XML elements,"""dev@spark.apache.org"" <dev@spark.apache.org>","When compiling the master checkout of spark. The Intellij compile fails
with:

    Error:(45, 8) not found: value $scope
      <div class=""row-fluid"">
       ^
which is caused by HTML elements in classes like HistoryPage.scala:

    val content =
      <div class=""row-fluid"">
        <div class=""span12"">...

How can I compile these classes that have html node elements in them?

Thanks in advance.
"
Graham Dennis <graham.dennis@gmail.com>,"Tue, 12 Aug 2014 18:44:14 +1000",Re: [SPARK-2878] Kryo serialisation with custom Kryo registrator failing,Reynold Xin <rxin@databricks.com>,"I've submitted a work-in-progress pull request for this issue that I'd like
feedback on.  See https://github.com/apache/spark/pull/1890 . I've also
submitted a pull request for the related issue that the exceptions hit when
trying to use a custom kryo registrator are being swallowed:
https://github.com/apache/spark/pull/1827

The approach in my pull request is to get the Worker processes to download
the application jars and add them to the Executor class path at launch
time. There are a couple of things that still need to be done before this
can be merged:
1. At the moment, the first time a task runs in the executor, the
application jars are downloaded again.  My solution here would be to make
the executor not download any jars that already exist.  Previously, the
driver & executor kept track of the timestamp of jar files and would
redownload 'updated' jars, however this never made sense as the previous
version of the updated jar may have already been loaded into the executor,
so the updated jar may have no effect.  As my current pull request removes
the timestamp for jars, just checking whether the jar exists will allow us
to avoid downloading the jars again.
2. Tests. :-)

A side-benefit of my pull request is that you will be able to use custom
serialisers that are distributed in a user jar.  Currently, the serialiser
instance is created in the Executor process before the first task is
received and therefore before any user jars are downloaded.  As this PR
adds user jars to the Executor process at launch time, this won't be an
issue.



e
ng
t
ly
nt
ress this,
Äôd like to
r‚Äù
t
happening
read, which has
n
ts
‚Äôt previously been
plication
h
 instance,
in this
er, than
 keen for
Äôm
to just
any other
Äôs
have any
y
n
t.
"
fireflyc <fireflyc@163.com>,"Tue, 12 Aug 2014 08:12:34 +0800",Re: fair scheduler,"""dev@spark.apache.org"" <dev@spark.apache.org>","@Crystal
You can use spark on yarn. Yarn have fair scheduler,modified yarn-site.xml.

∑¢◊‘Œ“µƒ iPad

ia@gmail.com> –¥µ¿£∫
parkContext (i.e. within an application), not for separate applications on the standalone cluster manager. It has no effect there. To"
RJ Nowling <rnowling@gmail.com>,"Tue, 12 Aug 2014 14:20:40 -0400",Re: Contributing to MLlib: Proposal for Clustering Algorithms,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,

I wanted to follow up.

I have a prototype for an optimized version of hierarchical k-means.  I
wanted to get some feedback on my apporach.

Jeremy's implementation splits the largest cluster in each round.  Is it
better to do it that way or to split each cluster in half?

Are there are any open-source examples that are being widely used in
production?

Thanks!







-- 
em rnowling@gmail.com
c 954.496.2314
"
RJ Nowling <rnowling@gmail.com>,"Tue, 12 Aug 2014 14:20:40 -0400",Re: Contributing to MLlib: Proposal for Clustering Algorithms,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,

I wanted to follow up.

I have a prototype for an optimized version of hierarchical k-means.  I
wanted to get some feedback on my apporach.

Jeremy's implementation splits the largest cluster in each round.  Is it
better to do it that way or to split each cluster in half?

Are there are any open-source examples that are being widely used in
production?

Thanks!







-- 
em rnowling@gmail.com
c 954.496.2314
"
Zhan Zhang <zhazhan@gmail.com>,"Tue, 12 Aug 2014 15:29:10 -0700 (PDT)",Re: Spark testsuite error for hive 0.13.,dev@spark.incubator.apache.org,"Problem solved by a walkaround with create database and use database.



--

---------------------------------------------------------------------


"
Debasish Das <debasish.das83@gmail.com>,"Tue, 12 Aug 2014 16:39:07 -0700",Re: Using mllib-1.1.0-SNAPSHOT on Spark 1.0.1,Matt Forbes <matt@tellapart.com>,"I figured out the issue....the driver memory was at 512 MB and for our
datasets, the following code needed more memory...

// Materialize usersOut and productsOut.

usersOut.count()

productsOut.count()

Thanks.

Deb



"
Andrew Ash <andrew@andrewash.com>,"Tue, 12 Aug 2014 22:06:27 -0700",FileNotFoundException with _temporary in the name,dev@spark.apache.org,"Hi Spark devs,

Several people on the mailing list have seen issues with
FileNotFoundExceptions related to _temporary in the name.  I've personally
observed this several times, as have a few of my coworkers on various Spark
clusters.

Any ideas what might be going on?

I've collected the various stack traces from various mailing list posts and
put their stacktraces and a link back to the original report at
https://issues.apache.org/jira/browse/SPARK-2984

Thanks!
Andrew
"
Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"Wed, 13 Aug 2014 06:20:01 -0700 (PDT)",Re: Contributing to MLlib: Proposal for Clustering Algorithms,dev@spark.incubator.apache.org,"Hi all,

I am also interested in specifying a common framework.
And I am trying to implement a hierarchical k-means and a hierarchical
clustering like single-link method with LSH.
https://issues.apache.org/jira/browse/SPARK-2966

If you have designed the standardized clustering algorithms API, please let
me know.


best,
Yu Ishikawa



--

---------------------------------------------------------------------


"
Ignacio Zendejas <ignacio.zendejas.cs@gmail.com>,"Wed, 13 Aug 2014 11:09:35 -0700","A Comparison of Platforms for Implementing and Running Very Large
 Scale Machine Learning Algorithms",dev@spark.apache.org,"Has anyone had a chance to look at this paper (with title in subject)?
http://www.cs.rice.edu/~lp6/comparison.pdf

Interesting that they chose to use Python alone. Do we know how much faster
Scala is vs. Python in general, if at all?

As with any and all benchmarks, I'm sure there are caveats, but it'd be
nice to have a response to the question above for starters.

Thanks,
Ignacio
"
Reynold Xin <rxin@databricks.com>,"Wed, 13 Aug 2014 12:29:16 -0700","Re: A Comparison of Platforms for Implementing and Running Very Large
 Scale Machine Learning Algorithms",Ignacio Zendejas <ignacio.zendejas.cs@gmail.com>,"They only compared their own implementations of couple algorithms on
different platforms rather than comparing the different platforms
themselves (in the case of Spark -- PySpark). I can write two variants of
an algorithm on Spark and make them perform drastically differently.

I have no doubt if you implement a ML algorithm in Python itself without
any native libraries, the performance will be sub-optimal.

What PySpark really provides is:

- Using Spark transformations in Python
- ML algorithms implemented in Scala (leveraging native numerical libraries
for high performance), and callable in Python

The paper claims ""Python is now one of the most popular languages for
ML-oriented programming"", and that's why they went ahead with Python.
However, as I understand, very few people actually implement algorithms in
Python directly because of the sub-optimal performance. Most people
implement algorithms in other languages (e.g. C / Java), and expose APIs in
Python for ease-of-use. This is what we are trying to do with PySpark as
well.



"
Jeremy Freeman <freeman.jeremy@gmail.com>,"Wed, 13 Aug 2014 13:00:15 -0700 (PDT)","Re: A Comparison of Platforms for Implementing and Running Very
 Large Scale Machine Learning Algorithms",dev@spark.incubator.apache.org,"Our experience matches Reynold's comments; pure-Python implementations of
anything are generally sub-optimal compared to pure Scala implementations,
or Scala versions exposed to Python (which are faster, but still slower than
pure Scala). It also seems on first glance that some of the implementations
in the paper themselves might not have been optimal (regardless of Python vs
Scala).

All that said, we have found it useful to implement some workflows purely in
Python, mainly when we want to exploit libraries like NumPy, SciPy, or
Scikit Learn, or incorporate existing Python code bases, in which case the
flexibility is worth a drop in performance, at least for us! This might also
make more sense for specialized routines as opposed to core, low-level
algorithms.



--

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 13 Aug 2014 16:04:39 -0400","Re: A Comparison of Platforms for Implementing and Running Very Large
 Scale Machine Learning Algorithms",Reynold Xin <rxin@databricks.com>,"<https://github.com/vertica/DistributedR>, which is coming out of
HP/Vertica and seems to be their proposition for machine learning at scale.

It would be interesting to see some kind of comparison between that and
MLlib (and perhaps also SparkR <https://github.com/amplab-extras/SparkR-pkg>?),
especially since Distributed R has a concept of distributed arrays and
works on data in-memory. Docs are here.
<https://github.com/vertica/DistributedR/tree/master/doc/platform>

Nick



"
Ignacio Zendejas <ignacio.zendejas.cs@gmail.com>,"Wed, 13 Aug 2014 14:16:14 -0700","Re: A Comparison of Platforms for Implementing and Running Very Large
 Scale Machine Learning Algorithms",Nicholas Chammas <nicholas.chammas@gmail.com>,"Yep, I thought it was a bogus comparison.

I should rephrase my question as it was poorly phrased: on average, how
much faster is Spark v. PySpark (I didn't really mean Scala v. Python)?
I've only used Spark and don't have a chance to test this at the moment so
if anybody has these numbers or general estimates (10x, etc), that'd be
great.

@Jeremy, if you can discuss this, what's an example of a project you
implemented using these libraries + PySpark?

Thanks everyone!





"
Reynold Xin <rxin@databricks.com>,"Wed, 13 Aug 2014 14:16:28 -0700","Re: A Comparison of Platforms for Implementing and Running Very Large
 Scale Machine Learning Algorithms",Nicholas Chammas <nicholas.chammas@gmail.com>,"Actually I believe the same person started both projects.

The Distributed R project from HP was started by Shivaram Venkataraman when
he was there. He since moved to Berkeley AMPLab to pursue a PhD and SparkR
was his latest project.




"
Reynold Xin <rxin@databricks.com>,"Wed, 13 Aug 2014 14:20:24 -0700","Re: A Comparison of Platforms for Implementing and Running Very Large
 Scale Machine Learning Algorithms",Nicholas Chammas <nicholas.chammas@gmail.com>,"BTW you can find the original Presto (rebranded as Distributed R) paper
here:
http://eurosys2013.tudos.org/wp-content/uploads/2013/paper/Venkataraman.pdf



"
Robert C Senkbeil <rcsenkbe@us.ibm.com>,"Wed, 13 Aug 2014 16:21:09 -0500",Added support for :cp <jar> to the Spark Shell,"""dev@spark.apache.org""@us.ibm.com","

I've created a new pull request, which can be found at
https://github.com/apache/spark/pull/1929. Since Spark is using Scala
2.10.3 and there is a known issue with Scala 2.10.x not supporting the :cp
command (https://issues.scala-lang.org/browse/SI-6502), the Spark shell
does not have the ability to add jars to the classpath after it has been
started.

The advantage of dynamically adding the jars versus restarting the
shell/interpreter (global) is that you can keep your shell's current state
(you don't lose your RDDs or anything). The previously-supported Scala
2.9.x implementation wiped the interpreter and replayed all of the
commands. This isn't ideal for Spark since some operations can still be
quite heavy. Furthermore, if some operations involved loading external
data, there is the potential for said data to have changed if you replay
the commands.

Signed,
Chip Senkbeil"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Wed, 13 Aug 2014 14:21:19 -0700","Re: A Comparison of Platforms for Implementing and Running Very Large
 Scale Machine Learning Algorithms",Reynold Xin <rxin@databricks.com>,"Yeah I worked on DistributedR while I was an intern at HP Labs, but it has
evolved a lot since then. I don't think its a direct comparison as
DistributedR is a pure R implementation in a distributed setting while
SparkR is a wrapper around the Scala / Java implementations in Spark.

That said, it would be an interesting exercise to compare them and I hope
to do it at some point.

Shivaram



"
Reynold Xin <rxin@databricks.com>,"Wed, 13 Aug 2014 14:28:45 -0700",Re: Added support for :cp <jar> to the Spark Shell,Robert C Senkbeil <rcsenkbe@us.ibm.com>,"I haven't read the code yet, but if it is what I think it is, this is
SUPER, UBER, HUGELY useful.

satisfactory answer ....
https://groups.google.com/forum/#!msg/scala-internals/_cZ1pK7q6cU/xyBQA0DdcYwJ



"
Davies Liu <davies@databricks.com>,"Wed, 13 Aug 2014 14:31:49 -0700","Re: A Comparison of Platforms for Implementing and Running Very Large
 Scale Machine Learning Algorithms",Ignacio Zendejas <ignacio.zendejas.cs@gmail.com>,"
A quick comparison by word count on 4.3G text file (local mode),

Spark:  40 seconds
PySpark: 2 minutes and 16 seconds

So PySpark is 3.4x slower than Spark.

---------------------------------------------------------------------


"
Graham Dennis <graham.dennis@gmail.com>,"Thu, 14 Aug 2014 07:47:48 +1000",Re: [SPARK-2878] Kryo serialisation with custom Kryo registrator failing,Reynold Xin <rxin@databricks.com>,"I now have a complete pull request for this issue that I'd like to get
reviewed and committed.  The PR is available here:
https://github.com/apache/spark/pull/1890 and includes a testcase for the
issue I described.  I've also submitted a related PR (
https://github.com/apache/spark/pull/1827) that causes exceptions raised
while attempting to run the custom kryo registrator not to be swallowed.

Thanks,
Graham



it
d
,
s
s
r
re
s
ing
st
ent
:
dress
‚Äôd like to
 happening
hread, which has
s
an
‚Äôt previously been
pplication
ch
o
 in this
ier, than
m keen for
Äôm
 to just
 any other
Äôs
 have any
on
"
Davies Liu <davies@databricks.com>,"Wed, 13 Aug 2014 14:52:03 -0700","Re: A Comparison of Platforms for Implementing and Running Very Large
 Scale Machine Learning Algorithms",Ignacio Zendejas <ignacio.zendejas.cs@gmail.com>,"
I also tried DPark, which is a pure Python clone of Spark:

DPark: 53 seconds

so it's 2 times faster than PySpark, because of it does not have
the over head of passing data between JVM and Python.

---------------------------------------------------------------------


"
aniketadnaik <aniket.adnaik@gmail.com>,"Wed, 13 Aug 2014 16:36:50 -0700 (PDT)",Need info on Spark's Communication/Networking layer...,dev@spark.incubator.apache.org,"Hi,
I am new to Spark and want to explore more on Spark's master-worker/Cluster
manager communication architecture.
Any documents ? or code pointers will be helpful to start with.
Thanks!



--

---------------------------------------------------------------------


"
Rajiv Abraham <rajiv.abraham@gmail.com>,"Wed, 13 Aug 2014 19:57:11 -0400",Re: Need info on Spark's Communication/Networking layer...,aniketadnaik <aniket.adnaik@gmail.com>,"Hi Aniket,
Perhaps this video will help:
https://www.youtube.com/watch?v=HG2Yd-3r4-M&list=PLTPXxbhUt-YWGNTaDj6HSjnHMxiTD1HCR&index=1

You can see other upto date videos and slides here at :
http://spark-summit.org/2014/training

Best regards,
Rajiv


2014-08-13 19:36 GMT-04:00 aniketadnaik <aniket.adnaik@gmail.com>:



-- 
Take care,
Rajiv
"
=?UTF-8?B?54mb5YWG5o23?= <nzjemail@gmail.com>,"Thu, 14 Aug 2014 11:12:55 +0800",acquire and give back resources dynamically,dev@spark.apache.org,"Dear all:

Does spark can acquire resources from and give back resources to
YARN dynamically ?


-- 
*Regards,*
*Zhaojie*
"
Reynold Xin <rxin@databricks.com>,"Wed, 13 Aug 2014 21:51:42 -0700",proposal for pluggable block transfer interface,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi devs,

I posted a design doc proposing an interface for pluggable block transfer
(used in shuffle, broadcast, block replication, etc). This is expected to
be done in 1.2 time frame.

It should make our code base cleaner, and enable us to provide alternative
implementations of block transfers (e.g. via the new Netty module that I'm
working on, or possibly via MapR file system).

https://issues.apache.org/jira/browse/SPARK-3019

Please take a look and comment on the JIRA ticket. Thanks.
"
Debasish Das <debasish.das83@gmail.com>,"Wed, 13 Aug 2014 23:36:33 -0700",Kryo serialization issues,dev <dev@spark.apache.org>,"Hi,

Is there a JIRA for this bug ?

I have seen it multiple times during our ALS runs now...some runs don't
show while some runs fail due to the error msg

https://github.com/GrahamDennis/spark-kryo-serialisation/blob/master/README.md

performance will get impacted...

Thanks.
Deb
"
Jeremy Freeman <freeman.jeremy@gmail.com>,"Thu, 14 Aug 2014 02:38:18 -0400",Re: A Comparison of Platforms for Implementing and Running Very Large Scale Machine Learning Algorithms,Ignacio Zendejas <ignacio.zendejas.cs@gmail.com>,"@Ignacio, happy to share, here's a link to a library we've been developing (https://github.com/freeman-lab/thunder). As just a couple examples, we have pipelines that use fourier transforms and other signal processing from scipy, and others that do massiv"
Debasish Das <debasish.das83@gmail.com>,"Wed, 13 Aug 2014 23:56:03 -0700",Re: [SPARK-2878] Kryo serialisation with custom Kryo registrator failing,Graham Dennis <graham.dennis@gmail.com>,"Sorry I just saw Graham's email after sending my previous email about this
bug...

I have been seeing this same issue on our ALS runs last week but I thought
it was due my hacky way to run mllib 1.1 snapshot on core 1.0...

What's the status of this PR ? Will this fix be back-ported to 1.0.1 as we
are running 1.0.1 stable standalone cluster ?

Till the PR merges does it make sense to not use Kryo ? What are the other
recommended efficient serializers ?

Thanks.
Deb



is
ke
s
m
ix
to
m
re:
address
‚Äôd like to
d
at
is
 thread, which has
ar
n‚Äôt previously
ryo
e.
o
/
as in this
y,
rlier,
ôm keen
‚Äôm
ar to just
re any
‚Äôs
ld have
,
ds
"
Graham Dennis <graham.dennis@gmail.com>,"Thu, 14 Aug 2014 16:59:08 +1000",Re: [SPARK-2878] Kryo serialisation with custom Kryo registrator failing,Debasish Das <debasish.das83@gmail.com>,"Hi Deb,

The only alternative serialiser is the JavaSerialiser (the default).
 Theoretically Spark supports custom serialisers, but due to a related
issue, custom serialisers currently can't live in application jars and must
be available to all executors at launch.  My PR fixes this issue as well,
allowing custom serialisers to be shipped in application jars.

Graham



s
t
e
r
e
e
s
e
us
w
om
R
n
r
m
ere:
 address
I‚Äôd like
nd
 is
ù thread, which
I
en‚Äôt previously
e
kryo
yo
 /
(as in
arlier,
ôm keen
‚Äôm
jar to
are any
‚Äôs
r
uld have
s,
"
Reynold Xin <rxin@databricks.com>,"Thu, 14 Aug 2014 00:37:10 -0700",Re: [SPARK-2878] Kryo serialisation with custom Kryo registrator failing,Graham Dennis <graham.dennis@gmail.com>,"Graham,

Thanks for working on this. This is an important bug to fix.

I don't have the whole context and obviously I haven't spent nearly as much
time on this as you have, but I'm wondering what if we always pass the
executor's ClassLoader to the Kryo serializer? Will that solve this problem?





st
d
.
d
h
he
PR
an
:
n
y
s
n
om
here:
d address
 I‚Äôd like
s
n is
ù thread, which
ven‚Äôt previously
he
e
 kryo
ryo
n /
 (as in
earlier,
Äôm
‚Äôm
 jar to
 are any
er
ould have
e
"
Graham Dennis <graham.dennis@gmail.com>,"Thu, 14 Aug 2014 17:53:46 +1000",Re: [SPARK-2878] Kryo serialisation with custom Kryo registrator failing,Reynold Xin <rxin@databricks.com>,"Hi Reynold,

That would solve this specific issue, but you'd need to be careful that you
never created a serialiser instance before the first task is received.
 Currently in Executor.TaskRunner.run a closure serialiser instance is
created before any application jars are downloaded, but that could be
moved.  To me, this seems a little fragile.

However there is a related issue where you can't ship a custom serialiser
in an application jar because the serialiser is instantiated when the
SparkEnv object is created, which is before any tasks are received by the
executor.  The above approach wouldn't help with this problem.
 Additionally, the YARN scheduler currently uses this approach of adding
the application jar to the Executor classpath, so it would make things a
bit more uniform.

Cheers,
Graham



he
em?
ust
,
.
d.
:
'd
ch
r
en
e
d
 here:
nd address
d I‚Äôd
es
on is
ù thread, which
t
n
aven‚Äôt previously
the
he
,
s kryo
Kryo
on /
d (as in
 earlier,
Äôm
n jar to
e are any
would
ne
"
Debasish Das <debasish.das83@gmail.com>,"Thu, 14 Aug 2014 00:57:17 -0700",Re: [SPARK-2878] Kryo serialisation with custom Kryo registrator failing,Graham Dennis <graham.dennis@gmail.com>,"By the way I have seen this same problem while deploying 1.1.0-SNAPSHOT on
YARN as well...

So it is a common problem in both standalone and YARN mode deployment...



.
the
lem?
must
l,
..
s
m
t
e
o
d
s
s
e
he
e here:
and
nd I‚Äôd
,
t
ion is
Äù thread,
e
haven‚Äôt
 the
is kryo
k
 Kryo
ion /
nd (as in
r
n
Äôm
e
on jar to
re are
 would
e
"
Graham Dennis <graham.dennis@gmail.com>,"Thu, 14 Aug 2014 18:10:50 +1000",Re: [SPARK-2878] Kryo serialisation with custom Kryo registrator failing,Debasish Das <debasish.das83@gmail.com>,"In part, my assertion was based on a comment by sryza on my PR (
https://github.com/apache/spark/pull/1890#issuecomment-51805750), however I
thought I had also seen it in the YARN code base.  However, now that I look
for it, I can't find where this happens, so perhaps I was imagining the
YARN behaviour.



n
d.
r
e
 the
blem?
m
 must
ll,
:
...
et
,
ld
is
8
t
ue here:
 and
and I‚Äôd
tion is
Äù thread,
se
 haven‚Äôt
e the
his kryo
t
rk
m Kryo
tion /
und (as
on
‚Äôm
he
t
ion jar
ere are
s would
re
t
"
Reynold Xin <rxin@databricks.com>,"Thu, 14 Aug 2014 02:23:04 -0700",Re: [SPARK-2878] Kryo serialisation with custom Kryo registrator failing,Graham Dennis <graham.dennis@gmail.com>,"Graham,

SparkEnv only creates a KryoSerializer, but as I understand that serializer
doesn't actually initializes the registrator since that is only called when
newKryo() is called when KryoSerializerInstance is initialized.

Basically I'm thinking a quick fix for 1.2:

1. Add a classLoader field to KryoSerializer; initialize new
KryoSerializerInstance with that class loader

2. Set that classLoader to the executor's class loader when Executor is
initialized.

Then all deser calls should be using the executor's class loader.





.
the
lem?
must
l,
..
s
m
t
e
o
d
s
s
e
he
e here:
and
nd I‚Äôd
,
t
ion is
Äù thread,
e
haven‚Äôt
 the
is kryo
k
 Kryo
ion /
nd (as in
r
n
Äôm
e
on jar to
re are
 would
e
"
Graham Dennis <graham.dennis@gmail.com>,"Thu, 14 Aug 2014 19:35:50 +1000",Re: [SPARK-2878] Kryo serialisation with custom Kryo registrator failing,Reynold Xin <rxin@databricks.com>,"That should work, but would you also make these changes to the
JavaSerializer?  The API of these is the same so that you can select one or
the other (or in theory a custom serializer)?  This also wouldn't address
the problem of shipping custom *serializers* (not kryo registrators) in
user jars.


y
d.
d.
r
e
 the
blem?
m
 must
ll,
:
...
et
,
ld
is
8
t
ue here:
 and
and I‚Äôd
tion is
Äù thread,
se
 haven‚Äôt
e the
his kryo
t
rk
m Kryo
tion /
und (as
on
‚Äôm
he
t
ion jar
ere are
s would
re
t
"
Reynold Xin <rxin@databricks.com>,"Thu, 14 Aug 2014 10:59:39 -0700",Re: [SPARK-2878] Kryo serialisation with custom Kryo registrator failing,Graham Dennis <graham.dennis@gmail.com>,"Yes, I understand it might not work for custom serializer, but that is a
much less common path.

Basically I want a quick fix for 1.1 release (which is coming up soon). I
would not be comfortable making big changes to class path late into the
release cycle. We can do that for 1.2.






or
ly
ed.
ed.
ived
g
a
pass
s
d
d must
ell,
t
0...
t
t
e
n
sue here:
y and
 and I‚Äôd
ation is
Äù thread,
s
s haven‚Äôt
ve the
p
this kryo
om Kryo
ation /
ound (as
ion
‚Äôm
tion jar
here are
is would
e
.
at
"
Matei Zaharia <matei.zaharia@gmail.com>,"Thu, 14 Aug 2014 12:21:46 -0700","Re: A Comparison of Platforms for Implementing and Running Very
 Large Scale Machine Learning Algorithms","dev@spark.apache.org, Ignacio Zendejas
 <ignacio.zendejas.cs@gmail.com>","Just as a note on this paper, apart from implementing the algorithms in naive Python, they also run it in a fairly inefficient way. In particular their implementations send the model out with every task closure, which is really expensive for a large model, and bring it back with collectAsMap(). It would be much more efficient to send it e.g. with SparkContext.broadcast() or keep it distributed on the cluster throughout the computation, instead of making the drive node a bottleneck for communication.

Implementing ML algorithms well by hand is unfortunately difficult, and this is why we have MLlib. The hope is that you either get your desired algorithm out of the box or get a higher-level primitive (e.g. stochastic gradient descent) that you can plug some functions into, without worrying about the communication.

Matei


Has anyone had a chance to look at this paper (with title in subject)? 
http://www.cs.rice.edu/~lp6/comparison.pdf 

Interesting that they chose to use Python alone. Do we know how much faster 
Scala is vs. Python in general, if at all? 

As with any and all benchmarks, I'm sure there are caveats, but it'd be 
nice to have a response to the question above for starters. 

Thanks, 
Ignacio 
"
Ignacio Zendejas <ignacio.zendejas.cs@gmail.com>,"Thu, 14 Aug 2014 13:25:07 -0700","Re: A Comparison of Platforms for Implementing and Running Very Large
 Scale Machine Learning Algorithms",Matei Zaharia <matei.zaharia@gmail.com>,"Thanks, Jeremy! That's awesome. There's a group at Facebook that is
considering using Spark, so to have more projects to refer to is great.

And Matei, I completely agree. MLlib is very exciting. I respect how well
you guys are managing the project for quality. This will set the Spark
ecosystem apart beyond the already impressive gains in performance and
productivity.

cheers,
Ignacio




"
Mingyu Kim <mkim@palantir.com>,"Thu, 14 Aug 2014 22:39:24 +0000","[SPARK-3050] Spark program running with 1.0.2 jar cannot run
 against a 1.0.1 cluster","""dev@spark.apache.org"" <dev@spark.apache.org>","I ran a really simple code that runs with Spark 1.0.2 jar and connects to a
Spark 1.0.1 cluster, but it fails with java.io.InvalidClassException. I
filed the bug at https://issues.apache.org/jira/browse/SPARK-3050.

I assumed the minor and patch releases shouldnπt break compatibility. Is
that correct?

Thanks,
Mingyu


"
Gary Malouf <malouf.gary@gmail.com>,"Thu, 14 Aug 2014 18:43:12 -0400","Re: [SPARK-3050] Spark program running with 1.0.2 jar cannot run
 against a 1.0.1 cluster",Mingyu Kim <mkim@palantir.com>,"To be clear, is it 'compiled' against 1.0.2 or it packaged with it?



ity. Is
"
Debasish Das <debasish.das83@gmail.com>,"Thu, 14 Aug 2014 17:45:13 -0700",Re: [SPARK-2878] Kryo serialisation with custom Kryo registrator failing,Reynold Xin <rxin@databricks.com>,"Is there a fix that I can test ? I have the flows setup for both standalone
and YARN runs...

Thanks.
Deb




 or
s
nly
zed.
s
m
t
ved.
d
eived
ng
 a
 pass
is
ed
nd must
well,
.0...
1
e
:
o
e
e
l
e
k
t
t
:
e
on
,
ssue here:
ry and
s and I‚Äôd
sation is
‚Äù thread,
ts haven‚Äôt
ave the
 this kryo
tom Kryo
sation /
found (as
tion
ation jar
there are
his would
e
ce
/
h.
e
"
Reynold Xin <rxin@databricks.com>,"Thu, 14 Aug 2014 17:48:16 -0700",Re: [SPARK-2878] Kryo serialisation with custom Kryo registrator failing,Debasish Das <debasish.das83@gmail.com>,"Here: https://github.com/apache/spark/pull/1948




:
I
e or
ss
only
ized.
hat
ed
ceived
ing
s a
s pass
his
.
ted
and must
 well,
1.0...
o
0
d:
he
e
k
r
 on
n,
issue
try and
is and
isation
‚Äù thread,
m
cts haven‚Äôt
have the
r this
stom Kryo
isation /
 found
ption
e
cation
 there
s
this
 /
e
"
Patrick Wendell <pwendell@gmail.com>,"Thu, 14 Aug 2014 18:32:13 -0700","Re: [SPARK-3050] Spark program running with 1.0.2 jar cannot run
 against a 1.0.1 cluster",Gary Malouf <malouf.gary@gmail.com>,"I commented on the bug. For driver mode, you'll need to get the
corresponding version of spark-submit for Spark 1.0.2.



"
scwf <wangfei1@huawei.com>,"Fri, 15 Aug 2014 11:01:17 +0800",mvn test error,<dev@spark.apache.org>,"env: ubuntu 14.04 + spark master buranch

mvn -Pyarn -Phive -Phadoop-2.4 -Dhadoop.version=2.4.0 -DskipTests clean package

mvn -Pyarn -Phadoop-2.4 -Phive test

test error:

DriverSuite:
Spark assembly has been built with Hive, including Datanucleus jars on classpath
- driver should exit after finishing *** FAILED ***
   SparkException was thrown during property evaluation. (DriverSuite.scala:40)
     Message: Process List(./bin/spark-class, org.apache.spark.DriverWithoutCleanup, local) exited with code 1
     Occurred at table row 0 (zero based, not counting headings), which had values (
       master = local
     )

SparkSubmitSuite:
Spark assembly has been built with Hive, including Datanucleus jars on classpath
- launch simple application with spark-submit *** FAILED ***
   org.apache.spark.SparkException: Process List(./bin/spark-submit, --class, org.apache.spark.deploy.SimpleApplicationTest, --name, testApp, --master, local, file:/tmp/1408015655220-0/testJar-1408015655220.jar) exited with code 1

   at org.apache.spark.util.Utils$.executeAndGetOutput(Utils.scala:810)
   at org.apache.spark.deploy.SparkSubmitSuite.runSparkSubmit(SparkSubmitSuite.scala:311)
   at org.apache.spark.deploy.SparkSubmitSuite$$anonfun$14.apply$mcV$sp(SparkSubmitSuite.scala:291)
   at org.apache.spark.deploy.SparkSubmitSuite$$anonfun$14.apply(SparkSubmitSuite.scala:284)
   at org.apache.spark.deploy.SparkSubmitSuite$$anonfun$14.apply(SparkSubmitSuite.scala:284)
   at org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22)
   at org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22)
   at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
   at org.scalatest.Transformer.apply(Transformer.scala:22)
   ...
Spark assembly has been built with Hive, including Datanucleus jars on classpath
- spark submit includes jars passed in through --jar *** FAILED ***
   org.apache.spark.SparkException: Process List(./bin/spark-submit, --class, org.apache.spark.deploy.JarCreationTest, --name, testApp, --master, local-cluster[2,1,512], --jars, file:/tmp/1408015659416-0/testJar-1408015659471.jar,fi
le:/tmp/1408015659472-0/testJar-1408015659513.jar, file:/tmp/1408015659415-0/testJar-1408015659416.jar) exited with code 1
   at org.apache.spark.util.Utils$.executeAndGetOutput(Utils.scala:810)
   at org.apache.spark.deploy.SparkSubmitSuite.runSparkSubmit(SparkSubmitSuite.scala:311)
   at org.apache.spark.deploy.SparkSubmitSuite$$anonfun$15.apply$mcV$sp(SparkSubmitSuite.scala:305)
   at org.apache.spark.deploy.SparkSubmitSuite$$anonfun$15.apply(SparkSubmitSuite.scala:294)
   at org.apache.spark.deploy.SparkSubmitSuite$$anonfun$15.apply(SparkSubmitSuite.scala:294)
   at org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22)
   at org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22)
   at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
   at org.scalatest.Transformer.apply(Transformer.scala:22)
   ...


but only test the specific suite as follows will be ok:
mvn -Pyarn -Phadoop-2.4 -Phive -DwildcardSuites=org.apache.spark.DriverSuite test

it seems when run with ""mvn -Pyarn -Phadoop-2.4 -Phive test"",the process with Utils.executeAndGetOutput started can not exited successfully (exitcode is not zero)

anyone has idea for this?




-- 

Best Regards
Fei Wang

--------------------------------------------------------------------------------



---------------------------------------------------------------------


"
scwf <wangfei1@huawei.com>,"Fri, 15 Aug 2014 11:21:59 +0800",[sql]enable spark sql cli support spark sql,<dev@spark.apache.org>,"hi all,
   now spark sql cli only support spark hql, i think we can enable this cli to support spark sql, do you think it's necessary?

-- 

Best Regards
Fei Wang

--------------------------------------------------------------------------------



---------------------------------------------------------------------


"
"""Cheng, Hao"" <hao.cheng@intel.com>","Fri, 15 Aug 2014 05:13:33 +0000",RE: [sql]enable spark sql cli support spark sql,"scwf <wangfei1@huawei.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Actually the SQL Parser (another SQL dialect in SparkSQL) is quite weak, and only support some basic queries, not sure what's the plan for its enhancement.

   now spark sql cli only support spark hql, i think we can enable this cli to support spark sql, do you think it's necessary?

-- 

Best Regards
Fei Wang

--------------------------------------------------------------------------------



---------------------------------------------------------------------
mands, e-mail: dev-help@spark.apache.org


---------------------------------------------------------------------


"
Cheng Lian <lian.cs.zju@gmail.com>,"Fri, 15 Aug 2014 13:56:31 +0800",Re: [sql]enable spark sql cli support spark sql,"""Cheng, Hao"" <hao.cheng@intel.com>","In the long run, as Michael suggested in his Spark Summit 14 talk, weíd like to implement SQL-92, maybe with the help of Optiq.


weak, and only support some basic queries, not sure what's the plan for its enhancement.
cli to support spark sql, do you think it's necessary?
--------------------------------------------------------------------------------
additional commands, e-mail: dev-help@spark.apache.org


---------------------------------------------------------------------


"
"""Cheng, Hao"" <hao.cheng@intel.com>","Fri, 15 Aug 2014 08:16:35 +0000",RE: [sql]enable spark sql cli support spark sql,Cheng Lian <lian.cs.zju@gmail.com>,"If so, probably we need to add the SQL dialects switching support for SparkSQLCLI, as Fei suggested. What do you think the priority for this?

In the long run, as Michael suggested in his Spark Summit 14 talk, we'd like to implement SQL-92, maybe with the help of Optiq.


and only support some basic queries, not sure what's the plan for its enhancement.
i to support spark sql, do you think it's necessary?


---------------------------------------------------------------------


"
Cheng Lian <lian.cs.zju@gmail.com>,"Fri, 15 Aug 2014 17:20:53 +0800",Re: [sql]enable spark sql cli support spark sql,"""Cheng, Hao"" <hao.cheng@intel.com>","It would be good to have, but Iím afraid itís not super useful for users as the current SqlParser is very limited, especially it doesnít support either DDL statements or any DML statements other then INSERT.


SparkSQLCLI, as Fei suggested. What do you think the priority for this?
we'd like to implement SQL-92, maybe with the help of Optiq.
weak, and only support some basic queries, not sure what's the plan for its enhancement.
cli to support spark sql, do you think it's necessary?
----------------------------------------------------------------------


---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 15 Aug 2014 10:37:02 -0400",Re: -1s on pull requests?,dev <dev@spark.apache.org>,"


This is now captured in this JIRA issue
<https://issues.apache.org/jira/browse/SPARK-2912> and completed in this PR
<https://github.com/apache/spark/pull/1816> which has been merged in to
master.

Example of old style: tests starting
<https://github.com/apache/spark/pull/1819#issuecomment-51416510> / tests
finished <https://github.com/apache/spark/pull/1819#issuecomment-51417477>
(with
new classes)

Example of new style: tests starting
<https://github.com/apache/spark/pull/1816#issuecomment-51855254> / tests
finished <https://github.com/apache/spark/pull/1816#issuecomment-51855255>
(with
new classes)

Nick
"
jerryye <jerryye@gmail.com>,"Fri, 15 Aug 2014 08:18:34 -0700 (PDT)",spark.akka.frameSize stalls job in 1.1.0,dev@spark.incubator.apache.org,"Hi All,
I'm not sure if I should file a JIRA or if I'm missing something obvious
since the test code I'm trying is so simple. I've isolated the problem I'm
seeing to a memory issue but I don't know what parameter I need to tweak, it
does seem related to spark.akka.frameSize. If I sample my RDD with 35% of
the data, everything runs to completion, with more than 35%, it fails. In
standalone mode, I can run on the full RDD without any problems. 

// works 
val samples = sc.textFile(""s3n://geonames"").sample(false,0.35) // 64MB,
2849439 Lines 

// fails 
val samples = sc.textFile(""s3n://geonames"").sample(false,0.4) // 64MB,
2849439 Lines 

Any ideas? 

1) RDD size is causing the problem. The code below as is fails but if I swap
smallSample for samples, the code runs end to end on both cluster and
standalone. 
2) The error I get is: 
rg.apache.spark.SparkException: Job aborted due to stage failure: Task 3.0:1
failed 4 times, most recent failure: TID 12 on host
ip-10-251-14-74.us-west-2.compute.internal failed for unknown reason 
Driver stacktrace: 
        at
org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1044)
        at
org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1028) 
3) Using the 1.1.0 branch the driver freezes instead of aborting with the
previous error in #2.
4) In 1.1.0, changing spark.akka.frameSize also has the effect of no
progress in the driver.

Code: 
val smallSample = sc.parallelize(Array(""foo word"", ""bar word"", ""baz word"")) 

val samples = sc.textFile(""s3n://geonames"") // 64MB, 2849439 Lines of short
strings 

val counts = new collection.mutable.HashMap[String, Int].withDefaultValue(0) 

samples.toArray.foreach(counts(_) += 1) 

val result = samples.map( 
  l => (l, counts.get(l)) 
) 

result.count 

Settings (with or without Kryo doesn't matter): 
export SPARK_JAVA_OPTS=""-Xms5g -Xmx10g -XX:MaxPermSize=10g"" 
export SPARK_MEM=10g 
spark.akka.frameSize 40 
#spark.serializer org.apache.spark.serializer.KryoSerializer 
#spark.kryoserializer.buffer.mb 1000 
spark.executor.memory 58315m 
spark.executor.extraLibraryPath /root/ephemeral-hdfs/lib/native/ 
spark.executor.extraClassPath /root/ephemeral-hdfs/conf



--

---------------------------------------------------------------------


"
Xiangrui Meng <mengxr@gmail.com>,"Fri, 15 Aug 2014 08:35:54 -0700",Re: spark.akka.frameSize stalls job in 1.1.0,jerryye <jerryye@gmail.com>,"Did you set driver memory? You can confirm it in the Executors tab of
the WebUI. Btw, the code may only work in local mode. In a cluster
mode, counts will be serialized to remote workers and the result is
not fetched by the driver after foreach. You can use RDD.countByValue
instead. -Xiangrui


---------------------------------------------------------------------


"
Debasish Das <debasish.das83@gmail.com>,"Fri, 15 Aug 2014 08:58:00 -0700",Re: [SPARK-2878] Kryo serialisation with custom Kryo registrator failing,Reynold Xin <rxin@databricks.com>,"I am still a bit confused that why this issue did not show up in 0.9...at
that time there was no spark-submit and the context was constructed with
low level calls...

Kryo register for ALS was always in my application code..

Was this bug introduced in 1.0 or it was always there ?

a
he
ne or
ess
n
 only
lized.
s
that
ted
eceived
.
ding
gs a
we
 that
e to a
ion jars
s issue
jars.
I
 1.0...
t
e
e
s
s
 issue
 try and
his and
n
e
.
lisation
‚Äù thread,
ects haven‚Äôt
 have the
k
or this
ustom Kryo
lisation /
e found
eption
,
ication
f there
e
 this
r
n
e
"
Patrick Wendell <pwendell@gmail.com>,"Fri, 15 Aug 2014 09:04:26 -0700",Tests failing,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi All,

I noticed that all PR tests run overnight had failed due to timeouts. The
patch that updates the netty shuffle I believe somehow inflated to the
build time significantly. That patch had been tested, but one change was
made before it was merged that was not tested.

I've reverted the patch for now to see if it brings the build times back
down.

- Patrick
"
Vijayakumar Ramdoss <nellaivijay@live.com>,"Fri, 15 Aug 2014 14:08:17 -0400",Re: Need info on Spark's Communication/Networking layer...,Rajiv Abraham <rajiv.abraham@gmail.com>,"Thanks Rajiv

Sent from my iPhone

te:
SjnHMxiTD1HCR&index=1
er
ark-s-Communication-Networking-layer-tp7836.html

---------------------------------------------------------------------


"
Mingyu Kim <mkim@palantir.com>,"Fri, 15 Aug 2014 18:13:36 +0000","Re: [SPARK-3050] Spark program running with 1.0.2 jar cannot run
 against a 1.0.1 cluster","Patrick Wendell <pwendell@gmail.com>, Gary Malouf <malouf.gary@gmail.com>","Thanks for your response. I think I misinterpreted the
stability/compatibility guarantee with 1.0 release. It seems like the
compatibility is only at the API level.

This is interesting because it means any system/product that is built on top
of Spark and uses Spark with a long-running SparkContext connecting to the
cluster over network, will need to make sure it has the exact same version
of Spark jar as the cluster, even to the patch version. This would be
analogous to having to compile Spark against a very specific version of
Hadoop, as opposed to currently being able to use the Spark package with
CDH4 against most of the CDH4 Hadoop clusters.

Is it correct that Spark is focusing and prioritizing around the
spark-submit use cases than the aforementioned use cases? I just wanted to
better understand the future direction/prioritization of spark.

Thanks,
Mingyu

From:  Patrick Wendell <pwendell@gmail.com>
Date:  Thursday, August 14, 2014 at 6:32 PM
To:  Gary Malouf <malouf.gary@gmail.com>
Cc:  Mingyu Kim <mkim@palantir.com>, ""dev@spark.apache.org""
<dev@spark.apache.org>
Subject:  Re: [SPARK-3050] Spark program running with 1.0.2 jar cannot run
against a 1.0.1 cluster

I commented on the bug. For driver mode, you'll need to get the
corresponding version of spark-submit for Spark 1.0.2.


 to
. I
ra/br
BR1uc
4k%3D
.
Is



"
jerryye <jerryye@gmail.com>,"Fri, 15 Aug 2014 11:24:58 -0700 (PDT)",Re: spark.akka.frameSize stalls job in 1.1.0,dev@spark.incubator.apache.org,"Hi Xiangrui,
I wasn't setting spark.driver.memory. I'll try that and report back.

In terms of this running on the cluster, my assumption was that calling foreach on an array(I converted samples using toArray) would mean counts is propagated locally. The object would then be serialized to executors fully propagated. Is this correct?

I'm actually trying to load a trie and used the hashmap as an example of loading data into an object that needs to be serialized. Is there a better way of doing this?

- jerry






--"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Fri, 15 Aug 2014 11:28:04 -0700",Re: Tests failing,Patrick Wendell <pwendell@gmail.com>,"Also I think Jenkins doesn't post build timeouts to github. Is there anyway
we can fix that ?

"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 15 Aug 2014 14:44:54 -0400",Re: Tests failing,Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Shivaram,

Can you point us to an example of that happening? The Jenkins console
output, that is.

Nick



"
Patrick Wendell <pwendell@gmail.com>,"Fri, 15 Aug 2014 12:18:42 -0700",Re: Tests failing,Nicholas Chammas <nicholas.chammas@gmail.com>,"We'll need to build timeouts into our own reporting infrastructure - it
shouldn't be too bad but we just need to script it. Unfortunately the
Jenkins plug-in is either ""all or nothing"" in what it reports, so we can't
have it report timeouts unless we want all the other fairly noisy messages
from it.



"
Patrick Wendell <pwendell@gmail.com>,"Fri, 15 Aug 2014 12:28:23 -0700","Re: [SPARK-3050] Spark program running with 1.0.2 jar cannot run
 against a 1.0.1 cluster",Mingyu Kim <mkim@palantir.com>,"Hey Mingyu,

For this reason we are encouraging all users to run spark-submit. In Spark
we capture closures and send them over the network from the driver to the
executors. These are then deserialized on the executor. So if your driver
program has different versions of certain classes than exist on the
executor, it doesn't work well. We've even run into stranger issues, where
the exact same version of Spark was used at the driver and the executor,
but they were compiled at different times. Since Scala doesn't guarantee
stable naming for certain types of anonymous classes, the class names
didn't match up and it caused errors at runtime.

The most straightforward way to deal with this is to inject, at run-time,
the exact version of Spark that the cluster expects if you are running the
standalone mode.

I think we'd be totally open to improving this to provide ""API stability""
for the case you are working with, i.e. the case where you have spark 1.0.X
at the driver and 1.0.Y on the executors. But it will require looking at
what exactly causes incompatibility and seeing if there is a solution. In
this case I think we changed a publicly exposed class (the RDD class) in
some way that caused compatibility issues... even though we didn't change
any binary signatures.

BTW - this is not relevant to YARN mode where you ship Spark with your job
so there is no ""cluster version of Spark"".

- Patrick



o
d
f
o
.
jira/browse/SPARK-3050&k=fDZpZZQMmYwf27OU23GmAQ%3D%3D%0A&r=UKDOcu6qL3KsoZhpOohNBR1ucPNmWnbd3eEJ9hVUdMk%3D%0A&m=qvQ59wZwD7EuezjTuLzmNTRUamDRDnI7%2F0%2BnULtXk4k%3D%0A&s=b7abf7638a3e6fac2ddac9d8f0ca52f1a92945465abfb2e2d996a96d2301fec5>
Is
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Fri, 15 Aug 2014 12:33:33 -0700",Re: Tests failing,Nicholas Chammas <nicholas.chammas@gmail.com>,"Jenkins runs for this PR https://github.com/apache/spark/pull/1960 timed
out without notification. The relevant Jenkins logs are at

https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/18588/consoleFull
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/18592/consoleFull
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/18597/consoleFull



"
jerryye <jerryye@gmail.com>,"Fri, 15 Aug 2014 12:41:47 -0700 (PDT)",Re: spark.akka.frameSize stalls job in 1.1.0,dev@spark.incubator.apache.org,"Setting spark.driver.memory has no effect. It's still hanging trying to
compute result.count when I'm sampling greater than 35% regardless of what
value of spark.driver.memory I'm setting.

Here's my settings:
export SPARK_JAVA_OPTS=""-Xms5g -Xmx10g -XX:MaxPermSize=10g""
export SPARK_MEM=10g

in conf/spark-defaults:
spark.driver.memory 1500
spark.serializer org.apache.spark.serializer.KryoSerializer
spark.kryoserializer.buffer.mb 500
spark.executor.memory 58315m
spark.executor.extraLibraryPath /root/ephemeral-hdfs/lib/native/
spark.executor.extraClassPath /root/ephemeral-hdfs/conf



--

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 15 Aug 2014 16:31:05 -0400",Re: Tests failing,Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"OK, I've captured this in SPARK-3076
<https://issues.apache.org/jira/browse/SPARK-3076>.

Patrick,

Is the problem that this run-tests
<https://github.com/apache/spark/blob/0afe5cb65a195d2f14e8dfcefdbec5dac023651f/dev/run-tests-jenkins#L151>
step
times out, and that is currently not handled gracefully? To be more
specific, it hangs for 120 minutes, times out, but the parent script for
some reason is also terminated. Does that sound right?

Nick



"
Patrick Wendell <pwendell@gmail.com>,"Fri, 15 Aug 2014 13:43:25 -0700",Re: Tests failing,Nicholas Chammas <nicholas.chammas@gmail.com>,"Hey Nicholas,

Yeah so Jenkins has it's own timeout mechanism and it will just kill the
entire build after 120 minutes. But since run-tests is sitting in the
middle of the tests, it can't actually post a failure message.

I think run-tests-jenkins should just wrap the call to run-tests in a call
in its own timeout. It might be possible to just use this:

http://linux.die.net/man/1/timeout

- Patrick



"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 15 Aug 2014 16:55:46 -0400",Re: Tests failing,Patrick Wendell <pwendell@gmail.com>,"So 2 hours is a hard cap on how long a build can run. Okie doke.

Perhaps then I'll wrap the run-tests step as you suggest and limit it to
100 minutes or something, and cleanly report if it times out.

Sound good?



"
Patrick Wendell <pwendell@gmail.com>,"Fri, 15 Aug 2014 14:04:44 -0700",Re: Tests failing,Nicholas Chammas <nicholas.chammas@gmail.com>,"Yeah I was thinking something like that. Basically we should just have a
variable for the timeout and I can make sure it's under the configured
Jenkins time.



"
Neil Ferguson <nferguson@gmail.com>,"Fri, 15 Aug 2014 23:05:41 +0100","Re: ""Dynamic variables"" in Spark",dev@spark.apache.org,"I've opened SPARK-3051 (https://issues.apache.org/jira/browse/SPARK-3051)
based on this thread.

Neil



"
Xiangrui Meng <mengxr@gmail.com>,"Fri, 15 Aug 2014 15:28:49 -0700",Re: spark.akka.frameSize stalls job in 1.1.0,jerryye <jerryye@gmail.com>,"Did you verify the driver memory in the Executor tab of the WebUI? I
think you need `--driver-memory 8g` with spark-shell or spark-submit
instead of setting it in spark-defaults.conf.


---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 15 Aug 2014 18:47:42 -0400",Re: Tests failing,Patrick Wendell <pwendell@gmail.com>,"*Bam. <https://github.com/apache/spark/pull/1974#issuecomment-52368527>*



"
Stephen Boesch <javadba@gmail.com>,"Fri, 15 Aug 2014 16:07:24 -0700",Extra libs for bin/spark-shell - specifically for hbase,"""dev@spark.apache.org"" <dev@spark.apache.org>","Although this has been discussed a number of times here, I am still unclear
how to add user jars to the spark-shell:

a) for importing classes for use directly within the shell interpreter

b) for  invoking SparkContext commands with closures referencing user
supplied classes contained within jar's.

Similarly to other posts, I have gone through:

 updating bin/spark-env.sh
 SPARK_CLASSPATH
 SPARK_SUBMIT_OPTS
  creating conf/spark-defaults.conf  and adding
 spark.executor.extraClassPath
--driver-class-path
  etc

Hopefully there would be something along the lines of  a single entry added
to some claspath somewhere like this

   SPARK_CLASSPATH/driver-class-path/spark.executor.extraClassPath (or
whatever is the correct option..)  =
$HBASE_HOME/*:$HBASE_HOME/lib/*:$SPARK_CLASSPATH

Any ideas here?

thanks
"
jerryye <jerryye@gmail.com>,"Fri, 15 Aug 2014 17:11:07 -0700 (PDT)",Re: spark.akka.frameSize stalls job in 1.1.0,dev@spark.incubator.apache.org,"Hi Xiangrui,
You were right, I had to use --driver_memory instead of setting it in
spark-defaults.conf.

However, now my just hangs with the following message:
4/08/15 23:54:46 INFO scheduler.TaskSetManager: Serialized task 1.0:0 as
29433434 bytes in 202 ms
14/08/15 23:54:46 INFO scheduler.TaskSetManager: Starting task 1.0:1 as TID
3 on executor 1: ip-10-226-198-31.us-west-2.compute.internal (PROCESS_LOCAL)
14/08/15 23:54:46 INFO scheduler.TaskSetManager: Serialized task 1.0:1 as
29433434 bytes in 203 ms

Any ideas on where else to look?







--"
Xiangrui Meng <mengxr@gmail.com>,"Fri, 15 Aug 2014 22:02:08 -0700",Re: spark.akka.frameSize stalls job in 1.1.0,jerryye <jerryye@gmail.com>,"Just saw you used toArray on an RDD. That copies all data to the
driver and it is deprecated. countByValue is what you need:

val samples = sc.textFile(""s3n://geonames"")
val counts = samples.countByValue()
val result = samples.map(l => (l, counts.getOrElse(l, 0L))

Could you also try to use the latest branch-1.1 or master with the
default akka.frameSize setting? The serialized task size should be
small because we now use broadcast RDD objects.

-Xiangrui

ID
AL)
rs
o
meSize-stalls-job-in-1-1-0-tp7865p7877.html
n
meSize-stalls-job-in-1-1-0-tp7865p7883.html
Servlet.jtp?macro=unsubscribe_by_code&node=7865&code=amVycnl5ZUBnbWFpbC5jb218Nzg2NXwtNTI4OTc1MTAz>
Servlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
.n3.nabble.com/spark-akka-frameSize-stalls-job-in-1-1-0-tp7865p7886.html
.com.

---------------------------------------------------------------------


"
Sandy Ryza <sandy.ryza@cloudera.com>,"Sat, 16 Aug 2014 00:11:21 -0700",Re: Extra libs for bin/spark-shell - specifically for hbase,Stephen Boesch <javadba@gmail.com>,"Hi Stephen,

Have you tried the --jars option (with jars separated by commas)?  It
should make the given jars available both to the driver and the executors.
 I believe one caveat currently is that if you give it a folder it won't
pick up all the jars inside.

-Sandy



"
Jerry Ye <jerryye@gmail.com>,"Sat, 16 Aug 2014 00:16:17 -0700",Re: spark.akka.frameSize stalls job in 1.1.0,Xiangrui Meng <mengxr@gmail.com>,"Hi Xiangrui,
I actually tried branch-1.1 and master and it resulted in the job being
stuck at the TaskSetManager:
14/08/16 06:55:48 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0
with 2 tasks
14/08/16 06:55:48 INFO scheduler.TaskSetManager: Starting task 1.0:0 as TID
2 on executor 8: ip-10-226-199-225.us-west-2.compute.internal
(PROCESS_LOCAL)
14/08/16 06:55:48 INFO scheduler.TaskSetManager: Serialized task 1.0:0 as
28055875 bytes in 162 ms
14/08/16 06:55:48 INFO scheduler.TaskSetManager: Starting task 1.0:1 as TID
3 on executor 0: ip-10-249-53-62.us-west-2.compute.internal (PROCESS_LOCAL)
14/08/16 06:55:48 INFO scheduler.TaskSetManager: Serialized task 1.0:1 as
28055875 bytes in 178 ms

It's been 10 minutes with no progress on relatively small data. I'll let it
run overnight and update in the morning. Is there some place that I should
look to see what is happening? I tried to ssh into the executor and look at
/root/spark/logs but there wasn't anything informative there.

I'm sure using CountByValue works fine but my use of a HashMap is only an
example. In my actual task, I'm loading a Trie data structure to perform
efficient string matching between a dataset of locations and strings
possibly containing mentions of locations.

This seems like a common thing, to process input with a relatively memory
intensive object like a Trie. I hope I'm not missing something obvious. Do
you know of any example code like my use case?

Thanks!

- jerry





s
as
f
eSize-stalls-job-in-1-1-0-tp7865p7877.html
-
eSize-stalls-job-in-1-1-0-tp7865p7883.html
re
rvlet.jtp?macro=unsubscribe_by_code&node=7865&code=amVycnl5ZUBnbWFpbC5jb218Nzg2NXwtNTI4OTc1MTAz
rvlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml
eSize-stalls-job-in-1-1-0-tp7865p7886.html
"
fireflyc <fireflyc@163.com>,"Sat, 16 Aug 2014 16:45:46 +0800",Re: acquire and give back resources dynamically,=?GB2312?B?xaPV173d?= <nzjemail@gmail.com>,"http://spark.apache.org/docs/latest/running-on-yarn.html
Spark just a Yarn application


ail@gmail.com> –¥µ¿£∫


---------------------------------------------------------------------


"
Sujee Maniyam <sujee@sujee.net>,"Sat, 16 Aug 2014 23:04:55 -0700",akka error : play framework (2.3.3) and spark (1.0.2),"Spark User Group <user@spark.apache.org>, dev@spark.apache.org","Hi

I am trying to connect to Spark from Play framework. Getting the following
Akka error...

[ERROR] [08/16/2014 17:12:05.249]
[spark-akka.actor.default-dispatcher-3] [ActorSystem(spark)] Uncaught
fatal error from thread [spark-akka.actor.default-dispatcher-3]
shutting down ActorSystem [spark]
java.lang.AbstractMethodError
  at akka.actor.dungeon.FaultHandling$class.akka$actor$dungeon$FaultHandling$$finishTerminate(FaultHandling.scala:210)
  at akka.actor.dungeon.FaultHandling$class.terminate(FaultHandling.scala:172)
  at akka.actor.ActorCell.terminate(ActorCell.scala:369)
  at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:462)
  at akka.actor.ActorCell.systemInvoke(ActorCell.scala:478)
  at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:263)
  at akka.dispatch.Mailbox.run(Mailbox.scala:219)
  at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)
  at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
  at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
  at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
  at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)


full stack trace : https://gist.github.com/sujee/ff14fd602b76314e693d

source code here : https://github.com/sujee/play-spark-test

I have also found this thread mentioning Akka in-compatibility How to run
Play 2.2.x with Akka 2.3.x?
<http://stackoverflow.com/questions/22779882/how-to-run-play-2-2-x-with-akka-2-3-x>

Stack overflow thread :
http://stackoverflow.com/questions/25346657/akka-error-play-framework-2-3-3-and-spark-1-0-2

any suggestions?

thanks!

Sujee Maniyam (http://sujee.net | http://www.linkedin.com/in/sujeemaniyam )
"
Manu Suryavansh <suryavanshi.manu@gmail.com>,"Sat, 16 Aug 2014 23:15:26 -0700",Re: akka error : play framework (2.3.3) and spark (1.0.2),Sujee Maniyam <sujee@sujee.net>,"Hi,

I tried the Spark(1.0.0)+Play(2.3.3) example from the Knoldus blog -
http://blog.knoldus.com/2014/06/18/play-with-spark-building-apache-spark-with-play-framework/
and
it worked for me. The project is here -
https://github.com/knoldus/Play-Spark-Scala

Regards,
Manu






-- 
Manu Suryavansh
"
Sujee Maniyam <sujee@sujee.net>,"Sun, 17 Aug 2014 00:58:14 -0700",Re: akka error : play framework (2.3.3) and spark (1.0.2),Manu Suryavansh <suryavanshi.manu@gmail.com>,"thanks Manu..

For me, the sample app works only in 'local' mode.
If I tried to connect a spark cluster (even one running locally :
spark://localhost:7077)  I get the following error

spark.master=spark://localhost:7077
[error] o.a.s.s.c.SparkDeploySchedulerBackend - Application has been
killed. Reason: Master removed our application: FAILED
[error] application -

! @6j8im8dfj - Internal server error, for (GET) [/] ->

play.api.Application$$anon$1: Execution exception[[SparkException: Job
aborted due to stage failure: Master removed our application: FAILED]]



Sujee Maniyam (http://sujee.net | http://www.linkedin.com/in/sujeemaniyam )



"
Mingyu Kim <mkim@palantir.com>,"Sun, 17 Aug 2014 16:47:31 +0000","Re: [SPARK-3050] Spark program running with 1.0.2 jar cannot run
 against a 1.0.1 cluster",Patrick Wendell <pwendell@gmail.com>,"Thanks for the clarification. I donπt have a deep knowledge of Scala, but I
thought this was going to be reasonable to support since Java serialization
framework provides relatively easy ways to support these kinds of backwards
compatibility. I can see how this could be harder with closures.

Supporting at least the stability between different patch versions would
help a lot.

Mingyu

From:  Patrick Wendell <pwendell@gmail.com>
Date:  Friday, August 15, 2014 at 12:28 PM
To:  Mingyu Kim <mkim@palantir.com>
Cc:  Gary Malouf <malouf.gary@gmail.com>, ""dev@spark.apache.org""
<dev@spark.apache.org>
Subject:  Re: [SPARK-3050] Spark program running with 1.0.2 jar cannot run
against a 1.0.1 cluster

Hey Mingyu, 

For this reason we are encouraging all users to run spark-submit. In Spark
we capture closures and send them over the network from the driver to the
executors. These are then deserialized on the executor. So if your driver
program has different versions of certain classes than exist on the
executor, it doesn't work well. We've even run into stranger issues, where
the exact same version of Spark was used at the driver and the executor, but
they were compiled at different times. Since Scala doesn't guarantee stable
naming for certain types of anonymous classes, the class names didn't match
up and it caused errors at runtime.

The most straightforward way to deal with this is to inject, at run-time,
the exact version of Spark that the cluster expects if you are running the
standalone mode.

I think we'd be totally open to improving this to provide ""API stability""
for the case you are working with, i.e. the case where you have spark 1.0.X
at the driver and 1.0.Y on the executors. But it will require looking at
what exactly causes incompatibility and seeing if there is a solution. In
this case I think we changed a publicly exposed class (the RDD class) in
some way that caused compatibility issues... even though we didn't change
any binary signatures.

BTW - this is not relevant to YARN mode where you ship Spark with your job
so there is no ""cluster version of Spark"".

- Patrick


ility
e API
top
e
n of
ous
t
bmit
ding
e:
s to
n. I
ira/b
hNBR1
tXk4k
c5> .
 Is



"
Jerry Ye <jerryye@gmail.com>,"Sat, 16 Aug 2014 09:18:37 -0700",Re: spark.akka.frameSize stalls job in 1.1.0,Xiangrui Meng <mengxr@gmail.com>,"The job ended up running overnight with no progress. :-(



d
o
as
s
g
of
meSize-stalls-job-in-1-1-0-tp7865p7877.html
--
meSize-stalls-job-in-1-1-0-tp7865p7883.html
ervlet.jtp?macro=unsubscribe_by_code&node=7865&code=amVycnl5ZUBnbWFpbC5jb218Nzg2NXwtNTI4OTc1MTAz
ervlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml
meSize-stalls-job-in-1-1-0-tp7865p7886.html
"
DouWenjuan <wenjuandou@gmail.com>,"Sun, 17 Aug 2014 22:28:06 -0700 (PDT)",[Graphx] some problem about using SVDPlusPlus,dev@spark.incubator.apache.org,"The implementation of SVDPlusPlus shows that it produces two new graph in
each iteration which will also be cached to memory. However, as the
iteration goes on, more and more graph will be cached and out of memory
happens. So I think it maybe need to unpersist old graph which will not be
used any more and add a few lines of code, the details are showed as
follows:
def run(edges: RDD[Edge[Double]], conf: Conf)
    : (Graph[(DoubleMatrix, DoubleMatrix, Double, Double), Double], Double)
=
  {

    // Generate default vertex attribute
    def defaultF(rank: Int): (DoubleMatrix, DoubleMatrix, Double, Double) =
{
      val v1 = new DoubleMatrix(rank)
      val v2 = new DoubleMatrix(rank)
      for (i <- 0 until rank) {
        v1.put(i, Random.nextDouble())
        v2.put(i, Random.nextDouble())
      }
      (v1, v2, 0.0, 0.0)
    }

    // calculate global rating mean
    edges.cache()

    val (rs, rc) = edges.map(e => (e.attr, 1L)).reduce((a, b) => (a._1 +
b._1, a._2 + b._2))
    val u = rs / rc

    *var preG: Graph[(DoubleMatrix, DoubleMatrix, Double, Double), Double] =
null*

    // construct graph
    var g = Graph.fromEdges(edges, defaultF(conf.rank)).cache()
   * preG = g*

    // Calculate initial bias and norm
    val t0 = g.mapReduceTriplets(
      et => Iterator((et.srcId, (1L, et.attr)), (et.dstId, (1L, et.attr))),
        (g1: (Long, Double), g2: (Long, Double)) => (g1._1 + g2._1, g1._2 +
g2._2))

    g = g.outerJoinVertices(t0) {
      (vid: VertexId, vd: (DoubleMatrix, DoubleMatrix, Double, Double),
       msg: Option[(Long, Double)]) =>
        (vd._1, vd._2, msg.get._2 / msg.get._1, 1.0 /
scala.math.sqrt(msg.get._1))
    }

    def mapTrainF(conf: Conf, u: Double)
        (et: EdgeTriplet[(DoubleMatrix, DoubleMatrix, Double, Double),
Double])
      : Iterator[(VertexId, (DoubleMatrix, DoubleMatrix, Double))] = {
      val (usr, itm) = (et.srcAttr, et.dstAttr)
      val (p, q) = (usr._1, itm._1)
      var pred = u + usr._3 + itm._3 + q.dot(usr._2)
      pred = math.max(pred, conf.minVal)
      pred = math.min(pred, conf.maxVal)
      val err = et.attr - pred
      val updateP = q.mul(err)
        .subColumnVector(p.mul(conf.gamma7))
        .mul(conf.gamma2)
      val updateQ = usr._2.mul(err)
        .subColumnVector(q.mul(conf.gamma7))
        .mul(conf.gamma2)
      val updateY = q.mul(err * usr._4)
        .subColumnVector(itm._2.mul(conf.gamma7))
        .mul(conf.gamma2)
      Iterator((et.srcId, (updateP, updateY, (err - conf.gamma6 * usr._3) *
conf.gamma1)),
        (et.dstId, (updateQ, updateY, (err - conf.gamma6 * itm._3) *
conf.gamma1)))
    }

    for (i <- 0 until conf.maxIters) {

      // Phase 1, calculate pu + |N(u)|^(-0.5)*sum(y) for user nodes
      g.cache()
      *preG.unpersistVertices(blocking = false)
      preG.edges.unpersist(blocking = false)
      preG = g*
      val t1 = g.mapReduceTriplets(
        et => Iterator((et.srcId, et.dstAttr._2)),
        (g1: DoubleMatrix, g2: DoubleMatrix) => g1.addColumnVector(g2))

      g = g.outerJoinVertices(t1) {
        (vid: VertexId, vd: (DoubleMatrix, DoubleMatrix, Double, Double),
         msg: Option[DoubleMatrix]) =>
          if (msg.isDefined) (vd._1, vd._1
            .addColumnVector(msg.get.mul(vd._4)), vd._3, vd._4) else vd
      }

      // Phase 2, update p for user nodes and q, y for item nodes
      g.cache()
      *preG.unpersistVertices(blocking = false)
      preG.edges.unpersist(blocking = false)
      preG = g*
      val t2 = g.mapReduceTriplets(
        mapTrainF(conf, u),
        (g1: (DoubleMatrix, DoubleMatrix, Double), g2: (DoubleMatrix,
DoubleMatrix, Double)) =>
          (g1._1.addColumnVector(g2._1), g1._2.addColumnVector(g2._2), g1._3
+ g2._3))

      g = g.outerJoinVertices(t2) {
        (vid: VertexId,
         vd: (DoubleMatrix, DoubleMatrix, Double, Double),
         msg: Option[(DoubleMatrix, DoubleMatrix, Double)]) =>
          (vd._1.addColumnVector(msg.get._1),
vd._2.addColumnVector(msg.get._2),
            vd._3 + msg.get._3, vd._4)
      }

    }

    // calculate error on training set
    def mapTestF(conf: Conf, u: Double)
        (et: EdgeTriplet[(DoubleMatrix, DoubleMatrix, Double, Double),
Double])
      : Iterator[(VertexId, Double)] =
    {
      val (usr, itm) = (et.srcAttr, et.dstAttr)
      val (p, q) = (usr._1, itm._1)
      var pred = u + usr._3 + itm._3 + q.dot(usr._2)
      pred = math.max(pred, conf.minVal)
      pred = math.min(pred, conf.maxVal)
      val err = (et.attr - pred) * (et.attr - pred)
      Iterator((et.dstId, err))
    }
    g.cache()
    val t3 = g.mapReduceTriplets(mapTestF(conf, u), (g1: Double, g2: Double)
=> g1 + g2)
    g = g.outerJoinVertices(t3) {
      (vid: VertexId, vd: (DoubleMatrix, DoubleMatrix, Double, Double), msg:
Option[Double]) =>
        if (msg.isDefined) (vd._1, vd._2, vd._3, msg.get) else vd
    }

    (g, u)
  }

Bold black lines are the code I added. I hoped that in each iteration when
new graph was cached, old graph would be unpersist. However, the fact seems
to be that both new graph and old graph are unpersist because the time used
for outerJoiinVertices and mapReduceTriples become longer and longer as the
iteration goes on. So I guess that no graph is stored and should be
recomputed in each iteration. In addition, in logs I find ""WARN
impl.ShippableVertexPartitionOps: Joining two VertexPartitions with
different indexes is slow  "".
So how can I correctly unpersist old graph?
Thanks



--

---------------------------------------------------------------------


"
Zhan Zhang <zzhang@hortonworks.com>,"Sun, 17 Aug 2014 23:35:08 -0700",Re: spark.akka.frameSize stalls job in 1.1.0,Jerry Ye <jerryye@gmail.com>,"Is it because countByValue or toArray put too much stress on the driver, if there are many unique words 
To me it is a typical word count problem, then you can solve it as follows (correct me if I am wrong)

val textFile = sc.textFile(ìfile"")
val counts = textFile.flatMap(line => line.split("" "")).map(word => (word, 1)).reduceByKey((a, b) => a + b)
counts.saveAsTextFile(ìfileî)//any way you donít want to collect results to master, and instead putting them in file.

Thanks.

Zhan Zhang


s
s
nd
n
y
Do


-- 
CONFIDENTIALITY NOTICE
NOTICE: This message is intended for the use of the individual or entity to 
which it is addressed and may contain information that is confidential, 
privileged and exempt from disclosure under applicable law. If the reader 
of this message is not the intended recipient, you are hereby notified that 
any printing, copying, dissemination, distribution, disclosure or 
forwarding of this communication is strictly prohibited. If you have 
received this communication in error, please contact the sender immediately 
and delete it from your system. Thank You.

---------------------------------------------------------------------


"
Stephen Boesch <javadba@gmail.com>,"Mon, 18 Aug 2014 09:00:17 -0700",Markdown viewer for the docs,"""dev@spark.apache.org"" <dev@spark.apache.org>","Which viewer is capable of seeing all of the content in the spark docs
-including the (apparent) extensions?

An example page:
https://github.com/apache/spark/blob/master/docs/mllib-linear-methods.md


Local MD viewers/editors that I have  tried include:   mdcharm,  retext and
haroopad: one of these handle the TOC,  the math symbols, or proper
formatting of the scala code

 Even directly opening the md file from  github.com with the browser those
same issues appear: no TOC, math, or proper code formatting.   I am tried
both FF and chrome (on ubuntu 12.0.4)


Any tips from  the creators/maintainers of these pages  Thanks!
"
Jerry Ye <jerryye@gmail.com>,"Mon, 18 Aug 2014 08:54:12 -0700",Re: spark.akka.frameSize stalls job in 1.1.0,Zhan Zhang <zzhang@hortonworks.com>,"Hi Zhan,
Thanks for looking into this. I'm actually using the hash map as an example
of the simplest snippet of code that is failing for me. I know that this is
just the word count. In my actual problem I'm using a Trie data structure
to find substring matches.


:

s
(word,
want to collect results
g
0
s
s
et
rm
.
to
at
ly
"
Kan Zhang <kzhang@apache.org>,"Mon, 18 Aug 2014 09:20:13 -0700",Re: Markdown viewer for the docs,Stephen Boesch <javadba@gmail.com>,"If you are willing to compile it, ""The markdown code can be compiled to
HTML using the [Jekyll tool](http://jekyllrb.com)."" More in docs/README.md.



"
Zhan Zhang <zzhang@hortonworks.com>,"Mon, 18 Aug 2014 09:20:08 -0700",Re: spark.akka.frameSize stalls job in 1.1.0,Jerry Ye <jerryye@gmail.com>,"Not sure exactly how you use it. My understanding is that in spark it would be better to keep the overhead of driver as less as possible. Is it possible to broadcast trie to executors, do computation there and then aggregate the counters (??) in reduct phase?

Thanks.

Zhan Zhang


le of the simplest snippet of code that is failing for me. I know that this is just the word count. In my actual problem I'm using a Trie data structure to find substring matches.
te:
if there are many unique words
s (correct me if I am wrong)
(word, 1)).reduceByKey((a, b) => a + b)
sults to master, and instead putting them in file.
g
0
s
 as
s
 as
et
 and
 an
rm
ory
. Do
to
at
ly


-- 
CONFIDENTIALITY NOTICE
NOTICE: This message is intended for the use of the individual or entity to 
which it is addressed and may contain information that is confidential, 
privileged and exempt from disclosure under applicable law. If the reader 
of this message is not the intended recipient, you are hereby notified that 
any printing, copying, dissemination, distribution, disclosure or 
forwarding of this communication is strictly prohibited. If you have 
received this communication in error, please contact the sender immediately 
and delete it from your system. Thank You.
"
zycodefish <opensourcecodefish@gmail.com>,"Mon, 18 Aug 2014 09:46:32 -0700 (PDT)",Shuffle overlapping,dev@spark.incubator.apache.org,"Hi all,

I'm reading the implementation of the shuffle in Spark. 
My understanding is that it's not overlapping with upstream stage.

Is it helpful to overlap the computation of upstream stage w/ the shuffle (I
mean the network copy, like in Hadoop)? If it is, is there any plan to
implement it in the any version?

--Z



--

---------------------------------------------------------------------


"
Josh Rosen <rosenville@gmail.com>,"Mon, 18 Aug 2014 09:51:17 -0700",Re: Shuffle overlapping,zycodefish <opensourcecodefish@gmail.com>,"I think there's some discussion of this at
https://issues.apache.org/jira/browse/SPARK-2387 and
https://github.com/apache/spark/pull/1328.

- Josh



"
jerryye <jerryye@gmail.com>,"Mon, 18 Aug 2014 09:54:09 -0700 (PDT)",Re: spark.akka.frameSize stalls job in 1.1.0,dev@spark.incubator.apache.org,"I've been trying different approaches of this: populating the trie on the driver and serializing the instance to executors, broadcasting the strings in an array and populating the trie on the executors, and variants of what I'm broadcasting or serializing. All approaches seem to have a memory issue.

Xiangrui has been able to run this snippet on his cluster without problems and we're trying to identify the difference.

- jerry


ld be better to keep the overhead of driver as less as possible. Is it possible to broadcast trie to executors, do computation there and then aggregate the counters (??) in reduct phase? 
mple of the simplest snippet of code that is failing for me. I know that this is just the word count. In my actual problem I'm using a Trie data structure to find substring matches. 
, if there are many unique words 
ows (correct me if I am wrong) 
t want to collect results to master, and instead putting them in file. 
ing 
1.0 
 as 
:0 as 
 as 
:1 as 
 let 
 I 
or and 
. 
ly an 
form 
 
emory 
us. Do 
y to 
 
er 
that 
tely 
to 
 
 
at 
ly 
below:
eSize-stalls-job-in-1-1-0-tp7865p7901.html
001551n1h70@n3.nabble.com 




--
3.nabble.com/spark-akka-frameSize-stalls-job-in-1-1-0-tp7865p7904.html
om."
Gary Malouf <malouf.gary@gmail.com>,"Mon, 18 Aug 2014 17:48:26 -0400",Spark 1.1.0 Progress,"""dev@spark.apache.org"" <dev@spark.apache.org>","I understand there must still being work done preventing the cutting of an
RC, is the specific remaining items tracked just through Jira?
"
Patrick Wendell <pwendell@gmail.com>,"Mon, 18 Aug 2014 15:17:55 -0700",Re: Spark 1.1.0 Progress,Gary Malouf <malouf.gary@gmail.com>,"Hey Gary,

There are couple of blockers in Spark core and SQL - but we're quite close.
The goal was to have rc1 on Friday (ish) of last week... I think by tonight
I will be able to cut one. If not, I'll cut a preview release tonight that
does a full package but doesn't trigger an official vote yet so that people
can test it.

bit.ly/1tgfZrQ

- Patrick



"
scwf <wangfei1@huawei.com>,"Tue, 19 Aug 2014 13:27:05 +0800",Re: mvn test error,<dev@spark.apache.org>,"hi, all
   I notice that jenkins may also throw this error when running tests(https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/18688/consoleFull).


This is because in Utils.executeAndGetOutput our progress exitCode is not 0, may be we should logWarning here rather than throw a exception?

Utils.executeAndGetOutput {
     val exitCode = process.waitFor()
     stdoutThread.join()   // Wait for it to finish reading output
     if (exitCode != 0) {
       throw new SparkException(""Process "" + command + "" exited with code "" + exitCode)
     }
}

any idea?




-- 

Best Regards
Fei Wang

--------------------------------------------------------------------------------



---------------------------------------------------------------------


"
Cheng Lian <lian.cs.zju@gmail.com>,"Tue, 19 Aug 2014 13:35:34 +0800",Re: mvn test error,scwf <wangfei1@huawei.com>,"The exception indicates that the forked process doesn‚Äôt executed as
expected, thus the test case *should* fail.

Instead of replacing the exception with a logWarning, capturing and
printing stdout/stderr of the forked process can be helpful for diagnosis.
Currently the only information we have at hand is the process exit code,
it‚Äôs hard to determine the reason why the forked process fails.
‚Äã



""
n
houtCleanup,
rSuite
"
Debasish Das <debasish.das83@gmail.com>,"Mon, 18 Aug 2014 22:49:07 -0700",Spark on YARN webui,dev <dev@spark.apache.org>,"Hi,

We are running the snapshots (new spark features) on YARN and I was
wondering if the webui is available on YARN mode...

The deployment document does not mention webui on YARN mode...

Is it available ?

Thanks.
Deb
"
scwf <wangfei1@huawei.com>,"Tue, 19 Aug 2014 14:55:50 +0800",Re: mvn test error,Cheng Lian <lian.cs.zju@gmail.com>,"hi,Cheng Lian
   thanks, printing stdout/stderr of the forked process is more reasonable.



-- 

Best Regards
Fei Wang

--------------------------------------------------------------------------------



---------------------------------------------------------------------


"
Debasish Das <debasish.das83@gmail.com>,"Tue, 19 Aug 2014 00:19:58 -0700",Re: [SPARK-2878] Kryo serialisation with custom Kryo registrator failing,Reynold Xin <rxin@databricks.com>,"@rxin With the fixes, I could run it fine on top of branch-1.0


Exception in thread ""main"" org.apache.spark.SparkException: Job aborted due
to stage failure: Task 247 in stage 52.0 failed 4 times, most recent
failure: Lost task 247.3 in stage 52.0 (TID 1"
Cheng Lian <lian.cs.zju@gmail.com>,"Tue, 19 Aug 2014 15:42:00 +0800",Re: mvn test error,scwf <wangfei1@huawei.com>,"Just FYI, thought this might be helpful, I'm refactoring Hive Thrift server
test suites. These suites also fork new processes and suffer similar
issues. Stdout and stderr of forked processes are logged in the new version
of test suites with utilities under scala.sys.process package
https://github.com/apache/spark/pull/1856/files



.
 as
s.
s(
s
sts
__DriverWithoutCleanup,
ationTest,
08015655220.jar)
:
est,
1
:
park.DriverSuite
ly
"
Tom Graves <tgraves_cs@yahoo.com.INVALID>,"Tue, 19 Aug 2014 07:17:28 -0700",Re: Spark on YARN webui,"Debasish Das <debasish.das83@gmail.com>, dev <dev@spark.apache.org>","yes the webui works on yarn. You should be able to go to the Yarn ResourceManager UI and it will have a link to the web UI for a running spark application. †You can also set it up to save the history and view it after it has finished. †History info can be found here:†Monitoring and Instrumentation - Spark 1.0.2 Documentation
† 
† † † † † 
Monitoring and Instrumentation - Spark 1.0.2 Documentation
Monitoring and Instrumentation There are several ways to monitor Spark applications: web UIs, metrics, and external instrumentation. Web Interfaces   
View on spark.apache.Hi,

We are running the snapshots (new spark features) on YARN and I was
wondering if the webui is available on YARN mode...

The deployment document does not mention webui on YARN mode...

Is it available ?

Thanks.
Deb"
alexliu68 <alex_liu68@yahoo.com>,"Tue, 19 Aug 2014 09:53:35 -0700 (PDT)",Spark SQL Query and join different data sources.,dev@spark.incubator.apache.org,"Is there anyone make the query join different data sources work? especially
Join hive table with other data sources.

For example, hql uses HiveContext, and it needs first call ""use
<database_name>"" and other datasources use SqlContext, how can SqlContext
know Hive tables? I follow https://spark.apache.org/sql/ example to join
hive table with Json table, but I can't make it work. Am I missing anything
here?

Alex



--

---------------------------------------------------------------------


"
Chris Fregly <chris@fregly.com>,"Tue, 19 Aug 2014 16:09:10 -0700",Re: Data Locality In Spark,dev@spark.apache.org,"and even the same process where the data might be cached.


these are the different locality levels:

PROCESS_LOCAL
NODE_LOCAL
RACK_LOCAL
ANY

relevant code:
https://github.com/apache/spark/blob/7712e724ad69dd0b83754e938e9799d13a4d43b9/core/src/test/scala/org/apache/spark/scheduler/TaskSetManagerSuite.scala#L150

https://github.com/apache/spark/blob/63bdb1f41b4895e3a9444f7938094438a94d3007/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala#L250

relevant docs:
see the spark.locality configuration attributes here:
https://spark.apache.org/docs/latest/configuration.html



"
Debasish Das <debasish.das83@gmail.com>,"Tue, 19 Aug 2014 20:51:39 -0700",Lost executor on YARN ALS iterations,dev <dev@spark.apache.org>,"Hi,

During the 4th ALS iteration, I am noticing that one of the executor gets
disconnected:

14/08/19 23:40:00 ERROR network.ConnectionManager: Corresponding
SendingConnectionManagerId not found

14/08/19 23:40:00 INFO cluster.YarnClientSchedulerBackend: Executor 5
disconnected, so removing it

14/08/19 23:40:00 ERROR cluster.YarnClientClusterScheduler: Lost executor 5
on tblpmidn42adv-hdp.tdc.vzwcorp.com: remote Akka client disassociated

14/08/19 23:40:00 INFO scheduler.DAGScheduler: Executor lost: 5 (epoch 12)
Any idea if this is a bug related to akka on YARN ?

I am using master

Thanks.
Deb
"
Xiangrui Meng <mengxr@gmail.com>,"Tue, 19 Aug 2014 23:29:31 -0700",Re: Lost executor on YARN ALS iterations,Debasish Das <debasish.das83@gmail.com>,"Hi Deb,

I think this may be the same issue as described in
https://issues.apache.org/jira/browse/SPARK-2121 . We know that the
container got killed by YARN because it used much more memory that it
requested. But we haven't figured out the root cause yet.

+Sandy

Best,
Xiangrui


---------------------------------------------------------------------


"
Debasish Das <debasish.das83@gmail.com>,"Wed, 20 Aug 2014 00:27:39 -0700",Re: Lost executor on YARN ALS iterations,Xiangrui Meng <mengxr@gmail.com>,"I could reproduce the issue in both 1.0 and 1.1 using YARN...so this is
definitely a YARN related problem...

At least for me right now only deployment option possible is standalone...




"
Sandy Ryza <sandy.ryza@cloudera.com>,"Wed, 20 Aug 2014 00:39:56 -0700",Re: Lost executor on YARN ALS iterations,Debasish Das <debasish.das83@gmail.com>,"Hi Debasish,

The fix is to raise spark.yarn.executor.memoryOverhead until this goes
away.  This controls the buffer between the JVM heap size and the amount of
memory requested from YARN (JVMs can take up memory beyond their heap
size). You should also make sure that, in the YARN NodeManager
configuration, yarn.nodemanager.vmem-check-enabled is set to false.

-Sandy



"
Debasish Das <debasish.das83@gmail.com>,"Wed, 20 Aug 2014 09:02:55 -0700",Akka usage in Spark,dev <dev@spark.apache.org>,"Hi,

There have been some recent changes in the way akka is used in spark and I
feel they are major changes...

Is there a design document / JIRA / experiment on large datasets that
highlight the impact of changes (1.0 vs 1.1) ? Basically it will be great
to understand where akka is used in the code base...

If I don't have to broadcast big variables but use akka's programming model
(use actors directly) on Spark's actorsystem is that allowed ? I understand
that it might look hacky :-)

Thanks.
Deb
"
Cody Koeninger <cody@koeninger.org>,"Wed, 20 Aug 2014 14:39:37 -0500",Limit on number of simultaneous Spark frameworks on Mesos?,"user@mesos.apache.org, ""dev@spark.apache.org"" <dev@spark.apache.org>","I'm seeing situations where starting e.g. a 4th spark job on Mesos results
in none of the jobs making progress.  This happens even with
--executor-memory set to values that should not come close to exceeding the
availability per node, and even if the 4th job is doing something
completely trivial (e.g. parallelize 1 to 10000 and sum).  Killing one of
the jobs typically allows the others to start proceeding.

While jobs are hung, I see the following in mesos master logs:

I0820 19:28:02.651296 24666 master.cpp:2282] Sending 7 offers to framework
20140820-170154-1315739402-5050-24660-0020
I0820 19:28:02.654502 24668 master.cpp:1578] Processing reply for offers: [
20140820-170154-1315739402-5050-24660-96624 ] on slave
20140724-150750-1315739402-5050-25405-6 (dn-04) for framework
20140820-170154-1315739402-5050-24660-0020
I0820 19:28:02.654722 24668 hierarchical_allocator_process.hpp:590]
Framework 20140820-170154-1315739402-5050-24660-0020 filtered slave
20140724-150750-1315739402-5050-25405-6 for 1secs

Am I correctly interpreting that to mean that spark is being offered
resources, but is rejecting them?  Is there a way (short of patching spark
to add more logging) to figure out why resources are being rejected?

This is on the default fine-grained mode.
"
Patrick Wendell <pwendell@gmail.com>,"Wed, 20 Aug 2014 13:42:52 -0700",Re: Akka usage in Spark,Debasish Das <debasish.das83@gmail.com>,"Hey Deb,

Can you be specific what changes you are mentioning? We have not, to my
knowledge, made major architectural changes around akka use.

I think in general we don't want people to be using Spark's actor system
directly - it is an internal communication component in Spark and could
e.g. be re-factored later to not use akka at all. Could you elaborate a bit
more on your use case?

- Patrick



"
Cody Koeninger <cody@koeninger.org>,"Wed, 20 Aug 2014 15:52:25 -0500",Re: Limit on number of simultaneous Spark frameworks on Mesos?,user@mesos.apache.org,"At least some of the jobs are typically doing work that would make it
difficult to share, e.g. accessing hdfs.  I'll see if I can get a smaller
reproducible case.



"
Debasish Das <debasish.das83@gmail.com>,"Wed, 20 Aug 2014 15:19:23 -0700",Re: Akka usage in Spark,Patrick Wendell <pwendell@gmail.com>,"Hi Patrick,

Last few days I came across some bugs which got exposed due to ALS runs on
large scale data...although it was not related to the akka changes but
during the debug I found across some akka related changes that might have
an impact of overall performance...one example is the following:

https://github.com/apache/spark/pull/1907

@dbtsai explained it to me a bit yesterday that in 1.1 RDDs are no longer
sent through akka msgs but over http-channels...If there is a document
detailing the architecture that is currently in-place (like how the core
changed from 1.0 to 1.1) it will help a lot in debugging the jobs which are
built upon the libraries like mllib and optimize them further for
efficiency...

For using the Spark actor system directly:

I spent few weeks December 2013 to make the Scalafish code (
https://github.com/azymnis/scalafish) operational on 10 nodes...It uses
scalding for matrix partitioning and actorSystem to coordinate the
updates...It is a cool use of akka but getting an actor system operational
is difficult...

Since Spark already has tested version of actor system running on both
standalone and yarn modes, I am planning to port scalafish to spark using
actor model...That's one of the use-cases I am looking for...

Another use-case that I am considering is to send msgs directly from kafka
queues to spark actorSystem for processing to get Storm like
latency...basically window sizes of 1-2 ms and no overhead of using an RDD
if possible...

Thanks.
Deb



"
DB Tsai <dbtsai@dbtsai.com>,"Wed, 20 Aug 2014 15:28:11 -0700",Re: Akka usage in Spark,Debasish Das <debasish.das83@gmail.com>,"To be specific, I was discussing this PR with Debasish which reduces
lots of issues when sending big objects to executors without using
broadcast explicitly.

Broadcast RDD object once per TaskSet (instead of sending it for every task)
https://issues.apache.org/jira/browse/SPARK-2521

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



---------------------------------------------------------------------


"
Debasish Das <debasish.das83@gmail.com>,"Wed, 20 Aug 2014 15:34:36 -0700",Re: Akka usage in Spark,DB Tsai <dbtsai@dbtsai.com>,"Yeah that's the one we discussed...sorry I pointed to a different one that
I was reading...



"
Chester Chen <chester@alpinenow.com>,"Wed, 20 Aug 2014 16:39:06 -0700",is Branch-1.1 SBT build broken for yarn-alpha ?,"""dev@spark.apache.org"" <dev@spark.apache.org>","I just updated today's build and tried branch-1.1 for both yarn and
yarn-alpha.

For yarn build, this command seem to work fine.

sbt/sbt -Pyarn -Dhadoop.version=2.3.0-cdh5.0.1 projects

for yarn-alpha

sbt/sbt -Pyarn-alpha -Dhadoop.version=2.0.5-alpha projects

I got the following

Any ideas


Chester

·öõ |branch-1.1|$  *sbt/sbt -Pyarn-alpha -Dhadoop.version=2.0.5-alpha
projects*

Using /Library/Java/JavaVirtualMachines/1.6.0_51-b11-457.jdk/Contents/Home
as default JAVA_HOME.

Note, this will be overridden by -java-home if it is set.

[info] Loading project definition from
/Users/chester/projects/spark/project/project

[info] Loading project definition from
/Users/chester/.sbt/0.13/staging/ec3aa8f39111944cc5f2/sbt-pom-reader/project

[warn] Multiple resolvers having different access mechanism configured with
same name 'sbt-plugin-releases'. To avoid conflict, Remove duplicate
project resolvers (`resolvers`) or rename publishing resolver (`publishTo`).

[info] Loading project definition from /Users/chester/projects/spark/project

org.apache.maven.model.building.ModelBuildingException: 1 problem was
encountered while building the effective model for
org.apache.spark:spark-yarn-alpha_2.10:1.1.0

*[FATAL] Non-resolvable parent POM: Could not find artifact
org.apache.spark:yarn-parent_2.10:pom:1.1.0 in central (
http://repo.maven.apache.org/maven2 <http://repo.maven.apache.org/maven2>)
and 'parent.relativePath' points at wrong local POM @ line 20, column 11*


 at
org.apache.maven.model.building.DefaultModelProblemCollector.newModelBuildingException(DefaultModelProblemCollector.java:195)

at
org.apache.maven.model.building.DefaultModelBuilder.readParentExternally(DefaultModelBuilder.java:841)

at
org.apache.maven.model.building.DefaultModelBuilder.readParent(DefaultModelBuilder.java:664)

at
org.apache.maven.model.building.DefaultModelBuilder.build(DefaultModelBuilder.java:310)

at
org.apache.maven.model.building.DefaultModelBuilder.build(DefaultModelBuilder.java:232)

at
com.typesafe.sbt.pom.MvnPomResolver.loadEffectivePom(MavenPomResolver.scala:61)

at com.typesafe.sbt.pom.package$.loadEffectivePom(package.scala:41)

at
com.typesafe.sbt.pom.MavenProjectHelper$.makeProjectTree(MavenProjectHelper.scala:128)

at
com.typesafe.sbt.pom.MavenProjectHelper$$anonfun$12.apply(MavenProjectHelper.scala:129)

at
com.typesafe.sbt.pom.MavenProjectHelper$$anonfun$12.apply(MavenProjectHelper.scala:129)

at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)

at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)

at
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)

at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)

at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)

at scala.collection.AbstractTraversable.map(Traversable.scala:105)

at
com.typesafe.sbt.pom.MavenProjectHelper$.makeProjectTree(MavenProjectHelper.scala:129)

at
com.typesafe.sbt.pom.MavenProjectHelper$$anonfun$12.apply(MavenProjectHelper.scala:129)

at
com.typesafe.sbt.pom.MavenProjectHelper$$anonfun$12.apply(MavenProjectHelper.scala:129)

at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)

at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)

at
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)

at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)

at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)

at scala.collection.AbstractTraversable.map(Traversable.scala:105)

at
com.typesafe.sbt.pom.MavenProjectHelper$.makeProjectTree(MavenProjectHelper.scala:129)

at
com.typesafe.sbt.pom.MavenProjectHelper$.makeReactorProject(MavenProjectHelper.scala:49)

at com.typesafe.sbt.pom.PomBuild$class.projectDefinitions(PomBuild.scala:28)

at SparkBuild$.projectDefinitions(SparkBuild.scala:165)

at sbt.Load$.sbt$Load$$projectsFromBuild(Load.scala:458)

at sbt.Load$$anonfun$24.apply(Load.scala:415)

at sbt.Load$$anonfun$24.apply(Load.scala:415)

at scala.collection.immutable.Stream.flatMap(Stream.scala:442)

at sbt.Load$.loadUnit(Load.scala:415)

at sbt.Load$$anonfun$15$$anonfun$apply$11.apply(Load.scala:256)

at sbt.Load$$anonfun$15$$anonfun$apply$11.apply(Load.scala:256)

at
sbt.BuildLoader$$anonfun$componentLoader$1$$anonfun$apply$4$$anonfun$apply$5$$anonfun$apply$6.apply(BuildLoader.scala:93)

at
sbt.BuildLoader$$anonfun$componentLoader$1$$anonfun$apply$4$$anonfun$apply$5$$anonfun$apply$6.apply(BuildLoader.scala:92)

at sbt.BuildLoader.apply(BuildLoader.scala:143)

at sbt.Load$.loadAll(Load.scala:312)

at sbt.Load$.loadURI(Load.scala:264)

at sbt.Load$.load(Load.scala:260)

at sbt.Load$.load(Load.scala:251)

at sbt.Load$.apply(Load.scala:134)

at sbt.Load$.defaultLoad(Load.scala:37)

at sbt.BuiltinCommands$.doLoadProject(Main.scala:473)

at sbt.BuiltinCommands$$anonfun$loadProjectImpl$2.apply(Main.scala:467)

at sbt.BuiltinCommands$$anonfun$loadProjectImpl$2.apply(Main.scala:467)

at
sbt.Command$$anonfun$applyEffect$1$$anonfun$apply$2.apply(Command.scala:60)

at
sbt.Command$$anonfun$applyEffect$1$$anonfun$apply$2.apply(Command.scala:60)

at
sbt.Command$$anonfun$applyEffect$2$$anonfun$apply$3.apply(Command.scala:62)

at
sbt.Command$$anonfun$applyEffect$2$$anonfun$apply$3.apply(Command.scala:62)

at sbt.Command$.process(Command.scala:95)

at sbt.MainLoop$$anonfun$1$$anonfun$apply$1.apply(MainLoop.scala:100)

at sbt.MainLoop$$anonfun$1$$anonfun$apply$1.apply(MainLoop.scala:100)

at sbt.State$$anon$1.process(State.scala:179)

at sbt.MainLoop$$anonfun$1.apply(MainLoop.scala:100)

at sbt.MainLoop$$anonfun$1.apply(MainLoop.scala:100)

at sbt.ErrorHandling$.wideConvert(ErrorHandling.scala:18)

at sbt.MainLoop$.next(MainLoop.scala:100)

at sbt.MainLoop$.run(MainLoop.scala:93)

at sbt.MainLoop$$anonfun$runWithNewLog$1.apply(MainLoop.scala:71)

at sbt.MainLoop$$anonfun$runWithNewLog$1.apply(MainLoop.scala:66)

at sbt.Using.apply(Using.scala:25)

at sbt.MainLoop$.runWithNewLog(MainLoop.scala:66)

at sbt.MainLoop$.runAndClearLast(MainLoop.scala:49)

at sbt.MainLoop$.runLoggedLoop(MainLoop.scala:33)

at sbt.MainLoop$.runLogged(MainLoop.scala:25)

at sbt.StandardMain$.runManaged(Main.scala:57)

at sbt.xMain.run(Main.scala:29)

at xsbt.boot.Launch$$anonfun$run$1.apply(Launch.scala:109)

at xsbt.boot.Launch$.withContextLoader(Launch.scala:129)

at xsbt.boot.Launch$.run(Launch.scala:109)

at xsbt.boot.Launch$$anonfun$apply$1.apply(Launch.scala:36)

at xsbt.boot.Launch$.launch(Launch.scala:117)

at xsbt.boot.Launch$.apply(Launch.scala:19)

at xsbt.boot.Boot$.runImpl(Boot.scala:44)

at xsbt.boot.Boot$.main(Boot.scala:20)

at xsbt.boot.Boot.main(Boot.scala)

[error] org.apache.maven.model.building.ModelBuildingException: 1 problem
was encountered while building the effective model for
org.apache.spark:spark-yarn-alpha_2.10:1.1.0

[error] [FATAL] Non-resolvable parent POM: Could not find artifact
org.apache.spark:yarn-parent_2.10:pom:1.1.0 in central (
http://repo.maven.apache.org/maven2) and 'parent.relativePath' points at
wrong local POM @ line 20, column 11

[error] Use 'last' for the full log.

Project loading failed: (r)etry, (q)uit, (l)ast, or (i)gnore? q
"
Chester Chen <chester@alpinenow.com>,"Wed, 20 Aug 2014 16:42:01 -0700",Re: is Branch-1.1 SBT build broken for yarn-alpha ?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Just tried on master branch, and the master branch works fine for yarn-alpha


:

lpha
e
ect
`).
)
dingException(DefaultModelProblemCollector.java:195)
DefaultModelBuilder.java:841)
elBuilder.java:664)
lder.java:310)
lder.java:232)
la:61)
er.scala:128)
per.scala:129)
per.scala:129)
la:244)
la:244)
a:59)
er.scala:129)
per.scala:129)
per.scala:129)
la:244)
la:244)
a:59)
er.scala:129)
elper.scala:49)
y$5$$anonfun$apply$6.apply(BuildLoader.scala:93)
y$5$$anonfun$apply$6.apply(BuildLoader.scala:92)
0)
0)
2)
2)
"
"""=?gb18030?B?d2l0Z28=?="" <witgo@qq.com>","Thu, 21 Aug 2014 14:04:56 +0800","=?gb18030?B?u9i4tKO6IGlzIEJyYW5jaC0xLjEgU0JUIGJ1aWxk?=
 =?gb18030?B?IGJyb2tlbiBmb3IgeWFybi1hbHBoYSA/?=","""=?gb18030?B?Q2hlc3RlciBDaGVu?="" <chester@alpinenow.com>, ""=?gb18030?B?ZGV2?="" <dev@spark.apache.org>","There's a related discussion 
https://issues.apache.org/jira/browse/SPARK-2815




------------------ ‘≠ º” º˛ ------------------
∑¢º˛»À: ""Chester Chen""<chester@alpinenow.com>; 
∑¢ÀÕ ±º‰: 2014ƒÍ8‘¬21»’(–«∆⁄Àƒ) …œŒÁ7:42
 ’º˛»À: ""dev""<dev@spark.apache.org>; 
÷˜Ã‚: Re: is Branch-1.1 SBT build broken for yarn-alpha ?



Just tried on master branch, and the master branch works fine for yarn-alpha


On Wed, Aug 20, 2014 at 4:39 PM, Chester Chen <chester@alpinenow.com> wrote:

> I just updated today's build and tried branch-1.1 for both yarn and
> yarn-alpha.
>
> For yarn build, this command seem to work fine.
>
> sbt/sbt -Pyarn -Dhadoop.version=2.3.0-cdh5.0.1 projects
>
> for yarn-alpha
>
> sbt/sbt -Pyarn-alpha -Dhadoop.version=2.0.5-alpha projects
>
> I got the following
>
> Any ideas
>
>
> Chester
>
> Å4Ø1 |branch-1.1|$  *sbt/sbt -Pyarn-alpha -Dhadoop.version=2.0.5-alpha
> projects*
>
> Using /Library/Java/JavaVirtualMachines/1.6.0_51-b11-457.jdk/Contents/Home
> as default JAVA_HOME.
>
> Note, this will be overridden by -java-home if it is set.
>
> [info] Loading project definition from
> /Users/chester/projects/spark/project/project
>
> [info] Loading project definition from
> /Users/chester/.sbt/0.13/staging/ec3aa8f39111944cc5f2/sbt-pom-reader/project
>
> [warn] Multiple resolvers having different access mechanism configured
> with same name 'sbt-plugin-releases'. To avoid conflict, Remove duplicate
> project resolvers (`resolvers`) or rename publishing resolver (`publishTo`).
>
> [info] Loading project definition from
> /Users/chester/projects/spark/project
>
> org.apache.maven.model.building.ModelBuildingException: 1 problem was
> encountered while building the effective model for
> org.apacheresolvable parent POM: Could not find artifact
> org.> http://repo.maven.apache.org/maven2 <http://repo.maven.apache.org/maven2>)
> and 'parent.relativePath' points at wrong local POM @ line 20, column 11*
>
>
>  at
> org.apache.maven.model.building.DefaultModelProblemCollector.newModelBuildingException(DefaultModelProblemCollector.java:195)
>
> at
> org.apache.maven.model.building.DefaultModelBuilder.readParentExternally(DefaultModelBuilder.java:841)
>
> at
> org.apache.maven.model.building.DefaultModelBuilder.readParent(DefaultModelBuilder.java:664)
>
> at
> org.apache.maven.model.building.DefaultModelBuilder.build(DefaultModelBuilder.java:310)
>
> at
> org.apache.maven.model.building.DefaultModelBuilder.build(DefaultModelBuilder.java:232)
>
> at
> com.typesafe.sbt.pom.MvnPomResolver.loadEffectivePom(MavenPomResolver.scala:61)
>
> at com.typesafe.sbt.pom.package$.loadEffectivePom(package.scala:41)
>
> at
> com.typesafe.sbt.pom.MavenProjectHelper$.makeProjectTree(MavenProjectHelper.scala:128)
>
> at
> com.typesafe.sbt.pom.MavenProjectHelper$$anonfun$12.apply(MavenProjectHelper.scala:129)
>
> at
> com.typesafe.sbt.pom.MavenProjectHelper$$anonfun$12.apply(MavenProjectHelper.scala:129)
>
> at
> scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
>
> at
> scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
>
> at
> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
>
> at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
>
> at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
>
> at scala.collection.AbstractTraversable.map(Traversable.scala:105)
>
> at
> com.typesafe.sbt.pom.MavenProjectHelper$.makeProjectTree(MavenProjectHelper.scala:129)
>
> at
> com.typesafe.sbt.pom.MavenProjectHelper$$anonfun$12.apply(MavenProjectHelper.scala:129)
>
> at
> com.typesafe.sbt.pom.MavenProjectHelper$$anonfun$12.apply(MavenProjectHelper.scala:129)
>
> at
> scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
>
> at
> scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
>
> at
> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
>
> at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
>
> at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
>
> at scala.collection.AbstractTraversable.map(Traversable.scala:105)
>
> at
> com.typesafe.sbt.pom.MavenProjectHelper$.makeProjectTree(MavenProjectHelper.scala:129)
>
> at
> com.typesafe.sbt.pom.MavenProjectHelper$.makeReactorProject(MavenProjectHelper.scala:49)
>
> at
> com.typesafe.sbt.pom.PomBuild$class.projectDefinitions(PomBuild.scala:28)
>
> at SparkBuild$.projectDefinitions(SparkBuild.scala:165)
>
> at sbt.Load$.sbt$Load$$projectsFromBuild(Load.scala:458)
>
> at sbt.Load$$anonfun$24.apply(Load.scala:415)
>
> at sbt.Load$$anonfun$24.apply(Load.scala:415)
>
> at scala.collection.immutable.Stream.flatMap(Stream.scala:442)
>
> at sbt.Load$.loadUnit(Load.scala:415)
>
> at sbt.Load$$anonfun$15$$anonfun$apply$11.apply(Load.scala:256)
>
> at sbt.Load$$anonfun$15$$anonfun$apply$11.apply(Load.scala:256)
>
> at
> sbt.BuildLoader$$anonfun$componentLoader$1$$anonfun$apply$4$$anonfun$apply$5$$anonfun$apply$6.apply(BuildLoader.scala:93)
>
> at
> sbt.BuildLoader$$anonfun$componentLoader$1$$anonfun$apply$4$$anonfun$apply$5$$anonfun$apply$6.apply(BuildLoader.scala:92)
>
> at sbt.BuildLoader.apply(BuildLoader.scala:143)
>
> at sbt.Load$.loadAll(Load.scala:312)
>
> at sbt.Load$.loadURI(Load.scala:264)
>
> at sbt.Load$.load(Load.scala:260)
>
> at sbt.Load$.load(Load.scala:251)
>
> at sbt.Load$.apply(Load.scala:134)
>
> at sbt.Load$.defaultLoad(Load.scala:37)
>
> at sbt.BuiltinCommands$.doLoadProject(Main.scala:473)
>
> at sbt.BuiltinCommands$$anonfun$loadProjectImpl$2.apply(Main.scala:467)
>
> at sbt.BuiltinCommands$$anonfun$loadProjectImpl$2.apply(Main.scala:467)
>
> at
> sbt.Command$$anonfun$applyEffect$1$$anonfun$apply$2.apply(Command.scala:60)
>
> at
> sbt.Command$$anonfun$applyEffect$1$$anonfun$apply$2.apply(Command.scala:60)
>
> at
> sbt.Command$$anonfun$applyEffect$2$$anonfun$apply$3.apply(Command.scala:62)
>
> at
> sbt.Command$$anonfun$applyEffect$2$$anonfun$apply$3.apply(Command.scala:62)
>
> at sbt.Command$.process(Command.scala:95)
>
> at sbt.MainLoop$$anonfun$1$$anonfun$apply$1.apply(MainLoop.scala:100)
>
> at sbt.MainLoop$$anonfun$1$$anonfun$apply$1.apply(MainLoop.scala:100)
>
> at sbt.State$$anon$1.process(State.scala:179)
>
> at sbt.MainLoop$$anonfun$1.apply(MainLoop.scala:100)
>
> at sbt.MainLoop$$anonfun$1.apply(MainLoop.scala:100)
>
> at sbt.ErrorHandling$.wideConvert(ErrorHandling.scala:18)
>
> at sbt.MainLoop$.next(MainLoop.scala:100)
>
> at sbt.MainLoop$.run(MainLoop.scala:93)
>
> at sbt.MainLoop$$anonfun$runWithNewLog$1.apply(MainLoop.scala:71)
>
> at sbt.MainLoop$$anonfun$runWithNewLog$1.apply(MainLoop.scala:66)
>
> at sbt.Using.apply(Using.scala:25)
>
> at sbt.MainLoop$.runWithNewLog(MainLoop.scala:66)
>
> at sbt.MainLoop$.runAndClearLast(MainLoop.scala:49)
>
> at sbt.MainLoop$.runLoggedLoop(MainLoop.scala:33)
>
> at sbt.MainLoop$.runLogged(MainLoop.scala:25)
>
> at sbt.StandardMain$.runManaged(Main.scala:57)
>
> at sbt.xMain.run(Main.scala:29)
>
> at xsbt.boot.Launch$$anonfun$run$1.apply(Launch.scala:109)
>
> at xsbt.boot.Launch$.withContextLoader(Launch.scala:129)
>
> at xsbt.boot.Launch$.run(Launch.scala:109)
>
> at xsbt.boot.Launch$$anonfun$apply$1.apply(Launch.scala:36)
>
> at xsbt.boot.Launch$.launch(Launch.scala:117)
>
> at xsbt.boot.Launch$.apply(Launch.scala:19)
>
> at xsbt.boot.Boot$.runImpl(Boot.scala:44)
>
> at xsbt.boot.Boot$.main(Boot.scala:20)
>
> at xsbt.boot.Boot.main(Boot.scala)
>
> [error] org.apache.maven.model.building.ModelBuildingException: 1 problem
> was encountered while building the effective model for
> oor] [FATAL] Non-resolvable parent POM: Could not find 0 in central (
> http://repo.maven.apache.org/maven2) and 'parent.relativePath' points at
> wrong local POM @ line 20, column 11
>
> [error] Use 'last' for the full log.
>
> Project loading failed: (r)etry, (q)uit, (l)ast, or (i)gnore? q
>"
Andrew Ash <andrew@andrewash.com>,"Wed, 20 Aug 2014 23:35:29 -0700",Hang on Executor classloader lookup for the remote REPL URL classloader,dev@spark.apache.org,"Hi Spark devs,

I'm seeing a stacktrace where the classloader that reads from the REPL is
hung, and blocking all progress on that executor.  Below is that hung
thread's stacktrace, and also the stacktrace of another hung thread.

I thought maybe there was an issue with the REPL's JVM on the other side,
but didn't see anything useful in that stacktrace either.

Any ideas what I should be looking for?

Thanks!
Andrew


""Executor task launch worker-0"" daemon prio=10 tid=0x00007f780c208000
nid=0x6ae9 runnable [0x00007f78c2eeb000]
   java.lang.Thread.State: RUNNABLE
        at java.net.SocketInputStream.socketRead0(Native Method)
        at java.net.SocketInputStream.read(SocketInputStream.java:152)
        at java.net.SocketInputStream.read(SocketInputStream.java:122)
        at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
        at java.io.BufferedInputStream.read1(BufferedInputStream.java:275)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:334)
        - locked <0x00007f7e13ea9560> (a java.io.BufferedInputStream)
        at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:687)
        at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:633)
        at
sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1323)
        - locked <0x00007f7e13e9eeb0> (a
sun.net.www.protocol.http.HttpURLConnection)
        at java.net.URL.openStream(URL.java:1037)
        at
org.apache.spark.repl.ExecutorClassLoader.findClassLocally(ExecutorClassLoader.scala:86)
        at
org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:63)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
        - locked <0x00007f7fc9018980> (a
org.apache.spark.repl.ExecutorClassLoader)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:270)
        at org.apache.avro.util.ClassUtils.forName(ClassUtils.java:102)
        at org.apache.avro.util.ClassUtils.forName(ClassUtils.java:82)
        at
org.apache.avro.specific.SpecificData.getClass(SpecificData.java:132)
        at
org.apache.avro.specific.SpecificDatumReader.setSchema(SpecificDatumReader.java:69)
        at
org.apache.avro.file.DataFileStream.initialize(DataFileStream.java:126)
        at
org.apache.avro.file.DataFileReader.<init>(DataFileReader.java:97)
        at
org.apache.avro.file.DataFileReader.openReader(DataFileReader.java:59)
        at
org.apache.avro.mapred.AvroRecordReader.<init>(AvroRecordReader.java:41)
        at
org.apache.avro.mapred.AvroInputFormat.getRecordReader(AvroInputFormat.java:71)
        at
org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:193)
        at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:184)
        at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:93)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)


And the other threads are stuck on the Class.forName0() method too:

""Executor task launch worker-4"" daemon prio=10 tid=0x00007f780c20f000
nid=0x6aed waiting for monitor entry [0x00007f78c2ae8000]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:270)
        at org.apache.avro.util.ClassUtils.forName(ClassUtils.java:102)
        at org.apache.avro.util.ClassUtils.forName(ClassUtils.java:79)
        at
org.apache.avro.specific.SpecificData.getClass(SpecificData.java:132)
        at
org.apache.avro.specific.SpecificDatumReader.setSchema(SpecificDatumReader.java:69)
        at
org.apache.avro.file.DataFileStream.initialize(DataFileStream.java:126)
        at
org.apache.avro.file.DataFileReader.<init>(DataFileReader.java:97)
        at
org.apache.avro.file.DataFileReader.openReader(DataFileReader.java:59)
        at
org.apache.avro.mapred.AvroRecordReader.<init>(AvroRecordReader.java:41)
        at
org.apache.avro.mapred.AvroInputFormat.getRecordReader(AvroInputFormat.java:71)
        at
org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:193)
        at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:184)
        at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:93)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)

asdf
"
Mayur Rustagi <mayur.rustagi@gmail.com>,"Thu, 21 Aug 2014 12:34:55 +0530",Re: Akka usage in Spark,Debasish Das <debasish.das83@gmail.com>,"The stream receiver seems to leverage actor receivers
     http://spark.apache.org/docs/0.8.1/streaming-custom-receivers.html
But spark system doesnt lend itself to a messaging kind of a structure..
more of a DAG kind
Just curious are you looking for the actor subsystem to act on messages or
just looking to use them as a local/distributed message bus

Mayur Rustagi
Ph: +1 (760) 203 3257
http://www.sigmoidanalytics.com
@mayur_rustagi <https://twitter.com/mayur_rustagi>




"
Patrick Wendell <pwendell@gmail.com>,"Thu, 21 Aug 2014 01:12:59 -0700",[SNAPSHOT] Snapshot2 of Spark 1.1 has been posted,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi All,

I've packaged and published a snapshot release of Spark 1.1 for testing.
This is very close to RC1 and we are distributing it for testing. Please
test this and report any issues on this thread.

The tag of this release is v1.1.0-snapshot1 (commit e1535ad3):
*https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=e1535ad3c6f7400f2b7915ea91da9c60510557ba
<https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=e1535ad3c6f7400f2b7915ea91da9c60510557ba>*

The release files, including signatures, digests, etc can be found at:
*http://people.apache.org/~pwendell/spark-1.1.0-snapshot2/
<http://people.apache.org/~pwendell/spark-1.1.0-snapshot1/>*

Release artifacts are signed with the following key:
*https://people.apache.org/keys/committer/pwendell.asc
<https://people.apache.org/keys/committer/pwendell.asc>*

The staging repository for this release can be found at:
[NOTE: Apache Sonatype is down preventing us from cutting this]
https://repository.apache.org/content/repositories/orgapachespark-1026/
<https://repository.apache.org/content/repositories/orgapachespark-1024/>


To learn more about Apache Spark, please see
http://spark.apache.org/
"
Patrick Wendell <pwendell@gmail.com>,"Thu, 21 Aug 2014 01:27:02 -0700",Re: [SNAPSHOT] Snapshot2 of Spark 1.1 has been posted,"""dev@spark.apache.org"" <dev@spark.apache.org>","The docs for this release are also available here:

http://people.apache.org/~pwendell/spark-1.1.0-snapshot2-docs/



"
Sean Owen <sowen@cloudera.com>,"Thu, 21 Aug 2014 11:17:48 +0100",Re: is Branch-1.1 SBT build broken for yarn-alpha ?,Chester Chen <chester@alpinenow.com>,"Maven is just telling you that there is no version 1.1.0 of
yarn-parent, and indeed, it has not been released. To build the branch
you would need to ""mvn install"" to compile and make available local
copies of artifacts along the way. (You may have these for
1.1.0-SNAPSHOT locally already). Use Maven, not SBT, for building
releases.


---------------------------------------------------------------------


"
Maisnam Ns <maisnam.ns@gmail.com>,"Thu, 21 Aug 2014 15:55:14 +0530",Spark Contribution,dev@spark.apache.org,"Hi,

Can someone help me with some links on how to contribute for Spark

Regards
mns
"
Aniket Bhatnagar <aniket.bhatnagar@gmail.com>,"Thu, 21 Aug 2014 16:54:53 +0530",Kinesis streaming integration in upcoming 1.1,dev@spark.apache.org,"Hi everyone

I started looking at Kinesis integration and it looks promising.  However,
I feel like it can be improved. Here are my thoughts:

1. It assumes that AWS credentials are provided
by DefaultAWSCredentialsProviderChain and there is no way to change the
behavior. I would have liked to have an ability to provide a different
AWSCredentialsProvider.

2. I feel like modules in extras need to be independent from Spark build
and should perhaps be in separate repository/repositories. I had to
download most recent checkout of Spark and slap kinesis-asl into Spark
1.0.2 to create a custom spark-streaming-kinesis-asl_2.10-1.0.2.jar that I
can use in my Spark jobs. Ideally, people would want extra modules to be
cross built against different versions of Spark. Having independent
repositories can enable us to deliver build for extras packages faster than
Spark releases and they would be readily available to earlier versions of
Spark. This can free up Spark developers to focus on enhancements in the
core framework instead of managing spark-* integration pull requests.

3. Maybe it's just me, but I could have liked a Context like API for
creating Kinesis streams instead of using KinesisUtils. It makes it a
little more consistent with rest of the Spark API. We could have have
a KinesisContext which goes like this:
class KinesisStreamingContext(@transient ssc: StreamingContext,
endpointUrl: String, defaultCredentialsProvider: AWSCredentialsProvider) {

  def createStream(streamName: String,
      checkpointInterval: Duration,
      initialPositionInStream: InitialPositionInStream,
      storageLevel: StorageLevel,
      credentialsProvider: AWSCredentialsProvider =
defaultCredentialsProvider) {...}
}

4. The example KinesisWordCountASL creates numShards receiver instances
which makes sense. Maybe the API should provide ability to provide
parallelism and default to numShards?

I can submit pull requests for some of the above items, provided the
community agrees and nobody else is working on it.

Thanks,
Aniket
"
chutium <teng.qiu@gmail.com>,"Thu, 21 Aug 2014 04:35:01 -0700 (PDT)",Re: Spark SQL Query and join different data sources.,dev@spark.incubator.apache.org,"as far as i know, HQL queries try to find the schema info of all the tables
in this query from hive metastore, so it is not possible to join tables from
sqlContext using hiveContext.hql

but this should work:

hiveContext.hql(""select ..."").regAsTable(""a"")
sqlContext.jsonFile(""xxx"").regAsTable(""b"")

then

sqlContext.sql( a join b )


i created a ticket SPARK-2710 to add ResultSets from JDBC connection as a
new data source, but no predicate push down yet, also, it is not available
for HQL

so, if you are looking for something that can query different data sources
with full SQL92 syntax, facebook presto is still the only choice, they have
some kind of JDBC connector in deveopment, and there are some unofficial
implementations...

but i am looking forward to seeing the progress of Spark SQL, after
SPARK-2179 SQLContext can handle any
kind of structured data with a sequence of DataTypes as schema, although
turning the data into Rows is still a little bit tricky...



--

---------------------------------------------------------------------


"
Mridul Muralidharan <mridul@gmail.com>,"Thu, 21 Aug 2014 19:15:02 +0530",Re: is Branch-1.1 SBT build broken for yarn-alpha ?,"Chester Chen <chester@alpinenow.com>, Patrick Wendell <pwendell@gmail.com>","Weird that Patrick did not face this while creating the RC.
Essentially the yarn alpha pom.xml has not been updated properly in
the 1.1 branch.

Just change version to '1.1.1-SNAPSHOT' for yarn/alpha/pom.xml (to
make it same as any other pom).


Regards,
Mridul


:
lpha
e
ect
th
`).
ect
)
dingException(DefaultModelProblemCollector.java:195)
DefaultModelBuilder.java:841)
elBuilder.java:664)
lder.java:310)
lder.java:232)
la:61)
er.scala:128)
per.scala:129)
per.scala:129)
la:244)
la:244)
a:59)
er.scala:129)
per.scala:129)
per.scala:129)
la:244)
la:244)
a:59)
er.scala:129)
elper.scala:49)
28)
y$5$$anonfun$apply$6.apply(BuildLoader.scala:93)
y$5$$anonfun$apply$6.apply(BuildLoader.scala:92)
0)
0)
2)
2)

---------------------------------------------------------------------


"
"""Chester @work"" <chester@alpinenow.com>","Thu, 21 Aug 2014 08:03:51 -0700",Re: is Branch-1.1 SBT build broken for yarn-alpha ?,Mridul Muralidharan <mridul@gmail.com>,"Do we have Jenkins tests these ? Should be pretty easy to setup just to test basic build

Sent from my iPhone


te:
lpha
e
ect
th
`).
ect
)

dingException(DefaultModelProblemCollector.java:195)
DefaultModelBuilder.java:841)
elBuilder.java:664)
lder.java:310)
lder.java:232)
la:61)
er.scala:128)
per.scala:129)
per.scala:129)
la:244)
la:244)
a:59)
er.scala:129)
per.scala:129)
per.scala:129)
la:244)
la:244)
a:59)
er.scala:129)
elper.scala:49)
28)
y$5$$anonfun$apply$6.apply(BuildLoader.scala:93)
y$5$$anonfun$apply$6.apply(BuildLoader.scala:92)
0)
0)
2)
2)


---------------------------------------------------------------------


"
"""Yan Zhou.sc"" <Yan.Zhou.sc@huawei.com>","Thu, 21 Aug 2014 17:47:46 +0000",RE: Spark SQL Query and join different data sources.,"chutium <teng.qiu@gmail.com>,
        ""dev@spark.incubator.apache.org""
	<dev@spark.incubator.apache.org>","I doubt it will work as expected.

Note that hiveContext.hql(""select ..."").regAsTable(""a"") will create a SchemaRDD before register the SchemaRDD with the (Hive) catalog;
While sqlContext.jsonFile(""xxx"").regAsTable(""b"") will create a SchemaRDD before register the SchemaRDD with the SparkSQL catalog(SimpleCatalog).
The logic plans of the two SchemaRDDs are of the same type; but the physical plans are, and should be, different.
The issue is that the transformation of the logical plans to physical plans are controlled by the ""strategies"" of ""contexts""; namely the sqlContext
transforms a logical plan to a physical plan suitable for SchemaRDD's execution from an in-memory data source, while HiveContext 
transforms a logical plan to a physical plan suitable for SchemaRDD's execution from a Hive data source. So

sqlContext.sql( a join b ) will generate a physical plan for the in-memory data source for both a and b; and
hiveContext.sql(a join b) will generate a physical plan for Hive data source for both a and b.

What's really needed is a storage transparency from the semantic layer if SparkSQL wants to go the data federation route.


If one could manage to create a SchemaRDD on Hive data through just the SQLhinted by the SparkSQL web page https://spark.apache.org/sql/ in the following code snippet:

sqlCtx.jsonFile(""s3n://..."")
  .registerAsTable(""json"")
 schema_rdd = sqlCtx.sql(""""""
   SELECT * 
   FROM hiveTable
   JOIN json ..."""""")

he/she might be able to perform the join of data sets of different types. I just have not tried.


In terms of SQL-92 conforming, Presto might be better than HiveQL; while in terms of federation, Hive is actually very good at it.




 in this query from hive metastore, so it is not possible to join tables from sqlContext using hiveContext.hql

but this should work:

hiveContext.hql(""select ..."").regAsTable(""a"")
sqlContext.jsonFile(""xxx"").regAsTable(""b"")

then

sqlContext.sql( a join b )


i created a ticket SPARK-2710 to add ResultSets from JDBC connection as a new data source, but no predicate push down yet, also, it is not available for HQL

so, if you are looking for something that can query different data sources with full SQL92 syntax, facebook presto is still the only choice, they have some kind of JDBC connector in deveopment, and there are some unofficial implementations...

but i am looking forward to seeing the progress of Spark SQL, after
SPARK-2179 SQLContext can handle any
kind of structured data with a sequence of DataTypes as schema, although turning the data into Rows is still a little bit tricky...



--
3.nabble.com/Spark-SQL-Query-and-join-different-data-sources-tp7914p7937.html
om.

---------------------------------------------------------------------
mands, e-mail: dev-help@spark.apache.org


---------------------------------------------------------------------


"
Chester Chen <chester@alpinenow.com>,"Thu, 21 Aug 2014 11:36:07 -0700",Re: is Branch-1.1 SBT build broken for yarn-alpha ?,Mridul Muralidharan <mridul@gmail.com>,"Mridul,
     Thanks for the suggestion.

     I just updated the build today and changed the yarn/alpha/pom.xml to

   <version>1.1.1-SNAPSHOT</version>

then the command worked.

I will create a JIRA and PR for it.


Chester





5-alpha
ect
dingException(DefaultModelProblemCollector.java:195)
DefaultModelBuilder.java:841)
elBuilder.java:664)
lder.java:310)
lder.java:232)
la:61)
er.scala:128)
per.scala:129)
per.scala:129)
la:244)
la:244)
a:59)
4)
er.scala:129)
per.scala:129)
per.scala:129)
la:244)
la:244)
a:59)
4)
er.scala:129)
elper.scala:49)
y$5$$anonfun$apply$6.apply(BuildLoader.scala:93)
y$5$$anonfun$apply$6.apply(BuildLoader.scala:92)
)
)
0)
0)
2)
2)
"
Nishkam Ravi <nravi@cloudera.com>,"Thu, 21 Aug 2014 11:53:03 -0700",Re: Lost executor on YARN ALS iterations,Sandy Ryza <sandy.ryza@cloudera.com>,"Can someone from Databricks test and commit this PR? This is not a complete
solution, but would provide some relief.
https://github.com/apache/spark/pull/1391

Thanks,
Nishkam



"
Henry Saputra <henry.saputra@gmail.com>,"Thu, 21 Aug 2014 13:36:18 -0700",Re: Spark Contribution,Maisnam Ns <maisnam.ns@gmail.com>,"The Apache Spark wiki on how to contribute should be great place to
start: https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark

- Henry


---------------------------------------------------------------------


"
npanj <nitinpanj@gmail.com>,"Thu, 21 Aug 2014 14:14:52 -0700 (PDT)",PARSING_ERROR from kryo,dev@spark.incubator.apache.org,"Hi All,

I am getting PARSING_ERROR while running my job on the code checked out up
to commit# db56f2df1b8027171da1b8d2571d1f2ef1e103b6. I am running this job
on EC2.

Any idea if there is something wrong with my config?

Here is my config: 
--
    .set(""spark.executor.extraJavaOptions"", ""-XX:+UseCompressedOops
-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps"")
      .set(""spark.storage.memoryFraction"", ""0.2"")
      .set(""spark.serializer"", ""org.apache.spark.serializer.KryoSerializer"")
      .set(""spark.kryo.registrator"",
""org.apache.spark.graphx.GraphKryoRegistrator"")
      .set(""spark.akka.frameSize"", ""20"")
      .set(""spark.akka.timeout"", ""300"")
      .set(""spark.shuffle.memoryFraction"", ""0.5"")
      .set(""spark.core.connection.ack.wait.timeout"", ""1800"")
--



--
Job aborted due to stage failure: Task 947 in stage 11.0 failed 4 times,
most recent failure: Lost task 947.3 in stage 11.0 (TID 12750,
ip-10-167-149-118.ec2.internal): com.esotericsoftware.kryo.KryoException:
java.io.IOException: failed to uncompress the chunk: PARSING_ERROR(2)
Serialization trace:
vids (org.apache.spark.graphx.impl.VertexAttributeBlock)
        com.esotericsoftware.kryo.io.Input.fill(Input.java:142)
        com.esotericsoftware.kryo.io.Input.require(Input.java:169)
        com.esotericsoftware.kryo.io.Input.readLong_slow(Input.java:719)
        com.esotericsoftware.kryo.io.Input.readLong(Input.java:665)
       
com.esotericsoftware.kryo.serializers.DefaultArraySerializers$LongArraySerializer.read(DefaultArraySerializers.java:127)
       
com.esotericsoftware.kryo.serializers.DefaultArraySerializers$LongArraySerializer.read(DefaultArraySerializers.java:107)
        com.esotericsoftware.kryo.Kryo.readObjectOrNull(Kryo.java:699)
       
com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.read(FieldSerializer.java:611)
       
com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:221)
        com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)
        com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:43)
        com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:34)
        com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)
       
org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:119)
       
org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:129)
        org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
       
org.apache.spark.storage.BlockManager$LazyProxyIterator$1.hasNext(BlockManager.scala:1038)
        scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
       
org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
       
org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
        scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
        scala.collection.Iterator$class.foreach(Iterator.scala:727)
        scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
       
org.apache.spark.graphx.impl.VertexPartitionBaseOps.innerJoinKeepLeft(VertexPartitionBaseOps.scala:192)
       
org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:78)
       
org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:75)
       
org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:73)
        scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
        scala.collection.Iterator$class.foreach(Iterator.scala:727)
        scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
       
org.apache.spark.shuffle.hash.HashShuffleWriter.write(HashShuffleWriter.scala:57)
       
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:147)
       
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
        org.apache.spark.scheduler.Task.run(Task.scala:51)
       
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:189)
       
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:745)
--



--

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 21 Aug 2014 19:06:07 -0400",Re: Spark Contribution,Henry Saputra <henry.saputra@gmail.com>,"We should add this link to the readme on GitHub btw.

2014ÎÖÑ 8Ïõî 21Ïùº Î™©ÏöîÏùº, Henry Saputra<henry.saputra@gmail.com>ÎãòÏù¥ ÏûëÏÑ±Ìïú Î©îÏãúÏßÄ:

"
Evan Chan <velvia.github@gmail.com>,"Thu, 21 Aug 2014 16:43:24 -0700",Spark-JobServer moving to a new location,"dev@spark.apache.org, ""user@spark.apache.org"" <user@spark.apache.org>","Dear community,

Wow, I remember when we first open sourced the job server, at the
first Spark Summit in December.  Since then, more and more of you have
started using it and contributing to it.   It is awesome to see!

If you are not familiar with the spark job server, it is a REST API
for managing your Spark jobs and job history and status.

In order to make sure the project can continue to move forward
independently, new features developed and contributions merged, we are
moving the project to a new github organization.  The new location is:

https://github.com/spark-jobserver/spark-jobserver


The git commit history is still there, but unfortunately the pull
requests don't migrate over.   I'll be contacting each of you with
open PRs to move them over to the new location.

Happy Hacking!

Evan (@velvia)
Kelvin (@kelvinchu)
Daniel (@dan-null)

---------------------------------------------------------------------


"
Debasish Das <debasish.das83@gmail.com>,"Thu, 21 Aug 2014 16:47:07 -0700",Re: Lost executor on YARN ALS iterations,Sandy Ryza <sandy.ryza@cloudera.com>,"Sandy,

I put spark.yarn.executor.memoryOverhead 1024 on spark-defaults.conf but I
don't see environment variable on spark properties on the webui->environment

Does it need to go in spark-env.sh ?

Thanks.
Deb



"
jerryye <jerryye@gmail.com>,"Thu, 21 Aug 2014 17:13:50 -0700 (PDT)",saveAsTextFile makes no progress without caching RDD,dev@spark.incubator.apache.org,"Hi, 
Cross-posting this from users list.

I'm running on branch-1.1 and trying to do a simple transformation to a
relatively small dataset of 64GB and saveAsTextFile essentially hangs and
tasks are stuck in running mode with the following code: 

// Stalls with tasks running for over an hour with no tasks finishing.
Smallest partition is 10MB 
val data = sc.textFile(""s3n://input"") 
val reformatted = data.map(t =>
t.replace(""Test("","""").replace("")"","""").replaceAll("","",""\t"")) 
reformatted.saveAsTextFile(""s3n://transformed"") 

// This runs but stalls doing GC after filling up 150% of 650GB of memory 
val data = sc.textFile(""s3n://input"") 
val reformatted = data.map(t =>
t.replace(""Test("","""").replace("")"","""").replaceAll("","",""\t"")).cache 
reformatted.saveAsTextFile(""s3n://transformed"") 

Any idea if this is a parameter issue and there is something I should try
out? 

Thanks! 

- jerry 



--

---------------------------------------------------------------------


"
jerryye <jerryye@gmail.com>,"Thu, 21 Aug 2014 17:56:14 -0700 (PDT)","Re: saveAsTextFile to s3 on spark does not work, just hangs",dev@spark.incubator.apache.org,"bump.

I'm seeing the same issue with branch-1.1. Caching the RDD before running
saveAsTextFile gets things running but the job stalls 2/3 of the way by
using too much memory.



--

---------------------------------------------------------------------


"
Niranda Perera <niranda@wso2.com>,"Thu, 21 Aug 2014 17:13:05 +0530",Storage Handlers in Spark SQL,dev@spark.apache.org,"Hi,

I have been playing around with Spark for the past few days, and evaluating
the possibility of migrating into Spark (Spark SQL) from Hive/Hadoop.

I am working on the WSO2 Business Activity Monitor (WSO2 BAM,
https://docs.wso2.com/display/BAM241/WSO2+Business+Activity+Monitor+Documentation
) which has currently employed Hive. We are considering Spark as a
successor for Hive, given it's performance enhancement.

We have currently employed several custom storage-handlers in Hive.
Example:
WSO2 JDBC and Cassandra storage handlers:
https://docs.wso2.com/display/BAM241/JDBC+Storage+Handler+for+Hive
https://docs.wso2.com/display/BAM241/Creating+Hive+Queries+to+Analyze+Data#CreatingHiveQueriestoAnalyzeData-cas

I would like to know where Spark SQL can work with these storage
handlers (while using HiveContext may be) ?

Best regards
-- 
*Niranda Perera*
Software Engineer, WSO2 Inc.
Mobile: +94-71-554-8430
Twitter: @n1r44 <https://twitter.com/N1R44>
"
alexliu68 <alex_liu68@yahoo.com>,"Thu, 21 Aug 2014 14:54:37 -0700 (PDT)",RE: Spark SQL Query and join different data sources.,dev@spark.incubator.apache.org,"Presto is so far good at joining different sources/databases.

I tried a simple join query in Spark SQL, it fails as the followings errors

val a = cql(""select test.a  from test JOIN test1 on test.a = test1.a"")
a: org.apache.spark.sql.SchemaRDD = 
SchemaRDD[0] at RDD at SchemaRDD.scala:104
== Query Plan ==
Project [a#7]
 Filter (a#7 = a#21)
  CartesianProduct 

org.apache.spark.SparkException: Job aborted due to stage failure: Task
0.0:0 failed 4 times, most recent failure: Exception failure in TID 3 on
host 127.0.0.1:
org.apache.spark.sql.catalyst.errors.package$TreeNodeException: No function
to evaluate expression. type: AttributeReference, tree: a#7
       
org.apache.spark.sql.catalyst.expressions.AttributeReference.eval(namedExpressions.scala:158)
       
org.apache.spark.sql.catalyst.expressions.EqualTo.eval(predicates.scala:146)
       
org.apache.spark.sql.execution.Filter$$anonfun$2$$anonfun$apply$1.apply(basicOperators.scala:54)
       
org.apache.spark.sql.execution.Filter$$anonfun$2$$anonfun$apply$1.apply(basicOperators.scala:54)
        scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:390)
        scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        scala.collection.Iterator$class.foreach(Iterator.scala:727)
        scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
       
scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
       
scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
       
scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
        scala.collection.AbstractIterator.to(Iterator.scala:1157)
       
        scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
       
        scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
        org.apache.spark.rdd.RDD$$anonfun$16.apply(RDD.scala:731)
        org.apache.spark.rdd.RDD$$anonfun$16.apply(RDD.scala:731)
       
org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1083)
       
org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1083)
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
        org.apache.spark.scheduler.Task.run(Task.scala:51)
       
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)
       
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:745)

It looks like Spark SQL has long way to go to be compatible with SQL




--

---------------------------------------------------------------------


"
Evan Chan <velvia.github@gmail.com>,"Thu, 21 Aug 2014 23:43:49 -0700",Too late to contribute for 1.1.0?,dev@spark.apache.org,"I'm hoping to get in some doc enhancements and small bug fixes for Spark SQL.

Also possibly a small new API to list the tables in sqlContext.

Oh, and to get the doc page I had talked about before, a list of
community Spark projects.

thanks,
Evan

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Thu, 21 Aug 2014 23:45:53 -0700",Re: Too late to contribute for 1.1.0?,Evan Chan <velvia.github@gmail.com>,"I believe docs changes can go in anytime (because we can just publish new
versions of docs).

Critical bug fixes can still go in too.



"
Rajendran Appavu <apprajen@in.ibm.com>,"Fri, 22 Aug 2014 14:51:37 +0530",Adding support for a new object store,dev@spark.apache.org,"                                                                                                                           
 I am new to Spark source code and looking to see if i can add push-down support of spark filters to the storage (in my    
 case an object store). I am willing to consider how this can be generically done for any store that we might want to      
 integrate with spark. I am looking to know the areas that I should look into to provide support for a new data store in   
 this context. Following below are some of the questions I have to start with:                                             
                                                                                                                           
 1. Do we need to create a new RDD class for the new store that we want to support? From Spark Context, we create an RDD   
 and the operations on data including the filter are performed through the RDD methods.                                    
                                                                                                                           
 2. When we specify the code for filter task in the RDD.filter() method, how does it get communicated to the Executor on   
 the data node? Does the Executor need to compile this code on the fly and execute it? or how does it work? ( I have       
 looked at the code for sometime, but not yet got to figuring this out, so i am looking for some pointers that can help me 
 come a little up-to-speed in this part of the code)                                                                       
                                                                                                                           
 3. How long the Executor holds the memory? and how does it decide when to release the memory/cache?                       
                                                                                                                           
 Thank you in advance.                                                                                                     
                                                                                                                           
                                                                                                                           



Regards,
Rajendran.


---------------------------------------------------------------------


"
chutium <teng.qiu@gmail.com>,"Fri, 22 Aug 2014 06:07:53 -0700 (PDT)",RE: Spark SQL Query and join different data sources.,dev@spark.incubator.apache.org,"oops, thanks Yan, you are right, i got

scala> sqlContext.sql(""select * from a join b"").take(10)
java.lang.RuntimeException: Table Not Found: b
        at scala.sys.package$.error(package.scala:27)
        at
org.apache.spark.sql.catalyst.analysis.SimpleCatalog$$anonfun$1.apply(Catalog.scala:90)
        at
org.apache.spark.sql.catalyst.analysis.SimpleCatalog$$anonfun$1.apply(Catalog.scala:90)
        at scala.Option.getOrElse(Option.scala:120)
        at
org.apache.spark.sql.catalyst.analysis.SimpleCatalog.lookupRelation(Catalog.scala:90)

and with hql

scala> hiveContext.hql(""select * from a join b"").take(10)
warning: there were 1 deprecation warning(s); re-run with -deprecation for
details
14/08/22 14:48:45 INFO parse.ParseDriver: Parsing command: select * from a
join b
14/08/22 14:48:45 INFO parse.ParseDriver: Parse Completed
14/08/22 14:48:45 ERROR metadata.Hive:
NoSuchObjectException(message:default.a table not found)
        at
org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_table_result$get_table_resultStandardScheme.read(ThriftHiveMetastore.java:27129)
        at
org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_table_result$get_table_resultStandardScheme.read(ThriftHiveMetastore.java:27097)
        at
org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_table_result.read(ThriftHiveMetastore.java:27028)
        at
org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)
        at
org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_table(ThriftHiveMetastore.java:936)
        at
org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_table(ThriftHiveMetastore.java:922)
        at
org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getTable(HiveMetaStoreClient.java:854)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:601)
        at
org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:89)
        at com.sun.proxy.$Proxy17.getTable(Unknown Source)
        at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:950)
        at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:924)
        at
org.apache.spark.sql.hive.HiveMetastoreCatalog.lookupRelation(HiveMetastoreCatalog.scala:59)


so sqlContext is looking up table from
org.apache.spark.sql.catalyst.analysis.SimpleCatalog, Catalog.scala
hiveContext looking up from org.apache.spark.sql.hive.HiveMetastoreCatalog,
HiveMetastoreCatalog.scala

maybe we can do something in sqlContext to register a hive table as
Spark-SQL-Table, need to read column info, partition info, location, SerDe,
Input/OutputFormat and maybe StorageHandler also, from the hive metastore...




--

---------------------------------------------------------------------


"
pnepywoda <pnepywoda@palantir.com>,"Fri, 22 Aug 2014 09:50:20 -0700 (PDT)",take() reads every partition if the first one is empty,dev@spark.incubator.apache.org,"https://github.com/apache/spark/commit/42571d30d0d518e69eecf468075e4c5a823a2ae8#diff-1d55e54678eff2076263f2fe36150c17R771
the logic for take() reads ALL partitions if the first one (or first k) are
empty. This has actually lead to OOMs when we had many partitions
(thousands) and unfortunately the first one was empty.

Wouldn't a better implementation strategy be

numPartsToTry = partsScanned * 2

instead of

numPartsToTry = totalParts - 1

(this doubling is similar to most memory allocation strategies)

Thanks!
- Paul



--

---------------------------------------------------------------------


"
Andrew Ash <andrew@andrewash.com>,"Fri, 22 Aug 2014 12:36:16 -0700",Re: take() reads every partition if the first one is empty,pnepywoda <pnepywoda@palantir.com>,"Hi Paul,

I agree that jumping straight from reading N rows from 1 partition to N
rows from ALL partitions is pretty aggressive.  The exponential growth
strategy of doubling the partition count every time seems better -- 1, 2,
4, 8, 16, ... will be much more likely to prevent OOMs than the 1 -> ALL
strategy.

Andrew



"
pnepywoda <pnepywoda@palantir.com>,"Fri, 22 Aug 2014 12:52:43 -0700 (PDT)",Re: take() reads every partition if the first one is empty,dev@spark.incubator.apache.org,"What's the process at this point? Does someone make a bug? Should I make a
bug? (do I even have permission to?)



--

---------------------------------------------------------------------


"
Andrew Ash <andrew@andrewash.com>,"Fri, 22 Aug 2014 13:06:36 -0700",Re: take() reads every partition if the first one is empty,pnepywoda <pnepywoda@palantir.com>,"Yep, anyone can create a bug at https://issues.apache.org/jira/browse/SPARK

Then if you make a pull request on GitHub and have the bug number in the
header like ""[SPARK-1234] Make take() less OOM-prone"", then the PR gets
linked to the Jira ticket.  I think that's the best way to get feedback on
a fix.



"
Jeffrey Picard <jpicard@placeiq.com>,"Fri, 22 Aug 2014 16:14:26 -0400",Graphx GraphLoader Coalesce Shuffle,dev@spark.apache.org,"Hey all,

Iíve often found that my spark programs run much more stable with a higher number of partitions, and a lot of the graphs I deal with will have a few hundred large part files. I was wondering if having a parameter in GraphLoader, defaulting to false, to set the shuffle parameter in coalesce is something that might be added to graphx, or if there was a good reason for not including it? Iíve been using this patch myself for a couple weeks.

óJeff

diff --git a/graphx/src/main/scala/org/apache/spark/graphx/GraphLoader.scala b/graphx/src/main/scala/org/apache/spark/graphx/GraphLoader.scala
index f4c7936..b2f9e9c 100644
--- a/graphx/src/main/scala/org/apache/spark/graphx/GraphLoader.scala
+++ b/graphx/src/main/scala/org/apache/spark/graphx/GraphLoader.scala
@@ -58,13 +58,14 @@ object GraphLoader extends Logging {
       canonicalOrientation: Boolean = false,
       minEdgePartitions: Int = 1,
       edgeStorageLevel: StorageLevel = StorageLevel.MEMORY_ONLY,
-      vertexStorageLevel: StorageLevel = StorageLevel.MEMORY_ONLY)
+      vertexStorageLevel: StorageLevel = StorageLevel.MEMORY_ONLY,
+      shuffle: Boolean = false)
     : Graph[Int, Int] =
   {
     val startTime = System.currentTimeMillis

     // Parse the edge data table directly into edge partitions
-    val lines = sc.textFile(path, minEdgePartitions).coalesce(minEdgePartitions)
+    val lines = sc.textFile(path, minEdgePartitions).coalesce(minEdgePartitions, shuffle)
     val edges = lines.mapPartitionsWithIndex { (pid, iter) =>
       val builder = new EdgePartitionBuilder[Int, Int]
       iter.foreach { line =>
"
Ted Yu <yuzhihong@gmail.com>,"Fri, 22 Aug 2014 13:55:26 -0700","reference to dstream in package org.apache.spark.streaming which is
 not available","""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi,
Using the following command on (refreshed) master branch:
mvn clean package -DskipTests

I got:

constituent[36]: file:/homes/hortonzy/apache-maven-3.1.1/conf/logging/
---------------------------------------------------
java.lang.reflect.InvocationTargetException
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at
org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
at
org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
at
org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
Caused by: scala.reflect.internal.Types$TypeError: bad symbolic reference.
A signature in TestSuiteBase.class refers to term dstream
in package org.apache.spark.streaming which is not available.
It may be completely missing from the current classpath, or the version on
the classpath might be incompatible with the version used when compiling
TestSuiteBase.class.
at
scala.reflect.internal.pickling.UnPickler$Scan.toTypeError(UnPickler.scala:847)
at
scala.reflect.internal.pickling.UnPickler$Scan$LazyTypeRef.complete(UnPickler.scala:854)
at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1231)
at
scala.reflect.internal.Types$TypeMap$$anonfun$noChangeToSymbols$1.apply(Types.scala:4280)
at
scala.reflect.internal.Types$TypeMap$$anonfun$noChangeToSymbols$1.apply(Types.scala:4280)
at
scala.collection.LinearSeqOptimized$class.forall(LinearSeqOptimized.scala:70)
at scala.collection.immutable.List.forall(List.scala:84)
at scala.reflect.internal.Types$TypeMap.noChangeToSymbols(Types.scala:4280)
at scala.reflect.internal.Types$TypeMap.mapOver(Types.scala:4293)
at scala.reflect.internal.Types$TypeMap.mapOver(Types.scala:4196)
at scala.reflect.internal.Types$AsSeenFromMap.apply(Types.scala:4638)
at scala.reflect.internal.Types$TypeMap.mapOver(Types.scala:4202)
at scala.reflect.internal.Types$AsSeenFromMap.apply(Types.scala:4638)
at scala.reflect.internal.Types$Type.asSeenFrom(Types.scala:754)
at scala.reflect.internal.Types$Type.memberInfo(Types.scala:773)
at xsbt.ExtractAPI.defDef(ExtractAPI.scala:224)
at xsbt.ExtractAPI.xsbt$ExtractAPI$$definition(ExtractAPI.scala:315)
at
xsbt.ExtractAPI$$anonfun$xsbt$ExtractAPI$$processDefinitions$1.apply(ExtractAPI.scala:296)
at
xsbt.ExtractAPI$$anonfun$xsbt$ExtractAPI$$processDefinitions$1.apply(ExtractAPI.scala:296)
at
scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
at
scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
at
scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251)
at scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:108)
at xsbt.ExtractAPI.xsbt$ExtractAPI$$processDefinitions(ExtractAPI.scala:296)
at xsbt.ExtractAPI$$anonfun$mkStructure$4.apply(ExtractAPI.scala:293)
at xsbt.ExtractAPI$$anonfun$mkStructure$4.apply(ExtractAPI.scala:293)
at xsbt.Message$$anon$1.apply(Message.scala:8)
at xsbti.SafeLazy$$anonfun$apply$1.apply(SafeLazy.scala:8)
at xsbti.SafeLazy$Impl._t$lzycompute(SafeLazy.scala:20)
at xsbti.SafeLazy$Impl._t(SafeLazy.scala:18)
at xsbti.SafeLazy$Impl.get(SafeLazy.scala:24)
at xsbt.ExtractAPI$$anonfun$forceStructures$1.apply(ExtractAPI.scala:138)
at xsbt.ExtractAPI$$anonfun$forceStructures$1.apply(ExtractAPI.scala:138)
at scala.collection.immutable.List.foreach(List.scala:318)
at xsbt.ExtractAPI.forceStructures(ExtractAPI.scala:138)
at xsbt.ExtractAPI.forceStructures(ExtractAPI.scala:139)
at xsbt.API$ApiPhase.processScalaUnit(API.scala:54)
at xsbt.API$ApiPhase.processUnit(API.scala:38)
at xsbt.API$ApiPhase$$anonfun$run$1.apply(API.scala:34)
at xsbt.API$ApiPhase$$anonfun$run$1.apply(API.scala:34)
at scala.collection.Iterator$class.foreach(Iterator.scala:727)
at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
at xsbt.API$ApiPhase.run(API.scala:34)
at scala.tools.nsc.Global$Run.compileUnitsInternal(Global.scala:1583)
at scala.tools.nsc.Global$Run.compileUnits(Global.scala:1557)
at scala.tools.nsc.Global$Run.compileSources(Global.scala:1553)
at scala.tools.nsc.Global$Run.compile(Global.scala:1662)
at xsbt.CachedCompiler0.run(CompilerInterface.scala:123)
at xsbt.CachedCompiler0.run(CompilerInterface.scala:99)
at xsbt.CompilerInterface.run(CompilerInterface.scala:27)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at sbt.compiler.AnalyzingCompiler.call(AnalyzingCompiler.scala:102)
at sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala:48)
at sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala:41)
at
sbt.compiler.AggressiveCompile$$anonfun$3$$anonfun$compileScala$1$1.apply$mcV$sp(AggressiveCompile.scala:99)
at
sbt.compiler.AggressiveCompile$$anonfun$3$$anonfun$compileScala$1$1.apply(AggressiveCompile.scala:99)
at
sbt.compiler.AggressiveCompile$$anonfun$3$$anonfun$compileScala$1$1.apply(AggressiveCompile.scala:99)
at
sbt.compiler.AggressiveCompile.sbt$compiler$AggressiveCompile$$timed(AggressiveCompile.scala:166)
at
sbt.compiler.AggressiveCompile$$anonfun$3.compileScala$1(AggressiveCompile.scala:98)
at
sbt.compiler.AggressiveCompile$$anonfun$3.apply(AggressiveCompile.scala:143)
at
sbt.compiler.AggressiveCompile$$anonfun$3.apply(AggressiveCompile.scala:87)
at sbt.inc.IncrementalCompile$$anonfun$doCompile$1.apply(Compile.scala:39)
at sbt.inc.IncrementalCompile$$anonfun$doCompile$1.apply(Compile.scala:37)
at sbt.inc.IncrementalCommon.cycle(Incremental.scala:99)
at sbt.inc.Incremental$$anonfun$1.apply(Incremental.scala:38)
at sbt.inc.Incremental$$anonfun$1.apply(Incremental.scala:37)
at sbt.inc.Incremental$.manageClassfiles(Incremental.scala:65)
at sbt.inc.Incremental$.compile(Incremental.scala:37)
at sbt.inc.IncrementalCompile$.apply(Compile.scala:27)
at sbt.compiler.AggressiveCompile.compile2(AggressiveCompile.scala:157)
at sbt.compiler.AggressiveCompile.compile1(AggressiveCompile.scala:71)
at com.typesafe.zinc.Compiler.compile(Compiler.scala:184)
at com.typesafe.zinc.Compiler.compile(Compiler.scala:164)
at sbt_inc.SbtIncrementalCompiler.compile(SbtIncrementalCompiler.java:92)
at
scala_maven.ScalaCompilerSupport.incrementalCompile(ScalaCompilerSupport.java:303)
at scala_maven.ScalaCompilerSupport.compile(ScalaCompilerSupport.java:119)
at scala_maven.ScalaCompilerSupport.doExecute(ScalaCompilerSupport.java:99)
at scala_maven.ScalaMojoSupport.execute(ScalaMojoSupport.java:482)
at scala_maven.ScalaTestCompileMojo.execute(ScalaTestCompileMojo.java:48)
at
org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:106)
at
org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
at
org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
at
org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
at
org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:84)
at
org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:59)
at
org.apache.maven.lifecycle.internal.LifecycleStarter.singleThreadedBuild(LifecycleStarter.java:183)
at
org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:161)
at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:317)
at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:152)
at org.apache.maven.cli.MavenCli.execute(MavenCli.java:555)
at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:214)
at org.apache.maven.cli.MavenCli.main(MavenCli.java:158)
... 8 more

Has anyone seen similar error ?

Cheers
"
Evan Chan <velvia.github@gmail.com>,"Fri, 22 Aug 2014 15:40:50 -0700",[Spark SQL] off-heap columnar store,dev@spark.apache.org,"Hey guys,

What is the plan for getting Tachyon/off-heap support for the columnar
compressed store?  It's not in 1.1 is it?

In particular:
 - being able to set TACHYON as the caching mode
 - loading of hot columns or all columns
 - write-through of columnar store data to HDFS or backing store
 - being able to start a context and query directly from Tachyon's
cached columnar data

I think most of this was in Shark 0.9.1.

Also, how likely is the wire format for the columnar compressed data
to change?  That would be a problem for write-through or persistence.

thanks,
Evan

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Fri, 22 Aug 2014 23:58:28 +0100","Re: reference to dstream in package org.apache.spark.streaming which
 is not available",Ted Yu <yuzhihong@gmail.com>,"Yes, master hasn't compiled for me for a few days. It's fixed in:

https://github.com/apache/spark/pull/1726
https://github.com/apache/spark/pull/2075

Could a committer sort this out?

Sean



---------------------------------------------------------------------


"
Hari Shreedharan <hshreedharan@cloudera.com>,"Fri, 22 Aug 2014 16:07:26 -0700","Re: reference to dstream in package org.apache.spark.streaming which
 is not available",Sean Owen <sowen@cloudera.com>,"Sean - I think only the ones in 1726 are enough. It is weird that any 
class that uses the test-jar actually requires the streaming jar to be 
added explicitly. Shouldn't maven take care of this?

I posted some comments on the PR.

-- 

Thanks,
Hari


"
Reynold Xin <rxin@databricks.com>,"Fri, 22 Aug 2014 17:08:22 -0700",Re: Spark Contribution,Nicholas Chammas <nicholas.chammas@gmail.com>,"Great idea. Added the link
https://github.com/apache/spark/blob/master/README.md




aputra<henry.saputra@gmail.com>ÎãòÏù¥ ÏûëÏÑ±Ìïú Î©îÏãúÏßÄ:
"
npanj <nitinpanj@gmail.com>,"Fri, 22 Aug 2014 17:43:39 -0700 (PDT)","Graphx seems to be broken while Creating a large graph(6B nodes in
 my case)",dev@spark.incubator.apache.org,"While creating a graph with 6B nodes and 12B edges, I noticed that
*'numVertices' api returns incorrect result*; 'numEdges' reports correct
number. For few times(with different dataset > 2.5B nodes) I have also
notices that numVertices is returned as -ive number; so I suspect that there
is some overflow (may be we are using Int for some field?).

Environment: Standalone mode running on EC2 . Using latest code from master
branch upto commit #db56f2df1b8027171da1b8d2571d1f2ef1e103b6 .

Here is some details of experiments I have done so far: 
1. Input: numNodes=6101995593 ; noEdges=12163784626
Graph returns: numVertices=1807028297 ; numEdges=12163784626
2. Input : numNodes=*2157586441* ; noEdges=2747322705
Graph Returns: numVertices=*-2137380855* ; numEdges=2747322705
3. Input: numNodes=1725060105 ; noEdges=204176821
Graph: numVertices=1725060105 ; numEdges=2041768213 


You can find the code to generate this bug here:
https://gist.github.com/npanj/92e949d86d08715bf4bf

(I have also filed this jira ticket:
https://issues.apache.org/jira/browse/SPARK-3190)





--

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Fri, 22 Aug 2014 17:51:34 -0700","Re: reference to dstream in package org.apache.spark.streaming which
 is not available",Hari Shreedharan <hshreedharan@cloudera.com>,"Hey All,

We can sort this out ASAP. Many of the Spark committers were at a company
offsite for the last 72 hours, so sorry that it is broken.

- Patrick



"
Tathagata Das <tathagata.das1565@gmail.com>,"Fri, 22 Aug 2014 18:34:49 -0700","Re: reference to dstream in package org.apache.spark.streaming which
 is not available",Patrick Wendell <pwendell@gmail.com>,"Figured it out. Fixing this ASAP.

TD



"
Tathagata Das <tathagata.das1565@gmail.com>,"Fri, 22 Aug 2014 18:47:21 -0700","Re: reference to dstream in package org.apache.spark.streaming which
 is not available",Patrick Wendell <pwendell@gmail.com>,"The real fix is that the spark sink suite does not really need to use to
the spark-streaming test jars. Removing that dependency altogether, and
submitting a PR.

TD



"
Maisnam Ns <maisnam.ns@gmail.com>,"Sat, 23 Aug 2014 10:04:38 +0530",Re: Spark Contribution,Reynold Xin <rxin@databricks.com>,"Thanks all, for adding this link .



Saputra<henry.saputra@gmail.com>ÎãòÏù¥ ÏûëÏÑ±Ìïú Î©îÏãúÏßÄ:
k
"
Jeffrey Picard <jpicard@placeiq.com>,"Sat, 23 Aug 2014 02:42:55 -0400",Re: Graphx seems to be broken while Creating a large graph(6B nodes in my case),npanj <nitinpanj@gmail.com>,"Iím seeing this issue also. I have graph with with 5828339535 vertices and 7398447992 edges, graph.numVertices returns 1533266498 and graph.numEdges is correct and returns 7398447992. I also am having an issue that Iím beginning to suspect is caused by the same underlying problem where connected components stops after one iteration, returning an incorrect graph.

correct
there
master
http://apache-spark-developers-list.1001551.n3.nabble.com/Graphx-seems-to-be-broken-while-Creating-a-large-graph-6B-nodes-in-my-case-tp7966.html
Nabble.com.

"
Sean Owen <sowen@cloudera.com>,"Sat, 23 Aug 2014 10:10:50 +0100",Re: Spark Contribution,Reynold Xin <rxin@databricks.com>,"Can I ask a related question, since I have a PR open to touch up
README.md as we speak (SPARK-3069)?

If this text is in a file called CONTRIBUTING.md, then it will cause a
link to appear on the pull request screen, inviting people to review
the contribution guidelines:

https://github.com/blog/1184-contributing-guidelines

This is mildly important as the project wants to make it clear that
you agree that your contribution is licensed under the AL2, since
there is no formal ICLA.

How about I propose moving the text to CONTRIBUTING.md with a pointer
in README.md? or keep it both places?

Saputra<henry.saputra@gmail.com>ÎãòÏù¥ ÏûëÏÑ±Ìïú Î©îÏãúÏßÄ:
k

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sat, 23 Aug 2014 10:18:36 -0400",Re: Spark Contribution,Sean Owen <sowen@cloudera.com>,"That sounds like a good idea.

Continuing along those lines, what do people think of moving the
contributing page entirely from the wiki to GitHub? It feels like the right
place for it since GitHub is where we take contributions, and it also lets
people make improvements to it.

Nick


2014ÎÖÑ 8Ïõî 23Ïùº ÌÜ†ÏöîÏùº, Sean Owen<sowen@cloudera.com>ÎãòÏù¥ ÏûëÏÑ±Ìïú Î©îÏãúÏßÄ:

y Saputra<henry.saputra@gmail.com <javascript:;>>ÎãòÏù¥
-
"
Gary Malouf <malouf.gary@gmail.com>,"Sat, 23 Aug 2014 10:57:01 -0400",Mesos/Spark Deadlock,"""dev@spark.apache.org"" <dev@spark.apache.org>, dev@mesos.apache.org","I just wanted to bring up a significant Mesos/Spark issue that makes the
combo difficult to use for teams larger than 4-5 people.  It's covered in
https://issues.apache.org/jira/browse/MESOS-1688.  My understanding is that
Spark's use of executors in fine-grained mode is a very different behavior
than many of the other common frameworks for Mesos.
"
Maisnam Ns <maisnam.ns@gmail.com>,"Sat, 23 Aug 2014 22:52:11 +0530",Re: Spark Contribution,Nicholas Chammas <nicholas.chammas@gmail.com>,"Sure, it's really a good idea to have a CONTRIBUTING.md file with details
on how to contribute e.g cloning,branching,changes , commit  along with
corresponding git commands. That way someone who wants to contribute will
surely get the benefit of a quick short documentation on contribution.

Maisnam



ht
s
en<sowen@cloudera.com>ÎãòÏù¥ ÏûëÏÑ±Ìïú Î©îÏãúÏßÄ:
:
ry Saputra<henry.saputra@gmail.com>ÎãòÏù¥ ÏûëÏÑ±Ìïú Î©îÏãúÏßÄ:
k
--
"
Matei Zaharia <matei.zaharia@gmail.com>,"Sat, 23 Aug 2014 16:16:08 -0700",Re: Mesos/Spark Deadlock,"""=?utf-8?Q?dev=40spark.apache.org?="" <dev@spark.apache.org>, Gary
 Malouf <malouf.gary@gmail.com>, dev@mesos.apache.org","Hey Gary, just as a workaround, note that you can use Mesos in coarse-grained mode by setting spark.mesos.coarse=true. Then it will hold onto CPUs for the duration of the job.

Matei


I just wanted to bring up a significant Mesos/Spark issue that makes the 
combo difficult to use for teams larger than 4-5 people. It's covered in 
https://issues.apache.org/jira/browse/MESOS-1688. My understanding is that 
Spark's use of executors in fine-grained mode is a very different behavior 
than many of the other common frameworks for Mesos. 
"
Gary Malouf <malouf.gary@gmail.com>,"Sat, 23 Aug 2014 19:30:27 -0400",Re: Mesos/Spark Deadlock,Matei Zaharia <matei.zaharia@gmail.com>,"Hi Matei,

We have an analytics team that uses the cluster on a daily basis.  They use
two types of 'run modes':

1) For running actual queries, they set the spark.executor.memory to
something between 4 and 8GB of RAM/worker.

2) A shell that takes a minimal amount of memory on workers (128MB) for
prototyping out a larger query.  This allows them to not take up RAM on the
cluster when they do not really need it.

We see the deadlocks when there are a few shells in either case.  From the
usage patterns we have, coarse-grained mode would be a challenge as we have
to constantly remind people to kill their shells as soon as their queries
finish.

Am I correct in viewing Mesos in coarse-grained mode as being similar to
Spark Standalone's cpu allocation behavior?





"
Ron Gonzalez <zlgonzalez@yahoo.com.INVALID>,"Sun, 24 Aug 2014 16:12:40 -0700",Problems running examples in IDEA,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,
   After getting the code base to compile, I tried running some of the 
scala examples.
   They all fail since it can't find classes like SparkConf.
   If I change the iml file to convert provided scope from PROVIDED to 
COMPILE, I am able to run them. It's simple by doing the following in 
the root directory of the spark code base: find . -name ""*.iml"" | xargs 
sed -i.bak 's/PROVIDED/COMPILE/g'.
   Is this expected? I'd really rather not modify the iml files since 
they were sourced from the pom xml files, so if you guys have some tips 
on doing this better, that would be great...

Thanks,
Ron

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Mon, 25 Aug 2014 00:21:44 +0100",Re: Problems running examples in IDEA,"""dev@spark.apache.org"" <dev@spark.apache.org>","The examples aren't runnable quite like this. It's intended that they
are submitted to a cluster with spark-submit, which would among other
things provide Spark at runtime.

I think you might get them to run this way if you set master to
""local[*]"" and indeed made a run profile that also included Spark on
the classpath.

You would never modify the .iml files anyway. You can change the Maven
pom.xml files if you were to need to modify a dependency scope.


---------------------------------------------------------------------


"
Ron Gonzalez <zlgonzalez@yahoo.com.INVALID>,"Sun, 24 Aug 2014 16:55:01 -0700",Re: Problems running examples in IDEA,"Sean Owen <sowen@cloudera.com>, 
 ""dev@spark.apache.org"" <dev@spark.apache.org>","Oh ok. So from the code base, local execution is dependent on everyone's 
way then, right? I am indeed changing the code to add the master to 
local[*], but still getting the no classdef found errors.

If that's the case, then I think I'm ok then...

Thanks,
Ron




---------------------------------------------------------------------


"
scwf <wangfei1@huawei.com>,"Mon, 25 Aug 2014 10:11:40 +0800",Re: Working Formula for Hive 0.13?,Michael Armbrust <michael@databricks.com>,"   I have worked for a branch update the hive version to hive-0.13(by org.apache.hive)---https://github.com/scwf/spark/tree/hive-0.13
I am wondering whether it's ok to make a PR now because hive-0.13 version is not compatible with hive-0.12 and here i used org.apache.hive.





---------------------------------------------------------------------


"
Matei Zaharia <matei.zaharia@gmail.com>,"Sun, 24 Aug 2014 21:52:00 -0700",Re: Mesos/Spark Deadlock,Gary Malouf <malouf.gary@gmail.com>,"Yeah, Mesos in coarse-grained mode probably wouldn't work here. It's too bad that this happens in fine-grained mode -- would be really good to fix. I'll see if we can get the workaround in¬†https://github.com/apache/spark/pull/1860 into Spark 1.1. Incidentally have you tried that?

Matei

rote:

Hi Matei,

We have an analytics team that uses the cluster on a daily basis. ¬†They use two types of 'run modes':

1) For running actual queries, they set the spark.executor.memory to something between 4 and 8GB of RAM/worker. ¬†

2) A shell that takes a minimal amount of memory on workers (128MB) for prototyping out a larger query. ¬†This allows them to not take up RAM on the cluster when they do not really need it.

We see the deadlocks when there are a few shells in either case. ¬†From the usage patterns we have, coarse-grained mode would be a challenge as we have to constantly remind people to kill their shells as soon as their queries finish. ¬†

Am I correct in viewing Mesos in coarse-grained mode as being similar to Spark Standalone's cpu allocation behavior?




Hey Gary, just as a workaround, note that you can use Mesos in coarse-grained mode by setting spark.mesos.coarse=true. Then it will hold onto CPUs for the duration of the job.

Matei

rote:

I just wanted to bring up a significant Mesos/Spark issue that makes the
combo difficult to use for teams larger than 4-5 people. It's covered in
https://issues.apache.org/jira/browse/MESOS-1688. My understanding is that
Spark's use of executors in fine-grained mode is a very different behavior
than many of the other common frameworks for Mesos.

"
Timothy Chen <tnachen@gmail.com>,"Sun, 24 Aug 2014 21:54:16 -0700",Re: Mesos/Spark Deadlock,"""dev@mesos.apache.org"" <dev@mesos.apache.org>","+1 to have the work around in.

I'll be investigating from the Mesos side too.

Tim

ote:
bad that this happens in fine-grained mode -- would be really good to fix. I'll see if we can get the workaround in https://github.com/apache/spark/pull/1860 into Sp"
Dibyendu Bhattacharya <dibyendu.bhattachary@gmail.com>,"Mon, 25 Aug 2014 11:09:49 +0530",Re: Low Level Kafka Consumer for Spark,Tathagata Das <tathagata.das1565@gmail.com>,"Hi,

Just to give you some update on this low level consumer  (
https://github.com/dibbhatt/kafka-spark-consumer), we at Pearson have been
doing good amount load and stress testing for last few weeks and initial
test results are very impressive. We did not see any data loss, no issue
related to ""Out of Memory""  (as like existing consumer) with heavy load and
no issues related to Kafka offsets managements .

Regards,
Dibyendu



r
.
ought?
he
y
t+Management
o
eckpointing.html
d
g
r
n
e
"
"""Lizhengbing (bing, BIPA)"" <zhengbing.li@huawei.com>","Mon, 25 Aug 2014 09:14:03 +0000","I want to contribute MLlib two quality measures(ARHR and HR) for
 top N recommendation system. Is this meaningful?","""dev@spark.apache.org"" <dev@spark.apache.org>","Hi:
In paper ""Item-Based Top-N Recommendation Algorithms""(https://stuyresearch.googlecode.com/hg/blake/resources/10.1.1.102.4451.pdf), there are two parameters measuring the quality of recommendation: HR and ARHR.
If I use ALS(Implicit) for top-N recommendation system, I want to check it's quality. ARHR and HR are two good quality measures.
I want to contribute them to spark MLlib.  So I want to know whether this is meaningful?


(1) If n is the total number of customers/users,  the hit-rate of the recommendation algorithm was computed as
hit-rate (HR) = Number of hits / n

(2)If h is the number of hits that occurred at positions p1, p2, . . . , ph within the top-N lists (i.e., 1 ò pi ò N), then the average reciprocal hit-rank is equal to:
[cid:image001.png@01CFC086.8EE1FF40]i
.
"
Gary Malouf <malouf.gary@gmail.com>,"Mon, 25 Aug 2014 08:05:30 -0400",Re: Mesos/Spark Deadlock,Timothy Chen <tnachen@gmail.com>,"We have not tried the work-around because there are other bugs in there
that affected our set-up, though it seems it would help.



"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 25 Aug 2014 14:06:08 -0400",Re: Pull requests will be automatically linked to JIRA when submitted,Patrick Wendell <pwendell@gmail.com>,"FYI: Looks like the Mesos folk also have a bot to do automatic linking, but
it appears to have been provided to them somehow by ASF.

See this comment as an example:
https://issues.apache.org/jira/browse/MESOS-1688?focusedCommentId=14109078&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14109078

Might be a small win to push this work to a bot ASF manages if we can get
access to it (and if we have no concerns about depending on an another
external service).

Nick



ut
y,
d
ASF
he
es it with
ra-bitbucket-connector-plugin
d+GitHub+accounts+to+JIRA
lugins.jira-bitbucket-connector-plugin/86ff1a21-44fb-4227-aa4f-44c77aec2c97.png
39232929
ly
er
"
npanj <nitinpanj@gmail.com>,"Mon, 25 Aug 2014 11:13:39 -0700 (PDT)","Re: [SPARK-2878] Kryo serialisation with custom Kryo registrator
 failing",dev@spark.incubator.apache.org,"I am running the code with @rxin's patch in standalone mode.  In my case I am
registering ""org.apache.spark.graphx.GraphKryoRegistrator"" . 

Recently I started to see ""com.esotericsoftware.kryo.KryoException:
java.io.IOException: failed to uncompress the chunk: PARSING_ERROR"" . Has
anyone seen this? Could it be related to this issue? > Here it trace: 
--
vids (org.apache.spark.graphx.impl.VertexAttributeBlock)
        com.esotericsoftware.kryo.io.Input.fill(Input.java:142)
        com.esotericsoftware.kryo.io.Input.require(Input.java:169)
        com.esotericsoftware.kryo.io.Input.readLong_slow(Input.java:710)
        com.esotericsoftware.kryo.io.Input.readLong(Input.java:665)
       
com.esotericsoftware.kryo.serializers.DefaultArraySerializers$LongArraySerializer.read(DefaultArraySerializers.java:127)
       
com.esotericsoftware.kryo.serializers.DefaultArraySerializers$LongArraySerializer.read(DefaultArraySerializers.java:107)
        com.esotericsoftware.kryo.Kryo.readObjectOrNull(Kryo.java:699)
       
com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.read(FieldSerializer.java:611)
       
com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:221)
        com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)
        com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:43)
        com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:34)
        com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)
       
org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:133)
       
org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
        org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
       
org.apache.spark.storage.BlockManager$LazyProxyIterator$1.hasNext(BlockManager.scala:1054)
        scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
       
org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
       
org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
        scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
        scala.collection.Iterator$class.foreach(Iterator.scala:727)
        scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
       
org.apache.spark.graphx.impl.VertexPartitionBaseOps.innerJoinKeepLeft(VertexPartitionBaseOps.scala:192)
       
org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:78)
       
org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:75)
       
org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:73)
        scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
       
org.apache.spark.graphx.EdgeRDD$$anonfun$mapEdgePartitions$1.apply(EdgeRDD.scala:87)
       
org.apache.spark.graphx.EdgeRDD$$anonfun$mapEdgePartitions$1.apply(EdgeRDD.scala:85)
        org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:596)
        org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:596)
       
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
       
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
       
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
       
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
        org.apache.spark.scheduler.Task.run(Task.scala:54)
       
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:202)
       
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

--




--

---------------------------------------------------------------------


"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 25 Aug 2014 12:02:39 -0700",Re: Mesos/Spark Deadlock,"Timothy Chen <tnachen@gmail.com>, Gary Malouf
 <malouf.gary@gmail.com>, dev@mesos.apache.org","BTW it seems to me that even without that patch, you should be getting tasks launched as long as you leave at least 32 MB of memory free on each machine (that is, the sum of the executor memory sizes is not exactly the same as the total size of the machine). Then Mesos will be able to re-offer that machine whenever CPUs free up.

Matei


We have not tried the work-around because there are other bugs in there 
that affected our set-up, though it seems it would help. 



"
amnonkhen <amnon.is@gmail.com>,"Mon, 25 Aug 2014 12:08:59 -0700 (PDT)","Re: saveAsTextFile to s3 on spark does not work, just hangs",dev@spark.incubator.apache.org,"Hi jerryye,
Maybe if you voted up my question on Stack Overflow it would get some
traction and we would get nearer to a solution.
Thanks,
  Amnon



--

---------------------------------------------------------------------


"
Andrew Ash <andrew@andrewash.com>,"Mon, 25 Aug 2014 12:16:23 -0700",Re: take() reads every partition if the first one is empty,pnepywoda <pnepywoda@palantir.com>,"Filed as https://issues.apache.org/jira/browse/SPARK-3211



"
Xiangrui Meng <mengxr@gmail.com>,"Mon, 25 Aug 2014 12:28:10 -0700","Re: I want to contribute MLlib two quality measures(ARHR and HR) for
 top N recommendation system. Is this meaningful?","""Lizhengbing (bing, BIPA)"" <zhengbing.li@huawei.com>","The evaluation metrics are definitely useful. How do they differ from
traditional IR metrics like prec@k and ndcg@k? -Xiangrui



f),
§ *N*), then the
"
Cody Koeninger <cody@koeninger.org>,"Mon, 25 Aug 2014 14:59:59 -0500",Re: Mesos/Spark Deadlock,Matei Zaharia <matei.zaharia@gmail.com>,"I definitely saw a case where

a. the only job running was a 256m shell
b. I started a 2g job
c. a little while later the same user as in a started another 256m shell

it started again.

This is on nodes with ~15G of memory, on which we have successfully run 8G
jobs.



"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 25 Aug 2014 13:07:13 -0700",Re: Mesos/Spark Deadlock,Cody Koeninger <cody@koeninger.org>,"This is kind of weird then, seems perhaps unrelated to this issue (or at least to the way I understood it). Is the problem maybe that Mesos saw 0 MB being freed and didn't re-offer the machine *even though there was more than 32 MB free overall*?

Matei


I definitely saw a case where

a. the only job running was a 256m shell
b. I started a 2g job
c. a little while later the same user as in a started another 256m shell

shells, it started again.

This is on nodes with ~15G of memory, on which we have successfully run 8G jobs.


BTW it seems to me that even without that patch, you should be getting tasks launched as long as you leave at least 32 MB of memory free on each machine (that is, the sum of the executor memory sizes is not exactly the same as the total size of the machine). Then Mesos will be able to re-offer that machine whenever CPUs free up.

Matei

rote:

We have not tried the work-around because there are other bugs in there
that affected our set-up, though it seems it would help.


e:

om>
too
ix.
m)
y
o
or
the
om
 we
 to
.com>
hold
m)
the
 in


"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 25 Aug 2014 13:08:21 -0700",Re: Mesos/Spark Deadlock,Cody Koeninger <cody@koeninger.org>,"Anyway it would be good if someone from the Mesos side investigates this and proposes a solution. The 32 MB per task hack isn't completely foolproof either (e.g. people might allocate all the RAM to their executor and thus stop being able to launch tasks), so maybe we wait on a Mesos fix for this one.

Matei


This is kind of weird then, seems perhaps unrelated to this issue (or at least to the way I understood it). Is the problem maybe that Mesos saw 0 MB being freed and didn't re-offer the machine *even though there was more than 32 MB free overall*?

Matei


I definitely saw a case where

a. the only job running was a 256m shell
b. I started a 2g job
c. a little while later the same user as in a started another 256m shell

shells, it started again.

This is on nodes with ~15G of memory, on which we have successfully run 8G jobs.


BTW it seems to me that even without that patch, you should be getting tasks launched as long as you leave at least 32 MB of memory free on each machine (that is, the sum of the executor memory sizes is not exactly the same as the total size of the machine). Then Mesos will be able to re-offer that machine whenever CPUs free up.

Matei

rote:

We have not tried the work-around because there are other bugs in there
that affected our set-up, though it seems it would help.


e:

om>
too
ix.
m)
y
o
or
the
om
 we
 to
.com>
hold
m)
the
 in


"
Michael Armbrust <michael@databricks.com>,"Mon, 25 Aug 2014 13:08:42 -0700",Re: Working Formula for Hive 0.13?,scwf <wangfei1@huawei.com>,"Thanks for working on this!  Its unclear at the moment exactly how we are
going to handle this, since the end goal is to be compatible with as many
versions of Hive as possible.  That said, I think it would be great to open
a PR in this case.  Even if we don't merge it, thats a good way to get it
on people's radar and have a discussion about the changes that are required.



s
?
h
 the
e
l,
"
Michael Armbrust <michael@databricks.com>,"Mon, 25 Aug 2014 13:13:57 -0700",Re: [Spark SQL] off-heap columnar store,Evan Chan <velvia.github@gmail.com>,"

It is not in 1.1 and there are not concrete plans for adding it at this
point.  Currently, there is more engineering investment going into caching
parquet data in Tachyon instead.  This approach is going to have much
better support for nested data, leverages other work being done on parquet,
and alleviates your concerns about wire format compatibility.

That said, if someone really wants to try and implement it, I don't think
it would be very hard.  The primary issue is going to be designing a clean
interface that is not too tied to this one implementation.



We aren't making any guarantees at the moment that it won't change.  Its
currently only intended for temporary caching of data.
"
Michael Armbrust <michael@databricks.com>,"Mon, 25 Aug 2014 13:17:07 -0700",Re: Storage Handlers in Spark SQL,Niranda Perera <niranda@wso2.com>,"- dev list
+ user list

You should be able to query Spark SQL using JDBC, starting with the 1.1
release.  There is some documentation is the repo
<https://github.com/apache/spark/blob/master/docs/sql-programming-guide.md#running-the-thrift-jdbc-server>,
a"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 25 Aug 2014 13:26:02 -0700","Re: saveAsTextFile to s3 on spark does not work, just hangs","amnonkhen <amnon.is@gmail.com>, dev@spark.incubator.apache.org","Was the original issue with Spark 1.1 (i.e. master branch) or an earlier release?


Matei


Hi jerryye, 
Maybe if you voted up my question on Stack Overflow it would get some 
traction and we would get nearer to a solution. 
Thanks, 
Amnon 



-- 

--------------------------------------------------------------------- 

"
Patrick Wendell <pwendell@gmail.com>,"Mon, 25 Aug 2014 13:34:04 -0700","Re: saveAsTextFile to s3 on spark does not work, just hangs",Matei Zaharia <matei.zaharia@gmail.com>,"shell process and on the executors and attach the results. It could be that
somehow you are encountering a deadlock somewhere.



"
Patrick Wendell <pwendell@gmail.com>,"Mon, 25 Aug 2014 13:35:39 -0700",Re: Pull requests will be automatically linked to JIRA when submitted,Nicholas Chammas <nicholas.chammas@gmail.com>,"Hey Nicholas,

That seems promising - I prefer having a proper link to having that fairly
verbose comment though, because in some cases there will be dozens of
comments and it could get lost. I wonder if they could do something where
it posts a link instead...

- Patrick



"
jerryye <jerryye@gmail.com>,"Mon, 25 Aug 2014 13:37:24 -0700 (PDT)","Re: saveAsTextFile to s3 on spark does not work, just hangs",dev@spark.incubator.apache.org,"Hi Matei,
At least in my case, the s3 bucket is in the same region. Running count()
works and so does generating synthetic data. What I saw was that the job
would hang for over an hour with no progress but tasks would immediately
start finishing if I cached the data.

- jerry







--"
Timothy Chen <tnachen@gmail.com>,"Mon, 25 Aug 2014 14:28:18 -0700",Re: Mesos/Spark Deadlock,Matei Zaharia <matei.zaharia@gmail.com>,"Hi Matei,

I'm going to investigate from both Mesos and Spark side will hopefully
have a good long term solution. In the mean time having a work around
to start with is going to unblock folks.

Tim


---------------------------------------------------------------------


"
Andrew Lee <alee526@hotmail.com>,"Mon, 25 Aug 2014 14:29:12 -0700",RE: Working Formula for Hive 0.13?,"Michael Armbrust <michael@databricks.com>, scwf <wangfei1@huawei.com>","llowing are the 4 major ones that I can see why people are asking to upgrade to Hive 0.13.1 recently.
1. Performance and bug fix, patches. (Usual case)
2. Native support for Parquet format, no need to provide custom JARs and SerDe like Hive 0.12. (Depends, driven by data format and queries)
3. Support of Tez engine which gives performance improvement in several use cases (Performance improvement)
4. Security enhancement in Hive 0.13.1 has improved a lot (Security concerns, ACLs, etc)
These are the major benefits I see to upgrade to Hive 0.13.1+ from Hive 0.12.0.
There may be others out there that I'm not aware of, but I do see it coming.
my 2 cents.
ny
open
it
ed.
on
e
t
ny
st
?  Is
es?
ote:
.
rote:
 the
x$2:
a
ith
ks.com>
s
ith
e the
mail.com>
t
mail.com>
 Spark
the
om>
o
,
.com>
,
th
t
il.com
,
 at
,
ial,
fied
ure or
e
 		 	   		  "
jerryye <jerryye@gmail.com>,"Mon, 25 Aug 2014 14:55:26 -0700 (PDT)","Re: saveAsTextFile to s3 on spark does not work, just hangs",dev@spark.incubator.apache.org,"Hi Patrick,
Here's the process:
java -cp
/root/ephemeral-hdfs/conf::::/root/ephemeral-hdfs/conf:/root/spark/conf:/root/spark/assembly/target/scala-2.10/spark-assembly-1.1.1-SNAPSHOT-hadoop1.0.4.jar
-XX:MaxPermSize=128m -Djava.library.path=/root/ephemeral-hdfs/lib/native/
-Xms5g -Xmx10g -XX:MaxPermSize=10g -Dspark.akka.timeout=300
-Dspark.driver.port=59156 -Xms5g -Xmx10g -XX:MaxPermSize=10g -Xms58315M
-Xmx58315M org.apache.spark.executor.CoarseGrainedExecutorBackend
akka.tcp://spark@ip-10-226-198-178.us-west-2.compute.internal:59156/user/CoarseGrainedScheduler
5 ip-10-38-9-181.us-west-2.compute.internal 8
akka.tcp://sparkWorker@ip-10-38-9-181.us-west-2.compute.internal:34533/user/Worker
app-20140825214225-0001

Attached is the requested stack trace.






jstack.txt (92K) <http://apache-spark-developers-list.1001551.n3.nabble.com/attachment/8006/0/jstack.txt>




--"
Graham Dennis <graham.dennis@gmail.com>,"Tue, 26 Aug 2014 07:59:21 +1000",Re: [SPARK-2878] Kryo serialisation with custom Kryo registrator failing,npanj <nitinpanj@gmail.com>,"Hi,

Unless you manually patched Spark, if you have Reynoldís patch for SPARK-2878, you also have the patch for SPARK-2893 which makes the underlying cause much more obvious and explicit.  So the below is unlikely to be related to SPARK-2878.

Graham


case I am
Has

com.esotericsoftware.kryo.io.Input.readLong_slow(Input.java:710)
com.esotericsoftware.kryo.serializers.DefaultArraySerializers$LongArraySerializer.read(DefaultArraySerializers.java:127)
com.esotericsoftware.kryo.serializers.DefaultArraySerializers$LongArraySerializer.read(DefaultArraySerializers.java:107)
com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.read(FieldSerializer.java:611)
com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:221)
com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)
com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:43)
com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:34)
com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)
org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:133)
org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
org.apache.spark.storage.BlockManager$LazyProxyIterator$1.hasNext(BlockManager.scala:1054)
org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
org.apache.spark.graphx.impl.VertexPartitionBaseOps.innerJoinKeepLeft(VertexPartitionBaseOps.scala:192)
org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:78)
org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:75)
org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:73)
org.apache.spark.graphx.EdgeRDD$$anonfun$mapEdgePartitions$1.apply(EdgeRDD.scala:87)
org.apache.spark.graphx.EdgeRDD$$anonfun$mapEdgePartitions$1.apply(EdgeRDD.scala:85)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
http://apache-spark-developers-list.1001551.n3.nabble.com/SPARK-2878-Kryo-serialisation-with-custom-Kryo-registrator-failing-tp7719p7989.html
Nabble.com.


---------------------------------------------------------------------


"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 25 Aug 2014 15:06:44 -0700",Re: Mesos/Spark Deadlock,Timothy Chen <tnachen@gmail.com>,"My problem is that I'm not sure this workaround would solve things, given the issue described here (where there was a lot of memory free but it didn't get re-offered). If you think it does, it would be good to explain why it behaves like that.

Matei


Hi Matei, 

I'm going to investigate from both Mesos and Spark side will hopefully 
have a good long term solution. In the mean time having a work around 
to start with is going to unblock folks. 

Tim 

"
amnonkhen <amnon.is@gmail.com>,"Mon, 25 Aug 2014 15:07:12 -0700 (PDT)","Re: saveAsTextFile to s3 on spark does not work, just hangs",dev@spark.incubator.apache.org,"Hi Matei,
The original issue happened on a spark-1.0.2-bin-hadoop2 installation.
I will try the synthetic operation and see if I get the same results or not.
Amnon







--"
Henry Saputra <henry.saputra@gmail.com>,"Mon, 25 Aug 2014 15:09:21 -0700",Re: [Spark SQL] off-heap columnar store,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Michael,

This is great news.
Any initial proposal or design about the caching to Tachyon that you
can share so far?

I don't think there is a JIRA ticket open to track this feature yet.

- Henry


---------------------------------------------------------------------


"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 25 Aug 2014 15:31:39 -0700","Re: saveAsTextFile to s3 on spark does not work, just hangs","amnonkhen <amnon.is@gmail.com>, dev@spark.incubator.apache.org","Got it. Another thing that would help is if you spot any exceptions or failed tasks in the web UI (http://<driver>:4040).

Matei


Hi Matei, 
The original issue happened on a spark-1.0.2-bin-hadoop2 installation. 
I will try the synthetic operation and see if I get the same results or not. 
Amnon 







-- "
Timothy Chen <tnachen@gmail.com>,"Mon, 25 Aug 2014 15:32:11 -0700",Re: Mesos/Spark Deadlock,Matei Zaharia <matei.zaharia@gmail.com>,"I don't think it solves Cody's problem which still need more
investigating, but I believe it does solve the problem you described
earlier.

I just confirmed with Mesos folks that we no longer need the minimum
memory requirement so we'll be dropping that soon and the workaround
might not be needed for the next mesos release.

Tim


---------------------------------------------------------------------


"
yao <yaoshengzhe@gmail.com>,"Mon, 25 Aug 2014 16:53:39 -0700",too many CancelledKeyException throwed from ConnectionManager,dev@spark.apache.org,"Hi Folks,

We are testing our home-made KMeans algorithm using Spark on Yarn.
Recently, we've found that the application failed frequently when doing
clustering over 300,000,000 users (each user is represented by a feature
vector and the whole data set is around 600,000,000). After digging into
the job log, we've found that there are many CancelledKeyException throwed
by ConnectionManager but not observed other exceptions. We double frequent
CancelledKeyException brings the whole application down since the
application often failed on the third or fourth iteration for large
datasets. Welcome to any directional suggestions.

*Errors in job log*:
java.nio.channels.CancelledKeyException
        at
org.apache.spark.network.ConnectionManager.run(ConnectionManager.scala:363)
        at
org.apache.spark.network.ConnectionManager$$anon$4.run(ConnectionManager.scala:116)
14/08/25 19:04:32 INFO ConnectionManager: Removing ReceivingConnection to
ConnectionManagerId(lsv-289.rfiserve.net,43199)
14/08/25 19:04:32 ERROR ConnectionManager: Corresponding
SendingConnectionManagerId not found
14/08/25 19:04:32 INFO ConnectionManager: Key not valid ?
sun.nio.ch.SelectionKeyImpl@2570cd62
14/08/25 19:04:32 INFO ConnectionManager: key already cancelled ?
sun.nio.ch.SelectionKeyImpl@2570cd62
java.nio.channels.CancelledKeyException
        at
org.apache.spark.network.ConnectionManager.run(ConnectionManager.scala:363)
        at
org.apache.spark.network.ConnectionManager$$anon$4.run(ConnectionManager.scala:116)
14/08/25 19:04:32 INFO ConnectionManager: Removing ReceivingConnection to
ConnectionManagerId(lsv-289.rfiserve.net,56727)
14/08/25 19:04:32 INFO ConnectionManager: Removing SendingConnection to
ConnectionManagerId(lsv-289.rfiserve.net,56727)
14/08/25 19:04:32 INFO ConnectionManager: Removing SendingConnection to
ConnectionManagerId(lsv-289.rfiserve.net,56727)
14/08/25 19:04:32 INFO ConnectionManager: Key not valid ?
sun.nio.ch.SelectionKeyImpl@37c8b85a
14/08/25 19:04:32 INFO ConnectionManager: key already cancelled ?
sun.nio.ch.SelectionKeyImpl@37c8b85a
java.nio.channels.CancelledKeyException
        at
org.apache.spark.network.ConnectionManager.run(ConnectionManager.scala:287)
        at
org.apache.spark.network.ConnectionManager$$anon$4.run(ConnectionManager.scala:116)
14/08/25 19:04:32 INFO ConnectionManager: Removing SendingConnection to
ConnectionManagerId(lsv-668.rfiserve.net,41913)
14/08/25 19:04:32 INFO ConnectionManager: Removing ReceivingConnection to
ConnectionManagerId(lsv-668.rfiserve.net,41913)
14/08/25 19:04:32 INFO ConnectionManager: Key not valid ?
sun.nio.ch.SelectionKeyImpl@fcea3a4
14/08/25 19:04:32 ERROR ConnectionManager: Corresponding
SendingConnectionManagerId not found
14/08/25 19:04:32 INFO ConnectionManager: key already cancelled ?
sun.nio.ch.SelectionKeyImpl@fcea3a4


Best
Shengzhe
"
Ankur Dave <ankurdave@gmail.com>,"Mon, 25 Aug 2014 18:23:19 -0700",Re: Graphx seems to be broken while Creating a large graph(6B nodes in my case),"Jeffrey Picard <jpicard@placeiq.com>, npanj <nitinpanj@gmail.com>","I posted the fix on the JIRA ticket (https://issues.apache.org/jira/browse/SPARK-3190). To update the user list, this is indeed an integer overflow problem when summing up the partition sizes. The fix is to use Longs for the sum: https://github.com/apache/spark/pull/2106.

Ankur


---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 26 Aug 2014 00:02:01 -0400",Handling stale PRs,dev <dev@spark.apache.org>,"Check this out:
https://github.com/apache/spark/pulls?q=is%3Aopen+is%3Apr+sort%3Aupdated-asc

We're hitting close to 300 open PRs. Those are the least recently updated
ones.

I think having a low number of stale (i.e. not recently updated) PRs is a
good thing to shoot for. It doesn't leave contributors hanging (which feels
bad for contributors), and reduces project clutter (which feels bad for
maintainers/committers).

What is our approach to tackling this problem?

I think communicating and enforcing a clear policy on how stale PRs are
handled might be a good way to reduce the number of stale PRs we have
without making contributors feel rejected.

I don't know what such a policy would look like, but it should be
enforceable and ""lightweight""--i.e. it shouldn't feel like a hammer used to
reject people's work, but rather a necessary tool to keep the project's
contributions relevant and manageable.

Nick
"
rapelly kartheek <kartheek.mbms@gmail.com>,"Tue, 26 Aug 2014 09:35:31 +0530",RDD replication in Spark,dev@spark.apache.org,"Hi,

I've exercised multiple options available for persist() including  RDD
replication. I have gone thru the classes that involve in caching/storing
the RDDS at different levels. StorageLevel class plays a pivotal role by
recording whether to use memory or disk or to replicate the RDD on multiple
nodes.
The class LocationIterator iterates over the preferred machines one by one  for
each partition that is replicated. I got a rough idea of CoalescedRDD.
Please correct me if I am wrong.

But I am looking for the code that chooses the resources to replicate the
RDDs. Can someone please tell me how replication takes place and how do we
choose the resources for replication. I just want to know as to where
should I look into to understand how the replication happens.



Thank you so much!!!

regards

-Karthik
"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 25 Aug 2014 21:57:56 -0700",Re: Handling stale PRs,"Nicholas Chammas <nicholas.chammas@gmail.com>, dev
 <dev@spark.apache.org>","Hey Nicholas,

In general we've been looking at these periodically (at least I have) and asking people to close out of date ones, but it's true that the list has gotten fairly large. We should probably have an expiry time of a few months and close them automatically. I agree that it's daunting to see so many open PRs.

Matei


Check this out: 
https://github.com/apache/spark/pulls?q=is%3Aopen+is%3Apr+sort%3Aupdated-asc 

We're hitting close to 300 open PRs. Those are the least recently updated 
ones. 

I think having a low number of stale (i.e. not recently updated) PRs is a 
good thing to shoot for. It doesn't leave contributors hanging (which feels 
bad for contributors), and reduces project clutter (which feels bad for 
maintainers/committers). 

What is our approach to tackling this problem? 

I think communicating and enforcing a clear policy on how stale PRs are 
handled might be a good way to reduce the number of stale PRs we have 
without making contributors feel rejected. 

I don't know what such a policy would look like, but it should be 
enforceable and ""lightweight""--i.e. it shouldn't feel like a hammer used to 
reject people's work, but rather a necessary tool to keep the project's 
contributions relevant and manageable. 

Nick 
"
Amnon Khen <amnon.is@gmail.com>,"Tue, 26 Aug 2014 08:22:01 +0300","Re: saveAsTextFile to s3 on spark does not work, just hangs",Matei Zaharia <matei.zaharia@gmail.com>,"There were no failures nor exceptions.



r
.
to-s3-on-spark-does-not-work-just-hangs-tp7795p7991.html
n
to-s3-on-spark-does-not-work-just-hangs-tp7795p8000.html
rvlet.jtp?macro=unsubscribe_by_code&node=7795&code=YW1ub24uaXNAZ21haWwuY29tfDc3OTV8LTkxODIwMjYzNg==>
rvlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
to-s3-on-spark-does-not-work-just-hangs-tp7795p8008.html
"
Patrick Wendell <pwendell@gmail.com>,"Mon, 25 Aug 2014 22:25:00 -0700","Re: saveAsTextFile to s3 on spark does not work, just hangs",Amnon Khen <amnon.is@gmail.com>,"Hey Amnon,

So just to make sure I understand - you also saw the same issue with 1.0.2?
Just asking because whether or not this regresses the 1.0.2 behavior is
important for our own bug tracking.

- Patrick



to-s3-on-spark-does-not-work-just-hangs-tp7795p7991.html
to-s3-on-spark-does-not-work-just-hangs-tp7795p8000.html
rvlet.jtp?macro=unsubscribe_by_code&node=7795&code=YW1ub24uaXNAZ21haWwuY29tfDc3OTV8LTkxODIwMjYzNg==
rvlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml
to-s3-on-spark-does-not-work-just-hangs-tp7795p8008.html
"
Patrick Wendell <pwendell@gmail.com>,"Mon, 25 Aug 2014 23:02:00 -0700",Re: Handling stale PRs,Matei Zaharia <matei.zaharia@gmail.com>,"Hey Nicholas,

Thanks for bringing this up. There are a few dimensions to this... one is
that it's actually precedurally difficult for us to close pull requests.
I've proposed several different solutions to ASF infra to streamline the
process, but thus far they haven't been open to any of my ideas:

https://issues.apache.org/jira/browse/INFRA-7918
https://issues.apache.org/jira/browse/INFRA-8241

The more important thing, maybe, is how we want to deal with this
culturally. And I think we need to do a better job of making sure no pull
requests go unattended (i.e. waiting for committer feedback). If patches go
stale, it should be because the user hasn't responded, not us.

Another thing is that we should, IMO, err on the side of explicitly saying
""no"" or ""not yet"" to patches, rather than letting them linger without
attention. We do get patches where the user is well intentioned, but it is
a feature that doesn't make sense to add, or isn't well thought out or
explained, or the review effort would be so large it's not within our
capacity to look at just yet.

Most other ASF projects I know just ignore these patches. I'd prefer if we
took the approach of politely explaining why in the current form the patch
isn't acceptable and closing it (potentially w/ tips on how to improve it
or narrow the scope).

- Patrick





"
Evan Chan <velvia.github@gmail.com>,"Tue, 26 Aug 2014 00:37:41 -0700",Re: [Spark SQL] off-heap columnar store,Michael Armbrust <michael@databricks.com>,"What would be the timeline for the parquet caching work?

The reason I'm asking about the columnar compressed format is that
there are some problems for which Parquet is not practical.


---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Tue, 26 Aug 2014 00:47:24 -0700",Re: [Spark SQL] off-heap columnar store,Evan Chan <velvia.github@gmail.com>,"

Caching parquet files in tachyon with saveAsParquetFile and then reading
them with parquetFile should already work. You can use SQL on these tables
by using registerTempTable.

Some of the general parquet work that we have been doing includes: #1935
<https://github.com/apache/spark/pull/1935>, SPARK-2721
<https://issues.apache.org/jira/browse/SPARK-2721>, SPARK-3036
<https://issues.apache.org/jira/browse/SPARK-3036>, SPARK-3037
<https://issues.apache.org/jira/browse/SPARK-3037> and #1819
<https://github.com/apache/spark/pull/1819>

The reason I'm asking about the columnar compressed format is that


Can you elaborate?
"
Milos Nikolic <milos.nikolic83@gmail.com>,"Tue, 26 Aug 2014 10:01:00 +0200",Partitioning strategy changed in Spark 1.0.x?,dev@spark.apache.org,"Hi guys, 

Iíve noticed some changes in the behavior of partitioning under Spark 1.0.x. 
Iíd appreciate if someone could explain what has changed in the meantime. 

Here is a small example. I want to create two RDD[(K, V)] objects and then 
collocate partitions with the same K on one node. When the same partitioner 
for two RDDs is used, partitions with the same K end up being on different nodes.

    // Let's say I have 10 nodes 
    val partitioner = new HashPartitioner(10)     
    
    // Create RDD 
    val rdd = sc.parallelize(0 until 10).map(k => (k, computeValue(k))) 
    
    // Partition twice using the same partitioner 
    rdd.partitionBy(partitioner).foreach { case (k, v) => println(""Dummy1 -> k = "" + k) } 
    rdd.partitionBy(partitioner).foreach { case (k, v) => println(""Dummy2 -> k = "" + k) } 

The output on one node is: 
    Dummy1 -> k = 2 
    Dummy2 -> k = 7 

I was expecting to see the same keys on each node. That was happening under Spark 0.9.2, but not under Spark 1.0.x. 

Anyone has an idea what has changed in the meantime? Or how to get corresponding partitions on one node? 

Thanks in advance,
Milos
---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Tue, 26 Aug 2014 09:57:16 +0100",Re: Handling stale PRs,dev <dev@spark.apache.org>,"
Agree, this drives me crazy. It kills part of JIRA's usefulness. Spark
is blessed/cursed with incredible inbound load, but would love to
still see the project get this right-er than, say, Hadoop.


Stale JIRAs are a symptom, not a problem per se. I also want to see
the backlog cleared, but automatically closing doesn't help, if the
problem is too many JIRAs and not enough committer-hours to look at
them. Some noise gets closed, but some easy or important fixes may
disappear as well.


Completely agree. The solution is partly more supply of committer time
on JIRAs. But that is detracting from the work the committers
themselves want to do. More of the solution is reducing demand by
helping people create useful, actionable, non-duplicate JIRAs from the
start. Or encouraging people to resolve existing JIRAs and shepherding
those in.

Elsewhere, I've found people reluctant to close JIRA for fear of
offending or turning off contributors. I think the opposite is true.
There is nothing wrong with ""no"" or ""not now"" especially accompanied
with constructive feedback. Better to state for the record what is not
being looked at and why, than let people work on and open the same
JIRAs repeatedly.

I have also found in the past that a culture of tolerating eternal
JIRAs led people to file JIRAs in order to forget about a problem --
it's in JIRA, and it's ""in progress"", so it feels like someone else is
going to fix it later and so it can be forgotten now.

For what it's worth, I think these project and culture mechanics are
so important and it's my #1 concern for Spark at this stage. This
challenge exists so much more here, exactly because there is so much
potential. I'd love to help by trying to identify and close stale
JIRAs but am afraid that tagging them is just adding to the heap of
work.

---------------------------------------------------------------------


"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Tue, 26 Aug 2014 10:53:10 +0000",Gradient descent and runMiniBatchSGD,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

I've implemented back propagation algorithm using Gradient class and a simple update using Updater class. Then I run the algorithm with mllib's GradientDescent class. I have troubles in scaling out this implementation. I thought that if I partition my data into the number of workers then performance will increase, because each worker will run a step of gradient descent on its partition of data. But this does not happen and each worker seems to process all data (if miniBatchFraction == 1.0 as in mllib's logisic regression implementation). For me, this doesn't make sense, because then only single Worker will provide the same performance. Could someone elaborate on this and correct me if I am wrong. How can I scale out the algorithm with many Workers?

Best regards, Alexander
"
Gary Malouf <malouf.gary@gmail.com>,"Tue, 26 Aug 2014 07:43:15 -0400",CoHadoop Papers,"""dev@spark.apache.org"" <dev@spark.apache.org>","attempts to try to co-locate related data blocks.  He pointed to this
paper: http://www.vldb.org/pvldb/vol4/p575-eltabakh.pdf from 2011 on the
CoHadoop research and the performance improvements it yielded for
Map/Reduce jobs.

Would leveraging these ideas for writing data from Spark make sense/be
worthwhile?
"
Matthew Farrellee <matt@redhat.com>,"Tue, 26 Aug 2014 08:18:32 -0400",Re: Handling stale PRs,"Sean Owen <sowen@cloudera.com>, dev <dev@spark.apache.org>","
totally agree, this applies to patches as well as jiras. i'll add that 
projects who let things simply linger are missing an opportunity to 
engage their community.

spark should capitalize on its momentum to build a smoothly running 
community (vs not and accept an unbounded backlog as inevitable).



engagement in the community really needs to go both ways. it's 
reasonable for PRs that stop merging or have open comments that need 
resolution by the PRer to be loudly timed out. a similar thing goes for 
jiras, if there's a request for more information to resolve a bug and 
that information does not appear, half of the communication is gone and 
a loud time out is reasonable.

easy and important are in the eyes of the beholder. timeouts can go both 
ways. a jira or pr that has been around for a period of time (say 1/3 
the to-close timeout) should bump up for evaluation, hopefully resulting 
in few ""easy"" or ""important"" issues falling through the cracks.

fyi, i'm periodically going through the pyspark jiras, trying to 
reproduce issues, coalesce duplicates and ask for details. i've not been 
given any sort of permission to do this, i don't have any special 
position in the community to do this - in a well functioning community 
everyone should feel free to jump in and help.



saying no/not-yet is a vitally important piece of information.



well stated!



there's some value in these now-i-can-forget jira, though i'm not 
personally a fan. it can be good to keep them around and reachable by 
search, but they should be clearly marked as no/not-yet or something 
similar.



+1 concern and potential!


best,


matt

---------------------------------------------------------------------


"
Gary Malouf <malouf.gary@gmail.com>,"Tue, 26 Aug 2014 08:20:45 -0400",Re: CoHadoop Papers,"""dev@spark.apache.org"" <dev@spark.apache.org>","It appears support for this type of control over block placement is going
out in the next version of HDFS:
https://issues.apache.org/jira/browse/HDFS-2576



"
Kousuke Saruta <sarutak@oss.nttdata.co.jp>,"Tue, 26 Aug 2014 21:31:42 +0900",Re: too many CancelledKeyException throwed from ConnectionManager,"yao <yaoshengzhe@gmail.com>, dev@spark.apache.org","Hi Shengzhe

I faced to same situation.

I think, Connection and ConnectionManager have some race condition issues
and the error you mentioned may be caused by the issues.
Now I'm trying to resolve the issue in 
https://github.com/apache/spark/pull/2019.
Please check it out.

- Kousuke



---------------------------------------------------------------------


"
Madhu <madhu@madhu.com>,"Tue, 26 Aug 2014 06:22:09 -0700 (PDT)",Re: Handling stale PRs,dev@spark.incubator.apache.org,"Sean Owen wrote

Agreed. All of the problems mentioned in this thread are symptoms. There's
no shortage of talent and enthusiasm within the Spark community. The people
and the product are wonderful. The process: not so much. Spark has been
wildly successful, some growing pains are to be expected.

Given 100+ contributors, Spark is a big project. As with big data, big
projects can run into scaling issues. There's no magic to running a
successful big project, but it does require greater planning and discipline.
JIRA is great for issue tracking, but it's not a replacement for a project
plan. Quarterly releases are a great idea, everyone knows the schedule. What
we need is concise plan for each release with a clear scope statement.
Without knowing what is in scope and out of scope for a release, we end up
with a laundry list of things to do, but no clear goal. Laundry lists don't
scale well.

I don't mind helping with planning and documenting releases. This is
especially helpful for new contributors who don't know where to start. I
have done that successfully on many projects using Jira and Confluence, so I
know it can be done. To address immediate concerns of open PRs and
excessive, overlapping Jira issues, we probably have to create a meta issue
and assign resources to fix it. I don't mind helping with that also.



-----
--
Madhu
https://www.linkedin.com/in/msiddalingaiah
--

---------------------------------------------------------------------


"
Christopher Nguyen <ctn@adatao.com>,"Tue, 26 Aug 2014 07:40:05 -0700",Re: CoHadoop Papers,Gary Malouf <malouf.gary@gmail.com>,"Gary, do you mean Spark and HDFS separately, or Spark's use of HDFS?

If the former, Spark does support copartitioning.

Hadoop does also make attempts to collocate data, e.g., rack awareness. I'm
sure the paper makes useful contributions for its set of use cases.

Sent while mobile. Pls excuse typos etc.

"
chutium <teng.qiu@gmail.com>,"Tue, 26 Aug 2014 07:53:13 -0700 (PDT)","HiveContext, schemaRDD.printSchema get different dataTypes, feature
 or a bug? really strange and surprised...",dev@spark.incubator.apache.org,"is there any dataType auto convert or detect or something in HiveContext ?all
columns of a table is defined as string in hive metastoreone column is
total_price with values like 123.45, then this column will be recognized as
dataType Float in HiveContext...this is a feature or a bug? it really
surprised me... how is it implemented? if it is a feature, can i turn it
off? i want to get a schemaRDD with exactly the same datatype defined in
hive metadata, i know the column total_price should be float values, but
they must not be, and what happens if there is some broken line in my huge
CSV file? or maybe some total_price is 9,123.45 or $123.45 or
something==============================================================some
example for this in our env.MapR v3 cluster, newest spark github master
clone from yesterdaybuilt withsbt/sbt -Dhadoop.version=1.0.3-mapr-3.0.3
-Phive assemblyhive-site.xml
configured==============================================================spark-shell
scripts:val hiveContext = new
org.apache.spark.sql.hive.HiveContext(sc)hiveContext.sql(""use
our_live_db"")hiveContext.sql(""desc formatted
et_fullorders"").collect.foreach(println)......14/08/26 15:47:09 INFO
SparkContext: Job finished: collect at SparkPlan.scala:85, took 0.0305408
s[# col_name             data_type               comment             ][               
][sid                    string                  from deserializer  
][request_id             string                  from deserializer  
][*times_dq               string*                  from deserializer  
][*total_price            string*                  from deserializer  
][order_id               string                  from deserializer   ][               
][# Partition Information                 ][# col_name             data_type              
comment             ][                ][wt_date                string                 
None                ][country                string                  None               
][                ][# Detailed Table Information            ][Database:             
our_live_db            ][Owner:                 client02             
][CreateTime:            Fri Jan 31 12:23:40 CET 2014     ][LastAccessTime:       
UNKNOWN                  ][Protect Mode:          None                    
][Retention:             0                        ][Location:             
maprfs:/mapr/cluster01.xxx.net/common/external_tables/et_fullorders    
][Table Type:            EXTERNAL_TABLE           ][Table Parameters:              
][       EXTERNAL                TRUE                ][      
transient_lastDdlTime   1391167420          ][                ][# Storage
Information           ][SerDe Library:        
com.bizo.hive.serde.csv.CSVSerde         ][InputFormat:          
org.apache.hadoop.mapred.TextInputFormat         ][OutputFormat:         
org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat      
][Compressed:            No                       ][Num Buckets:          
-1                       ][Bucket Columns:        []                      
][Sort Columns:          []                       ][Storage Desc Params:           
][       separatorChar           ;                   ][      
serialization.format    1                   ]then, create a schemaRDD from
this tableval result = hiveContext.sql(""select sid, order_id, total_price,
times_dq from et_fullorders where wt_date='2014-04-14' and country='uk'
limit 5"")ok now, printSchema...scala> result.printSchemaroot |-- sid: string
(nullable = true) |-- order_id: string (nullable = true) |-- *total_price:
float* (nullable = true) |-- *times_dq: timestamp* (nullable =
true)total_price was STRING but now in schemaRDD is FLOATandtimes_dq, now is
TIMESTAMPreally strange and surprised...and more strange is:scala>
result.map(row => row.getString(2)).collect.foreach(println)i
got240.0045.8321.6795.83120.83butscala> result.map(row =>
row.getFloat(2)).collect.foreach(println)14/08/26 16:01:24 ERROR Executor:
Exception in task 0.0 in stage 9.0 (TID 8)java.lang.ClassCastException:
java.lang.String cannot be cast to java.lang.Float        at
scala.runtime.BoxesRunTime.unboxToFloat(BoxesRunTime.java:114)==============================================================btw,
files in this external table are gzipped csv files:14/08/26 15:49:56 INFO
HadoopRDD: Input split:
maprfs:/mapr/cluster01.xxx.net/common/external_tables/et_fullorders/wt_date=2014-04-14/country=uk/getFullOrders_2014-04-14.csv.gz:0+16990and
the data in it:scala>
result.collect.foreach(println)[5000000001402123123,12344000123454,240.00,2014-04-14
00:03:49.082000][5000000001402110123,12344000123455,45.83,2014-04-14
00:04:13.639000][5000000001402129123,12344000123458,21.67,2014-04-14
00:09:12.276000][5000000001402092123,12344000132457,95.83,2014-04-14
00:09:42.228000][5000000001402135123,12344000123460,120.83,2014-04-14
00:12:44.742000]we use CSVSerDe
https://drone.io/github.com/ogrodnek/csv-serde/files/target/csv-serde-1.1.2-0.11.0-all.jarmaybe
this is a reason?but why the 1st and 2nd column, will not be recognized as
bigint or double or something...?Thanks for any idea



--"
chutium <teng.qiu@gmail.com>,"Tue, 26 Aug 2014 07:54:35 -0700 (PDT)","HiveContext, schemaRDD.printSchema get different dataTypes, feature
 or a bug? really strange and surprised...",dev@spark.incubator.apache.org,"is there any dataType auto convert or detect or something in HiveContext ?

all columns of a table is defined as string in hive metastore

one column is total_price with values like 123.45, then this column will be
recognized as dataType Float in HiveContext...

this is a feature or a bug? it really surprised me... how is it implemented?
if it is a feature, can i turn it off? i want to get a schemaRDD with
exactly the same datatype defined in hive metadata, i know the column
total_price should be float values, but they must not be, and what happens
if there is some broken line in my huge CSV file? or maybe some total_price
is 9,123.45 or $123.45 or something

==============================================================

some example for this in our env.

MapR v3 cluster, newest spark github master clone from yesterday

built with
sbt/sbt -Dhadoop.version=1.0.3-mapr-3.0.3 -Phive assembly

hive-site.xml configured

==============================================================

spark-shell scripts:

val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)
hiveContext.sql(""use our_live_db"")
hiveContext.sql(""desc formatted et_fullorders"").collect.foreach(println)
...
...
14/08/26 15:47:09 INFO SparkContext: Job finished: collect at
SparkPlan.scala:85, took 0.0305408 s
[# col_name             data_type               comment             ]
[                ]
[sid                    string                  from deserializer   ]
[request_id             string                  from deserializer   ]
[*times_dq               string*                  from deserializer   ]
[*total_price            string*                  from deserializer   ]
[order_id               string                  from deserializer   ]
[                ]
[# Partition Information                 ]
[# col_name             data_type               comment             ]
[                ]
[wt_date                string                  None                ]
[country                string                  None                ]
[                ]
[# Detailed Table Information            ]
[Database:              our_live_db            ]
[Owner:                 client02              ]
[CreateTime:            Fri Jan 31 12:23:40 CET 2014     ]
[LastAccessTime:        UNKNOWN                  ]
[Protect Mode:          None                     ]
[Retention:             0                        ]
[Location:             
maprfs:/mapr/cluster01.xxx.net/common/external_tables/et_fullorders     ]
[Table Type:            EXTERNAL_TABLE           ]
[Table Parameters:               ]
[       EXTERNAL                TRUE                ]
[       transient_lastDdlTime   1391167420          ]
[                ]
[# Storage Information           ]
[SerDe Library:         com.bizo.hive.serde.csv.CSVSerde         ]
[InputFormat:           org.apache.hadoop.mapred.TextInputFormat         ]
[OutputFormat:         
org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat       ]
[Compressed:            No                       ]
[Num Buckets:           -1                       ]
[Bucket Columns:        []                       ]
[Sort Columns:          []                       ]
[Storage Desc Params:            ]
[       separatorChar           ;                   ]
[       serialization.format    1                   ]

then, create a schemaRDD from this table

val result = hiveContext.sql(""select sid, order_id, total_price, times_dq
from et_fullorders where wt_date='2014-04-14' and country='uk' limit 5"")

ok now, printSchema...

scala> result.printSchema
root
 |-- sid: string (nullable = true)
 |-- order_id: string (nullable = true)
 |-- *total_price: float* (nullable = true)
 |-- *times_dq: timestamp* (nullable = true)


total_price was STRING but now in schemaRDD is FLOAT
and
times_dq, now is TIMESTAMP

really strange and surprised...

and more strange is:

scala> result.map(row => row.getString(2)).collect.foreach(println)

i got
240.00
45.83
21.67
95.83
120.83

but

scala> result.map(row => row.getFloat(2)).collect.foreach(println)

14/08/26 16:01:24 ERROR Executor: Exception in task 0.0 in stage 9.0 (TID 8)
java.lang.ClassCastException: java.lang.String cannot be cast to
java.lang.Float
        at scala.runtime.BoxesRunTime.unboxToFloat(BoxesRunTime.java:114)

==============================================================

btw, files in this external table are gzipped csv files:
14/08/26 15:49:56 INFO HadoopRDD: Input split:
maprfs:/mapr/cluster01.xxx.net/common/external_tables/et_fullorders/wt_date=2014-04-14/country=uk/getFullOrders_2014-04-14.csv.gz:0+16990

and the data in it:

scala> result.collect.foreach(println)
[5000000001402123123,12344000123454,240.00,2014-04-14 00:03:49.082000]
[5000000001402110123,12344000123455,45.83,2014-04-14 00:04:13.639000]
[5000000001402129123,12344000123458,21.67,2014-04-14 00:09:12.276000]
[5000000001402092123,12344000132457,95.83,2014-04-14 00:09:42.228000]
[5000000001402135123,12344000123460,120.83,2014-04-14 00:12:44.742000]

we use CSVSerDe
https://drone.io/github.com/ogrodnek/csv-serde/files/target/csv-serde-1.1.2-0.11.0-all.jar

maybe this is a reason?

but why the 1st and 2nd column, will not be recognized as bigint or double
or something...?

Thanks for any idea




--

---------------------------------------------------------------------


"
RJ Nowling <rnowling@gmail.com>,"Tue, 26 Aug 2014 10:58:51 -0400",Re: Gradient descent and runMiniBatchSGD,"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Hi Alexander,

Can you post a link to the code?

RJ






-- 
em rnowling@gmail.com
c 954.496.2314
"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Tue, 26 Aug 2014 15:15:02 +0000",RE: Gradient descent and runMiniBatchSGD,RJ Nowling <rnowling@gmail.com>,"Hi, RJ

https://github.com/avulanov/spark/blob/neuralnetwork/mllib/src/main/scala/org/apache/spark/mllib/classification/NeuralNetwork.scala

Unit tests are in the same branch.

Alexander

From: RJ Nowling [mailto:rnowling@gmail.com]
Sent: Tuesday, August 26, 2014 6:59 PM
To: Ulanov, Alexander
Cc: dev@spark.apache.org
Subject: Re: Gradient descent and runMiniBatchSGD

Hi Alexander,

Can you post a link to the code?

RJ

On Tue, Aug 26, 2014 at 6:53 AM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
Hi,

I've implemented back propagation algorithm using Gradient class and a simple update using Updater class. Then I run the algorithm with mllib's GradientDescent class. I have troubles in scaling out this implementation. I thought that if I partition my data into the number of workers then performance will increase, because each worker will run a step of gradient descent on its partition of data. But this does not happen and each worker seems to process all data (if miniBatchFraction == 1.0 as in mllib's logisic regression implementation). For me, this doesn't make sense, because then only single Worker will provide the same performance. Could someone elaborate on this and correct me if I am wrong. How can I scale out the algorithm with many Workers?

Best regards, Alexander



--
em r314
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 26 Aug 2014 11:15:39 -0400",Re: Handling stale PRs,Patrick Wendell <pwendell@gmail.com>,"


Amen to this. Aiming for such a culture would set Spark apart from other
projects in a great way.

I've proposed several different solutions to ASF infra to streamline the


I've added myself as a watcher on those 2 INFRA issues. Sucks that the only
solution on offer right now requires basically polluting the commit history.

Short of moving Spark's repo to a non-ASF-managed GitHub account, do you
think another bot could help us manage the number of stale PRs?

I'm thinking a solution as follows might be very helpful:

   - Extend Spark QA / Jenkins to run on a weekly schedule and check for
   stale PRs. Let's say a stale PR is an open one that hasn't been updated in
   N months.
   - Spark QA maintains a list of known committers on its side.
   - During its weekly check of stale PRs, Spark QA takes the following
   action:
      - If the last person to comment on a PR was a committer, post to the
      PR asking for an update from the contributor.
      - If the last person to comment on a PR was a contributor, add the PR
      to a list. Email this list of *hanging PRs* out to the dev list on a
      weekly basis and ask committers to update them.
      - If the last person to comment on a PR was Spark QA asking the
      contributor to update it, then add the PR to a list. Email this
list of *abandoned
      PRs* to the dev list for the record (or for closing, if that becomes
      possible in the future).

This doesn't solve the problem of not being able to close PRs, but it does
help make sure no PR is left hanging for long.

What do you think? I'd be interested in implementing this solution if we
like it.

Nick
"
chutium <teng.qiu@gmail.com>,"Tue, 26 Aug 2014 08:29:44 -0700 (PDT)","Re: HiveContext, schemaRDD.printSchema get different dataTypes,
 feature or a bug? really strange and surprised...",dev@spark.incubator.apache.org,"oops, i tried on a managed table, column types will not be changed

so it is mostly due to the serde lib CSVSerDe
(https://github.com/ogrodnek/csv-serde/blob/master/src/main/java/com/bizo/hive/serde/csv/CSVSerde.java#L123)
or maybe CSVReader from opencsv?...

but if the columns are defined as string, no matter what type returned from
custom SerDe or CSVReader, they should be cast to string at the end right?

why do not use the schema from hive metadata directly?



--

---------------------------------------------------------------------


"
Gary Malouf <malouf.gary@gmail.com>,"Tue, 26 Aug 2014 11:37:25 -0400",Re: CoHadoop Papers,,"Christopher, can you expand on the co-partitioning support?

We have a number of spark SQL tables (saved in parquet format) that all
could be considered to have a common hash key.  Our analytics team wants to
do frequent joins across these different data-sets based on this key.  It
makes sense that if the data for each key across 'tables' was co-located on
the same server, shuffles could be minimized and ultimately performance
could be much better.

implementing this type of behavior though there are a lot of complications
to make it work I believe.



"
Patrick Wendell <pwendell@gmail.com>,"Tue, 26 Aug 2014 09:13:43 -0700","Submit to the ""Powered By Spark"" Page!","user@spark.apache.org, dev@spark.apache.org","Hi All,

I want to invite users to submit to the Spark ""Powered By"" page. This page
is a great way for people to learn about Spark use cases. Since Spark
activity has increased a lot in the higher level libraries and people often
ask who uses each one, we'll include information about which components
each organization uses as well. If you are interested, simply respond to
this e-mail (or e-mail me off-list) with:

1) Organization name
2) URL
3) Which Spark components you use: Core, SQL, Streaming, MLlib, GraphX
4) A 1-2 sentence description of your use case.

I'll post any new entries here:
https://cwiki.apache.org/confluence/display/SPARK/Powered+By+Spark

- Patrick
"
Xiangrui Meng <mengxr@gmail.com>,"Tue, 26 Aug 2014 09:54:31 -0700",Re: Gradient descent and runMiniBatchSGD,"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","miniBatchFraction uses RDD.sample to get the mini-batch, and sample
still needs to visit the elements one after another. So it is not
efficient if the task is not computation heavy and this is why
setMiniBatchFraction is marked as experimental. If we can detect that
the partition iterator is backed by an ArrayBuffer, maybe we can do a
skip iterator to skip elements. -Xiangrui

/org/apache/spark/mllib/classification/NeuralNetwork.scala
mple update using Updater class. Then I run the algorithm with mllib's GradientDescent class. I have troubles in scaling out this implementation. I thought that if I partition my data into the number of workers then performance will increase, because each worker will run a step of gradient descent on its partition of data. But this does not happen and each worker seems to process all data (if miniBatchFraction == 1.0 as in mllib's logisic regression implementation). For me, this doesn't make sense, because then only single Worker will provide the same performance. Could someone elaborate on this and correct me if I am wrong. How can I scale out the algorithm with many Workers?

---------------------------------------------------------------------


"
Josh Rosen <rosenville@gmail.com>,"Tue, 26 Aug 2014 11:21:29 -0700",Re: Handling stale PRs,"Nicholas Chammas <nicholas.chammas@gmail.com>, Patrick Wendell
 <pwendell@gmail.com>","Last weekend, I started hacking on a Google App Engine app for helping with pull request review (screenshot:¬†http://i.imgur.com/wwpZKYZ.png). ¬†Some of my basic goals (not all implemented yet):

- Users sign in using GitHub and can browse a list of pull requests, including links to associated JIRAs, Jenkins statuses, a quick preview of the last comment, etc.

- Pull requests are auto-classified based on which components they modify (by looking at the diff).

- From the app‚Äôs own internal database of PRs, we can build dashboards to find ‚Äúabandoned‚Äù PRs, graph average time to first review, etc.

- Since we authenticate users with GitHub, we can enable administrative functions via this dashboard (e.g. ‚Äúassign this PR to me‚Äù, ‚Äúvote to close in the weekly auto-close commit‚Äù, etc.

Right now, I‚Äôve implemented GItHub OAuth support and code to update the issues database using the GitHub API. ¬†Because we have access to the full API, it‚Äôs pretty easy to do fancy things like parsing the reason for Jenkins failure, etc. ¬†You could even imagine some fancy mashup tools to pull up JIRAs and pull requests side-by in iframes.

After I hack on this a bit more, I plan to release a public preview version; if we find this tool useful, I‚Äôll clean it up and open-source the app so folks can contribute to it.

- Josh


rote:  

tips  


Amen to this. Aiming for such a culture would set Spark apart from other  
projects in a great way.  

I've proposed several different solutions to ASF infra to streamline the  


I've added myself as a watcher on those 2 INFRA issues. Sucks that the only  
solution on offer right now requires basically polluting the commit history.  

Short of moving Spark's repo to a non-ASF-managed GitHub account, do you  
think another bot could help us manage the number of stale PRs?  

I'm thinking a solution as follows might be very helpful:  

- Extend Spark QA / Jenkins to run on a weekly schedule and check for  
stale PRs. Let's say a stale PR is an open one that hasn't been updated in  
N months.  
- Spark QA maintains a list of known committers on its side.  
- During its weekly check of stale PRs, Spark QA takes the following  
action:  
- If the last person to comment on a PR was a committer, post to the  
PR asking for an update from the contributor.  
- If the last person to comment on a PR was a contributor, add the PR  
to a list. Email this list of *hanging PRs* out to the dev list on a  
weekly basis and ask committers to update them.  
- If the last person to comment on a PR was Spark QA asking the  
contributor to update it, then add the PR to a list. Email this  
list of *abandoned  
PRs* to the dev list for the record (or for closing, if that becomes  
possible in the future).  

This doesn't solve the problem of not being able to close PRs, but it does  
help make sure no PR is left hanging for long.  

What do you think? I'd be interested in implementing this solution if we  
like it.  

Nick  
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 26 Aug 2014 14:37:28 -0400",Re: Handling stale PRs,Josh Rosen <rosenville@gmail.com>,"OK, that sounds pretty cool.

Josh,

Do you see this app as encompassing or supplanting the functionality I
described as well?

Nick



oards to
 etc.
, ‚Äúvote to close
te the
for
ource the
n
s
"
Josh Rosen <rosenville@gmail.com>,"Tue, 26 Aug 2014 11:40:27 -0700",Re: Handling stale PRs,Nicholas Chammas <nicholas.chammas@gmail.com>,"Sure; App Engine supports cron and sending emails. ¬†We can configure the app with Spark QA‚Äôs credentials in order to allow it to post comments on issues, etc.

- Josh


OK, that sounds pretty cool.

Josh,

Do you see this app as encompassing or supplanting the functionality I described as well?

Nick


e:
Last weekend, I started hacking on a Google App Engine app for helping with pull request review (screenshot:¬†http://i.imgur.com/wwpZKYZ.png). ¬†Some of my basic goals (not all implemented yet):

- Users sign in using GitHub and can browse a list of pull requests, including links to associated JIRAs, Jenkins statuses, a quick preview of the last comment, etc.

- Pull requests are auto-classified based on which components they modify (by looking at the diff).

- From the app‚Äôs own internal database of PRs, we can build dashboards to find ‚Äúabandoned‚Äù PRs, graph average time to first review, etc.

- Since we authenticate users with GitHub, we can enable administrative functions via this dashboard (e.g. ‚Äúassign this PR to me‚Äù, ‚Äúvote to close in the weekly auto-close commit‚Äù, etc.

Right now, I‚Äôve implemented GItHub OAuth support and code to update the issues database using the GitHub API. ¬†Because we have access to the full API, it‚Äôs pretty easy to do fancy things like parsing the reason for Jenkins failure, etc. ¬†You could even imagine some fancy mashup tools to pull up JIRAs and pull requests side-by in iframes.

After I hack on this a bit more, I plan to release a public preview version; if we find this tool useful, I‚Äôll clean it up and open-source the app so folks can contribute to it.

- Josh


rote:

tips


Amen to this. Aiming for such a culture would set Spark apart from other
projects in a great way.

I've proposed several different solutions to ASF infra to streamline the


I've added myself as a watcher on those 2 INFRA issues. Sucks that the only
solution on offer right now requires basically polluting the commit history.

Short of moving Spark's repo to a non-ASF-managed GitHub account, do you
think another bot could help us manage the number of stale PRs?

I'm thinking a solution as follows might be very helpful:

- Extend Spark QA / Jenkins to run on a weekly schedule and check for
stale PRs. Let's say a stale PR is an open one that hasn't been updated in
N months.
- Spark QA maintains a list of known committers on its side.
- During its weekly check of stale PRs, Spark QA takes the following
action:
- If the last person to comment on a PR was a committer, post to the
PR asking for an update from the contributor.
- If the last person to comment on a PR was a contributor, add the PR
to a list. Email this list of *hanging PRs* out to the dev list on a
weekly basis and ask committers to update them.
- If the last person to comment on a PR was Spark QA asking the
contributor to update it, then add the PR to a list. Email this
list of *abandoned
PRs* to the dev list for the record (or for closing, if that becomes
possible in the future).

This doesn't solve the problem of not being able to close PRs, but it does
help make sure no PR is left hanging for long.

What do you think? I'd be interested in implementing this solution if we
like it.

Nick

"
npanj <nitinpanj@gmail.com>,"Tue, 26 Aug 2014 11:56:07 -0700 (PDT)","Re: [SPARK-2878] Kryo serialisation with custom Kryo registrator
 failing",dev@spark.incubator.apache.org,"I have both SPARK-2878 and SPARK-2893. 



--

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 26 Aug 2014 16:16:02 -0400",spark-ec2 1.0.2 creates EC2 cluster at wrong version,dev <dev@spark.apache.org>,"I downloaded the source code release for 1.0.2 from here
<http://spark.apache.org/downloads.html> and launched an EC2 cluster using
spark-ec2.

After the cluster finishes launching, I fire up the shell and check the
version:

scala> sc.version
res1: String = 1.0.1

The startup banner also shows the same thing. Hmm...

So I dig around and find that the spark_ec2.py script has the default Spark
version set to 1.0.1.

Derp.

  parser.add_option(""-v"", ""--spark-version"", default=""1.0.1"",
      help=""Version of Spark to use: 'X.Y.Z' or a specific git hash"")

Is there any way to fix the release? It‚Äôs a minor issue, but could be very
confusing. And how can we prevent this from happening again?

Nick
‚Äã
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Tue, 26 Aug 2014 13:24:17 -0700",Re: spark-ec2 1.0.2 creates EC2 cluster at wrong version,Nicholas Chammas <nicholas.chammas@gmail.com>,"This is a chicken and egg problem in some sense. We can't change the ec2
script till we have made the release and uploaded the binaries -- But once
that is done, we can't update the script.

I think the model we support so far  is that you can launch the latest
spark version from the master branch on github. I guess we can try to add
something in the release process that updates the script but doesn't commit
it ? The release managers might be able to add more.

Thanks
Shivaram



g
rk
d be very
"
RJ Nowling <rnowling@gmail.com>,"Tue, 26 Aug 2014 16:36:23 -0400",Re: Gradient descent and runMiniBatchSGD,Xiangrui Meng <mengxr@gmail.com>,"Xiangrui,

I posted a note on my JIRA for MiniBatch KMeans about the same problem --
sampling running in O(n).

Can you elaborate on ways to get more efficient sampling?  I think this
will be important for a variety of stochastic algorithms.

RJ






-- 
em rnowling@gmail.com
c 954.496.2314
"
RJ Nowling <rnowling@gmail.com>,"Tue, 26 Aug 2014 16:37:34 -0400",Re: Gradient descent and runMiniBatchSGD,Xiangrui Meng <mengxr@gmail.com>,"Also, another idea: may algorithms that use sampling tend to do so multiple
times.  It may be beneficial to allow a transformation to a representation
that is more efficient for multiple rounds of sampling.






-- 
em rnowling@gmail.com
c 954.496.2314
"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Tue, 26 Aug 2014 20:49:48 +0000",Re: Gradient descent and runMiniBatchSGD,Xiangrui Meng <mengxr@gmail.com>,"Hi Xiangrui,

Thanks for explanation, but I'm still missing something. In my experiments, if miniBatchFraction == 1.0, no matter how the data is partitioned (2, 4, 8, 16 partitions), the algorithm executes more or less in the same time. (I have 16 Workers). Reduce from runMiniBatchSGD takes most of the time for 2 partitions, mapPartitionWithIndex -- for 16. What I would expect is that the time reduces proportional to the number of data partitions because each partition will be processed on separate Worker hopefully. Why the time does not reduce?

Btw processing of one instance in my algorithm is a heavy computation, this is exact reason why I want to parallelize it.

Best regards, Alexander

26.08.2014, ◊ 20:54, ""Xiangrui Meng"" <mengxr@gmail.com<mailto:mengxr@gmail.com>> Œ¡–…”¡Ã(¡):

miniBatchFraction uses RDD.sample to get the mini-batch, and sample
still needs to visit the elements one after another. So it is not
efficient if the task is not computation heavy and this is why
setMiniBatchFraction is marked as experimental. If we can detect that
the partition iterator is backed by an ArrayBuffer, maybe we can do a
skip iterator to skip elements. -Xiangrui

Hi, RJ

https://github.com/avulanov/spark/blob/neuralnetwork/mllib/src/main/scala/org/apache/spark/mllib/classification/NeuralNetwork.scala

Unit tests are in the same branch.

Alexander

From: RJ Nowling [mailto:rnowling@gmail.com]
Sent: Tuesday, August 26, 2014 6:59 PM
To: Ulanov, Alexander
Cc: dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: Gradient descent and runMiniBatchSGD

Hi Alexander,

Can you post a link to the code?

RJ

Hi,

I've implemented back propagation algorithm using Gradient class and a simple update using Updater class. Then I run the algorithm with mllib's GradientDescent class. I have troubles in scaling out this implementation. I thought that if I partition my data into the number of workers then performance will increase, because each worker will run a step of gradient descent on its partition of data. But this does not happen and each worker seems to process all data (if miniBatchFraction == 1.0 as in mllib's logisic regression implementation). For me, this doesn't make sense, because then only single Worker will provide the same performance. Could someone elaborate on this and correct me if I am wrong. How can I scale out the algorithm with many Workers?

Best regards, Alexander



--
em rnowling@gmail.com<mailto:rnowling@gmail.com><mailto:rnowling@gmail.com>
c 954.496.2314

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 26 Aug 2014 18:15:39 -0400",Re: Handling stale PRs,Josh Rosen <rosenville@gmail.com>,"By the way, as a reference point, I just stumbled across the Discourse
GitHub project and their list of pull requests
<https://github.com/discourse/discourse/pulls> looks pretty neat.

~2,200 closed PRs, 6 open. Least recently updated PR dates to 8 days ago.
Project started ~1.5 years ago.

Dunno how many committers Discourse has, but it looks like they've managed
their PRs well. I hope we can do as well in this regard as they have.

Nick



ents on
f
hboards
iew, etc.
ù, ‚Äúvote to close
date the
l
 for
source the
in
es
"
Michael Armbrust <michael@databricks.com>,"Tue, 26 Aug 2014 15:50:13 -0700",Re: CoHadoop Papers,Gary Malouf <malouf.gary@gmail.com>,"It seems like there are two things here:
 - Co-locating blocks with the same keys to avoid network transfer.
 - Leveraging partitioning information to avoid a shuffle when data is
already partitioned correctly (even if those partitions aren't yet on the
same machine).

The former seems more complicated and probably requires the support from
Hadoop you linked to.  However, the latter might be easier as there is
already a framework for reasoning about partitioning and the need to
shuffle in the Spark SQL planner.



"
yao <yaoshengzhe@gmail.com>,"Tue, 26 Aug 2014 16:43:13 -0700",Re: too many CancelledKeyException throwed from ConnectionManager,Kousuke Saruta <sarutak@oss.nttdata.co.jp>,"Wow, great job. We will take a look and try our application again with your
patch.



"
Gary Malouf <malouf.gary@gmail.com>,"Tue, 26 Aug 2014 20:19:08 -0400",Re: CoHadoop Papers,Michael Armbrust <michael@databricks.com>,"Hi Michael,

I think once that work is into HDFS, it will be great to expose this
functionality via Spark.  This is something worth pursuing because it could
grant orders of magnitude perf improvements in cases when people need to
join data.

The second item would be very interesting, could yield significant
performance boosts.

Best,

Gary



"
jay vyas <jayunit100.apache@gmail.com>,"Tue, 26 Aug 2014 20:49:50 -0400",OutOfMemoryError when running sbt/sbt test,dev@spark.apache.org,"Hi spark.

I've been trying to build spark, but I've been getting lots of oome
exceptions.

https://gist.github.com/jayunit100/d424b6b825ce8517d68c

For the most part, they are of the form:

java.lang.OutOfMemoryError: unable to create new native thread

I've attempted to hard code the ""get_mem_opts"" function, which is in the
sbt-launch-lib.bash file, to
have various very high parameter sizes (i.e. -Xms5g"") with high
MaxPermSize, etc... and to no avail.

Any thoughts on this would be appreciated.

I know of others having the same problem as well.

Thanks!

-- 
jay vyas
"
Mubarak Seyed <spark.devuser@gmail.com>,"Tue, 26 Aug 2014 17:58:08 -0700",Re: OutOfMemoryError when running sbt/sbt test,jay vyas <jayunit100.apache@gmail.com>,"What is your ulimit value?



"
Matei Zaharia <matei.zaharia@gmail.com>,"Tue, 26 Aug 2014 18:39:30 -0700",Re: spark-ec2 1.0.2 creates EC2 cluster at wrong version,"shivaram@eecs.berkeley.edu, Nicholas Chammas
 <nicholas.chammas@gmail.com>","This shouldn't be a chicken-and-egg problem, since the script fetches the AMI from a known URL. Seems like an issue in publishing this release.


This is a chicken and egg problem in some sense. We can't change the ec2  
script till we have made the release and uploaded the binaries -- But once  
that is done, we can't update the script.  

I think the model we support so far is that you can launch the latest  
spark version from the master branch on github. I guess we can try to add  
something in the release process that updates the script but doesn't commit  
it ? The release managers might be able to add more.  

Thanks  
Shivaram  



ing  
  
 Spark  
"",  
could be very  
"
Jay Vyas <jayunit100.apache@gmail.com>,"Tue, 26 Aug 2014 22:34:53 -0400",Re: OutOfMemoryError when running sbt/sbt test,Mubarak Seyed <spark.devuser@gmail.com>,"Thanks...! Some questions below.

1) you are suggesting that maybe this OOME is a symptom/red herring , and the true cause of it is that a thread can't span because of ulimit... If so possibly this could be flagged early on in the build.  And -- where are so many threads coming from that I need to up my limit?   Is this a new feature added to spark recently, and if so will it effect deployments scenarios as well?

And 

2) possibly SBT_OPTS is where the memory settings should be ? If so, then why do we have the get_mem_opts wrapper function coded to send memory manually as Xmx/Xms options?
  execRunner ""$java_cmd"" \
    ${SBT_OPTS:-$default_sbt_opts} \
    $(get_mem_opts $sbt_mem) \
    ${java_opts} \
    ${java_args[@]} \
    -jar ""$sbt_jar"" \
    ""${sbt_commands[@]}"" \
    ""${residual_args[@]}""



:
rote:
"
Anand Avati <avati@gluster.org>,"Tue, 26 Aug 2014 20:01:16 -0700",Re: OutOfMemoryError when running sbt/sbt test,jay vyas <jayunit100.apache@gmail.com>,"Hi Jay,
The recommended way to build spark from source is through the maven system.
You would want to follow the steps in
https://spark.apache.org/docs/latest/building-with-maven.html to set the
MAVEN_OPTS to prevent OOM build errors.

Thanks



"
Madhu <madhu@madhu.com>,"Tue, 26 Aug 2014 20:11:23 -0700 (PDT)",Re: Handling stale PRs,dev@spark.incubator.apache.org,"Nicholas Chammas wrote

Discourse developers appear to  eat their own dog food
<https://meta.discourse.org>  .
Improved collaboration and a shared vision might be a reason for their
success.




-----
--
Madhu
https://www.linkedin.com/in/msiddalingaiah
--

---------------------------------------------------------------------


"
Tathagata Das <tathagata.das1565@gmail.com>,"Tue, 26 Aug 2014 20:16:36 -0700",Re: spark-ec2 1.0.2 creates EC2 cluster at wrong version,Matei Zaharia <matei.zaharia@gmail.com>,"Yes, this was an oversight on my part. I have opened a JIRA for this.
https://issues.apache.org/jira/browse/SPARK-3242

For the time being the workaround should be providing the version 1.0.2
explicitly as part of the script.

TD



e
it
uld be
"
"""Lizhengbing (bing, BIPA)"" <zhengbing.li@huawei.com>","Wed, 27 Aug 2014 06:58:55 +0000","reply: I want to contribute MLlib two quality measures(ARHR and HR)
 for top N recommendation system. Is this meaningful?",Xiangrui Meng <mengxr@gmail.com>,"In fact,  prec@k is similar to HR and ndcg@k is similar to ARHR
After my study, I cannot find a best measure to evaluate recommendation system

Xiangrui, do you think it is reasonable to create a class to provide popular measures for evaluating recommendation system?

Popular measures of recommendation system include precision, coverage, diversity‚Ä¶
Most measures can be found in the book(Recommender_systems_handbook)




Âèë‰ª∂‰∫∫: Xiangrui Meng [mailto:mengxr@gmail.com]
ÂèëÈÄÅÊó∂Èó¥: 2014Âπ¥8Êúà26Êó• 3:28
Êî∂‰ª∂‰∫∫: Lizhengbing (bing, BIPA)
ÊäÑÈÄÅ: dev@spark.apache.org
‰∏ªÈ¢ò: Re: I want to contribute MLlib two quality measures(ARHR and HR) for top N recommendation system. Is this meaningful?

The evaluation metrics are definitely useful. How do they differ from traditional IR metrics like prec@k and ndcg@k? -Xiangrui

On Mon, Aug 25, 2014 at 2:14 AM, Lizhengbing (bing, BIPA) <zhengbing.li@huawei.com<mailto:zhengbing.li@huawei.com>> wrote:
Hi:
In paper ‚ÄúItem-Based Top-N Recommendation Algorithms‚Äù(https://stuyresearch.googlecode.com/hg/blake/resources/10.1.1.102.4451.pdf), there are two parameters measuring the quality of recommendation: HR and ARHR.
If I use ALS(Implicit) for top-N recommendation system, I want to check it‚Äôs quality. ARHR and HR are two good quality measures.
I want to contribute them to spark MLlib.  So I want to know whether this is meaningful?


(1) If n is the total number of customers/users,  the hit-rate of the recommendation algorithm was computed as
hit-rate (HR) = Number of hits / n

(2)If h is the number of hits that occurred at positions p1, p2, . . . , ph within the top-N lists (i.e., 1 ‚â§ pi ‚â§ N), then the average reciprocal hit-rank is equal to:
i
.

"
RJ Nowling <rnowling@gmail.com>,"Wed, 27 Aug 2014 09:04:49 -0400",Re: Contributing to MLlib: Proposal for Clustering Algorithms,Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"Hi Yu,

A standardized API has not been implemented yet.  I think it would be
better to implement the other clustering algorithms then extract a common
API.  Others may feel differently.  :)

Just a note, there was a pre-existing JIRA for hierarchical KMeans
SPARK-2429 <https://issues.apache.org/jira/browse/SPARK-2429> I filed.  I
added a comment about previous discussion on the mailing list, example code
provided by a Jeremy Freeman, and a couple of papers I found.

Feel free to take this over -- I've played with it but haven't had time to
finish it.  I'd be happy to review the resulting code and discuss
approaches with you.

RJ






-- 
em rnowling@gmail.com
c 954.496.2314
"
Jeremy Freeman <freeman.jeremy@gmail.com>,"Wed, 27 Aug 2014 12:18:46 -0400",Re: Contributing to MLlib: Proposal for Clustering Algorithms,RJ Nowling <rnowling@gmail.com>,"Hey RJ,

Sorry for the delay, I'd be happy to take a look at this if you can post the code!

I think splitting the largest cluster in each round is fairly common, but ideally it would be an option to do it one way or the other.

-- Jeremy

---------------------
jeremy freeman, phd
neuroscientist
@thefreemanlab


I
it
be
clustering, and
algorithms,
class
help
http://apache-spark-developers-list.1001551.n3.nabble.com/Contributing-to-MLlib-Proposal-for-Clustering-Algorithms-tp7212p7398.html

"
Jeremy Freeman <freeman.jeremy@gmail.com>,"Wed, 27 Aug 2014 12:18:46 -0400",Re: Contributing to MLlib: Proposal for Clustering Algorithms,RJ Nowling <rnowling@gmail.com>,"Hey RJ,

Sorry for the delay, I'd be happy to take a look at this if you can post the code!

I think splitting the largest cluster in each round is fairly common, but ideally it would be an option to do it one way or the other.

-- Jeremy

---------------------
jeremy freeman, phd
neuroscientist
@thefreemanlab


I
it
be
clustering, and
algorithms,
class
help
http://apache-spark-developers-list.1001551.n3.nabble.com/Contributing-to-MLlib-Proposal-for-Clustering-Algorithms-tp7212p7398.html

"
RJ Nowling <rnowling@gmail.com>,"Wed, 27 Aug 2014 12:27:00 -0400",Re: Contributing to MLlib: Proposal for Clustering Algorithms,Jeremy Freeman <freeman.jeremy@gmail.com>,"Thanks, Jeremy.  I'm abandoning my initial approach, and I'll work on
optimizing your example (so it doesn't do the breeze-vector conversions
every time KMeans is called).  I need to finish a few other projects first,
though, so it may be a couple weeks.

In the mean time, Yu also created a JIRA for a hierarchical KMeans
implementation.  I pointed him to your example and a couple papers I found.

If you or Yu beat me to getting an implementation in, I'd be happy to
review it.  :)





-- 
em rnowling@gmail.com
c 954.496.2314
"
RJ Nowling <rnowling@gmail.com>,"Wed, 27 Aug 2014 12:27:00 -0400",Re: Contributing to MLlib: Proposal for Clustering Algorithms,Jeremy Freeman <freeman.jeremy@gmail.com>,"Thanks, Jeremy.  I'm abandoning my initial approach, and I'll work on
optimizing your example (so it doesn't do the breeze-vector conversions
every time KMeans is called).  I need to finish a few other projects first,
though, so it may be a couple weeks.

In the mean time, Yu also created a JIRA for a hierarchical KMeans
implementation.  I pointed him to your example and a couple papers I found.

If you or Yu beat me to getting an implementation in, I'd be happy to
review it.  :)





-- 
em rnowling@gmail.com
c 954.496.2314
"
Reynold Xin <rxin@databricks.com>,"Wed, 27 Aug 2014 13:56:55 -0700",Re: Adding support for a new object store,Rajendran Appavu <apprajen@in.ibm.com>,"Hi Rajendran,

I'm assuming you have some concept of schema and you are intending to
integrate with SchemaRDD instead of normal RDDs.

More responses inline below.




You can create a new RDD type for a new storage system, and you can create
a new table scan operator in sql to read.



Right now the best way to do this is to hack the sql strategies, which does
some predicate pushdown into table scan:
https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala

We are in the process of proposing an API that allows external data stores
to hook into the planner. Expect a design proposal in early/mid Sept.

a good idea to start prototyping by hacking the planner, and migrate to the
planner hook API once that is ready.



Executors by default actually don't hold any data in memory. Spark requires
explicit caching of data, i.e. it's only when rdd.cache() is called then
will Spark executors put the content of that RDD in-memory. The executor
has a thing called BlockManager that does eviction based on LRU.



"
Cheng Lian <lian.cs.zju@gmail.com>,"Wed, 27 Aug 2014 14:06:52 -0700",Re: RDD replication in Spark,rapelly kartheek <kartheek.mbms@gmail.com>,"You may start from here
<https://github.com/apache/spark/blob/4fa2fda88fc7beebb579ba808e400113b512533b/core/src/main/scala/org/apache/spark/storage/BlockManager.scala#L706-L712>
.
‚Äã



le
e
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 27 Aug 2014 17:11:01 -0400",Re: Handling stale PRs,Josh Rosen <rosenville@gmail.com>,"

BTW Josh, how can we stay up-to-date on your work on this tool? A JIRA
issue, perhaps?

Nick
"
RJ Nowling <rnowling@gmail.com>,"Wed, 27 Aug 2014 17:17:59 -0400","[GraphX] JIRA / PR to fix breakage in GraphGenerator.logNormalGraph
 in PR #720","""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,

 PR #720 <https://github.com/apache/spark/pull/720> made multiple changes
to GraphGenerator.logNormalGraph including:

   - Replacing the call to functions for generating random vertices and
   edges with in-line implementations with different equations. Based on
   reading the Pregel paper, I believe the in-line functions are incorrect.
   - Hard-coding of RNG seeds so that method now generates the same graph
   for a given number of vertices, edges, mu, and sigma -- user is not able to
   override seed or specify that seed should be randomly generated.
   - Backwards-incompatible change to logNormalGraph signature with
   introduction of new required parameter.
   - Failed to update scala docs and programming guide for API changes
   - Added a Synthetic Benchmark in the examples.

I submitted JIRA SPARK-3263
<https://issues.apache.org/jira/browse/SPARK-3263> and PR #2168
<https://github.com/apache/spark/pull/2168> to revert some of these changes
and fix usage of the RNGs:

   - Removes the in-line calls and calls original vertex / edge generation
   functions again
   - Adds an optional seed parameter for deterministic behavior (when
   desired)
   - Keeps the number of partitions parameter that was added.
   - Keeps compatibility with the synthetic benchmark example
   - Maintains backwards-compatible API

 I would appreciate feedback and people taking a look.  :)

Thanks!
RJ

-- 
em rnowling@gmail.com
c 954.496.2314
"
Reynold Xin <rxin@databricks.com>,"Wed, 27 Aug 2014 14:22:07 -0700",Re: Adding support for a new object store,Rajendran Appavu <apprajen@in.ibm.com>,"Linking to the JIRA tracking APIs to hook into the planner:
https://issues.apache.org/jira/browse/SPARK-3248





"
Nishkam Ravi <nravi@cloudera.com>,"Wed, 27 Aug 2014 14:36:26 -0700",Re: Handling stale PRs,Nicholas Chammas <nicholas.chammas@gmail.com>,"Wonder if it would make sense to introduce a notion of 'Reviewers' as an
intermediate tier to help distribute the load? While anyone can review and
comment on an open PR, reviewers would be able to say aye or nay subject to
confirmation by a committer?

Thanks,
Nishkam



"
Cheng Lian <lian.cs.zju@gmail.com>,"Wed, 27 Aug 2014 15:30:57 -0700","Re: HiveContext, schemaRDD.printSchema get different dataTypes,
 feature or a bug? really strange and surprised...",chutium <teng.qiu@gmail.com>,"I believe in your case, the ‚Äúmagic‚Äù happens in TableReader.fillObject
<https://github.com/apache/spark/blob/4fa2fda88fc7beebb579ba808e400113b512533b/core/src/main/scala/org/apache/spark/storage/BlockManager.scala#L706-L712>.
Here we unwrap the field value according to the object inspector of that
field. It seems that somehow a FloatObjectInspector is specified for the
total_price field. I don‚Äôt think CSVSerde is responsible for this, since it
sets all field object inspectors to javaStringObjectInspector (here
<https://github.com/ogrodnek/csv-serde/blob/f315c1ae4b21a8288eb939e7c10f3b29c1a854ef/src/main/java/com/bizo/hive/serde/csv/CSVSerde.java#L59-L61>
).

Which version of Spark SQL are you using? If you are using a snapshot
version, please provide the exact Git commit hash. Thanks!
‚Äã



hive/serde/csv/CSVSerde.java#L123
om
?
emaRDD-printSchema-get-different-dataTypes-feature-or-a-bug-really-strange-and-surpri-tp8035p8039.html
"
Patrick Wendell <pwendell@gmail.com>,"Wed, 27 Aug 2014 15:39:31 -0700",Re: Handling stale PRs,Nishkam Ravi <nravi@cloudera.com>,"Hey Nishkam,

To some extent we already have this process - many community members
help review patches and some earn a reputation where committer's will
take an LGTM from them seriously. I'd be interested in seeing if any
other projects recognize people who do this.

- Patrick


---------------------------------------------------------------------


"
Josh Rosen <rosenville@gmail.com>,"Wed, 27 Aug 2014 15:48:07 -0700",Re: Handling stale PRs,Nicholas Chammas <nicholas.chammas@gmail.com>,"I have a very simple dashboard running at¬†http://spark-prs.appspot.com/. ¬†Currently, this mirrors the functionality of Patrick‚Äôs github-shim, but it should be very easy to extend with other features.

The source is at¬†https://github.com/databricks/spark-pr-dashboard¬†(pull requests and issues welcome!)


e:
Last weekend, I started hacking on a Google App Engine app for helping with pull request review (screenshot:¬†http://i.imgur.com/wwpZKYZ.png).

BTW Josh, how can we stay up-to-date on your work on this tool? A JIRA issue, perhaps?

Nick"
shane knapp <sknapp@berkeley.edu>,"Wed, 27 Aug 2014 16:13:38 -0700","jenkins maintenance/downtime, aug 28th, 730am-9am PDT","dev@spark.apache.org, amp-infra <amp-infra@googlegroups.com>","tomorrow morning i will be upgrading jenkins to the latest/greatest (1.577).

at 730am, i will put jenkins in to a quiet period, so no new builds will be
accepted.  once any running builds are finished, i will be taking jenkins
down for the upgrade.

depending on what and how many jobs are running, i'm expecting this to
take, at most, an hour.

i'll send out an update tomorrow morning right before i begin, and will
send out updates and an all-clear once we're up and running again.

1.577 release notes:
http://jenkins-ci.org/changelog

please let me know if there are any questions/concerns.  thanks in advance!

shane
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 27 Aug 2014 19:33:17 -0400",Re: Handling stale PRs,Josh Rosen <rosenville@gmail.com>,"Alright! That was quick. :)



im, but it
:
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 27 Aug 2014 19:46:38 -0400","Re: jenkins maintenance/downtime, aug 28th, 730am-9am PDT",shane knapp <sknapp@berkeley.edu>,"Looks like we're currently at 1.568 so we should be getting a nice slew of
UI tweaks and bug fixes. Neat!



"
Nishkam Ravi <nravi@cloudera.com>,"Wed, 27 Aug 2014 18:26:34 -0700",Re: Handling stale PRs,Patrick Wendell <pwendell@gmail.com>,"I see. Yeah, it would be interesting to know if any other project has
considered formalizing this notion. It may also enable assignment of
reviews (potentially automated using Josh's system) and maybe anonymity as
come without certain undesired side-effects.

Thanks,
Nishkam



"
Mayur Rustagi <mayur.rustagi@gmail.com>,"Thu, 28 Aug 2014 10:33:25 +0530",Update on Pig on Spark initiative,"""user@spark.apache.org"" <user@spark.apache.org>, dev <dev@spark.apache.org>","Hi,
We have migrated Pig functionality on top of Spark passing 100% e2e for
success cases in pig test suite. That means UDF, Joins & other
functionality is working quite nicely. We are in the process of merging
with Apache Pig trunk(something that should happen over the next 2 weeks).
Meanwhile if you are interested in giving it a go, you can try it at
https://github.com/sigmoidanalytics/spork
This contains all the major changes but may not have all the patches
required for 100% e2e, if you are trying it out let me know any issues you
face

Whole bunch of folks contributed on this

Julien Le Dem (Twitter),  Praveen R (Sigmoid Analytics), Akhil Das (Sigmoid
Analytics), Bill Graham (Twitter), Dmitriy Ryaboy (Twitter), Kamal Banga
(Sigmoid Analytics), Anish Haldiya (Sigmoid Analytics),  Aniket Mokashi
 (Google), Greg Owen (DataBricks), Amit Kumar Behera (Sigmoid Analytics),
Mahesh Kalakoti (Sigmoid Analytics)

Not to mention Spark & Pig communities.

Regards
Mayur Rustagi
Ph: +1 (760) 203 3257
http://www.sigmoidanalytics.com
@mayur_rustagi <https://twitter.com/mayur_rustagi>
"
Matei Zaharia <matei.zaharia@gmail.com>,"Wed, 27 Aug 2014 22:11:36 -0700",Re: Update on Pig on Spark initiative,"""=?utf-8?Q?user=40spark.apache.org?="" <user@spark.apache.org>, dev
 <dev@spark.apache.org>, Mayur Rustagi <mayur.rustagi@gmail.com>","Awesome to hear this, Mayur! Thanks for putting this together.

Matei


Hi,
We have migrated Pig functionality on top of Spark passing 100% e2e for success cases in pig test suite. That means UDF, Joins & other functionality is working quite nicely. We are in the process of merging with Apache Pig trunk(something that should happen over the next 2 weeks).¬†
Meanwhile if you are interested in giving it a go, you can try it at¬†https://github.com/sigmoidanalytics/spork
This contains all the major changes but may not have all the patches required for 100% e2e, if you are trying it out let me know any issues you face

Whole bunch of folks contributed on this¬†

Julien Le Dem (Twitter), ¬†Praveen R (Sigmoid Analytics), Akhil Das (Sigmoid Analytics), Bill Graham (Twitter), Dmitriy Ryaboy (Twitter), Kamal Banga (Sigmoid Analytics), Anish Haldiya (Sigmoid Analytics), ¬†Aniket Mokashi ¬†(Google), Greg Owen (DataBricks),¬†Amit Kumar Behera (Sigmoid Analytics), Mahesh Kalakoti (Sigmoid Analytics)

Not to mention Spark & Pig communities.¬†

Regards
Mayur Rustagi
Ph: +1 (760) 203 3257
http://www.sigmoidanalytics.com
@mayur_rustagi

"
wenchen <cloud0fan@outlook.com>,"Wed, 27 Aug 2014 22:20:59 -0700 (PDT)",[Spark SQL] query nested structure data,dev@spark.incubator.apache.org,"I am going to dig into this issue:
https://issues.apache.org/jira/browse/SPARK-2096
However, I noticed that there is already a NestedSqlParser in sql/core/test
org.apache.spark.sql.parquet. 
I checked this parser and it could solve the issue I mentioned before. But
why the author of the parser mark it as temporarily? Does this parser break
some spark sql grammar which I haven't noticed?



--

---------------------------------------------------------------------


"
Graham Dennis <graham.dennis@gmail.com>,"Thu, 28 Aug 2014 16:51:20 +1000",Preferred Executor launch path?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,

In the process of trying to resolve SPARK-3166 (inability to ship custom
serialisers in application jars)
https://issues.apache.org/jira/browse/SPARK-3166 I've discovered that
there's a bit of duplicated code for building the command for launching
Executors across SparkDeploySchedulerBackend.scala,
MesosSchedulerBackend.scala, and CoarseMesosSchedulerBackend.scala

Importantly, there is a slight difference in their behaviour where
SparkDeploySchedulerBackend doesn't launch the Executor with the
spark-class script, but instead tries to do something similar in
CommandUtils.scala.  MesosSchedulerBackend.scala and
CoarseMesosSchedulerBackend.scala both use the spark-class script.  Is the
latter the preferred approach?  So should I refactor all of these to use
spark-class, or is there a reason for the differing behaviour?

Secondly, the goal of SPARK-3166 is to have the user jar available to the
executor process at launch time (rather than when the first task is
received).  I'd like to get some feedback on what the preferred classpath
order should be.  The items to be ordered to determine the classpath are:

* The output of the compute-classpath script
* The config option spark.executor.extraClassPath
* The application jar (and anything added via SparkContext.addJar)

Complicating the matter is that the 'deploy' backend currently supports the
""spark.files.userClassPathFirst"" option, but this is not supported by the
Mesos backends (and I don't think it's supported by the YARN backend).

Ignoring the ""userClassPathFirst"" option, the current behaviour for the
classpath is effectively:
1. The output of compute-classpath
2. The config option spark.executor.extraClassPath
3. The application jar (and anything added via SparkContext.addJar).

What should the preferred order be if userClassPathFirst is true?
 Currently the behaviour for the Deploy backend is effectively:
1. The application jar (and anything added via SparkContext.addJar)
2. The output of compute-classpath
3. The config option spark.executor.extraClassPath

To me it makes more sense for this to be in the order (application jar;
spark.executor.extraClassPath; compute-classpath).  Agree? Disagree?

Thanks,
Graham
"
"""=?UTF-8?B?5rSq5aWH?="" <qiping.lqp@alibaba-inc.com>","Thu, 28 Aug 2014 16:26:05 +0800","=?UTF-8?B?ZGVsZXRlZDogICAgc3FsL2hpdmUvc3JjL3Rlc3QvcmVzb3VyY2VzL2dvbGRlbi9jYXNlIHNl?=
  =?UTF-8?B?bnNpdGl2aXR5IG9uIHdpbmRvd3M=?=","""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

I want to contribute some code to mllib, I forked apache/spark to my own repository (chouqin/spark), 
and used `git clone https://github.com/chouqin/spark.git` to checkout the code my windows system.
In this directory, I aster
Your branch is up-to-date with 'origin/master'.

Changes not staged for commit:
(use ""git add/rm <file>..."" to update what will be committed)
(use ""git checkout -- <file>..."" to discard changes in working directory)

deleted: sql/hive/src/test/resources/golden/case sensitivity: Hive table-0-5d14d21a239daa42b086cc895215009a
``` 

I don't know why because nothing has been done. If I want to make some change, I have to be careful not to commit this deletion of file,
This is every inconvenient for me because I always use `git add .` to stage all changes, now I have to add every file individually.

Can someone give me any suggestions to deal with this, my system is Windows 7 and git version is 1.9.2.msysgit.0.
Thanks for your help.Qiping"
Cheng Lian <lian.cs.zju@gmail.com>,"Thu, 28 Aug 2014 01:47:11 -0700",Re: deleted: sql/hive/src/test/resources/golden/case sensitivity on windows,=?UTF-8?B?5rSq5aWH?= <qiping.lqp@alibaba-inc.com>,"Colon is not allowed to be part of a Windows file name and I think Git just
cannot create this file while cloning. Remove the colon in the name string
of this test case
<https://github.com/chouqin/spark/blob/76e3ba4264c4a0bc2c33ae6ac862fc40bc302d83/sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/HiveQuerySuite.scala#L312>
should solve the problem.

Would you mind to file a JIRA and a PR to fix this?
‚Äã



"
shane knapp <sknapp@berkeley.edu>,"Thu, 28 Aug 2014 07:19:01 -0700","Re: jenkins maintenance/downtime, aug 28th, 730am-9am PDT","dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>","reminder:  this is starting in 10 minutes



"
shane knapp <sknapp@berkeley.edu>,"Thu, 28 Aug 2014 07:46:06 -0700","Re: jenkins maintenance/downtime, aug 28th, 730am-9am PDT","dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>","jenkins is now coming down.



"
shane knapp <sknapp@berkeley.edu>,"Thu, 28 Aug 2014 07:51:57 -0700","Re: jenkins maintenance/downtime, aug 28th, 730am-9am PDT","dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>","jenkins is upgraded, but a few jobs sneaked in before i could do the plugin
updates.  i've put jenkins in quiet mode again, and once the spark builds
finish, i'll restart jenkins to enable the plugin updates and we'll be good
to go.

let's all take a moment to bask in the glory of the shiny new UI!  :)



"
Patrick Wendell <pwendell@gmail.com>,"Thu, 28 Aug 2014 09:02:31 -0700",[VOTE] Release Apache Spark 1.1.0 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version 1.1.0!

The tag to be voted on is v1.1.0-rc1 (commit f0718324):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=f07183249b74dd857069028bf7d570b35f265585

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-1.1.0-rc1/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1028/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-1.1.0-rc1-docs/

Please vote on releasing this package as Apache Spark 1.1.0!

The vote is open until Sunday, August 31, at 17:00 UTC and passes if
a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.1.0
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

== What justifies a -1 vote for this release? ==
This vote is happening very late into the QA period compared with
previous votes, so -1 votes should only occur for significant
regressions from 1.0.2. Bugs already present in 1.0.X will not block
this release.

== What default changes should I be aware of? ==
1. The default value of ""spark.io.compression.codec"" is now ""snappy""
--> Old behavior can be restored by switching to ""lzf""

2. PySpark now performs external spilling during aggregations.
--> Old behavior can be restored by setting ""spark.shuffle.spill"" to ""false"".

I'll send a bit more later today with feature information for the
release. In the mean time I want to put this out there for
consideration.

- Patrick

---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Thu, 28 Aug 2014 09:12:11 -0700","Re: jenkins maintenance/downtime, aug 28th, 730am-9am PDT","dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>","this one job is blocking the jenkins restart:
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/19406/

i'm about to kill it so that i can get this done.  i'll restart the job
after jenkins is back up.



"
shane knapp <sknapp@berkeley.edu>,"Thu, 28 Aug 2014 09:15:58 -0700","Re: jenkins maintenance/downtime, aug 28th, 730am-9am PDT","dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>","all clear:  jenkins and all plugins have been updated!



"
Mridul Muralidharan <mridul@gmail.com>,"Thu, 28 Aug 2014 21:57:17 +0530",Re: [VOTE] Release Apache Spark 1.1.0 (RC1),Patrick Wendell <pwendell@gmail.com>,"Is SPARK-3277 applicable to 1.1 ?
If yes, until it is fixed, I am -1 on the release (I am on break, so can't
verify or help fix, sorry).

Regards
Mridul

"
Reynold Xin <rxin@databricks.com>,"Thu, 28 Aug 2014 09:46:16 -0700","Re: jenkins maintenance/downtime, aug 28th, 730am-9am PDT",shane knapp <sknapp@berkeley.edu>,"Thanks for doing this, Shane.


"
Josh Rosen <rosenville@gmail.com>,"Thu, 28 Aug 2014 10:00:02 -0700","Re: deleted: sql/hive/src/test/resources/golden/case
 sensitivity on windows","Cheng Lian <lian.cs.zju@gmail.com>, =?utf-8?Q?=E6=B4=AA=E5=A5=87?=
 <qiping.lqp@alibaba-inc.com>","RE: building Spark on Windows: earlier this week, I tried running the Maven build on Windows 8 using the master branch and ran into a few issues. ¬†I‚Äôve opened a PR to fix them¬†https://github.com/apache/spark/pull/2165.

ote:
Colon is not allowed to be part of a Windows file name and I think Git just  
cannot create this file while cloning. Remove the colon in the name string  
of this test case  
<https://github.com/chouqin/spark/blob/76e3ba4264c4a0bc2c33ae6ac862fc40bc302d83/sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/HiveQuerySuite.scala#L312>  
should solve the problem.  

Would you mind to file a JIRA and a PR to fix this?  
‚Äã  



n  
ut the  
put  
ctory)  
 
to  
"
shane knapp <sknapp@berkeley.edu>,"Thu, 28 Aug 2014 10:00:10 -0700","Re: jenkins maintenance/downtime, aug 28th, 730am-9am PDT",amp-infra <amp-infra@googlegroups.com>,"no problem!

also, i retriggered:
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/19406
it's currently:
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/19411



"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Thu, 28 Aug 2014 10:27:16 -0700","New SparkR mailing list, JIRA","""user@spark.apache.org"" <user@spark.apache.org>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Hi

I'd like to announce a couple of updates to the SparkR project. In order to
facilitate better collaboration for new features and development we have a
new mailing list, issue tracker for SparkR.

- The new JIRA is hosted at https://sparkr.atlassian.net/browse/SPARKR/ and
we have migrated all existing Github issues to the JIRA. Please submit any
bugs / improvements to this JIRA going forward.

- There is a new mailing list sparkr-dev@googlegroups.com that will be used
for design discussions for new features and development related issues. We
will still be answering to user issues on Apache Spark mailing lists.

Please let me know if have any questions.

Thanks
Shivaram
"
Patrick Wendell <pwendell@gmail.com>,"Thu, 28 Aug 2014 13:00:58 -0700",Re: [VOTE] Release Apache Spark 1.1.0 (RC1),Mridul Muralidharan <mridul@gmail.com>,"Mridul - thanks for sending this along and for the debugging comments
on the JIRA. I think we have a handle on the issue and we'll patch it
and spin a new RC. We can also update the test coverage to cover LZ4.

- Patrick


---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Thu, 28 Aug 2014 13:03:51 -0700","""emergency"" jenkins restart, aug 29th, 730am-9am PDT -- plus a postmortem","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","as with all software upgrades, sometimes things don't always work as
expected.

a recent change to stapler[1], to verbosely
report NotExportableExceptions[2] is spamming our jenkins log file with
stack traces, which is growing rather quickly (1.2G since 9am).  this has
been reported to the jenkins jira[3], and a fix has been pushed and will be
rolled out ""soon""[4].

this isn't affecting any builds, and jenkins is happily humming along.

in the interim, so that we don't run out of disk space, i will be
redirecting the jenkins logs tommorow morning to /dev/null for the long
weekend.

once a real fix has been released, i will update any packages needed and
redirect the logging back to the log file.

other than a short downtime, this will have no user-facing impact.

please let me know if you have any questions/concerns.

thanks for your patience!

shane ""the new guy""  :)

[1] -- https://wiki.jenkins-ci.org/display/JENKINS/Architecture
[2] --
https://github.com/stapler/stapler/commit/ed2cb8b04c1514377f3a8bfbd567f050a67c6e1c
[3] --
https://issues.jenkins-ci.org/browse/JENKINS-24458?focusedCommentId=209247
[4] --
https://github.com/stapler/stapler/commit/e2b39098ca1f61a58970b8a41a3ae79053cf30e3
"
Mridul Muralidharan <mridul@gmail.com>,"Fri, 29 Aug 2014 03:57:23 +0530",Re: [VOTE] Release Apache Spark 1.1.0 (RC1),Patrick Wendell <pwendell@gmail.com>,"Thanks for being on top of this Patrick ! And apologies for not being able
to help more.

Regards,
Mridul

"
Bill Bejeck <bbejeck@gmail.com>,"Thu, 28 Aug 2014 18:52:29 -0400",Jira tickets for starter tasks,dev@spark.apache.org,"Hi,

How do I get a starter task jira ticket assigned to myself? Or do I just do
the work and issue a pull request with the associated jira number?

Thanks,
Bill
"
Cheng Lian <lian.cs.zju@gmail.com>,"Thu, 28 Aug 2014 15:55:14 -0700",Re: Jira tickets for starter tasks,Bill Bejeck <bbejeck@gmail.com>,"You can just start the work :)



"
Josh Rosen <rosenville@gmail.com>,"Thu, 28 Aug 2014 15:56:56 -0700",Re: Jira tickets for starter tasks,"Bill Bejeck <bbejeck@gmail.com>, dev@spark.apache.org","A JIRA admin needs to add you to the ‚Äò‚ÄôContributors‚Äù role group in order to allow you to assign issues to yourself. ¬†I‚Äôve added this email address to that group, so you should be set!

- Josh


:

Hi,  

How do I get a starter task jira ticket assigned to myself? Or do I just do  
the work and issue a pull request with the associated jira number?  

Thanks,  
Bill  
"
HongQi <qiping.lqp@alibaba-inc.com>,"Thu, 28 Aug 2014 18:14:50 -0700 (PDT)","Re: deleted: sql/hive/src/test/resources/golden/case sensitivity on
 windows",dev@spark.incubator.apache.org,"OK, I will create a PR to fix this. thanks for your comments.



--

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Thu, 28 Aug 2014 19:12:40 -0700",Re: [VOTE] Release Apache Spark 1.1.0 (RC1),Mridul Muralidharan <mridul@gmail.com>,"Okay I'm cancelling this vote in favor of RC2.


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Thu, 28 Aug 2014 19:14:28 -0700",[VOTE] Release Apache Spark 1.1.0 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version 1.1.0!

The tag to be voted on is v1.1.0-rc2 (commit 711aebb3):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=711aebb329ca28046396af1e34395a0df92b5327

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-1.1.0-rc2/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1029/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-1.1.0-rc2-docs/

Please vote on releasing this package as Apache Spark 1.1.0!

The vote is open until Monday, September 01, at 03:11 UTC and passes if
a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.1.0
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

== Regressions fixed since RC1 ==
LZ4 compression issue: https://issues.apache.org/jira/browse/SPARK-3277

== What justifies a -1 vote for this release? ==
This vote is happening very late into the QA period compared with
previous votes, so -1 votes should only occur for significant
regressions from 1.0.2. Bugs already present in 1.0.X will not block
this release.

== What default changes should I be aware of? ==
1. The default value of ""spark.io.compression.codec"" is now ""snappy""
--> Old behavior can be restored by switching to ""lzf""

2. PySpark now performs external spilling during aggregations.
--> Old behavior can be restored by setting ""spark.shuffle.spill"" to ""false"".

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Thu, 28 Aug 2014 20:32:11 -0700",Re: [VOTE] Release Apache Spark 1.1.0 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","I'll kick off the vote with a +1.


---------------------------------------------------------------------


"
Burak Yavuz <byavuz@stanford.edu>,"Thu, 28 Aug 2014 20:53:29 -0700 (PDT)",Re: [VOTE] Release Apache Spark 1.1.0 (RC2),Patrick Wendell <pwendell@gmail.com>,"+1. Tested MLlib algorithms on Amazon EC2, algorithms show speed-ups between 1.5-5x compared to the 1.0.2 release.



---------------------------------------------------------------------



----------------------------------------------------------------"
Timothy Chen <tnachen@gmail.com>,"Thu, 28 Aug 2014 21:27:16 -0700",Re: [VOTE] Release Apache Spark 1.1.0 (RC2),Burak Yavuz <byavuz@stanford.edu>,"+1 Make-distrubtion works, and also tested simple spark jobs on Spark
on Mesos on 8 node Mesos cluster.

Tim


---------------------------------------------------------------------


"
Cheng Lian <lian.cs.zju@gmail.com>,"Thu, 28 Aug 2014 21:57:59 -0700",Re: [VOTE] Release Apache Spark 1.1.0 (RC2),Timothy Chen <tnachen@gmail.com>,"+1. Tested Spark SQL Thrift server and CLI against a single node standalone
cluster.



"
Sean Owen <sowen@cloudera.com>,"Fri, 29 Aug 2014 07:04:56 +0100",Re: [VOTE] Release Apache Spark 1.1.0 (RC2),Patrick Wendell <pwendell@gmail.com>,"+1 I tested the source and Hadoop 2.4 release. Checksums and
signatures are OK. Compiles fine with Java 8 on OS X. Tests... don't
fail any more than usual.

FWIW I've also been using the 1.1.0-SNAPSHOT for some time in another
project and have encountered"
Patrick Wendell <pwendell@gmail.com>,"Thu, 28 Aug 2014 23:26:42 -0700",Re: [VOTE] Release Apache Spark 1.1.0 (RC2),Sean Owen <sowen@cloudera.com>,"Hey Sean,

The reason there are no longer CDH-specific builds is that all newer
versions of CDH and HDP work with builds for the upstream Hadoop
projects. I dropped CDH4 in favor of a  newer Hadoop version (2.4) and
the Hadoop-without-Hive (also 2.4) build.

For MapR - we can't officially post those artifacts on ASF web space
when we make the final release, we can only link to them as being
hosted by MapR specifically since they use non-compatible licenses.
However, I felt that providing these during a testing period was
alright, with the goal of increasing test coverage. I couldn't find
any policy against posting these on personal web space during RC
voting. However, we can remove them if there is one.

Dropping CDH4 was more because it is now pretty old, but we can add it
back if people want. The binary packaging is a slightly separate
question from release votes, so I can always add more binary packages
whenever. And on this, my main concern is covering the most popular
Hadoop versions to lower the bar for users to build and test Spark.

- Patrick


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Fri, 29 Aug 2014 07:30:21 +0100",Re: [VOTE] Release Apache Spark 1.1.0 (RC2),Patrick Wendell <pwendell@gmail.com>,"(Copying my reply since I don't know if it goes to the mailing list)

Great, thanks for explaining the reasoning. You're saying these aren't
going into the final release? I think that moots any issue surrounding
distributing them then.

This is all I know of from the ASF:
https://community.apache.org/projectIndependence.html I don't read it
as expressly forbidding this kind of thing although you can see how it
bumps up against the spirit. There's not a bright line -- what about
Tomcat providing binaries compiled for Windows for example? does that
favor an OS vendor?

what you want with snapshots and RCs. The only issue there is maybe
releasing something different than was in the RC; is that at all
confusing? Just needs a note.

I think this theoretical issue doesn't exist if these binaries aren't
released, so I see no reason to not proceed.

The rest is a different question about whether you want to spend time
maintaining this profile and candidate. The vendor already manages
their build I think and -- and I don't know -- may even prefer not to
have a different special build floating around. There's also the
theoretical argument that this turns off other vendors from adopting
Spark if it's perceived to be too connected to other vendors. I'd like
to maximize Spark's distribution and there's some argument you do this
by not making vendor profiles. But as I say a different question to
just think about over time...

(oh and PS for my part I think it's a good thing that CDH4 binaries
were removed. I wasn't arguing for resurrecting them)


---------------------------------------------------------------------


"
Evan Chan <velvia.github@gmail.com>,"Thu, 28 Aug 2014 23:31:29 -0700",Re: [Spark SQL] off-heap columnar store,Michael Armbrust <michael@databricks.com>,"
Sure.

- Organization or co has no Hadoop, but significant investment in some
other NoSQL store.
- Need to efficiently add a new column to existing data
- Need to mark some existing rows as deleted or replace small bits of
existing data

For these use cases, it would be much more efficient and practical if
we didn't have to take the origin of the data from the datastore,
convert it to Parquet first.  Doing so loses significant latency and
causes Ops headaches in having to maintain HDFS.     It would be great
to be able to load data directly into the columnar format, into the
InMemoryColumnarCache.

---------------------------------------------------------------------


"
Matei Zaharia <matei.zaharia@gmail.com>,"Thu, 28 Aug 2014 23:39:42 -0700",Re: [VOTE] Release Apache Spark 1.1.0 (RC2),"Sean Owen <sowen@cloudera.com>, Patrick Wendell
 <pwendell@gmail.com>","Personally I'd actually consider putting CDH4 back if there are still users on it. It's always better to be inclusive, and the convenience of a one-click download is high. Do we have a sense on what % of CDH users still use CDH4?

Matei


(Copying my reply since I don't know if it goes to the mailing list) 

Great, thanks for explaining the reasoning. You're saying these aren't 
going into the final release? I think that moots any issue surrounding 
distributing them then. 

This is all I know of from the ASF: 
https://community.apache.org/projectIndependence.html I don't read it 
as expressly forbidding this kind of thing although you can see how it 
bumps up against the spirit. There's not a bright line -- what about 
Tomcat providing binaries compiled for Windows for example? does that 
favor an OS vendor? 

what you want with snapshots and RCs. The only issue there is maybe 
releasing something different than was in the RC; is that at all 
confusing? Just needs a note. 

I think this theoretical issue doesn't exist if these binaries aren't 
released, so I see no reason to not proceed. 

The rest is a different question about whether you want to spend time 
maintaining this profile and candidate. The vendor already manages 
their build I think and -- and I don't know -- may even prefer not to 
have a different special build floating around. There's also the 
theoretical argument that this turns off other vendors from adopting 
Spark if it's perceived to be too connected to other vendors. I'd like 
to maximize Spark's distribution and there's some argument you do this 
by not making vendor profiles. But as I say a different question to 
just think about over time... 

(oh and PS for my part I think it's a good thing that CDH4 binaries 
were removed. I wasn't arguing for resurrecting them) 


--------------------------------------------------------------------- 

"
Patrick Wendell <pwendell@gmail.com>,"Thu, 28 Aug 2014 23:42:54 -0700",Re: [VOTE] Release Apache Spark 1.1.0 (RC2),Sean Owen <sowen@cloudera.com>,"Yeah, we can't/won't post MapR binaries on the ASF web space for the
release. However, I have been linking to them (at their request) with
a clear identifier that it is an incompatible license and a 3rd party
build.

The only ""vendor specific"" build property we provide is compatibility
with different Hadoop FileSystem clients, since unfortunately there is
not a universally adopted client/server protocol. I think our goal has
always been to provide a path for using ""ASF Spark"" with
vendor-specific filesystems. Some vendors perform backports or
enhancements... and this of course we would never want to manage in
the upstream project.

In terms of vendor support for this approach - In the early days
Cloudera asked us to add CDH4 repository and more recently Pivotal and
MapR also asked us to allow linking against their hadoop-client
libraries. So we've added these based on direct requests from vendors.
Given the ubiquity of the Hadoop FileSystem API, it's hard for me to
imagine ruffling feathers by supporting this. But if we get feedback
in that direction over time we can of course consider a different
approach.

- Patrick




---------------------------------------------------------------------


"
Zhan Zhang <zhazhan@gmail.com>,"Thu, 28 Aug 2014 23:51:01 -0700 (PDT)",RE: Working Formula for Hive 0.13?,dev@spark.incubator.apache.org,"I have preliminary patch against spark1.0.2, which is attached to spark-2706.
Now I am working on supporting both hive-0.12 and hive-0.13.1 with
non-intrusive way (not breaking any existing hive-0.12 when introduce
supporting new version). I will attach a proposal to solve multi-version
support issue to spark-2706 soon.

Thanks.

Zhan Zhang



--

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Fri, 29 Aug 2014 07:56:34 +0100",Re: [VOTE] Release Apache Spark 1.1.0 (RC2),Patrick Wendell <pwendell@gmail.com>,"
By this, you mean that it's easy to control the Hadoop version in the
build and set it to some other vendor-specific release? Yes that seems
ideal. Making the build flexible, and adding the repository references
to pom.xml is part of enabling that -- to me, no question that's good.

So you can always roll your own build for your cluster, if you need
to. I understand the role of the cdh4 / mapr3 / mapr4 binaries as just
a convenience.

But it's a convenience for people who...
- are installing Spark on a cluster (i.e. not an end user)
- that doesn't have it in their distro already
- whose distro isn't compatible with a plain vanilla Hadoop distro

That can't be many. CDH4.6+ is most of the installed CDH base and it
already has Spark. I thought MapR already had Spark built in. The
audience seems small enough, and the convenience relatively small
enough (is it hard to run the distribution script?) that it caused me
to ask whether it was worth bothering providing these, especially give
the possible ASF sensitivity.

I say crack on; you get my point.

---------------------------------------------------------------------


"
Andrew Ash <andrew@andrewash.com>,"Fri, 29 Aug 2014 00:57:17 -0700",Re: [VOTE] Release Apache Spark 1.1.0 (RC2),Sean Owen <sowen@cloudera.com>,"FWIW we use CDH4 extensively and would very much appreciate having a
prebuilt version of Spark for it.

We're doing a CDH 4.4 to 4.7 upgrade across all the clusters now and have
plans for a 5.x transition after that.

"
Archit Thakur <archit279thakur@gmail.com>,"Fri, 29 Aug 2014 14:03:36 +0530",,"dev@spark.incubator.apache.org, user@spark.incubator.apache.org","Hi,

My requirement is to run Spark on Yarn without using the script
spark-submit.

I have a servlet and a tomcat server. As and when request comes, it creates
a new SC and keeps it alive for the further requests, I ma setting my
master in sparkConf

as sparkConf.setMaster(""yarn-cluster"")

but the request is stuck indefinitely.

This works when I set
sparkConf.setMaster(""yarn-client"")

I am not sure, why is it not launching job in yarn-cluster mode.

Any thoughts?

Thanks and Regards,
Archit Thakur.
"
Archit Thakur <archit279thakur@gmail.com>,"Fri, 29 Aug 2014 15:06:34 +0530",,"dev@spark.incubator.apache.org, user@spark.incubator.apache.org, 
	user@spark.apache.org","including user@spark.apache.org.



"
Koert Kuipers <koert@tresata.com>,"Fri, 29 Aug 2014 09:42:09 -0400",Re: [VOTE] Release Apache Spark 1.1.0 (RC2),Andrew Ash <andrew@andrewash.com>,"i suspect there are more cdh4 than cdh5 clusters. most people plan to move
to cdh5 within say 6 months.



"
"""Chester @work"" <chester@alpinenow.com>","Fri, 29 Aug 2014 06:58:57 -0700",,Archit Thakur <archit279thakur@gmail.com>,"Archit
     We are using yarn-cluster mode , and calling spark via Client class directly from servlet server. It works fine. 
    To establish a communication channel to give further requests, 
     It should be possible with yarn client, but not with yarn server. Yarn client mode, spark driver is outside the yarn cluster; so it can issue more commands. In yarn cluster, all programs including spark driver is running inside the yarn cluster. There is no communication channel with the client until the job finishes.

If you job is to keep spark context alive, and wait for other commands, then this should wait forever. 

I am actually working on some improvements on this and experiment in our product, I will create PRs when I feel conformable with the solution

1) change Client API to allow the caller to know yarn app resource capacity before passing arguments
2) add YarnApplicationListener to the Client 
3) provide communication channel between application and spark Yarn client in cluster. 

The #1) is not directly related to the communication discussed here

#2) allows the application to have application life cycle call back as to app start end in progress failure etc with yarn resources allocations 

I changed #1 and #2 in forked spark, and it's worked well in cdh5, and I am testing against 2.0.5-alpha as well. 

For #3) I did not change in spark currently, as I am not sure the best approach yet. I put the change in the application runner which launch the spark yarn client in the cluster. 

The runner in yarn cluster get applications host and port information  from the passed configuration (args), then creates an Akka actor using spark context actor system, send a hand shake message to the caller outside the cluster, after that you will have a two way communications 

With this approach, I can send spark listener call backs to the app, error messages, app level messages etc. 

The runner inside the cluster can also receive requests from outside cluster such as stop. 

We are not sure Akka approach is the best, so I am still experimenting it. So far it does what we wants .

Hope this helps

Chester


Sent from my iPhone

te:
mit.
es a new SC and keeps it alive for the further requests, I ma setting my master in sparkConf
"
shane knapp <sknapp@berkeley.edu>,"Fri, 29 Aug 2014 07:32:32 -0700","Re: ""emergency"" jenkins restart, aug 29th, 730am-9am PDT -- plus a postmortem","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","reminder:   this is happening right now.  jenkins is currently in quiet
mode, and in ~30 minutes, will be briefly going down.



"
Madhu <madhu@madhu.com>,"Fri, 29 Aug 2014 08:06:46 -0700 (PDT)",Re: Jira tickets for starter tasks,dev@spark.incubator.apache.org,"Cheng Lian-2 wrote

Given 100+ contributors, starting work without a JIRA issue assigned to you
could lead to duplication of effort by well meaning people that have no idea
they are working on the same issue. This does happen and I don't think it's
a good thing.

Just my $0.02



-----
--
Madhu
https://www.linkedin.com/in/msiddalingaiah
--

---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Fri, 29 Aug 2014 08:19:01 -0700","Re: ""emergency"" jenkins restart, aug 29th, 730am-9am PDT -- plus a postmortem","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","this is done.



"
Ye Xianjin <advancedxy@gmail.com>,"Fri, 29 Aug 2014 19:02:22 +0800",Re: [VOTE] Release Apache Spark 1.1.0 (RC2),Matei Zaharia <matei.zaharia@gmail.com>,"We just used CDH 4.7 for our production cluster. And I believe we won't use CDH 5 in the next year.

Sent from my iPhone

s on it. It's always better to be inclusive, and the convenience of a one-click download is high. Do we have a sense on what % of CDH users still use CDH4?
te: 



13a73cc 

mbox/%3CCANC1h_u%3DN0YKFu3pDaEVYz5ZcQtjQnXEjQA2ReKmoS%2Bye7%3Do%3DA%40mail.gmail.com%3E)
rote: 
n 1.1.0! 
1aebb329ca28046396af1e34395a0df92b5327 
 
 
 
 


alse"". 


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Fri, 29 Aug 2014 09:17:53 -0700",Re: [VOTE] Release Apache Spark 1.1.0 (RC2),Ye Xianjin <advancedxy@gmail.com>,"Okay I'll plan to add cdh4 binary as well for the final release!

---
sent from my phone

ll
:
3a73cc
t
ox/%3CCANC1h_u%3DN0YKFu3pDaEVYz5ZcQtjQnXEjQA2ReKmoS%2Bye7%3Do%3DA%40mail.gmail.com%3E
ebb329ca28046396af1e34395a0df92b5327
-
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 29 Aug 2014 12:24:06 -0400",Re: [VOTE] Release Apache Spark 1.1.0 (RC2),Patrick Wendell <pwendell@gmail.com>,"There were several formatting and typographical errors in the SQL docs that
I've fixed in this PR <https://github.com/apache/spark/pull/2201>. Dunno if
we want to roll that into the release.



.zaharia@gmail.com>
 a
t
g
t
e
s
d
it
s
t
er
t
3a73cc
e
t
ox/%3CCANC1h_u%3DN0YKFu3pDaEVYz5ZcQtjQnXEjQA2ReKmoS%2Bye7%3Do%3DA%40mail.gmail.com%3E
ebb329ca28046396af1e34395a0df92b5327
d
es
ck
y""
to
"
Patrick Wendell <pwendell@gmail.com>,"Fri, 29 Aug 2014 09:55:44 -0700",Re: [VOTE] Release Apache Spark 1.1.0 (RC2),Nicholas Chammas <nicholas.chammas@gmail.com>,"Hey Nicholas,

Thanks for this, we can merge in doc changes outside of the actual
release timeline, so we'll make sure to loop those changes in before
we publish the final 1.1 docs.

- Patrick

at
t
i.zaharia@gmail.com>
l
f
't
ng
t
it
t
t
e
o
ke
is
m>
r
nd
e
es
't
ut
e713a73cc
ne
ut
e
.mbox/%3CCANC1h_u%3DN0YKFu3pDaEVYz5ZcQtjQnXEjQA2ReKmoS%2Bye7%3Do%3DA%40mail.gmail.com%3E
11aebb329ca28046396af1e34395a0df92b5327
nd
/
----
-

---------------------------------------------------------------------


"
Ron's Yahoo! <zlgonzalez@yahoo.com.INVALID>,"Fri, 29 Aug 2014 10:03:25 -0700",Re: Jira tickets for starter tasks,Josh Rosen <rosenville@gmail.com>,"Hi Josh,
  Can you add me as well?

Thanks,
Ron


in order to allow you to assign issues to yourself.  Iíve added this email address to that group, so you should be set!
just do  


---------------------------------------------------------------------


"
Josh Rosen <rosenville@gmail.com>,"Fri, 29 Aug 2014 10:05:52 -0700",Re: Jira tickets for starter tasks,=?utf-8?Q?Ron's_Yahoo=21?= <zlgonzalez@yahoo.com>,"Added you; you should be set!

If anyone else wants me to add them, please email me off-list so that we don‚Äôt end up flooding the dev list with replies. Thanks!



Hi Josh,  
Can you add me as well?  

Thanks,  
Ron  


ù role group in order to allow you to assign issues to yourself. I‚Äôve added this email address to that group, so you should be set!  
te:  
ust do  

"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 29 Aug 2014 13:18:32 -0400",Re: [VOTE] Release Apache Spark 1.1.0 (RC2),Patrick Wendell <pwendell@gmail.com>,"[Let me know if I should be posting these comments in a different thread.]

Should the default Spark version in spark-ec2
<https://github.com/apache/spark/blob/e1535ad3c6f7400f2b7915ea91da9c60510557ba/ec2/spark_ec2.py#L86>
be updated for this release?

Nick
‚Äã



tei.zaharia@gmail.com>
s
t)
w
ut
e
ng
o
s
s.
nd
dd
ar
k.
3a73cc
ox/%3CCANC1h_u%3DN0YKFu3pDaEVYz5ZcQtjQnXEjQA2ReKmoS%2Bye7%3Do%3DA%40mail.gmail.com%3E
ey
k
ebb329ca28046396af1e34395a0df92b5327
:
th
l""
"
shane knapp <sknapp@berkeley.edu>,"Fri, 29 Aug 2014 10:26:54 -0700",new jenkins plugin installed and ready for use,"amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","i have always found the 'Rebuild' plugin super useful:
https://wiki.jenkins-ci.org/display/JENKINS/Rebuild+Plugin

this is installed and enables.  enjoy!

shane
"
Patrick Wendell <pwendell@gmail.com>,"Fri, 29 Aug 2014 11:18:05 -0700",Re: [VOTE] Release Apache Spark 1.1.0 (RC2),Nicholas Chammas <nicholas.chammas@gmail.com>,"Oh darn - I missed this update. GRR, unfortunately I think this means
I'll need to cut a new RC. Thanks for catching this Nick.

]
?
atei.zaharia@gmail.com>
e
rs
d
ow
o
be
s
t
to
es
)
g
,
3ede713a73cc
e
p
,
408.mbox/%3CCANC1h_u%3DN0YKFu3pDaEVYz5ZcQtjQnXEjQA2ReKmoS%2Bye7%3Do%3DA%40mail.gmail.com%3E
rk
=711aebb329ca28046396af1e34395a0df92b5327
029/
t:
-------
----

---------------------------------------------------------------------


"
Yi Tian <tianyi.asiainfo@gmail.com>,"Sat, 30 Aug 2014 02:38:13 +0800",Re: Compie error with XML elements,Devl Devel <devl.development@gmail.com>,"Hi, Devl!

I got the same problem.

You can try to upgrade your scala plugins to  0.41.2

It works on my mac.




---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 29 Aug 2014 14:43:38 -0400",Re: [VOTE] Release Apache Spark 1.1.0 (RC2),Patrick Wendell <pwendell@gmail.com>,"Sounds good. As an FYI, we had this problem with the 1.0.2 release
<https://issues.apache.org/jira/browse/SPARK-3242>. Is there perhaps some
kind of automated check we can make to catch this for us in the future?
Where would it go?


:

cs
e.
m
<matei.zaharia@gmail.com>
m
s
l
d
e
'd
do
n
op
as
RC
n
te
.
3a73cc
ox/%3CCANC1h_u%3DN0YKFu3pDaEVYz5ZcQtjQnXEjQA2ReKmoS%2Bye7%3Do%3DA%40mail.gmail.com%3E
ebb329ca28046396af1e34395a0df92b5327
e
d
t
ot
s.
"
Cheng Lian <lian.cs.zju@gmail.com>,"Fri, 29 Aug 2014 11:52:35 -0700",Re: [VOTE] Release Apache Spark 1.1.0 (RC2),Patrick Wendell <pwendell@gmail.com>,"Just noticed one thing: although --with-hive is deprecated by -Phive,
make-distribution.sh still relies on $SPARK_HIVE (which was controlled by
--with-hive) to determine whether to include datanucleus jar files. This
means we have to do something like SPARK_HIVE=true ./make-distribution.sh
... to enable Hive support. Otherwise datanucleus jars are not included in
lib/.

This issue is similar to SPARK-3234
<https://issues.apache.org/jira/browse/SPARK-3234>, both
SPARK_HADOOP_VERSION and SPARK_HIVE are controlled by some deprecated
command line options.
‚Äã



cs
e.
m
<matei.zaharia@gmail.com>
m
s
l
d
e
'd
do
n
op
as
RC
n
te
.
3a73cc
ox/%3CCANC1h_u%3DN0YKFu3pDaEVYz5ZcQtjQnXEjQA2ReKmoS%2Bye7%3Do%3DA%40mail.gmail.com%3E
ebb329ca28046396af1e34395a0df92b5327
e
d
t
ot
s.
"
Marcelo Vanzin <vanzin@cloudera.com>,"Fri, 29 Aug 2014 11:53:17 -0700",Re: [VOTE] Release Apache Spark 1.1.0 (RC2),Nicholas Chammas <nicholas.chammas@gmail.com>,"In our internal projects we use this bit of code in the maven pom to
create a properties file with build information (sorry for the messy
indentation). Then we have code that reads this property file
somewhere and provides that info. This should make it easier to not
have to change version numbers in Scala/Java/Python code ever again.
:-)

Shouldn't be hard to do something like that in sbt (actually should be
much easier).


      <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-antrun-plugin</artifactId>
        <version>1.6</version>
        <executions>
          <execution>
            <id>build-info</id>
            <phase>compile</phase>
            <goals>
              <goal>run</goal>
            </goals>
            <configuration>
              <target>
                <taskdef
                        resource=""net/sf/antcontrib/antcontrib.properties""
                        classpathref=""maven.plugin.classpath""/>
                    <if>
                      <not>
                        <isset property=""build.hash""/>
                      </not>
                      <then>
                        <exec executable=""git""
                              outputproperty=""build.hash"">
                          <arg line=""rev-parse HEAD""/>
                        </exec>
                      </then>
                    </if>
                    <echo>buildRevision: ${build.hash}</echo>
                    <echo file=""${build.info}""
                        message=""version=${project.version}${line.separator}"" />
                    <echo file=""${build.info}"" append=""true""
                        message=""hash=${build.hash}${line.separator}"" /                    <echo file=""${build.info}"" append=""true"" />
              </target>
            </configuration>
          </execution>
        </executions>
        <dependencies>
          <dependency>
            <groupId>ant-contrib</groupId>
            <artifactId>ant-contrib</artifactId>
            <version>1.0b3</version>
            <exclusions>
              <exclusion>
                <groupId>ant</groupId>
                <artifactId>ant</artifactId>
              </exclusion>
            </exclusions>
          </dependency>
        </dependencies>
      </plugin>
    </plugins>

te:
ocs
se.
om
:
e
 <matei.zaharia@gmail.com>
e
om
e
e
es
-
ll
nd
he
I'd
 do
on
l
oop
b
was
t
 RC
an
ate
..
n
/
13a73cc
box/%3CCANC1h_u%3DN0YKFu3pDaEVYz5ZcQtjQnXEjQA2ReKmoS%2Bye7%3Do%3DA%40mail.gmail.com%3E
.
aebb329ca28046396af1e34395a0df92b5327
be
d
!
nd
d
nt
not
ns.



-- 
Marcelo

---------------------------------------------------------------------


"
Jeremy Freeman <freeman.jeremy@gmail.com>,"Fri, 29 Aug 2014 13:33:16 -0700 (PDT)",Re: [VOTE] Release Apache Spark 1.1.0 (RC2),dev@spark.incubator.apache.org,"+1. Validated several custom analysis pipelines on a private cluster in
standalone mode. Tested new PySpark support for arbitrary Hadoop input
formats, works great!

-- Jeremy



--

---------------------------------------------------------------------


"
smalpani <sudershan.malpani@gmail.com>,"Fri, 29 Aug 2014 12:20:53 -0700 (PDT)",Need to check approach for continuing development on Spark,dev@spark.incubator.apache.org,"Hi,

We are developing an app in Spring in which we are using Cassandra and
calling datastax api's from Java to query it. The internal library is
responsible for calling cassandra and other data sources like RDS. We are
calling several client API's from Spark provided by the client-jar to
perform certain operations on that data like:
1. Reading data from S3 and inserting in cassandra by providing the objects
through API and then internally the API will store in cassandra.
2. Taking the data from cassandra through API as objects and then processing
on that data to generate metrics and saving it in cassandra through APi's
only.
3. Then internally through those API's only calculating aggregates and
separating data in bands etc.

The whole project is driven by Spring. Please let me know if we are
approaching towards it fine.




--

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Fri, 29 Aug 2014 15:20:08 -0700",Re: Compie error with XML elements,Yi Tian <tianyi.asiainfo@gmail.com>,"In some cases IntelliJ's Scala compiler can't compile valid Scala
source files. Hopefully they fix (or have fixed) this in a newer
version.

- Patrick


---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sat, 30 Aug 2014 03:41:39 -0400",Re: Handling stale PRs,Patrick Wendell <pwendell@gmail.com>,"


Just an FYI: Seems like the GitHub-sanctioned work-around to having
issues-only permissions is to have a second, issues-only repository
<https://help.github.com/articles/issues-only-access-permissions>. Not a
very attractive work-around...

Nick
"
Patrick Wendell <pwendell@gmail.com>,"Sat, 30 Aug 2014 15:03:11 -0700",Re: [VOTE] Release Apache Spark 1.1.0 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","Thanks to Nick Chammas and Cheng Lian who pointed out two issues with
the release candidate. I'll cancel this in favor of RC3.


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Sat, 30 Aug 2014 15:07:52 -0700",[VOTE] Release Apache Spark 1.1.0 (RC3),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version 1.1.0!

The tag to be voted on is v1.1.0-rc3 (commit b2d0493b):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=b2d0493b223c5f98a593bb6d7372706cc02bebad

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-1.1.0-rc3/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1030/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-1.1.0-rc3-docs/

Please vote on releasing this package as Apache Spark 1.1.0!

The vote is open until Tuesday, September 02, at 23:07 UTC and passes if
a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.1.0
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

== Regressions fixed since RC1 ==
- Build issue for SQL support: https://issues.apache.org/jira/browse/SPARK-3234
- EC2 script version bump to 1.1.0.

== What justifies a -1 vote for this release? ==
This vote is happening very late into the QA period compared with
previous votes, so -1 votes should only occur for significant
regressions from 1.0.2. Bugs already present in 1.0.X will not block
this release.

== What default changes should I be aware of? ==
1. The default value of ""spark.io.compression.codec"" is now ""snappy""
--> Old behavior can be restored by switching to ""lzf""

2. PySpark now performs external spilling during aggregations.
--> Old behavior can be restored by setting ""spark.shuffle.spill"" to ""false"".

---------------------------------------------------------------------


"
Yi Tian <tianyi.asiainfo@gmail.com>,"Sun, 31 Aug 2014 10:38:46 +0800",[SPARK-3324] make yarn module as a unified maven jar project,dev@spark.apache.org,"Hi everyone!

I found the YARN module has nonstandard path structure like:

${SPARK_HOME}
  |--yarn
     |--alpha (contains yarn api support for 0.23 and 2.0.x)
     |--stable (contains yarn api support for 2.2 and later)
     |     |--pom.xml (spark-yarn)
     |--common (Common codes not depending on specific version of Hadoop)
     |--pom.xml (yarn-parent)

When we use maven to compile yarn module, maven will import 'alpha' or 'stable' module according to profile setting.
And the submodule like 'stable' use the build propertie defined in yarn/pom.xml to import common codes to sourcePath.
It will cause IntelliJ can't directly recognize sources in common directory as sourcePath.

I thought we should change the yarn module to a unified maven jar project, 
and add specify different version of yarn api via maven profile setting.

I created a JIRA ticket: https://issues.apache.org/jira/browse/SPARK-3324

Any advice will be appreciated .




"
Reynold Xin <rxin@databricks.com>,"Sat, 30 Aug 2014 20:03:08 -0700",Fwd: Partitioning strategy changed in Spark 1.0.x?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Sending the response back to the dev list so this is indexable and
searchable by others.

---------- Forwarded message ----------
From: Milos Nikolic <milos.nikolic83@gmail.com>
Date: Sat, Aug 30, 2014 at 5:50 PM
Subject: Re: Partitioning strategy changed in Spark 1.0.x?
To: Reynold Xin <rxin@databricks.com>


Thank you, your insights were very helpful, and we managed to find a
solution that works for us.

Best,
Milos



I don't think you can ever expect the mapping from data to physical nodes
in Spark, even in Spark 0.9. That is because the scheduler needs to be
fault-tolerant. What if the node is busy or the node is down?

What happens is the partitioning of data is deterministic, i.e. certain
data is always hashed into certain partitions (given the same partition
count). And if you don't run foreach twice, but instead simply zip the two
RDDs that are both hash partitioned using the same partitioner, then the
scheduler will not create extra stages.

e.g.

    // Let's say I have 10 nodes
    val partitioner = new HashPartitioner(10)

    // Create RDD
    val rdd = sc.parallelize(0 until 10).map(k => (k, computeValue(k)))

    // Partition twice using the same partitioner
    val p1 = rdd.partitionBy(partitioner)
    val p2 = rdd.partitionBy(partitioner)
    p1.zip(p2)       <--- this should work





in
e node.
is
m
 Spark
 meantime.
k)))
"
Sean Owen <sowen@cloudera.com>,"Sun, 31 Aug 2014 09:19:22 +0100",Re: [SPARK-3324] make yarn module as a unified maven jar project,Yi Tian <tianyi.asiainfo@gmail.com>,"This isn't possible since the two versions of YARN are mutually
incompatible at compile-time. However see my comments about how this
could be restructured to be a little more standard, and so that
IntelliJ would parse it out of the box.

Still I imagine it is not worth it if YARN alpha will go away at some
point and IntelliJ can easily be told where the extra src/ is.


---------------------------------------------------------------------


"
Yi Tian <tianyi.asiainfo@gmail.com>,"Sun, 31 Aug 2014 20:32:57 +0800",Re: [SPARK-3324] make yarn module as a unified maven jar project,Sean Owen <sowen@cloudera.com>,"Hi Sean

Before compile-time, maven could dynamically add either stable or alpha source to the yarn/ project.

So there are no incompatible at the compile-time.

Here are an example:

yarn/pom.xml

      <plugin>
        <groupId>org.codehaus.mojo</groupId>
        <artifactId>build-helper-maven-plugin</artifactId>
        <executions>
          <execution>
            <id>add-scala-sources</id>
            <phase>generate-sources</phase>
            <goals>
              <goal>add-source</goal>
            </goals>
            <configuration>
              <sources>
                <source>common/src/main/scala</source>
                <source>${yarn.api}/src/main/scala</source>
              </sources>
            </configuration>
          </execution>
        </executions>
      </plugin>



Hadoop)
or 'stable' module according to profile setting.
yarn/pom.xml to import common codes to sourcePath.
directory as sourcePath.
project,
setting.
https://issues.apache.org/jira/browse/SPARK-3324


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Sun, 31 Aug 2014 15:29:26 +0100",Re: [SPARK-3324] make yarn module as a unified maven jar project,Yi Tian <tianyi.asiainfo@gmail.com>,"Yes, alpha and stable need to stay in two separate modules. I think
this is a little less standard than simply having three modules:
common, stable, alpha.


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Sun, 31 Aug 2014 17:01:41 +0100",Re: [VOTE] Release Apache Spark 1.1.0 (RC3),Patrick Wendell <pwendell@gmail.com>,"All the signatures are correct. The licensing all looks fine. The
source builds fine.

Now, let me ask about unit tests, since I had a more detailed look,
which I should have done before.


dev/run-tests fails two tests (1 Hive, 1 Kafka Streaming) for me
locally on 1.1.0-rc3. Does anyone else see that? It may be my env.
Although I still see the Hive failure on Debian too:

[info] - SET commands semantics for a HiveContext *** FAILED ***
[info]   Expected Array(""spark.sql.key.usedfortestonly=test.val.0"",
""spark.sql.key.usedfortestonlyspark.sql.key.usedfortestonly=test.val.0test.val.0""),
but got Array(""spark.sql.key.usedfortestonlyspark.sql.key.usedfortestonly=test.val.0test.val.0"",
""spark.sql.key.usedfortestonly=test.val.0"") (HiveQuerySuite.scala:541)


Python lint checks fail for files in python/build/py4j. These aren't
Spark files and are only present in this location in the release. The
check should simply be updated later to ignore this. Not a blocker.


Evidently, the SBT tests pass, usually, in master:
https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/SparkPullRequestBuilder/
But Maven tests have not passed in master for a long time:
https://amplab.cs.berkeley.edu/jenkins/view/Spark/

I can reproduce this with Maven for 1.1.0-rc3. It feels funny to ship
with a repeatable Maven build failure, since Maven is the build of
record for release. Whatever is being tested is probably OK since SBT
passes, so it need not block release. I'll look for a fix as well.

A simple ""sbt test"" always fails for me, and that just may be because
the build is now only meaningful with further configuration. SBT tests
are mostly passing if not consistently for all profiles:
https://amplab.cs.berkeley.edu/jenkins/view/Spark/  These also sort of
feel funny, although nothing seems like an outright blocker.

I guess I'll add a non-binding +0 -- none of these are necessarily a
blocker but adds up to feeling a bit iffy about the state of tests in
the context of a release.


---------------------------------------------------------------------


"
Will Benton <willb@redhat.com>,"Sun, 31 Aug 2014 13:11:21 -0400 (EDT)",Re: [VOTE] Release Apache Spark 1.1.0 (RC3),Sean Owen <sowen@cloudera.com>,"
best,
wb

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Sun, 31 Aug 2014 18:18:42 +0100",Re: [VOTE] Release Apache Spark 1.1.0 (RC3),Will Benton <willb@redhat.com>,"Fantastic. As it happens, I just fixed up Mahout's tests for Java 8
and observed a lot of the same type of failure.

I'm about to submit PRs for the two issues I identified. AFAICT these
3 then cover the failures I mentioned:

https://issues.apache.org/jira/browse/SPARK-3329
https://issues.apache.org/jira/browse/SPARK-3330
https://issues.apache.org/jira/browse/SPARK-3331

I'd argue that none necessarily block a release, since they just
represent a problem with test-only code in Java 8, with the test-only
context of Jenkins and multiple profiles, and with a trivial
configuration in a style check for Python. Should be fixed but none
indicate a bug in the release.

test.val.0""),
.val.0test.val.0"",
hine using Oracle JDK 8 but not on Fedora using OpenJDK.)  I've also seen similar errors in topic branches (but not on master) that seem to indicate that tests depend on sets of pairs arriving from Hive in a particular order; it seems that this isn't a safe assumption.
thub.com/apache/spark/pull/2220

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Sun, 31 Aug 2014 11:35:06 -0700",Re: [VOTE] Release Apache Spark 1.1.0 (RC3),Sean Owen <sowen@cloudera.com>,"For my part I'm +1 on this, though Sean it would be great separately
to fix the test environment.

For those who voted on rc2, this is almost identical, so feel free to
+1 unless you think there are issues with the two minor bug fixes.

0test.val.0""),
t.val.0test.val.0"",
)
chine using Oracle JDK 8 but not on Fedora using OpenJDK.)  I've also seen similar errors in topic branches (but not on master) that seem to indicate that tests depend on sets of pairs arriving from Hive in a particular order; it seems that this isn't a safe assumption.
ithub.com/apache/spark/pull/2220

---------------------------------------------------------------------


"
chutium <teng.qiu@gmail.com>,"Sun, 31 Aug 2014 11:57:39 -0700 (PDT)","Re: HiveContext, schemaRDD.printSchema get different dataTypes,
 feature or a bug? really strange and surprised...",dev@spark.incubator.apache.org,"Hi Cheng, thank you very much for helping me to finally find out the secret
of this magic...

actually we defined this external table with
    SID STRING
    REQUEST_ID STRING
    TIMES_DQ TIMESTAMP
    TOTAL_PRICE FLOAT
    ...

using ""desc table ext_fullorders"" it is only shown as
[# col_name             data_type               comment             ]
...
[times_dq               string                  from deserializer   ]
[total_price            string                  from deserializer   ]
...
because, as you said, CSVSerde sets all field object inspectors to
javaStringObjectInspector
and therefore there are comments ""from deserializer""

but in StorageDescriptor, are the real user defined types,
using ""desc extended table ext_fullorders"" we can see his
sd:StorageDescriptor
is:
FieldSchema(name:times_dq, type:timestamp, comment:null),
FieldSchema(name:total_price, type:float, comment:null)

and Spark HiveContext reads the schema info from this StorageDescriptor
https://github.com/apache/spark/blob/7e191fe29bb09a8560cd75d453c4f7f662dff406/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMetastoreCatalog.scala#L316

so, in the SchemaRDD, the fields in Row were filled with strings (via
fillObject, all of values were retrieved from CSVSerDe with
javaStringObjectInspector)

but Spark considers that some of them are float or timestamp (schema info
were got from sd:StorageDescriptor)

crazy...

and sorry for update on the weekend...

a little more about how i fand this problem and why it is a trouble for us.

we use the new spark thrift server, to query normal managed hive table, it
works fine

but when we try to access the external tables with custom SerDe such as this
CSVSerDe, then we will get this ClassCastException, such as:
java.lang.ClassCastException: java.lang.String cannot be cast to
java.lang.Float

the reason is
https://github.com/apache/spark/blob/d94a44d7caaf3fe7559d9ad7b10872fa16cf81ca/sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/server/SparkSQLOperationManager.scala#L104-L105

here Spark's thrift server try to get a float value from SparkRow, because
in the schema info (sd:StorageDescriptor) this column is float, but actually
in SparkRow, this field was filled with string value...



--

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sun, 31 Aug 2014 17:14:23 -0400",Re: [VOTE] Release Apache Spark 1.1.0 (RC3),Patrick Wendell <pwendell@gmail.com>,"-1: I believe I've found a regression from 1.0.2. The report is captured in
SPARK-3333 <https://issues.apache.org/jira/browse/SPARK-3333>.



"
chutium <teng.qiu@gmail.com>,"Sun, 31 Aug 2014 15:38:06 -0700 (PDT)",Re: [VOTE] Release Apache Spark 1.1.0 (RC3),dev@spark.incubator.apache.org,"has anyone tried to build it on hadoop.version=2.0.0-mr1-cdh4.3.0 or
hadoop.version=1.0.3-mapr-3.0.3 ?

see comments in
https://issues.apache.org/jira/browse/SPARK-3124
https://github.com/apache/spark/pull/2035

i built spark snapshot on hadoop.version=1.0.3-mapr-3.0.3
and the ticket creator built on hadoop.version=2.0.0-mr1-cdh4.3.0

both hadoop version do not work

on 1.0.3-mapr3.0.3

when i try to start spark-shell

i got:

14/08/23 23:29:46 INFO SecurityManager: Changing view acls to: client09,
14/08/23 23:29:46 INFO SecurityManager: Changing modify acls to: client09,
14/08/23 23:29:46 INFO SecurityManager: SecurityManager: authentication
disabled; ui acls disabled; users with view permissions: Set(client09, );
users with modify permissions: Set(client09, )
14/08/23 23:29:50 INFO Slf4jLogger: Slf4jLogger started
14/08/23 23:29:50 INFO Remoting: Starting remoting
14/08/23 23:29:50 ERROR ActorSystemImpl: Uncaught fatal error from thread
[spark-akka.actor.default-dispatcher-2] shutting down ActorSystem [spark]
java.lang.VerifyError: (class:
org/jboss/netty/channel/socket/nio/NioWorkerPool, method: createWorker
signature:
(Ljava/util/concurrent/Executor;)Lorg/jboss/netty/channel/socket/nio/AbstractNioWorker;)
Wrong return type in function
        at
akka.remote.transport.netty.NettyTransport.<init>(NettyTransport.scala:282)
        at
akka.remote.transport.netty.NettyTransport.<init>(NettyTransport.scala:239)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native
Method)
        at
sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
        at
sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
        at
akka.actor.ReflectiveDynamicAccess$$anonfun$createInstanceFor$2.apply(DynamicAccess.scala:78)
        at scala.util.Try$.apply(Try.scala:161)
...
...
...


it seems this netty jar conflict affects not only SQL component and some
test-case



--

---------------------------------------------------------------------


"
