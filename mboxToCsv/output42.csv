mariusvniekerk <marius.v.niekerk@gmail.com>,"Mon, 31 Oct 2016 20:30:46 -0700 (MST)","Re: Python Spark Improvements (forked from Spark Improvement
 Proposals)",dev@spark.apache.org,"So i've been working on some very very early stage apache arrow integration. 
My current plan it to emulate some of how the R function execution works.  
If there are any other people working on similar efforts it would be good
idea to combine efforts.

I can see how much effort is involved in converting that PR to a spark
package so that people can try to use it.  I think this is something that we
want some more community iteration on maybe?





--

---------------------------------------------------------------------


"
Holden Karau <holden@pigscanfly.ca>,"Mon, 31 Oct 2016 20:32:10 -0700",Re: Python Spark Improvements (forked from Spark Improvement Proposals),mariusvniekerk <marius.v.niekerk@gmail.com>,"I believe Bryan is also working on this a little - and I'm a little busy
with the other stuff but would love to stay in the loop on Arrow progress :)



-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
Reynold Xin <rxin@databricks.com>,"Tue, 1 Nov 2016 00:09:45 -0700",Re: Odp.: Spark Improvement Proposals,Marcelo Vanzin <vanzin@cloudera.com>,"Most things looked OK to me too, although I do plan to take a closer look
after Nov 1st when we cut the release branch for 2.1.


:

he
e
n
as
m
d
we
y
g
e
e
re
s,
e
),
d
s
so
r
ff
n
h
re
w
"
Sean Owen <sowen@cloudera.com>,"Tue, 01 Nov 2016 09:22:14 +0000",Re: Updating Parquet dep to 1.9,"Michael Allman <michael@videoamp.com>, dev@spark.apache.org","Yes this came up from a different direction:
https://issues.apache.org/jira/browse/SPARK-18140

I think it's fine to pursue an upgrade to fix these several issues. The
question is just how well it will play with other components, so bears some
testing and evaluation of the changes from 1.8, but yes this would be good.


"
Holden Karau <holden@pigscanfly.ca>,"Tue, 1 Nov 2016 08:29:16 -0700",Re: Python Spark Improvements (forked from Spark Improvement Proposals),mariusvniekerk <marius.v.niekerk@gmail.com>,"https://issues.apache.org/jira/browse/SPARK-13534 :)




-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
Ryan Blue <rblue@netflix.com.INVALID>,"Tue, 1 Nov 2016 09:05:24 -0700",Re: Updating Parquet dep to 1.9,Sean Owen <sowen@cloudera.com>,"1.9.0 includes some fixes intended specifically for Spark:

* PARQUET-389: Evaluates push-down predicates for missing columns as though
they are null. This is to address Spark's work-around that requires reading
and merging file schemas, even for metastore tables.
* PARQUET-654: Adds an option to disable record-level predicate push-down,
but keep row group evaluation. This allows Spark to skip row groups based
on stats and dictionaries, but implement its own vectorized record
filtering.

The Parquet community also evaluated performance to ensure no performance
regressions from moving to the ByteBuffer read path.

There is one concern about 1.9.0 that will be addressed in 1.9.1, which is
that stats calculations were incorrectly using unsigned byte order for
string comparison. This means that min/max stats can't be used if the data
contains (or may contain) UTF8 characters with the msb set. 1.9.0 won't
return the bad min/max values for correctness, but there is a property to
override this behavior for data that doesn't use the affected code points.

Upgrading to 1.9.0 depends on how the community wants to handle the sort
order bug: whether correctness or performance should be the default.

rb




-- 
Ryan Blue
Software Engineer
Netflix
"
Zak H <zak.hassan1010@gmail.com>,"Tue, 1 Nov 2016 13:00:25 -0400",Question about using collaborative filtering in MLlib,dev@spark.apache.org,"Hi,

I'm using the Java Api for Dataframe api for Spark-Mllib. Should I be using
the RDD api instead as I'm not sure if this functionality has been ported
over to dataframes, correct me if I'm wrong.

My goal is to evaluate spark's recommendation capabilities. I'm looking at
this example:

http://spark.apache.org/docs/latest/ml-collaborative-filtering.html

Looking at the java docs I can see there is a method:
http://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/recommendation/MatrixFactorizationModel.html

""public RDD <http://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html><scala.Tuple2<Object,Rating
<http://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/recommendation/Rating.html>[]>>
recommendUsersForProducts(int num)""


For some reason the recommendProductsForUsers method isn't available in the
java api:
model.recommendProductsForUsers

Is there something I'm missing here:

I've posted my code here on this gist. I am using the dataframe api for
mllib. I know there may be work to port over functionality from RDD's.

https://gist.github.com/zmhassan/6ccdda8b4ad86f9b1924477c65ed5d45

Thanks,
Zak
"
Reynold Xin <rxin@databricks.com>,"Tue, 1 Nov 2016 13:03:09 -0700",Re: Updating Parquet dep to 1.9,Ryan Blue <rblue@netflix.com.invalid>,"Ryan want to submit a pull request?



"
Ryan Blue <rblue@netflix.com.INVALID>,"Tue, 1 Nov 2016 13:22:00 -0700",Re: Updating Parquet dep to 1.9,Reynold Xin <rxin@databricks.com>,"I can when I'm finished with a couple other issues if no one gets to it
first.

Michael, if you're interested in updating to 1.9.0 I'm happy to help review
that PR.




-- 
Ryan Blue
Software Engineer
Netflix
"
Michael Armbrust <michael@databricks.com>,"Tue, 1 Nov 2016 15:47:42 -0700",Re: JIRA Components for Streaming,"""dev@spark.apache.org"" <dev@spark.apache.org>","I did this
<https://issues.apache.org/jira/browse/SPARK/component/12331043/?selectedTab=com.atlassian.jira.jira-projects-plugin:component-issues-panel>.
Please help me correct any issues I may have missed.


"
Reynold Xin <rxin@databricks.com>,"Tue, 1 Nov 2016 16:42:48 -0700",view canonicalization - looking for database gurus to chime in,"""dev@spark.apache.org"" <dev@spark.apache.org>","I know there are a lot of people with experience on developing database
internals on this list. Please take a look at this proposal for a new,
simpler way to handle view canonicalization in Spark SQL:
https://issues.apache.org/jira/browse/SPARK-18209

It sounds much simpler than what we currently do in 2.0/2.1, but I'm not
sure if there are obvious holes that I missed.
"
Sam Goodwin <sam.goodwin89@gmail.com>,"Wed, 02 Nov 2016 04:15:41 +0000",Re: getting encoder implicits to be more accurate,Michael Armbrust <michael@databricks.com>,"You don't need compiler time macros for this, you can do it quite easily
using shapeless. I've been playing with a project which borrows ideas from
spray-json and spray-json-shapeless to implement Row marshalling for
arbitrary case classes. It's checked and generated at compile time,
supports arbitrary/nested case classes, and allows custom types. It is also
entirely pluggable meaning you can bypass the default implementations and
provide your own, just like any type class.

https://github.com/upio/spark-sql-formats


*From:* Michael Armbrust <michael@databricks.com>
*Date:* October 26, 2016 at 12:50:23 PM PDT
*To:* Koert Kuipers <koert@tresata.com>
*Cc:* Ryan Blue <rblue@netflix.com>, ""dev@spark.apache.org"" <
dev@spark.apache.org>
*Subject:* *Re: getting encoder implicits to be more accurate*

Sorry, I realize that set is only one example here, but I don't think that
making the type of the implicit more narrow to include only ProductN or
something eliminates the issue.  Even with that change, we will fail to
generate an encoder with the same error if you, for example, have a field
of your case class that is an unsupported type.



Short of changing this to compile-time macros, I think we are stuck with
this class of errors at runtime.  The simplest solution seems to be to
expand the set of thing we can handle as much as possible and allow users
to turn on a kryo fallback for expression encoders.  I'd be hesitant to
make this the default though, as behavior would change with each release
that adds support for more types.  I would be very supportive of making
this fallback a built-in option though.




yup, it doesnt really solve the underlying issue.

we fixed it internally by having our own typeclass that produces encoders
and that does check the contents of the products, but we did this by simply
supporting Tuple1 - Tuple22 and Option explicitly, and not supporting
Product, since we dont have a need for case classes

if case classes extended ProductN (which they will i think in scala 2.12?)
then we could drop Product and support Product1 - Product22 and Option
explicitly while checking the classes they contain. that would be the
cleanest.




Isn't the problem that Option is a Product and the class it contains isn't
checked? Adding support for Set fixes the example, but the problem would
happen with any class there isn't an encoder for, right?




Hmm, that is unfortunate.  Maybe the best solution is to add support for
sets?  I don't think that would be super hard.




i am trying to use encoders as a typeclass where if it fails to find an
ExpressionEncoder it falls back to KryoEncoder.

the issue seems to be that ExpressionEncoder claims a little more than it
can handle here:
  implicit def newProductEncoder[T <: Product : TypeTag]: Encoder[T] =
Encoders.product[T]

this ""claims"" to handle for example Option[Set[Int]], but it really cannot
handle Set so it leads to a runtime exception.

would it be useful to make this a little more specific? i guess the
challenge is going to be case classes which unfortunately dont extend
Product1, Product2, etc.







--

Ryan Blue

Software Engineer

Netflix
"
Reynold Xin <rxin@databricks.com>,"Tue, 1 Nov 2016 21:51:38 -0700",[VOTE] Release Apache Spark 2.0.2 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version
2.0.2. The vote is open until Fri, Nov 4, 2016 at 22:00 PDT and passes if a
majority of at least 3+1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 2.0.2
[ ] -1 Do not release this package because ...


The tag to be voted on is v2.0.2-rc2
(a6abe1ee22141931614bf27a4f371c46d8379e33)

This release candidate resolves 84 issues:
https://s.apache.org/spark-2.0.2-jira

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.0.2-rc2-bin/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1210/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.0.2-rc2-docs/


Q: How can I help test this release?
A: If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions from 2.0.1.

Q: What justifies a -1 vote for this release?
A: This is a maintenance release in the 2.0.x series. Bugs already present
in 2.0.1, missing features, or bugs related to new features will not
necessarily block this release.

Q: What fix version should I use for patches merging into branch-2.0 from
now on?
A: Please mark the fix version as 2.0.3, rather than 2.0.2. If a new RC
(i.e. RC3) is cut, I will change the fix version of those patches to 2.0.2.
"
vijoshi <vijoshi5@in.ibm.com>,"Tue, 1 Nov 2016 21:54:17 -0700 (MST)",Re: [VOTE] Release Apache Spark 2.0.2 (RC1),dev@spark.apache.org,"Hi, 

Have encountered an issue with History Server in 2.0 - and updated
https://issues.apache.org/jira/browse/SPARK-16808 with a comment detailing
the problem. This is a regression in 2.0 from 1.6, so this issue exists
since 2.0.1. Encountered this very recently when we evaluated moving to 2.0
from 1.6. But the issue is bad enough to virtually make it not possible to
adopt 2.0.x where spark history server runs behind a proxy. 



--

---------------------------------------------------------------------


"
vijoshi <vijoshi5@in.ibm.com>,"Tue, 1 Nov 2016 21:58:20 -0700 (MST)",Re: [VOTE] Release Apache Spark 2.0.2 (RC2),dev@spark.apache.org,"
Hi, 

Have encountered an issue with History Server in 2.0 - and updated
https://issues.apache.org/jira/browse/SPARK-16808 with a comment detailing
the problem. This is a regression in 2.0 from 1.6, so this issue exists
since 2.0.1. Encountered this very recently when we evaluated moving to 2.0
from 1.6. But the issue is bad enough to virtually make it not possible to
adopt 2.0.x where spark history server runs behind a proxy.

Regards,
Vinayak



--

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Tue, 1 Nov 2016 22:03:15 -0700",Re: [VOTE] Release Apache Spark 2.0.2 (RC2),vijoshi <vijoshi5@in.ibm.com>,"Vinayak,

Thanks for the email. This is really not the thread meant for reporting
existing regressions. It's best just commenting on the jira ticket and even
better submit a fix for it.


"
vijoshi <vijoshi5@in.ibm.com>,"Tue, 1 Nov 2016 22:44:01 -0700 (MST)",Re: [VOTE] Release Apache Spark 2.0.2 (RC2),dev@spark.apache.org,"Sure - given the nature of the bug, it looked like it may have gone under the
radar in prior 2.0 releases (test cases pass) so thought to bring attention
to this for some evaluation of the criticality this issue. Will take further
discussion to the ticket. 



--

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Tue, 1 Nov 2016 22:49:29 -0700",[ANNOUNCE] Apache Spark branch-2.1,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,

Following the release schedule as outlined in the wiki, I just created
branch-2.1 to form the basis of the 2.1 release. As of today we have less
than 50 open issues for 2.1.0. The next couple of weeks we as a community
should focus on testing and bug fixes and burn down the number of
outstanding tickets to 0. My general feeling looking at the last 3 months
of development is that this release as a whole focuses more than the past
on stability, bug fixes and internal refactoring (for cleaning up some of
the debts accumulated during 2.0 development).


What does this mean for committers?

1. For patches that should go into Spark 2.1.0, make sure you also merge
them into not just master, but also branch-2.1.

2. Switch the focus from new feature development to bug fixes and
documentation. For ""new features"" that already have high quality,
outstanding pull requests, shepard them in in the next couple of days.

3. Please un-target or re-target issues if they don't make sense for 2.1.
If a ticket is not ""bug fix"" and still has no high quality patch, it should
probably be retargeted to 2.2.

4. If possible, reach out to users and start testing branch-2.1 to find
bugs. The more testing we can do on real workloads before the release, the
less bugs we will find in the actual Spark 2.1 release.
"
Sean Owen <sowen@cloudera.com>,"Wed, 02 Nov 2016 08:18:13 +0000",Anyone seeing a lot of Spark emails go to Gmail spam?,dev <dev@spark.apache.org>,"I couldn't figure out why I was missing a lot of dev@ announcements, and
have just realized hundreds of messages to dev@ over the past month or so
have been marked as spam for me by Gmail. I have no idea why but it's
usually messages from Michael and Reynold, but not all of them. I'll see
replies to the messages but not the original. Who knows. I can make a
filter. I just wanted to give a heads up in case anyone else has been
silently missing a lot of messages.
"
"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Wed, 2 Nov 2016 04:32:43 -0700 (MST)",Handling questions in the mailing lists,dev@spark.apache.org,"Hi,
I know this is a little off topic but I wanted to raise an issue about handling questions in the mailing list (this is true both for the user mailing list and the dev but since there are other options such as stack overflow for user questions, this is more problematic in dev).
Let's say I ask a question (as I recently did). Unfortunately this was during spark summit in Europe so probably people were busy. In any case no one answered.
The problem is, that if no one answers very soon, the question will almost certainly remain unanswered because new messages will simply drown it.

This is a common issue not just for questions but for any comment or idea which is not immediately picked up.

I believe we should have a method of handling this.
Generally, I would say these types of things belong in stack overflow, after all, the way it is built is perfect for this. More seasoned spark contributors and committers can periodically check out unanswered questions and answer them.
The problem is that stack overflow (as well as other targets such as the databricks forums) tend to have a more user based orientation. This means that any spark internal question will almost certainly remain unanswered.

I was wondering if we could come up with a solution for this.

Assaf.





--"
Sean Owen <sowen@cloudera.com>,"Wed, 02 Nov 2016 12:06:46 +0000",Re: Handling questions in the mailing lists,"""assaf.mendelson"" <assaf.mendelson@rsa.com>, dev@spark.apache.org","I think that unfortunately mailing lists don't scale well. This one has
thousands of subscribers with different interests and levels of experience.
For any given person, most messages will be irrelevant. I also find that a
lot of questions on user@ are not well-asked, aren't an SSCCE (
http://sscce.org/), not something most people are going to bother replying
to even if they could answer. I almost entirely ignore user@ because there
are higher-priority channels like PRs to deal with, that already have
hundreds of messages per day. This is why little of it gets an answer --
too noisy.

We have to have official mailing lists, in any event, to have some official
channel for things like votes and announcements. It's not wrong to ask
questions on user@ of course, but a lot of the questions I see could have
been answered with research of existing docs or looking at the code. I
think that given the scale of the list, it's not wrong to assert that this
is sort of a prerequisite for asking thousands of people to answer one's
question. But we can't enforce that.

The situation will get better to the extent people ask better questions,
help other people ask better questions, and answer good questions. I'd
encourage anyone feeling this way to try to help along those dimensions.






s was
o
t
ns
ions-in-the-mailing-lists-tp19690.html>
"
"""Mendelson, Assaf"" <Assaf.Mendelson@rsa.com>","Wed, 2 Nov 2016 12:11:42 +0000",RE: Handling questions in the mailing lists,"Sean Owen <sowen@cloudera.com>,
        ""dev@spark.apache.org""
	<dev@spark.apache.org>","What I am suggesting is basically to fix that.
For example, we might say that mailing list A is only for voting, mailing list B is only for PR and have something like stack overflow for developer questions (I would even go as far as to have beginner, intermediate and advanced mailing list for users and beginner/advanced for dev).

This can easily be done using stack overflow tags, however, that would probably be harder to manage.
Maybe using special jira tags and manage it in jira?

Anyway as I said, the main issue is not user questions (except maybe advanced ones) but more for dev questions. It is so easy to get lost in the chatter that it makes it very hard for people to learn spark internalsâ€¦
Assaf.

From: Sean Owen [ 2016 2:07 PM
To: Mendelson, Assaf; dev@spark.apache.org
Subject: Re: Handling questions in the mailing lists

I think that unfortunately mailing lists don't scale well. This one has thousands of subscribers with different interests and levels of experience. For any given person, most messages will be irrelevant. I also find that a lot of questions on user@ are not well-asked, aren't an SSCCE (http://sscce.org/), not something most people are going to bother replying to even if they could answer. I almost entirely ignore user@ because there are higher-priority channels like PRs to deal with, that already have hundreds of messages per day. This is why little of it gets an answer -- too noisy.

We have to have official mailing lists, in any event, to have some official channel for things like votes and announcements. It's not wrong to ask questions on user@ of course, but a lot of the questions I see could have been answered with research of existing docs or looking at the code. I think that given the scale of the list, it's not wrong to assert that this is sort of a prerequisite for asking thousands of people to answer one's question. But we can't enforce that.

The situation will get better to the extent people ask better questions, help other people ask better questions, and answer good questions. I'd encourage anyone feeling this way to try to help along those dimensions.





On Wed, Nov 2, 2016 at 11:32 AM assaf.mendelson <assaf.mendelson@rsa.com<mailto:assaf.mendelson@rsa.com>> wrote:
Hi,
I know this is a little off topic but I wanted to raise an issue about handling questions in the mailing list (this is true both for the user mailing list and the dev but since there are other options such as stack overflow for user questions, this is more problematic in dev).
Letâ€™s say I ask a question (as I recently did). Unfortunately this was during spark summit in Europe so probably people were busy. In any case no one answered.
The problem is, that if no one answers very soon, the question will almost certainly remain unanswered because new messages will simply drown it.

This is a common issue not just for questions but for any comment or idea which is not immediately picked up.

I believe we should have a method of handling this.
Generally, I would say these types of things belong in stack overflow, after all, the way it is built is perfect for this. More seasoned spark contributors and committers can periodically check out unanswered questions and answer them.
The problem is that stack overflow (as well as other targets such as the databricks forums) tend to have a more user based orientation. This means that any spark internal question will almost certainly remain unanswered.

I was wondering if we could come up with a solution for this.

Assaf.


________________________________
View this message in context: Handling questions in the mailing lists<http://apache-spark-developers-list.1001551.n3.nabble.com/Handling-questions-in-the-mailing-lists-tp19690.html>
Sent from the Apache Spark Developers List mailing list archive<http://apache-spark-developers-list.1001551.n3.nabble.com/> at Nabble.com.
"
Sean Owen <sowen@cloudera.com>,"Wed, 02 Nov 2016 12:39:01 +0000",Re: Handling questions in the mailing lists,"""Mendelson, Assaf"" <Assaf.Mendelson@rsa.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","There's already reviews@ and issues@. dev@ is for project development
itself and I think is OK. You're suggesting splitting up user@ and I
sympathize with the motivation. Experience tells me that we'll have a
beginner@ that's then totally ignored, and people will quickly learn to
post to advanced@ to get attention, and we'll be back where we started.
Putting it in JIRA doesn't help. I don't think this a problem that is
merely down to lack of process. It actually requires cultivating a culture
change on the community list.


r
he
€¦
e.
a
an
to
 I
s
s was
o
t
ns
ions-in-the-mailing-lists-tp19690.html>
"
Pete Robbins <robbinspg@gmail.com>,"Wed, 02 Nov 2016 13:48:36 +0000",Re: Anyone seeing a lot of Spark emails go to Gmail spam?,"Sean Owen <sowen@cloudera.com>, dev <dev@spark.apache.org>","I have gmail filters to add labels and skip inbox for anything sent to
dev@spark user@spark etc but still get the occasional message marked as spam



"
Michael Allman <michael@videoamp.com>,"Wed, 2 Nov 2016 08:31:31 -0700",Re: Updating Parquet dep to 1.9,Ryan Blue <rblue@netflix.com>,"Sounds great. Regarding the min/max stats issue, is that an issue with the way the files are written or read? What's the Parquet project issue for that bug? What's the 1.9.1 release timeline look like?

I will aim to have a PR in by the end of the week. I feel strongly that either this or https://github.com/apache/spark/pull/15538 <https://github.com/apache/spark/pull/15538> needs to make it into 2.1. The logging output issue is really bad. I would probably call it a blocker.

Michael


it first.
review that PR.
though they are null. This is to address Spark's work-around that requires reading and merging file schemas, even for metastore tables.
push-down, but keep row group evaluation. This allows Spark to skip row groups based on stats and dictionaries, but implement its own vectorized record filtering.
performance regressions from moving to the ByteBuffer read path.
which is that stats calculations were incorrectly using unsigned byte order for string comparison. This means that min/max stats can't be used if the data contains (or may contain) UTF8 characters with the msb set. 1.9.0 won't return the bad min/max values for correctness, but there is a property to override this behavior for data that doesn't use the affected code points.
sort order bug: whether correctness or performance should be the default.
https://issues.apache.org/jira/browse/SPARK-18140 <https://issues.apache.org/jira/browse/SPARK-18140>
The question is just how well it will play with other components, so bears some testing and evaluation of the changes from 1.8, but yes this would be good.
not, I can at least get started on it and publish a PR.
<mailto:dev-unsubscribe@spark.apache.org>

"
Cody Koeninger <cody@koeninger.org>,"Wed, 2 Nov 2016 10:36:26 -0500",Re: Handling questions in the mailing lists,Sean Owen <sowen@cloudera.com>,"So concrete things people could do

- users could tag subject lines appropriately to the component they're
asking about

- contributors could monitor user@ for tags relating to components
they've worked on.
I'd be surprised if my miss rate for any mailing list questions
well-labeled as Kafka was higher than 5%

- committers could be more aggressive about soliciting and merging PRs
to improve documentation.
It's a lot easier to answer even poorly-asked questions with a link to
relevant docs.

elf
's
to
ty
g
er
the
â€¦
ce.
 a
ing
re
 too
 to
. I
is
is was
no
st
a
ons
s
.

---------------------------------------------------------------------


"
Ricardo Almeida <ricardo.almeida@actnowib.com>,"Wed, 2 Nov 2016 17:23:04 +0100",Re: Handling questions in the mailing lists,Cody Koeninger <cody@koeninger.org>,"I fell Assaf point is quite relevant if we want to move this project
forward from the Spark user perspective (as I do). In fact, we're still
using 20th century tools (mailing lists) with some add-ons (like Stack
Overflow).

As usually, Sean and Cody's contributions are very to the point.
I fell it is indeed a matter of of culture (hard to enforce) and tools
(much easier). Isn't it?


e
@
of
d
n
â€¦
s
's
s,
s.
ck
this was
e
k
he
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 02 Nov 2016 17:13:49 +0000",Re: Handling questions in the mailing lists,"Ricardo Almeida <ricardo.almeida@actnowib.com>, Cody Koeninger <cody@koeninger.org>","Weâ€™ve discussed several times upgrading our communication tools, as far
back as 2014 and maybe even before that too. The bottom line is that we
canâ€™t due to ASF rules requiring the use of ASF-managed mailing lists.

For some history, see this discussion:

   -
   https://mail-archives.apache.org/mod_mbox/spark-user/201412.mbox/%3CCAOhmDzfL2COdysV8r5hZN8f=NqXM=f=oY5NO2dHWJ_kVEoP+Ng@mail.gmail.com%3E
   -
   https://mail-archives.apache.org/mod_mbox/spark-user/201501.mbox/%3CCAOhmDzec1JdsXQq3dDwAv7eLnzRidSkrsKKG0xKw=TKTxY_sYw@mail.gmail.com%3E

(Itâ€™s ironic that itâ€™s difficult to follow the past discussion on why we
canâ€™t change our official communication tools due to those very toolsâ€¦)

Nick
â€‹


e
@
of
d
n
â€¦
s
's
s,
s.
ck
this was
e
k
he
"
Matei Zaharia <matei.zaharia@gmail.com>,"Wed, 2 Nov 2016 10:16:09 -0700",Re: Anyone seeing a lot of Spark emails go to Gmail spam?,Pete Robbins <robbinspg@gmail.com>,"It might be useful to ask Apache Infra whether they have any information on these (e.g. what do their own spam metrics say, do they get any feedback from Google, etc). Unfortunately mailing lists seem to be less and less well supported by most email providers.

Matei

dev@spark user@spark etc but still get the occasional message marked as spam
and have just realized hundreds of messages to dev@ over the past month or so have been marked as spam for me by Gmail. I have no idea why but it's usually messages from Michael and Reynold, but not all of them. I'll see replies to the messages but not the original. Who knows. I can make a filter. I just wanted to give a heads up in case anyone else has been silently missing a lot of messages.

"
Reynold Xin <rxin@databricks.com>,"Wed, 2 Nov 2016 10:21:54 -0700",Re: Handling questions in the mailing lists,Nicholas Chammas <nicholas.chammas@gmail.com>,"Actually after talking with more ASF members, I believe the only policy is
that development decisions have to be made and announced on ASF properties
(dev list or jira), but user questions don't have to.

I'm going to double check this. If it is true, I would actually recommend
us moving entirely over the Q&A part of the user list to stackoverflow, or
at least make that the recommended way rather than the existing user list
which is not very scalable.

m>

as far
ists.
.com%3E
AOhmDzfL2COdysV8r5hZN8f=NqXM=f=oY5NO2dHWJ_kVEoP+Ng@mail.gmail.com%3E>
%3E
AOhmDzec1JdsXQq3dDwAv7eLnzRidSkrsKKG0xKw=TKTxY_sYw@mail.gmail.com%3E>
ssion on why we
oolsâ€¦)
k
ld
e
r
t
'd
ut
er
y this was
t.
w,
s
"
Russell Spitzer <russell.spitzer@gmail.com>,"Wed, 02 Nov 2016 17:29:44 +0000",Re: Anyone seeing a lot of Spark emails go to Gmail spam?,"Matei Zaharia <matei.zaharia@gmail.com>, Pete Robbins <robbinspg@gmail.com>","I had one bounce message last week, but haven't seen anything else, I also
do the skip inbox filter thing though.


"
Sean Owen <sowen@cloudera.com>,"Wed, 02 Nov 2016 17:44:20 +0000",Re: [VOTE] Release Apache Spark 2.0.2 (RC2),"Reynold Xin <rxin@databricks.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Sigs, license, etc are OK. There are no Blockers for 2.0.2, though here are
the 4 issues still open:

SPARK-14387 Enable Hive-1.x ORC compatibility with
spark.sql.hive.convertMetastoreOrc
SPARK-17957 Calling outer join and na.fill(0) and then inner join will miss
rows
SPARK-17981 Incorrectly Set Nullability to False in FilterExec
SPARK-18160 spark.files & spark.jars should not be passed to driver in yarn
mode

Running with Java 8, -Pyarn -Phive -Phive-thriftserver -Phadoop-2.7 on
Ubuntu 16, I am seeing consistent failures in this test below. I think we
very recently changed this so it could be legitimate. But does anyone else
see something like this? I have seen other failures in this test due to OOM
but my MAVEN_OPTS allows 6g of heap, which ought to be plenty.


- SPARK-18189: Fix serialization issue in KeyValueGroupedDataset *** FAILED
***
  isContain was true Interpreter output contained 'Exception':
  Welcome to
        ____              __
       / __/__  ___ _____/ /__
      _\ \/ _ \/ _ `/ __/  '_/
     /___/ .__/\_,_/_/ /_/\_\   version 2.0.2
        /_/

  Using Scala version 2.11.8 (OpenJDK 64-Bit Server VM, Java 1.8.0_102)
  Type in expressions to have them evaluated.
  Type :help for more information.

  scala>
  scala> keyValueGrouped:
org.apache.spark.sql.KeyValueGroupedDataset[Int,(Int, Int)] =
org.apache.spark.sql.KeyValueGroupedDataset@70c30f72

  scala> mapGroups: org.apache.spark.sql.Dataset[(Int, Int)] = [_1: int,
_2: int]

  scala> broadcasted: org.apache.spark.broadcast.Broadcast[Int] =
Broadcast(0)

  scala>
  scala>
  scala> dataset: org.apache.spark.sql.Dataset[Int] = [value: int]

  scala> org.apache.spark.SparkException: Job aborted due to stage failure:
Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in
stage 0.0 (TID 0, localhost):
com.google.common.util.concurrent.ExecutionError:
java.lang.ClassCircularityError:
io/netty/util/internal/__matchers__/org/apache/spark/network/protocol/MessageMatcher
  at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2261)
  at com.google.common.cache.LocalCache.get(LocalCache.java:4000)
  at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:4004)
  at
com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
  at
org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:841)
  at
org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create(GenerateSafeProjection.scala:188)
  at
org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create(GenerateSafeProjection.scala:36)
  at
org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:825)
  at
org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:822)
  at
org.apache.spark.sql.execution.ObjectOperator$.deserializeRowToObject(objects.scala:137)
  at
org.apache.spark.sql.execution.AppendColumnsExec$$anonfun$9.apply(objects.scala:251)
  at
org.apache.spark.sql.execution.AppendColumnsExec$$anonfun$9.apply(objects.scala:250)
  at
org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803)
  at
org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803)
  at
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
  at
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
  at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
  at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
  at org.apache.spark.scheduler.Task.run(Task.scala:86)
  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
  at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  at java.lang.Thread.run(Thread.java:745)
  Caused by: java.lang.ClassCircularityError:
io/netty/util/internal/__matchers__/org/apache/spark/network/protocol/MessageMatcher
  at java.lang.Class.forName0(Native Method)
  at java.lang.Class.forName(Class.java:348)
  at
io.netty.util.internal.JavassistTypeParameterMatcherGenerator.generate(JavassistTypeParameterMatcherGenerator.java:62)
  at
io.netty.util.internal.JavassistTypeParameterMatcherGenerator.generate(JavassistTypeParameterMatcherGenerator.java:54)
  at
io.netty.util.internal.TypeParameterMatcher.get(TypeParameterMatcher.java:42)
  at
io.netty.util.internal.TypeParameterMatcher.find(TypeParameterMatcher.java:78)
  at
io.netty.handler.codec.MessageToMessageEncoder.<init>(MessageToMessageEncoder.java:60)
  at
org.apache.spark.network.protocol.MessageEncoder.<init>(MessageEncoder.java:34)
  at
org.apache.spark.network.TransportContext.<init>(TransportContext.java:78)
  at
org.apache.spark.rpc.netty.NettyRpcEnv.downloadClient(NettyRpcEnv.scala:354)
  at
org.apache.spark.rpc.netty.NettyRpcEnv.openChannel(NettyRpcEnv.scala:324)
  at org.apache.spark.repl.ExecutorClassLoader.org
$apache$spark$repl$ExecutorClassLoader$$getClassFileInputStreamFromSparkRPC(ExecutorClassLoader.scala:90)
  at
org.apache.spark.repl.ExecutorClassLoader$$anonfun$1.apply(ExecutorClassLoader.scala:57)
  at
org.apache.spark.repl.ExecutorClassLoader$$anonfun$1.apply(ExecutorClassLoader.scala:57)
  at
org.apache.spark.repl.ExecutorClassLoader.findClassLocally(ExecutorClassLoader.scala:161)
  at
org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:80)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
  at java.lang.Class.forName0(Native Method)
  at java.lang.Class.forName(Class.java:348)
  at
io.netty.util.internal.JavassistTypeParameterMatcherGenerator.generate(JavassistTypeParameterMatcherGenerator.java:62)
  at
io.netty.util.internal.JavassistTypeParameterMatcherGenerator.generate(JavassistTypeParameterMatcherGenerator.java:54)
  at
io.netty.util.internal.TypeParameterMatcher.get(TypeParameterMatcher.java:42)
  at
io.netty.util.internal.TypeParameterMatcher.find(TypeParameterMatcher.java:78)
  at
io.netty.handler.codec.MessageToMessageEncoder.<init>(MessageToMessageEncoder.java:60)
  at
org.apache.spark.network.protocol.MessageEncoder.<init>(MessageEncoder.java:34)
  at
org.apache.spark.network.TransportContext.<init>(TransportContext.java:78)
  at
org.apache.spark.rpc.netty.NettyRpcEnv.downloadClient(NettyRpcEnv.scala:354)
  at
org.apache.spark.rpc.netty.NettyRpcEnv.openChannel(NettyRpcEnv.scala:324)
  at org.apache.spark.repl.ExecutorClassLoader.org
$apache$spark$repl$ExecutorClassLoader$$getClassFileInputStreamFromSparkRPC(ExecutorClassLoader.scala:90)
  at
org.apache.spark.repl.ExecutorClassLoader$$anonfun$1.apply(ExecutorClassLoader.scala:57)
  at
org.apache.spark.repl.ExecutorClassLoader$$anonfun$1.apply(ExecutorClassLoader.scala:57)
  at
org.apache.spark.repl.ExecutorClassLoader.findClassLocally(ExecutorClassLoader.scala:161)
  at
org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:80)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:411)
  at
org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:34)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
  at
org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:30)
  at java.lang.Class.forName0(Native Method)
  at java.lang.Class.forName(Class.java:348)
  at
org.codehaus.janino.ClassLoaderIClassLoader.findIClass(ClassLoaderIClassLoader.java:78)
  at org.codehaus.janino.IClassLoader.loadIClass(IClassLoader.java:254)
  at org.codehaus.janino.UnitCompiler.findTypeByName(UnitCompiler.java:6893)
  at
org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:5331)
  at
org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:5207)
  at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:5188)
  at org.codehaus.janino.UnitCompiler.access$12600(UnitCompiler.java:185)
  at
org.codehaus.janino.UnitCompiler$16.visitReferenceType(UnitCompiler.java:5119)
  at org.codehaus.janino.Java$ReferenceType.accept(Java.java:2880)
  at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:5159)
  at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:5414)
  at org.codehaus.janino.UnitCompiler.access$12400(UnitCompiler.java:185)
  at
org.codehaus.janino.UnitCompiler$16.visitArrayType(UnitCompiler.java:5117)
  at org.codehaus.janino.Java$ArrayType.accept(Java.java:2954)
  at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:5159)
  at org.codehaus.janino.UnitCompiler.access$16700(UnitCompiler.java:185)
  at
org.codehaus.janino.UnitCompiler$31.getParameterTypes2(UnitCompiler.java:8533)
  at
org.codehaus.janino.IClass$IInvocable.getParameterTypes(IClass.java:835)
  at org.codehaus.janino.IClass$IMethod.getDescriptor2(IClass.java:1063)
  at org.codehaus.janino.IClass$IInvocable.getDescriptor(IClass.java:849)
  at org.codehaus.janino.IClass.getIMethods(IClass.java:211)
  at org.codehaus.janino.IClass.getIMethods(IClass.java:199)
  at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:409)
  at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:393)
  at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:185)
  at
org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:347)
  at
org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1139)
  at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
  at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:322)
  at
org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:383)
  at
org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:315)
  at
org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:233)
  at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:192)
  at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:84)
  at
org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:887)
  at
org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:950)
  at
org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:947)
  at
com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
  at
com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
  at
com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
  at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)
  ... 26 more




"
"""Dongjoon Hyun""<dongjoon@apache.org>","Wed, 02 Nov 2016 18:32:26 -0000",Re: [VOTE] Release Apache Spark 2.0.2 (RC2),<dev@spark.apache.org>,"Hi, Sean.

The same failure blocks me, too.

- SPARK-18189: Fix serialization issue in KeyValueGroupedDataset *** FAILED ***

I used `-Pyarn -Phadoop-2.7 -Pkinesis-asl -Phive -Phive-thriftserver -Dsparkr` on CentOS 7 / OpenJDK1.8.0_111.

Dongjoon.


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Wed, 2 Nov 2016 11:36:17 -0700",Re: [VOTE] Release Apache Spark 2.0.2 (RC2),Dongjoon Hyun <dongjoon@apache.org>,"Looks like there is an issue with Maven (likely just the test itself
though). We should look into it.



"
Adam Roberts <AROBERTS@uk.ibm.com>,"Wed, 2 Nov 2016 18:47:28 +0000",Re: [VOTE] Release Apache Spark 2.0.2 (RC2),Reynold Xin <rxin@databricks.com>,"I'm seeing the same failure but manifesting itself as a stackoverflow, 
various operating systems and architectures (RHEL 71, CentOS 72, SUSE 12, 
Ubuntu 14 04 and 16 04 LTS)

Build and test options:
mvn -T 1C -Psparkr -Pyarn -Phadoop-2.7 -Phive -Phive-thriftserver 
-DskipTests clean package

mvn -Pyarn -Phadoop-2.7 -Phive -Phive-thriftserver 
-Dtest.exclude.tags=org.apache.spark.tags.DockerTest -fn test

-Xss2048k -Dspark.buffer.pageSize=1048576 -Xmx4g

Stacktrace (this is with IBM's latest SDK for Java 8):

  scala> org.apache.spark.SparkException: Job aborted due to stage 
failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost 
task 0.0 in stage 0.0 (TID 0, localhost): 
com.google.common.util.concurrent.ExecutionError: 
java.lang.StackOverflowError: operating system stack overflow
        at 
com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2261)
        at com.google.common.cache.LocalCache.get(LocalCache.java:4000)
        at 
com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:4004)
        at 
com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
        at 
org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:849)
        at 
org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create(GenerateSafeProjection.scala:188)
        at 
org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create(GenerateSafeProjection.scala:36)
        at 
org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:833)
        at 
org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:830)
        at 
org.apache.spark.sql.execution.ObjectOperator$.deserializeRowToObject(objects.scala:137)
... omitted the rest for brevity

Would also be useful to include this small but useful change that looks to 
have only just missed the cut: https://github.com/apache/spark/pull/14409




From:   Reynold Xin <rxin@databricks.com>
To:     Dongjoon Hyun <dongjoon@apache.org>
Cc:     ""dev@spark.apache.org"" <dev@spark.apache.org>
Date:   02/11/2016 18:37
Subject:        Re: [VOTE] Release Apache Spark 2.0.2 (RC2)



Looks like there is an issue with Maven (likely just the test itself 
though). We should look into it.


Hi, Sean.

The same failure blocks me, too.

- SPARK-18189: Fix serialization issue in KeyValueGroupedDataset *** 
FAILED ***

I used `-Pyarn -Phadoop-2.7 -Pkinesis-asl -Phive -Phive-thriftserver 
-Dsparkr` on CentOS 7 / OpenJDK1.8.0_111.

Dongjoon.

are
miss
yarn
we
else
OOM
FAILED
failure:
in
io/netty/util/internal/__matchers__/org/apache/spark/network/protocol/MessageMatcher
com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2261)
com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:841)
org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create(GenerateSafeProjection.scala:188)
org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create(GenerateSafeProjection.scala:36)
org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:825)
org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:822)
org.apache.spark.sql.execution.ObjectOperator$.deserializeRowToObject(objects.scala:137)
org.apache.spark.sql.execution.AppendColumnsExec$$anonfun$9.apply(objects.scala:251)
org.apache.spark.sql.execution.AppendColumnsExec$$anonfun$9.apply(objects.scala:250)
org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803)
org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803)
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
io/netty/util/internal/__matchers__/org/apache/spark/network/protocol/MessageMatcher
io.netty.util.internal.JavassistTypeParameterMatcherGenerator.generate(JavassistTypeParameterMatcherGenerator.java:62)
io.netty.util.internal.JavassistTypeParameterMatcherGenerator.generate(JavassistTypeParameterMatcherGenerator.java:54)
io.netty.util.internal.TypeParameterMatcher.get(TypeParameterMatcher.java:42)
io.netty.util.internal.TypeParameterMatcher.find(TypeParameterMatcher.java:78)
io.netty.handler.codec.MessageToMessageEncoder.<init>(MessageToMessageEncoder.java:60)
org.apache.spark.network.protocol.MessageEncoder.<init>(MessageEncoder.java:34)
org.apache.spark.network.TransportContext.<init>(TransportContext.java:78)
org.apache.spark.rpc.netty.NettyRpcEnv.downloadClient(NettyRpcEnv.scala:354)
org.apache.spark.rpc.netty.NettyRpcEnv.openChannel(NettyRpcEnv.scala:324)
$apache$spark$repl$ExecutorClassLoader$$getClassFileInputStreamFromSparkRPC(ExecutorClassLoader.scala:90)
org.apache.spark.repl.ExecutorClassLoader$$anonfun$1.apply(ExecutorClassLoader.scala:57)
org.apache.spark.repl.ExecutorClassLoader$$anonfun$1.apply(ExecutorClassLoader.scala:57)
org.apache.spark.repl.ExecutorClassLoader.findClassLocally(ExecutorClassLoader.scala:161)
org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:80)
io.netty.util.internal.JavassistTypeParameterMatcherGenerator.generate(JavassistTypeParameterMatcherGenerator.java:62)
io.netty.util.internal.JavassistTypeParameterMatcherGenerator.generate(JavassistTypeParameterMatcherGenerator.java:54)
io.netty.util.internal.TypeParameterMatcher.get(TypeParameterMatcher.java:42)
io.netty.util.internal.TypeParameterMatcher.find(TypeParameterMatcher.java:78)
io.netty.handler.codec.MessageToMessageEncoder.<init>(MessageToMessageEncoder.java:60)
org.apache.spark.network.protocol.MessageEncoder.<init>(MessageEncoder.java:34)
org.apache.spark.network.TransportContext.<init>(TransportContext.java:78)
org.apache.spark.rpc.netty.NettyRpcEnv.downloadClient(NettyRpcEnv.scala:354)
org.apache.spark.rpc.netty.NettyRpcEnv.openChannel(NettyRpcEnv.scala:324)
$apache$spark$repl$ExecutorClassLoader$$getClassFileInputStreamFromSparkRPC(ExecutorClassLoader.scala:90)
org.apache.spark.repl.ExecutorClassLoader$$anonfun$1.apply(ExecutorClassLoader.scala:57)
org.apache.spark.repl.ExecutorClassLoader$$anonfun$1.apply(ExecutorClassLoader.scala:57)
org.apache.spark.repl.ExecutorClassLoader.findClassLocally(ExecutorClassLoader.scala:161)
org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:80)
org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:34)
org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:30)
org.codehaus.janino.ClassLoaderIClassLoader.findIClass(ClassLoaderIClassLoader.java:78)
org.codehaus.janino.UnitCompiler.findTypeByName(UnitCompiler.java:6893)
org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:5331)
org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:5207)
org.codehaus.janino.UnitCompiler.access$12600(UnitCompiler.java:185)
org.codehaus.janino.UnitCompiler$16.visitReferenceType(UnitCompiler.java:5119)
org.codehaus.janino.UnitCompiler.access$12400(UnitCompiler.java:185)
org.codehaus.janino.UnitCompiler$16.visitArrayType(UnitCompiler.java:5117)
org.codehaus.janino.UnitCompiler.access$16700(UnitCompiler.java:185)
org.codehaus.janino.UnitCompiler$31.getParameterTypes2(UnitCompiler.java:8533)
org.codehaus.janino.IClass$IInvocable.getDescriptor(IClass.java:849)
org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:347)
org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1139)
org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:383)
org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:315)
org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:887)
org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:950)
org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:947)
com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)
version
if a
at:
https://repository.apache.org/content/repositories/orgapachespark-1210/
http://people.apache.org/~pwendell/spark-releases/spark-2.0.2-rc2-docs/
taking an
present
from
RC
2.0.2.

---------------------------------------------------------------------



Unless stated otherwise above:
IBM United Kingdom Limited - Registered in England and Wales with number 
741598. 
Registered office: PO Box 41, North Harbour, Portsmouth, Hampshire PO6 3AU
"
Prajwal Tuladhar <praj@infynyxx.com>,"Wed, 2 Nov 2016 19:04:30 +0000",Re: Anyone seeing a lot of Spark emails go to Gmail spam?,Russell Spitzer <russell.spitzer@gmail.com>,"Some messages from Apache mailing lists (Spark and ZK) were being marked as
spam by Gmail. After manually unmarking them as Spam few times, it seems to
have worked for me.




-- 
--
Cheers,
Praj
"
Kalpana Jalawadi <kalpana.jalawadi@gmail.com>,"Thu, 3 Nov 2016 00:35:47 +0530",BiMap BroadCast Variable - Kryo Serialization Issue,"user@spark.apache.org, dev@spark.apache.org","Hi,

I am getting Nullpointer exception due to Kryo Serialization issue, while
trying to read a BiMap broadcast variable. Attached is the code snippets.
Pointers shared here didn't help - link1
<http://stackoverflow.com/questions/33156095/spark-serialization-issue-with-hashmap>,
link2
<http://stackoverflow.com/questions/23962796/kryo-readobject-cause-nullpointerexception-with-arraylist>.
Spark version used is 1.6.x, but this was working with 1.3.x version.

Any help in this regard is much appreciated.

Exception:

com.esotericsoftware.kryo.KryoException: java.lang.NullPointerException
App > Serialization trace:
App > value (com.demo.BiMapWrapper)
App > at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1238)
App > at
org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:165)
App > at
org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:64)
App > at
org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:64)
App > at
org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:88)
App > at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)
App > at
com.manthan.aaas.algo.associationmining.impl.Test.lambda$execute$6abf5fd0$1(Test.java:39)
App > at
org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1015)
App > at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
App > at scala.collection.Iterator$class.foreach(Iterator.scala:727)
App > at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
App > at
scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
App > at
scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
App > at
scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
App > at scala.collection.AbstractIterator.to(Iterator.scala:1157)
App > at
App > at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
App > at
App > at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
App > at
org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:927)
App > at
org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:927)
App > at
org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1978)
App > at
org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1978)
App > at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
App > at org.apache.spark.scheduler.Task.run(Task.scala:89)
App > at
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
App > at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
App > at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
App > at java.lang.Thread.run(Thread.java:745)
App > Caused by: com.esotericsoftware.kryo.KryoException:
java.lang.NullPointerException
App > Serialization trace:
App > value (com.demo.BiMapWrapper)
App > at
com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.read(FieldSerializer.java:626)
App > at
com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:221)
App > at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)
App > at
org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:228)
App > at
org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:217)
App > at
org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:178)
App > at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1231)
App > ... 29 more
App > Caused by: java.lang.NullPointerException
App > at com.google.common.collect.HashBiMap.seekByKey(HashBiMap.java:180)
App > at com.google.common.collect.HashBiMap.put(HashBiMap.java:230)
App > at com.google.common.collect.HashBiMap.put(HashBiMap.java:218)
App > at
com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:135)
App > at
com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:17)
App > at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:648)
App > at
com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.read(FieldSerializer.java:605)
App > ... 35 more
App >
App > 16/11/02 18:39:01 dispatcher-event-loop-2 INFO TaskSetManager:
Starting task 17.1 in stage 1.0 (TID 19, ip-10-0-1-237.ec2.internal,
partition 17,PROCESS_LOCAL, 2076 bytes)
App > 16/11/02 18:39:01 task-result-getter-3 INFO TaskSetManager: Lost task
17.1 in stage 1.0 (TID 19) on executor ip-10-0-1-237.ec2.internal:
java.io.IOException (com.esotericsoftware.kryo.KryoException:
java.lang.NullPointerException
App > Serialization trace:
App > value (com.demo.BiMapWrapper)) [duplicate 1]
App > 16/11/02 18:39:01 dispatcher-event-loop-4 INFO TaskSetManager:
Starting task 17.2 in stage 1.0 (TID 20, ip-10-0-1-237.ec2.internal,
partition 17,PROCESS_LOCAL, 2076 bytes)
App > 16/11/02 18:39:01 task-result-getter-0 INFO TaskSetManager: Lost task
17.2 in stage 1.0 (TID 20) on executor ip-10-0-1-237.ec2.internal:
java.io.IOException (com.esotericsoftware.kryo.KryoException:
java.lang.NullPointerException
App > Serialization trace:
App > value (com.demo.BiMapWrapper)) [duplicate 2]
App > 16/11/02 18:39:01 dispatcher-event-loop-1 INFO TaskSetManager:
Starting task 17.3 in stage 1.0 (TID 21, ip-10-0-1-237.ec2.internal,
partition 17,PROCESS_LOCAL, 2076 bytes)
App > 16/11/02 18:39:01 task-result-getter-1 INFO TaskSetManager: Lost task
17.3 in stage 1.0 (TID 21) on executor ip-10-0-1-237.ec2.internal:
java.io.IOException (com.esotericsoftware.kryo.KryoException:
java.lang.NullPointerException
App > Serialization trace:
App > value (com.demo.BiMapWrapper)) [duplicate 3]

Best,
Kalpana
public class Test {
	BiMapWrapper biMapWrapper;
	
	

	public Test() {
		BiMap<String, Long> biMap = HashBiMap.create();
		biMap.put(""k1"", 1L);
		biMap.put(""k2"", 2L);
		this.biMapWrapper = new BiMapWrapper(biMap);
	}



	public void execute() {
		
		Broadcast<BiMapWrapper> broadcast = CurrentContext.getCurrentContext().broadcast(biMapWrapper);
		
		List<String> temp = new ArrayList<String>();
		temp.add(""k1"");
		
		JavaRDD<String> tempRDD = CurrentContext.getCurrentContext().parallelize(temp);
		
		JavaRDD<Long> tempRDD2 = tempRDD.map(item -> {
			
			if(broadcast.value().getVal().containsKey(item)){
				return 1L;
			}
			return 0L;
		});
		
		System.out.println(""tempRDD2 :: "" + tempRDD2.collect());
		
	}

}

======================


public class BiMapWrapper {

	BiMap<String, Long> value = HashBiMap.create();

	public BiMapWrapper(BiMap<String, Long> value) {
		this.value.putAll(value);
	}
	
	public BiMap<String, Long> getVal(){
		return value;
	}
}

====================

public class MyKryoRegistrator  implements KryoRegistrator, Serializable{
	 @Override
	  public void registerClasses(Kryo kryo) {
	      // Product POJO associated to a product Row from the DataFrame      
		 kryo.setInstantiatorStrategy(new StdInstantiatorStrategy());
	      kryo.register(BiMapWrapper.class); 
	  }
	 
	 
}

---------------------------------------------------------------------"
Ryan Blue <rblue@netflix.com.INVALID>,"Wed, 2 Nov 2016 13:43:29 -0700",Re: Updating Parquet dep to 1.9,Michael Allman <michael@videoamp.com>,"The stats problem is on the write side. Parquet compares byte buffers (used
for UTF8 strings also) using byte-wise comparison, but got it wrong and
compares the Java byte values, which are signed. UTF8 ordering is the same
as byte-wise comparison, but only if the bytes are compared as unsigned
values. So Parquet ends up with the wrong min and max if there are
characters where the sign bit / msb is set. For ASCII, the results are
identical, but other character sets, like latin1, end up with accented
characters out of order.

Parquet 1.9.0 suppresses the min and max values when the sort order that
produced them is incorrect to fix the correctness bug in applications like
SparkSQL. There is a property to override this if you know your data has
only ASCII characters, but by default min and max are not considered
reliable and are not used to eliminate row groups with predicate push-down.
Other types aren't affected and row group filters will still work.

1.9.0 also adds dictionary filtering to predicate push-down, which can be
used in many cases to skip row groups as well. This doesn't use the min and
max values so it will still work.

The issue for the stats ordering bug is PARQUET-686. Writes will be fixed
in 1.9.1, which I'd like to have out in the next couple of weeks.

My overall recommendation is to do the update to 1.9.0, which fixes the
logging problem, too.

rb




-- 
Ryan Blue
Software Engineer
Netflix
"
Cristian Opris <cristian.b.opris@gmail.com>,"Wed, 2 Nov 2016 22:24:25 +0000",Structured streaming aggregation - update mode,dev@spark.apache.org,"Hi,

I've been looking at planned jiras for this, but can't find anything. Is
this something that may be added soon ? It's not clear to me how
aggregation can realistically be used in a production scenario
without this..

Thanks,
Cristian
"
Michael Armbrust <michael@databricks.com>,"Wed, 2 Nov 2016 15:33:50 -0700",Re: Structured streaming aggregation - update mode,Cristian Opris <cristian.b.opris@gmail.com>,"Yeah, agreed.  As mentioned here
<https://github.com/apache/spark/pull/15702>, its near the top of my list.
I just opened SPARK-18234
<https://issues.apache.org/jira/browse/SPARK-18234> to track.


"
Yuhao Yang <hhbyyh@gmail.com>,"Wed, 2 Nov 2016 16:09:34 -0700",Re: Question about using collaborative filtering in MLlib,Zak H <zak.hassan1010@gmail.com>,"Hi Zak,

Indeed the function is missing in DataFrame-based API. I can probably
provide some quick prototype to see if it we can merge the function into
next release. I would send update here and feel free to give it a try.

Regards,
Yuhao

2016-11-01 10:00 GMT-07:00 Zak H <zak.hassan1010@gmail.com>:

"
Holden Karau <holden@pigscanfly.ca>,"Wed, 2 Nov 2016 16:48:25 -0700",Blocked PySpark changes,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Spark Developers & Maintainers,

I know we've been talking a lot about what we want changes we want in
PySpark to help keep it interesting and usable (see
http://apache-spark-developers-list.1001551.n3.nabble.com/Python-Spark-Improvements-forked-from-Spark-Improvement-Proposals-td19422.html).
that a reason behind the slow pace of a lot of the PySpark development is
the lack of dedicated Python reviewers.

For changes which are based around parity with an existing component,
Python contributors like myself can sometimes get reviewers from the
component (like ML) to take a look at our Python changes - but for core
changes it's even harder to get reviewers.

The general Python PR review dashboard
<https://spark-prs.appspot.com/#python> shows the a number of PRs
languishing - but to specifically call out a few:

   -

   pip installability - https://github.com/apache/spark/pull/15659
   -

   KMeans summary in Python - https://github.com/apache/spark/pull/13557
   -

   The various Anaconda/Virtualenv support PRs (none of them have had any
   luck with committer bandwidth)
   -

   PySpark ML models should have params finally starting to get committer
   review - but blocked for months (
   https://github.com/apache/spark/pull/14653 )
   -

   Python meta algorithms in Scala -
   https://github.com/apache/spark/pull/13794 (out of sync with master but
   waiting for months for a committer to say if they are interested in the
   feature or not)


For those following a lot of Python JIRAs you also probably noticed a lot
of Python related JIRAs being re-targeted for future versions that keep
getting bumped back.

The lack of core Python reviewers will make things like Arrow integration
difficult to achieve unless the situation changes.

This isn't meant to say that the current Python reviewers aren't good -
there just isn't enough Python committer bandwidth available to move these
things forward. The normal solution to this is adding more committers with
that focus area.

I'd love to hear y'alls thoughts on this.

Cheers,

Holden :)


-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
Reynold Xin <rxin@databricks.com>,"Wed, 2 Nov 2016 17:38:02 -0700",Re: [VOTE] Release Apache Spark 1.6.3 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","This vote is cancelled and I'm sending out a new vote for rc2 now.



"
Reynold Xin <rxin@databricks.com>,"Wed, 2 Nov 2016 17:40:03 -0700",[VOTE] Release Apache Spark 1.6.3 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version
1.6.3. The vote is open until Sat, Nov 5, 2016 at 18:00 PDT and passes if a
majority of at least 3+1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.6.3
[ ] -1 Do not release this package because ...


The tag to be voted on is v1.6.3-rc2
(1e860747458d74a4ccbd081103a0542a2367b14b)

This release candidate addresses 52 JIRA tickets:
https://s.apache.org/spark-1.6.3-jira

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.6.3-rc2-bin/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1212/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.6.3-rc2-docs/


=======================================
== How can I help test this release?
=======================================
If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions from 1.6.2.

================================================
== What justifies a -1 vote for this release?
================================================
This is a maintenance release in the 1.6.x series.  Bugs already present in
1.6.2, missing features, or bugs related to new features will not
necessarily block this release.
"
Chris Lin <chungfu27@hotmail.com>,"Wed, 2 Nov 2016 22:49:06 -0700 (MST)",Evolutionary algorithm (EA) in Spark,dev@spark.apache.org,"Hi All,

I would like to know if there is any plan to implement evolutionary
algorithm in Spark ML, such as particle swarm optimization, genetic
algorithm, ant colony optimization, etc.
Therefore, if someone is working on this in Spark or has already done, I
would like to contribute to it and get some guidance on how to go about it.

Regards,
Chris Lin



--

---------------------------------------------------------------------


"
Chris Lin <chungfu27@hotmail.com>,"Wed, 2 Nov 2016 22:52:37 -0700 (MST)",Evolutionary algorithm (EA) in Spark,dev@spark.apache.org,"Hi All,

I would like to know if there is any plan to implement evolutionary
algorithm in Spark ML, such as particle swarm optimization, genetic
algorithm, ant colony optimization, etc.
Therefore, if someone is working on this in Spark or has already done, I
would like to contribute to it and get some guidance on how to go about it.

Regards,
Chris Lin



--

---------------------------------------------------------------------


"
Nick Pentreath <nick.pentreath@gmail.com>,"Thu, 03 Nov 2016 06:48:29 +0000",Re: Question about using collaborative filtering in MLlib,"""dev@spark.apache.org"" <dev@spark.apache.org>","I have a PR for it - https://github.com/apache/spark/pull/12574

Sadly I've been tied up and haven't had a chance to work further on it.

The main issue outstanding is deciding on the transform semantics as well
as performance testing.

Any comments / feedback welcome especially on transform semantics.

N
"
Krishna Kalyan <krishnakalyan3@gmail.com>,"Thu, 3 Nov 2016 16:16:47 +0100",Running Unit Tests in pyspark failure,dev <dev@spark.apache.org>,"Hello,
I am trying to run unit tests on pyspark.

When I try to run unit test I am faced with errors.
krishna@Krishna:~/Experiment/spark$ ./python/run-tests
Running PySpark tests. Output is in /Users/krishna/Experiment/
spark/python/unit-tests.log
Will test against the following Python executables: ['python2.6']
Will test the following Python modules: ['pyspark-core', 'pyspark-ml',
'pyspark-mllib', 'pyspark-sql', 'pyspark-streaming']
Please install unittest2 to test with Python 2.6 or earlier
Had test failures in pyspark.sql.tests with python2.6; see logs.

and when I try to Install unittest2, It says requirement already satisfied.

krishna@Krishna:~/Experiment/spark$ sudo pip install --upgrade unittest2
Password:
Requirement already up-to-date: unittest2 in /usr/local/lib/python2.7/site-
packages
Requirement already up-to-date: argparse in
/usr/local/lib/python2.7/site-packages
(from unittest2)
Requirement already up-to-date: six>=1.4 in
/usr/local/lib/python2.7/site-packages
(from unittest2)
Requirement already up-to-date: traceback2 in
/usr/local/lib/python2.7/site-packages
(from unittest2)
Requirement already up-to-date: linecache2 in
/usr/local/lib/python2.7/site-packages
(from traceback2->unittest2)

Help!

Thanks,
Krishna
"
Krishna Kalyan <krishnakalyan3@gmail.com>,"Thu, 3 Nov 2016 17:46:59 +0100",Re: Running Unit Tests in pyspark failure,dev <dev@spark.apache.org>,"I could resolve this by passing the argument below
 ./python/run-tests --python-executables=python2.7

Thanks,
Krishna


"
Per Ullberg <per.ullberg@klarna.com>,"Thu, 3 Nov 2016 17:55:41 +0100",.,dev@spark.apache.org,"-- 

*Per Ullberg*
Data Vault Tech Lead
Odin Uppsala
+46 701612693 <+46+701612693>

Klarna AB (publ)
SveavÃ¤gen 46, 111 34 Stockholm
Tel: +46 8 120 120 00 <+46812012000>
Reg no: 556737-0431
klarna.com
"
Michael Armbrust <michael@databricks.com>,"Thu, 3 Nov 2016 10:58:27 -0700",Re: [VOTE] Release Apache Spark 1.6.3 (RC2),Reynold Xin <rxin@databricks.com>,1
=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@databricks.com>,"Thu, 3 Nov 2016 20:57:09 +0100",Re: [VOTE] Release Apache Spark 1.6.3 (RC2),Michael Armbrust <michael@databricks.com>,1
emlyn <Emlyn.Corrin@microsoft.com>,"Thu, 3 Nov 2016 13:15:25 -0700 (MST)",AnalysisException in first/last during aggregation since 2.0.1,dev@spark.apache.org,"The following simple (pyspark) code fails in Spark 2.0.1:


It only fails with all three arguments to .agg, removing any of them
prevents the failure. Similar code in Java also fails in the same way, so it
isn't specific to the Python API.

It runs without error in Spark 2.0.0, so I suspect it might be caused by the
fix to SPARK-16648.

I've opened a Jira ticket (SPARK-18172) but thought I'd also post to the
mailing list to make sure it's noticed.

The full stack trace is below:





--

---------------------------------------------------------------------


"
Yin Huai <yhuai@databricks.com>,"Thu, 3 Nov 2016 13:29:38 -0700",Re: [VOTE] Release Apache Spark 1.6.3 (RC2),=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@databricks.com>,"+1


n
if a
================
================
=========================
=========================
t
"
Davies Liu <davies@databricks.com>,"Thu, 3 Nov 2016 14:30:34 -0700",Re: [VOTE] Release Apache Spark 1.6.3 (RC2),Reynold Xin <rxin@databricks.com>,"+1


---------------------------------------------------------------------


"
"""Dongjoon Hyun""<dongjoon@apache.org>","Fri, 04 Nov 2016 01:43:56 -0000",Re: [VOTE] Release Apache Spark 1.6.3 (RC2),<dev@spark.apache.org>,"+1 (non-binding)

It's built and tested on CentOS 6.8 / OpenJDK 1.8.0_111, too.

Cheers,
Dongjoon.


---------------------------------------------------------------------


"
Jeff Zhang <zjffdu@gmail.com>,"Fri, 04 Nov 2016 02:03:34 +0000",Re: [VOTE] Release Apache Spark 1.6.3 (RC2),"Dongjoon Hyun <dongjoon@apache.org>, dev@spark.apache.org","+1

Dongjoon Hyun <dongjoon@apache.org>äºŽ2016å¹´11æœˆ4æ—¥å‘¨äº” ä¸Šåˆ9:44å†™é“ï¼š

:
s
t:
/
================
================
an
=========================
=========================
"
Liwei Lin <lwlin7@gmail.com>,"Fri, 4 Nov 2016 10:17:06 +0800",Re: [VOTE] Release Apache Spark 1.6.3 (RC2),dev <dev@spark.apache.org>,"+1 (non-binding)

Cheers,
Liwei


—¥å‘¨äº” ä¸Šåˆ9:44å†™é“ï¼š
=================
=================
==========================
==========================
"
"""Sean Busbey""<busbey@apache.org>","Fri, 04 Nov 2016 03:43:52 -0000",Re: [VOTE] Release Apache Spark 1.6.3 (RC2),<dev@spark.apache.org>,"Heads up that 1.6.3 RC2 might be impacted by the change of the JSON.org licenses to category-x (disallowed dependency license) described in SPARK-18262.

Not sure if I'll have time to evaluate in time to cast a non-binding -1 before the voting window closes.

-
busbey


---------------------------------------------------------------------


"
Kousuke Saruta <sarutak@oss.nttdata.co.jp>,"Fri, 4 Nov 2016 13:51:35 +0900",Re: [VOTE] Release Apache Spark 1.6.3 (RC2),"Reynold Xin <rxin@databricks.com>,
 ""dev@spark.apache.org"" <dev@spark.apache.org>","+1 (non-binding)

- Kousuke



---------------------------------------------------------------------


"
Joseph Bradley <joseph@databricks.com>,"Thu, 3 Nov 2016 22:56:09 -0700",Re: [VOTE] Release Apache Spark 1.6.3 (RC2),sarutak@oss.nttdata.co.jp,1
Sean Owen <sowen@cloudera.com>,"Fri, 04 Nov 2016 09:00:18 +0000",Re: [VOTE] Release Apache Spark 1.6.3 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>, Reynold Xin <rxin@databricks.com>","Likewise, ran my usual tests on Ubuntu with
yarn/hive/hive-thriftserver/hadoop-2.6 on JDK 8 and all passed. Sigs and
licenses are OK. +1


 a
===============
===============
========================
========================
"
Ricardo Almeida <ricardo.almeida@actnowib.com>,"Fri, 4 Nov 2016 17:06:33 +0100",Re: [VOTE] Release Apache Spark 1.6.3 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1 (non-binding)

tested over Ubuntu / OpenJDK 1.8.0_111


adoop-2.6
f a
===============
===============
========================
========================
"
Yin Huai <yhuai@databricks.com>,"Fri, 4 Nov 2016 09:26:24 -0700",Re: [VOTE] Release Apache Spark 2.0.2 (RC2),Reynold Xin <rxin@databricks.com>,1
vonnagy <ivan@vadio.com>,"Fri, 4 Nov 2016 10:14:48 -0700 (MST)",Re: Continuous warning while consuming using new kafka-spark010 API,dev@spark.apache.org,"Nitin,

I am getting the similar issues using Spark 2.0.1 and Kafka 0.10. I have to
jobs, one that uses a Kafka stream and one that uses just the KafkaRDD. 

With the KafkaRDD, I continually get the ""Failed to get records"". I have
adjusted the polling with `spark.streaming.kafka.consumer.poll.ms` and the
size of records with Kafka's `max.poll.records`. Even when it gets records
it is extremely slow.

When working with multiple KafkaRDDs in parallel I get the dreaded
`ConcurrentModificationException`. The Spark logic is supposed to use a
CachedKafkaConsumer based on the topic and partition. This is supposed to
guarantee thread safety, but I continually get this error along with the
polling timeout.

Has anyone else tried to use Spark 2 with Kafka 0.10 and had any success. At
this point it is completely useless in my experience. With Spark 1.6 and
Kafka 0.8.x, I never had these problems.



--

---------------------------------------------------------------------


"
"""Owen O'Malley"" <omalley@apache.org>","Fri, 4 Nov 2016 10:30:27 -0700",Hadoop Summit EU 2017,dev <dev@spark.apache.org>,"The DataWorks Summit EU 2017 (including Hadoop Summit) is going to be in
Munich April 5-6 2017
.  Iâ€™ve pasted the text from the CFP below.

Would you like to share your knowledge with the best and brightest in the
data community? If so, we encourage you to submit an abstract for DataWorks
Summit with Hadoop Summit being held on April 5-6, 2017 at The ICM â€“
International Congress Center Munich.

DataWorks Summit with Hadoop Summit is the premier event for business and
technical audiences who want to learn how data is transforming business and
the underlying technologies that are driving that change.

Our 2017 tracks include:
Â·         Applications
Â·         Enterprise Adoption
Â·         Data Processing & Warehousing
Â·         Apache Hadoop Core Internals
Â·         Governance & Security
Â·         IoT & Streaming
Â·         Cloud & Operations
Â·         Apache Spark & Data Science
For questions or additional information, please contact Joshua Woodward.

Deadline: Friday, November 11, 2017.
Submission Link: http://dataworkssummit.com/munich-2017/abstracts/submit-
abstract/

.. Owen
"
Cody Koeninger <cody@koeninger.org>,"Fri, 4 Nov 2016 12:31:37 -0500",Re: Continuous warning while consuming using new kafka-spark010 API,vonnagy <ivan@vadio.com>,"I answered the duplicate post on the user mailing list, I'd say keep
the discussion there.


---------------------------------------------------------------------


"
Cody Koeninger <cody@koeninger.org>,"Fri, 4 Nov 2016 12:39:29 -0500",Anyone want to weigh in on a Kafka DStreams api change?,"""dev@spark.apache.org"" <dev@spark.apache.org>","SPARK-17510

https://github.com/apache/spark/pull/15132

It's for allowing tweaking of rate limiting on a per-partition basis

---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Fri, 4 Nov 2016 11:20:35 -0700",Re: [VOTE] Release Apache Spark 2.0.2 (RC2),Reynold Xin <rxin@databricks.com>,1
Joseph Bradley <joseph@databricks.com>,"Fri, 4 Nov 2016 11:38:33 -0700",Re: [VOTE] Release Apache Spark 2.0.2 (RC2),Michael Armbrust <michael@databricks.com>,1
Sean Owen <sowen@cloudera.com>,"Fri, 04 Nov 2016 18:44:36 +0000",Re: [VOTE] Release Apache Spark 2.0.2 (RC2),"Reynold Xin <rxin@databricks.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","I guess it's worth explicitly stating that I think we need another RC one
way or the other because this test seems to consistently fail. It was a
(surprising) last-minute regression. I think I'd have to say -1 only for
this.

Reverting https://github.com/apache/spark/pull/15706 for branch-2.0 would
unblock this. There's also some discussion about an alternative resolution
for the test problem.


"
=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@databricks.com>,"Fri, 4 Nov 2016 19:49:27 +0100",Re: [VOTE] Release Apache Spark 2.0.2 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>",1
Reynold Xin <rxin@databricks.com>,"Fri, 4 Nov 2016 15:28:06 -0700",Re: [VOTE] Release Apache Spark 2.0.2 (RC2),Sean Owen <sowen@cloudera.com>,"I will cut a new one once https://github.com/apache/spark/pull/15774 gets
in.



"
Weiqing Yang <yangweiqing001@gmail.com>,"Fri, 4 Nov 2016 21:27:18 -0700",Re: [VOTE] Release Apache Spark 1.6.3 (RC2),Ricardo Almeida <ricardo.almeida@actnowib.com>,"+1 (non-binding)

Built and tested on CentOS Linux release 7.0.1406 ï¼ openjdk version
""1.8.0_111"".


hadoop-2.6
m
:
n
if a
================
================
=========================
=========================
t
"
"""=?gb18030?B?xLjR08Tqo6hZREK8vMr11qez1qOp?="" <1820150327@qq.com>","Sat, 5 Nov 2016 20:27:55 +0800","why visitCreateFileFormat doesn`t support  hive STORED BY ,just support store as ","""=?gb18030?B?dXNlcg==?="" <user@spark.apache.org>, ""=?gb18030?B?ZGV2?="" <dev@spark.apache.org>","why visitCreateFileFormat doesn`t support  hive STORED BY ,just support story as 
when i update spark1.6.2 to spark2.0.1  
so what i want to ask is .does it on plan to support hive stored by ?  or never support that ?
configureOutputJobProperties is quit important ,is there any other method to instand?

 override def visitCreateFileFormat(
      ctx: CreateFileFormatContext): CatalogStorageFormat = withOrigin(ctx) {
    (ctx.fileFormat, ctx.storageHandler) match {
      // Expected format: INPUTFORMAT input_format OUTPUTFORMAT output_format
      case (c: TableFileFormatContext, null) =>
        visitTableFileFormat(c)
      // Expected format: SEQUENCEFILE | TEXTFILE | RCFILE | ORC | PARQUET | AVRO
      case (c: GenericFileFormatContext, null) =>
        visitGenericFileFormat(c)
      case (null, storageHandler) =>
        operationNotAllowed(""STORED BY"", ctx)
      case _ =>
        throw new ParseException(""Expected either STORED AS or STORED BY, not both"", ctx)
    }
  }"
Reynold Xin <rxin@databricks.com>,"Sat, 5 Nov 2016 18:53:32 -0700",[VOTE] Release Apache Spark 1.6.3 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","The vote has passed with the following +1 votes and no -1 votes.

+1

Reynold Xin*
Herman van HÃ¶vell tot Westerflier
Yin Huai*
Davies Liu
Dongjoon Hyun
Jeff Zhang
Liwei Lin
Kousuke Saruta
Joseph Bradley*
Sean Owen*
Ricardo Almeida
Weiqing Yang

* = binding

I will work on packaging the release.





 a
===============
===============
========================
========================
"
shyla <deshpandeshyla@gmail.com>,"Sun, 6 Nov 2016 18:13:17 -0700 (MST)","Structured Streaming with Kafka Source, does it work??",dev@spark.apache.org,"I am trying to do Structured Streaming with Kafka Source. Please let me know
where I can find some sample code for this. Thanks



--

---------------------------------------------------------------------


"
Jayaradha Natarajan <jayaradhaa@gmail.com>,"Sun, 6 Nov 2016 17:17:06 -0800","Re: Structured Streaming with Kafka Source, does it work??",shyla <deshpandeshyla@gmail.com>,"Shyla!

Check
https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html

Thanks,
Jayaradha


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 07 Nov 2016 02:25:20 +0000",Using mention-bot to automatically ping potential reviewers,Spark dev list <dev@spark.apache.org>,"Howdy folks,

I wonder if anybody has ever used Facebook's mention-bot in a project:

https://github.com/facebook/mention-bot

Seems like a useful tool to help address the problem of figuring out who to
ping for review.

If you've used it, what was your experience? Do you think it would be
helpful for Spark?

Nick
"
Matei Zaharia <matei.zaharia@gmail.com>,"Sun, 6 Nov 2016 18:32:57 -0800","Re: Structured Streaming with Kafka Source, does it work??",shyla deshpande <deshpandeshyla@gmail.com>,"The Kafka source will only appear in 2.0.2 -- see this thread for the current release candidate: https://lists.apache.org/thread.html/597d630135e9eb3ede54bb0cc0b61a2b57b189588f269a64b58c9243@%3Cdev.spark.apache.org%3E . You can try that right now if you want from the staging Maven repo shown there. The vote looks likely to pass so an actual release should hopefully also be out soon.

Matei

socket text stream . I think structured streaming with kafka source not yet supported.
me some sample code or direction.
https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html <https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html>
me know
http://apache-spark-developers-list.1001551.n3.nabble.com/Structured-Streaming-with-Kafka-Source-does-it-work-tp19748.html <http://apache-spark-developers-list.1001551.n3.nabble.com/Structured-Streaming-with-Kafka-Source-does-it-work-tp19748.html>
Nabble.com.
<mailto:dev-unsubscribe@spark.apache.org>

"
Holden Karau <holden@pigscanfly.ca>,"Sun, 6 Nov 2016 18:43:34 -0800",Re: Using mention-bot to automatically ping potential reviewers,Nicholas Chammas <nicholas.chammas@gmail.com>,"So according the documentation it mostly uses blame lines which _might_ not
be the best fit for Spark (since many of the people in the blame lines
aren't going to have permission to commit the code). (Although it's
possible that the algorithm that is actually used does more than the one
described in the documentation).

I'd love to know what peoples experiences of using this with other projects
with a similar structure as Spark (small set of committers to a large set
of contributors).

Even with that reservation it could be interesting to try out since this is
a really big problem for new contributors looking to participate in Spark
and the spark-prs dashboard doesn't seem to be catching everything.




-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
Reynold Xin <rxin@databricks.com>,"Sun, 6 Nov 2016 19:03:43 -0800",Re: Handling questions in the mailing lists,Nicholas Chammas <nicholas.chammas@gmail.com>,"OK I've checked on the ASF member list (which is private so there is no
public archive).

It is not against any ASF rule to recommend StackOverflow as a place for
users to ask questions. I don't think we can or should delete the existing
user@spark list either, but we can certainly make SO more visible than it
is.




s
s
r
 as far
lists.
CAOhmDzfL2COdysV8r5hZN8f=NqXM=f=oY5NO2dHWJ_kVEoP+Ng@mail.gmail.com%3ECAOhmDzec1JdsXQq3dDwAv7eLnzRidSkrsKKG0xKw=TKTxY_sYw@mail.gmail.com%3E>
ussion on why we
toolsâ€¦)
A
e
t
ve
ly this
r
s
ts
"
Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Mon, 7 Nov 2016 05:06:28 +0100",Re: Handling questions in the mailing lists,dev@spark.apache.org,"You have to remember that Stack Overflow crowd (like me) is highly
opinionated, so many questions, which could be just fine on the mailing
list, will be quickly downvoted and / or closed as off-topic. Just
saying...

-- 
Best, 
Maciej



"
Reynold Xin <rxin@databricks.com>,"Sun, 6 Nov 2016 20:08:32 -0800",Re: Handling questions in the mailing lists,Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"You have substantially underestimated how opinionated people can be on
mailing lists too :)


g
d
or
t
, as far
 lists.
CCAOhmDzfL2COdysV8r5hZN8f=NqXM=f=oY5NO2dHWJ_kVEoP+Ng@mail.gmail.com%3E>
CCAOhmDzec1JdsXQq3dDwAv7eLnzRidSkrsKKG0xKw=TKTxY_sYw@mail.gmail.com%3E>
cussion on why we
 toolsâ€¦)
:
e
s
o
@
RA
e
be
e
d
e
ely this
l
s
"
Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Mon, 7 Nov 2016 06:54:17 +0100",Re: Handling questions in the mailing lists,Reynold Xin <rxin@databricks.com>,"Damn, I always thought that mailing list is only for nice and welcoming
people and there is nothing to do for me here >:)

To be serious though, there are many questions on the users list which
would fit just fine on SO but it is not true in general. There are
dozens of questions which are to broad, opinion based, ask for external
resources and so on. If you want to direct users to SO you have to help
them to decide if it is the right channel. Otherwise it will just create
a really bad experience for both seeking help and active answerers.
Former ones will be downvoted and bashed, latter ones will have to deal
with handling all the junk and the number of active Spark users with
moderation privileges is really low (with only Massg and me being able
to directly close duplicates).

Believe me, I've seen this before.


"
Reynold Xin <rxin@databricks.com>,"Sun, 6 Nov 2016 22:32:38 -0800",Re: Handling questions in the mailing lists,Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"This is an excellent point. If we do go ahead and feature SO as a way for
users to ask questions more prominently, as someone who knows SO very well,
would you be willing to help write a short guideline (ideally the shorter
the better, which makes it hard) to direct what goes to user@ and what goes
to SO?



s
ll
is
s).
ng
:
e
s, as far
e
g lists.
3CCAOhmDzfL2COdysV8r5hZN8f=NqXM=f=oY5NO2dHWJ_kVEoP+Ng@mail.gmail.com%3E>
3CCAOhmDzec1JdsXQq3dDwAv7eLnzRidSkrsKKG0xKw=TKTxY_sYw@mail.gmail.com%3E>
scussion on why
very toolsâ€¦)
e
s
re
Rs
to
:
r@
e
t
r
.
s
tely this
y
n
t
-
"
"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Mon, 7 Nov 2016 00:09:41 -0700 (MST)",RE: Handling questions in the mailing lists,dev@spark.apache.org,"There are other options as well. For example hosting an answerhub (www.answerhub.com<http://www.answerhub.com>) or other similar separate Q&A service.
BTW, I believe the main issue is not how opinionated people are but who is answering questions.
Today there are already people asking (and getting answers) on SO (including myself). The problem is that many people do not go to SO.
The problem I see is how to â€œbumpâ€ up questions which are not being answered to someone more likely to be able to answer them. Simple questions can be answered by many people, many of them even newbies who ran into the issue themselves.
The main issue is that the more complex the question, the less people there are who can answer it and those peopleâ€™s bandwidth is already clogged by other questions.
We could for example try to create tags on SO for â€œbasic questionsâ€, â€œmediumâ€, â€œadvancedâ€. Provide guidelines to ask first on basic, if not answered after X days then add the medium tag etc. Downvote people who donâ€™t go by the process. This would mean that committers for example can look at advanced only tag and have a manageable number of questions they can help with while others can answer medium and basic.

I agree that some things are not good for SO. Basically stuff which asks for opinion is such but most cases in the mailing list are either â€œhow do I solve this bugâ€ or â€œhow do I do Xâ€. Either of those two are good for SO.


Assaf.



From: rxin [via Apache Spark Developers List] [mailto:ml-node+s1001551n19757h56@n3.nabble.com]
Sent: Monday, November 07, 2016 8:33 AM
To: Mendelson, Assaf
Subject: Re: Handling questions in the mailing lists

This is an excellent point. If we do go ahead and feature SO as a way for users to ask questions more prominently, as someone who knows SO very well, would you be willing to help write a short guideline (ideally the shorter the better, which makes it hard) to direct what goes to user@ and what goes to SO?



Damn, I always thought that mailing list is only for nice and welcoming people and there is nothing to do for me here >:)

To be serious though, there are many questions on the users list which would fit just fine on SO but it is not true in general. There are dozens of questions which are to broad, opinion based, ask for external resources and so on. If you want to direct users to SO you have to help them to decide if it is the right channel. Otherwise it will just create a really bad experience for both seeking help and active answerers. Former ones will be downvoted and bashed, latter ones will have to deal with handling all the junk and the number of active Spark users with moderation privileges is really low (with only Massg and me being able to directly close duplicates).

Believe me, I've seen this before.
You have substantially underestimated how opinionated people can be on mailing lists too :)


You have to remember that Stack Overflow crowd (like me) is highly opinionated, so many questions, which could be just fine on the mailing list, will be quickly downvoted and / or closed as off-topic. Just saying...

--

Best,

Maciej

OK I've checked on the ASF member list (which is private so there is no public archive).

It is not against any ASF rule to recommend StackOverflow as a place for users to ask questions. I don't think we can or should delete the existing user@spark list either, but we can certainly make SO more visible than it is.



Actually after talking with more ASF members, I believe the only policy is that development decisions have to be made and announced on ASF properties (dev list or jira), but user questions don't have to.

I'm going to double check this. If it is true, I would actually recommend us moving entirely over the Q&A part of the user list to stackoverflow, or at least make that the recommended way rather than the existing user list which is not very scalable.



Weâ€™ve discussed several times upgrading our communication tools, as far back as 2014 and maybe even before that too. The bottom line is that we canâ€™t due to ASF rules requiring the use of ASF-managed mailing lists.

For some history, see this discussion:
Â·         https://mail-archives.apache.org/mod_mbox/spark-user/201412.mbox/%3CCAOhmDzfL2COdysV8r5hZN8f=NqXM=f=oY5NO2dHWJ_kVEoP+Ng@mail.gmail.com%3E
Â·         https://mail-archives.apache.org/mod_mbox/spark-user/201501.mbox/%3CCAOhmDzec1JdsXQq3dDwAv7eLnzRidSkrsKKG0xKw=TKTxY_sYw@mail.gmail.com%3E

(Itâ€™s ironic that itâ€™s difficult to follow the past discussion on why we canâ€™t change our official communication tools due to those very toolsâ€¦)

Nick
â€‹

I fell Assaf point is quite relevant if we want to move this project forward from the Spark user perspective (as I do). In fact, we're still using 20th century tools (mailing lists) with some add-ons (like Stack Overflow).

As usually, Sean and Cody's contributions are very to the point.
I fell it is indeed a matter of of culture (hard to enforce) and tools (much easier). Isn't it?

So concrete things people could do

- users could tag subject lines appropriately to the component they're
asking about

- contributors could monitor user@ for tags relating to components
they've worked on.
I'd be surprised if my miss rate for any mailing list questions
well-labeled as Kafka was higher than 5%

- committers could be more aggressive about soliciting and merging PRs
to improve documentation.
It's a lot easier to answer even poorly-asked questions with a link to
relevant docs.

elf
's
to
ty
ndEmail.jtp?type=node&node=19757&i=7>>
g
er
the
â€¦
ode=19757&i=8>]
e=19757&i=9>
ce.
 a
ing
re
 too
 to
. I
is
ndEmail.jtp?type=node&node=19757&i=10>>
is was
no
st
a
ons
s
.
---------------------------------------------------------------------
=19757&i=11>rg







________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/Handling-questions-in-the-mailing-lists-tp19690p19757.html
To start a new topic under Apache Spark Developers List, email ml-node+s1001551n1h20@n3.nabble.com<mailto:ml-node+s1001551n1h20@n3.nabble.com>
spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=1&code=YXNzYWYubWVuZGVsc29uQHJzYS5jb218MXwtMTI4OTkxNTg1Mg==>.
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>




--
3.nabble.com/Handling-questions-in-the-mailing-lists-tp19690p19758.html
om."
Matei Zaharia <matei.zaharia@gmail.com>,"Sun, 6 Nov 2016 23:45:24 -0800",Re: Handling questions in the mailing lists,"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Even for the mailing list, I'd love to have a short set of instructions on how to submit your questions (maybe on http://spark.apache.org/community.html or maybe in the welcome email when you subscribe). It would be great if someone added that. After all, we have such instructions for contributing PRs, for example.

Matei

(www.answerhub.com <http://www.answerhub.com/>) or other similar separate Q&A service.
who is answering questions.
(including myself). The problem is that many people do not go to SO.
are not being answered to someone more likely to be able to answer them. Simple questions can be answered by many people, many of them even newbies who ran into the issue themselves.
there are who can answer it and those peopleâ€™s bandwidth is already clogged by other questions.
questionsâ€, â€œmediumâ€, â€œadvancedâ€. Provide guidelines to ask first on basic, if not answered after X days then add the medium tag etc. Downvote people who donâ€™t go by the process. This would mean that committers for example can look at advanced only tag and have a manageable number of questions they can help with while others can answer medium and basic.
asks for opinion is such but most cases in the mailing list are either â€œhow do I solve this bugâ€ or â€œhow do I do Xâ€. Either of those two are good for SO.
email] <x-msg://40/user/SendEmail.jtp?type=node&node=19758&i=0>] 
for users to ask questions more prominently, as someone who knows SO very well, would you be willing to help write a short guideline (ideally the shorter the better, which makes it hard) to direct what goes to user@ and what goes to SO?
welcoming people and there is nothing to do for me here >:)
would fit just fine on SO but it is not true in general. There are dozens of questions which are to broad, opinion based, ask for external resources and so on. If you want to direct users to SO you have to help them to decide if it is the right channel. Otherwise it will just create a really bad experience for both seeking help and active answerers. Former ones will be downvoted and bashed, latter ones will have to deal with handling all the junk and the number of active Spark users with moderation privileges is really low (with only Massg and me being able to directly close duplicates).
mailing lists too :)
opinionated, so many questions, which could be just fine on the mailing list, will be quickly downvoted and / or closed as off-topic. Just saying...
no public archive).
for users to ask questions. I don't think we can or should delete the existing user@spark list either, but we can certainly make SO more visible than it is.
policy is that development decisions have to be made and announced on ASF properties (dev list or jira), but user questions don't have to. 
recommend us moving entirely over the Q&A part of the user list to stackoverflow, or at least make that the recommended way rather than the existing user list which is not very scalable. 
tools, as far back as 2014 and maybe even before that too. The bottom line is that we canâ€™t due to ASF rules requiring the use of ASF-managed mailing lists.
https://mail-archives.apache.org/mod_mbox/spark-user/201412.mbox/%3CCAOhmDzfL2COdysV8r5hZN8f=NqXM=f=oY5NO2dHWJ_kVEoP+Ng@...%3E <https://mail-archives.apache.org/mod_mbox/spark-user/201412.mbox/%3CCAOhmDzfL2COdysV8r5hZN8f=NqXM=f=oY5NO2dHWJ_kVEoP+Ng@mail.gmail.com%3E>
https://mail-archives.apache.org/mod_mbox/spark-user/201501.mbox/%3CCAOhmDzec1JdsXQq3dDwAv7eLnzRidSkrsKKG0xKw=TKTxY_sYw@...%3E <https://mail-archives.apache.org/mod_mbox/spark-user/201501.mbox/%3CCAOhmDzec1JdsXQq3dDwAv7eLnzRidSkrsKKG0xKw=TKTxY_sYw@mail.gmail.com%3E>
discussion on why we canâ€™t change our official communication tools due to those very toolsâ€¦)
forward from the Spark user perspective (as I do). In fact, we're still using 20th century tools (mailing lists) with some add-ons (like Stack Overflow).
(much easier). Isn't it?
development itself
sympathize
that's
advanced@ to
JIRA
lack of
community
<x-msg://40/user/SendEmail.jtp?type=node&node=19757&i=7>>
mailing
developer
and
would
maybe
lost in the
internalsâ€¦
<x-msg://40/user/SendEmail.jtp?type=node&node=19757&i=8>]
<x-msg://40/user/SendEmail.jtp?type=node&node=19757&i=9>
has
experience.
that a
are going to bother replying
because there
have
answer -- too
wrong to
could
code. I
that this
one's
questions,
I'd
dimensions.
<x-msg://40/user/SendEmail.jtp?type=node&node=19757&i=10>>
about
user
stack
Unfortunately this was
case no
almost
it.
or idea
overflow,
spark
questions
as the
means
unanswered.
lists
<x-msg://40/user/SendEmail.jtp?type=node&node=19757&i=11>rg
discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/Handling-questions-in-the-mailing-lists-tp19690p19757.html <http://apache-spark-developers-list.1001551.n3.nabble.com/Handling-questions-in-the-mailing-lists-tp19690p19757.html>
email] <x-msg://40/user/SendEmail.jtp?type=node&node=19758&i=1> 
<applewebdata://565D0BC6-3106-4B28-AA01-03AA37A9758E>.
<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
lists <http://apache-spark-developers-list.1001551.n3.nabble.com/Handling-questions-in-the-mailing-lists-tp19690p19758.html>
<http://apache-spark-developers-list.1001551.n3.nabble.com/> at Nabble.com.

"
Reynold Xin <rxin@databricks.com>,"Mon, 7 Nov 2016 01:11:25 -0800",Re: Odp.: Spark Improvement Proposals,"""dev@spark.apache.org"" <dev@spark.apache.org>","I just looked through the entire thread again tonight - there are a lot of
great ideas being discussed. Thanks Cody for taking the first crack at the
proposal.

I want to first comment on the context. Spark is one of the most innovative
and important projects in (big) data -- overall technical decisions made in
Apache Spark are sound. But of course, a project as large and active as
Spark always have room for improvement, and we as a community should strive
to take it to the next level.

To that end, the two biggest areas for improvements in my opinion are:

1. Visibility: There are so much happening that it is difficult to know
what really is going on. For people that don't follow closely, it is
difficult to know what the important initiatives are. Even for people that
do follow, it is difficult to know what specific things require their
attention, since the number of pull requests and JIRA tickets are high and
it's difficult to extract signal from noise.

2. Solicit user (broadly defined, including developers themselves) input
more proactively: At the end of the day the project provides value because
users use it. Users can't tell us exactly what to build, but it is
important to get their inputs.


I've taken Cody's doc and edited it:
https://docs.google.com/document/d/1-Zdi_W-wtuxS9hTK0P9qb2x-nRanvXmnZ7SUi4qMljg/edit#heading=h.36ut37zh7w2b
 (I've made all my modifications trackable)

There are couple high level changes I made:

1. I've consulted a board member and he recommended lazy consensus as
opposed to voting. The reason being in voting there can easily be a ""loser'
that gets outvoted.

2. I made it lighter weight, and renamed ""strategy"" to ""optional design
sketch"". Echoing one of the earlier email: ""IMHO so far aside from tagging
things and linking them elsewhere simply having design docs and prototypes
implementations in PRs is not something that has not worked so far"".

3. I made some the language tweaks to focus more on visibility. For
example, ""The purpose of an SIP is to inform and involve"", rather than just
""involve"". SIPs should also have at least two emails that go to dev@.


While I was editing this, I thought we really needed a suggested template
for design doc too. I will get to that too ...



n
l
en
d
s
w
4
nd
n
t
r
ty
ng
We
a
,
n
le
d
r
s
a
h
n
re
s
e
ld
ts
d
"
<Ioannis.Deligiannis@nomura.com>,"Mon, 7 Nov 2016 09:48:45 +0000",RE: Handling questions in the mailing lists,"<matei.zaharia@gmail.com>, <assaf.mendelson@rsa.com>","My two cents (As a user/consumer)â€¦

I have been following & using Spark in financial services before version 1 and before it migrated questions from Google Groups to apache mailing lists (which was a shame â˜¹ ).

SO:
There has been some momentum lately on SO, but as questions were not â€œmonitored/answeredâ€ by Spark experts, the motivation of posting a question was low and in turn the quality of questions as well. As most of us know, SO is usually the first place to look for info and can greatly reduce the need to turn to user/dev groups so it would be great if there was more attention to it.

Spark mailing lists:
As the consensus appears to be, questions tend to get lost if not picked-up within 1-2 days. Re-sending the same question feels â€œabusiveâ€ to me so would then give up. Provided that a good question takes time, putting effort in a question that can easily be ignored results to mailing a â€œbadâ€ question (see what happens?) or no question at all. As you have probably observed, a few users will mail a question to â€œdevâ€ with â€œâ€¦no answers in user listâ€¦â€ as they incorrectly assume that no-one can answer their question.

JIRA:
I find that â€œissuesâ€ are being quite aggressively closed down.  Iâ€™ve seen this twice (one I reported myself and found the second ticket while looking for a solution) and for this reason it doesnâ€™t encourage users spending the time and effort to use. Personally, I also feel that there is some bias on what is in-scope and out-of-scope.

My preference would be that SO would be the first place that someone would post a question. If a few â€œexpertsâ€ are found regularly answering questions, eventually Spark users will start using it more and reduce â€œuserâ€ load by easily finding previous answers (or SO community marking a duplicates). The same â€œexpertsâ€ can also encourage users to â€œescalateâ€ to JIRA, dev/user groups once a question has been properly filtered which is quite common.

PS. Personally, I would not follow any â€œbespoke/externalâ€ process on SO E.g. down-voting on SO for any other reason that being a bad question as per SO rules.


From: Matei Zaharia [mailto:matei.zaharia@gmail.com]
Sent: 07 November 2016 07:45
To: assaf.mendelson
Cc: dev@spark.apache.org
Subject: Re: Handling questions in the mailing lists

Even for the mailing list, I'd love to have a short set of instructions on how to submit your questions (maybe on http://spark.apache.org/community.html<https://urldefense.proofpoint.com/v2/url?u=http-3A__spark.apache.org_community.html&d=DQMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=Vf-yZoTpLgwZzwUCoQTMr4UFD_R0nx0naxh_SWUHfho&s=MIQDl3ZflIuyNs62JLog9_vi0dD4xyo96x2w7XwGV3w&e=> or maybe in the welcome email when you subscribe). It would be great if someone added that. After all, we have such instructions for contributing PRs, for example.

Matei

On Nov 6, 2016, at 11:09 PM, assaf.mendelson <assaf.mendelson@rsa.com<mailto:assaf.mendelson@rsa.com>> wrote:

There are other options as well. For example hosting an answerhub (www.answerhub.com<https://urldefense.proofpoint.com/v2/url?u=http-3A__www.answerhub.com_&d=DQMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=Vf-yZoTpLgwZzwUCoQTMr4UFD_R0nx0naxh_SWUHfho&s=2oSovyR4k9m576OtymFnf4nQ4Xksk94HX543bDeEVQI&e=>) or other similar separate Q&A service.
BTW, I believe the main issue is not how opinionated people are but who is answering questions.
Today there are already people asking (and getting answers) on SO (including myself). The problem is that many people do not go to SO.
The problem I see is how to â€œbumpâ€ up questions which are not being answered to someone more likely to be able to answer them. Simple questions can be answered by many people, many of them even newbies who ran into the issue themselves.
The main issue is that the more complex the question, the less people there are who can answer it and those peopleâ€™s bandwidth is already clogged by other questions.
We could for example try to create tags on SO for â€œbasic questionsâ€, â€œmediumâ€, â€œadvancedâ€. Provide guidelines to ask first on basic, if not answered after X days then add the medium tag etc. Downvote people who donâ€™t go by the process. This would mean that committers for example can look at advanced only tag and have a manageable number of questions they can help with while others can answer medium and basic.

I agree that some things are not good for SO. Basically stuff which asks for opinion is such but most cases in the mailing list are either â€œhow do I solve this bugâ€ or â€œhow do I do Xâ€. Either of those two are good for SO.


Assaf.



From: rxin [via Apache Spark Developers List] [mailto:ml-node+[hidden email]<x-msg://40/user/SendEmail.jtp?type=node&node=19758&i=0>]
Sent: Monday, November 07, 2016 8:33 AM
To: Mendelson, Assaf
Subject: Re: Handling questions in the mailing lists

This is an excellent point. If we do go ahead and feature SO as a way for users to ask questions more prominently, as someone who knows SO very well, would you be willing to help write a short guideline (ideally the shorter the better, which makes it hard) to direct what goes to user@ and what goes to SO?


On Sun, Nov 6, 2016 at 9:54 PM, Maciej Szymkiewicz <[hidden email]<x-msg://40/user/SendEmail.jtp?type=node&node=19757&i=0>> wrote:
Damn, I always thought that mailing list is only for nice and welcoming people and there is nothing to do for me here >:)
To be serious though, there are many questions on the users list which would fit just fine on SO but it is not true in general. There are dozens of questions which are to broad, opinion based, ask for external resources and so on. If you want to direct users to SO you have to help them to decide if it is the right channel. Otherwise it will just create a really bad experience for both seeking help and active answerers. Former ones will be downvoted and bashed, latter ones will have to deal with handling all the junk and the number of active Spark users with moderation privileges is really low (with only Massg and me being able to directly close duplicates).
Believe me, I've seen this before.
On 11/07/2016 05:08 AM, Reynold Xin wrote:
You have substantially underestimated how opinionated people can be on mailing lists too :)

On Sunday, November 6, 2016, Maciej Szymkiewicz <[hidden email]<x-msg://40/user/SendEmail.jtp?type=node&node=19757&i=1>> wrote:
You have to remember that Stack Overflow crowd (like me) is highly opinionated, so many questions, which could be just fine on the mailing list, will be quickly downvoted and / or closed as off-topic. Just saying...

--

Best,

Maciej

On 11/07/2016 04:03 AM, Reynold Xin wrote:
OK I've checked on the ASF member list (which is private so there is no public archive).

It is not against any ASF rule to recommend StackOverflow as a place for users to ask questions. I don't think we can or should delete the existing user@spark list either, but we can certainly make SO more visible than it is.



On Wed, Nov 2, 2016 at 10:21 AM, Reynold Xin <[hidden email]<x-msg://40/user/SendEmail.jtp?type=node&node=19757&i=2>> wrote:
Actually after talking with more ASF members, I believe the only policy is that development decisions have to be made and announced on ASF properties (dev list or jira), but user questions don't have to.

I'm going to double check this. If it is true, I would actually recommend us moving entirely over the Q&A part of the user list to stackoverflow, or at least make that the recommended way rather than the existing user list which is not very scalable.


On Wednesday, November 2, 2016, Nicholas Chammas <[hidden email]<x-msg://40/user/SendEmail.jtp?type=node&node=19757&i=3>> wrote:
Weâ€™ve discussed several times upgrading our communication tools, as far back as 2014 and maybe even before that too. The bottom line is that we canâ€™t due to ASF rules requiring the use of ASF-managed mailing lists.
For some history, see this discussion:
1.      https://mail-archives.apache.org/mod_mbox/spark-user/201412.mbox/%3CCAOhmDzfL2COdysV8r5hZN8f=NqXM=f=oY5NO2dHWJ_kVEoP+Ng@...%3E<https://urldefense.proofpoint.com/v2/url?u=https-3A__mail-2Darchives.apache.org_mod-5Fmbox_spark-2Duser_201412.mbox_-253CCAOhmDzfL2COdysV8r5hZN8f-3DNqXM-3Df-3DoY5NO2dHWJ-5FkVEoP-2BNg-40mail.gmail.com-253E&d=DQMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=Vf-yZoTpLgwZzwUCoQTMr4UFD_R0nx0naxh_SWUHfho&s=fILmWaylBzYeV5-XRmdm75cBbKG57kiU81cArNLLbdA&e=>
2.      https://mail-archives.apache.org/mod_mbox/spark-user/201501.mbox/%3CCAOhmDzec1JdsXQq3dDwAv7eLnzRidSkrsKKG0xKw=TKTxY_sYw@...%3E<https://urldefense.proofpoint.com/v2/url?u=https-3A__mail-2Darchives.apache.org_mod-5Fmbox_spark-2Duser_201501.mbox_-253CCAOhmDzec1JdsXQq3dDwAv7eLnzRidSkrsKKG0xKw-3DTKTxY-5FsYw-40mail.gmail.com-253E&d=DQMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=Vf-yZoTpLgwZzwUCoQTMr4UFD_R0nx0naxh_SWUHfho&s=_snNLu3ds5DSqCrMJ30_tq_qhaCPD6I72Sc25p0idmY&e=>
(Itâ€™s ironic that itâ€™s difficult to follow the past discussion on why we canâ€™t change our official communication tools due to those very toolsâ€¦)
Nick
â€‹

On Wed, Nov 2, 2016 at 12:24 PM Ricardo Almeida <[hidden email]<x-msg://40/user/SendEmail.jtp?type=node&node=19757&i=4>> wrote:
I fell Assaf point is quite relevant if we want to move this project forward from the Spark user perspective (as I do). In fact, we're still using 20th century tools (mailing lists) with some add-ons (like Stack Overflow).

As usually, Sean and Cody's contributions are very to the point.
I fell it is indeed a matter of of culture (hard to enforce) and tools (much easier). Isn't it?

On 2 November 2016 at 16:36, Cody Koeninger <[hidden email]<x-msg://40/user/SendEmail.jtp?type=node&node=19757&i=5>> wrote:
So concrete things people could do

- users could tag subject lines appropriately to the component they're
asking about

- contributors could monitor user@ for tags relating to components
they've worked on.
I'd be surprised if my miss rate for any mailing list questions
well-labeled as Kafka was higher than 5%

- committers could be more aggressive about soliciting and merging PRs
to improve documentation.
It's a lot easier to answer even poorly-asked questions with a link to
relevant docs.

On Wed, Nov 2, 2016 at 7:39 AM, Sean Owen <[hidden email]<x-msg://40/user/SendEmail.jtp?type=node&node=19757&i=6>> wrote:
> There's already reviews@ and issues@. dev@ is for project development itself
> and I think is OK. You're suggesting splitting up user@ and I sympathize
> with the motivation. Experience tells me that we'll have a beginner@ that's
> then totally ignored, and people will quickly learn to post to advanced@ to
> get attention, and we'll be back where we started. Putting it in JIRA
> doesn't help. I don't think this a problem that is merely down to lack of
> process. It actually requires cultivating a culture change on the community
> list.
>
> On Wed, Nov 2, 2016 at 12:11 PM Mendelson, Assaf <[hidden email]<x-msg://40/user/SendEmail.jtp?type=node&node=19757&i=7>>
> wrote:
>>
>> What I am suggesting is basically to fix that.
>>
>> For example, we might say that mailing list A is only for voting, mailing
>> list B is only for PR and have something like stack overflow for developer
>> questions (I would even go as far as to have beginner, intermediate and
>> advanced mailing list for users and beginner/advanced for dev).
>>
>>
>>
>> This can easily be done using stack overflow tags, however, that would
>> probably be harder to manage.
>>
>> Maybe using special jira tags and manage it in jira?
>>
>>
>>
>> Anyway as I said, the main issue is not user questions (except maybe
>> advanced ones) but more for dev questions. It is so easy to get lost in the
>> chatter that it makes it very hard for people to learn spark internalsâ€¦
>>
>> Assaf.
>>
>>
>>
>> From: Sean Owen [mailto:[hidden email]<x-msg://40/user/SendEmail.jtp?type=node&node=19757&i=8>]
>> Sent: Wednesday, November 02, 2016 2:07 PM
>> To: Mendelson, Assaf; [hidden email]<x-msg://40/user/SendEmail.jtp?type=node&node=19757&i=9>
>> Subject: Re: Handling questions in the mailing lists
>>
>>
>>
>> I think that unfortunately mailing lists don't scale well. This one has
>> thousands of subscribers with different interests and levels of experience.
>> For any given person, most messages will be irrelevant. I also find that a
>> lot of questions on user@ are not well-asked, aren't an SSCCE
>> (http://sscce.org/<https://urldefense.proofpoint.com/v2/url?u=http-3A__sscce.org_&d=DQMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=Vf-yZoTpLgwZzwUCoQTMr4UFD_R0nx0naxh_SWUHfho&s=BhbHzFmq52GPq1-vnJpo1WYJSivxTYV2DTqLE2lcomU&e=>), not something most people are going to bother replying
>> to even if they could answer. I almost entirely ignore user@ because there
>> are higher-priority channels like PRs to deal with, that already have
>> hundreds of messages per day. This is why little of it gets an answer -- too
>> noisy.
>>
>>
>>
>> We have to have official mailing lists, in any event, to have some
>> official channel for things like votes and announcements. It's not wrong to
>> ask questions on user@ of course, but a lot of the questions I see could
>> have been answered with research of existing docs or looking at the code. I
>> think that given the scale of the list, it's not wrong to assert that this
>> is sort of a prerequisite for asking thousands of people to answer one's
>> question. But we can't enforce that.
>>
>>
>>
>> The situation will get better to the extent people ask better questions,
>> help other people ask better questions, and answer good questions. I'd
>> encourage anyone feeling this way to try to help along those dimensions.
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>> On Wed, Nov 2, 2016 at 11:32 AM assaf.mendelson <[hidden email]<x-msg://40/user/SendEmail.jtp?type=node&node=19757&i=10>>
>> wrote:
>>
>> Hi,
>>
>> I know this is a little off topic but I wanted to raise an issue about
>> handling questions in the mailing list (this is true both for the user
>> mailing list and the dev but since there are other options such as stack
>> overflow for user questions, this is more problematic in dev).
>>
>> Letâ€™s say I ask a question (as I recently did). Unfortunately this was
>> during spark summit in Europe so probably people were busy. In any case no
>> one answered.
>>
>> The problem is, that if no one answers very soon, the question will almost
>> certainly remain unanswered because new messages will simply drown it.
>>
>>
>>
>> This is a common issue not just for questions but for any comment or idea
>> which is not immediately picked up.
>>
>>
>>
>> I believe we should have a method of handling this.
>>
>> Generally, I would say these types of things belong in stack overflow,
>> after all, the way it is built is perfect for this. More seasoned spark
>> contributors and committers can periodically check out unanswered questions
>> and answer them.
>>
>> The problem is that stack overflow (as well as other targets such as the
>> databricks forums) tend to have a more user based orientation. This means
>> that any spark internal question will almost certainly remain unanswered.
>>
>>
>>
>> I was wondering if we could come up with a solution for this.
>>
>>
>>
>> Assaf.
>>
>>
>>
>>
>>
>> ________________________________
>>
>> View this message in context: Handling questions in the mailing lists
>> Sent from the Apache Spark Developers List mailing list archive at
>> Nabble.com<https://urldefense.proofpoint.com/v2/url?u=http-3A__nabble.com&d=DQMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=Vf-yZoTpLgwZzwUCoQTMr4UFD_R0nx0naxh_SWUHfho&s=DOSXyWQ25VrvEJ61e9vezaFFqQ6ERTNkf2btm8y3JEA&e=>.
---------------------------------------------------------------------
To unsubscribe e-mail: [hidden email]<x-msg://40/user/SendEmail.jtp?type=node&node=19757&i=11>rg








________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/Handling-questions-in-the-mailing-lists-tp19690p19757.html<https://urldefense.proofpoint.com/v2/url?u=http-3A__apache-2Dspark-2Ddevelopers-2Dlist.1001551.n3.nabble.com_Handling-2Dquestions-2Din-2Dthe-2Dmailing-2Dlists-2Dtp19690p19757.html&d=DQMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=Vf-yZoTpLgwZzwUCoQTMr4UFD_R0nx0naxh_SWUHfho&s=ZX7C3jXG0WAWbui6GXRkT15WDj5s6Yb9U_uCYr0p7Ew&e=>
To start a new topic under Apache Spark Developers List, email [hidden email]<x-msg://40/user/SendEmail.jtp?type=node&node=19758&i=1>
To unsubscribe from Apache Spark Developers List, click here.
NAML<https://urldefense.proofpoint.com/v2/url?u=http-3A__apache-2Dspark-2Ddevelopers-2Dlist.1001551.n3.nabble.com_template_NamlServlet.jtp-3Fmacro-3Dmacro-5Fviewer-26id-3Dinstant-5Fhtml-2521nabble-253Aemail.naml-26base-3Dnabble.naml.namespaces.BasicNamespace-2Dnabble.view.web.template.NabbleNamespace-2Dnabble.view.web.template.NodeNamespace-26breadcrumbs-3Dnotify-5Fsubscribers-2521nabble-253Aemail.naml-2Dinstant-5Femails-2521nabble-253Aemail.naml-2Dsend-5Finstant-5Femail-2521nabble-253Aemail.naml&d=DQMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=Vf-yZoTpLgwZzwUCoQTMr4UFD_R0nx0naxh_SWUHfho&s=ZSjjNhkCGzaVsA9UhbpYXaKujat6vR7r6SdBFmyzWdc&e=>

________________________________
View this message in context: RE: Handling questions in the mailing lists<https://urldefense.proofpoint.com/v2/url?u=http-3A__apache-2Dspark-2Ddevelopers-2Dlist.1001551.n3.nabble.com_Handling-2Dquestions-2Din-2Dthe-2Dmailing-2Dlists-2Dtp19690p19758.html&d=DQMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=Vf-yZoTpLgwZzwUCoQTMr4UFD_R0nx0naxh_SWUHfho&s=ElcVsDry_U8-DCD_Awaa6kExNKJo0gQ6Dpbp0HNMUUI&e=>
Sent from the Apache Spark Developers List mailing list archive<https://urldefense.proofpoint.com/v2/url?u=http-3A__apache-2Dspark-2Ddevelopers-2Dlist.1001551.n3.nabble.com_&d=DQMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=Vf-yZoTpLgwZzwUCoQTMr4UFD_R0nx0naxh_SWUHfho&s=dXRHr7SeGMH3ksDwIya1xGEJGhVRQU4TYLf3dun_L5k&e=> at Nabble.com<https://urldefense.proofpoint.com/v2/url?u=http-3A__nabble.com&d=DQMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=Vf-yZoTpLgwZzwUCoQTMr4UFD_R0nx0naxh_SWUHfho&s=DOSXyWQ25VrvEJ61e9vezaFFqQ6ERTNkf2btm8y3JEA&e=>.



This e-mail (including any attachments) is private and confidential, may contain proprietary or privileged information and is intended for the named recipient(s) only. Unintended recipients are strictly prohibited from taking action on the basis of information in this e-mail and must contact the sender immediately, delete this e-mail (and all attachments) and destroy any hard copies. Nomura will not accept responsibility or liability for the accuracy or completeness of, or the presence of any virus or disabling code in, this e-mail. If verification is sought please request a hard copy. Any reference to the terms of executed transactions should be treated as preliminary only and subject to formal written confirmation by Nomura. Nomura reserves the right to retain, monitor and intercept e-mail communications through its networks (subject to and in accordance with applicable laws). No confidentiality or privilege is waived or lost by Nomura by any mistransmission of this e-mail. Any reference to ""Nomura"" is a reference to any entity in the Nomura Holdings, Inc. group. Please read our Electronic Communications Legal Notice which forms part of this e-mail: http://www.Nomura.com/email_disclaimer.htm

"
Ricardo Almeida <ricardo.almeida@actnowib.com>,"Mon, 7 Nov 2016 11:26:59 +0100",Re: Handling questions in the mailing lists,Ioannis.Deligiannis@nomura.com,"Thanks Reynold for reviewing the ASF rules.
Albeit the potential issues mentioned, I feel using StackOverflow would be
a improvement. And yes, some guidelines/instructions have the potential to
improve the questions and the ""escalation"" process.


1
ts
posting a question
usiveâ€ to
ng
€œbadâ€
â€œâ€¦no answers in
swer their
down.  Iâ€™ve seen
ng
ending the
n
d
nswering
munity marking a
to â€œescalateâ€ to
 process on SO
n
ommunity.html&d=DQMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=Vf-yZoTpLgwZzwUCoQTMr4UFD_R0nx0naxh_SWUHfho&s=MIQDl3ZflIuyNs62JLog9_vi0dD4xyo96x2w7XwGV3w&e=>
&d=DQMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=Vf-yZoTpLgwZzwUCoQTMr4UFD_R0nx0naxh_SWUHfho&s=2oSovyR4k9m576OtymFnf4nQ4Xksk94HX543bDeEVQI&e=>)
s
 not being
ns
e
dy clogged
sâ€,
to ask first on basic, if not
ple can
œhow do I
se two are good for SO.
l,
:
s
ll
is
s).
g
s
s
r
as far
ists.
%
pache.org_mod-5Fmbox_spark-2Duser_201412.mbox_-253CCAOhmDzfL2COdysV8r5hZN8f-3DNqXM-3Df-3DoY5NO2dHWJ-5FkVEoP-2BNg-40mail.gmail.com-253E&d=DQMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=Vf-yZoTpLgwZzwUCoQTMr4UFD_R0nx0naxh_SWUHfho&s=fILmWaylBzYeV5-XRmdm75cBbKG57kiU81cArNLLbdA&e=>
%
pache.org_mod-5Fmbox_spark-2Duser_201501.mbox_-253CCAOhmDzec1JdsXQq3dDwAv7eLnzRidSkrsKKG0xKw-3DTKTxY-5FsYw-40mail.gmail.com-253E&d=DQMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=Vf-yZoTpLgwZzwUCoQTMr4UFD_R0nx0naxh_SWUHfho&s=_snNLu3ds5DSqCrMJ30_tq_qhaCPD6I72Sc25p0idmY&e=>
ssion on why we
oolsâ€¦)
e
@
of
d
n
â€¦
s
FaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=Vf-yZoTpLgwZzwUCoQTMr4UFD_R0nx0naxh_SWUHfho&s=BhbHzFmq52GPq1-vnJpo1WYJSivxTYV2DTqLE2lcomU&e=>),
's
s,
s.
ck
this was
e
k
he
FaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=Vf-yZoTpLgwZzwUCoQTMr4UFD_R0nx0naxh_SWUHfho&s=DOSXyWQ25VrvEJ61e9vezaFFqQ6ERTNkf2btm8y3JEA&e=>
evelopers-2Dlist.1001551.n3.nabble.com_Handling-2Dquestions-2Din-2Dthe-2Dmailing-2Dlists-2Dtp19690p19757.html&d=DQMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=Vf-yZoTpLgwZzwUCoQTMr4UFD_R0nx0naxh_SWUHfho&s=ZX7C3jXG0WAWbui6GXRkT15WDj5s6Yb9U_uCYr0p7Ew&e=>
evelopers-2Dlist.1001551.n3.nabble.com_template_NamlServlet.jtp-3Fmacro-3Dmacro-5Fviewer-26id-3Dinstant-5Fhtml-2521nabble-253Aemail.naml-26base-3Dnabble.naml.namespaces.BasicNamespace-2Dnabble.view.web.template.NabbleNamespace-2Dnabble.view.web.template.NodeNamespace-26breadcrumbs-3Dnotify-5Fsubscribers-2521nabble-253Aemail.naml-2Dinstant-5Femails-2521nabble-253Aemail.naml-2Dsend-5Finstant-5Femail-2521nabble-253Aemail.naml&d=DQMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=Vf-yZoTpLgwZzwUCoQTMr4UFD_R0nx0naxh_SWUHfho&s=ZSjjNhkCGzaVsA9UhbpYXaKujat6vR7r6SdBFmyzWdc&e=>
evelopers-2Dlist.1001551.n3.nabble.com_Handling-2Dquestions-2Din-2Dthe-2Dmailing-2Dlists-2Dtp19690p19758.html&d=DQMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=Vf-yZoTpLgwZzwUCoQTMr4UFD_R0nx0naxh_SWUHfho&s=ElcVsDry_U8-DCD_Awaa6kExNKJo0gQ6Dpbp0HNMUUI&e=>
evelopers-2Dlist.1001551.n3.nabble.com_&d=DQMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=Vf-yZoTpLgwZzwUCoQTMr4UFD_R0nx0naxh_SWUHfho&s=dXRHr7SeGMH3ksDwIya1xGEJGhVRQU4TYLf3dun_L5k&e=>
FaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=Vf-yZoTpLgwZzwUCoQTMr4UFD_R0nx0naxh_SWUHfho&s=DOSXyWQ25VrvEJ61e9vezaFFqQ6ERTNkf2btm8y3JEA&e=>
ed
ty
a
s
l:
"
Cody Koeninger <cody@koeninger.org>,"Mon, 7 Nov 2016 09:51:46 -0600",Re: Odp.: Spark Improvement Proposals,Reynold Xin <rxin@databricks.com>,"Thanks for picking up on this.

Maybe I fail at google docs, but I can't see any edits on the document
you linked.

Regarding lazy consensus, if the board in general has less of an issue
with that, sure.  As long as it is clearly announced, lasts at least
72 hours, and has a clear outcome.

The other points are hard to comment on without being able to see the
text in question.


f
e
ve
in
ve
hat
 to
it
the
e
ant
4qMljg/edit#heading=h.36ut37zh7w2b
r'
g
s
le,
k
n
t-proposals.md
on
ll
n
ed
ms
ow
.
14
s
un
at
or
d
h
 a
s
s,
in
ed
er
es
ta
d
ch
in
s
d
es
be
e
od
o
,
t

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Mon, 7 Nov 2016 10:10:54 -0800",Re: Spark Improvement Proposals,Cody Koeninger <cody@koeninger.org>,"Oops. Let me try figure that out.


e
,
e
t
te
y
k.
l
he
d
it
,
rn
is
e
he
ng
g
t
ll
vs
s)
rm
t
d
ll
d
I
m
"
Reynold Xin <rxin@databricks.com>,"Mon, 7 Nov 2016 11:55:17 -0800",Re: Spark Improvement Proposals,Cody Koeninger <cody@koeninger.org>,"It turned out suggested edits (trackable) don't show up for non-owners, so
I've just merged all the edits in place. It should be visible now.


t
s
w
o
ut
n
ly
,
e
d
r
t
ed
s,
l
s
ce
w
e
o
s
s
ut
k
n
t
nd
n
 I
'm
g
"
Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Mon, 7 Nov 2016 23:24:33 +0100",Re: Handling questions in the mailing lists,Reynold Xin <rxin@databricks.com>,"Just a couple of random thoughts regarding Stack Overflow...

  * If we are thinking about shifting focus towards SO all attempts of
    micromanaging should be discarded right in the beginning. Especially
    things like meta tags, which are discouraged and ""burninated""
    (https://meta.stackoverflow.com/tags/burninate-request/info) , or
    thread bumping. Depending on a context these won't be manageable, go
    against community guidelines or simply obsolete. 
  * Lack of expertise is unlikely an issue. Even now there is a number
    of advanced Spark users on SO. Of course the more the merrier.

Things that can be easily improved:

  * Identifying, improving and promoting canonical questions and
    answers. It means closing duplicate, suggesting edits to improve
    existing answers, providing alternative solutions. This can be also
    used to identify gaps in the documentation.
  * Providing a set of clear posting guidelines to reduce effort
    required to identify the problem (think about
    http://stackoverflow.com/q/5963269 a.k.a How to make a great R
    reproducible example?)
  * Helping users decide if question is a good fit for SO (see below).
    API questions are great fit, debugging problems like ""my cluster is
    slow"" are not.
  * Actively cleaning (closing, deleting) off-topic and low quality
    questions. The less junk to sieve through the better chance of good
    questions being answered.
  * Repurposing and actively moderating SO docs
    (https://stackoverflow.com/documentation/apache-spark/topics). Right
    now most of the stuff that goes there is useless, duplicated or
    plagiarized, or border case SPAM.
  * Encouraging community to monitor featured
    (https://stackoverflow.com/questions/tagged/apache-spark?sort=featured)
    and active & upvoted & unanswered
    (https://stackoverflow.com/unanswered/tagged/apache-spark) questions.
  * Implementing some procedure to identify questions which are likely
    to be bugs or a material for feature requests. Personally I am quite
    often tempted to simply send a link to dev list, but I don't think
    it is really acceptable.
  * Animating Spark related chat room. I tried this a couple of times
    but to no avail. Without a certain critical mass of users it just
    won't work.




Sure, I'll be happy to help if I can.


-- 
Maciej Szymkiewicz

"
Chan Chor Pang <chin-sh@indetail.co.jp>,"Tue, 8 Nov 2016 10:30:13 +0900",Re: REST api for monitoring Spark Streaming,dev@spark.apache.org,"hi everyone

it seems that there is not much who interested in creating a api for 
Streaming.
never the less I still really want the api for monitoring.
so i tried to see if i can implement by my own.

after some test,
i believe i can achieve the goal by
1. implement a package(org.apache.spark.streaming.status.api.v1) that 
serve the same purpose as org.apache.spark.status.api.v1
2. register the api path through StreamingTab
and 3. retrive the streaming informateion through 
StreamingJobProgressListener

what my most concern now is will my implementation be able to merge to 
the main stream.

im new to open source project, so anyone could please show me some light?
how should/could i proceed to make my implementation to be able to merge 
to the main stream.


here is my test code base on v1.6.0
###################################
diff --git 
a/streaming/src/main/scala/org/apache/spark/streaming/status/api/v1/JacksonMessageWriter.scala 
b/streaming/src/main/scala/org/apache/spark/streaming/status/api/v1/JacksonMessageWriter.scala
new file mode 100644
index 0000000..690e2d8
--- /dev/null
+++ 
b/streaming/src/main/scala/org/apache/spark/streaming/status/api/v1/JacksonMessageWriter.scala
@@ -0,0 +1,68 @@
+package org.apache.spark.streaming.status.api.v1
+
+import java.io.OutputStream
+import java.lang.annotation.Annotation
+import java.lang.reflect.Type
+import java.text.SimpleDateFormat
+import java.util.{Calendar, SimpleTimeZone}
+import javax.ws.rs.Produces
+import javax.ws.rs.core.{MediaType, MultivaluedMap}
+import javax.ws.rs.ext.{MessageBodyWriter, Provider}
+
+import com.fasterxml.jackson.annotation.JsonInclude
+import com.fasterxml.jackson.databind.{ObjectMapper, SerializationFeature}
+
+@Provider
+@Produces(Array(MediaType.APPLICATION_JSON))
+private[v1] class JacksonMessageWriter extends MessageBodyWriter[Object]{
+
+  val mapper = new ObjectMapper() {
+    override def writeValueAsString(t: Any): String = {
+      super.writeValueAsString(t)
+    }
+  }
+ 
mapper.registerModule(com.fasterxml.jackson.module.scala.DefaultScalaModule)
+  mapper.enable(SerializationFeature.INDENT_OUTPUT)
+  mapper.setSerializationInclusion(JsonInclude.Include.NON_NULL)
+  mapper.setDateFormat(JacksonMessageWriter.makeISODateFormat)
+
+  override def isWriteable(
+      aClass: Class[_],
+      `type`: Type,
+      annotations: Array[Annotation],
+      mediaType: MediaType): Boolean = {
+      true
+  }
+
+  override def writeTo(
+      t: Object,
+      aClass: Class[_],
+      `type`: Type,
+      annotations: Array[Annotation],
+      mediaType: MediaType,
+      multivaluedMap: MultivaluedMap[String, AnyRef],
+      outputStream: OutputStream): Unit = {
+    t match {
+      //case ErrorWrapper(err) => outputStream.write(err.getBytes(""utf-8""))
+      case _ => mapper.writeValue(outputStream, t)
+    }
+  }
+
+  override def getSize(
+      t: Object,
+      aClass: Class[_],
+      `type`: Type,
+      annotations: Array[Annotation],
+      mediaType: MediaType): Long = {
+    -1L
+  }
+}
+
+private[spark] object JacksonMessageWriter {
+  def makeISODateFormat: SimpleDateFormat = {
+    val iso8601 = new SimpleDateFormat(""yyyy-MM-dd'T'HH:mm:ss.SSS'GMT'"")
+    val cal = Calendar.getInstance(new SimpleTimeZone(0, ""GMT""))
+    iso8601.setCalendar(cal)
+    iso8601
+  }
+}
diff --git 
a/streaming/src/main/scala/org/apache/spark/streaming/status/api/v1/StreamingApiRootResource.scala 
b/streaming/src/main/scala/org/apache/spark/streaming/status/api/v1/StreamingApiRootResource.scala
new file mode 100644
index 0000000..f4e43dd
--- /dev/null
+++ 
b/streaming/src/main/scala/org/apache/spark/streaming/status/api/v1/StreamingApiRootResource.scala
@@ -0,0 +1,74 @@
+package org.apache.spark.streaming.status.api.v1
+
+import org.apache.spark.status.api.v1.UIRoot
+import org.eclipse.jetty.server.handler.ContextHandler
+import org.eclipse.jetty.servlet.ServletContextHandler
+import org.eclipse.jetty.servlet.ServletHolder
+
+import com.sun.jersey.spi.container.servlet.ServletContainer
+
+import javax.servlet.ServletContext
+import javax.ws.rs.Path
+import javax.ws.rs.Produces
+import javax.ws.rs.core.Context
+import org.apache.spark.streaming.ui.StreamingJobProgressListener
+
+
+@Path(""/v1"")
+private[v1] class StreamingApiRootResource extends 
UIRootFromServletContext{
+
+  @Path(""streaminginfo"")
+  def getStreamingInfo(): StreamingInfoResource = {
+    new StreamingInfoResource(uiRoot,listener)
+  }
+
+}
+
+private[spark] object StreamingApiRootResource {
+
+  def getServletHandler(uiRoot: UIRoot, 
listener:StreamingJobProgressListener): ServletContextHandler = {
+
+    val jerseyContext = new 
ServletContextHandler(ServletContextHandler.NO_SESSIONS)
+    jerseyContext.setContextPath(""/streamingapi"")
+    val holder: ServletHolder = new 
ServletHolder(classOf[ServletContainer])
+ 
holder.setInitParameter(""com.sun.jersey.config.property.resourceConfigClass"",
+      ""com.sun.jersey.api.core.PackagesResourceConfig"")
+ holder.setInitParameter(""com.sun.jersey.config.property.packages"",
+      ""org.apache.spark.streaming.status.api.v1"")
+ 
//holder.setInitParameter(ResourceConfig.PROPERTY_CONTAINER_REQUEST_FILTERS,
+    //  classOf[SecurityFilter].getCanonicalName)
+    UIRootFromServletContext.setUiRoot(jerseyContext, uiRoot)
+    UIRootFromServletContext.setListener(jerseyContext, listener)
+    jerseyContext.addServlet(holder, ""/*"")
+    jerseyContext
+  }
+}
+
+private[v1] object UIRootFromServletContext {
+
+  private val attribute = getClass.getCanonicalName
+
+  def setListener(contextHandler:ContextHandler, listener: 
StreamingJobProgressListener):Unit={
+   contextHandler.setAttribute(attribute+""_listener"", listener)
+  }
+
+  def getListener(context:ServletContext):StreamingJobProgressListener={
+ 
context.getAttribute(attribute+""_listener"").asInstanceOf[StreamingJobProgressListener]
+  }
+
+  def setUiRoot(contextHandler: ContextHandler, uiRoot: UIRoot): Unit = {
+    contextHandler.setAttribute(attribute, uiRoot)
+  }
+
+  def getUiRoot(context: ServletContext): UIRoot = {
+    context.getAttribute(attribute).asInstanceOf[UIRoot]
+  }
+}
+
+private[v1] trait UIRootFromServletContext {
+  @Context
+  var servletContext: ServletContext = _
+
+  def uiRoot: UIRoot = UIRootFromServletContext.getUiRoot(servletContext)
+  def listener: StreamingJobProgressListener = 
UIRootFromServletContext.getListener(servletContext)
+}
diff --git 
a/streaming/src/main/scala/org/apache/spark/streaming/status/api/v1/StreamingInfoResource.scala 
b/streaming/src/main/scala/org/apache/spark/streaming/status/api/v1/StreamingInfoResource.scala
new file mode 100644
index 0000000..d5fc11b
--- /dev/null
+++ 
b/streaming/src/main/scala/org/apache/spark/streaming/status/api/v1/StreamingInfoResource.scala
@@ -0,0 +1,22 @@
+package org.apache.spark.streaming.status.api.v1
+
+import org.apache.spark.status.api.v1.SimpleDateParam
+import org.apache.spark.status.api.v1.UIRoot
+
+import javax.ws.rs.GET
+import javax.ws.rs.Produces
+import javax.ws.rs.core.MediaType
+import org.apache.spark.streaming.StreamingContext
+import org.apache.spark.streaming.ui.StreamingJobProgressListener
+
+@Produces(Array(MediaType.APPLICATION_JSON))
+private[v1] class StreamingInfoResource(uiRoot: UIRoot, listener: 
StreamingJobProgressListener){
+
+  @GET
+  def streamingInfo()
+  :Iterator[StreamingInfo]={
+    var v = listener.numTotalCompletedBatches
+    Iterator(new StreamingInfo(""testname"",v))
+
+  }
+}
\ No newline at end of file
diff --git 
a/streaming/src/main/scala/org/apache/spark/streaming/status/api/v1/api.scala 
b/streaming/src/main/scala/org/apache/spark/streaming/status/api/v1/api.scala
new file mode 100644
index 0000000..958dd41
--- /dev/null
+++ 
b/streaming/src/main/scala/org/apache/spark/streaming/status/api/v1/api.scala
@@ -0,0 +1,6 @@
+package org.apache.spark.streaming.status.api.v1
+
+class StreamingInfo private[streaming](
+    val name:String,
+    val completedBatchCount:Long)
+
\ No newline at end of file
diff --git 
a/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingTab.scala 
b/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingTab.scala
index bc53f2a..877abf4 100644
--- 
a/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingTab.scala
+++ 
b/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingTab.scala
@@ -22,6 +22,7 @@ import org.apache.spark.streaming.StreamingContext
  import org.apache.spark.ui.{SparkUI, SparkUITab}

  import StreamingTab._
+import org.apache.spark.streaming.status.api.v1.StreamingApiRootResource

  /**
   * Spark Web UI tab that shows statistics of a streaming job.
@@ -39,6 +40,9 @@ private[spark] class StreamingTab(val ssc: 
StreamingContext)
    ssc.sc.addSparkListener(listener)
    attachPage(new StreamingPage(this))
    attachPage(new BatchPage(this))
+
+  //register streaming api
+ 
parent.attachHandler(StreamingApiRootResource.getServletHandler(parent,listener));

    def attach() {
      getSparkUI(ssc).attachTab(this)




---------------------------------------------------------------------


"
Tathagata Das <tathagata.das1565@gmail.com>,"Mon, 7 Nov 2016 18:44:12 -0800",Re: REST api for monitoring Spark Streaming,Chan Chor Pang <chin-sh@indetail.co.jp>,"This may be a good addition. I suggest you read our guidelines on
contributing code to Spark.

https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-PreparingtoContributeCodeChanges

Its long document but it should have everything for you to figure out how
to contribute your changes. I hope to see your changes in a Github PR soon!

TD


"
Chan Chor Pang <chin-sh@indetail.co.jp>,"Tue, 8 Nov 2016 13:46:12 +0900",Re: REST api for monitoring Spark Streaming,Tathagata Das <tathagata.das1565@gmail.com>,"Thank you

this should take me at least a few days, and will let you know as soon 
as the PR ready.



"
Denny Lee <denny.g.lee@gmail.com>,"Tue, 08 Nov 2016 05:52:09 +0000",Re: Handling questions in the mailing lists,"Maciej Szymkiewicz <mszymkiewicz@gmail.com>, Reynold Xin <rxin@databricks.com>","To help track and get the verbiage for the Spark community page and welcome
email jump started, here's a working document for us to work with:
https://docs.google.com/document/d/1N0pKatcM15cqBPqFWCqIy6jdgNzIoacZlYDCjufBh2s/edit#

Hope this will help us collaborate on this stuff a little faster.


hings
ing
ntify
ow""
d)
ften
lly
t
l,
m
s
ll
is
s).
g
s
s
r
as far
ists.
OhmDzfL2COdysV8r5hZN8f=NqXM=f=oY5NO2dHWJ_kVEoP+Ng@mail.gmail.com%3E
OhmDzec1JdsXQq3dDwAv7eLnzRidSkrsKKG0xKw=TKTxY_sYw@mail.gmail.com%3E
ssion on why we
oolsâ€¦)
e
@
of
d
n
â€¦
s
's
s,
s.
ck
this was
e
k
he
"
Reynold Xin <rxin@databricks.com>,"Mon, 7 Nov 2016 22:07:40 -0800",[ANNOUNCE] Announcing Apache Spark 1.6.3,"""dev@spark.apache.org"" <dev@spark.apache.org>, user <user@spark.apache.org>","We are happy to announce the availability of Spark 1.6.3! This maintenance
release includes fixes across several areas of Spark and encourage users on
the 1.6.x line to upgrade to 1.6.3.

Head to the project's download page to download the new version:
http://spark.apache.org/downloads.html
"
Reynold Xin <rxin@databricks.com>,"Mon, 7 Nov 2016 22:09:30 -0800",[VOTE] Release Apache Spark 2.0.2 (RC3),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version
2.0.2. The vote is open until Thu, Nov 10, 2016 at 22:00 PDT and passes if
a majority of at least 3+1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 2.0.2
[ ] -1 Do not release this package because ...


The tag to be voted on is v2.0.2-rc3
(584354eaac02531c9584188b143367ba694b0c34)

This release candidate resolves 84 issues:
https://s.apache.org/spark-2.0.2-jira

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.0.2-rc3-bin/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1214/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.0.2-rc3-docs/


Q: How can I help test this release?
A: If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions from 2.0.1.

Q: What justifies a -1 vote for this release?
A: This is a maintenance release in the 2.0.x series. Bugs already present
in 2.0.1, missing features, or bugs related to new features will not
necessarily block this release.

Q: What fix version should I use for patches merging into branch-2.0 from
now on?
A: Please mark the fix version as 2.0.3, rather than 2.0.2. If a new RC
(i.e. RC4) is cut, I will change the fix version of those patches to 2.0.2.
"
=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@databricks.com>,"Tue, 8 Nov 2016 12:38:04 +0100",Re: [VOTE] Release Apache Spark 2.0.2 (RC3),Reynold Xin <rxin@databricks.com>,1
Ricardo Almeida <ricardo.almeida@actnowib.com>,"Tue, 8 Nov 2016 14:50:51 +0100",Re: [VOTE] Release Apache Spark 2.0.2 (RC3),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1 (non-binding)

over Ubuntu 16.10, Java 8 (OpenJDK 1.8.0_111) built with Hadoop 2.7.3,
YARN, Hive



if
0
m
.2.
"
Zakaria Hili <zakahili@gmail.com>,"Tue, 8 Nov 2016 16:33:57 +0100",Issue + Resolution: Kmeans Spark Performances (ML package),dev@spark.apache.org,"Hello,

I'm newbie in spark, but I think that I found a small problem that can
affect spark Kmeans performances.
Before starting to explain the problem, I want to explain the warning that
I faced.

I tried to use Spark Kmeans with Dataframes to cluster my data

df_Part = assembler.transform(df_Part)
df_Part.cache()while (k<=max_cluster) and (wssse > seuilStop):
                    kmeans = KMeans().setK(k)
                    model = kmeans.fit(df_Part)
                    wssse = model.computeCost(df_Part)
                    k=k+1

but when I run the code I receive the warning :
WARN KMeans: The input data is not directly cached, which may hurt
performance if its parent RDDs are also uncached.

I searched in spark source code to find the source of this problem, then I
realized there is two classes responsible for this warning:

(mllib/src/main/scala/org/apache/spark/mllib/clustering/KMeans.scala )

(mllib/src/main/scala/org/apache/spark/ml/clustering/KMeans.scala )



When my  dataframe is cached, the fit method transform my dataframe into an
internally rdd which is not cached.

Dataframe -> rdd -> run Training Kmeans Algo(rdd)


-> The first class (ml package) responsible for converting the dataframe
into rdd then call Kmeans Algorithm

->The second class (mllib package) implements Kmeans Algorithm, and here
spark verify if the rdd is cached, if not a warning will be generated.


So, the solution of this (small) problem is to cache the rdd before running
Kmeans Algorithm.

https://github.com/ZakariaHili/spark/blob/master/mllib/src/
main/scala/org/apache/spark/ml/clustering/KMeans.scala

All what we need is to add two lines:

Cache rdd just after dataframe transformation, then uncached it after
training algorithm


[image: Images intÃ©grÃ©es 2]


I hope that I was clear.

If you think that I was wrong, please let me know.


Sincerely,

Zakaria HILI
á§
"
Cody Koeninger <cody@koeninger.org>,"Tue, 8 Nov 2016 10:13:25 -0600",Re: Spark Improvement Proposals,Reynold Xin <rxin@databricks.com>,"So there are some minor things (the Where section heading appears to
be dropped; wherever this document is posted it needs to actually link
to a jira filter showing current / past SIPs) but it doesn't look like
I can comment on the google doc.

The major substantive issue that I have is that this version is
significantly less clear as to the outcome of an SIP.

The apache example of lazy consensus at
http://apache.org/foundation/voting.html#LazyConsensus involves an
explicit announcement of an explicit deadline, which I think are
necessary for clarity.



o
:
ot
t
as
:
ow
to
7SUi4qMljg/edit#heading=h.36ut37zh7w2b
gn
e,
te
g>
ed
ement-proposals.md
or
it
s
al
o
o
is
n
ow
se
to
ls
ls
m
rk
o
k
an
it
in
e
?
y
ng

---------------------------------------------------------------------


"
Michael Segel <msegel_hadoop@hotmail.com>,"Tue, 8 Nov 2016 16:24:29 +0000",Re: Handling questions in the mailing lists,Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Guysâ€¦ please take what I say with a grain of saltâ€¦

The issue is that the input is a stream of messages where they are addressed in a LIFO manner.  This means that messages may be ignored. The stream of data (user@spark for example) is semi-structured in that the stream contains a lot of messages, some which could be noise or repeats not really organized by content.


So why not try to solve this as a Big Data problemâ€¦ Youâ€™re streaming data in to the â€˜lakeâ€™ and upon ingestion, you need to scan / index / and tag the message so that it could be easier to find.

Now you can create user tools to search the messages. (e.g. SparkSQL â€¦ , ML, etcâ€¦) So you can find a target set of messages and see how many times they have been viewed, answeredâ€¦ even query who answered themâ€¦ (e.g. Dean Wampler on Spark/Scala issues answered 30 questions this past month.   or Owen was answering questions that focused on spark securityâ€¦ )  What features came up the most in the questionsâ€¦  etc â€¦


I guess the point Iâ€™m trying to make is that you should consider rolling your own tool set, or looking beyond just SO.

Some have taken to glitter to set up online communities where discussions and questions can be answeredâ€¦ but looking at tools like glitter (github) , Atlassian, and SOâ€¦ its a disjoint toolset.

Why not choose one, or decide to roll your own and move on with it?  (Either under Apache, or outside on your own.)


I apologize for my mini rant.

-Mike

On Nov 7, 2016, at 4:24 PM, Maciej Szymkiewicz <mszymkiewicz@gmail.com<mailto:mszymkiewicz@gmail.com>> wrote:


Just a couple of random thoughts regarding Stack Overflow...

  *   If we are thinking about shifting focus towards SO all attempts of micromanaging should be discarded right in the beginning. Especially things like meta tags, which are discouraged and ""burninated"" (https://meta.stackoverflow.com/tags/burninate-request/info) , or thread bumping. Depending on a context these won't be manageable, go against community guidelines or simply obsolete.
  *   Lack of expertise is unlikely an issue. Even now there is a number of advanced Spark users on SO. Of course the more the merrier.

Things that can be easily improved:

  *   Identifying, improving and promoting canonical questions and answers. It means closing duplicate, suggesting edits to improve existing answers, providing alternative solutions. This can be also used to identify gaps in the documentation.
  *   Providing a set of clear posting guidelines to reduce effort required to identify the problem (think abouthttp://stackoverflow.com/q/5963269 a.k.a How to make a great R reproducible example?)
  *   Helping users decide if question is a good fit for SO (see below). API questions are great fit, debugging problems like ""my cluster is slow"" are not.
  *   Actively cleaning (closing, deleting) off-topic and low quality questions. The less junk to sieve through the better chance of good questions being answered.
  *   Repurposing and actively moderating SO docs (https://stackoverflow.com/documentation/apache-spark/topics). Right now most of the stuff that goes there is useless, duplicated or plagiarized, or border case SPAM.
  *   Encouraging community to monitor featured (https://stackoverflow.com/questions/tagged/apache-spark?sort=featured) and active & upvoted & unanswered (https://stackoverflow.com/unanswered/tagged/apache-spark) questions.
  *   Implementing some procedure to identify questions which are likely to be bugs or a material for feature requests. Personally I am quite often tempted to simply send a link to dev list, but I don't think it is really acceptable.
  *   Animating Spark related chat room. I tried this a couple of times but to no avail. Without a certain critical mass of users it just won't work.


On 11/07/2016 07:32 AM, Reynold Xin wrote:
This is an excellent point. If we do go ahead and feature SO as a way for users to ask questions more prominently, as someone who knows SO very well, would you be willing to help write a short guideline (ideally the shorter the better, which makes it hard) to direct what goes to user@ and what goes to SO?

Sure, I'll be happy to help if I can.



On Sun, Nov 6, 2016 at 9:54 PM, Maciej Szymkiewicz <mszymkiewicz@gmail.com<mailto:mszymkiewicz@gmail.com>> wrote:

Damn, I always thought that mailing list is only for nice and welcoming people and there is nothing to do for me here >:)

To be serious though, there are many questions on the users list which would fit just fine on SO but it is not true in general. There are dozens of questions which are to broad, opinion based, ask for external resources and so on. If you want to direct users to SO you have to help them to decide if it is the right channel. Otherwise it will just create a really bad experience for both seeking help and active answerers. Former ones will be downvoted and bashed, latter ones will have to deal with handling all the junk and the number of active Spark users with moderation privileges is really low (with only Massg and me being able to directly close duplicates).

Believe me, I've seen this before.

On 11/07/2016 05:08 AM, Reynold Xin wrote:
You have substantially underestimated how opinionated people can be on mailing lists too :)

On Sunday, November 6, 2016, Maciej Szymkiewicz <mszymkiewicz@gmail.com<mailto:mszymkiewicz@gmail.com>> wrote:

You have to remember that Stack Overflow crowd (like me) is highly opinionated, so many questions, which could be just fine on the mailing list, will be quickly downvoted and / or closed as off-topic. Just saying...

--
Best,
Maciej

On 11/07/2016 04:03 AM, Reynold Xin wrote:
OK I've checked on the ASF member list (which is private so there is no public archive).

It is not against any ASF rule to recommend StackOverflow as a place for users to ask questions. I don't think we can or should delete the existing user@spark list either, but we can certainly make SO more visible than it is.



On Wed, Nov 2, 2016 at 10:21 AM, Reynold Xin <rxin@databricks.com> wrote:
Actually after talking with more ASF members, I believe the only policy is that development decisions have to be made and announced on ASF properties (dev list or jira), but user questions don't have to.

I'm going to double check this. If it is true, I would actually recommend us moving entirely over the Q&A part of the user list to stackoverflow, or at least make that the recommended way rather than the existing user list which is not very scalable.


On Wednesday, November 2, 2016, Nicholas Chammas <nicholas.chammas@gmail.com> wrote:

Weâ€™ve discussed several times upgrading our communication tools, as far back as 2014 and maybe even before that too. The bottom line is that we canâ€™t due to ASF rules requiring the use of ASF-managed mailing lists.

For some history, see this discussion:

  *   https://mail-archives.apache.org/mod_mbox/spark-user/201412.mbox/%3CCAOhmDzfL2COdysV8r5hZN8f=NqXM=f=oY5NO2dHWJ_kVEoP+Ng@mail.gmail.com%3E
  *   https://mail-archives.apache.org/mod_mbox/spark-user/201501.mbox/%3CCAOhmDzec1JdsXQq3dDwAv7eLnzRidSkrsKKG0xKw=TKTxY_sYw@mail.gmail.com%3E

(Itâ€™s ironic that itâ€™s difficult to follow the past discussion on why we canâ€™t change our official communication tools due to those very toolsâ€¦)

Nick

On Wed, Nov 2, 2016 at 12:24 PM Ricardo Almeida <ricardo.almeida@actnowib.com> wrote:
I fell Assaf point is quite relevant if we want to move this project forward from the Spark user perspective (as I do). In fact, we're still using 20th century tools (mailing lists) with some add-ons (like Stack Overflow).

As usually, Sean and Cody's contributions are very to the point.
I fell it is indeed a matter of of culture (hard to enforce) and tools (much easier). Isn't it?

On 2 November 2016 at 16:36, Cody Koeninger <cody@koeninger.org> wrote:
So concrete things people could do

- users could tag subject lines appropriately to the component they're
asking about

- contributors could monitor user@ for tags relating to components
they've worked on.
I'd be surprised if my miss rate for any mailing list questions
well-labeled as Kafka was higher than 5%

- committers could be more aggressive about soliciting and merging PRs
to improve documentation.
It's a lot easier to answer even poorly-asked questions with a link to
relevant docs.

On Wed, Nov 2, 2016 at 7:39 AM, Sean Owen <sowen@cloudera.com> wrote:
> There's already reviews@ and issues@. dev@ is for project development itself
> and I think is OK. You're suggesting splitting up user@ and I sympathize
> with the motivation. Experience tells me that we'll have a beginner@ that's
> then totally ignored, and people will quickly learn to post to advanced@ to
> get attention, and we'll be back where we started. Putting it in JIRA
> doesn't help. I don't think this a problem that is merely down to lack of
> process. It actually requires cultivating a culture change on the community
> list.
>
> On Wed, Nov 2, 2016 at 12:11 PM Mendelson, Assaf <Assaf.Mendelson@rsa.com>
> wrote:
>>
>> What I am suggesting is basically to fix that.
>>
>> For example, we might say that mailing list A is only for voting, mailing
>> list B is only for PR and have something like stack overflow for developer
>> questions (I would even go as far as to have beginner, intermediate and
>> advanced mailing list for users and beginner/advanced for dev).
>>
>>
>>
>> This can easily be done using stack overflow tags, however, that would
>> probably be harder to manage.
>>
>> Maybe using special jira tags and manage it in jira?
>>
>>
>>
>> Anyway as I said, the main issue is not user questions (except maybe
>> advanced ones) but more for dev questions. It is so easy to get lost in the
>> chatter that it makes it very hard for people to learn spark internalsâ€¦
>>
>> Assaf.
>>
>>
>>
>> From: Sean Owen [mailto:sowen@cloudera.com]
>> Sent: Wednesday, November 02, 2016 2:07 PM
>> To: Mendelson, Assaf; dev@spark.apache.org
>> Subject: Re: Handling questions in the mailing lists
>>
>>
>>
>> I think that unfortunately mailing lists don't scale well. This one has
>> thousands of subscribers with different interests and levels of experience.
>> For any given person, most messages will be irrelevant. I also find that a
>> lot of questions on user@ are not well-asked, aren't an SSCCE
>> (http://sscce.org/), not something most people are going to bother replying
>> to even if they could answer. I almost entirely ignore user@ because there
>> are higher-priority channels like PRs to deal with, that already have
>> hundreds of messages per day. This is why little of it gets an answer -- too
>> noisy.
>>
>>
>>
>> We have to have official mailing lists, in any event, to have some
>> official channel for things like votes and announcements. It's not wrong to
>> ask questions on user@ of course, but a lot of the questions I see could
>> have been answered with research of existing docs or looking at the code. I
>> think that given the scale of the list, it's not wrong to assert that this
>> is sort of a prerequisite for asking thousands of people to answer one's
>> question. But we can't enforce that.
>>
>>
>>
>> The situation will get better to the extent people ask better questions,
>> help other people ask better questions, and answer good questions. I'd
>> encourage anyone feeling this way to try to help along those dimensions.
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>> On Wed, Nov 2, 2016 at 11:32 AM assaf.mendelson <assaf.mendelson@rsa.com>
>> wrote:
>>
>> Hi,
>>
>> I know this is a little off topic but I wanted to raise an issue about
>> handling questions in the mailing list (this is true both for the user
>> mailing list and the dev but since there are other options such as stack
>> overflow for user questions, this is more problematic in dev).
>>
>> Letâ€™s say I ask a question (as I recently did). Unfortunately this was
>> during spark summit in Europe so probably people were busy. In any case no
>> one answered.
>>
>> The problem is, that if no one answers very soon, the question will almost
>> certainly remain unanswered because new messages will simply drown it.
>>
>>
>>
>> This is a common issue not just for questions but for any comment or idea
>> which is not immediately picked up.
>>
>>
>>
>> I believe we should have a method of handling this.
>>
>> Generally, I would say these types of things belong in stack overflow,
>> after all, the way it is built is perfect for this. More seasoned spark
>> contributors and committers can periodically check out unanswered questions
>> and answer them.
>>
>> The problem is that stack overflow (as well as other targets such as the
>> databricks forums) tend to have a more user based orientation. This means
>> that any spark internal question will almost certainly remain unanswered.
>>
>>
>>
>> I was wondering if we could come up with a solution for this.
>>
>>
>>
>> Assaf.
>>
>>
>>
>>
>>
>> ________________________________
>>
>> View this message in context: Handling questions in the mailing lists
>> Sent from the Apache Spark Developers List mailing list archive at
>> Nabble.com<http://nabble.com>.

---------------------------------------------------------------------
To unsubscribe e-mail: dev-unsubscribe@spark.apache.org








--
Maciej Szymkiewicz

"
Ryan Blue <rblue@netflix.com.INVALID>,"Tue, 8 Nov 2016 09:14:17 -0800",Re: Spark Improvement Proposals,Cody Koeninger <cody@koeninger.org>,"
First, why lazy consensus? The proposal was for consensus, which is at
least three +1 votes and no vetos. Consensus has no losing side, it
requires getting to a point where there is agreement. Isn't that agreement
what we want to achieve with these proposals?

Second, lazy consensus only removes the requirement for three +1 votes. Why
would we not want at least three committers to think something is a good
idea before adopting the proposal?

rb


:
t
e
e
d
t
as
.
@
er
n
ut
k
is
ot
e
e
n
to
th
t
ve
ys
or
y
 -
me
s
in
ed
ut
s
)
e
y,
t
.
rk
dd
.


-- 
Ryan Blue
Software Engineer
Netflix
"
"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Tue, 8 Nov 2016 09:59:03 -0800",Re: [VOTE] Release Apache Spark 2.0.2 (RC3),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1


n
 if
l
0.2.
"
Joseph Bradley <joseph@databricks.com>,"Tue, 8 Nov 2016 10:40:28 -0800",Re: Issue + Resolution: Kmeans Spark Performances (ML package),Zakaria Hili <zakahili@gmail.com>,"Hi Zakaria,

Thanks for reporting this.  This actually isn't too big of an issue since
the user can cache the Dataset passed to spark.ml.clustering.KMeans before
fitting, but the message does mislead the user.  A patch to fix that
message would be good, though this issue should be fixed before too long as
we move more implementations from spark.mllib to spark.ml.

Thanks!
Joseph


t
I
"
Zakaria Hili <zakahili@gmail.com>,"Tue, 8 Nov 2016 19:50:58 +0100",Re: Issue + Resolution: Kmeans Spark Performances (ML package),Joseph Bradley <joseph@databricks.com>,"Hi Joseph,
Thank you for your reply, but caching the dataframe (input) doesn't resolve
the problem,
As you can see in my piece of code, I already cached my dataframe
<df_Part.cache()>, but Internally ml.clustering.KMeans converts DataFrame
to RDD[mllib.linalg.Vector].
Then, Executes mllib.clustering.KMeans.
So, while you cache DataFrame, RDD which is used internally is not cached.

Regards
Zakaria

2016-11-08 19:40 GMT+01:00 Joseph Bradley <joseph@databricks.com>:

e
as


-- 
Zakaria HILI

IngÃ©nieur Big Data

Practice CSD | Capgemini | Toulouse | Bureau EJ4-11

TÃ©l. : +33(0)7 53 65 36 85

www.capgemini.com

109, avenue du GÃ©nÃ©ral Eisenhower - 31036 Toulouse Cedex 1 â€“ France

- Emails :

-ZakariaHILI@Gmail.com

-Zakaria.Hili@Capgemini.com

-Zakaria.hili@Enseirb-matmeca.fr
"
Sean Owen <sowen@cloudera.com>,"Tue, 08 Nov 2016 21:17:39 +0000",Re: [VOTE] Release Apache Spark 2.0.2 (RC3),"Reynold Xin <rxin@databricks.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","+1 binding

(See comments on last vote; same results, except, the regression we
identified is fixed now.)


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 08 Nov 2016 21:42:04 +0000",Diffing execution plans to understand an optimizer bug,Spark dev list <dev@spark.apache.org>,"Iâ€™m trying to understand what I think is an optimizer bug. To do that, Iâ€™d
like to compare the execution plans for a certain query with and without a
certain change, to understand how that change is impacting the plan.

How would I do that in PySpark? Iâ€™m working with 2.0.1, but I can use
master if it helps.

explain()
<http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.explain>
is helpful but is limited in two important ways:

   1. It prints to screen and doesnâ€™t offer another way to access the plan
   or capture it.
   2.

   The printed plan includes auto-generated IDs that make diffing
   impossible. e.g.

    == Physical Plan ==
    *Project [struct(primary_key#722, person#550, dataset_name#671)


Any suggestions on what to do? Any relevant JIRAs I should follow?

Nick
â€‹
"
Reynold Xin <rxin@databricks.com>,"Tue, 8 Nov 2016 13:47:16 -0800",Re: Diffing execution plans to understand an optimizer bug,Nicholas Chammas <nicholas.chammas@gmail.com>,"If you want to peek into the internals and do crazy things, it is much
easier to do it in Scala with df.queryExecution.

For explain string output, you can work around the comparison simply by
doing replaceAll(""#\\d+"", ""#x"")

similar to the patch here:
https://github.com/apache/spark/commit/fd90541c35af2bccf0155467bec8cea7c8865046#diff-432455394ca50800d5de508861984ca5R217



m

that, Iâ€™d
a
 use
sql.DataFrame.explain>
 the
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 08 Nov 2016 22:00:40 +0000",Re: Diffing execution plans to understand an optimizer bug,Reynold Xin <rxin@databricks.com>,"Hmm, it doesnâ€™t seem like I can access the output of
df._jdf.queryExecution().hiveResultString() from Python, and until I can
boil the issue down a bit, Iâ€™m stuck with using Python.

Iâ€™ll have a go at using regexes to strip some stuff from the printed plans.
The one thatâ€™s working for me to strip the IDs is #\d+L?.

Nick
â€‹


865046#diff-432455394ca50800d5de508861984ca5R217
that, Iâ€™d
a
 use
sql.DataFrame.explain>
 the
"
Michael Armbrust <michael@databricks.com>,"Tue, 8 Nov 2016 14:03:27 -0800",Re: [VOTE] Release Apache Spark 2.0.2 (RC3),Sean Owen <sowen@cloudera.com>,1
"""Dongjoon Hyun""<dongjoon@apache.org>","Tue, 08 Nov 2016 22:18:04 -0000",Re: [VOTE] Release Apache Spark 2.0.2 (RC3),<dev@spark.apache.org>,"+1 (non-binding)

It's built and tested on CentOS 6.8 / OpenJDK 1.8.0_111 with `-Pyarn -Phadoop-2.7 -Pkinesis-asl -Phive -Phive-thriftserver -Psparkr` profile.

Cheers!
Dongjoon.


---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 08 Nov 2016 22:36:12 +0000",Re: Diffing execution plans to understand an optimizer bug,Reynold Xin <rxin@databricks.com>,"SPARK-18367 <https://issues.apache.org/jira/browse/SPARK-18367>: limit()
makes the lame walk again


ted
865046#diff-432455394ca50800d5de508861984ca5R217
that, Iâ€™d
a
 use
sql.DataFrame.explain>
 the
"
=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@databricks.com>,"Tue, 8 Nov 2016 23:42:35 +0100",Re: Diffing execution plans to understand an optimizer bug,Nicholas Chammas <nicholas.chammas@gmail.com>,"Replied in the ticket.


nted
 that,
 and
n use
.sql.DataFrame.explain>
s the
"
Mark Grover <mark@apache.org>,"Tue, 8 Nov 2016 15:26:57 -0800",Connectors using new Kafka consumer API,"dev <dev@spark.apache.org>, Cody Koeninger <cody@koeninger.org>","Hi all,
We currently have a new direct stream connector, thanks to work by Cody and
others on SPARK-12177.

However, that can't be used in secure clusters that require Kerberos
authentication. That's because Kafka currently doesn't support delegation
tokens (KAFKA-1696 <https://issues.apache.org/jira/browse/KAFKA-1696>).
Unfortunately, very little work has been done on that JIRA, so, in my
opinion, folks who want to use secure Kafka (using the norm - Kerberos)
can't do so because Spark Streaming can't consume from it today.

The right way is, of course, to get delegation tokens in Kafka but honestly
I don't know if that's happening in the near future. I am wondering if we
should consider something to remedy this - for example, we could come up
with a receiver based connector based on the new Kafka consumer API that'd
support kerberos authentication. It won't require delegation tokens since
there's only a very small number of executors talking to Kafka. Of course,
for anyone who cares about high throughput and other direct connector
benefits would have to use direct connector. Another thing we could do is
ship the keytab to the executors in the direct connector, so delegation
tokens are not required but the latter would be a pretty comprising
solution, and I'd prefer not doing that.

What do folks think? Would love to hear your thoughts, especially about the
receiver.

Thanks!
Mark
"
Joseph Bradley <joseph@databricks.com>,"Tue, 8 Nov 2016 15:35:29 -0800",Re: Issue + Resolution: Kmeans Spark Performances (ML package),Zakaria Hili <zakahili@gmail.com>,"Hi Zakaria,

Caching the DataFrame can ""solve the problem"" in terms of efficiency in
many cases.  It is true it will not eliminate the warning message.  I'd be
fine adding the RDD caching, if you're interested in creating a JIRA and
sending a PR.

Joseph


.
e
re
 as
n
o
e
e
€“ France
"
"""Jagadeesan As"" <as2@us.ibm.com>","Wed, 9 Nov 2016 09:05:16 +0530",Re: [VOTE] Release Apache Spark 2.0.2 (RC3),Reynold Xin <rxin@databricks.com>,"+1 (non binding) Ubuntu 14.04.2 OpenJDK-1.8.0_72 Pyarn -Phadoop-2.7 
-Psparkr -Pkinesis-asl -Phive-thriftserver 
 
Cheers,
Jagadeesan A S



From:   Reynold Xin <rxin@databricks.com>
To:     ""dev@spark.apache.org"" <dev@spark.apache.org>
Date:   08-11-16 1"
Liwei Lin <lwlin7@gmail.com>,"Wed, 9 Nov 2016 11:38:04 +0800",Re: [VOTE] Release Apache Spark 2.0.2 (RC3),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1 (non-binding)

Cheers,
Liwei


n
 if
l
0.2.
"
Cody Koeninger <cody@koeninger.org>,"Tue, 8 Nov 2016 21:59:08 -0600",Re: Connectors using new Kafka consumer API,Mark Grover <mark@apache.org>,"Have you asked the assignee on the Kafka jira whether they'd be
willing to accept help on it?


---------------------------------------------------------------------


"
Mark Grover <mark@apache.org>,"Tue, 8 Nov 2016 20:05:27 -0800",Re: Connectors using new Kafka consumer API,Cody Koeninger <cody@koeninger.org>,"I think they are open to others helping, in fact, more than one person has
worked on the JIRA so far. And, it's been crawling really slowly and that's
preventing adoption of Spark's new connector in secure Kafka environments.


"
Weiqing Yang <yangweiqing001@gmail.com>,"Tue, 8 Nov 2016 20:21:04 -0800",Re: [VOTE] Release Apache Spark 2.0.2 (RC3),Liwei Lin <lwlin7@gmail.com>," +1 (non binding)


Environment: CentOS Linux release 7.0.1406 (Core) / openjdk version
""1.8.0_111""



./build/mvn -Pyarn -Phadoop-2.7 -Pkinesis-asl -Phive -Phive-thriftserver
-Dpyspark -Dsparkr -DskipTests clean package

./build/mvn -Pyarn -Phadoop-2.7 -Pkinesis-asl -Phive -Phive-thriftserver
-Dpyspark -Dsparkr test




:
nd
:
/
/
g
ll
C
.0.2.
"
vaquar khan <vaquar.khan@gmail.com>,"Wed, 9 Nov 2016 00:14:21 -0600",Re: [VOTE] Release Apache Spark 2.0.2 (RC3),Weiqing Yang <yangweiqing001@gmail.com>,"*+1 (non binding)*


and
t:
/
te,
ill
to


-- 
Regards,
Vaquar Khan
+1 -224-436-0783

IT Architect / Lead Consultant
Greater Chicago
"
"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Wed, 9 Nov 2016 00:02:41 -0700 (MST)",RE: Handling questions in the mailing lists,dev@spark.apache.org,"I like the document and I think it is good but I still feel like we are missing an important part here.

Look at SO today. There are:

-           4658 unanswered questions under apache-spark tag.

-          394 unanswered questions under spark-dataframe tag.

-          639 unanswered questions under apache-spark-sql

-          859 unanswered questions under pyspark

Just moving people to ask there will not help. The whole issue is having people answer the questions.

The problem is that many of these questions do not fit SO (but are already there so they are noise), are bad (i.e. unclear or hard to answer), orphaned etc. while some are simply harder than what people with some experience in spark can handle and require more expertise.
The problem is that people with the relevant expertise are drowning in noise. This. Is true for the mailing list and this is true for SO.

For this reason I believe that just moving people to SO will not solve anything.

My original thought was that if we had different tags then different people could watch open questions on these tags and therefore have a much lower noise. I thought that we would have a low tier (current one) of people just not following the documentation (which would remain as noise), then a beginner tier where we could have people downvoting bad questions but in most cases the community can answer the questions because they are common, then a â€œmediumâ€ tier which would mean harder questions but that can still be answered by advanced users and lastly an â€œadvancedâ€ tier to which committers can actually subscribed to (and adding sub tags for subsystems would improve this even more).

I was not aware of SO policy for meta tags (the burnination link is about removing tags completely so I am not sure how it applies, I believe this link https://stackoverflow.blog/2010/08/the-death-of-meta-tags/ is more relevant).
There was actually a discussion along the lines in SO (http://meta.stackoverflow.com/questions/253338/filtering-questions-by-difficulty-level).

The fact that SO did not solve this issue, does not mean we shouldnâ€™t either.

The way I see it, some tags can easily be used even with the meta tags limitation. For example, using spark-internal-development tag can be used to ask questions for development of spark. There are already tags for some spark subsystems (there is a apachae-spark-sql tag, a pyspark tag, a spark-streaming tag etc.). The main issue I see and the one we canâ€™t seem to get around is dividing between simple questions that the community should answer and hard questions which only advanced users can answer.

Maybe SO isnâ€™t the correct platform for that but even within it we can try to find a non meta name for spark beginner questions vs. spark advanced questions.
Assaf.


From: Denny Lee [via Apache Spark Developers List] [mailto:ml-node+s1001551n19770h59@n3.nabble.com]
Sent: Tuesday, November 08, 2016 7:53 AM
To: Mendelson, Assaf
Subject: Re: Handling questions in the mailing lists

To help track and get the verbiage for the Spark community page and welcome email jump started, here's a working document for us to work with: https://docs.google.com/document/d/1N0pKatcM15cqBPqFWCqIy6jdgNzIoacZlYDCjufBh2s/edit#<https://docs.google.com/document/d/1N0pKatcM15cqBPqFWCqIy6jdgNzIoacZlYDCjufBh2s/edit>

Hope this will help us collaborate on this stuff a little faster.


Just a couple of random thoughts regarding Stack Overflow...

  *   If we are thinking about shifting focus towards SO all attempts of micromanaging should be discarded right in the beginning. Especially things like meta tags, which are discouraged and ""burninated"" (https://meta.stackoverflow.com/tags/burninate-request/info) , or thread bumping. Depending on a context these won't be manageable, go against community guidelines or simply obsolete.
  *   Lack of expertise is unlikely an issue. Even now there is a number of advanced Spark users on SO. Of course the more the merrier.

Things that can be easily improved:

  *   Identifying, improving and promoting canonical questions and answers. It means closing duplicate, suggesting edits to improve existing answers, providing alternative solutions. This can be also used to identify gaps in the documentation.
  *   Providing a set of clear posting guidelines to reduce effort required to identify the problem (think about http://stackoverflow.com/q/5963269 a.k.a How to make a great R reproducible example?)
  *   Helping users decide if question is a good fit for SO (see below). API questions are great fit, debugging problems like ""my cluster is slow"" are not.
  *   Actively cleaning (closing, deleting) off-topic and low quality questions. The less junk to sieve through the better chance of good questions being answered.
  *   Repurposing and actively moderating SO docs (https://stackoverflow.com/documentation/apache-spark/topics). Right now most of the stuff that goes there is useless, duplicated or plagiarized, or border case SPAM.
  *   Encouraging community to monitor featured (https://stackoverflow.com/questions/tagged/apache-spark?sort=featured) and active & upvoted & unanswered (https://stackoverflow.com/unanswered/tagged/apache-spark) questions.
  *   Implementing some procedure to identify questions which are likely to be bugs or a material for feature requests. Personally I am quite often tempted to simply send a link to dev list, but I don't think it is really acceptable.
  *   Animating Spark related chat room. I tried this a couple of times but to no avail. Without a certain critical mass of users it just won't work.



This is an excellent point. If we do go ahead and feature SO as a way for users to ask questions more prominently, as someone who knows SO very well, would you be willing to help write a short guideline (ideally the shorter the better, which makes it hard) to direct what goes to user@ and what goes to SO?

Sure, I'll be happy to help if I can.






Damn, I always thought that mailing list is only for nice and welcoming people and there is nothing to do for me here >:)

To be serious though, there are many questions on the users list which would fit just fine on SO but it is not true in general. There are dozens of questions which are to broad, opinion based, ask for external resources and so on. If you want to direct users to SO you have to help them to decide if it is the right channel. Otherwise it will just create a really bad experience for both seeking help and active answerers. Former ones will be downvoted and bashed, latter ones will have to deal with handling all the junk and the number of active Spark users with moderation privileges is really low (with only Massg and me being able to directly close duplicates).

Believe me, I've seen this before.
You have substantially underestimated how opinionated people can be on mailing lists too :)


You have to remember that Stack Overflow crowd (like me) is highly opinionated, so many questions, which could be just fine on the mailing list, will be quickly downvoted and / or closed as off-topic. Just saying...

--

Best,

Maciej

OK I've checked on the ASF member list (which is private so there is no public archive).

It is not against any ASF rule to recommend StackOverflow as a place for users to ask questions. I don't think we can or should delete the existing user@spark list either, but we can certainly make SO more visible than it is.



Actually after talking with more ASF members, I believe the only policy is that development decisions have to be made and announced on ASF properties (dev list or jira), but user questions don't have to.

I'm going to double check this. If it is true, I would actually recommend us moving entirely over the Q&A part of the user list to stackoverflow, or at least make that the recommended way rather than the existing user list which is not very scalable.



Weâ€™ve discussed several times upgrading our communication tools, as far back as 2014 and maybe even before that too. The bottom line is that we canâ€™t due to ASF rules requiring the use of ASF-managed mailing lists.

For some history, see this discussion:
Â·         https://mail-archives.apache.org/mod_mbox/spark-user/201412.mbox/%3CCAOhmDzfL2COdysV8r5hZN8f=NqXM=f=oY5NO2dHWJ_kVEoP+Ng@...%3E<https://mail-archives.apache.org/mod_mbox/spark-user/201412.mbox/%3CCAOhmDzfL2COdysV8r5hZN8f=NqXM=f=oY5NO2dHWJ_kVEoP+Ng@mail.gmail.com%3E>
Â·         https://mail-archives.apache.org/mod_mbox/spark-user/201501.mbox/%3CCAOhmDzec1JdsXQq3dDwAv7eLnzRidSkrsKKG0xKw=TKTxY_sYw@...%3E<https://mail-archives.apache.org/mod_mbox/spark-user/201501.mbox/%3CCAOhmDzec1JdsXQq3dDwAv7eLnzRidSkrsKKG0xKw=TKTxY_sYw@mail.gmail.com%3E>

(Itâ€™s ironic that itâ€™s difficult to follow the past discussion on why we canâ€™t change our official communication tools due to those very toolsâ€¦)

Nick
â€‹

I fell Assaf point is quite relevant if we want to move this project forward from the Spark user perspective (as I do). In fact, we're still using 20th century tools (mailing lists) with some add-ons (like Stack Overflow).

As usually, Sean and Cody's contributions are very to the point.
I fell it is indeed a matter of of culture (hard to enforce) and tools (much easier). Isn't it?

So concrete things people could do

- users could tag subject lines appropriately to the component they're
asking about

- contributors could monitor user@ for tags relating to components
they've worked on.
I'd be surprised if my miss rate for any mailing list questions
well-labeled as Kafka was higher than 5%

- committers could be more aggressive about soliciting and merging PRs
to improve documentation.
It's a lot easier to answer even poorly-asked questions with a link to
relevant docs.

elf
's
to
ty
ndEmail.jtp?type=node&node=19770&i=8>>
g
er
the
â€¦
ode=19770&i=9>]
e=19770&i=10>
ce.
 a
ing
re
 too
 to
. I
is
ndEmail.jtp?type=node&node=19770&i=11>>
is was
no
st
a
ons
s
.
---------------------------------------------------------------------
=19770&i=12>







--

Maciej Szymkiewicz

________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/Handling-questions-in-the-mailing-lists-tp19690p19770.html
To start a new topic under Apache Spark Developers List, email ml-node+s1001551n1h20@n3.nabble.com<mailto:ml-node+s1001551n1h20@n3.nabble.com>
spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=1&code=YXNzYWYubWVuZGVsc29uQHJzYS5jb218MXwtMTI4OTkxNTg1Mg==>.
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>




--
3.nabble.com/Handling-questions-in-the-mailing-lists-tp19690p19798.html
om."
Denny Lee <denny.g.lee@gmail.com>,"Wed, 09 Nov 2016 07:53:18 +0000",Re: Handling questions in the mailing lists,"""assaf.mendelson"" <assaf.mendelson@rsa.com>, dev@spark.apache.org","Agreed that by simply just moving the questions to SO will not solve
anything but I think the call out about the meta-tags is that we need to
abide by SO rules and if we were to just jump in and start creating
meta-tags, we would be violating at minimum the spirit and at maximum the
actual conventions around SO.

Saying this, perhaps we could suggest tags that we place in the header of
the question whether it be SO or the mailing lists that will help us sort
through all of these questions faster just as you suggested.  The Proposed
Community Mailing Lists / StackOverflow Changes
<https://docs.google.com/document/d/1N0pKatcM15cqBPqFWCqIy6jdgNzIoacZlYDCjufBh2s/edit#heading=h.xshc1bv4sn3p>
has
been updated to include suggested tags.  WDYT?


y
,
tions but that can
 tier to which
ficulty-level
™t
e
™t seem
e can try
den
:
ufBh2s/edit#
jufBh2s/edit>
hings
ing
ntify
ow""
d)
ften
lly
t
l,
s
ll
is
s).
g
s
s
r
as far
ists.
DzfL2COdysV8r5hZN8f=NqXM=f=oY5NO2dHWJ_kVEoP+Ng@...%3E
mDzfL2COdysV8r5hZN8f=NqXM=f=oY5NO2dHWJ_kVEoP+Ng@mail.gmail.com%3E>
Dzec1JdsXQq3dDwAv7eLnzRidSkrsKKG0xKw=TKTxY_sYw@...%3E
mDzec1JdsXQq3dDwAv7eLnzRidSkrsKKG0xKw=TKTxY_sYw@mail.gmail.com%3E>
ssion on why we
oolsâ€¦)
e
@
of
d
n
â€¦
s
's
s,
s.
ck
this was
e
k
he
ons-in-the-mailing-lists-tp19690p19770.html
ervlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
ions-in-the-mailing-lists-tp19690p19798.html>
"
"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Wed, 9 Nov 2016 03:18:51 -0700 (MST)",RE: Handling questions in the mailing lists,dev@spark.apache.org,"I was just wondering, before we move on to SO.
Do we have enough contributors with enough reputation do manage things in SO?
We would need contributors with enough reputation to have relevant privilages.
For example: creating tags (requires 1500 reputation), edit questions and answers (2000), create tag synonums (2500), approve tag wiki edits (5000), access to moderator tools (10000, this is required to delete questions etc.), protect questions (15000).
All of these are important if we plan to have SO as a main resource.
I know I originally suggested SO, however, if we do not have contributors with the required privileges and the willingness to help manage everything then I am not sure this is a good fit.
Assaf.

From: Denny Lee [via Apache Spark Developers List] [mailto:ml-node+s1001551n19799h58@n3.nabble.com]
Sent: Wednesday, November 09, 2016 9:54 AM
To: Mendelson, Assaf
Subject: Re: Handling questions in the mailing lists

Agreed that by simply just moving the questions to SO will not solve anything but I think the call out about the meta-tags is that we need to abide by SO rules and if we were to just jump in and start creating meta-tags, we would be violating at minimum the spirit and at maximum the actual conventions around SO.

Saying this, perhaps we could suggest tags that we place in the header of the question whether it be SO or the mailing lists that will help us sort through all of these questions faster just as you suggested.  The Proposed Community Mailing Lists / StackOverflow Changes<https://docs.google.com/document/d/1N0pKatcM15cqBPqFWCqIy6jdgNzIoacZlYDCjufBh2s/edit#heading=h.xshc1bv4sn3p> has been updated to include suggested tags.  WDYT?

I like the document and I think it is good but I still feel like we are missing an important part here.

Look at SO today. There are:

-           4658 unanswered questions under apache-spark tag.

-          394 unanswered questions under spark-dataframe tag.

-          639 unanswered questions under apache-spark-sql

-          859 unanswered questions under pyspark

Just moving people to ask there will not help. The whole issue is having people answer the questions.

The problem is that many of these questions do not fit SO (but are already there so they are noise), are bad (i.e. unclear or hard to answer), orphaned etc. while some are simply harder than what people with some experience in spark can handle and require more expertise.
The problem is that people with the relevant expertise are drowning in noise. This. Is true for the mailing list and this is true for SO.

For this reason I believe that just moving people to SO will not solve anything.

My original thought was that if we had different tags then different people could watch open questions on these tags and therefore have a much lower noise. I thought that we would have a low tier (current one) of people just not following the documentation (which would remain as noise), then a beginner tier where we could have people downvoting bad questions but in most cases the community can answer the questions because they are common, then a â€œmediumâ€ tier which would mean harder questions but that can still be answered by advanced users and lastly an â€œadvancedâ€ tier to which committers can actually subscribed to (and adding sub tags for subsystems would improve this even more).

I was not aware of SO policy for meta tags (the burnination link is about removing tags completely so I am not sure how it applies, I believe this link https://stackoverflow.blog/2010/08/the-death-of-meta-tags/ is more relevant).
There was actually a discussion along the lines in SO (http://meta.stackoverflow.com/questions/253338/filtering-questions-by-difficulty-level).

The fact that SO did not solve this issue, does not mean we shouldnâ€™t either.

The way I see it, some tags can easily be used even with the meta tags limitation. For example, using spark-internal-development tag can be used to ask questions for development of spark. There are already tags for some spark subsystems (there is a apachae-spark-sql tag, a pyspark tag, a spark-streaming tag etc.). The main issue I see and the one we canâ€™t seem to get around is dividing between simple questions that the community should answer and hard questions which only advanced users can answer.

Maybe SO isnâ€™t the correct platform for that but even within it we can try to find a non meta name for spark beginner questions vs. spark advanced questions.
Assaf.


From: Denny Lee [via Apache Spark Developers List] [mailto:[hidden email]</user/SendEmail.jtp?type=node&node=19799&i=1>[hidden email]<http://user/SendEmail.jtp?type=node&node=19798&i=0>]
Sent: Tuesday, November 08, 2016 7:53 AM
To: Mendelson, Assaf

Subject: Re: Handling questions in the mailing lists

To help track and get the verbiage for the Spark community page and welcome email jump started, here's a working document for us to work with: https://docs.google.com/document/d/1N0pKatcM15cqBPqFWCqIy6jdgNzIoacZlYDCjufBh2s/edit#<https://docs.google.com/document/d/1N0pKatcM15cqBPqFWCqIy6jdgNzIoacZlYDCjufBh2s/edit>

Hope this will help us collaborate on this stuff a little faster.

Just a couple of random thoughts regarding Stack Overflow...

  *   If we are thinking about shifting focus towards SO all attempts of micromanaging should be discarded right in the beginning. Especially things like meta tags, which are discouraged and ""burninated"" (https://meta.stackoverflow.com/tags/burninate-request/info) , or thread bumping. Depending on a context these won't be manageable, go against community guidelines or simply obsolete.
  *   Lack of expertise is unlikely an issue. Even now there is a number of advanced Spark users on SO. Of course the more the merrier.

Things that can be easily improved:

  *   Identifying, improving and promoting canonical questions and answers. It means closing duplicate, suggesting edits to improve existing answers, providing alternative solutions. This can be also used to identify gaps in the documentation.
  *   Providing a set of clear posting guidelines to reduce effort required to identify the problem (think about http://stackoverflow.com/q/5963269 a.k.a How to make a great R reproducible example?)
  *   Helping users decide if question is a good fit for SO (see below). API questions are great fit, debugging problems like ""my cluster is slow"" are not.
  *   Actively cleaning (closing, deleting) off-topic and low quality questions. The less junk to sieve through the better chance of good questions being answered.
  *   Repurposing and actively moderating SO docs (https://stackoverflow.com/documentation/apache-spark/topics). Right now most of the stuff that goes there is useless, duplicated or plagiarized, or border case SPAM.
  *   Encouraging community to monitor featured (https://stackoverflow.com/questions/tagged/apache-spark?sort=featured) and active & upvoted & unanswered (https://stackoverflow.com/unanswered/tagged/apache-spark) questions.
  *   Implementing some procedure to identify questions which are likely to be bugs or a material for feature requests. Personally I am quite often tempted to simply send a link to dev list, but I don't think it is really acceptable.
  *   Animating Spark related chat room. I tried this a couple of times but to no avail. Without a certain critical mass of users it just won't work.



This is an excellent point. If we do go ahead and feature SO as a way for users to ask questions more prominently, as someone who knows SO very well, would you be willing to help write a short guideline (ideally the shorter the better, which makes it hard) to direct what goes to user@ and what goes to SO?

Sure, I'll be happy to help if I can.

Damn, I always thought that mailing list is only for nice and welcoming people and there is nothing to do for me here >:)

To be serious though, there are many questions on the users list which would fit just fine on SO but it is not true in general. There are dozens of questions which are to broad, opinion based, ask for external resources and so on. If you want to direct users to SO you have to help them to decide if it is the right channel. Otherwise it will just create a really bad experience for both seeking help and active answerers. Former ones will be downvoted and bashed, latter ones will have to deal with handling all the junk and the number of active Spark users with moderation privileges is really low (with only Massg and me being able to directly close duplicates).

Believe me, I've seen this before.
You have substantially underestimated how opinionated people can be on mailing lists too :)

You have to remember that Stack Overflow crowd (like me) is highly opinionated, so many questions, which could be just fine on the mailing list, will be quickly downvoted and / or closed as off-topic. Just saying...

--

Best,

Maciej

OK I've checked on the ASF member list (which is private so there is no public archive).

It is not against any ASF rule to recommend StackOverflow as a place for users to ask questions. I don't think we can or should delete the existing user@spark list either, but we can certainly make SO more visible than it is.


Actually after talking with more ASF members, I believe the only policy is that development decisions have to be made and announced on ASF properties (dev list or jira), but user questions don't have to.

I'm going to double check this. If it is true, I would actually recommend us moving entirely over the Q&A part of the user list to stackoverflow, or at least make that the recommended way rather than the existing user list which is not very scalable.



Weâ€™ve discussed several times upgrading our communication tools, as far back as 2014 and maybe even before that too. The bottom line is that we canâ€™t due to ASF rules requiring the use of ASF-managed mailing lists.

For some history, see this discussion:
â€¢         https://mail-archives.apache.org/mod_mbox/spark-user/201412.mbox/%3CCAOhmDzfL2COdysV8r5hZN8f=NqXM=f=oY5NO2dHWJ_kVEoP+Ng@...%3E<https://mail-archives.apache.org/mod_mbox/spark-user/201412.mbox/%3CCAOhmDzfL2COdysV8r5hZN8f=NqXM=f=oY5NO2dHWJ_kVEoP+Ng@mail.gmail.com%3E>
â€¢         https://mail-archives.apache.org/mod_mbox/spark-user/201501.mbox/%3CCAOhmDzec1JdsXQq3dDwAv7eLnzRidSkrsKKG0xKw=TKTxY_sYw@...%3E<https://mail-archives.apache.org/mod_mbox/spark-user/201501.mbox/%3CCAOhmDzec1JdsXQq3dDwAv7eLnzRidSkrsKKG0xKw=TKTxY_sYw@mail.gmail.com%3E>

(Itâ€™s ironic that itâ€™s difficult to follow the past discussion on why we canâ€™t change our official communication tools due to those very toolsâ€¦)

Nick
â€‹
I fell Assaf point is quite relevant if we want to move this project forward from the Spark user perspective (as I do). In fact, we're still using 20th century tools (mailing lists) with some add-ons (like Stack Overflow).

As usually, Sean and Cody's contributions are very to the point.
I fell it is indeed a matter of of culture (hard to enforce) and tools (much easier). Isn't it?
So concrete things people could do

- users could tag subject lines appropriately to the component they're
asking about

- contributors could monitor user@ for tags relating to components
they've worked on.
I'd be surprised if my miss rate for any mailing list questions
well-labeled as Kafka was higher than 5%

- committers could be more aggressive about soliciting and merging PRs
to improve documentation.
It's a lot easier to answer even poorly-asked questions with a link to
relevant docs.

elf
's
to
ty
ser/SendEmail.jtp?type=node&node=19770&i=8>>

g
er
the
â€¦
node&node=19770&i=9>]

de&node=19770&i=10>

ce.
 a
ing
re
 too
 to
. I
is
ser/SendEmail.jtp?type=node&node=19770&i=11>>

is was
no
st
a
ons
s
.
---------------------------------------------------------------------
&node=19770&i=12>







--

Maciej Szymkiewicz

________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/Handling-questions-in-the-mailing-lists-tp19690p19770.html
To start a new topic under Apache Spark Developers List, email [hidden email]<http://user/SendEmail.jtp?type=node&node=19798&i=1>
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>

________________________________
View this message in context: RE: Handling questions in the mailing lists<http://apache-spark-developers-list.1001551.n3.nabble.com/Handling-questions-in-the-mailing-lists-tp19690p19798.html>
he-spark-developers-list.1001551.n3.nabble.com/> at Nabble.com.

________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/Handling-questions-in-the-mailing-lists-tp19690p19799.html
To start a new topic under Apache Spark Developers List, email ml-node+s1001551n1h20@n3.nabble.com<mailto:ml-node+s1001551n1h20@n3.nabble.com>
spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=1&code=YXNzYWYubWVuZGVsc29uQHJzYS5jb218MXwtMTI4OTkxNTg1Mg==>.
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>




--
3.nabble.com/Handling-questions-in-the-mailing-lists-tp19690p19800.html
om."
Gerard Maas <gerard.maas@gmail.com>,"Wed, 9 Nov 2016 14:35:49 +0100",Re: Handling questions in the mailing lists,Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Great discussion. Glad to see it happening and lucky to have seen it on the
mailing list due to its high volume.

I had this same conversation with Patrick Wendell few Spark Summits ago. At
the time, SO was not even listed as a resource and the idea was to make it
the primary ""go-to"" place for questions.

Having contributed to both the list (in its early days) and SO, the biggest
hurdle IMO is how to deal with lazy people. These days, at SO, I spend more
time leaving comments than answering in an attempt to moderate the
requirement of ""show some effort"" and clarify unclear questions.

It's my impression that the mailing list is much more friendly with ""plz
send me da code"" folk and indeed would answer questions that would
otherwise get down-voted or closed at SO. That also shows in the high email
volume, which at the same time lowers its value for many of us who get
overwhelmed. It's hard to separate authentic efforts in getting started,
which deserve help and encouraging vs moderating ""work dumpers"" that abuse
resources to get their thing done. Also, beginner questions always repeat
and a mailing list has no features to help with that.

The model I had in imagined roughly follows the ""Odersky scale"":
 - Users new with the technology and basic ""how to"" questions belong in
Stack Overflow. => The search and de-duplication features should help in
getting an answer if already present, reducing the load.
 - Advanced discussions and troubleshooting belong in users@
 - Library bugs, new features and improvements belong in dev@

Off course, there's no hard line between these levels and it would require
contributor discretion aided with some routing procedure:

- Spark documentation should establish Stack Overflow as the main go-to
resource.
- Contributors on the list should friendly redirect ""intro level questions""
to Stack Overflow.
- SO contributors should redirect potential bugs and questions deserving a
deeper discussion to @users or @dev as needed
- @users -> @dev as today
- Cross-posting SO + @users should be discouraged. The idea is to create
efficient channels.

A good resource on how and where to ask questions would be a great routing
channel between the levels above.
I'm willing to help with moderation efforts on ""Spark Overflow"" :-) to get
this going.

The Spark community has always been very welcoming and that spirit should
be preserved. We just need to channel the efforts in a more efficient way.

my 2c,

Gerard.



hings
ing
ntify
ow""
/
ed>)
ften
lly
t
l,
m
s
es
y
ill
 is
es).
r
ing
y
he
ls, as
hat we
ng lists.
%3CCAOhmDzfL2COdysV8r5hZN8f=NqXM=f=oY5NO2dHWJ_kVEoP+Ng@mail.gmail.com%3E>
%3CCAOhmDzec1JdsXQq3dDwAv7eLnzRidSkrsKKG0xKw=TKTxY_sYw@mail.gmail.com%3E>
iscussion on why
 very toolsâ€¦)
ke
ls
,
me
e
ately
t
d
d
h
at
"
=?gb2312?B?zfTR8w==?= <tiandiwoxin@icloud.com>,"Thu, 10 Nov 2016 01:02:15 +0800","Would ""alter table add column"" be supported in the future?","""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

I notice that ¡°alter table add column¡± command is banned in spark 2.0.

Any plans on supporting it in the future? (After all it was supported in spark 1.6.x)

Thanks.
---------------------------------------------------------------------


"
Cody Koeninger <cody@koeninger.org>,"Wed, 9 Nov 2016 11:25:25 -0600",Re: Connectors using new Kafka consumer API,Mark Grover <mark@apache.org>,"Ok... in general it seems to me like effort would be better spent
trying to help upstream, as opposed to us making a 5th slightly
different interface to kafka (currently have 0.8 receiver, 0.8
dstream, 0.10 dstream, 0.10 structured stream)


---------------------------------------------------------------------


"
=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@databricks.com>,"Wed, 9 Nov 2016 18:55:51 +0100","Re: Would ""alter table add column"" be supported in the future?",=?UTF-8?B?5rGq5rSL?= <tiandiwoxin@icloud.com>,"This currently not on any roadmap I know of. You can open a JIRA ticket for
this if you want to.


in spark 2.0.
"
Denny Lee <denny.g.lee@gmail.com>,"Wed, 09 Nov 2016 18:00:40 +0000",Re: Handling questions in the mailing lists,"Gerard Maas <gerard.maas@gmail.com>, Maciej Szymkiewicz <mszymkiewicz@gmail.com>","Here here! :)  Completely agree with you - here's the latest updates
to Proposed
Community Mailing Lists / StackOverflow Changes
<https://docs.google.com/document/d/1N0pKatcM15cqBPqFWCqIy6jdgNzIoacZlYDCjufBh2s/edit#>.
Keep them coming though at this point, I'd like to limit new verbiage to
prevent it from being too long hence not being read.  Modifications and
suggestions are absolutely welcome - just asking that we don't make it too
much longer.  Thanks!



e
il
e
n
e
a
g
t
.
hings
ing
ntify
ow""
d)
ften
lly
t
l,
m
s
ll
is
s).
g
s
s
r
as far
ists.
OhmDzfL2COdysV8r5hZN8f=NqXM=f=oY5NO2dHWJ_kVEoP+Ng@mail.gmail.com%3E
OhmDzec1JdsXQq3dDwAv7eLnzRidSkrsKKG0xKw=TKTxY_sYw@mail.gmail.com%3E
ssion on why we
oolsâ€¦)
e
@
of
d
n
â€¦
s
's
s,
s.
ck
this was
e
k
he
"
Denny Lee <denny.g.lee@gmail.com>,"Wed, 09 Nov 2016 21:02:25 +0000",Re: [VOTE] Release Apache Spark 2.0.2 (RC3),"vaquar khan <vaquar.khan@gmail.com>, Weiqing Yang <yangweiqing001@gmail.com>","+1 (non binding)




f
n
t
2.
"
Yin Huai <yhuai@databricks.com>,"Wed, 9 Nov 2016 13:14:48 -0800",Re: [VOTE] Release Apache Spark 2.0.2 (RC3),Denny Lee <denny.g.lee@gmail.com>,"+!


:
if
m
.2.
"
Yin Huai <yhuai@databricks.com>,"Wed, 9 Nov 2016 13:14:55 -0800",Re: [VOTE] Release Apache Spark 2.0.2 (RC3),Denny Lee <denny.g.lee@gmail.com>,"+1


r
r
:
n
 if
l
0.2.
"
Ryan Blue <rblue@netflix.com.INVALID>,"Wed, 9 Nov 2016 15:15:58 -0800",Re: [VOTE] Release Apache Spark 2.0.2 (RC3),Yin Huai <yhuai@databricks.com>,"+1


m
nd
:
/
/
g
ll
C
.0.2.


-- 
Ryan Blue
Software Engineer
Netflix
"
Pratik Sharma <pratik9891@gmail.com>,"Wed, 9 Nov 2016 15:38:20 -0800",Re: [VOTE] Release Apache Spark 2.0.2 (RC3),Ryan Blue <rblue@netflix.com.INVALID>,"+1 (non-binding)

:
ote:
.8.0_111"" 
er -Dpyspark -Dsparkr -DskipTests clean package
er -Dpyspark -Dsparkr test
 YARN, Hive
e:
on 2.0.2. The vote is open until Thu, Nov 10, 2016 at 22:00 PDT and passes if a majority of at least 3+1 PMC votes are cast.
694b"
Krishna Kalyan <krishnakalyan3@gmail.com>,"Thu, 10 Nov 2016 02:33:01 +0100",Contributing to Spark in GSoC 2017,dev <dev@spark.apache.org>,"Hello,
I am Krishna, currently a 2nd year Masters student in (MSc. in Data Mining)
currently in Barcelona studying at UniversitÃ© Polytechnique de Catalogne.
I know its a little early for GSoC, however I wanted to get  a head start
working with the spark community.
Is there anyone who would be mentoring GSoC 2017?.
Could anyone please guide on how to go about it?.

Related Experience:
My masters is mostly focussed on data mining and machine learning
techniques. Before my masters, I was a  data engineer with IBM (India). I
was responsible for managing 50 node Hadoop Cluster for more than a year.
Most of my time was spent optimising and writing ETL (Apache Pig) jobs. Our
daily batch job aggregated more than 30gbs of CDR+Weblogs in our cluster.

I am the most comfortable with Python and R. (Not a Scala expert, I am sure
that I can pick it up quickly)

 My CV could be viewed by following the link below.
(https://github.com/krishnakalyan3/Resume/raw/master/Resume.pdf)

My Spark Pull Requests
(
https://github.com/apache/spark/pulls?utf8=%E2%9C%93&q=is%3Apr%20author%3Akrishnakalyan3%20
)

Thank you so much,
Krishna
"
Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Thu, 10 Nov 2016 06:52:43 +0100",Re: Handling questions in the mailing lists,dev@spark.apache.org,"If you take a look at the statistics
(https://data.stackexchange.com/stackoverflow/query/575406) you'll see
that majority of the unanswered questions:

  * have seen no activity in the last year OR
  * don't have positive score OR
  * have been asked by inactive or new users.

This is usually a good indicator that question is poor quality and / or
abandoned and for different reasons hasn't been picked by the removal
process (https://stackoverflow.com/help/roomba). This is not unusual for
Stack Overflow and with a little bit of organized effort could be
cleaned in a few weeks.

Arguably, for a technology with a large number of moving parts, Spark
has pretty decent /answer rate/ and definitely better than many
comparable projects.

Regarding tagging. Putting community rules aside clean questions which
can be answered with relatively low effort are usually resolved in a few
days. What is left is either to time consuming or complex or just not
not worth the time. If you have a lot of time the former ones can be
easily selected using predefined filters and the rest usually qualifies
for closing.

Still, I believe there is a really important missing point here. All of
that requires a lot of effort and it is slightly unrealistic to expect 
that the number of people willing and having time to contribute will
suddenly grow. So the focus should be on having a knowledge base which
can reduce number of questions to be answered. SO has good visibility,
large number of existing answers, and very good tools. 


-- 
Maciej Szymkiewicz

"
Yu Wei <yu2003w@hotmail.com>,"Thu, 10 Nov 2016 11:27:50 +0000","Failed to run spark jobs on  mesos due to ""hadoop"" not found.",dev <dev@spark.apache.org>,"Hi Guys,

I failed to launch spark jobs on mesos. Actually I submitted the job to cluster successfully.

But the job failed to run.

I1110 18:25:11.095507   301 fetcher.cpp:498] Fetcher Info: {""cache_directory"":""\/tmp\/mesos\/fetch\/slaves\/1f8e621b-3cbf-4b86-a1c1-9e2cf77265ee-S7\/root"",""items"":[{""action"":""BYPASS_CACHE"",""uri"":{""extract"":true,""value"":""hdfs:\/\/192.168.111.74:9090\/bigdata\/package\/spark-examples_2.11-2.0.1.jar""}}],""sandbox_directory"":""\/var\/lib\/mesos\/agent\/slaves\/1f8e621b-3cbf-4b86-a1c1-9e2cf77265ee-S7\/frameworks\/1f8e621b-3cbf-4b86-a1c1-9e2cf77265ee-0002\/executors\/driver-20161110182510-0001\/runs\/b561328e-9110-4583-b740-98f9653e7fc2"",""user"":""root""}
I1110 18:25:11.099799   301 fetcher.cpp:409] Fetching URI 'hdfs://192.168.111.74:9090/bigdata/package/spark-examples_2.11-2.0.1.jar'
I1110 18:25:11.099820   301 fetcher.cpp:250] Fetching directly into the sandbox directory
I1110 18:25:11.099862   301 fetcher.cpp:187] Fetching URI 'hdfs://192.168.111.74:9090/bigdata/package/spark-examples_2.11-2.0.1.jar'
E1110 18:25:11.101842   301 shell.hpp:106] Command 'hadoop version 2>&1' failed; this is the output:
sh: hadoop: command not found
Failed to fetch 'hdfs://192.168.111.74:9090/bigdata/package/spark-examples_2.11-2.0.1.jar': Failed to create HDFS client: Failed to execute 'hadoop version 2>&1'; the command was either not found or exited with a non-zero exit status: 127
Failed to synchronize with agent (it's probably exited


Actually I installed hadoop on each agent node.


Any advice?


Thanks,

Jared, (??)
Software developer
Interested in open source software, big data, Linux
"
WangJianfei <wangjianfei15@otcaix.iscas.ac.cn>,"Thu, 10 Nov 2016 06:43:22 -0700 (MST)","If we run sc.textfile(path,xxx) many times, will the elements be
 the same in each partition",dev@spark.apache.org,"Hi Devs:
    If  i run sc.textFile(path,xxx) many times, will the elements be the
same(same element,same order)in each partitions?
My experiment show that it's the same, but which may not cover all the
cases. Thank you!



--

---------------------------------------------------------------------


"
Manish Malhotra <manish.malhotra.work@gmail.com>,"Thu, 10 Nov 2016 08:42:49 -0800",Spark Streaming: question on sticky session across batches ?,"dev@spark.apache.org, user@spark.apache.org","Hello Spark Devs/Users,

Im trying to solve the use case with Spark Streaming 1.6.2 where for every
batch ( say 2 mins) data needs to go to the same reducer node after
grouping by key.
The underlying storage is Cassandra and not HDFS.

This is a map-reduce job, where also trying to use the partitions of the
Cassandra table to batch the data for the same partition.

The requirement of sticky session/partition across batches is because the
operations which we need to do, needs to read data for every key and then
merge this with the current batch aggregate values. So, currently when
there is no stickyness across batches, we have to read for every key, merge
and then write back. and reads are very expensive. So, if we have sticky
session, we can avoid read in every batch and have a cache of till last
batch aggregates across batches.

So, there are few options, can think of:

1. to change the TaskSchedulerImpl, as its using Random to identify the
node for mapper/reducer before starting the batch/phase.
Not sure if there is a custom scheduler way of achieving it?

2. Can custom RDD can help to find the node for the key-->node.
there is a getPreferredLocation() method.
But not sure, whether this will be persistent or can vary for some edge
cases?

Thanks in advance for you help and time !

Regards,
Manish
"
"""Dongjoon Hyun""<dongjoon@apache.org>","Thu, 10 Nov 2016 16:48:49 -0000",Is `randomized aggregation test` testsuite stable?,<dev@spark.apache.org>,"Hi, All.

Recently, I observed frequent failures of `randomized aggregation test` of ObjectHashAggregateSuite in SparkPullRequestBuilder.

SPARK-17982   https://github.com/apache/spark/pull/15546 (Today)
SPARK-18123   https://github.com/apache/spark/pull/15664 (Today)
SPARK-18169   https://github.com/apache/spark/pull/15682 (Today)
SPARK-18292   https://github.com/apache/spark/pull/15789 (4 days ago. It's gone after `retest`)

I'm wondering if anyone meet those failures? Should I file a JIRA issue for this?

Bests,
Dongjoon.

---------------------------------------------------------------------


"
Cheng Lian <lian.cs.zju@gmail.com>,"Thu, 10 Nov 2016 11:21:43 -0800",Re: Is `randomized aggregation test` testsuite stable?,"Dongjoon Hyun <dongjoon@apache.org>, dev@spark.apache.org","Hey Dongjoon,

Thanks for reporting. I'm looking into these OOM errors. Already 
reproduced them locally but haven't figured out the root cause yet. 
Gonna disable them temporarily for now.

Sorry for the inconvenience!

Cheng




---------------------------------------------------------------------


"
"""Dongjoon Hyun""<dongjoon@apache.org>","Thu, 10 Nov 2016 19:30:36 -0000",Re: Is `randomized aggregation test` testsuite stable?,<dev@spark.apache.org>,"Great! Thank you so much, Cheng!

Bests,
Dongjoon.


---------------------------------------------------------------------


"
Mark Grover <mark@apache.org>,"Thu, 10 Nov 2016 12:10:56 -0800",Re: Connectors using new Kafka consumer API,Cody Koeninger <cody@koeninger.org>,"Ok, I understand your point, thanks. Let me see what I can be done there. I
may come back if it doesn't work out there:-)


"
Cheng Lian <lian.cs.zju@gmail.com>,"Thu, 10 Nov 2016 12:30:06 -0800",Re: Is `randomized aggregation test` testsuite stable?,"Dongjoon Hyun <dongjoon@apache.org>, dev@spark.apache.org","JIRA: https://issues.apache.org/jira/browse/SPARK-18403

PR: https://github.com/apache/spark/pull/15845

Will merge it as soon as Jenkins passes.

Cheng



---------------------------------------------------------------------


"
Holden Karau <holden@pigscanfly.ca>,"Thu, 10 Nov 2016 13:53:52 -0800",Re: Handling questions in the mailing lists,"""assaf.mendelson"" <assaf.mendelson@rsa.com>","That's a good question, looking at
http://stackoverflow.com/tags/apache-spark/topusers shows a few
contributors who have already been active on SO including some committers
and  PMC members with very high overall SO reputations for any
administrative needs (as well as a number of other contributors besides
just PMC/committers).


,
g
den
d
jufBh2s/edit#heading=h.xshc1bv4sn3p> has
y
,
tions but that can
 tier to which
™t
e
™t seem
e can try
 email]
:
jufBh2s/edit>
hings
ing
ntify
ow""
/
ed>)
ften
lly
t
l,
s
ll
is
s).
g
s
s
r
as far
ists.
...%3E
mDzfL2COdysV8r5hZN8f=NqXM=f=oY5NO2dHWJ_kVEoP+Ng@mail.gmail.com%3E>
3E
mDzec1JdsXQq3dDwAv7eLnzRidSkrsKKG0xKw=TKTxY_sYw@mail.gmail.com%3E>
ssion on why we
oolsâ€¦)
e
@
of
d
n
â€¦
s
's
s,
s.
ck
this was
e
k
he
ervlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
ions-in-the-mailing-lists-tp19690p19798.html>
ervlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
ions-in-the-mailing-lists-tp19690p19800.html>



-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
Kousuke Saruta <sarutak@oss.nttdata.co.jp>,"Fri, 11 Nov 2016 11:05:36 +0900",Re: [VOTE] Release Apache Spark 2.0.2 (RC3),"Reynold Xin <rxin@databricks.com>, dev <dev@spark.apache.org>","+1 (non-binding)




---------------------------------------------------------------------


"
Tathagata Das <tathagata.das1565@gmail.com>,"Thu, 10 Nov 2016 18:06:40 -0800",Re: [VOTE] Release Apache Spark 2.0.2 (RC3),Kousuke Saruta <sarutak@oss.nttdata.co.jp>,"+1 binding


if
0
<
/
m
.2.
"
Mingjie Tang <tangrock@gmail.com>,"Thu, 10 Nov 2016 19:44:29 -0800",Re: [VOTE] Release Apache Spark 2.0.2 (RC3),Tathagata Das <tathagata.das1565@gmail.com>,"+1 (non-binding)


p
n
 if
<
/
l
0.2.
"
Adam Roberts <AROBERTS@uk.ibm.com>,"Fri, 11 Nov 2016 10:48:58 +0000",Re: [VOTE] Release Apache Spark 2.0.2 (RC3),dev <dev@spark.apache.org>,"+1 (non-binding)

Build: mvn -T 1C -Psparkr -Pyarn -Phadoop-2.7 -Phive -Phive-thriftserver 
-DskipTests clean package
Test: mvn -Pyarn -Phadoop-2.7 -Phive -Phive-thriftserver 
-Dtest.exclude.tags=org.apache.spark.tags.DockerTest -fn test
Test options: -Xs"
WangJianfei <wangjianfei15@otcaix.iscas.ac.cn>,"Fri, 11 Nov 2016 06:13:21 -0700 (MST)","Reduce the memory usage if we do same first in GradientBoostedTrees
 if subsamplingRate< 1.0",dev@spark.apache.org,"when we train the mode, we will use the data with a subSampleRate, so if the
subSampleRate < 1.0 , we can do a sample first to reduce the memory usage.
se the code below in GradientBoostedTrees.boost()

 while (m < numIterations && !doneLearning) {
      // Update data with pseudo-residuals å‰©ä½™è¯¯å·®
      val data = predError.zip(input).map { case ((pred, _), point) =>
        LabeledPoint(-loss.gradient(pred, point.label), point.features)
      }

      timer.start(s""building tree $m"")
      logDebug(""###################################################"")
      logDebug(""Gradient boosting tree iteration "" + m)
      logDebug(""###################################################"")
      val dt = new DecisionTreeRegressor().setSeed(seed + m)
      val model = dt.train(data, treeStrategy)
   




--
3.nabble.com/Reduce-the-memory-usage-if-we-do-same-first-in-GradientBoostedTrees-if-subsamplingRate-1-0-tp19826.html
om.

---------------------------------------------------------------------


"
"""Dongjoon Hyun""<dongjoon@apache.org>","Fri, 11 Nov 2016 18:23:48 -0000",Re: [VOTE] Release Apache Spark 2.0.2 (RC3),<dev@spark.apache.org>,"Hi.

Now, do we have Apache Spark 2.0.2? :)

Bests,
Dongjoon.


---------------------------------------------------------------------


"
robert <zhourobert@yahoo.com>,"Fri, 11 Nov 2016 12:53:50 -0700 (MST)",spark sql query of nested json lists data,dev@spark.apache.org,"I am new to the spark sql development. I have a json file with nested arrays.
I can extract/query these arrays. However, when I add order by clause, I get
exceptions: here is the step:
1)     val a = sparkSession.sql(""SELECT Tables.TableName, Tables.TableType,
Tables.TableExecOrder, Tables.Columns FROM tblConfig LATERAL VIEW
explode(TargetTable.Tables[0]) s AS Tables"")
    a.show(5)
        output: 
+---------+---------+--------------+--------------------+
|TableName|TableType|TableExecOrder|             Columns|
+---------+---------+--------------+--------------------+
|      TB0|    Final|             0|[[name,INT], [nam...|
|      TB1|     temp|             2|[[name,INT], [nam...|
|      TB2|     temp|             1|[[name,INT], [nam...|
+---------+---------+--------------+--------------------+

2)     a.createOrReplaceTempView(""aa"")
    sparkSession.sql(""SELECT TableName, TableExecOrder, Columns FROM aa
order by TableExecOrder"").show(5)

       Output: exception

16/11/11 11:17:00 ERROR TaskResultGetter: Exception while getting task
result
com.esotericsoftware.kryo.KryoException: java.lang.NullPointerException
Serialization trace:
underlying (org.apache.spark.util.BoundedPriorityQueue)
	at
com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:144)
	at
com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790)
	at com.twitter.chill.SomeSerializer.read(SomeSerializer.scala:25)
	at com.twitter.chill.SomeSerializer.read(SomeSerializer.scala:19)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790)
	at
org.apache.spark.serializer.KryoSerializerInstance.deserialize(KryoSerializer.scala:312)
	at org.apache.spark.scheduler.DirectTaskResult.value(TaskResult.scala:87)
	at
org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply$mcV$sp(TaskResultGetter.scala:66)
	at
org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply(TaskResultGetter.scala:57)
	at
org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply(TaskResultGetter.scala:57)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1793)
	at
org.apache.spark.scheduler.TaskResultGetter$$anon$2.run(TaskResultGetter.scala:56)
	at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
	at
org.apache.spark.sql.catalyst.expressions.codegen.LazilyGeneratedOrdering.compare(GenerateOrdering.scala:157)
	at
org.apache.spark.sql.catalyst.expressions.codegen.LazilyGeneratedOrdering.compare(GenerateOrdering.scala:148)
	at scala.math.Ordering$$anon$4.compare(Ordering.scala:111)
	at java.util.PriorityQueue.siftUpUsingComparator(PriorityQueue.java:669)
	at java.util.PriorityQueue.siftUp(PriorityQueue.java:645)
	at java.util.PriorityQueue.offer(PriorityQueue.java:344)
	at java.util.PriorityQueue.add(PriorityQueue.java:321)
	at
com.twitter.chill.java.PriorityQueueSerializer.read(PriorityQueueSerializer.java:78)
	at
com.twitter.chill.java.PriorityQueueSerializer.read(PriorityQueueSerializer.java:31)
	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708)
	at
com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125)
	... 15 more


how can I fix this?



--

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Fri, 11 Nov 2016 11:58:29 -0800",Re: [VOTE] Release Apache Spark 2.0.2 (RC3),"""dev@spark.apache.org"" <dev@spark.apache.org>","The vote has passed with the following +1s and no -1. I will work on
packaging the release.

+1:

Reynold Xin*
Herman van HÃ¶vell tot Westerflier
Ricardo Almeida
Shixiong (Ryan) Zhu
Sean Owen*
Michael Armbrust*
Dongjoon Hyun
Jagadeesan As
Liwei Lin
Weiqing Yang
Vaquar Khan
Denny Lee
Yin Huai*
Ryan Blue
Pratik Sharma
Kousuke Saruta
Tathagata Das*
Mingjie Tang
Adam Roberts

* = binding



f
n
t
2.
"
Jacek Laskowski <jacek@japila.pl>,"Fri, 11 Nov 2016 22:12:33 +0100",withExpr private method duplication in Column and functions objects?,dev <dev@spark.apache.org>,"Hi,

Any reason for withExpr duplication in Column [1] and functions [2]
objects? It looks like it could be less private and be at least
private[sql]?

private def withExpr(newExpr: Expression): Column = new Column(newExpr)

[1] https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/Column.scala#L152
[2] https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/functions.scala#L60

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Fri, 11 Nov 2016 13:37:10 -0800",Re: withExpr private method duplication in Column and functions objects?,Jacek Laskowski <jacek@japila.pl>,"private[sql] has no impact in Java, and these functions are literally one
line of code. It's overkill to think about code duplication for functions
that simple.




"
Jacek Laskowski <jacek@japila.pl>,"Sat, 12 Nov 2016 13:39:55 +0100",ShuffleExchange#nodeName...a duplication...perhaps?!,dev <dev@spark.apache.org>,"Hi,

While reviewing ShuffleExchange physical operator [1] I found the following:

  override def nodeName: String = {
    val extraInfo = coordinator match {
      case Some(exchangeCoordinator) if exchangeCoordinator.isEstimated =>
        s""(coordinator id: ${System.identityHashCode(coordinator)})""
      case Some(exchangeCoordinator) if !exchangeCoordinator.isEstimated =>
        s""(coordinator id: ${System.identityHashCode(coordinator)})""
      case None => """"
    }

It *appears* that the first two cases give the same result so...a
minor duplication perhaps? (Makes reading the code slightly more
involved).

[1] https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExchange.scala#L46-L53

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
WangJianfei <wangjianfei15@otcaix.iscas.ac.cn>,"Sat, 12 Nov 2016 06:57:46 -0700 (MST)","does The Design of spark consider the scala parallelize
 collections?",dev@spark.apache.org,"Hi devs:
   According to scala doc, we can see the scala has parallelize collections,
according to my experient, surely, parallelize collections can accelerate
the operation,such as(map). so i want to know does spark has used the scala
parallelize collections and even will spark consider thant? thank you!



--

---------------------------------------------------------------------


"
Hyukjin Kwon <gurwls223@gmail.com>,"Sun, 13 Nov 2016 02:27:20 +0900",Component naming in the PR title,dev <dev@spark.apache.org>,"Hi all,


First of all, this might be minor but I just have been curious of different
PR titles in particular component part. So, I looked through Spark wiki
again and I found the description not quite the same with the PRs.

It seems, it is said,
Pull Request

...

   1.

      The PR title should be of the form [SPARK-xxxx] [COMPONENT] Title,
      where SPARK-xxxx is the relevant JIRA number, COMPONENT is one of the PR
      categories shown at https://spark-prs.appspot.com/ and Title may be
      the JIRA's title or a more specific title describing the PR itself.


...

So, It seems the component should be one of

SQL, MLlib, Core, Python, Scheduler, Build, Docs, Streaming, Mesos, Web UI,
YARN, GraphX, R.

If the component refers the ones specified in the JIRA, then, it seems it
should be one of

Block Manager, Build, Deploy, Documentation, DStream, EC2, Examples,
GraphX, Input/Output, Java API, Mesos, ML, MLlib, Optimizer, Project Infra,
PySpark, Scheduler, Shuffle, Spark Core, Spark Shell, Spark Submit, SparkR,
SQL, Structured Streaming, Tests, Web UI, Windows, YARN

It seems they are a bit different with the PRs. I hope this is clarified in
more details (including whether it should be all upper-cased or just the
same with the component name maybe).


Thanks.
"
Denny Lee <denny.g.lee@gmail.com>,"Sun, 13 Nov 2016 06:51:28 +0000",Re: Handling questions in the mailing lists,Reynold Xin <rxin@databricks.com>,"Hey Reynold,

Looks like we all of the proposed changes into Proposed Community Mailing
Lists / StackOverflow Changes
<https://docs.google.com/document/d/1N0pKatcM15cqBPqFWCqIy6jdgNzIoacZlYDCjufBh2s/edit#heading=h.xshc1bv4sn3p>.
Anything else we can do to update the Spark Community page / welcome email?


Meanwhile, let's all start answering questions on SO, eh?! :)
Denny


,
g
den
d
jufBh2s/edit#heading=h.xshc1bv4sn3p> has
y
,
tions but that can
 tier to which
ficulty-level
™t
e
™t seem
e can try
 email]
:
ufBh2s/edit#
jufBh2s/edit>
hings
ing
ntify
ow""
d)
ften
lly
t
l,
s
ll
is
s).
g
s
s
r
as far
ists.
DzfL2COdysV8r5hZN8f=NqXM=f=oY5NO2dHWJ_kVEoP+Ng@...%3E
mDzfL2COdysV8r5hZN8f=NqXM=f=oY5NO2dHWJ_kVEoP+Ng@mail.gmail.com%3E>
Dzec1JdsXQq3dDwAv7eLnzRidSkrsKKG0xKw=TKTxY_sYw@...%3E
mDzec1JdsXQq3dDwAv7eLnzRidSkrsKKG0xKw=TKTxY_sYw@mail.gmail.com%3E>
ssion on why we
oolsâ€¦)
e
@
of
d
n
â€¦
s
's
s,
s.
ck
this was
e
k
he
ons-in-the-mailing-lists-tp19690p19770.html
ervlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
ions-in-the-mailing-lists-tp19690p19798.html>
ons-in-the-mailing-lists-tp19690p19799.html
ervlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
ions-in-the-mailing-lists-tp19690p19800.html>
"
"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Sun, 13 Nov 2016 04:03:06 -0700 (MST)",how does isDistinct work on expressions,dev@spark.apache.org,"Hi,
I am trying to understand how aggregate functions are implemented internally.
I see that the expression is wrapped using toAggregateExpression using isDistinct.
I can't figure out where the code that makes the data distinct is located. I am trying to figure out how the input data is converted into a distinct version.
Thanks,
                Assaf.




--"
"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Sun, 13 Nov 2016 04:28:45 -0700 (MST)",Converting spark types and standard scala types,dev@spark.apache.org,"Hi,
I am trying to write a new aggregate function (https://issues.apache.org/jira/browse/SPARK-17691) and I wanted it to support all ordered types.
I have several  issues though:

1.       How to convert the type of the child expression to a Scala standard type (e.g. I need an Array[Int] for IntegerType and an Array[Double] for DoubleType). The only method I found so far is to do a match for each of the types. Is there a better way?

2.       What would be the corresponding scala type for DecimalType, TimestampType, DateType and BinaryType? I also couldn't figure out how to do a case for DecimalType. Do I need to do a specific case for each of its internal types?

3.       Should BinaryType be a legal type for such a function?

4.       I need to serialize the relevant array of type (i.e. turn it into an Array[Byte] for working with TypedImperativeAggregate). Currently, I use java.io.{ByteArrayOutputStream, ByteArrayInputStream, ObjectInputStream, ObjectOutputStream}. Is there another way which is more standard (e.g. get a ""Serialize"" function which knows what to use:  java serialization, kyro serialization etc. based on spark configuration?)
Thanks,
                Assaf.





--"
Jacek Laskowski <jacek@japila.pl>,"Sun, 13 Nov 2016 12:57:58 +0100",,dev <dev@spark.apache.org>,"Hi,

It's just a (minor?) example of how to use catalyst.dsl package [1],
but am currently reviewing deserialize [2] and got a question.

CatalystSerde.deserialize [3] is exactly the deserialize operator
(referred above) and since CatalystSerde.deserialize's used in few
places like Dataset.rdd [4] as follows:

val deserialized = CatalystSerde.deserialize[T](logicalPlan)

I'm wondering why the deserialize dsl operator is not used instead
that would make the line as follows:

val deserialized = deserialize(logicalPlan)

which looks so much nicer to my eyes.

Any reason for using CatalystSerde.deserialize[T](logicalPlan) instead
of this seemingly simpler deserialize operator? Is this because it's
just a matter of find-and-replace and no one had time for this?

Please help me understand this area better. Thanks!

[1] https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/dsl/package.scala
[2] https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/dsl/package.scala#L304
[3] https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/object.scala#L32
[4] https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L2498

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Sun, 13 Nov 2016 20:46:59 +0100",Re: how does isDistinct work on expressions,"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Hi,

I might not have been there yet, but since I'm with the code every day
I might be close...

When you say ""aggregate functions"", are you about typed or untyped
ones? Just today I reviewed the typed ones and honestly took me some
time to figure out what belongs to where. Are you creating a new UDAF?
What have you done already? GitHub perhaps?

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski


 located. I

---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Sun, 13 Nov 2016 20:50:07 +0100",Re: Component naming in the PR title,Hyukjin Kwon <gurwls223@gmail.com>,"Hi Hyukjin,

What's worked for me so the Spark committers have accepted was to use
the first group

SQL, MLlib, Core, Python, Scheduler, Build, Docs, Streaming, Mesos,
Web UI, YARN, GraphX, R

with all the letters uppercase.

It's less to remember so I'd vote for keeping it in use (or be acceptable).

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Sun, 13 Nov 2016 20:01:50 +0000",Re: Component naming in the PR title,"Hyukjin Kwon <gurwls223@gmail.com>, dev <dev@spark.apache.org>","Yes they really correspond to, if anything, the categories at
spark-prs.appspot.com . They aren't that consistently used however and
there isn't really a definite list. It is really mostly of use for the fact
that it tags emails in a way people can filter semi-effectively. So I think
we have left it at an informal mechanism rather than make yet another list
of categories to maintain.


"
=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@databricks.com>,"Sun, 13 Nov 2016 12:02:20 -0800",Re: how does isDistinct work on expressions,Jacek Laskowski <jacek@japila.pl>,"Hi,

You should take a look at
https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteDistinctAggregates.scala

Spark SQL does not directly support the aggregation of multiple distinct
groups. For example select count(distinct a), count(distinct b) from
tbl_x containts
distinct groups a  & b. The RewriteDistinctAggregates rewrites this into an
two aggregates, the first aggregate takes care of deduplication and the
second aggregate does the actual aggregation.

HTH


is
"
Reynold Xin <rxin@databricks.com>,"Sun, 13 Nov 2016 12:30:29 -0800",Re: does The Design of spark consider the scala parallelize collections?,WangJianfei <wangjianfei15@otcaix.iscas.ac.cn>,"Some places in Spark do use it:

 val models = Range(0, numClasses).par.map { index =>
sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/ScalaReflectionSuite.scala:
           (0 until 10).par.foreach { _ =>
sql/core/src/test/scala/org/apache/spark/sql/execution/SQLExecutionSuite.scala:
     (1 to 100).par.foreach { _ =>
sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/SQLQuerySuite.scala:
   (1 to 100).par.map { i =>
streaming/src/main/scala/org/apache/spark/streaming/DStreamGraph.scala:
 inputStreams.par.foreach(_.start())
streaming/src/main/scala/org/apache/spark/streaming/DStreamGraph.scala:
 inputStreams.par.foreach(_.stop())


Most of the usage are in tests, not the actual execution path. Parallel
collection is fairly complicated and difficult to manage (implicit thread
pools). It is good for more the basic thread management, but Spark itself
has much more sophisticated parallelization built-in.



"
Hyukjin Kwon <gurwls223@gmail.com>,"Mon, 14 Nov 2016 07:02:50 +0900",Re: Component naming in the PR title,Sean Owen <sowen@cloudera.com>,"I see. I was just curious as I find myself hesitating when I open a PR time
to time.

Thank you both for echoing!


"
Reynold Xin <rxin@databricks.com>,"Sun, 13 Nov 2016 17:30:00 -0800",statistics collection and propagation for cost-based optimizer,"""dev@spark.apache.org"" <dev@spark.apache.org>","I want to bring this discussion to the dev list to gather broader feedback,
as there have been some discussions that happened over multiple JIRA
tickets (SPARK-16026 <https://issues.apache.org/jira/browse/SPARK-16026>,
etc) and GitHub pull requests about what statistics to collect and how to
use them.

There are some basic statistics on columns that are obvious to use and we
don't need to debate these: estimated size (in bytes), row count, min, max,
number of nulls, number of distinct values, average column length, max
column length.

In addition, we want to be able to estimate selectivity for equality and
range predicates better, especially taking into account skewed values and
outliers.

Before I dive into the different options, let me first explain count-min
sketch: Count-min sketch is a common sketch algorithm that tracks frequency
counts. It has the following nice properties:
- sublinear space
- can be generated in one-pass in a streaming fashion
- can be incrementally maintained (i.e. for appending new data)
- it's already implemented in Spark
- more accurate for frequent values, and less accurate for less-frequent
values, i.e. it tracks skewed values well.
- easy to compute inner product, i.e. trivial to compute the count-min
sketch of a join given two count-min sketches of the join tables


Proposal 1 is is to use a combination of count-min sketch and equi-height
histograms. In this case, count-min sketch will be used for selectivity
estimation on equality predicates, and histogram will be used on range
predicates.

Proposal 2 is to just use count-min sketch on equality predicates, and then
simple selected_range / (max - min) will be used for range predicates. This
will be less accurate than using histogram, but simpler because we don't
need to collect histograms.

Proposal 3 is a variant of proposal 2, and takes into account that skewed
values can impact selectivity heavily. In 3, we track the list of heavy
hitters (HH, most frequent items) along with count-min sketch on the
column. Then:
- use count-min sketch on equality predicates
- for range predicates, estimatedFreq =  sum(freq(HHInRange)) + range /
(max - min)

Proposal 4 is to not use any sketch, and use histogram for high cardinality
columns, and exact (value, frequency) pairs for low cardinality columns
(e.g. num distinct value <= 255).

Proposal 5 is a variant of proposal 4, and adapts it to track exact (value,
frequency) pairs for the most frequent values only, so we can still have
that for high cardinality columns. This is actually very similar to
count-min sketch, but might use less space, although requiring two passes
to compute the initial value, and more difficult to compute the inner
product for joins.
"
Reynold Xin <rxin@databricks.com>,"Sun, 13 Nov 2016 17:53:31 -0800",Re: statistics collection and propagation for cost-based optimizer,"""dev@spark.apache.org"" <dev@spark.apache.org>","eps = 0.1% and confidence 0.87, uncompressed, is 48k bytes.

To look up what that means, see
http://spark.apache.org/docs/latest/api/java/org/apache/spark/util/sketch/CountMinSketch.html






"
"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Mon, 14 Nov 2016 00:46:48 -0700 (MST)",RE: how does isDistinct work on expressions,dev@spark.apache.org,"Thanks for the pointer. It makes more sense now.
Assaf.

From: Herman van HÃ¶vell tot Westerflier-2 [via Apache Spark Developers List] [mailto:ml-node+s1001551n19842h91@n3.nabble.com]
Sent: Sunday, November 13, 2016 10:03 PM
To: Mendelson, Assaf
Subject: Re: how does isDistinct work on expressions

Hi,

You should take a look at https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteDistinctAggregates.scala

Spark SQL does not directly support the aggregation of multiple distinct groups. For example select count(distinct a), count(distinct b) from tbl_x containts distinct groups a  & b. The RewriteDistinctAggregates rewrites this into an two aggregates, the first aggregate takes care of deduplication and the second aggregate does the actual aggregation.

HTH

Hi,

I might not have been there yet, but since I'm with the code every day
I might be close...

When you say ""aggregate functions"", are you about typed or untyped
ones? Just today I reviewed the typed ones and honestly took me some
time to figure out what belongs to where. Are you creating a new UDAF?
What have you done already? GitHub perhaps?

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski


 located. I
---------------------------------------------------------------------
=19842&i=2>


________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/how-does-isDistinct-work-on-expressions-tp19836p19842.html
To start a new topic under Apache Spark Developers List, email ml-node+s1001551n1h20@n3.nabble.com<mailto:ml-node+s1001551n1h20@n3.nabble.com>
spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=1&code=YXNzYWYubWVuZGVsc29uQHJzYS5jb218MXwtMTI4OTkxNTg1Mg==>.
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>




--
3.nabble.com/how-does-isDistinct-work-on-expressions-tp19836p19847.html
om."
Yu Wei <yu2003w@hotmail.com>,"Mon, 14 Nov 2016 10:58:38 +0000",Two questions about running spark on mesos,"""dev@mesos.apache.org"" <dev@mesos.apache.org>, dev <dev@spark.apache.org>","Hi Guys,


Two questions about running spark on mesos.

1, Does spark configuration of conf/slaves still work when running spark on mesos?

    According to my observations, it seemed that conf/slaves still took effect when running spark-shell.

    However, it doesn't take effect when deploying in cluster mode.

    Is this expected behavior?

   Or did I miss anything?


2, Could I kill submitted jobs when running spark on mesos in cluster mode?

    I launched spark on mesos in cluster mode. Then submitted a long running job succeeded.

    Then I want to kill the job.

    How could I do that? Is there any similar commands as launching spark on yarn?




Thanks,

Jared, (??)
Software developer
Interested in open source software, big data, Linux
"
Yu Wei <yu2003w@hotmail.com>,"Mon, 14 Nov 2016 11:45:08 +0000",subscribe,dev <dev@spark.apache.org>,"

Thanks,

Jared, (??)
Software developer
Interested in open source software, big data, Linux
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Mon, 14 Nov 2016 09:32:58 -0800",Re: statistics collection and propagation for cost-based optimizer,Reynold Xin <rxin@databricks.com>,"Do we have any query workloads for which we can benchmark these
proposals in terms of performance ?

Thanks
Shivaram


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Mon, 14 Nov 2016 09:55:04 -0800",Re: statistics collection and propagation for cost-based optimizer,"""shivaram@eecs.berkeley.edu"" <shivaram@eecs.berkeley.edu>","Historically tpcds and tpch. There is certainly a chance of overfitting one
or two benchmarks. Note that those will probably be impacted more by the
way we set the parameters for CBO rather than using x or y for summary
statistics.


"
Michael Gummelt <mgummelt@mesosphere.io>,"Mon, 14 Nov 2016 10:40:37 -0800",Re: Two questions about running spark on mesos,Yu Wei <yu2003w@hotmail.com>,"1. I had never even heard of conf/slaves until this email, and I only see
it referenced in the docs next to Spark Standalone, so I doubt that works.

2. Yes.  See the --kill option in spark-submit.

Also, we're considering dropping the Spark dispatcher in DC/OS in favor of
Metronome, which will be our consolidated method of running any one-off
jobs.  The dispatcher is really just a lesser maintained and more
feature-sparse metronome.  If I were you, I would look into running
Metronome rather than the dispatcher (or just run DC/OS).


e?



-- 
Michael Gummelt
Software Engineer
Mesosphere
"
Joseph Wu <joseph@mesosphere.io>,"Mon, 14 Nov 2016 11:40:21 -0800",Re: Two questions about running spark on mesos,dev <dev@mesos.apache.org>,"1) You should read through this page:
https://spark.apache.org/docs/latest/running-on-mesos.html
I (Mesos person) can't answer any questions that aren't already answered on
that page :)

2) Your normal spark commands (whatever they are) should still work
regardless of the backend.


"
Koert Kuipers <koert@tresata.com>,"Mon, 14 Nov 2016 15:03:12 -0500",Re: getting encoder implicits to be more accurate,Sam Goodwin <sam.goodwin89@gmail.com>,"that makes sense. we have something like that inhouse as well, but not as
nice and not using shapeless (we simply relied on sbt-boilerplate to handle
all tuples and do not support case classes).

however the frustrating part is that spark sql already has this more or
less. look for example at ExpressionEncoder.fromRow and
ExpressionEncoder.toRow. but these methods use InternalRow while the rows
exposed to me as a user are not that.

at this point i am more tempted to simply open up InternalRow at a few
places strategically than to maintain another inhouse row marshalling
class. once i have InternalRows looks of good stuff is available to me to
use.





"
Koert Kuipers <koert@tresata.com>,"Mon, 14 Nov 2016 15:04:34 -0500",Re: getting encoder implicits to be more accurate,Sam Goodwin <sam.goodwin89@gmail.com>,"sorry last line should be:
once i have InternalRows lots of good stuff is available to me to use.


"
Koert Kuipers <koert@tresata.com>,"Mon, 14 Nov 2016 15:10:07 -0500",Re: getting encoder implicits to be more accurate,Sam Goodwin <sam.goodwin89@gmail.com>,"agreed on your point that this can be done without macros


"
Manish Malhotra <manish.malhotra.work@gmail.com>,"Mon, 14 Nov 2016 12:19:30 -0800",Re: Spark Streaming: question on sticky session across batches ?,"dev@spark.apache.org, user@spark.apache.org","sending again.
any help is appreciated !

thanks in advance.


"
Koert Kuipers <koert@tresata.com>,"Mon, 14 Nov 2016 15:31:24 -0500",Re: getting encoder implicits to be more accurate,Sam Goodwin <sam.goodwin89@gmail.com>,"just taking it for a quick spin it looks great, with correct support for
nested rows and using option for nullability.

scala> val format = implicitly[RowFormat[(String, Seq[(String,
Option[Int])])]]
format: com.github.upio.spark.sql.RowFormat[(String, Seq[(String,
Option[Int])])] = com.github.upio.spark.sql.FamilyFormats$$anon$3@2c0961e2

scala> format.schema
res12: org.apache.spark.sql.types.StructType =
StructType(StructField(_1,StringType,false),
StructField(_2,ArrayType(StructType(StructField(_1,StringType,false),
StructField(_2,IntegerType,true)),true),false))

scala> val x = format.read(Row(""a"", Seq(Row(""a"", 5))))
x: (String, Seq[(String, Option[Int])]) = (a,List((a,Some(5))))

scala> format.write(x)
res13: org.apache.spark.sql.Row = [a,List([a,5])]




"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 14 Nov 2016 20:49:42 +0000",Re: [VOTE] Release Apache Spark 2.0.2 (RC3),"Reynold Xin <rxin@databricks.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Has the release already been made? I didn't see any announcement, but
Homebrew has already updated to 2.0.2.
2016ë…„ 11ì›” 11ì¼ (ê¸ˆ) ì˜¤í›„ 2:59, Reynold Xin <rxin@databricks.com>ë‹˜ì´ ìž‘ì„±:

f
n
t
2.
"
Koert Kuipers <koert@tresata.com>,"Mon, 14 Nov 2016 15:52:14 -0500",Re: getting encoder implicits to be more accurate,Sam Goodwin <sam.goodwin89@gmail.com>,"sorry this message by me was confusing. i was frustrated about how hard it
is to use the Encoder machinery myself directly on Row objects, this is
unrelated to the question if a shapeless based approach like sam suggest
would be better way to do encoders in general


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Mon, 14 Nov 2016 13:14:47 -0800",Re: [VOTE] Release Apache Spark 2.0.2 (RC3),Nicholas Chammas <nicholas.chammas@gmail.com>,"The release is available on http://www.apache.org/dist/spark/ and its
on Maven central
http://repo1.maven.org/maven2/org/apache/spark/spark-core_2.11/2.0.2/

I guess Reynold hasn't yet put together the release notes / updates to
the website.

Thanks
Shivaram

, Reynold Xin <rxin@databricks.com>ë‹˜ì´ ìž‘ì„±:
:
n
 if a
l not
om
0.2.

---------------------------------------------------------------------


"
Sam Goodwin <sam.goodwin89@gmail.com>,"Mon, 14 Nov 2016 21:32:20 +0000",Re: getting encoder implicits to be more accurate,Koert Kuipers <koert@tresata.com>,"I wouldn't recommend using a Tuple as you end up with column names like
""_1"" and ""_2"", but it will still work :)

ExpressionEncoder can do the same thing but it doesn't support custom
types, and as far as I can tell, does not support custom implementations.
I.e. is it possible for me to write my own Encoder logic and completely
bypass ExpressionEncoder? The trait definition has no useful methods so it
doesn't seem straight-forward. If the Encoder trait was opened up so people
could provide their own implementations then I don't see this as an issue
anymore. It would allow for external Encoder libraries like mine while not
neglecting Java (non-scala) developers. Is there ""magic"" happening behind
the scenes stopping us from doing this?


"
Josh Rosen <joshrosen@databricks.com>,"Mon, 14 Nov 2016 21:53:44 +0000",Re: [VOTE] Release Apache Spark 2.0.2 (RC3),"shivaram@eecs.berkeley.edu, Nicholas Chammas <nicholas.chammas@gmail.com>","He pushed the 2.0.2 release docs but there's a problem with Git mirroring
of the Spark website repo which is interfering with the publishing:
https://issues.apache.org/jira/browse/INFRA-12913



59, Reynold Xin <rxin@databricks.com>ë‹˜ì´ ìž‘ì„±:
t:
/
ng
n
RC
"
Michael Armbrust <michael@databricks.com>,"Mon, 14 Nov 2016 14:00:46 -0800",Re: getting encoder implicits to be more accurate,Sam Goodwin <sam.goodwin89@gmail.com>,"I would definitly like to open up APIs for people to write their own
encoders.  The challenge thus far has been that Encoders use internal APIs
that have not been stable for translating the data into the tungsten
format.  We also make use of the analyzer to figure out the mapping from
columns to fields (also not a stable API)  This is the only ""magic"" that is
happening.

If someone wants to propose a stable / fast API here it would be great to
start the discussion.  Its an often requested feature.


"
Sean Owen <sowen@cloudera.com>,"Mon, 14 Nov 2016 23:08:11 +0000",Re: [VOTE] Release Apache Spark 2.0.2 (RC3),"Nicholas Chammas <nicholas.chammas@gmail.com>, Reynold Xin <rxin@databricks.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","Yes, it's on Maven. We have some problem syncing the web site changes at
the moment though those are committed too. I think that's the only piece
before a formal announcement.


m>

, Reynold Xin <rxin@databricks.com>ë‹˜ì´ ìž‘ì„±:
f
n
t
2.
"
leo9r <lezcano.leo@gmail.com>,"Mon, 14 Nov 2016 17:19:50 -0700 (MST)","Re: Spark-SQL parameters like shuffle.partitions should be stored
 in the lineage",dev@spark.apache.org,"Hi Daniel,

I completely agree with your request. As the amount of data being processed
with SparkSQL grows, tweaking sql.shuffle.partitions becomes a common need
to prevent OOM and performance degradation. The fact that
sql.shuffle.partitions cannot be set several times in the same job/action,
because of the reason you explain, is a big inconvenient for the development
of ETL pipelines.

Have you got any answer or feedback in this regard?

Thanks,
Leo Lezcano



--

---------------------------------------------------------------------


"
Mark Hamstra <mark@clearstorydata.com>,"Mon, 14 Nov 2016 19:27:03 -0800","Re: Spark-SQL parameters like shuffle.partitions should be stored in
 the lineage",leo9r <lezcano.leo@gmail.com>,"Take a look at spark.sql.adaptive.enabled and the ExchangeCoordinator.  A
single, fixed-sized sql.shuffle.partitions is not the only way to control
the number of partitions in an Exchange -- if you are willing to deal with
code that is still off by by default.


"
Reynold Xin <rxin@databricks.com>,"Mon, 14 Nov 2016 21:12:24 -0800",Re: [VOTE] Release Apache Spark 2.0.2 (RC3),Sean Owen <sowen@cloudera.com>,"The issue is now resolved.



9, Reynold Xin <rxin@databricks.com>ë‹˜ì´ ìž‘ì„±:
:
if
m
.2.
"
Reynold Xin <rxin@databricks.com>,"Mon, 14 Nov 2016 21:14:24 -0800",[ANNOUNCE] Apache Spark 2.0.2,"""dev@spark.apache.org"" <dev@spark.apache.org>, user <user@spark.apache.org>","We are happy to announce the availability of Spark 2.0.2!

Apache Spark 2.0.2 is a maintenance release containing 90 bug fixes along
with Kafka 0.10 support and runtime metrics for Structured Streaming. This
release is based on the branch-2.0 maintenance branch of Spark. We strongly
recommend all 2.0.x users to upgrade to this stable release.

To download Apache Spark 2.0.12 visit http://spark.apache.org/downloads.html

We would like to acknowledge all community members for contributing patches
to this release.
"
Yogesh Mahajan <ymahajan@snappydata.io>,"Tue, 15 Nov 2016 12:03:18 +0530",Re: statistics collection and propagation for cost-based optimizer,Reynold Xin <rxin@databricks.com>,"Thanks Reynold for the detailed proposals. A few questions/clarifications -

1) How the existing rule based operator co-exist with CBO? The existing
rules are heuristics/empirical based, i am assuming rules like predicate
pushdown or project pruning will co-exist with CBO and we just want to
accurately estimate the filter factor and cardinality to make it more
accurate? With predicate pushdown, a filter is mostly executed at an early
stage of a query plan and the cardinality estimate of a predicate can
improve the precision of cardinality estimates.

2. Will the query transformations be now based on the cost calculation? If
yes, then what happens when the cost of execution of the transformed
statement is higher than the cost of untransformed query?

3. Is there any upper limit on space used for storing the frequency
histogram? 255? And in case of more distinct values, we can even consider
height balanced histogram in Oracle.

4. The first three proposals are new and not mentioned in the CBO design
spec. CMS is good but it's less accurate compared the traditional
histograms. This is a  major trade-off  we need to consider.

5. Are we going to consider system statistics- such as speed of CPU or disk
access as a cost function? How about considering shuffle cost, output
partitioning etc?

6. Like the current rule based optimizer, will this CBO also be an
'extensible optimizer'? If yes, what functionality users can extend?

7. Why this CBO will be disabled by default? â€œspark.sql.cbo"" is false by
default as it's just experimental ?

8. ANALYZE TABLE, analyzeColumns etc ... all look good.

9. From the release point of view, how this is planned ? Will all this be
implemented in one go or in phases?

Thanks,
Yogesh Mahajan
http://www.snappydata.io/blog <http://snappydata.io>


y
:
d
,
x
n
ty
e
d
we
vy
ge /
"
Yogesh Mahajan <ymahajan@snappydata.io>,"Tue, 15 Nov 2016 12:16:25 +0530",Re: statistics collection and propagation for cost-based optimizer,Reynold Xin <rxin@databricks.com>,"It looks like Huawei team have run TPC-H benchmark and some real-world test
cases and their results show good performance gain in 2X-5X speedup
depending on data volume.
Can we share the numbers and query wise rational behind the gain? Are there
anything done on spark master yet? Or the implementation is not yet
completed?

Thanks,
Yogesh Mahajan
http://www.snappydata.io/blog <http://snappydata.io>


y
f
t
alse by
:
ry
ax
s
in
ge
nd
nge
n
r
"
Reynold Xin <rxin@databricks.com>,"Mon, 14 Nov 2016 22:47:45 -0800",Re: statistics collection and propagation for cost-based optimizer,Yogesh Mahajan <ymahajan@snappydata.io>,"They are not yet complete. The benchmark was done with an implementation of
cost-based optimizer Huawei had internally for Spark 1.5 (or some even
older version).


s
ly
r
ut
false by
e
y
ary
h
y
e
e
"
"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Tue, 15 Nov 2016 00:09:12 -0700 (MST)",RE: [ANNOUNCE] Apache Spark 2.0.2,dev@spark.apache.org,"While you can download spark 2.0.2, the description is still spark 2.0.1:
Our latest stable version is Apache Spark 2.0.1, released on Oct 3, 2016 (release notes)<http://spark.apache.org/releases/spark-release-2-0-1.html> (git tag)<https://github.com/apache/spark/releases/tag/v2.0.1>


From: rxin [via Apache Spark Developers List] [mailto:ml-node+s1001551n19870h59@n3.nabble.com]
Sent: Tuesday, November 15, 2016 7:15 AM
To: Mendelson, Assaf
Subject: [ANNOUNCE] Apache Spark 2.0.2

We are happy to announce the availability of Spark 2.0.2!

Apache Spark 2.0.2 is a maintenance release containing 90 bug fixes along with Kafka 0.10 support and runtime metrics for Structured Streaming. This release is based on the branch-2.0 maintenance branch of Spark. We strongly recommend all 2.0.x users to upgrade to this stable release.

To download Apache Spark 2.0.12 visit http://spark.apache.org/downloads.html

We would like to acknowledge all community members for contributing patches to this release.



________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/ANNOUNCE-Apache-Spark-2-0-2-tp19870.html
To start a new topic under Apache Spark Developers List, email ml-node+s1001551n1h20@n3.nabble.com<mailto:ml-node+s1001551n1h20@n3.nabble.com>
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>




--"
Reynold Xin <rxin@databricks.com>,"Mon, 14 Nov 2016 23:11:13 -0800",Re: [ANNOUNCE] Apache Spark 2.0.2,"""assaf.mendelson"" <assaf.mendelson@rsa.com>","It's on there on the page (both the release notes and the download version
dropdown).

The one line text is outdated. I'm just going to delete that text as a
matter of fact so we don't run into this issue in the future.



(release
s
ly
-
ervlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
ache-Spark-2-0-2-tp19874.html>
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Mon, 14 Nov 2016 23:13:39 -0800",Re: [ANNOUNCE] Apache Spark 2.0.2,Reynold Xin <rxin@databricks.com>,"FWIW 2.0.1 is also used in the 'Link With Spark' and 'Spark Source
Code Management' sections in that page.

Shivaram


---------------------------------------------------------------------


"
"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Tue, 15 Nov 2016 00:14:13 -0700 (MST)",RE: statistics collection and propagation for cost-based optimizer,dev@spark.apache.org,"I am not sure I understand when the statistics would be calculated. Would they always be calculated or just when analyze is called?
Would it be possible to save analysis results as part of dataframe saving (e.g. when writing it to parquet) or do we have to have a consistent hive installation?
Would it be possible to provide the hints manually? For example for streaming if I know the data in the beginning is not a representative of the entire stream?

From: rxin [via Apache Spark Developers List] [mailto:ml-node+s1001551n19873h52@n3.nabble.com]
Sent: Tuesday, November 15, 2016 8:48 AM
To: Mendelson, Assaf
Subject: Re: statistics collection and propagation for cost-based optimizer

They are not yet complete. The benchmark was done with an implementation of cost-based optimizer Huawei had internally for Spark 1.5 (or some even older version).

It looks like Huawei team have run TPC-H benchmark and some real-world test cases and their results show good performance gain in 2X-5X speedup depending on data volume.
Can we share the numbers and query wise rational behind the gain? Are there anything done on spark master yet? Or the implementation is not yet completed?

Thanks,
Yogesh Mahajan
http://www.snappydata.io/blog<http://snappydata.io>


Thanks Reynold for the detailed proposals. A few questions/clarifications -

1) How the existing rule based operator co-exist with CBO? The existing rules are heuristics/empirical based, i am assuming rules like predicate pushdown or project pruning will co-exist with CBO and we just want to accurately estimate the filter factor and cardinality to make it more accurate? With predicate pushdown, a filter is mostly executed at an early stage of a query plan and the cardinality estimate of a predicate can improve the precision of cardinality estimates.

2. Will the query transformations be now based on the cost calculation? If yes, then what happens when the cost of execution of the transformed statement is higher than the cost of untransformed query?

3. Is there any upper limit on space used for storing the frequency histogram? 255? And in case of more distinct values, we can even consider height balanced histogram in Oracle.

4. The first three proposals are new and not mentioned in the CBO design spec. CMS is good but it's less accurate compared the traditional histograms. This is a  major trade-off  we need to consider.

5. Are we going to consider system statistics- such as speed of CPU or disk access as a cost function? How about considering shuffle cost, output partitioning etc?

6. Like the current rule based optimizer, will this CBO also be an 'extensible optimizer'? If yes, what functionality users can extend?

7. Why this CBO will be disabled by default? â€œspark.sql.cbo"" is false by default as it's just experimental ?

8. ANALYZE TABLE, analyzeColumns etc ... all look good.

9. From the release point of view, how this is planned ? Will all this be implemented in one go or in phases?

Thanks,
Yogesh Mahajan
http://www.snappydata.io/blog<http://snappydata.io>

Historically tpcds and tpch. There is certainly a chance of overfitting one or two benchmarks. Note that those will probably be impacted more by the way we set the parameters for CBO rather than using x or y for summary statistics.


Do we have any query workloads for which we can benchmark these
proposals in terms of performance ?

Thanks
Shivaram

h
/CountMinSketch.html
e
e
ax,
d
ncy
t
es.
d
umn.
/
ity
ill
s to
ct




________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/statistics-collection-and-propagation-for-cost-based-optimizer-tp19845p19873.html
To start a new topic under Apache Spark Developers List, email ml-node+s1001551n1h20@n3.nabble.com
spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=1&code=YXNzYWYubWVuZGVsc29uQHJzYS5jb218MXwtMTI4OTkxNTg1Mg==>.
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>




--
3.nabble.com/statistics-collection-and-propagation-for-cost-based-optimizer-tp19845p19876.html
om."
Reynold Xin <rxin@databricks.com>,"Mon, 14 Nov 2016 23:15:25 -0800",Re: [ANNOUNCE] Apache Spark 2.0.2,Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Good catch. Updated!



"
"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Tue, 15 Nov 2016 00:23:39 -0700 (MST)",separate spark and hive,dev@spark.apache.org,"Hi,
Today, we basically force people to use hive if they want to get the full use of spark SQL.
When doing the default installation this means that a derby.log and metastore_db directory are created where we run from.
The problem with this is that if we run multiple scripts from the same working directory we have a problem.
The solution we employ locally is to always run from different directory as we ignore hive in practice (this of course means we lose the ability to use some of the catalog options in spark session).
The only other solution is to create a full blown hive installation with proper configuration (probably for a JDBC solution).

I would propose that in most cases there shouldn't be any hive use at all. Even for catalog elements such as saving a permanent table, we should be able to configure a target directory and simply write to it (doing everything file based to avoid the need for locking). Hive should be reserved for those who actually use it (probably for backward compatibility).

Am I missing something here?
Assaf.




--"
Reynold Xin <rxin@databricks.com>,"Mon, 14 Nov 2016 23:30:36 -0800",Re: separate spark and hive,"""assaf.mendelson"" <assaf.mendelson@rsa.com>","I agree with the high level idea, and thus SPARK-15691
<https://issues.apache.org/jira/browse/SPARK-15691>.

In reality, it's a huge amount of work to create & maintain a custom
catalog. It might actually make sense to do, but it just seems a lot of
work to do right now and it'd take a toll on interoperability.

If you don't need persistent catalog, you can just run Spark without Hive
mode, can't you?





o
e at all.
-and-hive-tp19879.html>
"
"""Mendelson, Assaf"" <Assaf.Mendelson@rsa.com>","Tue, 15 Nov 2016 07:44:30 +0000",RE: separate spark and hive,Reynold Xin <rxin@databricks.com>,"The default generation of spark context is actually a hive context.
I tried to find on the documentation what are the differences between hive context and sql context and couldnâ€™t find it for spark 2.0 (I know for previous versions there were a couple of functions which required hive context as well as window functions but those seem to have all been fixed for spark 2.0).
Furthermore, I canâ€™t seem to find a way to configure spark not to use hive. I can only find how to compile it without hive (and having to build from source each time is not a good idea for a production system).

I would suggest that working without hive should be either a simple configuration or even the default and that if there is any missing functionality it should be documented.
Assaf.


From: Reynold Xi5, 2016 9:31 AM
To: Mendelson, Assaf
Cc: dev@spark.apache.org
Subject: Re: separate spark and hive

I agree with the high level idea, and thus SPARK-15691<https://issues.apache.org/jira/browse/SPARK-15691>.

In reality, it's a huge amount of work to create & maintain a custom catalog. It might actually make sense to do, but it just seems a lot of work to do right now and it'd take a toll on interoperability.

If you don't need persistent catalog, you can just run Spark without Hive mode, can't you?




On Mon, Nov 14, 2016 at 11:23 PM, assaf.mendelson <assaf.mendelson@rsa.com<mailto:assaf.mendelson@rsa.com>> wrote:
Hi,
Today, we basically force people to use hive if they want to get the full use of spark SQL.
When doing the default installation this means that a derby.log and metastore_db directory are created where we run from.
The problem with this is that if we run multiple scripts from the same working directory we have a problem.
The solution we employ locally is to always run from different directory as we ignore hive in practice (this of course means we lose the ability to use some of the catalog options in spark session).
The only other solution is to create a full blown hive installation with proper configuration (probably for a JDBC solution).

I would propose that in most cases there shouldnâ€™t be any hive use at all. Even for catalog elements such as saving a permanent table, we should be able to configure a target directory and simply write to it (doing everything file based to avoid the need for locking). Hive should be reserved for those who actually use it (probably for backward compatibility).

Am I missing something here?
Assaf.

________________________________
View this message in context: separate spark and hive<http://apache-spark-developers-list.1001551.n3.nabble.com/separate-spark-and-hive-tp19879.html>
Sent from the Apache Spark Developers List mailing list archive<http://apache-spark-developers-list.1001551.n3.nabble.com/> at Nabble.com.

"
Reynold Xin <rxin@databricks.com>,"Mon, 14 Nov 2016 23:45:35 -0800",Re: separate spark and hive,"""Mendelson, Assaf"" <Assaf.Mendelson@rsa.com>","If you just start a SparkSession without calling enableHiveSupport it
actually won't use the Hive catalog support.



e
w for
 use
m>
o
e at all.
-and-hive-tp19879.html>
"
"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Tue, 15 Nov 2016 01:11:47 -0700 (MST)",RE: separate spark and hive,dev@spark.apache.org,"Spark shell (and pyspark) by default create the spark session with hive support (also true when the session is created using getOrCreate, at least in pyspark)
At a minimum there should be a way to configure it using spark-defaults.conf
Assaf.

From: rxin [via Apache Spark Developers List] [mailto:ml-node+s1001551n19882h31@n3.nabble.com]
Sent: Tuesday, November 15, 2016 9:46 AM
To: Mendelson, Assaf
Subject: Re: separate spark and hive

If you just start a SparkSession without calling enableHiveSupport it actually won't use the Hive catalog support.


The default generation of spark context is actually a hive context.
I tried to find on the documentation what are the differences between hive context and sql context and couldnâ€™t find it for spark 2.0 (I know for previous versions there were a couple of functions which required hive context as well as window functions but those seem to have all been fixed for spark 2.0).
Furthermore, I canâ€™t seem to find a way to configure spark not to use hive. I can only find how to compile it without hive (and having to build from source each time is not a good idea for a production system).

I would suggest that working without hive should be either a simple configuration or even the default and that if there is any missing functionality it should be documented.
Assaf.


From: Reynold Xin [mailto:[hidden email]</user/SendEmail.jtp?type=node&node=19882&i=1>]
Sent: Tuesday, November 15, 2016 9:31 AM
To: Mendelson, Assaf
Cc: [hidden email]</user/SendEmail.jtp?type=node&node=19882&i=2>
Subject: Re: separate spark and hive

I agree with the high level idea, and thus SPARK-15691<https://issues.apache.org/jira/browse/SPARK-15691>.

In reality, it's a huge amount of work to create & maintain a custom catalog. It might actually make sense to do, but it just seems a lot of work to do right now and it'd take a toll on interoperability.

If you don't need persistent catalog, you can just run Spark without Hive mode, can't you?




Hi,
Today, we basically force people to use hive if they want to get the full use of spark SQL.
When doing the default installation this means that a derby.log and metastore_db directory are created where we run from.
The problem with this is that if we run multiple scripts from the same working directory we have a problem.
The solution we employ locally is to always run from different directory as we ignore hive in practice (this of course means we lose the ability to use some of the catalog options in spark session).
The only other solution is to create a full blown hive installation with proper configuration (probably for a JDBC solution).

I would propose that in most cases there shouldnâ€™t be any hive use at all. Even for catalog elements such as saving a permanent table, we should be able to configure a target directory and simply write to it (doing everything file based to avoid the need for locking). Hive should be reserved for those who actually use it (probably for backward compatibility).

Am I missing something here?
Assaf.

________________________________
View this message in context: separate spark and hive<http://apache-spark-developers-list.1001551.n3.nabble.com/separate-spark-and-hive-tp19879.html>
he-spark-developers-list.1001551.n3.nabble.com/> at Nabble.com.



________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/separate-spark-and-hive-tp19879p19882.html
To start a new topic under Apache Spark Developers List, email ml-node+s1001551n1h20@n3.nabble.com<mailto:ml-node+s1001551n1h20@n3.nabble.com>
spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=1&code=YXNzYWYubWVuZGVsc29uQHJzYS5jb218MXwtMTI4OTkxNTg1Mg==>.
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>




--
3.nabble.com/separate-spark-and-hive-tp19879p19883.html
om."
"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Tue, 15 Nov 2016 01:38:03 -0700 (MST)",RE: separate spark and hive,dev@spark.apache.org,"After looking at the code, I found that spark.sql.catalogImplementation is set to â€œhiveâ€. I would proposed that it should be set to â€œin-memoryâ€ by default (or at least have this in the documentation, the configuration documentation at http://spark.apache.org/docs/latest/configuration.html has no mentioning of hive at all)
Assaf.

From: Mendelson, Assaf
Sent: Tuesday, November 15, 2016 10:11 AM
To: 'rxin [via Apache Spark Developers List]'
Subject: RE: separate spark and hive

Spark shell (and pyspark) by default create the spark session with hive support (also true when the session is created using getOrCreate, at least in pyspark)
At a minimum there should be a way to configure it using spark-defaults.conf
Assaf.

From: rxin [via Apache Spark Developers List] [mailto:ml-node+s1001551n19882h31@n3.nabble.com]
Sent: Tuesday, November 15, 2016 9:46 AM
To: Mendelson, Assaf
Subject: Re: separate spark and hive

If you just start a SparkSession without calling enableHiveSupport it actually won't use the Hive catalog support.


The default generation of spark context is actually a hive context.
I tried to find on the documentation what are the differences between hive context and sql context and couldnâ€™t find it for spark 2.0 (I know for previous versions there were a couple of functions which required hive context as well as window functions but those seem to have all been fixed for spark 2.0).
Furthermore, I canâ€™t seem to find a way to configure spark not to use hive. I can only find how to compile it without hive (and having to build from source each time is not a good idea for a production system).

I would suggest that working without hive should be either a simple configuration or even the default and that if there is any missing functionality it should be documented.
Assaf.


From: Reynold Xin [mailto:[hidden email]</user/SendEmail.jtp?type=node&node=19882&i=1>]
Sent: Tuesday, November 15, 2016 9:31 AM
To: Mendelson, Assaf
Cc: [hidden email]</user/SendEmail.jtp?type=node&node=19882&i=2>
Subject: Re: separate spark and hive

I agree with the high level idea, and thus SPARK-15691<https://issues.apache.org/jira/browse/SPARK-15691>.

In reality, it's a huge amount of work to create & maintain a custom catalog. It might actually make sense to do, but it just seems a lot of work to do right now and it'd take a toll on interoperability.

If you don't need persistent catalog, you can just run Spark without Hive mode, can't you?




Hi,
Today, we basically force people to use hive if they want to get the full use of spark SQL.
When doing the default installation this means that a derby.log and metastore_db directory are created where we run from.
The problem with this is that if we run multiple scripts from the same working directory we have a problem.
The solution we employ locally is to always run from different directory as we ignore hive in practice (this of course means we lose the ability to use some of the catalog options in spark session).
The only other solution is to create a full blown hive installation with proper configuration (probably for a JDBC solution).

I would propose that in most cases there shouldnâ€™t be any hive use at all. Even for catalog elements such as saving a permanent table, we should be able to configure a target directory and simply write to it (doing everything file based to avoid the need for locking). Hive should be reserved for those who actually use it (probably for backward compatibility).

Am I missing something here?
Assaf.

________________________________
View this message in context: separate spark and hive<http://apache-spark-developers-list.1001551.n3.nabble.com/separate-spark-and-hive-tp19879.html>
he-spark-developers-list.1001551.n3.nabble.com/> at Nabble.com.



________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/separate-spark-and-hive-tp19879p19882.html
To start a new topic under Apache Spark Developers List, email ml-node+s1001551n1h20@n3.nabble.com<mailto:ml-node+s1001551n1h20@n3.nabble.com>
spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=1&code=YXNzYWYubWVuZGVsc29uQHJzYS5jb218MXwtMTI4OTkxNTg1Mg==>.
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>




--
3.nabble.com/separate-spark-and-hive-tp19879p19884.html
om."
leo9r <lezcano.leo@gmail.com>,"Tue, 15 Nov 2016 01:50:20 -0700 (MST)","Re: Spark-SQL parameters like shuffle.partitions should be stored
 in the lineage",dev@spark.apache.org,"That's great insight Mark, I'm looking forward to give it a try!!

According to jira's  Adaptive execution in Spark
<https://issues.apache.org/jira/browse/SPARK-9850>  , it seems that some
functionality was added in Spark 1.6.0 and the rest is still in progress.
Are there any improvements to the SparkSQL adaptive behavior in Spark 2.0+
that you know?

Thanks and best regards,
Leo



--

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Tue, 15 Nov 2016 10:46:55 +0000","Re: Spark-SQL parameters like shuffle.partitions should be stored in
 the lineage","leo9r <lezcano.leo@gmail.com>, dev@spark.apache.org","consider using the programmatic API in part, to let you control individual
jobs?


"
=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@databricks.com>,"Tue, 15 Nov 2016 11:49:00 +0100",Re: separate spark and hive,"""assaf.mendelson"" <assaf.mendelson@rsa.com>","You can start a spark without hive support by setting the spark.sql.
catalogImplementation configuration to in-memory, for example:


I would not change the default from Hive to Spark-only just yet.


to â€œin-memoryâ€ by
t
e
w for
 use
&i=2>
o
e at all.
-and-hive-tp19879.html>
ervlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
-and-hive-tp19879p19884.html>
"
Anton Okolnychyi <anton.okolnychyi@gmail.com>,"Tue, 15 Nov 2016 12:26:59 +0100",Fwd:,"dev@spark.apache.org, user <user@spark.apache.org>","Hi,

I have experienced a problem using the Datasets API in Spark 1.6, while
almost identical code works fine in Spark 2.0.
The problem is related to encoders and custom aggregators.

*Spark 1.6 (the aggregation produces an empty map):*

  implicit val intStringMapEncoder: Encoder[Map[Int, String]] =
ExpressionEncoder()
  // implicit val intStringMapEncoder: Encoder[Map[Int, String]] =
org.apache.spark.sql.Encoders.kryo[Map[Int, String]]

  val sparkConf = new SparkConf()
    .setAppName(""IVU DS Spark 1.6 Test"")
    .setMaster(""local[4]"")
  val sparkContext = new SparkContext(sparkConf)
  val sparkSqlContext = new SQLContext(sparkContext)

  import sparkSqlContext.implicits._

  val stopPointDS = Seq(TestStopPoint(""33"", 1, ""id#1""), TestStopPoint(""33"",
2, ""id#2"")).toDS()

  val stopPointSequenceMap = new Aggregator[TestStopPoint, Map[Int,
String], Map[Int, String]] {
    override def zero = Map[Int, String]()
    override def reduce(map: Map[Int, String], stopPoint: TestStopPoint) = {
      map.updated(stopPoint.sequenceNumber, stopPoint.id)
    }
    override def merge(map: Map[Int, String], anotherMap: Map[Int, String])
= {
      map ++ anotherMap
    }
    override def finish(reduction: Map[Int, String]) = reduction
  }.toColumn

  val resultMap = stopPointDS
    .groupBy(_.line)
    .agg(stopPointSequenceMap)
    .collect()
    .toMap

In spark.sql.execution.TypedAggregateExpression.scala, I see that the
reduce is done correctly, but Spark cannot read the reduced values in the
merge phase.
If I replace the ExperessionEncoder with Kryo-based one (commented in the
presented code), then it works fine.

*Spark 2.0 (works correctly):*

  val spark = SparkSession
    .builder()
    .appName(""IVU DS Spark 2.0 Test"")
    .config(""spark.sql.warehouse.dir"", ""file:///D://sparkSql"")
    .master(""local[4]"")
    .getOrCreate()

  import spark.implicits._

  val stopPointDS = Seq(TestStopPoint(""33"", 1, ""id#1""), TestStopPoint(""33"",
2, ""id#2"")).toDS()

  val stopPointSequenceMap = new Aggregator[TestStopPoint, Map[Int,
String], Map[Int, String]] {
    override def zero = Map[Int, String]()
    override def reduce(map: Map[Int, String], stopPoint: TestStopPoint) = {
      map.updated(stopPoint.sequenceNumber, stopPoint.id)
    }
    override def merge(map: Map[Int, String], anotherMap: Map[Int, String])
= {
      map ++ anotherMap
    }
    override def finish(reduction: Map[Int, String]) = reduction
    override def bufferEncoder: Encoder[Map[Int, String]] =
ExpressionEncoder()
    override def outputEncoder: Encoder[Map[Int, String]] =
ExpressionEncoder()
  }.toColumn

  val resultMap = stopPointDS
    .groupByKey(_.line)
    .agg(stopPointSequenceMap)
    .collect()
    .toMap
I know that Spark 1.6 has only a preview of the Datasets concept and a lot
changed in 2.0. However, I would like to know if I am doing anything wrong
in my 1.6 code.

Thanks in advance,
Anton
"
"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Tue, 15 Nov 2016 07:49:15 -0700 (MST)",RE: Handling questions in the mailing lists,dev@spark.apache.org,"Should probably also update the helping others section in the how to contribute section (https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-ContributingbyHelpingOtherUsers)
Assaf.

From: Denny Lee [via Apache Spark Developers List] [mailto:ml-node+s1001551n19835h74@n3.nabble.com]
Sent: Sunday, November 13, 2016 8:52 AM
To: Mendelson, Assaf
Subject: Re: Handling questions in the mailing lists

Hey Reynold,

Looks like we all of the proposed changes into Proposed Community Mailing Lists / StackOverflow Changes<https://docs.google.com/document/d/1N0pKatcM15cqBPqFWCqIy6jdgNzIoacZlYDCjufBh2s/edit#heading=h.xshc1bv4sn3p>.  Anything else we can do to update the Spark Community page / welcome email?

Meanwhile, let's all start answering questions on SO, eh?! :)
Denny

That's a good question, looking at http://stackoverflow.com/tags/apache-spark/topusers shows a few contributors who have already been active on SO including some committers and  PMC members with very high overall SO reputations for any administrative needs (as well as a number of other contributors besides just PMC/committers).

I was just wondering, before we move on to SO.
Do we have enough contributors with enough reputation do manage things in SO?
We would need contributors with enough reputation to have relevant privilages.
For example: creating tags (requires 1500 reputation), edit questions and answers (2000), create tag synonums (2500), approve tag wiki edits (5000), access to moderator tools (10000, this is required to delete questions etc.), protect questions (15000).
All of these are important if we plan to have SO as a main resource.
I know I originally suggested SO, however, if we do not have contributors with the required privileges and the willingness to help manage everything then I am not sure this is a good fit.
Assaf.

From: Denny Lee [via Apache Spark Developers List] [mailto:[hidden email]</user/SendEmail.jtp?type=node&node=19835&i=2>[hidden email]<http://user/SendEmail.jtp?type=node&node=19800&i=0>]
Sent: Wednesday, November 09, 2016 9:54 AM
To: Mendelson, Assaf
Subject: Re: Handling questions in the mailing lists

Agreed that by simply just moving the questions to SO will not solve anything but I think the call out about the meta-tags is that we need to abide by SO rules and if we were to just jump in and start creating meta-tags, we would be violating at minimum the spirit and at maximum the actual conventions around SO.

Saying this, perhaps we could suggest tags that we place in the header of the question whether it be SO or the mailing lists that will help us sort through all of these questions faster just as you suggested.  The Proposed Community Mailing Lists / StackOverflow Changes<https://docs.google.com/document/d/1N0pKatcM15cqBPqFWCqIy6jdgNzIoacZlYDCjufBh2s/edit#heading=h.xshc1bv4sn3p> has been updated to include suggested tags.  WDYT?

I like the document and I think it is good but I still feel like we are missing an important part here.

Look at SO today. There are:

-           4658 unanswered questions under apache-spark tag.

-          394 unanswered questions under spark-dataframe tag.

-          639 unanswered questions under apache-spark-sql

-          859 unanswered questions under pyspark

Just moving people to ask there will not help. The whole issue is having people answer the questions.

The problem is that many of these questions do not fit SO (but are already there so they are noise), are bad (i.e. unclear or hard to answer), orphaned etc. while some are simply harder than what people with some experience in spark can handle and require more expertise.
The problem is that people with the relevant expertise are drowning in noise. This. Is true for the mailing list and this is true for SO.

For this reason I believe that just moving people to SO will not solve anything.

My original thought was that if we had different tags then different people could watch open questions on these tags and therefore have a much lower noise. I thought that we would have a low tier (current one) of people just not following the documentation (which would remain as noise), then a beginner tier where we could have people downvoting bad questions but in most cases the community can answer the questions because they are common, then a â€œmediumâ€ tier which would mean harder questions but that can still be answered by advanced users and lastly an â€œadvancedâ€ tier to which committers can actually subscribed to (and adding sub tags for subsystems would improve this even more).

I was not aware of SO policy for meta tags (the burnination link is about removing tags completely so I am not sure how it applies, I believe this link https://stackoverflow.blog/2010/08/the-death-of-meta-tags/ is more relevant).
There was actually a discussion along the lines in SO (http://meta.stackoverflow.com/questions/253338/filtering-questions-by-difficulty-level).

The fact that SO did not solve this issue, does not mean we shouldnâ€™t either.

The way I see it, some tags can easily be used even with the meta tags limitation. For example, using spark-internal-development tag can be used to ask questions for development of spark. There are already tags for some spark subsystems (there is a apachae-spark-sql tag, a pyspark tag, a spark-streaming tag etc.). The main issue I see and the one we canâ€™t seem to get around is dividing between simple questions that the community should answer and hard questions which only advanced users can answer.

Maybe SO isnâ€™t the correct platform for that but even within it we can try to find a non meta name for spark beginner questions vs. spark advanced questions.
Assaf.


From: Denny Lee [via Apache Spark Developers List] [mailto:[hidden email]<http://user/SendEmail.jtp?type=node&node=19799&i=1>[hidden email]<http://user/SendEmail.jtp?type=node&node=19798&i=0>]
Sent: Tuesday, November 08, 2016 7:53 AM
To: Mendelson, Assaf

Subject: Re: Handling questions in the mailing lists

To help track and get the verbiage for the Spark community page and welcome email jump started, here's a working document for us to work with: https://docs.google.com/document/d/1N0pKatcM15cqBPqFWCqIy6jdgNzIoacZlYDCjufBh2s/edit#<https://docs.google.com/document/d/1N0pKatcM15cqBPqFWCqIy6jdgNzIoacZlYDCjufBh2s/edit>

Hope this will help us collaborate on this stuff a little faster.

Just a couple of random thoughts regarding Stack Overflow...

  *   If we are thinking about shifting focus towards SO all attempts of micromanaging should be discarded right in the beginning. Especially things like meta tags, which are discouraged and ""burninated"" (https://meta.stackoverflow.com/tags/burninate-request/info) , or thread bumping. Depending on a context these won't be manageable, go against community guidelines or simply obsolete.
  *   Lack of expertise is unlikely an issue. Even now there is a number of advanced Spark users on SO. Of course the more the merrier.

Things that can be easily improved:

  *   Identifying, improving and promoting canonical questions and answers. It means closing duplicate, suggesting edits to improve existing answers, providing alternative solutions. This can be also used to identify gaps in the documentation.
  *   Providing a set of clear posting guidelines to reduce effort required to identify the problem (think about http://stackoverflow.com/q/5963269 a.k.a How to make a great R reproducible example?)
  *   Helping users decide if question is a good fit for SO (see below). API questions are great fit, debugging problems like ""my cluster is slow"" are not.
  *   Actively cleaning (closing, deleting) off-topic and low quality questions. The less junk to sieve through the better chance of good questions being answered.
  *   Repurposing and actively moderating SO docs (https://stackoverflow.com/documentation/apache-spark/topics). Right now most of the stuff that goes there is useless, duplicated or plagiarized, or border case SPAM.
  *   Encouraging community to monitor featured (https://stackoverflow.com/questions/tagged/apache-spark?sort=featured) and active & upvoted & unanswered (https://stackoverflow.com/unanswered/tagged/apache-spark) questions.
  *   Implementing some procedure to identify questions which are likely to be bugs or a material for feature requests. Personally I am quite often tempted to simply send a link to dev list, but I don't think it is really acceptable.
  *   Animating Spark related chat room. I tried this a couple of times but to no avail. Without a certain critical mass of users it just won't work.



This is an excellent point. If we do go ahead and feature SO as a way for users to ask questions more prominently, as someone who knows SO very well, would you be willing to help write a short guideline (ideally the shorter the better, which makes it hard) to direct what goes to user@ and what goes to SO?

Sure, I'll be happy to help if I can.

Damn, I always thought that mailing list is only for nice and welcoming people and there is nothing to do for me here >:)

To be serious though, there are many questions on the users list which would fit just fine on SO but it is not true in general. There are dozens of questions which are to broad, opinion based, ask for external resources and so on. If you want to direct users to SO you have to help them to decide if it is the right channel. Otherwise it will just create a really bad experience for both seeking help and active answerers. Former ones will be downvoted and bashed, latter ones will have to deal with handling all the junk and the number of active Spark users with moderation privileges is really low (with only Massg and me being able to directly close duplicates).

Believe me, I've seen this before.
You have substantially underestimated how opinionated people can be on mailing lists too :)

You have to remember that Stack Overflow crowd (like me) is highly opinionated, so many questions, which could be just fine on the mailing list, will be quickly downvoted and / or closed as off-topic. Just saying...

--

Best,

Maciej

OK I've checked on the ASF member list (which is private so there is no public archive).

It is not against any ASF rule to recommend StackOverflow as a place for users to ask questions. I don't think we can or should delete the existing user@spark list either, but we can certainly make SO more visible than it is.


Actually after talking with more ASF members, I believe the only policy is that development decisions have to be made and announced on ASF properties (dev list or jira), but user questions don't have to.

I'm going to double check this. If it is true, I would actually recommend us moving entirely over the Q&A part of the user list to stackoverflow, or at least make that the recommended way rather than the existing user list which is not very scalable.



Weâ€™ve discussed several times upgrading our communication tools, as far back as 2014 and maybe even before that too. The bottom line is that we canâ€™t due to ASF rules requiring the use of ASF-managed mailing lists.

For some history, see this discussion:
â€¢         https://mail-archives.apache.org/mod_mbox/spark-user/201412.mbox/%3CCAOhmDzfL2COdysV8r5hZN8f=NqXM=f=oY5NO2dHWJ_kVEoP+Ng@...%3E<https://mail-archives.apache.org/mod_mbox/spark-user/201412.mbox/%3CCAOhmDzfL2COdysV8r5hZN8f=NqXM=f=oY5NO2dHWJ_kVEoP+Ng@mail.gmail.com%3E>
â€¢         https://mail-archives.apache.org/mod_mbox/spark-user/201501.mbox/%3CCAOhmDzec1JdsXQq3dDwAv7eLnzRidSkrsKKG0xKw=TKTxY_sYw@...%3E<https://mail-archives.apache.org/mod_mbox/spark-user/201501.mbox/%3CCAOhmDzec1JdsXQq3dDwAv7eLnzRidSkrsKKG0xKw=TKTxY_sYw@mail.gmail.com%3E>

(Itâ€™s ironic that itâ€™s difficult to follow the past discussion on why we canâ€™t change our official communication tools due to those very toolsâ€¦)

Nick
â€‹
I fell Assaf point is quite relevant if we want to move this project forward from the Spark user perspective (as I do). In fact, we're still using 20th century tools (mailing lists) with some add-ons (like Stack Overflow).

As usually, Sean and Cody's contributions are very to the point.
I fell it is indeed a matter of of culture (hard to enforce) and tools (much easier). Isn't it?
So concrete things people could do

- users could tag subject lines appropriately to the component they're
asking about

- contributors could monitor user@ for tags relating to components
they've worked on.
I'd be surprised if my miss rate for any mailing list questions
well-labeled as Kafka was higher than 5%

- committers could be more aggressive about soliciting and merging PRs
to improve documentation.
It's a lot easier to answer even poorly-asked questions with a link to
relevant docs.

elf
's
to
ty
ser/SendEmail.jtp?type=node&node=19770&i=8>>

g
er
the
â€¦
node&node=19770&i=9>]

de&node=19770&i=10>

ce.
 a
ing
re
 too
 to
. I
is
ser/SendEmail.jtp?type=node&node=19770&i=11>>

is was
no
st
a
ons
s
.
---------------------------------------------------------------------
&node=19770&i=12>







--

Maciej Szymkiewicz

________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/Handling-questions-in-the-mailing-lists-tp19690p19770.html
To start a new topic under Apache Spark Developers List, email [hidden email]<http://user/SendEmail.jtp?type=node&node=19798&i=1>
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>

________________________________
View this message in context: RE: Handling questions in the mailing lists<http://apache-spark-developers-list.1001551.n3.nabble.com/Handling-questions-in-the-mailing-lists-tp19690p19798.html>
he-spark-developers-list.1001551.n3.nabble.com/> at Nabble.com.

________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/Handling-questions-in-the-mailing-lists-tp19690p19799.html
To start a new topic under Apache Spark Developers List, email [hidden email]<http://user/SendEmail.jtp?type=node&node=19800&i=1>
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>

________________________________
View this message in context: RE: Handling questions in the mailing lists<http://apache-spark-developers-list.1001551.n3.nabble.com/Handling-questions-in-the-mailing-lists-tp19690p19800.html>
he-spark-developers-list.1001551.n3.nabble.com/> at Nabble.com.



--
Cell : <a href=""tel:(425)%20233-8271"" value=""+14252338271"" class=""gmail_msg"" target=""_blank"">425-233-8271
Twitter: https://twitter.com/holdenkarau

________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/Handling-questions-in-the-mailing-lists-tp19690p19835.html
To start a new topic under Apache Spark Developers List, email ml-node+s1001551n1h20@n3.nabble.com<mailto:ml-node+s1001551n1h20@n3.nabble.com>
spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=1&code=YXNzYWYubWVuZGVsc29uQHJzYS5jb218MXwtMTI4OTkxNTg1Mg==>.
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>




--
3.nabble.com/Handling-questions-in-the-mailing-lists-tp19690p19891.html
om."
=?GBK?B?zfXHxcqv?= <uit_wangqiaoshi@163.com>,"Tue, 15 Nov 2016 23:32:49 +0800 (CST)",How statistical key rune time,dev@spark.apache.org,"hi guys!




Is there a way! Try to statistics top N of  run time,the  datas for key shuffle or transform after shuffle ,eg,reduceByKey, groupByKey, reduceByKey.  

So could find at a glance,which key problems."
Mark Hamstra <mark@clearstorydata.com>,"Tue, 15 Nov 2016 10:37:51 -0800","Re: Spark-SQL parameters like shuffle.partitions should be stored in
 the lineage",leo9r <lezcano.leo@gmail.com>,"AFAIK, the adaptive shuffle partitioning still isn't completely ready to be
made the default, and there are some corner issues that need to be
addressed before this functionality is declared finished and ready.  E.g.,
Partition into an even larger partition before the ExchangeCoordinator
decides to create a new one.  That can be worked around by changing the
logic to ""If including the nextShuffleInputSize would exceed the target
partition size, then start a new partition"":
https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ExchangeCoordinator.scala#L173

If you're willing to work around those kinds of issues to fit your use
case, then I do know that the adaptive shuffle partitioning can be made to
work well even if it is not perfect.  It would be nice, though, to see
adaptive partitioning be finished and hardened to the point where it
becomes the default, because a fixed number of shuffle partitions has some
significant limitations and problems.


"
Mark Hamstra <mark@clearstorydata.com>,"Tue, 15 Nov 2016 10:42:16 -0800","Re: Spark-SQL parameters like shuffle.partitions should be stored in
 the lineage",Sean Owen <sowen@cloudera.com>,"You still have the problem that even within a single Job it is often the
case that not every Exchange really wants to use the same number of shuffle
partitions.


"
aakash aakash <email2aakash@gmail.com>,"Tue, 15 Nov 2016 10:58:34 -0800",Fwd: using Spark Streaming with Kafka 0.9/0.10,dev@spark.apache.org,"Re-posting it at dev group.

Thanks and Regards,
Aakash


---------- Forwarded message ----------
From: aakash aakash <email2aakash@gmail.com>
Date: Mon, Nov 14, 2016 at 4:10 PM
Subject: using Spark Streaming with Kafka 0.9/0.10
To: user-subscribe@spark.apache.org


Hi,

I am planning to use Spark Streaming to consume messages from Kafka 0.9. I
have couple of questions regarding this :


   - I see APIs are annotated with @Experimental. So can you please tell me
   when are we planning to make it production ready ?
   - Currently, I see we are using Kafka 0.10 and so curious to know why
   not we started with 0.9 Kafka instead of 0.10 Kafka. As I see 0.10 kafka
   client would not be compatible with 0.9 client since there are some changes
   in arguments in consumer API.
   - Current API extends InputDstream and as per document it means RDD will
   be generated by running a service/thread only on the driver node instead of
   worker node. Can you please explain to me why we are doing this and what is
   required to make sure that it runs on worker node.


Thanks in advance !

Regards,
Aakash
"
Joseph Bradley <joseph@databricks.com>,"Tue, 15 Nov 2016 11:54:16 -0800","Re: Reduce the memory usage if we do same first in
 GradientBoostedTrees if subsamplingRate< 1.0",WangJianfei <wangjianfei15@otcaix.iscas.ac.cn>,"Thanks for the suggestion.  That would be faster, but less accurate in most
cases.  It's generally better to use a new random sample on each iteration,
based on literature and results I've seen.
Joseph


.
·®
"
Artur Sukhenko <artur.sukhenko@gmail.com>,"Tue, 15 Nov 2016 20:45:43 +0000",NodeManager heap size with ExternalShuffleService,dev@spark.apache.org,"Hello guys,

When you enable ExternalShuffleService (spark-shuffle) in NodeManager,
there are no suggestions of increasing NM heap size in Spark docs or
anywhere else, shouldn't we include this in spark's documentation?

I have seen NM take a lot of memory 5+ gb with default 1g, and in case of
its GC pauses spark can become very slow when tasks are doing shuffle. I
don't think users are aware of NM becoming bottleneck.


Sincerely,
Artur Sukhenko
-- 
--
Artur Sukhenko
"
Marcelo Vanzin <vanzin@cloudera.com>,"Tue, 15 Nov 2016 13:21:10 -0800",Running lint-java during PR builds?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey all,

Is there a reason why lint-java is not run during PR builds? I see it
seems to be maven-only, is it really expensive to run after an sbt
build?

I see a lot of PRs coming in to fix Java style issues, and those all
seem a little unnecessary. Either we're enforcing style checks or
we're not, and right now it seems we aren't.

-- 
Marcelo

---------------------------------------------------------------------


"
"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Tue, 15 Nov 2016 13:40:26 -0800",Re: Running lint-java during PR builds?,Marcelo Vanzin <vanzin@cloudera.com>,"I remember it's because you need to run `mvn install` before running
lint-java if the maven cache is empty, and `mvn install` is pretty heavy.


"
Cody Koeninger <cody@koeninger.org>,"Tue, 15 Nov 2016 14:55:51 -0800",Re: using Spark Streaming with Kafka 0.9/0.10,aakash aakash <email2aakash@gmail.com>,"It'd probably be worth no longer marking the 0.8 interface as
experimental.  I don't think it's likely to be subject to active
development at this point.

You can use the 0.8 artifact to consume from a 0.9 broker

Where are you reading documentation indicating that the direct stream
only runs on the driver?  It runs consumers on the worker nodes.



---------------------------------------------------------------------


"
WangJianfei <wangjianfei15@otcaix.iscas.ac.cn>,"Tue, 15 Nov 2016 18:02:27 -0700 (MST)","=?UTF-8?Q?=E5=9B=9E=E5=A4=8D=EF=BC=9A_Reduce_the_memory_u?=
 =?UTF-8?Q?sage_if_we_do_same_first?=
 =?UTF-8?Q?_inGradientBoostedTrees_if_subsamplingRate<_1.0?=",dev@spark.apache.org,"with predError.zip(input) ,we get RDD data,  so we can just do a sample on predError or input, if so, we can't use zip(the elements number must be the same in each partition),thank you!




------------------ åŽŸå§‹é‚®ä»¶ ------------------
å‘ä»¶äºº: ""Joseph Bradley [via Apache Spark Developers List]"";<ml-node+s1001551n19899h63@n3.nabble.com>;
å‘é€æ—¶é—´: 2016å¹´11æœˆ16æ—¥(æ˜ŸæœŸä¸‰) å‡Œæ™¨3:54
æ”¶ä»¶äºº: ""WangJianfei""<wangjianfei15@otcaix.iscas.ac.cn
ä¸»é¢˜: Re: Reduce the memory usage if we do same first inGradientBoostedTrees if subsamplingRate< 1.0



 	Thanks for the suggestion.  That would be faster, but less accurate in most cases.  It's generally better to use a new random sample on each iteration, based on literature and results I've seen.Joseph


when we train the mode, we will use the data with a subSampleRate, so if the
 subSampleRate < 1.0 , we can do a sample first to reduce the memory usage.
 se the code below in GradientBoostedTrees.boost()
 
  while (m < numIterations && !doneLearning) {
       // Update data with pseudo-residuals å‰©ä½™è¯¯å·®
       val data = predError.zip(input).map { case ((pred, _), point) =>
         LabeledPoint(-loss.gradient(pred, point.label), point.features)
       }
 
       timer.start(s""building tree $m"")
       logDebug(""###################################################"")
       logDebug(""Gradient boosting tree iteration "" + m)
       logDebug(""###################################################"")
       val dt = new DecisionTreeRegressor().setSeed(seed + m)
       val model = dt.train(data, treeStrategy)
 
 
 
 
 
 --
n3.nabble.com/Reduce-the-memory-usage-if-we-do-same-first-in-GradientBoostedTrees-if-subsamplingRate-1-0-tp19826.html
com.
 
 ---------------------------------------------------------------------
 
 


 	 	 	 	
 	
 	
 	 		If you reply to this email, your message will be added to the discussion below:
 		http://apache-spark-developers-list.1001551.n3.nabble.com/Reduce-the-memory-usage-if-we-do-sample-first-in-GradientBoostedTrees-with-the-condition-that-subsam0-tp19826p19899.html 	
le first in GradientBoostedTrees with the condition that subsamplingRate< 1.0, click here.
 		NAML



--
3.nabble.com/Reduce-the-memory-usage-if-we-do-same-first-inGradientBoostedTrees-if-subsamplingRate-1-0-tp19904.html
om."
WangJianfei <wangjianfei15@otcaix.iscas.ac.cn>,"Tue, 15 Nov 2016 18:05:55 -0700 (MST)","Re: Reduce the memory usage if we do same first in
 GradientBoostedTrees if subsamplingRate< 1.0",dev@spark.apache.org,"with predError.zip(input) ,we get RDD data,  so we can just do a sample on
predError or input, if so, we can't use zip(the elements number must be the
same in each partition),thank you!



--

---------------------------------------------------------------------


"
aakash aakash <email2aakash@gmail.com>,"Tue, 15 Nov 2016 17:50:18 -0800",Re: using Spark Streaming with Kafka 0.9/0.10,Cody Koeninger <cody@koeninger.org>,"
We are currently using ""Camus
<http://docs.confluent.io/1.0/camus/docs/intro.html>"" in production and one
of the main goal to move to Spark is to use new Kafka Consumer API  of
Kafka 0.9 and in our case we need the security provisions available in 0.9,
that why we cannot use 0.8 client.

only runs on the driver?

I might be wrong here, but I see that new
<http://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html>
kafka+Spark stream code extend the InputStream
<http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.dstream.InputDStream>
and its documentation says :

* Input streams that can generate RDDs from new data by running a
service/thread only on the driver node (that is, without running a receiver
on worker nodes) *
Thanks and regards,
Aakash Pradeep



"
Reynold Xin <rxin@databricks.com>,"Tue, 15 Nov 2016 18:07:35 -0800",Re: NodeManager heap size with ExternalShuffleService,"Artur Sukhenko <artur.sukhenko@gmail.com>, dev@spark.apache.org","Can you submit a pull request to add that to the documentation?



Hello guys,

When you enable ExternalShuffleService (spark-shuffle) in NodeManager,
there are no suggestions of increasing NM heap size in Spark docs or
anywhere else, shouldn't we include this in spark's documentation?

I have seen NM take a lot of memory 5+ gb with default 1g, and in case of
its GC pauses spark can become very slow when tasks are doing shuffle. I
don't think users are aware of NM becoming bottleneck.


Sincerely,
Artur Sukhenko
--
--
Artur Sukhenko
"
Cody Koeninger <cody@koeninger.org>,"Tue, 15 Nov 2016 19:47:59 -0800",Re: using Spark Streaming with Kafka 0.9/0.10,aakash aakash <email2aakash@gmail.com>,"Generating / defining an RDDis not the same thing as running the
compute() method of an rdd .  The direct stream definitely runs kafka
consumers on the executors.

If you want more info, the blog post and video linked from
https://github.com/koeninger/kafka-exactly-once refers to the 0.8
implementation, but the general design is similar for the 0.10
version.

I think the likelihood of an official release supporting 0.9 is fairly
slim at this point, it's a year out of date and wouldn't be a drop-in
dependency change.



---------------------------------------------------------------------


"
aakash aakash <email2aakash@gmail.com>,"Tue, 15 Nov 2016 22:29:05 -0800",Re: using Spark Streaming with Kafka 0.9/0.10,Cody Koeninger <cody@koeninger.org>,"Thanks for the link and info Cody !


Regards,
Aakash



"
kant kodali <kanth909@gmail.com>,"Wed, 16 Nov 2016 01:44:39 -0800","How do I convert json_encoded_blob_column into a data frame? (This
 may be a feature request)","dev <dev@spark.apache.org>, ""user @spark"" <user@spark.apache.org>","https://spark.apache.org/docs/2.0.2/sql-programming-guide.html#json-datasets

""Spark SQL can automatically infer the schema of a JSON dataset and load it
as a DataFrame. This conversion can be done using SQLContext.read.json() on
either an RDD of String, or a JSON file.""

val df = spark.sql(""SELECT json_encoded_blob_column from table_name""); // A
cassandra query (cassandra stores blobs in hexadecimal )
json_encoded_blob_column
is encoded in hexadecimal. It will be great to have these blobs interpreted
and be loaded as a data frame but for now is there anyway to load or parse
json_encoded_blob_column into a data frame?
"
Hyukjin Kwon <gurwls223@gmail.com>,"Wed, 16 Nov 2016 19:49:00 +0900","Re: How do I convert json_encoded_blob_column into a data frame?
 (This may be a feature request)",kant kodali <kanth909@gmail.com>,"Maybe it sounds like you are looking for from_json/to_json functions after
en/decoding properly.


"
<NNigam@in.imshealth.com>,"Wed, 16 Nov 2016 11:29:47 +0000",Insert data into hive internal tables using hiveContext <spark1.6> ,"<dev@spark.apache.org>, <user@spark.apache.org>","Hi,

I am getting below error while inserting data into hive internal tables using hivecontext. Is there any way to resolve this issue. I am getting below error while execution:


[cid:image002.jpg@01D2402A.C173EDD0]

Thanks,
Nitisha Nigam
********************** IMPORTANT--PLEASE READ ************************ This electronic message, including its attachments, is CONFIDENTIAL and may contain PROPRIETARY or LEGALLY PRIVILEGED or PROTECTED information and is intended for the authorized recipient of the sender. If you are not the intended recipient, you are hereby notified that any use, disclosure, copying, or distribution of this message or any of the information included in it is unauthorized and strictly prohibited. If you have received this message in error, please immediately notify the sender by reply e-mail and permanently delete this message and its attachments, along with any copies thereof, from all locations received (e.g., computer, mobile device, etc.). Thank you. ************************************************************************
"
Ricardo Almeida <ricardo.almeida@actnowib.com>,"Wed, 16 Nov 2016 14:07:35 +0100",Re: separate spark and hive,Spark dev list <dev@spark.apache.org>,"Great to know about the ""spark.sql.catalogImplementation"" configuration
property.
I can't find this anywhere but in Jacek Laskowski's ""Mastering Apache Spark
2.0"" Gitbook.

I guess we should document on Spark Configuration page


 to â€œin-memoryâ€ by
st
(I know for
d
o use
d
e
l
to
se at
ld
k-and-hive-tp19879.html>
n
Servlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
k-and-hive-tp19879p19884.html>
"
Sean Owen <sowen@cloudera.com>,"Wed, 16 Nov 2016 13:57:08 +0000",Re: Handling questions in the mailing lists,"""assaf.mendelson"" <assaf.mendelson@rsa.com>, dev <dev@spark.apache.org>, 
	Denny Lee <denny.g.lee@gmail.com>","I updated the wiki to point to the /community.html page. (We're going to
migrate the wiki real soon now anyway)

I updated the /community.html page per this thread too. PR:
https://github.com/apache/spark-website/pull/16


Should probably also update the helping others section in the how to
contribute section (<a href=""
https://cwiki.apache.org/confluence/display/SPARK/Contributing&#43;to&#43;Spark#ContributingtoSpark-ContributingbyHelpingOtherUsers
<https://cwiki.apache.org/confluence/display/SPARK/Contributing+to&%2343;Spark%23ContributingtoSpark-ContributingbyHelpingOtherUsers>
"">
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-ContributingbyHelpingOtherUsers)


Assaf.



From: Denny Lee [via Apache Spark Developers List] [mailto:ml-node+[hidden
email] <http://user/SendEmail.jtp?type=node&node=19891&i=0>]
Sent: Sunday, November 13, 2016 8:52 AM



To: Mendelson, Assaf
Subject: Re: Handling questions in the mailing lists







Hey Reynold,






Looks like we all of the proposed changes into Proposed Community Mailing
Lists / StackOverflow Changes
<https://docs.google.com/document/d/1N0pKatcM15cqBPqFWCqIy6jdgNzIoacZlYDCjufBh2s/edit#heading=h.xshc1bv4sn3p>.
Anything else we can do to update the Spark Community page / welcome email?





Meanwhile, let's all start answering questions on SO, eh?! :)


Denny







That's a good question, looking at
http://stackoverflow.com/tags/apache-spark/topusers shows a few
contributors who have already been active on SO including some committers
and PMC members with very high overall SO reputations for any
administrative needs (as well as a number of other contributors besides
just PMC/committers).






I was just wondering, before we move on to SO.

Do we have enough contributors with enough reputation do manage things in
SO?

We would need contributors with enough reputation to have relevant
privilages.

For example: creating tags (requires 1500 reputation), edit questions and
answers (2000), create tag synonums (2500), approve tag wiki edits (5000),
access to moderator tools (10000, this is required to delete questions
etc.), protect questions (15000).

All of these are important if we plan to have SO as a main resource.

I know I originally suggested SO, however, if we do not have contributors
with the required privileges and the willingness to help manage everything
then I am not sure this is a good fit.

Assaf.





From: Denny Lee [via Apache Spark Developers List] [mailto:[hidden email]
<http://user/SendEmail.jtp?type=node&node=19835&i=2>[hidden email]
<http://user/SendEmail.jtp?type=node&node=19800&i=0>]




Sent: Wednesday, November 09, 2016 9:54 AM
To: Mendelson, Assaf
Subject: Re: Handling questions in the mailing lists






Agreed that by simply just moving the questions to SO will not solve
anything but I think the call out about the meta-tags is that we need to
abide by SO rules and if we were to just jump in and start creating
meta-tags, we would be violating at minimum the spirit and at maximum the
actual conventions around SO.





Saying this, perhaps we could suggest tags that we place in the header of
the question whether it be SO or the mailing lists that will help us sort
through all of these questions faster just as you suggested. The Proposed
Community Mailing Lists / StackOverflow Changes
<https://docs.google.com/document/d/1N0pKatcM15cqBPqFWCqIy6jdgNzIoacZlYDCjufBh2s/edit#heading=h.xshc1bv4sn3p>
has been updated to include suggested tags. WDYT?
"
Georg Heiler <georg.kf.heiler@gmail.com>,"Wed, 16 Nov 2016 14:29:02 +0000",Develop custom Estimator / Transformer for pipeline,dev@spark.apache.org,"HI,

I want to develop a library with custom Estimator / Transformers for spark.
So far not a lot of documentation could be found but
http://stackoverflow.com/questions/37270446/how-to-roll-a-custom-estimator-in-pyspark-mllib


Suggest that:
Generally speaking, there is no documentation because as for Spark 1.6 /
2.0 most of the related API is not intended to be public. It should change
in Spark 2.1.0 (see SPARK-7146
<https://issues.apache.org/jira/browse/SPARK-7146>).

Where can I already find documentation today?
Is it true that my library would require residing in Sparks`s namespace
similar to https://github.com/collectivemedia/spark-ext to utilize all the
handy functionality?

Kind Regards,
Georg
"
Denny Lee <denny.g.lee@gmail.com>,"Wed, 16 Nov 2016 14:32:21 +0000",Re: Handling questions in the mailing lists,"Sean Owen <sowen@cloudera.com>, ""assaf.mendelson"" <assaf.mendelson@rsa.com>, 
	dev <dev@spark.apache.org>","Awesome stuff! Thanks Sean! :-)


"
Artur Sukhenko <artur.sukhenko@gmail.com>,"Wed, 16 Nov 2016 16:31:15 +0000",Re: NodeManager heap size with ExternalShuffleService,"Reynold Xin <rxin@databricks.com>, dev@spark.apache.org","Sure Reynold,

Here is pull request - [YARN][DOC] Increasing NodeManager's heap size with
External Shuffle Service <https://github.com/apache/spark/pull/15906>


Can you submit a pull request to add that to the documentation?



Hello guys,

When you enable ExternalShuffleService (spark-shuffle) in NodeManager,
there are no suggestions of increasing NM heap size in Spark docs or
anywhere else, shouldn't we include this in spark's documentation?

I have seen NM take a lot of memory 5+ gb with default 1g, and in case of
its GC pauses spark can become very slow when tasks are doing shuffle. I
don't think users are aware of NM becoming bottleneck.


Sincerely,
Artur Sukhenko
--
--
Artur Sukhenko

-- 
--
Artur Sukhenko
"
Michael Armbrust <michael@databricks.com>,"Wed, 16 Nov 2016 10:50:22 -0800","Re: How do I convert json_encoded_blob_column into a data frame?
 (This may be a feature request)",Hyukjin Kwon <gurwls223@gmail.com>,"

Which are new built-in functions that will be released with Spark 2.1.
"
"""Dongjoon Hyun""<dongjoon@apache.org>","Wed, 16 Nov 2016 19:50:44 -0000",Re: Running lint-java during PR builds?,<dev@spark.apache.org>,"Hi, Marcelo and Ryan.

That was the main purpose of my proposal about Travis.CI.
IMO, that is the only way to achieve that without any harmful side-effect on Jenkins infra.

Spark is already ready for that. Like AppVoyer, if one of you files an INFRA jira issue to enable that, they will turn on that. Then, we can try it and see the result. Also, you can turn off easily again if you don't want.

Without this, we will consume more community efforts. For example, we merged lint-java error fix PR seven hours ago, but the master branch still has one lint-java error.

https://travis-ci.org/dongjoon-hyun/spark/jobs/176351319

Actually, I've been monitoring the history here. (It's synced every 30 minutes.)

https://travis-ci.org/dongjoon-hyun/spark/builds

Could we give a change to this?

Bests,
Dongjoon.


---------------------------------------------------------------------


"
Hoang Bao Thien <hbthien0410@gmail.com>,"Thu, 17 Nov 2016 05:45:17 +0700",Kafka segmentation,dev@spark.apache.org,"Hi all,

I would like to ask a question related to the size of Kafka stream. I want
to put data (e.g., file *.csv) to Kafka then use Spark streaming to get the
output from Kafka and then save to Hive by using SparkSQL. The file csv is
about 100MB with ~250K messages/rows (Each row has about 10 fields of
integer). I see that Spark Streaming first received two partitions/batches,
the first is of 60K messages and the second is of 50K msgs. But from the
third batch, Spark just received 200 messages for each batch (or partition).
I think that this problem is coming from Kafka or some configuration in
Spark. I already tried to configure with the setting
""auto.offset.reset=largest"", but every batch only gets 200 messages.

Could you please tell me how to fix this problem?
Thank you so much.

Best regards,
Alex
"
Mohit Jaggi <mohitjaggi@gmail.com>,"Wed, 16 Nov 2016 14:47:20 -0800",SparkILoop doesn't run,"user <user@spark.apache.org>,
 Dev <dev@spark.apache.org>","I am trying to use SparkILoop to write some tests(shown below) but the test hangs with the following stack trace. Any idea what is going on?


import org.apache.log4j.{Level, LogManager}
import org.apache.spark.repl.SparkILoop
import org.scalatest.{BeforeAndAfterAll, FunSuite}

class SparkReplSpec extends FunSuite with BeforeAndAfterAll {

  override def beforeAll(): Unit = {
  }

  override def afterAll(): Unit = {
  }

  test(""yay!"") {
    val rootLogger = LogManager.getRootLogger
    val logLevel = rootLogger.getLevel
    rootLogger.setLevel(Level.ERROR)

    val output = SparkILoop.run(
      """"""
        |println(""hello"")
      """""".stripMargin)

    println(s""[[[[ $output ]]]]"")

  }
}

/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/bin/java -Dspark.master=local[*] -Didea.launcher.port=7532 ""-Didea.launcher.bin.path=/Applications/IntelliJ IDEA CE.app/Contents/bin"" -Dfile.encoding=UTF-8 -classpath ""/Users/mohit/Library/Application Support/IdeaIC2016.2/Scala/lib/scala-plugin-runners.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/deploy.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/ext/cldrdata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/ext/dnsns.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/ext/jaccess.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/ext/jfxrt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/ext/localedata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/ext/nashorn.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/ext/sunec.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/ext/sunjce_provider.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/ext/sunpkcs11.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/ext/zipfs.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/javaws.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/jfxswt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/management-agent.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/plugin.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/resources.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/lib/ant-javafx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/lib/dt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/lib/javafx-mx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/lib/jconsole.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/lib/packager.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/lib/sa-jdi.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/lib/tools.jar:/Users/mohit/code/datagears-play/target/scala-2.11/test-classes:/Users/mohit/code/datagears-play/target/scala-2.11/classes:/Users/mohit/code/datagears-play/macros/target/scala-2.11/classes:/Users/mohit/.ivy2/cache/org.xerial.snappy/snappy-java/bundles/snappy-java-1.1.2.4.jar:/Users/mohit/.ivy2/cache/org.apache.spark/spark-unsafe_2.11/jars/spark-unsafe_2.11-2.0.0.jar:/Users/mohit/.ivy2/cache/org.apache.spark/spark-tags_2.11/jars/spark-tags_2.11-2.0.0.jar:/Users/mohit/.ivy2/cache/org.apache.spark/spark-streaming_2.11/jars/spark-streaming_2.11-2.0.0.jar:/Users/mohit/.ivy2/cache/org.apache.spark/spark-sql_2.11/jars/spark-sql_2.11-2.0.0.jar:/Users/mohit/.ivy2/cache/org.apache.spark/spark-sketch_2.11/jars/spark-sketch_2.11-2.0.0.jar:/Users/mohit/.ivy2/cache/org.apache.spark/spark-repl_2.11/jars/spark-repl_2.11-2.0.0.jar:/Users/mohit/.ivy2/cache/org.apache.spark/spark-network-shuffle_2.11/jars/spark-network-shuffle_2.11-2.0.0.jar:/Users/mohit/.ivy2/cache/org.apache.spark/spark-network-common_2.11/jars/spark-network-common_2.11-2.0.0.jar:/Users/mohit/.ivy2/cache/org.apache.spark/spark-mllib_2.11/jars/spark-mllib_2.11-2.0.0.jar:/Users/mohit/.ivy2/cache/org.apache.spark/spark-mllib-local_2.11/jars/spark-mllib-local_2.11-2.0.0.jar:/Users/mohit/.ivy2/cache/org.apache.spark/spark-launcher_2.11/jars/spark-launcher_2.11-2.0.0.jar:/Users/mohit/.ivy2/cache/org.apache.spark/spark-graphx_2.11/jars/spark-graphx_2.11-2.0.0.jar:/Users/mohit/.ivy2/cache/org.apache.spark/spark-core_2.11/jars/spark-core_2.11-2.0.0.jar:/Users/mohit/.ivy2/cache/org.apache.spark/spark-catalyst_2.11/jars/spark-catalyst_2.11-2.0.0.jar:/Users/mohit/.ivy2/cache/net.sf.py4j/py4j/jars/py4j-0.10.1.jar:/Users/mohit/.ivy2/cache/xml-apis/xml-apis/jars/xml-apis-1.4.01.jar:/Users/mohit/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar:/Users/mohit/code/activator-dist-1.3.10/repository/xalan/xalan/2.7.2/jars/xalan.jar:/Users/mohit/code/activator-dist-1.3.10/repository/xalan/serializer/2.7.2/jars/serializer.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.webbitserver/webbit/0.4.14/jars/webbit.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.w3c.css/sac/1.3/jars/sac.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.slf4j/slf4j-api/1.7.16/jars/slf4j-api.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.slf4j/jul-to-slf4j/1.7.16/jars/jul-to-slf4j.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.slf4j/jcl-over-slf4j/1.7.16/jars/jcl-over-slf4j.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.seleniumhq.selenium/selenium-support/2.48.2/jars/selenium-support.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.seleniumhq.selenium/selenium-safari-driver/2.48.2/jars/selenium-safari-driver.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.seleniumhq.selenium/selenium-remote-driver/2.48.2/jars/selenium-remote-driver.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.seleniumhq.selenium/selenium-leg-rc/2.48.2/jars/selenium-leg-rc.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.seleniumhq.selenium/selenium-java/2.48.2/jars/selenium-java.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.seleniumhq.selenium/selenium-ie-driver/2.48.2/jars/selenium-ie-driver.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.seleniumhq.selenium/selenium-htmlunit-driver/2.48.2/jars/selenium-htmlunit-driver.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.seleniumhq.selenium/selenium-firefox-driver/2.48.2/jars/selenium-firefox-driver.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.seleniumhq.selenium/selenium-edge-driver/2.48.2/jars/selenium-edge-driver.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.seleniumhq.selenium/selenium-chrome-driver/2.48.2/jars/selenium-chrome-driver.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.seleniumhq.selenium/selenium-api/2.48.2/jars/selenium-api.jar:/Users/mohit/.ivy2/cache/org.scalatestplus.play/scalatestplus-play_2.11/jars/scalatestplus-play_2.11-1.5.0-RC1.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.scalatest/scalatest_2.11/2.2.6/bundles/scalatest_2.11.jar:/Users/mohit/.ivy2/cache/org.scalacheck/scalacheck_2.11/jars/scalacheck_2.11-1.12.5.jar:/Users/mohit/.ivy2/cache/org.scala-stm/scala-stm_2.11/jars/scala-stm_2.11-0.7.jar:/Users/mohit/.ivy2/cache/org.scala-sbt/test-interface/jars/test-interface-1.0.jar:/Users/mohit/.ivy2/cache/org.scala-lang.modules/scala-xml_2.11/bundles/scala-xml_2.11-1.0.1.jar:/Users/mohit/.ivy2/cache/org.scala-lang.modules/scala-parser-combinators_2.11/bundles/scala-parser-combinators_2.11-1.0.4.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.scala-lang.modules/scala-java8-compat_2.11/0.7.0/bundles/scala-java8-compat_2.11.jar:/Users/mohit/.ivy2/cache/org.scala-lang/scala-reflect/jars/scala-reflect-2.11.7.jar:/Users/mohit/.ivy2/cache/org.scala-lang/scala-library/jars/scala-library-2.11.8.jar:/Users/mohit/.ivy2/cache/org.scala-graph/graph-core_2.11/jars/graph-core_2.11-1.11.2.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.reactivestreams/reactive-streams/1.0.0/jars/reactive-streams.jar:/Users/mohit/.ivy2/cache/org.ow2.asm/asm-util/jars/asm-util-5.1.jar:/Users/mohit/.ivy2/cache/org.ow2.asm/asm-tree/jars/asm-tree-5.1.jar:/Users/mohit/.ivy2/cache/org.ow2.asm/asm-commons/jars/asm-commons-5.1.jar:/Users/mohit/.ivy2/cache/org.ow2.asm/asm/jars/asm-5.1.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.joda/joda-convert/1.8.1/jars/joda-convert.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.javassist/javassist/3.20.0-GA/bundles/javassist.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.hamcrest/hamcrest-core/1.3/jars/hamcrest-core.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.fluentlenium/fluentlenium-core/0.10.9/jars/fluentlenium-core.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.eclipse.jetty.websocket/websocket-common/9.2.15.v20160210/jars/websocket-common.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.eclipse.jetty.websocket/websocket-client/9.2.15.v20160210/jars/websocket-client.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.eclipse.jetty.websocket/websocket-api/9.2.15.v20160210/jars/websocket-api.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.eclipse.jetty/jetty-util/9.2.15.v20160210/jars/jetty-util.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.eclipse.jetty/jetty-io/9.2.15.v20160210/jars/jetty-io.jar:/Users/mohit/.ivy2/cache/org.clapper/grizzled-scala_2.11/jars/grizzled-scala_2.11-3.0.0.jar:/Users/mohit/.ivy2/cache/org.clapper/classutil_2.11/jars/classutil_2.11-1.0.13.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.asynchttpclient/netty-resolver-dns/2.0.0-RC9/jars/netty-resolver-dns.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.asynchttpclient/netty-resolver/2.0.0-RC9/jars/netty-resolver.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.asynchttpclient/netty-codec-dns/2.0.0-RC9/jars/netty-codec-dns.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.asynchttpclient/async-http-client/2.0.0-RC9/jars/async-http-client.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.apache.httpcomponents/httpmime/4.5.2/jars/httpmime.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.apache.httpcomponents/httpcore/4.4.4/jars/httpcore.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.apache.httpcomponents/httpcore/4.0.1/jars/httpcore.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.apache.httpcomponents/httpclient/4.5.2/jars/httpclient.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.apache.httpcomponents/httpclient/4.0.1/jars/httpclient.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.apache.commons/commons-lang3/3.4/jars/commons-lang3.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.apache.commons/commons-exec/1.3/jars/commons-exec.jar:/Users/mohit/.ivy2/cache/oauth.signpost/signpost-core/jars/signpost-core-1.2.1.2.jar:/Users/mohit/.ivy2/cache/oauth.signpost/signpost-commonshttp4/jars/signpost-commonshttp4-1.2.1.2.jar:/Users/mohit/code/activator-dist-1.3.10/repository/net.sourceforge.nekohtml/nekohtml/1.9.22/jars/nekohtml.jar:/Users/mohit/code/activator-dist-1.3.10/repository/net.sourceforge.htmlunit/htmlunit-core-js/2.17/jars/htmlunit-core-js.jar:/Users/mohit/code/activator-dist-1.3.10/repository/net.sourceforge.htmlunit/htmlunit/2.20/jars/htmlunit.jar:/Users/mohit/code/activator-dist-1.3.10/repository/net.sourceforge.cssparser/cssparser/0.9.18/jars/cssparser.jar:/Users/mohit/code/activator-dist-1.3.10/repository/net.sf.ehcache/ehcache-core/2.6.11/jars/ehcache-core.jar:/Users/mohit/code/activator-dist-1.3.10/repository/net.java.dev.jna/jna-platform/4.1.0/jars/jna-platform.jar:/Users/mohit/code/activator-dist-1.3.10/repository/net.java.dev.jna/jna/4.1.0/jars/jna.jar:/Users/mohit/code/activator-dist-1.3.10/repository/junit/junit/4.12/jars/junit.jar:/Users/mohit/.ivy2/cache/joda-time/joda-time/jars/joda-time-2.9.4.jar:/Users/mohit/.ivy2/cache/javax.transaction/jta/jars/jta-1.1.jar:/Users/mohit/code/activator-dist-1.3.10/repository/javax.inject/javax.inject/1/jars/javax.inject.jar:/Users/mohit/.ivy2/cache/io.netty/netty-transport-native-epoll/jars/netty-transport-native-epoll-4.0.33.Final.jar:/Users/mohit/code/activator-dist-1.3.10/repository/io.netty/netty-transport/4.0.34.Final/jars/netty-transport.jar:/Users/mohit/code/activator-dist-1.3.10/repository/io.netty/netty-handler/4.0.34.Final/jars/netty-handler.jar:/Users/mohit/code/activator-dist-1.3.10/repository/io.netty/netty-common/4.0.34.Final/jars/netty-common.jar:/Users/mohit/code/activator-dist-1.3.10/repository/io.netty/netty-codec-http/4.0.34.Final/jars/netty-codec-http.jar:/Users/mohit/code/activator-dist-1.3.10/repository/io.netty/netty-codec/4.0.34.Final/jars/netty-codec.jar:/Users/mohit/code/activator-dist-1.3.10/repository/io.netty/netty-buffer/4.0.34.Final/jars/netty-buffer.jar:/Users/mohit/code/activator-dist-1.3.10/repository/commons-logging/commons-logging/1.2/jars/commons-logging.jar:/Users/mohit/.ivy2/cache/commons-logging/commons-logging/jars/commons-logging-1.1.1.jar:/Users/mohit/code/activator-dist-1.3.10/repository/commons-io/commons-io/2.4/jars/commons-io.jar:/Users/mohit/code/activator-dist-1.3.10/repository/commons-codec/commons-codec/1.10/jars/commons-codec.jar:/Users/mohit/.ivy2/cache/com.zaxxer/HikariCP-java6/bundles/HikariCP-java6-2.3.7.jar:/Users/mohit/.ivy2/cache/com.typesafe.slick/slick_2.11/bundles/slick_2.11-3.1.0.jar:/Users/mohit/.ivy2/cache/com.typesafe.slick/slick-hikaricp_2.11/bundles/slick-hikaricp_2.11-3.1.0.jar:/Users/mohit/code/activator-dist-1.3.10/repository/com.typesafe.play/twirl-api_2.11/1.1.1/jars/twirl-api_2.11.jar:/Users/mohit/code/activator-dist-1.3.10/repository/com.typesafe.play/play_2.11/2.5.0/jars/play_2.11.jar:/Users/mohit/code/activator-dist-1.3.10/repository/com.typesafe.play/play-ws_2.11/2.5.0/jars/play-ws_2.11.jar:/Users/mohit/code/activator-dist-1.3.10/repository/com.typesafe.play/play-test_2.11/2.5.0/jars/play-test_2.11.jar:/Users/mohit/code/activator-dist-1.3.10/repository/com.typesafe.play/play-streams_2.11/2.5.0/jars/play-streams_2.11.jar:/Users/mohit/.ivy2/cache/com.typesafe.play/play-slick_2.11/jars/play-slick_2.11-1.1.1.jar:/Users/mohit/.ivy2/cache/com.typesafe.play/play-slick-evolutions_2.11/jars/play-slick-evolutions_2.11-1.1.1.jar:/Users/mohit/code/activator-dist-1.3.10/repository/com.typesafe.play/play-server_2.11/2.5.0/jars/play-server_2.11.jar:/Users/mohit/code/activator-dist-1.3.10/repository/com.typesafe.play/play-netty-utils/2.5.0/jars/play-netty-utils.jar:/Users/mohit/code/activator-dist-1.3.10/repository/com.typesafe.play/play-netty-server_2.11/2.5.0/jars/play-netty-server_2.11.jar:/Users/mohit/.ivy2/cache/com.typesafe.play/play-logback_2.11/jars/play-logback_2.11-2.5.0.jar:/Users/mohit/.ivy2/cache/com.typesafe.play/play-json_2.11/jars/play-json_2.11-2.5.9.jar:/Users/mohit/.ivy2/cache/com.typesafe.play/play-jdbc-evolutions_2.11/jars/play-jdbc-evolutions_2.11-2.5.0.jar:/Users/mohit/.ivy2/cache/com.typesafe.play/play-jdbc-api_2.11/jars/play-jdbc-api_2.11-2.5.0.jar:/Users/mohit/.ivy2/cache/com.typesafe.play/play-iteratees_2.11/jars/play-iteratees_2.11-2.5.9.jar:/Users/mohit/.ivy2/cache/com.typesafe.play/play-functional_2.11/jars/play-functional_2.11-2.5.9.jar:/Users/mohit/code/activator-dist-1.3.10/repository/com.typesafe.play/play-exceptions/2.5.0/jars/play-exceptions.jar:/Users/mohit/.ivy2/cache/com.typesafe.play/play-datacommons_2.11/jars/play-datacommons_2.11-2.5.9.jar:/Users/mohit/.ivy2/cache/com.typesafe.play/play-cache_2.11/jars/play-cache_2.11-2.5.0.jar:/Users/mohit/code/activator-dist-1.3.10/repository/com.typesafe.play/build-link/2.5.0/jars/build-link.jar:/Users/mohit/code/activator-dist-1.3.10/repository/com.typesafe.netty/netty-reactive-streams-http/1.0.2/jars/netty-reactive-streams-http.jar:/Users/mohit/code/activator-dist-1.3.10/repository/com.typesafe.netty/netty-reactive-streams/1.0.2/jars/netty-reactive-streams.jar:/Users/mohit/code/activator-dist-1.3.10/repository/com.typesafe.akka/akka-stream_2.11/2.4.2/jars/akka-stream_2.11.jar:/Users/mohit/code/activator-dist-1.3.10/repository/com.typesafe.akka/akka-slf4j_2.11/2.4.2/jars/akka-slf4j_2.11.jar:/Users/mohit/code/activator-dist-1.3.10/repository/com.typesafe.akka/akka-actor_2.11/2.4.2/jars/akka-actor_2.11.jar:/Users/mohit/code/activator-dist-1.3.10/repository/com.typesafe/ssl-config-core_2.11/0.1.3/bundles/ssl-config-core_2.11.jar:/Users/mohit/code/activator-dist-1.3.10/repository/com.typesafe/ssl-config-akka_2.11/0.1.3/bundles/ssl-config-akka_2.11.jar:/Users/mohit/code/activator-dist-1.3.10/repository/com.typesafe/config/1.3.0/bundles/config.jar:/Users/mohit/code/activator-dist-1.3.10/repository/com.novocode/junit-interface/0.11/jars/junit-interface.jar:/Users/mohit/code/activator-dist-1.3.10/repository/com.h2database/h2/1.4.191/jars/h2.jar:/Users/mohit/code/activator-dist-1.3.10/repository/com.google.inject.extensions/guice-assistedinject/4.0/jars/guice-assistedinject.jar:/Users/mohit/code/activator-dist-1.3.10/repository/com.google.inject/guice/4.0/jars/guice.jar:/Users/mohit/code/activator-dist-1.3.10/repository/com.google.guava/guava/19.0/bundles/guava.jar:/Users/mohit/code/activator-dist-1.3.10/repository/com.google.code.gson/gson/2.3.1/jars/gson.jar:/Users/mohit/code/activator-dist-1.3.10/repository/com.google.code.findbugs/jsr305/3.0.1/jars/jsr305.jar:/Users/mohit/.ivy2/cache/com.fasterxml.jackson.datatype/jackson-datatype-jsr310/bundles/jackson-datatype-jsr310-2.7.6.jar:/Users/mohit/.ivy2/cache/com.fasterxml.jackson.datatype/jackson-datatype-jdk8/bundles/jackson-datatype-jdk8-2.7.6.jar:/Users/mohit/.ivy2/cache/com.fasterxml.jackson.core/jackson-databind/bundles/jackson-databind-2.7.6.jar:/Users/mohit/.ivy2/cache/com.fasterxml.jackson.core/jackson-core/bundles/jackson-core-2.7.6.jar:/Users/mohit/.ivy2/cache/com.fasterxml.jackson.core/jackson-annotations/bundles/jackson-annotations-2.7.6.jar:/Users/mohit/code/activator-dist-1.3.10/repository/ch.qos.logback/logback-core/1.1.4/jars/logback-core.jar:/Users/mohit/code/activator-dist-1.3.10/repository/ch.qos.logback/logback-classic/1.1.4/jars/logback-classic.jar:/Users/mohit/code/activator-dist-1.3.10/repository/cglib/cglib-nodep/2.1_3/jars/cglib-nodep.jar:/Users/mohit/code/activator-dist-1.3.10/repository/aopalliance/aopalliance/1.0/jars/aopalliance.jar:/Users/mohit/.ivy2/cache/com.clearspring.analytics/stream/jars/stream-2.7.0.jar:/Users/mohit/.ivy2/cache/com.esotericsoftware/kryo-shaded/bundles/kryo-shaded-3.0.3.jar:/Users/mohit/.ivy2/cache/com.esotericsoftware/minlog/bundles/minlog-1.3.0.jar:/Users/mohit/.ivy2/cache/com.fasterxml.jackson.module/jackson-module-paranamer/bundles/jackson-module-paranamer-2.6.5.jar:/Users/mohit/.ivy2/cache/com.fasterxml.jackson.module/jackson-module-scala_2.11/bundles/jackson-module-scala_2.11-2.6.5.jar:/Users/mohit/.ivy2/cache/com.github.fommil.netlib/core/jars/core-1.1.2.jar:/Users/mohit/.ivy2/cache/com.github.rwl/jtransforms/jars/jtransforms-2.4.0.jar:/Users/mohit/.ivy2/cache/com.google.protobuf/protobuf-java/bundles/protobuf-java-2.5.0.jar:/Users/mohit/.ivy2/cache/com.ning/compress-lzf/bundles/compress-lzf-1.0.3.jar:/Users/mohit/.ivy2/cache/com.thoughtworks.paranamer/paranamer/jars/paranamer-2.6.jar:/Users/mohit/.ivy2/cache/com.twitter/chill-java/jars/chill-java-0.8.0.jar:/Users/mohit/.ivy2/cache/com.twitter/chill_2.11/jars/chill_2.11-0.8.0.jar:/Users/mohit/.ivy2/cache/com.univocity/univocity-parsers/jars/univocity-parsers-2.1.1.jar:/Users/mohit/.ivy2/cache/commons-beanutils/commons-beanutils/jars/commons-beanutils-1.7.0.jar:/Users/mohit/.ivy2/cache/commons-beanutils/commons-beanutils-core/jars/commons-beanutils-core-1.8.0.jar:/Users/mohit/.ivy2/cache/commons-cli/commons-cli/jars/commons-cli-1.2.jar:/Users/mohit/.ivy2/cache/commons-collections/commons-collections/jars/commons-collections-3.2.1.jar:/Users/mohit/.ivy2/cache/commons-configuration/commons-configuration/jars/commons-configuration-1.6.jar:/Users/mohit/.ivy2/cache/commons-digester/commons-digester/jars/commons-digester-1.8.jar:/Users/mohit/.ivy2/cache/commons-httpclient/commons-httpclient/jars/commons-httpclient-3.1.jar:/Users/mohit/.ivy2/cache/commons-lang/commons-lang/jars/commons-lang-2.5.jar:/Users/mohit/.ivy2/cache/commons-net/commons-net/jars/commons-net-2.2.jar:/Users/mohit/.ivy2/cache/io.dropwizard.metrics/metrics-core/bundles/metrics-core-3.1.2.jar:/Users/mohit/.ivy2/cache/io.dropwizard.metrics/metrics-graphite/bundles/metrics-graphite-3.1.2.jar:/Users/mohit/.ivy2/cache/io.dropwizard.metrics/metrics-json/bundles/metrics-json-3.1.2.jar:/Users/mohit/.ivy2/cache/io.dropwizard.metrics/metrics-jvm/bundles/metrics-jvm-3.1.2.jar:/Users/mohit/code/activator-dist-1.3.10/repository/io.netty/netty/3.8.0.Final/bundles/netty.jar:/Users/mohit/.ivy2/cache/io.netty/netty-all/jars/netty-all-4.0.29.Final.jar:/Users/mohit/.ivy2/cache/javax.annotation/javax.annotation-api/jars/javax.annotation-api-1.2.jar:/Users/mohit/.ivy2/cache/javax.servlet/javax.servlet-api/jars/javax.servlet-api-3.1.0.jar:/Users/mohit/.ivy2/cache/javax.validation/validation-api/jars/validation-api-1.1.0.Final.jar:/Users/mohit/.ivy2/cache/javax.ws.rs/javax.ws.rs-api/jars/javax.ws.rs-api-2.0.1.jar:/Users/mohit/.ivy2/cache/log4j/log4j/bundles/log4j-1.2.17.jar:/Users/mohit/.ivy2/cache/net.java.dev.jets3t/jets3t/jars/jets3t-0.7.1.jar:/Users/mohit/.ivy2/cache/net.jpountz.lz4/lz4/jars/lz4-1.3.0.jar:/Users/mohit/.ivy2/cache/net.sf.opencsv/opencsv/jars/opencsv-2.3.jar:/Users/mohit/.ivy2/cache/net.sourceforge.f2j/arpack_combined_all/jars/arpack_combined_all-0.1.jar:/Users/mohit/.ivy2/cache/org.antlr/antlr4-runtime/jars/antlr4-runtime-4.5.3.jar:/Users/mohit/.ivy2/cache/org.apache.avro/avro/jars/avro-1.7.7.jar:/Users/mohit/.ivy2/cache/org.apache.avro/avro-ipc/jars/avro-ipc-1.7.7.jar:/Users/mohit/.ivy2/cache/org.apache.avro/avro-ipc/jars/avro-ipc-1.7.7-tests.jar:/Users/mohit/.ivy2/cache/org.apache.avro/avro-mapred/jars/avro-mapred-1.7.7-hadoop2.jar:/Users/mohit/.ivy2/cache/org.apache.commons/commons-compress/jars/commons-compress-1.4.1.jar:/Users/mohit/.ivy2/cache/org.apache.commons/commons-math/jars/commons-math-2.1.jar:/Users/mohit/.ivy2/cache/org.apache.commons/commons-math3/jars/commons-math3-3.4.1.jar:/Users/mohit/.ivy2/cache/org.apache.curator/curator-client/bundles/curator-client-2.4.0.jar:/Users/mohit/.ivy2/cache/org.apache.curator/curator-framework/bundles/curator-framework-2.4.0.jar:/Users/mohit/.ivy2/cache/org.apache.curator/curator-recipes/bundles/curator-recipes-2.4.0.jar:/Users/mohit/.ivy2/cache/org.apache.hadoop/hadoop-annotations/jars/hadoop-annotations-2.2.0.jar:/Users/mohit/.ivy2/cache/org.apache.hadoop/hadoop-auth/jars/hadoop-auth-2.2.0.jar:/Users/mohit/.ivy2/cache/org.apache.hadoop/hadoop-client/jars/hadoop-client-2.2.0.jar:/Users/mohit/.ivy2/cache/org.apache.hadoop/hadoop-common/jars/hadoop-common-2.2.0.jar:/Users/mohit/.ivy2/cache/org.apache.hadoop/hadoop-hdfs/jars/hadoop-hdfs-2.2.0.jar:/Users/mohit/.ivy2/cache/org.apache.hadoop/hadoop-mapreduce-client-app/jars/hadoop-mapreduce-client-app-2.2.0.jar:/Users/mohit/.ivy2/cache/org.apache.hadoop/hadoop-mapreduce-client-common/jars/hadoop-mapreduce-client-common-2.2.0.jar:/Users/mohit/.ivy2/cache/org.apache.hadoop/hadoop-mapreduce-client-core/jars/hadoop-mapreduce-client-core-2.2.0.jar:/Users/mohit/.ivy2/cache/org.apache.hadoop/hadoop-mapreduce-client-jobclient/jars/hadoop-mapreduce-client-jobclient-2.2.0.jar:/Users/mohit/.ivy2/cache/org.apache.hadoop/hadoop-mapreduce-client-shuffle/jars/hadoop-mapreduce-client-shuffle-2.2.0.jar:/Users/mohit/.ivy2/cache/org.apache.hadoop/hadoop-yarn-api/jars/hadoop-yarn-api-2.2.0.jar:/Users/mohit/.ivy2/cache/org.apache.hadoop/hadoop-yarn-client/jars/hadoop-yarn-client-2.2.0.jar:/Users/mohit/.ivy2/cache/org.apache.hadoop/hadoop-yarn-common/jars/hadoop-yarn-common-2.2.0.jar:/Users/mohit/.ivy2/cache/org.apache.hadoop/hadoop-yarn-server-common/jars/hadoop-yarn-server-common-2.2.0.jar:/Users/mohit/.ivy2/cache/org.apache.ivy/ivy/jars/ivy-2.4.0.jar:/Users/mohit/.ivy2/cache/org.apache.mesos/mesos/jars/mesos-0.21.1-shaded-protobuf.jar:/Users/mohit/.ivy2/cache/org.apache.parquet/parquet-column/jars/parquet-column-1.7.0.jar:/Users/mohit/.ivy2/cache/org.apache.parquet/parquet-common/jars/parquet-common-1.7.0.jar:/Users/mohit/.ivy2/cache/org.apache.parquet/parquet-encoding/jars/parquet-encoding-1.7.0.jar:/Users/mohit/.ivy2/cache/org.apache.parquet/parquet-format/jars/parquet-format-2.3.0-incubating.jar:/Users/mohit/.ivy2/cache/org.apache.parquet/parquet-generator/jars/parquet-generator-1.7.0.jar:/Users/mohit/.ivy2/cache/org.apache.parquet/parquet-hadoop/jars/parquet-hadoop-1.7.0.jar:/Users/mohit/.ivy2/cache/org.apache.parquet/parquet-jackson/jars/parquet-jackson-1.7.0.jar:/Users/mohit/.ivy2/cache/org.apache.xbean/xbean-asm5-shaded/bundles/xbean-asm5-shaded-4.4.jar:/Users/mohit/.ivy2/cache/org.apache.zookeeper/zookeeper/jars/zookeeper-3.4.5.jar:/Users/mohit/.ivy2/cache/org.codehaus.jackson/jackson-core-asl/jars/jackson-core-asl-1.9.13.jar:/Users/mohit/.ivy2/cache/org.codehaus.jackson/jackson-mapper-asl/jars/jackson-mapper-asl-1.9.13.jar:/Users/mohit/.ivy2/cache/org.codehaus.janino/commons-compiler/jars/commons-compiler-2.7.8.jar:/Users/mohit/.ivy2/cache/org.codehaus.janino/janino/jars/janino-2.7.8.jar:/Users/mohit/.ivy2/cache/org.fusesource.leveldbjni/leveldbjni-all/bundles/leveldbjni-all-1.8.jar:/Users/mohit/.ivy2/cache/org.glassfish.hk2/hk2-api/jars/hk2-api-2.4.0-b34.jar:/Users/mohit/.ivy2/cache/org.glassfish.hk2/hk2-locator/jars/hk2-locator-2.4.0-b34.jar:/Users/mohit/.ivy2/cache/org.glassfish.hk2/hk2-utils/jars/hk2-utils-2.4.0-b34.jar:/Users/mohit/.ivy2/cache/org.glassfish.hk2/osgi-resource-locator/jars/osgi-resource-locator-1.0.1.jar:/Users/mohit/.ivy2/cache/org.glassfish.hk2.external/aopalliance-repackaged/jars/aopalliance-repackaged-2.4.0-b34.jar:/Users/mohit/.ivy2/cache/org.glassfish.hk2.external/javax.inject/jars/javax.inject-2.4.0-b34.jar:/Users/mohit/.ivy2/cache/org.glassfish.jersey.bundles.repackaged/jersey-guava/bundles/jersey-guava-2.22.2.jar:/Users/mohit/.ivy2/cache/org.glassfish.jersey.containers/jersey-container-servlet/jars/jersey-container-servlet-2.22.2.jar:/Users/mohit/.ivy2/cache/org.glassfish.jersey.containers/jersey-container-servlet-core/jars/jersey-container-servlet-core-2.22.2.jar:/Users/mohit/.ivy2/cache/org.glassfish.jersey.core/jersey-client/jars/jersey-client-2.22.2.jar:/Users/mohit/.ivy2/cache/org.glassfish.jersey.core/jersey-common/jars/jersey-common-2.22.2.jar:/Users/mohit/.ivy2/cache/org.glassfish.jersey.core/jersey-server/jars/jersey-server-2.22.2.jar:/Users/mohit/.ivy2/cache/org.glassfish.jersey.media/jersey-media-jaxb/jars/jersey-media-jaxb-2.22.2.jar:/Users/mohit/.ivy2/cache/org.jpmml/pmml-model/jars/pmml-model-1.2.15.jar:/Users/mohit/.ivy2/cache/org.jpmml/pmml-schema/jars/pmml-schema-1.2.15.jar:/Users/mohit/.ivy2/cache/org.json4s/json4s-ast_2.11/jars/json4s-ast_2.11-3.2.11.jar:/Users/mohit/.ivy2/cache/org.json4s/json4s-core_2.11/jars/json4s-core_2.11-3.2.11.jar:/Users/mohit/.ivy2/cache/org.json4s/json4s-jackson_2.11/jars/json4s-jackson_2.11-3.2.11.jar:/Users/mohit/.ivy2/cache/org.mortbay.jetty/jetty-util/jars/jetty-util-6.1.26.jar:/Users/mohit/.ivy2/cache/org.objenesis/objenesis/jars/objenesis-2.1.jar:/Users/mohit/.ivy2/cache/org.roaringbitmap/RoaringBitmap/bundles/RoaringBitmap-0.5.11.jar:/Users/mohit/.ivy2/cache/org.scala-lang/scala-compiler/jars/scala-compiler-2.11.8.jar:/Users/mohit/.ivy2/cache/org.scala-lang/scala-reflect/jars/scala-reflect-2.11.8.jar:/Users/mohit/.ivy2/cache/org.scala-lang/scalap/jars/scalap-2.11.0.jar:/Users/mohit/.ivy2/cache/org.scala-lang.modules/scala-xml_2.11/bundles/scala-xml_2.11-1.0.4.jar:/Users/mohit/.ivy2/cache/org.scalanlp/breeze-macros_2.11/jars/breeze-macros_2.11-0.11.2.jar:/Users/mohit/.ivy2/cache/org.scalanlp/breeze_2.11/jars/breeze_2.11-0.11.2.jar:/Users/mohit/.ivy2/cache/org.slf4j/slf4j-log4j12/jars/slf4j-log4j12-1.7.16.jar:/Users/mohit/.ivy2/cache/org.spark-project.spark/unused/jars/unused-1.0.0.jar:/Users/mohit/.ivy2/cache/org.spire-math/spire-macros_2.11/jars/spire-macros_2.11-0.7.4.jar:/Users/mohit/.ivy2/cache/org.spire-math/spire_2.11/jars/spire_2.11-0.7.4.jar:/Users/mohit/.ivy2/cache/org.tukaani/xz/jars/xz-1.0.jar:/Users/mohit/.ivy2/cache/oro/oro/jars/oro-2.0.8.jar:/Users/mohit/.ivy2/cache/xmlenc/xmlenc/jars/xmlenc-0.52.jar:/Users/mohit/.ivy2/cache/net.razorvine/pyrolite/jars/pyrolite-4.9.jar:/Applications/IntelliJ IDEA CE.app/Contents/lib/idea_rt.jar"" com.intellij.rt.execution.application.AppMain org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner -s dag.execution.SparkReplSpec -showProgressMessages true -C org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestReporter
Testing started at 2:40 PM ...
java.lang.NoSuchMethodError: scala.reflect.internal.Symbols$Symbol.isDeferredNotJavaDefault()Z
	at scala.tools.nsc.typechecker.RefChecks$RefCheckTransformer$$anonfun$12.apply(RefChecks.scala:607)
	at scala.tools.nsc.typechecker.RefChecks$RefCheckTransformer$$anonfun$12.apply(RefChecks.scala:607)
	at scala.collection.TraversableLike$$anonfun$partition$1.apply(TraversableLike.scala:314)
	at scala.collection.TraversableLike$$anonfun$partition$1.apply(TraversableLike.scala:314)
	at scala.reflect.internal.Scopes$Scope.foreach(Scopes.scala:373)
	at scala.collection.TraversableLike$class.partition(TraversableLike.scala:314)
	at scala.reflect.internal.Scopes$Scope.partition(Scopes.scala:51)
	at scala.tools.nsc.typechecker.RefChecks$RefCheckTransformer.checkNoAbstractMembers$1(RefChecks.scala:607)
	at scala.tools.nsc.typechecker.RefChecks$RefCheckTransformer.checkAllOverrides(RefChecks.scala:752)
	at scala.tools.nsc.typechecker.RefChecks$RefCheckTransformer.transform(RefChecks.scala:1672)
	at scala.tools.nsc.typechecker.RefChecks$RefCheckTransformer.transform(RefChecks.scala:111)
	at scala.reflect.api.Trees$Transformer.transformTemplate(Trees.scala:2563)
	at scala.reflect.internal.Trees$$anonfun$itransform$4.apply(Trees.scala:1408)
	at scala.reflect.internal.Trees$$anonfun$itransform$4.apply(Trees.scala:1407)
	at scala.reflect.api.Trees$Transformer.atOwner(Trees.scala:2600)
	at scala.reflect.internal.Trees$class.itransform(Trees.scala:1406)
	at scala.reflect.internal.SymbolTable.itransform(SymbolTable.scala:16)
	at scala.reflect.internal.SymbolTable.itransform(SymbolTable.scala:16)
	at scala.reflect.api.Trees$Transformer.transform(Trees.scala:2555)
	at scala.tools.nsc.typechecker.RefChecks$RefCheckTransformer.transform(RefChecks.scala:1785)
	at scala.tools.nsc.typechecker.RefChecks$RefCheckTransformer.transformStat(RefChecks.scala:1262)
	at scala.tools.nsc.typechecker.RefChecks$RefCheckTransformer$$anonfun$transformStats$1.apply(RefChecks.scala:1165)
	at scala.tools.nsc.typechecker.RefChecks$RefCheckTransformer$$anonfun$transformStats$1.apply(RefChecks.scala:1165)
	at scala.collection.immutable.List.flatMap(List.scala:327)
	at scala.tools.nsc.typechecker.RefChecks$RefCheckTransformer.transformStats(RefChecks.scala:1165)
	at scala.tools.nsc.typechecker.RefChecks$RefCheckTransformer.transformStats(RefChecks.scala:111)
	at scala.reflect.internal.Trees$$anonfun$itransform$7.apply(Trees.scala:1426)
	at scala.reflect.internal.Trees$$anonfun$itransform$7.apply(Trees.scala:1426)
	at scala.reflect.api.Trees$Transformer.atOwner(Trees.scala:2600)
	at scala.reflect.internal.Trees$class.itransform(Trees.scala:1425)
	at scala.reflect.internal.SymbolTable.itransform(SymbolTable.scala:16)
	at scala.reflect.internal.SymbolTable.itransform(SymbolTable.scala:16)
	at scala.reflect.api.Trees$Transformer.transform(Trees.scala:2555)
	at scala.tools.nsc.typechecker.RefChecks$RefCheckTransformer.transform(RefChecks.scala:1785)
	at scala.tools.nsc.typechecker.RefChecks$RefCheckTransformer.transform(RefChecks.scala:111)
	at scala.tools.nsc.ast.Trees$Transformer.transformUnit(Trees.scala:147)
	at scala.tools.nsc.transform.Transform$Phase.apply(Transform.scala:30)
	at scala.tools.nsc.Global$GlobalPhase$$anonfun$applyPhase$1.apply$mcV$sp(Global.scala:440)
	at scala.tools.nsc.Global$GlobalPhase.withCurrentUnit(Global.scala:431)
	at scala.tools.nsc.Global$GlobalPhase.applyPhase(Global.scala:440)
	at scala.tools.nsc.Global$GlobalPhase$$anonfun$run$1.apply(Global.scala:398)
	at scala.tools.nsc.Global$GlobalPhase$$anonfun$run$1.ap"
"
Nathan Lande <nathanlande@gmail.com>,Wed"," 16 Nov 2016 14:54:26 -0800""","Re: How do I convert json_encoded_blob_column into a data frame?
 (This may be a feature request)",kant kodali <kanth909@gmail.com>,"I'm looking forward to 2.1 but, in the meantime, you can pull out the
specific column into an RDD of JSON objects, pass this RDD into the
read.json() and then join the results back onto your initial DF.

Here is an example of what we do to unpack headers from Avro log data:

def jsonLoad(path):
    #
    #load in the df
    raw = (sqlContext.read
            .format('com.databricks.spark.avro')
            .load(path)
        )
    #
    #define json blob, add primary key elements (hi and lo)
    #
    JSONBlob = concat(
        lit('{'),
        concat(lit('""lo"":'), col('header.eventId.lo').cast('string'),
lit(',')),
        concat(lit('""hi"":'), col('header.eventId.hi').cast('string'),
lit(',')),
        concat(lit('""response"":'), decode('requestResponse.response',
'UTF-8')),
        lit('}')
    )
    #
    #extract the JSON blob as a string
    rawJSONString = raw.select(JSONBlob).rdd.map(lambda x: str(x[0]))
    #
    #transform the JSON string into a DF struct object
    structuredJSON = sqlContext.read.json(rawJSONString)
    #
    #join the structured JSON back onto the initial DF using the hi and lo
join keys
    final = (raw.join(structuredJSON,
                ((raw['header.eventId.lo'] == structuredJSON['lo']) &
(raw['header.eventId.hi'] == structuredJSON['hi'])),
                'left_outer')
            .drop('hi')
            .drop('lo')
        )
    #
    #win
    return final


"
kant kodali <kanth909@gmail.com>,"Wed, 16 Nov 2016 16:39:09 -0800","Re: How do I convert json_encoded_blob_column into a data frame?
 (This may be a feature request)",Nathan Lande <nathanlande@gmail.com>,"1. I have a Cassandra Table where one of the columns is blob. And this blob
contains a JSON encoded String however not all the blob's across the
Cassandra table for that column are same (some blobs have difference json's
than others) so In that case what is the best way to approach it? Do we
need to put /group all the JSON Blobs that have same structure (same keys)
into each individual data frame? For example, say if I have 5 json blobs
that have same structure and another 3 JSON blobs that belongs to some
other structure In this case do I need to create two data frames? (Attached
is a screen shot of 2 rows of how my json looks like)
2. In my case, toJSON on RDD doesn't seem to help a lot. Attached a screen
shot. Looks like I got the same data frame as my original one.

Thanks much for these examples.





---------------------------------------------------------------------"
wszxyh <wszxyh@163.com>,"Thu, 17 Nov 2016 10:58:52 +0800 (CST)",Multiple streaming aggregations in structured streaming,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi


Multiple streaming aggregations are not yet supported. When will it be supported? Is it in the plan?


Thanks"
Nathan Lande <nathanlande@gmail.com>,"Wed, 16 Nov 2016 20:41:00 -0800","Re: How do I convert json_encoded_blob_column into a data frame?
 (This may be a feature request)",kant kodali <kanth909@gmail.com>,"If you are dealing with a bunch of different schemas in 1 field, figuring
out a strategy to deal with that will depend on your data and does not
really have anything to do with spark since mapping your JSON payloads to
tractable data structures will depend on business logic.

The strategy of pulling out a blob into its on rdd and feeding it into the
JSON loader should work for any data source once you have your data
strategy figured out.


"
Niranda Perera <niranda.perera@gmail.com>,"Thu, 17 Nov 2016 12:14:37 +0530",SQL Syntax for pivots,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,

I see that the pivot functionality is being added to spark DFs from 1.6
onward.

I am interested to see if there is a Spark SQL syntax available for
pivoting? example: Slide 11 of [1]

*pandas (Python) - pivot_table(df, values='D', index=['A', 'B'],
columns=['C'], aggfunc=np.sum) *

*reshape2 (R) - dcast(df, A + B ~ C, sum) *

*Oracle 11g - SELECT * FROM df PIVOT (sum(D) FOR C IN ('small', 'large')) p*


Best

[1]
http://www.slideshare.net/SparkSummit/pivoting-data-with-sparksql-by-andrew-ray

-- 
Niranda Perera
@n1r44 <https://twitter.com/N1R44>
+94 71 554 8430
https://www.linkedin.com/in/niranda
https://pythagoreanscript.wordpress.com/
"
Reynold Xin <rxin@databricks.com>,"Wed, 16 Nov 2016 22:49:29 -0800",Re: SQL Syntax for pivots,Niranda Perera <niranda.perera@gmail.com>,"Not right now.



"
Reynold Xin <rxin@databricks.com>,"Wed, 16 Nov 2016 23:20:25 -0800",issues with github pull request notification emails missing,"""dev@spark.apache.org"" <dev@spark.apache.org>","I've noticed that a lot of github pull request notifications no longer come
to my inbox. In the past I'd get an email for every reply to a pull request
that I subscribed to (i.e. commented on). Lately I noticed for a lot of
them I didn't get any emails, but if I opened the pull requests directly on
github, I'd see the new replies. I've looked at spam folder and none of the
missing notifications are there. So it's either github not sending the
notifications, or the emails are lost in transit.

The way it manifests is that I often comment on a pull request, and then I
don't know whether the contributor (author) has updated it or not. From the
contributor's point of view, it looks like I've been ignoring the pull
request.

I think this started happening when github switched over to the new code
review mode (
https://github.com/blog/2256-a-whole-new-github-universe-announcing-new-tools-forums-and-features
)


Did anybody else notice this issue?
"
Holden Karau <holden@pigscanfly.ca>,"Thu, 17 Nov 2016 07:30:18 +0000",Re: issues with github pull request notification emails missing,"Reynold Xin <rxin@databricks.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","+1 it seems like I'm missing a number of my GitHub email notifications
lately (although since I run my own mail server and forward I've been
assuming it's my own fault).


I've also had issues with having greatly delayed notifications on some of
my own pu"
"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Thu, 17 Nov 2016 01:19:31 -0700 (MST)",structured streaming and window functions,dev@spark.apache.org,"Hi,
I have been trying to figure out how structured streaming handles window functions efficiently.
The portion I understand is that whenever new data arrived, it is grouped by the time and the aggregated data is added to the state.
However, unlike operations like sum etc. window functions need the original data and can change when data arrives late.
So if I understand correctly, this would mean that we would have to save the original data and rerun on it to calculate the window function every time new data arrives.
Is this correct? Are there ways to go around this issue?

Assaf.




--"
kant kodali <kanth909@gmail.com>,"Thu, 17 Nov 2016 01:28:30 -0800",Another Interesting Question on SPARK SQL,"""user @spark"" <user@spark.apache.org>, dev <dev@spark.apache.org>","â€‹
Which parts in the diagram above are executed by DataSource connectors and
which parts are executed by Tungsten? or to put it in another way which
phase in the diagram above does Tungsten leverages the Datasource
connectors (such as say cassandra connector ) ?

My understanding so far is that connectors come in during Physical planning
phase but I am not sure if the connectors take logical plan as an input?

Thanks,
kant
"
=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@databricks.com>,"Thu, 17 Nov 2016 03:17:15 -0800",Re: Another Interesting Question on SPARK SQL,kant kodali <kanth909@gmail.com>,"The diagram you have included, is a depiction of the steps Catalyst (the
spark optimizer) takes to create an executable plan. Tungsten mainly comes
into play during code generation and the actual execution.

A datasource is represented by a LogicalRelation during analysis &
optimization. The spark planner takes such a LogicalRelation and plans it
as either RowDataSourceScanExec or an BatchedDataSourceScanExec depending
on the datasource. Both scan nodes support whole stage code generation.

HTH



d
n
"
=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@databricks.com>,"Thu, 17 Nov 2016 03:27:16 -0800",Re: structured streaming and window functions,"""assaf.mendelson"" <assaf.mendelson@rsa.com>","What kind of window functions are we talking about? Structured streaming
only supports time window aggregates, not the more general sql window
function (sum(x) over (partition by ... order by ...)) aggregates.

The basic idea is that you use incremental aggregation and store the
aggregation buffer (not the end result) in a state store after each
increment. When an new batch comes in, you perform aggregation on that
batch, merge the result of that aggregation with the buffer in the state
store, update the state store and return the new result.

This is much harder than it sounds, because you need to maintain state in a
fault tolerant way and you need to have some eviction policy (watermarks
for instance) for aggregation buffers to prevent the state store from
reaching an infinite size.


"
"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Thu, 17 Nov 2016 05:52:48 -0700 (MST)",RE: structured streaming and window functions,dev@spark.apache.org,"Is there a plan to support sql window functions?
I will give an example of use: Letâ€™s say we have login logs. What we want to do is for each user we would want to add the number of failed logins for each successful login. How would you do it with structured streaming?
As this is currently not supported, is there a plan on how to support it in the future?
Assaf.

From: Herman van HÃ¶vell tot Westerflier-2 [via Apache Spark Developers List] [mailto:ml-node+s1001551n19933h12@n3.nabble.com]
Sent: Thursday, November 17, 2016 1:27 PM
To: Mendelson, Assaf
Subject: Re: structured streaming and window functions

What kind of window functions are we talking about? Structured streaming only supports time window aggregates, not the more general sql window function (sum(x) over (partition by ... order by ...)) aggregates.

The basic idea is that you use incremental aggregation and store the aggregation buffer (not the end result) in a state store after each increment. When an new batch comes in, you perform aggregation on that batch, merge the result of that aggregation with the buffer in the state store, update the state store and return the new result.

This is much harder than it sounds, because you need to maintain state in a fault tolerant way and you need to have some eviction policy (watermarks for instance) for aggregation buffers to prevent the state store from reaching an infinite size.

Hi,
I have been trying to figure out how structured streaming handles window functions efficiently.
The portion I understand is that whenever new data arrived, it is grouped by the time and the aggregated data is added to the state.
However, unlike operations like sum etc. window functions need the original data and can change when data arrives late.
So if I understand correctly, this would mean that we would have to save the original data and rerun on it to calculate the window function every time new data arrives.
Is this correct? Are there ways to go around this issue?

Assaf.

________________________________
View this message in context: structured streaming and window functions<http://apache-spark-developers-list.1001551.n3.nabble.com/structured-streaming-and-window-functions-tp19930.html>
he-spark-developers-list.1001551.n3.nabble.com/> at Nabble.com.


________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/structured-streaming-and-window-functions-tp19930p19933.html
To start a new topic under Apache Spark Developers List, email ml-node+s1001551n1h20@n3.nabble.com<mailto:ml-node+s1001551n1h20@n3.nabble.com>
spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=1&code=YXNzYWYubWVuZGVsc29uQHJzYS5jb218MXwtMTI4OTkxNTg1Mg==>.
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>




--
3.nabble.com/structured-streaming-and-window-functions-tp19930p19934.html
om."
Ofir Manor <ofir.manor@equalum.io>,"Thu, 17 Nov 2016 17:12:34 +0200",Re: structured streaming and window functions,"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Assaf, I think what you are describing is actually sessionizing, by user,
where a session is ended by a successful login event.
If so, this is tracked by https://issues.apache.org/jira/browse/SPARK-10816
(didn't start yet)

Ofir Manor

Co-Founder & CTO | Equalum

Mobile: +972-54-7801286 | Email: ofir.manor@equalum.io


 we want
or
pers
s
eaming-and-window-functions-tp19930.html>
ervlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
eaming-and-window-functions-tp19930p19934.html>
"
"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Thu, 17 Nov 2016 08:16:30 -0700 (MST)",RE: structured streaming and window functions,dev@spark.apache.org,"It is true that this is sessionizing but I brought it as an example for finding an ordered pattern in the data.
In general, using simple window (e.g. 24 hours) in structured streaming is explain in the grouping by time and is very clear.
What I was trying to figure out is how to do streaming of cases where you actually have to have some sorting to find patterns, especially when some of the data may come in late.
I was trying to figure out if there is plan to support this and if so, what would be the performance implications.
Assaf.

From: Ofir Manor [via Apache Spark Developers List] [mailto:ml-node+s1001551n19935h74@n3.nabble.com]
Sent: Thursday, November 17, 2016 5:13 PM
To: Mendelson, Assaf
Subject: Re: structured streaming and window functions

Assaf, I think what you are describing is actually sessionizing, by user, where a session is ended by a successful login event.
If so, this is tracked by https://issues.apache.org/jira/browse/SPARK-10816 (didn't start yet)


Ofir Manor

Co-Founder & CTO | Equalum

Mobile: <a href=""tel:%2B972-54-7801286"" value=""+972507470820"" target=""_blank"">+972-54-7801286 | Email: [hidden email]</user/SendEmail.jtp?type=node&node=19935&i=0>

Is there a plan to support sql window functions?
I will give an example of use: Letâ€™s say we have login logs. What we want to do is for each user we would want to add the number of failed logins for each successful login. How would you do it with structured streaming?
As this is currently not supported, is there a plan on how to support it in the future?
Assaf.

From: Herman van HÃ¶vell tot Westerflier-2 [via Apache Spark Developers List] [mailto:[hidden email]</user/SendEmail.jtp?type=node&node=19935&i=2>[hidden email]<http://user/SendEmail.jtp?type=node&node=19934&i=0>]
Sent: Thursday, November 17, 2016 1:27 PM
To: Mendelson, Assaf
Subject: Re: structured streaming and window functions

What kind of window functions are we talking about? Structured streaming only supports time window aggregates, not the more general sql window function (sum(x) over (partition by ... order by ...)) aggregates.

The basic idea is that you use incremental aggregation and store the aggregation buffer (not the end result) in a state store after each increment. When an new batch comes in, you perform aggregation on that batch, merge the result of that aggregation with the buffer in the state store, update the state store and return the new result.

This is much harder than it sounds, because you need to maintain state in a fault tolerant way and you need to have some eviction policy (watermarks for instance) for aggregation buffers to prevent the state store from reaching an infinite size.

Hi,
I have been trying to figure out how structured streaming handles window functions efficiently.
The portion I understand is that whenever new data arrived, it is grouped by the time and the aggregated data is added to the state.
However, unlike operations like sum etc. window functions need the original data and can change when data arrives late.
So if I understand correctly, this would mean that we would have to save the original data and rerun on it to calculate the window function every time new data arrives.
Is this correct? Are there ways to go around this issue?

Assaf.

________________________________
View this message in context: structured streaming and window functions<http://apache-spark-developers-list.1001551.n3.nabble.com/structured-streaming-and-window-functions-tp19930.html>
he-spark-developers-list.1001551.n3.nabble.com/> at Nabble.com.


________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/structured-streaming-and-window-functions-tp19930p19933.html
To start a new topic under Apache Spark Developers List, email [hidden email]<http://user/SendEmail.jtp?type=node&node=19934&i=1>
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>

________________________________
View this message in context: RE: structured streaming and window functions<http://apache-spark-developers-list.1001551.n3.nabble.com/structured-streaming-and-window-functions-tp19930p19934.html>

he-spark-developers-list.1001551.n3.nabble.com/> at Nabble.com.


________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/structured-streaming-and-window-functions-tp19930p19935.html
To start a new topic under Apache Spark Developers List, email ml-node+s1001551n1h20@n3.nabble.com<mailto:ml-node+s1001551n1h20@n3.nabble.com>
spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=1&code=YXNzYWYubWVuZGVsc29uQHJzYS5jb218MXwtMTI4OTkxNTg1Mg==>.
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>




--
3.nabble.com/structured-streaming-and-window-functions-tp19930p19936.html
om."
Ofir Manor <ofir.manor@equalum.io>,"Thu, 17 Nov 2016 17:57:01 +0200",Re: structured streaming and window functions,"""assaf.mendelson"" <assaf.mendelson@rsa.com>","I agree with you, I think that once we will have sessionization, we could
aim for richer processing capabilities per session. As far as I image it, a
session is an ordered sequence of data, that we could apply computation on
it (like CEP).


Ofir Manor

Co-Founder & CTO | Equalum

Mobile: +972-54-7801286 | Email: ofir.manor@equalum.io


s
dden
6""
n
 we want
or
pers
s
eaming-and-window-functions-tp19930.html>
ervlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
eaming-and-window-functions-tp19930p19934.html>
ervlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
eaming-and-window-functions-tp19930p19936.html>
"
Mohit Jaggi <mohitjaggi@gmail.com>,"Thu, 17 Nov 2016 08:47:08 -0800",Fwd: SparkILoop doesn't run,Dev <dev@spark.apache.org>,"I am trying to use SparkILoop to write some tests(shown below) but the test
hangs with the following stack trace. Any idea what is going on?


import org.apache.log4j.{Level, LogManager}
import org.apache.spark.repl.SparkILoop
import org.scalatest.{BeforeAndAfterAll, FunSuite}

class SparkReplSpec extends FunSuite with BeforeAndAfterAll {

  override def beforeAll(): Unit = {
  }

  override def afterAll(): Unit = {
  }

  test(""yay!"") {
    val rootLogger = LogManager.getRootLogger
    val logLevel = rootLogger.getLevel
    rootLogger.setLevel(Level.ERROR)

    val output = SparkILoop.run(
      """"""
        |println(""hello"")
      """""".stripMargin)

    println(s""[[[[ $output ]]]]"")

  }
}


/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/bin/java
-Dspark.master=local[*] -Didea.launcher.port=7532
""-Didea.launcher.bin.path=/Applications/IntelliJ IDEA CE.app/Contents/bin""
-Dfile.encoding=UTF-8 -classpath ""/Users/mohit/Library/Application
Support/IdeaIC2016.2/Scala/lib/scala-plugin-runners.jar:/Library/Java/
JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/
charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_
66.jdk/Contents/Home/jre/lib/deploy.jar:/Library/Java/
JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/
ext/cldrdata.jar:/Library/Java/JavaVirtualMachines/jdk1.
8.0_66.jdk/Contents/Home/jre/lib/ext/dnsns.jar:/Library/
Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/
lib/ext/jaccess.jar:/Library/Java/JavaVirtualMachines/jdk1.
8.0_66.jdk/Contents/Home/jre/lib/ext/jfxrt.jar:/Library/
Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/
lib/ext/localedata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_
66.jdk/Contents/Home/jre/lib/ext/nashorn.jar:/Library/Java/
JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/
ext/sunec.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_
66.jdk/Contents/Home/jre/lib/ext/sunjce_provider.jar:/Library/Java/
JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/
ext/sunpkcs11.jar:/Library/Java/JavaVirtualMachines/jdk1.
8.0_66.jdk/Contents/Home/jre/lib/ext/zipfs.jar:/Library/
Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/
lib/javaws.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_
66.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/
JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/
jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_
66.jdk/Contents/Home/jre/lib/jfxswt.jar:/Library/Java/
JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/
jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_
66.jdk/Contents/Home/jre/lib/management-agent.jar:/Library/
Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/
lib/plugin.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_
66.jdk/Contents/Home/jre/lib/resources.jar:/Library/Java/
JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/
rt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_
66.jdk/Contents/Home/lib/ant-javafx.jar:/Library/Java/
JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/lib/dt.jar:/Library/Java/
JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/lib/
javafx-mx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_
66.jdk/Contents/Home/lib/jconsole.jar:/Library/Java/
JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/lib/
packager.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_
66.jdk/Contents/Home/lib/sa-jdi.jar:/Library/Java/
JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/lib/
tools.jar:/Users/mohit/code/datagears-play/target/scala-2.
11/test-classes:/Users/mohit/code/datagears-play/target/
scala-2.11/classes:/Users/mohit/code/datagears-play/
macros/target/scala-2.11/classes:/Users/mohit/.ivy2/cache/org.xerial.snappy/
snappy-java/bundles/snappy-java-1.1.2.4.jar:/Users/mohit/
.ivy2/cache/org.apache.spark/spark-unsafe_2.11/jars/spark-
unsafe_2.11-2.0.0.jar:/Users/mohit/.ivy2/cache/org.apache.
spark/spark-tags_2.11/jars/spark-tags_2.11-2.0.0.jar:/
Users/mohit/.ivy2/cache/org.apache.spark/spark-streaming_
2.11/jars/spark-streaming_2.11-2.0.0.jar:/Users/mohit/.
ivy2/cache/org.apache.spark/spark-sql_2.11/jars/spark-sql_
2.11-2.0.0.jar:/Users/mohit/.ivy2/cache/org.apache.spark/
spark-sketch_2.11/jars/spark-sketch_2.11-2.0.0.jar:/Users/
mohit/.ivy2/cache/org.apache.spark/spark-repl_2.11/jars/
spark-repl_2.11-2.0.0.jar:/Users/mohit/.ivy2/cache/org.
apache.spark/spark-network-shuffle_2.11/jars/spark-
network-shuffle_2.11-2.0.0.jar:/Users/mohit/.ivy2/cache/
org.apache.spark/spark-network-common_2.11/jars/
spark-network-common_2.11-2.0.0.jar:/Users/mohit/.ivy2/
cache/org.apache.spark/spark-mllib_2.11/jars/spark-mllib_2.
11-2.0.0.jar:/Users/mohit/.ivy2/cache/org.apache.spark/
spark-mllib-local_2.11/jars/spark-mllib-local_2.11-2.0.0.
jar:/Users/mohit/.ivy2/cache/org.apache.spark/spark-
launcher_2.11/jars/spark-launcher_2.11-2.0.0.jar:/
Users/mohit/.ivy2/cache/org.apache.spark/spark-graphx_2.
11/jars/spark-graphx_2.11-2.0.0.jar:/Users/mohit/.ivy2/
cache/org.apache.spark/spark-core_2.11/jars/spark-core_2.
11-2.0.0.jar:/Users/mohit/.ivy2/cache/org.apache.spark/
spark-catalyst_2.11/jars/spark-catalyst_2.11-2.0.0.jar:
/Users/mohit/.ivy2/cache/net.sf.py4j/py4j/jars/py4j-0.10.1.
jar:/Users/mohit/.ivy2/cache/xml-apis/xml-apis/jars/xml-
apis-1.4.01.jar:/Users/mohit/.ivy2/cache/xerces/xercesImpl/
jars/xercesImpl-2.11.0.jar:/Users/mohit/code/activator-
dist-1.3.10/repository/xalan/xalan/2.7.2/jars/xalan.jar:/
Users/mohit/code/activator-dist-1.3.10/repository/xalan/
serializer/2.7.2/jars/serializer.jar:/Users/mohit/
code/activator-dist-1.3.10/repository/org.webbitserver/
webbit/0.4.14/jars/webbit.jar:/Users/mohit/code/activator-
dist-1.3.10/repository/org.w3c.css/sac/1.3/jars/sac.jar:/
Users/mohit/code/activator-dist-1.3.10/repository/org.
slf4j/slf4j-api/1.7.16/jars/slf4j-api.jar:/Users/mohit/
code/activator-dist-1.3.10/repository/org.slf4j/jul-to-
slf4j/1.7.16/jars/jul-to-slf4j.jar:/Users/mohit/code/activator-dist-1.3.10/
repository/org.slf4j/jcl-over-slf4j/1.7.16/jars/jcl-over-
slf4j.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.seleniumhq.
selenium/selenium-support/2.48.2/jars/selenium-support.
jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.seleniumhq.
selenium/selenium-safari-driver/2.48.2/jars/selenium-
safari-driver.jar:/Users/mohit/code/activator-dist-1.3.
10/repository/org.seleniumhq.selenium/selenium-remote-
driver/2.48.2/jars/selenium-remote-driver.jar:/Users/
mohit/code/activator-dist-1.3.10/repository/org.seleniumhq.
selenium/selenium-leg-rc/2.48.2/jars/selenium-leg-rc.jar:/
Users/mohit/code/activator-dist-1.3.10/repository/org.
seleniumhq.selenium/selenium-java/2.48.2/jars/selenium-
java.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.seleniumhq.
selenium/selenium-ie-driver/2.48.2/jars/selenium-ie-driver.
jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.seleniumhq.
selenium/selenium-htmlunit-driver/2.48.2/jars/selenium-
htmlunit-driver.jar:/Users/mohit/code/activator-dist-1.3.
10/repository/org.seleniumhq.selenium/selenium-firefox-
driver/2.48.2/jars/selenium-firefox-driver.jar:/Users/
mohit/code/activator-dist-1.3.10/repository/org.seleniumhq.
selenium/selenium-edge-driver/2.48.2/jars/selenium-edge-
driver.jar:/Users/mohit/code/activator-dist-1.3.10/
repository/org.seleniumhq.selenium/selenium-chrome-
driver/2.48.2/jars/selenium-chrome-driver.jar:/Users/
mohit/code/activator-dist-1.3.10/repository/org.seleniumhq.
selenium/selenium-api/2.48.2/jars/selenium-api.jar:/Users/
mohit/.ivy2/cache/org.scalatestplus.play/scalatestplus-play_2.11/jars/
scalatestplus-play_2.11-1.5.0-RC1.jar:/Users/mohit/code/
activator-dist-1.3.10/repository/org.scalatest/scalatest_2.11/2.2.6/bundles/
scalatest_2.11.jar:/Users/mohit/.ivy2/cache/org.scalacheck/scalacheck_2.11/
jars/scalacheck_2.11-1.12.5.jar:/Users/mohit/.ivy2/cache/
org.scala-stm/scala-stm_2.11/jars/scala-stm_2.11-0.7.jar:/
Users/mohit/.ivy2/cache/org.scala-sbt/test-interface/jars/
test-interface-1.0.jar:/Users/mohit/.ivy2/cache/org.scala-
lang.modules/scala-xml_2.11/bundles/scala-xml_2.11-1.0.1.
jar:/Users/mohit/.ivy2/cache/org.scala-lang.modules/scala-
parser-combinators_2.11/bundles/scala-parser-combinators_2.11-1.0.4.jar:/
Users/mohit/code/activator-dist-1.3.10/repository/org.
scala-lang.modules/scala-java8-compat_2.11/0.7.0/
bundles/scala-java8-compat_2.11.jar:/Users/mohit/.ivy2/
cache/org.scala-lang/scala-reflect/jars/scala-reflect-2.
11.7.jar:/Users/mohit/.ivy2/cache/org.scala-lang/scala-
library/jars/scala-library-2.11.8.jar:/Users/mohit/.ivy2/
cache/org.scala-graph/graph-core_2.11/jars/graph-core_2.
11-1.11.2.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.
reactivestreams/reactive-streams/1.0.0/jars/reactive-
streams.jar:/Users/mohit/.ivy2/cache/org.ow2.asm/asm-
util/jars/asm-util-5.1.jar:/Users/mohit/.ivy2/cache/org.
ow2.asm/asm-tree/jars/asm-tree-5.1.jar:/Users/mohit/.
ivy2/cache/org.ow2.asm/asm-commons/jars/asm-commons-5.1.
jar:/Users/mohit/.ivy2/cache/org.ow2.asm/asm/jars/asm-5.1.
jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.joda/joda-
convert/1.8.1/jars/joda-convert.jar:/Users/mohit/code/activator-dist-1.3.10/
repository/org.javassist/javassist/3.20.0-GA/bundles/
javassist.jar:/Users/mohit/code/activator-dist-1.3.10/
repository/org.hamcrest/hamcrest-core/1.3/jars/hamcrest-core.jar:/Users/
mohit/code/activator-dist-1.3.10/repository/org.fluentlenium/fluentlenium-
core/0.10.9/jars/fluentlenium-core.jar:/Users/mohit/code/
activator-dist-1.3.10/repository/org.eclipse.jetty.
websocket/websocket-common/9.2.15.v20160210/jars/websocket-
common.jar:/Users/mohit/code/activator-dist-1.3.10/
repository/org.eclipse.jetty.websocket/websocket-client/9.
2.15.v20160210/jars/websocket-client.jar:/Users/mohit/code/
activator-dist-1.3.10/repository/org.eclipse.jetty.
websocket/websocket-api/9.2.15.v20160210/jars/websocket-
api.jar:/Users/mohit/code/activator-dist-1.3.10/
repository/org.eclipse.jetty/jetty-util/9.2.15.v20160210/
jars/jetty-util.jar:/Users/mohit/code/activator-dist-1.3.
10/repository/org.eclipse.jetty/jetty-io/9.2.15.
v20160210/jars/jetty-io.jar:/Users/mohit/.ivy2/cache/org.
clapper/grizzled-scala_2.11/jars/grizzled-scala_2.11-3.0.
0.jar:/Users/mohit/.ivy2/cache/org.clapper/classutil_2.
11/jars/classutil_2.11-1.0.13.jar:/Users/mohit/code/activator-dist-1.3.10/
repository/org.asynchttpclient/netty-resolver-dns/2.0.0-RC9/jars/
netty-resolver-dns.jar:/Users/mohit/code/activator-dist-1.3.
10/repository/org.asynchttpclient/netty-resolver/2.0.0-RC9/jars/netty-
resolver.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.
asynchttpclient/netty-codec-dns/2.0.0-RC9/jars/netty-
codec-dns.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.
asynchttpclient/async-http-client/2.0.0-RC9/jars/async-
http-client.jar:/Users/mohit/code/activator-dist-1.3.10/
repository/org.apache.httpcomponents/httpmime/4.5.2/
jars/httpmime.jar:/Users/mohit/code/activator-dist-1.3.
10/repository/org.apache.httpcomponents/httpcore/4.4.4/
jars/httpcore.jar:/Users/mohit/code/activator-dist-1.3.
10/repository/org.apache.httpcomponents/httpcore/4.0.1/
jars/httpcore.jar:/Users/mohit/code/activator-dist-1.3.
10/repository/org.apache.httpcomponents/httpclient/4.5.
2/jars/httpclient.jar:/Users/mohit/code/activator-dist-1.3.
10/repository/org.apache.httpcomponents/httpclient/4.0.
1/jars/httpclient.jar:/Users/mohit/code/activator-dist-1.3.
10/repository/org.apache.commons/commons-lang3/3.4/
jars/commons-lang3.jar:/Users/mohit/code/activator-dist-1.3.
10/repository/org.apache.commons/commons-exec/1.3/jars/
commons-exec.jar:/Users/mohit/.ivy2/cache/oauth.signpost/
signpost-core/jars/signpost-core-1.2.1.2.jar:/Users/mohit/
.ivy2/cache/oauth.signpost/signpost-commonshttp4/jars/
signpost-commonshttp4-1.2.1.2.jar:/Users/mohit/code/activator-dist-1.3.10/
repository/net.sourceforge.nekohtml/nekohtml/1.9.22/jars/
nekohtml.jar:/Users/mohit/code/activator-dist-1.3.10/
repository/net.sourceforge.htmlunit/htmlunit-core-js/2.
17/jars/htmlunit-core-js.jar:/Users/mohit/code/activator-
dist-1.3.10/repository/net.sourceforge.htmlunit/htmlunit/
2.20/jars/htmlunit.jar:/Users/mohit/code/activator-dist-1.3.
10/repository/net.sourceforge.cssparser/cssparser/0.9.18/
jars/cssparser.jar:/Users/mohit/code/activator-dist-1.3.
10/repository/net.sf.ehcache/ehcache-core/2.6.11/jars/
ehcache-core.jar:/Users/mohit/code/activator-dist-1.3.10/
repository/net.java.dev.jna/jna-platform/4.1.0/jars/jna-
platform.jar:/Users/mohit/code/activator-dist-1.3.10/
repository/net.java.dev.jna/jna/4.1.0/jars/jna.jar:/Users/
mohit/code/activator-dist-1.3.10/repository/junit/junit/4.
12/jars/junit.jar:/Users/mohit/.ivy2/cache/joda-time/
joda-time/jars/joda-time-2.9.4.jar:/Users/mohit/.ivy2/
cache/javax.transaction/jta/jars/jta-1.1.jar:/Users/mohit/
code/activator-dist-1.3.10/repository/javax.inject/javax.
inject/1/jars/javax.inject.jar:/Users/mohit/.ivy2/cache/
io.netty/netty-transport-native-epoll/jars/netty-
transport-native-epoll-4.0.33.Final.jar:/Users/mohit/code/
activator-dist-1.3.10/repository/io.netty/netty-transport/4.0.34.Final/jars/
netty-transport.jar:/Users/mohit/code/activator-dist-1.3.
10/repository/io.netty/netty-handler/4.0.34.Final/jars/
netty-handler.jar:/Users/mohit/code/activator-dist-1.3.
10/repository/io.netty/netty-common/4.0.34.Final/jars/
netty-common.jar:/Users/mohit/code/activator-dist-1.3.10/
repository/io.netty/netty-codec-http/4.0.34.Final/jars/
netty-codec-http.jar:/Users/mohit/code/activator-dist-1.3.
10/repository/io.netty/netty-codec/4.0.34.Final/jars/netty-
codec.jar:/Users/mohit/code/activator-dist-1.3.10/repository/io.netty/netty-
buffer/4.0.34.Final/jars/netty-buffer.jar:/Users/mohit/
code/activator-dist-1.3.10/repository/commons-logging/
commons-logging/1.2/jars/commons-logging.jar:/Users/
mohit/.ivy2/cache/commons-logging/commons-logging/jars/
commons-logging-1.1.1.jar:/Users/mohit/code/activator-
dist-1.3.10/repository/commons-io/commons-io/2.4/jars/commons-io.jar:/Users/
mohit/code/activator-dist-1.3.10/repository/commons-codec/
commons-codec/1.10/jars/commons-codec.jar:/Users/
mohit/.ivy2/cache/com.zaxxer/HikariCP-java6/bundles/
HikariCP-java6-2.3.7.jar:/Users/mohit/.ivy2/cache/com.
typesafe.slick/slick_2.11/bundles/slick_2.11-3.1.0.jar:/
Users/mohit/.ivy2/cache/com.typesafe.slick/slick-hikaricp_
2.11/bundles/slick-hikaricp_2.11-3.1.0.jar:/Users/mohit/
code/activator-dist-1.3.10/repository/com.typesafe.play/
twirl-api_2.11/1.1.1/jars/twirl-api_2.11.jar:/Users/
mohit/code/activator-dist-1.3.10/repository/com.typesafe.
play/play_2.11/2.5.0/jars/play_2.11.jar:/Users/mohit/
code/activator-dist-1.3.10/repository/com.typesafe.play/
play-ws_2.11/2.5.0/jars/play-ws_2.11.jar:/Users/mohit/code/
activator-dist-1.3.10/repository/com.typesafe.play/
play-test_2.11/2.5.0/jars/play-test_2.11.jar:/Users/
mohit/code/activator-dist-1.3.10/repository/com.typesafe.
play/play-streams_2.11/2.5.0/jars/play-streams_2.11.jar:/
Users/mohit/.ivy2/cache/com.typesafe.play/play-slick_2.11/
jars/play-slick_2.11-1.1.1.jar:/Users/mohit/.ivy2/cache/
com.typesafe.play/play-slick-evolutions_2.11/jars/play-
slick-evolutions_2.11-1.1.1.jar:/Users/mohit/code/activator-dist-1.3.10/
repository/com.typesafe.play/play-server_2.11/2.5.0/jars/
play-server_2.11.jar:/Users/mohit/code/activator-dist-1.3.
10/repository/com.typesafe.play/play-netty-utils/2.5.0/
jars/play-netty-utils.jar:/Users/mohit/code/activator-
dist-1.3.10/repository/com.typesafe.play/play-netty-
server_2.11/2.5.0/jars/play-netty-server_2.11.jar:/Users/
mohit/.ivy2/cache/com.typesafe.play/play-logback_2.
11/jars/play-logback_2.11-2.5.0.jar:/Users/mohit/.ivy2/
cache/com.typesafe.play/play-json_2.11/jars/play-json_2.11-
2.5.9.jar:/Users/mohit/.ivy2/cache/com.typesafe.play/play-
jdbc-evolutions_2.11/jars/play-jdbc-evolutions_2.11-2.5.
0.jar:/Users/mohit/.ivy2/cache/com.typesafe.play/play-
jdbc-api_2.11/jars/play-jdbc-api_2.11-2.5.0.jar:/Users/
mohit/.ivy2/cache/com.typesafe.play/play-iteratees_
2.11/jars/play-iteratees_2.11-2.5.9.jar:/Users/mohit/.ivy2/
cache/com.typesafe.play/play-functional_2.11/jars/play-
functional_2.11-2.5.9.jar:/Users/mohit/code/activator-
dist-1.3.10/repository/com.typesafe.play/play-exceptions/
2.5.0/jars/play-exceptions.jar:/Users/mohit/.ivy2/cache/
com.typesafe.play/play-datacommons_2.11/jars/play-
datacommons_2.11-2.5.9.jar:/Users/mohit/.ivy2/cache/com.
typesafe.play/play-cache_2.11/jars/play-cache_2.11-2.5.0.
jar:/Users/mohit/code/activator-dist-1.3.10/repository/com.typesafe.play/
build-link/2.5.0/jars/build-link.jar:/Users/mohit/code/
activator-dist-1.3.10/repository/com.typesafe.netty/
netty-reactive-streams-http/1.0.2/jars/netty-reactive-
streams-http.jar:/Users/mohit/code/activator-dist-1.3.10/
repository/com.typesafe.netty/netty-reactive-streams/1.0.2/
jars/netty-reactive-streams.jar:/Users/mohit/code/activator-dist-1.3.10/
repository/com.typesafe.akka/akka-stream_2.11/2.4.2/jars/
akka-stream_2.11.jar:/Users/mohit/code/activator-dist-1.3.
10/repository/com.typesafe.akka/akka-slf4j_2.11/2.4.2/
jars/akka-slf4j_2.11.jar:/Users/mohit/code/activator-
dist-1.3.10/repository/com.typesafe.akka/akka-actor_2.11/
2.4.2/jars/akka-actor_2.11.jar:/Users/mohit/code/activator-dist-1.3.10/
repository/com.typesafe/ssl-config-core_2.11/0.1.3/
bundles/ssl-config-core_2.11.jar:/Users/mohit/code/activator-dist-1.3.10/
repository/com.typesafe/ssl-config-akka_2.11/0.1.3/
bundles/ssl-config-akka_2.11.jar:/Users/mohit/code/activator-dist-1.3.10/
repository/com.typesafe/config/1.3.0/bundles/config.jar:/Users/mohit/code/
activator-dist-1.3.10/repository/com.novocode/junit-
interface/0.11/jars/junit-interface.jar:/Users/mohit/
code/activator-dist-1.3.10/repository/com.h2database/h2/
1.4.191/jars/h2.jar:/Users/mohit/code/activator-dist-1.3.
10/repository/com.google.inject.extensions/guice-
assistedinject/4.0/jars/guice-assistedinject.jar:/Users/
mohit/code/activator-dist-1.3.10/repository/com.google.
inject/guice/4.0/jars/guice.jar:/Users/mohit/code/activator-dist-1.3.10/
repository/com.google.guava/guava/19.0/bundles/guava.jar:/
Users/mohit/code/activator-dist-1.3.10/repository/com.
google.code.gson/gson/2.3.1/jars/gson.jar:/Users/mohit/
code/activator-dist-1.3.10/repository/com.google.code.
findbugs/jsr305/3.0.1/jars/jsr305.jar:/Users/mohit/.ivy2/
cache/com.fasterxml.jackson.datatype/jackson-datatype-
jsr310/bundles/jackson-datatype-jsr310-2.7.6.jar:/
Users/mohit/.ivy2/cache/com.fasterxml.jackson.datatype/
jackson-datatype-jdk8/bundles/jackson-datatype-jdk8-2.7.6.
jar:/Users/mohit/.ivy2/cache/com.fasterxml.jackson.core/
jackson-databind/bundles/jackson-databind-2.7.6.jar:/
Users/mohit/.ivy2/cache/com.fasterxml.jackson.core/
jackson-core/bundles/jackson-core-2.7.6.jar:/Users/mohit/.
ivy2/cache/com.fasterxml.jackson.core/jackson-annotations/bundles/jackson-
annotations-2.7.6.jar:/Users/mohit/code/activator-dist-1.3.
10/repository/ch.qos.logback/logback-core/1.1.4/jars/
logback-core.jar:/Users/mohit/code/activator-dist-1.3.10/
repository/ch.qos.logback/logback-classic/1.1.4/jars/
logback-classic.jar:/Users/mohit/code/activator-dist-1.3.
10/repository/cglib/cglib-nodep/2.1_3/jars/cglib-nodep.
jar:/Users/mohit/code/activator-dist-1.3.10/repository/aopalliance/
aopalliance/1.0/jars/aopalliance.jar:/Users/mohit/.
ivy2/cache/com.clearspring.analytics/stream/jars/stream-
2.7.0.jar:/Users/mohit/.ivy2/cache/com.esotericsoftware/
kryo-shaded/bundles/kryo-shaded-3.0.3.jar:/Users/mohit/.ivy2/cache/com.
esotericsoftware/minlog/bundles/minlog-1.3.0.jar:/
Users/mohit/.ivy2/cache/com.fasterxml.jackson.module/
jackson-module-paranamer/bundles/jackson-module-paranamer-2.6.5.jar:/Users/
mohit/.ivy2/cache/com.fasterxml.jackson.module/jackson-module-scala_2.11/
bundles/jackson-module-scala_2.11-2.6.5.jar:/Users/mohit/.
ivy2/cache/com.github.fommil.netlib/core/jars/core-1.1.2.
jar:/Users/mohit/.ivy2/cache/com.github.rwl/jtransforms/
jars/jtransforms-2.4.0.jar:/Users/mohit/.ivy2/cache/com.
google.protobuf/protobuf-java/bundles/protobuf-java-2.5.0.
jar:/Users/mohit/.ivy2/cache/com.ning/compress-lzf/bundles/
compress-lzf-1.0.3.jar:/Users/mohit/.ivy2/cache/com.thoughtworks.paranamer/
paranamer/jars/paranamer-2.6.jar:/Users/mohit/.ivy2/cache/
com.twitter/chill-java/jars/chill-java-0.8.0.jar:/Users/
mohit/.ivy2/cache/com.twitter/chill_2.11/jars/chill_2.11-0.
8.0.jar:/Users/mohit/.ivy2/cache/com.univocity/univocity-
parsers/jars/univocity-parsers-2.1.1.jar:/Users/mohit/.ivy2/cache/commons-
beanutils/commons-beanutils/jars/commons-beanutils-1.7.0.
jar:/Users/mohit/.ivy2/cache/commons-beanutils/commons-
beanutils-core/jars/commons-beanutils-core-1.8.0.jar:/
Users/mohit/.ivy2/cache/commons-cli/commons-cli/jars/
commons-cli-1.2.jar:/Users/mohit/.ivy2/cache/commons-collections/commons-
collections/jars/commons-collections-3.2.1.jar:/Users/
mohit/.ivy2/cache/commons-configuration/commons-configuration/jars/commons-
configuration-1.6.jar:/Users/mohit/.ivy2/cache/commons-
digester/commons-digester/jars/commons-digester-1.8.jar:
/Users/mohit/.ivy2/cache/commons-httpclient/commons-httpclient/jars/commons-
httpclient-3.1.jar:/Users/mohit/.ivy2/cache/commons-lang/commons-lang/jars/
commons-lang-2.5.jar:/Users/mohit/.ivy2/cache/commons-net/
commons-net/jars/commons-net-2.2.jar:/Users/mohit/.ivy2/
cache/io.dropwizard.metrics/metrics-core/bundles/metrics-
core-3.1.2.jar:/Users/mohit/.ivy2/cache/io.dropwizard.
metrics/metrics-graphite/bundles/metrics-graphite-3.1.
2.jar:/Users/mohit/.ivy2/cache/io.dropwizard.metrics/
metrics-json/bundles/metrics-json-3.1.2.jar:/Users/mohit/.
ivy2/cache/io.dropwizard.metrics/metrics-jvm/bundles/
metrics-jvm-3.1.2.jar:/Users/mohit/code/activator-dist-1.3.
10/repository/io.netty/netty/3.8.0.Final/bundles/netty.jar:
/Users/mohit/.ivy2/cache/io.netty/netty-all/jars/netty-
all-4.0.29.Final.jar:/Users/mohit/.ivy2/cache/javax.
annotation/javax.annotation-api/jars/javax.annotation-api-
1.2.jar:/Users/mohit/.ivy2/cache/javax.servlet/javax.servlet-api/jars/javax.
servlet-api-3.1.0.jar:/Users/mohit/.ivy2/cache/javax.
validation/validation-api/jars/validation-api-1.1.0.
Final.jar:/Users/mohit/.ivy2/cache/javax.ws.rs/javax.ws.rs-
api/jars/javax.ws.rs-api-2.0.1.jar:/Users/mohit/.ivy2/
cache/log4j/log4j/bundles/log4j-1.2.17.jar:/Users/mohit/
.ivy2/cache/net.java.dev.jets3t/jets3t/jars/jets3t-0.7.
1.jar:/Users/mohit/.ivy2/cache/net.jpountz.lz4/lz4/
jars/lz4-1.3.0.jar:/Users/mohit/.ivy2/cache/net.sf.
opencsv/opencsv/jars/opencsv-2.3.jar:/Users/mohit/.ivy2/
cache/net.sourceforge.f2j/arpack_combined_all/jars/
arpack_combined_all-0.1.jar:/Users/mohit/.ivy2/cache/org.
antlr/antlr4-runtime/jars/antlr4-runtime-4.5.3.jar:/
Users/mohit/.ivy2/cache/org.apache.avro/avro/jars/avro-1.
7.7.jar:/Users/mohit/.ivy2/cache/org.apache.avro/avro-
ipc/jars/avro-ipc-1.7.7.jar:/Users/mohit/.ivy2/cache/org.
apache.avro/avro-ipc/jars/avro-ipc-1.7.7-tests.jar:/
Users/mohit/.ivy2/cache/org.apache.avro/avro-mapred/jars/
avro-mapred-1.7.7-hadoop2.jar:/Users/mohit/.ivy2/cache/org.
apache.commons/commons-compress/jars/commons-compress-1.4.1.jar:/Users/
mohit/.ivy2/cache/org.apache.commons/commons-math/jars/
commons-math-2.1.jar:/Users/mohit/.ivy2/cache/org.apache.
commons/commons-math3/jars/commons-math3-3.4.1.jar:/
Users/mohit/.ivy2/cache/org.apache.curator/curator-client/
bundles/curator-client-2.4.0.jar:/Users/mohit/.ivy2/cache/
org.apache.curator/curator-framework/bundles/curator-
framework-2.4.0.jar:/Users/mohit/.ivy2/cache/org.apache.
curator/curator-recipes/bundles/curator-recipes-2.4.0.
jar:/Users/mohit/.ivy2/cache/org.apache.hadoop/hadoop-
annotations/jars/hadoop-annotations-2.2.0.jar:/Users/
mohit/.ivy2/cache/org.apache.hadoop/hadoop-auth/jars/
hadoop-auth-2.2.0.jar:/Users/mohit/.ivy2/cache/org.apache.
hadoop/hadoop-client/jars/hadoop-client-2.2.0.jar:/
Users/mohit/.ivy2/cache/org.apache.hadoop/hadoop-common/
jars/hadoop-common-2.2.0.jar:/Users/mohit/.ivy2/cache/org.
apache.hadoop/hadoop-hdfs/jars/hadoop-hdfs-2.2.0.jar:/
Users/mohit/.ivy2/cache/org.apache.hadoop/hadoop-mapreduce-client-app/jars/
hadoop-mapreduce-client-app-2.2.0.jar:/Users/mohit/.ivy2/
cache/org.apache.hadoop/hadoop-mapreduce-client-
common/jars/hadoop-mapreduce-client-common-2.2.0.jar:/
Users/mohit/.ivy2/cache/org.apache.hadoop/hadoop-mapreduce-client-core/jars/
hadoop-mapreduce-client-core-2.2.0.jar:/Users/mohit/.ivy2/
cache/org.apache.hadoop/hadoop-mapreduce-client-jobclient/jars/hadoop-
mapreduce-client-jobclient-2.2.0.jar:/Users/mohit/.ivy2/
cache/org.apache.hadoop/hadoop-mapreduce-client-
shuffle/jars/hadoop-mapreduce-client-shuffle-2.2.0.jar:/
Users/mohit/.ivy2/cache/org.apache.hadoop/hadoop-yarn-api/
jars/hadoop-yarn-api-2.2.0.jar:/Users/mohit/.ivy2/cache/
org.apache.hadoop/hadoop-yarn-client/jars/hadoop-yarn-
client-2.2.0.jar:/Users/mohit/.ivy2/cache/org.apache.hadoop/
hadoop-yarn-common/jars/hadoop-yarn-common-2.2.0.jar:/
Users/mohit/.ivy2/cache/org.apache.hadoop/hadoop-yarn-
server-common/jars/hadoop-yarn-server-common-2.2.0.jar:/
Users/mohit/.ivy2/cache/org.apache.ivy/ivy/jars/ivy-2.4.0.
jar:/Users/mohit/.ivy2/cache/org.apache.mesos/mesos/jars/
mesos-0.21.1-shaded-protobuf.jar:/Users/mohit/.ivy2/cache/
org.apache.parquet/parquet-column/jars/parquet-column-1.
7.0.jar:/Users/mohit/.ivy2/cache/org.apache.parquet/
parquet-common/jars/parquet-common-1.7.0.jar:/Users/mohit/
.ivy2/cache/org.apache.parquet/parquet-encoding/jars/
parquet-encoding-1.7.0.jar:/Users/mohit/.ivy2/cache/org.
apache.parquet/parquet-format/jars/parquet-format-2.3.0-
incubating.jar:/Users/mohit/.ivy2/cache/org.apache.parquet/
parquet-generator/jars/parquet-generator-1.7.0.jar:/
Users/mohit/.ivy2/cache/org.apache.parquet/parquet-hadoop/
jars/parquet-hadoop-1.7.0.jar:/Users/mohit/.ivy2/cache/org.
apache.parquet/parquet-jackson/jars/parquet-jackson-
1.7.0.jar:/Users/mohit/.ivy2/cache/org.apache.xbean/xbean-
asm5-shaded/bundles/xbean-asm5-shaded-4.4.jar:/Users/
mohit/.ivy2/cache/org.apache.zookeeper/zookeeper/jars/
zookeeper-3.4.5.jar:/Users/mohit/.ivy2/cache/org.
codehaus.jackson/jackson-core-asl/jars/jackson-core-asl-1.9.
13.jar:/Users/mohit/.ivy2/cache/org.codehaus.jackson/
jackson-mapper-asl/jars/jackson-mapper-asl-1.9.13.jar:
/Users/mohit/.ivy2/cache/org.codehaus.janino/commons-compiler/jars/commons-
compiler-2.7.8.jar:/Users/mohit/.ivy2/cache/org.codehaus.janino/janino/jars/
janino-2.7.8.jar:/Users/mohit/.ivy2/cache/org.fusesource.
leveldbjni/leveldbjni-all/bundles/leveldbjni-all-1.8.
jar:/Users/mohit/.ivy2/cache/org.glassfish.hk2/hk2-api/
jars/hk2-api-2.4.0-b34.jar:/Users/mohit/.ivy2/cache/org.
glassfish.hk2/hk2-locator/jars/hk2-locator-2.4.0-b34.
jar:/Users/mohit/.ivy2/cache/org.glassfish.hk2/hk2-utils/
jars/hk2-utils-2.4.0-b34.jar:/Users/mohit/.ivy2/cache/org.
glassfish.hk2/osgi-resource-locator/jars/osgi-resource-
locator-1.0.1.jar:/Users/mohit/.ivy2/cache/org.glassfish.hk2.external/
aopalliance-repackaged/jars/aopalliance-repackaged-2.4.0-
b34.jar:/Users/mohit/.ivy2/cache/org.glassfish.hk2.
external/javax.inject/jars/javax.inject-2.4.0-b34.jar:/
Users/mohit/.ivy2/cache/org.glassfish.jersey.bundles.
repackaged/jersey-guava/bundles/jersey-guava-2.22.2.
jar:/Users/mohit/.ivy2/cache/org.glassfish.jersey.
containers/jersey-container-servlet/jars/jersey-container-
servlet-2.22.2.jar:/Users/mohit/.ivy2/cache/org.glassfish.jersey.containers/
jersey-container-servlet-core/jars/jersey-container-servlet-
core-2.22.2.jar:/Users/mohit/.ivy2/cache/org.glassfish.
jersey.core/jersey-client/jars/jersey-client-2.22.2.jar:
/Users/mohit/.ivy2/cache/org.glassfish.jersey.core/jersey-
common/jars/jersey-common-2.22.2.jar:/Users/mohit/.ivy2/
cache/org.glassfish.jersey.core/jersey-server/jars/
jersey-server-2.22.2.jar:/Users/mohit/.ivy2/cache/org.
glassfish.jersey.media/jersey-media-jaxb/jars/jersey-media-
jaxb-2.22.2.jar:/Users/mohit/.ivy2/cache/org.jpmml/pmml-
model/jars/pmml-model-1.2.15.jar:/Users/mohit/.ivy2/cache/
org.jpmml/pmml-schema/jars/pmml-schema-1.2.15.jar:/Users/
mohit/.ivy2/cache/org.json4s/json4s-ast_2.11/jars/json4s-
ast_2.11-3.2.11.jar:/Users/mohit/.ivy2/cache/org.json4s/
json4s-core_2.11/jars/json4s-core_2.11-3.2.11.jar:/Users/
mohit/.ivy2/cache/org.json4s/json4s-jackson_2.11/jars/
json4s-jackson_2.11-3.2.11.jar:/Users/mohit/.ivy2/cache/
org.mortbay.jetty/jetty-util/jars/jetty-util-6.1.26.jar:/
Users/mohit/.ivy2/cache/org.objenesis/objenesis/jars/
objenesis-2.1.jar:/Users/mohit/.ivy2/cache/org.roaringbitmap/RoaringBitmap/
bundles/RoaringBitmap-0.5.11.jar:/Users/mohit/.ivy2/cache/
org.scala-lang/scala-compiler/jars/scala-compiler-2.11.8.
jar:/Users/mohit/.ivy2/cache/org.scala-lang/scala-reflect/
jars/scala-reflect-2.11.8.jar:/Users/mohit/.ivy2/cache/org.
scala-lang/scalap/jars/scalap-2.11.0.jar:/Users/mohit/.ivy2/
cache/org.scala-lang.modules/scala-xml_2.11/bundles/scala-
xml_2.11-1.0.4.jar:/Users/mohit/.ivy2/cache/org.scalanlp/breeze-macros_2.11/
jars/breeze-macros_2.11-0.11.2.jar:/Users/mohit/.ivy2/
cache/org.scalanlp/breeze_2.11/jars/breeze_2.11-0.11.2.
jar:/Users/mohit/.ivy2/cache/org.slf4j/slf4j-log4j12/jars/
slf4j-log4j12-1.7.16.jar:/Users/mohit/.ivy2/cache/org.
spark-project.spark/unused/jars/unused-1.0.0.jar:/Users/
mohit/.ivy2/cache/org.spire-math/spire-macros_2.11/jars/
spire-macros_2.11-0.7.4.jar:/Users/mohit/.ivy2/cache/org.
spire-math/spire_2.11/jars/spire_2.11-0.7.4.jar:/Users/
mohit/.ivy2/cache/org.tukaani/xz/jars/xz-1.0.jar:/Users/
mohit/.ivy2/cache/oro/oro/jars/oro-2.0.8.jar:/Users/
mohit/.ivy2/cache/xmlenc/xmlenc/jars/xmlenc-0.52.jar:/
Users/mohit/.ivy2/cache/net.razorvine/pyrolite/jars/pyrolite-4.9.jar:/
Applications/IntelliJ IDEA CE.app/Contents/lib/idea_rt.jar""
com.intellij.rt.execution.application.AppMain org.jetbrains.plugins.scala.
testingSupport.scalaTest.ScalaTestRunner -s dag.execution.SparkReplSpec
-showProgressMessages true -C org.jetbrains.plugins.scala.
testingSupport.scalaTest.ScalaTestReporter
*Testing started at 2:40 PM ...*
*java.lang.NoSuchMethodError:
scala.reflect.internal.Symbols$Symbol.isDeferredNotJavaDefault()Z*
* at
scala.tools.nsc.typechecker.RefChecks$RefCheckTransformer$$anonfun$12.apply(RefChecks.scala:607)*
at scala.tools.nsc.typechecker.RefChecks$RefCheckTransformer$
$anonfun$12.apply(RefChecks.scala:607)
at scala.collection.TraversableLike$$anonfun$partition$1.apply(
TraversableLike.scala:314)
at scala.collection.TraversableLike$$anonfun$partition$1.apply(
TraversableLike.scala:314)
at scala.reflect.internal.Scopes$Scope.foreach(Scopes.scala:373)
at scala.collection.TraversableLike$class.partition(TraversableLike.
scala:314)
at scala.reflect.internal.Scopes$Scope.partition(Scopes.scala:51)
at scala.tools.nsc.typechecker.RefChecks$RefCheckTransformer.
checkNoAbstractMembers$1(RefChecks.scala:607)
at scala.tools.nsc.typechecker.RefChecks$RefCheckTransformer.
checkAllOverrides(RefChecks.scala:752)
at scala.tools.nsc.typechecker.RefChecks$RefCheckTransformer.
transform(RefChecks.scala:1672)
at scala.tools.nsc.typechecker.RefChecks$RefCheckTransformer.
transform(RefChecks.scala:111)
at scala.reflect.api.Trees$Transformer.transformTemplate(Trees.scala:2563)
at scala.reflect.internal.Trees$$anonfun$itransform$4.apply(
Trees.scala:1408)
at scala.reflect.internal.Trees$$anonfun$itransform$4.apply(
Trees.scala:1407)
at scala.reflect.api.Trees$Transformer.atOwner(Trees.scala:2600)
at scala.reflect.internal.Trees$class.itransform(Trees.scala:1406)
at scala.reflect.internal.SymbolTable.itransform(SymbolTable.scala:16)
at scala.reflect.internal.SymbolTable.itransform(SymbolTable.scala:16)
at scala.reflect.api.Trees$Transformer.transform(Trees.scala:2555)
at scala.tools.nsc.typechecker.RefChecks$RefCheckTransformer.
transform(RefChecks.scala:1785)
at scala.tools.nsc.typechecker.RefChecks$RefCheckTransformer.
transformStat(RefChecks.scala:1262)
at scala.tools.nsc.typechecker.RefChecks$RefCheckTransformer$
$anonfun$transformStats$1.apply(RefChecks.scala:1165)
at scala.tools.nsc.typechecker.RefChecks$RefCheckTransformer$
$anonfun$transformStats$1.apply(RefChecks.scala:1165)
at scala.collection.immutable.List.flatMap(List.scala:327)
at scala.tools.nsc.typechecker.RefChecks$RefCheckTransformer.
transformStats(RefChecks.scala:1165)
at scala.tools.nsc.typechecker.RefChecks$RefCheckTransformer.
transformStats(RefChecks.scala:111)
at scala.reflect.internal.Trees$$anonfun$itransform$7.apply(
Trees.scala:1426)
at scala.reflect.internal.Trees$$anonfun$itransform$7.apply(
Trees.scala:1426)
at scala.reflect.api.Trees$Transformer.atOwner(Trees.scala:2600)
at scala.reflect.internal.Trees$class.itransform(Trees.scala:1425)
at scala.reflect.internal.SymbolTable.itransform(SymbolTable.scala:16)
at scala.reflect.internal.SymbolTable.itransform(SymbolTable.scala:16)
at scala.reflect.api.Trees$Transformer.transform(Trees.scala:2555)
at scala.tools.nsc.typechecker.RefChecks$RefCheckTransformer.
transform(RefChecks.scala:1785)
at scala.tools.nsc.typechecker.RefChecks$RefCheckTransformer.
transform(RefChecks.scala:111)
at scala.tools.nsc.ast.Trees$Tra"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 17 Nov 2016 17:10:00 +0000",Green dot in web UI DAG visualization,Spark dev list <dev@spark.apache.org>,"Some questions about this DAG visualization:

[image: Screen Shot 2016-11-17 at 11.57.14 AM.png]

1. What's the meaning of the green dot?
2. Should this be documented anywhere (if it isn't already)? Preferably a
tooltip or something directly in the UI would explain the significance.

Nick
"
=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@databricks.com>,"Thu, 17 Nov 2016 09:14:48 -0800",Re: Green dot in web UI DAG visualization,Nicholas Chammas <nicholas.chammas@gmail.com>,"Should I be able to see something?


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 17 Nov 2016 17:18:07 +0000",Re: Green dot in web UI DAG visualization,=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@databricks.com>,"Hmm... somehow the image didn't show up.

How about now?

[image: Screen Shot 2016-11-17 at 11.57.14 AM.png]


Should I be able to see something?


Some questions about this DAG visualization:

[image: Screen Shot 2016-11-17 at 11.57.14 AM.png]

1. What's the meaning of the green dot?
2. Should this be documented anywhere (if it isn't already)? Preferably a
tooltip or something directly in the UI would explain the significance.

Nick
"
Reynold Xin <rxin@databricks.com>,"Thu, 17 Nov 2016 09:19:41 -0800",Re: Green dot in web UI DAG visualization,Nicholas Chammas <nicholas.chammas@gmail.com>,"Ha funny. Never noticed that.

m>

e.
"
Suhas Gaddam <suhas.g.2011@gmail.com>,"Thu, 17 Nov 2016 09:21:47 -0800",Re: Green dot in web UI DAG visualization,Reynold Xin <rxin@databricks.com>,"""Second, one of the RDDs is cached in the first stage (denoted by the green
highlight). Since the enclosing operation involves reading from HDFS,
caching this RDD means future computations on this RDD can access at least
a subset of the original file from memory instead of from HDFS.""

from
https://databricks.com/blog/2015/06/22/understanding-your-spark-application-through-visualization.html


<
y
ce.
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 17 Nov 2016 17:23:21 +0000",Re: Green dot in web UI DAG visualization,"Suhas Gaddam <suhas.g.2011@gmail.com>, Reynold Xin <rxin@databricks.com>","Nice catch Suhas, and thanks for the reference. Sounds like we need a tweak
to the UI so this little feature is self-documenting.

Will file a JIRA, unless someone else wants to take this one and file the
JIRA themselves.


S,
t
on-through-visualization.html
"
Michael Allman <michael@videoamp.com>,"Thu, 17 Nov 2016 09:59:48 -0800",Jackson Spark/app incompatibility and how to resolve it,Spark Dev List <dev@spark.apache.org>,"Hello,

I'm running into an issue with a Spark app I'm building, which depends on a library which depends on Jackson 2.8, which fails at runtime because Spark brings in Jackson 2.6. I'm looking for a solution. As a workaround, I've patched our build of Spark to use Jackson 2.8. That's working, however given all the trouble associated with attempting a Jackson upgrade in the past (see https://issues.apache.org/jira/browse/SPARK-14989 <https://issues.apache.org/jira/browse/SPARK-14989?jql=project%20=%20SPARK%20AND%20text%20~%20jackson> and https://github.com/apache/spark/pull/13417 <https://github.com/apache/spark/pull/13417>), I'm wondering if I should submit a PR for that. Is shading Spark's Jackson deps another option? Any other suggestions for an acceptable way to fix this incompatibility with apps using a newer version of Jackson?

FWIW, Jackson claims to support backward compatibility within minor releases (https://github.com/FasterXML/jackson-docs#on-jackson-versioning <https://github.com/FasterXML/jackson-docs#on-jackson-versioning>). So in theory, apps that depend on an upgraded Spark version should work even if they ask for an older version.

Cheers,

Michael"
Mohit Jaggi <mohitjaggi@gmail.com>,"Thu, 17 Nov 2016 11:16:36 -0800",Re: SparkILoop doesn't run,user <user@spark.apache.org>,"Thanks Holden. I did post to the user list but since this is not a common case, I am trying the developer list as well. Yes there is a reason: I get code from somewhere e.g. a notebook. This type of code did work for me before.

Mohit Jaggi
Founder,
Data Orchard LLC
www.dataorchardllc.com




reason you are trying to use the SparkILoop for tests?
test hangs with the following stack trace. Any idea what is going on?
/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/bin/java -Dspark.master=local[*] -Didea.launcher.port=7532 ""-Didea.launcher.bin.path=/Applications/IntelliJ IDEA CE.app/Contents/bin"" -Dfile.encoding=UTF-8 -classpath ""/Users/mohit/Library/Application Support/IdeaIC2016.2/Scala/lib/scala-plugin-runners.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/deploy.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/ext/cldrdata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/ext/dnsns.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/ext/jaccess.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/ext/jfxrt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/ext/localedata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/ext/nashorn.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/ext/sunec.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/ext/sunjce_provider.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/ext/sunpkcs11.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/ext/zipfs.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/javaws.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/jfxswt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/management-agent.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/plugin.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/resources.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/lib/ant-javafx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/lib/dt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/lib/javafx-mx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/lib/jconsole.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/lib/packager.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/lib/sa-jdi.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/lib/tools.jar:/Users/mohit/code/datagears-play/target/scala-2.11/test-classes:/Users/mohit/code/datagears-play/target/scala-2.11/classes:/Users/mohit/code/datagears-play/macros/target/scala-2.11/classes:/Users/mohit/.ivy2/cache/org.xerial.snappy/snappy-java/bundles/snappy-java-1.1.2.4.jar:/Users/mohit/.ivy2/cache/org.apache.spark/spark-unsafe_2.11/jars/spark-unsafe_2.11-2.0.0.jar:/Users/mohit/.ivy2/cache/org.apache.spark/spark-tags_2.11/jars/spark-tags_2.11-2.0.0.jar:/Users/mohit/.ivy2/cache/org.apache.spark/spark-streaming_2.11/jars/spark-streaming_2.11-2.0.0.jar:/Users/mohit/.ivy2/cache/org.apache.spark/spark-sql_2.11/jars/spark-sql_2.11-2.0.0.jar:/Users/mohit/.ivy2/cache/org.apache.spark/spark-sketch_2.11/jars/spark-sketch_2.11-2.0.0.jar:/Users/mohit/.ivy2/cache/org.apache.spark/spark-repl_2.11/jars/spark-repl_2.11-2.0.0.jar:/Users/mohit/.ivy2/cache/org.apache.spark/spark-network-shuffle_2.11/jars/spark-network-shuffle_2.11-2.0.0.jar:/Users/mohit/.ivy2/cache/org.apache.spark/spark-network-common_2.11/jars/spark-network-common_2.11-2.0.0.jar:/Users/mohit/.ivy2/cache/org.apache.spark/spark-mllib_2.11/jars/spark-mllib_2.11-2.0.0.jar:/Users/mohit/.ivy2/cache/org.apache.spark/spark-mllib-local_2.11/jars/spark-mllib-local_2.11-2.0.0.jar:/Users/mohit/.ivy2/cache/org.apache.spark/spark-launcher_2.11/jars/spark-launcher_2.11-2.0.0.jar:/Users/mohit/.ivy2/cache/org.apache.spark/spark-graphx_2.11/jars/spark-graphx_2.11-2.0.0.jar:/Users/mohit/.ivy2/cache/org.apache.spark/spark-core_2.11/jars/spark-core_2.11-2.0.0.jar:/Users/mohit/.ivy2/cache/org.apache.spark/spark-catalyst_2.11/jars/spark-catalyst_2.11-2.0.0.jar:/Users/mohit/.ivy2/cache/net.sf.py4j/py4j/jars/py4j-0.10.1.jar:/Users/mohit/.ivy2/cache/xml-apis/xml-apis/jars/xml-apis-1.4.01.jar:/Users/mohit/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.11.0.jar:/Users/mohit/code/activator-dist-1.3.10/repository/xalan/xalan/2.7.2/jars/xalan.jar:/Users/mohit/code/activator-dist-1.3.10/repository/xalan/serializer/2.7.2/jars/serializer.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.webbitserver/webbit/0.4.14/jars/webbit.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.w3c.css/sac/1.3/jars/sac.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.slf4j/slf4j-api/1.7.16/jars/slf4j-api.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.slf4j/jul-to-slf4j/1.7.16/jars/jul-to-slf4j.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.slf4j/jcl-over-slf4j/1.7.16/jars/jcl-over-slf4j.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.seleniumhq.selenium/selenium-support/2.48.2/jars/selenium-support.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.seleniumhq.selenium/selenium-safari-driver/2.48.2/jars/selenium-safari-driver.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.seleniumhq.selenium/selenium-remote-driver/2.48.2/jars/selenium-remote-driver.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.seleniumhq.selenium/selenium-leg-rc/2.48.2/jars/selenium-leg-rc.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.seleniumhq.selenium/selenium-java/2.48.2/jars/selenium-java.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.seleniumhq.selenium/selenium-ie-driver/2.48.2/jars/selenium-ie-driver.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.seleniumhq.selenium/selenium-htmlunit-driver/2.48.2/jars/selenium-htmlunit-driver.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.seleniumhq.selenium/selenium-firefox-driver/2.48.2/jars/selenium-firefox-driver.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.seleniumhq.selenium/selenium-edge-driver/2.48.2/jars/selenium-edge-driver.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.seleniumhq.selenium/selenium-chrome-driver/2.48.2/jars/selenium-chrome-driver.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.seleniumhq.selenium/selenium-api/2.48.2/jars/selenium-api.jar:/Users/mohit/.ivy2/cache/org.scalatestplus.play/scalatestplus-play_2.11/jars/scalatestplus-play_2.11-1.5.0-RC1.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.scalatest/scalatest_2.11/2.2.6/bundles/scalatest_2.11.jar:/Users/mohit/.ivy2/cache/org.scalacheck/scalacheck_2.11/jars/scalacheck_2.11-1.12.5.jar:/Users/mohit/.ivy2/cache/org.scala-stm/scala-stm_2.11/jars/scala-stm_2.11-0.7.jar:/Users/mohit/.ivy2/cache/org.scala-sbt/test-interface/jars/test-interface-1.0.jar:/Users/mohit/.ivy2/cache/org.scala-lang.modules/scala-xml_2.11/bundles/scala-xml_2.11-1.0.1.jar:/Users/mohit/.ivy2/cache/org.scala-lang.modules/scala-parser-combinators_2.11/bundles/scala-parser-combinators_2.11-1.0.4.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.scala-lang.modules/scala-java8-compat_2.11/0.7.0/bundles/scala-java8-compat_2.11.jar:/Users/mohit/.ivy2/cache/org.scala-lang/scala-reflect/jars/scala-reflect-2.11.7.jar:/Users/mohit/.ivy2/cache/org.scala-lang/scala-library/jars/scala-library-2.11.8.jar:/Users/mohit/.ivy2/cache/org.scala-graph/graph-core_2.11/jars/graph-core_2.11-1.11.2.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.reactivestreams/reactive-streams/1.0.0/jars/reactive-streams.jar:/Users/mohit/.ivy2/cache/org.ow2.asm/asm-util/jars/asm-util-5.1.jar:/Users/mohit/.ivy2/cache/org.ow2.asm/asm-tree/jars/asm-tree-5.1.jar:/Users/mohit/.ivy2/cache/org.ow2.asm/asm-commons/jars/asm-commons-5.1.jar:/Users/mohit/.ivy2/cache/org.ow2.asm/asm/jars/asm-5.1.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.joda/joda-convert/1.8.1/jars/joda-convert.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.javassist/javassist/3.20.0-GA/bundles/javassist.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.hamcrest/hamcrest-core/1.3/jars/hamcrest-core.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.fluentlenium/fluentlenium-core/0.10.9/jars/fluentlenium-core.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.eclipse.jetty.websocket/websocket-common/9.2.15.v20160210/jars/websocket-common.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.eclipse.jetty.websocket/websocket-client/9.2.15.v20160210/jars/websocket-client.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.eclipse.jetty.websocket/websocket-api/9.2.15.v20160210/jars/websocket-api.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.eclipse.jetty/jetty-util/9.2.15.v20160210/jars/jetty-util.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.eclipse.jetty/jetty-io/9.2.15. <http://9.2.15./>v20160210/jars/jetty-io.jar:/Users/mohit/.ivy2/cache/org.clapper/grizzled-scala_2.11/jars/grizzled-scala_2.11-3.0.0.jar:/Users/mohit/.ivy2/cache/org.clapper/classutil_2.11/jars/classutil_2.11-1.0.13.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.asynchttpclient/netty-resolver-dns/2.0.0-RC9/jars/netty-resolver-dns.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.asynchttpclient/netty-resolver/2.0.0-RC9/jars/netty-resolver.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.asynchttpclient/netty-codec-dns/2.0.0-RC9/jars/netty-codec-dns.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.asynchttpclient/async-http-client/2.0.0-RC9/jars/async-http-client.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.apache.httpcomponents/httpmime/4.5.2/jars/httpmime.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.apache.httpcomponents/httpcore/4.4.4/jars/httpcore.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.apache.httpcomponents/httpcore/4.0.1/jars/httpcore.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.apache.httpcomponents/httpclient/4.5.2/jars/httpclient.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.apache.httpcomponents/httpclient/4.0.1/jars/httpclient.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.apache.commons/commons-lang3/3.4/jars/commons-lang3.jar:/Users/mohit/code/activator-dist-1.3.10/repository/org.apache.commons/commons-exec/1.3/jars/commons-exec.jar:/Users/mohit/.ivy2/cache/oauth.signpost/signpost-core/jars/signpost-core-1.2.1.2.jar:/Users/mohit/.ivy2/cache/oauth.signpost/signpost-commonshttp4/jars/signpost-commonshttp4-1.2.1.2.jar:/Users/mohit/code/activator-dist-1.3.10/repository/net.sourceforge.nekohtml/nekohtml/1.9.22/jars/nekohtml.jar:/Users/mohit/code/activator-dist-1.3.10/repository/net.sourceforge.htmlunit/htmlunit-core-js/2.17/jars/htmlunit-core-js.jar:/Users/mohit/code/activator-dist-1.3.10/repository/net.sourceforge.htmlunit/htmlunit/2.20/jars/htmlunit.jar:/Users/mohit/code/activator-dist-1.3.10/repository/net.sourceforge.cssparser/cssparser/0.9.18/jars/cssparser.jar:/Users/mohit/code/activator-dist-1.3.10/repository/net.sf.ehcache/ehcache-core/2.6.11/jars/ehcache-core.jar:/Users/mohit/code/activator-dist-1.3.10/repository/net.java.dev.jna/jna-platform/4.1.0/jars/jna-platform.jar:/Users/mohit/code/activator-dist-1.3.10/repository/net.java.dev.jna/jna/4.1.0/jars/jna.jar:/Users/mohit/code/activator-dist-1.3.10/repository/junit/junit/4.12/jars/junit.jar:/Users/mohit/.ivy2/cache/joda-time/joda-time/jars/joda-time-2.9.4.jar:/Users/mohit/.ivy2/cache/javax.transaction/jta/jars/jta-1.1.jar:/Users/mohit/code/activator-dist-1.3.10/repository/javax.inject/javax.inject/1/jars/javax.inject.jar:/Users/mohit/.ivy2/cache/io.netty/netty-transport-native-epoll/jars/netty-transport-native-epoll-4.0.33.Final.jar:/Users/mohit/code/activator-dist-1.3.10/repository/io.netty/netty-transport/4.0.34.Final/jars/netty-transport.jar:/Users/mohit/code/activator-dist-1.3.10/repository/io.netty/netty-handler/4.0.34.Final/jars/netty-handler.jar:/Users/mohit/code/activator-dist-1.3.10/repository/io.netty/netty-common/4.0.34.Final/jars/netty-common.jar:/Users/mohit/code/activator-dist-1.3.10/repository/io.netty/netty-codec-http/4.0.34.Final/jars/netty-codec-http.jar:/Users/mohit/code/activator-dist-1.3.10/repository/io.netty/netty-codec/4.0.34.Final/jars/netty-codec.jar:/Users/mohit/code/activator-dist-1.3.10/repository/io.netty/netty-buffer/4.0.34.Final/jars/netty-buffer.jar:/Users/mohit/code/activator-dist-1.3.10/repository/commons-logging/commons-logging/1.2/jars/commons-logging.jar:/Users/mohit/.ivy2/cache/commons-logging/commons-logging/jars/commons-logging-1.1.1.jar:/Users/mohit/code/activator-dist-1.3.10/repository/commons-io/commons-io/2.4/jars/commons-io.jar:/Users/mohit/code/activator-dist-1.3.10/repository/commons-codec/commons-codec/1.10/jars/commons-codec.jar:/Users/mohit/.ivy2/cache/com.zaxxer/HikariCP-java6/bundles/HikariCP-java6-2.3.7.jar:/Users/mohit/.ivy2/cache/com.typesafe.slick/slick_2.11/bundles/slick_2.11-3.1.0.jar:/Users/mohit/.ivy2/cache/com.typesafe.slick/slick-hikaricp_2.11/bundles/slick-hikaricp_2.11-3.1.0.jar:/Users/mohit/code/activator-dist-1.3.10/repository/com.typesafe.play/twirl-api_2.11/1.1.1/jars/twirl-api_2.11.jar:/Users/mohit/code/activator-dist-1.3.10/repository/com.typesafe.play/play_2.11/2.5.0/jars/play_2.11.jar:/Users/mohit/code/activator-dist-1.3.10/repository/com.typesafe.play/play-ws_2.11/2.5.0/jars/play-ws_2.11.jar:/Users/mohit/code/activator-dist-1.3.10/repository/com.typesafe.play/play-test_2.11/2.5.0/jars/play-test_2.11.jar:/Users/mohit/code/activator-dist-1.3.10/repository/com.typesafe.play/play-streams_2.11/2.5.0/jars/play-streams_2.11.jar:/Users/mohit/.ivy2/cache/com.typesafe.play/play-slick_2.11/jars/play-slick_2.11-1.1.1.jar:/Users/mohit/.ivy2/cache/com.typesafe.play/play-slick-evolutions_2.11/jars/play-slick-evolutions_2.11-1.1.1.jar:/Users/mohit/code/activator-dist-1.3.10/repository/com.typesafe.play/play-server_2.11/2.5.0/jars/play-server_2.11.jar:/Users/mohit/code/activator-dist-1.3.10/repository/com.typesafe.play/play-netty-utils/2.5.0/jars/play-netty-utils.jar:/Users/mohit/code/activator-dist-1.3.10/repository/com.typesafe.play/play-netty-server_2.11/2.5.0/jars/play-netty-server_2.11.jar:/Users/mohit/.ivy2/cache/com.typesafe.play/play-logback_2.11/jars/play-logback_2.11-2.5.0.jar:/Users/mohit/.ivy2/cache/com.typesafe.play/play-json_2.11/jars/play-json_2.11-2.5.9.jar:/Users/mohit/.ivy2/cache/com.typesafe.play/play-jdbc-evolutions_2.11/jars/play-jdbc-evolutions_2.11-2.5.0.jar:/Users/mohit/.ivy2/cache/com.typesafe.play/play-jdbc-api_2.11/jars/play-jdbc-api_2.11-2.5.0.jar:/Users/mohit/.ivy2/cache/com.typesafe.play/play-iteratees_2.11/jars/play-iteratees_2.11-2.5.9.jar:/Users/mohit/.ivy2/cache/com.typesafe.play/play-functional_2.11/jars/play-functional_2.11-2.5.9.jar:/Users/mohit/code/activator-dist-1.3.10/repository/com.typesafe.play/play-exceptions/2.5.0/jars/play-exceptions.jar:/Users/mohit/.ivy2/cache/com.typesafe.play/play-datacommons_2.11/jars/play-datacommons_2.11-2.5.9.jar:/Users/mohit/.ivy2/cache/com.typesafe.play/play-cache_2.11/jars/play-cache_2.11-2.5.0.jar:/Users/mohit/code/activator-dist-1.3.10/repository/com.typesafe.play/build-link/2.5.0/jars/build-link.jar:/Users/mohit/code/activator-dist-1.3.10/repository/com.typesafe.netty/netty-reactive-streams-http/1.0.2/jars/netty-reactive-streams-http.jar:/Users/mohit/code/activator-dist-1.3.10/repository/com.typesafe.netty/netty-reactive-streams/1.0.2/jars/netty-reactive-streams.jar:/Users/mohit/code/activator-dist-1.3.10/repository/com.typesafe.akka/akka-stream_2.11/2.4.2/jars/akka-stream_2.11.jar:/Users/mohit/code/activator-dist-1.3.10/repository/com.typesafe.akka/akka-slf4j_2.11/2.4.2/jars/akka-slf4j_2.11.jar:/Users/mohit/code/activator-dist-1.3.10/repository/com.typesafe.akka/akka-actor_2.11/2.4.2/jars/akka-actor_2.11.jar:/Users/mohit/code/activator-dist-1.3.10/repository/com.typesafe/ssl-config-core_2.11/0.1.3/bundles/ssl-config-core_2.11.jar:/Users/mohit/code/activator-dist-1.3.10/repository/com.typesafe/ssl-config-akka_2.11/0.1.3/bundles/ssl-config-akka_2.11.jar:/Users/mohit/code/activator-dist-1.3.10/repository/com.typesafe/config/1.3.0/bundles/config.jar:/Users/mohit/code/activator-dist-1.3.10/repository/com.novocode/junit-interface/0.11/jars/junit-interface.jar:/Users/mohit/code/activator-dist-1.3.10/repository/com.h2database/h2/1.4.191/jars/h2.jar:/Users/mohit/code/activator-dist-1.3.10/repository/com.google.inject.extensions/guice-assistedinject/4.0/jars/guice-assistedinject.jar:/Users/mohit/code/activator-dist-1.3.10/repository/com.google.inject/guice/4.0/jars/guice.jar:/Users/mohit/code/activator-dist-1.3.10/repository/com.google.guava/guava/19.0/bundles/guava.jar:/Users/mohit/code/activator-dist-1.3.10/repository/com.google.code.gson/gson/2.3.1/jars/gson.jar:/Users/mohit/code/activator-dist-1.3.10/repository/com.google.code.findbugs/jsr305/3.0.1/jars/jsr305.jar:/Users/mohit/.ivy2/cache/com.fasterxml.jackson.datatype/jackson-datatype-jsr310/bundles/jackson-datatype-jsr310-2.7.6.jar:/Users/mohit/.ivy2/cache/com.fasterxml.jackson.datatype/jackson-datatype-jdk8/bundles/jackson-datatype-jdk8-2.7.6.jar:/Users/mohit/.ivy2/cache/com.fasterxml.jackson.core/jackson-databind/bundles/jackson-databind-2.7.6.jar:/Users/mohit/.ivy2/cache/com.fasterxml.jackson.core/jackson-core/bundles/jackson-core-2.7.6.jar:/Users/mohit/.ivy2/cache/com.fasterxml.jackson.core/jackson-annotations/bundles/jackson-annotations-2.7.6.jar:/Users/mohit/code/activator-dist-1.3.10/repository/ch.qos.logback/logback-core/1.1.4/jars/logback-core.jar:/Users/mohit/code/activator-dist-1.3.10/repository/ch.qos.logback/logback-classic/1.1.4/jars/logback-classic.jar:/Users/mohit/code/activator-dist-1.3.10/repository/cglib/cglib-nodep/2.1_3/jars/cglib-nodep.jar:/Users/mohit/code/activator-dist-1.3.10/repository/aopalliance/aopalliance/1.0/jars/aopalliance.jar:/Users/mohit/.ivy2/cache/com.clearspring.analytics/stream/jars/stream-2.7.0.jar:/Users/mohit/.ivy2/cache/com.esotericsoftware/kryo-shaded/bundles/kryo-shaded-3.0.3.jar:/Users/mohit/.ivy2/cache/com.esotericsoftware/minlog/bundles/minlog-1.3.0.jar:/Users/mohit/.ivy2/cache/com.fasterxml.jackson.module/jackson-module-paranamer/bundles/jackson-module-paranamer-2.6.5.jar:/Users/mohit/.ivy2/cache/com.fasterxml.jackson.module/jackson-module-scala_2.11/bundles/jackson-module-scala_2.11-2.6.5.jar:/Users/mohit/.ivy2/cache/com.github.fommil.netlib/core/jars/core-1.1.2.jar:/Users/mohit/.ivy2/cache/com.github.rwl/jtransforms/jars/jtransforms-2.4.0.jar:/Users/mohit/.ivy2/cache/com.google.protobuf/protobuf-java/bundles/protobuf-java-2.5.0.jar:/Users/mohit/.ivy2/cache/com.ning/compress-lzf/bundles/compress-lzf-1.0.3.jar:/Users/mohit/.ivy2/cache/com.thoughtworks.paranamer/paranamer/jars/paranamer-2.6.jar:/Users/mohit/.ivy2/cache/com.twitter/chill-java/jars/chill-java-0.8.0.jar:/Users/mohit/.ivy2/cache/com.twitter/chill_2.11/jars/chill_2.11-0.8.0.jar:/Users/mohit/.ivy2/cache/com.univocity/univocity-parsers/jars/univocity-parsers-2.1.1.jar:/Users/mohit/.ivy2/cache/commons-beanutils/commons-beanutils/jars/commons-beanutils-1.7.0.jar:/Users/mohit/.ivy2/cache/commons-beanutils/commons-beanutils-core/jars/commons-beanutils-core-1.8.0.jar:/Users/mohit/.ivy2/cache/commons-cli/commons-cli/jars/commons-cli-1.2.jar:/Users/mohit/.ivy2/cache/commons-collections/commons-collections/jars/commons-collections-3.2.1.jar:/Users/mohit/.ivy2/cache/commons-configuration/commons-configuration/jars/commons-configuration-1.6.jar:/Users/mohit/.ivy2/cache/commons-digester/commons-digester/jars/commons-digester-1.8.jar:/Users/mohit/.ivy2/cache/commons-httpclient/commons-httpclient/jars/commons-httpclient-3.1.jar:/Users/mohit/.ivy2/cache/commons-lang/commons-lang/jars/commons-lang-2.5.jar:/Users/mohit/.ivy2/cache/commons-net/commons-net/jars/commons-net-2.2.jar:/Users/mohit/.ivy2/cache/io.dropwizard.metrics/metrics-core/bundles/metrics-core-3.1.2.jar:/Users/mohit/.ivy2/cache/io.dropwizard.metrics/metrics-graphite/bundles/metrics-graphite-3.1.2.jar:/Users/mohit/.ivy2/cache/io.dropwizard.metrics/metrics-json/bundles/metrics-json-3.1.2.jar:/Users/mohit/.ivy2/cache/io.dropwizard.metrics/metrics-jvm/bundles/metrics-jvm-3.1.2.jar:/Users/mohit/code/activator-dist-1.3.10/repository/io.netty/netty/3.8.0.Final/bundles/netty.jar:/Users/mohit/.ivy2/cache/io.netty/netty-all/jars/netty-all-4.0.29.Final.jar:/Users/mohit/.ivy2/cache/javax.annotation/javax.annotation-api/jars/javax.annotation-api-1.2.jar:/Users/mohit/.ivy2/cache/javax.servlet/javax.servlet-api/jars/javax.servlet-api-3.1.0.jar:/Users/mohit/.ivy2/cache/javax.validation/validation-api/jars/validation-api-1.1.0.Final.jar:/Users/mohit/.ivy2/cache/javax.ws.rs/javax.ws.rs-api/jars/javax.ws.rs-api-2.0.1.jar:/Users/mohit/.ivy2/cache/log4j/log4j/bundles/log4j-1.2.17.jar:/Users/mohit/.ivy2/cache/net.java.dev.jets3t/jets3t/jars/jets3t-0.7.1.jar:/Users/mohit/.ivy2/cache/net.jpountz.lz4/lz4/jars/lz4-1.3.0.jar:/Users/mohit/.ivy2/cache/net.sf.opencsv/opencsv/jars/opencsv-2.3.jar:/Users/mohit/.ivy2/cache/net.sourceforge.f2j/arpack_combined_all/jars/arpack_combined_all-0.1.jar:/Users/mohit/.ivy2/cache/org.antlr/antlr4-runtime/jars/antlr4-runtime-4.5.3.jar:/Users/mohit/.ivy2/cache/org.apache.avro/avro/jars/avro-1.7.7.jar:/Users/mohit/.ivy2/cache/org.apache.avro/avro-ipc/jars/avro-ipc-1.7.7.jar:/Users/mohit/.ivy2/cache/org.apache.avro/avro-ipc/jars/avro-ipc-1.7.7-tests.jar:/Users/mohit/.ivy2/cache/org.apache.avro/avro-mapred/jars/avro-mapred-1.7.7-hadoop2.jar:/Users/mohit/.ivy2/cache/org.apache.commons/commons-compress/jars/commons-compress-1.4.1.jar:/Users/mohit/.ivy2/cache/org.apache.commons/commons-math/jars/commons-math-2.1.jar:/Users/mohit/.ivy2/cache/org.apache.commons/commons-math3/jars/commons-math3-3.4.1.jar:/Users/mohit/.ivy2/cache/org.apache.curator/curator-client/bundles/curator-client-2.4.0.jar:/Users/mohit/.ivy2/cache/org.apache.curator/curator-framework/bundles/curator-framework-2.4.0.jar:/Users/mohit/.ivy2/cache/org.apache.curator/curator-recipes/bundles/curator-recipes-2.4.0.jar:/Users/mohit/.ivy2/cache/org.apache.hadoop/hadoop-annotations/jars/hadoop-annotations-2.2.0.jar:/Users/mohit/.ivy2/cache/org.apache.hadoop/hadoop-auth/jars/hadoop-auth-2.2.0.jar:/Users/mohit/.ivy2/cache/org.apache.hadoop/hadoop-client/jars/hadoop-client-2.2.0.jar:/Users/mohit/.ivy2/cache/org.apache.hadoop/hadoop-common/jars/hadoop-common-2.2.0.jar:/Users/mohit/.ivy2/cache/org.apache.hadoop/hadoop-hdfs/jars/hadoop-hdfs-2.2.0.jar:/Users/mohit/.ivy2/cache/org.apache.hadoop/hadoop-mapreduce-client-app/jars/hadoop-mapreduce-client-app-2.2.0.jar:/Users/mohit/.ivy2/cache/org.apache.hadoop/hadoop-mapreduce-client-common/jars/hadoop-mapreduce-client-common-2.2.0.jar:/Users/mohit/.ivy2/cache/org.apache.hadoop/hadoop-mapreduce-client-core/jars/hadoop-mapreduce-client-core-2.2.0.jar:/Users/mohit/.ivy2/cache/org.apache.hadoop/hadoop-mapreduce-client-jobclient/jars/hadoop-mapreduce-client-jobclient-2.2.0.jar:/Users/mohit/.ivy2/cache/org.apache.hadoop/hadoop-mapreduce-client-shuffle/jars/hadoop-mapreduce-client-shuffle-2.2.0.jar:/Users/mohit/.ivy2/cache/org.apache.hadoop/hadoop-yarn-api/jars/hadoop-yarn-api-2.2.0.jar:/Users/mohit/.ivy2/cache/org.apache.hadoop/hadoop-yarn-client/jars/hadoop-yarn-client-2.2.0.jar:/Users/mohit/.ivy2/cache/org.apache.hadoop/hadoop-yarn-common/jars/hadoop-yarn-common-2.2.0.jar:/Users/mohit/.ivy2/cache/org.apache.hadoop/hadoop-yarn-server-common/jars/hadoop-yarn-server-common-2.2.0.jar:/Users/mohit/.ivy2/cache/org.apache.ivy/ivy/jars/ivy-2.4.0.jar:/Users/mohit/.ivy2/cache/org.apache.mesos/mesos/jars/mesos-0.21.1-shaded-protobuf.jar:/Users/mohit/.ivy2/cache/org.apache.parquet/parquet-column/jars/parquet-column-1.7.0.jar:/Users/mohit/.ivy2/cache/org.apache.parquet/parquet-common/jars/parquet-common-1.7.0.jar:/Users/mohit/.ivy2/cache/org.apache.parquet/parquet-encoding/jars/parquet-encoding-1.7.0.jar:/Users/mohit/.ivy2/cache/org.apache.parquet/parquet-format/jars/parquet-format-2.3.0-incubating.jar:/Users/mohit/.ivy2/cache/org.apache.parquet/parquet-generator/jars/parquet-generator-1.7.0.jar:/Users/mohit/.ivy2/cache/org.apache.parquet/parquet-hadoop/jars/parquet-hadoop-1.7.0.jar:/Users/mohit/.ivy2/cache/org.apache.parquet/parquet-jackson/jars/parquet-jackson-1.7.0.jar:/Users/mohit/.ivy2/cache/org.apache.xbean/xbean-asm5-shaded/bundles/xbean-asm5-shaded-4.4.jar:/Users/mohit/.ivy2/cache/org.apache.zookeeper/zookeeper/jars/zookeeper-3.4.5.jar:/Users/mohit/.ivy2/cache/org.codehaus.jackson/jackson-core-asl/jars/jackson-core-asl-1.9.13.jar:/Users/mohit/.ivy2/cache/org.codehaus.jackson/jackson-mapper-asl/jars/jackson-mapper-asl-1.9.13.jar:/Users/mohit/.ivy2/cache/org.codehaus.janino/commons-compiler/jars/commons-compiler-2.7.8.jar:/Users/mohit/.ivy2/cache/org.codehaus.janino/janino/jars/janino-2.7.8.jar:/Users/mohit/.ivy2/cache/org.fusesource.leveldbjni/leveldbjni-all/bundles/leveldbjni-all-1.8.jar:/Users/mohit/.ivy2/cache/org.glassfish.hk2/hk2-api/jars/hk2-api-2.4.0-b34.jar:/Users/mohit/.ivy2/cache/org.glassfish.hk2/hk2-locator/jars/hk2-locator-2.4.0-b34.jar:/Users/mohit/.ivy2/cache/org.glassfish.hk2/hk2-utils/jars/hk2-utils-2.4.0-b34.jar:/Users/mohit/.ivy2/cache/org.glassfish.hk2/osgi-resource-locator/jars/osgi-resource-locator-1.0.1.jar:/Users/mohit/.ivy2/cache/org.glassfish.hk2.external/aopalliance-repackaged/jars/aopalliance-repackaged-2.4.0-b34.jar:/Users/mohit/.ivy2/cache/org.glassfish.hk2.external/javax.inject/jars/javax.inject-2.4.0-b34.jar:/Users/mohit/.ivy2/cache/org.glassfish.jersey.bundles.repackaged/jersey-guava/bundles/jersey-guava-2.22.2.jar:/Users/mohit/.ivy2/cache/org.glassfish.jersey.containers/jersey-container-servlet/jars/jersey-container-servlet-2.22.2.jar:/Users/mohit/.ivy2/cache/org.glassfish.jersey.containers/jersey-container-servlet-core/jars/jersey-container-servlet-core-2.22.2.jar:/Users/mohit/.ivy2/cache/org.glassfish.jersey.core/jersey-client/jars/jersey-client-2.22.2.jar:/Users/mohit/.ivy2/cache/org.glassfish.jersey.core/jersey-common/jars/jersey-common-2.22.2.jar:/Users/mohit/.ivy2/cache/org.glassfish.jersey.core/jersey-server/jars/jersey-server-2.22.2.jar:/Users/mohit/.ivy2/cache/org.glassfish.jersey.media/jersey-media-jaxb/jars/jersey-media-jaxb-2.22.2.jar:/Users/mohit/.ivy2/cache/org.jpmml/pmml-model/jars/pmml-model-1.2.15.jar:/Users/mohit/.ivy2/cache/org.jpmml/pmml-schema/jars/pmml-schema-1.2.15.jar:/Users/mohit/.ivy2/cache/org.json4s/json4s-ast_2.11/jars/json4s-ast_2.11-3.2.11.jar:/Users/mohit/.ivy2/cache/org.json4s/json4s-core_2.11/jars/json4s-core_2.11-3.2.11.jar:/Users/mohit/.ivy2/cache/org.json4s/json4s-jackson_2.11/jars/json4s-jackson_2.11-3.2.11.jar:/Users/mohit/.ivy2/cache/org.mortbay.jetty/jetty-util/jars/jetty-util-6.1.26.jar:/Users/mohit/.ivy2/cache/org.objenesis/objenesis/jars/objenesis-2.1.jar:/Users/mohit/.ivy2/cache/org.roaringbitmap/RoaringBitmap/bundles/RoaringBitmap-0.5.11.jar:/Users/mohit/.ivy2/cache/org.scala-lang/scala-compiler/jars/scala-compiler-2.11.8.jar:/Users/mohit/.ivy2/cache/org.scala-lang/scala-reflect/jars/scala-reflect-2.11.8.jar:/Users/mohit/.ivy2/cache/org.scala-lang/scalap/jars/scalap-2.11.0.jar:/Users/mohit/.ivy2/cache/org.scala-lang.modules/scala-xml_2.11/bundles/scala-xml_2.11-1.0.4.jar:/Users/mohit/.ivy2/cache/org.scalanlp/breeze-macros_2.11/jars/breeze-macros_2.11-0.11.2.jar:/Users/mohit/.ivy2/cache/org.scalanlp/breeze_2.11/jars/breeze_2.11-0.11.2.jar:/Users/mohit/.ivy2/cache/org.slf4j/slf4j-log4j12/jars/slf4j-log4j12-1.7.16.jar:/Users/mohit/.ivy2/cache/org.spark-project.spark/unused/jars/unused-1.0.0.jar:/Users/mohit/.ivy2/cache/org.spire-math/spire-macros_2.11/jars/spire-macros_2.11-0.7.4.jar:/Users/mohit/.ivy2/cache/org.spire-math/spire_2.11/jars/spire_2.11-0.7.4.jar:/Users/mohit/.ivy2/cache/org.tukaani/xz/jars/xz-1.0.jar:/Users/mohit/.ivy2/cache/oro/oro/jars/oro-2.0.8.jar:/Users/mohit/.ivy2/cache/xmlenc/xmlenc/jars/xmlenc-0.52.jar:/Users/mohit/.ivy2/cache/net.razorvine/pyrolite/jars/pyrolite-4.9.jar:/Applications/IntelliJ <http://javax.ws.rs/javax.ws.rs-api/jars/javax.ws.rs-api-2.0.1.jar:/Users/mohit/.ivy2/cache/log4j/log4j/bundles/log4j-1.2.17.jar:/Users/mohit/.ivy2/cache/net.java.dev.jets3t/jets3t/jars/jets3t-0.7.1.jar:/Users/mohit/.ivy2/cache/net.jpountz.lz4/lz4/jars/lz4-1.3.0.jar:/Users/mohit/.ivy2/cache/net.sf.opencsv/opencsv/jars/opencsv-2.3.jar:/Users/mohit/.ivy2/cache/net.sourceforge.f2j/arpack_combined_all/jars/arpack_combined_all-0.1.jar:/Users/mohit/.ivy2/cache/org.antlr/antlr4-runtime/jars/antlr4-runtime-4.5.3.jar:/Users/mohit/.ivy2/cache/org.apache.avro/avro/jars/avro-1.7.7.jar:/Users/mohit/.ivy2/cache/org.apache.avro/avro-ipc/jars/avro-ipc-1.7.7.jar:/Users/mohit/.ivy2/cache/org.apache.avro/avro-ipc/jars/avro-ipc-1.7.7-tests.jar:/Users/mohit/.ivy2/cache/org.apache.avro/avro-mapred/jars/avro-mapred-1.7.7-hadoop2.jar:/Users/mohit/.ivy2/cache/org.apache.commons/commons-compress/jars/commons-compress-1.4.1.jar:/Users/mohit/.ivy2/cache/org.apache.commons/commons-math/jars/commons-math-2.1.jar:/Users/mohit/.ivy2/cache/org.apache.commons/commons-math3/jars/commons-math3-3.4.1.jar:/Users/mohit/.ivy2/cache/org.apache.curator/curator-client/bundles/curator-client-2.4.0.jar:/Users/mohit/.ivy2/cache/org.apache.curator/curator-framework/bundles/curator-framework-2.4.0.jar:/Users/mohit/.ivy2/cache/org.apache.curator/curator-recipes/bundles/curator-recipes-2.4.0.jar:/Users/mohit/.ivy2/cache/org.apache.hadoop/hadoop-annotations/jars/hadoop-annotations-2.2.0.jar:/Users/mohit/.ivy2/cache/org.apache.hadoop/hadoop-auth/jars/hadoop-auth-2.2.0.jar:/Users/mohit/.ivy2/cache/org.apache.hadoop/hadoop-client/jars/hadoop-client-2.2.0.jar:/Users/mohit/.ivy2/cache/org.apache.hadoop/hadoop-common/jars/hadoop-common-2.2.0.jar:/Users/mohit/.ivy2/cache/org.apache.hadoop/hadoop-hdfs/jars/hadoop-hdfs-2.2.0.jar:/Users/mohit/.ivy2/cache/org.apache.hadoop/hadoop-mapreduce-client-app/jars/hadoop-mapreduce-client-app-2.2.0.jar:/Users/mohit/.ivy2/cache/org.apache.hadoop/hadoop-mapreduce-client-common/jars/hadoop-mapreduce-client-common-2.2.0.jar:/Users/mohit/.ivy2/cache/org.apache.hadoop/hadoop-mapreduce-client-core/jars/hadoop-mapreduce-client-core-2.2.0.jar:/Users/mohit/.ivy2/cache/org.apache.hadoop/hadoop-mapreduce-client-jobclient/jars/hadoop-mapreduce-client-jobclient-2.2.0.jar:/Users/mohit/.ivy2/cache/org.apache.hadoop/hadoop-mapreduce-client-shuffle/jars/hadoop-mapreduce-client-shuffle-2.2.0.jar:/Users/mohit/.ivy2/cache/org.apache.hadoop/hadoop-yarn-api/jars/hadoop-yarn-api-2.2.0.jar:/Users/mohit/.ivy2/cache/org.apache.hadoop/hadoop-yarn-client/jars/hadoop-yarn-client-2.2.0.jar:/Users/mohit/.ivy2/cache/org.apache.hadoop/hadoop-yarn-common/jars/hadoop-yarn-common-2.2.0.jar:/Users/mohit/.ivy2/cache/org.apache.hadoop/hadoop-yarn-server-common/jars/hadoop-yarn-server-common-2.2.0.jar:/Users/mohit/.ivy2/cache/org.apache.ivy/ivy/jars/ivy-2.4.0.jar:/Users/mohit/.ivy2/cache/org.apache.mesos/mesos/jars/mesos-0.21.1-shaded-protobuf.jar:/Users/mohit/.ivy2/cache/org.apache.parquet/parquet-column/jars/parquet-column-1.7.0.jar:/Users/mohit/.ivy2/cache/org.apache.parquet/parquet-common/jars/parquet-common-1.7.0.jar:/Users/mohit/.ivy2/cache/org.apache.parquet/parquet-encoding/jars/parquet-encoding-1.7.0.jar:/Users/mohit/.ivy2/cache/org.apache.parquet/parquet-format/jars/parquet-format-2.3.0-incubating.jar:/Users/mohit/.ivy2/cache/org.apache.parquet/parquet-generator/jars/parquet-generator-1.7.0.jar:/Users/mohit/.ivy2/cache/org.apache.parquet/parquet-hadoop/jars/parquet-hadoop-1.7.0.jar:/Users/mohit/.ivy2/cache/org.apache.parquet/parquet-jackson/jars/parquet-jackson-1.7.0.jar:/Users/mohit/.ivy2/cache/org.apache.xbean/xbean-asm5-shaded/bundles/xbean-asm5-shaded-4.4.jar:/Users/mohit/.ivy2/cache/org.apache.zookeeper/zookeeper/jars/zookeeper-3.4.5.jar:/Users/mohit/.ivy2/cache/org.codehaus.jackson/jackson-core-asl/jars/jackson-core-asl-1.9.13.jar:/Users/mohit/.ivy2/cache/org.codehaus.jackson/jackson-mapper-asl/jars/jackson-mapper-asl-1.9.13.jar:/Users/mohit/.ivy2/cache/org.codehaus.janino/commons-compiler/jars/commons-compiler-2.7.8.jar:/Users/mohit/.ivy2/cache/org.codehaus.janino/janino/jars/janino-2.7.8.jar:/Users"
"
kant kodali <kanth909@gmail.com>,Thu"," 17 Nov 2016 12:24:09 -0800""","Re: How do I convert json_encoded_blob_column into a data frame?
 (This may be a feature request)",Nathan Lande <nathanlande@gmail.com>,"Can we have a JSONType for Spark SQL?


"
Reynold Xin <rxin@databricks.com>,"Thu, 17 Nov 2016 12:44:27 -0800","Re: How do I convert json_encoded_blob_column into a data frame?
 (This may be a feature request)",kant kodali <kanth909@gmail.com>,"Adding a new data type is an enormous undertaking and very invasive. I
don't think it is worth it in this case given there are clear, simple
workarounds.



"
Joseph Bradley <joseph@databricks.com>,"Thu, 17 Nov 2016 13:16:16 -0800",Re: Develop custom Estimator / Transformer for pipeline,Georg Heiler <georg.kf.heiler@gmail.com>,"Hi Georg,

It's true we need better documentation for this.  I'd recommend checking
out simple algorithms within Spark for examples:
ml.feature.Tokenizer
ml.regression.IsotonicRegression

You should not need to put your library in Spark's namespace.  The shared
Params in SPARK-7146 are not necessary to create a custom algorithm; they
are just niceties.

Though there aren't great docs yet, you should be able to follow existing
examples.  And I'd like to add more docs in the future!

Good luck,
Joseph


"
Holden Karau <holden.karau@gmail.com>,"Thu, 17 Nov 2016 13:28:48 -0800",Re: Develop custom Estimator / Transformer for pipeline,"""Joseph K. Bradley"" <joseph@databricks.com>","I've been working on a blog post around this and hope to have it published
early next month ðŸ˜€


Hi Georg,

It's true we need better documentation for this.  I'd recommend checking
out simple algorithms within Spark for examples:
ml.feature.Tokenizer
ml.regression.IsotonicRegression

You should not need to put your library in Spark's namespace.  The shared
Params in SPARK-7146 are not necessary to create a custom algorithm; they
are just niceties.

Though there aren't great docs yet, you should be able to follow existing
examples.  And I'd like to add more docs in the future!

Good luck,
Joseph


e
"
Xiao Li <gatorsmile@gmail.com>,"Thu, 17 Nov 2016 14:10:01 -0800",Re: issues with github pull request notification emails missing,Holden Karau <holden@pigscanfly.ca>,"Just FYI, normally, when we ping a people, the github can show the full
name after we type the github id. Below is an example:

[image: å†…åµŒå›¾ç‰‡ 2]

Starting from last week, Reynold's full name is not shown. Does github
update their hash functions?

[image: å†…åµŒå›¾ç‰‡ 1]

Thanks,

Xiao Li



2016-11-16 23:30 GMT-08:00 Holden Karau <holden@pigscanfly.ca>:

and
m
ll
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 17 Nov 2016 22:21:50 +0000",Re: Green dot in web UI DAG visualization,"Suhas Gaddam <suhas.g.2011@gmail.com>, Reynold Xin <rxin@databricks.com>","https://issues.apache.org/jira/browse/SPARK-18495


S,
t
on-through-visualization.html
"
shane knapp <sknapp@berkeley.edu>,"Thu, 17 Nov 2016 14:33:12 -0800",[build system] massive jenkins infrastructure changes forthcoming,"dev <dev@spark.apache.org>, Andrew Audibert <andrew@alluxio.com>, 
	Reynold Xin <rxin@databricks.com>, Michael Armbrust <michael@databricks.com>, 
	Josh Rosen <joshrosen@databricks.com>, amp-infra <amp-infra@googlegroups.com>","TL;DR:  amplab is becomine riselab, and is much more C++ oriented.
centos 6 is so far behind, and i'm already having to roll C++
compilers and various libraries by hand.  centos 7 is an absolute
no-go, so we'll be moving the jenkins workers over to a recent (TBD)
version of ubuntu server.  also, we'll finally get jenkins upgraded to
the latest LTS version, as well as our insanely out of date plugins.
riselab (me) will still run the build system, btw.

oh, we'll also have a macOS worker!

well, that was still pretty long.  :)

anyways, you have the gist of it.  this is something we're going to do
slowly, so as to not interrupt any spark, alluxio or lab builds.

i'll be spinning up a master and two worker ubuntu nodes, and then
port a couple of builds over and get the major kinks worked out.
then, early next year, we can point the new master at the old workers,
and one-by-one reinstall and deploy them w/ubuntu.

i'll be reaching out to some individuals (you know who you are) as
things progress.

if we do this right, we'll have minimal service interruptions and end
up w/a clean and fresh jenkins.  this is the opposite of our current
jenkins, which is at least 4 years old and is super-glued and
duct-taped together.

the ubuntu staging servers should be ready early next week, but i
don't foresee much work happening until after thanksgiving.

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Thu, 17 Nov 2016 16:52:22 -0800",Re: [build system] massive jenkins infrastructure changes forthcoming,shane knapp <sknapp@berkeley.edu>,"Thanks for the headsup, Shane.



"
Nick Pentreath <nick.pentreath@gmail.com>,"Fri, 18 Nov 2016 01:52:43 +0000",Re: Develop custom Estimator / Transformer for pipeline,dev@spark.apache.org,"@Holden look forward to the blog post - I think a user guide PR based on it
would also be super useful :)


d
r-in-pyspark-mllib
e
"
Georg Heiler <georg.kf.heiler@gmail.com>,"Fri, 18 Nov 2016 06:06:42 +0000",Re: Develop custom Estimator / Transformer for pipeline,"Nick Pentreath <nick.pentreath@gmail.com>, dev@spark.apache.org","Looking forward to the blog post.
Thanks for for pointing me to some of the simpler classes.
Nick Pentreath <nick.pentreath@gmail.com> schrieb am Fr. 18. Nov. 2016 um
02:53:

d
r-in-pyspark-mllib
e
"
Sean Owen <sowen@cloudera.com>,"Fri, 18 Nov 2016 10:04:43 +0000",Re: issues with github pull request notification emails missing,"Xiao Li <gatorsmile@gmail.com>, Holden Karau <holden@pigscanfly.ca>","I have seen the same issue from time to time where I couldn't see a
person's alias in the popup. Happened yesterday for me with @joshrosen. No
idea why.

I was also missing emails for a while to spam, but it seemed like a Gmail
problem. It said I had marked messages from about 6 different people as
spam, which doesn't seem like something i could have done with a mis-click.
So, worth looking out for if you're missing emails and using gmail.

Also, I noted that if you just add an ""Approve"" in the new review system
that does not generate an email. Which is maybe a good thing.

I do still see a big problem with Gmail and these messages not being
5-6 threads, which makes managing discussions really quite annoying. I
don't know if it's somehow related.


Just FYI, normally, when we ping a people, the github can show the full
name after we type the github id. Below is an example:

[image: Screenshot 2016-11-17 14.05.55.png]

Starting from last week, Reynold's full name is not shown. Does github
update their hash functions?

[image: Screenshot 2016-11-17 14.05.38.png]

Thanks,

Xiao Li



2016-11-16 23:30 GMT-08:00 Holden Karau <holden@pigscanfly.ca>:

+1 it seems like I'm missing a number of my GitHub email notifications
lately (although since I run my own mail server and forward I've been
assuming it's my own fault).


I've also had issues with having greatly delayed notifications on some of
my own pull requests but that might be unrelated.


I've noticed that a lot of github pull request notifications no longer come
to my inbox. In the past I'd get an email for every reply to a pull request
that I subscribed to (i.e. commented on). Lately I noticed for a lot of
them I didn't get any emails, but if I opened the pull requests directly on
github, I'd see the new replies. I've looked at spam folder and none of the
missing notifications are there. So it's either github not sending the
notifications, or the emails are lost in transit.

The way it manifests is that I often comment on a pull request, and then I
don't know whether the contributor (author) has updated it or not. From the
contributor's point of view, it looks like I've been ignoring the pull
request.

I think this started happening when github switched over to the new code
review mode (
https://github.com/blog/2256-a-whole-new-github-universe-announcing-new-tools-forums-and-features
)


Did anybody else notice this issue?
"
Meethu Mathew <meethu.mathew@flytxt.com>,"Fri, 18 Nov 2016 16:56:29 +0530","Re: Failed to run spark jobs on mesos due to ""hadoop"" not found.",Yu Wei <yu2003w@hotmail.com>,"Hi,

Add HADOOP_HOME=/path/to/hadoop/folder in /etc/default/mesos-slave in all
mesos agents and restart mesos

Regards,
Meethu Mathew



"
shane knapp <sknapp@berkeley.edu>,"Fri, 18 Nov 2016 09:44:22 -0800",Re: [build system] massive jenkins infrastructure changes forthcoming,Reynold Xin <rxin@databricks.com>,"no problem!

i'm really looking forward to starting w/a much cleaner slate that
what we have now.

not only are we locked in a jenkins/plugin version dependency hell
that keeps us from updating easily and regularly (esp WRT security
issues), but our jenkins logs completely useless for debugging due to
stack trace spam.

no joke, about 80%+ of the logs are stack traces.  :(

anyways, i will post more updates as they come in.

shane

---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Fri, 18 Nov 2016 14:01:45 -0800",Re: Multiple streaming aggregations in structured streaming,wszxyh <wszxyh@163.com>,"Doing this generally is pretty hard.  We will likely support algebraic
aggregate eventually, but this is not currently slotted for 2.2.  Instead I
think we will add something like mapWithState that lets users compute
arbitrary stateful things.  What is your use case?



"
Jacek Laskowski <jacek@japila.pl>,"Sat, 19 Nov 2016 21:19:07 +0100",Analyzing and reusing cached Datasets,dev <dev@spark.apache.org>,"Hi,

There might be a bug in how analyzing Datasets or looking up cached
Datasets works. I'm on master.

âžœ  spark git:(master) âœ— ./bin/spark-submit --version
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.1.0-SNAPSHOT
      /_/

Using Scala version 2.11.8, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_112
Branch master
Compiled by user jacek on 2016-11-19T08:39:43Z
Revision 2a40de408b5eb47edba92f9fe92a42ed1e78bf98
Url https://github.com/apache/spark.git
Type --help for more information.

After reviewing CacheManager and how caching works for Datasets I
thought the following query would use the cached Dataset but it does
not.

// Cache Dataset -- it is lazy
scala> val df = spark.range(1).cache
df: org.apache.spark.sql.Dataset[Long] = [id: bigint]

// Trigger caching
scala> df.show
+---+
| id|
+---+
|  0|
+---+

// Visit http://localhost:4040/storage to see the Dataset cached. And it is.

// Use the cached Dataset in another query
// Notice InMemoryRelation in use for cached queries
// It works as expected.
scala> df.withColumn(""newId"", 'id).explain(extended = true)
== Parsed Logical Plan ==
'Project [*, 'id AS newId#16]
+- Range (0, 1, step=1, splits=Some(8))

== Analyzed Logical Plan ==
id: bigint, newId: bigint
Project [id#0L, id#0L AS newId#16L]
+- Range (0, 1, step=1, splits=Some(8))

== Optimized Logical Plan ==
Project [id#0L, id#0L AS newId#16L]
+- InMemoryRelation [id#0L], true, 10000, StorageLevel(disk, memory,
deserialized, 1 replicas)
      +- *Range (0, 1, step=1, splits=Some(8))

== Physical Plan ==
*Project [id#0L, id#0L AS newId#16L]
+- InMemoryTableScan [id#0L]
      +- InMemoryRelation [id#0L], true, 10000, StorageLevel(disk,
memory, deserialized, 1 replicas)
            +- *Range (0, 1, step=1, splits=Some(8))

I hoped that the following query would use the cached one but it does
not. Should it? I thought that QueryExecution.withCachedData [1] would
do the trick.

[1] https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala#L70

// The following snippet uses spark.range(1) which is the same as the
one cached above
// Why does the physical plan not use InMemoryTableScan and InMemoryRelation?
scala> spark.range(1).withColumn(""new"", 'id).explain(extended = true)
== Parsed Logical Plan ==
'Project [*, 'id AS new#29]
+- Range (0, 1, step=1, splits=Some(8))

== Analyzed Logical Plan ==
id: bigint, new: bigint
Project [id#26L, id#26L AS new#29L]
+- Range (0, 1, step=1, splits=Some(8))

== Optimized Logical Plan ==
Project [id#26L, id#26L AS new#29L]
+- Range (0, 1, step=1, splits=Some(8))

== Physical Plan ==
*Project [id#26L, id#26L AS new#29L]
+- *Range (0, 1, step=1, splits=Some(8))

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Sat, 19 Nov 2016 18:52:41 -0800",Re: Analyzing and reusing cached Datasets,Jacek Laskowski <jacek@japila.pl>,"You are hitting a weird optimization in withColumn.  Specifically, to avoid
building up huge trees with chained calls to this method, we collapse
projections eagerly (instead of waiting for the optimizer).

Typically we look for cached data in between analysis and optimization, so
that optimizations won't change out ability to recognized cached query
plans.  However, in this case the eager optimization is thwarting that.


"
Jacek Laskowski <jacek@japila.pl>,"Sun, 20 Nov 2016 11:24:55 +0100",Re: Analyzing and reusing cached Datasets,Michael Armbrust <michael@databricks.com>,"Hi Michael,

Thanks a lot for your prompt answer. I greatly appreciate it.

Having said that, I think we might be...cough...cough...wrong :)

I think the ""issue"" is in QueryPlan.sameResult [1] as its scaladoc says:

   * Since its likely undecidable to generally determine if two given
plans will produce the same
   * results, it is okay for this function to return false, even if
the results are actually
   * the same.  Such behavior will not affect correctness, only the
application of performance
   * enhancements like caching.  However, it is not acceptable to
return true if the results could
   * possibly be different.

   * By default this function performs a modified version of equality
that is tolerant of cosmetic
   * differences like attribute naming and or expression id
differences. Operators that
   * can do better should override this function.

[1] https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/QueryPlan.scala#L370

I don't think there is anything in Analyzer or SparkOptimizer to
prevent the cached optimization.

What do you think about:

1. Adding few TRACE messages in sameResult? (I'm doing it anyway to
hunt down the ""issue"")?
2. Defining an override for sameResult in Range (as LocalRelation and
other logical operators)?

Somehow I feel Spark could do better. Please guide (and help me get
better at this low-level infra of Spark SQL). Thanks!

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski


id
o
:
apache/spark/sql/execution/QueryExecution.scala#L70

---------------------------------------------------------------------


"
Aniket <aniket.bhatnagar@gmail.com>,"Sun, 20 Nov 2016 04:35:40 -0700 (MST)",Re: OutOfMemoryError on parquet SnappyDecompressor,dev@spark.apache.org,"Was anyone able  find a solution or recommended conf for this? I am running
into the same ""java.lang.OutOfMemoryError: Direct buffer memory"" but during
snappy compression.

Thanks,
Aniket






--"
Georg Heiler <georg.kf.heiler@gmail.com>,"Sun, 20 Nov 2016 11:48:06 +0000",Re: Develop custom Estimator / Transformer for pipeline,"Holden Karau <holden@pigscanfly.ca>, ""dev@spark.apache.org"" <dev@spark.apache.org>","The estimator should perform data cleaning tasks. This means some rows will
be dropped, some columns dropped, some columns added, some values replaced
in existing columns. IT should also store the mean or min for some numeric
columns as a NaN replacement.

However,

override def transformSchema(schema: StructType): StructType = {
   schema.add(StructField(""foo"", IntegerType))}

only supports adding fields? I am curious how am I supposed to handle this.
Should I create a new column for each affected column, drop the old one and
rename afterward?


Regards,

Georg

:
d
r-in-pyspark-mllib
e
"
Reynold Xin <rxin@databricks.com>,"Sun, 20 Nov 2016 12:17:40 -0800",github mirroring is broken,"""dev@spark.apache.org"" <dev@spark.apache.org>","FYI Github mirroring from Apache's official git repo to GitHub is broken
since Sat Nov 19, and as a result GitHub is now stale. Merged pull requests
won't show up in GitHub until ASF infra fixes the issue.
"
"""Xinyu Zhang"" <wszxyh@163.com>","Mon, 21 Nov 2016 15:51:28 +0800 (CST)",Re:Re: Multiple streaming aggregations in structured streaming,"""Michael Armbrust"" <michael@databricks.com>","

MapWithState is also very useful. 
I want to calculate UV in real time, but ""distinct count"" and ""multiple streaming aggregations"" are not supported.
Is there any method to calculate real-time UV in the current version?




At 2016-11-19 06:01:45, ""Michael Armbrust"" <michael@databricks.com> wrote:

Doing this generally is pretty hard.  We will likely support algebraic aggregate eventually, but this is not currently slotted for 2.2.  Instead I think we will add something like mapWithState that lets users compute arbitrary stateful things.  What is your use case?




On Wed, Nov 16, 2016 at 6:58 PM, wszxyh <wszxyh@163.com> wrote:

Hi


Multiple streaming aggregations are not yet supported. When will it be supported? Is it in the plan?


Thanks




 


"
Reynold Xin <rxin@databricks.com>,"Sun, 20 Nov 2016 23:56:21 -0800",Re: Re: Multiple streaming aggregations in structured streaming,Xinyu Zhang <wszxyh@163.com>,"Can you use the approximate count distinct?



"
Niranda Perera <niranda.perera@gmail.com>,"Mon, 21 Nov 2016 14:33:07 +0530","How is the order ensured in the jdbc relation provider when inserting
 data from multiple executors","""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

Say, I have a table with 1 column and 1000 rows. I want to save the result
in a RDBMS table using the jdbc relation provider. So I run the following
query,

""insert into table table2 select value, count(*) from table1 group by value
order by value""

While debugging, I found that the resultant df from select value, count(*)
from table1 group by value order by value would have around 200+ partitions
and say I have 4 executors attached to my driver. So, I would have 200+
writing tasks assigned to 4 executors. I want to understand, how these
executors are able to write the data to the underlying RDBMS table of
table2 without messing up the order.

I checked the jdbc insertable relation and in jdbcUtils [1] it does the
following

df.foreachPartition { iterator =>
      savePartition(getConnection, table, iterator, rddSchema, nullTypes,
batchSize, dialect)
    }

So, my understanding is, all of my 4 executors will parallely run the
savePartition function (or closure) where they do not know which one should
write data before the other!

In the savePartition method, in the comment, it says
""Saves a partition of a DataFrame to the JDBC database.  This is done in
   * a single database transaction in order to avoid repeatedly inserting
   * data as much as possible.""

I want to understand, how these parallel executors save the partition
without harming the order of the results? Is it by locking the database
resource, from each executor (i.e. ex0 would first obtain a lock for the
table and write the partition0, while ex1 ... ex3 would wait till the lock
is released )?

In my experience, there is no harm done to the order of the results at the
end of the day!

Would like to hear from you guys! :-)

[1]
https://github.com/apache/spark/blob/v1.6.2/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils.scala#L277

-- 
Niranda Perera
@n1r44 <https://twitter.com/N1R44>
+94 71 554 8430
https://www.linkedin.com/in/niranda
https://pythagoreanscript.wordpress.com/
"
Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Mon, 21 Nov 2016 10:32:23 +0100","Re: How is the order ensured in the jdbc relation provider when
 inserting data from multiple executors",dev@spark.apache.org,"In commonly used RDBM systems relations have no fixed order and physical
location of the records can change during routine maintenance
operations. Unless you explicitly order data during retrieval order you
see is incidental and not guaranteed. 

Conclusion: order of inserts just doesn't matter.


-- 
Best regards,
Maciej Szymkiewicz

"
Ryan Blue <rblue@netflix.com.INVALID>,"Mon, 21 Nov 2016 07:23:49 -0800",Re: OutOfMemoryError on parquet SnappyDecompressor,Aniket <aniket.bhatnagar@gmail.com>,"Aniket,

The solution was to add a sort so that only one file is written at a time,
which minimizes the memory footprint of columnar formats like Parquet.
That's been released for quite a while, so memory issues caused by Parquet
are more rare now. If you're using Parquet default settings and a recent
Spark version, you should be fine.

rb


e
l
3)
va:99)
5)
9)
201)
1)
93)
)
:339)
toreImpl.java:63)
eImpl.java:58)
)
)
va:130)
39)
:327)
:388)
:327)
:327)
6)
1157)
5)
181)
ava:1145)
java:615)
Servlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
or-on-parquet-SnappyDecompressor-tp8517p19965.html>



-- 
Ryan Blue
Software Engineer
Netflix
"
Aniket <aniket.bhatnagar@gmail.com>,"Mon, 21 Nov 2016 11:07:38 -0700 (MST)",Re: OutOfMemoryError on parquet SnappyDecompressor,dev@spark.apache.org,"Thanks Ryan. I am running into this rarer issue. For now, I have moved away
from parquet but if I will create a bug in jira if I am able to produce
code that easily reproduces this.

Thanks,
Aniket






--"
Ryan Blue <rblue@netflix.com.INVALID>,"Mon, 21 Nov 2016 10:43:28 -0800",Re: OutOfMemoryError on parquet SnappyDecompressor,Aniket <aniket.bhatnagar@gmail.com>,"It's unlikely that you're hitting this, unless you have several tasks
writing at once on the same executor. Parquet does have high memory
consumption, so the most likely explanation is either that you're close to
the memory limit for other reasons, or that you need to increase the amount
of overhead memory for off-heap tasks.

rb

:

]
0>>
 by
a
e
l
3)
va:99)
5)
9)
201)
1)
93)
)
:339)
toreImpl.java:63)
eImpl.java:58)
)
)
va:130)
39)
:327)
:388)
:327)
:327)
6)
1157)
5)
181)
ava:1145)
java:615)
Servlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
ror-on-parquet-SnappyDecompressor-tp8517p19965.html>
Servlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
or-on-parquet-SnappyDecompressor-tp8517p19973.html>



-- 
Ryan Blue
Software Engineer
Netflix
"
Joeri Hermans <joeri.raymond.e.hermans@cern.ch>,"Mon, 21 Nov 2016 20:25:31 +0000",MinMaxScaler behaviour,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,

I observed some weird behaviour while applying some feature transformations using MinMaxScaler. More specifically, I was wondering if this behaviour is intended and makes sense? Especially because I explicitly defined min and max.

Basically, I am preprocessing the MNIST dataset, and thereby scaling the features between the ranges 0 and 1 using the following code:

# Clear the dataset in the case you ran this cell before.
dataset = dataset.select(""features"", ""label"", ""label_encoded"")
# Apply MinMax normalization to the features.
scaler = MinMaxScaler(min=0.0, max=1.0, inputCol=""features"", outputCol=""features_normalized"")
# Compute summary statistics and generate MinMaxScalerModel.
scaler_model = scaler.fit(dataset)
# Rescale each feature to range [min, max].
dataset = scaler_model.transform(dataset)

Complete code is here: https://github.com/JoeriHermans/dist-keras/blob/development/examples/mnist.ipynb (Normalization section)

The original MNIST images are shown in original.png. Whereas the processed images are shown in processed.png. Note the 0.5 artifacts. I checked the source code of this particular estimator / transformer and found the following.

https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/feature/MinMaxScaler.scala#L191

According to the documentation:

 * <p><blockquote>
 *    $$
 *    Rescaled(e_i) = \frac{e_i - E_{min}}{E_{max} - E_{min}} * (max - min) + min
 *    $$
 * </blockquote></p>
 *
 * For the case $E_{max} == E_{min}$, $Rescaled(e_i) = 0.5 * (max + min)$.

So basically, when the difference between E_{max} and E_{min} is 0, we assing 0.5 as a raw value. I am wondering if this is helpful in any situation? Why not assign 0?



Kind regards,

Joeri
---------------------------------------------------------------------"
Sean Owen <sowen@cloudera.com>,"Mon, 21 Nov 2016 20:32:13 +0000",Re: MinMaxScaler behaviour,"Joeri Hermans <joeri.raymond.e.hermans@cern.ch>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","It's a degenerate case of course. 0, 0.5 and 1 all make about as much
sense. Is there a strong convention elsewhere to use 0?

Min/max scaling is the wrong thing to do for a data set like this anyway.
What you probably intend to do is scale each image so that its max
intensity is 1 and min intensity is 0, but that's different. Scaling each
pixel across all images doesn't make as much sense.


"
Joeri Hermans <joeri.raymond.e.hermans@cern.ch>,"Mon, 21 Nov 2016 20:43:57 +0000",RE: MinMaxScaler behaviour,"Sean Owen <sowen@cloudera.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","I see. I think I read the documentation a little bit too quick :)

My apologies.


Kind regards,

Joeri
________________________________________
From: Sean Owen [sowen@cloudera.com]
Sent: 21 November 2016 21:32
To: Joeri Hermans; dev@spark.apache.org
Subject: Re: MinMaxScaler behaviour

It's a degenerate case of course. 0, 0.5 and 1 all make about as much sense. Is there a strong convention elsewhere to use 0?

Min/max scaling is the wrong thing to do for a data set like this anyway. What you probably intend to do is scale each image so that its max intensity is 1 and min intensity is 0, but that's different. Scaling each pixel across all images doesn't make as much sense.

Hi all,

I observed some weird behaviour while applying some feature transformations using MinMaxScaler. More specifically, I was wondering if this behaviour is intended and makes sense? Especially because I explicitly defined min and max.

Basically, I am preprocessing the MNIST dataset, and thereby scaling the features between the ranges 0 and 1 using the following code:

# Clear the dataset in the case you ran this cell before.
dataset = dataset.select(""features"", ""label"", ""label_encoded"")
# Apply MinMax normalization to the features.
scaler = MinMaxScaler(min=0.0, max=1.0, inputCol=""features"", outputCol=""features_normalized"")
# Compute summary statistics and generate MinMaxScalerModel.
scaler_model = scaler.fit(dataset)
# Rescale each feature to range [min, max].
dataset = scaler_model.transform(dataset)

Complete code is here: https://github.com/JoeriHermans/dist-keras/blob/development/examples/mnist.ipynb (Normalization section)

The original MNIST images are shown in original.png. Whereas the processed images are shown in processed.png. Note the 0.5 artifacts. I checked the source code of this particular estimator / transformer and found the following.

https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/feature/MinMaxScaler.scala#L191

According to the documentation:

 * <p><blockquote>
 *    $$
 *    Rescaled(e_i) = \frac{e_i - E_{min}}{E_{max} - E_{min}} * (max - min) + min
 *    $$
 * </blockquote></p>
 *
 * For the case $E_{max} == E_{min}$, $Rescaled(e_i) = 0.5 * (max + min)$.

So basically, when the difference between E_{max} and E_{min} is 0, we assing 0.5 as a raw value. I am wondering if this is helpful in any situation? Why not assign 0?



Kind regards,

Joeri
---------------------------------------------------------------------
ibe@spark.apache.org>

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 21 Nov 2016 21:16:51 +0000",Re: Memory leak warnings in Spark 2.0.1,"vonnagy <ivan@vadio.com>, dev@spark.apache.org","I'm also curious about this. Is there something we can do to help
troubleshoot these leaks and file useful bug reports?


"
Joseph Bradley <joseph@databricks.com>,"Mon, 21 Nov 2016 15:19:35 -0800",Please limit commits for branch-2.1,"""dev@spark.apache.org"" <dev@spark.apache.org>","To committers and contributors active in MLlib,

Thanks everyone who has started helping with the QA tasks in SPARK-18316!
I'd like to request that we stop committing non-critical changes to MLlib,
including the Python and R APIs, since still-changing public APIs make it
hard to QA.  We need have already started to sign off on some QA tasks, but
we may need to re-open them if changes are committed, especially if those
changes are to public APIs.  There's no need to push Python and R wrappers
into 2.1 at the last minute.

Let's focus on completing QA, after which we can resume committing API
changes to master (not branch-2.1).

Thanks everyone!
Joseph


-- 

Joseph Bradley

Software Engineer - Machine Learning

Databricks, Inc.

[image: http://databricks.com] <http://databricks.com/>
"
Jose Soltren <jose@cloudera.com>,"Mon, 21 Nov 2016 21:31:45 -0600",[SPARK-16654][CORE][WIP] Add UI coverage for Application Level Blacklisting,dev@spark.apache.org,"Hi - I'm proposing a patch set for UI coverage of Application Level
Blacklisting:

https://github.com/jsoltren/spark/pull/1

This patch set builds on top of Imran Rashid's pending pull request,

[SPARK-8425][CORE] Application Level Blacklisting #14079
https://github.com/apache/spark/pull/14079/commits

The best way I could find to send this for review was to fork and
clone apache/spark, pull PR 14079, branch, apply my UI changes, and
issue a pull request of the UI changes into PR 14079. If there is a
better way, forgive me, I would love to hear it.

Attached is a screen shot of the updated UI.

I would appreciate feedback on this WIP patch. I will issue a formal
pull request once PR 14079 is merged.

Cheers,
--JosÃ©

---------------------------------------------------------------------"
Reynold Xin <rxin@databricks.com>,"Tue, 22 Nov 2016 04:22:34 +0000",Re: [SPARK-16654][CORE][WIP] Add UI coverage for Application Level Blacklisting,"Jose Soltren <jose@cloudera.com>, dev@spark.apache.org","You can submit a pull request against Imran's branch for the pull request.


"
Sachith Withana <sachith@wso2.com>,"Tue, 22 Nov 2016 10:33:08 +0530",Re: How to convert spark data-frame to datasets?,Oscar Batori <oscarbatori@gmail.com>,"Hi Minudika,

To add to what Oscar said, this blog post [1] should clarify it for you.
And this should be posted in the user-list not the dev.

[1]
https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html

Cheers,
Sachith




-- 
Sachith Withana
Software Engineer; WSO2 Inc.; http://wso2.com
E-mail: sachith AT wso2.com
M: +94715518127
Linked-In: <http://goog_416592669>https://lk.linkedin.com/in/sachithwithana
"
Minudika Malshan <minudika001@gmail.com>,"Tue, 22 Nov 2016 12:12:34 +0530",Re: How to convert spark data-frame to datasets?,Sachith Withana <sachith@wso2.com>,"Hi,

Thanks all for the support. And sorry for the mistake done by posting here
instead of users list. :)

BR





-- 
*Minudika Malshan*
Undergraduate
Department of Computer Science and Engineering
University of Moratuwa
Sri Lanka.
<https://lk.linkedin.com/pub/minudika-malshan/100/656/a80>
"
Sean Owen <sowen@cloudera.com>,"Tue, 22 Nov 2016 09:53:06 +0000",Re: Please limit commits for branch-2.1,"Joseph Bradley <joseph@databricks.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Maybe I missed it, but did anyone declare a QA period? In the past I've not
seen this, and just seen people start talking retrospectively about how
""we're in QA now"" until it stops. We have
https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage saying it
is already over, but clearly we're not doing RCs.

We should make this more formal and predictable. We probably need a clearer
definition of what changes in QA. I'm moving the wiki to spark.apache.org
now and could try to put up some words around this when I move this page
above today.


"
nirandap <niranda.perera@gmail.com>,"Tue, 22 Nov 2016 04:11:31 -0700 (MST)","Re: How is the order ensured in the jdbc relation provider when
 inserting data from multiple executors",dev@spark.apache.org,"Hi Maciej,

Thank you for your reply.

I have 2 queries.
1. I can understand your explanation. But in my experience, when I check
the final RDBMS table, I see that the results follow the expected order,
without an issue. Is this just a coincidence?

2. I was further looking into this. So, say I run this query
""select value, count(*) from table1 group by value order by value""

and I call df.collect() in the resultant dataframe. From my experience, I
see that the given values follow the expected order. May I know how spark
manages to retain the order of the results in a collect operation?

Best






-- 
Niranda Perera
@n1r44 <https://twitter.com/N1R44>
+94 71 554 8430
https://www.linkedin.com/in/niranda
https://pythagoreanscript.wordpress.com/




--"
Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Tue, 22 Nov 2016 12:53:42 +0100","Re: How is the order ensured in the jdbc relation provider when
 inserting data from multiple executors",dev@spark.apache.org,"
Not exactly a coincidence. This is typically a result of a physical
location on the disk. If writes and reads are sequential, (this is
usually the case) you'll see things in the expected order, but you have
to remember that location on disk is not stable. For example if you
perform some updates, deletes and VACUM ALL (PostgreSQL) physical
location on disk will change and with it things you see.

There of course more advanced mechanisms out there. For example modern
columnar RDBMS like HANA use techniques like dimensions sorting and
differential stores so even the initial order may differ. There probably
some other solutions which choose different strategies (maybe some times
series oriented projects?) I am not aware of.

order of partitions defines the global ordering. All what collect does
is just preserving this order by creating an array of results for each
partition and flattening it.

-- 
Maciej Szymkiewicz

"
Dirceu Semighini Filho <dirceu.semighini@gmail.com>,"Tue, 22 Nov 2016 11:51:32 -0200",[SparkStreaming] 1 SQL tab for each SparkStreaming batch in SparkUI,"""dev@spark.apache.org"" <dev@spark.apache.org>","Has anybody seen this behavior (see tha attached picture) in Spark
Streaming?
It started to happen here after I changed the HiveContext creation to
stream.foreachRDD {
rdd =>
val hiveContext = new HiveContext(rdd.sparkContext)
}

Is this expected?

Kind Regards,
Dirceu

---------------------------------------------------------------------"
Koert Kuipers <koert@tresata.com>,"Tue, 22 Nov 2016 11:17:52 -0500",Re: [SparkStreaming] 1 SQL tab for each SparkStreaming batch in SparkUI,Dirceu Semighini Filho <dirceu.semighini@gmail.com>,"you are creating a new hive context per microbatch? is that a good idea?


"
Jose Soltren <jose@cloudera.com>,"Tue, 22 Nov 2016 10:21:11 -0600",Re: [SPARK-16654][CORE][WIP] Add UI coverage for Application Level Blacklisting,Reynold Xin <rxin@databricks.com>,"Hi Reynold - Alright, if that makes things easier, here is the same
patch set against Imran's squito:blacklist-SPARK-8425 branch.

https://github.com/squito/spark/pull/6

Cheers,
--JosÃ©


.

---------------------------------------------------------------------


"
Dirceu Semighini Filho <dirceu.semighini@gmail.com>,"Tue, 22 Nov 2016 14:25:15 -0200",Re: [SparkStreaming] 1 SQL tab for each SparkStreaming batch in SparkUI,Koert Kuipers <koert@tresata.com>,"Hi Koert,
Certainly it's not a good idea, I was trying to use SQLContext.getOrCreate
but it will return a SQLContext and not a HiveContext.
As I'm using a checkpoint, whenever I start the context by reading the
checkpoint it didn't create my hive context, unless I create it foreach
microbach.
I didn't find a way to use the same hivecontext for all batches.
Does anybody know where can I find how to do this?




2016-11-22 14:17 GMT-02:00 Koert Kuipers <koert@tresata.com>:

"
Reynold Xin <rxin@databricks.com>,"Tue, 22 Nov 2016 17:30:55 +0000",Re: Please limit commits for branch-2.1,"Joseph Bradley <joseph@databricks.com>, Sean Owen <sowen@cloudera.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","I did send an email out with those information on Nov 1st. It is not meant
to be in new feature development mode anymore.

FWIW, I will cut an RC today to remind people of that. The RC will fail,
but it can serve as a good reminder.


"
Sean Owen <sowen@cloudera.com>,"Tue, 22 Nov 2016 17:37:16 +0000",Re: Please limit commits for branch-2.1,"Reynold Xin <rxin@databricks.com>, Joseph Bradley <joseph@databricks.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","Thanks, this was another message that went to spam for me:

http://apache-spark-developers-list.1001551.n3.nabble.com/ANNOUNCE-Apache-Spark-branch-2-1-td19688.html

Looks great -- cutting branch = in RC period.


"
"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Tue, 22 Nov 2016 10:33:06 -0800",Re: [SparkStreaming] 1 SQL tab for each SparkStreaming batch in SparkUI,Dirceu Semighini Filho <dirceu.semighini@gmail.com>,"If you create a HiveContext before starting StreamingContext, then
`SQLContext.getOrCreate` in foreachRDD will return the HiveContext you
created. You can just call asInstanceOf[HiveContext] to convert it to
HiveContext.


"
Zak H <zak.hassan1010@gmail.com>,"Tue, 22 Nov 2016 16:50:46 -0500","Is it possible to pass ""-javaagent=customAgent.jar"" into spark as a JAVA_OPTS",dev@spark.apache.org,"Hi,

I'm interested in passing an agent that will expose jmx metrics from spark
to my agent. I wanted to know if anyone has tried this and if so what
environment variable do I need to set ?

Do I set: $SPARK_DAEMON_JAVA_OPTS ??

http://docs.oracle.com/javase/7/docs/api/java/lang/instrument/package-summary.html


Thank you,
Zak Hassan
"
Reynold Xin <rxin@databricks.com>,"Tue, 22 Nov 2016 23:59:10 -0800",Re: Memory leak warnings in Spark 2.0.1,Nicholas Chammas <nicholas.chammas@gmail.com>,"See https://issues.apache.org/jira/browse/SPARK-18557
<https://issues.apache.org/jira/browse/SPARK-18557>


"
Jiang Xingbo <jiangxb1987@gmail.com>,"Wed, 23 Nov 2016 02:57:34 -0700 (MST)",Re: view canonicalization - looking for database gurus to chime in,dev@spark.apache.org,"Hi all,

I have recently prepared a design document for Spark SQL robust view
canonicalization, in the doc we defined the expected behavior and described
a late binding approach, views created by older versions of Spark/HIVE are
still supposed to work under this new approach.
For more details, please review:
https://docs.google.com/document/d/16NEDA9ejzAQAkm_tRqEVpaUAXgXuUo5gTQsbnItOUsA

This doc is accomplished under the guidance of Herman van Hovell, I've
learned a lot from the process.
Thanks to Reynold Xin, Srinath Shankar for the valuable advices and for
helping improve the quality of this design doc.
Any suggestions and discussions are welcomed, thank you all!

Sincerely,
Xingbo



--

---------------------------------------------------------------------


"
Artur Sukhenko <artur.sukhenko@gmail.com>,"Wed, 23 Nov 2016 11:09:36 +0000","Re: Is it possible to pass ""-javaagent=customAgent.jar"" into spark as
 a JAVA_OPTS","Zak H <zak.hassan1010@gmail.com>, dev@spark.apache.org","Hello Zak,

I believe this video from Spark Summit would be useful for you:
https://youtu.be/EB1-7AXQOhM

They are talking about extending Spark with Java agents.


--
Artur Sukhenko
"
Sean Owen <sowen@cloudera.com>,"Wed, 23 Nov 2016 11:29:07 +0000",Spark Wiki now migrated to spark.apache.org,dev <dev@spark.apache.org>,"I completed the migration. You can see the results live right now at
http://spark.apache.org, and
https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage

A summary of the changes:
https://issues.apache.org/jira/browse/SPARK-18073

The substance of the changes:
https://github.com/apache/spark-website/pull/19
https://github.com/apache/spark-website/pull/25

No information has been lost. Old wikis still either exist as they were
with an ""end of life"" notice, or, point to the new location of the
information.

I ported the content basically as-is, only making minor changes to fix
obviously out of date content.

I did alter the menu structure, most significantly to add a ""Developer""
menu.

Of course, we can change it further. Please comment if you see any errors,
don't like some of the choices, etc.

Note that all the release docs are now also updated according in branch 2.1.
"
"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Wed, 23 Nov 2016 05:36:39 -0700 (MST)",Aggregating over sorted data,dev@spark.apache.org,"Hi,
An issue I have encountered frequently is the need to look at data in an ordered manner per key.
A common way of doing this can be seen in the classic map reduce as the shuffle stage provides sorted data per key and one can therefore do a lot with that.
It is of course relatively easy to achieve this by using RDD but that would mean moving to RDD and back which has a non-insignificant performance penalty (beyond the fact that we lose any catalyst optimization).
We can use SQL window functions but that is not an ideal solution either. Beyond the fact that window functions are much slower than aggregate functions (as we need to generate a result for every record), we also can't join them together (i.e. if we have two window functions on the same window, it is still two separate scans).

Ideally, I would have liked to see something like: df.groupBy(col1).sortBy(col2).agg(...) and have the aggregations work on the sorted data. That would enable to use both the existing functions and UDAF where we can assume the order (and do any windowing we need as part of the function itself which is relatively easy in many cases).

I have tried to look for this and seen many questions on the subject but no answers.

I was hoping I missed something (I have seen that the SQL CLUSTER BY command repartitions and sorts accordingly but from my understanding it does not promise that this would remain true if we do a groupby afterwards). If I didn't, I believe this should be a feature to add (I can open a JIRA if people think it is a good idea).
Assaf.





--"
Lydia Ickler <icklerly@googlemail.com>,"Wed, 23 Nov 2016 13:37:38 +0100","PowerIterationClustering can't handle ""large"" files",dev@spark.apache.org,"Hi all,

I have a question regarding the Power Iteration Clustering.
I have an input file (tab separated edge list) which I read in and map it to the required format of RDD[(Long, Long, Double)] to then apply PIC.
So far so goodâ€¦ 
The implementation works fine if the input is small (up to 50MB). 
But it crashes if I try to apply it to a file of size 650 MB.
My technical setup is a compute cluster with 1 master 2 workers. The executor memory is set to 50 GB and in total 24 cores are available.

Is it normal that  the program crashes at such a file size?
I attached my program code as well as the error output.

I hope someone can help me!
Best regards, 
Lydia


16/11/23 13:34:19 INFO spark.SparkContext: Running Spark version 2.1.0-SNAPSHOT
16/11/23 13:34:20 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/11/23 13:34:20 INFO spark.SecurityManager: Changing view acls to: icklerly
16/11/23 13:34:20 INFO spark.SecurityManager: Changing modify acls to: icklerly
16/11/23 13:34:20 INFO spark.SecurityManager: Changing view acls groups to: 
16/11/23 13:34:20 INFO spark.SecurityManager: Changing modify acls groups to: 
16/11/23 13:34:20 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(icklerly); groups with view permissions: Set(); users  with modify permissions: Set(icklerly); groups with modify permissions: Set()
16/11/23 13:34:20 INFO util.Utils: Successfully started service 'sparkDriver' on port 36371.
16/11/23 13:34:20 INFO spark.SparkEnv: Registering MapOutputTracker
16/11/23 13:34:20 INFO spark.SparkEnv: Registering BlockManagerMaster
16/11/23 13:34:20 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
16/11/23 13:34:20 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
16/11/23 13:34:20 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-80b089a7-be21-4d14-ab6f-7e0ef1f14396
16/11/23 13:34:20 INFO memory.MemoryStore: MemoryStore started with capacity 396.3 MB
16/11/23 13:34:20 INFO spark.SparkEnv: Registering OutputCommitCoordinator
16/11/23 13:34:20 INFO util.log: Logging initialized @1120ms
16/11/23 13:34:20 INFO server.Server: jetty-9.2.z-SNAPSHOT
16/11/23 13:34:20 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3543df7d{/jobs,null,AVAILABLE}
16/11/23 13:34:20 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7c541c15{/jobs/json,null,AVAILABLE}
16/11/23 13:34:20 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3542162a{/jobs/job,null,AVAILABLE}
16/11/23 13:34:20 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@698122b2{/jobs/job/json,null,AVAILABLE}
16/11/23 13:34:20 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4212a0c8{/stages,null,AVAILABLE}
16/11/23 13:34:20 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1e7aa82b{/stages/json,null,AVAILABLE}
16/11/23 13:34:20 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3b2c0e88{/stages/stage,null,AVAILABLE}
16/11/23 13:34:20 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5bd82fed{/stages/stage/json,null,AVAILABLE}
16/11/23 13:34:20 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@c1bd0be{/stages/pool,null,AVAILABLE}
16/11/23 13:34:20 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@476b0ae6{/stages/pool/json,null,AVAILABLE}
16/11/23 13:34:20 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1c6804cd{/storage,null,AVAILABLE}
16/11/23 13:34:20 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@655f7ea{/storage/json,null,AVAILABLE}
16/11/23 13:34:20 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@549949be{/storage/rdd,null,AVAILABLE}
16/11/23 13:34:20 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4b3a45f1{/storage/rdd/json,null,AVAILABLE}
16/11/23 13:34:20 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@17a87e37{/environment,null,AVAILABLE}
16/11/23 13:34:20 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3eeb318f{/environment/json,null,AVAILABLE}
16/11/23 13:34:20 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@20a14b55{/executors,null,AVAILABLE}
16/11/23 13:34:20 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@39ad977d{/executors/json,null,AVAILABLE}
16/11/23 13:34:20 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6da00fb9{/executors/threadDump,null,AVAILABLE}
16/11/23 13:34:20 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@a202ccb{/executors/threadDump/json,null,AVAILABLE}
16/11/23 13:34:20 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@20f12539{/static,null,AVAILABLE}
16/11/23 13:34:20 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@75b25825{/,null,AVAILABLE}
16/11/23 13:34:20 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@18025ced{/api,null,AVAILABLE}
16/11/23 13:34:20 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@13cf7d52{/jobs/job/kill,null,AVAILABLE}
16/11/23 13:34:20 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3a3e4aff{/stages/stage/kill,null,AVAILABLE}
16/11/23 13:34:20 INFO server.ServerConnector: Started ServerConnector@2cae1042{HTTP/1.1}{0.0.0.0:4040}
16/11/23 13:34:20 INFO server.Server: Started @1207ms
16/11/23 13:34:20 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
16/11/23 13:34:20 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://130.73.20.224:4040
16/11/23 13:34:20 INFO spark.SparkContext: Added JAR file:/home/icklerly/spark-master/examples/target/scala-2.11/jars/spark-examples_2.11-2.1.0-SNAPSHOT.jar at spark://130.73.20.224:36371/jars/spark-examples_2.11-2.1.0-SNAPSHOT.jar with timestamp 1479904460674
16/11/23 13:34:20 INFO client.StandaloneAppClient$ClientEndpoint: Connecting to master spark://medlab04:7077...
16/11/23 13:34:20 INFO client.TransportClientFactory: Successfully created connection to medlab04/130.73.20.224:7077 after 25 ms (0 ms spent in bootstraps)
16/11/23 13:34:20 INFO cluster.StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20161123133420-0006
16/11/23 13:34:20 INFO client.StandaloneAppClient$ClientEndpoint: Executor added: app-20161123133420-0006/0 on worker-20161123131030-130.73.21.134-38384 (130.73.21.134:38384) with 12 cores
16/11/23 13:34:20 INFO cluster.StandaloneSchedulerBackend: Granted executor ID app-20161123133420-0006/0 on hostPort 130.73.21.134:38384 with 12 cores, 50.0 GB RAM
16/11/23 13:34:20 INFO client.StandaloneAppClient$ClientEndpoint: Executor added: app-20161123133420-0006/1 on worker-20161123131042-130.73.20.224-35492 (130.73.20.224:35492) with 12 cores
16/11/23 13:34:20 INFO cluster.StandaloneSchedulerBackend: Granted executor ID app-20161123133420-0006/1 on hostPort 130.73.20.224:35492 with 12 cores, 50.0 GB RAM
16/11/23 13:34:20 INFO client.StandaloneAppClient$ClientEndpoint: Executor updated: app-20161123133420-0006/1 is now RUNNING
16/11/23 13:34:20 INFO client.StandaloneAppClient$ClientEndpoint: Executor updated: app-20161123133420-0006/0 is now RUNNING
16/11/23 13:34:20 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36463.
16/11/23 13:34:20 INFO netty.NettyBlockTransferService: Server created on 130.73.20.224:36463
16/11/23 13:34:20 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
16/11/23 13:34:20 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 130.73.20.224, 36463, None)
16/11/23 13:34:20 INFO storage.BlockManagerMasterEndpoint: Registering block manager 130.73.20.224:36463 with 396.3 MB RAM, BlockManagerId(driver, 130.73.20.224, 36463, None)
16/11/23 13:34:20 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 130.73.20.224, 36463, None)
16/11/23 13:34:20 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 130.73.20.224, 36463, None)
16/11/23 13:34:21 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@486be205{/metrics/json,null,AVAILABLE}
16/11/23 13:34:21 INFO cluster.StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
16/11/23 13:34:21 WARN mllib.PIC$: Start:I
16/11/23 13:34:21 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 128.0 KB, free 396.2 MB)
16/11/23 13:34:21 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.4 KB, free 396.2 MB)
16/11/23 13:34:21 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 130.73.20.224:36463 (size: 14.4 KB, free: 396.3 MB)
16/11/23 13:34:21 INFO spark.SparkContext: Created broadcast 0 from textFile at PIC.scala:28
16/11/23 13:34:21 WARN mllib.PIC$: End:I
16/11/23 13:34:22 WARN scheduler.TaskSetManager: Lost task 3.0 in stage 2.0 (TID 13, 130.73.21.134, executor 0): java.lang.ClassCastException: cannot assign instance of scala.collection.immutable.List$SerializationProxy to field org.apache.spark.rdd.RDD.org$apache$spark$rdd$RDD$$dependencies_ of type scala.collection.Seq in instance of org.apache.spark.rdd.MapPartitionsRDD
	at java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2133)
	at java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1305)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2024)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:373)
	at scala.collection.immutable.List$SerializationProxy.readObject(List.scala:479)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1909)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:373)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:85)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

16/11/23 13:34:22 ERROR scheduler.TaskSetManager: Task 3 in stage 2.0 failed 4 times; aborting job
Exception in thread ""main"" org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 2.0 failed 4 times, most recent failure: Lost task 3.3 in stage 2.0 (TID 23, 130.73.21.134, executor 0): java.lang.ClassCastException: cannot assign instance of scala.collection.immutable.List$SerializationProxy to field org.apache.spark.rdd.RDD.org$apache$spark$rdd$RDD$$dependencies_ of type scala.collection.Seq in instance of org.apache.spark.rdd.MapPartitionsRDD
	at java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2133)
	at java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1305)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2024)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:373)
	at scala.collection.immutable.List$SerializationProxy.readObject(List.scala:479)
	at sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1909)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:373)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:85)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1436)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1424)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)
	at eduler.scala:1651)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1606)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1595)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1914)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1977)
	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1078)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358)
	at org.apache.spark.rdd.RDD.fold(RDD.scala:1072)
	at org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$sum$1.apply$mcD$sp(DoubleRDDFunctions.scala:35)
	at org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$sum$1.apply(DoubleRDDFunctions.scala:35)
	at org.apache.spark.rdd.DoubleRDDFunctions$$anonfun$sum$1.apply(DoubleRDDFunctions.scala:35)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358)
	at org.apache.spark.rdd.DoubleRDDFunctions.sum(DoubleRDDFunctions.scala:34)
	at org.apache.spark.mllib.clustering.PowerIterationClustering$.initDegreeVector(PowerIterationClustering.scala:447)
	at org.apache.spark.mllib.clustering.PowerIterationClustering.run(PowerIterationClustering.scala:209)
	at org.apache.spark.examples.mllib.PIC$.main(PIC.scala:42)
	at org.apache.spark.examples.mllib.PIC.main(PIC.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.ClassCastException: cannot assign instance of scala.collection.immutable.List$SerializationProxy to field org.apache.spark.rdd.RDD.org$apache$spark$rdd$RDD$$dependencies_ of type scala.collection.Seq in instance of org.apache.spark.rdd.MapPartitionsRDD
	at java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2133)
	at java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1305)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2024)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:373)
	at scala.collection.immutable.List$SerializationProxy.readObject(List.scala:479)
	at sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1909)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:373)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:85)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
"
Artur Sukhenko <artur.sukhenko@gmail.com>,"Wed, 23 Nov 2016 14:46:20 +0000",[Spark Thriftserver] connection timeout option?,dev <dev@spark.apache.org>,"Hello devs,

Lets say there is/are user(s) who are using: T*ableau
desktop+spark+sparkSQL* and *Hive server* *2* is installed but they use
*spark* for the thrift server connection.

They are trying to configure spark to drop Thrift Connection when there is
inactivity for this specific user and the timer expire.

Available parameters(mentioned below) which can achieve these results are
not working for spark thriftserver.
~~~~
      1- *hive.server2.long.polling.timeout*
      2- *hive.server2.idle.session.timeout*
      3- *hive.server2.idle.operation.timeout*
~~~~

Do we have any other available parameter which can achieve idle timeout
results for spark(thrift server) ?
Will this require a new development or if its even possible ?

Sincerely,

Artur
-- 
--
Artur Sukhenko
"
Holden Karau <holden@pigscanfly.ca>,"Wed, 23 Nov 2016 16:53:59 +0000",Re: Spark Wiki now migrated to spark.apache.org,"Sean Owen <sowen@cloudera.com>, dev <dev@spark.apache.org>","That's awesome thanks for doing the migration :)


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 23 Nov 2016 17:13:41 +0000",Re: Memory leak warnings in Spark 2.0.1,Reynold Xin <rxin@databricks.com>,"ðŸ‘  Thanks for the reference and PR.


r
-
nings-in-Spark-2-0-1-tp19424.html
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 23 Nov 2016 17:17:01 +0000",Re: Spark Wiki now migrated to spark.apache.org,"Holden Karau <holden@pigscanfly.ca>, Sean Owen <sowen@cloudera.com>, dev <dev@spark.apache.org>","Same here. Nice to be able to deprecate most of the docs living on the wiki
and refer to them on GitHub.


"
Roman Shaposhnik <roman@shaposhnik.org>,"Thu, 24 Nov 2016 00:11:19 +0300","FOSDEM 2017 HPC, Bigdata and Data Science DevRoom CFP is closing soon",rvs@apache.org,"Hi!

apologies for the extra wide distribution (this exhausts my once
a year ASF mail-to-all-bigdata-projects quota ;-)) but I wanted
to suggest that all of you should consider submitting talks
to FOSDEM 2017 HPC, Bigdata and Data Science DevRoom:
    https://hpc-bigdata-fosdem17.github.io/

It was a great success this year and we hope to make it an even
bigger success in 2017.

Besides -- FOSDEM is the biggest gathering of open source
developers on the face of the earth -- don't miss it!

Thanks,
Roman.

P.S. If you have any questions -- please email me directly and
see you all in Brussels!

---------------------------------------------------------------------


"
"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Wed, 23 Nov 2016 23:51:56 -0700 (MST)",RE: Handling questions in the mailing lists,dev@spark.apache.org,"Sorry to reawaken this, but I just noticed it is possible to propose new topic specific sites (http://area51.stackexchange.com/faq)  for stack overflow. So for example we might have a spark.stackexchange.com spark specific site.
The advantage of such a site are many. First of all it is spark specific. Secondly the reputation of people would be on spark and not on general questions and lastly (and most importantly in my opinion) it would have spark based moderators (which are all spark moderator as opposed to general technology).


I would suggest trying to set this up.

Thanks,
                Assaf


From: Denny Lee [via Apache Spark Developers List] [mailto:ml-node+s1001551n19916h85@n3.nabble.com]
Sent: Wednesday, November 16, 2016 4:33 PM
To: Mendelson, Assaf
Subject: Re: Handling questions in the mailing lists

Awesome stuff! Thanks Sean! :-)
I updated the wiki to point to the /community.html page. (We're going to migrate the wiki real soon now anyway)

I updated the /community.html page per this thread too. PR: https://github.com/apache/spark-website/pull/16



Should probably also update the helping others section in the how to contribute section (<a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing&#43;to&#43;Spark#ContributingtoSpark-ContributingbyHelpingOtherUsers<https://cwiki.apache.org/confluence/display/SPARK/Contributing+to&%2343;Spark%23ContributingtoSpark-ContributingbyHelpingOtherUsers>"">https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-ContributingbyHelpingOtherUsers)

Assaf.



From: Denny Lee [via Apache Spark Developers List] [mailto:[hidden email]</user/SendEmail.jtp?type=node&node=19916&i=2>[hidden email]<http://user/SendEmail.jtp?type=node&node=19891&i=0>]
Sent: Sunday, November 13, 2016 8:52 AM



To: Mendelson, Assaf
Subject: Re: Handling questions in the mailing lists







Hey Reynold,






Looks like we all of the proposed changes into Proposed Community Mailing Lists / StackOverflow Changes<https://docs.google.com/document/d/1N0pKatcM15cqBPqFWCqIy6jdgNzIoacZlYDCjufBh2s/edit#heading=h.xshc1bv4sn3p>. Anything else we can do to update the Spark Community page / welcome email?





Meanwhile, let's all start answering questions on SO, eh?! :)


Denny







That's a good question, looking at http://stackoverflow.com/tags/apache-spark/topusers shows a few contributors who have already been active on SO including some committers and PMC members with very high overall SO reputations for any administrative needs (as well as a number of other contributors besides just PMC/committers).






I was just wondering, before we move on to SO.

Do we have enough contributors with enough reputation do manage things in SO?

We would need contributors with enough reputation to have relevant privilages.

For example: creating tags (requires 1500 reputation), edit questions and answers (2000), create tag synonums (2500), approve tag wiki edits (5000), access to moderator tools (10000, this is required to delete questions etc.), protect questions (15000).

All of these are important if we plan to have SO as a main resource.

I know I originally suggested SO, however, if we do not have contributors with the required privileges and the willingness to help manage everything then I am not sure this is a good fit.

Assaf.





From: Denny Lee [via Apache Spark Developers List] [mailto:[hidden email]<http://user/SendEmail.jtp?type=node&node=19835&i=2>[hidden email]<http://user/SendEmail.jtp?type=node&node=19800&i=0>]




Sent: Wednesday, November 09, 2016 9:54 AM
To: Mendelson, Assaf
Subject: Re: Handling questions in the mailing lists






Agreed that by simply just moving the questions to SO will not solve anything but I think the call out about the meta-tags is that we need to abide by SO rules and if we were to just jump in and start creating meta-tags, we would be violating at minimum the spirit and at maximum the actual conventions around SO.





Saying this, perhaps we could suggest tags that we place in the header of the question whether it be SO or the mailing lists that will help us sort through all of these questions faster just as you suggested. The Proposed Community Mailing Lists / StackOverflow Changes<https://docs.google.com/document/d/1N0pKatcM15cqBPqFWCqIy6jdgNzIoacZlYDCjufBh2s/edit#heading=h.xshc1bv4sn3p> has been updated to include suggested tags. WDYT?


________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/Handling-questions-in-the-mailing-lists-tp19690p19916.html
To start a new topic under Apache Spark Developers List, email ml-node+s1001551n1h20@n3.nabble.com<mailto:ml-node+s1001551n1h20@n3.nabble.com>
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>




--"
Sean Owen <sowen@cloudera.com>,"Thu, 24 Nov 2016 10:31:12 +0000",Re: Handling questions in the mailing lists,"""assaf.mendelson"" <assaf.mendelson@rsa.com>, dev@spark.apache.org","I don't think there's nearly enough traffic to sustain a stand-alone SE. I
helped mod the Data Science SE and it's still not technically critical mass
after 2 years. It would just fracture the discussion to yet another place.


al
˜followâ€™
goal is to get at
(basically this is
ns
"
"""assaf.mendelson"" <assaf.mendelson@rsa.com>","Thu, 24 Nov 2016 03:44:56 -0700 (MST)",RE: Handling questions in the mailing lists,dev@spark.apache.org,"I am not sure what is enough traffic. Some of the SE groups already existing do not have that much traffic.
Specifically the  user mailing list has ~50 emails per day. It wouldnâ€™t be much of a stretch to extract 1-2 questions per day from that.  In the regular stackoverflow the apache-spark had more than 50 new questions in the last 24 hours alone (http://stackoverflow.com/questions/tagged/apache-spark?sort=newest&pageSize=50).

I believe this should be enough traffic (and the traffic would rise once quality answers begin to appear).


From: Sean Owen [via Apache Spark Developers List] [mailto:ml-node+s1001551n20007h6@n3.nabble.com]
Sent: Thursday, November 24, 2016 12:32 PM
To: Mendelson, Assaf
Subject: Re: Handling questions in the mailing lists

I don't think there's nearly enough traffic to sustain a stand-alone SE. I helped mod the Data Science SE and it's still not technically critical mass after 2 years. It would just fracture the discussion to yet another place.
Sorry to reawaken this, but I just noticed it is possible to propose new topic specific sites (http://area51.stackexchange.com/faq)  for stack overflow. So for example we might have a spark.stackexchange.com<http://spark.stackexchange.com> spark specific site.
The advantage of such a site are many. First of all it is spark specific. Secondly the reputation of people would be on spark and not on general questions and lastly (and most importantly in my opinion) it would have spark based moderators (which are all spark moderator as opposed to general technology).

The process of creating such a site is not complicated. Basically someone creates a proposal (I have no problem doing so). Then creating 5 example questions (something we want on the site) and get 5 people need to â€˜followâ€™ it within 3 days. This creates a â€œdefinitionâ€ phase. The goal is to get at least 40 questions that embody the goal of the site and have at least 10 net votes and enough people follow it. When enough traction has been made (enough questions and enough followers) then the site moves to commitment phase. In this phase users â€œcommitâ€ to being on the site (basically this is aimed to see the community of e means the site becomes active and it will become a full site if it sees enough traction.

I would suggest trying to set this up.

Thanks,
                Assaf


________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/Handling-questions-in-the-mailing-lists-tp19690p20007.html
To start a new topic under Apache Spark Developers List, email ml-node+s1001551n1h20@n3.nabble.com<mailto:ml-node+s1001551n1h20@n3.nabble.com>
spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=1&code=YXNzYWYubWVuZGVsc29uQHJzYS5jb218MXwtMTI4OTkxNTg1Mg==>.
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>




--
3.nabble.com/Handling-questions-in-the-mailing-lists-tp19690p20008.html
om."
Sean Owen <sowen@cloudera.com>,"Thu, 24 Nov 2016 10:52:39 +0000",Re: Handling questions in the mailing lists,"""assaf.mendelson"" <assaf.mendelson@rsa.com>, dev@spark.apache.org","Here's a view into the requirements, for example:
http://area51.stackexchange.com/proposals/76571/emacs

You're right there is a lot of activity on SO, easily 30-40 questions per
most questions relevant to it were still posted on SO or Cross Validated.
It struggles as an SE even though there is, out there, more than enough
activity that _should_ be on the specific SE.

There are more niche things that end up working as an SE, so I'm not dead
set against it, though it would remain unofficial and my gut is that it
might just split the conversation yet further. I'd leave it, however, to
anyone active on SO already to decide that it's worth a dedicated SE and
just do it.


€™t be
Size=50).
den
I
ss
.
al
˜followâ€™
goal is to get at
(basically this is
ns
ons-in-the-mailing-lists-tp19690p20007.html
ervlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
ions-in-the-mailing-lists-tp19690p20008.html>
"
<Ioannis.Deligiannis@nomura.com>,"Thu, 24 Nov 2016 11:40:08 +0000",RE: Handling questions in the mailing lists,"<sowen@cloudera.com>, <assaf.mendelson@rsa.com>, <dev@spark.apache.org>","â€¦my 0.00001 cent â˜º
As a Spark and SO user, I would not find a separate SE a good thing.

*Part of the SO beauty is that you can filter easily and track different topics from one dashboard.
*Being part of SO also gets good exposure as it raises awareness of Spark across a wider audience.
*High reputation users, even if they are say â€œpython centricâ€, add value by moderating/commenting.
*I donâ€™t think Spark-specific is a good thing either. Spark is typically combined with a huge range of other technologies (Avro, Parquet, Hadoop, Python, R, Scala, Akka, Java, HBase to name a few). Users that are specialists in these topics can provide value and help build quality in Spark tag. By getting a new SE you kind of exclude them.
*It will take time to build enough reputable users to share the moderation burden
*A high-rep Java user is likely to ask a good question. Forcing people to join an SE with rep being reset you will lose the ability to track your user (and may I say potential Evangelists) quality. By observation(no idea if true), questions by high-rep users attract much better attention than any user with 100 or less.
*Last but not least, high-rep users usually know, follow and impose SO rules and best practices quite well where a Spark centric SE might not be as rule-focused. Even though rules can sometimes be annoying, overall they build quality questions so more users get involved.


From: Sean Owen [mailto:sowen@cloudera.com]
Sent: 24 November 2016 10:53
To: assaf.mendelson; dev@spark.apache.org
Subject: Re: Handling questions in the mailing lists

Here's a view into the requirements, for example: http://area51.stackexchange.com/proposals/76571/emacs<https://urldefense.proofpoint.com/v2/url?u=http-3A__area51.stackexchange.com_proposals_76571_emacs&d=DgMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=MpViTGYJ6D4gvTaxzibcvkTcjzAglGjcAiOkSkJqHZA&s=JbblqRhi6skf8IQckq_B0uUmi-vtEU4-eByD_-XzH_0&e=>

You're right there is a lot of activity on SO, easily 30-40 questions per day. One thing I noticed about, for example, the Data Science SE is that most questions relevant to it were still posted on SO or Cross Validated. It struggles as an SE even though there is, out there, more than enough activity that _should_ be on the specific SE.

There are more niche things that end up working as an SE, so I'm not dead set against it, though it would remain unofficial and my gut is that it might just split the conversation yet further. I'd leave it, however, to anyone active on SO already to decide that it's worth a dedicated SE and just do it.

On Thu, Nov 24, 2016 at 10:45 AM assaf.mendelson <assaf.mendelson@rsa.com<mailto:assaf.mendelson@rsa.com>> wrote:
I am not sure what is enough traffic. Some of the SE groups already existing do not have that much traffic.
Specifically the  user mailing list has ~50 emails per day. It wouldnâ€™t be much of a stretch to extract 1-2 questions per day from that.  In the regular stackoverflow the apache-spark had more than 50 new questions in the last 24 hours alone (http://stackoverflow.com/questions/tagged/apache-spark?sort=newest&pageSize=50<https://urldefense.proofpoint.com/v2/url?u=http-3A__stackoverflow.com_questions_tagged_apache-2Dspark-3Fsort-3Dnewest-26pageSize-3D50&d=DgMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=MpViTGYJ6D4gvTaxzibcvkTcjzAglGjcAiOkSkJqHZA&s=PIWKkzz2E50ALvSppI-egkjBJr0ZJO7MFrLw48XUIqk&e=>).

I believe this should be enough traffic (and the traffic would rise once quality answers begin to appear).


From: Sean Owen [via Apache Spark Developers List] [mailto:ml-node+<mailto:ml-node%2B>[hidden email]<https://urldefense.proofpoint.com/v2/url?u=http-3A___user_SendEmail.jtp-3Ftype-3Dnode-26node-3D20008-26i-3D0&d=DgMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=MpViTGYJ6D4gvTaxzibcvkTcjzAglGjcAiOkSkJqHZA&s=CmvGVA6SmAfyMrgYe09vDeLguHlYysDT9MQjmpxqZsg&e=>]
Sent: Thursday, November 24, 2016 12:32 PM

To: Mendelson, Assaf
Subject: Re: Handling questions in the mailing lists

I don't think there's nearly enough traffic to sustain a stand-alone SE. I helped mod the Data Science SE and it's still not technically critical mass after 2 years. It would just fracture the discussion to yet another place.
On Thu, Nov 24, 2016 at 6:52 AM assaf.mendelson <[hidden email]<https://urldefense.proofpoint.com/v2/url?u=http-3A___user_SendEmail.jtp-3Ftype-3Dnode-26node-3D20007-26i-3D0&d=DgMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=MpViTGYJ6D4gvTaxzibcvkTcjzAglGjcAiOkSkJqHZA&s=t_Eyig5OkwFVjh1bJTau690DaZUMy3chrAYd8qfOcJ4&e=>> wrote:
Sorry to reawaken this, but I just noticed it is possible to propose new topic specific sites (http://area51.stackexchange.com/faq<https://urldefense.proofpoint.com/v2/url?u=http-3A__area51.stackexchange.com_faq&d=DgMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=MpViTGYJ6D4gvTaxzibcvkTcjzAglGjcAiOkSkJqHZA&s=77_HseiyPB7dPbk_5dTD1pSUdETnfCXO-lFSj260eBo&e=>)  for stack overflow. So for example we might have a spark.stackexchange.com<https://urldefense.proofpoint.com/v2/url?u=http-3A__spark.stackexchange.com&d=DgMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=MpViTGYJ6D4gvTaxzibcvkTcjzAglGjcAiOkSkJqHZA&s=_SSyTzp_nNyqJ-4JbSyuX79ARA-ZaR7GITWFnCvsm6M&e=> spark specific site.
The advantage of such a site are many. First of all it is spark specific. Secondly the reputation of people would be on spark and not on general questions and lastly (and most importantly in my opinion) it would have spark based moderators (which are all spark moderator as opposed to general technology).

The process of creating such a site is not complicated. Basically someone creates a proposal (I have no problem doing so). Then creating 5 example questions (something we want on the site) and get 5 people need to â€˜followâ€™ it within 3 days. This creates a â€œdefinitionâ€ phase. The goal is to get at least 40 questions that embody the goal of the site and have at least 10 net votes and enough people follow it. When enough traction has been made (enough questions and enough followers) then the site moves to commitment phase. In this phase users â€œcommitâ€ to being on the site (basically this is aimed to see the community of experts is big enough). Once all this happens the site moves into beta. This means the site becomes active and it will become a full site if it sees enough traction.

I would suggest trying to set this up.

Thanks,
                Assaf

If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/Handling-questions-in-the-mailing-lists-tp19690p20007.html<https://urldefense.proofpoint.com/v2/url?u=http-3A__apache-2Dspark-2Ddevelopers-2Dlist.1001551.n3.nabble.com_Handling-2Dquestions-2Din-2Dthe-2Dmailing-2Dlists-2Dtp19690p20007.html&d=DgMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=MpViTGYJ6D4gvTaxzibcvkTcjzAglGjcAiOkSkJqHZA&s=eegtwNTGbMVSJeNJ3YRsT6kfojNzsY3-yy7vbdLXfhY&e=>
To start a new topic under Apache Spark Developers List, email [hidden email]<https://urldefense.proofpoint.com/v2/url?u=http-3A___user_SendEmail.jtp-3Ftype-3Dnode-26node-3D20008-26i-3D1&d=DgMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=MpViTGYJ6D4gvTaxzibcvkTcjzAglGjcAiOkSkJqHZA&s=ZqXu1PBOL6cUGNIGWfZAGV6UQadiGFNAypvPh8j85fU&e=>
To unsubscribe from Apache Spark Developers List, click here.
NAML<https://urldefense.proofpoint.com/v2/url?u=http-3A__apache-2Dspark-2Ddevelopers-2Dlist.1001551.n3.nabble.com_template_NamlServlet.jtp-3Fmacro-3Dmacro-5Fviewer-26id-3Dinstant-5Fhtml-2521nabble-253Aemail.naml-26base-3Dnabble.naml.namespaces.BasicNamespace-2Dnabble.view.web.template.NabbleNamespace-2Dnabble.naml.namespaces.BasicNamespace-2Dnabble.view.web.template.NabbleNamespace-2Dnabble.naml.namespaces.BasicNamespace-2Dnabble.view.web.template.NabbleNamespace-2Dnabble.view.web.template.NodeNamespace-26breadcrumbs-3Dnotify-5Fsubscribers-2521nabble-253Aemail.naml-2Dinstant-5Femails-2521nabble-253Aemail.naml-2Dsend-5Finstant-5Femail-2521nabble-253Aemail.naml&d=DgMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=MpViTGYJ6D4gvTaxzibcvkTcjzAglGjcAiOkSkJqHZA&s=D62ut_PNCSYmyBNZaPNQfBfoLHshgf48g51EQGMImgo&e=>

________________________________
View this message in context: RE: Handling questions in the mailing lists<https://urldefense.proofpoint.com/v2/url?u=http-3A__apache-2Dspark-2Ddevelopers-2Dlist.1001551.n3.nabble.com_Handling-2Dquestions-2Din-2Dthe-2Dmailing-2Dlists-2Dtp19690p20008.html&d=DgMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=MpViTGYJ6D4gvTaxzibcvkTcjzAglGjcAiOkSkJqHZA&s=Wviow8ZKdlo1L45W-fNJw0YuXtIY46HX1FJ883Vsvr0&e=>
Sent from the Apache Spark Developers List mailing list archive<https://urldefense.proofpoint.com/v2/url?u=http-3A__apache-2Dspark-2Ddevelopers-2Dlist.1001551.n3.nabble.com_&d=DgMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=MpViTGYJ6D4gvTaxzibcvkTcjzAglGjcAiOkSkJqHZA&s=nC-nitRX9xPQIidJbx-rnkS7XyVcQ3JIPi85RvcqTec&e=> at Nabble.com.


This e-mail (including any attachments) is private and confidential, may contain proprietary or privileged information and is intended for the named recipient(s) only. Unintended recipients are strictly prohibited from taking action on the basis of information in this e-mail and must contact the sender immediately, delete this e-mail (and all attachments) and destroy any hard copies. Nomura will not accept responsibility or liability for the accuracy or completeness of, or the presence of any virus or disabling code in, this e-mail. If verification is sought please request a hard copy. Any reference to the terms of executed transactions should be treated as preliminary only and subject to formal written confirmation by Nomura. Nomura reserves the right to retain, monitor and intercept e-mail communications through its networks (subject to and in accordance with applicable laws). No confidentiality or privilege is waived or lost by Nomura by any mistransmission of this e-mail. Any reference to ""Nomura"" is a reference to any entity in the Nomura Holdings, Inc. group. Please read our Electronic Communications Legal Notice which forms part of this e-mail: http://www.Nomura.com/email_disclaimer.htm

"
eliasah <abouhaydar.elias@gmail.com>,"Thu, 24 Nov 2016 05:37:36 -0700 (MST)",Re: Handling questions in the mailing lists,dev@spark.apache.org,"Besides the traffic eventual issue, I don't believe that it would benefit
users to get a standalone site. Some great answers are provided by users
that aren't spark experts but maybe java, python, aws or even some system
experts why do we want to play alone ? 

We are trying nevertheless the animate the apache spark chat room which
isn't as obvious as one might want it to be. 

I'd rather things stay the way they are on SO. There is a bunch of us that
actually are very active and answer as much as we can and we'll be glad to
help.



--

---------------------------------------------------------------------


"
marco rocchi <rocchi.1407763@studenti.uniroma1.it>,"Thu, 24 Nov 2016 16:33:28 +0100",SparkUI via proxy,dev@spark.apache.org,"Hi,
I'm working with Apache Spark in order to develop my master thesis.I'm new
in spark and working with cluster. I searched through internet but I didn't
found a way to solve.
My problem is the following one: from my pc I can access to a master node
of a cluster only via proxy.
To connect to proxy and then to master node,I have to set up an ssh tunnel,
but from parctical point of view I have no idea of how in this way I can
interact with WebUI spark.
Anyone can help me?
Thanks in advance
"
Georg Heiler <georg.kf.heiler@gmail.com>,"Thu, 24 Nov 2016 16:41:08 +0000",Re: SparkUI via proxy,"marco rocchi <rocchi.1407763@studenti.uniroma1.it>, dev@spark.apache.org","Sehr Port forwarding will help you out.
marco rocchi <rocchi.1407763@studenti.uniroma1.it> schrieb am Do. 24. Nov.
2016 um 16:33:

"
nirandap <niranda.perera@gmail.com>,"Thu, 24 Nov 2016 19:35:08 -0700 (MST)","Re: How is the order ensured in the jdbc relation provider when
 inserting data from multiple executors",dev@spark.apache.org,"Hi Maciej,

my #1 point.
I put local[4] and shouldn't this be forcing spark to read from 4
partitions in parallel and write in parallel (by parallel I mean, the order
from which partition, the data is read from a set of 4 partitions, is
non-deterministic)? That was the reason why I was surprised to see that the
final results are in the same order.





-- 
Niranda Perera
@n1r44 <https://twitter.com/N1R44>
+94 71 554 8430
https://www.linkedin.com/in/niranda
https://pythagoreanscript.wordpress.com/




--"
Nitin Goyal <nitin2goyal@gmail.com>,"Fri, 25 Nov 2016 10:36:09 +0530",Parquet-like partitioning support in spark SQL's in-memory columnar cache,dev@spark.apache.org,"Hi,

Do we have any plan of supporting parquet-like partitioning support in
Spark SQL in-memory cache? Something like one RDD[CachedBatch] per
in-memory cache partition.


-Nitin
"
Reynold Xin <rxin@databricks.com>,"Thu, 24 Nov 2016 21:59:42 -0800","Re: Parquet-like partitioning support in spark SQL's in-memory
 columnar cache",Nitin Goyal <nitin2goyal@gmail.com>,"It's already there isn't it? The in-memory columnar cache format.



"
Prasun Ratn <prasun.ratn@gmail.com>,"Fri, 25 Nov 2016 12:59:46 +0530",Scaling issues due to contention in Random,Apache Spark Dev <dev@spark.apache.org>,"Hi,

I am seeing perf degradation in the Spark/Pi example on a single-node
setup (using local[K])

Using 1, 2, 4, and 8 cores, this is the execution time in seconds for
the same number of iterations:-
Random: 4.0, 7.0, 12.96, 17.96

If I change the code to use ThreadLocalRandom
(https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/SparkPi.scala#L35)
it scales properly:-
ThreadLocalRandom: 2.2, 1.4, 1.07, 1.00

I see a similar issue in Kryo serializer in another app - the push
function shows up at the top of profile data, but goes away completely
if I use ThreadLocalRandom

https://github.com/EsotericSoftware/kryo/blob/master/src/com/esotericsoftware/kryo/util/ObjectMap.java#L259

The JDK documentation
(https://docs.oracle.com/javase/7/docs/api/java/util/concurrent/ThreadLocalRandom.html)
says:

cts in concurrent programs will typically encounter much less overhead and contention. Use of ThreadLocalRandom? is particularly appropriate when multiple tasks (for example, each a ForkJoinTask? ) use random numbers in parallel in thread pools

I am using Spark 1.5 and Java 1.8.0_91.

Is there any reason to prefer Random over ThreadLocalRandom?

Thanks
Prasun

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Fri, 25 Nov 2016 08:06:42 +0000",Re: Scaling issues due to contention in Random,"Prasun Ratn <prasun.ratn@gmail.com>, Apache Spark Dev <dev@spark.apache.org>","SparkPi is just an example, so its performance doesn't really matter.
Simpler is better.
Kryo could be an issue but that would be a change in Kryo.


"
Ewan Leith <ewan.leith@realitymine.com>,"Fri, 25 Nov 2016 09:19:03 +0000",RE: SparkUI via proxy,"Georg Heiler <georg.kf.heiler@gmail.com>, marco rocchi
	<rocchi.1407763@studenti.uniroma1.it>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","This is more of a question for the spark userâ€™s list, but if you look at FoxyProxy and SSH tunnels itâ€™ll get you going.

These instructions from AWS for accessing EMR are a good start

http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/emr-ssh-tunnel.html

http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/emr-connect-master-node-proxy.html

Ewan

From: Georg Heiler [mailto:georg.kf.heiler@gmail.com]
Sent: 24 November 2016 16:41
To: marco rocchi <rocchi.1407763@studenti.uniroma1.it>; dev@spark.apache.org
Subject: Re: SparkUI via proxy

Sehr Port forwarding will help you out.
marco rocchi <rocchi.1407763@studenti.uniroma1.it<mailto:rocchi.1407763@studenti.uniroma1.it>> schrieb am Do. 24. Nov. 2016 um 16:33:
Hi,
I'm working with Apache Spark in order to develop my master thesis.I'm new in spark and working with cluster. I searched through internet but I didn't found a way to solve.
My problem is the following one: from my pc I can access to a master node of a cluster only via proxy.
To connect to proxy and then to master node,I have to set up an ssh tunnel, but from parctical point of view I have no idea of how in this way I can interact with WebUI spark.
Anyone can help me?
Thanks in advance



This email and any attachments to it may contain confidential information and are intended solely for the addressee.



If you are not the intended recipient of this email or if you believe you have received this email in error, please contact the sender and remove it from your system.Do not use, copy or disclose the information contained in this email or in any attachment.

RealityMine Limited may monitor email traffic data including the content of email for the purposes of security.

RealityMine Limited is a company registered in England and Wales. Registered number: 07920936 Registered office: Warren Bruce Court, Warren Bruce Road, Trafford Park, Manchester M17 1LB
"
Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Fri, 25 Nov 2016 13:50:55 +0100",[SQL][JDBC] Possible regression in JDBC reader,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

I've been reviewing my notes to https://git.io/v1UVC using Spark built
from 51b1c1551d3a7147403b9e821fcc7c8f57b4824c and it looks like JDBC
ignores both:

  * (columnName, lowerBound, upperBound, numPartitions)
  * predicates

and loads everything into a single partition. Can anyone confirm that?
It works just fine on 2.0.2 and before.

-- 
Best,
Maciej

"
Takeshi Yamamuro <linguin.m.s@gmail.com>,"Fri, 25 Nov 2016 22:02:29 +0900",Re: [SQL][JDBC] Possible regression in JDBC reader,Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Hi,

Seems we forget to pass `parts:Array[Partition]` into `JDBCRelation`.
This was removed in this commit:
https://github.com/apache/spark/commit/b3130c7b6a1ab4975023f08c3ab02ee8d2c7e995#diff-f70bda59304588cc3abfa3a9840653f4L237

// maropu




-- 
---
Takeshi Yamamuro
"
Sean Owen <sowen@cloudera.com>,"Fri, 25 Nov 2016 13:23:59 +0000",Re: [SQL][JDBC] Possible regression in JDBC reader,"Takeshi Yamamuro <linguin.m.s@gmail.com>, Maciej Szymkiewicz <mszymkiewicz@gmail.com>, 
	Hyukjin Kwon <gurwls223@gmail.com>, Xiao Li <gatorsmile@gmail.com>","See https://github.com/apache/spark/pull/15499#discussion_r89008564 in
particular. Hyukjin / Xiao do we need to undo part of this change?


"
Hyukjin Kwon <gurwls223@gmail.com>,"Fri, 25 Nov 2016 22:32:09 +0900",Re: [SQL][JDBC] Possible regression in JDBC reader,Sean Owen <sowen@cloudera.com>,"I believe https://github.com/apache/spark/pull/15975 fixes this regression.

I am sorry for the trouble.

2016-11-25 22:23 GMT+09:00 Sean Owen <sowen@cloudera.com>:

"
marco rocchi <rocchi.1407763@studenti.uniroma1.it>,"Fri, 25 Nov 2016 17:19:16 +0100",Re: SparkUI via proxy,Ewan Leith <ewan.leith@realitymine.com>,"Thanks for the helping.
I've created my ssh tunnel at port 4040, and setted browser firefox SOCKS
to localhost:4040.
Now When I run a job I can read from INFO message: ""SparkUI activated at
http://192.168.1.204:4040"". But if I open the browser and type local host
or http://192.168.1.204:4040, webUI doesn't appear.
Where I'm wrong?
The question could be stupid, but I never worked with spark over a cluster
:)

Thanks
Marco

2016-11-25 10:19 GMT+01:00 Ewan Leith <ewan.leith@realitymine.com>:

look at
w
't
y
t
n
"
Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Fri, 25 Nov 2016 17:28:10 +0100",Re: [SQL][JDBC] Possible regression in JDBC reader,Takeshi Yamamuro <linguin.m.s@gmail.com>,"Thank you.



-- 
Best,
Maciej

"
Ricardo Almeida <ricardo.almeida@actnowib.com>,"Fri, 25 Nov 2016 18:04:30 +0100",Re: SparkUI via proxy,marco rocchi <rocchi.1407763@studenti.uniroma1.it>,"Marco,

Depending on your configuration, maybe what you're looking for is:
localhost:4040

Check this two StackOverflow answers:
http://stackoverflow.com/questions/31460079/spark-ui-on-aws-emr
or similar questions. This is not a specific Spark issue.

Please check StackOverflow or post to User Mailing List, next time on this
type of question.



r
 look at
e
ay
n
u
it
in
n
"
marco rocchi <rocchi.1407763@studenti.uniroma1.it>,"Fri, 25 Nov 2016 18:31:02 +0100",Re: SparkUI via proxy,Ewan Leith <ewan.leith@realitymine.com>,"Thanks to all, I solved the problem.
I'm sorry if the question was off topic, next time I'll post to
stackoverflow.
Thanks a lot

2016-11-25 17:19 GMT+01:00 marco rocchi <rocchi.1407763@studenti.uniroma1.it

r
 look at
e
ay
n
u
it
in
n
"
vineet chadha <start.vineet@gmail.com>,"Fri, 25 Nov 2016 14:33:09 -0800",Third party library,"Apache Spark Dev <dev@spark.apache.org>, vineet chadha <start.vineet@gmail.com>","Hi,

I am trying to invoke C library from the Spark Stack using JNI interface
(here is sample  application code)


class SimpleApp {
 // ---Native methods
@native def foo (Top: String): String
}

object SimpleApp  {
   def main(args: Array[String]) {

    val conf = new
SparkConf().setAppName(""SimpleApplication"").set(""SPARK_LIBRARY_PATH"", ""lib"")
    val sc = new SparkContext(conf)
     System.loadLibrary(""foolib"")
    //instantiate the class
     val SimpleAppInstance = new SimpleApp
    //String passing - Working
    val ret = SimpleAppInstance.foo(""fooString"")
  }

Above code work fines.

I have setup LD_LIBRARY_PATH and spark.executor.extraClassPath,
spark.executor.extraLibraryPath
at worker node

How can i invoke JNI library from worker node ? Where should i load it in
executor ?
Calling  System.loadLibrary(""foolib"") inside the work node gives me
following error :

Exception in thread ""main"" java.lang.UnsatisfiedLinkError:

Any help would be really appreciated.
"
Reynold Xin <rxin@databricks.com>,"Fri, 25 Nov 2016 15:32:00 -0800",Re: Third party library,vineet chadha <start.vineet@gmail.com>,"bcc dev@ and add user@


This is more a user@ list question rather than a dev@ list question. You
can do something like this:

object MySimpleApp {
  def loadResources(): Unit = // define some idempotent way to load
resources, e.g. with a flag or lazy val

  def main() = {
    ...

    sc.parallelize(1 to 10).mapPartitions { iter =>
      MySimpleApp.loadResources()

      // do whatever you want with the iterator
    }
  }
}






"
Jacek Laskowski <jacek@japila.pl>,"Sun, 27 Nov 2016 15:23:36 +0100",Use of BroadcastFactory interface (after SPARK-12588 Remove HTTPBroadcast),dev <dev@spark.apache.org>,"Hi,

After SPARK-12588 Remove HTTPBroadcast [1], the one and only
implementation of BroadcastFactory is TorrentBroadcastFactory. No code
in Spark 2 uses BroadcastFactory (but TorrentBroadcastFactory) however
the scaladoc says [2]:

/**
 * An interface for all the broadcast implementations in Spark (to allow
 * multiple broadcast implementations). SparkContext uses a user-specified
 * BroadcastFactory implementation to instantiate a particular broadcast for the
 * entire Spark job.
 */

which is not correct since there is no way to plug in a custom
user-specified BroadcastFactory.

My first impression was to remove the seemingly-pluggable interface
BroadcastFactory completely since it's no longer pluggable and may
imply it is still pluggable.

But then I thought you, Spark devs, could argue it's just about fixing
the scaladoc (and leaving the interface intact).

I'm for removing the BroadcastFactory interface completely and leaving
TorrentBroadcastFactory alone (without extending something that's not
extendable despite being an interface) or...bringing
spark.broadcast.factory Spark property back to life in
BroadcastManager so it is indeed possible to plug a custom
BroadcastFactory (and hence Broadcast) in.

WDYT?

[1] https://issues.apache.org/jira/browse/SPARK-12588
[2] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/broadcast/BroadcastFactory.scala#L25-L30

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Anton Okolnychyi <okolnychyyanton@gmail.com>,"Sun, 27 Nov 2016 18:23:12 +0100",Typo in the programming guide?,dev@spark.apache.org,"Hi guys,

I am looking at the Accumulator section in the latest programming guide.
Is there a typo in the sample code? Shouldn't the add() method accept only
one param in Spark 2.0? It looks like the signature is inherited
from AccumulatorParam, which was there before.

object VectorAccumulatorV2 extends AccumulatorV2[MyVector, MyVector] {
  val vec_ : MyVector = MyVector.createZeroVector
  def reset(): MyVector = {
    vec_.reset()
  }
  def add(v1: MyVector, v2: MyVector): MyVector = {
    vec_.add(v2)
  }
  ...}


Sincerely,
Anton
"
Sean Owen <sowen@cloudera.com>,"Sun, 27 Nov 2016 17:57:49 +0000",Re: Typo in the programming guide?,"Anton Okolnychyi <okolnychyyanton@gmail.com>, dev@spark.apache.org","Yes, feel free to open a [MINOR] PR to fix that.


"
"""Dongjoon Hyun""<dongjoon@apache.org>","Sun, 27 Nov 2016 20:49:29 -0000",Two major versions?,<dev@spark.apache.org>,"Hi, All.

Do we have a release plan of Apache Spark 1.6.4?

Up to my knowledge, Apache Spark community has been focusing on latest two versions.
There was no official release of Apache Spark *X.X.4* so far. It's also well-documented on Apache Spark home page (Versioning policy; http://spark.apache.org/versioning-policy.html)


So, personally, I don't expect Apache Spark 1.6.4. After Apache Spark 2.1 will be released very soon, 2.1 and 2.0 will be the two major versions which the most community effort is going to focus on.

However, *literally*, two major versions of Apache Spark will be Apache Spark 1.X (1.6.3) and Apache Spark 2.X. Since there is API compatibility issues between major versions, I guess 1.6.X will survive for a while like JDK7.

If possible, could we have a clear statement whether there is a plan for 1.6.4 on homepage?

Bests,
Dongjoon.

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Sun, 27 Nov 2016 12:50:44 -0800",Re: Two major versions?,Dongjoon Hyun <dongjoon@apache.org>,"I think this highly depends on what issues are found, e.g. critical bugs
that impact wide use cases, or security bugs.



"
"""Dongjoon Hyun""<dongjoon@apache.org>","Sun, 27 Nov 2016 21:32:01 -0000",Re: Two major versions?,<dev@spark.apache.org>,"Thank you, Reynold. Clear enough as a backporting and as a new release criteria.
Yes. Definitely. If there is no new commit on `branch-1.6`, there is no need to release.

However, Apache Spark 1.6.3 was a single release *accumulating* those kind of 52 patches.
Given that `branch-1.5` has over 100 patches and we don't expect `1.5.3`, `branch-1.6` looks to me extraordinary as the last emperor of Apache Spark 1.X.

Bests,
Dongjoon.


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Mon, 28 Nov 2016 12:17:38 +0000",Re: Two major versions?,"Dongjoon Hyun <dongjoon@apache.org>, dev@spark.apache.org","Yeah, there's no official position on this. BTW see the new home of what
info is published on this topic:
http://spark.apache.org/versioning-policy.html

The answer is indeed that minor releases have a target cadence, but
maintenance releases are as-needed, as defined by the release manager's
judgment.

From
https://issues.apache.org/jira/browse/SPARK/?selectedTab=com.atlassian.jira.jira-projects-plugin:versions-panel
you
can see that maintenance releases for a minor release seem to continue for
3-6 months in general, with 1.6.x going on for a longer period as the last
1.x minor release.

I wouldn't mind putting down some text to set a non-binding expectation
around this, that minor releases might be 'supported' for 3-6 months? until
2 more minor releases have succeeded it? Because in practice that's very
much how back-ports behave.

We also don't say anything about major releases but I think that may also
be too rare to put even informal statements around. Every couple years?


"
Dongjoon Hyun <dongjoon@apache.org>,"Mon, 28 Nov 2016 16:26:36 +0000",Re: Two major versions?,"Sean Owen <sowen@cloudera.com>, dev@spark.apache.org","Thank you, Sean.

Now, I agree with you that it's too rare to do something for Apache Spark
major versions.

Bests,
Dongjoon


"
Reynold Xin <rxin@databricks.com>,"Mon, 28 Nov 2016 17:25:43 -0800",[VOTE] Apache Spark 2.1.0 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version
2.1.0. The vote is open until Thursday, December 1, 2016 at 18:00 UTC and
passes if a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 2.1.0
[ ] -1 Do not release this package because ...


To learn more about Apache Spark, please see http://spark.apache.org/

The tag to be voted on is v2.1.0-rc1
(80aabc0bd33dc5661a90133156247e7a8c1bf7f5)

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.1.0-rc1-bin/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1216/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.1.0-rc1-docs/


=======================================
How can I help test this release?
=======================================
If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions.

===============================================================
What should happen to JIRA tickets still targeting 2.1.0?
===============================================================
Committers should look at those and triage. Extremely important bug fixes,
documentation, and API tweaks that impact compatibility should be worked on
immediately. Everything else please retarget to 2.1.1 or 2.2.0.
"
Prasanna Santhanam <tsp@apache.org>,"Tue, 29 Nov 2016 10:30:33 +0530",Re: [VOTE] Apache Spark 2.1.0 (RC1),dev@spark.apache.org,"
What would be a good JIRA filter to go through the changes coming in this
release?


"
Reynold Xin <rxin@databricks.com>,"Mon, 28 Nov 2016 21:06:21 -0800",Re: [VOTE] Apache Spark 2.1.0 (RC1),Prasanna Santhanam <tsp@apache.org>,"This one:
https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20fixVersion%20%3D%202.1.0




"
Nishadi Kirielle <ndimeshi@gmail.com>,"Tue, 29 Nov 2016 11:10:33 +0530",Bit-wise AND operation between integers,dev <dev@spark.apache.org>,"Hi all,

I am trying to use bitwise AND operation between integers on top of Spark
SQL. Is this functionality supported and if so, can I have any
documentation on how to use bitwise AND operation?

Thanks & regards

-- 
Nishadi Kirielle

Undergraduate
University of Moratuwa - Sri Lanka

Mobile : +94 70 204 5934
Blog : nishadikirielle.wordpress.com
"
Reynold Xin <rxin@databricks.com>,"Tue, 29 Nov 2016 00:42:32 -0500",Re: Bit-wise AND operation between integers,Nishadi Kirielle <ndimeshi@gmail.com>,"Bcc dev@ and add user@

The dev list is not meant for users to ask questions on how to use Spark.
For that you should use StackOverflow or the user@ list.


scala> sql(""select 1 & 2"").show()
+-------+
|(1 & 2)|
+-------+
|      0|
+-------+


scala> sql(""select 1 & 3"").show()
+-------+
|(1 & 3)|
+-------+
|      1|
+-------+



Hi all,

I am trying to use bitwise AND operation between integers on top of Spark
SQL. Is this functionality supported and if so, can I have any
documentation on how to use bitwise AND operation?

Thanks & regards

-- 
Nishadi Kirielle

Undergraduate
University of Moratuwa - Sri Lanka

Mobile : +94 70 204 5934
Blog : nishadikirielle.wordpress.com
"
Nishadi Kirielle <ndimeshi@gmail.com>,"Tue, 29 Nov 2016 11:38:10 +0530",Re: Bit-wise AND operation between integers,dev <dev@spark.apache.org>,"Sorry for using the wrong mailing list. Next time onward I will be using
the user list.
Thank you


"
Nitin Goyal <nitin2goyal@gmail.com>,"Tue, 29 Nov 2016 11:59:16 +0530","Re: Parquet-like partitioning support in spark SQL's in-memory
 columnar cache",Reynold Xin <rxin@databricks.com>,"+Cheng

Hi Reynold,

I think you are referring to bucketing in in-memory columnar cache.

I am proposing that if we have a parquet structure like following :-

/<parent-directory>/file1/id=1/<parquet-part-files>
/<parent-directory>/file1/id=2/<parquet-par"
Sean Owen <sowen@cloudera.com>,"Tue, 29 Nov 2016 09:23:04 +0000",Re: [VOTE] Apache Spark 2.1.0 (RC1),"Reynold Xin <rxin@databricks.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","We still have several blockers for 2.1, so I imagine at least one will mean
this won't be the final RC:

SPARK-18318 ML, Graph 2.1 QA: API: New Scala APIs, docs
SPARK-18319 ML, Graph 2.1 QA: API: Experimental, DeveloperApi, final,
sealed audit
SPARK-18326 SparkR 2.1 QA: New R APIs and API docs
SPARK-18516 Separate instantaneous state from progress performance
statistics
SPARK-18538 Concurrent Fetching DataFrameReader JDBC APIs Do Not Work
SPARK-18553 Executor loss may cause TaskSetManager to be leaked

However I understand the purpose here is of course to get started testing
early and we should all do so.

BTW here are the Critical issues still open:

SPARK-12347 Write script to run all MLlib examples for testing
SPARK-16032 Audit semantics of various insertion operations related to
partitioned tables
SPARK-17861 Store data source partitions in metastore and push partition
pruning into metastore
SPARK-18091 Deep if expressions cause Generated SpecificUnsafeProjection
code to exceed JVM code size limit
SPARK-18274 Memory leak in PySpark StringIndexer
SPARK-18316 Spark MLlib, GraphX 2.1 QA umbrella
SPARK-18322 ML, Graph 2.1 QA: Update user guide for new features & APIs
SPARK-18323 Update MLlib, GraphX websites for 2.1
SPARK-18324 ML, Graph 2.1 QA: Programming guide update and migration guide
SPARK-18329 Spark R 2.1 QA umbrella
SPARK-18330 SparkR 2.1 QA: Update user guide for new features & APIs
SPARK-18331 Update SparkR website for 2.1
SPARK-18332 SparkR 2.1 QA: Programming guide, migration guide, vignettes
updates
SPARK-18468 Flaky test:
org.apache.spark.sql.hive.HiveSparkSubmitSuite.SPARK-9757 Persist Parquet
relation with decimal column
SPARK-18549 Failed to Uncache a View that References a Dropped Table.
SPARK-18560 Receiver data can not be dataSerialized properly.



"
Marcelo Vanzin <vanzin@cloudera.com>,"Tue, 29 Nov 2016 09:31:12 -0800",Re: [VOTE] Apache Spark 2.1.0 (RC1),Reynold Xin <rxin@databricks.com>,"I'll send a -1 because of SPARK-18546. Haven't looked at anything else yet.




-- 
Marcelo

---------------------------------------------------------------------


"
Srinivas Potluri <shri.hadoop@gmail.com>,"Tue, 29 Nov 2016 23:29:18 +0530",Please add me,dev@spark.apache.org,"Hi,

I am interested to contribute code on Spark. Could you please add me into
the mailing list / DL.

Thanks,
*Srinivas Potluri*
"
Michael Allman <michael@videoamp.com>,"Tue, 29 Nov 2016 17:15:46 -0800",Can't read tables written in Spark 2.1 in Spark 2.0 (and earlier),Spark Dev List <dev@spark.apache.org>,"Hello,

When I try to read from a Hive table created by Spark 2.1 in Spark 2.0 or earlier, I get an error:

java.lang.ClassNotFoundException: Failed to load class for data source: hive.

Is there a way to get previous versions of Spark to read tables written with Spark 2.1?

Cheers,

Michael
---------------------------------------------------------------------


"
Michael Allman <michael@videoamp.com>,"Tue, 29 Nov 2016 17:51:40 -0800",Re: Can't read tables written in Spark 2.1 in Spark 2.0 (and earlier),Spark Dev List <dev@spark.apache.org>,"This is not an issue with all tables created in Spark 2.1, though I'm not sure why some work and some do not. I have found that a table created as such

sql(""create table test stored as parquet as select 1"")

in Spark 2.1 cannot be read in previous versions of Spark.

Michael


or earlier, I get an error:
source: hive.
written with Spark 2.1?


---------------------------------------------------------------------


"
WangJianfei <wangjianfei15@otcaix.iscas.ac.cn>,"Tue, 29 Nov 2016 19:18:10 -0700 (MST)",Question about spark.mllib.GradientDescent,dev@spark.apache.org,"Hi devs:
   I think it's unnecessary to use c1._1 += c2.1 in combOp operation, I
think it's the same if we use c1._1+c2._1, see the code below :
in GradientDescent.scala

   val (gradientSum, lossSum, miniBatchSize) = data.sample(false,
miniBatchFraction, 42 + i)
        .treeAggregate((BDV.zeros[Double](n), 0.0, 0L))(
          seqOp = (c, v) => {
            // c: (grad, loss, count), v: (label, features)
            // c._1 å³ grad will be updated in gradient.compute
            val l = gradient.compute(v._2, v._1, bcWeights.value,
Vectors.fromBreeze(c._1))
            (c._1, c._2 + l, c._3 + 1)
          },
          combOp = (c1, c2) => {
            // c: (grad, loss, count)
            (c1._1 += c2._1, c1._2 + c2._2, c1._3 + c2._3)
          })



--
3.nabble.com/Question-about-spark-mllib-GradientDescent-tp20052.html
om.

---------------------------------------------------------------------


"
Saikat Kanjilal <sxk1969@hotmail.com>,"Wed, 30 Nov 2016 04:14:01 +0000","Spark-9487, Need some insight","""dev@spark.apache.org"" <dev@spark.apache.org>","Hello Spark dev community,

I took this the following jira item (https://github.com/apache/spark/pull/15848) and am looking for some general pointers, it seems that I am running into issues where things work successfully doing local development on my macbook pro but fail on jenkins for a multitiude of reasons and errors, here's an example,  if you see this build output report: https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/69297/ you will see the DataFrameStatSuite, now locally I am running these individual tests with this command: ./build/mvn test -P... -DwildcardSuites=none -Dtest=org.apache.spark.sql.DataFrameStatSuite.     It seems that I need to emulate a jenkins like environment locally, this seems sort of like an untenable hurdle, granted that my changes involve changing the total number of workers in the sparkcontext and if so should I be testing my changes in an environment that more closely resembles jenkins.  I really want to work on/complete this PR but I keep getting hamstrung by a dev environment that is not equivalent to our CI environment.



I'm guessing/hoping I'm not the first one to run into this so some insights. pointers to get past this would be very appreciated , would love to keep contributing and hoping this is a hurdle that's overcomeable with some tweaks to my dev environment.



Thanks in advance.
"
Sachith Withana <swsachith@gmail.com>,"Wed, 30 Nov 2016 11:30:01 +0530","Re: How is the order ensured in the jdbc relation provider when
 inserting data from multiple executors",nirandap <niranda.perera@gmail.com>,"Hi all,

To explain the scenario a bit more.

We need to retain the order when writing to the RDBMS tables.
The way we found was to execute the DB Write *job* for each partition which
is really costly.
we cannot control the count( due to the count being inferred from the
parent RDD).

When we execute the insert job, the executors are run in parallel to
execute the writing tasks which jumbles up the order.
Is there anyway we can execute the tasks sequentially? or any other way of
doing this?
We have noticed that you handle this from inside Spark itself, to retain
the order when writing to RDBMS from Spark.

Thanks,
Sachith




er
he
ly
ber
ll
I
k
r
l
s.
0+
d
how
e of
ould
n
ng
e
ock
4 71
n
lServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
der-ensured-in-the-jdbc-relation-provider-when-inserting-data-from-multiple-executors-tp19970p19985.html>
Servlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
er-ensured-in-the-jdbc-relation-provider-when-inserting-data-from-multiple-executors-tp19970p20016.html>



-- 
Thanks,
Sachith Withana
"
Matt Cheah <mcheah@palantir.com>,"Wed, 30 Nov 2016 06:22:04 +0000",Proposal for SPARK-18278,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi everyone,

Kubernetes is a technology that is a key player in the cluster computing world. Currently, running Spark applications on Kubernetes requires deploying a standalone Spark cluster on the Kubernetes cluster, and then running the jobs against the standalone Spark cluster. However, there would be many benefits to running Spark on Kubernetes natively, and so SPARK-18278<https://issues.apache.org/jira/browse/SPARK-18278> has been filed to track discussion around supporting Kubernetes as a cluster manager, in addition to the existing Mesos, YARN, and standalone cluster managers.

A first draft of a proposal outlining a potential long-term plan around this feature has been attached to the JIRA ticket. Any feedback and discussion would be greatly appreciated.

Thanks,

-Matt Cheah
"
Bu Jianjian <jafebabe@gmail.com>,"Tue, 29 Nov 2016 23:36:14 -0800",Re: Please add me,Srinivas Potluri <shri.hadoop@gmail.com>,"Hi Srinivas,

You can subscribe the mail list in the community page by yourself
http://spark.apache.org/community.html


"
WangJianfei <wangjianfei15@otcaix.iscas.ac.cn>,"Wed, 30 Nov 2016 01:51:43 -0700 (MST)","Why don't we imp some adaptive learning rate methods, such as
 adadelat, adam?",dev@spark.apache.org,"Hi devs:
    Normally, the adaptive learning rate methods can have a fast convergence
then standard SGD, so why don't we imp them?
see the link for more details 
http://sebastianruder.com/optimizing-gradient-descent/index.html#adadelta



--

---------------------------------------------------------------------


"
Nick Pentreath <nick.pentreath@gmail.com>,"Wed, 30 Nov 2016 08:54:30 +0000","Re: Why don't we imp some adaptive learning rate methods, such as
 adadelat, adam?","WangJianfei <wangjianfei15@otcaix.iscas.ac.cn>, dev@spark.apache.org","check out https://github.com/VinceShieh/Spark-AdaOptimizer


"
Sean Owen <sowen@cloudera.com>,"Wed, 30 Nov 2016 09:34:43 +0000",Re: [VOTE] Apache Spark 2.1.0 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","FWIW I am seeing several test failures, each more than once, but, none are
necessarily repeatable. These are likely just flaky tests but I thought I'd
flag these unless anyone else sees similar failures:


- SELECT a.i, b.i FROM oneToTen a JOIN oneToTen b ON a.i = b.i + 1 ***
FAILED ***
  org.apache.spark.SparkException: Job aborted due to stage failure: Task 1
in stage 9.0 failed 1 times, most recent failure: Lost task 1.0 in stage
9.0 (TID 19, localhost, executor driver): java.lang.NullPointerException
at
org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.<init>(Unknown
Source)
at
org.apache.spark.sql.catalyst.expressions.GeneratedClass.generate(Unknown
Source)
  ...


udf3Test(test.org.apache.spark.sql.JavaUDFSuite)  Time elapsed: 0.302 sec
 <<< ERROR!
java.lang.NoSuchMethodError:
org.apache.spark.sql.catalyst.JavaTypeInference$.inferDataType(Lcom/google/common/reflect/TypeToken;)Lscala/Tuple2;
at test.org.apache.spark.sql.JavaUDFSuite.udf3Test(JavaUDFSuite.java:107)



- SPARK-18360: default table path of tables in default database should
depend on the location of default database *** FAILED ***
  Timeout of './bin/spark-submit' '--class'
'org.apache.spark.sql.hive.SPARK_18360' '--name' 'SPARK-18360' '--master'
'local-cluster[2,1,1024]' '--conf' 'spark.ui.enabled=false' '--conf'
'spark.master.rest.enabled=false' '--driver-java-options'
'-Dderby.system.durability=test'
'file:/home/srowen/spark-2.1.0/sql/hive/target/tmp/spark-dc9f43f2-ded4-4bcf-947e-d5af6f0e1561/testJar-1480440084611.jar'
See the log4j logs for more detail.
...


- should clone and clean line object in ClosureCleaner *** FAILED ***
  isContain was true Interpreter output contained 'Exception':
  java.lang.IllegalStateException: Cannot call methods on a stopped
SparkContext.
  This stopped SparkContext was created at:




"
WangJianfei <wangjianfei15@otcaix.iscas.ac.cn>,"Wed, 30 Nov 2016 03:16:36 -0700 (MST)","Re: Why don't we imp some adaptive learning rate methods, such as
 adadelat, adam?",dev@spark.apache.org,"yes, thank you, i know this imp is very simple, but i want to know why spark
mllib imp this?



--

---------------------------------------------------------------------


"
Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Wed, 30 Nov 2016 17:00:05 +0100",Re: [VOTE] Apache Spark 2.1.0 (RC1),dev@spark.apache.org,"-1 (non binding) https://issues.apache.org/jira/browse/SPARK-16589 No
matter how useless in practice this shouldn't go to another major release.



-- 
Maciej Szymkiewicz

"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 30 Nov 2016 16:12:11 +0000",Re: [VOTE] Apache Spark 2.1.0 (RC1),"Maciej Szymkiewicz <mszymkiewicz@gmail.com>, dev@spark.apache.org","matter how useless in practice this shouldn't go to another major release.

I agree that that issue is a major one since it relates to correctness, but
since it's not a regression it technically does not merit a -1 vote on the
release.

Nick


"
Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Wed, 30 Nov 2016 17:15:47 +0100",Re: [VOTE] Apache Spark 2.1.0 (RC1),"Nicholas Chammas <nicholas.chammas@gmail.com>, dev@spark.apache.org","Sorry :) BTW There is another related issue here
https://issues.apache.org/jira/browse/SPARK-17756



nsafeProjection.<init>(Unknown
Unknown
m/google/common/reflect/TypeToken;)Lscala/Tuple2;
107)
ded4-4bcf-947e-d5af6f0e1561/testJar-1480440084611.jar'
***
rc1-bin/
.0-rc1-bin/>
ark-1216/
rc1-docs/
.0-rc1-docs/>
===================
===================
===========================================
===========================================
.0.
-------

-- 
Maciej Szymkiewicz

"
Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Wed, 30 Nov 2016 17:27:08 +0100","[SPARK-17845] [SQL][PYTHON] More self-evident window function frame
 boundary API","""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

I've been looking at the SPARK-17845 and I am curious if there is any
reason to make it a breaking change. In Spark 2.0 and below we could use:

    Window().partitionBy(""foo"").orderBy(""bar"").rowsBetween(-sys.maxsize,
sys.maxsize))

In 2.1.0 this code will silently produce incorrect results (ROWS BETWEEN
-1 PRECEDING AND UNBOUNDED FOLLOWING) Couldn't we use
Window.unboundedPreceding equal -sys.maxsize to ensure backward
compatibility?

-- 

Maciej Szymkiewicz


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Wed, 30 Nov 2016 09:43:19 -0800","Re: [SPARK-17845] [SQL][PYTHON] More self-evident window function
 frame boundary API",Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Can you give a repro? Anything less than -(1 << 63) is considered negative
infinity (i.e. unbounded preceding).


"
Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Wed, 30 Nov 2016 18:48:13 +0100","Re: [SPARK-17845] [SQL][PYTHON] More self-evident window function
 frame boundary API",Reynold Xin <rxin@databricks.com>,"The problem is that -(1 << 63) is -(sys.maxsize + 1) so the code which
used to work before is off by one.


-- 
Maciej Szymkiewicz

"
Reynold Xin <rxin@databricks.com>,"Wed, 30 Nov 2016 09:52:51 -0800","Re: [SPARK-17845] [SQL][PYTHON] More self-evident window function
 frame boundary API",Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Ah ok for some reason when I did the pull request sys.maxsize was much
larger than 2^63. Do you want to submit a patch to fix this?



"
Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Wed, 30 Nov 2016 19:04:41 +0100","Re: [SPARK-17845] [SQL][PYTHON] More self-evident window function
 frame boundary API",Reynold Xin <rxin@databricks.com>,"It is platform specific so theoretically can be larger, but 2**63 - 1 is
a standard on 64 bit platform and 2**31 - 1 on 32bit platform. I can
submit a patch but I am not sure how to proceed. Personally I would set

unboundedPreceding = -sys.maxsize

unboundedFollowing = sys.maxsize

to keep backwards compatibility.


-- 
Maciej Szymkiewicz

"
Reynold Xin <rxin@databricks.com>,"Wed, 30 Nov 2016 10:34:39 -0800","Re: [SPARK-17845] [SQL][PYTHON] More self-evident window function
 frame boundary API",Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Yes I'd define unboundedPreceding to -sys.maxsize, but also any value less
than min(-sys.maxsize, _JAVA_MIN_LONG) are considered unboundedPreceding
too. We need to be careful with long overflow when transferring data over
to Java.



"
Koert Kuipers <koert@tresata.com>,"Wed, 30 Nov 2016 17:56:09 -0500",Re: [VOTE] Apache Spark 2.1.0 (RC1),Reynold Xin <rxin@databricks.com>,"running our inhouse unit-tests (that work with spark 2.0.2) against spark
2.1.0-rc1 i see the following issues.

any test that use avro (spark-avro 3.1.0) have this error:
java.lang.AbstractMethodError
    at
org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.<init>(FileFormatWriter.scala:232)
    at
org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:182)
    at
org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:129)
    at
org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:128)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
    at org.apache.spark.scheduler.Task.run(Task.scala:99)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
    at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)


so looks like some api got changed or broken. i dont know if this is an
issue or if this is OK.

also a bunch of unit test related to reading and writing csv files fail.
the issue seems to be newlines inside quoted values. this worked before and
now it doesnt work anymore. i dont know if this was an accidentally
supported feature and its ok to be broken? i am not even sure it is a good
idea to support newlines inside quoted values. anyhow they still get
written out the same way as before, but now when reading it back in things
break down.



"
Michael Armbrust <michael@databricks.com>,"Wed, 30 Nov 2016 15:04:18 -0800",Re: [VOTE] Apache Spark 2.1.0 (RC1),Koert Kuipers <koert@tresata.com>,"Unfortunately the FileFormat APIs are not stable yet, so if you are using
spark-avro, we are going to need to update it for this release.


"
