Matei Zaharia <matei.zaharia@gmail.com>,"Sat, 31 Jan 2015 19:36:25 -0800",Re: [VOTE] Release Apache Spark 1.2.1 (RC2),MartinWeindel <martin.weindel@gmail.com>,"This looks like a pretty serious problem, thanks! Glad people are testing on Windows.

Matei

in
any
file
my
http://apache-spark-developers-list.1001551.n3.nabble.com/VOTE-Release-Apache-Spark-1-2-1-RC2-tp10317p10370.html
Nabble.com.


---------------------------------------------------------------------


"
Yafeng Guo <daniel.yafeng.guo@gmail.com>,"Sat, 31 Jan 2015 23:01:06 -0500",Intellij IDEA 14 env setup; NoClassDefFoundError when run examples,dev@spark.apache.org,"Hi,

I'm setting up a dev environment with Intellij IDEA 14. I selected profile
scala-2.10, maven-3, hadoop 2.4, hive, hive 0.13.1. The compilation passed.
But when I try to run LogQuery in examples, I met below issue:

Connected to the target VM, address: '127.0.0.1:37182', transport: 'socket'
Exception in thread ""main"" java.lang.NoClassDefFoundError:
org/apache/spark/SparkConf
at org.apache.spark.examples.LogQuery$.main(LogQuery.scala:46)
at org.apache.spark.examples.LogQuery.main(LogQuery.scala)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.SparkConf
at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
at java.security.AccessController.doPrivileged(Native Method)
at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
... 2 more
Disconnected from the target VM, address: '127.0.0.1:37182', transport:
'socket'

anyone met similar issue before? Thanks a lot

Regards,
Ya-Feng
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sun, 01 Feb 2015 04:19:35 +0000",Re: [VOTE] Release Apache Spark 1.2.1 (RC2),"Matei Zaharia <matei.zaharia@gmail.com>, MartinWeindel <martin.weindel@gmail.com>","Do we have any open JIRA issues to add automated testing on Windows to
Jenkins? I assume that's something we want to do.


"
Ted Yu <yuzhihong@gmail.com>,"Sat, 31 Jan 2015 20:43:07 -0800",Re: Intellij IDEA 14 env setup; NoClassDefFoundError when run examples,Yafeng Guo <daniel.yafeng.guo@gmail.com>,"Have you read / followed this ?

https://cwiki.apache.org/confluence/display/SPARK
/Useful+Developer+Tools#UsefulDeveloperTools-BuildingSparkinIntelliJIDEA

Cheers


"
Evan Chan <velvia.github@gmail.com>,"Sun, 1 Feb 2015 00:31:17 -0800",Re: renaming SchemaRDD -> DataFrame,Cheng Lian <lian.cs.zju@gmail.com>,"It is true that you can persist SchemaRdds / DataFrames to disk via
Parquet, but a lot of time and inefficiencies is lost.   The in-memory
columnar cached representation is completely different from the
Parquet file format, and I believe there has to be a translation into
a Row (because ultimately Spark SQL traverses Row's -- even the
InMemoryColumnarTableScan has to then convert the columns into Rows
frames process in a columnar fashion.   Columnar storage is good, but
nowhere near as good as columnar processing.

Another issue, which I don't know if it is solved yet, but it is
difficult for Tachyon to efficiently cache Parquet files without
understanding the file format itself.

I gave a talk at last year's Spark Summit on this topic.

I'm working on efforts to change this, however.  Shoot me an email at
velvia at gmail if you're interested in joining forces.


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Sun, 1 Feb 2015 10:44:44 +0000",Re: Intellij IDEA 14 env setup; NoClassDefFoundError when run examples,Yafeng Guo <daniel.yafeng.guo@gmail.com>,"How do you mean you run LogQuery? you would run these using the
run-example script rather than in IntelliJ.


---------------------------------------------------------------------


"
Mick Davies <michael.belldavies@gmail.com>,"Sun, 1 Feb 2015 04:03:31 -0700 (MST)",Caching tables at column level,dev@spark.apache.org,"I have been working a lot recently with denormalised tables with lots of
columns, nearly 600. We are using this form to avoid joins. 

I have tried to use cache table with this data, but it proves too expensive
as it seems to try to cache all the data in the table.

For data sets such as the one I am using you find that certain columns will
be hot, referenced frequently in queries, others will be used very
infrequently.

Therefore it would be great if caches could be column based. I realise that
this may not be optimal for all use cases, but I think it could be quite a
common need.  Has something like this been considered?

Thanks Mick



--

---------------------------------------------------------------------


"
Octavian Geagla <ogeagla@gmail.com>,"Sun, 1 Feb 2015 04:53:00 -0700 (MST)","Re: Any interest in 'weighting' VectorTransformer which does
 component-wise scaling?",dev@spark.apache.org,"I've added support for sparse vectors and created HadamardTF for the
pipeline, please take a look  on my branch
<https://github.com/ogeagla/spark/compare/spark-mllib-weighting>  .

Thanks!



--

---------------------------------------------------------------------


"
Yafeng Guo <daniel.yafeng.guo@gmail.com>,"Sun, 1 Feb 2015 21:29:26 +0800",Re: Intellij IDEA 14 env setup; NoClassDefFoundError when run examples,Sean Owen <sowen@cloudera.com>,"Finally it works.

@Sean, I'm trying to setup env in IDE so I can track into to Spark -- that
will help me understand Spark internal mechanism.

@Ted, thanks. I'm using Maven, not SBT, but thanks for the suggestion
anyway.

For others who might interested in:

I choose ""bigtop-dist"" profile so under ""Spark-Assembly"" maven will build a
fat jar file, which include all artifacts built. Then I added such fat jar
as a dependency of examples module, set it as ""runtime"".

Then I find I can debug and track code step by step.

This is maybe not a formal way but at least works for me.

Regards,
Ya-Feng


"
Ewan Higgs <ewan.higgs@ugent.be>,"Sun, 01 Feb 2015 16:31:34 +0100",Re: Custom Cluster Managers / Standalone Recovery Mode in Spark,"Anjana Fernando <lafernando@gmail.com>, dev@spark.apache.org","+1
the HPC community who often run slurm, pbs, and sge to control jobs (as 
opposed to Yarn and Mesos). Currently, there are several projects that 
launch Yarn clusters (or MR1 clusters) inside PBS Jobs [1] but this is 
not the ideal situation. It would b"
Michael Armbrust <michael@databricks.com>,"Sun, 1 Feb 2015 13:27:20 -0800",Re: Caching tables at column level,Mick Davies <michael.belldavies@gmail.com>,"Its not completely transparent, but you can do something like the following
today:

CACHE TABLE hotData AS SELECT columns, I, care, about FROM fullTable


"
Aaron Davidson <ilikerps@gmail.com>,"Sun, 1 Feb 2015 13:37:57 -0800",Re: Custom Cluster Managers / Standalone Recovery Mode in Spark,Anjana Fernando <lafernando@gmail.com>,"For the specific question of supplementing Standalone Mode with a custom
leader election protocol, this was actually already committed in master and
will be available in Spark 1.3:

https://github.com/apache/spark/pull/771/files

You can specify spark.deploy.recoveryMode = ""CUSTOM""
and spark.deploy.recoveryMode.factory to a class which
implements StandaloneRecoveryModeFactory. See the current implementations
of FileSystemRecoveryModeFactory and ZooKeeperRecoveryModeFactory.

I will update the JIRA you linked to be more current.


"
Michael Malak <michaelmalak@yahoo.com.INVALID>,"Mon, 2 Feb 2015 02:04:09 +0000 (UTC)",Word2Vec IndexedRDD,"""dev@spark.apache.org"" <dev@spark.apache.org>","1. Is IndexedRDD planned for 1.3? https://issues.apache.org/jira/browse/SPARK-2365

https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/feature/Word2Vec.scala#L425

---------------------------------------------------------------------


"
Anjana Fernando <lafernando@gmail.com>,"Mon, 2 Feb 2015 07:39:59 +0530",Re: Custom Cluster Managers / Standalone Recovery Mode in Spark,Aaron Davidson <ilikerps@gmail.com>,"Hi guys,

That's great to hear that this is available in Spark 1.3! .. I will play
around with this feature and let you know the results for integrating
Hazelcast. Also, may I know the tentative release date for Spark 1.3? ..

Cheers,
Anjana.


"
"""Shao, Saisai"" <saisai.shao@intel.com>","Mon, 2 Feb 2015 08:24:18 +0000",Questions about Spark standalone resource scheduler,"""dev@spark.apache.org"" <dev@spark.apache.org>, ""user@spark.apache.org""
	<user@spark.apache.org>","Hi all,

I have some questions about the future development of Spark's standalone resource scheduler. We've heard some users have the requirements to have multi-tenant support in standalone mode, like multi-user management, resource management and isolation, whitelist of users. Seems current Spark standalone do not support such kind of functionalities, while resource schedulers like Yarn offers such kind of advanced managements, I'm not sure what's the future target of standalone resource scheduler, will it only target on simple implementation, and for advanced usage shift to YARN? Or will it plan to add some simple multi-tenant related functionalities?

Thanks a lot for your comments.

BR
Jerry
"
Patrick Wendell <pwendell@gmail.com>,"Mon, 2 Feb 2015 00:48:32 -0800",Re: Questions about Spark standalone resource scheduler,"""Shao, Saisai"" <saisai.shao@intel.com>","Hey Jerry,

I think standalone mode will still add more features over time, but
the goal isn't really for it to become equivalent to what Mesos/YARN
are today. Or at least, I doubt Spark Standalone will ever attempt to
manage _other_ frameworks outside of Spark and become a general
purpose resource manager.

In terms of having better support for multi tenancy, meaning multiple
*Spark* instances, this is something I think could be in scope in the
future. For instance, we added H/A to the standalone scheduler a while
back, because it let us support H/A streaming apps in a totally native
way. It's a trade off of adding new features and keeping the scheduler
very simple and easy to use. We've tended to bias towards simplicity
as the main goal, since this is something we want to be really easy
""out of the box"".

some coarser grained scheduler, such as running in a cloud service. In
this case they really just want a simple ""inner"" cluster manager. This
may even be the majority of all Spark installations. This is slightly
different than Hadoop environments, where they might just want nice
integration into the existing Hadoop stack via something like YARN.

- Patrick


---------------------------------------------------------------------


"
"""Shao, Saisai"" <saisai.shao@intel.com>","Mon, 2 Feb 2015 09:20:07 +0000",RE: Questions about Spark standalone resource scheduler,Patrick Wendell <pwendell@gmail.com>,"Hi Patrick,

Thanks a lot for your detailed explanation. For now we have such requirements: whitelist the application submitter, user resources (CPU, MEMORY) quotas, resources allocations in Spark Standalone mode. These are quite specific requirements for production-use, generally these problem will become whether we need to offer a more advanced resource scheduler compared to current simple FIFO one. I think our aim is to not provide a general resource scheduler like Mesos/Yarn, we only support Spark, but we hope to add some Mesos/Yarn functionalities to better use of Spark standalone mode.

I admitted that resource scheduler may have some overlaps with cloud manager, whether to offer a powerful scheduler or use cloud manager is really a dilemma.

I think we can break down to some small features to improve the standalone mode. What's your opinion?

Thanks
Jerry

Hey Jerry,

I think standalone mode will still add more features over time, but the goal isn't really for it to become equivalent to what Mesos/YARN are today. Or at least, I doubt Spark Standalone will ever attempt to manage _other_ frameworks outside of Spark and become a general purpose resource manager.

In terms of having better support for multi tenancy, meaning multiple
*Spark* instances, this is something I think could be in scope in the future. For instance, we added H/A to the standalone scheduler a while back, because it let us support H/A streaming apps in a totally native way. It's a trade off of adding new features and keeping the scheduler very simple and easy to use. We've tended to bias towards simplicity as the main goal, since this is something we want to be really easy ""out of the box"".

oarser grained scheduler, such as running in a cloud service. In this case they really just want a simple ""inner"" cluster manager. This may even be the majority of all Spark installations. This is slightly different than Hadoop environments, where they might just want nice integration into the existing Hadoop stack via something like YARN.

- Patrick

:
add some simple multi-tenant related functionalities?

---------------------------------------------------------------------


"
ankits <ankitsoni9@gmail.com>,"Mon, 2 Feb 2015 13:23:45 -0700 (MST)",Re: Get size of rdd in memory,dev@spark.apache.org,"Thanks for your response. So AFAICT 

calling parallelize(1  to1024).map(i =>KV(i,
i.toString)).toSchemaRDD.cache().count(), will allow me to see the size of
the schemardd in memory

and parallelize(1  to1024).map(i =>KV(i, i.toString)).cache().count()  will
show me the size of a regular rdd.

But this will not show us the size when using cacheTable() right? Like if i
call

parallelize(1  to1024).map(i =>KV(i,
i.toString)).toSchemaRDD.registerTempTable(""test"")
sqc.cacheTable(""test"")
sqc.sql(""SELECT COUNT(*) FROM test"")

the web UI does not show us the size of the cached table. 





--

---------------------------------------------------------------------


"
Cheng Lian <lian.cs.zju@gmail.com>,"Mon, 02 Feb 2015 13:18:00 -0800",Re: Get size of rdd in memory,"ankits <ankitsoni9@gmail.com>, dev@spark.apache.org","Actually |SchemaRDD.cache()| behaves exactly the same as |cacheTable| 
since Spark 1.2.0. The reason why your web UI didn‚Äôt show you the cached 
table is that both |cacheTable| and |sql(""SELECT ..."")| are lazy :-) 
Simply add a |.collect()| after the |sql(...)| call.

Cheng


‚Äã
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 02 Feb 2015 21:44:14 +0000",Spark Master Maven with YARN build is broken,Spark dev list <dev@spark.apache.org>,"https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-Master-Maven-with-YARN/HADOOP_PROFILE=hadoop-2.4,label=centos/

Is this is a known issue? It seems to have been broken since last night.

Here‚Äôs a snippet from the build output of one of the builds
<https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-Maven-with-YARN/HADOOP_PROFILE=hadoop-2.4,label=centos/1308/console>
:

[error] bad symbolic reference. A signature in WebUI.class refers to
term eclipse
[error] in package org which is not available.
[error] It may be completely missing from the current classpath, or
the version on
[error] the classpath might be incompatible with the version used when
compiling WebUI.class.
[error] bad symbolic reference. A signature in WebUI.class refers to term jetty
[error] in value org.eclipse which is not available.
[error] It may be completely missing from the current classpath, or
the version on
[error] the classpath might be incompatible with the version used when
compiling WebUI.class.
[error]
[error]      while compiling:
/home/jenkins/workspace/Spark-Master-Maven-with-YARN/HADOOP_PROFILE/hadoop-2.4/label/centos/yarn/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala
[error]         during phase: erasure
[error]      library version: version 2.10.4
[error]     compiler version: version 2.10.4

Nick
‚Äã
"
Patrick Wendell <pwendell@gmail.com>,"Mon, 2 Feb 2015 14:00:03 -0800",Re: Spark Master Maven with YARN build is broken,Nicholas Chammas <nicholas.chammas@gmail.com>,"It's my fault, I'm sending a hot fix now.


---------------------------------------------------------------------


"
ankits <ankitsoni9@gmail.com>,"Mon, 2 Feb 2015 15:03:30 -0700 (MST)",Re: Get size of rdd in memory,dev@spark.apache.org,"Great, thank you very much. I was confused because this is in the docs:

https://spark.apache.org/docs/1.2.0/sql-programming-guide.html, and on the
""branch-1.2"" branch,
https://github.com/apache/spark/blob/branch-1.2/docs/sql-programming-guide.md

""Note that if you call schemaRDD.cache() rather than
sqlContext.cacheTable(...), tables will not be cached using the in-memory
columnar format, and therefore sqlContext.cacheTable(...) is strongly
recommended for this use case."".

If this is no longer accurate, i could make a PR to remove it.



--

---------------------------------------------------------------------


"
Cheng Lian <lian.cs.zju@gmail.com>,"Mon, 02 Feb 2015 14:16:34 -0800",Re: Get size of rdd in memory,"ankits <ankitsoni9@gmail.com>, dev@spark.apache.org","It's already fixed in the master branch. Sorry that we forgot to update 
this before releasing 1.2.0 and caused you trouble...

Cheng



---------------------------------------------------------------------


"
"""M. Dale"" <medale94@yahoo.com.INVALID>","Mon, 02 Feb 2015 17:18:08 -0500",Additional fix for Avro IncompatibleClassChangeError (SPARK-3039),dev@spark.apache.org,"SPARK-3039 ""Spark assembly for new hadoop API (hadoop 2) contains 
avro-mapred for hadoop 1 API""
was marked resolved with Spark 1.2.0 release. However, when I download the
pre-built Spark distro for Hadoop 2.4 and later 
(spark-1.2.0-bin-hadoop2.4.tgz) and run it
against Avro code compiled against Hadoop 2.4/new Hadoop API I still get:

java.lang.IncompatibleClassChangeError: Found interface 
org.apache.hadoop.mapreduce.TaskAttemptContext, but class was expected
     at 
org.apache.avro.mapreduce.AvroRecordReaderBase.initialize(AvroRecordReaderBase.java:87)
     at 
org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:135)

TaskAttemptContext was a class in the Hadoop 1.x series but became an 
interface
in Hadoop 2.x. Therefore there is a avro-mapred-1.7.6.jar and
avro-mapred-1.7.6-hadoop2.jar. For Hadoop 2.x the 
avro-mapred-1.7.6-hadoop2.jar
is needed.

So it seemed that spark-assembly-1.2.0-hadoop2.4.0.jar still did not contain
the org.apache.avro.mapreduce.AvroRecordReaderBase from 
avro-mapred-1.7.6-hadoop2.jar.

I then downloaded the source code and compiled with:
mvn -Pyarn -Phadoop-2.4 -Phive-0.13.1 -DskipTests clean package

The hadoop-2.4 profile sets:
<avro.mapred.classifier>hadoop2</avro.mapred.classifier> which through
dependency management should pull in the right hadoop2 version:

<dependency>
         <groupId>org.apache.avro</groupId>
         <artifactId>avro-mapred</artifactId>
         <version>${avro.version}</version>
<classifier>${avro.mapred.classifier}</classifier>
         <exclusions>

However, same IncompatibleClassChangeError after replacing the assembly jar.

I had cleaned my local ~/.m2/repository before the build and found that for
avro-mapred both 1.7.5 (no extension, i.e. hadoop1) and 1.7.6 (hadoop2) had
been downloaded. That seemed a likely culprit.

After installing the created jar files into my local repo (had to handcopy
poms/jars for repl/yarn subprojects) and then running:

mvn -Pyarn -Phadoop-2.4 -Phive-0.13.1 -DskipTests dependency:tree 
-Dincludes=org.apache.avro:avro-mapred

Building Spark Project Hive 1.2.0
[INFO] 
------------------------------------------------------------------------
[INFO]
[INFO] --- maven-dependency-plugin:2.4:tree (default-cli) @ 
spark-hive_2.10 ---
[INFO] org.apache.spark:spark-hive_2.10:jar:1.2.0
[INFO] +- org.spark-project.hive:hive-exec:jar:0.13.1a:compile
[INFO] |  \- org.apache.avro:avro-mapred:jar:1.7.5:compile
[INFO] \- org.apache.avro:avro-mapred:jar:hadoop2:1.7.6:compile
[INFO]

Showed that hive-exec brought in the avro-mapred-1.7.5.jar (hadoop1). 
Fix for
spark 1.2.x:

spark-1.2.0/sql/hive/pom.xml

     <dependency>
       <groupId>org.spark-project.hive</groupId>
       <artifactId>hive-exec</artifactId>
       <version>${hive.version}</version>
       <exclusions>
         <exclusion>
           <groupId>commons-logging</groupId>
           <artifactId>commons-logging</artifactId>
         </exclusion>
         <exclusion>
           <groupId>com.esotericsoftware.kryo</groupId>
           <artifactId>kryo</artifactId>
         </exclusion>
         <exclusion>
           <groupId>org.apache.avro</groupId>
           <artifactId>avro-mapred</artifactId>
         </exclusion>
       </exclusions>
     </dependency>

  Just add the last exclusion for avro-mapred (comparison at 
https://github.com/medale/spark/compare/apache:v1.2.1-rc2...medale:avro-hadoop2-v1.2.1-rc2).
  I was able to build and run against that fix with Avro code.

  Fix for current master: https://github.com/apache/spark/pull/4315

  Any feedback much appreciated,
  Markus

---------------------------------------------------------------------


"
Kannan Rajah <krajah@maprtech.com>,"Mon, 2 Feb 2015 14:26:43 -0800",Performance test for sort shuffle,dev@spark.apache.org,"Is there a recommended performance test for sort based shuffle? Something
similar to terasort on Hadoop. I couldn't find one on the spark-perf code
base.

https://github.com/databricks/spark-perf

--
Kannan
"
Daniil Osipov <daniil.osipov@shazam.com>,"Mon, 2 Feb 2015 16:16:47 -0800",[spark-sql] JsonRDD,dev <dev@spark.apache.org>,"Hey Spark developers,

Is there a good reason for JsonRDD being a Scala object as opposed to
class? Seems most other RDDs are classes, and can be extended.

The reason I'm asking is that there is a problem with Hive interoperability
with JSON DataFrames where jsonFile generates case sensitive schema, while
Hive expects case insensitive and fails with an exception during
saveAsTable if there are two columns with the same name in different case.

I'm trying to resolve the problem, but that requires me to extend JsonRDD,
which I can't do. Other RDDs are subclass friendly, why is JsonRDD
different?

Dan
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 03 Feb 2015 00:25:47 +0000",Building Spark with Pants,Spark dev list <dev@spark.apache.org>,"Does anyone here have experience with Pants
<http://pantsbuild.github.io/index.html> or interest in trying to build
Spark with it?

Pants has an interesting story. It was born at Twitter to help them build
their Scala, Java, and Python projects as several independent components in
one monolithic repo. (It was inspired by a similar build tool at Google
called blaze.) The mix of languages and sub-projects at Twitter seems
similar to the breakdown we have in Spark.

Pants has an interesting take on how a build system should work, and
Twitter and Foursquare (who use Pants as their primary build tool) claim it
helps enforce better build hygiene and maintainability.

Some relevant talks:

   - Building Scala Hygienically with Pants
   <https://www.youtube.com/watch?v=ukqke8iTuH0>
   - The Pants Build Tool at Twitter
   <https://engineering.twitter.com/university/videos/the-pants-build-tool-at-twitter>
   - Getting Started with the Pants Build System: Why Pants?
   <https://engineering.twitter.com/university/videos/getting-started-with-the-pants-build-system-why-pants>

At some point I may take a shot at converting Spark to use Pants as an
experiment and just see what it‚Äôs like.

Nick
‚Äã
"
Reynold Xin <rxin@databricks.com>,"Mon, 2 Feb 2015 16:30:37 -0800",Re: [spark-sql] JsonRDD,Daniil Osipov <daniil.osipov@shazam.com>,"It's bad naming - JsonRDD is actually not an RDD. It is just a set of util
methods.

The case sensitivity issues seem orthogonal, and would be great to be able
to control that with a flag.



"
Stephen Boesch <javadba@gmail.com>,"Mon, 2 Feb 2015 16:33:11 -0800",Re: Building Spark with Pants,Nicholas Chammas <nicholas.chammas@gmail.com>,"There is a significant investment in sbt and maven - and they are not at
all likely to be going away. A third build tool?  Note that there is also
the perspective of building within an IDE - which actually works presently
for sbt and with a little bit of tweaking with maven as well.

2015-02-02 16:25 GMT-08:00 Nicholas Chammas <nicholas.chammas@gmail.com>:

in
it
-twitter
e-pants-build-system-why-pants
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 03 Feb 2015 00:40:46 +0000",Re: Building Spark with Pants,Stephen Boesch <javadba@gmail.com>,"I'm asking from an experimental standpoint; this is not happening anytime
soon.

Of course, if the experiment turns out very well, Pants would replace both
sbt and Maven (like it has at Twitter, for example). Pants also works with
IDEs <http://pantsbuild.github.io/index.html#using-pants-with>.


y
d
t-twitter
he-pants-build-system-why-pants
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 03 Feb 2015 00:50:34 +0000",Re: Building Spark with Pants,Stephen Boesch <javadba@gmail.com>,"To reiterate, I'm asking from an experimental perspective. I'm not
proposing we change Spark to build with Pants or anything like that.

I'm interested in trying Pants out and I'm wondering if anyone else shares
my interest or already has experience with Pants that they can share.


h
:
o
ly
:
ld
s
m
"
Patrick Wendell <pwendell@gmail.com>,"Mon, 2 Feb 2015 19:37:50 -0800",Temporary jenkins issue,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey All,

I made a change to the Jenkins configuration that caused most builds
to fail (attempting to enable a new plugin), I've reverted the change
effective about 10 minutes ago.

If you've seen recent build failures like below, this was caused by
that change. Sorry about that.

====
ERROR: Publisher
com.google.jenkins.flakyTestHandler.plugin.JUnitFlakyResultArchiver
aborted due to exception
java.lang.NoSuchMethodError:
hudson.model.AbstractBuild.getTestResultAction()Lhudson/tasks/test/AbstractTestResultAction;
at com.google.jenkins.flakyTestHandler.plugin.FlakyTestResultAction.<init>(FlakyTestResultAction.java:78)
at com.google.jenkins.flakyTestHandler.plugin.JUnitFlakyResultArchiver.perform(JUnitFlakyResultArchiver.java:89)
at hudson.tasks.BuildStepMonitor$1.perform(BuildStepMonitor.java:20)
at hudson.model.AbstractBuild$AbstractBuildExecution.perform(AbstractBuild.java:770)
at hudson.model.AbstractBuild$AbstractBuildExecution.performAllBuildSteps(AbstractBuild.java:734)
at hudson.model.Build$BuildExecution.post2(Build.java:183)
at hudson.model.AbstractBuild$AbstractBuildExecution.post(AbstractBuild.java:683)
at hudson.model.Run.execute(Run.java:1784)
at hudson.matrix.MatrixRun.run(MatrixRun.java:146)
at hudson.model.ResourceController.execute(ResourceController.java:89)
at hudson.model.Executor.run(Executor.java:240)
====

- Patrick

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Mon, 2 Feb 2015 20:50:52 -0800",Re: [VOTE] Release Apache Spark 1.2.1 (RC2),Matei Zaharia <matei.zaharia@gmail.com>,"The windows issue reported only affects actually running Spark on
Windows (not job submission). However, I agree it's worth cutting a
new RC. I'm going to cancel this vote and propose RC3 with a single
additional patch. Let's try to vote that through so we can ship Spark
1.2.1.

- Patrick


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Mon, 2 Feb 2015 20:51:32 -0800",[RESULT] [VOTE] Release Apache Spark 1.2.1 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","This is cancelled in favor of RC2.


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Mon, 2 Feb 2015 20:57:28 -0800",[VOTE] Release Apache Spark 1.2.1 (RC3),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version 1.2.1!

The tag to be voted on is v1.2.1-rc3 (commit b6eaf77):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=b6eaf77d4332bfb0a698849b1f5f917d20d70e97

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-1.2.1-rc3/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1065/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-1.2.1-rc3-docs/

Changes from rc2:
A single patch fixing a windows issue.

Please vote on releasing this package as Apache Spark 1.2.1!

The vote is open until Friday, February 06, at 05:00 UTC and passes
if a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.2.1
[ ] -1 Do not release this package because ...

For a list of fixes in this release, see http://s.apache.org/Mpn.

To learn more about Apache Spark, please see
http://spark.apache.org/

---------------------------------------------------------------------


"
masaki rikitoku <rikima3132@gmail.com>,"Tue, 3 Feb 2015 13:56:27 +0900",IDF for ml pipeline,dev@spark.apache.org,"Hi all

I am trying the ml pipeline for text classfication now.

recently, i succeed to execute the pipeline processing in ml packages,
which consist of the original Japanese tokenizer, hashingTF,
logisticRegression.

then,  i failed to  executed the pipeline with idf in mllib package directly.

To use the idf feature in ml package,
do i have to implement the wrapper for idf in ml package like the hashingTF?

best

Masaki Rikitoku

---------------------------------------------------------------------


"
Xuelin Cao <xuelincao2014@gmail.com>,"Tue, 3 Feb 2015 13:48:53 +0800",Can spark provide an option to start reduce stage early?,"""dev@spark.apache.org"" <dev@spark.apache.org>","In hadoop MR, there is an option *mapred.reduce.slowstart.completed.maps*

which can be used to start reducer stage when X% mappers are completed. By
doing this, the data shuffling process is able to parallel with the map
process.

In a large multi-tenancy cluster, this option is usually tuned off. But, in
some cases, turn on the option could accelerate some high priority jobs.

Will spark provide similar option?
"
Krishna Sankar <ksankar42@gmail.com>,"Mon, 2 Feb 2015 22:34:05 -0800",Re: [VOTE] Release Apache Spark 1.2.1 (RC3),Patrick Wendell <pwendell@gmail.com>,"+1 (non-binding, of course)

1. Compiled OSX 10.10 (Yosemite) OK Total time: 11:13 min
     mvn clean package -Pyarn -Dyarn.version=2.6.0 -Phadoop-2.4
-Dhadoop.version=2.6.0 -Phive -DskipTests -Dscala-2.11
2. Tested pyspark, mlib - running as well as comp"
Ewan Higgs <ewan.higgs@ugent.be>,"Tue, 03 Feb 2015 08:52:59 +0100",Re: Performance test for sort shuffle,"Kannan Rajah <krajah@maprtech.com>, dev@spark.apache.org","Hi Kannan,
I have a branch here:

https://github.com/ehiggs/spark/tree/terasort

The code is in the examples. I don't do any fancy partitioning so it 
could be made quicker, I'm sure. But it should be a good baseline.

I have a WIP PR for spark-perf but I'm having trouble building it 
there[1]. I put it on the back burner until someone can get back to me 
on it.

Yours,
Ewan Higgs

[1] 
http://apache-spark-developers-list.1001551.n3.nabble.com/SparkSpark-perf-terasort-WIP-branch-tt10105.html



---------------------------------------------------------------------


"
scwf <wangfei1@huawei.com>,"Tue, 3 Feb 2015 15:53:05 +0800",Jenkins install reference,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi, all
   we want to set up a CI env for spark in our team, is there any reference of how to install jenkins over spark?
   Thanks

Fei


---------------------------------------------------------------------


"
Xiangrui Meng <mengxr@gmail.com>,"Tue, 3 Feb 2015 01:12:47 -0800",Re: IDF for ml pipeline,masaki rikitoku <rikima3132@gmail.com>,"Yes, we need a wrapper under spark.ml. Feel free to create a JIRA for
it. -Xiangrui


---------------------------------------------------------------------


"
Manoj Kumar <manojkumarsivaraj334@gmail.com>,"Tue, 3 Feb 2015 17:47:22 +0530",Accessing indices and values in SparseVector,dev@spark.apache.org,"Hello,

This is related to one of the issues that I'm working on. I am not sure if
this is expected behavior or not.

This works fine.
val sv2 = new SparseVector(3, Array(0, 2), Array(1.1, 3.0))
sv2.indices

But when I do this
val sv2: Vector = Vectors.sparse(3, Array(0, 2), Array(1.1, 3.0))
sv2.indices

It raises an error.

If agreed that this is not expected, I can send a Pull Request.
Thanks.



-- 
Godspeed,
Manoj Kumar,
http://manojbits.wordpress.com
<http://goog_1017110195>
http://github.com/MechCoder
"
Sean Owen <sowen@cloudera.com>,"Tue, 3 Feb 2015 06:18:01 -0600",Re: [VOTE] Release Apache Spark 1.2.1 (RC3),Patrick Wendell <pwendell@gmail.com>,"+1

The signatures are still fine.
Building for Hadoop 2.6 with YARN works; tests pass, except that
MQTTStreamSuite, which we established is a test problem and already
fixed in master.


--------------------------------------------------------------------"
Sean Owen <sowen@cloudera.com>,"Tue, 3 Feb 2015 06:22:33 -0600",Re: Accessing indices and values in SparseVector,Manoj Kumar <manojkumarsivaraj334@gmail.com>,"When you are describing an error, you should say what the error is.
Here I'm pretty sure it says there is no such member of Vector, right?
You explicitly made the type of sv2 Vector and not SparseVector, and
the trait does not have any indices member. No it's not a problem, and
I think the compiler tells you what's happening in this case.


---------------------------------------------------------------------


"
scwf <wangfei1@huawei.com>,"Tue, 3 Feb 2015 20:36:18 +0800",Re: Jenkins install reference,<dev@spark.apache.org>,"Here my question is:
     1 How to set jenkins to make it build for multi PR parallel?. or one machine only support one PR building?
     2 do we need install sbt on the CI machine since the script dev/run-tests will auto fetch the sbt jar ?

- Fei





---------------------------------------------------------------------


"
Manoj Kumar <manojkumarsivaraj334@gmail.com>,"Tue, 3 Feb 2015 18:31:59 +0530",Re: Accessing indices and values in SparseVector,Sean Owen <sowen@cloudera.com>,"Alright, thanks for the quick clarification.
"
shane knapp <sknapp@berkeley.edu>,"Tue, 3 Feb 2015 09:14:55 -0800",Re: Jenkins install reference,scwf <wangfei1@huawei.com>,"here's the wiki describing the system setup:
https://cwiki.apache.org/confluence/display/SPARK/Spark+QA+Infrastructure

we have 1 master and 8 worker nodes, 12 executors per worker (we'd be
better off w/more and smaller worker nodes however).

you don't need to install sbt -- it's in the build/ directory.

the pull request builder builds in parallel, but the master builds require
specific ports to be reserved and each build effectively locks down a
worker until it's done.  since we have 8 worker nodes, it's not *that* big
of a deal...

shane


"
Daniil Osipov <daniil.osipov@shazam.com>,"Tue, 3 Feb 2015 09:15:46 -0800",Re: [spark-sql] JsonRDD,Reynold Xin <rxin@databricks.com>,"Thanks Reynold,

Case sensitivity issues are definitely orthogonal. I'll submit a bug or PR.

Is there a way to rename the object to eliminate the confusion? Not sure
how locked down the API is at this time, but it seems like a potential
confusion point for developers.


"
Kay Ousterhout <keo@eecs.berkeley.edu>,"Tue, 3 Feb 2015 10:33:30 -0800",Re: Can spark provide an option to start reduce stage early?,Xuelin Cao <xuelincao2014@gmail.com>,"There's a JIRA tracking this here:
https://issues.apache.org/jira/browse/SPARK-2387


"
Dirceu Semighini Filho <dirceu.semighini@gmail.com>,"Tue, 3 Feb 2015 16:42:25 -0200",Re: [VOTE] Release Apache Spark 1.2.1 (RC3),,"Hi Patrick,
I work in an Startup and we want make one of our projects as open source.
This project is based on Spark, and it will help users to instantiate spark
clusters in a cloud environment.
But for that project we need to use the repl, hive and thrift-server.
Can the decision of not publishing this libraries be changed in this
release?

Kind Regards,
Dirceu

2015-02-03 10:18 GMT-02:00 Sean Owen <sowen@cloudera.com>:

"
jayhutfles <jayhutfles@gmail.com>,"Tue, 3 Feb 2015 07:28:47 -0700 (MST)",SparkSubmit.scala and stderr,dev@spark.apache.org,"Hi all,

I just saw that the SparkSubmit.scala class has the following lines:

  object SparkSubmit {
    ...
    // Exposed for testing
    private[spark] var printStream: PrintStream = System.err
    ...
  }

This causes all verbose logging messages elsewhere in SparkSubmit to go to
stderr, not stdout.  Is this by design?  Verbose messages don't necessarily
reflect errors.  But as the comment states that it's for testing, maybe I'm
misunderstanding its intent...

Thanks in advance
  -Jay



--

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 03 Feb 2015 18:48:15 +0000",Re: [VOTE] Release Apache Spark 1.2.1 (RC3),Dirceu Semighini Filho <dirceu.semighini@gmail.com>,"I believe this was changed for 1.2.1. Here are the relevant JIRA issues
<https://issues.apache.org/jira/browse/SPARK-5289?jql=project%20%3D%20SPARK%20AND%20fixVersion%20%3D%201.2.1%20AND%20text%20~%20%22publish%22%20order%20by%20priority>
.


"
Chip Senkbeil <chip.senkbeil@gmail.com>,"Tue, 03 Feb 2015 19:47:37 +0000",Re: [VOTE] Release Apache Spark 1.2.1 (RC3),"Nicholas Chammas <nicholas.chammas@gmail.com>, 
	Dirceu Semighini Filho <dirceu.semighini@gmail.com>","+1 Tested the REPL release against the Spark Kernel project
(compilation/testing/manual execution). Everything still checks out fine.

Signed,
Chip Senkbeil
IBM Emerging Technologies Software Engineer


"
Patrick Wendell <pwendell@gmail.com>,"Tue, 3 Feb 2015 11:59:09 -0800",[ANNOUNCE] branch-1.3 has been cut,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey All,

Just wanted to announce that we've cut the 1.3 branch which will
become the 1.3 release after community testing.

There are still some features that will go in (in higher level
libraries, and some stragglers in spark core), but overall this
indicates the end of major feature development for Spark 1.3 and a
transition into testing.

Within a few days I'll cut a snapshot package release for this so that
people can begin testing.

https://git-wip-us.apache.org/repos/asf?p=spark.git;a=shortlog;h=refs/heads/branch-1.3

- Patrick

---------------------------------------------------------------------


"
Yin Huai <yhuai@databricks.com>,"Tue, 3 Feb 2015 12:29:36 -0800",Re: [spark-sql] JsonRDD,Daniil Osipov <daniil.osipov@shazam.com>,"We probably will extract general purpose functions from JsonRDD and also do
the renaming through https://issues.apache.org/jira/browse/SPARK-5260.


"
Sean Owen <sowen@cloudera.com>,"Tue, 3 Feb 2015 15:56:05 -0600",Re: SparkSubmit.scala and stderr,jayhutfles <jayhutfles@gmail.com>,"Despite its name, stderr is frequently used as the destination for
anything that's not the output of the program, which includes log
messages. That way, for example, you can redirect the output of such a
program to capture its result without also capturing log or error
messages, which will still just print to the console.


---------------------------------------------------------------------


"
Marcelo Vanzin <vanzin@cloudera.com>,"Tue, 3 Feb 2015 14:18:10 -0800",Re: SparkSubmit.scala and stderr,jayhutfles <jayhutfles@gmail.com>,"Hi Jay,



The comment is there to tell someone reading the code that this field
is a `var` and not private just because test code (SparkSubmitSuite in
this case) needs to modify it, otherwise it wouldn't exist or would be
private. Similar in spirit to this annotation:

http://guava-libraries.googlecode.com/svn/tags/release09/javadoc/com/google/common/annotations/VisibleForTesting.html

(Which I'd probably have used in this case, but is not really common
in Spark code.)

-- 
Marcelo

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Tue, 3 Feb 2015 14:22:26 -0800",Re: SparkSubmit.scala and stderr,Marcelo Vanzin <vanzin@cloudera.com>,"We can use ScalaTest's privateMethodTester also instead of exposing that.


"
Matei Zaharia <matei.zaharia@gmail.com>,"Tue, 3 Feb 2015 14:34:10 -0800",Welcoming three new committers,dev <dev@spark.apache.org>,"Hi all,

The PMC recently voted to add three new committers: Cheng Lian, Joseph Bradley and Sean Owen. All three have been major contributors to Spark in the past year: Cheng on Spark SQL, Joseph on MLlib, and Sean on ML and many pieces throughout Spark Core. Join me in welcoming them as committers!

Matei
---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 03 Feb 2015 22:53:46 +0000",Re: Welcoming three new committers,"Matei Zaharia <matei.zaharia@gmail.com>, dev <dev@spark.apache.org>","Congratulations guys!


"
Ted Yu <yuzhihong@gmail.com>,"Tue, 3 Feb 2015 14:55:32 -0800",Re: Welcoming three new committers,Nicholas Chammas <nicholas.chammas@gmail.com>,"Congratulations, Cheng, Joseph and Sean.


"
"""Hari Shreedharan"" <hshreedharan@cloudera.com>","Tue, 03 Feb 2015 15:02:20 -0800 (PST)",Re: Welcoming three new committers,"""Ted Yu"" <yuzhihong@gmail.com>","Congrats Cheng, Joseph and Owen! Well done!




Thanks,¬†Hari


com
com>
Joseph
 in
and
"
"""Pritish Nawlakhe"" <pritish@nirvana-international.com>","Tue, 3 Feb 2015 18:10:03 -0500",RE: Welcoming three new committers,"""'Hari Shreedharan'"" <hshreedharan@cloudera.com>,
	""'Ted Yu'"" <yuzhihong@gmail.com>","Congrats and welcome back!!



Thank you!!

Regards
Pritish
Nirvana International Inc.

Big Data, Hadoop, Oracle EBS and IT Solutions
VA - SWaM, MD - MBE Certified Company
pritish@nirvana-international.com 
http://www.nirvana-international.com 
Twitter: @nirvanainternat 


Congrats Cheng, Joseph and Owen! Well done!




Thanks, Hari


committers!


---------------------------------------------------------------------


"
Timothy Chen <tnachen@gmail.com>,"Wed, 4 Feb 2015 07:17:46 +0800",Re: Welcoming three new committers,Pritish Nawlakhe <pritish@nirvana-international.com>,"Congrats all!

Tim


 Owen

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Tue, 3 Feb 2015 15:20:43 -0800",Re: 2GB limit for partitions?,Michael Albert <m_albert137@yahoo.com>,"cc dev list


How are you saving the data? There are two relevant 2GB limits:

1. Caching

2. Shuffle


For caching, a partition is turned into a single block.

For shuffle, each map partition is partitioned into R blocks, where R =
number of reduce tasks. It is unlikely a shuffle block > 2G, although it
can still happen.

I think the 2nd problem is easier to fix than the 1st, because we can
handle that in the network transport layer. It'd require us to divide the
transfer of a very large block into multiple smaller blocks.




"
Imran Rashid <irashid@cloudera.com>,"Tue, 3 Feb 2015 17:44:53 -0600",Re: 2GB limit for partitions?,Reynold Xin <rxin@databricks.com>,"Thanks for the explanations, makes sense.  For the record looks like this
was worked on a while back (and maybe the work is even close to a solution?)

https://issues.apache.org/jira/browse/SPARK-1476

and perhaps an independent solution was worked on here?

https://issues.apache.org/jira/browse/SPARK-1391



"
Evan Chan <velvia.github@gmail.com>,"Tue, 3 Feb 2015 16:01:55 -0800",Re: Welcoming three new committers,Timothy Chen <tnachen@gmail.com>,"Congrats everyone!!!


---------------------------------------------------------------------


"
Evan Chan <velvia.github@gmail.com>,"Tue, 3 Feb 2015 16:02:58 -0800",Re: SparkSubmit.scala and stderr,Reynold Xin <rxin@databricks.com>,"Why not just use SLF4J?


---------------------------------------------------------------------


"
"""masaki rikitoku"" <rikima3132@gmail.com>","Tue, 03 Feb 2015 16:40:20 -0800 (PST)",Re: IDF for ml pipeline,"""Xiangrui Meng"" <mengxr@gmail.com>","Thank you for your reply. I will do it.



‚Äî
Mailbox „Åã„ÇâÈÄÅ‰ø°


directly.
hashingTF?"
Corey Nolet <cjnolet@gmail.com>,"Tue, 3 Feb 2015 20:01:39 -0500",Re: Welcoming three new committers,Evan Chan <velvia.github@gmail.com>,"Congrats guys!


"
Xuefeng Wu <benewu@gmail.com>,"Wed, 4 Feb 2015 09:08:20 +0800",Re: Welcoming three new committers,Matei Zaharia <matei.zaharia@gmail.com>,"Congratulations£°well done. 

Yours, Xuefeng Wu Œ‚—©∑Â æ¥…œ

dley and Sean Owen. All three have been major contributors to Spark in the past year: Cheng on Spark SQL, Joseph on MLlib, and Sean on ML and many pieces throughout Spark Core. Join me in welcoming them as committers!

---------------------------------------------------------------------


"
Nan Zhu <zhunanmcgill@gmail.com>,"Tue, 3 Feb 2015 20:26:11 -0500",Re: Welcoming three new committers,Xuefeng Wu <benewu@gmail.com>,"Congratulations!

--  
Nan Zhu
http://codingcat.me



i Zaharia <matei.zaharia@gmail.com (mailto:matei.zaharia@gmail.com)> h Bradley and Sean Owen. All three have been major contributors to Spark in the past year: Cheng on Spark SQL, Joseph on MLlib, and Sean on ML and many pieces throughout Spark Core. Join me in welcoming them as committers!

v-unsubscribe@spark.apache.org)
o:dev-help@spark.apache.org)
unsubscribe@spark.apache.org)
dev-help@spark.apache.org)


"
Mridul Muralidharan <mridul@gmail.com>,"Tue, 3 Feb 2015 17:29:28 -0800",Re: Welcoming three new committers,Matei Zaharia <matei.zaharia@gmail.com>,"Congratulations !
Keep up the good work :-)

Regards
Mridul



"
Chao Chen <crazyjvm@gmail.com>,"Wed, 04 Feb 2015 09:32:41 +0800",Re: Welcoming three new committers,dev@spark.apache.org,"Congratulations guys, well done!

Âú® 15-2-4 ‰∏äÂçà9:26, Nan Zhu ÂÜôÈÅì:


---------------------------------------------------------------------


"
Mridul Muralidharan <mridul@gmail.com>,"Tue, 3 Feb 2015 17:32:55 -0800",Re: 2GB limit for partitions?,Imran Rashid <irashid@cloudera.com>,"That is fairly out of date (we used to run some of our jobs on it ... But
that is forked off 1.1 actually).

Regards
Mridul


"
Denny Lee <denny.g.lee@gmail.com>,"Wed, 04 Feb 2015 01:34:57 +0000",Re: Welcoming three new committers,"Chao Chen <crazyjvm@gmail.com>, dev@spark.apache.org","Awesome stuff - congratulations! :)


i Zaharia <matei.zaharia@gmail.com
h
ny
"
Debasish Das <debasish.das83@gmail.com>,"Tue, 3 Feb 2015 17:38:17 -0800",Re: Welcoming three new committers,Denny Lee <denny.g.lee@gmail.com>,"Congratulations !

Keep helping the community :-)


tei Zaharia <matei.zaharia@gmail.com
in
--
-
"
Shixiong Zhu <zsxwing@gmail.com>,"Wed, 4 Feb 2015 10:23:36 +0800",Re: Welcoming three new committers,"Cheng Lian <lian@databricks.com>, Joseph Bradley <joseph@databricks.com>, 
	Sean Owen <sowen@cloudera.com>","Congrats guys!

Best Regards,
Shixiong Zhu

2015-02-04 6:34 GMT+08:00 Matei Zaharia <matei.zaharia@gmail.com>:

"
Ye Xianjin <advancedxy@gmail.com>,"Wed, 4 Feb 2015 10:32:47 +0800",Re: Welcoming three new committers,Matei Zaharia <matei.zaharia@gmail.com>,"Congratulations!

-- 
Ye Xianjin
Sent with Sparrow (http://www.sparrowmailapp.com/?sig)





"
Joseph Bradley <joseph@databricks.com>,"Tue, 3 Feb 2015 18:33:52 -0800",Re: Welcoming three new committers,dev <dev@spark.apache.org>,"Thanks to everyone in the community for past collaborations, and I look
forward to continuing in the future!
Joseph


"
Zhan Zhang <zzhang@hortonworks.com>,"Wed, 4 Feb 2015 04:22:49 +0000",Re: Welcoming three new committers,Matei Zaharia <matei.zaharia@gmail.com>,"Congratulations!


adley and Sean Owen. All three have been major contributors to Spark in the past year: Cheng on Spark SQL, Joseph on MLlib, and Sean on ML and many pieces throughout Spark Core. Join me in welcoming them as committers!


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Tue, 3 Feb 2015 21:09:08 -0800",ASF Git / GitHub sync is down,"""dev@spark.apache.org"" <dev@spark.apache.org>","Haven't sync-ed anything for the last 4 hours. Seems like this little piece
of infrastructure always stops working around our own code freeze time ...
"
Manish Amde <manish9ue@gmail.com>,"Tue, 3 Feb 2015 21:15:06 -0800",Re: Welcoming three new committers,Zhan Zhang <zzhang@hortonworks.com>,"Congratulations Cheng, Joseph and Sean.


"
Reynold Xin <rxin@databricks.com>,"Tue, 3 Feb 2015 21:25:37 -0800",Re: ASF Git / GitHub sync is down,"""dev@spark.apache.org"" <dev@spark.apache.org>","I filed an INFRA ticket: https://issues.apache.org/jira/browse/INFRA-9115



I wish ASF can reconsider requests like this in order to handle downtime
gracefully https://issues.apache.org/jira/browse/INFRA-8738


"
prabeesh k <prabsmails@gmail.com>,"Wed, 4 Feb 2015 11:43:06 +0400",Re: Welcoming three new committers,Matei Zaharia <matei.zaharia@gmail.com>,"Congratulations!


"
Sean Owen <sowen@cloudera.com>,"Wed, 4 Feb 2015 06:10:19 -0600",Re: Welcoming three new committers,dev <dev@spark.apache.org>,"Thanks all, I appreciate the vote of trust. I'll do my best to help
keep JIRA and commits moving along, and am ramping up carefully this
week. Now get back to work reviewing things!

te:
adley and Sean Owen. All three have been major contributors to Spark in the past year: Cheng on Spark SQL, Joseph on MLlib, and Sean on ML and many pieces throughout Spark Core. Join me in welcoming them as committers!

---------------------------------------------------------------------


"
Nick Pentreath <nick.pentreath@gmail.com>,"Wed, 4 Feb 2015 14:25:28 +0200",Re: Welcoming three new committers,dev <dev@spark.apache.org>,"Congrats and welcome Sean, Joseph and Cheng!



"
Imran Rashid <irashid@cloudera.com>,"Wed, 4 Feb 2015 07:33:22 -0600",Re: 2GB limit for partitions?,Mridul Muralidharan <mridul@gmail.com>,"Hi Mridul,


do you think you'll keep working on this, or should this get picked up by
others?  Looks like there was a lot of work put into LargeByteBuffer, seems
promising.

thanks,
Imran


"
Al Thompson <athompson.opr@gmail.com>,"Wed, 4 Feb 2015 05:52:46 -0800",Hive window functions in 1.2+,user@spark.apache.org,"Hi All:

We want to use Hive window functions on Spark on time series data.
I take it from viewing issues SPARK-1442 and SPARK-4226 that work on window
function support is ongoing and remains unresolved.

Are there good workarounds to working with windows on time series stored on
Spark?
Does Streaming have a role in windowed analysis of time series?

Cheers,
Al
"
"""M. Dale"" <medale94@yahoo.com.INVALID>","Wed, 04 Feb 2015 10:30:44 -0500",1.2.1-rc3 - Avro input format for Hadoop 2 broken/fix?,dev@spark.apache.org,"SPARK-3039 ""Spark assembly for new hadoop API (hadoop 2) contains 
avro-mapred for hadoop 1 API"" was reopened
and prevents v.1.2.1-rc3 from using Avro Input format for Hadoop 2 
API/instances (it includes the hadoop1 avro-mapred library files).

What are the chances of getting the fix outlined here 
(https://github.com/medale/spark/compare/apache:v1.2.1-rc3...avro-hadoop2-v1.2.1-rc2) 
included in 1.2.1? My apologies, I do not know how to generate a pull 
request against a tag version.

I did add pull request https://github.com/apache/spark/pull/4315 for the 
current 1.3.0-SNAPSHOT master on this issue. Even though 1.3.0 build 
already does not include avro-mapred in the spark assembly jar this 
minor change improves dependence convergence.

Thanks,
Markus

---------------------------------------------------------------------


"
Jian Zhou <jzhou.research@gmail.com>,"Wed, 04 Feb 2015 15:52:35 +0000",Re: Welcoming three new committers,"Nick Pentreath <nick.pentreath@gmail.com>, dev <dev@spark.apache.org>","Congratulations!


"
Mridul Muralidharan <mridul@gmail.com>,"Wed, 4 Feb 2015 08:41:04 -0800",Re: 2GB limit for partitions?,Imran Rashid <irashid@cloudera.com>,"That work is from more than an year back and is not maintained anymore
since we do not use it inhouse now.
Also note that there have been quite a lot of changes in spark ...
Including some which break assumptions made in the patch, so it's value is
very low - having said that, do feel free to work on the jira and/or use
the patch if it helps !

Regards
Mridul


"
Patrick Wendell <pwendell@gmail.com>,"Wed, 4 Feb 2015 10:11:09 -0800",Re: 1.2.1-rc3 - Avro input format for Hadoop 2 broken/fix?,medale@acm.org,"Hi Markus,

That won't be included in 1.2.1 most likely because the release votes
have already started, and at that point we don't hold the release
except for major regression issues from 1.2.0. However, if this goes
through we can backport it into the 1.2 branch and it will end up in a
future maintenance release, or you can just build spark from that
branch as soon as it's in there.

- Patric


---------------------------------------------------------------------


"
Sergey Belousov <sergey.belousov@gmail.com>,"Wed, 4 Feb 2015 16:08:30 -0500",Spark Cluster vs Spark on YARN jar loading,dev@spark.apache.org,"Hi All

We have our farjar that using asynchbase throwing following exception.

ERROR [Executor task launch worker-2-EventThread:ClientCnxn$EventThread@610]
- Caught unexpected throwable
java.lang.IllegalAccessError: class
com.google.protobuf.ZeroCopyLiteralByteString cannot access its superclass
com.google.protobuf.LiteralByteString
at java.lang.ClassLoader.defineClass1(Native Method)
at java.lang.ClassLoader.defineClass(Unknown Source)
at java.security.SecureClassLoader.defineClass(Unknown Source)
at java.net.URLClassLoader.defineClass(Unknown Source)
at java.net.URLClassLoader.access$100(Unknown Source)
at java.net.URLClassLoader$1.run(Unknown Source)
at java.net.URLClassLoader$1.run(Unknown Source)
at java.security.AccessController.doPrivileged(Native Method)
at java.net.URLClassLoader.findClass(Unknown Source)
at java.lang.ClassLoader.loadClass(Unknown Source)
at java.lang.ClassLoader.loadClass(Unknown Source)
at org.hbase.async.Bytes.wrap(Bytes.java:287)
at org.hbase.async.RegionClient.<clinit>(RegionClient.java:580)
at
org.hbase.async.HBaseClient$RegionClientPipeline.init(HBaseClient.java:2655)
at org.hbase.async.HBaseClient.newClient(HBaseClient.java:2604)
at org.hbase.async.HBaseClient.access$2700(HBaseClient.java:179)
at
org.hbase.async.HBaseClient$ZKClient$ZKCallback.handleMetaZnode(HBaseClient.java:3301)
at
org.hbase.async.HBaseClient$ZKClient$ZKCallback.processResult(HBaseClient.java:3157)
at
org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:558)
at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:495)


*But when we try it with Spark on Yarn we do not have that problem. *
Can someone provide details as of what are differences that made that
particular problem go away when we use Spark on YARN?

I have some guess but I would really appreciate some infor from people who
has more than just guess.

Thank you
S
"
Sasha Kacanski <skacanski@gmail.com>,"Wed, 4 Feb 2015 16:13:21 -0500",ZMQ and python streaming,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

is it possible to integrate zmq with pyspark.streaming to receive messages
over TCP socket.

I seem to not be able to find working example for ZeroMQ implementation.

Regards,

-- 
Aleksandar Kacanski
"
Kay Ousterhout <kayousterhout@gmail.com>,"Wed, 4 Feb 2015 13:53:22 -0800",multi-line comment style,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,

The Spark Style Guide
<https://cwiki.apache.org/confluence/display/SPARK/Spark+Code+Style+Guide>
says multi-line comments should formatted as:

/*
 * This is a
 * very
 * long comment.
 */

But in my experience, we almost always use ""//"" for multi-line comments:

// This is a
// very
// long comment.

Here are some examples:

   - Recent commit by Reynold, king of style:
   https://github.com/apache/spark/commit/bebf4c42bef3e75d31ffce9bfdb331c16f34ddb1#diff-d616b5496d1a9f648864f4ab0db5a026R58
   - RDD.scala:
   https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/RDD.scala#L361
   - DAGScheduler.scala:
   https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L281


Any objections to me updating the style guide to reflect this?  As with
other style issues, I think consistency here is helpful (and formatting
multi-line comments as ""//"" does nicely visually distinguish code comments
from doc comments).

-Kay
"
Sean Owen <sowen@cloudera.com>,"Wed, 4 Feb 2015 15:58:41 -0600",Re: multi-line comment style,Kay Ousterhout <kayousterhout@gmail.com>,"interfere with commenting out blocks of code with /* */, which is a
small good thing. I am also accustomed to // style for multiline, and
reserve /** */ for javadoc / scaladoc. Meaning, seeing the /* */ style
inline always looks a little funny to me.


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Wed, 4 Feb 2015 14:05:38 -0800",Re: multi-line comment style,Sean Owen <sowen@cloudera.com>,"Personally I have no opinion, but agree it would be nice to standardize.

- Patrick


---------------------------------------------------------------------


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Wed, 4 Feb 2015 14:09:54 -0800",Re: multi-line comment style,Patrick Wendell <pwendell@gmail.com>,"FWIW I like the multi-line // over /* */ from a purely style standpoint.
The Google Java style guide[1] has some comment about code formatting tools
working better with /* */ but there doesn't seem to be any strong arguments
for one over the other I can find

Thanks
Shivaram

[1]
https://google-styleguide.googlecode.com/svn/trunk/javaguide.html#s4.8.6.1-block-comment-style


"
Reynold Xin <rxin@databricks.com>,"Wed, 4 Feb 2015 14:15:56 -0800",Re: multi-line comment style,shivaram@eecs.berkeley.edu,"We should update the style doc to reflect what we have in most places
(which I think is //).




"
scwf <wangfei1@huawei.com>,"Thu, 5 Feb 2015 09:32:29 +0800",Re: Welcoming three new committers,<dev@spark.apache.org>,"Congratulations!




---------------------------------------------------------------------


"
Stephen Boesch <javadba@gmail.com>,"Wed, 4 Feb 2015 19:25:13 -0800",Broken record a bit here: building spark on intellij with sbt,"""dev@spark.apache.org"" <dev@spark.apache.org>","For building in intellij with sbt my mileage has varied widely: it had
built as late as Monday (after the 1.3.0 release)  - and with zero
'special' steps: just ""import"" as sbt project.

 However I can not presently repeat the process.  The wiki page has the
latest instructions on how to build with maven - but not with sbt.  Is
there a resource for that?

https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-IntelliJ

The error I see is same as from a post in July

http://apache-spark-user-list.1001560.n3.nabble.com/Build-spark-with-Intellij-IDEA-13-td9904.html

Here is an excerpt:

uncaught exception during compilation: java.lang.AssertionError
Error:scalac: Error: assertion failed:
com.google.protobuf.InvalidProtocolBufferException
java.lang.AssertionError: assertion failed:
com.google.protobuf.InvalidProtocolBufferException
        at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1212)

The answer in the mailing list to that thread was about using maven .. so
that is not useful here.
"
Evan Chan <velvia.github@gmail.com>,"Wed, 4 Feb 2015 17:46:26 -0800",Spark Summit CFP - Tracks guidelines,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey guys,

Is there any guidance on what the different tracks for Spark Summit
West mean?  There are some new ones, like ""Third Party Apps"", which
seems like it would be similar to the ""Use Cases"".   Any further
guidance would be great.

thanks,
Evan

---------------------------------------------------------------------


"
Kay Ousterhout <keo@eecs.berkeley.edu>,"Wed, 4 Feb 2015 17:55:06 -0800",Re: Spark Summit CFP - Tracks guidelines,Evan Chan <velvia.github@gmail.com>,"Did you see the longer descriptions under the ""Learn More"" link?

Developer
This track will present technical deep dive content across a wide range of
advanced/basic topics.

Data Science
This track will focus on the practice of data science using Spark. Sessions
should cover innovative techniques, algorithms and systems that refine raw
data into actionable insight using visualization, statistics and machine
learning.

Use Cases
This track is about use cases of Spark in the enterprise and lessons
learned.

Third-Party Apps
This track will focus on the applications that run on Spark. Talks will
offer more of the vendor perspective for end-to-end apps: integrations,
benefits of leveraging Spark, trade-offs, etc.

Research
This track will focus on academic research. Talks range from systems
research involving Spark to research use cases (e.g. genomics).

Business
This track will focus on how businesses utilize Spark. Talks will offer
exploration into ROI, best practices, Compliance requirements for specific
industries, customer testimonials.


"
Xuelin Cao <xuelincao2014@gmail.com>,"Thu, 5 Feb 2015 11:44:10 +0800",When will Spark Streaming supports Kafka-simple consumer API?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

     In our environment, Kafka can only be used with simple consumer API,
like storm spout does.

     And, also, I found there are suggestions that "" Kafka connector of
Spark should not be used in production
<http://markmail.org/message/2lb776ta5sq6lgtw> because it is based on the
high-level consumer API of Kafka.""

    So, my question is, when will spark streaming supports Kafka simple
consumer API?
"
Tathagata Das <tathagata.das1565@gmail.com>,"Wed, 4 Feb 2015 20:26:40 -0800",Re: When will Spark Streaming supports Kafka-simple consumer API?,Xuelin Cao <xuelincao2014@gmail.com>,"1. There is already a third-party low-level kafka receiver -
http://spark-packages.org/package/5
2. There is a new experimental Kafka stream that will be available in Spark
1.3 release. This is based on the low level API, and might suffice your
purpose. JIRA - https://issues.apache.org/jira/browse/SPARK-4964

Can you elaborate on why you have to use SimpleConsumer in your environment?

TD



"
Akhil Das <akhil@sigmoidanalytics.com>,"Thu, 5 Feb 2015 11:16:29 +0530",Re: Broken record a bit here: building spark on intellij with sbt,Stephen Boesch <javadba@gmail.com>,"Here's the sbt version
https://docs.sigmoidanalytics.com/index.php/Step_by_Step_instructions_on_how_to_build_Spark_App_with_IntelliJ_IDEA


Thanks
Best Regards


"
Deep Pradhan <pradhandeep1991@gmail.com>,"Thu, 5 Feb 2015 11:37:07 +0530",Re: Broken record a bit here: building spark on intellij with sbt,Akhil Das <akhil@sigmoidanalytics.com>,"Akhil, it is not able to find the SBT package when I tried the steps given
in the site.


"
"""Xuelin Cao.2015"" <xuelincao2014@gmail.com>","Thu, 5 Feb 2015 03:31:08 -0700 (MST)",Re: When will Spark Streaming supports Kafka-simple consumer API?,dev@spark.apache.org,"Hi, Tathagata

     Thanks for the information, I'm trying to build 1.3 snapshot and make
another try.

     There are 2 reasons for why we use Kafka SimpleConsumer API
     1. Previously, in our company, all of the real time processing system
were build on Apache Storm. So, the kafka environment is set to only
support SimpleConsumer API. The kafka environment is controlled by another
group of engineers in our company, and for some reasons I don't know, they
only support SimpleConsumer API.

     2. There is a document advises do not use kafka + spark streaming in
the production environment, due to spark streaming only supports high level
API. see
*http://www.michael-noll.com/blog/2014/10/01/kafka-spark-streaming-integration-example-tutorial/#known-issues-in-spark-streaming
<http://www.michael-noll.com/blog/2014/10/01/kafka-spark-streaming-integration-example-tutorial/#known-issues-in-spark-streaming>*

         I'm not quite sure whether the advise is with bias to spark
streaming. But, since we don't have any successful project as our
reference, we need to be careful about it.








--"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Thu, 5 Feb 2015 19:55:21 +0000",Using CUDA within Spark / boosting linear algebra,"""dev@spark.apache.org"" <dev@spark.apache.org>","Dear Spark developers,

I am exploring how to make linear algebra operations faster within Spark. One way of doing this is to use Scala Breeze library that is bundled with Spark. For matrix operations, it employs Netlib-java that has a Java wrapper for BLAS (basic linear algebra subprograms) and LAPACK native binaries if they are available on the worker node. It also has its own optimized Java implementation of BLAS. It is worth mentioning, that native binaries provide better performance only for BLAS level 3, i.e. matrix-matrix operations or general matrix multiplication (GEMM). This is confirmed by GEMM test on Netlib-java page https://github.com/fommil/netlib-java. I also confirmed it with my experiments with training of artificial neural network https://github.com/apache/spark/pull/1290#issuecomment-70313952. However, I would like to boost performance more.

GPU is supposed to work fast with linear algebra and there is Nvidia CUDA implementation of BLAS, called cublas. I have one Linux server with Nvidia GPU and I was able to do the following. I linked cublas (instead of cpu-based blas) with Netlib-java wrapper and put it into Spark, so Breeze/Netlib is using it. Then I did some performance measurements with regards to artificial neural network batch learning in Spark MLlib that involves matrix-matrix multiplications. It turns out that for matrices of size less than ~1000x780 GPU cublas has the same speed as CPU blas. Cublas becomes slower for bigger matrices. It worth mentioning that it is was not a test for ONLY multipslowdown might be the overhead of copying the matrices from computer memory to graphic card memory and back. 

So, few questions:
1) Do these results with CUDA make sense? 
2) If the problem is with copy overhead, are there any libraries that allow to force intermediate results to stay in graphic card memory thus removing the overhead?
3) Any other options to speed-up linear algebra in Spark?

Thank you, Alexander

---------------------------------------------------------------------


"
"""Evan R. Sparks"" <evan.sparks@gmail.com>","Thu, 5 Feb 2015 12:08:32 -0800",Re: Using CUDA within Spark / boosting linear algebra,"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","I'd expect that we can make GPU-accelerated BLAS faster than CPU blas in
many cases.

You might consider taking a look at the codepaths that BIDMat (
https://github.com/BIDData/BIDMat) takes and comparing them to
netlib-java/breeze. John Canny et. al. have done a bunch of work optimizing
to make this work really fast from Scala. I've run it on my laptop and
compared to MKL and in certain cases it's 10x faster at matrix multiply.
There are a lot of layers of indirection here and you really want to avoid
data copying as much as possible.

We could also consider swapping out BIDMat for Breeze, but that would be a
big project and if we can figure out how to get breeze+cublas to comparable
performance that would be a big win.


"
Stephen Boesch <javadba@gmail.com>,"Thu, 5 Feb 2015 12:11:32 -0800",Re: Broken record a bit here: building spark on intellij with sbt,Akhil Das <akhil@sigmoidanalytics.com>,"Hi Akhil
  Those instructions you provided are showing how to manually build an sbt
project that may include adding spark dependencies.  Whereas my OP was
about how to open the existing spark sbt project .  These two are not
similar tasks.

2015-02-04 21:46 GMT-08:00 Akhil Das <akhil@sigmoidanalytics.com>:

"
Arush Kharbanda <arush@sigmoidanalytics.com>,"Fri, 6 Feb 2015 02:00:02 +0530",Re: Broken record a bit here: building spark on intellij with sbt,Stephen Boesch <javadba@gmail.com>,"I follow these ones to import sbt projects.

1. Install sbt plugins: Goto File -> Settings -> Plugins -> Install
IntelliJ Plugins -> Search for sbt and install it
2. File ->Import->browse the root of spark source code

I hope this helps








-- 

[image: Sigmoid Analytics] <http://htmlsig.com/www.sigmoidanalytics.com>

Arush Kharbanda || Technical Teamlead

arush@sigmoidanalytics.com || www.sigmoidanalytics.com
"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Thu, 5 Feb 2015 21:00:51 +0000",RE: Using CUDA within Spark / boosting linear algebra,"""Evan R. Sparks"" <evan.sparks@gmail.com>","Hi Evan,

Thank you for suggestion! BIDMat seems to have terrific speed. Do you know what makes them faster than netlib-java?

The same group has BIDMach library that implements machine learning. For some examples they use Caffe convolutional neural network library owned by another group in Berkeley. Could you elaborate on how these all might be connected with Spark Mllib? If you take BIDMat for linear algebra why don‚Äôt you take BIDMach for optimization and learning?

Best regards, Alexander

From: Evan R. Sparks [mailto:evan.sparks@gmail.com]
Sent: Thursday, February 05, 2015 12:09 PM
To: Ulanov, Alexander
Cc: dev@spark.apache.org
Subject: Re: Using CUDA within Spark / boosting linear algebra

I'd expect that we can make GPU-accelerated BLAS faster than CPU blas in many cases.

You might consider taking a look at the codepaths that BIDMat (https://github.com/BIDData/BIDMat) takes and comparing them to netlib-java/breeze. John Canny et. al. have done a bunch of work optimizing to make this work really fast from Scala. I've run it on my laptop and compared to MKL and in certain cases it's 10x faster at matrix multiply. There are a lot of layers of indirection here and you really want to avoid data copying as much as possible.

We could also consider swapping out BIDMat for Breeze, but that would be a big project and if we can figure out how to get breeze+cublas to comparable performance that would be a big win.

On Thu, Feb 5, 2015 at 11:55 AM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
Dear Spark developers,

I am exploring how to make linear algebra operations faster within Spark. One way of doing this is to use Scala Breeze library that is bundled with Spark. For matrix operations, it employs Netlib-java that has a Java wrapper for BLAS (basic linear algebra subprograms) and LAPACK native binaries if they are available on the worker node. It also has its own optimized Java implementation of BLAS. It is worth mentioning, that native binaries provide better performance only for BLAS level 3, i.e. matrix-matrix operations or general matrix multiplication (GEMM). This is confirmed by GEMM test on Netlib-java page https://github.com/fommil/netlib-java. I also confirmed it with my experiments with training of artificial neural network https://github.com/apache/spark/pull/1290#issuecomment-70313952. However, I would like to boost performance more.

GPU is supposed to work fast with linear algebra and there is Nvidia CUDA implementation of BLAS, called cublas. I have one Linux server with Nvidia GPU and I was able to do the following. I linked cublas (instead of cpu-based blas) with Netlib-java wrapper and put it into Spark, so Breeze/Netlib is using it. Then I did some performance measurements with regards to artificial neural network batch learning in Spark MLlib that involves matrix-matrix multiplications. It turns out that for matrices of size less than ~1000x780 GPU cublas has the same speed as CPU blas. Cublas becomes slower for bigger matrices. It worth mentioning that it is was not a test for ONLY multiplication since there are other operations involved. One of the reasons for slowdown might be the overhead of copying the matrices from computer memory to graphic card memory and back.

So, few questions:
1) Do these results with CUDA make sense?
2) If the problem is with copy overhead, are there any libraries that allow to force intermediate results to stay in graphic card memory thus removing the overhead?
3) Any other options to speed-up linear algebra in Spark?

Thank you, Alexander

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org>
For additional commands, e-mail: dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>

"
"""Evan R. Sparks"" <evan.sparks@gmail.com>","Thu, 5 Feb 2015 13:28:39 -0800",Re: Using CUDA within Spark / boosting linear algebra,"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","I'd be surprised of BIDMat+OpenBLAS was significantly faster than
netlib-java+OpenBLAS, but if it is much faster it's probably due to data
layout and fewer levels of indirection - it's definitely a worthwhile
experiment to run. The main speedups I've seen from using it come from
highly optimized GPU code for linear algebra. I know that in the past Canny
has gone as far as to write custom GPU kernels for performance-critical
regions of code.[1]

BIDMach is highly optimized for single node performance or performance on
batched in that way) the performance tends to fall off. Canny argues for
hardware/software codesign and as such prefers machine configurations that
are quite different than what we find in most commodity cluster nodes -
e.g. 10 disk cahnnels and 4 GPUs.

In contrast, MLlib was designed for horizontal scalability on commodity
clusters and works best on very big datasets - order of terabytes.

For the most part, these projects developed concurrently to address
slightly different use cases. That said, there may be bits of BIDMach we
could repurpose for MLlib - keep in mind we need to be careful about
maintaining cross-language compatibility for our Java and Python-users,
though.

- Evan

[1] - http://arxiv.org/abs/1409.5402
[2] - http://eecs.berkeley.edu/~hzhao/papers/BD.pdf


w
y
‚Äôt
ng
d
a
le
e
a
s
t
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 06 Feb 2015 00:16:12 +0000",PSA: Maven supports parallel builds,Spark dev list <dev@spark.apache.org>,"Y‚Äôall may already know this, but I haven‚Äôt seen it mentioned anywhere in
our docs on here and it‚Äôs a pretty easy win.

Maven supports parallel builds
<https://cwiki.apache.org/confluence/display/MAVEN/Parallel+builds+in+Maven+3>
with the -T command line option.

For example:

./build/mvn -T 1C -Dhadoop.version=1.2.1 -DskipTests clean package

This will have Maven use 1 thread per core on your machine to build Spark.

minutes. A machine with more cores should see a bigger improvement.

Note though that the docs mark this as experimental, so I wouldn‚Äôt change
our reference build to use this. But it should be useful, for example, in
Jenkins or when working locally.

Nick
‚Äã
"
Dirceu Semighini Filho <dirceu.semighini@gmail.com>,"Thu, 5 Feb 2015 22:52:30 -0200",Re: PSA: Maven supports parallel builds,Nicholas Chammas <nicholas.chammas@gmail.com>,"Thanks Nicholas, I didn't knew this.

2015-02-05 22:16 GMT-02:00 Nicholas Chammas <nicholas.chammas@gmail.com>:

ned anywhere in
n+3
.
5
t change
"
Patrick Wendell <pwendell@gmail.com>,"Thu, 5 Feb 2015 16:59:46 -0800",Re: PSA: Maven supports parallel builds,Dirceu Semighini Filho <dirceu.semighini@gmail.com>,"I've done this in the past, but back when I wasn't using Zinc it
didn't make a big difference. It's worth doing this in our jenkins
environment though.

- Patrick


---------------------------------------------------------------------


"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Fri, 6 Feb 2015 00:59:29 +0000",RE: Using CUDA within Spark / boosting linear algebra,"""Evan R. Sparks"" <evan.sparks@gmail.com>","Thank you for explanation! I‚Äôve watched the BIDMach presentation by John Canny and I am really inspired by his talk and comparisons with Spark MLlib.

I am very interested to find out what will be better within Spark: BIDMat or netlib-java with CPU or GPU natives. Could you suggest a fair way to benchmark them? Currently I do benchmarks on artificial neural networks in batch mode. While it is not a ‚Äúpure‚Äù test of linear algebra, it involves some other things that are essential to machine learning.

From: Evan R. Sparks [mailto:evan.sparks@gmail.com]
Sent: Thursday, February 05, 2015 1:29 PM
To: Ulanov, Alexander
Cc: dev@spark.apache.org
Subject: Re: Using CUDA within Spark / boosting linear algebra

I'd be surprised of BIDMat+OpenBLAS was significantly faster than netlib-java+OpenBLAS, but if it is much faster it's probably due to data layout and fewer levels of indirection - it's definitely a worthwhile experiment to run. The main speedups I've seen from using it come from highly optimized GPU code for linear algebra. I know that in the past Canny has gone as far as to write custom GPU kernels for performance-critical regions of code.[1]

BIDMach is highly optimized for single node performance or performance on small clusters.[2] Once data doesn't fit easily in GPU memory (or can be batched in that way) the performance tends to fall off. Canny argues for hardware/software codesign and as such prefers machine configurations that are quite different than what we find in most commodity cluster nodes - e.g. 10 disk cahnnels and 4 GPUs.

In contrast, MLlib was designed for horizontal scalability on commodity clusters and works best on very big datasets - order of terabytes.

For the most part, these projects developed concurrently to address slightly different use cases. That said, there may be bits of BIDMach we could repurpose for MLlib - keep in mind we need to be careful about maintaining cross-language compatibility for our Java and Python-users, though.

- Evan

[1] - http://arxiv.org/abs/1409.5402
[2] - http://eecs.berkeley.edu/~hzhao/papers/BD.pdf

On Thu, Feb 5, 2015 at 1:00 PM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
Hi Evan,

Thank you for suggestion! BIDMat seems to have terrific speed. Do you know what makes them faster than netlib-java?

The same group has BIDMach library that implements machine learning. For some examples they use Caffe convolutional neural network library owned by another group in Berkeley. Could you elaborate on how these all might be connected with Spark Mllib? If you take BIDMat for linear algebra why don‚Äôt you take BIDMach for optimization and learning?

Best regards, Alexander

From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>]
Sent: Thursday, February 05, 2015 12:09 PM
To: Ulanov, Alexander
Cc: dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: Using CUDA within Spark / boosting linear algebra

I'd expect that we can make GPU-accelerated BLAS faster than CPU blas in many cases.

You might consider taking a look at the codepaths that BIDMat (https://github.com/BIDData/BIDMat) takes and comparing them to netlib-java/breeze. John Canny et. al. have done a bunch of work optimizing to make this work really fast from Scala. I've run it on my laptop and compared to MKL and in certain cases it's 10x faster at matrix multiply. There are a lot of layers of indirection here and you really want to avoid data copying as much as possible.

We could also consider swapping out BIDMat for Breeze, but that would be a big project and if we can figure out how to get breeze+cublas to comparable performance that would be a big win.

On Thu, Feb 5, 2015 at 11:55 AM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
Dear Spark developers,

I am exploring how to make linear algebra operations faster within Spark. One way of doing this is to use Scala Breeze library that is bundled with Spark. For matrix operations, it employs Netlib-java that has a Java wrapper for BLAS (basic linear algebra subprograms) and LAPACK native binaries if they are available on the worker node. It also has its own optimized Java implementation of BLAS. It is worth mentioning, that native binaries provide better performance only for BLAS level 3, i.e. matrix-matrix operations or general matrix multiplication (GEMM). This is confirmed by GEMM test on Netlib-java page https://github.com/fommil/netlib-java. I also confirmed it with my experiments with training of artificial neural network https://github.com/apache/spark/pull/1290#issuecomment-70313952. However, I would like to boost performance more.

GPU is supposed to work fast with linear algebra and there is Nvidia CUDA implementation of BLAS, called cublas. I have one Linux server with Nvidia GPU and I was able to do the following. I linked cublas (instead of cpu-based blas) with Netlib-java wrapper and put it into Spark, so Breeze/Netlib is using it. Then I did some performance measurements with regards to artificial neural network batch learning in Spark MLlib that involves matrix-matrix multiplications. It turns out that for matrices of size less than ~1000x780 GPU cublas has the same speed as CPU blas. Cublas becomes slower for bigger matrices. It worth mentioning that it is was not a test for ONLY multiplication since there are other operations involved. One of the reasons for slowdown might be the overhead of copying the matrices from computer memory to graphic card memory and back.

So, few questions:
1) Do these results with CUDA make sense?
2) If the problem is with copy overhead, are there any libraries that allow to force intermediate results to stay in graphic card memory thus removing the overhead?
3) Any other options to speed-up linear algebra in Spark?

Thank you, Alexander

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org>
For additional commands, e-mail: dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>


"
shane knapp <sknapp@berkeley.edu>,"Thu, 5 Feb 2015 17:01:46 -0800",spark 1.3 sbt build seems to be broken,dev <dev@spark.apache.org>,"https://amplab.cs.berkeley.edu/jenkins/job/Spark-1.3-SBT/

we're seeing java OOMs and heap space errors:
https://amplab.cs.berkeley.edu/jenkins/job/Spark-1.3-SBT/AMPLAB_JENKINS_BUILD_PROFILE=hadoop1.0,label=centos/19/console
https://amplab.cs.berkeley.edu/jenkins/job/Spark-1.3-SBT/AMPLAB_JENKINS_BUILD_PROFILE=hadoop1.0,label=centos/18/console

memory leak?  i checked the systems (ganglia + logging in and 'free -g')
and there's nothing going on there.

20 is building right now:
https://amplab.cs.berkeley.edu/jenkins/job/Spark-1.3-SBT/20/console
"
shane knapp <sknapp@berkeley.edu>,"Thu, 5 Feb 2015 17:07:34 -0800",Re: spark 1.3 sbt build seems to be broken,dev <dev@spark.apache.org>,"here's the hash of the breaking commit:

Started on Feb 5, 2015 12:01:01 PM
Using strategy: Default
[poll] Last Built Revision: Revision
de112a2096a2b84ce2cac112f12b50b5068d6c35
(refs/remotes/origin/branch-1.3)
 > git ls-remote -h https://github.com/apache/spark.git branch-1.3 # timeout=10
[poll] Latest remote head revision is: fba2dc663a644cfe76a744b5cace93e9d6646a25
Done. Took 2.5 sec
Changes found


from:  https://amplab.cs.berkeley.edu/jenkins/job/Spark-1.3-SBT/18/pollingLog/



"
Joseph Bradley <joseph@databricks.com>,"Thu, 5 Feb 2015 17:29:29 -0800",Re: Using CUDA within Spark / boosting linear algebra,"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Hi Alexander,

Using GPUs with Spark would be very exciting.  Small comment: Concerning
your question earlier about keeping data stored on the GPU rather than
having to move it between main memory and GPU memory on each iteration, I
would guess this would be critical to getting good performance.  If you
could do multiple local iterations before aggregating results, then the
cost of data movement to the GPU could be amortized (and I believe that is
done in practice).  Having Spark be aware of the GPU and using it as
another part of memory sounds like a much bigger undertaking.

Joseph


by John
ib.
n
ra, it involves
ny
t
m
w
y
‚Äôt
ng
d
a
le
e
a
s
t
"
Aniket Bhatnagar <aniket.bhatnagar@gmail.com>,"Fri, 06 Feb 2015 11:39:27 +0000",Data source API | sizeInBytes should be to *Scan,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Spark SQL committers

I have started experimenting with data sources API and I was wondering if
it makes sense to move the method sizeInBytes from BaseRelation to Scan
interfaces. This is because that a relation may be able to leverage filter
push down to estimate size potentially making a very large relation
broadcast-able. Thoughts?

Aniket
"
Sean Owen <sowen@cloudera.com>,"Fri, 6 Feb 2015 08:45:20 -0600",Improving metadata in Spark JIRA,dev <dev@spark.apache.org>,"I've wasted no time in wielding the commit bit to complete a number of
small, uncontroversial changes. I wouldn't commit anything that didn't
already appear to have review, consensus and little risk, but please
let me know if anything looked a little too bold, so I can calibrate.


Anyway, I'd like to continue some small house-cleaning by improving
the state of JIRA's metadata, in order to let it give us a little
clearer view on what's happening in the project:

a. Add Component to every (open) issue that's missing one
b. Review all Critical / Blocker issues to de-escalate ones that seem
obviously neither
c. Correct open issues that list a Fix version that has already been released
d. Close all issues Resolved for a release that has already been released

The problem with doing so is that it will create a tremendous amount
of email to the list, like, several hundred. It's possible to make
bulk changes and suppress e-mail though, which could be done for all
but b.

Better to suppress the emails when making such changes? or just not
bother on some of these?

---------------------------------------------------------------------


"
Sandy Ryza <sandy.ryza@cloudera.com>,"Fri, 6 Feb 2015 09:50:15 -0800",Re: Improving metadata in Spark JIRA,Sean Owen <sowen@cloudera.com>,"JIRA updates don't go to this list, they go to issues@spark.apache.org.  I
don't think many are signed up for that list, and those that are probably
have a flood of emails anyway.

So I'd definitely be in favor of any JIRA cleanup that you're up for.

-Sandy


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 06 Feb 2015 18:50:43 +0000",Re: Improving metadata in Spark JIRA,"Sandy Ryza <sandy.ryza@cloudera.com>, Sean Owen <sowen@cloudera.com>","+9000 on cleaning up JIRA.

Thank you Sean for laying out some specific things to tackle. I will assist
with this.

Regarding email, I think Sandy is right. I only get JIRA email for issues
I'm watching.

Nick


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 06 Feb 2015 19:53:12 +0000",Re: Improving metadata in Spark JIRA,"Sandy Ryza <sandy.ryza@cloudera.com>, Sean Owen <sowen@cloudera.com>","Do we need some new components to be added to the JIRA project?

Like:

   -

   scheduler
    -

   YARN
    - spark-submit
   - ‚Ä¶?

Nick
‚Äã


y
"
"""Hari Shreedharan"" <hshreedharan@cloudera.com>","Fri, 06 Feb 2015 11:56:41 -0800 (PST)",Re: Improving metadata in Spark JIRA,"""Nicholas Chammas"" <nicholas.chammas@gmail.com>","+1. Jira cleanup would be good. Please let me know if I can help in some way!




Thanks,¬†Hari


issues

probably
of
didn't

seem
not
"
Patrick Wendell <pwendell@gmail.com>,"Fri, 6 Feb 2015 12:25:51 -0800",Re: Improving metadata in Spark JIRA,Nicholas Chammas <nicholas.chammas@gmail.com>,"Per Nick's suggestion I added two components:

1. Spark Submit
2. Spark Scheduler

I figured I would just add these since if we decide later we don't
want them, we can simply merge them into Spark Core.


---------------------------------------------------------------------


"
Josh Rosen <rosenville@gmail.com>,"Sat, 7 Feb 2015 18:09:23 -0800",Re: Temporary jenkins issue,"""=?utf-8?Q?dev=40spark.apache.org?="" <dev@spark.apache.org>, Patrick
 Wendell <pwendell@gmail.com>","It looks like this may be fixed soon in Jenkins:

https://issues.jenkins-ci.org/browse/JENKINS-25446
https://github.com/jenkinsci/flaky-test-handler-plugin/pull/1


Hey All, 

I made a change to the Jenkins configuration that caused most builds 
to fail (attempting to enable a new plugin), I've reverted the change 
effective about 10 minutes ago. 

If you've seen recent build failures like below, this was caused by 
that change. Sorry about that. 

==== 
ERROR: Publisher 
com.google.jenkins.flakyTestHandler.plugin.JUnitFlakyResultArchiver 
aborted due to exception 
java.lang.NoSuchMethodError: 
hudson.model.AbstractBuild.getTestResultAction()Lhudson/tasks/test/AbstractTestResultAction; 
at com.google.jenkins.flakyTestHandler.plugin.FlakyTestResultAction.<init>(FlakyTestResultAction.java:78) 
at com.google.jenkins.flakyTestHandler.plugin.JUnitFlakyResultArchiver.perform(JUnitFlakyResultArchiver.java:89) 
at hudson.tasks.BuildStepMonitor$1.perform(BuildStepMonitor.java:20) 
at hudson.model.AbstractBuild$AbstractBuildExecution.perform(AbstractBuild.java:770) 
at hudson.model.AbstractBuild$AbstractBuildExecution.performAllBuildSteps(AbstractBuild.java:734) 
at hudson.model.Build$BuildExecution.post2(Build.java:183) 
at hudson.model.AbstractBuild$AbstractBuildExecution.post(AbstractBuild.java:683) 
at hudson.model.Run.execute(Run.java:1784) 
at hudson.matrix.MatrixRun.run(MatrixRun.java:146) 
at hudson.model.ResourceController.execute(ResourceController.java:89) 
at hudson.model.Executor.run(Executor.java:240) 
==== 

- Patrick 

--------------------------------------------------------------------- 

"
Patrick Wendell <pwendell@gmail.com>,"Fri, 6 Feb 2015 17:12:40 -0800",Re: [VOTE] Release Apache Spark 1.2.1 (RC3),Matei Zaharia <matei.zaharia@gmail.com>,"I'll add a +1 as well.


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Fri, 6 Feb 2015 15:20:03 -0800",Re: Data source API | sizeInBytes should be to *Scan,Aniket Bhatnagar <aniket.bhatnagar@gmail.com>,"We thought about this today after seeing this email. I actually built a
patch for this (adding filter/column to data source stat estimation), but
ultimately dropped it due to the potential problems the change the cause.

The main problem I see is that column pruning/predicate pushdowns are
advisory, i.e. the data source might or might not apply those filters.

Without significantly complicating the data source API, it is hard for the
optimizer (and future cardinality estimation) to know whether the
filter/column pushdowns are advisory, and whether to incorporate that in
cardinality estimation.

Imagine this scenario: a data source applies a filter and estimates the
filter's selectivity is 0.1, then the data set is reduced to 10% of the
size. Catalyst's own cardinality estimation estimates the filter
selectivity to 0.1 again, and thus the estimated data size is now 1% of the
original data size, lowering than some threshold. Catalyst decides to
broadcast the table. The actual table size is actually 10x the size.






"
Patrick Wendell <pwendell@gmail.com>,"Fri, 6 Feb 2015 17:14:39 -0800",[RESULT] [VOTE] Release Apache Spark 1.2.1 (RC3),Matei Zaharia <matei.zaharia@gmail.com>,"This vote passes with 5 +1 votes (3 binding) and no 0 or -1 votes.

+1 Votes:
Krishna Sankar
Sean Owen*
Chip Senkbeil
Matei Zaharia*
Patrick Wendell*

0 Votes:
(none)

-1 Votes:
(none)


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Sat, 7 Feb 2015 17:23:09 -0800",Re: Improving metadata in Spark JIRA,Nicholas Chammas <nicholas.chammas@gmail.com>,"I think we already have a YARN component.

https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20component%20%3D%20YARN

I don't think JIRA allows it to be mandatory, but if it does, that
would be useful.


---------------------------------------------------------------------


"
"""Evan R. Sparks"" <evan.sparks@gmail.com>","Fri, 6 Feb 2015 17:18:57 -0800",Re: Using CUDA within Spark / boosting linear algebra,"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Getting breeze to pick up the right blas library is critical for
performance. I recommend using OpenBLAS (or MKL, if you already have it).
It might make sense to force BIDMat to use the same underlying BLAS library
as well.


r
s
m>
by John
ib.
n
ra, it involves
ny
t
m
w
y
‚Äôt
ng
d
a
le
e
a
s
t
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sun, 08 Feb 2015 01:29:43 +0000",Re: Improving metadata in Spark JIRA,Patrick Wendell <pwendell@gmail.com>,"Oh derp, missed the YARN component.

JIRA, does allow admins to make fields mandatory:
https://confluence.atlassian.com/display/JIRA/Specifying+Field+Behavior#SpecifyingFieldBehavior-Makingafieldrequiredoroptional

Nick


"
"""Evan R. Sparks"" <evan.sparks@gmail.com>","Fri, 6 Feb 2015 17:29:09 -0800",Spark SQL Window Functions,"""dev@spark.apache.org"" <dev@spark.apache.org>","Currently there's no standard way of handling time series data in Spark. We
were kicking around some ideas in the lab today and one thing that came up
was SQL Window Functions as a way to support them and query over time
series (do things like moving average, etc.)

These don't seem to be implemented in Spark SQL yet, but there's some
discussion on JIRA (https://issues.apache.org/jira/browse/SPARK-3587)
asking for them, and there have also been a couple of pull requests -
https://github.com/apache/spark/pull/3703 and
https://github.com/apache/spark/pull/2953.

Is any work currently underway here?
"
Aniket Bhatnagar <aniket.bhatnagar@gmail.com>,"Sat, 07 Feb 2015 21:23:37 +0000",Re: Data source API | sizeInBytes should be to *Scan,Reynold Xin <rxin@databricks.com>,"Thanks for looking into this. If this true, isn't this an issue today? The
default implementation of sizeInBytes is 1 + broadcast threshold. So, if
catalyst's cardinality estimation estimates even a small filter
selectivity, it will result in broadcasting the relation. Therefore,
shouldn't the default be much higher than broadcast threshold?

Also, since the default implementation of sizeInBytes already exists in
BaseRelation, I am not sure why the same/similar default implementation
can't be provided with in *Scan specific sizeInBytes functions and have
Catalyst always trust the size returned by DataSourceAPI (with default
implementation being to never broadcast). Another thing that could be done
is have sizeInBytes return Option[Long] so that Catalyst explicitly knows
when DataSource was able to optimize the size. The reason why I would push
for sizeInBytes in *Scan interfaces is because at times the data source
implementation can more accurately predict the size output. For example,
DataSource implementations for MongoDB, ElasticSearch, Cassandra, etc can
easy use filter push downs to query the underlying storage to predict the
size. Such predictions will be more accurate than Catalyst's prediction.
Therefore, if its not a fundamental change in Catalyst, I would think this
makes sense.


Thanks,
Aniket



"
fommil <sam.halliday@gmail.com>,"Fri, 6 Feb 2015 18:37:05 -0700 (MST)",Pull Requests on github,dev@spark.apache.org,"Hi all,

I'm the author of netlib-java and I noticed that the documentation in MLlib
was out of date and misleading, so I submitted a pull request on github
which will hopefully make things easier for everybody to understand the
benefits of system optimised natives and how to use them :-)

  https://github.com/apache/spark/pull/4448

However, it looks like there are a *lot* of outstanding PRs and that this is
just a mirror repository.

Will somebody please look at my PR and merge into the canonical source (and
let me know)?

Best regards,
Sam



--

---------------------------------------------------------------------


"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Sat, 7 Feb 2015 01:43:41 +0000",RE: Using CUDA within Spark / boosting linear algebra,"""Evan R. Sparks"" <evan.sparks@gmail.com>","Evan, could you elaborate on how to force BIDMat and netlib-java to force loading the right blas? For netlib, I there are few JVM flags, such as -Dcom.github.fommil.netlib.BLAS=com.github.fommil.netlib.F2jBLAS, so I can force it to use Java implementation. Not sure I understand how to force use a specific blas (not specific wrapper for blas).

Btw. I have installed openblas (yum install openblas), so I suppose that netlib is using it.

From: Evan R. Sparks [mailto:evan.sparks@gmail.com]
Sent: Friday, February 06, 2015 5:19 PM
To: Ulanov, Alexander
Cc: Joseph Bradley; dev@spark.apache.org
Subject: Re: Using CUDA within Spark / boosting linear algebra

Getting breeze to pick up the right blas library is critical for performance. I recommend using OpenBLAS (or MKL, if you already have it). It might make sense to force BIDMat to use the same underlying BLAS library as well.

On Fri, Feb 6, 2015 at 4:42 PM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
Hi Evan, Joseph

I did few matrix multiplication test and BIDMat seems to be ~10x faster than netlib-java+breeze (sorry for weird table formatting):

|A*B  size | BIDMat MKL | Breeze+Netlib-java native_system_linux_x86-64| Breeze+Netlib-java f2jblas |
+-----------------------------------------------------------------------+
|100x100*100x100 | 0,00205596 | 0,03810324 | 0,002556 |
|1000x1000*1000x1000 | 0,018320947 | 0,51803557 |1,638475459 |
|10000x10000*10000x10000 | 23,78046632 | 445,0935211 | 1569,233228 |

Configuration: Intel(R) Xeon(R) CPU E31240 3.3 GHz, 6GB RAM, Fedora 19 Linux, Scala 2.11.

Later I will make tests with Cuda. I need to install new Cuda version for this purpose.

Do you have any ideas why breeze-netlib with native blas is so much slower than BIDMat MKL?

Best regards, Alexander

From: Joseph Bradley [mailto:joseph@databricks.com<mailto:joseph@databricks.com>]
Sent: Thursday, February 05, 2015 5:29 PM
To: Ulanov, Alexander
Cc: Evan R. Sparks; dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: Using CUDA within Spark / boosting linear algebra

Hi Alexander,

Using GPUs with Spark would be very exciting.  Small comment: Concerning your question earlier about keeping data stored on the GPU rather than having to move it between main memory and GPU memory on each iteration, I would guess this would be critical to getting good performance.  If you could do multiple local iterations before aggregating results, then the cost of data movement to the GPU could be amortized (and I believe that is done in practice).  Having Spark be aware of the GPU and using it as another part of memory sounds like a much bigger undertaking.

Joseph

On Thu, Feb 5, 2015 at 4:59 PM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
Thank you for explanation! I‚Äôve watched the BIDMach presentation by John Canny and I am really inspired by his talk and comparisons with Spark MLlib.

I am very interested to find out what will be better within Spark: BIDMat or netlib-java with CPU or GPU natives. Could you suggest a fair way to benchmark them? Currently I do benchmarks on artificial neural networks in batch mode. While it is not a ‚Äúpure‚Äù test of linear algebra, it involves some other things that are essential to machine learning.

From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>]
Sent: Thursday, February 05, 2015 1:29 PM
To: Ulanov, Alexander
Cc: dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: Using CUDA within Spark / boosting linear algebra

I'd be surprised of BIDMat+OpenBLAS was significantly faster than netlib-java+OpenBLAS, but if it is much faster it's probably due to data layout and fewer levels of indirection - it's definitely a worthwhile experiment to run. The main speedups I've seen from using it come from highly optimized GPU code for linear algebra. I know that in the past Canny has gone as far as to write custom GPU kernels for performance-critical regions of code.[1]

BIDMach is highly optimized for single node performance or performance on small clusters.[2] Once data doesn't fit easily in GPU memory (or can be batched in that way) the performance tends to fall off. Canny argues for hardware/software codesign and as such prefers machine configurations that are quite different than what we find in most commodity cluster nodes - e.g. 10 disk cahnnels and 4 GPUs.

In contrast, MLlib was designed for horizontal scalability on commodity clusters and works best on very big datasets - order of terabytes.

For the most part, these projects developed concurrently to address slightly different use cases. That said, there may be bits of BIDMach we could repurpose for MLlib - keep in mind we need to be careful about maintaining cross-language compatibility for our Java and Python-users, though.

- Evan

[1] - http://arxiv.org/abs/1409.5402
[2] - http://eecs.berkeley.edu/~hzhao/papers/BD.pdf

On Thu, Feb 5, 2015 at 1:00 PM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
Hi Evan,

Thank you for suggestion! BIDMat seems to have terrific speed. Do you know what makes them faster than netlib-java?

The same group has BIDMach library that implements machine learning. For some examples they use Caffe convolutional neural network library owned by another group in Berkeley. Could you elaborate on how these all might be connected with Spark Mllib? If you take BIDMat for linear algebra why don‚Äôt you take BIDMach for optimization and learning?

Best regards, Alexander

From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
Sent: Thursday, February 05, 2015 12:09 PM
To: Ulanov, Alexander
Cc: dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.apache.org<mailto:dev@spark.apache.org>>
Subject: Re: Using CUDA within Spark / boosting linear algebra

I'd expect that we can make GPU-accelerated BLAS faster than CPU blas in many cases.

You might consider taking a look at the codepaths that BIDMat (https://github.com/BIDData/BIDMat) takes and comparing them to netlib-java/breeze. John Canny et. al. have done a bunch of work optimizing to make this work really fast from Scala. I've run it on my laptop and compared to MKL and in certain cases it's 10x faster at matrix multiply. There are a lot of layers of indirection here and you really want to avoid data copying as much as possible.

We could also consider swapping out BIDMat for Breeze, but that would be a big project and if we can figure out how to get breeze+cublas to comparable performance that would be a big win.

On Thu, Feb 5, 2015 at 11:55 AM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
Dear Spark developers,

I am exploring how to make linear algebra operations faster within Spark. One way of doing this is to use Scala Breeze library that is bundled with Spark. For matrix operations, it employs Netlib-java that has a Java wrapper for BLAS (basic linear algebra subprograms) and LAPACK native binaries if they are available on the worker node. It also has its own optimized Java implementation of BLAS. It is worth mentioning, that native binaries provide better performance only for BLAS level 3, i.e. matrix-matrix operations or general matrix multiplication (GEMM). This is confirmed by GEMM test on Netlib-java page https://github.com/fommil/netlib-java. I also confirmed it with my experiments with training of artificial neural network https://github.com/apache/spark/pull/1290#issuecomment-70313952. However, I would like to boost performance more.

GPU is supposed to work fast with linear algebra and there is Nvidia CUDA implementation of BLAS, called cublas. I have one Linux server with Nvidia GPU and I was able to do the following. I linked cublas (instead of cpu-based blas) with Netlib-java wrapper and put it into Spark, so Breeze/Netlib is using it. Then I did some performance measurements with regards to artificial neural network batch learning in Spark MLlib that involves matrix-matrix multiplications. It turns out that for matrices of size less than ~1000x780 GPU cublas has the same speed as CPU blas. Cublas becomes slower for bigger matrices. It worth mentioning that it is was not a test for ONLY multiplication since there are other operations involved. One of the reasons for slowdown might be the overhead of copying the matrices from computer memory to graphic card memory and back.

So, few questions:
1) Do these results with CUDA make sense?
2) If the problem is with copy overhead, are there any libraries that allow to force intermediate results to stay in graphic card memory thus removing the overhead?
3) Any other options to speed-up linear algebra in Spark?

Thank you, Alexander

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org><mailto:dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org>>
For additional commands, e-mail: dev-help@spark.apache.org<mailto:dev-help@spark.apache.org><mailto:dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>>


"
"""Evan R. Sparks"" <evan.sparks@gmail.com>","Fri, 6 Feb 2015 17:58:25 -0800",Re: Using CUDA within Spark / boosting linear algebra,"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","I would build OpenBLAS yourself, since good BLAS performance comes from
getting cache sizes, etc. set up correctly for your particular hardware -
this is often a very tricky process (see, e.g. ATLAS), but we found that on
relatively modern Xeon chips, OpenBLAS builds quickly and yields
performance competitive with MKL.

To make sure the right library is getting used, you have to make sure it's
first on the search path - export LD_LIBRARY_PATH=/path/to/blas/library.so
will do the trick here.

For some examples of getting netlib-java setup on an ec2 node and some
example benchmarking code we ran a while back, see:
https://github.com/shivaram/matrix-bench

In particular - build-openblas-ec2.sh shows you how to build the library
and set up symlinks correctly, and scala/run-netlib.sh shows you how to get
the path setup and get that library picked up by netlib-java.

In this way - you could probably get cuBLAS set up to be used by
netlib-java as well.

- Evan


 I
ce
ry
m>
r
s
m>
by John
ib.
n
ra, it involves
ny
t
m
w
y
‚Äôt
ng
d
a
le
e
a
s
t
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sun, 08 Feb 2015 01:08:29 +0000",Re: Improving metadata in Spark JIRA,Patrick Wendell <pwendell@gmail.com>,"By the way, isn't it possible to make the ""Component"" field mandatory when
people open new issues? Shouldn't we do that?

Btw Patrick, don't we need a YARN component? I think our JIRA components
should roughly match the components on the PR dashboard
<https://spark-prs.appspot.com/>.

Nick


"
Matei Zaharia <matei.zaharia@gmail.com>,"Fri, 6 Feb 2015 14:38:39 -0800",Re: [VOTE] Release Apache Spark 1.2.1 (RC3),Patrick Wendell <pwendell@gmail.com>,"+1

Tested on Mac OS X.

Matei


version 1.2.1!
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=b6eaf77d4332bfb0a698849b1f5f917d20d70e97
at:
https://repository.apache.org/content/repositories/orgapachespark-1065/


-------------------------"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Sat, 7 Feb 2015 00:42:23 +0000",RE: Using CUDA within Spark / boosting linear algebra,Joseph Bradley <joseph@databricks.com>,"Hi Evan, Joseph

I did few matrix multiplication test and BIDMat seems to be ~10x faster than netlib-java+breeze (sorry for weird table formatting):

|A*B  size | BIDMat MKL | Breeze+Netlib-java native_system_linux_x86-64| Breeze+Netlib-java f2jblas | 
+-----------------------------------------------------------------------+
|100x100*100x100 | 0,00205596 | 0,03810324 | 0,002556 |
|1000x1000*1000x1000 | 0,018320947 | 0,51803557 |1,638475459 |
|10000x10000*10000x10000 | 23,78046632 | 445,0935211 | 1569,233228 |

Configuration: Intel(R) Xeon(R) CPU E31240 3.3 GHz, 6GB RAM, Fedora 19 Linux, Scala 2.11.

Later I will make tests with Cuda. I need to install new Cuda version for this purpose. 

Do you have any ideas why breeze-netlib with native blas is so much slower than BIDMat MKL?

Best regards, Alexander

From: Joseph Bradley [mailto:joseph@databricks.com] 
Sent: Thursday, February 05, 2015 5:29 PM
To: Ulanov, Alexander
Cc: Evan R. Sparks; dev@spark.apache.org
Subject: Re: Using CUDA within Spark / boosting linear algebra

Hi Alexander,

Using GPUs with Spark would be very exciting.¬† Small comment: Concerning your question earlier about keeping data stored on the GPU rather than having to move it between main memory and GPU memory on each iteration, I would guess this would be critical to getting good performance.¬† If you could do multiple local iterations before aggregating results, then the cost of data movement to the GPU could be amortized (and I believe that is done in practice).¬† Having Spark be aware of the GPU and using it as another part of memory sounds like a much bigger undertaking.

Joseph

On Thu, Feb 5, 2015 at 4:59 PM, Ulanov, Alexander <alexander.ulanov@hp.com> wrote:
Thank you for explanation! I‚Äôve watched the BIDMach presentation by John Canny and I am really inspired by his talk and comparisons with Spark MLlib.

I am very interested to find out what will be better within Spark: BIDMat or netlib-java with CPU or GPU natives. Could you suggest a fair way to benchmark them? Currently I do benchmarks on artificial neural networks in batch mode. While it is not a ‚Äúpure‚Äù test of linear algebra, it involves some other things that are essential to machine learning.

From: Evan R. Sparks [mailto:evan.sparks@gmail.com]
Sent: Thursday, February 05, 2015 1:29 PM
To: Ulanov, Alexander
Cc: dev@spark.apache.org
Subject: Re: Using CUDA within Spark / boosting linear algebra

I'd be surprised of BIDMat+OpenBLAS was significantly faster than netlib-java+OpenBLAS, but if it is much faster it's probably due to data layout and fewer levels of indirection - it's definitely a worthwhile experiment to run. The main speedups I've seen from using it come from highly optimized GPU code for linear algebra. I know that in the past Canny has gone as far as to write custom GPU kernels for performance-critical regions of code.[1]

BIDMach is highly optimized for single node performance or performance on small clusters.[2] Once data doesn't fit easily in GPU memory (or can be batched in that way) the performance tends to fall off. Canny argues for hardware/software codesign and as such prefers machine configurations that are quite different than what we find in most commodity cluster nodes - e.g. 10 disk cahnnels and 4 GPUs.

In contrast, MLlib was designed for horizontal scalability on commodity clusters and works best on very big datasets - order of terabytes.

For the most part, these projects developed concurrently to address slightly different use cases. That said, there may be bits of BIDMach we could repurpose for MLlib - keep in mind we need to be careful about maintaining cross-language compatibility for our Java and Python-users, though.

- Evan

[1] - http://arxiv.org/abs/1409.5402
[2] - http://eecs.berkeley.edu/~hzhao/papers/BD.pdf

On Thu, Feb 5, 2015 at 1:00 PM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
Hi Evan,

Thank you for suggestion! BIDMat seems to have terrific speed. Do you know what makes them faster than netlib-java?

The same group has BIDMach library that implements machine learning. For some examples they use Caffe convolutional neural network library owned by another group in Berkeley. Could you elaborate on how these all might be connected with Spark Mllib? If you take BIDMat for linear algebra why don‚Äôt you take BIDMach for optimization and learning?

Best regards, Alexander

From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>]
Sent: Thursday, February 05, 2015 12:09 PM
To: Ulanov, Alexander
Cc: dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: Using CUDA within Spark / boosting linear algebra

I'd expect that we can make GPU-accelerated BLAS faster than CPU blas in many cases.

You might consider taking a look at the codepaths that BIDMat (https://github.com/BIDData/BIDMat) takes and comparing them to netlib-java/breeze. John Canny et. al. have done a bunch of work optimizing to make this work really fast from Scala. I've run it on my laptop and compared to MKL and in certain cases it's 10x faster at matrix multiply. There are a lot of layers of indirection here and you really want to avoid data copying as much as possible.

We could also consider swapping out BIDMat for Breeze, but that would be a big project and if we can figure out how to get breeze+cublas to comparable performance that would be a big win.

On Thu, Feb 5, 2015 at 11:55 AM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
Dear Spark developers,

I am exploring how to make linear algebra operations faster within Spark. One way of doing this is to use Scala Breeze library that is bundled with Spark. For matrix operations, it employs Netlib-java that has a Java wrapper for BLAS (basic linear algebra subprograms) and LAPACK native binaries if they are available on the worker node. It also has its own optimized Java implementation of BLAS. It is worth mentioning, that native binaries provide better performance only for BLAS level 3, i.e. matrix-matrix operations or general matrix multiplication (GEMM). This is confirmed by GEMM test on Netlib-java page https://github.com/fommil/netlib-java. I also confirmed it with my experiments with training of artificial neural network https://github.com/apache/spark/pull/1290#issuecomment-70313952. However, I would like to boost performance more.

GPU is supposed to work fast with linear algebra and there is Nvidia CUDA implementation of BLAS, called cublas. I have one Linux server with Nvidia GPU and I was able to do the following. I linked cublas (instead of cpu-based blas) with Netlib-java wrapper and put it into Spark, so Breeze/Netlib is using it. Then I did some performance measurements with regards to artificial neural network batch learning in Spark MLlib that involves matrix-matrix multiplications. It turns out that for matrices of size less than ~1000x780 GPU cublas has the same speed as CPU blas. Cublas becomes slower for bigger matrices. It worth mentioning that it is was not a test for ONLY multiplication since there are other operations involved. One of the reasons for slowdown might be the overhead of copying the matrices from computer memory to graphic card memory and back.

So, few questions:
1) Do these results with CUDA make sense?
2) If the problem is with copy overhead, are there any libraries that allow to force intermediate results to stay in graphic card memory thus removing the overhead?
3) Any other options to speed-up linear algebra in Spark?

Thank you, Alexander

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org>
For additional commands, e-mail: dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>


"
WangTaoTheTonic <barneystinson@aliyun.com>,"Fri, 6 Feb 2015 23:23:23 -0700 (MST)",Re: [VOTE] Release Apache Spark 1.2.1 (RC3),dev@spark.apache.org,"Should we merge this commit into branch1.2 too?

https://github.com/apache/spark/commit/2483c1efb6429a7d8a20c96d18ce2fec93a1aff9



--

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Fri, 6 Feb 2015 12:55:11 -0800",Unit tests,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey All,

The tests are in a not-amazing state right now due to a few compounding factors:

1. We've merged a large volume of patches recently.
2. The load on jenkins has been relatively high, exposing races and
other behavior not seen at lower load.

For those not familiar, the main issue is flaky (non deterministic)
test failures. Right now I'm trying to prioritize keeping the
PullReqeustBuilder in good shape since it will block development if it
is down.

For other tests, let's try to keep filing JIRA's when we see issues
and use the flaky-test label (see http://bit.ly/1yRif9S):

I may contact people regarding specific tests. This is a very high
priority to get in good shape. This kind of thing is no one's ""fault""
but just the result of a lot of concurrent development, and everyone
needs to pitch in to get back in a good place.

- Patrick

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sun, 08 Feb 2015 22:42:51 +0000",Re: Using CUDA within Spark / boosting linear algebra,"""Ulanov, Alexander"" <alexander.ulanov@hp.com>, Joseph Bradley <joseph@databricks.com>","Lemme butt in randomly here and say there is an interesting discussion on
this Spark PR <https://github.com/apache/spark/pull/4448> about
netlib-java, JBLAS, Breeze, and other things I know nothing of, that y'all
may find interesting. Among the participants is the author of netlib-java.


r
s
m>
by John
ib.
n
ra, it involves
ny
t
m
w
y
‚Äôt
ng
d
a
le
e
a
s
t
"
"""Likun (Jacky)"" <jacky.likun@huawei.com>","Mon, 9 Feb 2015 00:44:40 +0000",Re: Welcoming three new committers,Matei Zaharia <matei.zaharia@gmail.com>,"Congratulations guys! Keep helping this awesome community.

BR,
Jacky Li

- ∑¢◊‘ Smartisan T1 -

2015ƒÍ2‘¬4»’£¨…œŒÁ6:36”⁄ Matei Zaharia <matei.zaharia@gmail.com> –¥µ¿£∫

Hi all,

The PMC recently voted to add three new committers: Cheng Lian, Joseph Bradley and Sean Owen. All three have been major contributors to Spark in the past year: Cheng on Spark SQL, Joseph on MLlib, and Sean on ML and many pieces throughout Spark Core. Join me in welcoming them as committers!

Matei
---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org

"
Reynold Xin <rxin@databricks.com>,"Sun, 8 Feb 2015 21:56:35 -0800",Re: Spark SQL Window Functions,"""Evan R. Sparks"" <evan.sparks@gmail.com>","This is the original ticket:
https://issues.apache.org/jira/browse/SPARK-1442

I believe it will happen, one way or another :)



"
Akhil Das <akhil@sigmoidanalytics.com>,"Mon, 9 Feb 2015 11:34:06 +0530",Re: Pull Requests on github,fommil <sam.halliday@gmail.com>,"You can open a Jira issue pointing this PR to get it processed faster. :)

Thanks
Best Regards


"
Sean Owen <sowen@cloudera.com>,"Mon, 9 Feb 2015 10:41:54 +0000",Keep or remove Debian packaging in Spark?,dev <dev@spark.apache.org>,"This is a straw poll to assess whether there is support to keep and
fix, or remove, the Debian packaging-related config in Spark.

I see several oldish outstanding JIRAs relating to problems in the packaging:

https://issues.apache.org/jira/browse/SPARK-1799
https://issues.apache.org/jira/browse/SPARK-2614
https://issues.apache.org/jira/browse/SPARK-3624
https://issues.apache.org/jira/browse/SPARK-4436
(and a similar idea about making RPMs)
https://issues.apache.org/jira/browse/SPARK-665

The original motivation seems related to Chef:

https://issues.apache.org/jira/browse/SPARK-2614?focusedCommentId=14070908&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14070908

Mark's recent comments cast some doubt on whether it is essential:

https://github.com/apache/spark/pull/4277#issuecomment-72114226

and in recent conversations I didn't hear dissent to the idea of removing this.

Is this still useful enough to fix up? All else equal I'd like to
start to walk back some of the complexity of the build, but I don't
know how all-else-equal it is. Certainly, it sounds like nobody
intends these to be used to actually deploy Spark.

I don't doubt it's useful to someone, but can they maintain the
packaging logic elsewhere?

---------------------------------------------------------------------


"
Gil Vernik <GILV@il.ibm.com>,"Mon, 9 Feb 2015 13:22:31 +0200","Re: run time exceptions in Spark 1.2.0 manual build together with OpenStack
 hadoop driver",dev <dev@spark.apache.org>,"Hi All,

I understand that https://github.com/apache/spark/pull/3938 was closed and 
merged into Spark? And this suppose to fix this Jackson issue.
If so, is there any way to update binary distributions of Spark so that it 
will contain this fix? Current binary versions of Spark available for 
download were built with jackson 1.8.8 which makes them  impossible to use 
with Hadoop 2.6.0 jars

Thanks
Gil Vernik.






From:   Sean Owen <sowen@cloudera.com>
To:     Ted Yu <yuzhihong@gmail.com>
Cc:     Gil Vernik/Haifa/IBM@IBMIL, dev <dev@spark.apache.org>
Date:   18/01/2015 08:23 PM
Subject:        Re: run time exceptions in Spark 1.2.0 manual build 
together with OpenStack hadoop driver



Agree, I think this can / should be fixed with a slightly more
conservative version of https://github.com/apache/spark/pull/3938
related to SPARK-5108.

to
org.codehaus.jackson.map.introspect.JacksonAnnotationIntrospector.findDeserializationType(JacksonAnnotationIntrospector.java:524)
org.codehaus.jackson.map.deser.BasicDeserializerFactory.modifyTypeByAnnotation(BasicDeserializerFactory.java:732)
to
-->
be
-->
1.9.13,

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Mon, 9 Feb 2015 11:28:55 +0000","Re: run time exceptions in Spark 1.2.0 manual build together with
 OpenStack hadoop driver",Gil Vernik <GILV@il.ibm.com>,"Old releases can't be changed, but new ones can. This was merged into
the 1.3 branch for the upcoming 1.3.0 release.

If you really had to, you could do some surgery on existing
distributions to swap in/out Jackson.


---------------------------------------------------------------------


"
=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"Mon, 9 Feb 2015 14:46:27 +0100",Re: Unit tests,Patrick Wendell <pwendell@gmail.com>,"Hi Patrick,

Thanks for the heads up. I was trying to set up our own infrastructure for
testing Spark (essentially, running `run-tests` every night) on EC2. I
stumbled upon a number of flaky tests, but none of them look similar to
anything in Jira with the flaky-test tag. I wonder if there's something
wrong with our infrastructure, or I should simply open Jira tickets with
the failures I find. For example, one that appears fairly often on our
setup is in AkkaUtilsSuite ""remote fetch ssl on - untrusted server""
(exception `ActorNotFound`, instead of `TimeoutException`).

thanks,
iulian





-- 

--
Iulian Dragos

------
Reactive Apps on the JVM
www.typesafe.com
"
Mark Hamstra <mark@clearstorydata.com>,"Mon, 9 Feb 2015 07:31:09 -0800",Re: Keep or remove Debian packaging in Spark?,Sean Owen <sowen@cloudera.com>,"

I wouldn't go quite that far.  What we have now can serve as useful input
to a deployment tool like Chef, but the user is then going to need to add
some customization or configuration within the context of that tooling to
get Spark installed just the way they want.  So it is not so much that the
current Debian packaging can't be used as that it has never really been
intended to be a completely finished product that a newcomer could, for
example, use to install Spark completely and quickly to Ubuntu and have a
fully-functional environment in which they could then run all of the
examples, tutorials, etc.

Getting to that level of packaging (and maintenance) is something that I'm
not sure we want to do since that is a better fit with Bigtop and the
efforts of Cloudera, Horton Works, MapR, etc. to distribute Spark.


"
Patrick Wendell <pwendell@gmail.com>,"Mon, 9 Feb 2015 10:08:00 -0800",Re: Keep or remove Debian packaging in Spark?,Mark Hamstra <mark@clearstorydata.com>,"I have wondered whether we should sort of deprecated it more
officially, since otherwise I think people have the reasonable
expectation based on the current code that Spark intends to support
""complete"" Debian packaging as part of the upstream build. Having
something that's sort-of maintained but no one is helping review and
merge patches on it or make it fully functional, IMO that doesn't
benefit us or our users. There are a bunch of other projects that are
specifically devoted to packaging, so it seems like there is a clear
separation of concerns here.


---------------------------------------------------------------------


"
Josh Rosen <rosenville@gmail.com>,"Mon, 9 Feb 2015 10:45:16 -0800",Re: Unit tests,"=?utf-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>, 
 Patrick Wendell <pwendell@gmail.com>","Hi Iulian,

I think the AkakUtilsSuite failure that you observed has been fixed in¬†https://issues.apache.org/jira/browse/SPARK-5548¬†/¬†https://github.com/apache/spark/pull/4343

Hi Patrick,  

Thanks for the heads up. I was trying to set up our own infrastructure for  
testing Spark (essentially, running `run-tests` every night) on EC2. I  
stumbled upon a number of flaky tests, but none of them look similar to  
anything in Jira with the flaky-test tag. I wonder if there's something  
wrong with our infrastructure, or I should simply open Jira tickets with  
the failures I find. For example, one that appears fairly often on our  
setup is in AkkaUtilsSuite ""remote fetch ssl on - untrusted server""  
(exception `ActorNotFound`, instead of `TimeoutException`).  

thanks,  
iulian  



  
 
  


--  

--  
Iulian Dragos  

------  
Reactive Apps on the JVM  
www.typesafe.com  
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 09 Feb 2015 18:47:58 +0000",Re: Keep or remove Debian packaging in Spark?,"Patrick Wendell <pwendell@gmail.com>, Mark Hamstra <mark@clearstorydata.com>","+1 to an ""official"" deprecation + redirecting users to some other project
that will or already is taking this on.

Nate?



"
Xiangrui Meng <mengxr@gmail.com>,"Mon, 9 Feb 2015 12:18:08 -0800",Re: multi-line comment style,Reynold Xin <rxin@databricks.com>,"I like the `/* .. */` style more. Because it is easier for IDEs to
recognize it as a block comment. If you press enter in the comment
block with the `//` style, IDEs won't add `//` for you. -Xiangrui


---------------------------------------------------------------------


"
Xiangrui Meng <mengxr@gmail.com>,"Mon, 9 Feb 2015 12:25:28 -0800",Re: multi-line comment style,Reynold Xin <rxin@databricks.com>,"Btw, I think allowing `/* ... */` without the leading `*` in lines is
also useful. Check this line:
https://github.com/apache/spark/pull/4259/files#diff-e9dcb3b5f3de77fc31b3aff7831110eaR55,
where we put the R commands that can reproduce the test result. It is
easier if we write in the following style:

~~~
/*
 Using the following R code to load the data and train the model using
glmnet package.

 library(""glmnet"")
 data <- read.csv(""path"", header=FALSE, stringsAsFactors=FALSE)
 features <- as.matrix(data.frame(as.numeric(data$V2), as.numeric(data$V3)))
 label <- as.numeric(data$V1)
 weights <- coef(glmnet(features, label, family=""gaussian"", alpha = 0,
lambda = 0))
 */
~~~

So people can copy & paste the R commands directly.

Xiangrui


---------------------------------------------------------------------


"
mkhaitman <mark.khaitman@chango.com>,"Mon, 9 Feb 2015 14:26:48 -0700 (MST)",pyspark.daemon issues?,dev@spark.apache.org,"I've noticed a couple oddities with the pyspark.daemons which are causing us
a bit of memory problems within some of our heavy spark jobs, especially
when they run at the same time...

It seems that there is typically a 1-to-1 ratio of pyspark.daemons to cores
per executor during aggregations. By default the spark.python.worker.memory
is left at the default of 512MB, after which, the remainder of the
aggregations are supposed to spill to disk. 

However:
          *1)* I'm not entirely sure what cases would result in random
numbers of pyspark daemons which do not respect the python worker memory
limit. I've seen some go up to as far as 2GB each (well over the 512MB
limit) which is when we run into some crazy memory problems for jobs making
use of many cores on each executor. To be clear here, they ARE spilling to
disk as well, but also blowing past the memory limits at the same time
somehow. 

          *2)* Another scenario specifically relates to when we want to join
RDDs, where for example, say there are 4 cores per executor, and therefore 4
pyspark daemons during most aggregations. It seems that if a Join occurs, it
will spawn up 4 additional pyspark daemons as opposed to simply re-using the
ones that were already present during the aggregation stage that occurred
before it. This, combined with the case where the python worker memory limit
is not strictly respected, can pose problems for using way more memory per
node. 

The fact that the python worker memory appears to use memory *outside* of
the executor memory is what poses the biggest challenge for preventing
memory depletion on a node. Is there something obvious, or some environment
variable I may have missed that could potentially help with one/both of the
above memory concerns? Alternatively, any suggestions would be greatly
appreciated! :)

Thanks,
Mark.






--

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Mon, 9 Feb 2015 13:31:05 -0800",[ANNOUNCE] Apache Spark 1.2.1 Released,"""dev@spark.apache.org"" <dev@spark.apache.org>, ""user@spark.apache.org"" <user@spark.apache.org>","Hi All,

I've just posted the 1.2.1 maintenance release of Apache Spark. We
recommend all 1.2.0 users upgrade to this release, as this release
includes stability fixes across all components of Spark.

- Download this release: http://spark.apache.org/downloads.html
- View the release notes:
http://spark.apache.org/releases/spark-release-1-2-1.html
- Full list of JIRA issues resolved in this release: http://s.apache.org/Mpn

Thanks to everyone who helped work on this release!

- Patrick

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Mon, 9 Feb 2015 13:33:48 -0800",Re: multi-line comment style,Xiangrui Meng <mengxr@gmail.com>,"Clearly there isn't a strictly optimal commenting format (pro's and
cons for both '//' and '/*'). My thought is for consistency we should
just chose one and put in the style guide.


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Mon, 9 Feb 2015 13:36:42 -0800",Re: multi-line comment style,Patrick Wendell <pwendell@gmail.com>,"Why don't we just pick // as the default (by encouraging it in the style
guide), since it is mostly used, and then do not disallow /* */? I don't
think it is that big of a deal to have slightly deviations here since it is
dead simple to understand what's going on.



"
Andrew Or <andrew@databricks.com>,"Mon, 9 Feb 2015 13:48:44 -0800",Re: multi-line comment style,Reynold Xin <rxin@databricks.com>,"In my experience I find it much more natural to use // for short multi-line
comments (2 or 3 lines), and /* */ for long multi-line comments involving
one or more paragraphs. For short multi-line comments, there is no reason
not to use // if it just so happens that your first line exceeded 100
characters and you have to wrap it. For long multi-line comments, however,
using // all the way looks really awkward especially if you have multiple
paragraphs.

Thus, I would actually suggest that we don't try to pick a favorite and
document that both are acceptable. I don't expect developers to follow my
exact usage (i.e. with a tipping point of 2-3 lines) so I wouldn't enforce
anything specific either.

2015-02-09 13:36 GMT-08:00 Reynold Xin <rxin@databricks.com>:

"
Sandy Ryza <sandy.ryza@cloudera.com>,"Mon, 9 Feb 2015 14:22:47 -0800",Re: multi-line comment style,Andrew Or <andrew@databricks.com>,"+1 to what Andrew said, I think both make sense in different situations and
trusting developer discretion here is reasonable.


"
Sean Owen <sowen@cloudera.com>,"Mon, 9 Feb 2015 23:51:30 +0000",Re: Keep or remove Debian packaging in Spark?,Nicholas Chammas <nicholas.chammas@gmail.com>,"What about this straw man proposal: deprecate in 1.3 with some kind of
message in the build, and remove for 1.4? And add a pointer to any
third-party packaging that might provide similar functionality?


---------------------------------------------------------------------


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Mon, 9 Feb 2015 16:00:13 -0800",Re: spark-ec2 licensing clarification,Florian Verhein <florian@arkig.com>,"+spark dev list

Yes, we should add an Apache license to it -- Feel free to open a PR for
it. BTW though it is a part of the mesos github account, it is almost
exclusively used by the Spark Project AFAIK.

Longer term it may make sense to move it to a mor"
<nate@reactor8.com>,"Mon, 9 Feb 2015 16:10:50 -0800",RE: Keep or remove Debian packaging in Spark?,"""'Sean Owen'"" <sowen@cloudera.com>,
	""'Nicholas Chammas'"" <nicholas.chammas@gmail.com>","This could be something if the spark community wanted to not maintain debs/rpms directly via the project could direct interested efforts towards apache bigtop.  Right now debs/rpms of bigtop components, as well as related tests is a focus.

Something that would be great is if at least one spark committer with interests in config/pkg/testing could be liason and pt for bigtop efforts.

Right now focus on bigtop 0.9, which currently includes spark 1.2.  Jira for items included in 0.9 can be found here:

https://issues.apache.org/jira/browse/BIGTOP-1480



What about this straw man proposal: deprecate in 1.3 with some kind of message in the build, and remove for 1.4? And add a pointer to any third-party packaging that might provide similar functionality?





https://issues.apache.org/jira/browse/SPARK-2614?focusedCommentId=
14070908&page=com.atlassian.jira.plugin.system.issuetabpanels:comm

---------------------------------------------------------------------
commands, e-mail: dev-help@spark.apache.org



---------------------------------------------------------------------


"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Tue, 10 Feb 2015 00:48:16 +0000",RE: Using CUDA within Spark / boosting linear algebra,"""Evan R. Sparks"" <evan.sparks@gmail.com>","Hi Evan,

Thank you for explanation and useful link. I am going to build OpenBLAS, link it with Netlib-java and perform benchmark again.

Do I understand correctly that BIDMat binaries contain statically linked Intel MKL BLAS? It might be the reason why I am able to run BIDMat not having MKL BLAS installed on my server. If it is true, I wonder if it is OK because Intel sells this library. Nevertheless, it seems that in my case precompiled MKL BLAS performs better than precompiled OpenBLAS given that BIDMat and Netlib-java are supposed to be on par with JNI overheads.

Though, it might be interesting to link Netlib-java with Intel MKL, as you suggested. I wonder, are John Canny (BIDMat) and Sam Halliday (Netlib-java) interested to compare their libraries.

Best regards, Alexander

From: Evan R. Sparks [mailto:evan.sparks@gmail.com]
Sent: Friday, February 06, 2015 5:58 PM
To: Ulanov, Alexander
Cc: Joseph Bradley; dev@spark.apache.org
Subject: Re: Using CUDA within Spark / boosting linear algebra

I would build OpenBLAS yourself, since good BLAS performance comes from getting cache sizes, etc. set up correctly for your particular hardware - this is often a very tricky process (see, e.g. ATLAS), but we found that on relatively modern Xeon chips, OpenBLAS builds quickly and yields performance competitive with MKL.

To make sure the right library is getting used, you have to make sure it's first on the search path - export LD_LIBRARY_PATH=/path/to/blas/library.so will do the trick here.

For some examples of getting netlib-java setup on an ec2 node and some example benchmarking code we ran a while back, see: https://github.com/shivaram/matrix-bench

In particular - build-openblas-ec2.sh shows you how to build the library and set up symlinks correctly, and scala/run-netlib.sh shows you how to get the path setup and get that library picked up by netlib-java.

In this way - you could probably get cuBLAS set up to be used by netlib-java as well.

- Evan

On Fri, Feb 6, 2015 at 5:43 PM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
Evan, could you elaborate on how to force BIDMat and netlib-java to force loading the right blas? For netlib, I there are few JVM flags, such as -Dcom.github.fommil.netlib.BLAS=com.github.fommil.netlib.F2jBLAS, so I can force it to use Java implementation. Not sure I understand how to force use a specific blas (not specific wrapper for blas).

Btw. I have installed openblas (yum install openblas), so I suppose that netlib is using it.

From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>]
Sent: Friday, February 06, 2015 5:19 PM
To: Ulanov, Alexander
Cc: Joseph Bradley; dev@spark.apache.org<mailto:dev@spark.apache.org>

Subject: Re: Using CUDA within Spark / boosting linear algebra

Getting breeze to pick up the right blas library is critical for performance. I recommend using OpenBLAS (or MKL, if you already have it). It might make sense to force BIDMat to use the same underlying BLAS library as well.

On Fri, Feb 6, 2015 at 4:42 PM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
Hi Evan, Joseph

I did few matrix multiplication test and BIDMat seems to be ~10x faster than netlib-java+breeze (sorry for weird table formatting):

|A*B  size | BIDMat MKL | Breeze+Netlib-java native_system_linux_x86-64| Breeze+Netlib-java f2jblas |
+-----------------------------------------------------------------------+
|100x100*100x100 | 0,00205596 | 0,03810324 | 0,002556 |
|1000x1000*1000x1000 | 0,018320947 | 0,51803557 |1,638475459 |
|10000x10000*10000x10000 | 23,78046632 | 445,0935211 | 1569,233228 |

Configuration: Intel(R) Xeon(R) CPU E31240 3.3 GHz, 6GB RAM, Fedora 19 Linux, Scala 2.11.

Later I will make tests with Cuda. I need to install new Cuda version for this purpose.

Do you have any ideas why breeze-netlib with native blas is so much slower than BIDMat MKL?

Best regards, Alexander

From: Joseph Bradley [mailto:joseph@databricks.com<mailto:joseph@databricks.com>]
Sent: Thursday, February 05, 2015 5:29 PM
To: Ulanov, Alexander
Cc: Evan R. Sparks; dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: Using CUDA within Spark / boosting linear algebra

Hi Alexander,

Using GPUs with Spark would be very exciting.  Small comment: Concerning your question earlier about keeping data stored on the GPU rather than having to move it between main memory and GPU memory on each iteration, I would guess this would be critical to getting good performance.  If you could do multiple local iterations before aggregating results, then the cost of data movement to the GPU could be amortized (and I believe that is done in practice).  Having Spark be aware of the GPU and using it as another part of memory sounds like a much bigger undertaking.

Joseph

On Thu, Feb 5, 2015 at 4:59 PM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
Thank you for explanation! I‚Äôve watched the BIDMach presentation by John Canny and I am really inspired by his talk and comparisons with Spark MLlib.

I am very interested to find out what will be better within Spark: BIDMat or netlib-java with CPU or GPU natives. Could you suggest a fair way to benchmark them? Currently I do benchmarks on artificial neural networks in batch mode. While it is not a ‚Äúpure‚Äù test of linear algebra, it involves some other things that are essential to machine learning.

From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>]
Sent: Thursday, February 05, 2015 1:29 PM
To: Ulanov, Alexander
Cc: dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: Using CUDA within Spark / boosting linear algebra

I'd be surprised of BIDMat+OpenBLAS was significantly faster than netlib-java+OpenBLAS, but if it is much faster it's probably due to data layout and fewer levels of indirection - it's definitely a worthwhile experiment to run. The main speedups I've seen from using it come from highly optimized GPU code for linear algebra. I know that in the past Canny has gone as far as to write custom GPU kernels for performance-critical regions of code.[1]

BIDMach is highly optimized for single node performance or performance on small clusters.[2] Once data doesn't fit easily in GPU memory (or can be batched in that way) the performance tends to fall off. Canny argues for hardware/software codesign and as such prefers machine configurations that are quite different than what we find in most commodity cluster nodes - e.g. 10 disk cahnnels and 4 GPUs.

In contrast, MLlib was designed for horizontal scalability on commodity clusters and works best on very big datasets - order of terabytes.

For the most part, these projects developed concurrently to address slightly different use cases. That said, there may be bits of BIDMach we could repurpose for MLlib - keep in mind we need to be careful about maintaining cross-language compatibility for our Java and Python-users, though.

- Evan

[1] - http://arxiv.org/abs/1409.5402
[2] - http://eecs.berkeley.edu/~hzhao/papers/BD.pdf

On Thu, Feb 5, 2015 at 1:00 PM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
Hi Evan,

Thank you for suggestion! BIDMat seems to have terrific speed. Do you know what makes them faster than netlib-java?

The same group has BIDMach library that implements machine learning. For some examples they use Caffe convolutional neural network library owned by another group in Berkeley. Could you elaborate on how these all might be connected with Spark Mllib? If you take BIDMat for linear algebra why don‚Äôt you take BIDMach for optimization and learning?

Best regards, Alexander

From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
Sent: Thursday, February 05, 2015 12:09 PM
To: Ulanov, Alexander
Cc: dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.apache.org<mailto:dev@spark.apache.org>>
Subject: Re: Using CUDA within Spark / boosting linear algebra

I'd expect that we can make GPU-accelerated BLAS faster than CPU blas in many cases.

You might consider taking a look at the codepaths that BIDMat (https://github.com/BIDData/BIDMat) takes and comparing them to netlib-java/breeze. John Canny et. al. have done a bunch of work optimizing to make this work really fast from Scala. I've run it on my laptop and compared to MKL and in certain cases it's 10x faster at matrix multiply. There are a lot of layers of indirection here and you really want to avoid data copying as much as possible.

We could also consider swapping out BIDMat for Breeze, but that would be a big project and if we can figure out how to get breeze+cublas to comparable performance that would be a big win.

On Thu, Feb 5, 2015 at 11:55 AM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
Dear Spark developers,

I am exploring how to make linear algebra operations faster within Spark. One way of doing this is to use Scala Breeze library that is bundled with Spark. For matrix operations, it employs Netlib-java that has a Java wrapper for BLAS (basic linear algebra subprograms) and LAPACK native binaries if they are available on the worker node. It also has its own optimized Java implementation of BLAS. It is worth mentioning, that native binaries provide better performance only for BLAS level 3, i.e. matrix-matrix operations or general matrix multiplication (GEMM). This is confirmed by GEMM test on Netlib-java page https://github.com/fommil/netlib-java. I also confirmed it with my experiments with training of artificial neural network https://github.com/apache/spark/pull/1290#issuecomment-70313952. However, I would like to boost performance more.

GPU is supposed to work fast with linear algebra and there is Nvidia CUDA implementation of BLAS, called cublas. I have one Linux server with Nvidia GPU and I was able to do the following. I linked cublas (instead of cpu-based blas) with Netlib-java wrapper and put it into Spark, so Breeze/Netlib is using it. Then I did some performance measurements with regards to artificial neural network batch learning in Spark MLlib that involves matrix-matrix multiplications. It turns out that for matrices of size less than ~1000x780 GPU cublas has the same speed as CPU blas. Cublas becomes slower for bigger matrices. It worth mentioning that it is was not a test for ONLY multiplication since there are other operations involved. One of the reasons for slowdown might be the overhead of copying the matrices from computer memory to graphic card memory and back.

So, few questions:
1) Do these results with CUDA make sense?
2) If the problem is with copy overhead, are there any libraries that allow to force intermediate results to stay in graphic card memory thus removing the overhead?
3) Any other options to speed-up linear algebra in Spark?

Thank you, Alexander

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org><mailto:dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org>>
For additional commands, e-mail: dev-help@spark.apache.org<mailto:dev-help@spark.apache.org><mailto:dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>>


"
shane knapp <sknapp@berkeley.edu>,"Mon, 9 Feb 2015 17:18:14 -0800",adding some temporary jenkins worker nodes...,dev <dev@spark.apache.org>,"...to help w/the build backlog.  let's all welcome
amp-jenkins-slave-{01..03} back to the fray!
"
"""Chester @work"" <chester@alpinenow.com>","Mon, 9 Feb 2015 17:09:26 -0800",Re: Using CUDA within Spark / boosting linear algebra,"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Maybe you can ask prof john canny himself:-)  as I invited him to give a talk at Alpine data labs in March's meetup (SF big Analytics & SF machine learning joined meetup) , 3/11. To be announced in next day or so. 

Chester

Sent from my iPhone

rote:
ink it with Netlib-java and perform benchmark again.
ntel MKL BLAS? It might be the reason why I am able to run BIDMat not having MKL BLAS installed on my server. If it is true, I wonder if it is OK because Intel sells this library. Nevertheless, it seems that in my case precompiled MKL BLAS performs better than precompiled OpenBLAS given that BIDMat and Netlib-java are supposed to be on par with JNI overheads.
 suggested. I wonder, are John Canny (BIDMat) and Sam Halliday (Netlib-java) interested to compare their libraries.
tting cache sizes, etc. set up correctly for your particular hardware - this is often a very tricky process (see, e.g. ATLAS), but we found that on relatively modern Xeon chips, OpenBLAS builds quickly and yields performance competitive with MKL.
 first on the search path - export LD_LIBRARY_PATH=/path/to/blas/library.so will do the trick here.
mple benchmarking code we ran a while back, see: https://github.com/shivaram/matrix-bench
nd set up symlinks correctly, and scala/run-netlib.sh shows you how to get the path setup and get that library picked up by netlib-java.
va as well.
oading the right blas? For netlib, I there are few JVM flags, such as -Dcom.github.fommil.netlib.BLAS=com.github.fommil.netlib.F2jBLAS, so I can force it to use Java implementation. Not sure I understand how to force use a specific blas (not specific wrapper for blas).
etlib is using it.
l.com>]
ce. I recommend using OpenBLAS (or MKL, if you already have it). It might make sense to force BIDMat to use the same underlying BLAS library as well.
an netlib-java+breeze (sorry for weird table formatting):
reeze+Netlib-java f2jblas |
ux, Scala 2.11.
his purpose.
 than BIDMat MKL?
s.com>]
our question earlier about keeping data stored on the GPU rather than having to move it between main memory and GPU memory on each iteration, I would guess this would be critical to getting good performance.  If you could do multiple local iterations before aggregating results, then the cost of data movement to the GPU could be amortized (and I believe that is done in practice).  Having Spark be aware of the GPU and using it as another part of memory sounds like a much bigger undertaking.
ohn Canny and I am really inspired by his talk and comparisons with Spark MLlib.
r netlib-java with CPU or GPU natives. Could you suggest a fair way to benchmark them? Currently I do benchmarks on artificial neural networks in batch mode. While it is not a °∞pure°± test of linear algebra, it involves some other things that are essential to machine learning.
l.com>]
ava+OpenBLAS, but if it is much faster it's probably due to data layout and fewer levels of indirection - it's definitely a worthwhile experiment to run. The main speedups I've seen from using it come from highly optimized GPU code for linear algebra. I know that in the past Canny has gone as far as to write custom GPU kernels for performance-critical regions of code.[1]
hed in that way) the performance tends to fall off. Canny argues for hardware/software codesign and as such prefers machine configurations that are quite different than what we find in most commodity cluster nodes - e.g. 10 disk cahnnels and 4 GPUs.
usters and works best on very big datasets - order of terabytes.
ly different use cases. That said, there may be bits of BIDMach we could repurpose for MLlib - keep in mind we need to be careful about maintaining cross-language compatibility for our Java and Python-users, though.
<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexa what makes them faster than netlib-java?
ome examples they use Caffe convolutional neural network library owned by another group in Berkeley. Could you elaborate on how these all might be connected with Spark Mllib? If you take BIDMat for linear algebra why don°Øt you take BIDMach for optimization and learning?
l.com><mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
che.org<mailto:dev@spark.apache.org>>
any cases.
hub.com/BIDData/BIDMat) takes and comparing them to netlib-java/breeze. John Canny et. al. have done a bunch of work optimizing to make this work really fast from Scala. I've run it on my laptop and compared to MKL and in certain cases it's 10x faster at matrix multiply. There are a lot of layers of indirection here and you really want to avoid data copying as much as possible.
 big project and if we can figure out how to get breeze+cublas to comparable performance that would be a big win.
m<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexne way of doing this is to use Scala Breeze library that is bundled with Spark. For matrix operations, it employs Netlib-java that has a Java wrapper for BLAS (basic linear algebra subprograms) and LAPACK native binaries if they are available on the worker node. It also has its own optimized Java implementation of BLAS. It is worth mentioning, that native binaries provide better performance only for BLAS level 3, i.e. matrix-matrix operations or general matrix multiplication (GEMM). This is confirmed by GEMM test on Netlib-java page https://github.com/fommil/netlib-java. I also confirmed it with my experiments with training of artificial neural network https://github.com/apache/spark/pull/1290#issuecomment-70313952. However, I would like to boost performance more.
mplementation of BLAS, called cublas. I have one Linux server with Nvidia GPU and I was able to do the following. I linked cublas (instead of cpu-based blas) with Netlib-java wrapper and put it into Spark, so Breeze/Netlib is using it. Then I did some performance measurements with regards to artificial neural network batch learning in Spark MLlib that involves matrix-matrix multiplications. It turns out that for matrices of size less than ~1000x780 GPU cublas has the same speed as CPU blas. Cublas becomes slower for bigger matrices. It worth mentioning that it is was not a test for ONLY multiplication sght be the overhead of copying the matrices from computer memory to graphic card memory and back.
w to force intermediate results to stay in graphic card memory thus removing the overhead?
cribe@spark.apache.org><mailto:dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org>>
@spark.apache.org><mailto:dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>>

---------------------------------------------------------------------


"
"""Evan R. Sparks"" <evan.sparks@gmail.com>","Mon, 9 Feb 2015 18:06:06 -0800",Re: Using CUDA within Spark / boosting linear algebra,"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Great - perhaps we can move this discussion off-list and onto a JIRA
ticket? (Here's one: https://issues.apache.org/jira/browse/SPARK-5705)

It seems like this is going to be somewhat exploratory for a while (and
there's probably only a handful of us who really care about fast linear
algebra!)

- Evan


OK
u
a)
on
s
.so
et
m>
 I
ce
ry
m>
r
s
m>
by John
ib.
n
ra, it involves
ny
t
m
w
y
‚Äôt
ng
d
a
le
e
a
s
t
"
Andrew Ash <andrew@andrewash.com>,"Mon, 9 Feb 2015 23:09:36 -0500",Re: Pull Requests on github,Akhil Das <akhil@sigmoidanalytics.com>,"Sam, I see your PR was merged -- many thanks for sending it in and getting
it merged!

In general for future reference, the most effective way to contribute is
outlined on this wiki page:
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark


"
Meethu Mathew <meethu.mathew@flytxt.com>,"Tue, 10 Feb 2015 10:30:58 +0530",Mail to user@spark.apache.org failing,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

The mail id given in 
https://cwiki.apache.org/confluence/display/SPARK/Powered+By+Spark seems 
to be failing. Can anyone tell me how to get added to Powered By Spark list?

-- 

Regards,

*Meethu*
"
Patrick Wendell <pwendell@gmail.com>,"Mon, 9 Feb 2015 21:48:54 -0800",Re: Mail to user@spark.apache.org failing,Meethu Mathew <meethu.mathew@flytxt.com>,"Ah - we should update it to suggest mailing the dev@ list (and if
there is enough traffic maybe do something else).

I'm happy to add you if you can give an organization name, URL, a list
of which Spark components you are using, and a short description of
your use case..


---------------------------------------------------------------------


"
Judy Nash <judynash@exchange.microsoft.com>,"Tue, 10 Feb 2015 06:02:37 +0000",New Metrics Sink class not packaged in spark-assembly jar ,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hello,

Working on SPARK-5708<https://issues.apache.org/jira/browse/SPARK-5708> - Add Slf4jSink to Spark Metrics Sink.

Wrote a new Slf4jSink class (see patch attached), but the new class is not packaged as part of spark-assembly jar.

Do I need to update build config somewhere to have this packaged?

Current packaged class:
[cid:image001.png@01D044B4.1B17A1C0]

Thought I must have missed something basic but can't figure out why.

Thanks!
Judy

---------------------------------------------------------------------"
Patrick Wendell <pwendell@gmail.com>,"Mon, 9 Feb 2015 22:09:01 -0800",Re: Keep or remove Debian packaging in Spark?,"""Nate D'Amico"" <nate@reactor8.com>","Mark was involved in adding this code (IIRC) and has also been the
most active in maintaining it. So I'd be interested in hearing his
thoughts on that proposal. Mark - would you be okay deprecating this
and having Spark instead work with the upstream projects that focus on
packaging?

My feeling is that it's better to just have nothing than to have
something not usable out-of-the-box (which to your point, is a lot
more work).


---------------------------------------------------------------------


"
Denny Lee <denny.g.lee@gmail.com>,"Tue, 10 Feb 2015 06:11:41 +0000",Powered by Spark: Concur,"""dev@spark.apache.org"" <dev@spark.apache.org>","Forgot to add Concur to the ""Powered by Spark"" wiki:

Concur
https://www.concur.com
Spark SQL, MLLib
Using Spark for travel and expenses analytics and personalization

Thanks!
Denny
"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 9 Feb 2015 22:23:55 -0800",Re: Powered by Spark: Concur,Denny Lee <denny.g.lee@gmail.com>,"Thanks Denny; added you.

Matei



---------------------------------------------------------------------


"
Denny Lee <denny.g.lee@gmail.com>,"Tue, 10 Feb 2015 06:39:20 +0000",Re: Powered by Spark: Concur,Matei Zaharia <matei.zaharia@gmail.com>,"Thanks Matei - much appreciated!


"
Patrick Wendell <pwendell@gmail.com>,"Mon, 9 Feb 2015 22:43:20 -0800",Re: New Metrics Sink class not packaged in spark-assembly jar,Judy Nash <judynash@exchange.microsoft.com>,"Actually, to correct myself, the assembly jar is in
assembly/target/scala-2.11 (I think).


"
Patrick Wendell <pwendell@gmail.com>,"Mon, 9 Feb 2015 22:42:51 -0800",Re: New Metrics Sink class not packaged in spark-assembly jar,Judy Nash <judynash@exchange.microsoft.com>,"Hi Judy,

If you have added source files in the sink/ source folder, they should
are looking inside the ""/dist"" folder. That only gets populated if you run
""make-distribution"". The normal development process is just to do ""mvn
package"" and then look at the assembly jar that is contained in core/target.

- Patrick


"
Judy Nash <judynash@exchange.microsoft.com>,"Tue, 10 Feb 2015 07:06:26 +0000",RE: New Metrics Sink class not packaged in spark-assembly jar,Patrick Wendell <pwendell@gmail.com>,"Thanks Patrick! That was the issue.
Built the jars on windows env with mvn and forgot to run make-distributions.ps1  afterward, so was looking at old jars.

From: Patrick Wendell [mailto:pwendell@gmail.com]
Sent: Monday, February 9, 2015 10:43 PM
To: Judy Nash
Cc: dev@spark.apache.org
Subject: Re: New Metrics Sink class not packaged in spark-assembly jar

Actually, to correct myself, the assembly jar is in assembly/target/scala-2.11 (I think).

Hi Judy,

If you have added source files in the sink/ source folder, they should appelooking inside the ""/dist"" folder. That only gets populated if you run ""make-distribution"". The normal development process is just to do ""mvn package"" and then look at the assembly jar that is contained in core/target.

- Patrick

Hello,

Working on SPARK-5708<https://issues.apache.org/jira/browse/SPARK-5708> - Add Slf4jSink to Spark Metrics Sink.

Wrote a new Slf4jSink class (see patch attached), but the new class is not packaged as part of spark-assembly jar.

Do I need to update build config somewhere to have this packaged?

Current packaged class:
[cid:image001.png@01D044BC.8FE515C0]

Thought I must have missed something basic but can't figure out why.

Thanks!
Judy

---------------------------------------------------------------------
ribe@spark.apache.org>
spark.apache.org>


"
Paolo Platter <paolo.platter@agilelab.it>,"Tue, 10 Feb 2015 07:10:13 +0000",R: Powered by Spark: Concur,"Denny Lee <denny.g.lee@gmail.com>, Matei Zaharia <matei.zaharia@gmail.com>","Hi,

I checked the powered by wiki too and Agile Labs should be Agile Lab. The link is wrong too, it should be www.agilelab.it.
The description is correct.

Thanks a lot

Paolo

Inviata dal mio Windows Phone
________________________________
Da: Denny Lee<mailto:denny.g.lee@gmail.com>
Inviato: ˝10/˝02/˝2015 07:41
A: Matei Zaharia<mailto:matei.zaharia@gmail.com>
Cc: dev@spark.apache.org<mailto:dev@spark.apache.org>
Oggetto: Re: Powered by Spark: Concur

Thanks Matei - much appreciated!


"
fommil <sam.halliday@gmail.com>,"Tue, 10 Feb 2015 00:48:11 -0700 (MST)",Re: Pull Requests on github,dev@spark.apache.org,"Cool, thanks! Let me know if there are any more core numerical libraries
that you'd like to see to support Spark with optimised natives using a
similar packaging model at netlib-java.

I'm interested in fast random number generation next, and I keep wondering
if anybody would be interested in paying for FPGA or GPU / APU backends for
netlib-java. It would be a *lot* of work but I'd be very interested to talk
to an organisation with such a requirement and I'd be able to do it in less
time than they would internally.





--"
Patrick Wendell <pwendell@gmail.com>,"Mon, 9 Feb 2015 23:59:04 -0800",Re: Powered by Spark: Concur,Paolo Platter <paolo.platter@agilelab.it>,"Thanks Paolo - I've fixed it.


---------------------------------------------------------------------


"
Paolo Platter <paolo.platter@agilelab.it>,"Tue, 10 Feb 2015 08:02:39 +0000",R: Powered by Spark: Concur,Patrick Wendell <pwendell@gmail.com>,"Thank you!

Paolo

Inviata dal mio Windows Phone
________________________________
Da: Patrick Wendell<mailto:pwendell@gmail.com>
Inviato: ˝10/˝02/˝2015 08:59
A: Paolo Platter<mailto:paolo.platter@agilelab.it>
Cc: Denny Lee<mailto:denny.g.lee@gmail.com>; Matei Zaharia<mailto:matei.zaharia@gmail.com>; dev@spark.apache.org<mailto:dev@spark.apache.org>
Oggetto: Re: Powered by Spark: Concur

Thanks Paolo - I've fixed it.

 link is wrong too, it should be www.agilelab.it<http://www.agilelab.it>.
"
Brock Palen <brockp@umich.edu>,"Tue, 10 Feb 2015 08:39:48 -0500",,dev@spark.apache.org,"Sorry to pollute the list.

I am one half the HPC podcast www.rce-cast.com and we are looking to feature Spark on the show.

We are looking for a developer or two who can answer questions to educate the research community about Spark.

Please contact me off list.  It takes about an hour over the phone or Skype and is a friendly interview.

Thanks!

Brock Palen
www.umich.edu/~brockp
CAEN Advanced Computing
XSEDE Campus Champion
brockp@umich.edu
(734)936-1985




---------------------------------------------------------------------


"
=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"Tue, 10 Feb 2015 14:49:08 +0100",Re: Unit tests,Josh Rosen <rosenville@gmail.com>,"Thank, Josh, I missed that PR.


r


-- 

--
Iulian Dragos

------
Reactive Apps on the JVM
www.typesafe.com
"
Debasish Das <debasish.das83@gmail.com>,"Tue, 10 Feb 2015 08:01:13 -0800",Batch prediciton for ALS,dev <dev@spark.apache.org>,"Hi,

Will it be possible to merge this PR to 1.3 ?

https://github.com/apache/spark/pull/3098

The batch prediction API in ALS will be useful for us who want to cross
validate on prec@k and MAP...

Thanks.
Deb
"
jay vyas <jayunit100.apache@gmail.com>,"Tue, 10 Feb 2015 11:37:27 -0500",Re: Keep or remove Debian packaging in Spark?,Patrick Wendell <pwendell@gmail.com>,"@patrick @nate  good idea,  might as well join forces... right now in
bigtop we already have

- packaging of both deb and rpm versions of spark in bigtop, +
- puppet recipes which work for standalone deployment, +
- curation of e2e vagrant tests + bigpets"
Mark Hamstra <mark@clearstorydata.com>,"Tue, 10 Feb 2015 09:05:11 -0800",Re: Keep or remove Debian packaging in Spark?,Patrick Wendell <pwendell@gmail.com>,"Yeah, I'm fine with that.


"
Chester Chen <chester@alpinenow.com>,"Tue, 10 Feb 2015 10:11:16 -0800","FYI: Prof John Canny is giving a talk on ""Machine Learning at the
 limit"" in SF Big Analytics Meetup","""dev@spark.apache.org"" <dev@spark.apache.org>, ""user@spark.apache.org"" <user@spark.apache.org>","Just in case you are in San Francisco, we are having a meetup by Prof John
Canny

http://www.meetup.com/SF-Big-Analytics/events/220427049/


Chester
"
Imran Rashid <imran@therashids.com>,"Tue, 10 Feb 2015 12:44:02 -0600",new committer criteria,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi all,

We've been considering changing criteria for being a committer (
http://s.apache.org/VFw), but I don't think there are any conclusions yet.
I had proposed eliminating (or at least weakening) this requirement:


I realize it might be hard to nail down what the criteria are very
precisely; alternatively we could just start nominating individuals that
are good candidates, and see what sticks.  I can think of a few more
community members that might make the cut -- some that I think are almost
definitely in, others that it kinda depends where we draw the line.

thank,
Imran
"
Koert Kuipers <koert@tresata.com>,"Tue, 10 Feb 2015 14:47:55 -0500",Re: renaming SchemaRDD -> DataFrame,"""dev@spark.apache.org"" <dev@spark.apache.org>","so i understand the success or spark.sql. besides the fact that anything
with the words SQL in its name will have thousands of developers running
towards it because of the familiarity, there is also a genuine need for a
generic RDD that holds record-like objects, with field names and runtime
types. after all that is a successfull generic abstraction used in many
structured data tools.

but to me that abstraction is as simple as:

trait SchemaRDD extends RDD[Row] {
  def schema: StructType
}

and perhaps another abstraction to indicate it intends to be column
oriented (with a few methods to efficiently extract a subset of columns).
so that could be DataFrame.

such simple contracts would allow many people to write loaders for this
(say from csv) and whatnot.

what i do not understand why it has to be much more complex than this. but
if i look at DataFrame it has so much additional stuff, that has (in my
eyes) nothing to do with generic structured data analysis.

for example to implement DataFrame i need to implement about 40 additional
methods!? and for some the SQLness is obviously leaking into the
abstraction. for example why would i care about:
  def registerTempTable(tableName: String): Unit


best, koert


"
Matei Zaharia <matei.zaharia@gmail.com>,"Tue, 10 Feb 2015 11:57:24 -0800",Re: renaming SchemaRDD -> DataFrame,Koert Kuipers <koert@tresata.com>,"You're not really supposed to subclass DataFrame, instead you can make it from an RDD of Rows and a schema (e.g. with SQLContext.applySchema). Actually the Spark SQL data source API supports that too (org.apache.spark.sql.sources). Think of DataFrame as a container for structured data, not as a class that all data sources will have to implement. If you want to do something fancy like compute the Rows dynamically, your RDD can implement its own compute() method to do that.

Matei

anything
running
for a
runtime
many
columns).
this
but
my
additional
in-memory
efficient
of
uses a
a
cheap
Partition
something
like
<velvia.github@gmail.com>
at
<evan.sparks@gmail.com
R or
<rxin@databricks.com>
<velvia.github@gmail.com>
idea
certain
object,
<rxin@databricks.com>
sources
automatically
https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala
and now
all
stuff
talks
for
one
parts
<rxin@databricks.com
matter
out a
and
Scaladocs
have
have
<rxin@databricks.com>:
be
added
few
even
columnar
it is
that.
I
the
least
Schema
a
giving
since
Spark
and
SQL
data.
natural
API.""
spark
yet,
background
DataFrame in
common
and
API.
commonly
be an
map,
all
a
existing
dev-unsubscribe@spark.apache.org
---------------------------------------------------------------------
---------------------------------------------------------------------
---------------------------------------------------------------------


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Tue, 10 Feb 2015 11:58:01 -0800",Re: renaming SchemaRDD -> DataFrame,Koert Kuipers <koert@tresata.com>,"Koert,

Don't get too hang up on the name SQL. This is exactly what you want: a
collection with record-like objects with field names and runtime types.

Almost all of the 40 methods are transformations for structured data, such
as aggregation on a field, or filtering on a field. If all you have is the
old RDD style map/flatMap, then any transformation would lose the schema
information, making the extra schema information useless.





"
Koert Kuipers <koert@tresata.com>,"Tue, 10 Feb 2015 15:10:34 -0500",Re: renaming SchemaRDD -> DataFrame,"""dev@spark.apache.org"" <dev@spark.apache.org>","thanks matei its good to know i can create them like that

reynold, yeah somehow the words sql gets me going :) sorry...
yeah agreed that you need new transformations to preserve the schema info.
i misunderstood and thought i had to implement the bunch but that is
clearly not necessary as matei indicated.

allright i am clearly being slow/dense here, but now it makes sense to
me....







"
Reynold Xin <rxin@databricks.com>,"Tue, 10 Feb 2015 12:14:41 -0800",Re: renaming SchemaRDD -> DataFrame,Koert Kuipers <koert@tresata.com>,"It's a good point. I will update the documentation to say that this is not
meant to be subclassed externally.



"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Tue, 10 Feb 2015 22:11:56 +0000",RE: Using CUDA within Spark / boosting linear algebra,"""Evan R. Sparks"" <evan.sparks@gmail.com>","Thanks, Evan! It seems that ticket was marked as duplicate though the original one discusses slightly different topic. I was able to link netlib with MKL from BIDMat binaries. Indeed, MKL is statically linked inside a 60MB library.

|A*B  size | BIDMat MKL | Breeze+Netlib-MKL  from BIDMat| Breeze+Netlib-OpenBlas(native system)| Breeze+Netlib-f2jblas |
+-----------------------------------------------------------------------+
|100x100*100x100 | 0,00205596 | 0,000381 | 0,03810324 | 0,002556 |
|1000x1000*1000x1000 | 0,018320947 | 0,038316857 | 0,51803557 |1,638475459 |
|10000x10000*10000x10000 | 23,78046632 | 32,94546697 |445,0935211 | 1569,233228 |

It turn out that pre-compiled MKL is faster than precompiled OpenBlas on my machine. Probably, I‚Äôll add two more columns with locally compiled openblas and cuda.

Alexander

From: Evan R. Sparks [mailto:evan.sparks@gmail.com]
Sent: Monday, February 09, 2015 6:06 PM
To: Ulanov, Alexander
Cc: Joseph Bradley; dev@spark.apache.org
Subject: Re: Using CUDA within Spark / boosting linear algebra

Great - perhaps we can move this discussion off-list and onto a JIRA ticket? (Here's one: https://issues.apache.org/jira/browse/SPARK-5705)

It seems like this is going to be somewhat exploratory for a while (and there's probably only a handful of us who really care about fast linear algebra!)

- Evan

On Mon, Feb 9, 2015 at 4:48 PM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
Hi Evan,

Thank you for explanation and useful link. I am going to build OpenBLAS, link it with Netlib-java and perform benchmark again.

Do I understand correctly that BIDMat binaries contain statically linked Intel MKL BLAS? It might be the reason why I am able to run BIDMat not having MKL BLAS installed on my server. If it is true, I wonder if it is OK because Intel sells this library. Nevertheless, it seems that in my case precompiled MKL BLAS performs better than precompiled OpenBLAS given that BIDMat and Netlib-java are supposed to be on par with JNI overheads.

Though, it might be interesting to link Netlib-java with Intel MKL, as you suggested. I wonder, are John Canny (BIDMat) and Sam Halliday (Netlib-java) interested to compare their libraries.

Best regards, Alexander

From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>]
Sent: Friday, February 06, 2015 5:58 PM

To: Ulanov, Alexander
Cc: Joseph Bradley; dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: Using CUDA within Spark / boosting linear algebra

I would build OpenBLAS yourself, since good BLAS performance comes from getting cache sizes, etc. set up correctly for your particular hardware - this is often a very tricky process (see, e.g. ATLAS), but we found that on relatively modern Xeon chips, OpenBLAS builds quickly and yields performance competitive with MKL.

To make sure the right library is getting used, you have to make sure it's first on the search path - export LD_LIBRARY_PATH=/path/to/blas/library.so will do the trick here.

For some examples of getting netlib-java setup on an ec2 node and some example benchmarking code we ran a while back, see: https://github.com/shivaram/matrix-bench

In particular - build-openblas-ec2.sh shows you how to build the library and set up symlinks correctly, and scala/run-netlib.sh shows you how to get the path setup and get that library picked up by netlib-java.

In this way - you could probably get cuBLAS set up to be used by netlib-java as well.

- Evan

On Fri, Feb 6, 2015 at 5:43 PM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
Evan, could you elaborate on how to force BIDMat and netlib-java to force loading the right blas? For netlib, I there are few JVM flags, such as -Dcom.github.fommil.netlib.BLAS=com.github.fommil.netlib.F2jBLAS, so I can force it to use Java implementation. Not sure I understand how to force use a specific blas (not specific wrapper for blas).

Btw. I have installed openblas (yum install openblas), so I suppose that netlib is using it.

From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>]
Sent: Friday, February 06, 2015 5:19 PM
To: Ulanov, Alexander
Cc: Joseph Bradley; dev@spark.apache.org<mailto:dev@spark.apache.org>

Subject: Re: Using CUDA within Spark / boosting linear algebra

Getting breeze to pick up the right blas library is critical for performance. I recommend using OpenBLAS (or MKL, if you already have it). It might make sense to force BIDMat to use the same underlying BLAS library as well.

On Fri, Feb 6, 2015 at 4:42 PM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
Hi Evan, Joseph

I did few matrix multiplication test and BIDMat seems to be ~10x faster than netlib-java+breeze (sorry for weird table formatting):

|A*B  size | BIDMat MKL | Breeze+Netlib-java native_system_linux_x86-64| Breeze+Netlib-java f2jblas |
+-----------------------------------------------------------------------+
|100x100*100x100 | 0,00205596 | 0,03810324 | 0,002556 |
|1000x1000*1000x1000 | 0,018320947 | 0,51803557 |1,638475459 |
|10000x10000*10000x10000 | 23,78046632 | 445,0935211 | 1569,233228 |

Configuration: Intel(R) Xeon(R) CPU E31240 3.3 GHz, 6GB RAM, Fedora 19 Linux, Scala 2.11.

Later I will make tests with Cuda. I need to install new Cuda version for this purpose.

Do you have any ideas why breeze-netlib with native blas is so much slower than BIDMat MKL?

Best regards, Alexander

From: Joseph Bradley [mailto:joseph@databricks.com<mailto:joseph@databricks.com>]
Sent: Thursday, February 05, 2015 5:29 PM
To: Ulanov, Alexander
Cc: Evan R. Sparks; dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: Using CUDA within Spark / boosting linear algebra

Hi Alexander,

Using GPUs with Spark would be very exciting.  Small comment: Concerning your question earlier about keeping data stored on the GPU rather than having to move it between main memory and GPU memory on each iteration, I would guess this would be critical to getting good performance.  If you could do multiple local iterations before aggregating results, then the cost of data movement to the GPU could be amortized (and I believe that is done in practice).  Having Spark be aware of the GPU and using it as another part of memory sounds like a much bigger undertaking.

Joseph

On Thu, Feb 5, 2015 at 4:59 PM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
Thank you for explanation! I‚Äôve watched the BIDMach presentation by John Canny and I am really inspired by his talk and comparisons with Spark MLlib.

I am very interested to find out what will be better within Spark: BIDMat or netlib-java with CPU or GPU natives. Could you suggest a fair way to benchmark them? Currently I do benchmarks on artificial neural networks in batch mode. While it is not a ‚Äúpure‚Äù test of linear algebra, it involves some other things that are essential to machine learning.

From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>]
Sent: Thursday, February 05, 2015 1:29 PM
To: Ulanov, Alexander
Cc: dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: Using CUDA within Spark / boosting linear algebra

I'd be surprised of BIDMat+OpenBLAS was significantly faster than netlib-java+OpenBLAS, but if it is much faster it's probably due to data layout and fewer levels of indirection - it's definitely a worthwhile experiment to run. The main speedups I've seen from using it come from highly optimized GPU code for linear algebra. I know that in the past Canny has gone as far as to write custom GPU kernels for performance-critical regions of code.[1]

BIDMach is highly optimized for single node performance or performance on small clusters.[2] Once data doesn't fit easily in GPU memory (or can be batched in that way) the performance tends to fall off. Canny argues for hardware/software codesign and as such prefers machine configurations that are quite different than what we find in most commodity cluster nodes - e.g. 10 disk cahnnels and 4 GPUs.

In contrast, MLlib was designed for horizontal scalability on commodity clusters and works best on very big datasets - order of terabytes.

For the most part, these projects developed concurrently to address slightly different use cases. That said, there may be bits of BIDMach we could repurpose for MLlib - keep in mind we need to be careful about maintaining cross-language compatibility for our Java and Python-users, though.

- Evan

[1] - http://arxiv.org/abs/1409.5402
[2] - http://eecs.berkeley.edu/~hzhao/papers/BD.pdf

On Thu, Feb 5, 2015 at 1:00 PM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
Hi Evan,

Thank you for suggestion! BIDMat seems to have terrific speed. Do you know what makes them faster than netlib-java?

The same group has BIDMach library that implements machine learning. For some examples they use Caffe convolutional neural network library owned by another group in Berkeley. Could you elaborate on how these all might be connected with Spark Mllib? If you take BIDMat for linear algebra why don‚Äôt you take BIDMach for optimization and learning?

Best regards, Alexander

From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
Sent: Thursday, February 05, 2015 12:09 PM
To: Ulanov, Alexander
Cc: dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.apache.org<mailto:dev@spark.apache.org>>
Subject: Re: Using CUDA within Spark / boosting linear algebra

I'd expect that we can make GPU-accelerated BLAS faster than CPU blas in many cases.

You might consider taking a look at the codepaths that BIDMat (https://github.com/BIDData/BIDMat) takes and comparing them to netlib-java/breeze. John Canny et. al. have done a bunch of work optimizing to make this work really fast from Scala. I've run it on my laptop and compared to MKL and in certain cases it's 10x faster at matrix multiply. There are a lot of layers of indirection here and you really want to avoid data copying as much as possible.

We could also consider swapping out BIDMat for Breeze, but that would be a big project and if we can figure out how to get breeze+cublas to comparable performance that would be a big win.

On Thu, Feb 5, 2015 at 11:55 AM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
Dear Spark developers,

I am exploring how to make linear algebra operations faster within Spark. One way of doing this is to use Scala Breeze library that is bundled with Spark. For matrix operations, it employs Netlib-java that has a Java wrapper for BLAS (basic linear algebra subprograms) and LAPACK native binaries if they are available on the worker node. It also has its own optimized Java implementation of BLAS. It is worth mentioning, that native binaries provide better performance only for BLAS level 3, i.e. matrix-matrix operations or general matrix multiplication (GEMM). This is confirmed by GEMM test on Netlib-java page https://github.com/fommil/netlib-java. I also confirmed it with my experiments with training of artificial neural network https://github.com/apache/spark/pull/1290#issuecomment-70313952. However, I would like to boost performance more.

GPU is supposed to work fast with linear algebra and there is Nvidia CUDA implementation of BLAS, called cublas. I have one Linux server with Nvidia GPU and I was able to do the following. I linked cublas (instead of cpu-based blas) with Netlib-java wrapper and put it into Spark, so Breeze/Netlib is using it. Then I did some performance measurements with regards to artificial neural network batch learning in Spark MLlib that involves matrix-matrix multiplications. It turns out that for matrices of size less than ~1000x780 GPU cublas has the same speed as CPU blas. Cublas becomes slower for bigger matrices. It worth mentioning that it is was not a test for ONLY multiplication since there are other operations involved. One of the reasons for slowdown might be the overhead of copying the matrices from computer memory to graphic card memory and back.

So, few questions:
1) Do these results with CUDA make sense?
2) If the problem is with copy overhead, are there any libraries that allow to force intermediate results to stay in graphic card memory thus removing the overhead?
3) Any other options to speed-up linear algebra in Spark?

Thank you, Alexander

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org><mailto:dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org>>
For additional commands, e-mail: dev-help@spark.apache.org<mailto:dev-help@spark.apache.org><mailto:dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>>



"
Scott walent <scottwalent@gmail.com>,"Tue, 10 Feb 2015 15:08:15 -0800",Spark Summit East - March 18-19 - NYC,"dev@spark.apache.org, user@spark.apache.org","The inaugural Spark Summit East, an event to bring the Apache Spark
community together, will be in New York City on March 18, 2015. We are
excited about the growth of Spark and to bring the event to the east coast.

At Spark Summit East you can look forward to hearing from Matei Zaharia,
Databricks CEO Ion Stoica, representatives from Palantir, Goldman Sachs,
Baidu, Salesforce, Cloudera, Box, and many others. (See the full agenda at
http://spark-summit.org/east/2015)  All of these companies are utilizing
Spark. Come see what their experience has been and get a chance to talk
with some of the creators and committers.

If you are new to Spark or looking to improve on your knowledge of the
technology, there will be three levels of Spark Training: Intro to Spark,
Advanced Spark Training, and Data Science with Spark.

Space is limited, but we want to make sure those active in the community
are aware of the this new event in NYC. Use promo code ""DevList15"" for 15%
off your registration fee when registering before March 1, 2015.

Register at http://spark-summit.org/east/2015/register

Looking forward to seeing you there!

Best,
Scott & The Spark Summit Organizers
"
Yi Tian <tianyi.asiainfo@gmail.com>,"Wed, 11 Feb 2015 12:08:32 +0800",Build spark failed with maven,dev@spark.apache.org,"Hi, all

I got an ERROR when I build spark master branch with maven (commit: 
|2d1e916730492f5d61b97da6c483d3223ca44315|)

|[INFO]
[INFO] ------------------------------------------------------------------------
[INFO] Building Spark Project Catalyst 1.3.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO]
[INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-versions) @ spark-catalyst_2.10 ---
[INFO]
[INFO] --- build-helper-maven-plugin:1.8:add-source (add-scala-sources) @ spark-catalyst_2.10 ---
[INFO] Source directory: /Users/tianyi/github/community/apache-spark/sql/catalyst/src/main/scala added.
[INFO]
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ spark-catalyst_2.10 ---
[INFO]
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ spark-catalyst_2.10 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /Users/tianyi/github/community/apache-spark/sql/catalyst/src/main/resources
[INFO] Copying 3 resources
[INFO]
[INFO] --- scala-maven-plugin:3.2.0:compile (scala-compile-first) @ spark-catalyst_2.10 ---
[INFO] Using zinc server for incremental compilation
[INFO] compiler plugin: BasicArtifact(org.scalamacros,paradise_2.10.4,2.0.1,null)
[info] Compiling 69 Scala sources and 3 Java sources to /Users/tianyi/github/community/apache-spark/sql/catalyst/target/scala-2.10/classes...
[error] /Users/tianyi/github/community/apache-spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/dsl/package.scala:314: polymorphic expression cannot be instantiated to expected type;
[error]  found   : [T(in method apply)]org.apache.spark.sql.catalyst.dsl.ScalaUdfBuilder[T(in method apply)]
[error]  required: org.apache.spark.sql.catalyst.dsl.package.ScalaUdfBuilder[T(in method functionToUdfBuilder)]
[error]   implicit def functionToUdfBuilder[T: TypeTag](func: Function1[_, T]): ScalaUdfBuilder[T] = ScalaUdfBuilder(func)
|

Any suggestion?

‚Äã
"
Todd Gao <todd.gao.2013+spark@gmail.com>,"Wed, 11 Feb 2015 16:32:49 +0800",CallbackServer in PySpark Streaming,dev@spark.apache.org,"Hi all,

I am reading the code of PySpark and its Streaming module.

In PySpark Streaming, when the `compute` method of the instance of
PythonTransformedDStream is invoked, a connection to the CallbackServer
is created internally.
I wonder where is the CallbackServer for each PythonTransformedDStream
instance on the slave nodes in distributed environment.
Is there a CallbackServer running on every slave node?

thanks
Todd
"
James <alcaid1801@gmail.com>,"Wed, 11 Feb 2015 19:30:19 +0800",[GraphX] Estimating Average distance of a big graph using GraphX,dev@spark.apache.org,"Hello,

Recently  I am trying to estimate the average distance of a big graph using
spark with the help of [HyperAnf](http://dl.acm.org/citation.cfm?id=1963493
).

It works like Connect Componenet algorithm, while the attribute of a vertex
is a HyperLogLog counter that at k-th iteration it estimates the number of
vertices it could reaches less than k hops.

I have successfully run the code on a graph with 20M vertices. But I still
need help:


*I think the code could work more efficiently especially the ""Send message""
function, but I am not sure about what will happen if a vertex receive no
message at a iteration.*

Here is my code: https://github.com/alcaid1801/Erdos

Any returns is appreciated.
"
Davies Liu <davies@databricks.com>,"Wed, 11 Feb 2015 10:38:33 -0800",Re: CallbackServer in PySpark Streaming,Todd Gao <todd.gao.2013+spark@gmail.com>,"The CallbackServer is part of Py4j, it's only used in driver, not used
in slaves or workers.


---------------------------------------------------------------------


"
Aniket Bhatnagar <aniket.bhatnagar@gmail.com>,"Wed, 11 Feb 2015 18:47:03 +0000",Re: Data source API | sizeInBytes should be to *Scan,Reynold Xin <rxin@databricks.com>,"Circling back on this. Did you get a chance to re-look at this?

Thanks,
Aniket


"
Reynold Xin <rxin@databricks.com>,"Wed, 11 Feb 2015 11:08:50 -0800",Re: Data source API | sizeInBytes should be to *Scan,Aniket Bhatnagar <aniket.bhatnagar@gmail.com>,"Unfortunately this is not to happen for 1.3 (as a snapshot release is
already cut). We need to figure out how we are going to do cardinality
estimation before implementing this. If we need to do this in the future, I
think we can do it in a way that doesn't break existing APIs. Given I think
this won't bring much benefit right now (the only use for it is broadcast
joins), I think it is ok to push this till later.

The issue I asked still stands. What should the optimizer do w.r.t. filters
that are pushed into the data source? Should it ignore those filters, or
apply statistics again?

This also depends on how we want to do statistics. Hive (and a lot of other
database systems) does a scan to figure out statistics, and put all of
those statistics in a catalog. That is a more unified way to solve the
stats problem.

That said, in the world of federated databases, I can see why we might want
to push cardinality estimation to the data sources, since if the use case
is selecting a very small subset of the data from the sources, then it
might be hard for the statistics to be accurate in the catalog built from
data scan.




"
Peter Rudenko <petro.rudenko@gmail.com>,"Wed, 11 Feb 2015 21:13:03 +0200",[ml] Lost persistence for fold in crossvalidation.,dev@spark.apache.org,"Hi i have a problem. Using spark 1.2 with Pipeline + GridSearch + 
LogisticRegression. I‚Äôve reimplemented LogisticRegression.fit method and 
comment out instances.unpersist()

|override  def  fit(dataset:SchemaRDD, paramMap:ParamMap):LogisticRegressionModel  = {
     println(s""Fitting dataset ${dataset.take(1000).toSeq.hashCode()} with ParamMap $paramMap."")
     transformSchema(dataset.schema, paramMap, logging =true)
     import  dataset.sqlContext._
     val  map  =  this.paramMap ++ paramMap
     val  instances  =  dataset.select(map(labelCol).attr, map(featuresCol).attr)
       .map {
         case  Row(label:Double, features:Vector) =>
           LabeledPoint(label, features)
       }

     if  (instances.getStorageLevel ==StorageLevel.NONE) {
       println(""Instances not persisted"")
       instances.persist(StorageLevel.MEMORY_AND_DISK)
     }

      val  lr  =  (new  LogisticRegressionWithLBFGS)
       .setValidateData(false)
       .setIntercept(true)
     lr.optimizer
       .setRegParam(map(regParam))
       .setNumIterations(map(maxIter))
     val  lrm  =  new  LogisticRegressionModel(this, map, lr.run(instances).weights)
     //instances.unpersist()
     // copy model params
     Params.inheritValues(map,this, lrm)
     lrm
   }
|

CrossValidator feeds the same SchemaRDD for each parameter (same hash 
code), but somewhere cache being flushed. The memory is enough. Here‚Äôs 
the output:

|Fitting dataset 2051470010 with ParamMap {
     DRLogisticRegression-f35ae4d3-regParam: 0.1
}.
Instances not persisted
Fitting dataset 2051470010 with ParamMap {
     DRLogisticRegression-f35ae4d3-regParam: 0.01
}.
Instances not persisted
Fitting dataset 2051470010 with ParamMap {
     DRLogisticRegression-f35ae4d3-regParam: 0.001
}.
Instances not persisted
Fitting dataset 802615223 with ParamMap {
     DRLogisticRegression-f35ae4d3-regParam: 0.1
}.
Instances not persisted
Fitting dataset 802615223 with ParamMap {
     DRLogisticRegression-f35ae4d3-regParam: 0.01
}.
Instances not persisted
|

I have 3 parameters in GridSearch and 3 folds for CrossValidation:

|
val  paramGrid  =  new  ParamGridBuilder()
   .addGrid(model.regParam,Array(0.1,0.01,0.001))
   .build()

crossval.setEstimatorParamMaps(paramGrid)
crossval.setNumFolds(3)
|

I assume that the data should be read and cached 3 times (1 to 
numFolds).combinations(2) and be independent from number of parameters. 
But i have 9 times data being read and cached.

Thanks,
Peter Rudenko

‚Äã
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 11 Feb 2015 20:07:48 +0000",numpy on PyPy - potential benefit to PySpark,Spark dev list <dev@spark.apache.org>,"Random question for the PySpark and Python experts/enthusiasts on here:

How big of a deal would it be for PySpark and PySpark users if you could
run numpy on PyPy?

PySpark already supports running on PyPy
<https://github.com/apache/spark/pull/2144>, but libraries like MLlib that
use numpy are not supported.

There is an ongoing initiative to support numpy on PyPy
<http://morepypy.blogspot.com/2015/02/numpypy-status-january-2015.html>,
and they are taking donations <http://pypy.org/numpydonate.html> to support
the effort.

I‚Äôm wondering if any companies using PySpark in production would be
interested in pushing this initiative along, or if it‚Äôs not that big of a
deal.

Nick
‚Äã
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 11 Feb 2015 22:27:29 +0000",1.2.1 start-all.sh broken?,Spark dev list <dev@spark.apache.org>,"I just downloaded 1.2.1 pre-built for Hadoop 2.4+ and ran sbin/start-all.sh
on my OS X.

Failed to find Spark assembly in /path/to/spark-1.2.1-bin-hadoop2.4/lib
You need to build Spark before running this program.

Did the same for 1.2.0 and it worked fine.

Nick
‚Äã
"
Sean Owen <sowen@cloudera.com>,"Wed, 11 Feb 2015 22:34:08 +0000",Re: 1.2.1 start-all.sh broken?,Nicholas Chammas <nicholas.chammas@gmail.com>,"Seems to work OK for me on OS X. I ran ./sbin/start-all.sh from the
root. Both processes say they started successfully.


---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Wed, 11 Feb 2015 14:34:51 -0800",Re: 1.2.1 start-all.sh broken?,Nicholas Chammas <nicholas.chammas@gmail.com>,"I downloaded 1.2.1 tar ball for hadoop 2.4
I got:

ls lib/
datanucleus-api-jdo-3.2.6.jar  datanucleus-rdbms-3.2.9.jar
spark-assembly-1.2.1-hadoop2.4.0.jar
datanucleus-core-3.2.10.jar    spark-1.2.1-yarn-shuffle.jar
 spark-examples-1.2.1-hadoop2.4.0.jar

FYI


sh
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 11 Feb 2015 22:37:40 +0000",Re: 1.2.1 start-all.sh broken?,Ted Yu <yuzhihong@gmail.com>,"This is what get:

spark-1.2.1-bin-hadoop2.4$ ls -1 lib/
datanucleus-api-jdo-3.2.6.jar
datanucleus-core-3.2.10.jar
datanucleus-rdbms-3.2.9.jar
spark-1.2.1-yarn-shuffle.jar
spark-assembly-1.2.1-hadoop2.4.0.jar
spark-examples-1.2.1-hadoop2.4.0.jar

So that looks correct‚Ä¶ Hmm.

Nick
‚Äã


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 11 Feb 2015 22:41:18 +0000",Re: 1.2.1 start-all.sh broken?,Ted Yu <yuzhihong@gmail.com>,"Found it:

https://github.com/apache/spark/compare/v1.2.0...v1.2.1#diff-73058f8e51951ec0b4cb3d48ade91a1fR73

GRRR BASH WORD SPLITTING

My path has a space in it...

Nick


"
Ted Yu <yuzhihong@gmail.com>,"Wed, 11 Feb 2015 14:43:08 -0800",Re: 1.2.1 start-all.sh broken?,Nicholas Chammas <nicholas.chammas@gmail.com>,"I see.
'/path/to/spark-1.2.1-bin-hadoop2.4' didn't contain space :-)


1ec0b4cb3d48ade91a1fR73
b
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 11 Feb 2015 22:46:35 +0000",Re: 1.2.1 start-all.sh broken?,Ted Yu <yuzhihong@gmail.com>,"lol yeah, I changed the path for the email... turned out to be the issue
itself.


51ec0b4cb3d48ade91a1fR73
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 11 Feb 2015 22:47:55 +0000",Re: 1.2.1 start-all.sh broken?,Ted Yu <yuzhihong@gmail.com>,"The tragic thing here is that I was asked to review the patch that
introduced this
<https://github.com/apache/spark/pull/3377#issuecomment-68077315>, and
totally missed it... :(


"
Ted Yu <yuzhihong@gmail.com>,"Wed, 11 Feb 2015 15:07:51 -0800",Re: 1.2.1 start-all.sh broken?,Nicholas Chammas <nicholas.chammas@gmail.com>,"After some googling / trial and error, I got the following working (against
a directory with space in its name):

#!/usr/bin/env bash
OLDIFS=""$IFS""  # save it
IFS="""" # don't split on any white space
dir=""$1/*""
for f in ""$dir""; do
  cat $f
done
IFS=$OLDIFS # restore IFS

Cheers


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 11 Feb 2015 23:15:46 +0000",Re: 1.2.1 start-all.sh broken?,Ted Yu <yuzhihong@gmail.com>,"SPARK-5747 <https://issues.apache.org/jira/browse/SPARK-5747>: Review all
Bash scripts for word splitting bugs

I‚Äôll file sub-tasks under this issue. Feel free to pitch in people!

Nick
‚Äã


e
"
Todd Gao <todd.gao.2013+spark@gmail.com>,"Thu, 12 Feb 2015 09:44:37 +0800",Re: CallbackServer in PySpark Streaming,Davies Liu <davies@databricks.com>,"Thanks Davies.
I am not quite familiar with Spark Streaming. Do you mean that the compute
routine of DStream is only invoked in the driver node,
while only the compute routines of RDD are distributed to the slaves?


"
Davies Liu <davies@databricks.com>,"Wed, 11 Feb 2015 17:50:01 -0800",Re: CallbackServer in PySpark Streaming,Todd Gao <todd.gao.2013+spark@gmail.com>,"Yes.


---------------------------------------------------------------------


"
Todd Gao <todd.gao.2013+spark@gmail.com>,"Thu, 12 Feb 2015 09:57:23 +0800",Re: CallbackServer in PySpark Streaming,Davies Liu <davies@databricks.com>,"Oh I see! Thank you very much, Davies. You correct some of my wrong
understandings.


"
Patrick Wendell <pwendell@gmail.com>,"Wed, 11 Feb 2015 19:32:28 -0800",[ANNOUNCE] Spark 1.3.0 Snapshot 1,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey All,

I've posted Spark 1.3.0 snapshot 1. At this point the 1.3 branch is
ready for community testing and we are strictly merging fixes and
documentation across all components.

The release files, including signatures, digests, etc can be found at:
http://people.apache.org/~pwendell/spark-1.3.0-snapshot1/

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1068/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-1.3.0-snapshot1-docs/

Please report any issues with the release to this thread and/or to our
project JIRA. Thanks!

- Patrick

---------------------------------------------------------------------


"
"""fightfate@163.com"" <fightfate@163.com>","Thu, 12 Feb 2015 13:37:16 +0800",,"user <user@spark.apache.org>, 
	dev <dev@spark.apache.org>","Hi,

Really have no adequate solution got for this issue. Expecting any available analytical rules or hints.

Thanks,
Sun.



fightfate@163.com
 
From: fightfate@163.com
Date: 2015-02-09 11:56
To: user; dev
Subject: Re: Sort Shuffle performance issues about using AppendOnlyMap for large data sets
Hi,
Problem still exists. Any experts would take a look at this? 

Thanks,
Sun.



fightfate@163.com
 
From: fightfate@163.com
Date: 2015-02-06 17:54
To: user; dev
Subject: Sort Shuffle performance issues about using AppendOnlyMap for large data sets
Hi, all
Recently we had caught performance issues when using spark 1.2.0 to read data from hbase and do some summary work.
Our scenario means to : read large data sets from hbase (maybe 100G+ file) , form hbaseRDD, transform to schemardd, 
groupby and aggregate the data while got fewer new summary data sets, loading data into hbase (phoenix).

Our major issue lead to : aggregate large datasets to get summary data sets would consume too long time (1 hour +) , while that
should be supposed not so bad performance. We got the dump file attached and stacktrace from jstack like the following:

From the stacktrace and dump file we can identify that processing large datasets would cause frequent AppendOnlyMap growing, and 
leading to huge map entrysize. We had referenced the source code of org.apache.spark.util.collection.AppendOnlyMap and found that 
the map had been initialized with capacity of 64. That would be too small for our use case. 

So the question is : Does anyone had encounted such issues before? How did that be resolved? I cannot find any jira issues for such problems and 
if someone had seen, please kindly let us know.

More specified solution would goes to : Does any possibility exists for user defining the map capacity releatively in spark? If so, please
tell how to achieve that. 

Best Thanks,
Sun.

   Thread 22432: (state = IN_JAVA)
- org.apache.spark.util.collection.AppendOnlyMap.growTable() @bci=87, line=224 (Compiled frame; information may be imprecise)
- org.apache.spark.util.collection.SizeTrackingAppendOnlyMap.growTable() @bci=1, line=38 (Interpreted frame)
- org.apache.spark.util.collection.AppendOnlyMap.incrementSize() @bci=22, line=198 (Compiled frame)
- org.apache.spark.util.collection.AppendOnlyMap.changeValue(java.lang.Object, scala.Function2) @bci=201, line=145 (Compiled frame)
- org.apache.spark.util.collection.SizeTrackingAppendOnlyMap.changeValue(java.lang.Object, scala.Function2) @bci=3, line=32 (Compiled frame)
- org.apache.spark.util.collection.ExternalSorter.insertAll(scala.collection.Iterator) @bci=141, line=205 (Compiled frame)
- org.apache.spark.shuffle.sort.SortShuffleWriter.write(scala.collection.Iterator) @bci=74, line=58 (Interpreted frame)
- org.apache.spark.scheduler.ShuffleMapTask.runTask(org.apache.spark.TaskContext) @bci=169, line=68 (Interpreted frame)
- org.apache.spark.scheduler.ShuffleMapTask.runTask(org.apache.spark.TaskContext) @bci=2, line=41 (Interpreted frame)
- org.apache.spark.scheduler.Task.run(long) @bci=77, line=56 (Interpreted frame)
- org.apache.spark.executor.Executor$TaskRunner.run() @bci=310, line=196 (Interpreted frame)
- java.util.concurrent.ThreadPoolExecutor.runWorker(java.util.concurrent.ThreadPoolExecutor$Worker) @bci=95, line=1145 (Interpreted frame)
- java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=5, line=615 (Interpreted frame)
- java.lang.Thread.run() @bci=11, line=744 (Interpreted frame)


Thread 22431: (state = IN_JAVA)
- org.apache.spark.util.collection.AppendOnlyMap.growTable() @bci=87, line=224 (Compiled frame; information may be imprecise)
- org.apache.spark.util.collection.SizeTrackingAppendOnlyMap.growTable() @bci=1, line=38 (Interpreted frame)
- org.apache.spark.util.collection.AppendOnlyMap.incrementSize() @bci=22, line=198 (Compiled frame)
- org.apache.spark.util.collection.AppendOnlyMap.changeValue(java.lang.Object, scala.Function2) @bci=201, line=145 (Compiled frame)
- org.apache.spark.util.collection.SizeTrackingAppendOnlyMap.changeValue(java.lang.Object, scala.Function2) @bci=3, line=32 (Compiled frame)
- org.apache.spark.util.collection.ExternalSorter.insertAll(scala.collection.Iterator) @bci=141, line=205 (Compiled frame)
- org.apache.spark.shuffle.sort.SortShuffleWriter.write(scala.collection.Iterator) @bci=74, line=58 (Interpreted frame)
- org.apache.spark.scheduler.ShuffleMapTask.runTask(org.apache.spark.TaskContext) @bci=169, line=68 (Interpreted frame)
- org.apache.spark.scheduler.ShuffleMapTask.runTask(org.apache.spark.TaskContext) @bci=2, line=41 (Interpreted frame)
- org.apache.spark.scheduler.Task.run(long) @bci=77, line=56 (Interpreted frame)
- org.apache.spark.executor.Executor$TaskRunner.run() @bci=310, line=196 (Interpreted frame)
- java.util.concurrent.ThreadPoolExecutor.runWorker(java.util.concurrent.ThreadPoolExecutor$Worker) @bci=95, line=1145 (Interpreted frame)
- java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=5, line=615 (Interpreted frame)
- java.lang.Thread.run() @bci=11, line=744 (Interpreted frame)


fightfate@163.com
1 attachments
dump.png(42K) download preview 
"
lin <kurtt.lin@gmail.com>,"Thu, 12 Feb 2015 15:24:31 +0800",driver fail-over in Spark streaming 1.2.0,dev@spark.apache.org,"Hi, all

In Spark Streaming 1.2.0, when the driver fails and a new driver starts
with the most updated check-pointed data, will the former Executors
connects to the new driver, or will the new driver starts out its own set
of new Executors? In which piece of codes is that done?

Any reply will be appreciated :)

regards,

lin
"
Patrick Wendell <pwendell@gmail.com>,"Thu, 12 Feb 2015 00:12:54 -0800",,"""fightfate@163.com"" <fightfate@163.com>","The map will start with a capacity of 64, but will grow to accommodate
new data. Are you using the groupBy operator in Spark or are you using
Spark SQL's group by? This usually happens if you are grouping or
aggregating in a way that doesn't sufficiently condense the data
created from each input partition.

- Patrick


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Thu, 12 Feb 2015 00:13:44 -0800",Re: driver fail-over in Spark streaming 1.2.0,lin <kurtt.lin@gmail.com>,"It will create and connect to new executors. The executors are mostly
stateless, so the program can resume with new executors.


---------------------------------------------------------------------


"
"""fightfate@163.com"" <fightfate@163.com>","Thu, 12 Feb 2015 16:26:45 +0800",,"""Patrick Wendell"" <pwendell@gmail.com>","Hi, patrick

Really glad to get your reply. 
Yes, we are doing group by operations for our work. We know that this is common for growTable when processing large data sets.

The problem actually goes to : Do we have any possible chance to self-modify the initialCapacity using specifically for our 
application? Does spark provide such configs for achieving that goal? 

We know that this is trickle to get it working. Just want to know that how could this be resolved, or from other possible channel for
we did not cover.

Expecting for your kind advice.

Thanks,
Sun.



fightfate@163.com
 
From: Patrick Wendell
Date: 2015-02-12 16:12
To: fightfate@163.com
CC: user; dev
Subject: Re: Re: Sort Shuffle performance issues about using AppendOnlyMap for large data sets
The map will start with a capacity of 64, but will grow to accommodate
new data. Are you using the groupBy operator in Spark or are you using
Spark SQL's group by? This usually happens if you are grouping or
aggregating in a way that doesn't sufficiently condense the data
created from each input partition.
 
- Patrick
 
On Wed, Feb 11, 2015 at 9:37 PM, fightfate@163.com <fightfate@163.com> wrote:
> Hi,
>
> Really have no adequate solution got for this issue. Expecting any available
> analytical rules or hints.
>
> Thanks,
> Sun.
>
> ________________________________
> fightfate@163.com
>
>
> From: fightfate@163.com
> Date: 2015-02-09 11:56
> To: user; dev
> Subject: Re: Sort Shuffle performance issues about using AppendOnlyMap for
> large data sets
> Hi,
> Problem still exists. Any experts would take a look at this?
>
> Thanks,
> Sun.
>
> ________________________________
> fightfate@163.com
>
>
> From: fightfate@163.com
> Date: 2015-02-06 17:54
> To: user; dev
> Subject: Sort Shuffle performance issues about using AppendOnlyMap for large
> data sets
> Hi, all
> Recently we had caught performance issues when using spark 1.2.0 to read
> data from hbase and do some summary work.
> Our scenario means to : read large data sets from hbase (maybe 100G+ file) ,
> form hbaseRDD, transform to schemardd,
> groupby and aggregate the data while got fewer new summary data sets,
> loading data into hbase (phoenix).
>
> Our major issue lead to : aggregate large datasets to get summary data sets
> would consume too long time (1 hour +) , while that
> should be supposed not so bad performance. We got the dump file attached and
> stacktrace from jstack like the following:
>
> From the stacktrace and dump file we can identify that processing large
> datasets would cause frequent AppendOnlyMap growing, and
> leading to huge map entrysize. We had referenced the source code of
> org.apache.spark.util.collection.AppendOnlyMap and found that
> the map had been initialized with capacity of 64. That would be too small
> for our use case.
>
> So the question is : Does anyone had encounted such issues before? How did
> that be resolved? I cannot find any jira issues for such problems and
> if someone had seen, please kindly let us know.
>
> More specified solution would goes to : Does any possibility exists for user
> defining the map capacity releatively in spark? If so, please
> tell how to achieve that.
>
> Best Thanks,
> Sun.
>
>    Thread 22432: (state = IN_JAVA)
> - org.apache.spark.util.collection.AppendOnlyMap.growTable() @bci=87,
> line=224 (Compiled frame; information may be imprecise)
> - org.apache.spark.util.collection.SizeTrackingAppendOnlyMap.growTable()
> @bci=1, line=38 (Interpreted frame)
> - org.apache.spark.util.collection.AppendOnlyMap.incrementSize() @bci=22,
> line=198 (Compiled frame)
> -
> org.apache.spark.util.collection.AppendOnlyMap.changeValue(java.lang.Object,
> scala.Function2) @bci=201, line=145 (Compiled frame)
> -
> org.apache.spark.util.collection.SizeTrackingAppendOnlyMap.changeValue(java.lang.Object,
> scala.Function2) @bci=3, line=32 (Compiled frame)
> -
> org.apache.spark.util.collection.ExternalSorter.insertAll(scala.collection.Iterator)
> @bci=141, line=205 (Compiled frame)
> -
> org.apache.spark.shuffle.sort.SortShuffleWriter.write(scala.collection.Iterator)
> @bci=74, line=58 (Interpreted frame)
> -
> org.apache.spark.scheduler.ShuffleMapTask.runTask(org.apache.spark.TaskContext)
> @bci=169, line=68 (Interpreted frame)
> -
> org.apache.spark.scheduler.ShuffleMapTask.runTask(org.apache.spark.TaskContext)
> @bci=2, line=41 (Interpreted frame)
> - org.apache.spark.scheduler.Task.run(long) @bci=77, line=56 (Interpreted
> frame)
> - org.apache.spark.executor.Executor$TaskRunner.run() @bci=310, line=196
> (Interpreted frame)
> -
> java.util.concurrent.ThreadPoolExecutor.runWorker(java.util.concurrent.ThreadPoolExecutor$Worker)
> @bci=95, line=1145 (Interpreted frame)
> - java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=5, line=615
> (Interpreted frame)
> - java.lang.Thread.run() @bci=11, line=744 (Interpreted frame)
>
>
> Thread 22431: (state = IN_JAVA)
> - org.apache.spark.util.collection.AppendOnlyMap.growTable() @bci=87,
> line=224 (Compiled frame; information may be imprecise)
> - org.apache.spark.util.collection.SizeTrackingAppendOnlyMap.growTable()
> @bci=1, line=38 (Interpreted frame)
> - org.apache.spark.util.collection.AppendOnlyMap.incrementSize() @bci=22,
> line=198 (Compiled frame)
> -
> org.apache.spark.util.collection.AppendOnlyMap.changeValue(java.lang.Object,
> scala.Function2) @bci=201, line=145 (Compiled frame)
> -
> org.apache.spark.util.collection.SizeTrackingAppendOnlyMap.changeValue(java.lang.Object,
> scala.Function2) @bci=3, line=32 (Compiled frame)
> -
> org.apache.spark.util.collection.ExternalSorter.insertAll(scala.collection.Iterator)
> @bci=141, line=205 (Compiled frame)
> -
> org.apache.spark.shuffle.sort.SortShuffleWriter.write(scala.collection.Iterator)
> @bci=74, line=58 (Interpreted frame)
> -
> org.apache.spark.scheduler.ShuffleMapTask.runTask(org.apache.spark.TaskContext)
> @bci=169, line=68 (Interpreted frame)
> -
> org.apache.spark.scheduler.ShuffleMapTask.runTask(org.apache.spark.TaskContext)
> @bci=2, line=41 (Interpreted frame)
> - org.apache.spark.scheduler.Task.run(long) @bci=77, line=56 (Interpreted
> frame)
> - org.apache.spark.executor.Executor$TaskRunner.run() @bci=310, line=196
> (Interpreted frame)
> -
> java.util.concurrent.ThreadPoolExecutor.runWorker(java.util.concurrent.ThreadPoolExecutor$Worker)
> @bci=95, line=1145 (Interpreted frame)
> - java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=5, line=615
> (Interpreted frame)
> - java.lang.Thread.run() @bci=11, line=744 (Interpreted frame)
>
>
> fightfate@163.com
> 1 attachments
> dump.png(42K) download preview
"
Sean Owen <sowen@cloudera.com>,"Thu, 12 Feb 2015 08:42:12 +0000",How to track issues that must wait for Spark 2.x in JIRA?,dev <dev@spark.apache.org>,"Patrick and I were chatting about how to handle several issues which
clearly need a fix, and are easy, but can't be implemented until a
next major release like Spark 2.x since it would change APIs.
Examples:

https://issues.apache.org/jira/browse/SPARK-3266
https://issues.apache.org/jira/browse/SPARK-3369
https://issues.apache.org/jira/browse/SPARK-4819

We could simply make version 2.0.0 in JIRA. Although straightforward,
it might imply that release planning has begun for 2.0.0.

The version could be called ""2+"" for now to better indicate its status.

There is also a ""Later"" JIRA resolution. Although resolving the above
seems a little wrong, it might be reasonable if we're sure to revisit
""Later"", well, at some well defined later. The three issues above risk
getting lost in the shuffle.

We also wondered whether using ""Later"" is good or bad. It takes items
off the radar that aren't going to be acted on anytime soon -- and
there are lots of those right now. It might send a message that these
will be revisited when they are even less likely to if resolved.

Any opinions?

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Thu, 12 Feb 2015 00:47:16 -0800",Re: How to track issues that must wait for Spark 2.x in JIRA?,Sean Owen <sowen@cloudera.com>,"to 2.0, we can retag those that are not going to be fixed in 2.0 as 2.0.1
or 2.1.0 .


"
Patrick Wendell <pwendell@gmail.com>,"Thu, 12 Feb 2015 00:54:54 -0800",Re: How to track issues that must wait for Spark 2.x in JIRA?,Reynold Xin <rxin@databricks.com>,"Yeah my preferred is also having a more open ended ""2+"" for issues
that are clearly desirable but blocked by compatibility concerns.

What I would really want to avoid is major feature proposals sitting
around in our JIRA and tagged under some 2.X version. IMO JIRA isn't
the place for thoughts about very-long-term things. When we get these,
I'd be include to either close them as ""won't fix"" or ""later"".


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Thu, 12 Feb 2015 10:54:03 +0000",Re: How to track issues that must wait for Spark 2.x in JIRA?,Patrick Wendell <pwendell@gmail.com>,"Let me start with a version ""2+"" tag and at least write in the
description that it's only for issues that are clearly to be fixed,
but must wait until 2.x.


---------------------------------------------------------------------


"
James <alcaid1801@gmail.com>,"Thu, 12 Feb 2015 22:26:50 +0800",Why a program would receive null from send message of mapReduceTriplets,dev@spark.apache.org,"Hello,

When I am running the code on a much bigger size graph, I met
NullPointerException.

I found that is because the sendMessage() function receive a triplet that
edge.srcAttr or edge.dstAttr is null. Thus I wonder why it will happen as I
am sure every vertices have a attr.

Any returns is appreciated.

Alcaid


2015-02-11 19:30 GMT+08:00 James <alcaid1801@gmail.com>:

"
vha14 <vha14@msn.com>,"Thu, 12 Feb 2015 09:56:12 -0700 (MST)",Spark SQL value proposition in batch pipelines,dev@spark.apache.org,"My team is building a batch data processing pipeline using Spark API and
trying to understand if Spark SQL can help us. Below are what we found so
far:

- SQL's declarative style may be more readable in some cases (e.g. joining
of more than two RDDs), although some devs prefer the fluent style
regardless. 
- Cogrouping of more than 4 RDDs is not supported and it's not clear if
Spark SQL supports joining of arbitrary number of RDDs.
- It seems that Spark SQL's features such as optimization based on predicate
pushdown and dynamic schema inference are less applicable in a batch
environment.

Your inputs/suggestions are most welcome!

Thanks,
Vu Ha
CTO, Semantic Scholar
http://www.quora.com/What-is-Semantic-Scholar-and-how-will-it-work



--

---------------------------------------------------------------------


"
"""Evan R. Sparks"" <evan.sparks@gmail.com>","Thu, 12 Feb 2015 09:29:55 -0800",Re: Spark SQL value proposition in batch pipelines,vha14 <vha14@msn.com>,"Well, you can always join as many RDDs as you want by chaining them
together, e.g. a.join(b).join(c)... - I probably wouldn't join thousands of
RDDs in this way but 10 is probably doable.

That said - SparkSQL has an optimizer under the covers that can make clever
decisions e.g. pushing the predicates in the WHERE clause down to the base
data (even to external data sources if you have them), ordering joins, and
choosing between join implementations (like using broadcast joins instead
of the default shuffle-based hash join in RDD.join). These decisions can
make your queries run orders of magnitude faster than they would if you
implemented them using basic RDD transformations. The best part is at this
stage, I'd expect the optimizer will continue to improve - meaning many of
your queries will get faster with each new release.

I'm sure the SparkSQL devs can enumerate many other benefits - but as soon
as you're working with multiple tables and doing fairly textbook SQL stuff
- you likely want the engine figuring this stuff out for you rather than
hand coding it yourself. That said - with Spark, you can always drop back
to plain old RDDs and use map/reduce/filter/cogroup, etc. when you need to.


"
Reynold Xin <rxin@databricks.com>,"Thu, 12 Feb 2015 11:41:01 -0800",Re: Spark SQL value proposition in batch pipelines,"""Evan R. Sparks"" <evan.sparks@gmail.com>","Evan articulated it well.



"
vha14 <vha14@msn.com>,"Thu, 12 Feb 2015 15:29:29 -0700 (MST)",Re: Spark SQL value proposition in batch pipelines,dev@spark.apache.org,"This is super helpful, thanks Evan and Reynold!




--

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Thu, 12 Feb 2015 14:52:39 -0800",Re: Why a program would receive null from send message of mapReduceTriplets,James <alcaid1801@gmail.com>,"Can you use the new aggregateNeighbors method? I suspect the null is coming
from ""automatic join elimination"", which detects bytecode to see if you
need the src or dst vertex data. Occasionally it can fail to detect. In the
new aggregateNeighbors API, the caller needs to explicitly specifying that,
making it more robust.



"
vha14 <vha14@msn.com>,"Thu, 12 Feb 2015 16:53:22 -0700 (MST)",Re: renaming SchemaRDD -> DataFrame,dev@spark.apache.org,"Matei wrote (Jan 26, 2015; 5:31pm): ""The intent of Spark SQL though is to be
more than a SQL server -- it's meant to be a library for manipulating
structured data.""

I think this is an important but nuanced point. There are engineers who for
various reasons associate the term ""SQL"" with business analyst,
non-engineering scenarios. If Matei or someone from the Spark team could
clarify this misunderstanding with respect to the potential of Spark SQL, it
would be very useful. (Potential places: Quora, StackOverflow,
Databricks.com's blog.) 

Thanks,
Vu Ha 
CTO, Semantic Scholar
http://www.quora.com/What-is-Semantic-Scholar-and-how-will-it-work



--

---------------------------------------------------------------------


"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Fri, 13 Feb 2015 00:18:01 +0000",RE: Using CUDA within Spark / boosting linear algebra,"""Evan R. Sparks"" <evan.sparks@gmail.com>","Just to summarize this thread, I was finally able to make all performance comparisons that we discussed. It turns out that: 
BIDMat-cublas>>BIDMat MKL==netlib-mkl==netlib-openblas-compiled>netlib-openblas-yum-repo==netlib-cublas>netlib-blas>f2jblas

Below is the link to the spreadsheet with full results. 
https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J5r7kwKSPkY/edit?usp=sharing

One thing still needs exploration: does BIDMat-cublas perform copying to/from machine‚Äôs RAM?
 
-----Original Message-----
From: Ulanov, Alexander 
Sent: Tuesday, February 10, 2015 2:12 PM
To: Evan R. Sparks
Cc: Joseph Bradley; dev@spark.apache.org
Subject: RE: Using CUDA within Spark / boosting linear algebra

Thanks, Evan! It seems that ticket was marked as duplicate though the original one discusses slightly different topic. I was able to link netlib with MKL from BIDMat binaries. Indeed, MKL is statically linked inside a 60MB library.

|A*B  size | BIDMat MKL | Breeze+Netlib-MKL  from BIDMat| Breeze+Netlib-OpenBlas(native system)| Breeze+Netlib-f2jblas |
+-----------------------------------------------------------------------+
|100x100*100x100 | 0,00205596 | 0,000381 | 0,03810324 | 0,002556 |
|1000x1000*1000x1000 | 0,018320947 | 0,038316857 | 0,51803557 |1,638475459 |
|10000x10000*10000x10000 | 23,78046632 | 32,94546697 |445,0935211 | 1569,233228 |

It turn out that pre-compiled MKL is faster than precompiled OpenBlas on my machine. Probably, I‚Äôll add two more columns with locally compiled openblas and cuda.

Alexander

From: Evan R. Sparks [mailto:evan.sparks@gmail.com]
Sent: Monday, February 09, 2015 6:06 PM
To: Ulanov, Alexander
Cc: Joseph Bradley; dev@spark.apache.org
Subject: Re: Using CUDA within Spark / boosting linear algebra

Great - perhaps we can move this discussion off-list and onto a JIRA ticket? (Here's one: https://issues.apache.org/jira/browse/SPARK-5705)

It seems like this is going to be somewhat exploratory for a while (and there's probably only a handful of us who really care about fast linear algebra!)

- Evan

On Mon, Feb 9, 2015 at 4:48 PM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
Hi Evan,

Thank you for explanation and useful link. I am going to build OpenBLAS, link it with Netlib-java and perform benchmark again.

Do I understand correctly that BIDMat binaries contain statically linked Intel MKL BLAS? It might be the reason why I am able to run BIDMat not having MKL BLAS installed on my server. If it is true, I wonder if it is OK because Intel sells this library. Nevertheless, it seems that in my case precompiled MKL BLAS performs better than precompiled OpenBLAS given that BIDMat and Netlib-java are supposed to be on par with JNI overheads.

Though, it might be interesting to link Netlib-java with Intel MKL, as you suggested. I wonder, are John Canny (BIDMat) and Sam Halliday (Netlib-java) interested to compare their libraries.

Best regards, Alexander

From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>]
Sent: Friday, February 06, 2015 5:58 PM

To: Ulanov, Alexander
Cc: Joseph Bradley; dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: Using CUDA within Spark / boosting linear algebra

I would build OpenBLAS yourself, since good BLAS performance comes from getting cache sizes, etc. set up correctly for your particular hardware - this is often a very tricky process (see, e.g. ATLAS), but we found that on relatively modern Xeon chips, OpenBLAS builds quickly and yields performance competitive with MKL.

To make sure the right library is getting used, you have to make sure it's first on the search path - export LD_LIBRARY_PATH=/path/to/blas/library.so will do the trick here.

For some examples of getting netlib-java setup on an ec2 node and some example benchmarking code we ran a while back, see: https://github.com/shivaram/matrix-bench

In particular - build-openblas-ec2.sh shows you how to build the library and set up symlinks correctly, and scala/run-netlib.sh shows you how to get the path setup and get that library picked up by netlib-java.

In this way - you could probably get cuBLAS set up to be used by netlib-java as well.

- Evan

On Fri, Feb 6, 2015 at 5:43 PM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
Evan, could you elaborate on how to force BIDMat and netlib-java to force loading the right blas? For netlib, I there are few JVM flags, such as -Dcom.github.fommil.netlib.BLAS=com.github.fommil.netlib.F2jBLAS, so I can force it to use Java implementation. Not sure I understand how to force use a specific blas (not specific wrapper for blas).

Btw. I have installed openblas (yum install openblas), so I suppose that netlib is using it.

From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>]
Sent: Friday, February 06, 2015 5:19 PM
To: Ulanov, Alexander
Cc: Joseph Bradley; dev@spark.apache.org<mailto:dev@spark.apache.org>

Subject: Re: Using CUDA within Spark / boosting linear algebra

Getting breeze to pick up the right blas library is critical for performance. I recommend using OpenBLAS (or MKL, if you already have it). It might make sense to force BIDMat to use the same underlying BLAS library as well.

On Fri, Feb 6, 2015 at 4:42 PM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
Hi Evan, Joseph

I did few matrix multiplication test and BIDMat seems to be ~10x faster than netlib-java+breeze (sorry for weird table formatting):

|A*B  size | BIDMat MKL | Breeze+Netlib-java native_system_linux_x86-64| Breeze+Netlib-java f2jblas |
+-----------------------------------------------------------------------+
|100x100*100x100 | 0,00205596 | 0,03810324 | 0,002556 |
|1000x1000*1000x1000 | 0,018320947 | 0,51803557 |1,638475459 |
|10000x10000*10000x10000 | 23,78046632 | 445,0935211 | 1569,233228 |

Configuration: Intel(R) Xeon(R) CPU E31240 3.3 GHz, 6GB RAM, Fedora 19 Linux, Scala 2.11.

Later I will make tests with Cuda. I need to install new Cuda version for this purpose.

Do you have any ideas why breeze-netlib with native blas is so much slower than BIDMat MKL?

Best regards, Alexander

From: Joseph Bradley [mailto:joseph@databricks.com<mailto:joseph@databricks.com>]
Sent: Thursday, February 05, 2015 5:29 PM
To: Ulanov, Alexander
Cc: Evan R. Sparks; dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: Using CUDA within Spark / boosting linear algebra

Hi Alexander,

Using GPUs with Spark would be very exciting.  Small comment: Concerning your question earlier about keeping data stored on the GPU rather than having to move it between main memory and GPU memory on each iteration, I would guess this would be critical to getting good performance.  If you could do multiple local iterations before aggregating results, then the cost of data movement to the GPU could be amortized (and I believe that is done in practice).  Having Spark be aware of the GPU and using it as another part of memory sounds like a much bigger undertaking.

Joseph

On Thu, Feb 5, 2015 at 4:59 PM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
Thank you for explanation! I‚Äôve watched the BIDMach presentation by John Canny and I am really inspired by his talk and comparisons with Spark MLlib.

I am very interested to find out what will be better within Spark: BIDMat or netlib-java with CPU or GPU natives. Could you suggest a fair way to benchmark them? Currently I do benchmarks on artificial neural networks in batch mode. While it is not a ‚Äúpure‚Äù test of linear algebra, it involves some other things that are essential to machine learning.

From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>]
Sent: Thursday, February 05, 2015 1:29 PM
To: Ulanov, Alexander
Cc: dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: Using CUDA within Spark / boosting linear algebra

I'd be surprised of BIDMat+OpenBLAS was significantly faster than netlib-java+OpenBLAS, but if it is much faster it's probably due to data layout and fewer levels of indirection - it's definitely a worthwhile experiment to run. The main speedups I've seen from using it come from highly optimized GPU code for linear algebra. I know that in the past Canny has gone as far as to write custom GPU kernels for performance-critical regions of code.[1]

BIDMach is highly optimized for single node performance or performance on small clusters.[2] Once data doesn't fit easily in GPU memory (or can be batched in that way) the performance tends to fall off. Canny argues for hardware/software codesign and as such prefers machine configurations that are quite different than what we find in most commodity cluster nodes - e.g. 10 disk cahnnels and 4 GPUs.

In contrast, MLlib was designed for horizontal scalability on commodity clusters and works best on very big datasets - order of terabytes.

For the most part, these projects developed concurrently to address slightly different use cases. That said, there may be bits of BIDMach we could repurpose for MLlib - keep in mind we need to be careful about maintaining cross-language compatibility for our Java and Python-users, though.

- Evan

[1] - http://arxiv.org/abs/1409.5402
[2] - http://eecs.berkeley.edu/~hzhao/papers/BD.pdf

On Thu, Feb 5, 2015 at 1:00 PM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
Hi Evan,

Thank you for suggestion! BIDMat seems to have terrific speed. Do you know what makes them faster than netlib-java?

The same group has BIDMach library that implements machine learning. For some examples they use Caffe convolutional neural network library owned by another group in Berkeley. Could you elaborate on how these all might be connected with Spark Mllib? If you take BIDMat for linear algebra why don‚Äôt you take BIDMach for optimization and learning?

Best regards, Alexander

From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
Sent: Thursday, February 05, 2015 12:09 PM
To: Ulanov, Alexander
Cc: dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.apache.org<mailto:dev@spark.apache.org>>
Subject: Re: Using CUDA within Spark / boosting linear algebra

I'd expect that we can make GPU-accelerated BLAS faster than CPU blas in many cases.

You might consider taking a look at the codepaths that BIDMat (https://github.com/BIDData/BIDMat) takes and comparing them to netlib-java/breeze. John Canny et. al. have done a bunch of work optimizing to make this work really fast from Scala. I've run it on my laptop and compared to MKL and in certain cases it's 10x faster at matrix multiply. There are a lot of layers of indirection here and you really want to avoid data copying as much as possible.

We could also consider swapping out BIDMat for Breeze, but that would be a big project and if we can figure out how to get breeze+cublas to comparable performance that would be a big win.

On Thu, Feb 5, 2015 at 11:55 AM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
Dear Spark developers,

I am exploring how to make linear algebra operations faster within Spark. One way of doing this is to use Scala Breeze library that is bundled with Spark. For matrix operations, it employs Netlib-java that has a Java wrapper for BLAS (basic linear algebra subprograms) and LAPACK native binaries if they are available on the worker node. It also has its own optimized Java implementation of BLAS. It is worth mentioning, that native binaries provide better performance only for BLAS level 3, i.e. matrix-matrix operations or general matrix multiplication (GEMM). This is confirmed by GEMM test on Netlib-java page https://github.com/fommil/netlib-java. I also confirmed it with my experiments with training of artificial neural network https://github.com/apache/spark/pull/1290#issuecomment-70313952. However, I would like to boost performance more.

GPU is supposed to work fast with linear algebra and there is Nvidia CUDA implementation of BLAS, called cublas. I have one Linux server with Nvidia GPU and I was able to do the following. I linked cublas (instead of cpu-based blas) with Netlib-java wrapper and put it into Spark, so Breeze/Netlib is using it. Then I did some performance measurements with regards to artificial neural network batch learning in Spark MLlib that involves matrix-matrix multiplications. It turns out that for matrices of size less than ~1000x780 GPU cublas has the same speed as CPU blas. Cublas becomes slower for bigger matrices. It worth mentioning that it is was not a test for ONLY multiplication since there are other operations involved. One of the reasons for slowdown might be the overhead of copying the matrices from computer memory to graphic card memory and back.

So, few questions:
1) Do these results with CUDA make sense?
2) If the problem is with copy overhead, are there any libraries that allow to force intermediate results to stay in graphic card memory thus removing the overhead?
3) Any other options to speed-up linear algebra in Spark?

Thank you, Alexander

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org><mailto:dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org>>
For additional commands, e-mail: dev-help@spark.apache.org<mailto:dev-help@spark.apache.org><mailto:dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>>



"
Reynold Xin <rxin@databricks.com>,"Thu, 12 Feb 2015 22:50:13 -0800",Re: Why a program would receive null from send message of mapReduceTriplets,James <alcaid1801@gmail.com>,"Then maybe you actually had a null in your vertex attribute?



"
James <alcaid1801@gmail.com>,"Fri, 13 Feb 2015 14:47:53 +0800",Re: Why a program would receive null from send message of mapReduceTriplets,Reynold Xin <rxin@databricks.com>,"I changed the mapReduceTriplets() func to aggregateMessages(), but it still
failed.


2015-02-13 6:52 GMT+08:00 Reynold Xin <rxin@databricks.com>:

"
James <alcaid1801@gmail.com>,"Fri, 13 Feb 2015 14:57:26 +0800",Re: Why a program would receive null from send message of mapReduceTriplets,Reynold Xin <rxin@databricks.com>,"I am trying to run the data on spark-shell mode to find whether there is
something wrong in the code or data. As I could only reproduce the error on
a 50B edge graph.

2015-02-13 14:50 GMT+08:00 Reynold Xin <rxin@databricks.com>:

"
Mick Davies <michael.belldavies@gmail.com>,"Fri, 13 Feb 2015 02:40:24 -0700 (MST)",Re: Optimize encoding/decoding strings when using Parquet,dev@spark.apache.org,"I have put in a PR on Parquet to support dictionaries when filters are pushed
down, which should reduce binary conversion overhear when Spark pushes down
string predicates on columns that are dictionary encoded.

https://github.com/apache/incubator-parquet-mr/pull/117

It's blocked at the moment as I part of my parquet build fails on my Mac due
to issue getting thrift 0.7 installed. Installation instructions available
on Parquet do not seem to work I think due to this issue
https://issues.apache.org/jira/browse/THRIFT-2229
<https://issues.apache.org/jira/browse/THRIFT-2229>.

This is not directly related to Spark but I wondered if anyone has got
thrift 0.7 working on Mac Yosemite 10.0, or can suggest a work round.



--

---------------------------------------------------------------------


"
Mick Davies <michael.belldavies@gmail.com>,"Fri, 13 Feb 2015 02:54:45 -0700 (MST)",Re: Caching tables at column level,dev@spark.apache.org,"Thanks - we have tried this and it works nicely.



--

---------------------------------------------------------------------


"
James <alcaid1801@gmail.com>,"Fri, 13 Feb 2015 21:35:07 +0800",Re: Why a program would receive null from send message of mapReduceTriplets,Reynold Xin <rxin@databricks.com>,"I have a question:

*How could the attributes of triplets of a graph get update after
mapVertices() func? *

My code

```
// Initial the graph, assign a counter to each vertex that contains the
vertex id only
var anfGraph = graph.mapVertices { case (vid, _) =>
  val counter = new HyperLogLog(5)
  counter.offer(vid)
  counter
}

val nullVertex = anfGraph.triplets.filter(edge => edge.srcAttr ==
null).first

anfGraph.vertices.filter(_._1 == nullVertex).first
// I could see that the vertex has a not null attribute

// messages = anfGraph.aggregateMessages(msgFun, mergeMessage)   // <-
NullPointerException

```

I could found that some vertex attributes in some triplets are null, but
not all.


Alcaid


2015-02-13 14:50 GMT+08:00 Reynold Xin <rxin@databricks.com>:

"
Debasish Das <debasish.das83@gmail.com>,"Fri, 13 Feb 2015 07:46:41 -0800",mllib.recommendation Design,dev <dev@spark.apache.org>,"Hi,

I am bit confused on the mllib design in the master. I thought that core
algorithms will stay in mllib and ml will define the pipelines over the
core algorithm but looks like in master ALS is moved from mllib to ml...

I am refactoring my PR to a factorization package and I want to build it on
top of ml.recommendation.ALS (possibly extend from ml.recommendation.ALS
since first version will use very similar RDD handling as ALS and a
proximal solver that's being added to breeze)

https://issues.apache.org/jira/browse/SPARK-2426
https://github.com/scalanlp/breeze/pull/321

Basically I am not sure if we should merge it with recommendation.ALS since
this is more generic than recommendation. I am considering calling it
ConstrainedALS where user can specify different constraint for user and
product factors (Similar to GraphLab CF structure).

I am also working on ConstrainedALM where the underlying algorithm is no
longer ALS but nonlinear alternating minimization with constraints.
https://github.com/scalanlp/breeze/pull/364
This will let us do large rank matrix completion where there is no need to
construct gram matrices. I will open up the JIRA soon after getting initial
results

I am bit confused that where should I add the factorization package. It
will use the current ALS test-cases and I have to construct more test-cases
for sparse coding and PLSA formulations.

Thanks.
Deb
"
"""Mattmann, Chris A (3980)"" <chris.a.mattmann@jpl.nasa.gov>","Fri, 13 Feb 2015 20:14:50 +0000",FW: Trouble posting to the list,"""dev@spark.apache.org"" <dev@spark.apache.org>","FYI

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Chris Mattmann, Ph.D.
Chief Architect
Instrument Software and Science Data Systems Section (398)
NASA Jet Propulsion Laboratory Pasadena, CA 91109 USA
Office: 168-519, Mailstop: 168-527
Email: chris.a.mattmann@nasa.gov
WWW:  http://sunset.usc.edu/~mattmann/
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Adjunct Associate Professor, Computer Science Department
University of Southern California, Los Angeles, CA 90089 USA
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++






-----Original Message-----
From: Dima Zhiyanov <dimazhiyanov@hotmail.com>
Date: Thursday, February 12, 2015 at 7:04 AM
To: ""user-owner@spark.apache.org"" <user-owner@spark.apache.org>
Subject: Trouble posting to the list

>Hello
>
>After numerous attempts I am still unable to post to the list. After I
>click Subscribe I do not get an e-mail which allows me to confirm my
>subscription. Could you please add me manually?
>
>Thanks a lot
>Dima
>
>Sent from my iPhone


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sat, 14 Feb 2015 07:08:09 +0000",Re: Building Spark with Pants,Spark dev list <dev@spark.apache.org>,"FYI: Here is the matching discussion over on the Pants dev list.
<https://groups.google.com/forum/#!topic/pants-devel/rTaU-iIOIFE>

m

To reiterate, I'm asking from an experimental perspective. I'm not
s
e
ks
t
so
tly
e
"
Olivier Girardot <ssaboum@gmail.com>,"Sat, 14 Feb 2015 17:57:31 +0100",Re: Build spark failed with maven,Yi Tian <tianyi.asiainfo@gmail.com>,"Hi,
this was not reproduced for me, what kind of jdk are you using for the zinc
server ?

Regards,

Olivier.

2015-02-11 5:08 GMT+01:00 Yi Tian <tianyi.asiainfo@gmail.com>:

------
------
-catalyst_2.10 ---
 spark-catalyst_2.10 ---
catalyst/src/main/scala added.
talyst_2.10 ---
rk-catalyst_2.10 ---
/apache-spark/sql/catalyst/src/main/resources
-catalyst_2.10 ---
.1,null)
hub/community/apache-spark/sql/catalyst/target/scala-2.10/classes...[error] /Users/tianyi/github/community/apache-spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/dsl/package.scala:314: polymorphic expression cannot be instantiated to expected type;
ScalaUdfBuilder[T(in method apply)]
der[T(in method functionToUdfBuilder)]
, T]): ScalaUdfBuilder[T] = ScalaUdfBuilder(func)
"
The Watcher <watcherfr@gmail.com>,"Sun, 15 Feb 2015 12:03:41 +0100",Spark & Hive,dev@spark.apache.org,"I'm a little confused around Hive & Spark, can someone shed some light ?

Using Spark, I can access the Hive metastore and run Hive queries. Since I
am able to do this in stand-alone mode, it can't be using map-reduce to run
the Hive queries and I suppose it's building a query plan and executing it
all in Spark.

So, is this the same as
https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark%3A+Getting+Started
?
If not, why not and aren't they likely to merge at some point ?

If Spark really builds its own query plan, joins, etc without Hive's then
is everything that requires special SQL syntax in Hive supported : window
functions, cubes, rollups, skewed tables, etc

Thanks
"
vha14 <vha14@msn.com>,"Sun, 15 Feb 2015 13:15:58 -0700 (MST)",Re: A Spark Compilation Question,dev@spark.apache.org,"In IntelliJ:

- Open View -> Tool Windows -> Maven Projects
- Right click on Spark Project External Flume Sink
- Click Generate Sources and Update Folders

This should generate source code from sparkflume.avdl.

Vu~



--

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Sun, 15 Feb 2015 20:05:45 -0800",Re: Spark & Hive,The Watcher <watcherfr@gmail.com>,"Spark SQL is not the same as Hive on Spark.

Spark SQL is a query engine that is designed from ground up for Spark
without the historic baggage of Hive. It also does more than SQL now -- it
is meant for structured data processing (e.g. the new DataFrame API) and
SQL. Spark SQL is mostly compatible with Hive, but 100% compatibility is
not a goal (nor desired, since Hive has a lot of weird SQL semantics in the
course of its evolution).

Hive on Spark is meant to replace Hive's MapReduce runtime with Spark's.

For more information, see this blog post:
https://databricks.com/blog/2014/07/01/shark-spark-sql-hive-on-spark-and-the-future-of-sql-on-spark.html




"
Niranda Perera <niranda.perera@gmail.com>,"Mon, 16 Feb 2015 10:38:50 +0530",Replacing Jetty with TomCat,dev@spark.apache.org,"Hi,

We are thinking of integrating Spark server inside a product. Our current
product uses Tomcat as its webserver.

Is it possible to switch the Jetty webserver in Spark to Tomcat
off-the-shelf?

Cheers

-- 
Niranda
"
Reynold Xin <rxin@databricks.com>,"Sun, 15 Feb 2015 21:17:29 -0800",Re: Replacing Jetty with TomCat,Niranda Perera <niranda.perera@gmail.com>,"Most likely no. We are using the embedded mode of Jetty, rather than using
servlets.

Even if it is possible, you probably wouldn't want to embed Spark in your
application server ...



"
Niranda Perera <niranda.perera@gmail.com>,"Mon, 16 Feb 2015 10:54:30 +0530",Re: Replacing Jetty with TomCat,Reynold Xin <rxin@databricks.com>,"Hi Reynold,

Thank you for the response. Could you please clarify the need of Jetty
server inside Spark? Is it used for Spark core functionality or is it there
for Spark jobs UI purposes?

cheers




-- 
Niranda
"
Reynold Xin <rxin@databricks.com>,"Sun, 15 Feb 2015 21:27:58 -0800",Re: Replacing Jetty with TomCat,Niranda Perera <niranda.perera@gmail.com>,"Mostly UI.

However, we are also using Jetty as a file server I believe (for
distributing files from the driver to workers).



"
Sean Owen <sowen@cloudera.com>,"Mon, 16 Feb 2015 11:21:36 +0000",Re: Replacing Jetty with TomCat,Niranda Perera <niranda.perera@gmail.com>,"There's no particular reason you have to remove the embedded Jetty
server, right? it doesn't prevent you from using it inside another app
that happens to run in Tomcat. You won't be able to switch it out
without rewriting a fair bit of code, no, but you don't need to.


---------------------------------------------------------------------


"
"""Haopu Wang"" <HWang@qilinsoft.com>","Mon, 16 Feb 2015 20:27:36 +0800",HiveContext cannot be serialized,<dev@spark.apache.org>,"When I'm investigating this issue (in the end of this email), I take a
look at HiveContext's code and find this change
(https://github.com/apache/spark/commit/64945f868443fbc59cb34b34c16d782d
da0fb63d#diff-ff50aea397a607b79df9bec6f2a841db):

 

-  @transient protected[hive] lazy val hiveconf = new
HiveConf(classOf[SessionState])

-  @transient protected[hive] lazy val sessionState = {

-    val ss = new SessionState(hiveconf)

-    setConf(hiveconf.getAllProperties)  // Have SQLConf pick up the
initial set of HiveConf.

-    ss

-  }

+  @transient protected[hive] lazy val (hiveconf, sessionState) =

+    Option(SessionState.get())

+      .orElse {

 

With the new change, Scala compiler always generate a Tuple2 field of
HiveContext as below:

 

    private Tuple2 x$3;

    private transient OutputStream outputBuffer;

    private transient HiveConf hiveconf;

    private transient SessionState sessionState;

    private transient HiveMetastoreCatalog catalog;

 

That ""x$3"" field's key is HiveConf object that cannot be serialized. So
can you suggest how to resolve this issue? Thank you very much!

 

================================

 

I have a streaming application which registered temp table on a
HiveContext for each batch duration.

The application runs well in Spark 1.1.0. But I get below error from
1.1.1.

Do you have any suggestions to resolve it? Thank you!

 

java.io.NotSerializableException: org.apache.hadoop.hive.conf.HiveConf

    - field (class ""scala.Tuple2"", name: ""_1"", type: ""class
java.lang.Object"")

    - object (class ""scala.Tuple2"", (Configuration: core-default.xml,
core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml,
yarn-site.xml, hdfs-default.xml, hdfs-site.xml,
org.apache.hadoop.hive.conf.LoopingByteArrayInputStream@2158ce23,org.apa
che.hadoop.hive.ql.session.SessionState@49b6eef9))

    - field (class ""org.apache.spark.sql.hive.HiveContext"", name: ""x$3"",
type: ""class scala.Tuple2"")

    - object (class ""org.apache.spark.sql.hive.HiveContext"",
org.apache.spark.sql.hive.HiveContext@4e6e66a4)

    - field (class
""example.BaseQueryableDStream$$anonfun$registerTempTable$2"", name:
""sqlContext$1"", type: ""class org.apache.spark.sql.SQLContext"")

   - object (class
""example.BaseQueryableDStream$$anonfun$registerTempTable$2"",
<function1>)

    - field (class
""org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1"",
name: ""foreachFunc$1"", type: ""interface scala.Function1"")

    - object (class
""org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1"",
<function2>)

    - field (class ""org.apache.spark.streaming.dstream.ForEachDStream"",
name: ""org$apache$spark$streaming$dstream$ForEachDStream$$foreachFunc"",
type: ""interface scala.Function2"")

    - object (class ""org.apache.spark.streaming.dstream.ForEachDStream"",
org.apache.spark.streaming.dstream.ForEachDStream@5ccbdc20)

    - element of array (index: 0)

    - array (class ""[Ljava.lang.Object;"", size: 16)

    - field (class ""scala.collection.mutable.ArrayBuffer"", name:
""array"", type: ""class [Ljava.lang.Object;"")

    - object (class ""scala.collection.mutable.ArrayBuffer"",
ArrayBuffer(org.apache.spark.streaming.dstream.ForEachDStream@5ccbdc20))

    - field (class ""org.apache.spark.streaming.DStreamGraph"", name:
""outputStreams"", type: ""class scala.collection.mutable.ArrayBuffer"")

    - custom writeObject data (class
""org.apache.spark.streaming.DStreamGraph"")

    - object (class ""org.apache.spark.streaming.DStreamGraph"",
org.apache.spark.streaming.DStreamGraph@776ae7da)

    - field (class ""org.apache.spark.streaming.Checkpoint"", name:
""graph"", type: ""class org.apache.spark.streaming.DStreamGraph"")

    - root object (class ""org.apache.spark.streaming.Checkpoint"",
org.apache.spark.streaming.Checkpoint@5eade065)

    at java.io.ObjectOutputStream.writeObject0(Unknown Source)

 

 

 

"
=?utf-8?Q?Mark_Payne?= <mapayn3@g.uky.edu>,"Mon, 16 Feb 2015 16:22:08 +0000",=?utf-8?Q?Spark_Receivers?=,"""=?utf-8?Q?dev@spark.apache.org?="" <dev@spark.apache.org>","Hello,


I am one of the committers for Apache NiFi (incubating). I am looking to integrate NiFi with Spark streaming. I have created a custom Receiver to receive data from NiFi. I‚Äôve tested it locally, and things seem to work well.


I feel it would make more sense to have the NiFi Receiver in the Spark codebase along side the code for Flume, Kafka, etc., as this is where people are more likely to look to see what integrations are available. Looking there, though, it seems that all of those are ‚Äúfully integrated‚Äù into Spark, rather than being simple Receivers.


Is Spark interested in housing the code for Receivers to interact with other services, or should this just reside in the NiFi codebase?


Thanks for any pointers

-Mark"
Michael Armbrust <michael@databricks.com>,"Mon, 16 Feb 2015 10:41:53 -0800",Re: HiveContext cannot be serialized,Haopu Wang <HWang@qilinsoft.com>,"I'd suggest marking the HiveContext as @transient since its not valid to
use it on the slaves anyway.


"
Ryan Williams <ryan.blake.williams@gmail.com>,"Mon, 16 Feb 2015 18:46:18 +0000",Re: Building Spark with Pants,"Nicholas Chammas <nicholas.chammas@gmail.com>, Spark dev list <dev@spark.apache.org>","I worked on Pants at Foursquare for a while and when coming up to speed on
Spark was interested in the possibility of building it with Pants,
particularly because allowing developers to share/reuse each others'
compilation artifacts seems like it would be a boon to productivity; that
was/is Pants' ""killer feature"" for Foursquare, as mentioned on the
pants-devel thread.

Given the monumental nature of the task of making Spark build with Pants,
most of my enthusiasm was deflected to SPARK-1517
<https://issues.apache.org/jira/browse/SPARK-1517>, which deals with
publishing nightly builds (or better, exposing all assembly JARs built by
Jenkins?) that people could use rather than having to assemble their own.

Anyway, it's an intriguing idea, Nicholas, I'm glad you are pursuing it!


s
an
"
Reynold Xin <rxin@databricks.com>,"Mon, 16 Feb 2015 10:47:09 -0800",Re: HiveContext cannot be serialized,Michael Armbrust <michael@databricks.com>,"Michael - it is already transient. This should probably considered a bug in
the scala compiler, but we can easily work around it by removing the use of
destructuring binding.


"
Michael Armbrust <michael@databricks.com>,"Mon, 16 Feb 2015 10:59:15 -0800",Re: HiveContext cannot be serialized,Reynold Xin <rxin@databricks.com>,"I was suggesting you mark the variable that is holding the HiveContext
'@transient' since the scala compiler is not correctly propagating this
through the tuple extraction.  This is only a workaround.  We can also
remove the tuple extraction.


"
Reynold Xin <rxin@databricks.com>,"Mon, 16 Feb 2015 11:43:30 -0800",Re: HiveContext cannot be serialized,Michael Armbrust <michael@databricks.com>,"I submitted a patch

https://github.com/apache/spark/pull/4628


"
"""Haopu Wang"" <HWang@qilinsoft.com>","Tue, 17 Feb 2015 08:44:54 +0800",RE: HiveContext cannot be serialized,"""Reynold Xin"" <rxin@databricks.com>,
	""Michael Armbrust"" <michael@databricks.com>","Reynold and Michael, thank you so much for the quick response.

 

This problem also happens on branch-1.1, would you mind resolving it on
branch-1.1 also? Thanks again!

 

________________________________

From: Reynold Xin [mailto:rxin@databricks.com] 
Sent: Tuesday, February 17, 2015 3:44 AM
To: Michael Armbrust
Cc: Haopu Wang; dev@spark.apache.org
Subject: Re: HiveContext cannot be serialized

 

I submitted a patch

 

https://github.com/apache/spark/pull/4628

 


I was suggesting you mark the variable that is holding the HiveContext
'@transient' since the scala compiler is not correctly propagating this
through the tuple extraction.  This is only a workaround.  We can also
remove the tuple extraction.

 


Michael - it is already transient. This should probably considered a bug
in the scala compiler, but we can easily work around it by removing the
use of destructuring binding.

 


I'd suggest marking the HiveContext as @transient since its not valid to
use it on the slaves anyway.



(https://github.com/apache/spark/commit/64945f868443fbc59cb34b34c16d782d
So
================================
org.apache.hadoop.hive.conf.LoopingByteArrayInputStream@2158ce23,org.apa
""x$3"",
""org.apache.spark.streaming.dstream.ForEachDStream"",
""org$apache$spark$streaming$dstream$ForEachDStream$$foreachFunc"",
""org.apache.spark.streaming.dstream.ForEachDStream"",
ArrayBuffer(org.apache.spark.streaming.dstream.ForEachDStream@5ccbdc20))

 

 

 

"
Qiuzhuang Lian <qiuzhuang.lian@gmail.com>,"Tue, 17 Feb 2015 14:39:52 +0800","org.apache.spark.sql.sources.DDLException: Unsupported dataType:
 [1.1] failure: ``varchar'' expected but identifier char found in spark-sql",dev@spark.apache.org,"Hi,

I am not sure this has been reported already or not, I run into this error
under spark-sql shell as build from newest of spark git trunk,

spark-sql> describe qiuzhuang_hcatlog_import;
15/02/17 14:38:36 ERROR SparkSQLDriver: Failed in [describe
qiuzhuang_hcatlog_import]
org.apache.spark.sql.sources.DDLException: Unsupported dataType: [1.1]
failure: ``varchar'' expected but identifier char found

char(32)
^
at org.apache.spark.sql.sources.DDLParser.parseType(ddl.scala:52)
at
org.apache.spark.sql.hive.MetastoreRelation$SchemaAttribute.toAttribute(HiveMetastoreCatalog.scala:664)
at
org.apache.spark.sql.hive.MetastoreRelation$$anonfun$23.apply(HiveMetastoreCatalog.scala:674)
at
org.apache.spark.sql.hive.MetastoreRelation$$anonfun$23.apply(HiveMetastoreCatalog.scala:674)
at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
at scala.collection.Iterator$class.foreach(Iterator.scala:727)
at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
at scala.collection.AbstractTraversable.map(Traversable.scala:105)
at
org.apache.spark.sql.hive.MetastoreRelation.<init>(HiveMetastoreCatalog.scala:674)
at
org.apache.spark.sql.hive.HiveMetastoreCatalog.lookupRelation(HiveMetastoreCatalog.scala:185)
at org.apache.spark.sql.hive.HiveContext$$anon$2.org
$apache$spark$sql$catalyst$analysis$OverrideCatalog$$super$lookupRelation(HiveContext.scala:234)

As in hive 0.131, console, this commands works,

hive> describe qiuzhuang_hcatlog_import;
OK
id                      char(32)
assistant_no            varchar(20)
assistant_name          varchar(32)
assistant_type          int
grade                   int
shop_no                 varchar(20)
shop_name               varchar(64)
organ_no                varchar(20)
organ_name              varchar(20)
entry_date              string
education               int
commission              decimal(8,2)
tel                     varchar(20)
address                 varchar(100)
identity_card           varchar(25)
sex                     int
birthday                string
employee_type           int
status                  int
remark                  varchar(255)
create_user_no          varchar(20)
create_user             varchar(32)
create_time             string
update_user_no          varchar(20)
update_user             varchar(32)
update_time             string
Time taken: 0.49 seconds, Fetched: 26 row(s)
hive>


Regards,
Qiuzhuang
"
Josh Devins <josh@soundcloud.com>,"Tue, 17 Feb 2015 15:36:17 +0100",Fwd: [MLlib] Performance problem in GeneralizedLinearAlgorithm,dev@spark.apache.org,"Cross-posting as I got no response on the users mailing list last
week. Any response would be appreciated :)

Josh


---------- Forwarded message ----------
From: Josh Devins <josh@soundcloud.com>
Date: 9 February 2015 at 15:59
Subject: [MLlib] Performance problem in GeneralizedLinearAlgorithm
To: ""user@spark.apache.org"" <user@spark.apache.org>


I've been looking into a performance problem when using
LogisticRegressionWithLBFGS (and in turn GeneralizedLinearAlgorithm).
Here's an outline of what I've figured out so far and it would be
great to get some confirmation of the problem, some input on how
wide-spread this problem might be and any ideas on a nice way to fix
this.

Context:
- I will reference `branch-1.1` as we are currently on v1.1.1 however
this appears to still be a problem on `master`
- The cluster is run on YARN, on bare-metal hardware (no VMs)
- I've not filed a Jira issue yet but can do so
- This problem affects all algorithms based on
GeneralizedLinearAlgorithm (GLA) that use feature scaling (and less so
when not, but still a problem) (e.g. LogisticRegressionWithLBFGS)

Problem Outline:
- Starting at GLA line 177
(https://github.com/apache/spark/blob/branch-1.1/mllib/src/main/scala/org/apache/spark/mllib/regression/GeneralizedLinearAlgorithm.scala#L177),
a feature scaler is created using the `input` RDD
- Refer next to line 186 which then maps over the `input` RDD and
produces a new `data` RDD
(https://github.com/apache/spark/blob/branch-1.1/mllib/src/main/scala/org/apache/spark/mllib/regression/GeneralizedLinearAlgorithm.scala#L186)
- If you are using feature scaling or adding intercepts, the user
`input` RDD has been mapped over *after* the user has persisted it
(hopefully) and *before* going into the (iterative) optimizer on line
204 (https://github.com/apache/spark/blob/branch-1.1/mllib/src/main/scala/org/apache/spark/mllib/regression/GeneralizedLinearAlgorithm.scala#L204)
- Since the RDD `data` that is iterated over in the optimizer is
unpersisted, when we are running the cost function in the optimizer
(e.g. LBFGS -- https://github.com/apache/spark/blob/branch-1.1/mllib/src/main/scala/org/apache/spark/mllib/optimization/LBFGS.scala#L198),
the map phase will actually first go back and rerun the feature
scaling (map tasks on `input`) and then map with the cost function
(two maps pipelined into one stage)
- As a result, parts of the StandardScaler will actually be run again
(perhaps only because the variable is `lazy`?) and this can be costly,
see line 84 (https://github.com/apache/spark/blob/branch-1.1/mllib/src/main/scala/org/apache/spark/mllib/feature/StandardScaler.scala#L84)
- For small datasets and/or few iterations, this is not really a
problem, however we found that by adding a `data.persist()` right
before running the optimizer, we went from map iterations in the
optimizer that went from 5:30 down to 0:45

I had a very tough time coming up with a nice way to describe my
debugging sessions in an email so I hope this gets the main points
across. Happy to clarify anything if necessary (also by live
debugging/Skype/phone if that's helpful).

Thanks,

Josh

---------------------------------------------------------------------


"
"""Evan R. Sparks"" <evan.sparks@gmail.com>","Tue, 17 Feb 2015 08:25:45 -0800",Re: [MLlib] Performance problem in GeneralizedLinearAlgorithm,Josh Devins <josh@soundcloud.com>,"Josh - thanks for the detailed write up - this seems a little funny to me.
I agree that with the current code path there is extra work being done than
needs to be (e.g. the features are re-scaled at every iteration, but the
relatively costly process of fitting the StandardScaler should not be
re-done at each iteration. Instead, at each iteration, all points are
re-scaled according to the pre-computed standard-deviations in the
StandardScalerModel, and then an intercept is appended.

Just to be clear - you're currently calling .persist() before you pass data
to LogisticRegressionWithLBFGS?

Also - can you give some parameters about the problem/cluster size you're
solving this on? How much memory per node? How big are n and d, what is its
sparsity (if any) and how many iterations are you running for? Is 0:45 the
per-iteration time or total time for some number of iterations?

A useful test might be to call GeneralizedLinearAlgorithm useFeatureScaling
set to false (and maybe also addIntercept set to false) on persisted data,
and see if you see the same performance wins. If that's the case we've
isolated the issue and can start profiling to see where all the time is
going.

It would be great if you can open a JIRA.

Thanks!




"
Peter Rudenko <petro.rudenko@gmail.com>,"Tue, 17 Feb 2015 18:31:23 +0200",Re: [MLlib] Performance problem in GeneralizedLinearAlgorithm,dev@spark.apache.org,"It's fixed today: https://github.com/apache/spark/pull/4593

Thanks,
Peter Rudenko


---------------------------------------------------------------------


"
Yin Huai <yhuai@databricks.com>,"Tue, 17 Feb 2015 08:34:21 -0800","Re: org.apache.spark.sql.sources.DDLException: Unsupported dataType:
 [1.1] failure: ``varchar'' expected but identifier char found in spark-sql",Qiuzhuang Lian <qiuzhuang.lian@gmail.com>,"Hi Quizhuang,

Right now, char is not supported in DDL. Can you try varchar or string?

Thanks,

Yin


"
Xiangrui Meng <mengxr@gmail.com>,"Tue, 17 Feb 2015 15:12:21 -0800",Re: [ml] Lost persistence for fold in crossvalidation.,Peter Rudenko <petro.rudenko@gmail.com>,"There are three different regParams defined in the grid and there are
tree folds. For simplicity, we didn't split the dataset into three and
reuse them, but do the split for each fold. Then we need to cache 3*3
times. Note that the pipeline API is not yet optimized for
performance. It would be nice to optimize its perforamnce in 1.4.
-Xiangrui

rote:
hod and
e),
he output:
ut

---------------------------------------------------------------------


"
Xiangrui Meng <mengxr@gmail.com>,"Tue, 17 Feb 2015 15:19:57 -0800",Re: mllib.recommendation Design,Debasish Das <debasish.das83@gmail.com>,"The current ALS implementation allow pluggable solvers for
NormalEquation, where we put CholeskeySolver and NNLS solver. Please
check the current implementation and let us know how your constraint
solver would fit. For a general matrix factorization package, let's
make a JIRA and move our discussion there. -Xiangrui


---------------------------------------------------------------------


"
Xiangrui Meng <mengxr@gmail.com>,"Tue, 17 Feb 2015 15:22:58 -0800",Re: Batch prediciton for ALS,Debasish Das <debasish.das83@gmail.com>,"It may be too late to merge it into 1.3. I'm going to make another
pass on your PR today. -Xiangrui


---------------------------------------------------------------------


"
Debasish Das <debasish.das83@gmail.com>,"Tue, 17 Feb 2015 16:10:01 -0800",Re: Batch prediciton for ALS,Xiangrui Meng <mengxr@gmail.com>,"It will be really help us if we merge it but I guess it is already diverged
from the new ALS...I will also take a look at it again and try update with
the new ALS...


"
Debasish Das <debasish.das83@gmail.com>,"Tue, 17 Feb 2015 16:40:39 -0800",Re: mllib.recommendation Design,Xiangrui Meng <mengxr@gmail.com>,"There is a usability difference...I am not sure if recommendation.ALS would
like to add both userConstraint and productConstraint ? GraphLab CF for
example has it and we are ready to support all the features for modest
ranks where gram matrices can be made...

For large ranks I am still working on the code


"
Niranda Perera <niranda.perera@gmail.com>,"Wed, 18 Feb 2015 08:44:08 +0530",Re: Replacing Jetty with TomCat,Sean Owen <sowen@cloudera.com>,"Hi Sean,
The main issue we have is, running two web servers in a single product. we
think it would not be an elegant solution.

Could you please point me to the main areas where jetty server is tightly
coupled or extension points where I could plug tomcat instead of jetty?
If successful I could contribute it to the spark project. :-)

cheers







-- 
Niranda
"
Patrick Wendell <pwendell@gmail.com>,"Tue, 17 Feb 2015 19:22:48 -0800",Re: Replacing Jetty with TomCat,Niranda Perera <niranda.perera@gmail.com>,"Hey Niranda,

It seems to me a lot of effort to support multiple libraries inside of
Spark like this, so I'm not sure that's a great solution.

If you are building an application that embeds Spark, is it not
possible for you to continue to use Jetty for Spark's internal servers
and use tomcat for your own server's? I would guess that many complex
applications end up embedding multiple server libraries in various
places (Spark itself has different transport mechanisms, etc.)

- Patrick


---------------------------------------------------------------------


"
Corey Nolet <cjnolet@gmail.com>,"Tue, 17 Feb 2015 23:29:43 -0500",Re: Replacing Jetty with TomCat,Patrick Wendell <pwendell@gmail.com>,"Niranda,

I'm not sure if I'd say Spark's use of Jetty to expose its UI monitoring
layer constitutes a use of ""two web servers in a single product"". Hadoop
uses Jetty as well as do many other applications today that need embedded
http layers for serving up their monitoring UI to users. This is completely
aside from any web container an application developer would use to interact
with Spark and Hadoop and service domain-specific content to users. The two
are disjoint.

Many applications use Thrift as a means of establishing socket connections
You wouldn't say ""I want to swap out thrift for protobuf in Cassandra
because I want to use protobuf in my application and there shouldn't be two
different socket layer abstractions on my cluster.""

I could understand wanting to do this if you were being forced to deploy a
war file to a web container in order to do the monitoring but Spark's UI is
embedded within the code. If you are worried about having the Jetty
libraries on your classpath, you can exclude the Jetty dependencies from
your servlet code if you want to interact with a SparkContext in Tomcat.




"
Matt Cheah <mcheah@palantir.com>,"Wed, 18 Feb 2015 04:31:45 +0000","JavaRDD Aggregate initial value - Closure-serialized zero value
 reasoning?","""dev@spark.apache.org"" <dev@spark.apache.org>, Mingyu Kim
	<mkim@palantir.com>, Andrew Ash <aash@palantir.com>","Hi everyone,

I was using JavaPairRDDπs combineByKey() to compute all of my aggregations
before, since I assumed that every aggregation required a key. However, I
realized I could do my analysis using JavaRDDπs aggregate() instead and not
use a key.

I have set spark.serializer to use Kryo. As a result, JavaRDDπs combineByKey
requires that a ≥createCombiner≤ function is provided, and the return value
from that function must be serializable using Kryo. When I switched to using
rdd.aggregate I assumed that the zero value would also be strictly Kryo
serialized, as it is a data item and not part of a closure or the
aggregation functions. However, I got a serialization exception as the
closure serializer (only valid serializer is the Java serializer) was used
instead.

I was wondering the following:
1. What is the rationale for making the zero value be serialized using the
closure serializer? This isnπt part of the closure, but is an initial data
item.
2. Would it make sense for us to perhaps write a version of rdd.aggregate()
that takes a function as a parameter, that generates the zero value? This
would be more intuitive to be serialized using the closure serializer.
I believe aggregateByKey is also affected.

Thanks,

-Matt Cheah


"
Patrick Wendell <pwendell@gmail.com>,"Wed, 18 Feb 2015 00:12:08 -0800",[VOTE] Release Apache Spark 1.3.0 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version 1.3.0!

The tag to be voted on is v1.3.0-rc1 (commit f97b0d4a):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=f97b0d4a6b26504916816d7aefcf3132cd1da6c2

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-1.3.0-rc1/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1069/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-1.3.0-rc1-docs/

Please vote on releasing this package as Apache Spark 1.3.0!

The vote is open until Saturday, February 21, at 08:03 UTC and passes
if a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.3.0
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

== How can I help test this release? ==
If you are a Spark user, you can help us test this release by
taking a Spark 1.2 workload and running on this release candidate,
then reporting any regressions.

== What justifies a -1 vote for this release? ==
This vote is happening towards the end of the 1.3 QA period,
so -1 votes should only occur for significant regressions from 1.2.1.
Bugs already present in 1.2.X, minor regressions, or bugs related
to new features will not block this release.

- Patrick

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Wed, 18 Feb 2015 00:21:26 -0800",Merging code into branch 1.3,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey Committers,

Now that Spark 1.3 rc1 is cut, please restrict branch-1.3 merges to
the following:

1. Fixes for issues blocking the 1.3 release (i.e. 1.2.X regressions)
2. Documentation and tests.
3. Fixes for non-blocker issues that are surgical, low-risk, and/or
outside of the core.

If there is a lower priority bug fix (a non-blocker) that requires
nontrivial code changes, do not merge it into 1.3. If something seems
borderline, feel free to reach out to me and we can work through it
together.

This is what we've done for the last few releases to make sure rc's
become progressively more stable, and it is important towards helping
us cut timely releases.

Thanks!

- Patrick

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Wed, 18 Feb 2015 09:55:51 +0000",Re: Replacing Jetty with TomCat,Niranda Perera <niranda.perera@gmail.com>,"I do not think it makes sense to make the web server configurable.
Mostly because there's no real problem in running an HTTP service
internally based on Netty while you run your own HTTP service based on
something else like Tomcat. What's the problem?


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Wed, 18 Feb 2015 10:09:39 +0000",Re: JavaRDD Aggregate initial value - Closure-serialized zero value reasoning?,Matt Cheah <mcheah@palantir.com>,"The serializer is created with

val zeroBuffer = SparkEnv.get.serializer.newInstance().serialize(zeroValue)

Which is definitely not the closure serializer and so should respect
what you are setting with spark.serializer.

Maybe you can do a quick bit of debugging to see where that assumption
breaks down? like are you sure spark.serializer is set everywhere?

regations
ad and not
ombineByKey
d the return value
ing
d
tial data

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Wed, 18 Feb 2015 12:14:15 +0000",Re: [VOTE] Release Apache Spark 1.3.0 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","release for 1.3.0-RC1:

UISeleniumSuite:
*** RUN ABORTED ***
  java.lang.NoClassDefFoundError: org/w3c/dom/ElementTraversal
...


Patrick this link gives a 404:
https://people.apache.org/keys/committer/pwendell.asc


Finally, I already realized I failed to get the fix for
https://issues.apache.org/jira/browse/SPARK-5669 correct, and that has
to be correct for the release. I'll patch that up straight away,
sorry. I believe the result of the intended fix is still as I
described in SPARK-5669, so there is no bad news there. A local test
seems to confirm it and I'm waiting on Jenkins. If it's all good I'll
merge that fix. So, that much will need a new release, I apologize.


Please keep testing anyway!


Otherwise, I verified the signatures are correct, licenses are
correct, compiles on OS X and Ubuntu 14.



---------------------------------------------------------------------


"
Josh Rosen <rosenville@gmail.com>,"Wed, 18 Feb 2015 06:12:31 -0800",Re: JavaRDD Aggregate initial value - Closure-serialized zero value reasoning?,Matt Cheah <mcheah@palantir.com>,"It looks like this was fixed in
https://issues.apache.org/jira/browse/SPARK-4743 /
https://github.com/apache/spark/pull/3605.  Can you see whether that patch
fixes this issue for you?




regations
ad and not
 provided, and the
be
or
e
d
 an initial
he
osure
"
Andrew Ash <andrew@andrewash.com>,"Wed, 18 Feb 2015 07:09:39 -0800",Streaming partitions to driver for use in .toLocalIterator,dev <dev@spark.apache.org>,"Hi Spark devs,

I'm creating a streaming export functionality for RDDs and am having some
trouble with large partitions.  The RDD.toLocalIterator() call pulls over a
partition at a time to the driver, and then streams the RDD out from that
partition before pulling in the next one.  When you have large partitions
though, you can OOM the driver, especially when multiple of these exports
are happening in the same SparkContext.

it's hard to know a priori what the partition count should be, and I'd like
to avoid paying the shuffle cost if possible -- I think repartition to a
higher partition count forces a shuffle.

Is it feasible to rework this so the executor -> driver transfer in
.toLocalIterator is a steady stream rather than a partition at a time?

Thanks!
Andrew
"
Joe Wass <jwass@crossref.org>,"Wed, 18 Feb 2015 15:39:45 +0000",Issue SPARK-5008 (persistent-hdfs broken),dev@spark.apache.org,"I've recently run into problems caused by ticket SPARK-5008

https://issues.apache.org/jira/browse/SPARK-5008

This seems like quite a serious regression in 1.2.0, meaning that it's not
really possible to use persistent-hdfs. The config for the persistent-hdfs
points to the wrong part of the filesystem, so it comes up on the wrong
volume (and therefore has the wrong capacity). I'm working around it with
symlinks, but it's not ideal.

It doesn't look like it's scheduled to be fixed in any particular release.
Is there any indication of whether this is on anyone's todo list?

If no-one's looking into it then I could try having a look myself, but I'm
not (yet) familiar with the internals. From the discussion on the ticket it
doesn't look like a huge fix.

Cheers

Joe
"
Patrick Wendell <pwendell@gmail.com>,"Wed, 18 Feb 2015 10:13:48 -0800",Re: [VOTE] Release Apache Spark 1.3.0 (RC1),Sean Owen <sowen@cloudera.com>,"
This is a newer test suite. There is something flaky about it, we
should definitely fix it, IMO it's not a blocker though.


Works for me. Maybe it's some ephemeral issue?


Thanks for finding this. I'm going to leave this open for continued testing...

---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Wed, 18 Feb 2015 12:55:27 -0800","quick jenkins restart tomorrow morning, ~7am PST","dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>","i'll be kicking jenkins to up the open file limits on the workers.  it
should be a very short downtime, and i'll post updates on my progress
tomorrow.

shane
"
Imran Rashid <irashid@cloudera.com>,"Wed, 18 Feb 2015 15:01:02 -0600",Re: Streaming partitions to driver for use in .toLocalIterator,Andrew Ash <andrew@andrewash.com>,"This would be pretty tricky to do -- the issue is that right now
sparkContext.runJob has you pass in a function from a partition to *one*
result object that gets serialized and sent back: Iterator[T] => U, and
that idea is baked pretty deep into a lot of the internals, DAGScheduler,
Task, Executors, etc.

Maybe another possibility worth considering: should we make it easy to go
from N partitions to 2N partitions (or any other multiple obviously)
without requiring a shuffle?  for that matter, you should also be able to
go from 2N to N without a shuffle as well.  That change is also somewhat
involved, though.

Both are in theory possible, but I imagine they'd need really compelling
use cases.

An alternative would be to write your RDD to some other data store (eg,
hdfs) which has better support for reading data in a streaming fashion,
though you would probably be unhappy with the overhead.




"
shane knapp <sknapp@berkeley.edu>,"Wed, 18 Feb 2015 14:00:15 -0800","Re: quick jenkins restart tomorrow morning, ~7am PST","dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>","i'm actually going to do this now -- it's really quiet today.

there are two spark pull request builds running, which i will kill and
retrigger once jenkins is back up:
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/27689/
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/27690/


"
shane knapp <sknapp@berkeley.edu>,"Wed, 18 Feb 2015 14:06:35 -0800","Re: quick jenkins restart tomorrow morning, ~7am PST","dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>","this is done.


"
Matt Cheah <mcheah@palantir.com>,"Wed, 18 Feb 2015 22:28:15 +0000","Re: JavaRDD Aggregate initial value - Closure-serialized zero value
 reasoning?",Josh Rosen <rosenville@gmail.com>,"But RDD.aggregate() has this code:

    // Clone the zero value since we will also be serializing it as part of
tasks
    var jobResult = Utils.clone(zeroValue,
sc.env.closureSerializer.newInstance())

I do see the SparkEnv.get.serializer used in aggregateByKey however. Perhaps
we just missed it and need to apply the change to aggregate()? It seems
appropriate to target a fix for 1.3.0.

-Matt Cheah
From:  Josh Rosen <rosenville@gmail.com>
Date:  Wednesday, February 18, 2015 at 6:12 AM
To:  Matt Cheah <mcheah@palantir.com>
Cc:  ""dev@spark.apache.org"" <dev@spark.apache.org>, Mingyu Kim
<mkim@palantir.com>, Andrew Ash <aash@palantir.com>
Subject:  Re: JavaRDD Aggregate initial value - Closure-serialized zero
value reasoning?

It looks like this was fixed in
https://issues.apache.org/jira/browse/SPARK-4743
<https://urldefense.proofpoint.com/v2/url?u=https-3A__issues.apache.org_jira
&
r=hzwIMNQ9E99EMYGuqHI0kXhVbvX3nU3OSDadUnJxjAs&m=HsNLIeID8mKWH68HoNyb_x4jS5D3
WSrjQQZX1rW_e9w&s=lOqRteYjf7RRl41OfKvkfh7IaSs3wIW643Fz_Iwlekc&e=>  /
https://github.com/apache/spark/pull/3605
<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_apache_spar
Q
9E99EMYGuqHI0kXhVbvX3nU3OSDadUnJxjAs&m=HsNLIeID8mKWH68HoNyb_x4jS5D3WSrjQQZX1
rW_e9w&s=60tyF-5TbJyVlh7upvFFhNbxKFhh9bUCWJMp5D2wUN8&e=> .  Can you see
whether that patch fixes this issue for you?



s
ot
Key
ue
ing
tion
izer
e
a
()



"
Joseph Bradley <joseph@databricks.com>,"Wed, 18 Feb 2015 14:53:00 -0800",Re: [ml] Lost persistence for fold in crossvalidation.,Xiangrui Meng <mengxr@gmail.com>,"Now in JIRA form: https://issues.apache.org/jira/browse/SPARK-5844


ethod and
th
 the
"
Sean Owen <sowen@cloudera.com>,"Wed, 18 Feb 2015 22:58:54 +0000",Re: JavaRDD Aggregate initial value - Closure-serialized zero value reasoning?,Matt Cheah <mcheah@palantir.com>,"That looks, at the least, inconsistent. As far as I know this should
be changed so that the zero value is always cloned via the non-closure
serializer. Any objection to that?

of
aps
lue
h
gregations
I
ead and not
s provided, and the
 be
 or
he
ed
itial data
s

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Wed, 18 Feb 2015 15:04:32 -0800",Re: JavaRDD Aggregate initial value - Closure-serialized zero value reasoning?,Sean Owen <sowen@cloudera.com>,"Yes, that's a bug and should be using the standard serializer.


t
:
,
stead and
s
 is provided, and
I
e
initial
()
"
Xiangrui Meng <mengxr@gmail.com>,"Wed, 18 Feb 2015 15:07:44 -0800",Re: Batch prediciton for ALS,Debasish Das <debasish.das83@gmail.com>,"Please create a JIRA for it and we should discuss the APIs first
before updating the code. -Xiangrui


---------------------------------------------------------------------


"
Debasish Das <debasish.das83@gmail.com>,"Wed, 18 Feb 2015 15:10:58 -0800",Re: Batch prediciton for ALS,Xiangrui Meng <mengxr@gmail.com>,"You have a JIRA for it...

https://issues.apache.org/jira/browse/SPARK-3066

I added the PR on the JIRA...


"
Mingyu Kim <mkim@palantir.com>,"Wed, 18 Feb 2015 23:21:18 +0000",Re: Streaming partitions to driver for use in .toLocalIterator,"Imran Rashid <irashid@cloudera.com>, Andrew Ash <andrew@andrewash.com>","Another alternative would be to compress the partition in memory in a
streaming fashion instead of calling .toArray on the iterator. Would it be
an easier mitigation to the problem? Or, is it hard to compress the rows
one by one without materializing the full partition in memory using the
compression algo Spark uses currently?

Mingyu








---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Wed, 18 Feb 2015 23:25:07 +0000",Re: [VOTE] Release Apache Spark 1.3.0 (RC1),Patrick Wendell <pwendell@gmail.com>,"
Yes works now; I swear it didn't before! that's all set now. The
signing key is in that file.

---------------------------------------------------------------------


"
Matt Cheah <mcheah@palantir.com>,"Thu, 19 Feb 2015 00:47:11 +0000",[Performance] Possible regression in rdd.take()?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi everyone,

Between Spark 1.0.2 and Spark 1.1.1, I have noticed that rdd.take()
consistently has a slower execution time on the later release. I was
wondering if anyone else has had similar observations.

I have two setups where this reproduces. The first is a local test. I
launched a spark cluster with 4 worker JVMs on my Mac, and launched a
Spark-Shell. I retrieved the text file and immediately called rdd.take(N) on
it, where N varied. The RDD is a plaintext CSV, 4GB in size, split over 8
files, which ends up having 128 partitions, and a total of 80000000 rows.
The numbers I discovered between Spark 1.0.2 and Spark 1.1.1 are, with all
numbers being in seconds:
10000 items

Spark 1.0.2: 0.069281, 0.012261, 0.011083

Spark 1.1.1: 0.11577, 0.097636, 0.11321



40000 items

Spark 1.0.2: 0.023751, 0.069365, 0.023603

Spark 1.1.1: 0.224287, 0.229651, 0.158431



100000 items

Spark 1.0.2: 0.047019, 0.049056, 0.042568

Spark 1.1.1: 0.353277, 0.288965, 0.281751



400000 items

Spark 1.0.2: 0.216048, 0.198049, 0.796037

Spark 1.1.1: 1.865622, 2.224424, 2.037672

This small test suite indicates a consistently reproducible performance
regression.



I also notice this on a larger scale test. The cluster used is on EC2:

ec2 instance type: m2.4xlarge
10 slaves, 1 master
ephemeral storage
70 cores, 50 GB/box
In this case, I have a 100GB dataset split into 78 files totally 350 million
items, and I take the first 50,000 items from the RDD. In this case, I have
tested this on different formats of the raw data.

With plaintext files:

Spark 1.0.2: 0.422s, 0.363s, 0.382s

Spark 1.1.1: 4.54s, 1.28s, 1.221s, 1.13s



With snappy-compressed Avro files:

Spark 1.0.2: 0.73s, 0.395s, 0.426s

Spark 1.1.1: 4.618s, 1.81s, 1.158s, 1.333s

Again demonstrating a reproducible performance regression.

I was wondering if anyone else observed this regression, and if so, if
anyone would have any idea what could possibly have caused it between Spark
1.0.2 and Spark 1.1.1?

Thanks,

-Matt Cheah


"
Patrick Wendell <pwendell@gmail.com>,"Wed, 18 Feb 2015 16:54:49 -0800",Re: [Performance] Possible regression in rdd.take()?,Matt Cheah <mcheah@palantir.com>,"I believe the heuristic governing the way that take() decides to fetch
partitions changed between these versions. It could be that in certain
cases the new heuristic is worse, but it might be good to just look at
the source code and see, for your number of elements taken and number
of partitions, if there was any effective change in how aggressively
spark fetched partitions.

This was quite a while ago, but I think the change was made because in
many cases the newer code works more efficiently.

- Patrick


---------------------------------------------------------------------


"
Matt Cheah <mcheah@palantir.com>,"Thu, 19 Feb 2015 01:10:32 +0000",Re: [Performance] Possible regression in rdd.take()?,Patrick Wendell <pwendell@gmail.com>,"I actually tested Spark 1.2.0 with the code in the rdd.take() method
swapped out for what was in Spark 1.0.2. The run time was still slower,
which indicates to me something at work lower in the stack.

-Matt Cheah


"
Aaron Davidson <ilikerps@gmail.com>,"Wed, 18 Feb 2015 17:25:29 -0800",Re: [Performance] Possible regression in rdd.take()?,Matt Cheah <mcheah@palantir.com>,"You might be seeing the result of this patch:

https://github.com/apache/spark/commit/d069c5d9d2f6ce06389ca2ddf0b3ae4db72c5797

which was introduced in 1.1.1. This patch disabled the ability for take()
to run without launching a Spark job, which means that the latency is
significantly increased for small jobs (but not for large ones). You can
try enabling local execution and seeing if your problem goes away.


"
Matt Cheah <mcheah@palantir.com>,"Thu, 19 Feb 2015 01:54:23 +0000",Re: [Performance] Possible regression in rdd.take()?,Aaron Davidson <ilikerps@gmail.com>,"Ah okay, I turned on spark.localExecution.enabled and the performance
returned to what Spark 1.0.2 had. However I can see how users can
inadvertently incur memory and network strain in fetching the whole
partition to the driver.

Iπll evaluate on my side if we want to turn this on or not. Thanks for the
quick and accurate response!

-Matt CHeah

From:  Aaron Davidson <ilikerps@gmail.com>
Date:  Wednesday, February 18, 2015 at 5:25 PM
To:  Matt Cheah <mcheah@palantir.com>
Cc:  Patrick Wendell <pwendell@gmail.com>, ""dev@spark.apache.org""
<dev@spark.apache.org>, Mingyu Kim <mkim@palantir.com>, Sandor Van
Wassenhove <sandorw@palantir.com>
Subject:  Re: [Performance] Possible regression in rdd.take()?

You might be seeing the result of this patch:

https://github.com/apache/spark/commit/d069c5d9d2f6ce06389ca2ddf0b3ae4db72c5
797

which was introduced in 1.1.1. This patch disabled the ability for take() to
run without launching a Spark job, which means that the latency is
significantly increased for small jobs (but not for large ones). You can try
enabling local execution and seeing if your problem goes away.

:
I
a
ver
th
nce
2:
 I
if
n



"
Kannan Rajah <krajah@maprtech.com>,"Wed, 18 Feb 2015 18:33:21 -0800","Spark-SQL 1.2.0 ""sort by"" results are not consistent with Hive",dev@spark.apache.org,"According to hive documentation, ""sort by"" is supposed to order the results
for each reducer. So if we set a single reducer, then the results should be
sorted, right? But this is not happening. Any idea why? Looks like the
settings I am using to restrict the number of reducers is not having an
effect.

*Tried the following:*

Set spark.default.parallelism to 1

Set spark.sql.shuffle.partitions to 1

These were set in hive-site.xml and also inside spark shell.


*Spark-SQL*

create table if not exists testSortBy (key int, name string, age int);
LOAD DATA LOCAL INPATH '/home/mapr/sample-name-age.txt' OVERWRITE INTO TABLE
testSortBy;
select * from testSortBY;

1    Aditya    28
2    aash    25
3    prashanth    27
4    bharath    26
5    terry    27
6    nanda    26
7    pradeep    27
8    pratyay    26


set spark.default.parallelism=1;

set spark.sql.shuffle.partitions=1;

select name,age from testSortBy sort by age; aash 25 bharath 26 prashanth
27 Aditya 28 nanda 26 pratyay 26 terry 27 pradeep 27 *HIVE* select name,age
from testSortBy sort by age;

aash    25
bharath    26
nanda    26
pratyay    26
prashanth    27
terry    27
pradeep    27
Aditya    28


--
Kannan
"
Krishna Sankar <ksankar42@gmail.com>,"Wed, 18 Feb 2015 19:51:30 -0800",Re: [VOTE] Release Apache Spark 1.3.0 (RC1),Sean Owen <sowen@cloudera.com>,"+1 (non-binding, of course)

1. Compiled OSX 10.10 (Yosemite) OK Total time: 14:50 min
     mvn clean package -Pyarn -Dyarn.version=2.6.0 -Phadoop-2.4
-Dhadoop.version=2.6.0 -Phive -DskipTests -Dscala-2.11
2. Tested pyspark, mlib - running as well as comp"
Debasish Das <debasish.das83@gmail.com>,"Wed, 18 Feb 2015 23:20:55 -0800",If job fails shuffle space is not cleaned,dev <dev@spark.apache.org>,"Hi,

Some of my jobs failed due to no space left on device and on those jobs I
was monitoring the shuffle space...when the job failed shuffle space did
not clean and I had to manually clean it...

Is there a JIRA already tracking this issue ? If no one has been assigned
to it, I can take a look.

Thanks.
Deb
"
Niranda Perera <niranda.perera@gmail.com>,"Thu, 19 Feb 2015 13:41:39 +0530",Re: Replacing Jetty with TomCat,Sean Owen <sowen@cloudera.com>,"Hi Sean,
The issue we have here is that all our products are based on a single
platform and we try to make all our products coherent with our platform as
much as possible. so, having two web services in one instance would not be
a very elegant solution. That is why we were seeking a way to switch it to
Tomcat. But as I understand, it is not readily supported, hence we will
have to accept it as it is.

If we are not using the Spark UIs, is it possible to disable the UIs and
prevent the jetty server from starting, but yet use the core spark
functionality?

Hi Corey,
thank you for your ideas. Our biggest concern here was that it starts a new
webserver inside spark. opening up new ports etc. might be seen as security
threats when it comes to commercial distributions.

cheers







-- 
Niranda
"
Judy Nash <judynash@exchange.microsoft.com>,"Thu, 19 Feb 2015 08:26:11 +0000",RE: spark slave cannot execute without admin permission on windows,"Akhil Das <akhil@sigmoidanalytics.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","+ dev mailing list

If this is supposed to work, is there a regression then?

The spark core code shows the permission for copied file to \work is set to a+x at Line 442 of Utils.scala<https://github.com/apache/spark/blob/b271c265b742fa6947522eda4592e"
Sean Owen <sowen@cloudera.com>,"Thu, 19 Feb 2015 09:23:46 +0000",Re: Replacing Jetty with TomCat,Niranda Perera <niranda.perera@gmail.com>,"Sure, but you are not using Netty at all. It's invisible to you. It's
not as if you have to set up and maintain a Jetty container. I don't
think your single platform for your apps is relevant.

You can turn off the UI, but as Reynold said, the HTTP servers are
also part of the core data transport functionality and you can't turn
that off. It's not merely unsupported to swap this out with an
arbitrary container, it's not clear it would work with Tomcat without
re-integrating with its behavior and tuning. But it also shouldn't
matter to anyone.


---------------------------------------------------------------------


"
Ewan Higgs <ewan.higgs@ugent.be>,"Thu, 19 Feb 2015 11:11:20 +0100",Re: Replacing Jetty with TomCat,"Sean Owen <sowen@cloudera.com>, 
 Niranda Perera <niranda.perera@gmail.com>","To add to Sean and Reynold's point:

Please correct me if I'm wrong, but Spark depends on hadoop-common which 
also uses jetty in the HttpServer2 code. So even if you remove jetty 
from Spark by making it an optional dependency, it will be pulled in by 
Hadoop.

So you'll still see that your program that depends on hypothetical 
Spark-Tomcat will still pull in jetty jars.

-Ewan




---------------------------------------------------------------------


"
The Watcher <watcherfr@gmail.com>,"Thu, 19 Feb 2015 11:25:38 +0100",Hive SKEWED feature supported in Spark SQL ?,dev@spark.apache.org,"I have done some testing of inserting into tables defined in Hive using 1.2
and I can see that the PARTITION clause is honored : data files get created
in multiple subdirectories correctly.

I tried the SKEWED BY ON STORED AS DIRECTORIES clause on the CREATE TABLE
clause but I didn't see subdirectories being created in that case.

1) is SKEWED BY honored ? If so, has anyone run into directories not being
created ?

2) if it is not honored, does it matter ? Hive introduced this feature to
better handle joins where tables had a skewed distribution on keys joined
on so that the single mapper handling one of the keys didn't hold up the
whole process. Could that happen in Spark / Spark SQL ?

Thanks
"
mike@mbowles.com,"Thu, 19 Feb 2015 18:59:08 +0000",Have Friedman's glmnet algo running in Spark,dev@spark.apache.org,"Dev List, 
A couple of colleagues and I have gotten several versions of glmnet algo coded and running on Spark RDD. glmnet algo (http://www.jstatsoft.org/v33/i01/paper) is a very fast algorithm for generating coefficient paths solving penalized regression with elastic net penalties. The algorithm runs fast by taking an approach that generates solutions for a wide variety of penalty parameter. We're able to integrate into Mllib class structure a couple of different ways. The algorithm may fit better into the new pipeline structure since it naturally returns a multitide of models (corresponding to different vales of penalty parameters). That appears to fit better into pipeline than Mllib linear regression (for example). 

We've got regression running with the speed optimizations that Friedman recommends. We'll start working on the logistic regression version next. 

We're eager to make the code available as open source and would like to get some feedback about how best to do that. Any thoughts? 
Mike Bowles. 


"
Michael Armbrust <michael@databricks.com>,"Thu, 19 Feb 2015 11:58:44 -0800",Re: Hive SKEWED feature supported in Spark SQL ?,The Watcher <watcherfr@gmail.com>,"
It is not.

2) if it is not honored, does it matter ? Hive introduced this feature to

It could matter for very skewed data, though I have not heard many
complaints.  We could consider adding it in the future if people are having
problems with skewed data.
"
Michael Armbrust <michael@databricks.com>,"Thu, 19 Feb 2015 12:02:56 -0800",Re: [VOTE] Release Apache Spark 1.3.0 (RC1),Krishna Sankar <ksankar42@gmail.com>,"

We will write up a whole migration guide before the final release, but I
can quickly explain this one.  We made the implicit conversion
significantly less broad to avoid the chance of confusing conflicts.
However, now you have to call .toDF in order to force RDDs to become
DataFrames.
"
Krishna Sankar <ksankar42@gmail.com>,"Thu, 19 Feb 2015 12:50:19 -0800",Re: [VOTE] Release Apache Spark 1.3.0 (RC1),Michael Armbrust <michael@databricks.com>,"Excellent. Explicit toDF() works.
a) employees.toDF().registerTempTable(""Employees"") - works
b) Also affects saveAsParquetFile - orders.toDF().saveAsParquetFile

Adding to my earlier tests:
4.0 SQL from Scala and Python
4.1 result = sqlContext.sql(""SELECT * from Employees WHERE State = 'WA'"") OK
4.2 result = sqlContext.sql(""SELECT
OrderDetails.OrderID,ShipCountry,UnitPrice,Qty,Discount FROM Orders INNER
JOIN OrderDetails ON Orders.OrderID = OrderDetails.OrderID"") OK
4.3 result = sqlContext.sql(""SELECT ShipCountry, Sum(OrderDetails.UnitPrice
* Qty * Discount) AS ProductSales FROM Orders INNER JOIN OrderDetails ON
Orders.OrderID = OrderDetails.OrderID GROUP BY ShipCountry"") OK
4.4 saveAsParquetFile OK
4.5 Read and verify the 4.4 save - sqlContext.parquetFile,
registerTempTable, sql OK

Cheers & thanks Michael
<k/>




"
The Watcher <watcherfr@gmail.com>,"Thu, 19 Feb 2015 23:50:27 +0100","Spark SQL, Hive & Parquet data types",dev@spark.apache.org,"Still trying to get my head around Spark SQL & Hive.

1) Let's assume I *only* use Spark SQL to create and insert data into HIVE
tables, declared in a Hive meta-store.

Does it matter at all if Hive supports the data types I need with Parquet,
or is all that matters what Catalyst & spark's parquet relation support ?

Case in point : timestamps & Parquet
* Parquet now supports them as per
https://github.com/Parquet/parquet-mr/issues/218
* Hive only supports them in 0.14
So would I be able to read/write timestamps natively in Spark 1.2 ? Spark
1.3 ?

I have found this thread
http://apache-spark-user-list.1001560.n3.nabble.com/timestamp-not-implemented-yet-td15414.html
which seems to indicate that the data types supported by Hive would matter
to Spark SQL.
If so, why is that ? Doesn't the read path go through Spark SQL to read the
parquet file ?

2) Is there planned support for Hive 0.14 ?

Thanks
"
Timothy Chen <tnachen@gmail.com>,"Thu, 19 Feb 2015 17:50:21 -0800",Re: [VOTE] Release Apache Spark 1.3.0 (RC1),Krishna Sankar <ksankar42@gmail.com>,"+1 (non-binding)

Tested Mesos coarse/fine-grained mode with 4 nodes Mesos cluster with
simple shuffle/map task.

Will be testing with more complete suite (ie: spark-perf) once the
infrastructure is setup to do so.

Tim


---------------------------------"
Corey Nolet <cjnolet@gmail.com>,"Thu, 19 Feb 2015 21:45:41 -0500",Re: [VOTE] Release Apache Spark 1.3.0 (RC1),Sean Owen <sowen@cloudera.com>,"+1 (non-binding)

- Verified signatures using [1]
- Built on MacOSX Yosemite
- Built on Fedora 21

Each build was run with and Hadoop-2.4 version with yarn, hive, and
hive-thriftserver profiles

I am having trouble getting all the tests passing on a singl"
Mingyu Kim <mkim@palantir.com>,"Fri, 20 Feb 2015 08:30:23 +0000",The default CDH4 build uses avro-mapred hadoop1,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,

Related to https://issues.apache.org/jira/browse/SPARK-3039, the default CDH4 build, which is built with ""mvn -Dhadoop.version=2.0.0-mr1-cdh4.2.0 -DskipTests clean packageî, pulls in avro-mapred hadoop1, as opposed to avro-mapred hadoop2. This ends up in the same error as mentioned in the linked bug. (pasted below).

The right solution would be to create a hadoop-2.0 profile that sets avro.mapred.classifier to hadoop2, and to build CDH4 build with ì-Phadoop-2.0î option.

What do people think?

Mingyu

óóóóóóóóóó

java.lang.IncompatibleClassChangeError: Found interface org.apache.hadoop.mapreduce.TaskAttemptContext, but class was expected
       at org.apache.avro.mapreduce.AvroKeyInputFormat.createRecordReader(AvroKeyInputFormat.java:47)
       at org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:133)
       at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:107)
       at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:69)
       at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:280)
       at org.apache.spark.rdd.RDD.iterator(RDD.scala:247)
       at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
       at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:280)
       at org.apache.spark.rdd.RDD.iterator(RDD.scala:247)
       at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
       at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:280)
       at org.apache.spark.rdd.RDD.iterator(RDD.scala:247)
       at org.apache.spark.rdd.FilteredRDD.compute(FilteredRDD.scala:34)
       at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:280)
       at org.apache.spark.rdd.RDD.iterator(RDD.scala:247)
       at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
       at org.apache.spark.scheduler.Task.run(Task.scala:56)
       at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:200)
       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       at java.lang.Thread.run(Thread.java:745)

"
Niranda Perera <niranda.perera@gmail.com>,"Fri, 20 Feb 2015 14:03:10 +0530",OSGI bundles for spark project..,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

I am interested in a Spark OSGI bundle.

While checking the maven repository I found out that it is still not being
implemented.

Can we see an OSGI bundle being released soon? Is it in the Spark Project
roadmap?

Rgds
-- 
Niranda
"
Dirceu Semighini Filho <dirceu.semighini@gmail.com>,"Fri, 20 Feb 2015 08:01:25 -0200",Spark performance on 32 Cpus Server Cluster,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,
I'm running Spark 1.2.0, in Stand alone mode, on different cluster and
server sizes. All of my data is cached in memory.
Basically I have a mass of data, about 8gb, with about 37k of columns, and
I'm running different configs of an BinaryLogisticRegressionBFGS.
When I put spark to run on 9 servers (1 master and 8 slaves), with 32 cores
each. I noticed that the cpu usage was varying from 20% to 50% (counting
the cpu usage of 9 servers in the cluster).
First I tried to repartition the Rdds to the same number of total client
cores (256), but that didn't help. After I've tried to change the
property *spark.default.parallelism
* to the same number (256) but that didn't helped to increase the cpu usage.
Looking at the spark monitoring tool, I saw that some stages  took 52s to
be completed.
My last shot was trying to run some tasks in parallel, but when I start
running tasks in parallel (4 tasks) the total cpu time spent to complete
this has increased in about 10%, task parallelism didn't helped.
Looking at the monitoring tool I've noticed that when running tasks in
parallel, the stages complete together, if I have 4 stages running in
parallel (A,B,C and D), if A, B and C finishes, they will wait for D to
mark all this 4 stages as completed, is that right?
Is there any way to improve the cpu usage when running on large servers?
Spending more time when running tasks is an expected behaviour?

Kind Regards,
Dirceu
"
Sean Owen <sowen@cloudera.com>,"Fri, 20 Feb 2015 10:17:33 +0000",Re: Spark performance on 32 Cpus Server Cluster,Dirceu Semighini Filho <dirceu.semighini@gmail.com>,"It sounds like your computation just isn't CPU bound, right? or maybe
that only some stages are. It's not clear what work you are doing
beyond the core LR.

Stages don't wait on each other unless one depends on the other. You'd
have to clarify what you mean by running stages in parallel, like what
are the interdependencies.


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Fri, 20 Feb 2015 10:34:29 +0000",Re: The default CDH4 build uses avro-mapred hadoop1,Mingyu Kim <mkim@palantir.com>,"True, although a number of other little issues make me, personally,
not want to continue down this road:

- There are already a lot of build profiles to try to cover Hadoop versions
- I don't think it's quite right to have vendor-specific builds in
Spark to begin with
- We should be moving to only support Hadoop 2 soon IMHO anyway
- CDH4 is EOL in a few months I think

CDH4 build, which is built with ""mvn -Dhadoop.version=2.0.0-mr1-cdh4.2.0 -DskipTests clean package‚Äù, pulls in avro-mapred hadoop1, as opposed to avro-mapred hadoop2. This ends up in the same error as mentioned in the linked bug. (pasted below).
.mapred.classifier to hadoop2, and to build CDH4 build with ‚Äú-Phadoop-2.0‚Äù option.
‚Äî‚Äî
.mapreduce.TaskAttemptContext, but class was expected
(AvroKeyInputFormat.java:47)
cala:133)
7)
)
61)
a:200)
cutor.java:1145)
ecutor.java:615)

---------------------------------------------------------------------


"
Dirceu Semighini Filho <dirceu.semighini@gmail.com>,"Fri, 20 Feb 2015 10:18:32 -0200",Re: Spark performance on 32 Cpus Server Cluster,Sean Owen <sowen@cloudera.com>,"Hi Sean,
I'm trying to increase the cpu usage by running logistic regression in
different datasets in parallel. They shouldn't depend on each other.
I train several  logistic regression models from different column
combinations of a main dataset. I processed the combinations in a ParArray
in an attempt to increase cpu usage but id did not help.



2015-02-20 8:17 GMT-02:00 Sean Owen <sowen@cloudera.com>:

"
Sean Owen <sowen@cloudera.com>,"Fri, 20 Feb 2015 12:31:26 +0000",Re: Spark performance on 32 Cpus Server Cluster,Dirceu Semighini Filho <dirceu.semighini@gmail.com>,"Yes that makes sense, but it doesn't make the jobs CPU-bound. What is
the bottleneck? the model building or other stages? I would think you
can get the model building to be CPU bound, unless you have chopped it
up into really small partitions. I think it's best to look further
into what stages are slow, and what it seems to be spending time on --
GC? I/O?


---------------------------------------------------------------------


"
Cheng Lian <lian.cs.zju@gmail.com>,"Fri, 20 Feb 2015 21:44:51 +0800","Re: Spark SQL, Hive & Parquet data types","The Watcher <watcherfr@gmail.com>, dev@spark.apache.org","For the second question, we do plan to support Hive 0.14, possibly in 
Spark 1.4.0.

For the first question:

 1. In Spark 1.2.0, the Parquet support code doesn‚Äôt support timestamp
    type, so you can‚Äôt.
 2. In Spark 1.3.0, timestamp support was added, also Spark SQL uses its
    own Parquet support to handle both read path and write path when
    dealing with Parquet tables declared in Hive metastore, as long as
    you‚Äôre not writing to a partitioned table. So yes, you can.

The Parquet version bundled with Spark 1.3.0 is 1.6.0rc3, which supports 
timestamp type natively. However, the Parquet versions bundled with Hive 
0.13.1 and Hive 0.14.0 are 1.3.2 and 1.5.0 respectively. Neither of them 
supports timestamp type. Hive 0.14.0 ‚Äúsupports‚Äù read/write timestamp 
from/to Parquet by converting timestamps from/to Parquet binaries. 
Similarly, Impala converts timestamp into Parquet int96. This can be 
annoying for Spark SQL, because we must interpret Parquet files in 
different ways according to the original writer of the file. As Parquet 
matures, recent Parquet versions support more and more standard data 
types. Mappings from complex nested types to Parquet types are also 
being standardized 1 
<https://github.com/apache/incubator-parquet-mr/pull/83>.


‚Äã
"
The Watcher <watcherfr@gmail.com>,"Fri, 20 Feb 2015 15:58:38 +0100","Re: Spark SQL, Hive & Parquet data types",dev@spark.apache.org,"‚Äôre
to the work being done on ParquetRelation2 ?

We will indeed write to a partitioned table : do neither the read nor the
write path go through Spark SQL's parquet support in that case ? Is there a
JIRA/PR I can monitor to see when this would change ?

Thanks
"
Denny Lee <denny.g.lee@gmail.com>,"Fri, 20 Feb 2015 15:51:32 +0000",Spark 1.3 RC1 Generate schema based on string of schema,"""dev@spark.apache.org"" <dev@spark.apache.org>","In the Spark SQL 1.2 Programmers Guide, we can generate the schema based on
the string of schema via

val schema =
  StructType(
    schemaString.split("" "").map(fieldName => StructField(fieldName,
StringType, true)))

But when running this on Spark 1.3.0 (RC1), I get the error:

val schema =  StructType(schemaString.split("" "").map(fieldName =>
StructField(fieldName, StringType, true)))

<console>:26: error: not found: value StringType

       val schema =  StructType(schemaString.split("" "").map(fieldName =>
StructField(fieldName, StringType, true)))

I'm looking through the various datatypes within
org.apache.spark.sql.types.DataType
but thought I'd ask to see if I was missing something obvious here.

Thanks!
Denny
"
yash datta <saucam@gmail.com>,"Fri, 20 Feb 2015 21:37:47 +0530","Re: Spark SQL, Hive & Parquet data types",The Watcher <watcherfr@gmail.com>,"For the old parquet path (available in 1.2.1) , i made a few changes for
being able to read/write to a table partitioned on timestamp type column

https://github.com/apache/spark/pull/4469



ed
 a



-- 
When events unfold with calm and ease
When the winds that blow are merely breeze
Learn from nature, from birds and bees
Live your life in love, and let joy not cease.
"
Denny Lee <denny.g.lee@gmail.com>,"Fri, 20 Feb 2015 16:22:27 +0000",Re: Spark 1.3 RC1 Generate schema based on string of schema,"""dev@spark.apache.org"" <dev@spark.apache.org>","Oh, I just realized that I never imported all of sql._ .  My bad!



"
Niranda Perera <niranda.perera@gmail.com>,"Fri, 20 Feb 2015 22:18:38 +0530",Re: OSGI bundles for spark project..,"Sean Owen <sowen@cloudera.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Sean,

does it mean that Spark is not encouraged to be embedded on other products?





-- 
Niranda
"
Sean Owen <sowen@cloudera.com>,"Fri, 20 Feb 2015 16:54:01 +0000",Re: OSGI bundles for spark project..,Niranda Perera <niranda.perera@gmail.com>,"No, you usually run Spark apps via the spark-submit script, and the
Spark machinery is already deployed on a cluster. Although it's
possible to embed the driver and get it working that way, it's not
supported.


---------------------------------------------------------------------


"
Mingyu Kim <mkim@palantir.com>,"Fri, 20 Feb 2015 19:10:41 +0000",Re: The default CDH4 build uses avro-mapred hadoop1,Sean Owen <sowen@cloudera.com>,"Thanks for the explanation.

To be clear, I meant to speak for any hadoop 2 releases before 2.2, which
have profiles in Spark. I referred to CDH4, since thatπs the only Hadoop
2.0/2.1 version Spark ships a prebuilt package for.

I understand the hesitation of making a code change if Spark doesnπt plan
to support Hadoop 2.0/2.1 in general. (Please note, this is not specific
to CDH4) If so, can I propose alternative options until Spark moves to
only support hadoop2?

- Build the CDH4 package with ≥-Davro.mapred.classifier=hadoop2≤, and
update http://spark.apache.org/docs/latest/building-spark.html for all
≥2.0.*≤ examples.
- Build the CDH4 package as is, but note known issues clearly in the
≥download≤ page.
- Simply do not ship CDH4 prebuilt package, and let people figure it out
themselves. Preferably, note in documentation that
≥-Davro.mapred.classifier=hadoop2≤ should be used for all hadoop ≥2.0.*≤
builds.

Please let me know what you think!

Mingyu






ji
b6oO
Itkw
=
 in


---------------------------------------------------------------------


"
Tom Graves <tgraves_cs@yahoo.com.INVALID>,"Fri, 20 Feb 2015 22:56:26 +0000 (UTC)",Re: [VOTE] Release Apache Spark 1.3.0 (RC1),"Patrick Wendell <pwendell@gmail.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","Trying to run pyspark on yarn in client mode with basic wordcount example I see the following error when doing the collect:
Error from python worker:¬† /usr/bin/python: No module named sqlPYTHONPATH was:¬† /grid/3/tmp/yarn-local/usercache/tgraves/filecache/20/spark-assembly-1.3.0-hadoop2.6.0.1.1411101121.jarjava.io.EOFException¬† ¬† ¬† ¬† at java.io.DataInputStream.readInt(DataInputStream.java:392)¬† ¬† ¬† ¬† at org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:163)¬† ¬† ¬† ¬† at org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:86)¬† ¬† ¬† ¬† at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:62)¬† ¬† ¬† ¬† at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:105)¬† ¬† ¬† ¬† at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:69)¬† ¬† ¬† ¬† at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)¬† ¬† ¬† ¬† at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)¬† ¬† ¬† ¬† at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:308)¬† ¬† ¬† ¬† at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)¬† ¬† ¬† ¬† at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)¬† ¬† ¬† ¬† at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)¬† ¬† ¬† ¬† at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)¬† ¬† ¬† ¬† at org.apache.spark.scheduler.Task.run(Task.scala:64)¬† ¬† ¬† ¬† at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:197)¬† ¬† ¬† ¬† at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)¬† ¬† ¬† ¬† at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)¬† ¬† ¬† ¬†¬†at java.lang.Thread.run(Thread.java:722)
any ideas on this?
Tom 

   

 Please vote on releasing the following candidate as Apache Spark version 1.3.0!

The tag to be voted on is v1.3.0-rc1 (commit f97b0d4a):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=f97b0d4a6b26504916816d7aefcf3132cd1da6c2

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-1.3.0-rc1/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1069/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-1.3.0-rc1-docs/

Please vote on releasing this package as Apache Spark 1.3.0!

The vote is open until Saturday, February 21, at 08:03 UTC and passes
if a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.3.0
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

== How can I help test this release? ==
If you are a Spark user, you can help us test this release by
taking a Spark 1.2 workload and running on this release candidate,
then reporting any regressions.

== What justifies a -1 vote for this release? ==
This vote is happening towards the end of the 1.3 QA period,
so -1 votes should only occur for significant regressions from 1.2.1.
Bugs already present in 1.2.X, minor regressions, or bugs related
to new features will not block this release.

- Patrick

---------------------------------------------------------------------



   "
magellane a <magellanea@gmail.com>,"Sat, 21 Feb 2015 18:13:12 +0200",GSOC2015,dev@spark.apache.org,"Hi
Since we're approaching the GSOC2015 application process I have some
questions:

1) Will your organization be a part of GSOC2015 and what are the projects
that you will be interested in?
2) Since I'm not a contributor to apache spark, what are some starter tasks
I can work on to gain facility with the code base?

Thanks a lot

Regards,
"
Manoj Kumar <manojkumarsivaraj334@gmail.com>,"Sat, 21 Feb 2015 22:02:47 +0530",Re: GSOC2015,magellane a <magellanea@gmail.com>,"Hi,

For the starters task you can filter by Documentation and ""Priority-Minor""
and ""Priority-Trivial"" over here, since those are most probably the easiest
things to fix, https://issues.apache.org/jira/browse/SPARK/ . You can also
filter based on your expertise, i.e MLlib (for Machine Learning), SparkSQL
or GraphX.

HTH



-- 
Godspeed,
Manoj Kumar,
http://manojbits.wordpress.com
<http://goog_1017110195>
http://github.com/MechCoder
"
nitin <nitin2goyal@gmail.com>,"Sat, 21 Feb 2015 09:55:49 -0700 (MST)",Spark SQL - Long running job,dev@spark.apache.org,"Hi All,

I intend to build a long running spark application which fetches data/tuples
from parquet, does some processing(time consuming) and then cache the
processed table (InMemoryColumnarTableScan). My use case is good retrieval
time for SQL query(benefits of Spark SQL optimizer) and data
compression(in-built in in-memory caching). Now the problem is that if my
driver goes down, I will have to fetch the data again for all the tables and
compute it and cache which is time consuming.

Is it possible to persist processed/cached RDDs on disk such that my system
up time is less when restarted after failure/going down?

huge temporary shuffle files on local disk in temp folder and as per current
logic, shuffle files don't get deleted for running executors. This is
leading to my local disk getting filled up quickly and going out of space as
its a long running spark job. (running spark in yarn-client mode btw).

Thanks
-Nitin 



--

---------------------------------------------------------------------


"
Manoj Kumar <manojkumarsivaraj334@gmail.com>,"Sun, 22 Feb 2015 00:54:30 +0530",Google Summer of Code - ideas,dev <dev@spark.apache.org>,"Hello,

I've been working on the Spark codebase for quite some time right now,
especially on issues related to MLlib and a very small amount of PySpark
and SparkSQL (https://github.com/apache/spark/pulls/MechCoder) .

I would like to extend my work with Spark as a Google Summer of Code
project.
I want to know if there are specific projects related to MLlib that people
would like to see. (I notice, there is no idea page for GSoC yet). There
are a number of issues related to DecisionTrees, Ensembles, LDA (in the
issue tracker) that I find really interesting that could probably club into
a project, but if the spark community has anything else in mind, I could
work on the other issues pre-GSoC and try out something new during GSoC.

Looking forward!
-- 
Godspeed,
Manoj Kumar,
http://manojbits.wordpress.com
<http://goog_1017110195>
http://github.com/MechCoder
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sun, 22 Feb 2015 07:54:13 +0000",Re: Improving metadata in Spark JIRA,Patrick Wendell <pwendell@gmail.com>,"As of right now, there are no more open JIRA issues without an assigned
component
<https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20resolution%20%3D%20Unresolved%20AND%20component%20%3D%20EMPTY%20ORDER%20BY%20updated%20DESC>!
Hurray!

[image: yay]

Thanks to Sean and others for the cleanup!

Nick

m

Oh derp, missed the YARN component.
pecifyingFieldBehavior-Makingafieldrequiredoroptional
ts
ll
le
t
n
r
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sun, 22 Feb 2015 08:33:50 +0000",Git Achievements,Spark dev list <dev@spark.apache.org>,"For fun:

http://acha-acha.co/#/repo/https://github.com/apache/spark

I just added Spark to this site. Some of these ‚Äúachievements‚Äù are hilarious.

Leo Tolstoy: More than 10 lines in a commit message

Dangerous Game: Commit after 6PM friday

Nick
‚Äã
"
Sean Owen <sowen@cloudera.com>,"Sun, 22 Feb 2015 14:41:47 +0000",Re: Improving metadata in Spark JIRA,dev <dev@spark.apache.org>,"Open pull request count is down to 254 right now from ~325 several weeks
ago.
Open JIRA count is down slightly to 1262 from a peak over ~1320.
Obviously, in the face of an ever faster and larger stream of contributions.

There's a real positive impact of JIRA being a little more meaningful, a
little less backlog to keep looking at, getting commits in slightly faster,
slightly happier contributors, etc.


The virtuous circle can keep going. It'd be great if every contributor
could take a moment to look at his or her open PRs and JIRAs. Example
searches (replace with your user name / name):

https://github.com/apache/spark/pulls/srowen
https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20reporter%20%3D%20%22Sean%20Owen%22%20or%20assignee%20%3D%20%22Sean%20Owen%22

For PRs:

- if it appears to be waiting on your action or feedback,
  - push more changes and/or reply to comments, or
  - if it isn't work you can pursue in the immediate future, close the PR

- if it appears to be waiting on others,
  - if it's had feedback and it's unclear whether there's support to commit
as-is,
    - break down or reduce the change to something less controversial
    - close the PR as softly rejected
  - if there's no feedback or plainly waiting for action, ping @them

For JIRAs:

- If it's fixed along the way, or obsolete, resolve as Fixed or NotAProblem

- Do a quick search to see if a similar issue has been filed and is
resolved or has more activity; resolve as Duplicate if so

- Check that fields are assigned reasonably:
  - Meaningful title and description
  - Reasonable type and priority. Not everything is a major bug, and few
are blockers
  - 1+ Component
  - 1+ Affects version
  - Avoid setting target version until it looks like there's momentum to
merge a resolution

- If the JIRA has had no activity in a long time (6+ months), but does not
feel obsolete, try to move it to some resolution:
  - Request feedback, from specific people if desired, to feel out if there
is any other support for the change
  - Add more info, like a specific reproduction for bugs
  - Narrow scope of feature requests to something that contains a few
actionable steps, instead of broad open-ended wishes
  - Work on a fix. In an ideal world people are willing to work to resolve
JIRAs they open, and don't fire-and-forget


If everyone did this, not only would it advance the house-cleaning a bit
more, but I'm sure we'd rediscover some important work and issues that need
attention.



%20resolution%20%3D%20Unresolved%20AND%20component%20%3D%20EMPTY%20ORDER%20BY%20updated%20DESC>!
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sun, 22 Feb 2015 17:09:28 +0000",Re: Improving metadata in Spark JIRA,"Sean Owen <sowen@cloudera.com>, dev <dev@spark.apache.org>","Open pull request count is down to 254 right now from ~325 several weeks
ago.

This great. Ideally, we need to get this down to < 50 and keep it there.
Having so many open pull requests is just a bad signal to contributors. But
it will take some time to get there.


   - 1+ Component

 Sean, do you have permission to edit our JIRA settings? It should be
possible to enforce this in JIRA itself.


   - 1+ Affects version

 I don‚Äôt think this field makes sense for improvements, right?

Nick
‚Äã


r,
it
em
t
re
e
ed
"
Michael Malak <michaelmalak@yahoo.com.INVALID>,"Mon, 23 Feb 2015 02:04:48 +0000 (UTC)",textFile() ordering and header rows,Dev <dev@spark.apache.org>,"Since RDDs are generally unordered, aren't things like textFile().first() not guaranteed to return the first row (such as looking for a header row)? If so, doesn't that make the example in 
http://spark.apache.org/docs/1.2.1/quick-start.html#basics misleading?

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 23 Feb 2015 02:14:06 +0000",Re: textFile() ordering and header rows,"Michael Malak <michaelmalak@yahoo.com>, Dev <dev@spark.apache.org>","I guess on a technicality the docs just say ""first item in this RDD"", not
""first line in the source text file"". AFAIK there is no way apart from
filtering to remove header lines
<http://stackoverflow.com/a/24734612/877069>.

As long as first() always returns the same value for a given RDD, I think
it's fine, no?

Nick



"
Joseph Bradley <joseph@databricks.com>,"Sun, 22 Feb 2015 18:48:38 -0800",Re: Have Friedman's glmnet algo running in Spark,mike@mbowles.com,"Hi Mike,

glmnet has definitely been very successful, and it would be great to see
how we can improve optimization in MLlib!  There is some related work
ongoing; here are the JIRAs:

GLMNET implementation in Spark
<https://issues.apache.org/jira/browse/SPARK-1673>

LinearRegression with L1/L2 (elastic net) using OWLQN in new ML package
<https://issues.apache.org/jira/browse/SPARK-5253>

The GLMNET JIRA has actually been closed in favor of the latter JIRA.
However, if you're getting good results in your experiments, could you
please post them on the GLMNET JIRA and link them from the other JIRA?  If
it's faster and more scalable, that would be great to find out.

As far as where the code should go and the APIs, that can be discussed on
the JIRA.

I hope this helps, and I'll keep an eye out for updates on the JIRAs!

Joseph



"
Cheng Lian <lian.cs.zju@gmail.com>,"Mon, 23 Feb 2015 11:41:59 +0800",Re: Spark SQL - Long running job,"nitin <nitin2goyal@gmail.com>, dev@spark.apache.org","How about persisting the computed result table first before caching it? 
So that you only need to cache the result table after restarting your 
service without recomputing it. Somewhat like checkpointing.

Cheng



---------------------------------------------------------------------


"
nitin <nitin2goyal@gmail.com>,"Sun, 22 Feb 2015 22:38:30 -0700 (MST)",Re: Spark SQL - Long running job,dev@spark.apache.org,"I believe calling processedSchemaRdd.persist(DISK) and
processedSchemaRdd.checkpoint() only persists data and I will lose all the
RDD metadata and when I re-start my driver, that data is kind of useless for
me (correct me if I am wrong).

I thought of doing processedSchemaRdd.saveAsParquetFile (hdfs file system)
but I fear that in case my ""HDFS block size"" > ""partition file size"", I will
get more partitions when reading instead of original schemaRdd. 



--

---------------------------------------------------------------------


"
Mark Hamstra <mark@clearstorydata.com>,"Sun, 22 Feb 2015 23:20:30 -0800",Re: [VOTE] Release Apache Spark 1.3.0 (RC1),Patrick Wendell <pwendell@gmail.com>,"So what are we expecting of Hive 0.12.0 builds with this RC?  I know not
every combination of Hadoop and Hive versions, etc., can be supported, but
even an example build from the ""Building Spark"" page isn't looking too good
to me.

Working from f97b0d4, the example build command works: mvn -Pyarn
-Phadoop-2.4 -Dhadoop.version=2.4.0 -Phive -Phive-0.12.0
-Phive-thriftserver -DskipTests clean package
...but then running the tests results in multiple failures in the Hive and
Hive Thrift Server sub-projects.



"
Robin East <robin.east@xense.co.uk>,"Mon, 23 Feb 2015 11:59:17 +0000",Re: [VOTE] Release Apache Spark 1.3.0 (RC1),Patrick Wendell <pwendell@gmail.com>,"Running ec2 launch scripts gives me the following error:

ssl.SSLError: [Errno 1] _ssl.c:504: error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify failed

Full stack trace at
https://gist.github.com/insidedctm/4d41600bc22560540a26

Iím running OSX Mavericks 10.9.5

Iíll investigate further but wondered if anyone else has run into this.

Robin"
Cheng Lian <lian.cs.zju@gmail.com>,"Mon, 23 Feb 2015 20:25:53 +0800","Re: Spark SQL, Hive & Parquet data types","The Watcher <watcherfr@gmail.com>, dev@spark.apache.org","Yes, recently we improved ParquetRelation2 quite a bit. Spark SQL uses 
its own Parquet support to read partitioned Parquet tables declared in 
These improvements will be included in Spark 1.3.0.

Just created SPARK-5948 to track writing to partitioned Parquet tables.

Cheng



---------------------------------------------------------------------


"
Cheng Lian <lian.cs.zju@gmail.com>,"Mon, 23 Feb 2015 21:24:24 +0800",Re: Spark SQL - Long running job,"nitin <nitin2goyal@gmail.com>, dev@spark.apache.org","I meant using |saveAsParquetFile|. As for partition number, you can 
always control it with |spark.sql.shuffle.partitions| property.

Cheng


‚Äã
"
Josh Devins <josh@soundcloud.com>,"Mon, 23 Feb 2015 14:36:39 +0100",Re: [MLlib] Performance problem in GeneralizedLinearAlgorithm,"Peter Rudenko <petro.rudenko@gmail.com>, evan.sparks@gmail.com","Thanks for the pointer Peter, that change will indeed fix this bug and
it looks like it will make it into the upcoming 1.3.0 release.

@Evan, for reference, completeness and posterity:

ta to LogisticRegressionWithLBFGS?

No. I added persist in GeneralizedLinearAlgorithm right before the
`data` RDD goes into optimizer (LBFGS in our case). See here:
https://github.com/apache/spark/blob/branch-1.1/mllib/src/main/scala/org/apache/spark/mllib/regression/GeneralizedLinearAlgorithm.scala#L204

 solving this on? How much memory per node? How big are n and d, what is its sparsity (if any) and how many iterations are you running for? Is 0:45 the per-iteration time or total time for some number of iterations?

The vector is very sparse (few hundred entries) but 2.5M in size. The
dataset is about 30M examples to learn from. 16x machines, 64GB
memory, 32-cores.

Josh


:
e.
e
he
a,
:
g/apache/spark/mllib/regression/GeneralizedLinearAlgorithm.scala#L177
g/apache/spark/mllib/regression/GeneralizedLinearAlgorithm.scala#L186
g/apache/spark/mllib/regression/GeneralizedLinearAlgorithm.scala#L204
g/apache/spark/mllib/optimization/LBFGS.scala#L198
g/apache/spark/mllib/feature/StandardScaler.scala#L84

---------------------------------------------------------------------


"
Corey Nolet <cjnolet@gmail.com>,"Mon, 23 Feb 2015 08:55:15 -0500",Re: [VOTE] Release Apache Spark 1.3.0 (RC1),Robin East <robin.east@xense.co.uk>,"This vote was supposed to close on Saturday but it looks like no PMCs voted
(other than the implicit vote from Patrick). Was there a discussion offline
to cut an RC2? Was the vote extended?


 this.
"
The Watcher <watcherfr@gmail.com>,"Mon, 23 Feb 2015 15:05:15 +0100","Re: Spark SQL, Hive & Parquet data types",dev@spark.apache.org,"Ok, this is still a little confusing.

Since I am able in 1.2.0 to write to a partitioned Hive by registering my
SchemaRDD and calling INSERT into ""the hive partitionned table"" SELECT ""the
registrered"", what is the write-path in this case ? Full Hive with a
SparkSQL<->Hive bridge ?
If that were the case, why wouldn't SKEWED ON be honored (see another
thread I opened).

Thanks
"
Sean Owen <sowen@cloudera.com>,"Mon, 23 Feb 2015 14:05:13 +0000",Re: [VOTE] Release Apache Spark 1.3.0 (RC1),Corey Nolet <cjnolet@gmail.com>,"Yes my understanding from Patrick's comment is that this RC will not
be released, but, to keep testing. There's an implicit -1 out of the
gates there, I believe, and so the vote won't pass, so perhaps that's
why there weren't further binding votes. I'm sure that will be
formalized shortly.

FWIW here are 10 issues still listed as blockers for 1.3.0:

SPARK-5910 DataFrame.selectExpr(""col as newName"") does not work
SPARK-5904 SPARK-5166 DataFrame methods with varargs do not work in Java
SPARK-5873 Can't see partially analyzed plans
SPARK-5546 Improve path to Kafka assembly when trying Kafka Python API
SPARK-5517 SPARK-5166 Add input types for Java UDFs
SPARK-5463 Fix Parquet filter push-down
SPARK-5310 SPARK-5166 Update SQL programming guide for 1.3
SPARK-5183 SPARK-5180 Document data source API
SPARK-3650 Triangle Count handles reverse edges incorrectly
SPARK-3511 Create a RELEASE-NOTES.txt file in the repo



---------------------------------------------------------------------


"
Corey Nolet <cjnolet@gmail.com>,"Mon, 23 Feb 2015 09:13:15 -0500",Re: [VOTE] Release Apache Spark 1.3.0 (RC1),Sean Owen <sowen@cloudera.com>,"Thanks Sean. I glossed over the comment about SPARK-5669.


"
Patrick Wendell <pwendell@gmail.com>,"Mon, 23 Feb 2015 10:18:06 -0800",Re: [VOTE] Release Apache Spark 1.3.0 (RC1),Corey Nolet <cjnolet@gmail.com>,"So actually, the list of blockers on JIRA is a bit outdated. These
days I won't cut RC1 unless there are no known issues that I'm aware
of that would actually block the release (that's what the snapshot
ones are for). I'm going to clean those up and push others to do so
also.

The main issues I'm aware of that came about post RC1 are:
1. Python submission broken on YARN
2. The license issue in MLlib [now fixed].
3. Varargs broken for Java Dataframes [now fixed]

Re: Corey - yeah, as it stands now I try to wait if there are things
that look like implicit -1 votes.


---------------------------------------------------------------------


"
Marcelo Vanzin <vanzin@cloudera.com>,"Mon, 23 Feb 2015 10:29:50 -0800",Re: [VOTE] Release Apache Spark 1.3.0 (RC1),Patrick Wendell <pwendell@gmail.com>,"Hey Patrick,

Do you have a link to the bug related to Python and Yarn? I looked at
the blockers in Jira but couldn't find it.


-- 
Marcelo

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Mon, 23 Feb 2015 10:31:09 -0800",Re: [VOTE] Release Apache Spark 1.3.0 (RC1),Marcelo Vanzin <vanzin@cloudera.com>,"It's only been reported on this thread by Tom, so far.


---------------------------------------------------------------------


"
Marcelo Vanzin <vanzin@cloudera.com>,"Mon, 23 Feb 2015 10:33:41 -0800",Re: [VOTE] Release Apache Spark 1.3.0 (RC1),Tom Graves <tgraves_cs@yahoo.com>,"Hi Tom, are you using an sbt-built assembly by any chance? If so, take
a look at SPARK-5808.

I haven't had any problems with the maven-built assembly. Setting
SPARK_HOME on the executors is a workaround if you want to use the sbt
assembly.

 I see the following error when doing the collect:
 was:  /grid/3/tmp/yarn-local/usercache/tgraves/filecache/20/spark-assembly-1.3.0-hadoop2.6.0.1.1411101121.jarjava.io.EOFException        at java.io.DataInputStream.readInt(DataInputStream.java:392)        at org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:163)        at org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:86)        at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:62)        at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:105)        at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:69)        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)        at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:308)        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)        at org.apache.spark.scheduler.Task.run(Task.scala:64)        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:197)        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)        at java.lang.Thread.run(Thread.java:722)
 1.3.0!
0d4a6b26504916816d7aefcf3132cd1da6c2



-- 
Marcelo

---------------------------------------------------------------------


"
mkhaitman <mark.khaitman@chango.com>,"Mon, 23 Feb 2015 11:53:31 -0700 (MST)",StreamingContext textFileStream question,dev@spark.apache.org,"Hello,

I was interested in creating a StreamingContext textFileStream based job,
which runs for long durations, and can also recover from prolonged driver
failure... It seems like StreamingContext checkpointing is mainly used for
the case when the driver dies during the processing of an RDD, and to
recover that one RDD, but my question specifically relates to whether there
is a way to also recover which files were missed between the timeframe of
the driver dying and being started back up (whether manually or
automatically).

Any assistance/suggestions with this one would be greatly appreciated!

Thanks,
Mark.



--

---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Mon, 23 Feb 2015 11:13:02 -0800","[jenkins infra -- pls read ] installing anaconda, moving default
 python from 2.6 -> 2.7","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","good morning, developers!

TL;DR:

i will be installing anaconda and setting it in the system PATH so that
your python will default to 2.7, as well as it taking over management of
all of the sci-py packages.  this is potentially a big change, so i'll be
testing locally on my staging instance before deployment to the wide world.

deployment is *tentatively* next monday, march 2nd.

a little background:

the jenkins test infra is currently (and happily) managed by a set of tools
that allow me to set up and deploy new workers, manage their packages and
make sure that all spark and research projects can happily and successfully
build.

we're currently at the state where ~50 or so packages are installed and
configured on each worker.  this is getting a little cumbersome, as the
package-to-build dep tree is getting pretty large.

the biggest offender is the science-based python infrastructure.
 everything is blindly installed w/yum and pip, so it's hard to control
*exactly* what version of any given library is as compared to what's on a
dev's laptop.

the solution:

anaconda (https://store.continuum.io/cshop/anaconda/)!  everything is
centralized!  i can manage specific versions much easier!

what this means to you:

* python 2.7 will be the default system python.
* 2.6 will still be installed and available (/usr/bin/python or
/usr/bin/python/2.6)

what you need to do:
* install anaconda, have it update your PATH
* build locally and try to fix any bugs (for spark, this ""should just work"")
* if you have problems, reach out to me and i'll see what i can do to help.
 if we can't get your stuff running under python2.7, we can default to 2.6
via a job config change.

what i will be doing:
* setting up anaconda on my staging instance and spot-testing a lot of
builds before deployment

please let me know if there are any issues/concerns...  i'll be posting
updates this week and will let everyone know if there are any changes to
the Plan[tm].

your friendly devops engineer,

shane
"
"""Shao, Saisai"" <saisai.shao@intel.com>","Mon, 23 Feb 2015 19:17:05 +0000",RE: StreamingContext textFileStream question,mkhaitman <mark.khaitman@chango.com>,"Hi Mark,

For input streams like text input stream, only RDDs can be recovered from checkpoint, no missed files, if file is missed, actually an exception will be raised. If you use HDFS, HDFS will guarantee no data loss since it has 3 copies.Otherwise user logic has to guarantee no file deleted before recovering.

For input stream which is receiver based, like Kafka input stream or socket input stream, a WAL(write ahead log) mechanism can be enabled to store the received data as well as metadata, so data can be recovered from failure.

Thanks
Jerry


I was interested in creating a StreamingContext textFileStream based job, which runs for long durations, and can also recover from prolonged driver failure... It seems like StreamingContext checkpointing is mainly used for the case when the driver dies during the processing of an RDD, and to recover that one RDD, but my question specifically relates to whether there is a way to also recover which files were missed between the timeframe of the driver dying and being started back up (whether manually or automatically).

Any assistance/suggestions with this one would be greatly appreciated!

Thanks,
Mark.



--
3.nabble.com/StreamingContext-textFileStream-question-tp10742.html
om.

---------------------------------------------------------------------
mands, e-mail: dev-help@spark.apache.org


---------------------------------------------------------------------


"
mkhaitman <mark.khaitman@chango.com>,"Mon, 23 Feb 2015 12:32:40 -0700 (MST)",RE: StreamingContext textFileStream question,dev@spark.apache.org,"Hi Jerry,

Thanks for the quick response! Looks like I'll need to come up with an
alternative solution in the meantime,  since I'd like to avoid the other
input streams + WAL approach. :)

Thanks again,
Mark.



--

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 23 Feb 2015 19:36:44 +0000","Re: [jenkins infra -- pls read ] installing anaconda, moving default
 python from 2.6 -> 2.7","shane knapp <sknapp@berkeley.edu>, amp-infra <amp-infra@googlegroups.com>, 
	dev <dev@spark.apache.org>","The first concern for Spark will probably be to ensure that we still build
and test against Python 2.6, since that's the minimum version of Python we
support.

Otherwise this seems OK. We use numpy and other Python packages in PySpark,
but I don't think we're pinned to any particular version of those packages.

Nick


"
shane knapp <sknapp@berkeley.edu>,"Mon, 23 Feb 2015 11:50:04 -0800","Re: [jenkins infra -- pls read ] installing anaconda, moving default
 python from 2.6 -> 2.7",amp-infra <amp-infra@googlegroups.com>,"
 this could allow you to easily differentiate between ""baseline"" and
""latest and greatest"" if you wanted.  it'll have a little bit more
administrative overhead, due to more jobs needing configs, but offers more
flexibility.

let me know what you think.



shane
"
Michael Armbrust <michael@databricks.com>,"Mon, 23 Feb 2015 12:18:08 -0800",Re: [VOTE] Release Apache Spark 1.3.0 (RC1),Mark Hamstra <mark@clearstorydata.com>,"

I would definitely expect this to build and we do actually test that for
each PR.  We don't yet run the tests for both versions of Hive and thus
unfortunately these do get out of sync.  Usually these are just problems
diff-ing golden output or cases where we have added a test that uses a
feature not available in hive 12.

Have you seen problems with using Hive 12 outside of these test failures?
"
Mark Hamstra <mark@clearstorydata.com>,"Mon, 23 Feb 2015 12:30:53 -0800",Re: [VOTE] Release Apache Spark 1.3.0 (RC1),Michael Armbrust <michael@databricks.com>,"Nothing that I can point to, so this may only be a problem in test scope.
I am looking at a problem where some UDFs that run with 0.12 fail with
0.13; but that problem is already present in Spark 1.2.x, so it's not a
blocking regression for 1.3.  (Very likely a HiveFunctionWrapper serde
problem, but I haven't run it to ground yet.)


"
Cheng Lian <lian.cs.zju@gmail.com>,"Tue, 24 Feb 2015 10:45:28 +0800",Re: [VOTE] Release Apache Spark 1.3.0 (RC1),"Michael Armbrust <michael@databricks.com>, 
 Mark Hamstra <mark@clearstorydata.com>","My bad, had once fixed all Hive 12 test failures in PR #4107, but didn't 
got time to get it merged.

Considering the release is close, I can cherry-pick those Hive 12 fixes 
from #4107 and open a more surgical PR soon.

Cheng



---------------------------------------------------------------------


"
Cheng Lian <lian.cs.zju@gmail.com>,"Tue, 24 Feb 2015 11:08:45 +0800","Re: Spark SQL, Hive & Parquet data types","The Watcher <watcherfr@gmail.com>, dev@spark.apache.org","Ah, sorry for not being clear enough.

So now in Spark 1.3.0, we have two Parquet support implementations, the 
old one is tightly coupled with the Spark SQL framework, while the new 
one is based on data sources API. In both versions, we try to intercept 
operations over Parquet tables registered in metastore when possible for 
better performance (mainly filter push-down optimization and extra 
metadata for more accurate schema inference). The distinctions are:

 1.

    For old version (set |spark.sql.parquet.useDataSourceApi| to |false|)

    When |spark.sql.hive.convertMetastoreParquet| is set to |true|, we
    ‚Äúhijack‚Äù the read path. Namely whenever you query a Parquet table
    registered in metastore, we‚Äôre using our own Parquet implementation.

    For write path, we fallback to default Hive SerDe implementation
    (namely Spark SQL‚Äôs |InsertIntoHiveTable| operator).

 2.

    For new data source version (set
    |spark.sql.parquet.useDataSourceApi| to |true|, which is the default
    value in master and branch-1.3)

    When |spark.sql.hive.convertMetastoreParquet| is set to |true|, we
    ‚Äúhijack‚Äù both read and write path, but if you‚Äôre writing to a
    partitioned table, we still fallback to default Hive SerDe
    implementation.

For Spark 1.2.0, only 1 applies. Spark 1.2.0 also has a Parquet data 
source, but it‚Äôs not enabled if you‚Äôre not using data sources API 
specific DDL (|CREATE TEMPORARY TABLE <table-name> USING <data-source>|).

Cheng


‚Äã
"
Soumitra Kumar <kumar.soumitra@gmail.com>,"Mon, 23 Feb 2015 19:23:38 -0800",Re: [VOTE] Release Apache Spark 1.3.0 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1 (non-binding)

For: https://issues.apache.org/jira/browse/SPARK-3660

. Docs OK
. Example code is good

-Soumitra.



"
Tathagata Das <tathagata.das1565@gmail.com>,"Mon, 23 Feb 2015 22:52:44 -0800",Re: [VOTE] Release Apache Spark 1.3.0 (RC1),Soumitra Kumar <kumar.soumitra@gmail.com>,"Hey all,

I found a major issue where JobProgressListener (a listener used to keep
track of jobs for the web UI) never forgets stages in one of its data
structures. This is a blocker for long running applications.
https://issues.apache.org/jira/browse/SPARK-5967

I am testing a fix for this right now.

TD


"
Judy Nash <judynash@exchange.microsoft.com>,"Tue, 24 Feb 2015 08:40:48 +0000",RE: spark slave cannot execute without admin permission on windows,"Judy Nash <judynash@exchange.microsoft.com>, Akhil Das
	<akhil@sigmoidanalytics.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Update to the thread.

Upon investigation, this is a bug on windows. Windows does not grant user permission read permission to jar files by default.
Have created a pull request for SPARK-5914<https://issues.apache.org/jira/browse/SPARK-5914> to grant read permission to jar owner (slave service account in this case). With this fix, slave will be able to run without admin permission.
FYI: master & thrift server works fine with only user permission, so no issue there.

From: Judy Nash [mailto:judynash@exchange.microsoft.com]
Sent: Thursday, February 19, 2015 12:26 AM
To: Akhil Das; dev@spark.apache.org
Cc: user@spark.apache.org
Subject: RE: spark slave cannot execute without admin permission on windows

+ dev mailing list

If this is supposed to work, is there a regression then?

The spark core code shows the permission for copied file to \work is set to a+x at Line 442 of Utils.scala<https://github.com/apache/spark/blob/b271c265b742fa6947522eda4592e9e6a7fd1f3a/core/src/main/scala/org/apache/spark/util/Utils.scala> .
The example jar I used had all permissions including Read & Execute prior spark-submit:
[cid:image001.png@01D04FCA.85961CE0]
However after copied to worker node‚Äôs \work folder, only limited permission left on the jar with no execution right.
[cid:image002.png@01D04FCA.85961CE0]

From: Akhil Das [mailto:akhil@sigmoidanalytics.com]
Sent: Wednesday, February 18, 2015 10:er@spark.apache.org>
Subject: Re: spark slave cannot execute without admin permission on windows

You need not require admin permission, but just make sure all those jars has execute permission ( read/write access)

Thanks
Best Regards

On Thu, Feb 19, 2015 at 11:30 AM, Judy Nash <judynash@exchange.microsoft.com<mailto:judynash@exchange.microsoft.com>> wrote:
Hi,

Is it possible to configure spark to run without admin permission on windows?

My current setup run master & slave successfully with admin permission.
However, if I downgrade permission level from admin to user, SparkPi fails with the following exception on the slave node:
Exception in thread ""main"" org.apache.spark.SparkException: Job aborted due to s
tage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task
0.3 in stage 0.0 (TID 9, workernode0.jnashsparkcurr2.d10.internal.cloudapp.net<http://workernode0.jnashsparkcurr2.d10.internal.cloudapp.net>)
: java.lang.ClassNotFoundException: org.apache.spark.examples.SparkPi$$anonfun$1

        at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:270)

Upon investigation, it appears that sparkPi jar under spark_home\worker\appname\*.jar does not have execute permission set, causing spark not able to find class.

Advice would be very much appreciated.

Thanks,
Judy


"
nitin <nitin2goyal@gmail.com>,"Tue, 24 Feb 2015 08:46:54 -0700 (MST)","Does Spark delete shuffle files of lost executor in running
 system(on YARN)?",dev@spark.apache.org,"Hi All,

I noticed that Spark doesn't delete local shuffle files of a lost executor
in a running system(running in yarn-client mode). For long running system,
this might fill up disk space in case of frequent executor failures. Can we
delete these files when executor loss reported to driver?

Thanks
-Nitin



--

---------------------------------------------------------------------


"
Xiangrui Meng <mengxr@gmail.com>,"Tue, 24 Feb 2015 13:33:20 -0800",Re: Google Summer of Code - ideas,Manoj Kumar <manojkumarsivaraj334@gmail.com>,"Would you be interested in working on MLlib's Python API during the
summer? We want everything we implemented in Scala can be used in both
Java and Python, but we are not there yet. It would be great if
someone is willing to help. -Xiangrui


---------------------------------------------------------------------


"
mike@mbowles.com,"Tue, 24 Feb 2015 21:56:51 +0000",Re:  Have Friedman's glmnet algo running in Spark,"""Joseph Bradley"" <joseph@databricks.com>"," Joseph, 
Thanks for your reply. We'll take the steps you suggest - generate some timing comparisons and post them in the GLMNET JIRA with a link from the OWLQN JIRA. 

We've got the regression version of GLMNET programmed. The regression version only requires a pass through the data each time the active set of coefficients changes. That's usualy less than or equal to the number of decrements in the penalty coefficient (typical default = 100). The intermediate iterations can be done using results of previous passes through the full data set. We're expecting the number of data passes will be independent of either number of rows or columns in the data set. We're eager to demonstrate this scaling. Do you have any suggestions regarding data sets for large scale regression problems? It would be nice to demonstrate scaling for both number of rows and number of columns. 

Thanks for your help. 
Mike

Hi Mike,glmnet has definitely been very successful, and it would be great to seehow we can improve optimization in MLlib! There is some related workongoing; here are the JIRAs:GLMNET implementation in SparkLinearRegression with L1/L2 (elastic net) using OWLQN in new ML packageThe GLMNET JIRA has actually been closed in favor of the latter JIRA.However, if you're getting good results in your experiments, could youplease post them on the GLMNET JIRA and link them from the other JIRA? Ifit's faster and more scalable, that would be great to find out.As far as where the code should go and the APIs, that can be discussed onthe JIRA.I hope this helps, and  several versions of glmnet algo> coded and running on Spark RDD. glmnet algo (> http://www.jstatsoft.org/v33/i01/paper) is a very fast algorithm for> generating coefficient paths solving penalized regression with elastic net> penalties. The algorithm runs fast by taking an approach that generates> solutions for a wide variety of penalty parameter. We're able to integrate> into Mllib class structure a couple of different ways. The algorithm may> fit better into the new pipeline structure since it naturally returns a> multitide of models (corresponding to different vales of penalty> parameters). That appears to fit better into pipeline than Mllib linear> regression (for example).>> We've got regression running with the speed optimizations that Friedman> recommends. We'll start working on the logistic regression version next.>> We're eager to make the code available as open source and would like to> get some feedback about how best to do that. Any thoughts?> Mike Bowles.>>>
"
Michael Nazario <mnazario@palantir.com>,"Tue, 24 Feb 2015 22:54:23 +0000",PySpark SPARK_CLASSPATH doesn't distribute jars to executors,"""dev@spark.apache.org"" <dev@spark.apache.org>","Has anyone experienced a problem with the SPARK_CLASSPATH not distributing jars for PySpark? I have a detailed description of what I tried in the ticket below, and this seems like a problem that is not a configuration problem. The only other case I can think of is that configuration changed between Spark 1.1.1 and Spark 1.2.1 about distributing jars for PySpark.

https://issues.apache.org/jira/browse/SPARK-5977

Thanks,
Michael
"
Mike Hynes <91mbbh@gmail.com>,"Tue, 24 Feb 2015 18:02:12 -0500","[ERROR] bin/compute-classpath.sh: fails with false positive test for
 java 1.7 vs 1.6",dev@spark.apache.org,"./bin/compute-classpath.sh fails with error:

$> jar -tf assembly/target/scala-2.10/spark-assembly-1.3.0-SNAPSHOT-hadoop1.0.4.jar
nonexistent/class/path
java.util.zip.ZipException: invalid CEN header (bad signature)
	at java.util.zip.ZipFile.open(Native Method)
	at java.util.zip.ZipFile.<init>(ZipFile.java:132)
	at java.util.zip.ZipFile.<init>(ZipFile.java:93)
	at sun.tools.jar.Main.list(Main.java:997)
	at sun.tools.jar.Main.run(Main.java:242)
	at sun.tools.jar.Main.main(Main.java:1167)

However, I both compiled the distribution and am running spark with Java 1.7;
$ java -version
	java version ""1.7.0_75""
	OpenJDK Runtime Environment (IcedTea 2.5.4) (7u75-2.5.4-1~trusty1)
	OpenJDK 64-Bit Server VM (build 24.75-b04, mixed mode)
on a system running Ubuntu:
$ uname -srpov
Linux 3.13.0-44-generic #73-Ubuntu SMP Tue Dec 16 00:22:43 UTC 2014
x86_64 GNU/Linux
$ uname -srpo
Linux 3.13.0-44-generic x86_64 GNU/Linux

This problem was reproduced on Arch Linux:

$ uname -srpo
Linux 3.18.5-1-ARCH x86_64 GNU/Linux
with
$ java -version
java version ""1.7.0_75""
OpenJDK Runtime Environment (IcedTea 2.5.4) (Arch Linux build
7.u75_2.5.4-1-x86_64)
OpenJDK 64-Bit Server VM (build 24.75-b04, mixed mode)

In both of these cases, the problem is not the java versioning;
neither system even has a java 6 installation. This seems like a false
positive to me in compute-classpath.sh.

When I comment out the relevant lines in compute-classpath.sh, the
scripts start-{master,slaves,...}.sh all run fine, and I have no
problem launching applications.

Could someone please offer some insight into this issue?

Thanks,
Mike

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Tue, 24 Feb 2015 23:07:49 +0000","Re: [ERROR] bin/compute-classpath.sh: fails with false positive test
 for java 1.7 vs 1.6",Mike Hynes <91mbbh@gmail.com>,"So you mean that the script is checking for this error, and takes it
as a sign that you compiled with java 6.

Your command seems to confirm that reading the assembly jar does fail
on your system though. What version does the jar command show? are you
sure you don't have JRE 7 but JDK 6 installed?


---------------------------------------------------------------------


"
Joseph Bradley <joseph@databricks.com>,"Tue, 24 Feb 2015 15:27:03 -0800",Re: Have Friedman's glmnet algo running in Spark,mike@mbowles.com,"Hi Mike,

I'm not aware of a ""standard"" big dataset, but there are a number available:
* The YearPredictionMSD dataset from the LIBSVM datasets is sizeable (in #
instances but not # features):
www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/regression.html
* I've used this text dataset from which one can generate lots of n-gram
features (but not many instances): http://www.ark.cs.cmu.edu/10K/
* I've seen some papers use the KDD Cup datasets, which might be the best
option I know of.  The KDD Cup 2012 track 2 one seems promising.

Good luck!
Joseph


"
Mike Hynes <91mbbh@gmail.com>,"Tue, 24 Feb 2015 18:32:37 -0500","Re: [ERROR] bin/compute-classpath.sh: fails with false positive test
 for java 1.7 vs 1.6",Sean Owen <sowen@cloudera.com>,"I don't see any version flag for /usr/bin/jar, but I think I see the
problem now; the openjdk version is 7, but javac -version gives
1.6.0_34; so spark was compiled with java 6 despite the system using
jre 1.7.
Thanks for the sanity check! Now I just need to find out why javac is
downgraded on the system..



-- 
Thanks,
Mike

---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Tue, 24 Feb 2015 16:15:28 -0800","Re: [ERROR] bin/compute-classpath.sh: fails with false positive test
 for java 1.7 vs 1.6",Mike Hynes <91mbbh@gmail.com>,"it's not downgraded, it's your /etc/alternatives setup that's causing this.

you can update all of those entries by executing the following commands (as
root):

update-alternatives --install ""/usr/bin/java"" ""java""
""/usr/java/latest/bin/java"" 1
update-alternatives --install ""/usr/bin/javah"" ""javah""
""/usr/java/latest/bin/javah"" 1
update-alternatives --install ""/usr/bin/javac"" ""javac""
""/usr/java/latest/bin/javac"" 1
update-alternatives --install ""/usr/bin/jar"" ""jar""
""/usr/java/latest/bin/jar"" 1

(i have the latest jdk installed in /usr/java/.... with a /usr/java/latest/
symlink pointing to said jdk's dir)

assembly/target/scala-2.10/spark-assembly-1.3.0-SNAPSHOT-hadoop1.0.4.jar
Java
"
Andrew Ash <andrew@andrewash.com>,"Tue, 24 Feb 2015 17:57:47 -0800",Re: Streaming partitions to driver for use in .toLocalIterator,Mingyu Kim <mkim@palantir.com>,"I think a cheap way to repartition to a higher partition count without
shuffle would be valuable too.  Right now you can choose whether to execute
a shuffle when going down in partition count, but going up in partition
count always requires a shuffle.  For the need of having a smaller
partitions to make .toLocalIterator more efficient, no shuffle on increase
of partition count is necessary.

Filed as https://issues.apache.org/jira/browse/SPARK-5997


"
Denny Lee <denny.g.lee@gmail.com>,"Wed, 25 Feb 2015 03:21:44 +0000",Re: PySpark SPARK_CLASSPATH doesn't distribute jars to executors,"Michael Nazario <mnazario@palantir.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Can you try extraClassPath or driver-class-path and see if that helps with
the distribution?

"
Reynold Xin <rxin@databricks.com>,"Tue, 24 Feb 2015 21:54:39 -0800",Help vote for Spark talks at the Hadoop Summit,"""dev@spark.apache.org"" <dev@spark.apache.org>, user <user@spark.apache.org>","Hi all,

The Hadoop Summit uses community choice voting to decide which talks to
feature. It would be great if the community could help vote for Spark talks
so that Spark has a good showing at this event. You can make three votes on
each track. Below I've listed 3 talks that are important to Spark's
roadmap. Please give 3 votes to each of the following talks.

Committer Track: Lessons from Running Ultra Large Scale Spark Workloads on
Hadoop
https://hadoopsummit.uservoice.com/forums/283260-committer-track/suggestions/7074016

Data Science track: DataFrames: large-scale data science on Hadoop data
with Spark
https://hadoopsummit.uservoice.com/forums/283261-data-science-and-hadoop/suggestions/7074147

https://hadoopsummit.uservoice.com/forums/283266-the-future-of-apache-hadoop/suggestions/7074424


Thanks!
"
Cody Koeninger <cody@koeninger.org>,"Wed, 25 Feb 2015 09:53:05 -0600",UnusedStubClass in 1.3.0-rc1,"""dev@spark.apache.org"" <dev@spark.apache.org>","So when building 1.3.0-rc1 I see the following warning:

[WARNING] spark-streaming-kafka_2.10-1.3.0.jar, unused-1.0.0.jar define 1
overlappping classes:

[WARNING]   - org.apache.spark.unused.UnusedStubClass


and when trying to build an assembly of a project that was previously using
1.3 snapshots without difficulty, I see the following errors:


[error] (*:assembly) deduplicate: different file contents found in the
following:

[error]
/Users/cody/.m2/repository/org/apache/spark/spark-streaming-kafka_2.10/1.3.0/spark-streaming-kafka_2.10-1.3.0.jar:org/apache/spark/unused/UnusedStubClass.class

[error]
/Users/cody/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:org/apache/spark/unused/UnusedStubClass.class


This persists even after a clean / rebuild of both 1.3.0-rc1 and the
project using it.


I can just exclude that jar in the assembly definition, but is anyone else
seeing similar issues?  If so, might be worth resolving rather than make
users mess with assembly exclusions.

I see that this class was introduced a while ago, related to SPARK-3812 but
the jira issue doesn't have much detail.
"
Debasish Das <debasish.das83@gmail.com>,"Wed, 25 Feb 2015 08:50:55 -0800",Re: Have Friedman's glmnet algo running in Spark,Joseph Bradley <joseph@databricks.com>,"Any reason why the regularization path cannot be implemented using current
owlqn pr ?

We can change owlqn in breeze to fit your needs...

"
Patrick Wendell <pwendell@gmail.com>,"Wed, 25 Feb 2015 09:41:29 -0800",Re: UnusedStubClass in 1.3.0-rc1,Cody Koeninger <cody@koeninger.org>,"Hey Cody,

What build command are you using? In any case, we can actually comment
out the ""unused"" thing now in the root pom.xml. It existed just to
ensure that at least one dependency was listed in the shade plugin
configuration (otherwise, some work we do that requires the shade
plugin does not happen). However, now there are other things there. If
you just comment out the line in the root pom.xml adding this
dependency, does it work?

- Patrick


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Wed, 25 Feb 2015 09:42:02 -0800",Re: UnusedStubClass in 1.3.0-rc1,Cody Koeninger <cody@koeninger.org>,"This has been around for multiple versions of Spark, so I am a bit
surprised to see it not working in your build.

- Patrick


---------------------------------------------------------------------


"
Cody Koeninger <cody@koeninger.org>,"Wed, 25 Feb 2015 12:28:05 -0600",Re: UnusedStubClass in 1.3.0-rc1,Patrick Wendell <pwendell@gmail.com>,"I'm building with

mvn -Phadoop-2.4 -DskipTests install


Yeah, commenting out the unused dependency in the root pom.xml resolves
it.  I'm a little surprised that it cropped up now as well, I had built
against multiple different snapshots of 1.3 over the past couple weeks with
no problems.


Is it worth me putting in a PR to remove that class and the dependency
altogether?


"
Xiangrui Meng <mengxr@gmail.com>,"Wed, 25 Feb 2015 10:33:21 -0800",Re: Help vote for Spark talks at the Hadoop Summit,Reynold Xin <rxin@databricks.com>,"Made 3 votes to each of the talks. Looking forward to see them in
Hadoop Summit:) -Xiangrui


---------------------------------------------------------------------


"
mike@mbowles.com,"Wed, 25 Feb 2015 18:35:14 +0000",Re:  Have Friedman's glmnet algo running in Spark,"""Debasish Das"" <debasish.das83@gmail.com>,
        ""Joseph Bradley"" 
    <joseph@databricks.com>"," Hi Debasish, 
Any method that generates point solutions to the minimization problem could simply be run a number of times to generate the coefficient paths as a function of the penalty parameter. I think the only issues are how easy the method is to use and how much training and developer time is required to produce an answer. 

With regard to training time, Friedman says in his paper that they found problems where glmnet would generate the entire coefficient path more rapidly than sophisticated single point methods would generate single point solutions - not all problems, but some problems. Ryan Tibshirani (Robert's son) who's a professor and researcher at CMU in convex function optimization has echoed that assertion for the particular case of the elasticnet penalty function (that's from slides of his that are available online). So there's an open question about the training speed that i believe we can answer in fairly short order. I'm eager to explore that. Does OWLQN do a pass through the data for each iteration? The linear version of GLMNET ugh parameter space. 

With regard to developer time, glmnet doesn't require the user to supply a starting point for the penalty parameter. It calculates the starting point. That makes it completely automatic. you've probably been through the process of manually searching regularization parameter space with SVM. Pick out a set of regularization parameter values like 10 raised to the (-2 through +5 in steps of 1). See if there's a minimum in the range and ift for a new problem is that you just drop in the training set and out popr so) it doesn't converge. It alerts you that it didn't converge and you change one parameter and rerun. If you also drop in a test set then it even picks the optimum solution andproduces an estimate of out-of-sample error. 

We're going to make some speed/scaling runs on the synthetic data sets (in a range of sizes) that are used in Spark for testing linear regression. We need some wider data sets. Joseph mentioned some that we'll look at. I've got a gene expression data set that's 30k wide by 15k tall. That takes a few hours to train using R version of glmnet. We're also talking to some biology friends to find other interesting data sets. 

I really am eager to see the comparisons. And happy to help you tailor OWLQN to generate coefficient paths. We might be able to produce a hybrid of Friedman's algorithm using his basic algorithm outline but substituting OWLQN for his round-robin coordinate descent. But i'm a little cocerned that it's the round-robin coordinate descent that makes it possible to skip passing through the full data set for 4 out of 5 iterations. We might be able to work a way around that. 

I'm just eager to have parallel versions of the tools available. I'll keep you posted on our results. We should aim for running one another's code. I'll check with my colleagues and see when we'll have something we can hand out. We've delayed putting together a release version in favor of generating some scaling results, as Joseph suggested. Discussions like this may have some impact on what the release code looks like. 
Mike






Any reason why the regularization path cannot be implemented using current owlqn pr ?
We can change owlqn in breeze to fit your needs...

Hi Mike,

I'm not aware of a ""standard"" big dataset, but there are a number available:
* The YearPredictionMSD dataset from the LIBSVM datasets is sizeable (in #
instances but not # features):
www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/regression.html
* I've used this text dataset from which one can generate lots of n-gram
features (but not many instances): http://www.ark.cs.cmu.edu/10K/
* I've seen some papers use the KDD Cup datasets, which might be the best
option I know of. The KDD Cup 2012 track 2 one seems promising.

Good luck!
Joseph



e
of

ll
re
g
eat
work

If
as
A. I
eph
d
net
es >
ate
may
 a >
 >
ed
w


"
shane knapp <sknapp@berkeley.edu>,"Wed, 25 Feb 2015 10:37:45 -0800","Re: [jenkins infra -- pls read ] installing anaconda, moving default
 python from 2.6 -> 2.7","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","i'm going to punt on this until after the next spark 1.3 release (2-3
weeks?).  since i'll be installing a bunch of other packages (including
mongodb), i'd rather wait and be safe.  :)

the full install list is forthcoming, and i'll update the spark infra wiki
w/what's installed on the workers.

shane


"
Michael Nazario <mnazario@palantir.com>,"Wed, 25 Feb 2015 18:45:28 +0000",RE: PySpark SPARK_CLASSPATH doesn't distribute jars to executors,"Denny Lee <denny.g.lee@gmail.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","Neither of those helped. I'm still getting a ClassNotFoundException on the workers.
________________________________
From: Denny Lee [denny.g.lee@gmail.com]
Sent: Tuesday, February 24, 2015 7:21 PM
To: Michael Nazario; dev@spark.apache.org
Subject: Re: PySpark SPARK_CLASSPATH doesn't distribute jars to executors

Can you try extraClassPath or driver-class-path and see if that helps with the distribution?
Has anyone experienced a problem with the SPARK_CLASSPATH not distributing jars for PySpark? I have a detailed description of what I tried in the ticket below, and this seems like a problem that is not a configuration problem. The only other case I can think of is that configuration changed between Spark 1.1.1 and Spark 1.2.1 about distributing jars for PySpark.

https://issues.apache.org/jira/browse/SPARK-5977<https://urldefense.proofpoint.com/v2/url?u=https-3A__issues.apache.org_jira_browse_SPARK-2D5977&dKoYoLUUIQViRLGShPc1wislP1YdU4g&m=REuScW0kzWb6UlI-_aLgZdCGZ62fF2vfW9gRHXsxt_g&s=ZwF_JWb5QHA4P9d9XuZU0u7ZGw-00kOSgfomsg_vREE&e=>

Thanks,
Michael
"
Joseph Bradley <joseph@databricks.com>,"Wed, 25 Feb 2015 11:14:52 -0800",Re: Have Friedman's glmnet algo running in Spark,mike@mbowles.com,"Some of this discussion seems valuable enough to preserve on the JIRA; can
we move it there (and copy any relevant discussions from previous emails as
needed)?


"
Devl Devel <devl.development@gmail.com>,"Wed, 25 Feb 2015 22:13:09 +0000",Some praise and comments on Spark,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Spark Developers,

First, apologies if this doesn't belong on this list but the
comments/praise are relevant to all developers. This is just a small note
about what we really like about Spark, I/we don't mean to start a whole
long discussion thread in this forum but just share our positive
experiences with Spark thus far.

To start, as you can tell, we think that the Spark project is amazing and
we love it! Having put in nearly half a decade worth of sweat and tears
into production Hadoop, MapReduce clusters and application development it's
so refreshing to see something arguably simpler and more elegant to
supersede it.

These are the things we love about Spark and hope these principles continue:

-the one command build; make-distribution.sh. Simple, clean  and ideal for
deployment and devops and rebuilding on different environments and nodes.
-not having too much runtime and deploy config; as admins and developers we
are sick of setting props like io.sort and mapred.job.shuffle.merge.percent
and dfs file locations and temp directories and so on and on again and
again every time we deploy a job, new cluster, environment or even change
company.
-a fully built-in stack, one global project for SQL, dataframes, MLlib etc,
so there is no need to add on projects to it on as per Hive, Hue, Hbase
etc. This helps life and keeps everything in one place.
-single (global) user based operation - no creation of a hdfs mapred unix
user, makes life much simpler
-single quick-start daemons; master and slaves. Not having to worry about
JT, NN, DN , TT, RM, Hbase master ... and doing netstat and jps on hundreds
of clusters makes life much easier.
-proper code versioning, feature releases and release management.
- good & well organised documentation with good examples.

In addition to the comments above this is where we hope Spark never ends
up:

-tonnes of configuration properties and ""go faster"" type flags. For example
Hadoop and Hbase users will know that there are a whole catalogue of
properties for regions, caches, network properties, block sizes, etc etc.
Please don't end up here for example:
https://hadoop.apache.org/docs/r1.0.4/mapred-default.html, it is painful
having to configure all of this and then create a set of properties for
each environment and then tie this into CI and deployment tools.
-no more daemons and processes to have to monitor and manipulate and
restart and crash.
-a project that penalises developers (that will ultimately help promote
Spark to their managers and budget holders) with expensive training,
certification, books and accreditation. Ideally this open source should be
free, free training= more users = more commercial uptake.

Anyway, those are our thoughts for what they are worth, keep up the good
work, we just had to mention it. Again sorry if this is not the right place
or if there is another forum for this stuff.

Cheers
"
Reynold Xin <rxin@databricks.com>,"Wed, 25 Feb 2015 14:36:19 -0800",Re: Some praise and comments on Spark,Devl Devel <devl.development@gmail.com>,"Thanks for the email and encouragement, Devl. Responses to the 3 requests:

-tonnes of configuration properties and ""go faster"" type flags. For example
Hadoop and Hbase users will know that there are a whole catalogue of
properties for regions, caches, network properties, block sizes, etc etc.
Please don't end up here for example:
https://hadoop.apache.org/docs/r1.0.4/mapred-default.html, it is painful
having to configure all of this and then create a set of properties for
each environment and then tie this into CI and deployment tools.

As the project grows, it is unavoidable to introduce more config options,
in particular, we often use config options to test new modules that are
still experimental before making them the default (e.g. sort-based shuffle).

The philosophy here is to make it a very high bar to introduce new config
options, and make the default values sensible for most deployments, and
then whenever possible, figure out automatically what is the right setting.
Note that this in general is hard, but we expect for 99% of the users they
only need to know a very small number of options (e.g. setting the
serializer).


-no more daemons and processes to have to monitor and manipulate and
restart and crash.

At the very least you'd need the cluster manager itself to be a daemon
process because we can't defy the law of physics. But I don't think we want
to introduce anything beyond that.


-a project that penalises developers (that will ultimately help promote
Spark to their managers and budget holders) with expensive training,
certification, books and accreditation. Ideally this open source should be
free, free training= more users = more commercial uptake.

I definitely agree with you on making it easier to learn Spark. We are
making a lot of materials freely available, including two freely available
MOOCs on edX:
https://databricks.com/blog/2014/12/02/announcing-two-spark-based-moocs.html




"
Patrick Wendell <pwendell@gmail.com>,"Wed, 25 Feb 2015 14:51:27 -0800",Re: [VOTE] Release Apache Spark 1.3.0 (RC1),Tathagata Das <tathagata.das1565@gmail.com>,"Hey All,

Just a quick updated on this thread. Issues have continued to trickle
in. Not all of them are blocker level but enough to warrant another
RC:

I've been keeping the JIRA dashboard up and running with the latest
status (sorry, long link):
https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20%22Target%20Version%2Fs%22%20%3D%201.3.0%20AND%20(fixVersion%20IS%20EMPTY%20OR%20fixVersion%20!%3D%201.3.0)%20AND%20(Resolution%20IS%20EMPTY%20OR%20Resolution%20IN%20(Done%2C%20Fixed%2C%20Implemented))%20ORDER%20BY%20priority%2C%20component

continued voting!

- Patrick

.0-hadoop2.6.0.1.1411101121.jarjava.io.EOFException
Factory.scala:163)
onWorkerFactory.scala:86)
ry.scala:62)
05)
7)
8)
1)
a:1145)
va:615)
b0d4a6b26504916816d7aefcf3132cd1da6c2
at:
s
.
-

---------------------------------------------------------------------


"
"""Evan R. Sparks"" <evan.sparks@gmail.com>","Wed, 25 Feb 2015 14:53:58 -0800",Re: Using CUDA within Spark / boosting linear algebra,"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Thanks for compiling all the data and running these benchmarks, Alex. The
big takeaways here can be seen with this chart:
https://docs.google.com/spreadsheets/d/1aRm2IADRfXQV7G2vrcVh4StF50uZHl6kmAJeaZZggr0/pubchart?oid=1899767119&format=interactive

1) A properly configured GPU matrix multiply implementation (e.g.
BIDMat+GPU) can provide substantial (but less than an order of magnitude)
benefit over a well-tuned CPU implementation (e.g. BIDMat+MKL or
netlib-java+openblas-compiled).
2) A poorly tuned CPU implementation can be 1-2 orders of magnitude worse
than a well-tuned CPU implementation, particularly for larger matrices.
(netlib-f2jblas or netlib-ref) This is not to pick on netlib - this
basically agrees with the authors own benchmarks (
https://github.com/fommil/netlib-java)

I think that most of our users are in a situation where using GPUs may not
be practical - although we could consider having a good GPU backend
available as an option. However, *ALL* users of MLlib could benefit
(potentially tremendously) from using a well-tuned CPU-based BLAS
implementation. Perhaps we should consider updating the mllib guide with a
more complete section for enabling high performance binaries on OSX and
Linux? Or better, figure out a way for the system to fetch these
automatically.

- Evan




o==netlib-cublas>netlib-blas>f2jblas
5r7kwKSPkY/edit?usp=sharing
b
9
iled
m
OK
u
a)
on
s
.so
et
m
can
se
ry
m
r
s
m
by John
ib.
n
ra, it involves
ny
t
m
w
y
‚Äôt
ng
d
a
le
e
a
s
t
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 25 Feb 2015 22:57:57 +0000",Re: Some praise and comments on Spark,"Reynold Xin <rxin@databricks.com>, Devl Devel <devl.development@gmail.com>","Thanks for sharing the feedback about what works well for you!

It's nice to get that; as we all probably know, people generally reach out
only when they have problems.


"
Joseph Bradley <joseph@databricks.com>,"Wed, 25 Feb 2015 15:36:26 -0800",Re: Using CUDA within Spark / boosting linear algebra,"""Evan R. Sparks"" <evan.sparks@gmail.com>","Better documentation for linking would be very helpful!  Here's a JIRA:
https://issues.apache.org/jira/browse/SPARK-6019



AJeaZZggr0/pubchart?oid=1899767119&format=interactive
t
a
e
po==netlib-cublas>netlib-blas>f2jblas
J5r7kwKSPkY/edit?usp=sharing
ib
+
piled
 OK
t
-
 on
get
e
 can
use
.
ary
+
r
I
is
 by John
lib.
t
in
bra, it involves
nny
n
at
by
n‚Äôt
ing
id
.
h
ve
s
A
ia
f
as
ot
.
g
"
Manoj Kumar <manojkumarsivaraj334@gmail.com>,"Thu, 26 Feb 2015 09:07:13 +0530",Re: Google Summer of Code - ideas,Xiangrui Meng <mengxr@gmail.com>,"Hi,

I think that would be really good. Are there any specific issues that are
to be implemented as per priority?
"
Vikram Kone <vikramkone@gmail.com>,"Wed, 25 Feb 2015 23:56:31 -0800",Need advice for Spark newbie,dev@spark.apache.org,"Hi,
I'm a newbie when it comes to Spark and Hadoop eco system in general. Our
team has been predominantly a Microsoft shop that uses MS stack for most of
their BI needs. So we are talking SQL server  for storing relational data
and SQL Server Analysis services for building MOLAP cubes for sub-second
query analysis.
Lately, we have been hitting degradation in our cube query response times
as our data sizes grew considerably the past year. We are talking fact
tables which are in 1o-100 billions of rows range and a few dimensions in
the 10-100's of millions of rows. We tried vertically scaling up our SSAS
server but queries are still taking few minutes. In light of this, I was
entrusted with task of figuring out an open source solution that would
scale to our current and future needs for data analysis.
I looked at a bunch of open source tools like Apache Drill, Druid, AtScale,
Spark, Storm, Kylin etc and settled on exploring Spark as the first step
given it's recent rise in popularity and growing eco-system around it.
Since we are also interested in doing deep data analysis like machine
learning and graph algorithms on top our data, spark seems to be a good
solution.
I would like to build out a POC for our MOLAP cubes using spark with
HDFS/Hive as the datasource and see how it scales for our queries/measures
in real time with real data.
Roughly, these are the requirements for our team
1. Should be able to create facts, dimensions and measures from our data
sets in an easier way.
2. Cubes should be query able from Excel and Tableau.
3. Easily scale out by adding new nodes when data grows
4. Very less maintenance and highly stable for production level workloads
5. Sub second query latencies for COUNT DISTINCT measures (since majority
of our expensive measures are of this type) . Are ok with Approx Distinct
counts for better perf.

So given these requirements, is Spark the right solution to replace our
on-premise MOLAP cubes?
Are there any tutorials or documentation on how to build cubes using Spark?
Is that even possible? or even necessary? As long as our users can
pivot/slice & dice the measures quickly from client tools by dragging
dropping dimensions into rows/columns w/o the need to join to fact table,
we are ok with however the data is laid out. Doesn't have to be a cube. It
can be a flat file in hdfs for all we care. I would love to chat with some
one who has successfully done this kind of migration from OLAP cubes to
Spark in their team or company .

This is it for now. Looking forward to a great discussion.

P.S. We have decided on using Azure HDInsight as our managed hadoop system
in the cloud.
"
masaki rikitoku <rikima3132@gmail.com>,"Thu, 26 Feb 2015 18:31:44 +0900",number of partitions for hive schemaRDD,dev <dev@spark.apache.org>,"Hi all

now, I'm trying the SparkSQL with hivecontext.

when I execute the hql like the following.

---

val ctx = new org.apache.spark.sql.hive.HiveContext(sc)
import ctx._

val queries = ctx.hql(""select keyword from queries where dt =
'2015-02-01' limit 10000000"")

---

It seem that the number of the partitions ot the queries is set by 1.

Is this the specifications for schemaRDD, SparkSQL, HiveContext ?

Are there any means to set the number of partitions arbitrary value
except for explicit repartition


Masaki Rikitoku

---------------------------------------------------------------------


"
Cheng Lian <lian.cs.zju@gmail.com>,"Thu, 26 Feb 2015 18:13:52 +0800",Re: number of partitions for hive schemaRDD,"masaki rikitoku <rikima3132@gmail.com>, dev <dev@spark.apache.org>","Hi Masaki,

I guess what you saw is the partition number of the last stage, which 
must be 1 to perform the global phase of LIMIT. To tune partition number 
of normal shuffles like joins, you may resort to 
spark.sql.shuffle.partitions.

Cheng



---------------------------------------------------------------------


"
James <alcaid1801@gmail.com>,"Thu, 26 Feb 2015 20:38:26 +0800",graph.mapVertices() function obtain edge triplets with null attribute,"""dev@spark.apache.org"" <dev@spark.apache.org>","My code

```
// Initial the graph, assign a counter to each vertex that contains the
vertex id only
var anfGraph = graph.mapVertices { case (vid, _) =>
  val counter = new HyperLogLog(5)
  counter.offer(vid)
  counter
}

val nullVertex = anfGraph.triplets.filter(edge => edge.srcAttr ==
null).first
// There is an edge whose src attr is null

anfGraph.vertices.filter(_._1 == nullVertex).first
// I could see that the vertex has a not null attribute

// messages = anfGraph.aggregateMessages(msgFun, mergeMessage)   // <-
NullPointerException

```

My spark version:1.2.0

Alcaid
"
Dean Wampler <deanwampler@gmail.com>,"Thu, 26 Feb 2015 10:53:48 -0600",Re: Need advice for Spark newbie,Vikram Kone <vikramkone@gmail.com>,"Historically, many orgs. have replaced data warehouses with Hadoop clusters
and used Hive along with Impala (on Cloudera deployments) or Drill (on MapR
deployments) for SQL. Hive is older and slower, while Impala and Drill are
newer and faster, but you typically need both for their complementary
features, at least today.

Spark and Spark SQL are not yet complete replacements for them, but they'll
get there over time. The good news is, you can mix and match these tools,
as appropriate, because they can all work with the same datasets.

The challenge is all the tribal knowledge required to setup and manage
Hadoop clusters, to properly organize your data for best performance for
your needs, to use all these tools effectively, along with additional
Hadoop ETL tools, etc. Fortunately, tools like Tableau are already
integrated here.

However, none of this will be as polished and integrated as what you're
used to. You're trading that polish for greater scalability and flexibility.

HTH.


Dean Wampler, Ph.D.
Author: Programming Scala, 2nd Edition
<http://shop.oreilly.com/product/0636920033073.do> (O'Reilly)
Typesafe <http://typesafe.com>
@deanwampler <http://twitter.com/deanwampler>
http://polyglotprogramming.com


"
Steve Nunez <snunez@hortonworks.com>,"Thu, 26 Feb 2015 17:37:36 +0000",RE: Need advice for Spark newbie,Vikram Kone <vikramkone@gmail.com>,"Hi Vikram,



There was a recent presentation at Strata that you might find useful: Hive on Spark is Blazing Fast .. Or Is It?<http://www.slideshare.net/hortonworks/hive-on-spark-is-blazing-fast-or-is-it-final>



Generally those conclusions mirror my own observations: on large data sets, Hive still gives the best SQL performance and the curve drops off as the data sets get smaller. Of course if you also want to build models from the data than Spark is an attractive option with its unified programming model. HiveMall<https://github.com/myui/hivemall> might also be applicable in your case; I‚Äôve seen increasing adoption of it within certain industries.



If you are going cloud, HDInsights is a good choice. You can run both Spark and R on HDInsights<http://azure.microsoft.com/blog/2014/11/17/azure-hdinsight-clusters-can-now-be-customized-to-run-a-variety-of-hadoop-projects-including-spark-and-r/>, as well as get the newest version of Hive (0.14, with Stinger enhancements from Microsoft<http://www.slideshare.net/hugfrance/recent-enhancements-to-apache-hive-query-performance>) for ‚Äòfree‚Äô, so once you get your data into a wasb you can try all three methods and see which one works best for you. HDInsights works well for mixing & matching tools.



HTH,

-          SteveN



-----Original Message-----
From: Dean Wampler [mailto:deanwampler@gmail.com]
Sent: Thursday, 26 February, 2015 8:54
To: Vikram Kone
Cc: dev@spark.apache.org
Subject: Re: Need advice for Spark newbie



Historically, many orgs. have replaced data warehouses with Hadoop clusters and used Hive along with Impala (on Cloudera deployments) or Drill (on MapR

deployments) for SQL. Hive is older and slower, while Impala and Drill are newer and faster, but you typically need both for their complementary features, at least today.



Spark and Spark SQL are not yet complete replacements for them, but they'll get there over time. The good news is, you can mix and match these tools, as appropriate, because they can all work with the same datasets.



The challenge is all the tribal knowledge required to setup and manage Hadoop clusters, to properly organize your data for best performance for your needs, to use all these tools effectively, along with additional Hadoop ETL tools, etc. Fortunately, tools like Tableau are already integrated here.



However, none of this will be as polished and integrated as what you're used to. You're trading that polish for greater scalability and flexibility.



HTH.





Dean Wampler, Ph.D.

Author: Programming Scala, 2nd Edition

<http://shop.oreilly.com/product/0636920033073.do> (O'Reilly) Typesafe <http://typesafe.com> @deanwampler <http://twitter.com/deanwampler> http://polyglotprogramming.com



On Thu, Feb 26, 2015 at 1:56 AM, Vikram Kone <vikramkone@gmail.com<mailto:vikramkone@gmail.com>> wrote:



> Hi,

> I'm a newbie when it comes to Spark and Hadoop eco system in general.

> Our team has been predominantly a Microsoft shop that uses MS stack

> for most of their BI needs. So we are talking SQL server  for storing

> relational data and SQL Server Analysis services for building MOLAP

> cubes for sub-second query analysis.

> Lately, we have been hitting degradation in our cube query response

> times as our data sizes grew considerably the past year. We are

> talking fact tables which are in 1o-100 billions of rows range and a

> few dimensions in the 10-100's of millions of rows. We tried

> vertically scaling up our SSAS server but queries are still taking few

> minutes. In light of this, I was entrusted with task of figuring out

> an open source solution that would scale to our current and future needs for data analysis.

> I looked at a bunch of open source tools like Apache Drill, Druid,

> AtScale, Spark, Storm, Kylin etc and settled on exploring Spark as the

> first step given it's recent rise in popularity and growing eco-system around it.

> Since we are also interested in doing deep data analysis like machine

> learning and graph algorithms on top our data, spark seems to be a

> good solution.

> I would like to build out a POC for our MOLAP cubes using spark with

> HDFS/Hive as the datasource and see how it scales for our

> queries/measures in real time with real data.

> Roughly, these are the requirements for our team 1. Should be able to

> create facts, dimensions and measures from our data sets in an easier

> way.

> 2. Cubes should be query able from Excel and Tableau.

> 3. Easily scale out by adding new nodes when data grows 4. Very less

> maintenance and highly stable for production level workloads 5. Sub

> second query latencies for COUNT DISTINCT measures (since majority of

> our expensive measures are of this type) . Are ok with Approx Distinct

> counts for better perf.

>

> So given these requirements, is Spark the right solution to replace

> our on-premise MOLAP cubes?

> Are there any tutorials or documentation on how to build cubes using Spark?

> Is that even possible? or even necessary? As long as our users can

> pivot/slice & dice the measures quickly from client tools by dragging

> dropping dimensions into rows/columns w/o the need to join to fact

> table, we are ok with however the data is laid out. Doesn't have to be

> a cube. It can be a flat file in hdfs for all we care. I would love to

> chat with some one who has successfully done this kind of migration

> from OLAP cubes to Spark in their team or company .

>

> This is it for now. Looking forward to a great discussion.

>

> P.S. We have decided on using Azure HDInsight as our managed hadoop

> system in the cloud.

>
"
Sandor Van Wassenhove <sandorw@palantir.com>,"Thu, 26 Feb 2015 17:50:15 +0000",Re: [VOTE] Release Apache Spark 1.3.0 (RC1),"Patrick Wendell <pwendell@gmail.com>, Tathagata Das
	<tathagata.das1565@gmail.com>","FWIW, I tested the first rc and saw no regressions. I ran our benchmarks
built against spark 1.3 and saw results consistent with spark 1.2/1.2.1.


"
Xiangrui Meng <mengxr@gmail.com>,"Thu, 26 Feb 2015 13:16:13 -0800",Re: Using CUDA within Spark / boosting linear algebra,Joseph Bradley <joseph@databricks.com>,"Hey Alexander,

I don't quite understand the part where netlib-cublas is about 20x
slower than netlib-openblas. What is the overhead of using a GPU BLAS
with netlib-java?

CC'ed Sam, the author of netlib-java.

Best,
Xiangrui

te:
e
mAJeaZZggr0/pubchart?oid=1899767119&format=interactive
)
e
ot
 a
ce
epo==netlib-cublas>netlib-blas>f2jblas
9J5r7kwKSPkY/edit?usp=sharing
lib
a
-+
n
mpiled
,
d
s OK
e
at
 -
t on
y
 get
ce
I can
 use
t
).
rary
|
-+
or
g
 I
 is
n by John
Llib.
at
 in
ebra, it involves
a
anny
on
e
r
hat
e
r
 by
e
on‚Äôt
n
zing
.
oid
e
k.
th
ive
is
DA
dia
h
of
las
not
d.
rg

---------------------------------------------------------------------


"
Xiangrui Meng <mengxr@gmail.com>,"Thu, 26 Feb 2015 13:20:14 -0800",Re: Google Summer of Code - ideas,Manoj Kumar <manojkumarsivaraj334@gmail.com>,"There are couple things in Scala/Java but missing in Python API:

1. model import/export
2. evaluation metrics
3. distributed linear algebra
4. streaming algorithms

If you are interested, we can list/create target JIRAs and hunt them
down one by one.

Best,
Xiangrui


---------------------------------------------------------------------


"
Xiangrui Meng <mengxr@gmail.com>,"Thu, 26 Feb 2015 13:20:14 -0800",Re: Google Summer of Code - ideas,Manoj Kumar <manojkumarsivaraj334@gmail.com>,"There are couple things in Scala/Java but missing in Python API:

1. model import/export
2. evaluation metrics
3. distributed linear algebra
4. streaming algorithms

If you are interested, we can list/create target JIRAs and hunt them
down one by one.

Best,
Xiangrui


---------------------------------------------------------------------


"
Vikram Kone <vikramkone@gmail.com>,"Thu, 26 Feb 2015 13:23:30 -0800",Re: Need advice for Spark newbie,Dean Wampler <deanwampler@gmail.com>,"Dean
Thanks for the info. Are you saying that we can create star/snowflake data
models using spark so they can be queried from tableau ?


"
Vikram Kone <vikramkone@gmail.com>,"Thu, 26 Feb 2015 13:32:36 -0800",Re: Need advice for Spark newbie,Steve Nunez <snunez@hortonworks.com>,"Hi Steve
Thanks for the info. I will look into hivemail. Are you saying that we can
create star/snowflake data models using spark so they can be queried from
tableau ?


e
s-it-final>
s
m
in certain
rk
now-be-customized-to-run-a-variety-of-hadoop-projects-including-spark-and-r/>,
uery-performance>)
 try all three
ll
e
e
ty.
s
"
Victor Tso-Guillen <vtso@paxata.com>,"Thu, 26 Feb 2015 13:37:11 -0800",Re: Scheduler hang?,dev@spark.apache.org,"Okay I confirmed my suspicions of a hang. I made a request that stopped
progressing, though the already-scheduled tasks had finished. I made a
separate request that was small enough not to hang, and it kicked the hung
job enough to finish. I think what's happening is that the scheduler or the
local backend is not kicking the revive offers messaging at the right time,
but I have to dig into the code some more to nail the culprit. Anyone on
these list have experience in those code areas that could help?


"
Dean Wampler <deanwampler@gmail.com>,"Thu, 26 Feb 2015 15:46:05 -0600",Re: Need advice for Spark newbie,Vikram Kone <vikramkone@gmail.com>,"There's no support for star or snowflake models, per se. What you get with
Hadoop is access to all your data and the processing power to build the ad
hoc queries you want, when you need them, rather than having to figure out
a schema/model in advance.

I recommend that you also ask your questions on one of the Hadoop or Hive
user mailing lists, where you'll find people who have moved data warehouses
to Hadoop. Then you can use Spark for some of the tasks you'll do. This
""dev"" (developer) mailing list isn't really the place to discuss this
anyway. (The user list would be slightly better.)

dean

Dean Wampler, Ph.D.
Author: Programming Scala, 2nd Edition
<http://shop.oreilly.com/product/0636920033073.do> (O'Reilly)
Typesafe <http://typesafe.com>
@deanwampler <http://twitter.com/deanwampler>
http://polyglotprogramming.com


"
Sam Halliday <sam.halliday@gmail.com>,"Thu, 26 Feb 2015 21:49:28 +0000",Re: Using CUDA within Spark / boosting linear algebra,Xiangrui Meng <mengxr@gmail.com>,"Hi all,

I'm not surprised if the GPU is slow. It's about the bottleneck copying the
memory. Watch my talk, linked from the netlib-java github page, to
understand further. The only way to currently make use of a GPU is to do
all the operations using the GPU's kernel. You can find some prepackaged
high level algorithms than do this, but it's extremely limiting.

I believe hardware will fix this problem eventually, so I still advocate
using the netlib primitives. I'm particularly interested in APU approaches
and I'm very interested in finding somebody to fund me to look into it.
It's too much work for a side project.

Look on the last few slides of my talk to see the potential performance
gains.

Best regards, Sam

AJeaZZggr0/pubchart?oid=1899767119&format=interactive
.
d
o==netlib-cublas>netlib-blas>f2jblas
5r7kwKSPkY/edit?usp=sharing
e
compiled
)
nd
ar
t
s
om
e
s
o I
er
9
n
ou
he
ion by
to
lgebra, it
m
al
e
 -
ty
s,
d
n
s
at
s
s
us
"
Sam Halliday <sam.halliday@gmail.com>,"Thu, 26 Feb 2015 21:55:34 +0000",Re: Using CUDA within Spark / boosting linear algebra,Xiangrui Meng <mengxr@gmail.com>,"Btw, I wish people would stop cheating when comparing CPU and GPU timings
for things like matrix multiply :-P

Please always compare apples with apples and include the time it takes to
set up the matrices, send it to the processing unit, doing the calculation
AND copying it back to where you need to see the results.

Ignoring this method will make you believe that your GPU is thousands of
times faster than it really is. Again, jump to the end of my talk for
graphs and more discussion....  especially the bit about me being keen on
funding to investigate APU hardware further ;-) (I believe it will solve
the problem)

AJeaZZggr0/pubchart?oid=1899767119&format=interactive
.
d
o==netlib-cublas>netlib-blas>f2jblas
5r7kwKSPkY/edit?usp=sharing
e
compiled
)
nd
ar
t
s
om
e
s
o I
er
9
n
ou
he
ion by
to
lgebra, it
m
al
e
 -
ty
s,
d
n
s
at
s
s
us
"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Thu, 26 Feb 2015 22:01:09 +0000",RE: Using CUDA within Spark / boosting linear algebra,"Sam Halliday <sam.halliday@gmail.com>, Xiangrui Meng <mengxr@gmail.com>","Evan, thank you for the summary. I would like to add some more observations. The GPU that I used is 2.5 times cheaper than the CPU ($250 vs $100). They both are 3 years old. I've also did a small test with modern hardware, and the new GPU nVidia Titan was slightly more than 1 order of magnitude faster than Intel E5-2650 v2 for the same tests. However, it costs as much as CPU ($1200). My takeaway is that GPU is making a better price/value progress.



Xiangrui, I was also surprised that BIDMat-cuda was faster than netlib-cuda and the most reasonable explanation is that it holds the result in GPU memory, as Sam suggested. At the same time, it is OK because you can copy the result back from GPU only when needed. However, to be sure, I am going to ask the developer of BIDMat on his upcoming talk.



Best regards, Alexander


From: Sam HFebruary 26, 2015 1:56 PM
To: Xiangrui Meng
Cc: dev@spark.apache.org; Joseph Bradley; Ulanov, Alexander; Evan R. Sparks
Subject: Re: Using CUDA within Spark / boosting linear algebra


Btw, I wish people would stop cheating when comparing CPU and GPU timings for things like matrix multiply :-P

Please always compare apples with apples and include the time it takes to set up the matrices, send it to the processing unit, doing the calculation AND copying it back to where you need to see the results.

Ignoring this method will make you believe that your GPU is thousands of times faster than it really is. Again, jump to the end of my talk for graphs and more discussion....  especially the bit about me being keen on funding to investigate APU hardware further ;-) (I believe it will solve the problem)
On 26 Feb 2015 21:16, ""Xiangrui Meng"" <mengxr@gmail.com<mailto:mengxr@gmail.com>> wrote:
Hey Alexander,

I don't quite understand the part where netlib-cublas is about 20x
slower than netlib-openblas. What is the overhead of using a GPU BLAS
with netlib-java?

CC'ed Sam, the author of netlib-java.

Best,
Xiangrui

On Wed, Feb 25, 2015 at 3:36 PM, Joseph Bradley <joseph@databricks.com<mailto:joseph@databricks.com>> wrote:
> Better documentation for linking would be very helpful!  Here's a JIRA:
> https://issues.apache.org/jira/browse/SPARK-6019
>
>
> On Wed, Feb 25, 2015 at 2:53 PM, Evan R. Sparks <evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>
> wrote:
>
>> Thanks for compiling all the data and running these benchmarks, Alex. The
>> big takeaways here can be seen with this chart:
>>
>> https://docs.google.com/spreadsheets/d/1aRm2IADRfXQV7G2vrcVh4StF50uZHl6kmAJeaZZggr0/pubchart?oid=1899767119&format=interactive
>>
>> 1) A properly configured GPU matrix multiply implementation (e.g.
>> BIDMat+GPU) can provide substantial (but less than an order of magnitude)
>> benefit over a well-tuned CPU implementation (e.g. BIDMat+MKL or
>> netlib-java+openblas-compiled).
>> 2) A poorly tuned CPU implementation can be 1-2 orders of magnitude worse
>> than a well-tuned CPU implementation, particularly for larger matrices.
>> (netlib-f2jblas or netlib-ref) This is not to pick on netlib - this
>> basically agrees with the authors own benchmarks (
>> https://github.com/fommil/netlib-java)
>>
>> I think that most of our users are in a situation where using GPUs may not
>> be practical - although we could consider having a good GPU backend
>> available as an option. However, *ALL* users of MLlib could benefit
>> (potentially tremendously) from using a well-tuned CPU-based BLAS
>> implementation. Perhaps we should consider updating the mllib guide with a
>> more complete section for enabling high performance binaries on OSX and
>> Linux? Or better, figure out a way for the system to fetch these
>> automatically.
>>
>> - Evan
>>
>>
>>
>> On Thu, Feb 12, 2015 at 4:18 PM, Ulanov, Alexander <
>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
>>
>>> Just to summarize this thread, I was finally able to make all performance
>>> comparisons that we discussed. It turns out that:
>>> BIDMat-cublas>>BIDMat
>>> MKL==netlib-mkl==netlib-openblas-compiled>netlib-openblas-yum-repo==netlib-cublas>netlib-blas>f2jblas
>>>
>>> Below is the link to the spreadsheet with full results.
>>>
>>> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J5r7kwKSPkY/edit?usp=sharing
>>>
>>> One thing still needs exploration: does BIDMat-cublas perform copying
>>> to/from machine‚Äôs RAM?
>>>
>>> -----Original Message-----
>>> From: Ulanov, Alexander
>>> Sent: Tuesday, February 10, 2015 2:12 PM
>>> To: Evan R. Sparks
>>> Cc: Joseph Bradley; dev@spark.apache.org<mailto:dev@spark.apache.org>
>>> Subject: RE: Using CUDA within Spark / boosting linear algebra
>>>
>>> Thanks, Evan! It seems that ticket was marked as duplicate though the
>>> original one discusses slightly different topic. I was able to link netlib
>>> with MKL from BIDMat binaries. Indeed, MKL is statically linked inside a
>>> 60MB library.
>>>
>>> |A*B  size | BIDMat MKL | Breeze+Netlib-MKL  from BIDMat|
>>> Breeze+Netlib-OpenBlas(native system)| Breeze+Netlib-f2jblas |
>>> +-----------------------------------------------------------------------+
>>> |100x100*100x100 | 0,00205596 | 0,000381 | 0,03810324 | 0,002556 |
>>> |1000x1000*1000x1000 | 0,018320947 | 0,038316857 | 0,51803557
>>> |1,638475459 |
>>> |10000x10000*10000x10000 | 23,78046632 | 32,94546697 |445,0935211 |
>>> 1569,233228 |
>>>
>>> It turn out that pre-compiled MKL is faster than precompiled OpenBlas on
>>> my machine. Probably, I‚Äôll add two more columns with locally compiled
>>> openblas and cuda.
>>>
>>> Alexander
>>>
>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>]
>>> Sent: Monday, February 09, 2015 6:06 PM
>>> To: Ulanov, Alexander
>>> Cc: Joseph Bradley; dev@spark.apache.org<mailto:dev@spark.apache.org>
>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>>
>>> Great - perhaps we can move this discussion off-list and onto a JIRA
>>> ticket? (Here's one: https://issues.apache.org/jira/browse/SPARK-5705)
>>>
>>> It seems like this is going to be somewhat exploratory for a while (and
>>> there's probably only a handful of us who really care about fast linear
>>> algebra!)
>>>
>>> - Evan
>>>
>>> On Mon, Feb 9, 2015 at 4:48 PM, Ulanov, Alexander <
>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
>>> Hi Evan,
>>>
>>> Thank you for explanation and useful link. I am going to build OpenBLAS,
>>> link it with Netlib-java and perform benchmark again.
>>>
>>> Do I understand correctly that BIDMat binaries contain statically linked
>>> Intel MKL BLAS? It might be the reason why I am able to run BIDMat not
>>> having MKL BLAS installed on my server. If it is true, I wonder if it is OK
>>> because Intel sells this library. Nevertheless, it seems that in my case
>>> precompiled MKL BLAS performs better than precompiled OpenBLAS given that
>>> BIDMat and Netlib-java are supposed to be on par with JNI overheads.
>>>
>>> Though, it might be interesting to link Netlib-java with Intel MKL, as
>>> you suggested. I wonder, are John Canny (BIDMat) and Sam Halliday
>>> (Netlib-java) interested to compare their libraries.
>>>
>>> Best regards, Alexander
>>>
>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:
>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
>>> Sent: Friday, February 06, 2015 5:58 PM
>>>
>>> To: Ulanov, Alexander
>>> Cc: Joseph Bradley; dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.apache.org<mailto:dev@spark.apache.org>>
>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>>
>>> I would build OpenBLAS yourself, since good BLAS performance comes from
>>> getting cache sizes, etc. set up correctly for your particular hardware -
>>> this is often a very tricky process (see, e.g. ATLAS), but we found that on
>>> relatively modern Xeon chips, OpenBLAS builds quickly and yields
>>> performance competitive with MKL.
>>>
>>> To make sure the right library is getting used, you have to make sure
>>> it's first on the search path - export
>>> LD_LIBRARY_PATH=/path/to/blas/library.so will do the trick here.
>>>
>>> For some examples of getting netlib-java setup on an ec2 node and some
>>> example benchmarking code we ran a while back, see:
>>> https://github.com/shivaram/matrix-bench
>>>
>>> In particular - build-openblas-ec2.sh shows you how to build the library
>>> and set up symlinks correctly, and scala/run-netlib.sh shows you how to get
>>> the path setup and get that library picked up by netlib-java.
>>>
>>> In this way - you could probably get cuBLAS set up to be used by
>>> netlib-java as well.
>>>
>>> - Evan
>>>
>>> On Fri, Feb 6, 2015 at 5:43 PM, Ulanov, Alexander <
>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
>>> Evan, could you elaborate on how to force BIDMat and netlib-java to force
>>> loading the right blas? For netlib, I there are few JVM flags, such as
>>> -Dcom.github.fommil.netlib.BLAS=com.github.fommil.netlib.F2jBLAS, so I can
>>> force it to use Java implementation. Not sure I understand how to force use
>>> a specific blas (not specific wrapper for blas).
>>>
>>> Btw. I have installed openblas (yum install openblas), so I suppose that
>>> netlib is using it.
>>>
>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:
>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
>>> Sent: Friday, February 06, 2015 5:19 PM
>>> To: Ulanov, Alexander
>>> Cc: Joseph Bradley; dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.apache.org<mailto:dev@spark.apache.org>>
>>>
>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>>
>>> Getting breeze to pick up the right blas library is critical for
>>> performance. I recommend using OpenBLAS (or MKL, if you already have it).
>>> It might make sense to force BIDMat to use the same underlying BLAS library
>>> as well.
>>>
>>> On Fri, Feb 6, 2015 at 4:42 PM, Ulanov, Alexander <
>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
>>> Hi Evan, Joseph
>>>
>>> I did few matrix multiplication test and BIDMat seems to be ~10x faster
>>> than netlib-java+breeze (sorry for weird table formatting):
>>>
>>> |A*B  size | BIDMat MKL | Breeze+Netlib-java native_system_linux_x86-64|
>>> Breeze+Netlib-java f2jblas |
>>> +-----------------------------------------------------------------------+
>>> |100x100*100x100 | 0,00205596 | 0,03810324 | 0,002556 |
>>> |1000x1000*1000x1000 | 0,018320947 | 0,51803557 |1,638475459 |
>>> |10000x10000*10000x10000 | 23,78046632 | 445,0935211 | 1569,233228 |
>>>
>>> Configuration: Intel(R) Xeon(R) CPU E31240 3.3 GHz, 6GB RAM, Fedora 19
>>> Linux, Scala 2.11.
>>>
>>> Later I will make tests with Cuda. I need to install new Cuda version for
>>> this purpose.
>>>
>>> Do you have any ideas why breeze-netlib with native blas is so much
>>> slower than BIDMat MKL?
>>>
>>> Best regards, Alexander
>>>
>>> From: Joseph Bradley [mailto:joseph@databricks.com<mailto:joseph@databricks.com><mailto:
>>> joseph@databricks.com<mailto:joseph@databricks.com>>]
>>> Sent: Thursday, February 05, 2015 5:29 PM
>>> To: Ulanov, Alexander
>>> Cc: Evan R. Sparks; dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.apache.org<mailto:dev@spark.apache.org>>
>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>>
>>> Hi Alexander,
>>>
>>> Using GPUs with Spark would be very exciting.  Small comment: Concerning
>>> your question earlier about keeping data stored on the GPU rather than
>>> having to move it between main memory and GPU memory on each iteration, I
>>> would guess this would be critical to getting good performance.  If you
>>> could do multiple local iterations before aggregating results, then the
>>> cost of data movement to the GPU could be amortized (and I believe that is
>>> done in practice).  Having Spark be aware of the GPU and using it as
>>> another part of memory sounds like a much bigger undertaking.
>>>
>>> Joseph
>>>
>>> On Thu, Feb 5, 2015 at 4:59 PM, Ulanov, Alexander <
>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
>>> Thank you for explanation! I‚Äôve watched the BIDMach presentation by John
>>> Canny and I am really inspired by his talk and comparisons with Spark MLlib.
>>>
>>> I am very interested to find out what will be better within Spark: BIDMat
>>> or netlib-java with CPU or GPU natives. Could you suggest a fair way to
>>> benchmark them? Currently I do benchmarks on artificial neural networks in
>>> batch mode. While it is not a ‚Äúpure‚Äù test of linear algebra, it involves
>>> some other things that are essential to machine learning.
>>>
>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:
>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
>>> Sent: Thursday, February 05, 2015 1:29 PM
>>> To: Ulanov, Alexander
>>> Cc: dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.apache.org<mailto:dev@spark.apache.org>>
>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>>
>>> I'd be surprised of BIDMat+OpenBLAS was significantly faster than
>>> netlib-java+OpenBLAS, but if it is much faster it's probably due to data
>>> layout and fewer levels of indirection - it's definitely a worthwhile
>>> experiment to run. The main speedups I've seen from using it come from
>>> highly optimized GPU code for linear algebra. I know that in the past Canny
>>> has gone as far as to write custom GPU kernels for performance-critical
>>> regions of code.[1]
>>>
>>> BIDMach is highly optimized for single node performance or performance on
>>> small clusters.[2] Once data doesn't fit easily in GPU memory (or can be
>>> batched in that way) the performance tends to fall off. Canny argues for
>>> hardware/software codesign and as such prefers machine configurations that
>>> are quite different than what we find in most commodity cluster nodes -
>>> e.g. 10 disk cahnnels and 4 GPUs.
>>>
>>> In contrast, MLlib was designed for horizontal scalability on commodity
>>> clusters and works best on very big datasets - order of terabytes.
>>>
>>> For the most part, these projects developed concurrently to address
>>> slightly different use cases. That said, there may be bits of BIDMach we
>>> could repurpose for MLlib - keep in mind we need to be careful about
>>> maintaining cross-language compatibility for our Java and Python-users,
>>> though.
>>>
>>> - Evan
>>>
>>> [1] - http://arxiv.org/abs/1409.5402
>>> [2] - http://eecs.berkeley.edu/~hzhao/papers/BD.pdf
>>>
>>> On Thu, Feb 5, 2015 at 1:00 PM, Ulanov, Alexander <
>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:
>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>>> wrote:
>>> Hi Evan,
>>>
>>> Thank you for suggestion! BIDMat seems to have terrific speed. Do you
>>> know what makes them faster than netlib-java?
>>>
>>> The same group has BIDMach library that implements machine learning. For
>>> some examples they use Caffe convolutional neural network library owned by
>>> another group in Berkeley. Could you elaborate on how these all might be
>>> connected with Spark Mllib? If you take BIDMat for linear algebra why don‚Äôt
>>> you take BIDMach for optimization and learning?
>>>
>>> Best regards, Alexander
>>>
>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:
>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>><mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:
>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>>]
>>> Sent: Thursday, February 05, 2015 12:09 PM
>>> To: Ulanov, Alexander
>>> Cc: dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.apache.org<mailto:dev@spark.apache.org>><mailto:
>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.apache.org<mailto:dev@spark.apache.org>>>
>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>>
>>> I'd expect that we can make GPU-accelerated BLAS faster than CPU blas in
>>> many cases.
>>>
>>> You might consider taking a look at the codepaths that BIDMat (
>>> https://github.com/BIDData/BIDMat) takes and comparing them to
>>> netlib-java/breeze. John Canny et. al. have done a bunch of work optimizing
>>> to make this work really fast from Scala. I've run it on my laptop and
>>> compared to MKL and in certain cases it's 10x faster at matrix multiply.
>>> There are a lot of layers of indirection here and you really want to avoid
>>> data copying as much as possible.
>>>
>>> We could also consider swapping out BIDMat for Breeze, but that would be
>>> a big project and if we can figure out how to get breeze+cublas to
>>> comparable performance that would be a big win.
>>>
>>> On Thu, Feb 5, 2015 at 11:55 AM, Ulanov, Alexander <
>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:
>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>>> wrote:
>>> Dear Spark developers,
>>>
>>> I am exploring how to make linear algebra operations faster within Spark.
>>> One way of doing this is to use Scala Breeze library that is bundled with
>>> Spark. For matrix operations, it employs Netlib-java that has a Java
>>> wrapper for BLAS (basic linear algebra subprograms) and LAPACK native
>>> binaries if they are available on the worker node. It also has its own
>>> optimized Java implementation of BLAS. It is worth mentioning, that native
>>> binaries provide better performance only for BLAS level 3, i.e.
>>> matrix-matrix operations or general matrix multiplication (GEMM). This is
>>> confirmed by GEMM test on Netlib-java page
>>> https://github.com/fommil/netlib-java. I also confirmed it with my
>>> experiments with training of artificial neural network
>>> https://github.com/apache/spark/pull/1290#issuecomment-70313952.
>>> However, I would like to boost performance more.
>>>
>>> GPU is supposed to work fast with linear algebra and there is Nvidia CUDA
>>> implementation of BLAS, called cublas. I have one Linux server with Nvidia
>>> GPU and I was able to do the following. I linked cublas (instead of
>>> cpu-based blas) with Netlib-java wrapper and put it into Spark, so
>>> Breeze/Netlib is using it. Then I did some performance measurements with
>>> regards to artificial neural network batch learning in Spark MLlib that
>>> involves matrix-matrix multiplications. It turns out that for matrices of
>>> size less than ~1000x780 GPU cublas has the same speed as CPU blas. Cublas
>>> becomes slower for bigger matrices. It worth mentioning that it is was not
>>> a test for ONLY multiplication since there are other operations involved.
>>> One of the reasons for slowdown might be the overhead of copying the
>>> matrices from computer memory to graphic card memory and back.
>>>
>>> So, few questions:
>>> 1) Do these results with CUDA make sense?
>>> 2) If the problem is with copy overhead, are there any libraries that
>>> allow to force intermediate results to stay in graphic card memory thus
>>> removing the overhead?
>>> 3) Any other options to speed-up linear algebra in Spark?
>>>
>>> Thank you, Alexander
>>>
>>> ---------------------------------------------------------------------
>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org><mailto:
>>> dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org>><mailto:dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org>
>>> <mailto:dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org>>>
>>> For additional commands, e-mail: dev-help@spark.apache.org<mailto:dev-help@spark.apache.org><mailto:
>>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>><mailto:dev-help@spark.apache.org<mailto:dev-help@spark.apache.org><mailto:
>>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>>>
>>>
>>>
>>>
>>>
>>
"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Thu, 26 Feb 2015 22:04:29 +0000",RE: Using CUDA within Spark / boosting linear algebra,"Sam Halliday <sam.halliday@gmail.com>, Xiangrui Meng <mengxr@gmail.com>","Typo - CPU was 2.5 cheaper (not GPU!)

-----Original Message-----
From: Ulanov, Alexander 
Sent: Thursday, February 26, 2015 2:01 PM
To: Sam Halliday; Xiangrui Meng
Cc: dev@spark.apache.org; Joseph Bradley; Evan R. Sparks
Subject: RE: Using CUDA within Spark / boosting linear algebra

Evan, thank you for the summary. I would like to add some more observations. The GPU that I used is 2.5 times cheaper than the CPU ($250 vs $100). They both are 3 years old. I've also did a small test with modern hardware, and the new GPU nVidia Titan was slightly more than 1 order of magnitude faster than Intel E5-2650 v2 for the same tests. However, it costs as much as CPU ($1200). My takeaway is that GPU is making a better price/value progress.



Xiangrui, I was also surprised that BIDMat-cuda was faster than netlib-cuda and the most reasonable explanation is that it holds the result in GPU memory, as Sam suggested. At the same time, it is OK because you can copy the result back from GPU only when needed. However, to be sure, I am going to ask the developer of BIDMat on his upcoming talk.



Best regards, Alexander


From: Sam Halliday [mailto:sam.halliday@gmail.com]
Sent: Thursday, February 26, 2015 1:56 PM
To: Xiangrui Meng
Cc: dev@spark.apache.org; Joseph Bradley; Ulanov, Alexander; Evan R. Sparks
Subject: Re: Using CUDA within Spark / boosting linear algebra


Btw, I wish people would stop cheating when comparing CPU and GPU timings for things like matrix multiply :-P

Please always compare apples with apples and include the time it takes to set up the matrices, send it to the processing unit, doing the calculation AND copying it back to where you need to see the results.

Ignoring this method will make you believe that your GPU is thousands of times faster than it really is. Again, jump to the end of my talk for graphs and more discussion....  especially the bit about me being keen on funding to investigate APU hardware further ;-) (I believe it will solve the problem) On 26 Feb 2015 21:16, ""Xiangrui Meng"" <mengxr@gmail.com<mailto:mengxr@gmail.com>> wrote:
Hey Alexander,

I don't quite understand the part where netlib-cublas is about 20x slower than netlib-openblas. What is the overhead of using a GPU BLAS with netlib-java?

CC'ed Sam, the author of netlib-java.

Best,
Xiangrui

On Wed, Feb 25, 2015 at 3:36 PM, Joseph Bradley <joseph@databricks.com<mailto:joseph@databricks.com>> wrote:
> Better documentation for linking would be very helpful!  Here's a JIRA:
> https://issues.apache.org/jira/browse/SPARK-6019
>
>
> On Wed, Feb 25, 2015 at 2:53 PM, Evan R. Sparks 
> <evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>
> wrote:
>
>> Thanks for compiling all the data and running these benchmarks, Alex. 
>> The big takeaways here can be seen with this chart:
>>
>> https://docs.google.com/spreadsheets/d/1aRm2IADRfXQV7G2vrcVh4StF50uZH
>> l6kmAJeaZZggr0/pubchart?oid=1899767119&format=interactive
>>
>> 1) A properly configured GPU matrix multiply implementation (e.g.
>> BIDMat+GPU) can provide substantial (but less than an order of 
>> BIDMat+magnitude)
>> benefit over a well-tuned CPU implementation (e.g. BIDMat+MKL or
>> netlib-java+openblas-compiled).
>> 2) A poorly tuned CPU implementation can be 1-2 orders of magnitude 
>> worse than a well-tuned CPU implementation, particularly for larger matrices.
>> (netlib-f2jblas or netlib-ref) This is not to pick on netlib - this 
>> basically agrees with the authors own benchmarks (
>> https://github.com/fommil/netlib-java)
>>
>> I think that most of our users are in a situation where using GPUs 
>> may not be practical - although we could consider having a good GPU 
>> backend available as an option. However, *ALL* users of MLlib could 
>> benefit (potentially tremendously) from using a well-tuned CPU-based 
>> BLAS implementation. Perhaps we should consider updating the mllib 
>> guide with a more complete section for enabling high performance 
>> binaries on OSX and Linux? Or better, figure out a way for the system 
>> to fetch these automatically.
>>
>> - Evan
>>
>>
>>
>> On Thu, Feb 12, 2015 at 4:18 PM, Ulanov, Alexander < 
>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
>>
>>> Just to summarize this thread, I was finally able to make all 
>>> performance comparisons that we discussed. It turns out that:
>>> BIDMat-cublas>>BIDMat
>>> MKL==netlib-mkl==netlib-openblas-compiled>netlib-openblas-yum-repo==
>>> netlib-cublas>netlib-blas>f2jblas
>>>
>>> Below is the link to the spreadsheet with full results.
>>>
>>> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx3
>>> 78T9J5r7kwKSPkY/edit?usp=sharing
>>>
>>> One thing still needs exploration: does BIDMat-cublas perform 
>>> copying to/from machine‚Äôs RAM?
>>>
>>> -----Original Message-----
>>> From: Ulanov, Alexander
>>> Sent: Tuesday, February 10, 2015 2:12 PM
>>> To: Evan R. Sparks
>>> Cc: Joseph Bradley; 
>>> dev@spark.apache.org<mailto:dev@spark.apache.org>
>>> Subject: RE: Using CUDA within Spark / boosting linear algebra
>>>
>>> Thanks, Evan! It seems that ticket was marked as duplicate though 
>>> the original one discusses slightly different topic. I was able to 
>>> link netlib with MKL from BIDMat binaries. Indeed, MKL is statically 
>>> linked inside a 60MB library.
>>>
>>> |A*B  size | BIDMat MKL | Breeze+Netlib-MKL  from BIDMat|
>>> Breeze+Netlib-OpenBlas(native system)| Breeze+Netlib-f2jblas |
>>> +-----------------------------------------------------------------------+
>>> |100x100*100x100 | 0,00205596 | 0,000381 | 0,03810324 | 0,002556 |
>>> |1000x1000*1000x1000 | 0,018320947 | 0,038316857 | 0,51803557
>>> |1,638475459 |
>>> |10000x10000*10000x10000 | 23,78046632 | 32,94546697 |445,0935211 |
>>> 1569,233228 |
>>>
>>> It turn out that pre-compiled MKL is faster than precompiled 
>>> OpenBlas on my machine. Probably, I‚Äôll add two more columns with 
>>> locally compiled openblas and cuda.
>>>
>>> Alexander
>>>
>>> From: Evan R. Sparks 
>>> [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>]
>>> Sent: Monday, February 09, 2015 6:06 PM
>>> To: Ulanov, Alexander
>>> Cc: Joseph Bradley; 
>>> dev@spark.apache.org<mailto:dev@spark.apache.org>
>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>>
>>> Great - perhaps we can move this discussion off-list and onto a JIRA 
>>> ticket? (Here's one: 
>>> https://issues.apache.org/jira/browse/SPARK-5705)
>>>
>>> It seems like this is going to be somewhat exploratory for a while 
>>> (and there's probably only a handful of us who really care about 
>>> fast linear
>>> algebra!)
>>>
>>> - Evan
>>>
>>> On Mon, Feb 9, 2015 at 4:48 PM, Ulanov, Alexander < 
>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
>>> Hi Evan,
>>>
>>> Thank you for explanation and useful link. I am going to build 
>>> OpenBLAS, link it with Netlib-java and perform benchmark again.
>>>
>>> Do I understand correctly that BIDMat binaries contain statically 
>>> linked Intel MKL BLAS? It might be the reason why I am able to run 
>>> BIDMat not having MKL BLAS installed on my server. If it is true, I 
>>> wonder if it is OK because Intel sells this library. Nevertheless, 
>>> it seems that in my case precompiled MKL BLAS performs better than 
>>> precompiled OpenBLAS given that BIDMat and Netlib-java are supposed to be on par with JNI overheads.
>>>
>>> Though, it might be interesting to link Netlib-java with Intel MKL, 
>>> as you suggested. I wonder, are John Canny (BIDMat) and Sam Halliday
>>> (Netlib-java) interested to compare their libraries.
>>>
>>> Best regards, Alexander
>>>
>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:
>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
>>> Sent: Friday, February 06, 2015 5:58 PM
>>>
>>> To: Ulanov, Alexander
>>> Cc: Joseph Bradley; 
>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.a
>>> pache.org<mailto:dev@spark.apache.org>>
>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>>
>>> I would build OpenBLAS yourself, since good BLAS performance comes 
>>> from getting cache sizes, etc. set up correctly for your particular 
>>> hardware - this is often a very tricky process (see, e.g. ATLAS), 
>>> but we found that on relatively modern Xeon chips, OpenBLAS builds 
>>> quickly and yields performance competitive with MKL.
>>>
>>> To make sure the right library is getting used, you have to make 
>>> sure it's first on the search path - export 
>>> LD_LIBRARY_PATH=/path/to/blas/library.so will do the trick here.
>>>
>>> For some examples of getting netlib-java setup on an ec2 node and 
>>> some example benchmarking code we ran a while back, see:
>>> https://github.com/shivaram/matrix-bench
>>>
>>> In particular - build-openblas-ec2.sh shows you how to build the 
>>> library and set up symlinks correctly, and scala/run-netlib.sh shows 
>>> you how to get the path setup and get that library picked up by netlib-java.
>>>
>>> In this way - you could probably get cuBLAS set up to be used by 
>>> netlib-java as well.
>>>
>>> - Evan
>>>
>>> On Fri, Feb 6, 2015 at 5:43 PM, Ulanov, Alexander < 
>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
>>> Evan, could you elaborate on how to force BIDMat and netlib-java to 
>>> force loading the right blas? For netlib, I there are few JVM flags, 
>>> such as 
>>> -Dcom.github.fommil.netlib.BLAS=com.github.fommil.netlib.F2jBLAS, so 
>>> I can force it to use Java implementation. Not sure I understand how to force use a specific blas (not specific wrapper for blas).
>>>
>>> Btw. I have installed openblas (yum install openblas), so I suppose 
>>> that netlib is using it.
>>>
>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:
>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
>>> Sent: Friday, February 06, 2015 5:19 PM
>>> To: Ulanov, Alexander
>>> Cc: Joseph Bradley; 
>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.a
>>> pache.org<mailto:dev@spark.apache.org>>
>>>
>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>>
>>> Getting breeze to pick up the right blas library is critical for 
>>> performance. I recommend using OpenBLAS (or MKL, if you already have it).
>>> It might make sense to force BIDMat to use the same underlying BLAS 
>>> library as well.
>>>
>>> On Fri, Feb 6, 2015 at 4:42 PM, Ulanov, Alexander < 
>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
>>> Hi Evan, Joseph
>>>
>>> I did few matrix multiplication test and BIDMat seems to be ~10x 
>>> faster than netlib-java+breeze (sorry for weird table formatting):
>>>
>>> |A*B  size | BIDMat MKL | Breeze+Netlib-java 
>>> |native_system_linux_x86-64|
>>> Breeze+Netlib-java f2jblas |
>>> +-----------------------------------------------------------------------+
>>> |100x100*100x100 | 0,00205596 | 0,03810324 | 0,002556 |
>>> |1000x1000*1000x1000 | 0,018320947 | 0,51803557 |1,638475459 |
>>> |10000x10000*10000x10000 | 23,78046632 | 445,0935211 | 1569,233228 |
>>>
>>> Configuration: Intel(R) Xeon(R) CPU E31240 3.3 GHz, 6GB RAM, Fedora 
>>> 19 Linux, Scala 2.11.
>>>
>>> Later I will make tests with Cuda. I need to install new Cuda 
>>> version for this purpose.
>>>
>>> Do you have any ideas why breeze-netlib with native blas is so much 
>>> slower than BIDMat MKL?
>>>
>>> Best regards, Alexander
>>>
>>> From: Joseph Bradley [mailto:joseph@databricks.com<mailto:joseph@databricks.com><mailto:
>>> joseph@databricks.com<mailto:joseph@databricks.com>>]
>>> Sent: Thursday, February 05, 2015 5:29 PM
>>> To: Ulanov, Alexander
>>> Cc: Evan R. Sparks; 
>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.a
>>> pache.org<mailto:dev@spark.apache.org>>
>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>>
>>> Hi Alexander,
>>>
>>> Using GPUs with Spark would be very exciting.  Small comment: 
>>> Concerning your question earlier about keeping data stored on the 
>>> GPU rather than having to move it between main memory and GPU memory 
>>> on each iteration, I would guess this would be critical to getting 
>>> good performance.  If you could do multiple local iterations before 
>>> aggregating results, then the cost of data movement to the GPU could 
>>> be amortized (and I believe that is done in practice).  Having Spark 
>>> be aware of the GPU and using it as another part of memory sounds like a much bigger undertaking.
>>>
>>> Joseph
>>>
>>> On Thu, Feb 5, 2015 at 4:59 PM, Ulanov, Alexander < 
>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
>>> Thank you for explanation! I‚Äôve watched the BIDMach presentation by 
>>> John Canny and I am really inspired by his talk and comparisons with Spark MLlib.
>>>
>>> I am very interested to find out what will be better within Spark: 
>>> BIDMat or netlib-java with CPU or GPU natives. Could you suggest a 
>>> fair way to benchmark them? Currently I do benchmarks on artificial 
>>> neural networks in batch mode. While it is not a ‚Äúpure‚Äù test of 
>>> linear algebra, it involves some other things that are essential to machine learning.
>>>
>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:
>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
>>> Sent: Thursday, February 05, 2015 1:29 PM
>>> To: Ulanov, Alexander
>>> Cc: 
>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.a
>>> pache.org<mailto:dev@spark.apache.org>>
>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>>
>>> I'd be surprised of BIDMat+OpenBLAS was significantly faster than
>>> netlib-java+OpenBLAS, but if it is much faster it's probably due to 
>>> netlib-java+data
>>> layout and fewer levels of indirection - it's definitely a 
>>> worthwhile experiment to run. The main speedups I've seen from using 
>>> it come from highly optimized GPU code for linear algebra. I know 
>>> that in the past Canny has gone as far as to write custom GPU 
>>> kernels for performance-critical regions of code.[1]
>>>
>>> BIDMach is highly optimized for single node performance or 
>>> performance on small clusters.[2] Once data doesn't fit easily in 
>>> GPU memory (or can be batched in that way) the performance tends to 
>>> fall off. Canny argues for hardware/software codesign and as such 
>>> prefers machine configurations that are quite different than what we 
>>> find in most commodity cluster nodes - e.g. 10 disk cahnnels and 4 GPUs.
>>>
>>> In contrast, MLlib was designed for horizontal scalability on 
>>> commodity clusters and works best on very big datasets - order of terabytes.
>>>
>>> For the most part, these projects developed concurrently to address 
>>> slightly different use cases. That said, there may be bits of 
>>> BIDMach we could repurpose for MLlib - keep in mind we need to be 
>>> careful about maintaining cross-language compatibility for our Java 
>>> and Python-users, though.
>>>
>>> - Evan
>>>
>>> [1] - http://arxiv.org/abs/1409.5402 [2] - 
>>> http://eecs.berkeley.edu/~hzhao/papers/BD.pdf
>>>
>>> On Thu, Feb 5, 2015 at 1:00 PM, Ulanov, Alexander <
>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:
>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>>> wrote:
>>> Hi Evan,
>>>
>>> Thank you for suggestion! BIDMat seems to have terrific speed. Do 
>>> you know what makes them faster than netlib-java?
>>>
>>> The same group has BIDMach library that implements machine learning. 
>>> For some examples they use Caffe convolutional neural network 
>>> library owned by another group in Berkeley. Could you elaborate on 
>>> how these all might be connected with Spark Mllib? If you take 
>>> BIDMat for linear algebra why don‚Äôt you take BIDMach for optimization and learning?
>>>
>>> Best regards, Alexander
>>>
>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:
>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>><mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:
>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>>]
>>> Sent: Thursday, February 05, 2015 12:09 PM
>>> To: Ulanov, Alexander
>>> Cc: dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.apache.org<mailto:dev@spark.apache.org>><mailto:
>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.a
>>> pache.org<mailto:dev@spark.apache.org>>>
>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>>
>>> I'd expect that we can make GPU-accelerated BLAS faster than CPU 
>>> blas in many cases.
>>>
>>> You might consider taking a look at the codepaths that BIDMat (
>>> https://github.com/BIDData/BIDMat) takes and comparing them to 
>>> netlib-java/breeze. John Canny et. al. have done a bunch of work 
>>> optimizing to make this work really fast from Scala. I've run it on 
>>> my laptop and compared to MKL and in certain cases it's 10x faster at matrix multiply.
>>> There are a lot of layers of indirection here and you really want to 
>>> avoid data copying as much as possible.
>>>
>>> We could also consider swapping out BIDMat for Breeze, but that 
>>> would be a big project and if we can figure out how to get 
>>> breeze+cublas to comparable performance that would be a big win.
>>>
>>> On Thu, Feb 5, 2015 at 11:55 AM, Ulanov, Alexander <
>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:
>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>>> wrote:
>>> Dear Spark developers,
>>>
>>> I am exploring how to make linear algebra operations faster within Spark.
>>> One way of doing this is to use Scala Breeze library that is bundled 
>>> with Spark. For matrix operations, it employs Netlib-java that has a 
>>> Java wrapper for BLAS (basic linear algebra subprograms) and LAPACK 
>>> native binaries if they are available on the worker node. It also 
>>> has its own optimized Java implementation of BLAS. It is worth 
>>> mentioning, that native binaries provide better performance only for BLAS level 3, i.e.
>>> matrix-matrix operations or general matrix multiplication (GEMM). 
>>> This is confirmed by GEMM test on Netlib-java page 
>>> https://github.com/fommil/netlib-java. I also confirmed it with my 
>>> experiments with training of artificial neural network 
>>> https://github.com/apache/spark/pull/1290#issuecomment-70313952.
>>> However, I would like to boost performance more.
>>>
>>> GPU is supposed to work fast with linear algebra and there is Nvidia 
>>> CUDA implementation of BLAS, called cublas. I have one Linux server 
>>> with Nvidia GPU and I was able to do the following. I linked cublas 
>>> (instead of cpu-based blas) with Netlib-java wrapper and put it into 
>>> Spark, so Breeze/Netlib is using it. Then I did some performance 
>>> measurements with regards to artificial neural network batch 
>>> learning in Spark MLlib that involves matrix-matrix multiplications. 
>>> It turns out that for matrices of size less than ~1000x780 GPU 
>>> cublas has the same speed as CPU blas. Cublas becomes slower for 
>>> bigger matrices. It worth mentioning that it is was not a test for ONLY multiplication since there are other operations involved.
>>> One of the reasons for slowdown might be the overhead of copying the 
>>> matrices from computer memory to graphic card memory and back.
>>>
>>> So, few questions:
>>> 1) Do these results with CUDA make sense?
>>> 2) If the problem is with copy overhead, are there any libraries 
>>> that allow to force intermediate results to stay in graphic card 
>>> memory thus removing the overhead?
>>> 3) Any other options to speed-up linear algebra in Spark?
>>>
>>> Thank you, Alexander
>>>
>>> --------------------------------------------------------------------
>>> - To unsubscribe, e-mail: 
>>> dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org><mailto:
>>> dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache
>>> .org>><mailto:dev-unsubscribe@spark.apache.org<mailto:dev-unsubscrib
>>> e@spark.apache.org> 
>>> <mailto:dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spar
>>> k.apache.org>>> For additional commands, e-mail: 
>>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org><mailto:
>>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>><mailto:dev-help@spark.apache.org<mailto:dev-help@spark.apache.org><mailto:
>>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>>>
>>>
>>>
>>>
>>>
>>
"
"""Evan R. Sparks"" <evan.sparks@gmail.com>","Thu, 26 Feb 2015 14:06:41 -0800",Re: Using CUDA within Spark / boosting linear algebra,Sam Halliday <sam.halliday@gmail.com>,"I couldn't agree with you more, Sam. The GPU/Matrix guys typically don't
count their copy times, but claim that you should be doing *as much as
possible* on the GPU - so, maybe for some applications where you can
generate the data on the GPU this makes sense. But, in the context of Spark
we should be *very* careful about enumerating the applications we want GPU
support for and deciding whether it's appropriate to measure the overheads
of getting the data to the GPU.


n
:
mAJeaZZggr0/pubchart?oid=1899767119&format=interactive
s.
y
nd
po==netlib-cublas>netlib-blas>f2jblas
J5r7kwKSPkY/edit?usp=sharing
g
e
+
s
 compiled
5
ot
t
as
e
me
as
so
+
19
n
an
tion by
k
algebra, it
e
om
t
n
s
s
h
u
t
y
s
nd
d
e
wn
t
-
"
Sam Halliday <sam.halliday@gmail.com>,"Thu, 26 Feb 2015 22:21:29 +0000",RE: Using CUDA within Spark / boosting linear algebra,"""Ulanov\, Alexander"" <alexander.ulanov@hp.com>, Xiangrui Meng <mengxr@gmail.com>","I've had some email exchanges with the author of BIDMat: it does exactly
what you need to get the GPU benefit and writes higher level algorithms
entirely in the GPU kernels so that the memory stays there as long as
possible. The restriction with this approach is that it is only offering
high-level algorithms so is not a toolkit for applied mathematics
research and development --- but it works well as a toolkit for higher
level analysis (e.g. for analysts and practitioners).

I believe BIDMat's approach is the best way to get performance out of
GPU hardware at the moment but I also have strong evidence to suggest
that the hardware will catch up and the memory transfer costs between
CPU/GPU will disappear meaning that there will be no need for custom GPU
kernel implementations. i.e. please continue to use BLAS primitives when
writing new algorithms and only go to the GPU for an alternative
optimised implementation.

Note that CUDA and cuBLAS are *not* BLAS. They are BLAS-like, and offer
an API that looks like BLAS but takes pointers to special regions in the
GPU memory region. Somebody has written a wrapper around CUDA to create
a proper BLAS library but it only gives marginal performance over the
CPU because of the memory transfer overhead.

This slide from my talk

  http://fommil.github.io/scalax14/#/11/2

says it all. X axis is matrix size, Y axis is logarithmic time to do
DGEMM. Black line is the ""cheating"" time for the GPU and the green line
is after copying the memory to/from the GPU memory. APUs have the
potential to eliminate the green line.

Best regards,
Sam


""Ulanov, Alexander"" <alexander.ulanov@hp.com> writes:

ns. The GPU that I used is 2.5 times cheaper than the CPU ($250 vs $100). They both are 3 years old. I've also did a small test with modern hardware, and the new GPU nVidia Titan was slightly more than 1 order of magnitude faster than Intel E5-2650 v2 for the same tests. However, it costs as much as CPU ($1200). My takeaway is that GPU is making a better price/value progress.
da and the most reasonable explanation is that it holds the result in GPU memory, as Sam suggested. At the same time, it is OK because you can copy the result back from GPU only when needed. However, to be sure, I am going to ask the developer of BIDMat on his upcoming talk.
ks
 for things like matrix multiply :-P
 set up the matrices, send it to the processing unit, doing the calculation AND copying it back to where you need to see the results.
times faster than it really is. Again, jump to the end of my talk for graphs and more discussion....  especially the bit about me being keen on funding to investigate APU hardware further ;-) (I believe it will solve the problem)
ailto:evan.sparks@gmail.com>>
he
kmAJeaZZggr0/pubchart?oid=1899767119&format=interactive
e)
se
not
h a
nce
repo==netlib-cublas>netlib-blas>f2jblas
T9J5r7kwKSPkY/edit?usp=sharing
tlib
 a
--+
on
ompiled
gmail.com>]
S,
ed
is OK
se
hat
gmail.com><mailto:
mailto:dev@spark.apache.org<mailto:dev@spark.apache.org>>
e -
at on
ry
o get
rce
 I can
e use
at
gmail.com><mailto:
mailto:dev@spark.apache.org<mailto:dev@spark.apache.org>>
t).
brary
4|
--+
for
ricks.com><mailto:
mailto:dev@spark.apache.org<mailto:dev@spark.apache.org>>
ng
, I
t is
on by John
MLlib.
Mat
s in
gebra, it involves
gmail.com><mailto:
.apache.org<mailto:dev@spark.apache.org>>
ta
Canny
 on
be
or
that
we
er.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:
or
d by
be
don‚Äôt
gmail.com><mailto:
s@gmail.com<mailto:evan.sparks@gmail.com><mailto:
.apache.org<mailto:dev@spark.apache.org>><mailto:
che.org<mailto:dev@spark.apache.org>>>
in
izing
y.
void
be
er.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:
rk.
ith
tive
 is
UDA
idia
th
 of
blas
 not
ed.
subscribe@spark.apache.org><mailto:
rg>><mailto:dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org>
apache.org>>>
help@spark.apache.org><mailto:
v-help@spark.apache.org<mailto:dev-help@spark.apache.org><mailto:

-- 
Best regards,
Sam

---------------------------------------------------------------------"
Xiangrui Meng <mengxr@gmail.com>,"Thu, 26 Feb 2015 17:42:05 -0800",Re: Using CUDA within Spark / boosting linear algebra,Sam Halliday <sam.halliday@gmail.com>,"The copying overhead should be quadratic on n, while the computation
cost is cubic on n. I can understand that netlib-cublas is slower than
netlib-openblas on small problems. But I'm surprised to see that it is
still 20x slower on 10000x10000. I did the following on a g2.2xlarge
instance with BIDMat:

val n = 10000

val f = rand(n, n)
flip; f*f; val rf = flop

flip; val g = GMat(n, n); g.copyFrom(f); (g*g).toFMat(null); val rg = flop

flip; g*g; val rgg = flop

The CPU version finished in 12 seconds.
The CPU->GPU->CPU version finished in 2.2 seconds.
The GPU version finished in 1.7 seconds.

I'm not sure whether my CPU->GPU->CPU code simulates the netlib-cublas
path. But based on the result, the data copying overhead is definitely
not as big as 20x at n = 10000.

Best,
Xiangrui


e:
ons. The GPU that I used is 2.5 times cheaper than the CPU ($250 vs $100). They both are 3 years old. I've also did a small test with modern hardware, and the new GPU nVidia Titan was slightly more than 1 order of magnitude faster than Intel E5-2650 v2 for the same tests. However, it costs as much as CPU ($1200). My takeaway is that GPU is making a better price/value progress.
uda and the most reasonable explanation is that it holds the result in GPU memory, as Sam suggested. At the same time, it is OK because you can copy the result back from GPU only when needed. However, to be sure, I am going to ask the developer of BIDMat on his upcoming talk.
rks
s for things like matrix multiply :-P
o set up the matrices, send it to the processing unit, doing the calculation AND copying it back to where you need to see the results.
 times faster than it really is. Again, jump to the end of my talk for graphs and more discussion....  especially the bit about me being keen on funding to investigate APU hardware further ;-) (I believe it will solve the problem)
mailto:evan.sparks@gmail.com>>
The
6kmAJeaZZggr0/pubchart?oid=1899767119&format=interactive
de)
rse
.
 not
th a
d
ance
-repo==netlib-cublas>netlib-blas>f2jblas
8T9J5r7kwKSPkY/edit?usp=sharing
etlib
e a
---+
 on
compiled
@gmail.com>]
)
nd
ar
AS,
ked
t
 is OK
ase
that
s
@gmail.com><mailto:
<mailto:dev@spark.apache.org<mailto:dev@spark.apache.org>>
om
re -
hat on
e
ary
to get
orce
s
o I can
ce use
hat
@gmail.com><mailto:
<mailto:dev@spark.apache.org<mailto:dev@spark.apache.org>>
it).
ibrary
er
64|
---+
9
 for
bricks.com><mailto:
<mailto:dev@spark.apache.org<mailto:dev@spark.apache.org>>
ing
n
n, I
ou
he
at is
ion by John
 MLlib.
DMat
to
ks in
lgebra, it involves
@gmail.com><mailto:
k.apache.org<mailto:dev@spark.apache.org>>
ata
m
 Canny
al
e on
 be
for
 that
 -
ty
 we
s,
der.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:
For
ed by
 be
 don‚Äôt
@gmail.com><mailto:
ks@gmail.com<mailto:evan.sparks@gmail.com><mailto:
k.apache.org<mailto:dev@spark.apache.org>><mailto:
ache.org<mailto:dev@spark.apache.org>>>
 in
mizing
d
ly.
avoid
 be
der.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:
ark.
with
n
ative
s is
CUDA
vidia
ith
at
s of
ublas
s not
ved.
us
nsubscribe@spark.apache.org><mailto:
org>><mailto:dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org>
.apache.org>>>
-help@spark.apache.org><mailto:
ev-help@spark.apache.org<mailto:dev-help@spark.apache.org><mailto:

---------------------------------------------------------------------


"
Ryan Williams <ryan.blake.williams@gmail.com>,"Fri, 27 Feb 2015 02:10:58 +0000",Monitoring Spark with Graphite and Grafana,"user <user@spark.apache.org>, ""dev@spark.apache.org"" <dev@spark.apache.org>","If anyone is curious to try exporting Spark metrics to Graphite, I just
published a post about my experience doing that, building dashboards in
Grafana <http://grafana.org/>, and using them to monitor Spark jobs:
http://www.hammerlab.org/2015/02/27/monitoring-spark-with-graphite-and-grafana/

Code for generating Grafana dashboards tailored to the metrics emitted by
Spark is here: https://github.com/hammerlab/grafana-spark-dashboards.

If anyone else is interested in working on expanding MetricsSystem to make
this sort of thing more useful, let me know, I've been working on it a fair
amount and have a bunch of ideas about where it should go.

Thanks,

-Ryan
"
"""Shao, Saisai"" <saisai.shao@intel.com>","Fri, 27 Feb 2015 02:25:35 +0000",RE: Monitoring Spark with Graphite and Grafana,"Ryan Williams <ryan.blake.williams@gmail.com>, user
	<user@spark.apache.org>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Cool, great job‚ò∫.

Thanks
Jerry

From: Ryan Williams [mailto:ryan.blake.williams@gmail.com]
Sent: Thursday, February 26, 2015 6:11 PM
To: user; dev@spark.apache.org
Subject: Monitoring Spark with Graphite and Grafana

If anyone is curious to try exporting Spark metrics to Graphite, I just published a post about my experience doing that, building dashboards in Grafana<http://grafana.org/>, and using them to monitor Spark jobs: http://www.hammerlab.org/2015/02/27/monitoring-spark-with-graphite-and-grafana/

Code for generating Grafana dashboards tailored to the metrics emitted by Spark is here: https://github.com/hammerlab/grafana-spark-dashboards.

If anyone else is interested in working on expanding MetricsSystem to make this sort of thing more useful, let me know, I've been working on it a fair amount and have a bunch of ideas about where it should go.

Thanks,

-Ryan


"
Victor Tso-Guillen <vtso@paxata.com>,"Thu, 26 Feb 2015 19:55:04 -0800",Re: Scheduler hang?,dev@spark.apache.org,"Love to hear some input on this. I did get a standalone cluster up on my
local machine and the problem didn't present itself. I'm pretty confident
that means the problem is in the LocalBackend or something near it.


"
Victor Tso-Guillen <vtso@paxata.com>,"Thu, 26 Feb 2015 20:32:47 -0800",Re: Scheduler hang?,dev@spark.apache.org,"Of course, breakpointing on every status update and revive offers
invocation kept the problem from happening. Where could the race be?


"
Sam Halliday <sam.halliday@gmail.com>,"Fri, 27 Feb 2015 06:47:27 +0000",Re: Using CUDA within Spark / boosting linear algebra,Xiangrui Meng <mengxr@gmail.com>,"Don't use ""big O"" estimates, always measure. It used to work back in the
days when double multiplication was a bottleneck. The computation cost is
effectively free on both the CPU and GPU and you're seeing pure copying
costs. Also, I'm dubious that cublas is doing what you think it is. Can you
link me to the source code for DGEMM?

I show all of this in my talk, with explanations, I can't stress enough how
much I recommend that you watch it if you want to understand high
performance hardware acceleration for linear algebra :-)

 flop
y
g
U
n
e
rn
lt
an
A:
m
.
AJeaZZggr0/pubchart?oid=1899767119&format=interactive
o==netlib-cublas>netlib-blas>f2jblas
5r7kwKSPkY/edit?usp=sharing
ng
g
he
y compiled
g
A
n
.
g
re
w
 so
g
e
|
g
s
ation by
y
 algebra, it
le
s
t
ou
.
o
d
a
ve
a
e
at
--
g
:
"
Jeremy Freeman <freeman.jeremy@gmail.com>,"Fri, 27 Feb 2015 02:11:13 -0500",Re: Google Summer of Code - ideas,Xiangrui Meng <mengxr@gmail.com>,"For topic #4 (streaming ML in Python), thereís an existing JIRA, but progress seems to have stalled. Iíd be happy to help if you want to pick it up!

https://issues.apache.org/jira/browse/SPARK-4127

-------------------------
jeremyfreeman.net
@thefreemanlab


are to

"
Imran Rashid <irashid@cloudera.com>,"Fri, 27 Feb 2015 13:10:00 -0600",trouble with sbt building network-* projects?,dev <dev@spark.apache.org>,"Has anyone else noticed very strange build behavior in the network-*
projects?

maven seems to the doing the right, but sbt is very inconsistent.
Sometimes when it builds network-shuffle it doesn't know about any of the
code in network-common.  Sometimes it will completely skip the java unit
tests.  And then some time later, it'll suddenly decide it knows about some
more of the java unit tests.  Its not from a simple change, like touching a
test file, or a file the test depends on -- nor a restart of sbt.  I am
pretty confused.


maven had issues when I tried to add scala code to network-common, it would
compile the scala code but not make it available to java.  I'm working
around that by just coding in java anyhow.  I'd really like to be able to
run my tests in sbt, though, it makes the development iterations much
faster.

thanks,
Imran
"
Ted Yu <yuzhihong@gmail.com>,"Fri, 27 Feb 2015 11:14:20 -0800",Re: trouble with sbt building network-* projects?,Imran Rashid <irashid@cloudera.com>,"bq. to be able to run my tests in sbt, though, it makes the development
iterations much faster.

Was the preference for sbt due to long maven build time ?
Have you started Zinc on your machine ?

Cheers


"
Imran Rashid <irashid@cloudera.com>,"Fri, 27 Feb 2015 13:31:23 -0600",Re: trouble with sbt building network-* projects?,Ted Yu <yuzhihong@gmail.com>,"well, perhaps I just need to learn to use maven better, but currently I
find sbt much more convenient for continuously running my tests.  I do use
zinc, but I'm looking for continuous testing.  This makes me think I need
sbt for that:
http://stackoverflow.com/questions/11347633/is-there-a-java-continuous-testing-plugin-for-maven

1) I really like that in sbt I can run ""~test-only
com.foo.bar.SomeTestSuite"" (or whatever other pattern) and just leave that
running as I code, without having to go and explicitly trigger ""mvn test""
and wait for the result.

2) I find sbt's handling of sub-projects much simpler (when it works).  I'm
trying to make changes to network/common & network/shuffle, which means I
have to keep cd'ing into network/common, run mvn install, then go back to
network/shuffle and run some other mvn command over there.  I don't want to
run mvn at the root project level, b/c I don't want to wait for it to
compile all the other projects when I just want to run tests in
network/common.  Even with incremental compiling, in my day-to-day coding I
want to entirely skip compiling sql, graphx, mllib etc. -- I have to switch
branches often enough that i end up triggering a full rebuild of those
projects even when I haven't touched them.






"
Xiangrui Meng <mengxr@gmail.com>,"Fri, 27 Feb 2015 12:26:22 -0800",Re: Using CUDA within Spark / boosting linear algebra,Sam Halliday <sam.halliday@gmail.com>,"Hey Sam,

The running times are not ""big O"" estimates:


I think there is something wrong with the netlib/cublas combination.
Sam already mentioned that cuBLAS doesn't implement the CPU BLAS
interfaces. I checked the CUDA doc and it seems that to use GPU BLAS
through the CPU BLAS interface we need to use NVBLAS, which intercepts
some Level 3 CPU BLAS calls (including GEMM). So we need to load
nvblas.so first and then some CPU BLAS library in JNI. I wonder
whether the setup was correct.

Alexander, could you check whether GPU is used in the netlib-cublas
experiments? You can tell it by watching CPU/GPU usage.

Best,
Xiangrui

te:
ou
ow
= flop
ly
s
ng
PU
en
r
he
e
e
$250 vs
ern
 of
t costs
result
ou can
I am
s
for
n on
lve the
x.
ZHl6kmAJeaZZggr0/pubchart?oid=1899767119&format=interactive
yum-repo==netlib-cublas>netlib-blas>f2jblas
x378T9J5r7kwKSPkY/edit?usp=sharing
k
------+
|
ly
RA
xander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>>
y
en
s.
,
:
.apache.org<mailto:dev@spark.apache.org>>
d
ow
xander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>>
o
h
, so
e
:
.apache.org<mailto:dev@spark.apache.org>>
ve
S
xander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>>
------+
 |
a
h
:
.apache.org<mailto:dev@spark.apache.org>>
f
n
as
xander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>>
tation by
ay
r algebra, it
:
.apache.org<mailto:dev@spark.apache.org>>
o
es
s
ut
xander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:
xander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>>>
g.
:
parks@gmail.com<mailto:evan.sparks@gmail.com><mailto:
.apache.org<mailto:dev@spark.apache.org>><mailto:
.apache.org<mailto:dev@spark.apache.org>>>
to
xander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:
xander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>>>
ed
va
t
ia
h
f
s
.
he
---
he.org><mailto:
he.org>><mailto:dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org>
ark.apache.org>>>
:
o:dev-help@spark.apache.org<mailto:dev-help@spark.apache.org><mailto:

---------------------------------------------------------------------


"
Sam Halliday <sam.halliday@gmail.com>,"Fri, 27 Feb 2015 20:33:00 +0000",Re: Using CUDA within Spark / boosting linear algebra,Xiangrui Meng <mengxr@gmail.com>,"Also, check the JNILoader output.

Remember, for netlib-java to use your system libblas all you need to do is
setup libblas.so.3 like any native application would expect.

I haven't ever used the cublas ""real BLAS""  implementation, so I'd be
interested to hear about this. Do an 'ldd /usr/lib/libblas.so.3' to check
that all the runtime links are in order.

Btw, I have some DGEMM wrappers in my netlib-java performance module... and
I also planned to write more in MultiBLAS (until I mothballed the project
for the hardware to catch up, which is probably has and now I just need a
reason to look at it)

e
is
=
s
er
f
t
n
e
e
,
.
s.
ds
k
AS
AJeaZZggr0/pubchart?oid=1899767119&format=interactive
de
is
s
nd
it
de
SX
o==netlib-cublas>netlib-blas>f2jblas
5r7kwKSPkY/edit?usp=sharing
h
 |
1
ally
le
y
at
if
y
es
e.
d
AS,
o
r
ve
t
entation
k:
ear algebra, it
n
e
r
s.
o
y
a
op
t
to
in
ts
.
so
ib
is
ry
g
"
Ted Yu <yuzhihong@gmail.com>,"Fri, 27 Feb 2015 13:54:54 -0800",Re: trouble with sbt building network-* projects?,Imran Rashid <irashid@cloudera.com>,"bq. I have to keep cd'ing into network/common, run mvn install, then go
back to network/shuffle and run some other mvn command over there.

Yeah - been through this.

Having continuous testing for maven would be nice.


"
"""r7raul1984@163.com"" <r7raul1984@163.com>","Sat, 28 Feb 2015 15:58:43 +0800",How to create a Row from a List or Array in Spark using Scala,dev <dev@spark.apache.org>,"import org.apache.spark.sql.catalyst.expressions._

val values: JavaArrayList[Any] = new JavaArrayList()
computedValues = Row(values.get(0),values.get(1)) //It is not good by use get(index).  How to create a Row from a List or Array in Spark using Scala .



r7raul1984@163.com
"
"""DEVAN M.S."" <msdevanms@gmail.com>","Sat, 28 Feb 2015 14:07:37 +0530",Re: How to create a Row from a List or Array in Spark using Scala,"""r7raul1984@163.com"" <r7raul1984@163.com>","  In scala API its there, Row.fromSeq(ARRAY), I dnt know much more
about java api



Devan M.S. | Research Associate | Cyber Security | AMRITA VISHWA
VIDYAPEETHAM | Amritapuri | Cell +919946535290 |



"
Manoj Kumar <manojkumarsivaraj334@gmail.com>,"Sun, 1 Mar 2015 01:04:09 +0530",Re: Google Summer of Code - ideas,Jeremy Freeman <freeman.jeremy@gmail.com>,"Hi,

Thanks a lot.
Yes indeed, I am interested. I shall start looking at all the related
JIRA's in a while.



-- 
Godspeed,
Manoj Kumar,
http://manojbits.wordpress.com
<http://goog_1017110195>
http://github.com/MechCoder
"
Victor Tso-Guillen <vtso@paxata.com>,"Sat, 28 Feb 2015 11:51:06 -0800",Re: Scheduler hang?,dev@spark.apache.org,"Moving user to bcc.

What I found was that the TaskSetManager for my task set that had 5 tasks
had preferred locations set for 4 of the 5. Three had localhost/<driver>
and had completed. The one that had nothing had also completed. The last
one was set by our code to be my IP address. Local mode can hang on this
because of https://issues.apache.org/jira/browse/SPARK-4939 addressed by
https://github.com/apache/spark/pull/4147, which is obviously not an
optimal solution but since it's only local mode, it's very good enough. I'm
not going to wait for those seconds to tick by to complete the task, so
I'll fix the IP address reporting side for local mode in my code.


"
Sath <neiodavince@gmail.com>,"Sat, 28 Feb 2015 12:26:31 -0800",Re: Google Summer of Code - ideas,Manoj Kumar <manojkumarsivaraj334@gmail.com>,"All 

  I would like to contribute for the google summer of code projects. Please guide me to start the process 


Sath 



---------------------------------------------------------------------


"
