"""=?ISO-8859-1?B?R3VvcWlhbmcgTGk=?="" <witgo@qq.com>","Mon, 1 Jun 2015 10:04:44 +0800",Re:  [VOTE] Release Apache Spark 1.4.0 (RC3),"""=?ISO-8859-1?B?UGF0cmljayBXZW5kZWxs?="" <pwendell@gmail.com>","+1 (non-binding)




------------------ Original ------------------
From:  ""Sandy Ryza"";<sandy.ryza@cloudera.com>;
Date:  Mon, Jun 1, 2015 07:34 AM
To:  ""Krishna Sankar""<ksankar42@gmail.com>; 
Cc:  ""Patrick Wendell""<pwendell@gmail.com>; ""dev@spar"
Reynold Xin <rxin@databricks.com>,"Sun, 31 May 2015 21:57:01 -0700",Re: Why is RDD to PairRDDFunctions only via implicits?,"Justin Pihony <justin.pihony@gmail.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","dropping user list, adding dev.


Thanks, Justin, for the poc. This is a good idea to explore, especially for
Spark 2.0.



"
=?UTF-8?Q?Ren=C3=A9_Treffer?= <rtreffer@gmail.com>,"Mon, 1 Jun 2015 10:10:22 +0200",spark 1.4 - test-loading 1786 mysql tables / a few TB,dev@spark.apache.org,"Hi *,

I used to run into a few problems with the jdbc/mysql integration and
thought it would be nice to load our whole db, doing nothing but .map(_ =>
1).aggregate(0)(_+_,_+_) on the DataFrames.
SparkSQL has to load all columns and process them so this should reveal
type errors like
SPARK-7897 Column with an unsigned bigint should be treated as DecimalType
in JDBCRDD <https://issues.apache.org/jira/browse/SPARK-7897>
SPARK-7697 <https://issues.apache.org/jira/browse/SPARK-7697>Column with an
unsigned int should be treated as long in JDBCRDD

The test was done on the 1.4 branch (checkout 2-3 days ago, local build,
running standalone with a 350G heap).

1. Date/Timestamp 0000-00-00

org.apache.spark.SparkException: Job aborted due to stage failure: Task 15
in stage 18.0 failed 1 times, most recent failure: Lost task 15.0 in stage
18.0 (TID 186, localhost): java.sql.SQLException: Value '0000-00-00
00:00:00' can not be represented as java.sql.Timestamp
        at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:998)

org.apache.spark.SparkException: Job aborted due to stage failure: Task 0
in stage 26.0 failed 1 times, most recent failure: Lost task 0.0 in stage
26.0 (TID 636, localhost): java.sql.SQLException: Value '0000-00-00' can
not be represented as java.sql.Date
        at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:998)

This was the most common error when I tried to load tables with
Date/Timestamp types.
Can be worked around by subqueries or by specifying those types to be
string and handling them afterwards.

2. Keywords as column names fail

SparkSQL does not enclose column names, e.g.
SELECT key,value FROM tablename
fails and should be
SELECT `key`,`value` FROM tablename

org.apache.spark.SparkException: Job aborted due to stage failure: Task 0
in stage 157.0 failed 1 times, most recent failure: Lost task 0.0 in stage
157.0 (TID 4322, localhost):
com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an
error in your SQL syntax; check the manual that corresponds to your MySQL
server version for the right syntax to use near 'key,value FROM [XXXXXX]'

I'm not sure how to work around that issue except for manually writing
sub-queries (with all the performance problems that may cause).

3. Overloaded DB due to concurrency

While providing where clauses works well to parallelize the fetch it can
overload the DB, thus causing trouble (e.g. query/connection timeouts due
to an overloaded DB).
It would be nice to specify fetch parallelism independent from result
partitions (e.g. 100 partitions, but don't fetch more than 5 in parallel).
This can be emulated by loading just n partition at a time and doing a
union afterwards. (gouped(5).map(...)....)

Success

I've successfully loaded 8'573'651'154 rows from 1667 tables ( >93% success
rate). This is pretty awesome given that e.g. sqoop has failed horrible on
the same data.
Note that I didn't not verify that the retrieved data is valid. I've only
checked for fetch errors so far. But given that spark does not silence any
errors I'm quite confident that fetched data will be valid :-)

Regards,
  Rene Treffer
"
Reynold Xin <rxin@databricks.com>,"Mon, 1 Jun 2015 01:18:53 -0700",Re: spark 1.4 - test-loading 1786 mysql tables / a few TB,=?UTF-8?Q?Ren=C3=A9_Treffer?= <rtreffer@gmail.com>,"RenÃ©,

Thanks for sharing your experience. Are you using the DataFrame API or SQL?

(1) Any recommendations on what we do w.r.t. out of range values? Should we
silently turn them into a null? Maybe based on an option?

(2) Looks like a good idea to always quote column names. The small tricky
thing is each database seems to have its own unique quotes. Do you mind
filing a JIRA for this?

(3) It is somewhat hard to do because that'd require changing Spark's task
scheduler. The easiest way maybe to coalesce it into a smaller number of
partitions -- or we can coalesce for the user based on an option. Can you
file a JIRA for this also?

Thanks!










e:

=>
e
5
e
e
.
y
"
Reynold Xin <rxin@databricks.com>,"Mon, 1 Jun 2015 01:51:19 -0700",Re: spark 1.4 - test-loading 1786 mysql tables / a few TB,=?UTF-8?Q?Ren=C3=A9_Treffer?= <rtreffer@gmail.com>,"Never mind my comment about 3. You were talking about the read side, while
I was thinking about the write side. Your workaround actually is a pretty
good idea. Can you create a JIRA for that as well?


L?
k
=>
0
0
e
0
ge
L
'
e
).
y
ny
"
=?UTF-8?Q?Ren=C3=A9_Treffer?= <rtreffer@gmail.com>,"Mon, 1 Jun 2015 10:54:14 +0200",Re: spark 1.4 - test-loading 1786 mysql tables / a few TB,Reynold Xin <rxin@databricks.com>,"Hi,

I'm using sqlContext.jdbc(uri, table, where).map(_ =>
1).aggregate(0)(_+_,_+_) on an interactive shell (where ""where"" is an
Array[String] of 32 to 48 elements).  (The code is tailored to your db,
specifically through the where conditions, I'd have otherwise post it)
That should be the DataFrame API, but I'm just trying to load everything
and discard it as soon as possible :-)

(1) Never do a silent drop of the values by default: it kills confidence.
An option sounds reasonable.  Some sort of insight / log would be great.
(How many columns of what type were truncated? why?)
Note that I could declare the field as string via JdbcDialects (thank you
guys for merging that :-) ).
I have quite bad experiences with silent drops / truncates of columns and
thus _like_ the strict way of spark. It causes trouble but noticing later
that your data was corrupted during conversion is even worse.

(2) SPARK-8004 https://issues.apache.org/jira/browse/SPARK-8004

to document the behavior (s.th. like ""WARNING: this method tries to load as
many partitions as possible, make sure your database can handle the load or
load them in chunks and use union""). SPARK-8008
https://issues.apache.org/jira/browse/SPARK-8008

Regards,
  Rene Treffer
"
Steve Loughran <stevel@hortonworks.com>,"Mon, 1 Jun 2015 10:40:58 +0000","Re: please use SparkFunSuite instead of ScalaTest's FunSuite from
 now on",Reynold Xin <rxin@databricks.com>,"Is this backported to branch 1.3?


FYI we merged a patch that improves unit test log debugging. In order for that to work, all test suites have been changed to extend SparkFunSuite instead of ScalaTest's FunSuite. We also added a rule in the Scala style checker to fail Jenkins if FunSuite is used.

The patch that introduced SparkFunSuite: https://github.com/apache/spark/pull/6441

Style check patch: https://github.com/apache/spark/pull/6510


"
Tarek Auel <tarek.auel@gmail.com>,"Mon, 01 Jun 2015 15:54:17 +0000",GraphX: New graph operator,dev@spark.apache.org,"Hello,

Someone proposed in a Jira issue to implement new graph operations. Sean
Owen recommended to check first with the mailing list, if this is
interesting or not.

So I would like to know, if it is interesting for GraphX to implement the
operators like:
http://en.wikipedia.org/wiki/Graph_operations and/or
http://techieme.in/complex-graph-operations/

If yes, should they be integrated into GraphImpl (like mask, subgraph etc.)
or as external library? My feeling is that they are similar to mask.
Because of consistency they should be part of the graph implementation
itself.

What do you guys think? I really would like to bring GraphX forward and
help to implement some of these.

Looking forward to hear your opinions
Tarek
"
Reynold Xin <rxin@databricks.com>,"Mon, 1 Jun 2015 09:17:02 -0700",Re: please use SparkFunSuite instead of ScalaTest's FunSuite from now on,Steve Loughran <stevel@hortonworks.com>,"I don't think so.


"
Peter Rudenko <petro.rudenko@gmail.com>,"Mon, 1 Jun 2015 20:45:11 +0300",Re: [VOTE] Release Apache Spark 1.4.0 (RC3),"Guoqiang Li <witgo@qq.com>, Patrick Wendell <pwendell@gmail.com>","Still have problem using HiveContext from sbt. Hereâ€™s an example of 
dependencies:

|val sparkVersion = ""1.4.0-rc3"" lazy val root = Project(id = 
""spark-hive"", base = file("".""), settings = Project.defaultSettings ++ 
Seq( name := ""spark-1.4-hive"", scalaVersion := ""2.10.5"", 
scalaBinaryVersion := ""2.10"", resolvers += ""Spark RC"" at 
""https://repository.apache.org/content/repositories/orgapachespark-1110/"", 
libraryDependencies ++= Seq( ""org.apache.spark"" %% ""spark-core"" % 
sparkVersion, ""org.apache.spark"" %% ""spark-mllib"" % sparkVersion, 
""org.apache.spark"" %% ""spark-hive"" % sparkVersion, ""org.apache.spark"" %% 
""spark-sql"" % sparkVersion ) )) |

Launching sbt console with it and running:

|val conf = new SparkConf().setMaster(""local[4]"").setAppName(""test"") val 
sc = new SparkContext(conf) val sqlContext = new 
org.apache.spark.sql.hive.HiveContext(sc) val data = sc.parallelize(1 to 
10000) import sqlContext.implicits._ scala> data.toDF 
java.lang.IllegalArgumentException: Unable to locate hive jars to 
connect to metastore using classloader 
scala.tools.nsc.interpreter.IMain$TranslatingClassLoader. Please set 
spark.sql.hive.metastore.jars at 
org.apache.spark.sql.hive.HiveContext.metadataHive$lzycompute(HiveContext.scala:206) 
at 
org.apache.spark.sql.hive.HiveContext.metadataHive(HiveContext.scala:175) at 
org.apache.spark.sql.hive.HiveContext$anon$2.<init>(HiveContext.scala:367) 
at 
org.apache.spark.sql.hive.HiveContext.catalog$lzycompute(HiveContext.scala:367) 
at org.apache.spark.sql.hive.HiveContext.catalog(HiveContext.scala:366) 
at 
org.apache.spark.sql.hive.HiveContext$anon$1.<init>(HiveContext.scala:379) 
at 
org.apache.spark.sql.hive.HiveContext.analyzer$lzycompute(HiveContext.scala:379) 
at org.apache.spark.sql.hive.HiveContext.analyzer(HiveContext.scala:378) 
at 
org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:901) 
at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:134) at 
org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:51) at 
org.apache.spark.sql.SQLContext.createDataFrame(SQLContext.scala:474) at 
org.apache.spark.sql.SQLContext.createDataFrame(SQLContext.scala:456) at 
org.apache.spark.sql.SQLContext$implicits$.intRddToDataFrameHolder(SQLContext.scala:345) 
|

Thanks,
Peter Rudenko


â€‹
"
Andrew Or <andrew@databricks.com>,"Mon, 1 Jun 2015 10:45:37 -0700",Re: please use SparkFunSuite instead of ScalaTest's FunSuite from now on,Reynold Xin <rxin@databricks.com>,"It will be within the next few days

2015-06-01 9:17 GMT-07:00 Reynold Xin <rxin@databricks.com>:

"
Yin Huai <yhuai@databricks.com>,"Mon, 1 Jun 2015 11:10:37 -0700",Re: [VOTE] Release Apache Spark 1.4.0 (RC3),Peter Rudenko <petro.rudenko@gmail.com>,"Hi Peter,

Based on your error message, seems you were not using the RC3. For the
error thrown at HiveContext's line 206, we have changed the message to this
one
<https://github.com/apache/spark/blob/v1.4.0-rc3/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveContext.scala#L205-207>
just
before RC3. Basically, we will not print out the class loader name. Can you
check if a older version of 1.4 branch got used? Have you published a RC3
to your local maven repo? Can you clean your local repo cache and try again?

Thanks,

Yin


 of
t/repositories/orgapachespark-1110/"" <https://repository.apache.org/content/repositories/orgapachespark-1110/>,
 to metastore using classloader scala.tools.nsc.interpreter.IMain$TranslatingClassLoader. Please set spark.sql.hive.metastore.jars
Context.scala:206)
la:175)
ala:367)
xt.scala:367)
6)
ala:379)
ext.scala:379)
78)
ontext.scala:901)
74)
56)
(SQLContext.scala:345)
)
R
'"") OK
n
109a8746ec07c7c83995890fc2c0cd7a693730
"
Andrew Or <andrew@databricks.com>,"Mon, 1 Jun 2015 11:52:11 -0700",Re: [VOTE] Release Apache Spark 1.4.0 (RC3),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1 (binding)

Tested the standalone cluster mode REST submission gateway - submit /
status / kill
Tested simple applications on YARN client / cluster modes with and without
--jars
Tested python applications on YARN client / cluster modes with and without
"
Reynold Xin <rxin@databricks.com>,"Mon, 1 Jun 2015 14:01:53 -0700",Re: spark 1.4 - test-loading 1786 mysql tables / a few TB,=?UTF-8?Q?Ren=C3=A9_Treffer?= <rtreffer@gmail.com>,"Thanks, RenÃ©. I actually added a warning to the new JDBC reader/writer
interface for 1.4.0.

Even with that, I think we should support throttling JDBC; otherwise it's
too convenient for our users to DOS their production database servers!


  /**
   * Construct a [[DataFrame]] representing the database table accessible
via JDBC URL
   * url named table. Partitions of the table will be retrieved in parallel
based on the parameters
   * passed to this function.
   *
*   * Don't create too many partitions in parallel on a large cluster;
otherwise Spark might crash*
*   * your external database systems.*
   *
   * @param url JDBC database url of the form `jdbc:subprotocol:subname`
   * @param table Name of the table in the external database.
   * @param columnName the name of a column of integral type that will be
used for partitioning.
   * @param lowerBound the minimum value of `columnName` used to decide
partition stride
   * @param upperBound the maximum value of `columnName` used to decide
partition stride
   * @param numPartitions the number of partitions.  the range
`minValue`-`maxValue` will be split
   *                      evenly into this many partitions
   * @param connectionProperties JDBC database connection arguments, a list
of arbitrary string
   *                             tag/value. Normally at least a ""user"" and
""password"" property
   *                             should be included.
   *
   * @since 1.4.0
   */


e:

ad
"
Peter Rudenko <petro.rudenko@gmail.com>,"Tue, 2 Jun 2015 00:19:01 +0300",Re: [VOTE] Release Apache Spark 1.4.0 (RC3),Yin Huai <yhuai@databricks.com>,"Thanks Yin, tried on a clean VM - works now. But tests in my app still 
fails:

|[info] Cause: javax.jdo.JDOFatalDataStoreException: Unable to open a 
test connection to the given database. JDBC url = 
jdbc:derby:;databaseName=metastore_db;create=true, username = APP. 
Terminating connection pool (set lazyInit to true if you expect to start 
your database after your app). Original Exception: ------ [info] 
java.sql.SQLException: Failed to start database 'metastore_db' with 
class loader 
org.apache.spark.sql.hive.client.IsolatedClientLoader$anon$1@380628de, 
see the next exception for details. [info] at 
org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown 
Source) [info] at 
org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) 
[info] at org.apache.derby.impl.jdbc.Util.seeNextException(Unknown 
Source) [info] at 
org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(Unknown Source) 
[info] at org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown 
Source) [info] at 
org.apache.derby.impl.jdbc.EmbedConnection40.<init>(Unknown Source) 
[info] at org.apache.derby.jdbc.Driver40.getNewEmbedConnection(Unknown 
Source) [info] at org.apache.derby.jdbc.InternalDriver.connect(Unknown 
Source) [info] at org.apache.derby.jdbc.Driver20.connect(Unknown Source) 
[info] at org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source) 
[info] at java.sql.DriverManager.getConnection(DriverManager.java:571) 
[info] at java.sql.DriverManager.getConnection(DriverManager.java:187) 
[info] at 
com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361) 
[info] at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416) [info] at 
com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120) 
|

Iâ€™ve set

parallelExecution in Test := false,

Thanks,
Peter Rudenko


â€‹
"
Michael Armbrust <michael@databricks.com>,"Tue, 2 Jun 2015 07:43:01 +1000",Re: [VOTE] Release Apache Spark 1.4.0 (RC3),Peter Rudenko <petro.rudenko@gmail.com>,"Its no longer valid to start more than one instance of HiveContext in a
single JVM, as one of the goals of this refactoring was to allow connection
to more than one metastore from a single context.

For tests I suggest you use TestHive as we do in our unit tests.  It has a
reset() method you can use to cleanup state between tests/suites.

We could also add an explicit close() method to remove this restriction,
but if thats something you want to investigate we should move this off the
vote thread and onto JIRA.


st connection to the given database. JDBC url = jdbc:derby:;databaseName=metastore_db;create=true, username = APP. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------
h class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$anon$1@380628de, see the next exception for details.
ption(Unknown Source)
n Source)
urce)
nown Source)
ource)
 Source)
n Source)
)
ce)
)
)
.java:361)
Source.java:120)
is
org/apache/spark/sql/hive/HiveContext.scala#L205-207> just
ou
in?
e of
nt/repositories/orgapachespark-1110/"" <https://repository.apache.org/content/repositories/orgapachespark-1110/>,
t to metastore using classloader scala.tools.nsc.interpreter.IMain$TranslatingClassLoader. Please set spark.sql.hive.metastore.jars
eContext.scala:206)
ala:175)
cala:367)
ext.scala:367)
66)
cala:379)
text.scala:379)
378)
Context.scala:901)
474)
456)
r(SQLContext.scala:345)
t)
ER
A'"")
d109a8746ec07c7c83995890fc2c0cd7a693730
:
/
/
/
"
Sean Owen <sowen@cloudera.com>,"Mon, 1 Jun 2015 23:05:26 +0100",Re: [VOTE] Release Apache Spark 1.4.0 (RC3),Patrick Wendell <pwendell@gmail.com>,"I get a bunch of failures in VersionSuite with build/test params
""-Pyarn -Phive -Phadoop-2.6"":

- success sanity check *** FAILED ***
  java.lang.RuntimeException: [download failed:
org.jboss.netty#netty;3.2.2.Final!netty.jar(bundle), download failed:
commons-net#commons-net;3.1!commons-net.jar]
  at org.apache.spark.deploy.SparkSubmitUtils$.resolveMavenCoordinates(SparkSubmit.scala:978)

... but maybe I missed the memo about how to build for Hive? do I
still need another Hive profile?

Other tests, signatures, etc look good.


---------------------------------------------------------------------


"
Bobby Chowdary <bobby.chowdary03@gmail.com>,"Mon, 1 Jun 2015 17:18:15 -0700",Re: [VOTE] Release Apache Spark 1.4.0 (RC3),"""dev@spark.apache.org"" <dev@spark.apache.org>","Hive Context works on RC3 for Mapr after adding
spark.sql.hive.metastore.sharedPrefixes as suggested in SPARK-7819
<https://issues.apache.org/jira/browse/SPARK-7819>. However, there still
seems to be some other issues with native libraries, i get below warning
WARN NativeCodeLoader: Unable to load native-hadoop library for your
platform... using builtin-java classes where applicable. I tried adding
even after adding SPARK_LIBRARYPATH and --driver-library-path with no luck.

Built on MacOSX and running CentOS 7 JDK1.6 and JDK 1.8 (tried both)

 make-distribution.sh --tgz --skip-java-test -Phive -Phive-0.13.1 -Pmapr4
-Pnetlib-lgpl -Phive-thriftserver.
  Câ€‹


bmit.scala:978)
n
9a8746ec07c7c83995890fc2c0cd7a693730
"
Gerard Maas <gerard.maas@gmail.com>,"Tue, 2 Jun 2015 02:28:15 +0200",Re: [Streaming] Configure executor logging on Mesos,"Tim Chen <tim@mesosphere.io>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Tim,
(added dev, removed user)

I've created https://issues.apache.org/jira/browse/SPARK-8009 to track this.

-kr, Gerard.


r
e.
t
/0/marathon.json
 in
d
i
M is
it
e
fy a
ork
che/spark/Logging.scala#L128
che/spark/scheduler/cluster/mesos/MesosSchedulerBackend.scala#L77
d
k.
.uri,
s
h an
t
e
n
e
"
Patrick Wendell <pwendell@gmail.com>,"Mon, 1 Jun 2015 21:41:48 -0700",Re: [VOTE] Release Apache Spark 1.4.0 (RC3),Bobby Chowdary <bobby.chowdary03@gmail.com>,"Hey Bobby,

Those are generic warnings that the hadoop libraries throw. If you are
using MapRFS they shouldn't matter since you are using the MapR client
and not the default hadoop client.

Do you have any issues with functionality... or was it just seeing the
warnings that was the concern?

Thanks for helping test!

- Patrick


---------------------------------------------------------------------


"
Matt Cheah <mcheah@palantir.com>,"Tue, 2 Jun 2015 05:21:56 +0000",[SQL] Write parquet files under partition directories?,"""dev@spark.apache.org"" <dev@spark.apache.org>, Mingyu Kim
	<mkim@palantir.com>, Andrew Ash <aash@palantir.com>","Hi there,

I noticed in the latest Spark SQL programming guide
<https://spark.apache.org/docs/latest/sql-programming-guide.html> , there is
support for optimized reading of partitioned Parquet files that have a
particular directory structure (year=1/month=10/day=3, for example).
However, I see no analogous way to write DataFrames as Parquet files with
similar directory structures based on user-provided partitioning.

Generally, is it possible to write DataFrames as partitioned Parquet files
that downstream partition discovery can take advantage of later? I
considered extending the Parquet output format, but it looks like
ParquetTableOperations.scala has fixed the output format to
AppendingParquetOutputFormat.

Also, I was wondering if it would be valuable to contribute writing Parquet
in partition directories as a PR.

Thanks,

-Matt Cheah


"
Reynold Xin <rxin@databricks.com>,"Mon, 1 Jun 2015 22:25:28 -0700",Re: [SQL] Write parquet files under partition directories?,Matt Cheah <mcheah@palantir.com>,"There will be in 1.4.

df.write.partitionBy(""year"", ""month"", ""day"").parquet(""/path/to/output"")


"
Bobby Chowdary <bobby.chowdary03@gmail.com>,"Mon, 1 Jun 2015 22:33:57 -0700",Re: [VOTE] Release Apache Spark 1.4.0 (RC3),Patrick Wendell <pwendell@gmail.com>,"Hi Patrick,
                  Thanks for clarifying. No issues with functionality.
+1 (non-binding)

Thanks
Bobby


"
Ankur Dave <ankurdave@gmail.com>,"Mon, 1 Jun 2015 23:55:00 -0700",Re: GraphX: New graph operator,Tarek Auel <tarek.auel@gmail.com>,"I think it would be good to have more basic operators like union or
difference, as long as they have an efficient distributed implementation
and are plausibly useful.

If they can be written in terms of the existing GraphX API, it would be
best to put them into GraphOps to keep the core GraphX implementation
small. The `mask` operation should actually be in GraphOps -- it's only in
in GraphImpl for performance: it accesses EdgeRDDImpl#filter(epred, vpred),
which can't be a public EdgeRDD method because its semantics rely on an
implementation detail (vertex replication).

Ankur <http://www.ankurdave.com/>


"
Tarek Auel <tarek.auel@gmail.com>,"Tue, 02 Jun 2015 07:12:16 +0000",Re: GraphX: New graph operator,Ankur Dave <ankurdave@gmail.com>,"Okay thanks for your feedback.

What is the expected behavior of union? Like Union and/or union all of SQL?
Union all would be more or less trivial if we just concatenate the vertices
and edges (vertex Id conflicts have to be resolved). Should union look for
duplicates on the actual attribute (VD) or just the vertex Id? If it
compares the attribute it might be necessary to change the id of some
vertices in order to resolve conflicts.

Already a big thanks for your inputs !

"
RoyGaoVLIS <roygao@zju.edu.cn>,"Tue, 2 Jun 2015 01:25:37 -0700 (MST)",about Spark MLlib StandardScaler's Implementation,dev@spark.apache.org,"Hi,
	When I was trying to add test case for MLâ€™s StandardScaler, I found MLlibâ€™s
StandardScalerâ€™s output different from R with paramsï¼ˆwithMean false,
withScale trueï¼‰
	Because columns is divided by root-mean-square rather than standard
deviation in R, the scale function.
	Iâ€™ m confused about Spark MLlibâ€™s implementation.
	AnyBody can give me a hand ? thx



--
3.nabble.com/about-Spark-MLlib-StandardScaler-s-Implementation-tp12554.html
om.

---------------------------------------------------------------------


"
Steve Loughran <stevel@hortonworks.com>,"Tue, 2 Jun 2015 09:31:20 +0000","Re: please use SparkFunSuite instead of ScalaTest's FunSuite from
 now on",Andrew Or <andrew@databricks.com>,"thanks. All I'd need would be the base class, so that new tests can be written to work across branches.

-steve


It will be within the next few days

2015-06-01 9:17 GMT-07:00 Reynold Xin <rxin@databricks.com<mailto:rxin@databricks.com>>:
I don't think so.


Is this backported to branch 1.3?


FYI we merged a patch that improves unit test log debugging. In order for that to work, all test suites have been changed to extend SparkFunSuite instead of ScalaTest's FunSuite. We also added a rule in the Scala style checker to fail Jenkins if FunSuite is used.

The patch that introduced SparkFunSuite: https://github.com/apache/spark/pull/6441

Style check patch: https://github.com/apache/spark/pull/6510




"
Olivier Girardot <ssaboum@gmail.com>,"Tue, 02 Jun 2015 09:31:34 +0000",Re: [VOTE] Release Apache Spark 1.4.0 (RC3),"Bobby Chowdary <bobby.chowdary03@gmail.com>, Patrick Wendell <pwendell@gmail.com>","Hi everyone,
I think there's a blocker on PySpark the ""when"" functions in python seems
to be broken but the Scala API seems fine.
Here's a snippet demonstrating that with Spark 1.4.0 RC3 :

In [*1*]: df = sqlCtx.createDataFrame([(1, ""1""), (2, ""2""), (1, ""2""), (1,
""2"")], [""key"", ""value""])

In [*2*]: from pyspark.sql import functions as F

In [*8*]: df.select(df.key, F.when(df.key > 1, 0).when(df.key == 0,
2).otherwise(1)).show()
+---+---------------------------------+
| key |CASE WHEN (key = 0) THEN 2 ELSE 1|
+---+---------------------------------+
| 1| 1|
| 2| 1|
| 1| 1|
| 1| 1|
+---+---------------------------------+

When in Scala I get the expectes expression and behaviour :

scala> val df = sqlContext.createDataFrame(List((1, ""1""), (2, ""2""), (1,
""2""), (1, ""2""))).toDF(""key"", ""value"")

scala> import org.apache.spark.sql.functions._

scala> df.select(df(""key""), when(df(""key"") > 1, 0).when(df(""key"") === 2,
2).otherwise(1)).show()


+---+-------------------------------------------------------+

|key|CASE WHEN (key > 1) THEN 0 WHEN (key = 2) THEN 2 ELSE 1|
+---+-------------------------------------------------------+
| 1| 1|
| 2| 0|
| 1| 1|
| 1| 1|
+---+-------------------------------------------------------+

I've opened the Jira (https://issues.apache.org/jira/browse/SPARK-8038) and
fixed it here https://github.com/apache/spark/pull/6580

Regards,

Olivier.

Le mar. 2 juin 2015 Ã  07:34, Bobby Chowdary <bobby.chowdary03@gmail.com> a
Ã©crit :

g
ubmit.scala:978)
09a8746ec07c7c83995890fc2c0cd7a693730
1.
--
"
Mick Davies <michael.belldavies@gmail.com>,"Tue, 2 Jun 2015 04:25:26 -0700 (MST)",Unit tests can generate spurious shutdown messages,dev@spark.apache.org,"If I write unit tests that indirectly initialize org.apache.spark.util.Utils,
for example use sql types, but produce no logging, I get the following
unpleasant stack trace in my test output.

This caused by the the Utils class adding a shutdown hook which logs the
message logDebug(""Shutdown hook called""). We are using log4j 2 for logging
and if there has been no logging before this point then the static
initialization of log4j 2 tries to add a shutdown hook itself but can't
because JVM is already in shutdown.

Its only slightly annoying but could be easily 'fixed' by adding a line
like:
logDebug(""Adding shutdown hook) 
to Utils before adding the shutdown hook, so ensuring logging always
initialized. I am happy to make this change, unless there is a better
approach or considered too trivial.

ERROR StatusLogger catching java.lang.IllegalStateException: Shutdown in
progress
	at java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:66)
	at java.lang.Runtime.addShutdownHook(Runtime.java:211)
	at
org.apache.logging.log4j.core.util.DefaultShutdownCallbackRegistry.addShutdownHook(DefaultShutdownCallbackRegistry.java:136)
	at
org.apache.logging.log4j.core.util.DefaultShutdownCallbackRegistry.start(DefaultShutdownCallbackRegistry.java:125)
	at
org.apache.logging.log4j.core.impl.Log4jContextFactory.initializeShutdownCallbackRegistry(Log4jContextFactory.java:123)
	at
org.apache.logging.log4j.core.impl.Log4jContextFactory.<init>(Log4jContextFactory.java:89)
	at
org.apache.logging.log4j.core.impl.Log4jContextFactory.<init>(Log4jContextFactory.java:54)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at
sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at
sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:408)
	at java.lang.Class.newInstance(Class.java:438)
	at org.apache.logging.log4j.LogManager.<clinit>(LogManager.java:96)
	at
org.apache.logging.log4j.spi.AbstractLoggerAdapter.getContext(AbstractLoggerAdapter.java:102)
	at
org.apache.logging.slf4j.Log4jLoggerFactory.getContext(Log4jLoggerFactory.java:43)
	at
org.apache.logging.log4j.spi.AbstractLoggerAdapter.getLogger(AbstractLoggerAdapter.java:42)
	at
org.apache.logging.slf4j.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:29)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:285)
	at org.apache.spark.Logging$class.log(Logging.scala:52)
	at org.apache.spark.util.Utils$.log(Utils.scala:62)
	at org.apache.spark.Logging$class.initializeLogging(Logging.scala:138)
	at org.apache.spark.Logging$class.initializeIfNecessary(Logging.scala:107)
	at org.apache.spark.Logging$class.log(Logging.scala:51)
	at org.apache.spark.util.Utils$.log(Utils.scala:62)
	at org.apache.spark.Logging$class.logDebug(Logging.scala:63)
	at org.apache.spark.util.Utils$.logDebug(Utils.scala:62)
	at
org.apache.spark.util.Utils$$anon$4$$anonfun$run$1.apply$mcV$sp(Utils.scala:178)
	at
org.apache.spark.util.Utils$$anon$4$$anonfun$run$1.apply(Utils.scala:177)
	at
org.apache.spark.util.Utils$$anon$4$$anonfun$run$1.apply(Utils.scala:177)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.util.Utils$$anon$4.run(Utils.scala:177)








--

---------------------------------------------------------------------


"
Joseph Bradley <joseph@databricks.com>,"Tue, 2 Jun 2015 11:50:40 -0700",Re: about Spark MLlib StandardScaler's Implementation,RoyGaoVLIS <roygao@zju.edu.cn>,"Your understanding is correct: When used without centering (withMean =
false), the 2 implementations are different:
* R: normalize by RMS
* MLlib: normalize by stddev
With centering, they are the same.

It's hard to say which one is better a priori, but my guess is that most R
users center their data.  (Centering is nice to do, except on big data
where it makes vectors dense.)  Note that R does allow you to normalize by
stddev without centering:
https://stat.ethz.ch/R-manual/R-devel/library/base/html/scale.html

Joseph


r, I
Mean false,
rd
ib-StandardScaler-s-Implementation-tp12554.html
"
"""Eskilson,Aleksander"" <Alek.Eskilson@Cerner.com>","Tue, 2 Jun 2015 18:52:47 +0000",CSV Support in SparkR,"""dev@spark.apache.org"" <dev@spark.apache.org>","Are there any intentions to provide first class support for CSV files as one of the loadable file types in SparkR? Data brick’s spark-csv API [1] has support for SQL, Python, and Java/Scala, and implements most of the arguments of R’s read.table API [2], but currently there is no way to load CSV data in SparkR (1.4.0) besides separating our headers from the data, loading into an RDD, splitting by our delimiter, and then converting to a SparkR Data Frame with a vector of the columns gathered from the header.

Regards,
Alek Eskilson

[1] -- https://github.com/databricks/spark-csv
[2] -- http://www.inside-r.org/r-doc/utils/read.table

CONFIDENTIALITY NOTICE This message and any included attachments are from Cerner Corporation and are intended only for the addressee. The information contained in this message is confidential and may constitute inside or non-public information under international, federal, or state securities laws. Unauthorized forwarding, printing, copying, distribution, or use of such information is strictly prohibited and may be unlawful. If you are not the addressee, please promptly delete this message and notify the sender of the delivery error by e-mail or you may call Cerner's corporate offices in Kansas City, Missouri, U.S.A at (+1) (816)221-1024.
"
Burak Yavuz <brkyvz@gmail.com>,"Tue, 2 Jun 2015 12:03:11 -0700",Re: CSV Support in SparkR,"""Eskilson,Aleksander"" <Alek.Eskilson@cerner.com>","Hi,

cc'ing Shivaram here, because he worked on this yesterday.

If I'm not mistaken, you can use the following workflow:
```./bin/sparkR --packages com.databricks:spark-csv_2.10:1.0.3```

and then

```df <- read.df(sqlContext, ""/data"", ""csv"", header = ""true"")```

Best,
Burak


API [1]
y to load
m
on
ot
of
n
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Tue, 2 Jun 2015 12:08:36 -0700",Re: CSV Support in SparkR,Burak Yavuz <brkyvz@gmail.com>,"Hi Alek

As Burak said, you can already use the spark-csv with SparkR in the 1.4
release. So right now I use it with something like this

# Launch SparkR
./bin/sparkR --packages com.databricks:spark-csv_2.10:1.0.3
df <- read.df(sqlContext, ""./nycflights13.csv"", ""com.databricks.spark.csv"",
header=""true"")

You can also pass in other options to the spark csv as arguments to
`read.df`. Let us know if this works

Thanks
Shivaram



csv API [1]
ay to load
a
n,
f
y
"
zsampson <zsampson@palantir.com>,"Tue, 2 Jun 2015 12:34:55 -0700 (MST)",DataFrame.withColumn very slow when used iteratively?,dev@spark.apache.org,"Hey,

I'm seeing extreme slowness in withColumn when it's used in a loop. I'm
running this code:

for (int i = 0; i < NUM_ITERATIONS ++i) {
â€‚â€‚â€‚â€‚df = df.withColumn(""col""+i, new Column(new Literal(i,
DataTypes.IntegerType)));
}

where df is initially a trivial dataframe. Here are the results of running
with different values of NUM_ITERATIONS:

iterations	time
25	3s
50	11s
75	31s
100	76s
125	159s
150	283s

When I update the DataFrame by manually copying/appending to the column
array and using DataFrame.select, it runs in about half the time, but this
is still untenable at any significant number of iterations.

Any insight?



--
3.nabble.com/DataFrame-withColumn-very-slow-when-used-iteratively-tp12562.html
om.

---------------------------------------------------------------------


"
John Carrino <john.carrino@gmail.com>,"Tue, 2 Jun 2015 16:57:01 -0700",Re: Possible space improvements to shuffle,Josh Rosen <rosenville@gmail.com>,"Yes, I think that bug is what I want.  Thank you.

So I guess the current reason is that we don't want to buffer up numMapper
incoming streams. So we just iterate through each and transfer it over in
full because that is more network efficient?

I'm not sure I understand why you wouldn't want to sort on the composite
(parition_id, hash).  I think using the partitionKeyComparator should be
ok, because the other case of merging with spilled files uses it and that
works out ok.

The aggregation I am doing basically has as many output rows as input rows
so I am seeing a lot of memory pressure on the reduce side but it doesn't
have the same ability to spill like map does.

-jc




"
Josh Rosen <rosenville@gmail.com>,"Tue, 2 Jun 2015 14:08:10 -0700",Re: Possible space improvements to shuffle,John Carrino <john.carrino@gmail.com>,"The relevant JIRA that springs to mind is
https://issues.apache.org/jira/browse/SPARK-2926

If an aggregator and ordering are both defined, then the map side of
sort-based shuffle will sort based on the key ordering so that map-side
spills can be efficiently merged.  We do not currently do a sort-based
merge on the reduce side; implementing this is a little tricky because it
will require more map partitions' output to be buffered on the reduce
side.  I think that SPARK-2926 has some proposals of how to deal with this,
including hierarchical merging of reduce outputs.

RE: ExternalSorter#partitionedIterator, I don't think it's safe to do
!ordering.isDefined
&& !aggregator.isDefined.  If an aggregator is defined but we don't have an
ordering, then I don't think it makes sense to sort the keys based on their
hashcodes or some default ordering, since hashcode collisions would lead to
incorrect results for sort-based aggregation.


"
Matt Cheah <mcheah@palantir.com>,"Tue, 2 Jun 2015 22:57:21 +0000",Re: [SQL] Write parquet files under partition directories?,Reynold Xin <rxin@databricks.com>,"Excellent! Where can I find the code, pull request, and Spark ticket where
this was introduced?

Thanks,

-Matt Cheah

From:  Reynold Xin <rxin@databricks.com>
Date:  Monday, June 1, 2015 at 10:25 PM
To:  Matt Cheah <mcheah@palantir.com>
Cc:  ""dev@spark.apache.org"" <dev@spark.apache.org>, Mingyu Kim
<mkim@palantir.com>, Andrew Ash <aash@palantir.com>
Subject:  Re: [SQL] Write parquet files under partition directories?

There will be in 1.4.

df.write.partitionBy(""year"", ""month"", ""day"").parquet(""/path/to/output"")




"
Reynold Xin <rxin@databricks.com>,"Tue, 2 Jun 2015 15:58:47 -0700",Re: [SQL] Write parquet files under partition directories?,Matt Cheah <mcheah@palantir.com>,"Almost all dataframe stuff are tracked by this umbrella ticket:
https://issues.apache.org/jira/browse/SPARK-6116

For the reader/writer interface, it's here:

https://issues.apache.org/jira/browse/SPARK-7654

https://github.com/apache/spark/pull/6175


e
_docs_latest_sql-2Dprogramming-2Dguide.html&d=BQMFaQ&c=izlc9mHr637UR4lp&m=_7T9n01KFlQS8djMTP3ylblUaOYNr68mj286s8zIdQ8&s=VQxAw6mG9yopDs37lNi7H_CnYiFQumqDAn9A8881Xyc&e=>,
example).
h
"
"""Eskilson,Aleksander"" <Alek.Eskilson@Cerner.com>","Tue, 2 Jun 2015 19:56:09 +0000",Re: CSV Support in SparkR,"""shivaram@eecs.berkeley.edu"" <shivaram@eecs.berkeley.edu>,
        Burak Yavuz
	<brkyvz@gmail.com>","Hey, that’s pretty convenient. Unfortunately, although the package seems to pull fine into the session, I’m getting class not found exceptions with:

Caused by: org.apache.spark.SparkExcetion: Job aborted due to stage failure: Task 0 in stage 6.0 failed 4 times, most recent failure: Lost task 0.3 in stage 6.0: java.lang.ClassNotFoundException: com.databricks.spark.csv.CsvRelation$anonfun$buildScan$1

Which smells like a path issue to me, and I made sure the ivy repo was part of my PATH, but functions like showDF() still fail with that error. Did I miss a setting, or should the package inclusion in the sparkR execution load that in?

I’ve run
df <- read.df(sqlCtx, “./data.csv”, “com.databricks.spark.csv”, header=“true”, delimiter=“|”)
showDF(df, 10)

(my data is pipeline delimited, and the default SQL context is sqlCtx)

Thanks,
Alek

From: Shivaram Venkataraman <shivaram@eecs.berkeley.edu<mailto:shivaram@eecs.berkeley.edu>>
Reply-To: ""shivaram@eecs.berkeley.edu<mailto:shivaram@eecs.berkeley.edu>"" <shivaram@eecs.berkeley.edu<mailto:shivaram@eecs.berkeley.edu>>
Date: Tuesday, June 2, 2015 at 2:08 PM
To: Burak Yavuz <brkyvz@gmail.com<mailto:brkyvz@gmail.com>>
Cc: Aleksander Eskilson <Alek.Eskilson@cerner.com<mailto:Alek.Eskilson@cerner.com>>, ""dev@spark.apache.org<mailto:dev@spark.apache.org>"" <dev@spark.apache.org<mailto:dev@spark.apache.org>>, Shivaram Venkataraman <shivaram@eecs.berkeley.edu<mailto:shivaram@eecs.berkeley.edu>>
Subject: Re: CSV Support in SparkR

Hi Alek

As Burak said, you can already use the spark-csv with SparkR in the 1.4 release. So right now I use it with something like this

# Launch SparkR
./bin/sparkR --packages com.databricks:spark-csv_2.10:1.0.3
df <- read.df(sqlContext, ""./nycflights13.csv"", ""com.databricks.spark.csv"", header=""true"")

You can also pass in other options to the spark csv as arguments to `read.df`. Let us know if this works

Thanks
Shivaram


Hi,

cc'ing Shivaram here, because he worked on this yesterday.

If I'm not mistaken, you can use the following workflow:
```./bin/sparkR --packages com.databricks:spark-csv_2.10:1.0.3```

and then

```df <- read.df(sqlContext, ""/data"", ""csv"", header = ""true"")```

Best,
Burak

Are there any intentions to provide first class support for CSV files as one of the loadable file types in SparkR? Data brick’s spark-csv API [1] has support for SQL, Python, and Java/Scala, and implements most of the arguments of R’s read.table API [2], but currently there is no way to load CSV data in SparkR (1.4.0) besides separating our headers from the data, loading into an RDD, splitting by our delimiter, and then converting to a SparkR Data Frame with a vector of the columns gathered from the header.

Regards,
Alek Eskilson

[1] -- https://github.com/databricks/spark-csv<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_databricks_spark-2Dcsv&d=AwMFaQ&c=NRtzTzKNaCCmhN_9N2YJR-XrNU1huIgYP99yDsEzaJo&r=0vZw1rBdgaYvDJYLyKglbrax9kvQfRPdzxLUyWSyxPM&m=mPtlFYdyx5Rp7pZr-bQ15QMIrq4qE26ECfJCzoMwYhI&s=wT5PU54lVmR2R_o3GidPhDQD9kMMNVYotZEqCd4ASm4&e=>
[2] -- http://www.inside-r.org/r-doc/utils/read.table<https://urldefense.proofpoint.com/v2/url?u=http-3A__www.inside-2Dr.org_r-2Ddoc_utils_read.table&d=AwMFaQ&c=NRtzTzKNaCCmhN_9N2YJR-XrNU1huIgYP99yDsEzaJo&r=0vZw1rBdgaYvDJYLyKglbrax9kvQfRPdzxLUyWSyxPM&m=mPtlFYdyx5Rp7pZr-bQ15QMIrq4qE26ECfJCzoMwYhI&s=h87nnmV5D3soOFo5wasj1J34zbhvukHd1WcSitsjB6s&e=>
CONFIDENTIALITY NOTICE This message and any included attachments are from Cerner Corporation and are intended only for the addressee. The information contained in this message is confidential and may constitute inside or non-public information under international, federal, or state securities laws. Unauthorized forwarding, printing, copying, distribution, or use of such information is strictly prohibited and may be unlawful. If you are not the addressee, please promptly delete this message and notify the sender of the delivery error by e-mail or you may call Cerner's corporate offices in Kansas City, Missouri, U.S.A at (+1) (816)221-1024<tel:%28%2B1%29%20%28816%29221-1024>.


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Tue, 2 Jun 2015 12:57:39 -0700",Re: CSV Support in SparkR,"""Eskilson,Aleksander"" <Alek.Eskilson@cerner.com>","There was a bug in the SparkContext creation that I fixed yesterday.
https://github.com/apache/spark/commit/6b44278ef7cd2a278dfa67e8393ef30775c72726


If you build from master it should be fixed. Also I think we might have a
rc4 which should have this

Thanks
Shivaram


ge seems
ions with:
sk
ks.spark.csvâ€,
""
-csv API [1]
way to load
,
 a
.
bricks_spark-2Dcsv&d=AwMFaQ&c=NRtzTzKNaCCmhN_9N2YJR-XrNU1huIgYP99yDsEzaJo&r=0vZw1rBdgaYvDJYLyKglbrax9kvQfRPdzxLUyWSyxPM&m=mPtlFYdyx5Rp7pZr-bQ15QMIrq4qE26ECfJCzoMwYhI&s=wT5PU54lVmR2R_o3GidPhDQD9kMMNVYotZEqCd4ASm4&e=>
rg_r-2Ddoc_utils_read.table&d=AwMFaQ&c=NRtzTzKNaCCmhN_9N2YJR-XrNU1huIgYP99yDsEzaJo&r=0vZw1rBdgaYvDJYLyKglbrax9kvQfRPdzxLUyWSyxPM&m=mPtlFYdyx5Rp7pZr-bQ15QMIrq4qE26ECfJCzoMwYhI&s=h87nnmV5D3soOFo5wasj1J34zbhvukHd1WcSitsjB6s&e=>
e
on,
If
fy
.
"
"""Eskilson,Aleksander"" <Alek.Eskilson@Cerner.com>","Tue, 2 Jun 2015 19:59:17 +0000",Re: CSV Support in SparkR,"""shivaram@eecs.berkeley.edu"" <shivaram@eecs.berkeley.edu>","Ah, alright, cool. I’ll rebuild and let you know.

Thanks again,
Alek

From: Shivaram Venkataraman <shivaram@eecs.berkeley.edu<mailto:shivaram@eecs.berkeley.edu>>
Reply-To: ""shivaram@eecs.berkeley.edu<mailto:shivaram@eecs.berkeley.edu>"" <shivaram@eecs.berkeley.edu<mailto:shivaram@eecs.berkeley.edu>>
Date: Tuesday, June 2, 2015 at 2:57 PM
To: Aleksander Eskilson <Alek.Eskilson@cerner.com<mailto:Alek.Eskilson@cerner.com>>
Cc: ""shivaram@eecs.berkeley.edu<mailto:shivaram@eecs.berkeley.edu>"" <shivaram@eecs.berkeley.edu<mailto:shivaram@eecs.berkeley.edu>>, Burak Yavuz <brkyvz@gmail.com<mailto:brkyvz@gmail.com>>, ""dev@spark.apache.org<mailto:dev@spark.apache.org>"" <dev@spark.apache.org<mailto:dev@spark.apache.org>>
Subject: Re: CSV Support in SparkR

There was a bug in the SparkContext creation that I fixed yesterday. https://github.com/apache/spark/commit/6b44278ef7cd2a278dfa67e8393ef30775c72726<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_apache_spark_commit_6b44278ef7cd2a278dfa67e8393ef30775c72726&d=AwMFaQ&c=NRtzTzKNaCCmhN_9N2YJR-XrNU1huIgYP99yDsEzaJo&r=0vZw1rBdgaYvDJYLyKglbrax9kvQfRPdzxLUyWSyxPM&m=kO95UBEkBrQwNCQwa2x0MOiUxhLQvBQ1B2q5EDG_bt4&s=UjoHyjJhx1vf6fqNiq3P-MqcvN2FnssT16FJ8o98pF4&e=>

If you build from master it should be fixed. Also I think we might have a rc4 which should have this

Thanks
Shivaram

Hey, that’s pretty convenient. Unfortunately, although the package seems to pull fine into the session, I’m getting class not found exceptions with:

Caused by: org.apache.spark.SparkExcetion: Job aborted due to stage failure: Task 0 in stage 6.0 failed 4 times, most recent failure: Lost task 0.3 in stage 6.0: java.lang.ClassNotFoundException: com.databricks.spark.csv.CsvRelation$anonfun$buildScan$1

Which smells like a path issue to me, and I made sure the ivy repo was part of my PATH, but functions like showDF() still fail with that error. Did I miss a setting, or should the package inclusion in the sparkR execution load that in?

I’ve run
df <- read.df(sqlCtx, “./data.csv”, “com.databricks.spark.csv”, header=“true”, delimiter=“|”)
showDF(df, 10)

(my data is pipeline delimited, and the default SQL context is sqlCtx)

Thanks,
Alek

From: Shivaram Venkataraman <shivaram@eecs.berkeley.edu<mailto:shivaram@eecs.berkeley.edu>>
Reply-To: ""shivaram@eecs.berkeley.edu<mailto:shivaram@eecs.berkeley.edu>"" <shivaram@eecs.berkeley.edu<mailto:shivaram@eecs.berkeley.edu>>
Date: Tuesday, June 2, 2015 at 2:08 PM
To: Burak Yavuz <brkyvz@gmail.com<mailto:brkyvz@gmail.com>>
Cc: Aleksander Eskilson <Alek.Eskilson@cerner.com<mailto:Alek.Eskilson@cerner.com>>, ""dev@spark.apache.org<mailto:dev@spark.apache.org>"" <dev@spark.apache.org<mailto:dev@spark.apache.org>>, Shivaram Venkataraman <shivaram@eecs.berkeley.edu<mailto:shivaram@eecs.berkeley.edu>>
Subject: Re: CSV Support in SparkR

Hi Alek

As Burak said, you can already use the spark-csv with SparkR in the 1.4 release. So right now I use it with something like this

# Launch SparkR
./bin/sparkR --packages com.databricks:spark-csv_2.10:1.0.3
df <- read.df(sqlContext, ""./nycflights13.csv"", ""com.databricks.spark.csv"", header=""true"")

You can also pass in other options to the spark csv as arguments to `read.df`. Let us know if this works

Thanks
Shivaram


Hi,

cc'ing Shivaram here, because he worked on this yesterday.

If I'm not mistaken, you can use the following workflow:
```./bin/sparkR --packages com.databricks:spark-csv_2.10:1.0.3```

and then

```df <- read.df(sqlContext, ""/data"", ""csv"", header = ""true"")```

Best,
Burak

Are there any intentions to provide first class support for CSV files as one of the loadable file types in SparkR? Data brick’s spark-csv API [1] has support for SQL, Python, and Java/Scala, and implements most of the arguments of R’s read.table API [2], but currently there is no way to load CSV data in SparkR (1.4.0) besides separating our headers from the data, loading into an RDD, splitting by our delimiter, and then converting to a SparkR Data Frame with a vector of the columns gathered from the header.

Regards,
Alek Eskilson

[1] -- https://github.com/databricks/spark-csv<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_databricks_spark-2Dcsv&d=AwMFaQ&c=NRtzTzKNaCCmhN_9N2YJR-XrNU1huIgYP99yDsEzaJo&r=0vZw1rBdgaYvDJYLyKglbrax9kvQfRPdzxLUyWSyxPM&m=mPtlFYdyx5Rp7pZr-bQ15QMIrq4qE26ECfJCzoMwYhI&s=wT5PU54lVmR2R_o3GidPhDQD9kMMNVYotZEqCd4ASm4&e=>
[2] -- http://www.inside-r.org/r-doc/utils/read.table<https://urldefense.proofpoint.com/v2/url?u=http-3A__www.inside-2Dr.org_r-2Ddoc_utils_read.table&d=AwMFaQ&c=NRtzTzKNaCCmhN_9N2YJR-XrNU1huIgYP99yDsEzaJo&r=0vZw1rBdgaYvDJYLyKglbrax9kvQfRPdzxLUyWSyxPM&m=mPtlFYdyx5Rp7pZr-bQ15QMIrq4qE26ECfJCzoMwYhI&s=h87nnmV5D3soOFo5wasj1J34zbhvukHd1WcSitsjB6s&e=>
CONFIDENTIALITY NOTICE This message and any included attachments are from Cerner Corporation and are intended only for the addressee. The information contained in this message is confidential and may constitute inside or non-public information under international, federal, or state securities laws. Unauthorized forwarding, printing, copying, distribution, or use of such information is strictly prohibited and may be unlawful. If you are not the addressee, please promptly delete this message and notify the sender of the delivery error by e-mail or you may call Cerner's corporate offices in Kansas City, Missouri, U.S.A at (+1) (816)221-1024<tel:%28%2B1%29%20%28816%29221-1024>.



"
Ignacio Zendejas <iz@node.io>,"Tue, 2 Jun 2015 15:13:40 -0700",createDataframe from s3 results in error,dev@spark.apache.org,"I've run into an error when trying to create a dataframe. Here's the code:

--
from pyspark import StorageLevel
from pyspark.sql import Row

table = 'blah'
ssc = HiveContext(sc)

data = sc.textFile('s3://bucket/some.tsv')

def deserialize(s):
  p = s.strip().split('\t')
  p[-1] = float(p[-1])
  return Row(normalized_page_sha1=p[0], name=p[1], phrase=p[2],
created_at=p[3], layer_id=p[4], score=p[5])

blah = data.map(deserialize)
df = sqlContext.inferSchema(blah)

---

I've also tried s3n and using createDataFrame. Our setup is on EMR
instances, using the setup script Amazon provides. After lots of debugging,
I suspect there'll be a problem with this setup.

What's weird is that if I run this on pyspark shell, and re-run the last
line (inferSchema/createDataFrame), it actually works.

We're getting warnings like this:
http://pastebin.ca/3016476

Here's the actual error:
http://www.pastebin.ca/3016473

Any help would be greatly appreciated.

Thanks,
Ignacio
"
Reynold Xin <rxin@databricks.com>,"Tue, 2 Jun 2015 15:22:13 -0700",Re: createDataframe from s3 results in error,Ignacio Zendejas <iz@node.io>,"What version of Spark is this?


"
"""Eskilson,Aleksander"" <Alek.Eskilson@Cerner.com>","Tue, 2 Jun 2015 21:20:49 +0000",Re: CSV Support in SparkR,"""Eskilson,Aleksander"" <Alek.Eskilson@Cerner.com>,
        ""shivaram@eecs.berkeley.edu"" <shivaram@eecs.berkeley.edu>","Seems to work great in the master build. It’s really good to have this functionality.

Regards,
Alek Eskilson

From: <Eskilson>, Aleksander Eskilson <Alek.Eskilson@cerner.com<mailto:Alek.Eskilson@cerner.com>>
Date: Tuesday, June 2, 2015 at 2:59 PM
To: ""shivaram@eecs.berkeley.edu<mailto:shivaram@eecs.berkeley.edu>"" <shivaram@eecs.berkeley.edu<mailto:shivaram@eecs.berkeley.edu>>
Cc: Burak Yavuz <brkyvz@gmail.com<mailto:brkyvz@gmail.com>>, ""dev@spark.apache.org<mailto:dev@spark.apache.org>"" <dev@spark.apache.org<mailto:dev@spark.apache.org>>
Subject: Re: CSV Support in SparkR

Ah, alright, cool. I’ll rebuild and let you know.

Thanks again,
Alek

From: Shivaram Venkataraman <shivaram@eecs.berkeley.edu<mailto:shivaram@eecs.berkeley.edu>>
Reply-To: ""shivaram@eecs.berkeley.edu<mailto:shivaram@eecs.berkeley.edu>"" <shivaram@eecs.berkeley.edu<mailto:shivaram@eecs.berkeley.edu>>
Date: Tuesday, June 2, 2015 at 2:57 PM
To: Aleksander Eskilson <Alek.Eskilson@cerner.com<mailto:Alek.Eskilson@cerner.com>>
Cc: ""shivaram@eecs.berkeley.edu<mailto:shivaram@eecs.berkeley.edu>"" <shivaram@eecs.berkeley.edu<mailto:shivaram@eecs.berkeley.edu>>, Burak Yavuz <brkyvz@gmail.com<mailto:brkyvz@gmail.com>>, ""dev@spark.apache.org<mailto:dev@spark.apache.org>"" <dev@spark.apache.org<mailto:dev@spark.apache.org>>
Subject: Re: CSV Support in SparkR

There was a bug in the SparkContext creation that I fixed yesterday. https://github.com/apache/spark/commit/6b44278ef7cd2a278dfa67e8393ef30775c72726<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_apache_spark_commit_6b44278ef7cd2a278dfa67e8393ef30775c72726&d=AwMFaQ&c=NRtzTzKNaCCmhN_9N2YJR-XrNU1huIgYP99yDsEzaJo&r=0vZw1rBdgaYvDJYLyKglbrax9kvQfRPdzxLUyWSyxPM&m=kO95UBEkBrQwNCQwa2x0MOiUxhLQvBQ1B2q5EDG_bt4&s=UjoHyjJhx1vf6fqNiq3P-MqcvN2FnssT16FJ8o98pF4&e=>

If you build from master it should be fixed. Also I think we might have a rc4 which should have this

Thanks
Shivaram

Hey, that’s pretty convenient. Unfortunately, although the package seems to pull fine into the session, I’m getting class not found exceptions with:

Caused by: org.apache.spark.SparkExcetion: Job aborted due to stage failure: Task 0 in stage 6.0 failed 4 times, most recent failure: Lost task 0.3 in stage 6.0: java.lang.ClassNotFoundException: com.databricks.spark.csv.CsvRelation$anonfun$buildScan$1

Which smells like a path issue to me, and I made sure the ivy repo was part of my PATH, but functions like showDF() still fail with that error. Did I miss a setting, or should the package inclusion in the sparkR execution load that in?

I’ve run
df <- read.df(sqlCtx, “./data.csv”, “com.databricks.spark.csv”, header=“true”, delimiter=“|”)
showDF(df, 10)

(my data is pipeline delimited, and the default SQL context is sqlCtx)

Thanks,
Alek

From: Shivaram Venkataraman <shivaram@eecs.berkeley.edu<mailto:shivaram@eecs.berkeley.edu>>
Reply-To: ""shivaram@eecs.berkeley.edu<mailto:shivaram@eecs.berkeley.edu>"" <shivaram@eecs.berkeley.edu<mailto:shivaram@eecs.berkeley.edu>>
Date: Tuesday, June 2, 2015 at 2:08 PM
To: Burak Yavuz <brkyvz@gmail.com<mailto:brkyvz@gmail.com>>
Cc: Aleksander Eskilson <Alek.Eskilson@cerner.com<mailto:Alek.Eskilson@cerner.com>>, ""dev@spark.apache.org<mailto:dev@spark.apache.org>"" <dev@spark.apache.org<mailto:dev@spark.apache.org>>, Shivaram Venkataraman <shivaram@eecs.berkeley.edu<mailto:shivaram@eecs.berkeley.edu>>
Subject: Re: CSV Support in SparkR

Hi Alek

As Burak said, you can already use the spark-csv with SparkR in the 1.4 release. So right now I use it with something like this

# Launch SparkR
./bin/sparkR --packages com.databricks:spark-csv_2.10:1.0.3
df <- read.df(sqlContext, ""./nycflights13.csv"", ""com.databricks.spark.csv"", header=""true"")

You can also pass in other options to the spark csv as arguments to `read.df`. Let us know if this works

Thanks
Shivaram


Hi,

cc'ing Shivaram here, because he worked on this yesterday.

If I'm not mistaken, you can use the following workflow:
```./bin/sparkR --packages com.databricks:spark-csv_2.10:1.0.3```

and then

```df <- read.df(sqlContext, ""/data"", ""csv"", header = ""true"")```

Best,
Burak

Are there any intentions to provide first class support for CSV files as one of the loadable file types in SparkR? Data brick’s spark-csv API [1] has support for SQL, Python, and Java/Scala, and implements most of the arguments of R’s read.table API [2], but currently there is no way to load CSV data in SparkR (1.4.0) besides separating our headers from the data, loading into an RDD, splitting by our delimiter, and then converting to a SparkR Data Frame with a vector of the columns gathered from the header.

Regards,
Alek Eskilson

[1] -- https://github.com/databricks/spark-csv<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_databricks_spark-2Dcsv&d=AwMFaQ&c=NRtzTzKNaCCmhN_9N2YJR-XrNU1huIgYP99yDsEzaJo&r=0vZw1rBdgaYvDJYLyKglbrax9kvQfRPdzxLUyWSyxPM&m=mPtlFYdyx5Rp7pZr-bQ15QMIrq4qE26ECfJCzoMwYhI&s=wT5PU54lVmR2R_o3GidPhDQD9kMMNVYotZEqCd4ASm4&e=>
[2] -- http://www.inside-r.org/r-doc/utils/read.table<https://urldefense.proofpoint.com/v2/url?u=http-3A__www.inside-2Dr.org_r-2Ddoc_utils_read.table&d=AwMFaQ&c=NRtzTzKNaCCmhN_9N2YJR-XrNU1huIgYP99yDsEzaJo&r=0vZw1rBdgaYvDJYLyKglbrax9kvQfRPdzxLUyWSyxPM&m=mPtlFYdyx5Rp7pZr-bQ15QMIrq4qE26ECfJCzoMwYhI&s=h87nnmV5D3soOFo5wasj1J34zbhvukHd1WcSitsjB6s&e=>
CONFIDENTIALITY NOTICE This message and any included attachments are from Cerner Corporation and are intended only for the addressee. The information contained in this message is confidential and may constitute inside or non-public information under international, federal, or state securities laws. Unauthorized forwarding, printing, copying, distribution, or use of such information is strictly prohibited and may be unlawful. If you are not the addressee, please promptly delete this message and notify the sender of the delivery error by e-mail or you may call Cerner's corporate offices in Kansas City, Missouri, U.S.A at (+1) (816)221-1024<tel:%28%2B1%29%20%28816%29221-1024>.



"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Tue, 2 Jun 2015 14:23:21 -0700",Re: CSV Support in SparkR,"""Eskilson,Aleksander"" <Alek.Eskilson@cerner.com>","Thanks for testing. We should probably include a section for this in the
SparkR programming guide given how popular CSV files are in R. Feel free to
open a PR for that if you get a chance.

Shivaram


e this
c72726
_spark_commit_6b44278ef7cd2a278dfa67e8393ef30775c72726&d=AwMFaQ&c=NRtzTzKNaCCmhN_9N2YJR-XrNU1huIgYP99yDsEzaJo&r=0vZw1rBdgaYvDJYLyKglbrax9kvQfRPdzxLUyWSyxPM&m=kO95UBEkBrQwNCQwa2x0MOiUxhLQvBQ1B2q5EDG_bt4&s=UjoHyjJhx1vf6fqNiq3P-MqcvN2FnssT16FJ8o98pF4&e=>
age
 exceptions
ask
cks.spark.csvâ€,
g""
u
k-csv API [1]
e
 way to load
a,
o a
r.
abricks_spark-2Dcsv&d=AwMFaQ&c=NRtzTzKNaCCmhN_9N2YJR-XrNU1huIgYP99yDsEzaJo&r=0vZw1rBdgaYvDJYLyKglbrax9kvQfRPdzxLUyWSyxPM&m=mPtlFYdyx5Rp7pZr-bQ15QMIrq4qE26ECfJCzoMwYhI&s=wT5PU54lVmR2R_o3GidPhDQD9kMMNVYotZEqCd4ASm4&e=>
org_r-2Ddoc_utils_read.table&d=AwMFaQ&c=NRtzTzKNaCCmhN_9N2YJR-XrNU1huIgYP99yDsEzaJo&r=0vZw1rBdgaYvDJYLyKglbrax9kvQfRPdzxLUyWSyxPM&m=mPtlFYdyx5Rp7pZr-bQ15QMIrq4qE26ECfJCzoMwYhI&s=h87nnmV5D3soOFo5wasj1J34zbhvukHd1WcSitsjB6s&e=>
te
e
ion,
 If
ify
4
"
Ignacio Zendejas <iz@node.io>,"Tue, 2 Jun 2015 15:25:21 -0700",Re: createDataframe from s3 results in error,dev@spark.apache.org,"
""Spark 1.3.1 built for Hadoop 2.4.0

Build flags: -Pyarn -Phadoop-2.4 -Dhadoop.version=2.4.0 -DskipTests
-Pkinesis-asl -Pspark-ganglia-lgpl -Phadoop-provided -Phive
-Phive-thriftserver

""
And this stacktrace may be more useful:
http://pastebin.ca/3016483


"
Reynold Xin <rxin@databricks.com>,"Tue, 2 Jun 2015 15:26:28 -0700",Re: createDataframe from s3 results in error,Ignacio Zendejas <iz@node.io>,"Maybe an incompatible Hive package or Hive metastore?


"
Andrew Ash <andrew@andrewash.com>,"Tue, 2 Jun 2015 20:32:32 -0400",Re: DataFrame.withColumn very slow when used iteratively?,Reynold Xin <rxin@databricks.com>,"Would it be valuable to create a .withColumns([colName], [ColumnObject])
method that adds in bulk rather than iteratively?

Alternatively effort might be better spent in making .withColumn() singular
faster.


"
John Carrino <john.carrino@gmail.com>,"Tue, 2 Jun 2015 13:50:47 -0700",Possible space improvements to shuffle,"dev@spark.apache.org, mkim@palantir.com","defined, it does the sort using only the partition_id, instead of
(parition_id, hash).  This means that on the reduce side you need to pull
the entire dataset into memory before you can begin iterating over the
results.

I figure since we are doing a sort of the data anyway it doesn't seem more
expensive to sort by (parition, hash).  That way the reducer can do a merge
and only has the hold in memory the data for a single int hashCode before
it can combine then and start returning results form the iterator.

Has this already been discussed?  If so, can someone point me in the right
direction to find out more?

Thanks for any help!
-jc

p.s. I am using spark version 1.3.1.  The code I am looking at below is
from ExternalSorter#partitionedIterator.  I think maybe
!ordering.isDefined should also include ""&& !aggregator.isDefined""

   if (spills.isEmpty && partitionWriters == null) {
      // Special case: if we have only in-memory data, we don't need to
merge streams, and perhaps
      // we don't even need to sort by anything other than partition ID
      if (!ordering.isDefined) {
        // The user hasn't requested sorted keys, so only sort by partition
ID, not key

groupByPartition(collection.destructiveSortedIterator(partitionComparator))
      } else {
        // We do need to sort by both partition ID and key

groupByPartition(collection.destructiveSortedIterator(partitionKeyComparator))
      }
"
Reynold Xin <rxin@databricks.com>,"Tue, 2 Jun 2015 12:46:21 -0700",Re: DataFrame.withColumn very slow when used iteratively?,zsampson <zsampson@palantir.com>,"We improved this in 1.4. Adding 100 columns took 4s on my laptop.
https://issues.apache.org/jira/browse/SPARK-7276

Still not the fastest, but much faster.

scala> Seq((1, 2)).toDF(""a"", ""b"")
res6: org.apache.spark.sql.DataFrame = [a: int, b: int]

scala>

scala> val start = System.nanoTime
start: Long = 1433274299441224000

scala> for (i <- 1 to 100) {
     |   df = df.withColumn(""n"" + i, org.apache.spark.sql.functions.lit(0))
     | }

scala> val end = System.nanoTime
end: Long = 1433274303250091000

scala>

scala> println((end - start) / 1000 / 1000 / 1000)
3



"
Reynold Xin <rxin@databricks.com>,"Tue, 2 Jun 2015 19:39:55 -0700",Re: Unit tests can generate spurious shutdown messages,Mick Davies <michael.belldavies@gmail.com>,"Can you submit a pull request for it? Thanks.



"
Reynold Xin <rxin@databricks.com>,"Tue, 2 Jun 2015 19:43:15 -0700",Re: DataFrame.withColumn very slow when used iteratively?,Andrew Ash <andrew@andrewash.com>,".select itself is the bulk add right?


"
Patrick Wendell <pwendell@gmail.com>,"Tue, 2 Jun 2015 20:51:39 -0700",[RESULT] [VOTE] Release Apache Spark 1.4.0 (RC3),"""dev@spark.apache.org"" <dev@spark.apache.org>","This vote is cancelled in favor of RC4.

Thanks everyone for the thorough testing of this RC. We are really
close, but there were a few blockers found. I've cut a new RC to
incorporate those issues.

The following patches were merged during the RC3 testing period:

(blockers)
4940630 [SPARK-8020] [SQL] Spark SQL conf in spark-defaults.conf make
metadataHive get constructed too early
6b0f615 [SPARK-8038] [SQL] [PYSPARK] fix Column.when() and otherwise()
78a6723 [SPARK-7978] [SQL] [PYSPARK] DecimalType should not be singleton

(other fixes)
9d6475b [SPARK-6917] [SQL] DecimalType is not read back when
non-native type exists
cbaf595 [SPARK-8014] [SQL] Avoid premature metadata discovery when
writing a HadoopFsRelation with a save mode other than Append
fa292dc [SPARK-8015] [FLUME] Remove Guava dependency from flume-sink.
f71a09d [SPARK-8037] [SQL] Ignores files whose name starts with dot in
HadoopFsRelation
292ee1a [SPARK-8021] [SQL] [PYSPARK] make Python read/write API
consistent with Scala
87941ff [SPARK-8023][SQL] Add ""deterministic"" attribute to Expression
to avoid collapsing nondeterministic projects.
e6d5895 [SPARK-7965] [SPARK-7972] [SQL] Handle expressions containing
multiple window expressions and make parser match window frames in
case insensitive way
8ac2376 [SPARK-8026][SQL] Add Column.alias to Scala/Java DataFrame API
efc0e05 [SPARK-7982][SQL] DataFrame.stat.crosstab should use 0 instead
of null for pairs that don't appear
cbfb682a [SPARK-8028] [SPARKR] Use addJar instead of setJars in SparkR
a7c8b00 [SPARK-7958] [STREAMING] Handled exception in
StreamingContext.start() to prevent leaking of actors
a76c2e1 [SPARK-7899] [PYSPARK] Fix Python 3 pyspark/sql/types module conflict
f1d4e7e [SPARK-7227] [SPARKR] Support fillna / dropna in R DataFrame.
01f38f7 [SPARK-7979] Enforce structural type checker.
2c45009 [SPARK-7459] [MLLIB] ElementwiseProduct Java example
8938a74 [SPARK-7962] [MESOS] Fix master url parsing in rest submission client.
1513cff [SPARK-7957] Preserve partitioning when using randomSplit
9a88be1 [SPARK-6013] [ML] Add more Python ML examples for spark.ml
2bd4460 [SPARK-7954] [SPARKR] Create SparkContext in sparkRSQL init


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Tue, 2 Jun 2015 20:53:32 -0700",[VOTE] Release Apache Spark 1.4.0 (RC4),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version 1.4.0!

The tag to be voted on is v1.4.0-rc3 (commit 22596c5):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=
22596c534a38cfdda91aef18aa9037ab101e4251

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.4.0-rc4-bin/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
[published as version: 1.4.0]
https://repository.apache.org/content/repositories/orgapachespark-1111/
[published as version: 1.4.0-rc4]
https://repository.apache.org/content/repositories/orgapachespark-1112/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.4.0-rc4-docs/

Please vote on releasing this package as Apache Spark 1.4.0!

The vote is open until Saturday, June 06, at 05:00 UTC and passes
if a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.4.0
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

== What has changed since RC3 ==
In addition to may smaller fixes, three blocker issues were fixed:
4940630 [SPARK-8020] [SQL] Spark SQL conf in spark-defaults.conf make
metadataHive get constructed too early
6b0f615 [SPARK-8038] [SQL] [PYSPARK] fix Column.when() and otherwise()
78a6723 [SPARK-7978] [SQL] [PYSPARK] DecimalType should not be singleton

== How can I help test this release? ==
If you are a Spark user, you can help us test this release by
taking a Spark 1.3 workload and running on this release candidate,
then reporting any regressions.

== What justifies a -1 vote for this release? ==
This vote is happening towards the end of the 1.4 QA period,
so -1 votes should only occur for significant regressions from 1.3.1.
Bugs already present in 1.3.X, minor regressions, or bugs related
to new features will not block this release.

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Tue, 2 Jun 2015 22:47:06 -0700",Re: [VOTE] Release Apache Spark 1.4.0 (RC4),"""dev@spark.apache.org"" <dev@spark.apache.org>","He all - a tiny nit from the last e-mail. The tag is v1.4.0-rc4. The
exact commit and all other information is correct. (thanks Shivaram
who pointed this out).


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Tue, 2 Jun 2015 23:58:33 -0700",Re: GraphX: New graph operator,Tarek Auel <tarek.auel@gmail.com>,"Hi Tarek,

I took a quick look at the materials you shared. It actually seems to me
it'd be super easy to express a graph as two DataFrames: one for edges
(srcid, dstid, and other edge attributes) and one for vertices (vid, and
other vertex attributes).

Then

intersection is just

edges1.intersect(edges2)


""join"" is just

edges1.union(edges2).distinct





"
Tarek Auel <tarek.auel@gmail.com>,"Wed, 03 Jun 2015 07:13:47 +0000",Re: GraphX: New graph operator,Reynold Xin <rxin@databricks.com>,"Hi,

The graph is already there (GraphX) and has the two RDDs you described. My
question tries to get an idea, if the community thinks that it's a benefit
and would be a plus or not. If yes, I would like to contribute it to GraphX
(either as part of GraphOpts or as external library).

An interesting question is for me, if these operators take the attributes
into account (and ignore the ids) or if the id has to match too. I believe
that ignoring the id and focus on the attributes is harder but more
powerful. If I think of a graph with a node for the country US for instance
and I want to merge a second graph, which contains a U.S. node too, I would
expect that these two nodes will be merged (ignoring the id). Is this
thought valid, or does it more sense to merge based on the id?

Thanks for your inputs !

Tarek

"
Reynold Xin <rxin@databricks.com>,"Wed, 3 Jun 2015 00:14:59 -0700",Re: GraphX: New graph operator,Tarek Auel <tarek.auel@gmail.com>,"I'd think id is the unique identifier by default.



"
Devl Devel <devl.development@gmail.com>,"Wed, 3 Jun 2015 10:32:34 +0100",Stop Master and Slaves without SSH,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey All,

start-slaves.sh and stop-slaves.sh make use of SSH to connect to remote
clusters. Are there alternative methods to do this without SSH?

For example using:

./bin/spark-class org.apache.spark.deploy.worker.Worker spark://IP:PORT

is fine but there is no way to kill the Worker without using
stop-slave(s).sh or using ps -ef and then kill.

Are there alternatives available such as Hadoop's: hadoop-daemon.sh
start|stop xyz?

I noticed spark-daemon.sh exists but maybe we need to increase the
documentation around it, for instance:

 Usage: spark-daemon.sh [--config <conf-dir>] (start|stop|status)
<spark-command> <spark-instance-number> <args>

what are the valid spark-commands? Can this be used to start and stop
workers on the current node?

Many thanks
Devl
"
Lorenz Fischer <lorenz.fischer@gmail.com>,"Wed, 03 Jun 2015 13:43:13 +0000",MLlib: Anybody working on hierarchical topic models like HLDA?,dev@spark.apache.org,"Hi All

I'm working on a project in which I use the current LDA implementation that
has been contributed by Databricks' Joseph Bradley et al. for the recent
1.3.0 release (thanks guys!). While this is great, my project requires
several levels of topics, as I would like to offer users to drill down into
subtopics.

As I understand it, Hierarchical Latent Dirichlet Allocation (HLDA) would
offer such a hierarchy. Looking at the papers and talks by Blei [1,2] and
Jordan [3], I think I should be able to implement HLDA in Spark using the
Nested Chinese Restaurant Process (NCRP). However, as I have some time
constraints, I'm not sure if I will have the time to do it 'the proper way'.

In any case, I wanted to quickly ask around if anybody is already working
on this or on some other form of a hierarchical topic model. Maybe I could
contribute to these efforts instead of starting from scratch.

Best,
Lorenz

[1] http://www.cs.princeton.edu/~blei/papers/BleiGriffithsJordan2009.pdf
[2]
http://papers.nips.cc/paper/2466-hierarchical-topic-models-and-the-nested-chinese-restaurant-process.pdf
[3] https://www.youtube.com/watch?v=PxgW3lOrj60
"
"""Eskilson,Aleksander"" <Alek.Eskilson@Cerner.com>","Wed, 3 Jun 2015 14:51:50 +0000",SparkR DataFrame Column Casts esp. from CSV Files,"""dev@spark.apache.org"" <dev@spark.apache.org>","It appears that casting columns remains a bit of a trick in Spark’s DataFrames. This is an issue because tools like spark-csv will set column types to String by default and will not attempt to infer types. Although spark-csv supports specifying  types for columns in its options, it’s not clear how that might be integrated into SparkR (when loading the spark-csv package into the R session).

Looking at the column.R spec we can cast a column to a different data type with the cast function [1], but it’s notable that this is not a mutator, and it returns a column object as opposed to a DataFrame. It appears the column cast can only be ‘applied’ by using the withColumn() or mutate() (an alias for withColumn).

The other way to cast with Spark DataFrames is to write UDFs that operate on a column value and return a coerced value. It looks like SparkR doesn’t have UDFs just yet [2], but it seems like they’d be necessary to do a natural one-off column cast in R, something like

df.col1toInt <- withColumn(df, “intCol1”, udf(df$col1, function(x) as.numeric(x)))

(where col1 was originally ‘character’ type)

Currently it seems one has to
df.col1cast <- cast(df$col1, “int”)
df.col1toInt <- withColumn(df, df.col1cast)

If we wanted just our casted columns and not the original column from the data frame, we’d still have to do a select. There was a conversation about CSV files just yesterday. Types are already problematic, but they’re a very common data source in R, even at scale.

But only being able to coerce one column at a time is really unwieldy. Can the current spark-csv SQL API for specifying types [3] be extended SparkR? And are there any thoughts on implementing some kind of type inferencing perhaps based on a sampling of some number of rows (an implementation I’ve seen before)? R’s read.csv() and read.delim() get types by inferring from the whole file. Getting something that can achieve that functionality via explicit definition of types or sampling will probably be necessary to work with CSV files that have enough columns to merit R at Spark’s scale.

Regards,
Alek Eskilson

[1] - https://github.com/apache/spark/blob/master/R/pkg/R/column.R#L190
[2] - https://issues.apache.org/jira/browse/SPARK-6817
[3] - https://github.com/databricks/spark-csv#sql-api

CONFIDENTIALITY NOTICE This message and any included attachments are from Cerner Corporation and are intended only for the addressee. The information contained in this message is confidential and may constitute inside or non-public information under international, federal, or state securities laws. Unauthorized forwarding, printing, copying, distribution, or use of such information is strictly prohibited and may be unlawful. If you are not the addressee, please promptly delete this message and notify the sender of the delivery error by e-mail or you may call Cerner's corporate offices in Kansas City, Missouri, U.S.A at (+1) (816)221-1024.
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Wed, 3 Jun 2015 10:29:12 -0700",Re: SparkR DataFrame Column Casts esp. from CSV Files,"""Eskilson,Aleksander"" <Alek.Eskilson@cerner.com>","cc Hossein who knows more about the spark-csv options

You are right that the default CSV reader options end up creating all
columns as string. I know that the JSON reader infers the schema [1] but I
don't know if the CSV reader has any options to do that.  Regarding the
SparkR syntax to cast columns, I think there is a simpler way to do it by
just assigning to the same column name. For example I have a flights
DataFrame with the `year` column typed as string. To cast it to int I just
use

flights$year <- cast(flights$year, ""int"")

Now the dataframe has the same number of columns as before and you don't
need a selection.

However this still doesn't address the part about casting multiple columns
-- Could you file a new JIRA to track the need for casting multiple columns
or rather being able to set the schema after loading a DF ?

Thanks
Shivaram

[1]
http://spark.apache.org/docs/latest/sql-programming-guide.html#json-datasets



™s
™s not
v
t a
e withColumn() or
R
be necessary to do
unction(x)
rsation
â€™re
.delim() get types
be
m
on
ot
of
n
"
"""Eskilson,Aleksander"" <Alek.Eskilson@Cerner.com>","Wed, 3 Jun 2015 17:49:42 +0000",Re: SparkR DataFrame Column Casts esp. from CSV Files,"""shivaram@eecs.berkeley.edu"" <shivaram@eecs.berkeley.edu>","Hi Shivaram,

As far as databricks’ spark-csv API shows, it seems there’s currently only support for explicit definition of column types. In JSON we have nice typed fields, but in CSVs, all bets are off. In the SQL version of the API, it appears you specify the column types when you create the table you’re populating with CSV data.

Thanks for the clarification on individual column casting, I was missing the more obvious syntax.

I’ll file a JIRA for resetting the schema after loading a DF.

Thanks,
Alek


From: Shivaram Venkataraman <shivaram@eecs.berkeley.edu<mailto:shivaram@eecs.berkeley.edu>>
Reply-To: ""shivaram@eecs.berkeley.edu<mailto:shivaram@eecs.berkeley.edu>"" <shivaram@eecs.berkeley.edu<mailto:shivaram@eecs.berkeley.edu>>
Date: Wednesday, June 3, 2015 at 12:29 PM
To: Aleksander Eskilson <Alek.Eskilson@cerner.com<mailto:Alek.Eskilson@cerner.com>>
Cc: ""dev@spark.apache.org<mailto:dev@spark.apache.org>"" <dev@spark.apache.org<mailto:dev@spark.apache.org>>, ""hossein@databricks.com<mailto:hossein@databricks.com>"" <hossein@databricks.com<mailto:hossein@databricks.com>>
Subject: Re: SparkR DataFrame Column Casts esp. from CSV Files

cc Hossein who knows more about the spark-csv options

You are right that the default CSV reader options end up creating all columns as string. I know that the JSON reader infers the schema [1] but I don't know if the CSV reader has any options to do that.  Regarding the SparkR syntax to cast columns, I think there is a simpler way to do it by just assigning to the same column name. For example I have a flights DataFrame with the `year` column typed as string. To cast it to int I just use

flights$year <- cast(flights$year, ""int"")

Now the dataframe has the same number of columns as before and you don't need a selection.

However this still doesn't address the part about casting multiple columns -- Could you file a new JIRA to track the need for casting multiple columns or rather being able to set the schema after loading a DF ?

Thanks
Shivaram

[1] http://spark.apache.org/docs/latest/sql-programming-guide.html#json-datasets<https://urldefense.proofpoint.com/v2/url?u=http-3A__spark.apache.org_docs_latest_sql-2Dprogramming-2Dguide.html-23json-2Ddatasets&d=AwMFaQ&c=NRtzTzKNaCCmhN_9N2YJR-XrNU1huIgYP99yDsEzaJo&r=0vZw1rBdgaYvDJYLyKglbrax9kvQfRPdzxLUyWSyxPM&m=aCZhOxAn5Iu762hWogwQK__JsZigsbLZFMaz44UcKQw&s=BX3MuobG748zhfm7hc_SnZA4MnFbwgFreNVEjkzkENc&e=>

It appears that casting columns remains a bit of a trick in Spark’s DataFrames. This is an issue because tools like spark-csv will set column types to String by default and will not attempt to infer types. Although spark-csv supports specifying  types for columns in its options, it’s not clear how that might be integrated into SparkR (when loading the spark-csv package into the R session).

Looking at the column.R spec we can cast a column to a different data type with the cast function [1], but it’s notable that this is not a mutator, and it returns a column object as opposed to a DataFrame. It appears the column cast can only be ‘applied’ by using the withColumn() or mutate() (an alias for withColumn).

The other way to cast with Spark DataFrames is to write UDFs that operate on a column value and return a coerced value. It looks like SparkR doesn’t have UDFs just yet [2], but it seems like they’d be necessary to do a natural one-off column cast in R, something like

df.col1toInt <- withColumn(df, “intCol1”, udf(df$col1, function(x) as.numeric(x)))

(where col1 was originally ‘character’ type)

Currently it seems one has to
df.col1cast <- cast(df$col1, “int”)
df.col1toInt <- withColumn(df, df.col1cast)

If we wanted just our casted columns and not the original column from the data frame, we’d still have to do a select. There was a conversation about CSV files just yesterday. Types are already problematic, but they’re a very common data source in R, even at scale.

But only being able to coerce one column at a time is really unwieldy. Can the current spark-csv SQL API for specifying types [3] be extended SparkR? And are there any thoughts on implementing some kind of type inferencing perhaps based on a sampling of some number of rows (an implementation I’ve seen before)? R’s read.csv() and read.delim() get types by inferring from the whole file. Getting something that can achieve that functionality via explicit definition of types or sampling will probably be necessary to work with CSV files that have enough columns to merit R at Spark’s scale.

Regards,
Alek Eskilson

[1] - https://github.com/apache/spark/blob/master/R/pkg/R/column.R#L190<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_apache_spark_blob_master_R_pkg_R_column.R-23L190&d=AwMFaQ&c=NRtzTzKNaCCmhN_9N2YJR-XrNU1huIgYP99yDsEzaJo&r=0vZw1rBdgaYvDJYLyKglbrax9kvQfRPdzxLUyWSyxPM&m=aCZhOxAn5Iu762hWogwQK__JsZigsbLZFMaz44UcKQw&s=pETagpDWepAmeaxucEKv1BgoCjqqpIejSjZhXZFF_y8&e=>
[2] - https://issues.apache.org/jira/browse/SPARK-6817<https://urldefense.proofpoint.com/v2/url?u=https-3A__issues.apache.org_jira_browse_SPARK-2D6817&d=AwMFaQ&c=NRtzTzKNaCCmhN_9N2YJR-XrNU1huIgYP99yDsEzaJo&r=0vZw1rBdgaYvDJYLyKglbrax9kvQfRPdzxLUyWSyxPM&m=aCZhOxAn5Iu762hWogwQK__JsZigsbLZFMaz44UcKQw&s=tiLELUAU2Sgk680gUGLr9fR9YxEU6lJEs2e0gWenWhs&e=>
[3] - https://github.com/databricks/spark-csv#sql-api<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_databricks_spark-2Dcsv-23sql-2Dapi&d=AwMFaQ&c=NRtzTzKNaCCmhN_9N2YJR-XrNU1huIgYP99yDsEzaJo&r=0vZw1rBdgaYvDJYLyKglbrax9kvQfRPdzxLUyWSyxPM&m=aCZhOxAn5Iu762hWogwQK__JsZigsbLZFMaz44UcKQw&s=89QC5nymwl5GjjpMwUD--828WaTvjqik9glbCHR7T-8&e=>

CONFIDENTIALITY NOTICE This message and any included attachments are from Cerner Corporation and are intended only for the addressee. The information contained in this message is confidential and may constitute inside or non-public information under international, federal, or state securities laws. Unauthorized forwarding, printing, copying, distribution, or use of such information is strictly prohibited and may be unlawful. If you are not the addressee, please promptly delete this message and notify the sender of the delivery error by e-mail or you may call Cerner's corporate offices in Kansas City, Missouri, U.S.A at (+1) (816)221-1024<tel:%28%2B1%29%20%28816%29221-1024>.

"
Reynold Xin <rxin@databricks.com>,"Wed, 3 Jun 2015 10:53:01 -0700",Re: SparkR DataFrame Column Casts esp. from CSV Files,"""Eskilson,Aleksander"" <Alek.Eskilson@cerner.com>","I think Hossein does want to implement schema inference for CSV -- then
it'd be easy.

Another way you can do this is to use R dataframe/table to read the CSV
files in, and then convert it into a Spark DataFrames. Not going to be
scalable, but could work.


™s currently
e
I,
€™re
m""
I
t
le
ets
ocs_latest_sql-2Dprogramming-2Dguide.html-23json-2Ddatasets&d=AwMFaQ&c=NRtzTzKNaCCmhN_9N2YJR-XrNU1huIgYP99yDsEzaJo&r=0vZw1rBdgaYvDJYLyKglbrax9kvQfRPdzxLUyWSyxPM&m=aCZhOxAn5Iu762hWogwQK__JsZigsbLZFMaz44UcKQw&s=BX3MuobG748zhfm7hc_SnZA4MnFbwgFreNVEjkzkENc&e=>
™s
n
€™s not
sv
ot a
he withColumn() or
kR
 be necessary to do
function(x)
ersation
â€™re
d.delim() get types
t
 be
e_spark_blob_master_R_pkg_R_column.R-23L190&d=AwMFaQ&c=NRtzTzKNaCCmhN_9N2YJR-XrNU1huIgYP99yDsEzaJo&r=0vZw1rBdgaYvDJYLyKglbrax9kvQfRPdzxLUyWSyxPM&m=aCZhOxAn5Iu762hWogwQK__JsZigsbLZFMaz44UcKQw&s=pETagpDWepAmeaxucEKv1BgoCjqqpIejSjZhXZFF_y8&e=>
g_jira_browse_SPARK-2D6817&d=AwMFaQ&c=NRtzTzKNaCCmhN_9N2YJR-XrNU1huIgYP99yDsEzaJo&r=0vZw1rBdgaYvDJYLyKglbrax9kvQfRPdzxLUyWSyxPM&m=aCZhOxAn5Iu762hWogwQK__JsZigsbLZFMaz44UcKQw&s=tiLELUAU2Sgk680gUGLr9fR9YxEU6lJEs2e0gWenWhs&e=>
ricks_spark-2Dcsv-23sql-2Dapi&d=AwMFaQ&c=NRtzTzKNaCCmhN_9N2YJR-XrNU1huIgYP99yDsEzaJo&r=0vZw1rBdgaYvDJYLyKglbrax9kvQfRPdzxLUyWSyxPM&m=aCZhOxAn5Iu762hWogwQK__JsZigsbLZFMaz44UcKQw&s=89QC5nymwl5GjjpMwUD--828WaTvjqik9glbCHR7T-8&e=>
m
ion
f
not
 of
in
"
Hossein Falaki <hossein@databricks.com>,"Wed, 3 Jun 2015 10:55:34 -0700",Re: SparkR DataFrame Column Casts esp. from CSV Files,shivaram@eecs.berkeley.edu,"Yes, spark-csv does not infer types yet, but it is planned to be implemented soon.

To work around the current limitations (of spark-csv and SparkR), you can specify the schema in read.df() to get your desired types from spark-csv. For example:

myschema <- structType(structField(â€œid"", ""integer""), structField(â€œname"", ""stringâ€), structField(â€œlocationâ€, â€œstringâ€))
df <- read.df(sqlContext, ""path/to/file.csv"", source = â€œcom.databricks.spark.csvâ€, schema = myschema)

â€”Hossein

columns as string. I know that the JSON reader infers the schema [1] but I don't know if the CSV reader has any options to do that.  Regarding the SparkR syntax to cast columns, I think there is a simpler way to do it by just assigning to the same column name. For example I have a flights DataFrame with the `year` column typed as string. To cast it to int I just use
don't need a selection.
columns -- Could you file a new JIRA to track the need for casting multiple columns or rather being able to set the schema after loading a DF ?
http://spark.apache.org/docs/latest/sql-programming-guide.html#json-datasets <http://spark.apache.org/docs/latest/sql-programming-guide.html#json-datasets> 
s DataFrames. This is an issue because tools like spark-csv will set column types to String by default and will not attempt to infer types. Although spark-csv supports specifying  types for columns in its options, itâ€™s not clear how that might be integrated into SparkR (when loading the spark-csv package into the R session). 
type with the cast function [1], but itâ€™s notable that this is not a mutator, and it returns a column object as opposed to a DataFrame. It appears the column cast can only be â€˜appliedâ€™ by using the withColumn() or mutate() (an alias for withColumn). 
operate on a column value and return a coerced value. It looks like SparkR doesnâ€™t have UDFs just yet [2], but it seems like theyâ€™d be necessary to do a natural one-off column cast in R, something like
function(x) as.numeric(x)))
the data frame, weâ€™d still have to do a select. There was a conversation about CSV files just yesterday. Types are already problematic, but theyâ€™re a very common data source in R, even at scale. 
Can the current spark-csv SQL API for specifying types [3] be extended SparkR? And are there any thoughts on implementing some kind of type inferencing perhaps based on a sampling of some number of rows (an implementation Iâ€™ve seen before)? Râ€™s read.csv() and read.delim() get types by inferring from the whole file. Getting something that can achieve that functionality via explicit definition of types or sampling will probably be necessary to work with CSV files that have enough columns to merit R at Sparkâ€™s scale.
https://github.com/apache/spark/blob/master/R/pkg/R/column.R#L190 <https://github.com/apache/spark/blob/master/R/pkg/R/column.R#L190>
<https://issues.apache.org/jira/browse/SPARK-6817>
<https://github.com/databricks/spark-csv#sql-api>
from Cerner Corporation and are intended only for the addressee. The information contained in this message is confidential and may constitute inside or non-public information under international, federal, or state securities laws. Unauthorized forwarding, printing, copying, distribution, or use of such information is strictly prohibited and may be unlawful. If you are not the addressee, please promptly delete this message and notify the sender of the delivery error by e-mail or you may call Cerner's corporate offices in Kansas City, Missouri, U.S.A at (+1) (816)221-1024 <tel:%28%2B1%29%20%28816%29221-1024>.

"
"""Eskilson,Aleksander"" <Alek.Eskilson@Cerner.com>","Wed, 3 Jun 2015 18:04:10 +0000",Re: SparkR DataFrame Column Casts esp. from CSV Files,"Hossein Falaki <hossein@databricks.com>,
        ""shivaram@eecs.berkeley.edu""
	<shivaram@eecs.berkeley.edu>","Neat, thanks for the info Hossein. My use case was just to reset the schema for a CSV dataset, but if either a. I can specify it at load, or b. it will be inferred in the future, I’ll likely not need to cast columns, much less reset the whole schema. I’ll still file a JIRA for the capability, but with lower priority.

—Alek

From: Hossein Falaki <hossein@databricks.com<mailto:hossein@databricks.com>Date: Wednesday, June 3, 2015 at 12:55 PM
To: ""shivaram@eecs.berkeley.edu<mailto:shivaram@eecs.berkeley.edu>"" <shivaram@eecs.berkeley.edu<mailto:shivaram@eecs.berkeley.edu>>
Cc: Aleksander Eskilson <Alek.Eskilson@cerner.com<mailto:Alek.Eskilson@cerner.com>>, ""dev@spark.apache.org<mailto:dev@spark.apache.org>"" <dev@spark.apache.org<mailto:dev@spark.apache.org>>
Subject: Re: SparkR DataFrame Column Casts esp. from CSV Files

Yes, spark-csv does not infer types yet, but it is planned to be implemented soon.

To work around the current limitations (of spark-csv and SparkR), you can specify the schema in read.df() to get your desired types from spark-csv. For example:

myschema <- structType(structField(“id"", ""integer""), structField(“name"", ""string”), structField(“location”, “string”))
df <- read.df(sqlContext, ""path/to/file.csv"", source = “com.databricks.spark.csv”, schema = myschema)

—Hossein


cc Hossein who knows more about the spark-csv options

You are right that the default CSV reader options end up creating all columns as string. I know that the JSON reader infers the schema [1] but I don't know if the CSV reader has any options to do that.  Regarding the SparkR syntax to cast columns, I think there is a simpler way to do it by just assigning to the same column name. For example I have a flights DataFrame with the `year` column typed as string. To cast it to int I just use

flights$year <- cast(flights$year, ""int"")

Now the dataframe has the same number of columns as before and you don't need a selection.

However this still doesn't address the part about casting multiple columns -- Could you file a new JIRA to track the need for casting multiple columns or rather being able to set the schema after loading a DF ?

Thanks
Shivaram

[1] http://spark.apache.org/docs/latest/sql-programming-guide.html#json-datasets<https://urldefense.proofpoint.com/v2/url?u=http-3A__spark.apache.org_docs_latest_sql-2Dprogramming-2Dguide.html-23json-2Ddatasets&d=AwMFaQ&c=NRtzTzKNaCCmhN_9N2YJR-XrNU1huIgYP99yDsEzaJo&r=0vZw1rBdgaYvDJYLyKglbrax9kvQfRPdzxLUyWSyxPM&m=jttL5G8owvc7e3__uVdYKnu0D5nxr2rZnq2twPUTtyQ&s=HrpRObaR19Nr992p61rCA9h_44qxPkg3u3G9QPEGKcE&e=>

It appears that casting columns remains a bit of a trick in Spark’s DataFrames. This is an issue because tools like spark-csv will set column types to String by default and will not attempt to infer types. Although spark-csv supports specifying  types for columns in its options, it’s not clear how that might be integrated into SparkR (when loading the spark-csv package into the R session).

Looking at the column.R spec we can cast a column to a different data type with the cast function [1], but it’s notable that this is not a mutator, and it returns a column object as opposed to a DataFrame. It appears the column cast can only be ‘applied’ by using the withColumn() or mutate() (an alias for withColumn).

The other way to cast with Spark DataFrames is to write UDFs that operate on a column value and return a coerced value. It looks like SparkR doesn’t have UDFs just yet [2], but it seems like they’d be necessary to do a natural one-off column cast in R, something like

df.col1toInt <- withColumn(df, “intCol1”, udf(df$col1, function(x) as.numeric(x)))

(where col1 was originally ‘character’ type)

Currently it seems one has to
df.col1cast <- cast(df$col1, “int”)
df.col1toInt <- withColumn(df, df.col1cast)

If we wanted just our casted columns and not the original column from the data frame, we’d still have to do a select. There was a conversation about CSV files just yesterday. Types are already problematic, but they’re a very common data source in R, even at scale.

But only being able to coerce one column at a time is really unwieldy. Can the current spark-csv SQL API for specifying types [3] be extended SparkR? And are there any thoughts on implementing some kind of type inferencing perhaps based on a sampling of some number of rows (an implementation I’ve seen before)? R’s read.csv() and read.delim() get types by inferring from the whole file. Getting something that can achieve that functionality via explicit definition of types or sampling will probably be necessary to work with CSV files that have enough columns to merit R at Spark’s scale.

Regards,
Alek Eskilson

[1] - https://github.com/apache/spark/blob/master/R/pkg/R/column.R#L190<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_apache_spark_blob_master_R_pkg_R_column.R-23L190&d=AwMFaQ&c=NRtzTzKNaCCmhN_9N2YJR-XrNU1huIgYP99yDsEzaJo&r=0vZw1rBdgaYvDJYLyKglbrax9kvQfRPdzxLUyWSyxPM&m=jttL5G8owvc7e3__uVdYKnu0D5nxr2rZnq2twPUTtyQ&s=a_un2u_P_9iUC5QY4DQf4ayzukWk5ta9cbsGnaND3bA&e=>
[2] - https://issues.apache.org/jira/browse/SPARK-6817<https://urldefense.proofpoint.com/v2/url?u=https-3A__issues.apache.org_jira_browse_SPARK-2D6817&d=AwMFaQ&c=NRtzTzKNaCCmhN_9N2YJR-XrNU1huIgYP99yDsEzaJo&r=0vZw1rBdgaYvDJYLyKglbrax9kvQfRPdzxLUyWSyxPM&m=jttL5G8owvc7e3__uVdYKnu0D5nxr2rZnq2twPUTtyQ&s=dciAX1hsR4ZvwI8BZEgLV49GX7x9Bv5c3TbZZbUnZnA&e=>
[3] - https://github.com/databricks/spark-csv#sql-api<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_databricks_spark-2Dcsv-23sql-2Dapi&d=AwMFaQ&c=NRtzTzKNaCCmhN_9N2YJR-XrNU1huIgYP99yDsEzaJo&r=0vZw1rBdgaYvDJYLyKglbrax9kvQfRPdzxLUyWSyxPM&m=jttL5G8owvc7e3__uVdYKnu0D5nxr2rZnq2twPUTtyQ&s=zrmlIgWJY8jsATWoWYM9fvEVVVW9EDiWeBHTKMQpEMA&e=>

CONFIDENTIALITY NOTICE This message and any included attachments are from Cerner Corporation and are intended only for the addressee. The information contained in this message is confidential and may constitute inside or non-public information under international, federal, or state securities laws. Unauthorized forwarding, printing, copying, distribution, or use of such information is strictly prohibited and may be unlawful. If you are not the addressee, please promptly delete this message and notify the sender of the delivery error by e-mail or you may call Cerner's corporate offices in Kansas City, Missouri, U.S.A at (+1) (816)221-1024<tel:%28%2B1%29%20%28816%29221-1024>.


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Wed, 3 Jun 2015 12:12:26 -0700",Re: SparkR DataFrame Column Casts esp. from CSV Files,"""Eskilson,Aleksander"" <Alek.Eskilson@cerner.com>","Hmm - the schema=myschema doesn't seem to work in SparkR from my simple
local test. I'm filing a JIRA for this now


.
olumns,
""
(â€œname"",
gâ€))
I
t
le
ets
ocs_latest_sql-2Dprogramming-2Dguide.html-23json-2Ddatasets&d=AwMFaQ&c=NRtzTzKNaCCmhN_9N2YJR-XrNU1huIgYP99yDsEzaJo&r=0vZw1rBdgaYvDJYLyKglbrax9kvQfRPdzxLUyWSyxPM&m=jttL5G8owvc7e3__uVdYKnu0D5nxr2rZnq2twPUTtyQ&s=HrpRObaR19Nr992p61rCA9h_44qxPkg3u3G9QPEGKcE&e=>
™s
n
€™s not
sv
ot a
he withColumn() or
kR
 be necessary to do
function(x)
ersation
â€™re
d.delim() get types
t
 be
e_spark_blob_master_R_pkg_R_column.R-23L190&d=AwMFaQ&c=NRtzTzKNaCCmhN_9N2YJR-XrNU1huIgYP99yDsEzaJo&r=0vZw1rBdgaYvDJYLyKglbrax9kvQfRPdzxLUyWSyxPM&m=jttL5G8owvc7e3__uVdYKnu0D5nxr2rZnq2twPUTtyQ&s=a_un2u_P_9iUC5QY4DQf4ayzukWk5ta9cbsGnaND3bA&e=>
g_jira_browse_SPARK-2D6817&d=AwMFaQ&c=NRtzTzKNaCCmhN_9N2YJR-XrNU1huIgYP99yDsEzaJo&r=0vZw1rBdgaYvDJYLyKglbrax9kvQfRPdzxLUyWSyxPM&m=jttL5G8owvc7e3__uVdYKnu0D5nxr2rZnq2twPUTtyQ&s=dciAX1hsR4ZvwI8BZEgLV49GX7x9Bv5c3TbZZbUnZnA&e=>
ricks_spark-2Dcsv-23sql-2Dapi&d=AwMFaQ&c=NRtzTzKNaCCmhN_9N2YJR-XrNU1huIgYP99yDsEzaJo&r=0vZw1rBdgaYvDJYLyKglbrax9kvQfRPdzxLUyWSyxPM&m=jttL5G8owvc7e3__uVdYKnu0D5nxr2rZnq2twPUTtyQ&s=zrmlIgWJY8jsATWoWYM9fvEVVVW9EDiWeBHTKMQpEMA&e=>
m
ion
f
not
 of
in
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Wed, 3 Jun 2015 12:20:25 -0700",Re: SparkR DataFrame Column Casts esp. from CSV Files,Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"I created https://issues.apache.org/jira/browse/SPARK-8085 for this.


b.
columns,
g""
tionâ€, â€œstringâ€))
 I
y
st
ple
sets
docs_latest_sql-2Dprogramming-2Dguide.html-23json-2Ddatasets&d=AwMFaQ&c=NRtzTzKNaCCmhN_9N2YJR-XrNU1huIgYP99yDsEzaJo&r=0vZw1rBdgaYvDJYLyKglbrax9kvQfRPdzxLUyWSyxPM&m=jttL5G8owvc7e3__uVdYKnu0D5nxr2rZnq2twPUTtyQ&s=HrpRObaR19Nr992p61rCA9h_44qxPkg3u3G9QPEGKcE&e=>
€™s
mn
h
€™s not
csv
not a
the withColumn() or
rkR
d be necessary to do
 function(x)
versation
â€™re
ad.delim() get types
at
y be
0
he_spark_blob_master_R_pkg_R_column.R-23L190&d=AwMFaQ&c=NRtzTzKNaCCmhN_9N2YJR-XrNU1huIgYP99yDsEzaJo&r=0vZw1rBdgaYvDJYLyKglbrax9kvQfRPdzxLUyWSyxPM&m=jttL5G8owvc7e3__uVdYKnu0D5nxr2rZnq2twPUTtyQ&s=a_un2u_P_9iUC5QY4DQf4ayzukWk5ta9cbsGnaND3bA&e=>
rg_jira_browse_SPARK-2D6817&d=AwMFaQ&c=NRtzTzKNaCCmhN_9N2YJR-XrNU1huIgYP99yDsEzaJo&r=0vZw1rBdgaYvDJYLyKglbrax9kvQfRPdzxLUyWSyxPM&m=jttL5G8owvc7e3__uVdYKnu0D5nxr2rZnq2twPUTtyQ&s=dciAX1hsR4ZvwI8BZEgLV49GX7x9Bv5c3TbZZbUnZnA&e=>
bricks_spark-2Dcsv-23sql-2Dapi&d=AwMFaQ&c=NRtzTzKNaCCmhN_9N2YJR-XrNU1huIgYP99yDsEzaJo&r=0vZw1rBdgaYvDJYLyKglbrax9kvQfRPdzxLUyWSyxPM&m=jttL5G8owvc7e3__uVdYKnu0D5nxr2rZnq2twPUTtyQ&s=zrmlIgWJY8jsATWoWYM9fvEVVVW9EDiWeBHTKMQpEMA&e=>
e
on,
If
fy
.
"
atalay <atalayfurkan@yahoo.com>,"Wed, 3 Jun 2015 15:01:17 -0700 (MST)",Cleaning up workers' directories automatically,dev@spark.apache.org,"Hi everyone,
everytime our data comes and new updates occur in our cluster, an
undesirable file is being created in workers' directories.In order to
cleanup automatically I changed the variable value Spark (Standalone) Client
Advanced Configuration Snippet (Safety Valve) for spark-conf/spark-env.sh in
Gateway Default Group->Advanced Settings  as :

/*export SPARK_WORKER_OPTS=""-Dspark.worker.cleanup.enabled=true
-Dspark.worker.cleanup.interval=60 -Dspark.worker.cleanup.appDataTtl=60""*/

by using cloudera manager.
After i make the cluster restart, it makes change in spark/conf/spark-env.sh 
but  it does not make cleanup.Does anyone know where the mistake is or
another way of cleaning up automatically ?
i am using CDH 4 and Spark 1.2.2 in the cluster.



--

---------------------------------------------------------------------


"
Joseph Bradley <joseph@databricks.com>,"Wed, 3 Jun 2015 16:16:30 -0700",Re: MLlib: Anybody working on hierarchical topic models like HLDA?,Lorenz Fischer <lorenz.fischer@gmail.com>,"Hi Lorenz,

I'm not aware of people working on hierarchical topic models for MLlib, but
that would be cool to see.  Hopefully other devs know more!

Glad that the current LDA is helpful!

Joseph


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Wed, 3 Jun 2015 16:32:53 -0700",[ANNOUNCE] YARN support in Spark EC2,"""dev@spark.apache.org"" <dev@spark.apache.org>, ""user@spark.apache.org"" <user@spark.apache.org>","Hi all

We recently merged support for launching YARN clusters using Spark EC2
scripts as a part of
https://issues.apache.org/jira/browse/SPARK-3674. To use this you can pass
in hadoop-major-version as ""yarn"" to the spark-ec2 script and this will
setup Hadoop 2.4 HDFS, YARN and Spark built for YARN on the EC2 cluster.

Developers who work on features related to YARN might find this useful for
testing / benchmarking Spark with YARN. If anyone has questions or feedback
please let me know.

Thanks
Shivaram
"
Marcelo Vanzin <vanzin@cloudera.com>,"Wed, 3 Jun 2015 16:33:42 -0700",Ivy support in Spark vs. sbt,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey all,

I've been bit by something really weird lately and I'm starting to think
it's related to the ivy support we have in Spark, and running unit tests
that use that code.

The first thing that happens is that after running unit tests, sometimes my
sbt builds start failing with error saying something about ""dependency path
must be relative"" (sorry, don't have the exact error around). The
dependency path it prints is a ""file:"" URL.

I have a feeling that this is because Spark uses Ivy 2.4 while sbt uses Ivy
2.3, and those might be incompatible. So if they get mixed up, things can
break.

The second is that sometimes unit tests fail with some weird error
downloading dependencies. When checking the ivy metadata in ~/.ivy2/cache,
the offending dependencies are pointing to my local maven repo (I have
""maven-local"" as one of the entries in my ~/.sbt/repositories).

My feeling in this case is that Spark's version of Ivy somehow doesn't
handle that case.

So, long story short:

- Has anyone run into either of these problems?
- Is it possible to set some env variable or something during tests to
force them to use their own directory instead of messing up and breaking my
~/.ivy2?


-- 
Marcelo
"
"""Yang, Yuhao"" <yuhao.yang@intel.com>","Thu, 4 Jun 2015 03:13:18 +0000",RE: MLlib: Anybody working on hierarchical topic models like HLDA?,"Joseph Bradley <joseph@databricks.com>, Lorenz Fischer
	<lorenz.fischer@gmail.com>","Hi Lorenz,

  Iâ€™m trying to build a prototype of HDP for a customer based on the current LDA implementations. An initial version will probably be ready within the next one or two weeks. Iâ€™ll share it and hopefully we can join forces.

  One concern is that Iâ€™m not sure how widely it will be used in the industry or community. Hope itâ€™s popular enough to be accepted by Spark MLlib.

http://www.cs.berkeley.edu/~jordan/papers/hierarchical-dp.pdf
http://jmlr.csail.mit.edu/proceedings/papers/v15/wang11a/wang11a.pdf

Regards,
Yuhao

From: Joseph Bradley [mailto:joseph@databricks.com]
Sent: Thursday, June 4, 2015 7:17 AM
To: Lorenz Fischer
Cc: dev@spark.apache.org
Subject: Re: MLlib: Anybody working on hierarchical topic models like HLDA?

Hi Lorenz,

I'm not aware of people working on hierarchical topic models for MLlib, but that would be cool to see.  Hopefully other devs know more!

Glad that the current LDA is helpful!

Joseph

On Wed, Jun 3, 2015 at 6:43 AM, Lorenz Fischer <lorenz.fischer@gmail.com<mailto:lorenz.fischer@gmail.com>> wrote:
Hi All

I'm working on a project in which I use the current LDA implementation that has been contributed by Databricks' Joseph Bradley et al. for the recent 1.3.0 release (thanks guys!). While this is great, my project requires several levels of topics, as I would like to offer users to drill down into subtopics.

As I understand it, Hierarchical Latent Dirichlet Allocation (HLDA) would offer such a hierarchy. Looking at the papers and talks by Blei [1,2] and Jordan [3], I think I should be able to implement HLDA in Spark using the Nested Chinese Restaurant Process (NCRP). However, as I have some time constraints, I'm not sure if I will have the time to do it 'the proper way'.

In any case, I wanted to quickly ask around if anybody is already working on this or on some other form of a hierarchical topic model. Maybe I could contribute to these efforts instead of starting from scratch.

Best,
Lorenz

[1] http://www.cs.princeton.edu/~blei/papers/BleiGriffithsJordan2009.pdf
[2] http://papers.nips.cc/paper/2466-hierarchical-topic-models-and-the-nested-chinese-restaurant-process.pdf
[3] https://www.youtube.com/watch?v=PxgW3lOrj60

"
DB Tsai <dbtsai@dbtsai.com>,"Wed, 3 Jun 2015 22:00:33 -0700",Re: MLlib: Anybody working on hierarchical topic models like HLDA?,"""Yang, Yuhao"" <yuhao.yang@intel.com>","Is your HDP implementation based on distributed gibbs sampling? Thanks.

Sincerely,

DB Tsai
-------------------------------------------------------
Blog: https://www.dbtsai.com


the current
rces.
the
 Spark
A?
ut
at
to
y'.
 on
-chinese-restaurant-process.pdf

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Wed, 3 Jun 2015 23:59:59 -0700",Re: [VOTE] Release Apache Spark 1.4.0 (RC4),Patrick Wendell <pwendell@gmail.com>,"Let me give you the 1st

+1




"
"""Yang, Yuhao"" <yuhao.yang@intel.com>","Thu, 4 Jun 2015 09:12:13 +0000",RE: MLlib: Anybody working on hierarchical topic models like HLDA?,DB Tsai <dbtsai@dbtsai.com>,"Hi DB Tsai,

Not for now. My primary reference is http://jmlr.csail.mit.edu/proceedings/papers/v15/wang11a/wang11a.pdf .

And I'm seeking a way to maximum code reuse. Any suggestion will be welcome. Thanks.

Regards,
yuhao

-----Original Message-----
From: DB Tsai [mailto:dbtsai@dbtsai.com] 
Sent: Thursday, June 4, 2015 1:01 PM
To: Yang, Yuhao
Cc: Joseph Bradley; Lorenz Fischer; dev@spark.apache.org
Subject: Re: MLlib: Anybody working on hierarchical topic models like HLDA?

Is your HDP implementation based on distributed gibbs sampling? Thanks.

Sincerely,

DB Tsai
-------------------------------------------------------
Blog: https://www.dbtsai.com


On Wed, Jun 3, 2015 at 8:13 PM, Yang, Yuhao <yuhao.yang@intel.com> wrote:
> Hi Lorenz,
>
>
>
>   Iâ€™m trying to build a prototype of HDP for a customer based on the 
> current LDA implementations. An initial version will probably be ready 
> within the next one or two weeks. Iâ€™ll share it and hopefully we can join forces.
>
>
>
>   One concern is that Iâ€™m not sure how widely it will be used in the 
> industry or community. Hope itâ€™s popular enough to be accepted by 
> Spark MLlib.
>
>
>
> http://www.cs.berkeley.edu/~jordan/papers/hierarchical-dp.pdf
>
> http://jmlr.csail.mit.edu/proceedings/papers/v15/wang11a/wang11a.pdf
>
>
>
> Regards,
>
> Yuhao
>
>
>
> From: Joseph Bradley [mailto:joseph@databricks.com]
> Sent: Thursday, June 4, 2015 7:17 AM
> To: Lorenz Fischer
> Cc: dev@spark.apache.org
> Subject: Re: MLlib: Anybody working on hierarchical topic models like HLDA?
>
>
>
> Hi Lorenz,
>
>
>
> I'm not aware of people working on hierarchical topic models for 
> MLlib, but that would be cool to see.  Hopefully other devs know more!
>
>
>
> Glad that the current LDA is helpful!
>
>
>
> Joseph
>
>
>
> On Wed, Jun 3, 2015 at 6:43 AM, Lorenz Fischer 
> <lorenz.fischer@gmail.com>
> wrote:
>
> Hi All
>
>
>
> I'm working on a project in which I use the current LDA implementation 
> that has been contributed by Databricks' Joseph Bradley et al. for the 
> recent
> 1.3.0 release (thanks guys!). While this is great, my project requires 
> several levels of topics, as I would like to offer users to drill down 
> into subtopics.
>
>
>
> As I understand it, Hierarchical Latent Dirichlet Allocation (HLDA) 
> would offer such a hierarchy. Looking at the papers and talks by Blei 
> [1,2] and Jordan [3], I think I should be able to implement HLDA in 
> Spark using the Nested Chinese Restaurant Process (NCRP). However, as 
> I have some time constraints, I'm not sure if I will have the time to do it 'the proper way'.
>
>
>
> In any case, I wanted to quickly ask around if anybody is already 
> working on this or on some other form of a hierarchical topic model. 
> Maybe I could contribute to these efforts instead of starting from scratch.
>
>
>
> Best,
>
> Lorenz
>
>
>
> [1] 
> http://www.cs.princeton.edu/~blei/papers/BleiGriffithsJordan2009.pdf
>
> [2]
> http://papers.nips.cc/paper/2466-hierarchical-topic-models-and-the-nes
> ted-chinese-restaurant-process.pdf
>
> [3] https://www.youtube.com/watch?v=PxgW3lOrj60
>
>
"
Meethu Mathew <meethu.mathew@flytxt.com>,"Thu, 4 Jun 2015 15:46:17 +0530",Anyone facing problem in incremental building of individual project,dev@spark.apache.org,"Hi all,

â€‹I added some new code to MLlib. When I am trying to build only the mllib
project using  *mvn --projects mllib/ -DskipTests clean install*
*â€‹ *after setting
export S
PARK_PREPEND_CLASSES=true
â€‹, the build is getting stuck with the following message.



.
.
SHOT.jar
SHOT-shaded.jar

   .....

â€‹But  a full build completes as usual. Please help if anyone is facing the
same issue.

Regards,

Meethu Mathew
Senior Engineer
Flytxt
"
Sean Owen <sowen@cloudera.com>,"Thu, 4 Jun 2015 13:29:23 +0200",Re: Ivy support in Spark vs. sbt,Marcelo Vanzin <vanzin@cloudera.com>,"I've definitely seen the ""dependency path must be relative"" problem,
and fixed it by deleting the ivy cache, but I don't know more than
this.


---------------------------------------------------------------------


"
Steve Loughran <stevel@hortonworks.com>,"Thu, 4 Jun 2015 13:25:07 +0000","Re: Anyone facing problem in incremental building of individual
 project",Meethu Mathew <meethu.mathew@flytxt.com>,"
On 4 Jun 2015, at 11:16, Meethu Mathew <meethu.mathew@flytxt.com<mailto:meethu.mathew@flytxt.com>> wrote:

Hi all,

â€‹I added some new code to MLlib. When I am trying to build only the mllib project using  mvn --projects mllib/ -DskipTests clean install
â€‹ after setting
export S
PARK_PREPEND_CLASSES=true
â€‹, the build is getting stuck with the following message.


 Excluding org.jpmml:pmml-schema:jar:1.1.15 from the shaded jar.
[INFO] Excluding com.sun.xml.bind:jaxb-impl:jar:2.2.7 from the shaded jar.
[INFO] Excluding com.sun.xml.bind:jaxb-core:jar:2.2.7 from the shaded jar.
[INFO] Excluding javax.xml.bind:jaxb-api:jar:2.2.7 from the shaded jar.
[INFO] Incjar:2.10.4 from the shaded jar.
[INFO] Replacing original artifact with shaded artifact.
[INFO] Replacing /home/meethu/git/FlytxtRnD/spark/mllib/target/spark-mllib_2.10-1.4.0-SNAPSHOT.jar with /home/meethu/git/FlytxtRnD/spark/mllib/target/spark-mllib_2.10-1.4.0-SNAPSHOT-shaded.jar
[INFO] Dependency-reduced POM written at: /home/meethu/git/FlytxtRnD/spark/mllib/dependency-reduced-pom.xml
[INFO] Dependency-reduced POM written at: /home/meethu/git/FlytxtRnD/spark/mllib/dependency-reduced-pom.xml
[INFO] Dependency-reduced POM written at: /home/meethu/git/FlytxtRnD/spark/mllib/dependency-reduced-pom.xml
[INFO] Dependency-reduced POM written at: /home/meethu/git/FlytxtRnD/spark/mllib/dependency-reduced-pom.xml
   .....


I've seen something similar in a different build,

It looks like MSHADE-148: https://issues.apache.org/jira/browse/MSHADE-148
if you apply Tom White's patch, does your problem go away?
"
Chris Freeman <cfreeman@alteryx.com>,"Thu, 4 Jun 2015 14:58:23 +0000",Spark Packages: using sbt-spark-package tool with R,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey everyone,

Iâ€™m looking to develop a package for use with SparkR. This package would include custom R and Scala code and I was wondering if anyone had any insight into how I might be able to use the sbt-spark-package tool to publish something that needs to include an R package as well as a JAR created via SBT assembly.  I know thereâ€™s an existing option for including Python files but I havenâ€™t been able to crack the code on how I might be able to include R files.

Any advice is appreciated!

-Chris Freeman

"
Ravi Desai <rd7190@gmail.com>,"Thu, 04 Jun 2015 11:45:31 -0400",Where is the JIRA filter for new contributers?,"""dev@spark.apache.org"" <dev@spark.apache.org>","I am new to Spark and would like to contribute to it.  I recall seeing 
somewhere on the website a link to a JIRA filter for new contributers, 
but can't find that anymore.  Could someone point me to it?

Thanks,
-ravi

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Thu, 4 Jun 2015 17:51:03 +0200",Re: Where is the JIRA filter for new contributers?,Ravi Desai <rd7190@gmail.com>,"https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark

... which contains ...

https://issues.apache.org/jira/browse/SPARK-7993?jql=project%20%3D%20SPARK%20AND%20labels%20%3D%20Starter%20AND%20status%20in%20(Open%2C%20%22In%20Progress%22%2C%20Reopened)


---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Thu, 4 Jun 2015 09:21:57 -0700",Re: Anyone facing problem in incremental building of individual project,Steve Loughran <stevel@hortonworks.com>,"Andrew Or put in this workaround :

diff --git a/pom.xml b/pom.xml
index 0b1aaad..d03d33b 100644
--- a/pom.xml
+++ b/pom.xml
@@ -1438,6 +1438,8 @@
         <version>2.3</version>
         <configuration>
           <shadedArtifactAttached>false</shadedArtifactAttached>
+          <!-- Work around MSHADE-148 -->
+          <createDependencyReducedPom>false</createDependencyReducedPom>
           <artifactSet>
             <includes>
               <!-- At a minimum we must include this to force effective
pom generation -->

FYI


the
r.
r.
PSHOT.jar
PSHOT-shaded.jar
"
Ashwin Shankar <ashwinshankar77@gmail.com>,"Thu, 4 Jun 2015 09:33:27 -0700",Fwd: How to pass system properties in spark ?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Trying spark-dev mailing list to see if anyone knows.

---------- Forwarded message ----------
From: Ashwin Shankar <ashwinshankar77@gmail.com>
Date: Wed, Jun 3, 2015 at 5:38 PM
Subject: How to pass system properties in spark ?
To: ""user@spark.apache.org"" <user@spark.apache.org>


Hi,
I'm trying to use property substitution in my log4j.properties, so that
I can choose where to write spark logs at runtime.
The problem is that, system property passed to spark shell
doesn't seem to getting propagated to log4j.

*Here is log4j.properites(partial) with a parameter 'spark.log.path' :*
log4j.appender.logFile=org.apache.log4j.FileAppender
log4j.appender.logFile.File=*${spark.log.path}*
log4j.appender.logFile.layout=org.apache.log4j.PatternLayout
log4j.appender.logFile.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p
%c{1}: %m%n

*Here is how I pass the 'spark.log.path' variable on command line :*
$spark-shell --conf
spark.driver.extraJavaOptions=""-Dspark.log.path=/tmp/spark.log""

I also tried :
$spark-shell -Dspark.log.path=/tmp/spark.log

*Result : */tmp*/*spark.log not getting created when I run spark.

Any ideas why this is happening ?

*When I enable log4j debug I see that following :*
log4j: Setting property [file] to [].
log4j: setFile called: , true
log4j:ERROR setFile(null,true) call failed.
java.io.FileNotFoundException:  (No such file or directory)
at java.io.FileOutputStream.open(Native Method)

-- 
Thanks,
Ashwin





-- 
Thanks,
Ashwin
"
Burak Yavuz <brkyvz@gmail.com>,"Thu, 4 Jun 2015 09:47:15 -0700",Re: Ivy support in Spark vs. sbt,Sean Owen <sowen@cloudera.com>,"Hi Marcelo,

This is interesting. Can you please send me links to any failing builds if
you see that problem please. For now you can set a conf: `spark.jars.ivy`
to use a path except `~/.ivy2` for Spark.

Thanks,
Burak


"
Marcelo Vanzin <vanzin@cloudera.com>,"Thu, 4 Jun 2015 09:50:39 -0700",Re: Ivy support in Spark vs. sbt,Burak Yavuz <brkyvz@gmail.com>,"They're my local builds, so I wouldn't be able to send you any links... and
the error is generally from sbt, not the unit tests. But if there's any
info I can collect when I see the error, let me know.

I'll try ""spark.jars.ivy"". I wonder if we should just set that to the
system properties in Spark's root pom.




-- 
Marcelo
"
shane knapp <sknapp@berkeley.edu>,"Thu, 4 Jun 2015 10:04:40 -0700",Re: Ivy support in Spark vs. sbt,Sean Owen <sowen@cloudera.com>,"this has occasionally happened on our jenkins as well (twice since last
august), and deleting the cache fixes it right up.


"
Marcelo Vanzin <vanzin@cloudera.com>,"Thu, 4 Jun 2015 10:16:24 -0700",Re: Ivy support in Spark vs. sbt,shane knapp <sknapp@berkeley.edu>,"

Yes deleting the cache fixes things, but it's kinda annoying to have to do
that. And yesterday when I was testing a patch that actually used the ivy
feature, I had to do that multiple times... that slows things down a lot.




-- 
Marcelo
"
shane knapp <sknapp@berkeley.edu>,"Thu, 4 Jun 2015 10:23:55 -0700",Re: Ivy support in Spark vs. sbt,Marcelo Vanzin <vanzin@cloudera.com>,"interesting...  i definitely haven't seen it happen that often in our build
system, and when it has happened, i wasn't able to determine the cause.


"
Patrick Wendell <pwendell@gmail.com>,"Thu, 4 Jun 2015 13:09:50 -0700",Re: [VOTE] Release Apache Spark 1.4.0 (RC4),Reynold Xin <rxin@databricks.com>,"I will give +1 as well.


---------------------------------------------------------------------


"
Marcelo Vanzin <vanzin@cloudera.com>,"Thu, 4 Jun 2015 13:30:12 -0700",Re: Ivy support in Spark vs. sbt,shane knapp <sknapp@berkeley.edu>,"Here's one of the types of exceptions I get (this one when running
VersionsSuite from sql/hive):

[info] - 13: create client *** FAILED *** (1 second, 946 milliseconds)
[info]   java.lang.RuntimeException: [download failed:
org.apache.httpcomponents#httpclient;4.2.5!httpclient.jar, download failed:
commons-codec#commons-codec;1.4!commons-codec.jar]
[info]   at
org.apache.spark.deploy.SparkSubmitUtils$.resolveMavenCoordinates(SparkSubmit.scala:978)

This is the content of the ivy metadata file for that component:

#ivy cached data file for org.apache.httpcomponents#httpclient;4.2.5
#Thu Jun 04 13:26:10 PDT 2015
artifact\:ivy\#ivy\#xml\#1855381640.is-local=true
artifact\:ivy\#ivy\#xml\#1855381640.location=file\:/home/vanzin/.m2/repository/org/apache/httpcomponents/httpclient/4.2.5/httpclient-4.2.5.pom
artifact\:ivy\#ivy\#xml\#1855381640.exists=true
resolver=local-m2-cache
artifact\:httpclient\#pom.original\#pom\#-365933676.original=artifact\:httpclient\#pom.original\#pom\#-365933676
artifact\:ivy\#ivy\#xml\#1855381640.original=artifact\:httpclient\#pom.original\#pom\#-365933676
artifact.resolver=local-m2-cache
artifact\:httpclient\#pom.original\#pom\#-365933676.is-local=true
artifact\:httpclient\#pom.original\#pom\#-365933676.location=file\:/home/vanzin/.m2/repository/org/apache/httpcomponents/httpclient/4.2.5/httpclient-4.2.5.pom
artifact\:httpclient\#pom.original\#pom\#-365933676.exists=true


If I delete that file *and* the maven copy of those artifacts, then the
tests pass. But that's really annoying, since I have to use sbt and maven
for different things and I really like the fact that sbt can read the maven
cache directly.





-- 
Marcelo
"
Matei Zaharia <matei.zaharia@gmail.com>,"Thu, 4 Jun 2015 13:35:14 -0700",Re: [VOTE] Release Apache Spark 1.4.0 (RC4),Patrick Wendell <pwendell@gmail.com>,"+1 

Tested on Mac OS X

version

at:
http://people.apache.org/~pwendell/spark-releases/spark-1.4.0-rc4-bin/
https://repository.apache.org/content/repositories/orgapachespark-1111/
https://repository.apache.org/content/repositories/orgapachespark-1112/
ht"
Eron Wright <ewright@live.com>,"Thu, 4 Jun 2015 16:08:17 -0700",Re: Ivy support in Spark vs. sbt,"Sean Owen <sowen@cloudera.com>,
	Marcelo Vanzin <vanzin@cloudera.com>","I saw something like this last night, with a similar message.  Is this what youâ€™re referring to?

[error] org.deeplearning4j#dl4j-spark-ml;0.0.3.3.4.alpha1-SNAPSHOT!dl4j-spark-ml.jar origin location must be absolute: file:/Users/eron/.m2/repository/org/deeplearning4j/dl4j-spark-ml/0.0.3.3.4.alpha1-SNAPSHOT/dl4j-spark-ml-0.0.3.3.4.alpha1-SNAPSHOT.jar
java.lang.IllegalArgumentException: org.deeplearning4j#dl4j-spark-ml;0.0.3.3.4.alpha1-SNAPSHOT!dl4j-spark-ml.jar origin location must be absolute: file:/Users/eron/.m2/repository/org/deeplearning4j/dl4j-spark-ml/0.0.3.3.4.alpha1-SNAPSHOT/dl4j-spark-ml-0.0.3.3.4.alpha1-SNAPSHOT.jar
	at org.apache.ivy.util.Checks.checkAbsolute(Checks.java:57)
	at org.apache.ivy.core.cache.DefaultRepositoryCacheManager.getArchiveFileInCache(DefaultRepositoryCacheManager.java:385)
	at org.apache.ivy.core.cache.DefaultRepositoryCacheManager.download(DefaultRepositoryCacheManager.java:849)
	at org.apache.ivy.plugins.resolver.BasicResolver.download(BasicResolver.java:835)
	at org.apache.ivy.plugins.resolver.RepositoryResolver.download(RepositoryResolver.java:282)
	at org.apache.ivy.plugins.resolver.ChainResolver.download(ChainResolver.java:219)
	at org.apache.ivy.plugins.resolver.ChainResolver.download(ChainResolver.java:219)
	at org.apache.ivy.core.resolve.ResolveEngine.downloadArtifacts(ResolveEngine.java:388)
	at org.apache.ivy.core.resolve.ResolveEngine.resolve(ResolveEngine.java:331)
	at org.apache.ivy.Ivy.resolve(Ivy.java:517)
	at sbt.IvyActions$.sbt$IvyActions$$resolve(IvyActions.scala:266)
	at sbt.IvyActions$$anonfun$updateEither$1.apply(IvyActions.scala:175)
	at sbt.IvyActions$$anonfun$updateEither$1.apply(IvyActions.scala:157)
	at sbt.IvySbt$Module$$anonfun$withModule$1.apply(Ivy.scala:151)
	at sbt.IvySbt$Module$$anonfun$withModule$1.apply(Ivy.scala:151)
	at sbt.IvySbt$$anonfun$withIvy$1.apply(Ivy.scala:128)
	at sbt.IvySbt.sbt$IvySbt$$action$1(Ivy.scala:56)
	at sbt.IvySbt$$anon$4.call(Ivy.scala:64)
	at xsbt.boot.Locks$GlobalLock.withChannel$1(Locks.scala:93)
	at xsbt.boot.Locks$GlobalLock.xsbt$boot$Locks$GlobalLock$$withChannelRetries$1(Locks.scala:78)
	at xsbt.boot.Locks$GlobalLock$$anonfun$withFileLock$1.apply(Locks.scala:97)
	at xsbt.boot.Using$.withResource(Using.scala:10)
	at xsbt.boot.Using$.apply(Using.scala:9)
	at xsbt.boot.Locks$GlobalLock.ignoringDeadlockAvoided(Locks.scala:58)
	at xsbt.boot.Locks$GlobalLock.withLock(Locks.scala:48)
	at xsbt.boot.Locks$.apply0(Locks.scala:31)
	at xsbt.boot.Locks$.apply(Locks.scala:28)
	at sbt.IvySbt.withDefaultLogger(Ivy.scala:64)
	at sbt.IvySbt.withIvy(Ivy.scala:123)
	at sbt.IvySbt.withIvy(Ivy.scala:120)
	at sbt.IvySbt$Module.withModule(Ivy.scala:151)
	at sbt.IvyActions$.updateEither(IvyActions.scala:157)
	at sbt.Classpaths$$anonfun$sbt$Classpaths$$work$1$1.apply(Defaults.scala:1318)
	at sbt.Classpaths$$anonfun$sbt$Classpaths$$work$1$1.apply(Defaults.scala:1315)
	at sbt.Classpaths$$anonfun$doWork$1$1$$anonfun$85.apply(Defaults.scala:1345)
	at sbt.Classpaths$$anonfun$doWork$1$1$$anonfun$85.apply(Defaults.scala:1343)
	at sbt.Tracked$$anonfun$lastOutput$1.apply(Tracked.scala:35)
	at sbt.Classpaths$$anonfun$doWork$1$1.apply(Defaults.scala:1348)
	at sbt.Classpaths$$anonfun$doWork$1$1.apply(Defaults.scala:1342)
	at sbt.Tracked$$anonfun$inputChanged$1.apply(Tracked.scala:45)
	at sbt.Classpaths$.cachedUpdate(Defaults.scala:1360)
	at sbt.Classpaths$$anonfun$updateTask$1.apply(Defaults.scala:1300)
	at sbt.Classpaths$$anonfun$updateTask$1.apply(Defaults.scala:1275)
	at scala.Function1$$anonfun$compose$1.apply(Function1.scala:47)
	at sbt.$tilde$greater$$anonfun$$u2219$1.apply(TypeFunctions.scala:40)
	at sbt.std.Transform$$anon$4.work(System.scala:63)
	at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:226)
	at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:226)
	at sbt.ErrorHandling$.wideConvert(ErrorHandling.scala:17)
	at sbt.Execute.work(Execute.scala:235)
	at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:226)
	at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:226)
	at sbt.ConcurrentRestrictions$$anon$4$$anonfun$1.apply(ConcurrentRestrictions.scala:159)
	at sbt.CompletionService$$anon$2.call(CompletionService.scala:28)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)





:
 my
ath
ency
Ivy
n
e,
orce


---------------------------------------------------------------------


"
Andrew Or <andrew@databricks.com>,"Thu, 4 Jun 2015 17:47:49 -0700",Re: [VOTE] Release Apache Spark 1.4.0 (RC4),Matei Zaharia <matei.zaharia@gmail.com>,"+1 (binding)

Ran the same tests I did for RC3:

Tested the standalone cluster mode REST submission gateway - submit /
status / kill
Tested simple applications on YARN client / cluster modes with and without
--jars
Tested python applications on YARN clien"
Timothy Chen <tnachen@gmail.com>,"Thu, 4 Jun 2015 18:26:13 -0700",Re: [VOTE] Release Apache Spark 1.4.0 (RC4),Andrew Or <andrew@databricks.com>,"+1

Been testing cluster mode and client mode with mesos with 6 nodes cluster.

Everything works so far.

Tim

us / kill
 --jars
 --py-files*
ew feature)
put can be disabled)

e:
rote:
sion

t:
n/
11/
12/
cs/
e
()
eton
.

"
Calvin Jia <jia.calvin@gmail.com>,"Thu, 4 Jun 2015 18:35:22 -0700",Re: [VOTE] Release Apache Spark 1.4.0 (RC4),Timothy Chen <tnachen@gmail.com>,"+1

Tested with input from Tachyon and persist off heap.


"
Meethu Mathew <meethu.mathew@flytxt.com>,"Fri, 5 Jun 2015 10:39:12 +0530",Re: Anyone facing problem in incremental building of individual project,Ted Yu <yuzhihong@gmail.com>,"Hi,
I added
â€‹
 <createDependencyReducedPom> in my pom.xml and  the problem is solved.
            <!-- Work around MSHADE-148 -->
+
â€‹â€‹
 <createDependencyReducedPom>false</createDependencyReducedPom>

â€‹Thank you @Steveâ€‹  and @Ted


Regards,

Meethu Mathew
Senior Engineer
Flytxt

:
 the
d
APSHOT.jar
APSHOT-shaded.jar
"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Fri, 05 Jun 2015 05:45:58 +0000",PySpark on PyPi,dev <dev@spark.apache.org>,"Hi everyone,
Considering the python API as just a front needing the SPARK_HOME defined
anyway, I think it would be interesting to deploy the Python part of Spark
on PyPi in order to handle the dependencies in a Python project needing
PySpark via pip.

For now I just symlink the python/pyspark in my python install dir
site-packages/ in order for PyCharm or other lint tools to work properly.
I can do the setup.py work or anything.

What do you think ?

Regards,

Olivier.
"
Burak Yavuz <brkyvz@gmail.com>,"Thu, 4 Jun 2015 23:01:50 -0700",Re: [VOTE] Release Apache Spark 1.4.0 (RC4),Calvin Jia <jia.calvin@gmail.com>,"+1

Tested on Mac OS X

Burak


"
Krishna Sankar <ksankar42@gmail.com>,"Fri, 5 Jun 2015 00:10:34 -0700",Re: [VOTE] Release Apache Spark 1.4.0 (RC4),Patrick Wendell <pwendell@gmail.com>,"+1 (non-binding, of course)

1. Compiled OSX 10.10 (Yosemite) OK Total time: 25:42 min (My brand new
shiny MacBookPro12,1 : 16GB. Inaugurated the machine with compile & test
1.4.0-RC4 !)
     mvn clean package -Pyarn -Dyarn.version=2.6.0 -Phadoop-2.4
-Dha"
Reynold Xin <rxin@databricks.com>,"Fri, 5 Jun 2015 00:18:58 -0700",Re: [VOTE] Release Apache Spark 1.4.0 (RC4),Krishna Sankar <ksankar42@gmail.com>,"Enjoy your new shiny mbp.


"
=?UTF-8?Q?Fran=C3=A7ois_Garillot?= <francois.garillot@typesafe.com>,"Fri, 05 Jun 2015 10:38:47 +0000","Re: Regarding ""Connecting spark to Mesos"" documentation","Meethu Mathew <meethu.mathew@flytxt.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","The make-distribution script will indeed take maven options. If you want to
add this to the documentation, one possibility is that you could supplement
the information in that file :

https://github.com/apache/spark/blob/master/docs/running-on-mesos.md

with a pull-request.

You'll also find contributor guidelines here which should help:
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark


"
Sean Owen <sowen@cloudera.com>,"Fri, 5 Jun 2015 14:19:23 +0200",Re: [VOTE] Release Apache Spark 1.4.0 (RC4),Patrick Wendell <pwendell@gmail.com>,"Everything checks out again, and the tests pass for me on Ubuntu +
Java 7 with '-Pyarn -Phadoop-2.6', except that I always get
SparkSubmitSuite errors like ...

- success sanity check *** FAILED ***
  java.lang.RuntimeException: [download failed:
org.jboss.netty#netty;3.2.2.Final!netty.jar(bundle), download failed:
commons-net#commons-net;3.1!commons-net.jar]
  at org.apache.spark.deploy.SparkSubmitUtils$.resolveMavenCoordinates(SparkSubmit.scala:978)
  at org.apache.spark.sql.hive.client.IsolatedClientLoader$$anonfun$3.apply(IsolatedClientLoader.scala:62)
  ...

I also can't get hive tests to pass. Is anyone else seeing anything
like this? if not I'll assume this is something specific to the env --
or that I don't have the build invocation just right. It's puzzling
since it's so consistent, but I presume others' tests pass and Jenkins
does.



---------------------------------------------------------------------


"
Marcelo Vanzin <vanzin@cloudera.com>,"Fri, 5 Jun 2015 09:42:19 -0700",Re: [VOTE] Release Apache Spark 1.4.0 (RC4),Sean Owen <sowen@cloudera.com>,"

Aside from the separate thread I started, I see errors like these pretty
often when ivy is trying to download too many dependencies at the same time
(even when just starting sbt, for instance). Seems like it doesn't do
throttling very well. Retrying generally fixes these.

-- 
Marcelo
"
Mike Hynes <91mbbh@gmail.com>,"Fri, 5 Jun 2015 12:51:33 -0400",Scheduler question: stages with non-arithmetic numbering,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi folks,

When I look at the output logs for an iterative Spark program, I see
that the stage IDs are not arithmetically numbered---that is, there
are gaps between stages and I might find log information about Stage
0, 1,2, 5, but not 3 or 4.

As an example, the output from the Spark logs below shows what I mean:

# grep -rE ""Stage [[:digit:]]+"" spark_stderr  | grep finished
12048:INFO:DAGScheduler:Stage 0 (mapPartitions at blockMap.scala:1444)
finished in 7.820 s:
15994:INFO:DAGScheduler:Stage 1 (map at blockMap.scala:1810) finished
in 3.874 s:
18291:INFO:DAGScheduler:Stage 2 (count at blockMap.scala:1179)
finished in 2.237 s:
20121:INFO:DAGScheduler:Stage 4 (map at blockMap.scala:1817) finished
in 1.749 s:
21254:INFO:DAGScheduler:Stage 5 (count at blockMap.scala:1180)
finished in 1.082 s:
23422:INFO:DAGScheduler:Stage 7 (map at blockMap.scala:1810) finished
in 2.078 s:
24773:INFO:DAGScheduler:Stage 8 (count at blockMap.scala:1188)
finished in 1.317 s:
26455:INFO:DAGScheduler:Stage 10 (map at blockMap.scala:1817) finished
in 1.638 s:
27228:INFO:DAGScheduler:Stage 11 (count at blockMap.scala:1189)
finished in 0.732 s:
27494:INFO:DAGScheduler:Stage 14 (foreach at blockMap.scala:1302)
finished in 0.192 s:
27709:INFO:DAGScheduler:Stage 17 (foreach at blockMap.scala:1302)
finished in 0.170 s:
28018:INFO:DAGScheduler:Stage 20 (count at blockMap.scala:1201)
finished in 0.270 s:
28611:INFO:DAGScheduler:Stage 23 (map at blockMap.scala:1355) finished
in 0.455 s:
29598:INFO:DAGScheduler:Stage 24 (count at blockMap.scala:274)
finished in 0.928 s:
29954:INFO:DAGScheduler:Stage 27 (map at blockMap.scala:1355) finished
in 0.305 s:
30390:INFO:DAGScheduler:Stage 28 (count at blockMap.scala:275)
finished in 0.391 s:
30452:INFO:DAGScheduler:Stage 32 (first at
MatrixFactorizationModel.scala:60) finished in 0.028 s:
30506:INFO:DAGScheduler:Stage 36 (first at
MatrixFactorizationModel.scala:60) finished in 0.023 s:

Can anyone comment on this being normal behavior? Is it indicative of
faults causing stages to be resubmitted? I also cannot find the
missing stages in any stage's parent List(Stage x, Stage y, ...)

Thanks,
Mike


er
el
st
d
ote:
.
u
d
r
e


-- 
Thanks,
Mike

---------------------------------------------------------------------


"
Yin Huai <yhuai@databricks.com>,"Fri, 5 Jun 2015 10:52:06 -0700",Re: [VOTE] Release Apache Spark 1.4.0 (RC4),Sean Owen <sowen@cloudera.com>,"Sean,

Can you add ""-Phive -Phive-thriftserver"" and try those Hive tests?

Thanks,

Yin


"
Hari Shreedharan <hshreedharan@cloudera.com>,"Fri, 5 Jun 2015 10:55:55 -0700",Re: [VOTE] Release Apache Spark 1.4.0 (RC4),Yin Huai <yhuai@databricks.com>,"+1. Build looks good, ran a couple apps on YARN


Thanks,
Hari


"
Josh Rosen <rosenville@gmail.com>,"Fri, 5 Jun 2015 10:57:25 -0700",Re: PySpark on PyPi,Olivier Girardot <o.girardot@lateral-thoughts.com>,"This has been proposed before:
https://issues.apache.org/jira/browse/SPARK-1267

There's currently tighter coupling between the Python and Java halves of
PySpark than just requiring SPARK_HOME to be set; if we did this, I bet
we'd run into tons of issues when users try to run a newer version of the
Python half of PySpark against an older set of Java components or
vice-versa.


"
Ram Sriharsha <sriharsha.ram@gmail.com>,"Fri, 5 Jun 2015 11:05:31 -0700",Re: [VOTE] Release Apache Spark 1.4.0 (RC4),Hari Shreedharan <hshreedharan@cloudera.com>,"+1 , tested  with hadoop 2.6/ yarn on centos 6.5 after building  w/ -Pyarn
-Phadoop-2.4 -Dhadoop.version=2.6.0 -Phive -Phive-thriftserver and ran a
few SQL tests and the ML examples


"
Kousuke Saruta <sarutak@oss.nttdata.co.jp>,"Sat, 06 Jun 2015 03:38:40 +0900",Re: [VOTE] Release Apache Spark 1.4.0 (RC4),"Burak Yavuz <brkyvz@gmail.com>, Calvin Jia <jia.calvin@gmail.com>","+1
Built on Mac OS X with -Dhadoop.version=2.4.0 -Pyarn -Phive 
-Phive-thriftserver.
Tested on YARN (cluster/client) on CentOS 7.
Also WebUI incuding DAG and Timeline View work.


"
Sandy Ryza <sandy.ryza@cloudera.com>,"Fri, 5 Jun 2015 11:39:01 -0700",Re: [VOTE] Release Apache Spark 1.4.0 (RC4),Ram Sriharsha <sriharsha.ram@gmail.com>,"+1 (non-binding)

Built from source and ran some jobs against a pseudo-distributed YARN
cluster.

-Sandy


"
Bobby Chowdary <bobby.chowdary03@gmail.com>,"Fri, 5 Jun 2015 12:18:23 -0700",Re: [VOTE] Release Apache Spark 1.4.0 (RC4),"""dev@spark.apache.org"" <dev@spark.apache.org>","Not sure if its a blocker but there might be a minor issue with hive
context, there is also a work around

*Works:*

from pyspark.sql import HiveContext

sqlContext = HiveContext(sc)
df = sqlContext.sql(""select * from test.test1"")

*Does not Work:*

 df = sqlContext.table(""test.test1"")

Py4JJavaError: An error occurred while calling o260.table. :
org.apache.spark.sql.catalyst.analysis.NoSuchTableException     at
org.apache.spark.sql.hive.client.ClientInterface$anonfun$getTable$1.apply(ClientInterface.scala:112)
    at org.apache.spark.sql.hive.client.ClientInterface$anonfun$getTable$1.apply(ClientInterface.scala:112)
    at scala.Option.getOrElse(Option.scala:120)     at
org.apache.spark.sql.hive.client.ClientInterface$class.getTable(ClientInterface.scala:112)
    at org.apache.spark.sql.hive.client.ClientWrapper.getTable(ClientWrapper.scala:58)
    at org.apache.spark.sql.hive.HiveMetastoreCatalog.lookupRelation(HiveMetastoreCatalog.scala:227)
    at org.apache.spark.sql.hive.HiveContext$anon$2.org$apache$spark$sql$catalyst$analysis$OverrideCatalog$super$lookupRelation(HiveContext.scala:370)
    at org.apache.spark.sql.catalyst.analysis.OverrideCatalog$anonfun$lookupRelation$3.apply(Catalog.scala:165)
    at org.apache.spark.sql.catalyst.analysis.OverrideCatalog$anonfun$lookupRelation$3.apply(Catalog.scala:165)
    at scala.Option.getOrElse(Option.scala:120)     at
org.apache.spark.sql.catalyst.analysis.OverrideCatalog$class.lookupRelation(Catalog.scala:165)
    at org.apache.spark.sql.hive.HiveContext$anon$2.lookupRelation(HiveContext.scala:370)
    at org.apache.spark.sql.SQLContext.table(SQLContext.scala:754)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)     at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:497)     at
py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)     at
py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)
at py4j.Gateway.invoke(Gateway.java:259)     at
py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)
  at py4j.commands.CallCommand.execute(CallCommand.java:79)     at
py4j.GatewayConnection.run(GatewayConnection.java:207)     at
java.lang.Thread.run(Thread.java:745)  (<class
'py4j.protocol.Py4JJavaError'>, Py4JJavaError(u'An error occurred
while calling o260.table.\n', JavaObject id=o262), <traceback object
at 0x2e248c0>)

How ever which i swtich db context it works

*Works:*

 sqlContext.sql(""use test"")
 df = sqlContext.table(""test1"")

Bulit on Mac OSX  JDK6for Mapr Distribution and Running on CentOS 7.0 JDK8

make-distribution.sh --tgz -Pmapr4  -Phive -Pnetlib-lgpl -Phive-thriftserver

didnâ€™t have this issue in RC3 and tried it on scala as well.

Thanks
Bobby
â€‹
"
Marcelo Vanzin <vanzin@cloudera.com>,"Fri, 5 Jun 2015 13:05:13 -0700",Re: [VOTE] Release Apache Spark 1.4.0 (RC4),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1 (non-binding)

Ran some of our internal test suite (yarn + standalone) against the
hadoop-2.6 and without-hadoop binaries.




-- 
Marcelo
"
Jey Kottalam <jey@cs.berkeley.edu>,"Fri, 5 Jun 2015 13:12:46 -0700",Re: PySpark on PyPi,Josh Rosen <rosenville@gmail.com>,"Couldn't we have a pip installable ""pyspark"" package that just serves as a
shim to an existing Spark installation? Or it could even download the
latest Spark binary if SPARK_HOME isn't set during installation. Right now,
Spark doesn't play very well with the usual Python ecosystem. For example,
why do I need to use a strange incantation when booting up IPython if I
want to use PySpark in a notebook with MASTER=""local[4]""? It would be much
nicer to just type `from pyspark import SparkContext; sc =
SparkContext(""local[4]"")` in my notebook.

I did a test and it seems like PySpark's basic unit-tests do pass when
SPARK_HOME is set and Py4J is on the PYTHONPATH:


PYTHONPATH=$SPARK_HOME/python/:$SPARK_HOME/python/lib/py4j-0.8.2.1-src.zip:$PYTHONPATH
python $SPARK_HOME/python/pyspark/rdd.py

-Jey



"
Anant Chintamaneni <anant.chintamaneni@gmail.com>,"Fri, 5 Jun 2015 14:03:30 -0700","Fwd: Multi-node Docker based *Spark 1.3.1* clusters on
 VirtualBox(Mac)/EC2 instance",dev@spark.apache.org,"Hi All

If you are interested in getting fully configured multi-node distributed
Spark clusters up and running in minutes on VirtualBox (Mac) or a single
Amazon EC2 instance, there is a way. Not only that, you can run Spark jobs
against your local files, any HDFS or even NFS data
www.bluedata.com/free

Perfect for experimentation and learning Spark. You can create and delete a
cluster, ssh into the nodes (Docker Containers), install you software
packages etc.

Here are some screenshots



â€‹



â€‹
"
Yin Huai <yhuai@databricks.com>,"Fri, 5 Jun 2015 14:11:09 -0700",Re: [VOTE] Release Apache Spark 1.4.0 (RC4),Bobby Chowdary <bobby.chowdary03@gmail.com>,"Hi Bobby,

sqlContext.table(""test.test1"") is not officially supported in 1.3. For now,
please use the ""use database"" as a workaround. We will add it.

Thanks,

Yin


park.sql.catalyst.analysis.NoSuchTableException     at org.apache.spark.sql.hive.client.ClientInterface$anonfun$getTable$1.apply(ClientInterface.scala:112)     at org.apache.spark.sql.hive.client.ClientInterface$anonfun$getTable$1.apply(ClientInterface.scala:112)     at scala.Option.getOrElse(Option.scala:120)     at org.apache.spark.sql.hive.client.ClientInterface$class.getTable(ClientInterface.scala:112)     at org.apache.spark.sql.hive.client.ClientWrapper.getTable(ClientWrapper.scala:58)     at org.apache.spark.sql.hive.HiveMetastoreCatalog.lookupRelation(HiveMetastoreCatalog.scala:227)     at org.apache.spark.sql.hive.HiveContext$anon$2.org$apache$spark$sql$catalyst$analysis$OverrideCatalog$super$lookupRelation(HiveContext.scala:370)     at org.apache.spark.sql.catalyst.analysis.OverrideCatalog$anonfun$lookupRelation$3.apply(Catalog.scala:165)     at org.apache.spark.sql.catalyst.analysis.OverrideCatalog$anonfun$lookupRelation$3.apply(Catalog.scala:165)     at scala.Option.getOrElse(Option.scala:120)     at org.apache.spark.sql.catalyst.analysis.OverrideCatalog$class.lookupRelation(Catalog.scala:165)     at org.apache.spark.sql.hive.HiveContext$anon$2.lookupRelation(HiveContext.scala:370)     at org.apache.spark.sql.SQLContext.table(SQLContext.scala:754)     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)     at java.lang.reflect.Method.invoke(Method.java:497)     at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)     at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)     at py4j.Gateway.invoke(Gateway.java:259)     at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)     at py4j.commands.CallCommand.execute(CallCommand.java:79)     at py4j.GatewayConnection.run(GatewayConnection.java:207)     at java.lang.Thread.run(Thread.java:745)  (<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError(u'An error occurred while calling o260.table.\n', JavaObject id=o262), <traceback object at 0x2e248c0>)
8
ver
"
Bobby Chowdary <bobby.chowdary03@gmail.com>,"Fri, 5 Jun 2015 14:41:23 -0700",Re: [VOTE] Release Apache Spark 1.4.0 (RC4),Yin Huai <yhuai@databricks.com>,"Thanks Yin !

every thing else works great!

+1 (non-binding)


spark.sql.catalyst.analysis.NoSuchTableException     at org.apache.spark.sql.hive.client.ClientInterface$anonfun$getTable$1.apply(ClientInterface.scala:112)     at org.apache.spark.sql.hive.client.ClientInterface$anonfun$getTable$1.apply(ClientInterface.scala:112)     at scala.Option.getOrElse(Option.scala:120)     at org.apache.spark.sql.hive.client.ClientInterface$class.getTable(ClientInterface.scala:112)     at org.apache.spark.sql.hive.client.ClientWrapper.getTable(ClientWrapper.scala:58)     at org.apache.spark.sql.hive.HiveMetastoreCatalog.lookupRelation(HiveMetastoreCatalog.scala:227)     at org.apache.spark.sql.hive.HiveContext$anon$2.org$apache$spark$sql$catalyst$analysis$OverrideCatalog$super$lookupRelation(HiveContext.scala:370)     at org.apache.spark.sql.catalyst.analysis.OverrideCatalog$anonfun$lookupRelation$3.apply(Catalog.scala:165)     at org.apache.spark.sql.catalyst.analysis.OverrideCatalog$anonfun$lookupRelation$3.apply(Catalog.scala:165)     at scala.Option.getOrElse(Option.scala:120)     at org.apache.spark.sql.catalyst.analysis.OverrideCatalog$class.lookupRelation(Catalog.scala:165)     at org.apache.spark.sql.hive.HiveContext$anon$2.lookupRelation(HiveContext.scala:370)     at org.apache.spark.sql.SQLContext.table(SQLContext.scala:754)     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)     at java.lang.reflect.Method.invoke(Method.java:497)     at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)     at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)     at py4j.Gateway.invoke(Gateway.java:259)     at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)     at py4j.commands.CallCommand.execute(CallCommand.java:79)     at py4j.GatewayConnection.run(GatewayConnection.java:207)     at java.lang.Thread.run(Thread.java:745)  (<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError(u'An error occurred while calling o260.table.\n', JavaObject id=o262), <traceback object at 0x2e248c0>)
K8
rver
"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Fri, 05 Jun 2015 22:46:49 +0000",Re: PySpark on PyPi,"jey@cs.berkeley.edu, Josh Rosen <rosenville@gmail.com>","Ok, I get it. Now what can we do to improve the current situation, because
right now if I want to set-up a CI env for PySpark, I have to :
1- download a pre-built version of pyspark and unzip it somewhere on every
agent
2- define the SPARK_HOME env
3- symlink this distribution pyspark dir inside the python install dir
site-packages/ directory
and if I rely on additional packages (like databricks' Spark-CSV project),
I have to (except if I'm mistaken)
4- compile/assembly spark-csv, deploy the jar in a specific directory on
every agent
5- add this jar-filled directory to the Spark distribution's additional
classpath using the conf/spark-default file

Then finally we can launch our unit/integration-tests.
Some issues are related to spark-packages, some to the lack of python-based
dependency, and some to the way SparkContext are launched when using
pyspark.
I think step 1 and 2 are fair enough
4 and 5 may already have solutions, I didn't check and considering
spark-shell is downloading such dependencies automatically, I think if
nothing's done yet it will (I guess ?).

For step 3, maybe just adding a setup.py to the distribution would be
enough, I'm not exactly advocating to distribute a full 300Mb spark
distribution in PyPi, maybe there's a better compromise ?

Regards,

Olivier.

Le ven. 5 juin 2015 Ã  22:12, Jey Kottalam <jey@cs.berkeley.edu> a Ã©crit :

a
w,
,
uch
zip:$PYTHONPATH
e
rt
t
y.
"
Patrick Wendell <pwendell@gmail.com>,"Sat, 6 Jun 2015 09:01:05 -0700","[DISCUSS] Minimize use of MINOR, BUILD, and HOTFIX w/ no JIRA","""dev@spark.apache.org"" <dev@spark.apache.org>","Hey All,

Just a request here - it would be great if people could create JIRA's
for any and all merged pull requests. The reason is that when patches
get reverted due to build breaks or other issues, it is very difficult
to keep track of what is going on if there is no JIRA. Here is a list
of 5 patches we had to revert recently that didn't include a JIRA:

    Revert ""[MINOR] [BUILD] Use custom temp directory during build.""
    Revert ""[SQL] [TEST] [MINOR] Uses a temporary log4j.properties in
HiveThriftServer2Test to ensure expected logging behavior""
    Revert ""[BUILD] Always run SQL tests in master build.""
    Revert ""[MINOR] [CORE] Warn users who try to cache RDDs with
dynamic allocation on.""
    Revert ""[HOT FIX] [YARN] Check whether `/lib` exists before
listing its files""

The cost overhead of creating a JIRA relative to other aspects of
development is very small. If it's *really* a documentation change or
something small, that's okay.

But anything affecting the build, packaging, etc. These all need to
have a JIRA to ensure that follow-up can be well communicated to all
Spark developers.

Hopefully this is something everyone can get behind, but opened a
discussion here in case others feel differently.

- Patrick

---------------------------------------------------------------------


"
Mark Hamstra <mark@clearstorydata.com>,"Sat, 6 Jun 2015 19:35:33 -0700",Re: [VOTE] Release Apache Spark 1.4.0 (RC4),Patrick Wendell <pwendell@gmail.com>,1
"""=?ISO-8859-1?B?R3VvcWlhbmcgTGk=?="" <witgo@qq.com>","Sun, 7 Jun 2015 10:55:46 +0800",Re:  [VOTE] Release Apache Spark 1.4.0 (RC4),"""=?ISO-8859-1?B?UGF0cmljayBXZW5kZWxs?="" <pwendell@gmail.com>","+1 (non-binding)




------------------ Original ------------------
From:  ""Reynold Xin"";<rxin@databricks.com>;
Date:  Fri, Jun 5, 2015 03:18 PM
To:  ""Krishna Sankar""<ksankar42@gmail.com>; 
Cc:  ""Patrick Wendell""<pwendell@gmail.com>; ""dev@spark.a"
Akhil Das <akhil@sigmoidanalytics.com>,"Sun, 7 Jun 2015 21:36:07 +0530",Re: Scheduler question: stages with non-arithmetic numbering,Mike Hynes <91mbbh@gmail.com>,"Are you seeing the same behavior on the driver UI? (that running on port
4040), If you click on the stage id header you can sort the stages based on
IDs.

Thanks
Best Regards


iter
's
le
`
be
,
ng
t.
"
Patrick Wendell <pwendell@gmail.com>,"Sun, 7 Jun 2015 13:02:33 -0700",Re: Scheduler question: stages with non-arithmetic numbering,Akhil Das <akhil@sigmoidanalytics.com>,"Hey Mike,

Stage ID's are not guaranteed to be sequential because of the way the
DAG scheduler works (only increasing). In some cases stage ID numbers
are skipped when stages are generated.

Any stage/ID that appears in the Spark UI is an actual stage, so if
you see ID's in there, but they are not in the logs, then let us know
(that would be a bug).

- Patrick

e:
on
riter
e`
e
e
b,
)
d
e

---------------------------------------------------------------------


"
Joseph Bradley <joseph@databricks.com>,"Sun, 7 Jun 2015 15:01:52 -0700",Re: [VOTE] Release Apache Spark 1.4.0 (RC4),dev <dev@spark.apache.org>,1
Tathagata Das <tathagata.das1565@gmail.com>,"Sun, 7 Jun 2015 15:02:37 -0700",Re: [VOTE] Release Apache Spark 1.4.0 (RC4),Joseph Bradley <joseph@databricks.com>,1
Ajay Singal <asingal11@gmail.com>,"Sun, 7 Jun 2015 22:34:04 -0400",Re: [VOTE] Release Apache Spark 1.4.0 (RC4),Tathagata Das <tathagata.das1565@gmail.com>,1
Igor Costa <igorcosta@apache.org>,"Sun, 7 Jun 2015 23:41:26 -0300",Re: createDataframe from s3 results in error,Reynold Xin <rxin@databricks.com>,"Hey there Ignacio

Like Reynold said, It's related to your build of Spark, try to not compile
with Thrift.

Also, try to use this command to see what's the error and link to here.

sc.wholeTextFile(""s3://my-directory/2015*/ignacio/*"")


Ps( Are you using boto to connect? Which version?)


Igor



"
Igor Costa <igorcosta@apache.org>,"Sun, 7 Jun 2015 23:44:59 -0300",Re: Ivy support in Spark vs. sbt,Eron Wright <ewright@live.com>,"Marcelo

I've run this problem once, when I was starting with Spark, like you
mentioned. I found out that ivy get messy with diff sbt version.

My solution was using a previous compatible version with sbt to not cross
with version.


Best
Igor


jar
4.alpha1-SNAPSHOT/dl4j-spark-ml-0.0.3.3.4.alpha1-SNAPSHOT.jar
jar
4.alpha1-SNAPSHOT/dl4j-spark-ml-0.0.3.3.4.alpha1-SNAPSHOT.jar
ache(DefaultRepositoryCacheManager.java:385)
epositoryCacheManager.java:849)
:835)
olver.java:282)
:219)
:219)
.java:388)
$1(Locks.scala:78)
8)
5)
)
)
s.scala:159)
:1142)
a:617)
nk
ts
s
"
Mike Hynes <91mbbh@gmail.com>,"Mon, 8 Jun 2015 00:12:29 -0400",Stages with non-arithmetic numbering & Timing metrics in event logs,Patrick Wendell <pwendell@gmail.com>,"Hi Patrick and Akhil,

Thank you both for your responses. This is a bit of an extended email,
but I'd like to:
1. Answer your (Patrick) note about the ""missing"" stages since the IDs
do (briefly) appear in the event logs
2. Ask for advice/experience with extracting information from the
event logs in a columnar, delimiter-separated format.
3. Ask about the time metrics reported in the event logs; currently,
the elapsed time for a task does not equal the sum of the times for
its components

1. Event Logs + Stages:
=========================

As I said before, In the spark logs (the log4j configurable ones from
the driver), I only see references to some stages, where the stage IDs
are not arithmetically increasing. In the event logs, however, I will
see reference to *every* stage, although not all stages will have
tasks associated with them.

For instance, to examine the actual stages that have tasks, you can
see missing stages:
# grep -E '""Event"":""SparkListenerTaskEnd""' app.log \
#		| grep -Eo '""Stage ID"":[[:digit:]]+'  \
#		| sort -n|uniq | head -n 5
""Stage ID"":0
""Stage ID"":1
""Stage ID"":10
""Stage ID"":11
""Stage ID"":110

However, these ""missing"" stages *do* appear in the event logs as Stage
IDs in the jobs submitted, i.e: for
# grep -E '""Event"":""SparkListenerJobStart""' app.log | grep -Eo 'Stage
IDs"":\[.*\]' | head -n 5
Stage IDs"":[0,1,2]
Stage IDs"":[5,3,4]
Stage IDs"":[6,7,8]
Stage IDs"":[9,10,11]
Stage IDs"":[12,13,14]

I do not know if this amounts to a bug, since I am not familiar with
the scheduler in detail. The stages have seemingly been created
somewhere in the DAG, but then have no associated tasks and never
appear again.

2. Extracting Event Log Information
====================================
Currently we are running scalability tests, and are finding very poor
scalability for certain block matrix algorithms. I would like to have
finer detail about the communication time and bandwidth when data is
transferred between nodes.

I would really just like to have a file with nothing but task info in
a format such as:
timestamp (ms), task ID, hostname, execution time (ms), GC time (ms), ...
0010294, 1, slave-1, 503, 34, ...
0010392, 2, slave-2, 543, 32, ...
and similarly for jobs/stages/rdd_memory/shuffle output/etc.

I have extracted the relevant time fields from the spark event logs
with a sed script, but I wonder if there is an even more expedient
way. Unfortunately, I do not immediately see how to do this using the
$SPARK_HOME/conf/metrics.properties file and haven't come across a
blog/etc that describes this. Could anyone please comment on whether
or not a metrics configuation for this already exists?

3. Time Metrics in Spark Event Log
==================================
I am confused about the times reported for tasks in the event log.
There are launch and finish timestamps given for each task (call them
t1 and t2, respectively), as well as GC time (t_gc), execution time
(t_exec), and serialization times (t_ser, t_deser). However the times
do not add up as I would have expected. I would imagine that the
elapsed time t2 - t1 would be slightly larger than the sum of the
component times. However, I can find many instances in the event logs
where:
(t2 - t1) < (t_gc + t_ser + t_deser + t_exec)
The difference can be 500 ms or more, which is not negligible for my
current execution times of ~5000 ms. I have attached a plot that
illustrates this.

Regarding this, I'd like to ask:
1. How exactly are these times are being measured?
2. Should the sum of the component times equal the elapsed (clock)
time for the task?
3. If not, which component(s) is(are) being excluded, and when do they occur?
4. There are occasionally reported measurements for Shuffle Write
time, but not shuffle read time. Is there a method to determine the
time required to shuffle data? Could this be done by look at delays
between the first task in a new stage and the last task in the
previous stage?

Thank you very much for your time,
Mike


writer
!
;
l
a
""
n
k
s


-- 
Thanks,
Mike

---------------------------------------------------------------------"
invkrh <invkrh@gmail.com>,"Mon, 8 Jun 2015 06:33:31 -0700 (MST)",[SparkSQL ] What is Exchange in physical plan for ?,dev@spark.apache.org,"Hi,

DataFrame.explain() shows the physical plan of a query. I noticed there are
a lot of `Exchange`s in it, like below:

Project [period#20L,categoryName#0,regionName#10,action#15,list_id#16L]
 ShuffledHashJoin [region#18], [regionCode#9], BuildRight
  Exchange (HashPartitioning [region#18], 12)
   Project [categoryName#0,list_id#16L,period#20L,action#15,region#18]
    ShuffledHashJoin [refCategoryID#3], [category#17], BuildRight
     Exchange (HashPartitioning [refCategoryID#3], 12)
      Project [categoryName#0,refCategoryID#3]
       PhysicalRDD
[categoryName#0,familyName#1,parentRefCategoryID#2,refCategoryID#3],
MapPartitionsRDD[5] at mapPartitions at SQLContext.scala:439
     Exchange (HashPartitioning [category#17], 12)
      Project [timestamp_sec#13L AS
period#20L,category#17,region#18,action#15,list_id#16L]
       PhysicalRDD
[syslog#12,timestamp_sec#13L,timestamp_usec#14,action#15,list_id#16L,category#17,region#18,expiration_time#19],
MapPartitionsRDD[16] at map at SQLContext.scala:394
  Exchange (HashPartitioning [regionCode#9], 12)
   Project [regionName#10,regionCode#9]
    PhysicalRDD
[cityName#4,countryCode#5,countryName#6,dptCode#7,dptName#8,regionCode#9,regionName#10,zipCode#11],
MapPartitionsRDD[11] at mapPartitions at SQLContext.scala:439

I find also its class:
https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/Exchange.scala.

So what does it mean ? 

Thank you.

Hao.



--

---------------------------------------------------------------------


"
"""Cheng, Hao"" <hao.cheng@intel.com>","Mon, 8 Jun 2015 14:34:35 +0000",RE: [SparkSQL ] What is Exchange in physical plan for ?,"invkrh <invkrh@gmail.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","It means the data shuffling, and its arguments also show the partitioning strategy.


DataFrame.explain() shows the physical plan of a query. I noticed there are a lot of `Exchange`s in it, like below:

Project [period#20L,categoryName#0,regionName#10,action#15,list_id#16L]
 ShuffledHashJoin [region#18], [regionCode#9], BuildRight
  Exchange (HashPartitioning [region#18], 12)
   Project [categoryName#0,list_id#16L,period#20L,action#15,region#18]
    ShuffledHashJoin [refCategoryID#3], [category#17], BuildRight
     Exchange (HashPartitioning [refCategoryID#3], 12)
      Project [categoryName#0,refCategoryID#3]
       PhysicalRDD
[categoryName#0,familyName#1,parentRefCategoryID#2,refCategoryID#3],
MapPartitionsRDD[5] at mapPartitions at SQLContext.scala:439
     Exchange (HashPartitioning [category#17], 12)
      Project [timestamp_sec#13L AS
period#20L,category#17,region#18,action#15,list_id#16L]
       PhysicalRDD
[syslog#12,timestamp_sec#13L,timestamp_usec#14,action#15,list_id#16L,category#17,region#18,expiration_time#19],
MapPartitionsRDD[16] at map at SQLContext.scala:394
  Exchange (HashPartitioning [regionCode#9], 12)
   Project [regionName#10,regionCode#9]
    PhysicalRDD
[cityName#4,countryCode#5,countryName#6,dptCode#7,dptName#8,regionCode#9,regionName#10,zipCode#11],
MapPartitionsRDD[11] at mapPartitions at SQLContext.scala:439

I find also its class:
https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/Exchange.scala.

So what does it mean ? 

Thank you.

Hao.



--
3.nabble.com/SparkSQL-What-is-Exchange-in-physical-plan-for-tp12659.html
om.

---------------------------------------------------------------------
mands, e-mail: dev-help@spark.apache.org


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Mon, 8 Jun 2015 08:44:48 -0700",Re: [VOTE] Release Apache Spark 1.4.0 (RC4),"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi All,

Thanks for the continued voting! I'm going to leave this thread open
for another few days to continue to collect feedback.

- Patrick


---------------------------------------------------------------------


"
Peter Rudenko <petro.rudenko@gmail.com>,"Mon, 8 Jun 2015 19:17:44 +0300",[ml] Why all model classes are final?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi, previously all the models in ml package were private to package, so 
if i need to customize some models i inherit them in org.apache.spark.ml 
package in my project. But now new models 
(https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/classification/GBTClassifier.scala#L46) 
are final classes. So if i need to customize 1 line or so, i need to 
redefine the whole class. Any reasons to do so? As a developer,i 
understand all the risks of using Developer/Alpha API. That's why i'm 
using spark, because it provides a building blocks that i could easily 
customize and combine for my need.

Thanks,
Peter Rudenko

---------------------------------------------------------------------


"
Eron Wright <ewright@live.com>,"Mon, 8 Jun 2015 09:20:03 -0700",[sample code] deeplearning4j for Spark ML (@DeveloperAPI),"""dev@spark.apache.org"" <dev@spark.apache.org>","
The deeplearning4j framework provides a variety of distributed, neural network-based learning algorithms, including convolutional nets, deep auto-encoders, deep-belief nets, and recurrent nets.      Weâ€™re working on integration with the Spark ML pipeline, leveraging the developer API.   This announcement is to share some code and get feedback from the Spark community.

The integration code is located in the dl4j-spark-ml module in the deeplearning4j repository.

Major aspects of the integration work:
ML algorithms.  To bind the dl4j algorithms to the ML pipeline, we developed a new classifier and a new unsupervised learning estimator.   
ML attributes. We strove to interoperate well with other pipeline components.   ML Attributes are column-level metadata enabling information sharing between pipeline components.    See here how the classifier reads label metadata from a column provided by the new StringIndexer.
Large binary data.  It is challenging to work with large binary data in Spark.   An effective approach is to leverage PrunedScan and to carefully control partition sizes.  Here we explored this with a custom data source based on the new relation API.   
Column-based record readers.  Here we explored how to construct rows from a Hadoop input split by composing a number of column-level readers, with pruning support.
UDTs.   With Spark SQL it is possible to introduce new data types.   We prototyped an experimental Tensor type, here.
Spark Package.   We developed a spark package to make it easy to use the dl4j framework in spark-shell and with spark-submit.      See the deeplearning4j/dl4j-spark-ml repository for useful snippets involving the sbt-spark-package plugin.
Example code.   Examples demonstrate how the standardized ML API simplifies interoperability, such as with label preprocessing and feature scaling.   See the deeplearning4j/dl4j-spark-ml-examples repository for an expanding set of example pipelines.
Hope this proves useful to the community as we transition to exciting new concepts in Spark SQL and Spark ML.   Meanwhile, we have Spark working with multiple GPUs on AWS and we're looking forward to optimizations that will speed neural net training even more. 

Eron Wright
Contributor | deeplearning4j.org


"
Imran Rashid <irashid@cloudera.com>,"Mon, 8 Jun 2015 11:20:58 -0500",Re: Stages with non-arithmetic numbering & Timing metrics in event logs,Mike Hynes <91mbbh@gmail.com>,"Hi Mike,

all good questions, let me take a stab at answering them:

1. Event Logs + Stages:

Its normal for stages to get skipped if they are shuffle map stages, which
get read multiple times.  Eg., here's a little example program I wrote
earlier to demonstrate this: ""d3"" doesn't need to be re-shuffled since each
time its read w/ the same partitioner.  So skipping stages in this way is a
good thing:

val partitioner = new org.apache.spark.HashPartitioner(10)
val d3 = sc.parallelize(1 to 100).map { x => (x % 10) ->
x}.partitionBy(partitioner)
(0 until 5).foreach { idx =>
  val otherData = sc.parallelize(1 to (idx * 100)).map{ x => (x % 10) -x}.partitionBy(partitioner)
  println(idx + "" ---> "" + otherData.join(d3).count())
}

If you run this, f you look in the UI you'd see that all jobs except for
the first one have one stage that is skipped.  You will also see this in
the log:

15/06/08 10:52:37 INFO DAGScheduler: Parents of final stage: List(Stage 12,
Stage 13)

15/06/08 10:52:37 INFO DAGScheduler: Missing parents: List(Stage 13)

Admittedly that is not very clear, but that is sort of indicating to you
that the DAGScheduler first created stage 12 as a necessary step, and then
later on changed its mind by realizing that everything it needed for stage
12 already existed, so there was nothing to do.


2. Extracting Event Log Information

maybe you are interested in SparkListener ? Though unfortunately, I don't
know of a good blog post describing it, hopefully the docs are clear ...

3. Time Metrics in Spark Event Log

This is a great question.  I *think* the only exception is that t_gc is
really overlapped with t_exec.  So I think you should really expect

(t2 - t1) < (t_ser + t_deser + t_exec)

I am not 100% sure about this, though.  I'd be curious if that was
constraint was ever violated.


As for your question on shuffle read vs. shuffle write time -- I wouldn't
necessarily expect the same stage to have times for both shuffle read &
shuffle write -- in the simplest case, you'll have shuffle write times in
one, and shuffle read times in the next one.  But even taking that into
account, there is a difference in the way they work & are measured.
 shuffle read operations are pipelined and the way we measure shuffle read,
its just how much time is spent *waiting* for network transfer.  It could
be that there is no (measurable) wait time b/c the next blocks are fetched
before they are needed.  Shuffle writes occur in the normal task execution
thread, though, so we (try to) measure all of it.



=
============
==========
rt
ed
:
)
d
d
d
e
,
om>
r
g
o
"
"""Eskilson,Aleksander"" <Alek.Eskilson@Cerner.com>","Mon, 8 Jun 2015 20:38:34 +0000",SparkR Reading Tables from Hive,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi there,

I’m testing out the new SparkR-Hive interop right now. I’m noticing an apparent disconnect between the Hive store I have my data loaded and the store that sparkRHIve.init() connects to. For example, in beeline:

0: jdbc:hive2://quickstart.cloudera:10000> show databases;
+---------------+--+
| database_name |
+---------------+--+
| default       |
+---------------+--+
0: jdbc:hive2://quickstart.cloudera:10000> show tables;
+---------------+--+
| tab_name      |
+---------------+--+
| my_table      |
+---------------+--+

But in sparkR:

+---------+
| result  |
+---------+
| default |
+---------+
+-----------+-------------+
+ tableName | isTemporary |
+-----------+-------------+
+-----------+-------------+
+-----------+-------------+
+ tableName | isTemporary |
+-----------+-------------+
+-----------+-------------+

The data in my_table was landed into Hive from a CSV via kite-dataset. The installation of Spark I’m working with was built separately, and operates as standalone. Could it be that sparkRHive.init() is getting the wrong address of the Hive metastore? How could I peer into the context and see what the address is set to, and if it’s wrong, reset it?

Ultimately, I’d like to be able to read my_table from Hive into a SparkR DataFrame which ought to be possible with
But this fails with:
org.apache.spark.sql.AnalysisException: no such table my_table; line 1 pos 14
which is expected, I suppose, since we don’t see the table in the listing above.

Any thoughts?

Thanks,
Alek Eskilson

CONFIDENTIALITY NOTICE This message and any included attachments are from Cerner Corporation and are intended only for the addressee. The information contained in this message is confidential and may constitute inside or non-public information under international, federal, or state securities laws. Unauthorized forwarding, printing, copying, distribution, or use of such information is strictly prohibited and may be unlawful. If you are not the addressee, please promptly delete this message and notify the sender of the delivery error by e-mail or you may call Cerner's corporate offices in Kansas City, Missouri, U.S.A at (+1) (816)221-1024.
"
"""Eskilson,Aleksander"" <Alek.Eskilson@Cerner.com>","Mon, 8 Jun 2015 20:57:05 +0000",Re: SparkR Reading Tables from Hive,"""Eskilson,Aleksander"" <Alek.Eskilson@Cerner.com>,
        ""dev@spark.apache.org""
	<dev@spark.apache.org>","Resolved, my hive-site.xml wasn’t in the conf folder. I can load tables into DataFrames as expected.

Thanks,
Alek

From: <Eskilson>, Aleksander Eskilson <Alek.Eskilson@cerner.com<mailto:Alek.Eskilson@cerner.com>>
Date: Monday, June 8, 2015 at 3:38 PM
To: ""dev@spark.apache.org<mailto:dev@spark.apache.org>"" <dev@spark.apache.org<mailto:dev@spark.apache.org>>
Subject: SparkR Reading Tables from Hive

Hi there,

I’m testing out the new SparkR-Hive interop right now. I’m noticing an apparent disconnect between the Hive store I have my data loaded and the store that sparkRHIve.init() connects to. For example, in beeline:

0: jdbc:hive2://quickstart.cloudera:10000> show databases;
+---------------+--+
| database_name |
+---------------+--+
| default       |
+---------------+--+
0: jdbc:hive2://quickstart.cloudera:10000> show tables;
+---------------+--+
| tab_name      |
+---------------+--+
| my_table      |
+---------------+--+

But in sparkR:

+---------+
| result  |
+---------+
| default |
+---------+
+-----------+-------------+
+ tableName | isTemporary |
+-----------+-------------+
+-----------+-------------+
+-----------+-------------+
+ tableName | isTemporary |
+-----------+-------------+
+-----------+-------------+

The data in my_table was landed into Hive from a CSV via kite-dataset. The installation of Spark I’m working with was built separately, and operates as standalone. Could it be that sparkRHive.init() is getting the wrong address of the Hive metastore? How could I peer into the context and see what the address is set to, and if it’s wrong, reset it?

Ultimately, I’d like to be able to read my_table from Hive into a SparkR DataFrame which ought to be possible with
But this fails with:
org.apache.spark.sql.AnalysisException: no such table my_table; line 1 pos 14
which is expected, I suppose, since we don’t see the table in the listing above.

Any thoughts?

Thanks,
Alek Eskilson
CONFIDENTIALITY NOTICE This message and any included attachments are from Cerner Corporation and are intended only for the addressee. The information contained in this message is confidential and may constitute inside or non-public information under international, federal, or state securities laws. Unauthorized forwarding, printing, copying, distribution, or use of such information is strictly prohibited and may be unlawful. If you are not the addressee, please promptly delete this message and notify the sender of the delivery error by e-mail or you may call Cerner's corporate offices in Kansas City, Missouri, U.S.A at (+1) (816)221-1024.
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Mon, 8 Jun 2015 13:59:41 -0700",Re: SparkR Reading Tables from Hive,"""Eskilson,Aleksander"" <Alek.Eskilson@cerner.com>","Thanks for the confirmation - I was just going to send a pointer to the
documentation that talks about hive-site.xml.
http://people.apache.org/~pwendell/spark-releases/latest/sql-programming-guide.html#hive-tables

Thanks
Shivaram


 tables
™m noticing an
and
d
a SparkR
s
 listing
on
ot
of
n
"
"""Wang, Daoyuan"" <daoyuan.wang@intel.com>","Tue, 9 Jun 2015 00:50:41 +0000",RE: [VOTE] Release Apache Spark 1.4.0 (RC4),"Patrick Wendell <pwendell@gmail.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","+1

-----Original Message-----
From: Patrick Wendell [15 1:47 PM
To: dev@spark.apache.org
Subject: Re: [VOTE] Release Apache Spark 1.4.0 (RC4)

He all - a tiny nit from the last e-mail. The tag is v1.4.0-rc4. The exact commit and all other informat"
Denny Lee <denny.g.lee@gmail.com>,"Tue, 09 Jun 2015 00:56:19 +0000",Re: [VOTE] Release Apache Spark 1.4.0 (RC4),"""Wang, Daoyuan"" <daoyuan.wang@intel.com>, Patrick Wendell <pwendell@gmail.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>",1
Reynold Xin <rxin@databricks.com>,"Mon, 8 Jun 2015 17:59:40 -0700","Fwd: pull requests no longer closing by commit messages with ""closes #xxxx""","""dev@spark.apache.org"" <dev@spark.apache.org>","FYI.

---------- Forwarded message ----------
From: John Greet (GitHub Staff) <support@github.com>
Date: Mon, Jun 8, 2015 at 5:50 PM
Subject: Re: pull requests no longer closing by commit messages with
""closes #xxxx""
To: Reynold Xin <rxin@databricks.com>


Hi Reynold,

The problem here is that the commits closing those pull requests were
fetched by our mirroring process, which doesn't have permission to close
issues, instead of pushed by a user in the apache GitHub organization.

Usually the repository receives regular, I assume automated, pushes to its
master branch, but there was a gap in those pushes between 2pm PDT on June
5th and 1:16 PM PDT June 7th. This happened at least once before back in
November. Now that those pushes have resumed pull requests are closing
normally once again.

Let us know if you have any other questions.

Cheers,
John



the data space). We use GitHub as the primary way to accept contributions.
We use a custom merge script to merge pull requests rather than GitHub's
merge button in order to preserve a linear commit history. Part of the
merge script relies on the ""closes #xxxx"" feature to close the
corresponding pull requests.
even if the commits are merged with the message ""closes #xxxx"". Here are
two recent examples:
"
saurfang <forest.fang@outlook.com>,"Mon, 8 Jun 2015 19:49:57 -0700 (MST)",Re: [VOTE] Release Apache Spark 1.4.0 (RC4),dev@spark.apache.org,"+1

Build for Hadoop 2.4. Run a few jobs on YARN and tested spark.sql.unsafe
whose performance seems great!



--

---------------------------------------------------------------------


"
Devl Devel <devl.development@gmail.com>,"Tue, 9 Jun 2015 10:28:49 +0100",Recreating JIRA SPARK-8142,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi All

We are having some trouble with:

sparkConf.set(""spark.driver.userClassPathFirst"",""true"");
sparkConf.set(""spark.executor.userClassPathFirst"",""true"");

and would appreciate some independent verification. The issue comes down to
this:

Spark 1.3.1 hadoop 2.6 is deployed on the cluster. In my application code I
use maven to bring in:

hadoop-common 2.6.0 - provided
hadoop-client 2.6.0 provided
hadoop -hdfs 2.6.0 provided
spark-sql_s.10 provided
spark-core_2.10 provided
hbase-client 1.1.0 included.packaged
hbase -protocol 1.1.0 included/packaged
hbase -server 1.1.0 included/packaged

When I set userClasspath* to true I get a ClassCastException: Full details
are in

https://issues.apache.org/jira/browse/SPARK-8142

Can someone help verify this? I.e. If you have spark 1.3.1, Hadoop and
Hbase can you create a simple Spark job say to read an HBase table into a
RDD. Then set the Spark and Hadoop dependencies above as ""provided"" and
then set:

sparkConf.set(""spark.driver.userClassPathFirst"",""true"");
sparkConf.set(""spark.executor.userClassPathFirst"",""true"");

and repeat the job.

Do you get the same exception in the JIRA or missing classes or event run
into this https://issues.apache.org/jira/browse/SPARK-1867?

Please comment on the JIRA, it would be useful to have a second
verification of this. I know userClasspath* options are experimental but
it's good to know what's going on.

Cheers
Devl
"
"""jinkui.sjk"" <jinkui.sjk@alibaba-inc.com>","Tue, 9 Jun 2015 18:54:58 +0800",Re: GraphX: New graph operator,Reynold Xin <rxin@databricks.com>,"i have a pull request about this issue. https://github.com/apache/spark/pull/6685 <https://github.com/apache/spark/pull/6685>
the union operation of two graph is useful in practice. And itâ€™s necessary to provide operation on the Graph level.  

me it'd be super easy to express a graph as two DataFrames: one for edges (srcid, dstid, and other edge attributes) and one for vertices (vid, and other vertex attributes).
SQL? Union all would be more or less trivial if we just concatenate the vertices and edges (vertex Id conflicts have to be resolved). Should union look for duplicates on the actual attribute (VD) or just the vertex Id? If it compares the attribute it might be necessary to change the id of some vertices in order to resolve conflicts. 
difference, as long as they have an efficient distributed implementation and are plausibly useful.
be best to put them into GraphOps to keep the core GraphX implementation small. The `mask` operation should actually be in GraphOps -- it's only to be in GraphImpl for performance: it accesses EdgeRDDImpl#filter(epred, vpred), which can't be a public EdgeRDD method because its semantics rely on an implementation detail (vertex replication).
Sean Owen recommended to check first with the mailing list, if this is interesting or not.
the operators like:
<http://en.wikipedia.org/wiki/Graph_operations> and/or
<http://techieme.in/complex-graph-operations/> 
etc.) or as external library? My feeling is that they are similar to mask. Because of consistency they should be part of the graph implementation itself.
and help to implement some of these.

"
Sean McNamara <Sean.McNamara@Webtrends.com>,"Tue, 9 Jun 2015 15:52:12 +0000",Re: [VOTE] Release Apache Spark 1.4.0 (RC4),Patrick Wendell <pwendell@gmail.com>,"+1

tested /w OS X + deployed one of our streaming apps onto a staging yarn cluster.

Sean

1.4.0!


---------------------------------------------------------------------


"
Bharath Ravi Kumar <reachbach@gmail.com>,"Wed, 10 Jun 2015 09:40:04 +0530",Re: Spark on Mesos vs Yarn,"=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>, 
	Tim Chen <tim@mesosphere.io>, dev@spark.apache.org","All,

Despite the common origin of spark & mesos, the stability and adoption of
mesos, and the age of the spark-mesos binding, I find the mesos support
less mature, with fundamental shortcomings (like framework auth
<https://issues.apache.org/jira/browse/SPARK-6284>) remaining unresolved.
If there's shortage of developer time, I'd be glad to contribute, but it's
unclear if the committer group has sufficient time (and priority) to take
the mesos support forward. While it has been stated often that support for
mesos & yarn are equally important, that doesn't seem to translate to
visible progress. I'd be glad if my observation is incorrect as I seek
better focus and long term commitment on the mesos support.
As for the specific issue (6284), I'm happy to build, testing & eventually
deploy the patch in our production cluster, but I'd rather see it becoming
mainstream.
Thanks for your consideration.

-Bharath



g
e
fe.com
f
i
"
Niklas Nielsen <niklas@mesosphere.io>,"Tue, 9 Jun 2015 21:51:09 -0700",Re: Spark on Mesos vs Yarn,Bharath Ravi Kumar <reachbach@gmail.com>,"Hi Bharath (and rest of Spark dev list!),

Just a small shout out: I am a Apache Mesos Committer and would love to
help out with anything you need to get this going.

Cheers,
Nik


s
r
y
g
ng
be
-
"
Timothy Chen <tnachen@gmail.com>,"Tue, 9 Jun 2015 22:11:35 -0700",Re: Spark on Mesos vs Yarn,Niklas Nielsen <niklas@mesosphere.io>,"Hi Nik,

Bharath is mostly referring to Spark commiters in this thread.

Tim

:
elp
f
less
(and
ten
o
t as
ly
ng
ing
n for
a
y
t
d
uti
--

---------------------------------------------------------------------


"
Mike Hynes <91mbbh@gmail.com>,"Wed, 10 Jun 2015 01:17:02 -0400",Re: Stages with non-arithmetic numbering & Timing metrics in event logs,Imran Rashid <irashid@cloudera.com>,"Hi Imran,

Thank you for your email.

In examing the condition (t2 - t1) < (t_ser + t_deser + t_exec), I
have found it to be true, although I have not included the
t_{wait_for_read} in this, since it is---so far as I can tell---been
either zero or negligible compared to the task time.

Thanks,
Mike

h
ch
 a
 ->
2,
n
e
d,
d
n
=
============
.
==========
d
d
d
f
com>
s
t
e
-


-- 
Thanks,
Mike

---------------------------------------------------------------------


"
Mike Hynes <91mbbh@gmail.com>,"Wed, 10 Jun 2015 01:18:53 -0400",Re: Stages with non-arithmetic numbering & Timing metrics in event logs,Imran Rashid <irashid@cloudera.com>,"Ahhh---forgive my typo: what I mean is,
(t2 - t1) >= (t_ser + t_deser + t_exec)
is satisfied, empirically.

s
) ->
t
t
n
d
==
=============
===========
:
e
e
n
.com>
e
--


-- 
Thanks,
Mike

---------------------------------------------------------------------


"
"""wangtao (A)"" <wangtao111@huawei.com>","Wed, 10 Jun 2015 05:55:13 +0000",About akka used in spark,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi guys,

I see group id of akka used in spark is ""org.spark-project.akka"". What is its difference with the typesafe one? What is its version? And where can we get the source code?

Regards.
"
Tao Wang <wangtao111@huawei.com>,"Tue, 9 Jun 2015 23:04:00 -0700 (MST)",=?UTF-8?Q?=E7=AD=94=E5=A4=8D:_[VOTE]_Release_Apache_Spark_1.4.0_(RC4)?=,dev@spark.apache.org,"+1

Tested with building with Hadoop 2.7.0 and running with tests:

WordCount in yarn-client/yarn-cluster mode works fine;
Basic sql queries are passed;
â€œspark.sql.autoBroadcastJoinThresholdâ€ works fine;
Thrift Server is fine;
Running streaming with k"
Akhil Das <akhil@sigmoidanalytics.com>,"Wed, 10 Jun 2015 12:51:02 +0530",Re: About akka used in spark,"""wangtao (A)"" <wangtao111@huawei.com>","If you look at the maven repo, you can see its from typesafe only
http://mvnrepository.com/artifact/org.spark-project.akka/akka-actor_2.10/2.3.4-spark

For sbt, you can download the sources by adding withSources() like:

libraryDependencies += ""org.spark-project.akka"" % ""akka-actor_2.10"" %
""2.3.4-spark"" withSources() withJavadoc()



Thanks
Best Regards

:

â€. What is
we
"
Cheng Lian <lian.cs.zju@gmail.com>,"Wed, 10 Jun 2015 16:00:37 +0800",Re: About akka used in spark,"""wangtao (A)"" <wangtao111@huawei.com>, 
 ""dev@spark.apache.org"" <dev@spark.apache.org>","We only shaded protobuf dependencies because of compatibility issues. 
The source code is not modified.


"
Mike Hynes <91mbbh@gmail.com>,"Wed, 10 Jun 2015 14:41:44 -0400",Re: Stages with non-arithmetic numbering & Timing metrics in event logs,Imran Rashid <irashid@cloudera.com>,"Hi Imran,

Thank you again for your email.

I just want to ask one further question to clarify the implementation
of the shuffle block fetches. When you say that rather than sitting
idle, [the executor] will immediately start reading the local block, I
would guess that, in implementation, the executor is going to launch
concurrent threads to read both local and remote blocks, which it
seems to do in the initialize() method of
core/.../storage/ShuffleBlockFetcherIterator.scala. Is that the case
or would the Executor run all local fetch threads first?

The reason I ask is that if the slave machine on which the Executor is
running has some number of cores, c, then I  would have thought that
some of the threads launched would occupy some number, c_1, of the
cores and conduct the local reads (where c_1 <= c). The other threads
would occupy the other (c - c_1) cores' cycles until *all* necessary
blocks have been read, and depending on c and the number of blocks to
fetch so that none of the cores are idle if there are many blocks to
fetch. (I monitor the CPU utilization of our nodes throughout a job,
and generally find them under-utilized statistically speaking; that
is, their usage over the whole job is lower than expected, with short
burst of high usage, so I ask this question in a specific way for this
reason, since I can see trends in the probability density functions of
CPU utilization as the #partitions of our RDDs are increased).

ShuffleBlockFetcherIterator.scala:

  private[this] def initialize(): Unit = {
		...
    // Send out initial requests for blocks, up to our maxBytesInFlight
    while (fetchRequests.nonEmpty &&
      (bytesInFlight == 0 || bytesInFlight + fetchRequests.front.size
<= maxBytesInFlight)) {
      sendRequest(fetchRequests.dequeue())
    }
    val numFetches = remoteRequests.size - fetchRequests.size
    logInfo(""Started "" + numFetches + "" remote fetches in"" +
Utils.getUsedTimeMs(startTime))

    // Get Local Blocks
    fetchLocalBlocks()
    logDebug(""Got local blocks in "" + Utils.getUsedTimeMs(startTime))
  }
  private[this] def fetchLocalBlocks() {
    val iter = localBlocks.iterator
    while (iter.hasNext) {
      val blockId = iter.next()
      try {
        val buf = blockManager.getBlockData(blockId)
        shuffleMetrics.incLocalBlocksFetched(1)
        shuffleMetrics.incLocalBytesRead(buf.size)
        buf.retain()
        results.put(new SuccessFetchResult(blockId, 0, buf))
      } catch {
				...
      }
    }
  }

Obviously, I will have to sit down with core/.../network/nio/* and
core/.../shuffle/* and do my own homework on this, but from what I can
tell, the BlockDataManager relies on either
NioBlockTransferService.scala or the NettyBlockTransferService.scala
(which are set in SparkEnv.scala), both of which do the grunt work of
actually buffering and transferring the blocks' bytes. Finally, the
tasks in new stage for which the shuffle outputs have been fetched
will not commence until all of the block fetching threads (both local
and remote) have terminated.

Does the above paint an accurate picture? I would really appreciate
clarification on the concurrency, since I would like to determine why
our jobs have under-utilization and poor weak scaling efficiency.

I will cc this thread over to the dev list. I did not cc them in case
my previous question was trivial---I didn't want to spam the list
unnecessarily, since I do not see these kinds of questions posed there
frequently.

Thanks a bunch,
Mike


ed
l
s
e
t
e
n,
% 10)
s
d
d
e
:
,
===
h
==============
s
r
============
y
I
I
)
e
e
e
d
s
f
.
--


-- 
Thanks,
Mike

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Wed, 10 Jun 2015 12:13:20 -0700",[RESULT] [VOTE] Release Apache Spark 1.4.0 (RC4),"""dev@spark.apache.org"" <dev@spark.apache.org>","This vote passes! Thanks to everyone who voted. I will get the release
artifacts and notes up within a day or two.

+1 (23 votes):
Reynold Xin*
Patrick Wendell*
Matei Zaharia*
Andrew Or*
Timothy Chen
Calvin Jia
Burak Yavuz
Krishna Sankar
Hari Shreedharan
Ram Sriharsha*
Kousuke Saruta
Sandy Ryza
Marcelo Vanzin
Bobby Chowdary
Mark Hamstra
Guoqiang Li
Joseph Bradley
Sean McNamara
Tathagata Das*
Ajay Singal
Wang, Daoyuan
Denny Lee
Forest Fang

0:

-1:

* Binding


---------------------------------------------------------------------


"
Ashwin Shankar <ashwinshankar77@gmail.com>,"Wed, 10 Jun 2015 13:43:04 -0700",Problem with pyspark on Docker talking to YARN cluster,"""dev@spark.apache.org"" <dev@spark.apache.org>, ""user@spark.apache.org"" <user@spark.apache.org>","All,
I was wondering if any of you have solved this problem :

I have pyspark(ipython mode) running on docker talking to
a yarn cluster(AM/executors are NOT running on docker).

When I start pyspark in the docker container, it binds to port *49460.*

with the following error message :
*ERROR yarn.ApplicationMaster: Failed to connect to driver at :49460*

This makes sense because AM is trying to talk to container directly and
it cannot, it should be talking to the docker host instead.

*Question* :
How do we make Spark AM talk to host1:port1 of the docker host(not the
container), which would then
route it to container which is running pyspark on host2:port2 ?

hostA:portA), and before submitting the app to yarn, we could
reset driver's host/port to hostmachine's ip/port. So the AM can then talk
hostmachine's ip/port, which would be mapped
to the container.

Thoughts ?
-- 
Thanks,
Ashwin
"
=?UTF-8?Q?Grega_Ke=C5=A1pret?= <grega@celtra.com>,"Wed, 10 Jun 2015 14:53:44 -0700","Re: Approximate rank-based statistics (median, 95-th percentile,
 etc.) for Spark",Reynold Xin <rxin@databricks.com>,"I have some time to work on it now. What's a good way to continue the
discussions before coding it?

This e-mail list, JIRA or something else?


ep1&type=pdf
ote:
ry.
The
"
Reynold Xin <rxin@databricks.com>,"Wed, 10 Jun 2015 14:54:46 -0700","Re: Approximate rank-based statistics (median, 95-th percentile,
 etc.) for Spark",=?UTF-8?Q?Grega_Ke=C5=A1pret?= <grega@celtra.com>,"This email is good. Just one note -- a lot of people are swamped right
before Spark Summit, so you might not get prompt responses this week.


e:

I
e
rep1&type=pdf
rote:
ory.
 The
"
Eron Wright <ewright@live.com>,"Wed, 10 Jun 2015 15:55:39 -0700",RE: Problem with pyspark on Docker talking to YARN cluster,"Ashwin Shankar <ashwinshankar77@gmail.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>, ""user@spark.apache.org"" <user@spark.apache.org>","Options include:use 'spark.driver.host' and 'spark.driver.port' setting to stabilize the driver-side endpoint.  (ref)use host networking for your container, i.e. ""docker run --net=host ...""use yarn-cluster mode (see SPARK-5162)
Hope this helps,Eron

Date: Wed, 10 Jun 2015 13:43:04 -0700
Subject: Problem with pyspark on Docker talking to YARN cluster
From: ashwinshankar77@gmail.com
To: dev@spark.apache.org; user@spark.apache.org

All,I was wondering if any of you have solved this problem :
I have pyspark(ipython mode) running on docker talking toa yarn cluster(AM/executors are NOT running on docker).
When I start pyspark in the docker container, it binds to port 49460.
with the following error message :ERROR yarn.ApplicationMaster: Failed to connect to driver at :49460
This makes sense because AM is trying to talk to container directly andit cannot, it should be talking to the docker host instead.
Question :How do we make Spark AM talk to host1:port1 of the docker host(not the container), which would thenroute it to container which is running pyspark on host2:port2 ?
ortA), and before submitting the app to yarn, we could reset driver's host/port to hostmachine's ip/port. So the AM can then talk hostmachine's ip/port, which would be mappedto the container.
Thoughts ? -- 
Thanks,
Ashwin



 		 	   		  "
Ray Ortigas <rortigas@linkedin.com.INVALID>,"Wed, 10 Jun 2015 17:47:05 -0700","Re: Approximate rank-based statistics (median, 95-th percentile,
 etc.) for Spark",Reynold Xin <rxin@databricks.com>,"Hi Grega and Reynold,

Grega, if you still want to use t-digest, I filed this PR because I thought
your t-digest suggestion was a good idea.

https://github.com/tdunning/t-digest/pull/56

If it is helpful feel free to do whatever with it.

Regards,
Ray



ote:
:
PI
he
=rep1&type=pdf
mory.
. The
"
Hector Yee <hector.yee@gmail.com>,"Wed, 10 Jun 2015 18:23:57 -0700",Jcenter / bintray support for spark packages?,dev <dev@spark.apache.org>,"Hi Spark devs,

Is it possible to add jcenter or bintray support for Spark packages?

I'm trying to add our artifact which is on jcenter

https://bintray.com/airbnb/aerosolve

but I noticed in Spark packages it only accepts Maven coordinates.

-- 
Yee Yang Li Hector
google.com/+HectorYee

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Wed, 10 Jun 2015 18:30:05 -0700",Re: Jcenter / bintray support for spark packages?,Hector Yee <hector.yee@gmail.com>,"Hey Hector,

It's not a bad idea. I think we'd want to do this by virtue of
allowing custom repositories, so users can add bintray or others.

- Patrick


---------------------------------------------------------------------


"
Joseph Bradley <joseph@databricks.com>,"Wed, 10 Jun 2015 19:20:31 -0700","Re: [DISCUSS] Minimize use of MINOR, BUILD, and HOTFIX w/ no JIRA",Patrick Wendell <pwendell@gmail.com>,1
Joseph Bradley <joseph@databricks.com>,"Wed, 10 Jun 2015 19:31:40 -0700",Re: [ml] Why all model classes are final?,Peter Rudenko <petro.rudenko@gmail.com>,"Hi Peter,

We've tried to be cautious about making APIs public without need, to allow
for changes needed in the future which we can't foresee now.  Marking
classes as final is part of that.  While marking things as Experimental or
DeveloperApi is a sort of warning, we've often felt that even changing
those Experimental/Developer APIs is dangerous since people can come to
rely on those APIs.

However, customization is a very valid use case, and I agree that the
classes should be opened up in the future.  I hope that, as the Pipelines
API graduates from alpha, more users will give feedback about them, and
that will give us enough confidence in the API stability to make the
classes non-final.

Joseph


"
Dong Lei <donglei@microsoft.com>,"Thu, 11 Jun 2015 03:04:41 +0000","How to support dependency jars and files on HDFS in standalone
 cluster mode?","""dev@spark.apache.org"" <dev@spark.apache.org>","Hi spark-dev:

I can not use a hdfs location for the ""--jars"" or ""--files"" option when doing a spark-submit in a standalone cluster mode. For example:
                Spark-submit  ...   --jars hdfs://ip/1.jar  ....  hdfs://ip/app.jar (standalone cluster mode)
will not download 1.jar to driver's http file server(but the app.jar will be downloaded to the driver's dir).

I figure out the reason spark not downloading the jars is that when doing sc.addJar to http file server, the function called is Files.copy which does not support a remote location.
And I think if spark can download the jars and add them to http file server, the classpath is not correctly set, because the classpath contains remote location.

So I'm trying to make it work and come up with two options, but neither of them seem to be elegant, and I want to hear your advices:

Option 1:
Modify HTTPFileServer.addFileToDir, let it recognize a ""hdfs"" prefix.

This is not good because I think it breaks the scope of http file server.

Option 2:
Modify DriverRunner.downloadUserJar, let it download all the ""--jars"" and ""--files"" with the application jar.

This sounds more reasonable that option 1 for downloading files. But this way I need to read the ""spark.jars"" and ""spark.files"" on downloadUserJar or DriverRunnder.start and replace it with a local path. How can I do that?


Do you have a more elegant solution, or do we have a plan to support it in the furture?

Thanks
Dong Lei
"
Cheng Lian <lian.cs.zju@gmail.com>,"Thu, 11 Jun 2015 12:50:19 +0800","Re: How to support dependency jars and files on HDFS in standalone
 cluster mode?","Dong Lei <donglei@microsoft.com>, 
 ""dev@spark.apache.org"" <dev@spark.apache.org>","Since the jars are already on HDFS, you can access them directly in your 
Spark application without using --jars

Cheng


"
Ashwin Shankar <ashwinshankar77@gmail.com>,"Wed, 10 Jun 2015 22:09:27 -0700",Re: Problem with pyspark on Docker talking to YARN cluster,Eron Wright <ewright@live.com>,"Hi Eron, Thanks for your reply, but none of these options works for us.
something, its going to be used to bind on the client
side and the same will be passed to the AM. We need two variables,a) one to
bind to on the client side, b)another port which is opened up on the docker
host and will be used by the AM to talk back to the driver.

2. use host networking for your container, i.e. ""docker run --net=host ...""

We run containers in shared environment, and this option makes host network
stack accessible to all
containers in it, which could leads to security issues.

3. use yarn-cluster mode

 Pyspark interactive shell(ipython) doesn't have cluster mode. SPARK-5162
<https://issues.apache.org/jira/browse/SPARK-5162> is for spark-submit
python in cluster mode.

Thanks,
Ashwin





-- 
Thanks,
Ashwin
"
Nick Pentreath <nick.pentreath@gmail.com>,"Thu, 11 Jun 2015 07:48:29 +0200",Re: [sample code] deeplearning4j for Spark ML (@DeveloperAPI),Eron Wright <ewright@live.com>,"Looks very interesting, thanks for sharing this.

I haven't had much chance to do more than a quick glance over the code.
Quick question - are the Word2Vec and GLOVE implementations fully parallel
on Spark?


orking on
g4j-scaleout/spark/dl4j-spark-ml> in
ning4j-scaleout/spark/dl4j-spark-ml/src/main/scala/org/deeplearning4j/spark/ml/classification/MultiLayerNetworkClassification.scala> and
ning4j-scaleout/spark/dl4j-spark-ml/src/main/scala/org/deeplearning4j/spark/ml/Unsupervised.scala>.
tion
06050eda82a7d50ff77a8d957/deeplearning4j-scaleout/spark/dl4j-spark-ml/src/main/scala/org/deeplearning4j/spark/ml/classification/MultiLayerNetworkClassification.scala#L89> how
s/api/scala/index.html#org.apache.spark.ml.feature.StringIndexer>
o
ning4j-scaleout/spark/dl4j-spark-ml/src/main/scala/org/deeplearning4j/spark/sql/sources/lfw/LfwRelation.scala> we
4bd3c99d1eece6cb658f387f2/deeplearning4j-scaleout/spark/dl4j-spark-ml/src/main/scala/org/deeplearning4j/spark/sql/sources/lfw/LfwRelation.scala#L96> we
a
ning4j-scaleout/spark/dl4j-spark-ml/src/main/scala/org/deeplearning4j/spark/sql/types/tensors.scala>
the
ure
g
"
Dong Lei <donglei@microsoft.com>,"Thu, 11 Jun 2015 06:18:57 +0000","RE: How to support dependency jars and files on HDFS in standalone
 cluster mode?",Cheng Lian <lian.cs.zju@gmail.com>,"Thanks Cheng,

If I do not use --jars how can I tell spark to search the jars(and files) on HDFS?

Do you mean the driver will not need to setup a HTTP file server for this scenario and the worker will fetch the jars and files from HDFS?

Thanks
Dong Lei

From: Cheng Lian [mailto:lian.cs.zju@gmail.com]
Sent: Thursday, June 11, 2015 12:50 PM
To: Dong Lei; dev@spark.apache.org
Cc: Dianfei (Keith) Han
Subject: Re: How to support dependency jars and files on HDFS in standalone cluster mode?

Since the jars are already on HDFS, you can access them directly in your Spark application without using --jars

Cheng
Hi spark-dev:

I can not use a hdfs location for the ""--jars"" or ""--files"" option when doing a spark-submit in a standalone cluster mode. For example:
                Spark-submit  ...   --jars hdfs://ip/1.jar  ....  hdfs://ip/app.jar (standalone cluster mode)
will not download 1.jar to driver's http file server(but the app.jar will be downloaded to the driver's dir).

I figure out the reason spark not downloading the jars is that when doing sc.addJar to http file server, the function called is Files.copy which does not support a remote location.
And I think if spark can download the jars and add them to http file server, the classpath is not correctly set, because the classpath contains remote location.

So I'm trying to make it work and come up with two options, but neither of them seem to be elegant, and I want to hear your advices:

Option 1:
Modify HTTPFileServer.addFileToDir, let it recognize a ""hdfs"" prefix.

This is not good because I think it breaks the scope of http file server.

Option 2:
Modify DriverRunner.downloadUserJar, let it download all the ""--jars"" and ""--files"" with the application jar.

This sounds more reasonable that option 1 for downloading files. But this way I need to read the ""spark.jars"" and ""spark.files"" on downloadUserJar or DriverRunnder.start and replace it with a local path. How can I do that?


Do you have a more elegant solution, or do we have a plan to support it in the furture?

Thanks
Dong Lei

"
Cheng Lian <lian.cs.zju@gmail.com>,"Thu, 11 Jun 2015 14:31:57 +0800","Re: How to support dependency jars and files on HDFS in standalone
 cluster mode?",Dong Lei <donglei@microsoft.com>,"Oh sorry, I mistook --jars for --files. Yeah, for jars we need to add 
them to classpath, which is different from regular files.

Cheng


"
Dong Lei <donglei@microsoft.com>,"Thu, 11 Jun 2015 06:40:38 +0000","RE: How to support dependency jars and files on HDFS in standalone
 cluster mode?",Cheng Lian <lian.cs.zju@gmail.com>,"I think in standalone cluster mode, spark is supposed to do:

1.       Download jars, files to driver

2.       Set the driverâ€™s class path

3.       Driver setup a http file server to distribute these files

4.       Worker download from driver and setup classpath

Right?

But somehow, the first step fails.
Even if I can make the first step works(use option1), it seems that the classpath in driver is not correctly set.

Thanks
Dong Lei

From: Cheng Lian [mailto:lian.cs.zju@gmail.com]
Sent: Thursday, June 11, 2015 2:32 PM
To: Dong Lei
Cc: Dianfei (Keith) Han; dev@spark.apache.org
Subject: Re: How to support dependency jars and files on HDFS in standalone cluster mode?

Oh sorry, I mistook --jars for --files. Yeah, for jars we need to add them to classpath, which is different from regular files.

Cheng
On 6/11/15 2:18 PM, Dong Lei wrote:
Thanks Cheng,

If I do not use --jars how can I tell spark to search the jars(and files) on HDFS?

Do you mean the driver will not need to setup a HTTP file server for this scenario and the worker will fetch the jars and files from HDFS?

Thanks
Dong Lei

From: Cheng Lian [mailto:lian.cs.zju@gmail.com]
Sent: Thursday, June 11, 2015 12:50 PM
To: Dong Lei; dev@spark.apache.org<mailto:dev@spark.apache.org>
Cc: Dianfei (Keith) Han
Subject: Re: How to support dependency jars and files on HDFS in standalone cluster mode?

Since the jars are already on HDFS, you can access them directly in your Spark application without using --jars

Cheng
On 6/11/15 11:04 AM, Dong Lei wrote:
Hi spark-dev:

I can not use a hdfs location for the â€œ--jarsâ€ or â€œ--filesâ€ option when doing a spark-submit in a standalone cluster mode. For example:
                Spark-submit  â€¦   --jars hdfs://ip/1.jar  â€¦.  hdfs://ip/app.jar (standalone cluster mode)
will not download 1.jar to driverâ€™s http file server(but the app.jar will be downloaded to the driverâ€™s dir).

I figure out the reason spark not downloading the jars is that when doing sc.addJar to http file server, the function called is Files.copy which does not support a remote location.
And I think if spark can download the jars and add them to http file server, the classpath is not correctly set, because the classpath contains remote location.

So Iâ€™m trying to make it work and come up with two options, but neither of them seem to be elegant, and I want to hear your advices:

Option 1:
Modify HTTPFileServer.addFileToDir, let it recognize a â€œhdfsâ€ prefix.

This is not good because I think it breaks the scope of http file server.

Option 2:
Modify DriverRunner.downloadUserJar, let it download all the â€œ--jarsâ€ and â€œ--filesâ€ with the application jar.

This sounds more reasonable that option 1 for downloading files. But this way I need to read the â€œspark.jarsâ€ and â€œspark.filesâ€ on downloadUserJar or DriverRunnder.start and replace it with a local path. How can I do that?


Do you have a more elegant solution, or do we have a plan to support it in the furture?

Thanks
Dong Lei


"
Erik Erlandson <eje@redhat.com>,"Thu, 11 Jun 2015 10:32:33 -0400 (EDT)",Re: [ml] Why all model classes are final?,Peter Rudenko <petro.rudenko@gmail.com>,"I was able to work around this problem in several cases using the class 'enhancement' or 'extension' pattern to add some functionality to the decision tree model data structures.


X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 45D7517E5B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 11 Jun 2015 14:42:54 +0000 (UTC)
Received: (qmail 73848 invoked by uid 500); 11 Jun 2015 14:42:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 73769 invoked by uid 500); 11 Jun 2015 14:42:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 73748 invoked by uid 99); 11 Jun 2015 14:42:52 -0000
Received: from Unknown (HELO spamd1-us-west.apache.org) (209.188.14.142)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 11 Jun 2015 14:42:52 +0000
Received: from localhost (localhost [127.0.0.1])
	by spamd1-us-west.apache.org (ASF Mail Server at spamd1-us-west.apache.org) with ESMTP id 71840CD43F
	for <dev@spark.apache.org>; Thu, 11 Jun 2015 14:42:51 +0000 (UTC)
X-Virus-Scanned: Debian amavisd-new at spamd1-us-west.apache.org
X-Spam-Flag: NO
X-Spam-Score: 3.001
X-Spam-Level: ***
X-Spam-Status: No, score=3.001 tagged_above=-999 required=6.31
	tests=[HTML_MESSAGE=3, URIBL_BLOCKED=0.001] autolearn=disabled
Received: from mx1-eu-west.apache.org ([10.40.0.8])
	by localhost (spamd1-us-west.apache.org [10.40.0.7]) (amavisd-new, port 10024)
	with ESMTP id 4XUc6emXwP7f for <dev@spark.apache.org>;
	Thu, 11 Jun 2015 14:42:39 +0000 (UTC)
Received: from mail-wg0-f52.google.com (mail-wg0-f52.google.com [74.125.82.52])
	by mx1-eu-west.apache.org (ASF Mail Server at mx1-eu-west.apache.org) with ESMTPS id 615D420FF7
	for <dev@spark.apache.org>; Thu, 11 Jun 2015 14:42:38 +0000 (UTC)
Received: by wgez8 with SMTP id z8so6610956wge.0
        for <dev@spark.apache.org>; Thu, 11 Jun 2015 07:42:32 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=gWMpuVPqFWKP257+k4hY12rfcpg3zdwxq5VVrgCfhto=;
        b=cYJ/nUDQnFbRMZ6Tt1JRXvwOe3NeRiUH01GbT4JbIU3lRDQR1FnN1IVPep9Vjuj+me
         lJ1l1bNaHAYFTbPP9cSZlbuPUUMdxpR7l7HMdFuqejorr0i1ety3tu+GwBGlBzrCnAcS
         yNy79q8y49sIbsE6to/l30LnALWrfc6vLk1EK5fmphq+/gWBJf3ORbLgGDJKodB0qSf+
         r/Ahs1AiMWQn8ecVlrAf3QsZPZu9NwfOM9mXwJM0BsV8ykQ+OojK0H+4cpQd0iVtUSKG
         8tZrMmNexuuuO0e8hrFAjcPj7uJXGG620Mu/AD4eQJ6BllQsoBuy6bf8S1UtrKnSkdBP
         iS0g==
X-Gm-Message-State: ALoCoQmHPcHwcHrb94vKw+xQjXyfdlHqpufdpvc5OvuJlg4r27hIlul3N0Eoeyzpezqh0pSDgilW
X-Received: by 10.194.57.211 with SMTP id k19mr17320262wjq.53.1434033751996;
 Thu, 11 Jun 2015 07:42:31 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.27.14.38 with HTTP; Thu, 11 Jun 2015 07:42:11 -0700 (PDT)
References: <CABX=+Lr8Hv5GmkVxPKedLzWy21KZxeJZPFoh5f3RzO8XB6MRaA@mail.gmail.com>
 <CA+3qhFSoco_VvDQOvKi8E7ROR8ymBzv=y97j1QOjkav=RLRQVA@mail.gmail.com>
 <CABX=+Lr9+MF1X4YsfAxEZfJiVRUfwSwnearMNBMDCeWx1_fNtg@mail.gmail.com>
 <CABX=+Lp89s7yf9dhZfK8STFU3L=trUg8XVgorqZYhgj_frWhDg@mail.gmail.com>
 <CABX=+Lpdf2N8598WeC7_BsgMFeivJ40uMLWM9ZxYxFz6Lh5WVw@mail.gmail.com>
From: Imran Rashid <irashid@cloudera.com>
Date: Thu, 11 Jun 2015 09:42:11 -0500
Message-ID: <CA+3qhFS7uzzBd_1ocK0uNcRNk0xP1EQzA=frdngUx9=WLJY_7A@mail.gmail.com>
Subject: Re: Stages with non-arithmetic numbering & Timing metrics in event logs
To: Mike Hynes <91mbbh@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b86d808624fdc05183f0026

--047d7b86d808624fdc05183f0026
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

That is not exactly correct -- that being said I'm not 100% on these
details either so I'd appreciate you double checking  and / or another dev
confirming my description.


Spark actually has more threads going then the ""numCores"" you specify.
""numCores"" is really used for how many threads are actively executing
tasks.  There are more threads for doing the fetching (the details of which
I'm not that familiar with) -- that never cuts into the number of actively
executing tasks for ""numCores"".  There isn't a 1-to-1 correspondence
between one shuffle block and one task -- a shuffle-read task most likely
needs to fetch many shuffle blocks, with some local and some remote (in
most cases).  So, from your lingo above, c is numCores, but c_1 is just an
independent pool of threads.

this obvious follow up question is, if you've actually more than ""numCores""
threads going at the same time, how come cpu usage is low?  You could still
have your cpus stuck waiting on i/o, from disk or network, so they aren't
getting fully utilized.  And the cpu can also be idle waiting for memory,
if there are a lot of cache misses (I'm not sure how that will show up in
cpu monitoring).  If that were the case, that could even be from too many
threads, as a lot of time is spent context switching ... but I'm just
guessing now.

hope this helps,
Imran




n
.
a
a
st
g
s.
n
).
n
s,
x %
pt
3)
to
or
I
ar
gc
t
.
t
e
or
=3D=3D=3D=3D
e
an
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D
o
gs
t
a
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D
.
me
)
he
ys
ay
o
s
ng
,
t
t
)
)
)
9)
1)
)
)
he
BC
se
ge
e.
e
ed
ed
st
d
n

--047d7b86d808624fdc05183f0026
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D""ltr"">That is not exactly correct -- that being said I&#39;m not=
 100% on these details either so I&#39;d appreciate you double checking =C2=
=A0and / or another dev confirming my description.<div><br></div><div><br><=
div>Spark actually has more threads going then the &quot;numCores&quot; you=
 specify.=C2=A0 &quot;numCores&quot; is really used for how many threads ar=
e actively executing tasks.=C2=A0 There are more threads for doing the fetc=
hing (the details of which I&#39;m not that familiar with) -- that never cu=
ts into the number of actively executing tasks for &quot;numCores&quot;.=C2=
=A0 There isn&#39;t a 1-to-1 correspondence between one shuffle block and o=
ne task -- a shuffle-read task most likely needs to fetch many shuffle bloc=
ks, with some local and some remote (in most cases).=C2=A0 So, from your li=
ngo above, c is numCores, but c_1 is just an independent pool of threads.</=
div></div><div><br></div><div>this obvious follow up question is, if you&#3=
9;ve actually more than &quot;numCores&quot; threads going at the same time=
, how come cpu usage is low?=C2=A0 You could still have your cpus stuck wai=
ting on i/o, from disk or network, so they aren&#39;t getting fully utilize=
d.=C2=A0 And the cpu can also be idle waiting for memory, if there are a lo=
t of cache misses (I&#39;m not sure how that will show up in cpu monitoring=
).=C2=A0 If that were the case, that could even be from too many threads, a=
s a lot of time is spent context switching ... but I&#39;m just guessing no=
w.</div><div><br></div><div>hope this helps,</div><div>Imran</div><div><br>=
</div><div><br></div></div><div class=3D""gmail_extra""><br><div class=3D""gma=
;<a href=3D""mailto:91mbbh@gmail.com"" target=3D""_blank"">91mbbh@gmail.com</a>=
 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"">Hi Imran,<br>
<br>
Thank you again for your email.<br>
<br>
I just want to ask one further question to clarify the implementation<br>
of the shuffle block fetches. When you say that rather than sitting<br>
idle, [the executor] will immediately start reading the local block, I<br>
would guess that, in implementation, the executor is going to launch<br>
concurrent threads to read both local and remote blocks, which it<br>
seems to do in the initialize() method of<br>
core/.../storage/ShuffleBlockFetcherIterator.scala. Is that the case<br>
or would the Executor run all local fetch threads first?<br>
<br>
The reason I ask is that if the slave machine on which the Executor is<br>
running has some number of cores, c, then I=C2=A0 would have thought that<b=
r>
some of the threads launched would occupy some number, c_1, of the<br>
cores and conduct the local reads (where c_1 &lt;=3D c). The other threads<=
br>
would occupy the other (c - c_1) cores&#39; cycles until *all* necessary<br=
blocks have been read, and depending on c and the number of blocks to<br>
fetch so that none of the cores are idle if there are many blocks to<br>
fetch. (I monitor the CPU utilization of our nodes throughout a job,<br>
and generally find them under-utilized statistically speaking; that<br>
is, their usage over the whole job is lower than expected, with short<br>
burst of high usage, so I ask this question in a specific way for this<br>
reason, since I can see trends in the probability density functions of<br>
CPU utilization as the #partitions of our RDDs are increased).<br>
<br>
ShuffleBlockFetcherIterator.scala:<br>
<br>
=C2=A0 private[this] def initialize(): Unit =3D {<br>
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 ...<br>
=C2=A0 =C2=A0 // Send out initial requests for blocks, up to our maxBytesIn=
Flight<br>
=C2=A0 =C2=A0 while (fetchRequests.nonEmpty &amp;&amp;<br>
=C2=A0 =C2=A0 =C2=A0 (bytesInFlight =3D=3D 0 || bytesInFlight + fetchReques=
ts.front.size<br>
&lt;=3D maxBytesInFlight)) {<br>
=C2=A0 =C2=A0 =C2=A0 sendRequest(fetchRequests.dequeue())<br>
=C2=A0 =C2=A0 }<br>
=C2=A0 =C2=A0 val numFetches =3D remoteRequests.size - fetchRequests.size<b=
r>
=C2=A0 =C2=A0 logInfo(&quot;Started &quot; + numFetches + &quot; remote fet=
ches in&quot; +<br>
Utils.getUsedTimeMs(startTime))<br>
<br>
=C2=A0 =C2=A0 // Get Local Blocks<br>
=C2=A0 =C2=A0 fetchLocalBlocks()<br>
=C2=A0 =C2=A0 logDebug(&quot;Got local blocks in &quot; + Utils.getUsedTime=
Ms(startTime))<br>
=C2=A0 }<br>
=C2=A0 private[this] def fetchLocalBlocks() {<br>
=C2=A0 =C2=A0 val iter =3D localBlocks.iterator<br>
=C2=A0 =C2=A0 while (iter.hasNext) {<br>
=C2=A0 =C2=A0 =C2=A0 val blockId =3D iter.next()<br>
=C2=A0 =C2=A0 =C2=A0 try {<br>
=C2=A0 =C2=A0 =C2=A0 =C2=A0 val buf =3D blockManager.getBlockData(blockId)<=
br>
=C2=A0 =C2=A0 =C2=A0 =C2=A0 shuffleMetrics.incLocalBlocksFetched(1)<br>
=C2=A0 =C2=A0 =C2=A0 =C2=A0 shuffleMetrics.incLocalBytesRead(buf.size)<br>
=C2=A0 =C2=A0 =C2=A0 =C2=A0 buf.retain()<br>
=C2=A0 =C2=A0 =C2=A0 =C2=A0 results.put(new SuccessFetchResult(blockId, 0, =
buf))<br>
=C2=A0 =C2=A0 =C2=A0 } catch {<br>
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 ...<br>
=C2=A0 =C2=A0 =C2=A0 }<br>
=C2=A0 =C2=A0 }<br>
=C2=A0 }<br>
<br>
Obviously, I will have to sit down with core/.../network/nio/* and<br>
core/.../shuffle/* and do my own homework on this, but from what I can<br>
tell, the BlockDataManager relies on either<br>
NioBlockTransferService.scala or the NettyBlockTransferService.scala<br>
(which are set in SparkEnv.scala), both of which do the grunt work of<br>
actually buffering and transferring the blocks&#39; bytes. Finally, the<br>
tasks in new stage for which the shuffle outputs have been fetched<br>
will not commence until all of the block fetching threads (both local<br>
and remote) have terminated.<br>
<br>
Does the above paint an accurate picture? I would really appreciate<br>
clarification on the concurrency, since I would like to determine why<br>
our jobs have under-utilization and poor weak scaling efficiency.<br>
<br>
I will cc this thread over to the dev list. I did not cc them in case<br>
my previous question was trivial---I didn&#39;t want to spam the list<br>
unnecessarily, since I do not see these kinds of questions posed there<br>
frequently.<br>
<br>
Thanks a bunch,<br>
Mike<br>
<div class=3D""HOEnZb""><div class=3D""h5""><br>
<br>
&gt; Hi Mike,<br>
&gt;<br>
&gt; no, this is a good question, I can see how my response could be interp=
reted<br>
&gt; both ways.<br>
&gt;<br>
&gt; To be more precise:<br>
&gt; *nothing* is fetched until the shuffle-read stage starts.=C2=A0 So it =
is normal<br>
&gt; to see a spike in cluster bandwidth when that stage starts.=C2=A0 Ther=
e is a<br>
&gt; hard-boundary between stages -- that is, spark never starts any tasks =
in<br>
&gt; one stage until *all* tasks in the dependent stages have been complete=
d.<br>
&gt; (There has been on-and-off discussion about relaxing this, but IMO thi=
s is<br>
&gt; unlikely to change in the near future.)=C2=A0 So spark will wait for a=
ll of the<br>
&gt; tasks in the previous shuffle-write stage to finish, and then kick off=
 a<br>
&gt; bunch of shuffle-read tasks in the next stage, leading to the spike yo=
u<br>
&gt; see.<br>
&gt;<br>
&gt; I was referring to the way blocks are fetched within one of those<br>
 need a<br>
&gt; bunch of different blocks, from many executors.=C2=A0 But some of the =
blocks it<br>
&gt; needs will probably exist locally.=C2=A0 So the task first sends out a=
 request<br>
&gt; to fetch blocks remotely (leading to the spike), but rather than sitti=
ng<br>
&gt; idle, it will immediately start reading the local blocks.=C2=A0 Ideall=
y, by the<br>
&gt; time its done reading the local blocks, some of the remote blocks have=
<br>
&gt; already been fetched, so no time is spent *waiting* for the remote rea=
ds.<br>
&gt; As the remote blocks get read, spark sends out more requests, trying t=
o<br>
&gt; balance how much data needs to be buffered vs. preventing any waiting =
on<br>
&gt; remote reads (which can=C2=A0 be controlled by spark.reducer.maxSizeIn=
Flight).<br>
&gt;<br>
&gt; Hope that clarifies things!<br>
&gt;<br>
&gt; btw, you sent this last question to just me -- I think its a good ques=
tion,<br>
&gt; do you mind sending it to the list?=C2=A0 I figured that was accidenta=
l but<br>
&gt; wanted to check.<br>
&gt;<br>
&gt; Imran<br>
&gt;<br>
&gt;<br>
&gt;&gt; Hi Imran,<br>
y<br>
&gt;&gt; understand your comment that &quot;blocks are fetched before they =
are<br>
&gt;&gt; needed.&quot; Typically on our system, we see spikes in cluster ba=
ndwidth<br>
&gt;&gt; (with ganglia) at stage boundaries, so I previously assumed that a=
ll<br>
&gt;&gt; shuffle read occurred there. Do you mean that the blocks are fetch=
ed<br>
&gt;&gt; by the shuffle read iterator, and hence when tasks occur afterward=
s<br>
&gt;&gt; the necessary blocks have already been fetched?<br>
&gt;&gt; Thanks---I am sorry if this is an obvious question, but I&#39;d li=
ke to<br>
&gt;&gt; understand this as precisely as possible.<br>
&gt;&gt; Mike<br>
&gt;&gt;<br>
&gt;&gt; &gt; Ahhh---forgive my typo: what I mean is,<br>
&gt;&gt; &gt; (t2 - t1) &gt;=3D (t_ser + t_deser + t_exec)<br>
&gt;&gt; &gt; is satisfied, empirically.<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt;&gt; Hi Imran,<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt; Thank you for your email.<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt; In examing the condition (t2 - t1) &lt; (t_ser + t_deser =
+ t_exec), I<br>
&gt;&gt; &gt;&gt; have found it to be true, although I have not included th=
e<br>
&gt;&gt; &gt;&gt; t_{wait_for_read} in this, since it is---so far as I can =
tell---been<br>
&gt;&gt; &gt;&gt; either zero or negligible compared to the task time.<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt; Thanks,<br>
&gt;&gt; &gt;&gt; Mike<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt;&gt; Hi Mike,<br>
&gt;&gt; &gt;&gt;&gt;<br>
&gt;&gt; &gt;&gt;&gt; all good questions, let me take a stab at answering t=
hem:<br>
&gt;&gt; &gt;&gt;&gt;<br>
&gt;&gt; &gt;&gt;&gt; 1. Event Logs + Stages:<br>
&gt;&gt; &gt;&gt;&gt;<br>
&gt;&gt; &gt;&gt;&gt; Its normal for stages to get skipped if they are shuf=
fle map stages,<br>
&gt;&gt; &gt;&gt;&gt; which<br>
&gt;&gt; &gt;&gt;&gt; get read multiple times.=C2=A0 Eg., here&#39;s a litt=
le example program I<br>
&gt;&gt; &gt;&gt;&gt; wrote<br>
&gt;&gt; &gt;&gt;&gt; earlier to demonstrate this: &quot;d3&quot; doesn&#39=
;t need to be re-shuffled<br>
&gt;&gt; &gt;&gt;&gt; since<br>
&gt;&gt; &gt;&gt;&gt; each<br>
&gt;&gt; &gt;&gt;&gt; time its read w/ the same partitioner.=C2=A0 So skipp=
ing stages in this<br>
&gt;&gt; &gt;&gt;&gt; way<br>
&gt;&gt; &gt;&gt;&gt; is<br>
&gt;&gt; &gt;&gt;&gt; a<br>
&gt;&gt; &gt;&gt;&gt; good thing:<br>
&gt;&gt; &gt;&gt;&gt;<br>
&gt;&gt; &gt;&gt;&gt; val partitioner =3D new org.apache.spark.HashPartitio=
ner(10)<br>
&gt;&gt; &gt;&gt;&gt; val d3 =3D sc.parallelize(1 to 100).map { x =3D&gt; (=
x % 10) -&gt;<br>
&gt;&gt; &gt;&gt;&gt; x}.partitionBy(partitioner)<br>
&gt;&gt; &gt;&gt;&gt; (0 until 5).foreach { idx =3D&gt;<br>
&gt;&gt; &gt;&gt;&gt;=C2=A0 =C2=A0val otherData =3D sc.parallelize(1 to (id=
x * 100)).map{ x =3D&gt; (x % 10)<br>
&gt;&gt; -&gt;<br>
&gt;&gt; &gt;&gt;&gt; x}.partitionBy(partitioner)<br>
&gt;&gt; &gt;&gt;&gt;=C2=A0 =C2=A0println(idx + &quot; ---&gt; &quot; + oth=
erData.join(d3).count())<br>
&gt;&gt; &gt;&gt;&gt; }<br>
&gt;&gt; &gt;&gt;&gt;<br>
&gt;&gt; &gt;&gt;&gt; If you run this, f you look in the UI you&#39;d see t=
hat all jobs except<br>
&gt;&gt; for<br>
&gt;&gt; &gt;&gt;&gt; the first one have one stage that is skipped.=C2=A0 Y=
ou will also see this<br>
&gt;&gt; in<br>
&gt;&gt; &gt;&gt;&gt; the log:<br>
&gt;&gt; &gt;&gt;&gt;<br>
&gt;&gt; &gt;&gt;&gt; 15/06/08 10:52:37 INFO DAGScheduler: Parents of final=
 stage:<br>
&gt;&gt; &gt;&gt;&gt; List(Stage<br>
&gt;&gt; &gt;&gt;&gt; 12,<br>
&gt;&gt; &gt;&gt;&gt; Stage 13)<br>
&gt;&gt; &gt;&gt;&gt;<br>
&gt;&gt; &gt;&gt;&gt; 15/06/08 10:52:37 INFO DAGScheduler: Missing parents:=
 List(Stage 13)<br>
&gt;&gt; &gt;&gt;&gt;<br>
&gt;&gt; &gt;&gt;&gt; Admittedly that is not very clear, but that is sort o=
f indicating to<br>
&gt;&gt; you<br>
&gt;&gt; &gt;&gt;&gt; that the DAGScheduler first created stage 12 as a nec=
essary step, and<br>
&gt;&gt; &gt;&gt;&gt; then<br>
&gt;&gt; &gt;&gt;&gt; later on changed its mind by realizing that everythin=
g it needed for<br>
&gt;&gt; &gt;&gt;&gt; stage<br>
&gt;&gt; &gt;&gt;&gt; 12 already existed, so there was nothing to do.<br>
&gt;&gt; &gt;&gt;&gt;<br>
&gt;&gt; &gt;&gt;&gt;<br>
&gt;&gt; &gt;&gt;&gt; 2. Extracting Event Log Information<br>
&gt;&gt; &gt;&gt;&gt;<br>
&gt;&gt; &gt;&gt;&gt; maybe you are interested in SparkListener ? Though un=
fortunately, I<br>
&gt;&gt; &gt;&gt;&gt; don&#39;t<br>
&gt;&gt; &gt;&gt;&gt; know of a good blog post describing it, hopefully the=
 docs are clear<br>
&gt;&gt; ...<br>
&gt;&gt; &gt;&gt;&gt;<br>
&gt;&gt; &gt;&gt;&gt; 3. Time Metrics in Spark Event Log<br>
&gt;&gt; &gt;&gt;&gt;<br>
&gt;&gt; &gt;&gt;&gt; This is a great question.=C2=A0 I *think* the only ex=
ception is that t_gc<br>
&gt;&gt; &gt;&gt;&gt; is<br>
&gt;&gt; &gt;&gt;&gt; really overlapped with t_exec.=C2=A0 So I think you s=
hould really expect<br>
&gt;&gt; &gt;&gt;&gt;<br>
&gt;&gt; &gt;&gt;&gt; (t2 - t1) &lt; (t_ser + t_deser + t_exec)<br>
&gt;&gt; &gt;&gt;&gt;<br>
&gt;&gt; &gt;&gt;&gt; I am not 100% sure about this, though.=C2=A0 I&#39;d =
be curious if that was<br>
&gt;&gt; &gt;&gt;&gt; constraint was ever violated.<br>
&gt;&gt; &gt;&gt;&gt;<br>
&gt;&gt; &gt;&gt;&gt;<br>
&gt;&gt; &gt;&gt;&gt; As for your question on shuffle read vs. shuffle writ=
e time -- I<br>
&gt;&gt; &gt;&gt;&gt; wouldn&#39;t<br>
&gt;&gt; &gt;&gt;&gt; necessarily expect the same stage to have times for b=
oth shuffle read<br>
&gt;&gt; &gt;&gt;&gt; &amp;<br>
&gt;&gt; &gt;&gt;&gt; shuffle write -- in the simplest case, you&#39;ll hav=
e shuffle write<br>
&gt;&gt; &gt;&gt;&gt; times<br>
&gt;&gt; &gt;&gt;&gt; in<br>
&gt;&gt; &gt;&gt;&gt; one, and shuffle read times in the next one.=C2=A0 Bu=
t even taking that<br>
&gt;&gt; &gt;&gt;&gt; into<br>
&gt;&gt; &gt;&gt;&gt; account, there is a difference in the way they work &=
amp; are measured.<br>
&gt;&gt; &gt;&gt;&gt;=C2=A0 shuffle read operations are pipelined and the w=
ay we measure shuffle<br>
&gt;&gt; &gt;&gt;&gt; read,<br>
&gt;&gt; &gt;&gt;&gt; its just how much time is spent *waiting* for network=
 transfer.=C2=A0 It<br>
&gt;&gt; &gt;&gt;&gt; could<br>
&gt;&gt; &gt;&gt;&gt; be that there is no (measurable) wait time b/c the ne=
xt blocks are<br>
&gt;&gt; &gt;&gt;&gt; fetched<br>
&gt;&gt; &gt;&gt;&gt; before they are needed.=C2=A0 Shuffle writes occur in=
 the normal task<br>
&gt;&gt; &gt;&gt;&gt; execution<br>
&gt;&gt; &gt;&gt;&gt; thread, though, so we (try to) measure all of it.<br>
&gt;&gt; &gt;&gt;&gt;<br>
&gt;&gt; &gt;&gt;&gt;<br>
&gt;&gt; &gt;&gt;&gt;<br>
&gt;&gt; &gt;&gt;&gt;&gt; Hi Patrick and Akhil,<br>
&gt;&gt; &gt;&gt;&gt;&gt;<br>
&gt;&gt; &gt;&gt;&gt;&gt; Thank you both for your responses. This is a bit =
of an extended<br>
&gt;&gt; &gt;&gt;&gt;&gt; email,<br>
&gt;&gt; &gt;&gt;&gt;&gt; but I&#39;d like to:<br>
&gt;&gt; &gt;&gt;&gt;&gt; 1. Answer your (Patrick) note about the &quot;mis=
sing&quot; stages since the<br>
&gt;&gt; &gt;&gt;&gt;&gt; IDs<br>
&gt;&gt; &gt;&gt;&gt;&gt; do (briefly) appear in the event logs<br>
&gt;&gt; &gt;&gt;&gt;&gt; 2. Ask for advice/experience with extracting info=
rmation from the<br>
&gt;&gt; &gt;&gt;&gt;&gt; event logs in a columnar, delimiter-separated for=
mat.<br>
&gt;&gt; &gt;&gt;&gt;&gt; 3. Ask about the time metrics reported in the eve=
nt logs; currently,<br>
&gt;&gt; &gt;&gt;&gt;&gt; the elapsed time for a task does not equal the su=
m of the times for<br>
&gt;&gt; &gt;&gt;&gt;&gt; its components<br>
&gt;&gt; &gt;&gt;&gt;&gt;<br>
&gt;&gt; &gt;&gt;&gt;&gt; 1. Event Logs + Stages:<br>
&gt;&gt; &gt;&gt;&gt;&gt; =3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D<br>
&gt;&gt; &gt;&gt;&gt;&gt;<br>
&gt;&gt; &gt;&gt;&gt;&gt; As I said before, In the spark logs (the log4j co=
nfigurable ones<br>
&gt;&gt; &gt;&gt;&gt;&gt; from<br>
&gt;&gt; &gt;&gt;&gt;&gt; the driver), I only see references to some stages=
, where the stage<br>
&gt;&gt; &gt;&gt;&gt;&gt; IDs<br>
&gt;&gt; &gt;&gt;&gt;&gt; are not arithmetically increasing. In the event l=
ogs, however, I<br>
&gt;&gt; &gt;&gt;&gt;&gt; will<br>
&gt;&gt; &gt;&gt;&gt;&gt; see reference to *every* stage, although not all =
stages will have<br>
&gt;&gt; &gt;&gt;&gt;&gt; tasks associated with them.<br>
&gt;&gt; &gt;&gt;&gt;&gt;<br>
&gt;&gt; &gt;&gt;&gt;&gt; For instance, to examine the actual stages that h=
ave tasks, you can<br>
&gt;&gt; &gt;&gt;&gt;&gt; see missing stages:<br>
&gt;&gt; &gt;&gt;&gt;&gt; # grep -E &#39;&quot;Event&quot;:&quot;SparkListe=
nerTaskEnd&quot;&#39; app.log \<br>
&gt;&gt; &gt;&gt;&gt;&gt; #=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0=
 =C2=A0| grep -Eo &#39;&quot;Stage ID&quot;:[[:digit:]]+&#39;=C2=A0 \<br>
&gt;&gt; &gt;&gt;&gt;&gt; #=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0=
 =C2=A0| sort -n|uniq | head -n 5<br>
&gt;&gt; &gt;&gt;&gt;&gt; &quot;Stage ID&quot;:0<br>
&gt;&gt; &gt;&gt;&gt;&gt; &quot;Stage ID&quot;:1<br>
&gt;&gt; &gt;&gt;&gt;&gt; &quot;Stage ID&quot;:10<br>
&gt;&gt; &gt;&gt;&gt;&gt; &quot;Stage ID&quot;:11<br>
&gt;&gt; &gt;&gt;&gt;&gt; &quot;Stage ID&quot;:110<br>
&gt;&gt; &gt;&gt;&gt;&gt;<br>
&gt;&gt; &gt;&gt;&gt;&gt; However, these &quot;missing&quot; stages *do* ap=
pear in the event logs as<br>
&gt;&gt; &gt;&gt;&gt;&gt; Stage<br>
&gt;&gt; &gt;&gt;&gt;&gt; IDs in the jobs submitted, i.e: for<br>
&gt;&gt; &gt;&gt;&gt;&gt; # grep -E &#39;&quot;Event&quot;:&quot;SparkListe=
nerJobStart&quot;&#39; app.log | grep -Eo<br>
&gt;&gt; &gt;&gt;&gt;&gt; &#39;Stage<br>
&gt;&gt; &gt;&gt;&gt;&gt; IDs&quot;:\[.*\]&#39; | head -n 5<br>
&gt;&gt; &gt;&gt;&gt;&gt; Stage IDs&quot;:[0,1,2]<br>
&gt;&gt; &gt;&gt;&gt;&gt; Stage IDs&quot;:[5,3,4]<br>
&gt;&gt; &gt;&gt;&gt;&gt; Stage IDs&quot;:[6,7,8]<br>
&gt;&gt; &gt;&gt;&gt;&gt; Stage IDs&quot;:[9,10,11]<br>
&gt;&gt; &gt;&gt;&gt;&gt; Stage IDs&quot;:[12,13,14]<br>
&gt;&gt; &gt;&gt;&gt;&gt;<br>
&gt;&gt; &gt;&gt;&gt;&gt; I do not know if this amounts to a bug, since I a=
m not familiar with<br>
&gt;&gt; &gt;&gt;&gt;&gt; the scheduler in detail. The stages have seemingl=
y been created<br>
&gt;&gt; &gt;&gt;&gt;&gt; somewhere in the DAG, but then have no associated=
 tasks and never<br>
&gt;&gt; &gt;&gt;&gt;&gt; appear again.<br>
&gt;&gt; &gt;&gt;&gt;&gt;<br>
&gt;&gt; &gt;&gt;&gt;&gt; 2. Extracting Event Log Information<br>
&gt;&gt; &gt;&gt;&gt;&gt; =3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D<br>
&gt;&gt; &gt;&gt;&gt;&gt; Currently we are running scalability tests, and a=
re finding very<br>
&gt;&gt; &gt;&gt;&gt;&gt; poor<br>
&gt;&gt; &gt;&gt;&gt;&gt; scalability for certain block matrix algorithms. =
I would like to<br>
&gt;&gt; &gt;&gt;&gt;&gt; have<br>
&gt;&gt; &gt;&gt;&gt;&gt; finer detail about the communication time and ban=
dwidth when data is<br>
&gt;&gt; &gt;&gt;&gt;&gt; transferred between nodes.<br>
&gt;&gt; &gt;&gt;&gt;&gt;<br>
&gt;&gt; &gt;&gt;&gt;&gt; I would really just like to have a file with noth=
ing but task info<br>
&gt;&gt; &gt;&gt;&gt;&gt; in<br>
&gt;&gt; &gt;&gt;&gt;&gt; a format such as:<br>
&gt;&gt; &gt;&gt;&gt;&gt; timestamp (ms), task ID, hostname, execution time=
 (ms), GC time<br>
&gt;&gt; &gt;&gt;&gt;&gt; (ms),<br>
&gt;&gt; &gt;&gt;&gt;&gt; ...<br>
&gt;&gt; &gt;&gt;&gt;&gt; 0010294, 1, slave-1, 503, 34, ...<br>
&gt;&gt; &gt;&gt;&gt;&gt; 0010392, 2, slave-2, 543, 32, ...<br>
&gt;&gt; &gt;&gt;&gt;&gt; and similarly for jobs/stages/rdd_memory/shuffle =
output/etc.<br>
&gt;&gt; &gt;&gt;&gt;&gt;<br>
&gt;&gt; &gt;&gt;&gt;&gt; I have extracted the relevant time fields from th=
e spark event logs<br>
&gt;&gt; &gt;&gt;&gt;&gt; with a sed script, but I wonder if there is an ev=
en more expedient<br>
&gt;&gt; &gt;&gt;&gt;&gt; way. Unfortunately, I do not immediately see how =
to do this using<br>
&gt;&gt; &gt;&gt;&gt;&gt; the<br>
&gt;&gt; &gt;&gt;&gt;&gt; $SPARK_HOME/conf/metrics.properties file and have=
n&#39;t come across a<br>
&gt;&gt; &gt;&gt;&gt;&gt; blog/etc that describes this. Could anyone please=
 comment on whether<br>
&gt;&gt; &gt;&gt;&gt;&gt; or not a metrics configuation for this already ex=
ists?<br>
&gt;&gt; &gt;&gt;&gt;&gt;<br>
&gt;&gt; &gt;&gt;&gt;&gt; 3. Time Metrics in Spark Event Log<br>
&gt;&gt; &gt;&gt;&gt;&gt; =3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D<br>
&gt;&gt; &gt;&gt;&gt;&gt; I am confused about the times reported for tasks =
in the event log.<br>
&gt;&gt; &gt;&gt;&gt;&gt; There are launch and finish timestamps given for =
each task (call<br>
&gt;&gt; &gt;&gt;&gt;&gt; them<br>
&gt;&gt; &gt;&gt;&gt;&gt; t1 and t2, respectively), as well as GC time (t_g=
c), execution time<br>
&gt;&gt; &gt;&gt;&gt;&gt; (t_exec), and serialization times (t_ser, t_deser=
). However the<br>
&gt;&gt; &gt;&gt;&gt;&gt; times<br>
&gt;&gt; &gt;&gt;&gt;&gt; do not add up as I would have expected. I would i=
magine that the<br>
&gt;&gt; &gt;&gt;&gt;&gt; elapsed time t2 - t1 would be slightly larger tha=
n the sum of the<br>
&gt;&gt; &gt;&gt;&gt;&gt; component times. However, I can find many instanc=
es in the event<br>
&gt;&gt; &gt;&gt;&gt;&gt; logs<br>
&gt;&gt; &gt;&gt;&gt;&gt; where:<br>
&gt;&gt; &gt;&gt;&gt;&gt; (t2 - t1) &lt; (t_gc + t_ser + t_deser + t_exec)<=
br>
&gt;&gt; &gt;&gt;&gt;&gt; The difference can be 500 ms or more, which is no=
t negligible for my<br>
&gt;&gt; &gt;&gt;&gt;&gt; current execution times of ~5000 ms. I have attac=
hed a plot that<br>
&gt;&gt; &gt;&gt;&gt;&gt; illustrates this.<br>
&gt;&gt; &gt;&gt;&gt;&gt;<br>
&gt;&gt; &gt;&gt;&gt;&gt; Regarding this, I&#39;d like to ask:<br>
&gt;&gt; &gt;&gt;&gt;&gt; 1. How exactly are these times are being measured=
?<br>
&gt;&gt; &gt;&gt;&gt;&gt; 2. Should the sum of the component times equal th=
e elapsed (clock)<br>
&gt;&gt; &gt;&gt;&gt;&gt; time for the task?<br>
&gt;&gt; &gt;&gt;&gt;&gt; 3. If not, which component(s) is(are) being exclu=
ded, and when do<br>
&gt;&gt; &gt;&gt;&gt;&gt; they<br>
&gt;&gt; &gt;&gt;&gt;&gt; occur?<br>
&gt;&gt; &gt;&gt;&gt;&gt; 4. There are occasionally reported measurements f=
or Shuffle Write<br>
&gt;&gt; &gt;&gt;&gt;&gt; time, but not shuffle read time. Is there a metho=
d to determine the<br>
&gt;&gt; &gt;&gt;&gt;&gt; time required to shuffle data? Could this be done=
 by look at delays<br>
&gt;&gt; &gt;&gt;&gt;&gt; between the first task in a new stage and the las=
t task in the<br>
&gt;&gt; &gt;&gt;&gt;&gt; previous stage?<br>
&gt;&gt; &gt;&gt;&gt;&gt;<br>
&gt;&gt; &gt;&gt;&gt;&gt; Thank you very much for your time,<br>
&gt;&gt; &gt;&gt;&gt;&gt; Mike<br>
&gt;&gt; &gt;&gt;&gt;&gt;<br>
&gt;&gt; &gt;&gt;&gt;&gt;<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt; Hey Mike,<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt;<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt; Stage ID&#39;s are not guaranteed to be sequ=
ential because of the way<br>
&gt;&gt; the<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt; DAG scheduler works (only increasing). In so=
me cases stage ID<br>
&gt;&gt; numbers<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt; are skipped when stages are generated.<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt;<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt; Any stage/ID that appears in the Spark UI is=
 an actual stage, so<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt; if<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt; you see ID&#39;s in there, but they are not =
in the logs, then let us<br>
&gt;&gt; know<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt; (that would be a bug).<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt;<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt; - Patrick<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt;<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt; &lt;<a href=3D""mailto:akhil@sigmoidanalytics=
.com"">akhil@sigmoidanalytics.com</a>&gt;<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt; Are you seeing the same behavior on the =
driver UI? (that running<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt; on<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt; port<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt; 4040), If you click on the stage id head=
er you can sort the<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt; stages<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt; based<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt; on<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt; IDs.<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt; Thanks<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt; Best Regards<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt;<br>
nes &lt;<a href=3D""mailto:91mbbh@gmail.com"">91mbbh@gmail.com</a>&gt;<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt;&gt;<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt;&gt; Hi folks,<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt;&gt;<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt;&gt; When I look at the output logs for a=
n iterative Spark program, I<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt;&gt; see<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt;&gt; that the stage IDs are not arithmeti=
cally numbered---that is,<br>
&gt;&gt; there<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt;&gt; are gaps between stages and I might =
find log information about<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt;&gt; Stage<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt;&gt; 0, 1,2, 5, but not 3 or 4.<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt;&gt;<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt;&gt; As an example, the output from the S=
park logs below shows what I<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt;&gt; mean:<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt;&gt;<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt;&gt; # grep -rE &quot;Stage [[:digit:]]+&=
quot; spark_stderr=C2=A0 | grep finished<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt;&gt; 12048:INFO:DAGScheduler:Stage 0 (map=
Partitions at<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt;&gt; blockMap.scala:1444)<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt;&gt; finished in 7.820 s:<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt;&gt; 15994:INFO:DAGScheduler:Stage 1 (map=
 at blockMap.scala:1810)<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt;&gt; finished<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt;&gt; in 3.874 s:<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt;&gt; 18291:INFO:DAGScheduler:Stage 2 (cou=
nt at blockMap.scala:1179)<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt;&gt; finished in 2.237 s:<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt;&gt; 20121:INFO:DAGScheduler:Stage 4 (map=
 at blockMap.scala:1817)<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt;&gt; finished<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt;&gt; in 1.749 s:<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt;&gt; 21254:INFO:DAGScheduler:Stage 5 (cou=
nt at blockMap.scala:1180)<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt;&gt; finished in 1.082 s:<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt;&gt; 23422:INFO:DAGScheduler:Stage 7 (map=
 at blockMap.scala:1810)<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt;&gt; finished<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt;&gt; in 2.078 s:<br>
&gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt;&gt; 24773:INFO:DAGScheduler:Stage 8 (cou=
nt at blockMap.scala:"
"
shane knapp <sknapp@berkeley.edu>,Thu"," 11 Jun 2015 11:43:38 -0400""","Re: [DISCUSS] Minimize use of MINOR, BUILD, and HOTFIX w/ no JIRA",Joseph Bradley <joseph@databricks.com>,"+1, and i know i've been guilty of this in the past.  :)


"
Patrick Wendell <pwendell@gmail.com>,"Thu, 11 Jun 2015 09:05:06 -0700",[ANNOUNCE] Announcing Spark 1.4,"""dev@spark.apache.org"" <dev@spark.apache.org>, ""user@spark.apache.org"" <user@spark.apache.org>","Hi All,

I'm happy to announce the availability of Spark 1.4.0! Spark 1.4.0 is
the fifth release on the API-compatible 1.X line. It is Spark's
largest release ever, with contributions from 210 developers and more
than 1,000 commits!

A huge thanks go to all of the individuals and organizations involved
in development and testing of this release.

Visit the release notes [1] to read about the new features, or
download [2] the release today.

For errata in the contributions or release notes, please e-mail me
*directly* (not on-list).

Thanks to everyone who helped work on this release!

[1] http://spark.apache.org/releases/spark-release-1-4-0.html
[2] http://spark.apache.org/downloads.html

---------------------------------------------------------------------


"
Kay Ousterhout <keo@eecs.berkeley.edu>,"Thu, 11 Jun 2015 16:19:45 -0700",Re: Stages with non-arithmetic numbering & Timing metrics in event logs,Imran Rashid <irashid@cloudera.com>,"Hereâ€™s how the shuffle works.  This explains what happens for a single
task; this will happen in parallel for each task running on the machine,
and as Imran said, Spark runs up to â€œnumCoresâ€ tasks concurrently on each
machine.  There's also an answer to the original question about why CPU use
is low at the very bottom.

The key data structure used in fetching shuffle data is the â€œresultsâ€ queue
in ShuffleBlockFetcherIterator, which buffers data that we have in
serialized (and maybe compressed) form, but havenâ€™t yet deserialized /
processed.  The results queue is filled by many threads fetching data over
the network (the number of concurrent threads fetching data is equal to the
number of remote executors weâ€™re currently fetching data from) [0], and is
consumed by a single thread that deserializes the data and computes some
function over it (e.g., if youâ€™re doing rdd.count(), the thread
deserializes the data and counts the number of items).  As we fetch data
over the network, we track bytesInFlight, which is data that has been
requested (and possibly received) from a remote executor, but that hasnâ€™t
yet been deserialized / processed by the consumer thread.  So, this
includes all of the data in the â€œresultsâ€ queue, and possibly more data
thatâ€™s currently outstanding over the network.  We always issue as many
requests as we can, with the constraint that bytesInFlight remains less
than a specified maximum [1].

In a little more detail, hereâ€™s exactly what happens when a task begins
reading shuffled data:

(1) Issue requests [1.5] to fetch up to maxBytesInFlight bytes of data [1]
over the network (this happens here
<https://github.com/apache/spark/blob/95690a17d328f205c3398b9b477b4072b6fe908f/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala#L260>
).

These requests are all executed asynchronously using a ShuffleClient [2]
via the shuffleClient.fetchBlocks call
<https://github.com/apache/spark/blob/95690a17d328f205c3398b9b477b4072b6fe908f/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala#L149>
 [3].  We pass in a callback that, once a block has been successfully
fetched, sticks it on the â€œresultsâ€ queue.

from the local block manager (which memory maps the file) and then stick
the result onto the results queue
<https://github.com/apache/spark/blob/95690a17d328f205c3398b9b477b4072b6fe908f/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala#L230>.
Because we memory map the files, which is speedy, the local data typically
all ends up on the results in front of the remote data.

 but not
finished!) and weâ€™ve â€œreadâ€ (memory-mapped) the local data (i.e., (1) and
(2) have happened), ShuffleBlockFetcherIterator returns an iterator that
gets wrapped too many times to count [4] and eventually gets unrolled [5].
Each time next() is called on the iterator, it blocks waiting for an item
from the results queue.  This may return right away, or if the queue is
empty, will block waiting on new data from the network [6].  Before
returning from next(), we update our accounting for the bytes in flight:
the chunk of data we return is no longer considered in-flight, because itâ€™s
about to be processed, so we update the current bytesInFlight, and if it
wonâ€™t result in > maxBytesInFlight outstanding, send some more requests for
data.

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

Notes:

[0] Note that these threads consume almost no CPU resources, because they
just receive data from the OS and then execute a callback that sticks the
data on the results queue.

[1] We limit the data outstanding on the network to avoid using too much
memory to hold the data weâ€™ve fetched over the network but havenâ€™t yet
processed.

[1.5] Each request may include multiple shuffle blocks, where is a ""block""
is the data output for this reduce task by a particular map task.  All of
the reduce tasks for a shuffle read a total of # map tasks * # reduce tasks
shuffle blocks; each reduce task reads # map tasks blocks.  We do some hacks
<https://github.com/apache/spark/blob/95690a17d328f205c3398b9b477b4072b6fe908f/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala#L177>
to try to size these requests in a ""good"" way: we limit each request to
about maxBytesInFlight / 5, so that we can fetch from roughly 5 machines
concurrently without exceeding maxBytesInFlight.  5 is completely a magic
number here that was probably guessed by someone long long ago, and it
seems to work ok.

[2] The default configuration uses NettyBlockTransferService as the
ShuffleClient implementation (note that this extends BlockTransferService,
which extends ShuffleClient).

[3] If youâ€™re curious how the shuffle client fetches data, the default
Spark configuration results in exactly one TCP connection from an executor
to each other executor.  If executor A is getting shuffle data from
executor B, we start by sending an OpenBlocks message from A to B.  The
OpenBlocks message includes the list of blocks that A wants to fetch, and
causes the remote executor, B, to start to pull the corresponding data into
memory from disk (we typically memory map the files, so this may not
actually result in the data being read yet), and also to store some state
associated with this â€œstreamâ€ of data.  The remote executor, B, responds
with a stream ID that helps it to identify the connection.  Next, A
requests blocks one at a time from B using an ChunkFetchRequest message
(this happens here
<https://github.com/apache/spark/blob/95690a17d328f205c3398b9b477b4072b6fe9eBlockFetcher.java#L101>
in
<https://github.com/apache/spark/blob/master/network/common/src/main/java/org/apache/spark/network/client/TransportClient.java#L97>
in TransportClient; currently, we have a one-to-one mapping from a chunk to
a particular block).  Itâ€™s possible that there are many sets of shuffle
data being fetched concurrently between A and B (e.g., because many tasks
are run concurrently).  These requests are serialized, so one block is sent
at a time from B, and theyâ€™re sent in the order that the requests were
issued on A.

[4] In BlockStoreShuffleFetcher, which handles failures; then in
HashShuffleReader, which helps aggregate some of the data; etc.

[5] This happens in BlockManager.putIterator, if the RDD is going to be
cached; in the function passed in to ResultTask, if this is the last stage
in a job; or via the writer.write() call in ShuffleMapTask, if this is a
stage that generates intermediate shuffle data.

[6] We time how long we spend blocking on data from the network; this is
whatâ€™s shown as â€œfetch wait timeâ€ in Sparkâ€™s UI.

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

To answer the original question about why CPU use is low, this means that
the main thread (that pulls data off of the results queue) is blocking on
I/O.  It could be blocking to receive data over the network (which will be
included in the fetch wait time shown in the UI, described above), or it
could be blocking to read data from the local disk (because the shuffle
data that is read locally is memory-mapped, so the thread may block on disk
if the data hasnâ€™t been read into memory yet).  It could also be blocking
because itâ€™s writing new shuffle output to disk (also shown in the UI, as
shuffle write time), or because itâ€™s spilling intermediate data.
Everything except the spilling is shown in the handy-dandy new
visualization that Kousuke recently added
<https://github.com/apache/spark/commit/a5f7b3b9c7f05598a1cc8e582e5facee1029cd5e>to
the UI (available in 1.4 onwards): if you look at the stage view, and click
on â€œEvent Timelineâ€, youâ€™ll see a visualization of the tasks and how long
they spent blocked on various things (we should add the spilling to
thisâ€¦there is a long-outstanding JIRA for this).

Hope this helps!

-Kay


v
ch
y
n
u
l
om
e
a
in
d.
s
 a
u
 a
s
ng
o
on
en
s
(x %
 I
ct
s
t
d.
It
e
he
e
====
ge
e
r
===============
a
fo
nt
g
 a
=============
g.
e
r
k)
o
e
so
us
,
ut
d
9)
0)
8)
)
)
4)
)
5)
DBC
pe
ed
s
t
d
an
"
Gerard Maas <gerard.maas@gmail.com>,"Fri, 12 Jun 2015 01:46:34 +0200",Re: Stages with non-arithmetic numbering & Timing metrics in event logs,Kay Ousterhout <keo@eecs.berkeley.edu>,"Kay,

Excellent write-up. This should be preserved for reference somewhere
searchable.

-Gerard.




ingle
urrently on each
se
ltsâ€
zed /
r
he
], and is
â€™t
ibly more data
s many
begins
]
e908f/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala#L260>
e908f/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala#L149>
tick
e908f/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala#L230>.
y
d, but
he local data (i.e., (1)
d
n
â€™s
quests for
â€”â€”â€”â€”â€”â€”â€”â€”
â€™t yet
""
ks
e908f/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala#L177>
,
fault
r
to
or, B, responds
/org/apache/spark/network/client/TransportClient.java#L97>
to
huffle
nt
 were
e
™s UI.
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
e
sk
blocking
e UI, as
029cd5e>to
ck
f the tasks and how long
ev
ich
ly
y
an
ou
o
g
ll
rom
m
ze
 a
f
f
ou
d
y
e
to
t
:
I
is
 (x %
g
,
,
as
at
re
he
====
s
I
ve
s
er
===============
y
o
ng
s
=============
l
e
he
t
t
do
te
m
s,
ed
)
)
)
7)
5)
5)
JDBC
;
s
ut
r
-
"
Kay Ousterhout <keo@eecs.berkeley.edu>,"Thu, 11 Jun 2015 17:07:33 -0700",Re: Stages with non-arithmetic numbering & Timing metrics in event logs,Gerard Maas <gerard.maas@gmail.com>,"Good idea -- I've added this to the wiki:
https://cwiki.apache.org/confluence/display/SPARK/Shuffle+Internals.  Happy
to stick it elsewhere if folks think there's a more convenient place.


single
currently on each
use
ultsâ€
ized /
er
the
0], and is
â€™t
sibly more data
as many
 begins
fe908f/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala#L260>
fe908f/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala#L149>
stick
fe908f/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala#L230>.
ly
ed, but
the local data (i.e., (1)
ed
an
e
tâ€™s
equests for
â€”â€”â€”â€”â€”â€”â€”â€”
y
e
â€™t yet
.
 We
fe908f/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala#L177>
c
e,
efault
or
d
nto
e
tor, B, responds
a/org/apache/spark/network/client/TransportClient.java#L97>
 to
shuffle
s
ent
s were
ge
™s UI.
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
t
n
be
isk
 blocking
he UI, as
1029cd5e>to
ick
of the tasks and how long
dev
hich
ely
ly
 an
You
so
ng
ill
from
'm
s
t
ize
s
s
ve
g
ut
h
l
d
 I
I
d
e
e
d
I
e
e
k
d
s
=====
es
 I
u
as
r
d
================
ry
to
e
==============
ll
e
he
nt
at
e
e
D
,
t
0)
7)
0)
d
,
 JDBC
C;
ns
n
o
g
er
--
"
zsampson <zsampson@palantir.com>,"Thu, 11 Jun 2015 20:08:07 -0700 (MST)",When to expect UTF8String?,dev@spark.apache.org,"I'm hoping for some clarity about when to expect String vs UTF8String when
using the Java DataFrames API.

In upgrading to Spark 1.4, I'm dealing with a lot of errors where what was
once a String is now a UTF8String. The comments in the file and the related
commit message indicate that maybe it should be internal to SparkSQL's
implementation. 

However, when I add a column containing a custom subclass of Expression, the
row passed to the eval method contains instances of UTF8String. Ditto for
AggregateFunction.update. Is this expected? If so, when should I generally
know to deal with UTF8String objects?



--

---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Thu, 11 Jun 2015 21:05:34 -0700",Re: When to expect UTF8String?,zsampson <zsampson@palantir.com>,"Through the DataFrame API, users should never see UTF8String.

Expression (and any class in the catalyst package) is considered internal
and so uses the internal representation of various types.  Which type we
use here is not stable across releases.

Is there a reason you aren't defining a UDF instead?


"
Usman Ehtesham <uehtesham90@gmail.com>,"Fri, 12 Jun 2015 00:06:39 -0400",Contributing to pyspark,dev@spark.apache.org,"Hello,

I am currently taking a course in Apache Spark via EdX (
https://www.edx.org/course/introduction-big-data-apache-spark-uc-berkeleyx-cs100-1x)
and at the same time I try to look at the code for pySpark too. I wanted to
ask, if ideally I would like to contribute to pyspark specifically, how can
I do that? I do not intend to contribute to core Apache Spark any time soon
(mainly because I do not know Scala) but I am very comfortable in Python.

Any tips on how to contribute specifically to pyspark without being
affected by other parts of Spark would be greatly appreciated.

P.S.: I ask this because there is a small change/improvement I would like
to propose. Also since I just started learning Spark, I would like to also
read and understand the pyspark code as I learn about Spark. :)

Hope to hear from you soon.

Usman Ehtesham Gul
https://github.com/ueg1990
"
Manoj Kumar <manojkumarsivaraj334@gmail.com>,"Fri, 12 Jun 2015 10:27:54 +0530",Re: Contributing to pyspark,Usman Ehtesham <uehtesham90@gmail.com>,"Hi,

Thanks for your interest in PySpark.

The first thing is to have a look at the ""how to contribute"" guide
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark and
filter the JIRA's using the label PySpark.

If you have your own improvement in mind, you can file your a JIRA, discuss
and then send a Pull Request

HTH

Regards.





-- 
Godspeed,
Manoj Kumar,
http://manojbits.wordpress.com
<http://goog_1017110195>
http://github.com/MechCoder
"
Saisai Shao <sai.sai.shao@gmail.com>,"Fri, 12 Jun 2015 13:02:47 +0800",Re: Spark 1.4: Python API for getting Kafka offsets in direct mode?,Amit Ramesh <amit@yelp.com>,"Hi,

What is your meaning of getting the offsets from the RDD, from my
understanding, the offsetRange is a parameter you offered to KafkaRDD, why
do you still want to get the one previous you set into?

Thanks
Jerry

2015-06-12 12:36 GMT+08:00 Amit Ramesh <amit@yelp.com>:

"
Amit Ramesh <amit@yelp.com>,"Thu, 11 Jun 2015 22:48:48 -0700",Re: Spark 1.4: Python API for getting Kafka offsets in direct mode?,Saisai Shao <sai.sai.shao@gmail.com>,"Hi Jerry,

Take a look at this example:
https://spark.apache.org/docs/latest/streaming-kafka-integration.html#tab_scala_2

The offsets are needed because as RDDs get generated within spark the
offsets move further along. With direct Kafka mode the current offsets are
no more persisted in Zookeeper but rather within Spark itself. If you want
to be able to use zookeeper based monitoring tools to keep track of
progress, then this is needed.

In my specific case we need to persist Kafka offsets externally so that we
can continue from where we left off after a code deployment. In other
words, we need exactly-once processing guarantees across code deployments.
Spark does not support any state persistence across deployments so this is
something we need to handle on our own.

Hope that helps. Let me know if not.

Thanks!
Amit



"
Saisai Shao <sai.sai.shao@gmail.com>,"Fri, 12 Jun 2015 13:54:07 +0800",Re: Spark 1.4: Python API for getting Kafka offsets in direct mode?,Amit Ramesh <amit@yelp.com>,"OK, I get it, I think currently Python based Kafka direct API do not
provide such equivalence like Scala, maybe we should figure out to add this
into Python API also.

2015-06-12 13:48 GMT+08:00 Amit Ramesh <amit@yelp.com>:

"
Amit Ramesh <amit@yelp.com>,"Thu, 11 Jun 2015 22:59:08 -0700",Re: Spark 1.4: Python API for getting Kafka offsets in direct mode?,Saisai Shao <sai.sai.shao@gmail.com>,"Thanks, Jerry. That's what I suspected based on the code I looked at. Any
pointers on what is needed to build in this support would be great. This is
critical to the project we are currently working on.

Thanks!



"
Saisai Shao <sai.sai.shao@gmail.com>,"Fri, 12 Jun 2015 14:05:23 +0800",Re: Spark 1.4: Python API for getting Kafka offsets in direct mode?,Amit Ramesh <amit@yelp.com>,"Scala KafkaRDD uses a trait to handle this problem, but it is not so easy
and straightforward in Python, where we need to have a specific API to
handle this, I'm not sure is there any simple workaround to fix this, maybe
we should think carefully about it.

2015-06-12 13:59 GMT+08:00 Amit Ramesh <amit@yelp.com>:

"
Zhiwei Chan <z.w.chan.jason@gmail.com>,"Fri, 12 Jun 2015 17:04:14 +0800",A confusing ClassNotFoundException error,dev@spark.apache.org,"Hi all,

I encounter an error at spark 1.4.0, and I make an error example as
following. Both of the code can run OK on spark-shell, but the second code
encounter an error using spark-submit. The only different is that the
second code uses a literal function in the map(). but the first code uses a
defined function in the map().

Could anyone tell me why this error happen?  Thanks

======================the ok code =================
    val sparkConf = new SparkConf().setAppName(""Mode Example"")
    val sc = new SparkContext(sparkConf)
    val hive = new HiveContext(sc)

    val rdd = sc.parallelize(Range(0,1000))
    def fff = (input: Int)=>input+1
    val cnt = rdd.map(i => Array(1,2,3)).map(arr => {
      arr.map(fff)
    }).count()
    print(cnt)

==============not ok code ============
    val sparkConf = new SparkConf().setAppName(""Mode Example"")
    val sc = new SparkContext(sparkConf)
    val hive = new HiveContext(sc)

    val rdd = sc.parallelize(Range(0,1000))
    //def fff = (input: Int)=>input+1
    val cnt = rdd.map(i => Array(1,2,3)).map(arr => {
      arr.map(_+1)
    }).count()
    print(cnt)

Submit command: spark-submit --class com.yhd.ycache.magic.Model --jars
./SSExample-0.0.2-SNAPSHOT-jar-with-dependencies.jar  --master local
./SSExample-0.0.2-SNAPSHOT-jar-with-dependencies.jar

the error info:
Exception in thread ""main"" java.lang.ClassNotFoundException:
com.yhd.ycache.magic.Model$$anonfun$10$$anonfun$apply$1
at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
at java.security.AccessController.doPrivileged(Native Method)
at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
at java.lang.Class.forName0(Native Method)
at java.lang.Class.forName(Class.java:270)
at
org.apache.spark.util.InnerClosureFinder$$anon$4.visitMethodInsn(ClosureCleaner.scala:455)
at
com.esotericsoftware.reflectasm.shaded.org.objectweb.asm.ClassReader.accept(Unknown
Source)
at
com.esotericsoftware.reflectasm.shaded.org.objectweb.asm.ClassReader.accept(Unknown
Source)
at
org.apache.spark.util.ClosureCleaner$.getInnerClosureClasses(ClosureCleaner.scala:101)
at
org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:197)
at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:132)
at org.apache.spark.SparkContext.clean(SparkContext.scala:1891)
at org.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:294)
at org.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:293)
at
org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:148)
at
org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:109)
at org.apache.spark.rdd.RDD.withScope(RDD.scala:286)
at org.apache.spark.rdd.RDD.map(RDD.scala:293)
at com.yhd.ycache.magic.Model$.main(SSExample.scala:238)
at com.yhd.ycache.magic.Model.main(SSExample.scala)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at
org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:664)
at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:169)
at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:192)
at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:111)
at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
"
Sean Owen <sowen@cloudera.com>,"Fri, 12 Jun 2015 11:09:52 +0100",Remove Hadoop 1 support (Hadoop <2.2) for Spark 1.5?,dev <dev@spark.apache.org>,"How does the idea of removing support for Hadoop 1.x for Spark 1.5
strike everyone? Really, I mean, Hadoop < 2.2, as 2.2 seems to me more
consistent with the modern 2.x line than 2.1 or 2.0.

The arguments against are simply, well, someone out there might be
using these versions.

The arguments for are just simplification -- fewer gotchas in trying
to keep supporting older Hadoop, of which we've seen several lately.
We get to chop out a little bit of shim code and update to use some
non-deprecated APIs. Along with removing support for Java 6, it might
be a reasonable time to also draw a line under older Hadoop too.

I'm just gauging feeling now: for, against, indifferent?
I favor it, but would not push hard on it if there are objections.

---------------------------------------------------------------------


"
Cheng Lian <lian.cs.zju@gmail.com>,"Fri, 12 Jun 2015 19:02:32 +0800","Re: How to support dependency jars and files on HDFS in standalone
 cluster mode?",Dong Lei <donglei@microsoft.com>,"Would you mind to file a JIRA for this? Thanks!

Cheng


"
Manoj Kumar <manojkumarsivaraj334@gmail.com>,"Fri, 12 Jun 2015 18:02:42 +0530",Re: Contributing to pyspark,Usman Ehtesham Gul <uehtesham90@gmail.com>,"1, Yes, because the issues are in JIRA.
2. Nope, (at least as far as MLlib is concerned) because most if it are
just wrappers to the underlying Scala functions or methods and are not
implemented in pure Python.
3. I'm not sure about this. It seems to work fine for me!

HTH




-- 
Godspeed,
Manoj Kumar,
http://manojbits.wordpress.com
<http://goog_1017110195>
http://github.com/MechCoder
"
Steve Loughran <stevel@hortonworks.com>,"Fri, 12 Jun 2015 13:13:32 +0000",Re: Remove Hadoop 1 support (Hadoop <2.2) for Spark 1.5?,,"+1 for 2.2+

Not only are the APis in Hadoop 2 better, there's more people testing Hadoop 2.x & spark, and bugs in Hadoop itself being fixed.

(usual disclaimers, I work off branch-2.7 snapshots I build nightly, etc)



-----------------------------------"
Cody Koeninger <cody@koeninger.org>,"Fri, 12 Jun 2015 08:59:04 -0500",Re: Spark 1.4: Python API for getting Kafka offsets in direct mode?,Saisai Shao <sai.sai.shao@gmail.com>,"you to pass a message handler that gets full access to the kafka
MessageAndMetadata, including offset.

I don't know why the python api was developed with only one way to call
createDirectStream, but the first thing I'd look at would be adding that
functionality back in.  If someone wants help creating a patch for that,
just let me know.

Dealing with offsets on a per-message basis may not be as efficient as
dealing with them on a batch basis using the HasOffsetRanges interface...
but if efficiency was a primary concern, you probably wouldn't be using
Python anyway.


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 12 Jun 2015 15:45:00 +0000",Re: Remove Hadoop 1 support (Hadoop <2.2) for Spark 1.5?,Steve Loughran <stevel@hortonworks.com>,"I'm personally in favor, but I don't have a sense of how many people still
rely on Hadoop 1.

Nick

2015ë…„ 6ì›” 12ì¼ (ê¸ˆ) ì˜¤ì „ 9:13, Steve Loughran
stevel@hortonworks.com>ë‹˜ì´ ìž‘ì„±:

+1 for 2.2+
"
Patrick Wendell <pwendell@gmail.com>,"Fri, 12 Jun 2015 09:12:00 -0700",Re: Remove Hadoop 1 support (Hadoop <2.2) for Spark 1.5?,Nicholas Chammas <nicholas.chammas@gmail.com>,"I feel this is quite different from the Java 6 decision and personally
I don't see sufficient cause to do it.

I would like to understand though Sean - what is the proposal exactly?
Hadoop 2 itself supports all of the Hadoop 1 API's, so things like
removing the Hadoop 1 variant of sc.hadoopFile, etc, I don't think
that makes much sense since so many libraries still use those API's.
For YARN support, we already don't support Hadoop 1. So I'll assume
what you mean is to prevent or stop supporting from linking against
the Hadoop 1 filesystem binaries at runtime (is that right?).

The main reason I'd push back is that I do think there are still
people running the older versions. For instance at Databricks we use
the FileSystem library for talking to S3... every time we've tried to
upgrade to Hadoop 2.X there have been significant regressions in
performance and we've had to downgrade. That's purely anecdotal, but I
think you have people out there using the Hadoop 1 bindings for whom
upgrade would be a pain.

In terms of our maintenance cost, to me the much bigger cost for us
IMO is dealing with differences between e.g. 2.2, 2.4, and 2.6 where
major new API's were added. In comparison the Hadoop 1 vs 2 seems
fairly low with just a few bugs cropping up here and there. So unlike
Java 6 where you have a critical mass of maintenance issues, security
issues, etc, I just don't see as compelling a cost here.

To me the framework for deciding about these upgrades is the
maintenance cost vs the inconvenience for users.

- Patrick

l
 Steve Loughran
)

---------------------------------------------------------------------


"
Ram Sriharsha <sriharsha.ram@gmail.com>,"Fri, 12 Jun 2015 09:11:48 -0700",Re: Remove Hadoop 1 support (Hadoop <2.2) for Spark 1.5?,Nicholas Chammas <nicholas.chammas@gmail.com>,"+1 for Hadoop 2.2+


l
 Steve Loughran
)
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Fri, 12 Jun 2015 09:39:11 -0700",Re: Remove Hadoop 1 support (Hadoop <2.2) for Spark 1.5?,Ram Sriharsha <sriharsha.ram@gmail.com>,"My 2 cents: The biggest reason from my view for keeping Hadoop 1 support
was that our EC2 scripts which launch an environment for benchmarking /
testing / research only supported Hadoop 1 variants till very recently.  We
did add Hadoop 2.4 support a few weeks back but that it is still not the
default option.

My concern is that people have higher level projects which are linked to
Hadoop 1.0.4 + Spark, because that is the default environment on EC2, and
that users will be surprised when these applications stop working in Spark
1.5. I guess we could announce more widely and write transition guides, but
if the cost of supporting Hadoop1 is low enough, I'd vote to keeping it.

Thanks
Shivaram


, Steve Loughran
c)
e
"
Sean Owen <sowen@cloudera.com>,"Fri, 12 Jun 2015 17:42:24 +0100",Re: Remove Hadoop 1 support (Hadoop <2.2) for Spark 1.5?,Patrick Wendell <pwendell@gmail.com>,"
Not entirely; you can see some binary incompatibilities that have
bitten recently. A Hadoop 1 program does not in general work on Hadoop
2 because of this.

Part of my thinking is that I'm not clear Hadoop 1.x, and 2.0.x, fully
works anymore anyway. See for example SPARK-8057 recently. I recall
similar problems with Hadoop 2.0.x-era releases and the Spark build
for that which is basically the 'cdh4' build.

So one benefit is skipping whatever work would be needed to continue
to fix this up, and, the argument is there may be less loss of
functionality than it seems. The other is being able to use later
APIs. This much is a little minor.



Yeah, that's the question. Is anyone out there using 1.x? More
anecdotes wanted. That might be the most interesting question.

No CDH customers would have been for a long while now, for example.
(Still a small number of CDH 4 customers out there though, and that's
2.0.x or so, but that's a gray area.)

Is the S3 library thing really related to Hadoop 1.x? that comes from
jets3t and that's independent.



Really? I'd say the opposite. No APIs that are only in 2.2, let alone
only in a later version, can be in use now, right? 1.x wouldn't work
at all then. I don't know of any binary incompatibilities of the type
between 1.x and 2.x, which we have had to shim to make work.

In both cases dependencies have to be harmonized here and there, yes.
That won't change.

---------------------------------------------------------------------


"
Thomas Dudziak <tomdzk@gmail.com>,"Fri, 12 Jun 2015 11:18:21 -0700",Re: Remove Hadoop 1 support (Hadoop <2.2) for Spark 1.5?,Sean Owen <sowen@cloudera.com>,"-1 to this, we use it with an old Hadoop version (well, a fork of an old
version, 0.23). That being said, if there were a nice developer api that
separates Spark from Hadoop (or rather, two APIs, one for scheduling and
one for HDFS), then we'd be happy to"
Sean Owen <sowen@cloudera.com>,"Fri, 12 Jun 2015 19:21:51 +0100",Re: Remove Hadoop 1 support (Hadoop <2.2) for Spark 1.5?,Thomas Dudziak <tomdzk@gmail.com>,"I don't imagine that can be guaranteed to be supported anyway... the
0.x branch has never necessarily worked with Spark, even if it might
happen to. Is this really something you would veto for everyone
because of your deployment?


---------------------------------------------------------------------


"
Matei Zaharia <matei.zaharia@gmail.com>,"Fri, 12 Jun 2015 11:24:00 -0700",Re: Remove Hadoop 1 support (Hadoop <2.2) for Spark 1.5?,Sean Owen <sowen@cloudera.com>,"I don't like the idea of removing Hadoop 1 unless it becomes a significant maintenance burden, which I don't think it is. You'll always be surprised how many people use old software, even though various companies may no longer support them.

With Hadoop 2 in particular, I may be misremembering, but I believe that the experience on Windows is considerably worse because it requires these shell scripts to set permissions that it won't find if you just download Spark. That would be one reason to keep Hadoop 1 in the default build. But I could be wrong, it's been a while since I tried Windows.

Matei


old
that
and one


---------------------------------------------------------------------


"
Thomas Dudziak <tomdzk@gmail.com>,"Fri, 12 Jun 2015 11:29:01 -0700",Re: Remove Hadoop 1 support (Hadoop <2.2) for Spark 1.5?,Sean Owen <sowen@cloudera.com>,"0.23 (and hive 0.12) code base in Spark works well from our perspective, so
not sure what you are referring to. As I said, I'm happy to maintain my own
plugins but as it stands there is no sane way to do so in Spark because
there is no clear separation/developer APIs for these.

cheers,
Tom


"
Zack Sampson <zsampson@palantir.com>,"Fri, 12 Jun 2015 19:15:52 +0000",RE: When to expect UTF8String?,Michael Armbrust <michael@databricks.com>,"We are using Expression for two things.

1. Custom aggregators that do map-side combine.

2. UDFs with more than 22 arguments which is not supported by ScalaUdf, and to avoid wrapping a Java function interface in one of 22 different Scala function interfaces depending on the number of parameters.

Are there methods we can use to convert to/from the internal representation in these cases?
________________________________________
From: Michael Armbrust [michael@databricks.com]
Sent: Thursday, June 11, 2015 9:05 PM
To: Zack Sampson
Cc: dev@spark.apache.org
Subject: Re: When to expect UTF8String?

Through the DataFrame API, users should never see UTF8String.

Expression (and any class in the catalyst package) is considered internal and so uses the internal representation of various types.  Which type we use here is not stable across releases.

Is there a reason you aren't defining a UDF instead?

I'm hoping for some clarity about when to expect String vs UTF8String when
using the Java DataFrames API.

In upgrading to Spark 1.4, I'm dealing with a lot of errors where what was
once a String is now a UTF8String. The comments in the file and the related
commit message indicate that maybe it should be internal to SparkSQL's
implementation.

However, when I add a column containing a custom subclass of Expression, the
row passed to the eval method contains instances of UTF8String. Ditto for
AggregateFunction.update. Is this expected? If so, when should I generally
know to deal with UTF8String objects?



--
3.nabble.com/When-to-expect-UTF8String-tp12710.html<https://urldefense.proofpoint.com/v2/url?u=http-3A__apache-2Dspark-2Ddevelopers-2Dlist.1001551.n3.nabble.com_When-2Dto-2Dexpect-2DUTF8String-2Dtp12710.html&d=BQMFaQ&c=cuTioZt881I11O_M&m=03dQBm7iPTCL33eIdtabOwGkj02beDizwxfaDAv1Xhs&s=EhYOx1s29rjLhkJfDhjQ_9QFNdw0GZ_YxaV6ZiXuqas&e=>
om.

---------------------------------------------------------------------
ribe@spark.apache.org>
spark.apache.org>


---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Fri, 12 Jun 2015 14:27:48 -0700",Re: When to expect UTF8String?,Zack Sampson <zsampson@palantir.com>,"
This is something I'd hoping to add in Spark 1.5



I'm super open to suggestions here.  Mind possibly opening a JIRA with a
proposed interface?
"
=?UTF-8?B?SnVhbiBSb2Ryw61ndWV6IEhvcnRhbMOh?= <juan.rodriguez.hortala@gmail.com>,"Sat, 13 Jun 2015 00:17:49 +0200",Re: Spark 1.4: Python API for getting Kafka offsets in direct mode?,Cody Koeninger <cody@koeninger.org>,"Hi,

If you want I would be happy to work in this. I have worked with
KafkaUtils.createDirectStream before, in a pull request that wasn't
accepted https://github.com/apache/spark/pull/5367. I'm fluent with Python
and I'm starting to feel comfortable with Scala, so if someone opens a JIRA
I can take it.

Greetings,

Juan Rodriguez


2015-06-12 15:59 GMT+02:00 Cody Koeninger <cody@koeninger.org>:

"
Amit Ramesh <amit@yelp.com>,"Fri, 12 Jun 2015 17:23:50 -0700",Re: Spark 1.4: Python API for getting Kafka offsets in direct mode?,=?UTF-8?B?SnVhbiBSb2Ryw61ndWV6IEhvcnRhbMOh?= <juan.rodriguez.hortala@gmail.com>,"Hi Juan,

I have created a ticket for this:
https://issues.apache.org/jira/browse/SPARK-8337

Thanks!
Amit



ns
.
I to
aybe
.
d this
l#tab_scala_2
e
ts are
u want
 In
m
DD, why
D. Looks
ples (
tml).
tml#tab_scala_2.
g
"
srinivasraghavansr71 <sreenivas.raghavan7@gmail.com>,"Fri, 12 Jun 2015 20:16:33 -0700 (MST)",Contribution,dev@spark.apache.org,"Hi everyone,
                 I am interest to contribute new algorithms and optimize
existing algorithms in the area of graph algorithms and machine learning.
Please give me some ideas where to start. Is it possible for me to introduce
the notion of neural network in the apache spark



--

---------------------------------------------------------------------


"
Andrew Or <andrew@databricks.com>,"Fri, 12 Jun 2015 22:31:30 -0700",[NEW] Debugging test failures on Jenkins,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi everyone,

As of today, the pull request tester on Jenkins includes not only the
standard test output, but also the log4j logs for all drivers / executors /
other processes started during the tests.

This is motivated by the fact that some test failure messages are not super
helpful, e.g. ""futures timed out"" or ""subprocess exited with code 1"".
Having log4j logs will help us track down flaky tests more effectively.

Follow the following steps (screenshots attached!) to view them:
1. From your pull request, go to Jenkins through ""Test build X has finished""
2. You will be taken to the Console Output page. Click ""Status"" for your
build.
3. You will see a ""Build Artifact"" icon. Click into it.
4. Find the ""unit-tests.log"" file of interest

Since SPARK-7558, these log files mark the boundaries of tests within a
suite, so you should be able to find the logs for a particular test easily.
Note that this only applies to tests in branches 1.3 or newer.

Cheers,
-Andrew

[image: Images intÃ©grÃ©es 1]

[image: Images intÃ©grÃ©es 2]

[image: Images intÃ©grÃ©es 3]

[image: Images intÃ©grÃ©es 4]
"
Eron Wright <ewright@live.com>,"Sat, 13 Jun 2015 00:29:23 -0700",RE: Contribution,"srinivasraghavansr71 <sreenivas.raghavan7@gmail.com>,
	""dev@spark.apache.org"" <dev@spark.apache.org>","The deeplearning4j project provides neural net algorithms for Spark ML.   You may consider it sample code for extending Spark with new ML algorithms.

http://deeplearning4j.org/sparkmlhttps://github.com/deeplearning4j/deeplearning4j/tree/master/deeplearning4j-scaleout/spark/dl4j-spark-ml
-Eron
uce
.n3.nabble.com/Contribution-tp12739.html
.com.
 		 	   		  "
=?UTF-8?B?SnVhbiBSb2Ryw61ndWV6IEhvcnRhbMOh?= <juan.rodriguez.hortala@gmail.com>,"Sat, 13 Jun 2015 10:18:15 +0200",Re: Spark 1.4: Python API for getting Kafka offsets in direct mode?,Amit Ramesh <amit@yelp.com>,"Perfect! I'll start working on it

2015-06-13 2:23 GMT+02:00 Amit Ramesh <amit@yelp.com>:

ens
t
,
..
PI to
maybe
t.
dd this
ml#tab_scala_2
offsets
If you
ck of
. In
RDD, why
DD. Looks
mples (
html).
html#tab_scala_2.
ng
"
Steve Loughran <stevel@hortonworks.com>,"Sat, 13 Jun 2015 08:39:24 +0000",Re: Remove Hadoop 1 support (Hadoop <2.2) for Spark 1.5?,Patrick Wendell <pwendell@gmail.com>,"
> On 12 Jun 2015, at 17:12, Patrick Wendell <pwendell@gmail.com> wrote:
> 
>  For instance at Databricks we use
> the FileSystem library for talking to S3... every time we've tried to
> upgrade to Hadoop 2.X there have been significant regressions in
> performance and we've had to downgrade. That's purely anecdotal, but I
> think you have people out there using the Hadoop 1 bindings for whom
> upgrade would be a pain.

ah s3n. The unloved orphan FS, which has been fairly neglected as being non-strategic to anyone but Amazon, who have a private fork. 

s3n broke in hadopo 2.4 where the upgraded Jets3t went in with some patch which swallowed exceptions (nobody should ever do that) and as result would NPE on a seek(0) of a file of length(0). HADOOP-10457. Fixed in Hadoop 2.5

Hadoop 2.6 has left S3n on maintenance out of fear of breaking more things, future work is in s3a:,, which switched to the amazon awstoolkit JAR and moved the implementation to hadoop-aws JAR. S3a promises: speed, partitioned upload, better auth. 

But: it's not ready for serious use in Hadoop 2.6, so don't try. You need the Hadoop 2.7 patches, which are in ASF Hadoop 2.7, will be in HDP2.3, and have been picked up in CDH5.3. (HADOOP-11571). For Spark, the fact that the block size is being returned as 0 in getFileStatus() could be the killer.

Future work is going to improve performance and scale ( HADOOP-11694 )

Now, if spark is finding problems with s3a performance, tests for this would be great -complaints on JIRAs too. There's not enough functional testing of analytics workloads against the object stores, especially s3 and swift. If someone volunteers to add some optional test module for object store testing, I'll help review it and suggest some tests to generate stress

That can be done without the leap to Hadoop 2 â€”though the proposed HADOOP-9565 work allowing object stores to declare that they are and publish some of their consistency and atomicity semantics will be Hadoop 2.8+. If you want your output committers to recognise when the destination is an eventually constitent object store with O(n) directory rename and delete, that's where the code will be."
Akhil Das <akhil@sigmoidanalytics.com>,"Sat, 13 Jun 2015 16:25:18 +0530",Re: Contribution,srinivasraghavansr71 <sreenivas.raghavan7@gmail.com>,"This is a good start, if you haven't seen this already
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark

Thanks
Best Regards


"
"""=?ISO-8859-1?B?U2Vh?="" <261810726@qq.com>","Sun, 14 Jun 2015 00:21:44 +0800",About HostName display in SparkUI,"""=?ISO-8859-1?B?ZGV2?="" <dev@spark.apache.org>","In spark 1.4.0, I find that the Address is ip (it was hostname in v1.3.0), why? who did it?"
Patrick Wendell <pwendell@gmail.com>,"Sat, 13 Jun 2015 17:42:36 -0700",Re: Remove Hadoop 1 support (Hadoop <2.2) for Spark 1.5?,Steve Loughran <stevel@hortonworks.com>,"Yeah so Steve, hopefully it's self evident, but that is a perfect
example of the kind of annoying stuff we don't want to force users to
deal with by forcing an upgrade to 2.X. Compare the pain from Spark
users of trying to reason about what to do (and btw it seems like the
answer is simply that there isn't a good answer). And that will be
experienced by every Spark users who uses AWS and the Spark ec2
scripts, which are extremely popular.

Is this pain, in aggregate, more than our cost of having a few patches
to deal with runtime reflection stuff to make things work with Hadoop
1? My feeling is that it's much more efficient for us as the Spark
maintainers to pay this cost rather than to force a lot of our users
to deal with painful upgrades.

ote:
on-strategic to anyone but Amazon, who have a private fork.
 which swallowed exceptions (nobody should ever do that) and as result would NPE on a seek(0) of a file of length(0). HADOOP-10457. Fixed in Hadoop 2.5
s, future work is in s3a:,, which switched to the amazon awstoolkit JAR and moved the implementation to hadoop-aws JAR. S3a promises: speed, partitioned upload, better auth.
 the Hadoop 2.7 patches, which are in ASF Hadoop 2.7, will be in HDP2.3, and have been picked up in CDH5.3. (HADOOP-11571). For Spark, the fact that the block size is being returned as 0 in getFileStatus() could be the killer.
uld be great -complaints on JIRAs too. There's not enough functional testing of analytics workloads against the object stores, especially s3 and swift. If someone volunteers to add some optional test module for object store testing, I'll help review it and suggest some tests to generate stress
d HADOOP-9565 work allowing object stores to declare that they are and publish some of their consistency and atomicity semantics will be Hadoop 2.8+. If you want your output committers to recognise when the destination is an eventually constitent object store with O(n) directory rename and delete, that's where the code will be.

---------------------------------------------------------------------


"
StanZhai <mail@zhaishidan.cn>,"Sat, 13 Jun 2015 20:25:15 -0700 (MST)",Re: A confusing ClassNotFoundException error,dev@spark.apache.org,"I have encountered the similar error too at spark 1.4.0. 

The same code can be run on spark 1.3.1.

My code is(it can be run on spark-shell): 
===============================
  // hc is a instance of HiveContext
  val df = hc.sql(""select * from test limit 10"")
  val sb = new mutable.StringBuilder
  def mapHandle = (row: Row) => {
    val rowData = ArrayBuffer[String]()
    for (i <- 0 until row.size) {
      val d = row.get(i)

      d match {
        case data: ArrayBuffer[Any] =>
          sb.clear()
          sb.append('[')
          for (j <- 0 until data.length) {
            val elm = data(j)
            if (elm != null) {
              sb.append('""')
              sb.append(elm.toString)
              sb.append('""')
            } else {
              sb.append(""null"")
            }
            sb.append(',')
          }
          if (sb.length > 1) {
            sb.deleteCharAt(sb.length - 1)
          }
          sb.append(']')
          rowData += sb.toString()
        case _ =>
          rowData += (if (d != null) d.toString else null)
      }
    }
    rowData
  }
  df.map(mapHandle).foreach(println)


My submit script is: spark-submit --class cn.zhaishidan.trans.Main --master
local[8] test-spark.jar
===============the error================
java.lang.ClassNotFoundException:
cn.zhaishidan.trans.service.SparkHiveService$$anonfun$mapHandle$1$1$$anonfun$apply$1
	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:270)
	at
org.apache.spark.util.InnerClosureFinder$$anon$4.visitMethodInsn(ClosureCleaner.scala:455)
	at
com.esotericsoftware.reflectasm.shaded.org.objectweb.asm.ClassReader.accept(Unknown
Source)
	at
com.esotericsoftware.reflectasm.shaded.org.objectweb.asm.ClassReader.accept(Unknown
Source)
	at
org.apache.spark.util.ClosureCleaner$.getInnerClosureClasses(ClosureCleaner.scala:101)
	at
org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:197)
	at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:132)
	at org.apache.spark.SparkContext.clean(SparkContext.scala:1891)
	at org.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:294)
	at org.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:293)
	at
org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:148)
	at
org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:109)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:286)
	at org.apache.spark.rdd.RDD.map(RDD.scala:293)
	at org.apache.spark.sql.DataFrame.map(DataFrame.scala:1210)
	at
cn.zhaishidan.trans.service.SparkHiveService.formatDF(SparkHiveService.scala:66)
	at
cn.zhaishidan.trans.service.SparkHiveService.query(SparkHiveService.scala:80)
	at
cn.zhaishidan.trans.api.DatabaseApi$$anonfun$query$1.apply(DatabaseApi.scala:39)
	at
cn.zhaishidan.trans.api.DatabaseApi$$anonfun$query$1.apply(DatabaseApi.scala:30)
	at
cn.zhaishidan.trans.web.JettyUtils$$anon$1.getOrPost(JettyUtils.scala:56)
	at cn.zhaishidan.trans.web.JettyUtils$$anon$1.doGet(JettyUtils.scala:73)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:735)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:848)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)
	at
org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:501)
	at
org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)
	at
org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:428)
	at
org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)
	at
org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
	at org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:52)
	at
org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
	at org.eclipse.jetty.server.Server.handle(Server.java:370)
	at
org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)
	at
org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:971)
	at
org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1033)
	at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)
	at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)
	at
org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
	at
org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:667)
	at
org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)
	at
org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at
org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:745)




--

---------------------------------------------------------------------


"
Joseph Bradley <joseph@databricks.com>,"Sun, 14 Jun 2015 15:33:59 -0700",Re: Contribution,Akhil Das <akhil@sigmoidanalytics.com>,"+1 for checking out the Wiki on Contributing to Spark.  It gives helpful
pointers about finding starter JIRAs, the discussion & code review process,
and how we prioritize algorithms & other contributions.  After you read
that, I would recommend searching "
Dong Lei <donglei@microsoft.com>,"Mon, 15 Jun 2015 02:50:27 +0000","RE: How to support dependency jars and files on HDFS in standalone
 cluster mode?",Cheng Lian <lian.cs.zju@gmail.com>,"https://issues.apache.org/jira/browse/SPARK-8369  Created

And Iâ€™m working on a PR.

Thanks
Dong Lei

From: Cheng Lian [mailto:lian.cs.zju@gmail.com]
Sent: Friday, June 12, 2015 7:03 PM
To: Dong Lei
Cc: Dianfei (Keith) Han; dev@spark.apache.org
Subject: Re: How to support dependency jars and files on HDFS in standalone cluster mode?

Would you mind to file a JIRA for this? Thanks!

Cheng
On 6/11/15 2:40 PM, Dong Lei wrote:
I think in standalone cluster mode, spark is supposed to do:

1.       Download jars, files to driver

2.       Set the driverâ€™s class path

3.       Driver setup a http file server to distribute these files

4.       Worker download from driver and setup classpath

Right?

But somehow, the first step fails.
Even if I can make the first step works(use option1), it seems that the classpath in driver is not correctly set.

Thanks
Dong Lei

From: Cheng Lian [mailto:lian.cs.zju@gmail.com]
Sent: Thursday, June 11, 2015 2:32 PM
To: Dong Lei
Cc: Dianfei (Keith) Han; dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: How to support dependency jars and files on HDFS in standalone cluster mode?

Oh sorry, I mistook --jars for --files. Yeah, for jars we need to add them to classpath, which is different from regular files.

Cheng
On 6/11/15 2:18 PM, Dong Lei wrote:
Thanks Cheng,

If I do not use --jars how can I tell spark to search the jars(and files) on HDFS?

Do you mean the driver will not need to setup a HTTP file server for this scenario and the worker will fetch the jars and files from HDFS?

Thanks
Dong Lei

From: Cheng Lian [mailto:lian.cs.zju@gmail.com]
Sent: Thursday, June 11, 2015 12:50 PM
To: Dong Lei; dev@spark.apache.org<mailto:dev@spark.apache.org>
Cc: Dianfei (Keith) Han
Subject: Re: How to support dependency jars and files on HDFS in standalone cluster mode?

Since the jars are already on HDFS, you can access them directly in your Spark application without using --jars

Cheng
On 6/11/15 11:04 AM, Dong Lei wrote:
Hi spark-dev:

I can not use a hdfs location for the â€œ--jarsâ€ or â€œ--filesâ€ option when doing a spark-submit in a standalone cluster mode. For example:
                Spark-submit  â€¦   --jars hdfs://ip/1.jar  â€¦.  hdfs://ip/app.jar (standalone cluster mode)
will not download 1.jar to driverâ€™s http file server(but the app.jar will be downloaded to the driverâ€™s dir).

I figure out the reason spark not downloading the jars is that when doing sc.addJar to http file server, the function called is Files.copy which does not support a remote location.
And I think if spark can download the jars and add them to http file server, the classpath is not correctly set, because the classpath contains remote location.

So Iâ€™m trying to make it work and come up with two options, but neither of them seem to be elegant, and I want to hear your advices:

Option 1:
Modify HTTPFileServer.addFileToDir, let it recognize a â€œhdfsâ€ prefix.

This is not good because I think it breaks the scope of http file server.

Option 2:
Modify DriverRunner.downloadUserJar, let it download all the â€œ--jarsâ€ and â€œ--filesâ€ with the application jar.

This sounds more reasonable that option 1 for downloading files. But this way I need to read the â€œspark.jarsâ€ and â€œspark.filesâ€ on downloadUserJar or DriverRunnder.start and replace it with a local path. How can I do that?


Do you have a more elegant solution, or do we have a plan to support it in the furture?

Thanks
Dong Lei



"
Zhiwei Chan <z.w.chan.jason@gmail.com>,"Mon, 15 Jun 2015 10:59:15 +0800",Re: A confusing ClassNotFoundException error,StanZhai <mail@zhaishidan.cn>,"Hi Stan,

I have filed a issue on JIRA for this exception, and I have tested it on
spark 1.3.0 (local mode) without exception.

https://issues.apache.org/jira/browse/SPARK-8368


"
"""Haopu Wang"" <HWang@qilinsoft.com>","Mon, 15 Jun 2015 15:36:06 +0800",[SparkStreaming] NPE in DStreamCheckPointData.scala:125,"""user"" <user@spark.apache.org>,
	<dev@spark.apache.org>","I use the attached program to test checkpoint. It's quite simple.

 

When I run the program second time, it will load checkpoint data, that's
expected, however I see NPE in driver log.

 

Do you have any idea about the issue? I'm on Spark 1.4.0, thank you very
much!

 

====== logs ======

 

15/06/15 15:27:17 [THREAD ID=main] INFO FileInputDStream: Restoring
checkpoint data

15/06/15 15:27:17 [THREAD ID=main] INFO
FileInputDStream$FileInputDStreamCheckpointData: Restoring files for
time 1434353130000 ms - []

15/06/15 15:27:17 [THREAD ID=main] INFO
FileInputDStream$FileInputDStreamCheckpointData: Restoring files for
time 1434353140000 ms - []

15/06/15 15:27:17 [THREAD ID=main] INFO
FileInputDStream$FileInputDStreamCheckpointData: Restoring files for
time 1434353150000 ms - []

15/06/15 15:27:17 [THREAD ID=main] INFO
FileInputDStream$FileInputDStreamCheckpointData: Restoring files for
time 1434353160000 ms - []

15/06/15 15:27:17 [THREAD ID=main] INFO
FileInputDStream$FileInputDStreamCheckpointData: Restoring files for
time 1434353170000 ms - []

15/06/15 15:27:17 [THREAD ID=main] INFO
FileInputDStream$FileInputDStreamCheckpointData: Restoring files for
time 1434353180000 ms - []

15/06/15 15:27:17 [THREAD ID=main] INFO
FileInputDStream$FileInputDStreamCheckpointData: Restoring files for
time 1434353190000 ms - []

15/06/15 15:27:17 [THREAD ID=main] INFO FileInputDStream: Restored
checkpoint data

15/06/15 15:27:17 [THREAD ID=main] INFO MappedDStream: Restored
checkpoint data

15/06/15 15:27:17 [THREAD ID=main] INFO MappedDStream: Restored
checkpoint data

15/06/15 15:27:17 [THREAD ID=main] INFO ForEachDStream: Restored
checkpoint data

15/06/15 15:27:17 [THREAD ID=main] INFO DStreamGraph: Restored
checkpoint data

15/06/15 15:27:17 [THREAD ID=main] ERROR StreamingContext: Error
starting the context, marking it as stopped

java.io.IOException: java.lang.NullPointerException

       at
org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1242)

       at
org.apache.spark.streaming.dstream.DStreamCheckpointData.writeObject(DSt
reamCheckpointData.scala:123)

       at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

       at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav
a:57)

       at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessor
Impl.java:43)

       at java.lang.reflect.Method.invoke(Method.java:606)

       at
java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)

       at
java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)

       at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1
431)

       at
java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)

       at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:15
47)

       at
java.io.ObjectOutputStream.defaultWriteObject(ObjectOutputStream.java:44
0)

       at
org.apache.spark.streaming.dstream.DStream$$anonfun$writeObject$1.apply$
mcV$sp(DStream.scala:498)

       at
org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1239)

       at
org.apache.spark.streaming.dstream.DStream.writeObject(DStream.scala:493
)

       at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

       at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav
a:57)

       at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessor
Impl.java:43)

       at java.lang.reflect.Method.invoke(Method.java:606)

       at
java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)

       at
java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)

       at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1
431)

       at
java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)

       at
java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1377)

       at
java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1173)

       at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:15
47)

       at
java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)

       at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1
431)

       at
java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)

       at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:15
47)

       at
java.io.ObjectOutputStream.defaultWriteObject(ObjectOutputStream.java:44
0)

       at
org.apache.spark.streaming.DStreamGraph$$anonfun$writeObject$1.apply$mcV
$sp(DStreamGraph.scala:181)

       at
org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1239)

       at
org.apache.spark.streaming.DStreamGraph.writeObject(DStreamGraph.scala:1
76)

       at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

       at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav
a:57)

       at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessor
Impl.java:43)

       at java.lang.reflect.Method.invoke(Method.java:606)

       at
java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)

       at
java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)

       at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1
431)

       at
java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)

       at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:15
47)

       at
java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)

       at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1
431)

       at
java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)

       at
java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)

       at
org.apache.spark.streaming.Checkpoint$$anonfun$serialize$1.apply$mcV$sp(
Checkpoint.scala:113)

       at
org.apache.spark.streaming.Checkpoint$$anonfun$serialize$1.apply(Checkpo
int.scala:113)

       at
org.apache.spark.streaming.Checkpoint$$anonfun$serialize$1.apply(Checkpo
int.scala:113)

       at
org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1285)

       at
org.apache.spark.streaming.Checkpoint$.serialize(Checkpoint.scala:114)

       at
org.apache.spark.streaming.StreamingContext.validate(StreamingContext.sc
ala:547)

       at
org.apache.spark.streaming.StreamingContext.liftedTree1$1(StreamingConte
xt.scala:587)

       at
org.apache.spark.streaming.StreamingContext.start(StreamingContext.scala
:586)

       at
CheckPointTest$delayedInit$body.apply(CheckPointTest.scala:116)

       at scala.Function0$class.apply$mcV$sp(Function0.scala:40)

       at
scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12)

       at scala.App$$anonfun$main$1.apply(App.scala:71)

       at scala.App$$anonfun$main$1.apply(App.scala:71)

       at scala.collection.immutable.List.foreach(List.scala:318)

       at
scala.collection.generic.TraversableForwarder$class.foreach(TraversableF
orwarder.scala:32)

       at scala.App$class.main(App.scala:71)

       at CheckPointTest$.main(CheckPointTest.scala:7)

       at CheckPointTest.main(CheckPointTest.scala)

Caused by: java.lang.NullPointerException

       at
org.apache.spark.streaming.dstream.DStreamCheckpointData$$anonfun$writeO
bject$1.apply$mcV$sp(DStreamCheckpointData.scala:125)

       at
org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1239)

       ... 64 more

 


---------------------------------------------------------------------"
Akhil Das <akhil@sigmoidanalytics.com>,"Mon, 15 Jun 2015 14:36:04 +0530",Re: About HostName display in SparkUI,Sea <261810726@qq.com>,"In the conf/slaves file, are you having the ip addresses? or the hostnames?

Thanks
Best Regards


"
=?UTF-8?B?U2VyZ2lvIFJhbcOtcmV6?= <sramirezga@ugr.es>,"Mon, 15 Jun 2015 11:56:47 +0200",Spark hangs without notification (broadcasting),dev <dev@spark.apache.org>,"Hi everyone:

I am having several problems with an algorithm for MLLIB that I am 
developing. It uses large broadcasted variables with many iteration and 
breeze vectors as RDDs. The problem is that in some stages the spark 
program freezes without notification. I have tried to reduce the use of 
broadcasting and the size of the variables (from hash tables to simple 
arrays of bytes), but the problem appears again in others lines.

The code is here: 
https://github.com/sramirez/SparkFeatureSelection/blob/efficient-fs/src/main/scala/org/apache/spark/mllib/feature/InfoTheory.scala

There is a problem related with mine in JIRA: 
https://issues.apache.org/jira/browse/SPARK-5363
It seems fixed, but it is not so clear. Despite being related with 
PySpark, it also seems to reproduce in Scala.

I have tried several Spark versions: 1.2.0, 1.3.1, 1.4.0.

I would appreciate any clue or advise.

Thanks,

Sergio R.

---------------------------------------------------------------------


"
anshu shukla <anshushukla0@gmail.com>,"Mon, 15 Jun 2015 19:07:22 +0530",Using queueStream,"user@spark.apache.org, dev@spark.apache.org, 
	=?UTF-8?B?SnVhbiBSb2Ryw61ndWV6IEhvcnRhbMOh?= <juan.rodriguez.hortala@gmail.com>, 
	ayan guha <guha.ayan@gmail.com>","JavaDStream<String> inputStream = ssc.queueStream(rddQueue);

Can this   rddQueue  be of dynamic type  in nature .If yes  then how to
make it run untill rddQueue is not finished .

Any other way to get  rddQueue from a dynamically updatable Normal Queue .

-- 
Thanks & Regards,
SERC-IISC
Anshu Shukla
"
"""=?gb18030?B?U2Vh?="" <261810726@qq.com>","Tue, 16 Jun 2015 00:24:20 +0800","=?gb18030?B?UmWjuiBBYm91dCBIb3N0TmFtZSBkaXNwbGF5IGlu?=
 =?gb18030?B?IFNwYXJrVUk=?=","""=?gb18030?B?QWtoaWwgRGFz?="" <akhil@sigmoidanalytics.com>","In the conf/slaves file, I have hostnames. 
Before 1.4.0, it is okay. I view the code in class org.apache.spark.util.Utils, I alter function localHostName and localHostNameForURI, and it turns back to hostnames again. 
I just don't know why to change these basic functions. Hostname is nice. 




------------------ Ô­Ê¼ÓÊ¼þ ------------------
·¢¼þÈË: ""Akhil Das"";<akhil@sigmoidanalytics.com>;
·¢ËÍÊ±¼ä: 2015Äê6ÔÂ15ÈÕ(ÐÇÆÚÒ») ÏÂÎç5:36
ÊÕ¼þÈË: ""Sea""<261810726@qq.com>; 
³­ËÍ: ""dev""<dev@spark.apache.org>; 
Ö÷Ìâ: Re: About HostName display in SparkUI



In the conf/slaves file, are you having the ip addresses? or the hostnames?


ThanksBest Regards



 
On Sat, Jun 13, 2015 at 9:51 PM, Sea <261810726@qq.com> wrote:
In spark 1.4.0, I find that the Address is ip (it was hostname in v1.3.0), why? who did it?"
anshu shukla <anshushukla0@gmail.com>,"Tue, 16 Jun 2015 00:53:15 +0530",Problem: Custom Receiver for getting events from a Dynamic Queue,"user@spark.apache.org, dev@spark.apache.org","I have written a custom receiver for converting the tuples in the Dynamic
Queue/EventGen  to the Dstream.But i dont know why It is only processing
data for some time (3-4 sec.) only and then shows Queue as Empty .ANy
suggestions please ..>>

--code //


public class JavaCustomReceiver extends Receiver<String> implements
ISyntheticEventGen {


    EventGen eventGen;
    BlockingQueue<List<String>> eventQueue;
    String csvFileName;
    String outSpoutCSVLogFileName;
    double scalingFactor;

    public JavaCustomReceiver(String csvFileName, String
outSpoutCSVLogFileName, double scalingFactor) {
        super(StorageLevel.MEMORY_AND_DISK());

        this.csvFileName = csvFileName;
        this.outSpoutCSVLogFileName = outSpoutCSVLogFileName;
        this.scalingFactor = scalingFactor;

        this.eventGen = new EventGen(this,this.scalingFactor);
        this.eventGen.launch(this.csvFileName,
this.outSpoutCSVLogFileName); //Launch threads


        this.eventQueue = new LinkedBlockingQueue<List<String>>();
        System.out.println(""for watching queue"");
    }

    public void onStart() {
        // Start the thread that receives data over a connection
        new Thread()  {
            @Override public void run() {
                receive();
            }
        }.start();
    }

    public void onStop() {
        // There is nothing much to do as the thread calling receive()
        // is designed to stop by itself isStopped() returns false
    }

    /** Create a socket connection and receive data until receiver is stopped */
    private void receive() {

        try {
            // connect to the server
//            socket = new Socket(host, port);

//            BufferedReader reader = new BufferedReader(new
InputStreamReader(socket.getInputStream()));

            // Until stopped or connection broken continue reading
            while (!isStopped() ) {
                         List<String> entry = this.eventQueue.take();

                String str="""";
                for(String s:entry)
                str+=s+"","";
                System.out.println(""Received data '"" + str + ""'"");
                store(str);

            }
            // Restart in an attempt to connect again when server is
active again
            restart(""Trying to connect again"");
        }
        catch(Throwable t) {
            // restart if there is any other error
            restart(""Error receiving data"", t);
        }
    }

    @Override
    public StorageLevel storageLevel() {
        return StorageLevel.MEMORY_AND_DISK();
    }


    @Override
    public void receive(List<String> event) {
        // TODO Auto-generated method stub
        //System.out.println(""Called IN SPOUT### "");
        try {
            this.eventQueue.put(event);
        } catch (InterruptedException e) {
            // TODO Auto-generated catch block
            e.printStackTrace();
        }
    }

}


-- 
Thanks & Regards,
Anshu Shukla
"
Isca Harmatz <pop1998@gmail.com>,"Tue, 16 Jun 2015 07:45:35 +0300",Random Forest driver memory,dev@spark.apache.org,"hello,

i have noticed that the random forest implementation crashes when
to many trees/ to big maxDepth is used.

im guessing that this is something to do with the amount of nodes that need
to be
kept in driver's memory during the run.

but when i examined the nodes structure is seems rather small

does anyone now where does the memory issue come from?

thanks,
  Isca
"
Sean Owen <sowen@cloudera.com>,"Tue, 16 Jun 2015 14:24:29 +0200",Re: Sidebar: issues targeted for 1.4.0,"""dev@spark.apache.org"" <dev@spark.apache.org>","Question: what would happen if I cleared Target Version for everything
still marked Target Version = 1.4.0? There are 76 right now, and
clearly that's not correct.

56 were opened by committers, including issues like ""Do X for 1.4"".
I'd like to understand whether these are resolved but just weren't
closed, or else why so many issues are being filed as a todo and not
resolved? Slipping things here or there is OK, but these weren't even
slipped, just forgotten.


---------------------------------------------------------------------


"
Josh Rosen <rosenville@gmail.com>,"Tue, 16 Jun 2015 08:09:22 -0700",Re: Sidebar: issues targeted for 1.4.0,Sean Owen <sowen@cloudera.com>,"Whatever you do, DO NOT use the built-in JIRA 'releases' feature to migrate
issues from 1.4.0 to another version: the JIRA feature will have the
side-effect of automatically changing the target versions for issues that
have been closed, which is going to be really confusing. I've made this
mistake once myself and it was a bit of a hassle to clean up.


"
"""Huang, Jie"" <jie.huang@intel.com>","Tue, 16 Jun 2015 17:27:18 +0000",[SparkScore] Performance portal for Apache Spark,"""user@spark.apache.org"" <user@spark.apache.org>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","Hi All

We are happy to announce Performance portal for Apache Spark http://01org.github.io/sparkscore/ !
The Performance Portal for Apache Spark provides performance data on the Spark upsteam to the community to help identify issues, better understand performance differentials between versions, and help Spark customers get across the finish line faster. The Performance Portal generates two reports, regular (weekly) report and release based regression test report. We are currently using two benchmark suites which include HiBench (http://github.com/intel-bigdata/HiBench) and Spark-perf (https://github.com/databricks/spark-perf ). We welcome and look forward to your suggestions and feedbacks. More information and details provided below
Abount Performance Portal for Apache Spark
Our goal is to work with the Apache Spark community to further enhance the scalability and reliability of the Apache Spark. The data available on this site allows community members and potential Spark customers to closely track performance trend of the Apache Spark. Ultimately, we hope that this project will help community to fix performance issue quickly, thus providing better Apache spark code to end customers. The current workloads used in the benchmarking include HiBench (a benchmark suite to evaluate big data framework like Hadoop MR, Spark from Intel) and Spark-perf (a performance testing framework for Apache Spark from Databricks). Additional benchmarks will be added as they become available
Description
________________________________
Each data point represents each workload runtime percent compared with the previous week. Different lines represents different workloads running on spark yarn-client mode.
Hardware
________________________________
CPU type: Intel(r) Xeon(r) CPU E5-2697 v2 @ 2.70GHz
Memory: 128GB
NIC: 10GbE
Disk(s): 8 x 1TB SATA HDD
Software
________________________________
JAVA ver sion: 1.8.0_25
Hadoop version: 2.5.0-CDH5.3.2
HiBench version: 4.0
Spark on yarn-client mode
Cluster
________________________________
1 node for Master
10 nodes for Slave
Summary
The lower percent the better performance.
________________________________
Group

ww19

ww20

ww22

ww23

ww24

ww25

HiBench

9.1%

6.6%

6.0%

7.9%

-6.5%

-3.1%

spark-perf

4.1%

4.4%

-1.8%

4.1%

-4.7%

-4.6%


[cid:overall_workloads20150616.png]
Y-Axis: normalized completion time; X-Axis: Work Week.
The commit number can be found in the result table.
The performance score for each workload is normalized based on the elapsed time for 1.2 release.The lower the better.

HiBench
________________________________
JOB

ww19

ww20

ww22

ww23

ww24

ww25

commit

489700c8

8e3822a0

530efe3e

90c60692

db81b9d8

4eb48ed1

sleep

%

%

-2.1%

-2.9%

-4.1%

12.8%

wordcount

17.6%

11.4%

8.0%

8.3%

-18.6%

-10.9%

kmeans

92.1%

61.5%

72.1%

92.9%

86.9%

95.8%

scan

-4.9%

-7.2%

%

-1.1%

-25.5%

-21.0%

bayes

-24.3%

-20.1%

-18.3%

-11.1%

-29.7%

-31.3%

aggregation

5.6%

10.5%

%

9.2%

-15.3%

-15.0%

join

4.5%

1.2%

%

1.0%

-12.7%

-13.9%

sort

-3.3%

-0.5%

-11.9%

-12.5%

-17.5%

-17.3%

pagerank

2.2%

3.2%

4.0%

2.9%

-11.4%

-13.0%

terasort

-7.1%

-0.2%

-9.5%

-7.3%

-16.7%

-17.0%


Comments: null means no such workload running or workload failed in this time.
[cid:HiBench_workloads20150616.png]
Y-Axis: normalized completion time; X-Axis: Work Week.
The commit number can be found in the result table.
The performance score for each workload is normalized based on the elapsed time for 1.2 release.The lower the better.
spark-perf
________________________________
JOB

ww19

ww20

ww22

ww23

ww24

ww25

commit

489700c8

8e3822a0

530efe3e

90c60692

db81b9d8

4eb48ed1

agg

13.2%

7.0%

%

18.3%

5.2%

2.5%

agg-int

16.4%

21.2%

%

9.6%

4.0%

8.2%

agg-naive

4.3%

-2.4%

%

-0.8%

-6.7%

-6.8 %

scheduling

-6.1%

-8.9%

-14.5%

-2.1%

-6.4%

-6.5%

count-filter

4.1%

1.0%

6.6%

6.8%

-10.2%

-10.4%

count

4.8%

4.6%

6.7%

8.0%

-7.3%

-7.0%

sort

-8.1%

-2.5%

-6.2%

-7.0%

-14.6%

-14.4%

sort-int

4.5%

15.3%

-1.6%

-0.1%

-1.5%

-2.2%


Comments: null means no such workload running or workload failed in this time.
[cid:sparkperf_workloads20150616.png]
Y-Axis: normalized completion time; X-Axis: Work Week.
The commit number can be found in the result table.
The pe rformance score for each workload is normalized based on the elapsed time for 1.2 release.The lower the better.
Release
Summary
The lower percent the better performance.
________________________________
Group

1.2.1

1.3.0

1.3.1

1.4.0

HiBench

-1.0%

10.5%

8.4%

8.6%

spark-perf

3.2%

0.9%

1.9%

1.3%


[cid:overall_release20150616.png]
Y-Axis: normalized completion time; X-Axis: Release.
The performance score for each workload is normalized based on the elapsed time for 1.2 release.The lower the better.

HiBench
________________________________
JOB

1.2.1

1.3.0

1.3.1

1.4.0

sleep

%

%

%

-0.5%

wordcount

3.5%

5.4%

5.1%

8.7%

kmeans

6.0%

72.6%

82.7%

100.7%

scan

-0.7%

-3.2%

-1.9%

-4.4%

bayes

-19.7%

7.7%

-24.5%

-14.4%

aggregation

4.6%

7.1%

9.9%

9.3%

join

0.7%

4.0%

8.6%

1.3%

sort

-1.0%

2.1%

-1.8%

-10.4%

pagerank

1.5 %

2.2%

1.3%

5.4%

terasort

-3.7%

-3.3%

-3.7%

-9.5%


Comments: null means no such workload running or workload failed in this time.
[cid:HiBench_release20150616.png]
Y-Axis: normalized completion time; X-Axis: Release.
The commit number can be found in the result table.
The performance score for each workload is normalized based on the elapsed time for 1.2 release.The lower the better.
spark-perf
________________________________
JOB

1.2.1

1.3.0

1.3.1

1.4.0

agg

1.9%

3.1%

6.2%

5.0%

agg-int

6.4%

17.1%

18.0%

24.2%

agg-naive

-2.6%

-3.2%

-1.8%

-5.2%

scheduling

8.2%

-16.8%

-14.4%

-19.1%

count-filter

-0.4%

0.3%

-0.5%

0.4%

count

0.6%

-0.3%

0.4%

0.9%

sort

1.2%

-3.3%

-5.3%

-1.9%

sort-int

10.1%

10.0%

12.3%

6.0%


Comments: null means no such workload running or workload failed in this time.
[cid:sparkperf_release20150616.png]
Y-Axis: normalized completion time; X-Axis: Release.
The commit number can be found in the result table.
The performance score for each workload is normalized based on the elapsed time for 1.2 release.The lower the better.
________________________________
Copyright (c) 2015 Intel Corporation. All rights reserved. *Other names and brands may be claimed as the property of others.
Project Email: sparkscore@lists.01.org<mailto:sparkscore@lists.01.org> Please subscribe to the list at: https://lists.01.org/mailman/listinfo/sparkscore

---------------------------------------------------------------------"
Alessandro Baretta <alexbaretta@gmail.com>,"Tue, 16 Jun 2015 11:44:09 -0700",Spark-Shell 2.11 1.4.0-RC-03 does not add jars to class path,"""dev@spark.apache.org"" <dev@spark.apache.org>","This bug still exists in Spark-1.4.0. Is there a workaround for it?

https://issues.apache.org/jira/browse/SPARK-7944

Thanks,

Alex
"
abshkmodi <abshkmodi@gmail.com>,"Tue, 16 Jun 2015 23:00:12 -0700 (MST)",Read/write metrics for jobs which use S3,dev@spark.apache.org,"I mostly use Amazon S3 for reading input data and writing output data for my
spark jobs. I want to know the numbers of bytes read & written by my job
from S3.

In hadoop, there are FileSystemCounters for this, is there something similar
in spark ? If there is, can you please guide me on how to use it ?

I saw there are some read/write metrics in TaskMetrics.scala. Is there a way
to get this by specifying a DataReadMethod in TaskMetrics.scala ? 



--

---------------------------------------------------------------------


"
anshu shukla <anshushukla0@gmail.com>,"Wed, 17 Jun 2015 14:37:04 +0530",Implementing and Using a Custom Actor-based Receiver,"user@spark.apache.org, dev@spark.apache.org","Is there any good  sample code in java to implement  *Implementing and
Using a Custom Actor-based Receiver .*

-- 
Thanks & Regards,
Anshu Shukla
"
Isca Harmatz <pop1998@gmail.com>,"Wed, 17 Jun 2015 19:56:03 +0300",Re: Random Forest driver memory,dev@spark.apache.org,"hello,

does anyone has any help on the issue?


Isca


"
Patrick Wendell <pwendell@gmail.com>,"Wed, 17 Jun 2015 12:55:12 -0700",Re: Sidebar: issues targeted for 1.4.0,Josh Rosen <rosenville@gmail.com>,"Hey Sean,

Thanks for bringing this up - I went through and fixed about 10 of
them. Unfortunately there isn't a hard and fast way to resolve them. I
found all of the following:

- Features that missed the release and needed to be retargeted to 1.5.
- Bugs that missed the release and needed to be retargeted to 1.4.1.
- Issues that were not properly targeted (e.g. someone randomly set
the target version) and should probably be untargeted.

I'd like to encourage others to do this, especially the more active
developers on different components (Streaming, ML, etc).

I don't think we've defined clearly. Is it the target of the person
contributing the feature? Or in some sense the target of the
committership? My preference would be that targeting a JIRA has some
strong semantics - i.e. it means the commiter targeting has mentally
allocated time to review a patch for that feature in the timeline of
that release. I.e. prefer to have fewer targeted JIRA's for a release,
and also expect to get most of the targeted features merged into a
release. In the past I think targeting has meant different things to
different people.

- Patrick


---------------------------------------------------------------------


"
"""Heller, Chris"" <cheller@akamai.com>","Wed, 17 Jun 2015 20:03:56 +0000",Re: Sidebar: issues targeted for 1.4.0,"Patrick Wendell <pwendell@gmail.com>, Josh Rosen <rosenville@gmail.com>","I appreciate targets having the strong meaning you suggest, as its useful
to get a sense of what will realistically be included in a release.


Would it make sense (speaking as a relative outsider here) that we would
not enter into the RC phase of a release until all JIRA targeting that
release were complete?

If a JIRA targeting a release is blocking entry to the RC phase, and its
determined that the JIRA should not hold up the release, than it should
get re-targeted to the next release.

-Chris


"
Matei Zaharia <matei.zaharia@gmail.com>,"Wed, 17 Jun 2015 15:12:53 -0700",Welcoming some new committers,dev <dev@spark.apache.org>,"Hey all,

Over the past 1.5 months we added a number of new committers to the project, and I wanted to welcome them now that all of their respective forms, accounts, etc are in. Join me in welcoming the following new committers:

- Davies Liu
- DB Tsai
- Kousuke Saruta
- Sandy Ryza
- Yin Huai

Looking forward to more great contributions from all of these folks.

Matei
---------------------------------------------------------------------


"
Chester Chen <chester@alpinenow.com>,"Wed, 17 Jun 2015 15:18:36 -0700",Re: Welcoming some new committers,Matei Zaharia <matei.zaharia@gmail.com>,"Congratulations to All.

DB and Sandy, great works !



"
Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"Wed, 17 Jun 2015 15:20:07 -0700 (MST)",[SparkR] Have we already had any lint for SparkR?,dev@spark.apache.org,"Hi all, 

Have we already had any lint for R? I'm afraid I'm not familiar with the
inside of SparkR yet. 
As you know, we have lint for Scala and Python to check these codes.
So I think we should also check R codes automatically. And google-rlint
would be nice.

google-rlint - A program to lint the R code to follow the Google Style Guide
- Google Project Hosting https://code.google.com/p/google-rlint/


I tried to find an issue like that. I couldn't find one. 
I'm afraid we have already had a mechanism to check R codes.

Thanks,
Yu



-----
-- Yu Ishikawa
--

---------------------------------------------------------------------


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Wed, 17 Jun 2015 15:48:41 -0700",Re: [SparkR] Have we already had any lint for SparkR?,Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"We do have a JIRA open for this at
https://issues.apache.org/jira/browse/SPARK-6813 but I don't think anybody
is actively working on it yet. FWIW I think
https://github.com/jimhester/lintr looks more recently updated compared to
google-rlint, but we can discuss more about this on the JIRA


"
Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"Wed, 17 Jun 2015 15:56:31 -0700 (MST)",Re: [SparkR] Have we already had any lint for SparkR?,dev@spark.apache.org,"Hi Shivaram, 

Thank you for your reply and letting me know that.
I will join the discussion on JIRA later.

Thanks,
Yu



-----
-- Yu Ishikawa
--

---------------------------------------------------------------------


"
Thomas Dudziak <tomdzk@gmail.com>,"Wed, 17 Jun 2015 16:18:21 -0700",Hive 0.12 support in 1.4.0 ?,dev <dev@spark.apache.org>,"So I'm a little confused, has Hive 0.12 support disappeared in 1.4.0 ? The
release notes didn't mention anything, but the documentation doesn't list a
way to build for 0.12 anymore (
http://spark.apache.org/docs/latest/building-spark.html#building-with-hive-and-jdbc-support,
in fact it doesn't list anything other than 0.13), and I don't see any
maven profiles nor code for 0.12.

Tom
"
Xiangrui Meng <mengxr@gmail.com>,"Wed, 17 Jun 2015 17:12:24 -0700",Re: [sample code] deeplearning4j for Spark ML (@DeveloperAPI),Nick Pentreath <nick.pentreath@gmail.com>,"Hi Eron,

Please register your Spark Package on http://spark-packages.org, which
helps users find your work. Do you have some performance benchmark to
share? Thanks!

Best,
Xiangrui

l
working on
This
on
s
y
e
m
e
e
an
w
ith
l

---------------------------------------------------------------------


"
Sandy Ryza <sandy.ryza@cloudera.com>,"Wed, 17 Jun 2015 17:50:44 -0700",Re: [SparkScore] Performance portal for Apache Spark,"""Huang, Jie"" <jie.huang@intel.com>","This looks really awesome.


.
e
is
s
s
ig
al
e
mes
e
"
"""Duan, Jiangang"" <jiangang.duan@intel.com>","Thu, 18 Jun 2015 00:52:27 +0000",RE: [SparkScore] Performance portal for Apache Spark,"Sandy Ryza <sandy.ryza@cloudera.com>, ""Huang, Jie"" <jie.huang@intel.com>","We are looking for more workloads â€“ if you guys have any suggestions, let us know.

-jiangang

From: Sandy Ryza [mailto:sandy.ryza@cloudera.com]
Sent: Wednesday, June 17, 2015 5:51 PM
To: Huang, Jie
Cc: user@spark.apache.org; dev@spark.apache.org
Subject: Re: [SparkScore] Performance portal for Apache Spark

This looks really awesome.

On Tue, Jun 16, 2015 at 10:27 AM, Huang, Jie <jie.huang@intel.com<mailto:jie.huang@intel.com>> wrote:
Hi All

We are happy to announce Performance portal for Apache Spark http://01org.github.io/sparkscore/ !
The Performance Portal for Apache Spark provides performance data on the Spark upsteam to the community to help identify issues, better understand performance differentials between versions, and help Spark customers get across the finish line faster. The Performance Portal generates two reports, regular (weekly) report and release based regression test report. We are currently using two benchmark suites which include HiBench (http://github.com/intel-bigdata/HiBench) and Spark-perf (https://github.com/databricks/spark-perf ). We welcome and look forward to your suggestions and feedbacks. More information and details provided below
Abount Performance Portal for Apache Spark
Our goal is to work with the Apache Spark community to further enhance the scalability and reliability of the Apache Spark. The data available on this site allows community members and potential Spark customers to closely track performance trend of the Apache Spark. Ultimately, we hope that this project will help community to fix performance issue quickly, thus providing better Apache spark code to end customers. The current workloads used in the benchmarking include HiBench (a benchmark suite to evaluate big data framework like Hadoop MR, Spark from Intel) and Spark-perf (a performance testing framework for Apache Spark from Databricks). Additional benchmarks will be added as they become available
Description
________________________________
Each data point represents each workload runtime percent compared with the previous week. Different lines represents different workloads running on spark yarn-client mode.
Hardware
________________________________
CPU type: IntelÂ® XeonÂ® CPU E5-2697 v2 @ 2.70GHz
Memory: 128GB
NIC: 10GbE
Disk(s): 8 x 1TB SATA HDD
Software
________________________________
JAVA ver sion: 1.8.0_25
Hadoop version: 2.5.0-CDH5.3.2
HiBench version: 4.0
Spark on yarn-client mode
Cluster
________________________________
1 node for Master
10 nodes for Slave
Summary
The lower percent the better performance.
________________________________
Group

ww19

ww20

ww22

ww23

ww24

ww25

HiBench

9.1%

6.6%

6.0%

7.9%

-6.5%

-3.1%

spark-perf

4.1%

4.4%

-1.8%

4.1%

-4.7%

-4.6%


Y-Axis: normalized completion time; X-Axis: Work Week.
The commit number can be found in the result table.
The performance score for each workload is normalized based on the elapsed time for 1.2 release.The lower the better.

HiBench
________________________________
JOB

ww19

ww20

ww22

ww23

ww24

ww25

commit

489700c8

8e3822a0

530efe3e

90c60692

db81b9d8

4eb48ed1

sleep

%

%

-2.1%

-2.9%

-4.1%

12.8%

wordcount

17.6%

11.4%

8.0%

8.3%

-18.6%

-10.9%

kmeans

92.1%

61.5%

72.1%

92.9%

86.9%

95.8%

scan

-4.9%

-7.2%

%

-1.1%

-25.5%

-21.0%

bayes

-24.3%

-20.1%

-18.3%

-11.1%

-29.7%

-31.3%

aggregation

5.6%

10.5%

%

9.2%

-15.3%

-15.0%

join

4.5%

1.2%

%

1.0%

-12.7%

-13.9%

sort

-3.3%

-0.5%

-11.9%

-12.5%

-17.5%

-17.3%

pagerank

2.2%

3.2%

4.0%

2.9%

-11.4%

-13.0%

terasort

-7.1%

-0.2%

-9.5%

-7.3%

-16.7%

-17.0%


Comments: null means no such workload running or workload failed in this time.

Y-Axis: normalized completion time; X-Axis: Work Week.
The commit number can be found in the result table.
The performance score for each workload is normalized based on the elapsed time for 1.2 release.The lower the better.
spark-perf
________________________________
JOB

ww19

ww20

ww22

ww23

ww24

ww25

commit

489700c8

8e3822a0

530efe3e

90c60692

db81b9d8

4eb48ed1

agg

13.2%

7.0%

%

18.3%

5.2%

2.5%

agg-int

16.4%

21.2%

%

9.6%

4.0%

8.2%

agg-naive

4.3%

-2.4%

%

-0.8%

-6.7%

-6.8 %

scheduling

-6.1%

-8.9%

-14.5%

-2.1%

-6.4%

-6.5%

count-filter

4.1%

1.0%

6.6%

6.8%

-10.2%

-10.4%

count

4.8%

4.6%

6.7%

8.0%

-7.3%

-7.0%

sort

-8.1%

-2.5%

-6.2%

-7.0%

-14.6%

-14.4%

sort-int

4.5%

15.3%

-1.6%

-0.1%

-1.5%

-2.2%


Comments: null means no such workload running or workload failed in this time.

Y-Axis: normalized completion time; X-Axis: Work Week.
The commit number can be found in the result table.
The pe rformance score for each workload is normalized based on the elapsed time for 1.2 release.The lower the better.
Release
Summary
The lower percent the better performance.
________________________________
Group

1.2.1

1.3.0

1.3.1

1.4.0

HiBench

-1.0%

10.5%

8.4%

8.6%

spark-perf

3.2%

0.9%

1.9%

1.3%


Y-Axis: normalized completion time; X-Axis: Release.
The performance score for each workload is normalized based on the elapsed time for 1.2 release.The lower the better.

HiBench
________________________________
JOB

1.2.1

1.3.0

1.3.1

1.4.0

sleep

%

%

%

-0.5%

wordcount

3.5%

5.4%

5.1%

8.7%

kmeans

6.0%

72.6%

82.7%

100.7%

scan

-0.7%

-3.2%

-1.9%

-4.4%

bayes

-19.7%

7.7%

-24.5%

-14.4%

aggregation

4.6%

7.1%

9.9%

9.3%

join

0.7%

4.0%

8.6%

1.3%

sort

-1.0%

2.1%

-1.8%

-10.4%

pagerank

1.5 %

2.2%

1.3%

5.4%

terasort

-3.7%

-3.3%

-3.7%

-9.5%


Comments: null means no such workload running or workload failed in this time.

Y-Axis: normalized completion time; X-Axis: Release.
The commit number can be found in the result table.
The performance score for each workload is normalized based on the elapsed time for 1.2 release.The lower the better.
spark-perf
________________________________
JOB

1.2.1

1.3.0

1.3.1

1.4.0

agg

1.9%

3.1%

6.2%

5.0%

agg-int

6.4%

17.1%

18.0%

24.2%

agg-naive

-2.6%

-3.2%

-1.8%

-5.2%

scheduling

8.2%

-16.8%

-14.4%

-19.1%

count-filter

-0.4%

0.3%

-0.5%

0.4%

count

0.6%

-0.3%

0.4%

0.9%

sort

1.2%

-3.3%

-5.3%

-1.9%

sort-int

10.1%

10.0%

12.3%

6.0%


Comments: null means no such workload running or workload failed in this time.

Y-Axis: normalized completion time; X-Axis: Release.
The commit number can be found in the result table.
The performance score for each workload is normalized based on the elapsed time for 1.2 release.The lower the better.
________________________________
Copyright Â© 2015 Intel Corporation. All rights reserved. *Other names and brands may be claimed as the property of others.
Prots.01.org> Please subscribe to the list at: https://lists.01.org/mailman/listinfo/sparkscore


---------------------------------------------------------------------
To unsubscribe, e-mail: user-unsubscribe@spark.apache.org<mailto:user-unsubscribe@spark.apache.org>
For additional help@spark.apache.org>

"
Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"Wed, 17 Jun 2015 20:15:46 -0700 (MST)","[mllib] Refactoring some spark.mllib model classes in Python not
 inheriting JavaModelWrapper",dev@spark.apache.org,"Hi all,

I think we should refactor some machine learning model classes in Python to
reduce the software maintainability.
Inheriting JavaModelWrapper class, we can easily and directly call Scala API
for the model without PythonMLlibAPI.

In some case, a machine learning model class in Python has complicated
variables. That is, it is a little hard to implement import/export methods
and it is also a little troublesome to implement the function in both of
Scala and Python. And I think standardizing how to create a model class in
python is important.

What do you think about that?

Thanks,
Yu



-----
-- Yu Ishikawa
--

---------------------------------------------------------------------


"
Meethu Mathew <meethu.mathew@flytxt.com>,"Thu, 18 Jun 2015 10:28:11 +0530",[MLlib] Contributing algorithm for DP means clustering,dev@spark.apache.org,"Hi all,

At present, all the clustering algorithms in MLlib require the number of
clusters to be specified in advance.

The Dirichlet process (DP) is a popular non-parametric Bayesian mixture
model that allows for flexible clustering of data without having to specify
apriori the number of clusters. DP means is a non-parametric clustering
algorithm that uses a scale parameter 'lambda' to control the creation of
new clusters.

We have followed the distributed implementation of DP means which has been
proposed in the paper titled ""MLbase: Distributed Machine Learning Made
Easy"" by Xinghao Pan, Evan R. Sparks, Andre Wibisono.

I have raised a JIRA ticket at
https://issues.apache.org/jira/browse/SPARK-8402

Suggestions and guidance are welcome.

Regards,

Meethu Mathew
Senior Engineer
Flytxt
www.flytxt.com | Visit our blog <http://blog.flytxt.com/> | Follow us
<http://www.twitter.com/flytxt> | Connect on LinkedIn
<http://www.linkedin.com/company/22166?goback=%2Efcs_GLHD_flytxt_false_*2_*2_*2_*2_*2_*2_*2_*2_*2_*2_*2_*2&trk=ncsrch_hits>
"
"""Haopu Wang"" <HWang@qilinsoft.com>","Thu, 18 Jun 2015 13:44:02 +0800",RE: [SparkStreaming] NPE in DStreamCheckPointData.scala:125,"""Haopu Wang"" <HWang@qilinsoft.com>,
	""user"" <user@spark.apache.org>,
	<dev@spark.apache.org>","Can someone help? Thank you!

________________________________

From: Haopu Wang 
Sent: Monday, June 15, 2015 3:36 PM
To: user; dev@spark.apache.org
Subject: [SparkStreaming] NPE in DStreamCheckPointData.scala:125



I use the attached program to test checkpoint. It's quite simple.

 

When I run the program second time, it will load checkpoint data, that's
expected, however I see NPE in driver log.

 

Do you have any idea about the issue? I'm on Spark 1.4.0, thank you very
much!

 

====== logs ======

 

15/06/15 15:27:17 [THREAD ID=main] INFO FileInputDStream: Restoring
checkpoint data

15/06/15 15:27:17 [THREAD ID=main] INFO
FileInputDStream$FileInputDStreamCheckpointData: Restoring files for
time 1434353130000 ms - []

15/06/15 15:27:17 [THREAD ID=main] INFO
FileInputDStream$FileInputDStreamCheckpointData: Restoring files for
time 1434353140000 ms - []

15/06/15 15:27:17 [THREAD ID=main] INFO
FileInputDStream$FileInputDStreamCheckpointData: Restoring files for
time 1434353150000 ms - []

15/06/15 15:27:17 [THREAD ID=main] INFO
FileInputDStream$FileInputDStreamCheckpointData: Restoring files for
time 1434353160000 ms - []

15/06/15 15:27:17 [THREAD ID=main] INFO
FileInputDStream$FileInputDStreamCheckpointData: Restoring files for
time 1434353170000 ms - []

15/06/15 15:27:17 [THREAD ID=main] INFO
FileInputDStream$FileInputDStreamCheckpointData: Restoring files for
time 1434353180000 ms - []

15/06/15 15:27:17 [THREAD ID=main] INFO
FileInputDStream$FileInputDStreamCheckpointData: Restoring files for
time 1434353190000 ms - []

15/06/15 15:27:17 [THREAD ID=main] INFO FileInputDStream: Restored
checkpoint data

15/06/15 15:27:17 [THREAD ID=main] INFO MappedDStream: Restored
checkpoint data

15/06/15 15:27:17 [THREAD ID=main] INFO MappedDStream: Restored
checkpoint data

15/06/15 15:27:17 [THREAD ID=main] INFO ForEachDStream: Restored
checkpoint data

15/06/15 15:27:17 [THREAD ID=main] INFO DStreamGraph: Restored
checkpoint data

15/06/15 15:27:17 [THREAD ID=main] ERROR StreamingContext: Error
starting the context, marking it as stopped

java.io.IOException: java.lang.NullPointerException

       at
org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1242)

       at
org.apache.spark.streaming.dstream.DStreamCheckpointData.writeObject(DSt
reamCheckpointData.scala:123)

       at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

       at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav
a:57)

       at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessor
Impl.java:43)

       at java.lang.reflect.Method.invoke(Method.java:606)

       at
java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)

       at
java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)

       at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1
431)

       at
java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)

       at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:15
47)

       at
java.io.ObjectOutputStream.defaultWriteObject(ObjectOutputStream.java:44
0)

       at
org.apache.spark.streaming.dstream.DStream$$anonfun$writeObject$1.apply$
mcV$sp(DStream.scala:498)

       at
org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1239)

       at
org.apache.spark.streaming.dstream.DStream.writeObject(DStream.scala:493
)

       at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

       at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav
a:57)

       at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessor
Impl.java:43)

       at java.lang.reflect.Method.invoke(Method.java:606)

       at
java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)

       at
java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)

       at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1
431)

       at
java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)

       at
java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1377)

       at
java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1173)

       at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:15
47)

       at
java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)

       at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1
431)

       at
java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)

       at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:15
47)

       at
java.io.ObjectOutputStream.defaultWriteObject(ObjectOutputStream.java:44
0)

       at
org.apache.spark.streaming.DStreamGraph$$anonfun$writeObject$1.apply$mcV
$sp(DStreamGraph.scala:181)

       at
org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1239)

       at
org.apache.spark.streaming.DStreamGraph.writeObject(DStreamGraph.scala:1
76)

       at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

       at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav
a:57)

       at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessor
Impl.java:43)

       at java.lang.reflect.Method.invoke(Method.java:606)

       at
java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)

       at
java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)

       at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1
431)

       at
java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)

       at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:15
47)

       at
java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)

       at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1
431)

       at
java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)

       at
java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)

       at
org.apache.spark.streaming.Checkpoint$$anonfun$serialize$1.apply$mcV$sp(
Checkpoint.scala:113)

       at
org.apache.spark.streaming.Checkpoint$$anonfun$serialize$1.apply(Checkpo
int.scala:113)

       at
org.apache.spark.streaming.Checkpoint$$anonfun$serialize$1.apply(Checkpo
int.scala:113)

       at
org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1285)

       at
org.apache.spark.streaming.Checkpoint$.serialize(Checkpoint.scala:114)

       at
org.apache.spark.streaming.StreamingContext.validate(StreamingContext.sc
ala:547)

       at
org.apache.spark.streaming.StreamingContext.liftedTree1$1(StreamingConte
xt.scala:587)

       at
org.apache.spark.streaming.StreamingContext.start(StreamingContext.scala
:586)

       at
CheckPointTest$delayedInit$body.apply(CheckPointTest.scala:116)

       at scala.Function0$class.apply$mcV$sp(Function0.scala:40)

       at
scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12)

       at scala.App$$anonfun$main$1.apply(App.scala:71)

       at scala.App$$anonfun$main$1.apply(App.scala:71)

       at scala.collection.immutable.List.foreach(List.scala:318)

       at
scala.collection.generic.TraversableForwarder$class.foreach(TraversableF
orwarder.scala:32)

       at scala.App$class.main(App.scala:71)

       at CheckPointTest$.main(CheckPointTest.scala:7)

       at CheckPointTest.main(CheckPointTest.scala)

Caused by: java.lang.NullPointerException

       at
org.apache.spark.streaming.dstream.DStreamCheckpointData$$anonfun$writeO
bject$1.apply$mcV$sp(DStreamCheckpointData.scala:125)

       at
org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1239)

       ... 64 more

 

"
Sean Owen <sowen@cloudera.com>,"Thu, 18 Jun 2015 09:22:30 +0200",Re: Sidebar: issues targeted for 1.4.0,"""Heller, Chris"" <cheller@akamai.com>","I also like using Target Version meaningfully. It might be a little
much to require no Target Version = X before starting an RC. I do
think it's reasonable to not start the RC with Blockers open.

And here we started the RC with almost 100 TODOs for 1.4.0, most of
which did not get done. Not the end of the world, but, clearly some
other decisions were made in the past based on the notion that most of
those would get done. The 'targeting' is too optimistic. Given fixed
time, adding more TODOs generally means other stuff has to be taken
out for the release. If not, then it happens de facto anyway, which is
worse than managing it on purpose.

Anyway, thanks all for the attention to some cleanup. I'll wait a
short while and then fix up the rest of them as intelligently as I
can. Maybe I can push on this a little the next time we have a release
cycle to see how we're doing with use of Target Version.






---------------------------------------------------------------------


"
Nick Pentreath <nick.pentreath@gmail.com>,"Thu, 18 Jun 2015 10:22:29 +0200","Re: Approximate rank-based statistics (median, 95-th percentile,
 etc.) for Spark",Ray Ortigas <rortigas@linkedin.com.invalid>,"If it's going into the DataFrame API (which it probably should rather than
in RDD itself) - then it could become a UDT (similar to HyperLogLogUDT)
which would mean it doesn't have to implement Serializable, as it appears
that serialization is taken care of in the UDT def (e.g.
https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregates.scala#L254
)

If I understand correctly UDT SerDe correctly?


rote:
d
API
the
=rep1&type=pdf
e
emory.
n. The
"
"""=?ISO-8859-1?B?U2Vh?="" <261810726@qq.com>","Thu, 18 Jun 2015 21:15:40 +0800",Spark-sql(yarn-client) java.lang.NoClassDefFoundError: org/apache/spark/deploy/yarn/ExecutorLauncher,"""=?ISO-8859-1?B?ZGV2?="" <dev@spark.apache.org>","Hi, all:

I want to run spark sql on yarn(yarn-client), but ... I already set ""spark.yarn.jar"" and  ""spark.jars"" in conf/spark-defaults.conf.
./bin/spark-sql -f game.sql --executor-memory 2g --num-executors 100 > game.txt
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/spark/deploy/yarn/ExecutorLauncher
Caused by: java.lang.ClassNotFoundException: org.apache.spark.deploy.yarn.ExecutorLauncher
        at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
Could not find the main class: org.apache.spark.deploy.yarn.ExecutorLauncher.  Program will exit.





Any can help?"
"""=?ISO-8859-1?B?U2Vh?="" <261810726@qq.com>","Thu, 18 Jun 2015 21:39:41 +0800",Spark-sql(yarn-client) java.lang.NoClassDefFoundError: org/apache/spark/deploy/yarn/ExecutorLauncher,"""=?ISO-8859-1?B?ZGV2?="" <dev@spark.apache.org>","Hi, all:


I want to run spark sql on yarn(yarn-client), but ... I already set ""spark.yarn.jar"" and  ""spark.jars"" in conf/spark-defaults.conf.
./bin/spark-sql -f game.sql --executor-memory 2g --num-executors 100 > game.txt
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/spark/deploy/yarn/ExecutorLauncher
Caused by: java.lang.ClassNotFoundException: org.apache.spark.deploy.yarn.ExecutorLauncher
        at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
Could not find the main class: org.apache.spark.deploy.yarn.ExecutorLauncher.  Program will exit.





Anyone can help?"
Yin Huai <yhuai@databricks.com>,"Thu, 18 Jun 2015 08:19:54 -0700",Re: Spark-sql(yarn-client) java.lang.NoClassDefFoundError: org/apache/spark/deploy/yarn/ExecutorLauncher,Sea <261810726@qq.com>,"Is it the full stack trace?


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 18 Jun 2015 15:43:24 +0000",Re: Sidebar: issues targeted for 1.4.0,"Sean Owen <sowen@cloudera.com>, ""Heller, Chris"" <cheller@akamai.com>","taken
out for the release. If not, then it happens de facto anyway, which is
worse than managing it on purpose.

+1 to this.

I wouldn't mind helping go through open issues on JIRA targeted for the
next release around RC time to make sure that a) nothing major is getting
missed for the release and b) the JIRA backlog gets trimmed of the cruft
which is constantly building up. It's good housekeeping.

Nick


"
anshu shukla <anshushukla0@gmail.com>,"Thu, 18 Jun 2015 23:54:53 +0530",Latency between the RDD in Streaming,user@spark.apache.org,"Is there any  fixed way to find  among RDD in stream processing systems ,
in the Distributed set-up .

-- 
Thanks & Regards,
Anshu Shukla
"
Xiangrui Meng <mengxr@gmail.com>,"Thu, 18 Jun 2015 12:11:57 -0700","Re: [mllib] Refactoring some spark.mllib model classes in Python not
 inheriting JavaModelWrapper",Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"Hi Yu,

Reducing the code complexity on the Python side is certainly what we
want to see:) We didn't call Java directly in Python models because
Java methods don't work inside RDD closures, e.g.,

rdd.map(lambda x: model.predict(x[1]))

But I agree that for model save/load the implementation should be
simplified. Could you submit a PR and see how much code we can save?

Thanks,
Xiangrui


---------------------------------------------------------------------


"
Joseph Bradley <joseph@databricks.com>,"Thu, 18 Jun 2015 12:54:26 -0700",Re: Random Forest driver memory,Isca Harmatz <pop1998@gmail.com>,"Hi Isca,

Could you please give more details?  Data size, model parameters, stack
traces / logs, etc. to help get a better picture?

Thanks,
Joseph


"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Thu, 18 Jun 2015 21:26:00 +0000",Increase partition count (repartition) without shuffle,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

Is there a way to increase the amount of partition of RDD without causing shuffle? I've found JIRA issue https://issues.apache.org/jira/browse/SPARK-5997 however there is no implementation yet.

Just in case, I am reading data from ~300 big binary files, which results in 300 partitions, then I need to sort my RDD, but it crashes with outofmemory exception. If I change the number of partitions to 2000, sort works OK, but repartition itself takes a lot of time due to shuffle.

Best regards, Alexander
"
Sandy Ryza <sandy.ryza@cloudera.com>,"Thu, 18 Jun 2015 14:36:10 -0700",Re: Increase partition count (repartition) without shuffle,"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Hi Alexander,

There is currently no way to create an RDD with more partitions than its
parent RDD without causing a shuffle.

However, if the files are splittable, you can set the Hadoop configurations
that control split size to something smaller so that the HadoopRDD ends up
with more partitions.

-Sandy


"
Mridul Muralidharan <mridul@gmail.com>,"Thu, 18 Jun 2015 15:02:13 -0700",Re: Increase partition count (repartition) without shuffle,Sandy Ryza <sandy.ryza@cloudera.com>,"If you can scan input twice, you can of course do per partition count and
build custom RDD which can reparation without shuffle.
But nothing off the shelf as Sandy mentioned.

Regards
Mridul


g
s
t
"
"""=?gb18030?B?U2Vh?="" <261810726@qq.com>","Fri, 19 Jun 2015 11:05:05 +0800","=?gb18030?B?u9i4tKO6IFNwYXJrLXNxbCh5YXJuLWNsaWVudCkg?=
 =?gb18030?B?amF2YS5sYW5nLk5vQ2xhc3NEZWZGb3VuZEVycm9y?=
 =?gb18030?B?OiBvcmcvYXBhY2hlL3NwYXJrL2RlcGxveS95YXJu?=
 =?gb18030?B?L0V4ZWN1dG9yTGF1bmNoZXI=?=","""=?gb18030?B?WWluIEh1YWk=?="" <yhuai@databricks.com>","Thanks, Yin Huai
I work it out.
I use JDK1.7 to build Spark 1.4.0, but my yarn cluster run on JDK1.6. 
But java.version in pom.xml in 1.6 and the exception makes me confused




------------------ Ô­Ê¼ÓÊ¼þ ------------------
·¢¼þÈË: ""Yin Huai"";<yhuai@databricks.com>;
·¢ËÍÊ±¼ä: 2015Äê6ÔÂ18ÈÕ(ÐÇÆÚËÄ) ÍíÉÏ11:19
ÊÕ¼þÈË: ""Sea""<261810726@qq.com>; 
³­ËÍ: ""dev""<dev@spark.apache.org>; 
Ö÷Ìâ: Re: Spark-sql(yarn-client) java.lang.NoClassDefFoundError: org/apache/spark/deploy/yarn/ExecutorLauncher



Is it the full stack trace?

On Thu, Jun 18, 2015 at 6:39 AM, Sea <261810726@qq.com> wrote:
Hi, all:


I want to run spark sql on yarn(yarn-client), but ... I already set ""spark.yarn.jar"" and  ""spark.jars"" in conf/spark-defaults.conf.
./bin/spark-sql -f game.sql --executor-memory 2g --num-executors 100 > game.txt
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/spark/deploy/yarn/ExecutorLauncher
Caused by: java.lang.ClassNotFoundException: org.apache.spark.deploy.yarn.ExecutorLauncher
        at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
Could not find the main class: org.apache.spark.deploy.yarn.ExecutorLauncher.  Program will exit.







Anyone can help?"
Peter Rudenko <petro.rudenko@gmail.com>,"Fri, 19 Jun 2015 21:36:07 +0300",[Tungsten] NPE in UnsafeShuffleWriter.java,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi want to try new tungsten-sort shuffle manager, but on 1 stage 
executors start to die with NPE:

15/06/19 17:53:35 WARN TaskSetManager: Lost task 38.0 in stage 41.0 (TID 
3176, ip-10-50-225-214.ec2.internal): java.lang.NullPointerException
         at 
org.apache.spark.shuffle.unsafe.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:151)
         at 
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:70)
         at 
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
         at org.apache.spark.scheduler.Task.run(Task.scala:70)
         at 
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
         at 
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
         at 
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
         at java.lang.Thread.run(Thread.java:745)


Any suggestions?

Thanks,
Peter Rudenko
"
Josh Rosen <rosenville@gmail.com>,"Fri, 19 Jun 2015 11:51:19 -0700",Re: [Tungsten] NPE in UnsafeShuffleWriter.java,Peter Rudenko <petro.rudenko@gmail.com>,"Hey Peter,

I think that this is actually due to an error-handling issue: if you look
at the stack trace that you posted, the NPE is being thrown from an
error-handling branch of a `finally` block:

@Override public void write(scala.collection.Iterator<Product2<K, V>>
records) throws IOException { boolean success = false; try { while
(records.hasNext())
{ insertRecordIntoSorter(records.next()); } closeAndWriteOutput(); success =
true; } finally { if (!success) { sorter.cleanupAfterError(); // <---- this
is the line throwing the error } } }

I suspect that what's happening is that an exception is being thrown from
user / upstream code in the initial call to records.next(), but the
error-handling block is failing because sorter == null since we haven't
initialized it yet.

I'm going to file a JIRA for this and will try to add a set of regression
tests to the ShuffleSuite to make sure exceptions from user code aren't
swallowed like this.


"
Sean Owen <sowen@cloudera.com>,"Fri, 19 Jun 2015 20:54:41 +0200",Workaround for problems with OS X + JIRA Client,dev <dev@spark.apache.org>,"Not sure if many of you use JIRA Client
(http://almworks.com/jiraclient/overview.html) to keep tabs on JIRA --
definitely worth it -- but if you're on OS X, I wonder if you too have
suddenly been experiencing some type of SSL / keypair error on
syncing? It's something to do with a JIRA server update and the fact
that this app only knows how to run on Apple's Java 6, and it has some
lack of support for bigger keys.

Anyway ... if so, and you have Java 7 / 8 available locally as 'java',
this mostly works:

cd /Applications/JIRA\ Client.app/Contents/Resources/Java/lib
java -jar ../jiraclient.jar

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Fri, 19 Jun 2015 21:17:12 +0200",Stats on targets for 1.5.0,dev <dev@spark.apache.org>,"Quick point of reference for 1.5.0: 226 issues are Fixed for 1.5.0,
and 388 are Targeted for 1.5.0. So maybe 36% of things to be done for
1.5.0 are complete, and we're in theory 3 of 8 weeks into the merge
window, or 37.5%.

That's nicely on track! assuming, of course, that nothing else is
targeted for 1.5.0. History suggests that a lot more will be, since a
minor release has more usually had 1000+ JIRAs. However lots of
forward-looking JIRAs have been filed, so it may be that most planned
work is on the books already this time around.

I think it would be fantastic if this work was burned down before
adding big new chunks of work. The stat is worth keeping an eye on.

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 19 Jun 2015 21:47:06 +0000",Re: Stats on targets for 1.5.0,"Sean Owen <sowen@cloudera.com>, dev <dev@spark.apache.org>","adding big new chunks of work. The stat is worth keeping an eye on.

+1, keeping in mind that burning down work also means just targeting it for
a different release or closing it. :)

Nick



"
Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"Fri, 19 Jun 2015 18:31:23 -0700 (MST)","Re: [mllib] Refactoring some spark.mllib model classes in Python
 not inheriting JavaModelWrapper",dev@spark.apache.org,"Hi Xiangrui

I got it. I will try to refactor any model class not inheriting
JavaModelWrapper and show you it.

Thanks,
Yu



-----
-- Yu Ishikawa
--

---------------------------------------------------------------------


"
Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"Fri, 19 Jun 2015 22:29:47 -0700 (MST)",Re: Workaround for problems with OS X + JIRA Client,dev@spark.apache.org,"Hi Sean,

That sounds interesting. I didn't know the client. I will try it later.
Thank you for sharing the information.

Yu



-----
-- Yu Ishikawa
--

---------------------------------------------------------------------


"
Josh Rosen <rosenville@gmail.com>,"Fri, 19 Jun 2015 23:42:31 -0700",Re: [Tungsten] NPE in UnsafeShuffleWriter.java,Peter Rudenko <petro.rudenko@gmail.com>,"I've filed https://issues.apache.org/jira/browse/SPARK-8498 to fix this
error-handling code.


"
Debasish Das <debasish.das83@gmail.com>,"Sat, 20 Jun 2015 00:21:27 -0700",Impala created parquet tables,dev <dev@spark.apache.org>,"Hi,

I have some impala created parquet tables which hive 0.13.2 can read fine.

Now the same table when I want to read using Spark SQL 1.3 I am getting
exception class exception that parquet.hive.serde.ParquetHiveSerde not
found.

I am assuming that hive somewhere is putting the parquet-hive-bundle.jar in
hive classpath but I tried putting the parquet-hive-bundle.jar in
spark-1.3/conf/hive-site.xml through auxillary jar but even that did not
work.

Any input on fixing this will be really helpful.

Thanks.
Deb
"
anshu shukla <anshushukla0@gmail.com>,"Sat, 20 Jun 2015 19:57:32 +0530",Fwd: Verifying number of workers in Spark Streaming,"dev@spark.apache.org, Tathagata Das <tdas@databricks.com>, user <user@spark.apache.org>","Any suggestions please ..!!
How to know that  In stream  Processing  over the  cluster  of 8 machines
 all the machines/woker nodes are being used  (my cluster have  8 slaves )
 .
I am submitting job from master itself over the ec-2 cluster crated by the
ec-2 scripts available with spark. But i am not able  figure out that  my
job is  using all workers or not .





-- 
Thanks & Regards,
Anshu Shukla
SERC-IISC
"
Debasish Das <debasish.das83@gmail.com>,"Sat, 20 Jun 2015 07:30:58 -0700",Re: Welcoming some new committers,Chester Chen <chester@alpinenow.com>,"Congratulations to All.

DB great work in bringing quasi newton methods to Spark !


"
Debasish Das <debasish.das83@gmail.com>,"Sat, 20 Jun 2015 08:00:34 -0700",Velox Model Server,dev <dev@spark.apache.org>,"Hi,

The demo of end-to-end ML pipeline including the model server component at
Spark Summit was really cool.

I was wondering if the Model Server component is based upon Velox or it
uses a completely different architecture.

https://github.com/amplab/velox-modelserver

We are looking for an open source version of model server to build upon.

Thanks.
Deb
"
Andrew Or <andrew@databricks.com>,"Sat, 20 Jun 2015 09:47:47 -0700",Re: Welcoming some new committers,Debasish Das <debasish.das83@gmail.com>,"Welcome!

2015-06-20 7:30 GMT-07:00 Debasish Das <debasish.das83@gmail.com>:

"
Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"Sat, 20 Jun 2015 10:29:38 -0700 (MST)","[pyspark][mllib] What is the best way to treat int and long int
 between python2.6/python3.4 and Java?",dev@spark.apache.org,"Hi all,

I have done a survey about treating int type and long int type between
python2.6/python3.4 and Java as follows. When we want to return Long
value(s) from Java to python vice versa, what is the best way?
https://gist.github.com/yu-iskw/12e92c2d718ca41dea90

Based on that, Joseph and I are tackling ""[SPARK-6259] Python API for LDA"".
We wonder if we should create a wrapper class for the document of LDA or
not. Do you have any idea to implement it?
https://issues.apache.org/jira/browse/SPARK-6259

Thanks,
Yu



-----
-- Yu Ishikawa
--

---------------------------------------------------------------------


"
Ted Malaska <ted.malaska@cloudera.com>,"Sat, 20 Jun 2015 13:37:53 -0400",Re: Welcoming some new committers,Andrew Or <andrew@databricks.com>,"Super congrats.  Well earned.

"
Guru Medasani <gdmeda@gmail.com>,"Sat, 20 Jun 2015 14:28:15 -0500",Re: Welcoming some new committers,Matei Zaharia <matei.zaharia@gmail.com>,"Congratulations to all the new committers!

Guru Medasani
gdmeda@gmail.com



project, and I wanted to welcome them now that all of their respective forms, accounts, etc are in. Join me in welcoming the following new committers:


---------------------------------------------------------------------


"
Corey Nolet <cjnolet@gmail.com>,"Sat, 20 Jun 2015 16:12:43 -0400",Re: Welcoming some new committers,Guru Medasani <gdmeda@gmail.com>,"Congrats guys! Keep up the awesome work!


"
acidghost <andreajemmett@gmail.com>,"Sun, 21 Jun 2015 03:18:23 -0700 (MST)",unsafe/compile error,dev@spark.apache.org,"Hello there!I'm getting a compilation error when trying to run sbt
mllib/test.Here're the errors:
[info] Compiling 15 Java sources to
/media/SB-1TB/workarea/apache-spark/unsafe/target/scala-2.10/classes...[error]
/media/SB-1TB/workarea/apache-spark/unsafe/src/main/java/org/apache/spark/unsafe/memory/MemoryBlock.java:20:
error: cannot find symbol[error] import javax.annotation.Nullable;[error]                       
^[error]   symbol:   class Nullable[error]   location: package
javax.annotation[error]
/media/SB-1TB/workarea/apache-spark/unsafe/src/main/java/org/apache/spark/unsafe/memory/MemoryLocation.java:20:
error: cannot find symbol[error] import javax.annotation.Nullable;[error]                       
^[error]   symbol:   class Nullable[error]   location: package
javax.annotation[error]
/media/SB-1TB/workarea/apache-spark/unsafe/src/main/java/org/apache/spark/unsafe/memory/ExecutorMemoryManager.java:24:
error: package javax.annotation.concurrent does not exist[error] import
javax.annotation.concurrent.GuardedBy;[error]                                  
^[error]
/media/SB-1TB/workarea/apache-spark/unsafe/src/main/java/org/apache/spark/unsafe/memory/MemoryBlock.java:37:
error: cannot find symbol[error]   MemoryBlock(@Nullable Object obj, long
offset, long length) {[error]                ^[error]   symbol:   class
Nullable[error]   location: class MemoryBlock[error]
/media/SB-1TB/workarea/apache-spark/unsafe/src/main/java/org/apache/spark/unsafe/memory/MemoryLocation.java:28:
error: cannot find symbol[error]   @Nullable[error]    ^[error]   symbol:  
class Nullable[error]   location: class MemoryLocation[error]
/media/SB-1TB/workarea/apache-spark/unsafe/src/main/java/org/apache/spark/unsafe/memory/MemoryLocation.java:33:
error: cannot find symbol[error]   public MemoryLocation(@Nullable Object
obj, long offset) {[error]                          ^[error]   symbol:  
class Nullable[error]   location: class MemoryLocation[error]
/media/SB-1TB/workarea/apache-spark/unsafe/src/main/java/org/apache/spark/unsafe/memory/ExecutorMemoryManager.java:42:
error: cannot find symbol[error]   @GuardedBy(""this"")[error]    ^[error]  
symbol:   class GuardedBy[error]   location: class
ExecutorMemoryManager[error] 7 errors[error] (unsafe/compile:compile) javac
returned nonzero exit code[error] Total time: 1 s, completed Jun 21, 2015
12:13:26 PM
I'm testing a new feature and I'm unable to do that now.. Any suggestion?



--"
acidghost <andreajemmett@gmail.com>,"Sun, 21 Jun 2015 03:43:41 -0700 (MST)",Re: unsafe/compile error,dev@spark.apache.org,"After an sbt update the tests run. But all the ""cluster"" ones fail on ""task
size should be small in both training and prediction""



--

---------------------------------------------------------------------


"
salexln <salexln@gmail.com>,"Sun, 21 Jun 2015 09:57:35 -0700 (MST)",JIRA 2344 status (Fuzzy C-Means),dev@spark.apache.org,"Hi guys,

Does anyone know what is the status of this issue? 

(https://issues.apache.org/jira/browse/SPARK-2344)

Beniamino was supposed to add an implementation but it seems that his code
was removed from Github
(https://github.com/bdelpizzo/mllib-extension/blob/master/clustering/FCM.scala)
and he does not answer any emails...






--

---------------------------------------------------------------------


"
Burak Yavuz <brkyvz@gmail.com>,"Sun, 21 Jun 2015 10:32:05 -0700",Re: unsafe/compile error,acidghost <andreajemmett@gmail.com>,"You need to build an assembly jar for the cluster tests to pass. You may
use 'sbt assembly/assembly'.

Best,
Burak

"
Burak Yavuz <brkyvz@gmail.com>,"Sun, 21 Jun 2015 10:33:46 -0700",Re: unsafe/compile error,acidghost <andreajemmett@gmail.com>,"In addition, if you want to run a single suite, you may use:
with sbt.

"
acidghost <andreajemmett@gmail.com>,"Sun, 21 Jun 2015 11:15:59 -0700 (MST)",Re: unsafe/compile error,dev@spark.apache.org,"
But it's not working, runs all mllib suites.



--

---------------------------------------------------------------------


"
jcai <jonathon.cai@yale.edu>,"Sun, 21 Jun 2015 12:05:14 -0700 (MST)","Web UI and History Server are Inconsistent; Web UI sometimes cannot
 process logs",dev@spark.apache.org,"I am running a very simple program (WordCount) on Spark stand-alone mode. I
find that when I examine the
web UI, a couple bugs arise:

1. There is a discrepancy between the number denoting the duration of the
application when I run the history server and the number given by the web UI
(default address is master:8080). I checked more specific details, including
task and stage durations (when clicking on the application), and these
appear to be the same for both avenues.

2. Sometimes the web UI on master:8080 is unable to display more specific
information for an application that has finished (when clicking on the
application), even when there is a log file in the appropriate directory.
But when the history server is opened, it is able to read this file and
output information.

Any ideas on how to approach these?

I am trying to do accurate performance measurements on Spark workloads. If
the application durations on the web UI aren't accurate (or consistent) for
such simple workloads, 
that seems problematic.

I also opened up a  JIRA report
<https://issues.apache.org/jira/browse/SPARK-8512?jql=project%20%3D%20SPARK%20AND%20resolution%20%3D%20Unresolved%20AND%20priority%20%3D%20Major%20ORDER%20BY%20key%20DESC>  
on the issue.

-- Jonathon





--

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Sun, 21 Jun 2015 12:10:30 -0700",Re: unsafe/compile error,acidghost <andreajemmett@gmail.com>,"Put them in quotes, e.g.



"
Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"Sun, 21 Jun 2015 13:54:19 -0700 (MST)","[jenkins] ERROR: Publisher 'Publish JUnit test result report'
 failed: No test report files were found. Configuration error?",dev@spark.apache.org,"Hi all,

How do I deal with the error on the official Jenkins?
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/35412/console

```
Archiving unit tests logs...
Attempting to post to Github...
 > Post successful.
Archiving artifacts
WARN: No artifacts found that match the file pattern
""**/target/unit-tests.log"". Configuration error?
WARN: java.lang.InterruptedException: no matches found within 10000
Recording test results
ERROR: Publisher 'Publish JUnit test result report' failed: No test report
files were found. Configuration error?
Finished: FAILURE
```

It seems that the unit testing related to the PR passed. However, the
Jenkins posted ""Merged build finished. Test FAILed."" to github.
https://github.com/apache/spark/pull/6926

Thanks
Yu




-----
-- Yu Ishikawa
--

---------------------------------------------------------------------


"
Josh Rosen <rosenville@gmail.com>,"Sun, 21 Jun 2015 14:10:51 -0700",Re: [jenkins] ERROR: Publisher 'Publish JUnit test result report' failed: No test report files were found. Configuration error?,Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"This is a side effect of the new pull request tester script interacting badly with a Jenkins plugin, not anything caused by your changes. I'm working on a fix but in the meantime I'd just trust what SparkQA says.

Sent from my phone

ote:
onsole

n3.nabble.com/jenkins-ERROR-Publisher-Publish-JUnit-test-result-report-failed-No-test-report-files-were-found-Conf-tp12823.html
com.

---------------------------------------------------------------------


"
Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"Sun, 21 Jun 2015 14:31:17 -0700 (MST)","Re: [jenkins] ERROR: Publisher 'Publish JUnit test result report'
 failed: No test report files were found. Configuration error?",dev@spark.apache.org,"Hi Josh, 

Thank you for your continuous support. I'm looking forward to the fix.

Thanks,
Yu




-----
-- Yu Ishikawa
--

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Sun, 21 Jun 2015 16:28:30 -0700","Re: Approximate rank-based statistics (median, 95-th percentile,
 etc.) for Spark",Nick Pentreath <nick.pentreath@gmail.com>,"aggregate function be a byte array operated using Unsafe -- that plays very
nicely with the binary data processing we are doing (i.e. fast
serialization, no gc).

The downside is that we'd need to re-implement whatever algorithm is out
there in order for them to be usable with Unsafe/byte arrays.




n
rg/apache/spark/sql/catalyst/expressions/aggregates.scala#L254
:
I
ed
 API
 the
=rep1&type=pdf
f
ing
!
"
Andrea Jemmett <andreajemmett@gmail.com>,"Mon, 22 Jun 2015 08:56:32 +0200",Re: unsafe/compile error,Reynold Xin <rxin@databricks.com>,"Thank you, that worked!

*Andrea Jemmett*

2015-06-21 21:10 GMT+02:00 Reynold Xin <rxin@databricks.com>:

"
Yin Huai <yhuai@databricks.com>,"Mon, 22 Jun 2015 11:03:16 -0700",Re: Hive 0.12 support in 1.4.0 ?,Thomas Dudziak <tomdzk@gmail.com>,"Hi Tom,

In Spark 1.4, we have de-coupled the support of Hive's metastore and other
parts (parser, Hive udfs, and Hive SerDes). The execution engine of Spark
SQL in 1.4 will always use Hive 0.13.1. For the metastore connection part,
you can connect to either Hive 0.12 or 0.13.1's metastore. We have removed
old shims and profiles of specifying the Hive version (since execution
engine is always using Hive 0.13.1 and metastore client part can be
configured to use either Hive 0.12 or 0.13.1's metastore).

You can take a look at
https://spark.apache.org/docs/latest/sql-programming-guide.html#interacting-with-different-versions-of-hive-metastore
for connecting to Hive 0.12's metastore.

Let me know if you have any question.

Thanks,

Yin


"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Tue, 23 Jun 2015 01:44:29 +0000","Force Spark save parquet files with replication factor other than 3
 (default one)","""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

My Hadoop is configured to have replication ratio = 2. I've added $HADOOP_HOME/config to the PATH as suggested in http://apache-spark-user-list.1001560.n3.nabble.com/hdfs-replication-on-saving-RDD-td289.html. Spark (1.4) does rdd.saveAsTextFile with replication=2. However DataFrame.saveAsParquet is done with replication = 3. How can I force Spark Dataframe to save parquet files with replication factor other than 3 (default one)?

Best regards, Alexander
"
Niranda Perera <niranda.perera@gmail.com>,"Tue, 23 Jun 2015 11:33:58 +0530",custom REST port from spark-defaults.cof,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

is there a configuration setting to set a custom port number for the master
REST URL? can that be included in the spark-defaults.conf?

cheers
-- 
Niranda
@n1r44 <https://twitter.com/N1R44>
https://pythagoreanscript.wordpress.com/
"
StanZhai <mail@zhaishidan.cn>,"Tue, 23 Jun 2015 01:42:15 -0700 (MST)",[SparkSQL 1.4]Could not use concat with UDF in where clause,dev@spark.apache.org,"Hi all,

After upgraded the cluster from spark 1.3.1 to 1.4.0(rc4), I encountered the
following exception when use concat with UDF in where clause:

===================Exception====================
org.apache.spark.sql.catalyst.analysis.UnresolvedException: Invalid call to
dataType on unresolved object, tree:
'concat(HiveSimpleUdf#org.apache.hadoop.hive.ql.udf.UDFYear(date#1776),å¹´)
	at
org.apache.spark.sql.catalyst.analysis.UnresolvedFunction.dataType(unresolved.scala:82)
	at
org.apache.spark.sql.catalyst.analysis.HiveTypeCoercion$InConversion$$anonfun$apply$5$$anonfun$applyOrElse$15.apply(HiveTypeCoercion.scala:299)
	at
org.apache.spark.sql.catalyst.analysis.HiveTypeCoercion$InConversion$$anonfun$apply$5$$anonfun$applyOrElse$15.apply(HiveTypeCoercion.scala:299)
	at
scala.collection.LinearSeqOptimized$class.exists(LinearSeqOptimized.scala:80)
	at scala.collection.immutable.List.exists(List.scala:84)
	at
org.apache.spark.sql.catalyst.analysis.HiveTypeCoercion$InConversion$$anonfun$apply$5.applyOrElse(HiveTypeCoercion.scala:299)
	at
org.apache.spark.sql.catalyst.analysis.HiveTypeCoercion$InConversion$$anonfun$apply$5.applyOrElse(HiveTypeCoercion.scala:298)
	at
org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:222)
	at
org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:222)
	at
org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51)
	at
org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:221)
	at
org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$transformExpressionDown$1(QueryPlan.scala:75)
	at
org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:85)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at
scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at
org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDown(QueryPlan.scala:94)
	at
org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressions(QueryPlan.scala:64)
	at
org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformAllExpressions$1.applyOrElse(QueryPlan.scala:136)
	at
org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformAllExpressions$1.applyOrElse(QueryPlan.scala:135)
	at
org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:222)
	at
org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:222)
	at
org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51)
	at
org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:221)
	at
org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:242)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at
scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at
org.apache.spark.sql.catalyst.trees.TreeNode.transformChildrenDown(TreeNode.scala:272)
	at
org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:227)
	at
org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:242)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at
scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at
org.apache.spark.sql.catalyst.trees.TreeNode.transformChildrenDown(TreeNode.scala:272)
	at
org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:227)
	at
org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:212)
	at
org.apache.spark.sql.catalyst.plans.QueryPlan.transformAllExpressions(QueryPlan.scala:135)
	at
org.apache.spark.sql.catalyst.analysis.HiveTypeCoercion$InConversion$.apply(HiveTypeCoercion.scala:298)
	at
org.apache.spark.sql.catalyst.analysis.HiveTypeCoercion$InConversion$.apply(HiveTypeCoercion.scala:297)
	at
org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:61)
	at
org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:59)
	at
scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:111)
	at scala.collection.immutable.List.foldLeft(List.scala:84)
	at
org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:59)
	at
org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:51)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at
org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:51)
	at
org.apache.spark.sql.SQLContext$QueryExecution.analyzed$lzycompute(SQLContext.scala:922)
	at
org.apache.spark.sql.SQLContext$QueryExecution.analyzed(SQLContext.scala:922)
	at
org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:920)
	at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:131)
	at org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:51)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:744)
	at test.service.SparkHiveService.query(SparkHiveService.scala:79)
	...
	at java.lang.Thread.run(Thread.java:745)

=============The SQL is: ===================
select * from test where concat(year(date), 'å¹´') in ( '2015å¹´', '2014å¹´' )
limit 10

This SQL can be run in spark 1.3.1 but error in spark 1.4. I've tried run
some similar sql in spark 1.4.0, found the following sql could be run
correctly:

select * from test where concat(year(date), 'å¹´') = '2015å¹´' limit 10
select * from test where concat(sex, 'T') in ( 'MT' ) limit 10

In short, when I use 'concat', UDF and 'in' together in sql, I will get the
exception:  Invalid call to dataType on unresolved object.

Is catalyst changed from 1.3 to 1.4? Any suggestion?

Best, Stan



--
3.nabble.com/SparkSQL-1-4-Could-not-use-concat-with-UDF-in-where-clause-tp12832.html
om.

---------------------------------------------------------------------


"
anshu shukla <anshushukla0@gmail.com>,"Tue, 23 Jun 2015 15:19:19 +0530",Calculating tuple count /input rate with time,"user <user@spark.apache.org>, dev@spark.apache.org, 
	Tathagata Das <tdas@databricks.com>, ayan guha <guha.ayan@gmail.com>, 
	Akhil Das <akhil@sigmoidanalytics.com>","I am calculating input rate using the following logic.

And i think this foreachRDD is always running on driver (println are
seen on driver)

1- Is there any other way to do that in less cost .

2- Will this give me the correct count for rate  .


//code -

inputStream.foreachRDD(new Function<JavaRDD<String>, Void>() {
    @Override
    public Void call(JavaRDD<String> stringJavaRDD) throws Exception {
        System.out.println(System.currentTimeMillis()+"",spoutstringJavaRDD,""
+ stringJavaRDD.count() );
        return null;
    }
});



-- 
Thanks & Regards,
Anshu Shukla
"
Sean Owen <sowen@cloudera.com>,"Tue, 23 Jun 2015 11:47:12 +0300",OK to add committers active on JIRA to JIRA admin role?,dev <dev@spark.apache.org>,"There are some committers who are active on JIRA and sometimes need to
do things that require JIRA admin access -- in particular thinking of
adding a new person as ""Contributor"" in order to assign them a JIRA.
We can't change what roles can do what (think that INFRA ticket is
dead) but can add to the Admin role. Would anyone object to making a
few more committers JIRA Admins for this purpose?

---------------------------------------------------------------------


"
Nick Pentreath <nick.pentreath@gmail.com>,"Tue, 23 Jun 2015 11:19:13 +0200",HyperLogLogUDT,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey Spark devs

I've been looking at DF UDFs and UDAFs. The approx distinct is using
hyperloglog,
but there is only an option to return the count as a Long.

It can be useful to be able to return and store the actual data structure
(ie serialized HLL). This effectively allows one to do aggregation /
rollups over columns while still preserving the ability to get distinct
counts.

For example, one can store daily aggregates of events, grouped by various
columns, while storing for each grouping the HLL of say unique users. So
you can get the uniques per day directly but could also very easily do
arbitrary aggregates (say monthly, annually) and still be able to get a
unique count for that period by merging the daily HLLS.

I did this a while back as a Hive UDAF (https://github.com/MLnick/hive-udf)
which returns a Struct field containing a ""cardinality"" field and a
""binary"" field containing the serialized HLL.

I was wondering if there would be interest in something like this? I am not
so clear on how UDTs work with regards to SerDe - so could one adapt the
HyperLogLogUDT to be a Struct with the serialized HLL as a field as well as
count as a field? Then I assume this would automatically play nicely with
DataFrame I/O etc. The gotcha is one needs to then call
""approx_count_field.count"" (or is there a concept of a ""default field"" for
a Struct?).

Also, being able to provide the bitsize parameter may be useful...

The same thinking would apply potentially to other approximate (and
mergeable) data structures like T-Digest and maybe CMS.

Nick
"
Michael Armbrust <michael@databricks.com>,"Tue, 23 Jun 2015 10:56:57 -0700",Re: [SparkSQL 1.4]Could not use concat with UDF in where clause,StanZhai <mail@zhaishidan.cn>,"Can you file a JIRA please?


==================
to
å¹´)
lved.scala:82)
nfun$apply$5$$anonfun$applyOrElse$15.apply(HiveTypeCoercion.scala:299)
nfun$apply$5$$anonfun$applyOrElse$15.apply(HiveTypeCoercion.scala:299)
:80)
nfun$apply$5.applyOrElse(HiveTypeCoercion.scala:299)
nfun$apply$5.applyOrElse(HiveTypeCoercion.scala:298)
ala:222)
ala:222)
ala:51)
:221)
ryPlan.scala:75)
scala:85)
)
)
eryPlan.scala:94)
lan.scala:64)
sions$1.applyOrElse(QueryPlan.scala:136)
sions$1.applyOrElse(QueryPlan.scala:135)
ala:222)
ala:222)
ala:51)
:221)
ala:242)
)
)
de.scala:272)
:227)
ala:242)
)
)
de.scala:272)
:227)
)
ryPlan.scala:135)
ly(HiveTypeCoercion.scala:298)
ly(HiveTypeCoercion.scala:297)
fun$apply$1.apply(RuleExecutor.scala:61)
fun$apply$1.apply(RuleExecutor.scala:59)
la:111)
(RuleExecutor.scala:59)
(RuleExecutor.scala:51)
la:51)
text.scala:922)
922)
scala:920)
============
´', '2014å¹´' )
´' limit 10
he
uld-not-use-concat-with-UDF-in-where-clause-tp12832.html
"
vladio <vladio@palantir.com>,"Tue, 23 Jun 2015 11:26:11 -0700 (MST)",[DataFrame] partitionBy issues,dev@spark.apache.org,"Hi,

I'm running into a strange memory scaling issue when using the partitionBy
feature of DataFrameWriter.

I've generated a table (a CSV file) with 3 columns (A, B and C) and 32*32
different entries, with size on disk of about 20kb. There are 32 distinct
values for column A and 32 distinct values for column B and all these are
combined together (column C will contain a random number for each row - it
doesn't matter) producing a 32*32 elements data set. I've imported this into
Spark and I ran a partitionBy(""A"", ""B"") in order to test its performance.
This should create a nested directory structure with 32 folders, each of
them containing another 32 folders. It uses about 10Gb of RAM and it's
running slow. If I increase the number of entries in the table from 32*32 to
128*128, I get Java Heap Space Out Of Memory no matter what value I use for
Heap Space variabile. Is this a known bug?

Scala code:
var df = sqlContext.read.format(""com.databricks.spark.csv"").option(""header"",
""true"").load(""table.csv"")
df.write.partitionBy(""A"", ""B"").mode(""overwrite"").parquet(""table.parquetâ€)

How I ran the Spark shell:
bin/spark-shell --driver-memory 16g --master local[8] --packages
com.databricks:spark-csv_2.10:1.0.3

Attached you'll find table.csv which I used.  table.csv
<http://apache-spark-developers-list.1001551.n3.nabble.com/file/n12838/table.csv>  

Thank you,
Vlad



--
3.nabble.com/DataFrame-partitionBy-issues-tp12838.html
om.

---------------------------------------------------------------------


"
Tathagata Das <tdas@databricks.com>,"Tue, 23 Jun 2015 13:23:31 -0700",Re: Calculating tuple count /input rate with time,anshu shukla <anshushukla0@gmail.com>,"This should give accurate count for each batch, though for getting the rate
you have to make sure that you streaming app is stable, that is, batches
are processed as fast as they are received (scheduling delay in the spark
streaming UI is approx 0).

TD


"
"""Vasili I. Galchin"" <vigalchin@gmail.com>","Tue, 23 Jun 2015 13:35:04 -0700","how can I write a language ""wrapper""?","""dev@spark.apache.org"" <dev@spark.apache.org>","Hello,

      I want to add language support for another language(other than Scala,
Java et. al.). Where is documentation that explains to provide support for
a new language?

Thank you,

Vasili
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Tue, 23 Jun 2015 13:46:41 -0700","Re: how can I write a language ""wrapper""?","""Vasili I. Galchin"" <vigalchin@gmail.com>","Every language has its own quirks / features -- so I don't think there
exists a document on how to go about doing this for a new language. The
most related write up I know of is the wiki page on PySpark internals
https://cwiki.apache.org/confluence/display/SPARK/PySpark+Internals written
by Josh Rosen -- It covers some of the issues like closure capture,
serialization, JVM communication that you'll need to handle for a new
language.

Thanks
Shivaram


"
Matei Zaharia <matei.zaharia@gmail.com>,"Tue, 23 Jun 2015 14:48:24 -0700","Re: how can I write a language ""wrapper""?",shivaram@eecs.berkeley.edu,"Just FYI, it would be easiest to follow SparkR's example and add the DataFrame API first. Other APIs will be designed to work on DataFrames (most notably machine learning pipelines), and the surface of this API is much smaller than of the RDD API. This API will also give you great performance as we continue to optimize Spark SQL.

Matei

exists a document on how to go about doing this for a new language. The most related write up I know of is the wiki page on PySpark internals https://cwiki.apache.org/confluence/display/SPARK/PySpark+Internals <https://cwiki.apache.org/confluence/display/SPARK/PySpark+Internals> written by Josh Rosen -- It covers some of the issues like closure capture, serialization, JVM communication that you'll need to handle for a new language. 
Scala, Java et. al.). Where is documentation that explains to provide support for a new language?

"
Justin Uang <justin.uang@gmail.com>,"Tue, 23 Jun 2015 22:27:11 +0000",Python UDF performance at large scale,"""dev@spark.apache.org"" <dev@spark.apache.org>","BLUF: BatchPythonEvaluation's implementation is unusable at large scale,
but I have a proof-of-concept implementation that avoids caching the entire
dataset.

Hi,

We have been running into performance problems using Python UDFs with
DataFrames at large scale.

was to reuse the PythonRDD code. It caches the entire child RDD so that it
join the python lambda results with the original row (which may have java
objects that should be passed through).

In addition, it caches all the columns, even the ones that don't need to be
processed by the Python UDF. In the cases I was working with, I had a 500
column table, and i wanted to use a python UDF for one column, and it ended
up caching all 500 columns.

I have a working solution over here that does it in one pass over the data,
avoiding caching (
https://github.com/justinuang/spark/commit/c1a415a18d31226ac580f1a9df7985571d03199b).
With this patch, I go from a job that takes 20 minutes then OOMs, to a job
that finishes completely in 3 minutes. It is indeed quite hacky and prone
to deadlocks since there is buffering in many locations:

    - NEW: the ForkingIterator LinkedBlockingDeque
    - batching the rows before pickling them
    - os buffers on both sides
    - pyspark.serializers.BatchedSerializer

We can avoid deadlock by being very disciplined. For example, we can have
the ForkingIterator instead always do a check of whether the
LinkedBlockingDeque is full and if so:

Java
    - flush the java pickling buffer
    - send a flush command to the python process
    - os.flush the java side

Python
    - flush BatchedSerializer
    - os.flush()

I haven't added this yet. This is getting very complex however. Another
model would just be to change the protocol between the java side and the
worker to be a synchronous request/response. This has the disadvantage that
the CPU isn't doing anything when the batch is being sent across, but it
has the huge advantage of simplicity. In addition, I imagine that the
actual IO between the processes isn't that slow, but rather the
serialization of java objects into pickled bytes, and the
deserialization/serialization + python loops on the python side. Another
advantage is that we won't be taking more than 100% CPU since only one
thread is doing CPU work at a time between the executor and the python
interpreter.

Any thoughts would be much appreciated =)

Other improvements:
    - extract some code of the worker out of PythonRDD so that we can do a
mapPartitions directly in BatchedPythonEvaluation without resorting to the
hackery in ForkedRDD.compute(), which uses a cache to ensure that the other
RDD can get a handle to the same iterator.
    - read elements and use a size estimator to create the BlockingQueue to
make sure that we don't store too many things in memory when batching
    - patch Unpickler to not use StopException for control flow, which is
slowing down the java side
"
Davies Liu <davies@databricks.com>,"Tue, 23 Jun 2015 16:21:08 -0700",Re: Python UDF performance at large scale,Justin Uang <justin.uang@gmail.com>,"Thanks for looking into it, I'd like the idea of having
ForkingIterator. If we have unlimited buffer in it, then will not have
the problem of deadlock, I think. The writing thread will be blocked
by Python process, so there will be not much rows be buffered(still be
a reason to OOM). At least, this approach is better than current one.

Could you create a JIRA and sending out the PR?


---------------------------------------------------------------------


"
Justin Uang <justin.uang@gmail.com>,"Wed, 24 Jun 2015 03:21:15 +0000",Re: Python UDF performance at large scale,Davies Liu <davies@databricks.com>,"// + punya

Thanks for your quick response!

I'm not sure that using an unbounded buffer is a good solution to the
locking problem. For example, in the situation where I had 500 columns, I
am in fact storing 499 extra columns on the java side, which might make me
OOM if I have to store many rows. In addition, if I am using an
AutoBatchedSerializer, the java side might have to write 1 << 16 == 65536
rows before python starts outputting elements, in which case, the Java side
has to buffer 65536 complete rows. In general it seems fragile to rely on
blocking behavior in the Python coprocess. By contrast, it's very easy to
verify the correctness and performance characteristics of the synchronous
blocking solution.



"
Patrick Wendell <pwendell@gmail.com>,"Tue, 23 Jun 2015 22:37:29 -0700",[VOTE] Release Apache Spark 1.4.1,"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version 1.4.1!

This release fixes a handful of known issues in Spark 1.4.0, listed here:
http://s.apache.org/spark-1.4.1

The tag to be voted on is v1.4.1-rc1 (commit 60e08e5):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=
60e08e50751fe3929156de956d62faea79f5b801

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.4.1-rc1-bin/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
[published as version: 1.4.1]
https://repository.apache.org/content/repositories/orgapachespark-1118/
[published as version: 1.4.1-rc1]
https://repository.apache.org/content/repositories/orgapachespark-1119/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.4.1-rc1-docs/

Please vote on releasing this package as Apache Spark 1.4.1!

The vote is open until Saturday, June 27, at 06:32 UTC and passes
if a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.4.1
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

---------------------------------------------------------------------


"
Davies Liu <davies@databricks.com>,"Tue, 23 Jun 2015 23:26:51 -0700",Re: Python UDF performance at large scale,Justin Uang <justin.uang@gmail.com>,"Fare points, I also like simpler solutions.

The overhead of Python task could be a few of milliseconds, which
means we also should eval them as batches (one Python task per batch).

Decreasing the batch size for UDF sounds reasonable to me, together
with other tricks to reduce the data in socket/pipe buffer.

BTW, what do your UDF looks like? How about to use Jython to run
simple Python UDF (without some external libraries).


---------------------------------------------------------------------


"
StanZhai <mail@zhaishidan.cn>,"Wed, 24 Jun 2015 00:31:22 -0700 (MST)",Re: [SparkSQL 1.4]Could not use concat with UDF in where clause,dev@spark.apache.org,"Hi Michael Armbrust,

I have filed an issue on JIRA for this, 
https://issues.apache.org/jira/browse/SPARK-8588
<https://issues.apache.org/jira/browse/SPARK-8588>  



--

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Wed, 24 Jun 2015 11:54:48 +0300",Re: [VOTE] Release Apache Spark 1.4.1,Patrick Wendell <pwendell@gmail.com>,"There are 44 issues still targeted for 1.4.1. None are Blockers; 12
are Critical. ~80% were opened and/or set by committers. Compare with
90 issues resolved for 1.4.1.

I'm concerned that committers are targeting lots more for a release
suggests that an RC is premature. Why is 1.4.1 being put forth for
release now? It seems like people are saying they want a fair bit more
time to work on 1.4.1.

I suspect that in fact people would rather untarget / slip (again)
these JIRAs, but it calls into question again how the targeting is
consistently off by this much.

What unresolved JIRAs targeted for 1.4.1 are *really* still open for
1.4.1? like, what would go badly if all 32 non-Critical JIRAs were
untargeted now? is the reality that there are a handful of items to
get in before the final release, and those are hopefully the ~12
critical ones? How about some review of that before we ask people to
seriously test these bits?


---------------------------------------------------------------------


"
Punyashloka Biswal <punya.biswal@gmail.com>,"Wed, 24 Jun 2015 12:33:03 +0000",Re: Python UDF performance at large scale,"Davies Liu <davies@databricks.com>, Justin Uang <justin.uang@gmail.com>","Hi Davies,

In general, do we expect people to use CPython only for ""heavyweight"" UDFs
that invoke an external library? Are there any examples of using Jython,
especially performance comparisons to Java/Scala and CPython? When using
Jython, do you expect the driver to send code to the executor as a string,
or is there a good way to serialized Jython lambdas?

(For context, I was unable to serialize Nashorn lambdas when I tried to use
them in Spark.)

Punya

"
anshu shukla <anshushukla0@gmail.com>,"Wed, 24 Jun 2015 18:48:35 +0530",Loss of data due to congestion,"user <user@spark.apache.org>, dev@spark.apache.org","How spark guarantees  that no RDD will fail /lost during its life cycle .
Is there  something like ask in storm or its does it by default .

-- 
Thanks & Regards,
Anshu Shukla
"
"""Carr, J. Ryan"" <Ryan.Carr@jhuapl.edu>","Wed, 24 Jun 2015 14:55:04 +0000",[GraphX] Graph 500 graph generator,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Spark Devs,

  As part of a project at work, I have written a graph generator for RMAT graphs consistent with the specifications in the Graph 500 benchmark (http://www.graph500.org/specifications). We had originally planned to use the rmatGenerator function in GraphGenerators, but found that it wasn't suitable for generating graphs with billions of edges; the edges are generated in a single thread and stored in a Set, meaning it can't generate a graph larger than memory on a single JVM (and I think Sets are limited to Int.MaxValue elements anyway).

  The generator I have is essentially a more scalable version of rmatGenerator. We have used it to generate a graph with 2^32 vertices and 2^36 edges on our modestly-specced cluster of 16 machines. It seems like other people interested in Spark might want to play with some large RMAT graphs (or run the Graph 500 benchmark), so I would like to contribute my generator. It does have some minor differences from the current generator, though:

  1.  Vertex IDs are shuffled after the graph structure is generated, so the degree of a vertex cannot be predicted from its ID (without this step vertex 0 would always have the largest degree, followed by vertices 1,2,4,8, etc.). This is per the Graph 500 spec. It could be easily made optional.
  2.  Duplicate edges are not removed from the resulting graph. This could easily be done with a call to distinct() on the resulting edge list, but then there would be slightly fewer edges than one generated by the current rmatGenerator. Also this process would be very slow on large graphs due to skew.
  3.  Doesn't set the out degree as the vertex attribute. Again this would be simple to add, but it could be slow on the super vertices.

  My question for the Spark Devs is: Is this something you would want as part of GraphX (either as a replacement for the current rmatGenerator or a separate function in GraphGenerators)? Since it was developed at work I need to go through our legal department and QA processes to open-source it, and to fill out the paperwork I need to know whether I'll be submitting a pull request or standing it up as a separate project on GitHub.

Thanks!

-Ryan

--
J. Ryan Carr, Ph. D.

The Johns Hopkins University, Applied Physics Laboratory
11100 Johns Hopkins Rd., Laurel, MD 20723
Office: 240-228-9157
Cell: 443-744-1004
Email: Ryan.Carr@jhuapl.edu<mailto:Ryan.Carr@jhuapl.edu> or James.Carr@jhuapl.edu<mailto:James.Carr@jhuapl.edu>

"
Debasish Das <debasish.das83@gmail.com>,"Wed, 24 Jun 2015 08:18:39 -0700",Spark SQL 1.3 Exception,dev <dev@spark.apache.org>,"Hi,

I have Impala created table with the following io format and serde:

inputFormat:parquet.hive.DeprecatedParquetInputFormat,
outputFormat:parquet.hive.DeprecatedParquetOutputFormat,
serdeInfo:SerDeInfo(name:null,
serializationLib:parquet.hive.serde.ParquetHiveSerDe, parameters:{})
I am trying to read this table on Spark SQL 1.3 and see if caching improves
my query latency but I am getting exception:

java.lang.ClassNotFoundException: Class parquet.hive.serde.ParquetHiveSerDe
not found
I understand that in hive 0.13 (which I am using)
parquet.hive.serde.ParquetHiveSerDe is deprecated but it seems Impala still
used it to write the table.

I also tried to provide the bundle jar with --jars option to Spark 1.3
Shell / SQL which has org.apache.parquet.hive.serde.ParquetHiveSerDe but I
am confused how to configure to serde in SQLContext ?

The table which has the following io format and serde can be read fine by
Spark SQL 1.3:

inputFormat=org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat,
outputFormat=org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat,
serializationLib=org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe

Thanks.
Deb


"
Burak Yavuz <brkyvz@gmail.com>,"Wed, 24 Jun 2015 08:35:19 -0700",Re: [GraphX] Graph 500 graph generator,"""Carr, J. Ryan"" <Ryan.Carr@jhuapl.edu>","Hi Ryan,
If you can get past the paperwork, I'm sure this can make a great Spark
Package (http://spark-packages.org). People then can use it for
benchmarking purposes, and I'm sure people will be looking for graph
generators!

Best,
Burak

:

 (
€™t
t generate
to Int.MaxValue
tep
e
ist,
raphs
his
a
t,
itting a
l.edu
"
"""Vasili I. Galchin"" <vigalchin@gmail.com>","Wed, 24 Jun 2015 08:45:00 -0700","Re: how can I write a language ""wrapper""?",Matei Zaharia <matei.zaharia@gmail.com>,"Matei,

     Last night I downloaded the Spark bundle.
In order to save me time, can you give me the name of the SparkR example is
and where it is in the Sparc tree?

Thanks,

Bill


"
Patrick Wendell <pwendell@gmail.com>,"Wed, 24 Jun 2015 08:56:04 -0700",Re: [VOTE] Release Apache Spark 1.4.1,Sean Owen <sowen@cloudera.com>,"Hey Sean,

This is being shipped now because there is a severe bug in 1.4.0 that
can cause data corruption for Parquet users.

There are no blockers targeted for 1.4.1 - so I don't see that JIRA is
inconsistent with shipping a release now. The goal of having every
single targeted JIRA cleared by the time we start voting, I don't
think there is broad consensus and cultural adoption of that principle
yet. So I do not take it as a signal this release is premature (the
story has been the same for every previous release we've ever done).

The fact that we hit 90/124 of issues targeted at this release means
we are targeting such that we get around 70% of issues merged. That
actually doesn't seem so bad to me since there is some uncertainty in
the process. B

- Patrick


---------------------------------------------------------------------


"
Imran Rashid <irashid@cloudera.com>,"Wed, 24 Jun 2015 10:56:15 -0500",Re: OK to add committers active on JIRA to JIRA admin role?,Sean Owen <sowen@cloudera.com>,"+1

(partially b/c I would like jira admin myself)


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Wed, 24 Jun 2015 09:01:56 -0700","Re: how can I write a language ""wrapper""?","""Vasili I. Galchin"" <vigalchin@gmail.com>","The SparkR code is in the `R` directory i.e.
https://github.com/apache/spark/tree/master/R

Shivaram


"
Josh Rosen <rosenville@gmail.com>,"Wed, 24 Jun 2015 09:49:30 -0700",Re: [VOTE] Release Apache Spark 1.4.1,Patrick Wendell <pwendell@gmail.com>,"At least a couple of those issues are mistargeted; some of the flaky test
JIRAs + test improvement tasks should probably be targeted for 1.5.0
instead.


"
Akhil Das <akhil@sigmoidanalytics.com>,"Wed, 24 Jun 2015 23:37:21 +0530",Re: IPv6 support,"Kevin Liu <kevinliu@fb.com>, dev <dev@spark.apache.org>","+Dev list

Thanks
Best Regards


h
s
da-b96a-7120f118d002
8df2-89a528b35c9e
)
7)
8)
)
:62)
mpl.java:43)
)
)
7)
02)
rkILoopInit.scala:123)
rkILoopInit.scala:122)
.scala:122)
p$$process$1$$anonfun$apply$mcZ$sp$5.apply$mcV$sp(SparkILoop.scala:974)
:157)
nit."
jimfcarroll <jimfcarroll@gmail.com>,"Wed, 24 Jun 2015 11:21:34 -0700 (MST)",Problem with version compatibility,dev@spark.apache.org,"
Hello all,

I have a strange problem. I have a mesos spark cluster with Spark
1.4.0/Hadoop 2.4.0 installed and a client application use maven to include
the same versions.

However, I'm getting a serialUIDVersion problem on:

ERROR Remoting -
org.apache.spark.storage.BlockManagerMessages$RegisterBlockManager; local
class incompatible: stream classdesc serialVersionUID = 3833981923223309323,
local class serialVersionUID = -1833407448843930116

When I look in the jar file of the spark dependency in my maven repo I see:

spark-core_2.10-1.4.0.jar contains the line:
2917  10-Jun-2015  12:20:48 
org/apache/spark/storage/BlockManagerMessages$RegisterBlockManager$.class

However, on my mesos cluster the jar looks like this:

spark-assembly-1.4.0-hadoop2.4.0.jar contains the line:
3786   2-Jun-2015  18:23:00 
org/apache/spark/storage/BlockManagerMessages$RegisterBlockManager.class

Notice the classes aren't the same (different sizes), but I'm getting them
from the download page which points to here:
http://d3kbcqa49mib13.cloudfront.net/spark-1.4.0-bin-hadoop2.4.tgz

Thanks
Jim




--

---------------------------------------------------------------------


"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Wed, 24 Jun 2015 18:35:57 +0000",Force inner join to shuffle the smallest table,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

le is 2B rows, the second is 500K. They are stored in HDFS in Parquet. Spark v 1.4.
val big = sqlContext.paquetFile(""hdfs://big"")
data.registerTempTable(""big"")
val small = sqlContext.paquetFile(""hdfs://small"")
data.registerTempTable(""small"")
val result = sqlContext.sql(""select big.f1, big.f2 from big inner join small on big.s=small.s and big.d=small.d"")

This query fails in the middle due to one of the workers ""disk out of space"" with shuffle reported 1.8TB which is the maximum size of my spark working dirs (on total 7 worker nodes). This is surprising, because the ""big"" table takes 2TB disk space (unreplicated) and ""small"" about 5GB and I would expect that optimizer will shuffle the small table. How to force Spark to shuffle the small table? I tried to write ""small inner join big"" however it also fails with 1.8TB of shuffle.

Best regards, Alexander

"
jimfcarroll <jimfcarroll@gmail.com>,"Wed, 24 Jun 2015 11:47:47 -0700 (MST)",Re: Problem with version compatibility,dev@spark.apache.org,"These jars are simply incompatible. You can see this by looking at that class
in both the maven repo for 1.4.0 here:

http://central.maven.org/maven2/org/apache/spark/spark-core_2.10/1.4.0/spark-core_2.10-1.4.0.jar

as well as the spark-assembly jar inside the .tgz file you can get from the
official download here:

http://d3kbcqa49mib13.cloudfront.net/spark-1.4.0-bin-hadoop2.4.tgz

Am I missing something?

Thanks
Jim




--

---------------------------------------------------------------------


"
CC GP <chandrika.gopalakrishna@gmail.com>,"Wed, 24 Jun 2015 12:07:53 -0700",Re: Force inner join to shuffle the smallest table,"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Try below and see if it makes a difference:

val result = sqlContext.sql(â€œselect big.f1, big.f2 from small inner join
big on big.s=small.s and big.d=small.dâ€)

m

er join
t of
y spark
€œsmallâ€ about 5GB and I
oin bigâ€
"
Justin Uang <justin.uang@gmail.com>,"Wed, 24 Jun 2015 19:11:08 +0000",Re: Python UDF performance at large scale,"Punyashloka Biswal <punya.biswal@gmail.com>, Davies Liu <davies@databricks.com>","FYI, just submitted a PR to Pyrolite to remove their StopException.
https://github.com/irmen/Pyrolite/pull/30

With my benchmark, removing it basically made it about 2x faster.


"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Wed, 24 Jun 2015 19:27:15 +0000",RE: Force inner join to shuffle the smallest table,CC GP <chandrika.gopalakrishna@gmail.com>,"It also fails, as I mentioned in the original question.

From: CC GP [mailto:chandrika.gopalakrishna@gmail.com]
Sent: Wednesday, June 24, 2015 12:08 PM
To: Ulanov, Alexander
Cc: dev@spark.apache.org
Subject: Re: Force inner join to shuffle the smallest table

Try below and see if it makes a difference:

val result = sqlContext.sql(â€œselect big.f1, big.f2 from small inner join big on big.s=small.s and big.d=small.dâ€)

On Wed, Jun 24, 2015 at 11:35 AM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
Hi,

I try to inner join of two tables on two fields(string and double). One table is 2B rows, the second is 500K. They are stored in HDFS in Parquet. Spark v 1.4.
val big = sqlContext.paquetFile(â€œhdfs://bigâ€)
data.registerTempTable(â€œbigâ€)
val small = sqlContext.paquetFile(â€œhdfs://smallâ€)
data.registerTempTable(â€œsmallâ€)
val result = sqlContext.sql(â€œselect big.f1, big.f2 from big inner join small on big.s=small.s and big.d=small.dâ€)

This query fails in the middle due to one of the workers â€œdisk out of spaceâ€ with shuffle reported 1.8TB which is the maximum size of my spark working dirs (on total 7 worker nodes). This is surprising, because the â€œbigâ€ table takes 2TB disk space (unreplicated) and â€œsmallâ€ about 5GB and I would expect that optimizer will shuffle the small table. How to force Spark to shuffle the small table? I tried to write â€œsmall inner join bigâ€ however it also fails with 1.8TB of shuffle.

Best regards, Alexander


"
Stephen Carman <scarman@coldlight.com>,"Wed, 24 Jun 2015 19:32:58 +0000",Re: Force inner join to shuffle the smallest table,"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Have you tried shuffle compression?

spark.shuffle.compress (true|false)

if you have a filesystem capable also Iâ€™ve noticed file consolidation helps disk usage a bit.

spark.shuffle.consolidateFiles (true|false)

Steve

On Jun 24, 2015, at 3:27 PM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:

It also fails, as I mentioned in the original question.

From: CC GP [mailto:chandrika.gopalakrishna@gmail.com]
Sent: Wednesday, June 24, 2015 12:08 PM
To: Ulanov, Alexander
Cc: dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: Force inner join to shuffle the smallest table

Try below and see if it makes a difference:

val result = sqlContext.sql(â€œselect big.f1, big.f2 from small inner join big on big.s=small.s and big.d=small.dâ€)

On Wed, Jun 24, 2015 at 11:35 AM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
Hi,

I try to inner join of two tables on two fields(string and double). One table is 2B rows, the second is 500K. They are stored in HDFS in Parquet. Spark v 1.4.
val big = sqlContext.paquetFile(â€œhdfs://bigâ€)
data.registerTempTable(â€œbigâ€)
val small = sqlContext.paquetFile(â€œhdfs://smallâ€)
data.registerTempTable(â€œsmallâ€)
val result = sqlContext.sql(â€œselect big.f1, big.f2 from big inner join small on big.s=small.s and big.d=small.dâ€)

This query fails in the middle due to one of the workers â€œdisk out of spaceâ€ with shuffle reported 1.8TB which is the maximum size of my spark working dirs (on total 7 worker nodes). This is surprising, because the â€œbigâ€ table takes 2TB disk space (unreplicated) and â€œsmallâ€ about 5GB and I would expect that optimizer will shuffle the small table. How to force Spark to shuffle the small table? I tried to write â€œsmall inner join bigâ€ however it also fails with 1.8TB of shuffle.

Best regards, Alexander

This e-mail is intended solely for the above-mentioned recipient and it may contain confidential or privileged information. If you have received it in error, please notify us immediately and delete the e-mail. You must not copy, distribute, disclose or take any action in reliance on it. In addition, the contents of an attachment to this e-mail may contain software viruses which could damage your own computer system. While ColdLight Solutions, LLC has taken every reasonable precaution to minimize this risk, we cannot accept liability for any damage which you sustain as a result of software viruses. You should perform your own virus checks before opening the attachment.
"
Davies Liu <davies@databricks.com>,"Wed, 24 Jun 2015 16:27:32 -0700",Re: Python UDF performance at large scale,Justin Uang <justin.uang@gmail.com>,"batch size as 1, right?


---------------------------------------------------------------------


"
Justin Uang <justin.uang@gmail.com>,"Wed, 24 Jun 2015 23:39:50 +0000",Re: Python UDF performance at large scale,Davies Liu <davies@databricks.com>,"Correct, I was running with a batch size of about 100 when I did the tests,
because I was worried about deadlocks. Do you have any concerns regarding
the batched synchronous version of communication between the Java and
Python processes, and if not, should I file a ticket and starting writing
it?

"
xing <ehomecity@gmail.com>,"Wed, 24 Jun 2015 17:59:12 -0700 (MST)",parallelize method v.s. textFile method,dev@spark.apache.org,"We have a large file and we used to read chunks and then use parallelize
method (distData = sc.parallelize(chunk)) and then do the map/reduce chunk
by chunk. Recently we read the whole file using textFile method and found
the map/reduce job is much faster. Anybody can help us to understand why? We
have verified that reading file is NOT a bottleneck.



--

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Wed, 24 Jun 2015 18:04:07 -0700",Re: parallelize method v.s. textFile method,xing <ehomecity@gmail.com>,"If you read the file one by one and then use parallelize, it is read by a
single thread on a single machine.


"
xing <ehomecity@gmail.com>,"Wed, 24 Jun 2015 18:06:29 -0700 (MST)",Re: parallelize method v.s. textFile method,dev@spark.apache.org,"When we compare the performance, we already excluded this part of time
difference.



--

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Thu, 25 Jun 2015 01:17:02 +0000",Re: Problem with version compatibility,"jimfcarroll <jimfcarroll@gmail.com>, dev@spark.apache.org","They are different classes even. Your problem isn't class-not-found though.
You're also comparing different builds really. You should not be including
Spark code in your app.


"
Reynold Xin <rxin@databricks.com>,"Wed, 24 Jun 2015 18:25:51 -0700",Re: parallelize method v.s. textFile method,xing <ehomecity@gmail.com>,"How did you exclude it?

I am not sure if it is possible since each task needs to contain the
chunk of data.




---------------------------------------------------------------------


"
jimfcarroll <jimfcarroll@gmail.com>,"Wed, 24 Jun 2015 19:12:14 -0700 (MST)",Re: Problem with version compatibility,dev@spark.apache.org,"Hi Sean,

I'm running a Mesos cluster. My driver app is built using maven against the
maven 1.4.0 dependency.

The Mesos slave machines have the spark distribution installed from the
distribution link.

I have a hard time understanding how this isn't a standard app deployment
but maybe I'm missing something. 

If you build a driver app against 1.4.0 using maven and run it against a
mesos cluster that has the 1.4.0 binary distribution installed, your driver
wont run right.

I meant to publish this question on the user list so my apologies if it's in
the wrong place.

Jim




--

---------------------------------------------------------------------


"
anshu shukla <anshushukla0@gmail.com>,"Thu, 25 Jun 2015 11:03:53 +0530",Re: Loss of data due to congestion,ayan guha <guha.ayan@gmail.com>,"Thaks,
I  am talking about streaming.

"
Niranda Perera <niranda.perera@gmail.com>,"Thu, 25 Jun 2015 11:27:25 +0530","Error in invoking a custom StandaloneRecoveryModeFactory in java env
 (Spark v1.3.0)","""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,

I'm trying to implement a custom StandaloneRecoveryModeFactory in the Java
environment. Pls find the implementation here. [1] . I'm new to Scala,
hence I'm trying to use Java environment as much as possible.

when I start a master with spark.deploy.recoveryMode.factory property to be
""CUSTOM"", I encounter a NoSuchMethodException for my custom class's
constructor.
it has the following constructor.

 public AnalyticsStandaloneRecoveryModeFactory(SparkConf conf,
Serialization serializer)

but from the Master, it looks for a constructor for,
org.wso2.carbon.analytics.spark.core.util.master.AnalyticsStandaloneRecoveryModeFactory.<init>(org.apache.spark.SparkConf,
akka.serialization.Serialization$)

I see in the Spark source code for Master, that it uses reflection to get
the custom recovery mode factory class.

case ""CUSTOM"" =>
        val clazz =
Class.forName(conf.get(""spark.deploy.recoveryMode.factory""))
        val factory = clazz.getConstructor(conf.getClass,
Serialization.getClass)
          .newInstance(conf, SerializationExtension(context.system))
          .asInstanceOf[StandaloneRecoveryModeFactory]
        (factory.createPersistenceEngine(),
factory.createLeaderElectionAgent(this))

here, Serialization.getClass returns a akka.serialization.Serialization$
object, where as my custom class's constructor
accepts akka.serialization.Serialization object.

so I would like to know,
1. if this happens because I'm using this in the Java environment?
2. what is the workaround to this?

thanks

Please find the full stack trace of the error below.

 org.wso2.carbon.analytics.spark.core.util.master.AnalyticsStandaloneRecoveryModeFactory.<init>(org.apache.spark.SparkConf,
akka.serialization.Serialization$)
akka.actor.ActorInitializationException: exception during creation
at akka.actor.ActorInitializationException$.apply(Actor.scala:164)
at akka.actor.ActorCell.create(ActorCell.scala:596)
at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:456)
at akka.actor.ActorCell.systemInvoke(ActorCell.scala:478)
at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:263)
at akka.dispatch.Mailbox.run(Mailbox.scala:219)
at
akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)
at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
at
scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
at
scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: java.lang.NoSuchMethodException:
org.wso2.carbon.analytics.spark.core.util.master.AnalyticsStandaloneRecoveryModeFactory.<init>(org.apache.spark.SparkConf,
akka.serialization.Serialization$)
at java.lang.Class.getConstructor0(Class.java:2810)
at java.lang.Class.getConstructor(Class.java:1718)
at org.apache.spark.deploy.master.Master.preStart(Master.scala:165)
at akka.actor.Actor$class.aroundPreStart(Actor.scala:470)
at org.apache.spark.deploy.master.Master.aroundPreStart(Master.scala:52)
at akka.actor.ActorCell.create(ActorCell.scala:580)
... 9 more



[1]
https://github.com/nirandaperera/carbon-analytics/blob/spark_master_persistance/components/analytics-processors/org.wso2.carbon.analytics.spark.core/src/main/java/org/wso2/carbon/analytics/spark/core/util/master/AnalyticsStandaloneRecoveryModeFactory.java
<https://github.com/nirandaperera/carbon-analytics/blob/spark_master_persistance/components/analytics-processors/org.wso2.carbon.analytics.spark.core/src/main/java/org/wso2/carbon/analytics/spark/core/util/master/AnalyticsStandaloneRecoveryModeFactory.java>

-- 
Niranda
@n1r44 <https://twitter.com/N1R44>
https://pythagoreanscript.wordpress.com/
"
Josh Rosen <rosenville@gmail.com>,"Wed, 24 Jun 2015 23:02:26 -0700","Re: Error in invoking a custom StandaloneRecoveryModeFactory in java
 env (Spark v1.3.0)",Niranda Perera <niranda.perera@gmail.com>,"This sounds like https://issues.apache.org/jira/browse/SPARK-7436, which
has been fixed in Spark 1.4+ and in branch-1.3 (for Spark 1.3.2).


a
eryModeFactory.<init>(org.apache.spark.SparkConf,
veryModeFactory.<init>(org.apache.spark.SparkConf,
Dispatcher.scala:393)
a:1339)
9)
ava:107)
eryModeFactory.<init>(org.apache.spark.SparkConf,
stance/components/analytics-processors/org.wso2.carbon.analytics.spark.core/src/main/java/org/wso2/carbon/analytics/spark/core/util/master/AnalyticsStandaloneRecoveryModeFactory.java
istance/components/analytics-processors/org.wso2.carbon.analytics.spark.core/src/main/java/org/wso2/carbon/analytics/spark/core/util/master/AnalyticsStandaloneRecoveryModeFactory.java>
"
Niranda Perera <niranda.perera@gmail.com>,"Thu, 25 Jun 2015 11:42:51 +0530","Re: Error in invoking a custom StandaloneRecoveryModeFactory in java
 env (Spark v1.3.0)",Josh Rosen <rosenville@gmail.com>,"thanks Josh.

this looks very similar to my problem.


m
la,
veryModeFactory.<init>(org.apache.spark.SparkConf,
t
overyModeFactory.<init>(org.apache.spark.SparkConf,
tDispatcher.scala:393)
va:1339)
java:107)
veryModeFactory.<init>(org.apache.spark.SparkConf,
istance/components/analytics-processors/org.wso2.carbon.analytics.spark.core/src/main/java/org/wso2/carbon/analytics/spark/core/util/master/AnalyticsStandaloneRecoveryModeFactory.java
sistance/components/analytics-processors/org.wso2.carbon.analytics.spark.core/src/main/java/org/wso2/carbon/analytics/spark/core/util/master/AnalyticsStandaloneRecoveryModeFactory.java>


-- 
Niranda
@n1r44 <https://twitter.com/N1R44>
https://pythagoreanscript.wordpress.com/
"
=?UTF-8?B?6K+66ZOB?= <notyycn@gmail.com>,"Thu, 25 Jun 2015 14:25:36 +0800",how to implement my own datasource?,dev@spark.apache.org,"hi,

I can't find documentation about datasource api,  how to implement custom
datasource.  any hint is appreciated.    thanks.
"
Davies Liu <davies@databricks.com>,"Thu, 25 Jun 2015 00:05:46 -0700",Re: Python UDF performance at large scale,Justin Uang <justin.uang@gmail.com>,"I'm thinking that the batched synchronous version will be too slow
(with small batch size) or easy to OOM with large (batch size). If
it's not that hard, you can give it a try.


---------------------------------------------------------------------


"
Akhil Das <akhil@sigmoidanalytics.com>,"Thu, 25 Jun 2015 12:55:34 +0530",Re: IPv6 support,"Kevin Liu <kevinliu@fb.com>, dev <dev@spark.apache.org>","Its the BlockManager hostname
<https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/storage/BlockManager.scala#L190>
messing up this time, are you having spark.driver.host available in the
conf/spark-defaults.conf file?

Thanks
Best Regards


ch
is
did
s
s
s
8da-b96a-7120f118d002
-8df2-89a528b35c9e
.
5
8)
7)
a:62)
Impl.java:43)
5)
8)
)
902)
arkILoopInit.scala:123)
arkILoopInit.scala:122)
t.scala:122)
op$$process$1$$anonfun$apply$mcZ$sp$5.apply$mcV$sp(SparkILoop.scala:974)
a:157)
Init.scala:106)
op$$process$1.apply$mcZ$sp(SparkILoop.scala:991)
op$$process$1.apply(SparkILoop.scala:945)
op$$process$1.apply(SparkILoop.scala:945)
der.scala:135)
a:62)
Impl.java:43)
$$runMain(SparkSubmit.scala:664)
9)
.
7d00bf/core/src/main/scala/org/apache/spark/util/RpcUtils.scala#L33> is
/apache/spark/util/Utils.scala#L882>.
/jira/browse/SPARK-6440&k=ZVNjlDMF0FElm4dQtryO4A%3D%3D%0A&r=YviX1%2F1vaAZK%2BrqaSzu%2FMg%3D%3D%0A&m=06CDmFdO8vUzzjbqnMBz2CS55qwEN6lPy%2FypbKwNdow%3D%0A&s=464bc64957c6ddd8bb506c8ec449f3c26f8c6f4482dc6ea7356df4e0cb5c98e9>
hts?
e
rs
]
'
4b11-8db5-a6eaa5f0222b
5-accb-1bb66fddf705
.0.jar
-hadoop2.6.0.jar
a:53)
103)
ala:141)
va:62)
rImpl.java:43)
t$$runMain(SparkSubmit.scala:664)
07)
la:416)
)
va:62)
rImpl.java:43)
t$$runMain(SparkSubmit.scala:664)
a:53)
103)
ala:141)
va:62)
rImpl.java:43)
t$$runMain(SparkSubmit.scala:664)
4b11-8db5-a6eaa5f0222b,
ne
IPv6
t work. A
elp
hadoo
9b:face:0:9:0:51453/jars/spark-examples-1.3.1-hadoo&k=ZVNjlDMF0FElm4dQtryO4A%3D%3D%0A&r=YviX1%2F1vaAZK%2BrqaSzu%2FMg%3D%3D%0A&m=06CDmFdO8vUzzjbqnMBz2CS55qwEN6lPy%2FypbKwNdow%3D%0A&s=36fe35441ecafdbc99ac8e5605e91d0a4ea88855d27d1a964e55e04ae65c7fde>
hadoo
9b:face:0:9:0:51453/jars/spark-examples-1.3.1-hadoo&k=ZVNjlDMF0FElm4dQtryO4A%3D%3D%0A&r=YviX1%2F1vaAZK%2BrqaSzu%2FMg%3D%3D%0A&m=06CDmFdO8vUzzjbqnMBz2CS55qwEN6lPy%2FypbKwNdow%3D%0A&s=36fe35441ecafdbc99ac8e5605e91d0a4ea88855d27d1a964e55e04ae65c7fde>
Execu
Execu
raver
a:98)
a:98)
226)
ala:7
executor.Executor.org&k=ZVNjlDMF0FElm4dQtryO4A%3D%3D%0A&r=YviX1%2F1vaAZK%2BrqaSzu%2FMg%3D%3D%0A&m=cToWyWSkkQGPchtZEHgZAymvCC%2FYOX8btPSeh%2Bth5wM%3D%0A&s=69d2377deecbf9077810fa58426e84fc72e54932d7bf06064e130e9a3ac6af04>
ava:1
java:
ava:6
6)
"
=?UTF-8?B?SnVhbiBSb2Ryw61ndWV6IEhvcnRhbMOh?= <juan.rodriguez.hortala@gmail.com>,"Thu, 25 Jun 2015 10:27:02 +0200",Re: how to implement my own datasource?,=?UTF-8?B?6K+66ZOB?= <notyycn@gmail.com>,"Hi,

You can connect to by JDBC as described in
https://spark.apache.org/docs/latest/sql-programming-guide.html#jdbc-to-other-databases.
Other option is using HadoopRDD and NewHadoopRDD to connect to databases
compatible with Hadoop, like HBase, some examples can be found at chapter 5
of ""Learning Spark""
https://books.google.es/books?id=tOptBgAAQBAJ&pg=PT190&dq=learning+spark+hadooprdd&hl=en&sa=X&ei=4bqLVaDaLsXaU46NgfgL&ved=0CCoQ6AEwAA#v=onepage&q=%20hadooprdd&f=false
For Spark Streaming see the section ""Custom Sources"" of
https://spark.apache.org/docs/latest/streaming-programming-guide.html

Hope that helps.

Greetings,

Juan

2015-06-25 8:25 GMT+02:00 è¯ºé“ <notyycn@gmail.com>:

"
Sean Owen <sowen@cloudera.com>,"Thu, 25 Jun 2015 12:17:16 +0300",Re: Problem with version compatibility,jimfcarroll <jimfcarroll@gmail.com>,"-dev +user

That all sounds fine except are you packaging Spark classes with your
app? that's the bit I'm wondering about. You would mark it as a
'provided' dependency in Maven.


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Thu, 25 Jun 2015 12:38:36 +0300",Re: [VOTE] Release Apache Spark 1.4.1,Patrick Wendell <pwendell@gmail.com>,"That makes sense to me -- there's an urgent fix to get out. I missed
that part. Not that it really matters but was that expressed
elsewhere?

I know we tend to start the RC process even when a few more changes
are still in progress, to get a first wave or two of testing done
early, knowing that the RC won't be the final one. It makes sense for
some issues for X to be open when an RC is cut, if they are actually
truly intended for X.

44 seems like a lot, and I don't think it's good practice just because
that's how it's happened before. It looks like half of them weren't
actually important for 1.4.x as we're now down to 21. I don't disagree
with the idea that only ""most"" of the issues targeted for version X
will be in version X; the target expresses a ""stretch goal"". Given the
fast pace of change that's probably the only practical view.

I think we're just missing a step then: before RC of X, ask people to
review and update the target of JIRAs for X? In this case, it was a
good point to untarget stuff from 1.4.x entirely; I suspect everything
else should then be targeted at 1.4.2 by default with the exception of
a handful that people really do intend to work in for 1.4.1 before its
final release.

I know it sounds like pencil-pushing, but it's a cheap way to bring
some additional focus to release planning. RC time has felt like a
last-call to *begin* changes ad-hoc when it would go faster if it were
more intentional and constrained. Meaning faster RCs, meaning getting
back to a 3-month release cycle or less, and meaning less rush to push
stuff into a .0 release and less frequent need for a maintenance .1
version.

So what happens if all 1.4.1-targeted JIRAs are targeted to 1.4.2?
would that miss something that is definitely being worked on for
1.4.1?


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Thu, 25 Jun 2015 13:22:39 +0300",Github spam from naver user,dev <dev@spark.apache.org>,"Lots of spam like this popping up:
https://github.com/apache/spark/pull/6972#issuecomment-115130412

I've already reported this to Github to get it stopped and tried to
contact the user, FYI.

---------------------------------------------------------------------


"
jimfcarroll <jimfcarroll@gmail.com>,"Thu, 25 Jun 2015 06:01:14 -0700 (MST)",Re: Problem with version compatibility,dev@spark.apache.org,"Hi Sean,

I'm packaging spark with my (standalone) driver app using maven. Any
assemblies that are used on the mesos workers through extending the
classpath or providing the jars in the driver (via the SparkConf) isn't
packaged with spark (it seems obvious that would be a mistake).

I need, for example, ""RDD"" on my classpath in order for my driver app to
run. Are you saying I need to mark spark as provided in maven and include an
installed distribution's lib directory jars on my classpath?

I'm not using anything but the jar files from a Spark install in my driver
so that seemed superfluous (and slightly more difficult to manage the
deployment). Also, even if that's the case, I don't understand why the maven
dependency of the same version of a deployable distribution would have
different versions of classes in it than the deployable version itself.

Thanks for your patience.
Jim




--

---------------------------------------------------------------------


"
=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"Thu, 25 Jun 2015 15:18:52 +0200",Various forks,"""dev@spark.apache.org"" <dev@spark.apache.org>","Could someone point the source of the Spark-fork used to build
genjavadoc-plugin? Even more important it would be to know the reasoning
behind this fork.

Ironically, this hinders my attempts at removing another fork, the Spark
REPL fork (and the upgrade to Scala 2.11.7). See here
<https://github.com/apache/spark/pull/6903>. Since genjavadoc is a compiler
plugin, it is cross-compiled with the full Scala version, meaning someone
needs to publish a new version for 2.11.7.

Ideally, we'd have a list of all forks maintained by the Spark project. I
know about:

- org.spark-project/akka
- org.spark-project/hive
- org.spark-project/genjavadoc-plugin

Are there more? Where are they hosted, and what's the release process
around them?

thanks,
iulian
â€‹
-- 

--
Iulian Dragos

------
Reactive Apps on the JVM
www.typesafe.com
"
Peter Rudenko <petro.rudenko@gmail.com>,"Thu, 25 Jun 2015 16:35:41 +0300",[SQL] codegen on wide dataset throws StackOverflow,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi, i have a small but very wide dataset (2000 columns). Trying to 
optimize Dataframe pipeline for it, since it behaves very poorly 
comparing to rdd operation.
With spark.sql.codegen=true it throws StackOverflow:

15/06/25 16:27:16 INFO CacheManager: Partition rdd_12_3 not found, 
computing it 15/06/25 16:27:16 INFO HadoopRDD: Input split: 
file:/home/peter/validation.csv:0+337768 15/06/25 16:27:16 INFO 
CacheManager: Partition rdd_12_1 not found, computing it 15/06/25 
16:27:16 INFO HadoopRDD: Input split: 
file:/home/peter/work/train.csv:0+15540706 15/06/25 16:27:16 INFO 
CacheManager: Partition rdd_12_0 not found, computing it 15/06/25 
16:27:16 INFO HadoopRDD: Input split: 
file:/home/peter/holdout.csv:0+336296 15/06/25 16:27:16 INFO 
CacheManager: Partition rdd_12_2 not found, computing it 15/06/25 
16:27:16 INFO HadoopRDD: Input split: 
file:/home/peter/train.csv:15540706+14866642 15/06/25 16:27:17 ERROR 
Executor: Exception in task 1.0 in stage 1.0 (TID 2) 
org.spark-project.guava.util.concurrent.ExecutionError: 
java.lang.StackOverflowError at 
org.spark-project.guava.cache.LocalCache$Segment.get(LocalCache.java:2261) 
at org.spark-project.guava.cache.LocalCache.get(LocalCache.java:4000) at 
org.spark-project.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004) 
at 
org.spark-project.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874) 
at 
org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:105) 
at 
org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:102) 
at 
org.apache.spark.sql.execution.SparkPlan.newMutableProjection(SparkPlan.scala:170) 
at 
org.apache.spark.sql.execution.Project.buildProjection$lzycompute(basicOperators.scala:38) 
at 
org.apache.spark.sql.execution.Project.buildProjection(basicOperators.scala:38) 
at 
org.apache.spark.sql.execution.Project$$anonfun$1.apply(basicOperators.scala:41) 
at 
org.apache.spark.sql.execution.Project$$anonfun$1.apply(basicOperators.scala:40) 
at 
org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686) 
at 
org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686) 
at 
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35) 
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277) at 
org.apache.spark.rdd.RDD.iterator(RDD.scala:244) at 
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35) 
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277) at 
org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69) at 
org.apache.spark.rdd.RDD.iterator(RDD.scala:242) at 
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35) 
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277) at 
org.apache.spark.rdd.RDD.iterator(RDD.scala:244) at 
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35) 
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277) at 
org.apache.spark.rdd.RDD.iterator(RDD.scala:244) at 
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35) 
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277) at 
org.apache.spark.rdd.RDD.iterator(RDD.scala:244) at 
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:70) 
at 
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41) 
at org.apache.spark.scheduler.Task.run(Task.scala:70) at 
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213) at 
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 
at 
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 
at java.lang.Thread.run(Thread.java:745) Caused by: 
java.lang.StackOverflowError at 
scala.reflect.internal.Symbols$Symbol.fullNameInternal(Symbols.scala:1042) 
at 
scala.reflect.internal.Symbols$Symbol.fullNameAsName(Symbols.scala:1047) 
at 
scala.reflect.internal.Symbols$Symbol.fullNameInternal(Symbols.scala:1044) 
at 
scala.reflect.internal.Symbols$Symbol.fullNameAsName(Symbols.scala:1047) 
at 
scala.reflect.internal.Symbols$Symbol.fullNameInternal(Symbols.scala:1044) 
at 
scala.reflect.internal.Symbols$Symbol.fullNameAsName(Symbols.scala:1047) 
at 
scala.reflect.internal.Symbols$Symbol.fullNameInternal(Symbols.scala:1044) 
at 
scala.reflect.internal.Symbols$Symbol.fullNameAsName(Symbols.scala:1047) 
at 
scala.reflect.internal.Symbols$Symbol.fullNameInternal(Symbols.scala:1044) 
at 
scala.reflect.internal.Symbols$Symbol.fullNameAsName(Symbols.scala:1047) 
at 
scala.reflect.internal.Symbols$Symbol.fullNameInternal(Symbols.scala:1044) 
at 
scala.reflect.internal.Symbols$Symbol.fullNameAsName(Symbols.scala:1047) 
at scala.reflect.internal.Symbols$Symbol.fullName(Symbols.scala:1036) at 
scala.reflect.internal.Symbols$Symbol.fullName(Symbols.scala:1052) at 
scala.reflect.internal.Types$TypeRef.needsPreString(Types.scala:2462) at 
scala.reflect.internal.Types$TypeRef.preString(Types.scala:2465) at 
scala.reflect.internal.Types$TypeRef.safeToString(Types.scala:2514) at 
scala.reflect.internal.Types$class.typeToString(Types.scala:7345) at 
scala.reflect.runtime.JavaUniverse.scala$reflect$runtime$SynchronizedTypes$$super$typeToString(JavaUniverse.scala:12) 
at 
scala.reflect.runtime.SynchronizedTypes$class.typeToString(SynchronizedTypes.scala:79) 
at 
scala.reflect.runtime.JavaUniverse.typeToString(JavaUniverse.scala:12) 
at scala.reflect.internal.Types$Type.toString(Types.scala:1018) at 
scala.reflect.internal.Printers$TreePrinter.printTree(Printers.scala:398) at 
scala.reflect.internal.Printers$TreePrinter$$anonfun$print$1.apply(Printers.scala:446) 
at 
scala.reflect.internal.Printers$TreePrinter$$anonfun$print$1.apply(Printers.scala:443) 
at 
scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) 
at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34) 
at scala.reflect.internal.Printers$TreePrinter.print(Printers.scala:443) 
at 
scala.reflect.internal.Printers$TreePrinter.printOpt(Printers.scala:159) 
at 
scala.reflect.internal.Printers$TreePrinter.printTree(Printers.scala:218) at 
scala.reflect.internal.Printers$TreePrinter$$anonfun$print$1.apply(Printers.scala:446) 
at 
scala.reflect.internal.Printers$TreePrinter$$anonfun$print$1.apply(Printers.scala:443) 
at 
scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) 
at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34) 
at scala.reflect.internal.Printers$TreePrinter.print(Printers.scala:443) 
at 
scala.reflect.internal.Printers$TreePrinter$$anonfun$printColumn$2.apply(Printers.scala:95) 
at 
scala.reflect.internal.Printers$TreePrinter$$anonfun$printColumn$2.apply(Printers.scala:95) 
at 
scala.reflect.internal.Printers$TreePrinter.printSeq(Printers.scala:89) 
at 
scala.reflect.internal.Printers$TreePrinter.printSeq(Printers.scala:89) 
at 
scala.reflect.internal.Printers$TreePrinter.printSeq(Printers.scala:89) 
at 
scala.reflect.internal.Printers$TreePrinter.printSeq(Printers.scala:89) 
at 
scala.reflect.internal.Printers$TreePrinter.printSeq(Printers.scala:89) 
at 
scala.reflect.internal.Printers$TreePrinter.printSeq(Printers.scala:89) 
at 
scala.reflect.internal.Printers$TreePrinter.printSeq(Printers.scala:89) 
at scala.reflect.internal.Printers$TreePrinter.printSeq(Printers.scala:89)

             ...

For thin dataset it works, but fails for wide one.

Thanks,
Peter Rudenko
"
Sean Owen <sowen@cloudera.com>,"Thu, 25 Jun 2015 13:50:24 +0000",Re: Problem with version compatibility,"jimfcarroll <jimfcarroll@gmail.com>, dev@spark.apache.org","Yes spark-submit adds all this for you. You don't bring Spark classes in
your app


"
Justin Uang <justin.uang@gmail.com>,"Thu, 25 Jun 2015 14:49:35 +0000",Re: Python UDF performance at large scale,Davies Liu <davies@databricks.com>,"Sweet, filed here: https://issues.apache.org/jira/browse/SPARK-8632


"
jimfcarroll <jimfcarroll@gmail.com>,"Thu, 25 Jun 2015 08:05:09 -0700 (MST)",Re: Problem with version compatibility,dev@spark.apache.org,"Ah. I've avoided using spark-submit primarily because our use of Spark is as
part of an analytics library that's meant to be embedded in other
applications with their own lifecycle management.

use of spark-submit difficult (if not impossible).

Also, we're trying to avoid sending jars over the wire per-job and so we
install our library (minus the spark dependencies) on the mesos workers and
refer to it in the spark configuration using spark.executor.extraClassPath
and if I'm reading SparkSubmit.scala correctly, it looks like the user's
assembly ends up sent to the cluster (at least in the case of yarn) though I
could be wrong on this.

Is there a standard way of running an app that's in control of it's own
runtime lifecycle without spark-submit?

Thanks again.
Jim




--

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Thu, 25 Jun 2015 18:23:34 +0300",Re: Problem with version compatibility,jimfcarroll <jimfcarroll@gmail.com>,"Try putting your same Mesos assembly on the classpath of your client
then, to emulate what spark-submit does. I don't think you merely also
want to put it on the classpath but make sure nothing else from Spark
is coming from your app.

In 1.4 there is the 'launcher' API which makes programmatic access a
lot more feasible but still kinda needs you to get Spark code to your
driver program, and if it's not the same as on your cluster you'd
still risk some incompatibilities.


---------------------------------------------------------------------


"
Yana Kadiyska <yana.kadiyska@gmail.com>,"Thu, 25 Jun 2015 11:25:53 -0400",Re: Problem with version compatibility,Sean Owen <sowen@cloudera.com>,"Jim, I do something similar to you. I mark all dependencies as provided and
then make sure to drop the same version of spark-assembly in my war as I
have on the executors. I don't remember if dropping in server/lib works, I
think I ran into an issue with that. Would love to know ""best practices""
when it comes to Tomcat and Spark


"
Andrew Ash <andrew@andrewash.com>,"Thu, 25 Jun 2015 11:31:50 -0400",Re: [VOTE] Release Apache Spark 1.4.1,Sean Owen <sowen@cloudera.com>,"I would guess that many tickets targeted at 1.4.1 were set that way during
the tail end of the 1.4.0 voting process as people realized they wouldn't
make the .0 release in time.  In that case, they were likely aiming for a
1.4.x release, not necessarily 1.4.1 specifically.  Maybe creating a
""1.4.x"" target in Jira in addition to 1.4.0, 1.4.1, 1.4.2, etc would make
it more clear that these tickets are targeted at ""some 1.4 update release""
rather than specifically ""the 1.4.1 update"".


"
jcai <jonathon.cai@yale.edu>,"Thu, 25 Jun 2015 09:26:57 -0700 (MST)","Verifying Empirically Number of Performance-Heavy Threads and
 Parallelism",dev@spark.apache.org,"Hello,

I am doing some performance testing on Spark and 

1. I would like to verify Spark's parallelism, empirically. For example, I
would like to determine if the number of ""performance-heavy"" threads is
equal to SPARK_WORKER_CORES in standalone mode at a given moment.
Essentially, I want to ensure that the right amount of parallelism is
achieved when fiddling around with certain parameters in Spark. 

a. I've skimmed what Ganglia does but it doesn't seem to have thread-level
information.
b. In addition, running jconsole on the worker (executor) process on a
worker node yields the number of active threads, but I found this kind of
difficult to work with. I've been using a McGyver-ish technique where I
observe the process number that is spawned when running an application, and
then running ""jconsole procNum"" to get a visualization of number of threads
across the application. The thing is this takes a few seconds and so by the
time jconsole starts running, I already see 40 active threads.

Here's an image describing the sort of thing I'm getting:

<http://apache-spark-developers-list.1001551.n3.nabble.com/file/n12898/proc.png> 

In addition, jconsole doesn't show the ""performance"" associated with
specific threads.

Ideally I would like to monitor the process from the time before it's
spawned (starting out with 0 active threads) and observe the thread growth /
utilization over time. Not sure if this is possible to do with
Spark/jconsole.

c. Maybe someone who is familiar with JVM's / OS has a suggestion about how
to go about this, or if this is feasible to do. I am able to do some
verification about the utilization of cores by checking ""top"" on Linux for
low task numbers, but my machines  only have 4 physical cores -i.e. if only
1 task is running, top shows only one core being utilized, 2 tasks shows two
cores being utilized, etc. I want to do some testing with larger numbers of
threads, for example 8 or 16, which is greater than 4.  

2. There is a listing called ""active tasks"" in the executor tab, which
yields an indication of parallelism at a certain moment. Is it safe to
assume one task is assigned to one (separate) thread on a worker node? For
example, if there are 3 tasks running on a node, are there three separate
threads running?

3. In addition, could you point me to where in the source cluster-wide /
slave-node-specific ""parallelism"" is actually implemented in the Spark
source code, or is threading simply deferred to details specified by JVM or
YARN? For example, in  in a comment by Sean Owen
<http://stackoverflow.com/questions/27606541/why-does-spark-streaming-need-a-certain-number-of-cpu-cores-to-run-correctly> 
, it is stated that YARN needs to reserve entire physical cores and so it
seems that you can't set your SPARK_WORKER_CORES to a number greater than
the number of physical cores. Furthermore, it seems that ""cores"" is not a
very good name for specifying these parameters (I've been playing around
with local / standalone mode mostly), and perhaps, should possibly be
changed to just ""threads"" in the future?

Thanks,

Jonathon



--

---------------------------------------------------------------------


"
Raajay <raajay.v@gmail.com>,"Thu, 25 Jun 2015 11:33:35 -0500",Visualize Spark-SQL query plans,dev@spark.apache.org,"Hello,

I am trying to understand the code base of spark-SQL, especially the Query
Analyzer part. I understand that currently (as of Spark 1.4), the sql
module generates a set of Physical plans, but executes only the first in
the list (ref : core/src/main/scala/org/apache/spark/sql/SQLContext.scala,
lines ~930).


I wish to visualize all the query plans generated and possibly execute them
all as different jobs. How should I proceed in order to accomplish this?

Thanks
Raajay
"
jimfcarroll <jimfcarroll@gmail.com>,"Thu, 25 Jun 2015 11:11:41 -0700 (MST)",Re: Problem with version compatibility,dev@spark.apache.org,"Yana and Sean,

Thanks for the feedback. I can get it to work a number of ways, I'm just
wondering if there's a preferred means. 

contain the same version of several classes as the maven dependency. Is this
intentional?

Thanks again.
Jim




--

---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Thu, 25 Jun 2015 11:31:09 -0700",Re: how to implement my own datasource?,=?UTF-8?B?SnVhbiBSb2Ryw61ndWV6IEhvcnRhbMOh?= <juan.rodriguez.hortala@gmail.com>,"I'd suggest looking at the avro data source as an example implementation:

https://github.com/databricks/spark-avro

I also gave a talk a while ago: https://www.youtube.com/watch?v=GQSNJAzxOr8
Hi,

You can connect to by JDBC as described in
https://spark.apache.org/docs/latest/sql-programming-guide.html#jdbc-to-other-databases.
Other option is using HadoopRDD and NewHadoopRDD to connect to databases
compatible with Hadoop, like HBase, some examples can be found at chapter 5
of ""Learning Spark""
https://books.google.es/books?id=tOptBgAAQBAJ&pg=PT190&dq=learning+spark+hadooprdd&hl=en&sa=X&ei=4bqLVaDaLsXaU46NgfgL&ved=0CCoQ6AEwAA#v=onepage&q=%20hadooprdd&f=false
For Spark Streaming see the section ""Custom Sources"" of
https://spark.apache.org/docs/latest/streaming-programming-guide.html

Hope that helps.

Greetings,

Juan

2015-06-25 8:25 GMT+02:00 è¯ºé“ <notyycn@gmail.com>:

"
jimfcarroll <jimfcarroll@gmail.com>,"Thu, 25 Jun 2015 11:47:05 -0700 (MST)",Re: how to implement my own datasource?,dev@spark.apache.org,"
I'm not sure if this is what you're looking for but we have several custom
RDD implementations for internal data format/partitioning schemes.

The Spark api is really simple and consists primarily of being able to
implement 3 simple things:

1) You need a class that extends RDD that's lightweight because it needs to
be Serializable to machines on a cluster (therefore it shouldn't actually
contain the data, for example).
2) That class needs to implement getPartitions() to generate an array of
serializable Partition instances.
3) That class needs to implement compute(Partition p, TaskContext t) which
will (likely) be executed on a deserialized copy of your RDD class and
provided a deserialized instance of one of the partitions returned from
getPartitions() and needs to return an Iterator for the actual data within
the partition.





--

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Thu, 25 Jun 2015 15:47:34 -0700",Re: Github spam from naver user,Sean Owen <sowen@cloudera.com>,"We might be able to ask asf infra to help. Can you create a ticket?



"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Fri, 26 Jun 2015 02:15:10 +0000",RE: Force inner join to shuffle the smallest table,Stephen Carman <scarman@coldlight.com>,"The problem is that it shuffles the wrong table which even compressed wonâ€™t fit to my disk.

Actually, I found the source of the problem, although I could not reproduce it with synthetic data (but remains true for my original data: big table 2B rows, small 500K):

When I do join on two fields like this â€œselect big.f1, big.f2 from small inner join big on big.s=small.s and big.d=small.dâ€ then the query planner does ShuffledHashJoin with the bigger table and the query fails due to Shuffle write of the whole big table (out of space):
== Physical Plan ==
Project [bedId#28,sensorId#30L,time#31,label#32,fft#34]
ShuffledHashJoin [time#38,bedId#35], [time#31,bedId#28], BuildLeft
  Exchange (HashPartitioning [time#38,bedId#35], 2000), []
    PhysicalRDD [bedId#35,time#38], MapPartitionsRDD[75] at explain at <console>:25
  Exchange (HashPartitioning [time#31,bedId#28], 2000), []
   PhysicalRDD [bedId#28,sensorId#30L,fft#34,label#32,time#31], MapPartitionsRDD[77] at explain at <console>:25

When I do join on one field like this â€œselect big.f1, big.f2 from small inner join big on big.s=small.sâ€ then the query planner does BroadcastHashJoin which writes just whatâ€™s needed and therefore executes without problems:

== Physical Plan ==
Project [bedId#28,sensorId#30L,time#31,label#32,fft#34]
BroadcastHashJoin [time#38], [time#31], BuildLeft
  Limit 498340
   PhysicalRDD [time#38], MapPartitionsRDD[66] at explain at <console>:25
  PhysicalRDD [bedId#28,sensorId#30L,fft#34,label#32,time#31], MapPartitionsRDD[68] at explain at <console>:25

Could Spark SQL developers suggest why it happens?

Best regards, Alexander


From: Stephen Carman [mailto:scarman@coldlight.com]
Sent: Wednesday, June 24, 2015 12:33 PM
To: Ulanov, Alexander
Cc: CC GP; dev@spark.apache.org
Subject: Re: Force inner join to shuffle the smallest table

Have you tried shuffle compression?

spark.shuffle.compress (true|false)

if you have a filesystem capable also Iâ€™ve noticed file consolidation helps disk usage a bit.

spark.shuffle.consolidateFiles (true|false)

Steve

On Jun 24, 2015, at 3:27 PM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:

It also fails, as I mentioned in the original question.

From: CC GP [mailto:chandrika.gopalakrishna@gmail.com]
Sent: Wednesday, June 24, 2015 12:08 PM
To: Ulanov, Alexander
Cc: dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: Force inner join to shuffle the smallest table

Try below and see if it makes a difference:

val result = sqlContext.sql(â€œselect big.f1, big.f2 from small inner join big on big.s=small.s and big.d=small.dâ€)

On Wed, Jun 24, 2015 at 11:35 AM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
Hi,

I try to inner join of two tables on two fields(string and double). One table is 2B rows, the second is 500K. They are stored in HDFS in Parquet. Spark v 1.4.
val big = sqlContext.paquetFile(â€œhdfs://bigâ€)
data.registerTempTable(â€œbigâ€)
val small = sqlContext.paquetFile(â€œhdfs://smallâ€)
data.registerTempTable(â€œsmallâ€)
val result = sqlContext.sql(â€œselect big.f1, big.f2 from big inner join small on big.s=small.s and big.d=small.dâ€)

This query fails in the middle due to one of the workers â€œdisk out of spaceâ€ with shuffle reported 1.8TB which is the maximum size of my spark working dirs (on total 7 worker nodes). This is surprising, because the â€œbigâ€ table takes 2TB disk space (unreplicated) and â€œsmallâ€ about 5GB and I would expect that optimizer will shuffle the small table. How to force Spark to shuffle the small table? I tried to write â€œsmall inner join bigâ€ however it also fails with 1.8TB of shuffle.

Best regards, Alexander

This e-mail is intended solely for the above-mentioned recipient and it may contain confidential or privileged information. If you have received it in error, please notify us immediately and delete the e-mail. You must not copy, distribute, disclose or take any action in reliance on it. In addition, the contents of an attachment to this e-mail may contain software viruses which could damage your own computer system. While ColdLight Solutions, LLC has taken every reasonable precaution to minimize this risk, we cannot accept liability for any damage which you sustain as a result of software viruses. You should perform your own virus checks before opening the attachment.
"
giive chen <thegiive@gmail.com>,"Fri, 26 Jun 2015 10:45:13 +0800",Re: custom REST port from spark-defaults.cof,Niranda Perera <niranda.perera@gmail.com>,"HI Niranda

I think ""spark.master.rest.port"" is what you want.

Wisely Chen


---------------------------------------------------------------------


"
yash datta <saucam@gmail.com>,"Fri, 26 Jun 2015 11:38:41 +0530",External Shuffle service over yarn,dev@spark.apache.org,"Hi devs,

Can someone point out if there are any distinct advantages of using
external shuffle service over yarn (runs on node manager  as an auxiliary
service

https://issues.apache.org/jira/browse/SPARK-3797)  instead of the default
execution in the executor containers ?

Please also mention if you have seen any differences having used both ways ?

Thanks and Best Regards
Yash

-- 
When events unfold with calm and ease
When the winds that blow are merely breeze
Learn from nature, from birds and bees
Live your life in love, and let joy not cease.
"
Sean Owen <sowen@cloudera.com>,"Fri, 26 Jun 2015 06:29:37 +0000",Re: Github spam from naver user,Reynold Xin <rxin@databricks.com>,"Ultimately I think its Github that has to act to ban the user. I've already
asked for him to be blocked from the apache org on Github.


"
"""louis.hust"" <louis.hust@gmail.com>","Fri, 26 Jun 2015 14:37:52 +0800",Spark for distributed dbms cluster,user@spark.apache.org,"Hi, all

For now, spark is based on hadoop, I want use a database cluster instead of the hadoop,
so data distributed on each database in the cluster.

I want to know if spark suitable for this situation ?

Any idea will be appreciated!


---------------------------------------------------------------------


"
Sandy Ryza <sandy.ryza@cloudera.com>,"Thu, 25 Jun 2015 23:42:55 -0700",Re: External Shuffle service over yarn,yash datta <saucam@gmail.com>,"Hi Yash,

executors are discarded, your application is still able to get at the
shuffle data that they wrote out.

-Sandy


"
=?UTF-8?B?6K+66ZOB?= <notyycn@gmail.com>,"Fri, 26 Jun 2015 14:43:10 +0800",Re: how to implement my own datasource?,jimfcarroll <jimfcarroll@gmail.com>,"thank you guys, I'll read examples and give a try.


"
Aaron Davidson <ilikerps@gmail.com>,"Fri, 26 Jun 2015 00:54:09 -0700",Re: External Shuffle service over yarn,Sandy Ryza <sandy.ryza@cloudera.com>,"A second advantage is that it allows individual Executors to go into GC
pause (or even crash) and still allow other Executors to read shuffle data
and make progress, which tends to improve stability of memory-intensive
jobs.


"
Akhil Das <akhil@sigmoidanalytics.com>,"Fri, 26 Jun 2015 13:46:09 +0530",Re: Spark for distributed dbms cluster,"""louis.hust"" <louis.hust@gmail.com>","Which distributed database are you referring here? Spark can connect with
almost all those databases out there (You just need to pass the
Input/Output Format classes or there are a bunch of connectors also
available).

Thanks
Best Regards


"
Josh Rosen <rosenville@gmail.com>,"Fri, 26 Jun 2015 01:14:38 -0700",Re: [SQL] codegen on wide dataset throws StackOverflow,Peter Rudenko <petro.rudenko@gmail.com>,"Which Spark version are you using?  Can you file a JIRA for this issue?


"
"""=?gb18030?B?U2Vh?="" <261810726@qq.com>","Fri, 26 Jun 2015 17:06:16 +0800",Time is ugly in Spark Streaming....,"""=?gb18030?B?dXNlcg==?="" <user@spark.apache.org>, ""=?gb18030?B?ZGV2?="" <dev@spark.apache.org>","Hi, all


I find a problem in spark streaming, when I use the time in function foreachRDD... I find the time is very interesting.
val messages = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, topicsSet)
dataStream.map(x => createGroup(x._2, dimensions)).groupByKey().foreachRDD((rdd, time) => {
  try {
    if (!rdd.partitions.isEmpty) {
      rdd.foreachPartition(partition => {
        handlePartition(partition, timeType, time, dimensions, outputTopic, brokerList)
      })
    }
  } catch {
    case e: Exception => e.printStackTrace()
  }
})


val dateFormat = new SimpleDateFormat(""yyyy-MM-dd'T'HH:mm:ss"")
var date = dateFormat.format(new Date(time.milliseconds))

Then I insert the 'date' into Kafka , but I found .....


{""timestamp"":""2015-06-00T16:50:02"",""status"":""3"",""type"":""1"",""waittime"":""0"",""count"":17}
{""timestamp"":""2015-06-26T16:51:13"",""status"":""1"",""type"":""1"",""waittime"":""0"",""count"":34}
{""timestamp"":""2015-06-00T16:50:02"",""status"":""4"",""type"":""0"",""waittime"":""0"",""count"":279}
{""timestamp"":""2015-06-26T16:52:00"",""status"":""11"",""type"":""1"",""waittime"":""0"",""count"":9}
{""timestamp"":""0020-06-26T16:50:36"",""status"":""7"",""type"":""0"",""waittime"":""0"",""count"":1722}
{""timestamp"":""2015-06-10T16:51:17"",""status"":""0"",""type"":""0"",""waittime"":""0"",""count"":2958}
{""timestamp"":""2015-06-26T16:52:00"",""status"":""0"",""type"":""1"",""waittime"":""0"",""count"":114}
{""timestamp"":""2015-06-10T16:51:17"",""status"":""11"",""type"":""0"",""waittime"":""0"",""count"":2066}
{""timestamp"":""2015-06-26T16:52:00"",""status"":""1"",""type"":""0"",""waittime"":""0"",""count"":1539}"
Gerard Maas <gerard.maas@gmail.com>,"Fri, 26 Jun 2015 11:40:28 +0200",Re: Time is ugly in Spark Streaming....,Sea <261810726@qq.com>,"Are you sharing the SimpleDateFormat instance? This looks a lot more like
the non-thread-safe behaviour of SimpleDateFormat (that has claimed many
unsuspecting victims over the years), than any 'ugly' Spark Streaming. Try
writing the timestamps in millis to Kafka and compare.

-kr, Gerard.


"
"""Huang, Jie"" <jie.huang@intel.com>","Fri, 26 Jun 2015 11:24:38 +0000",[SparkScore]Performance portal for Apache Spark - WW26,"""user@spark.apache.org"" <user@spark.apache.org>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","Performance Portal for Apache Spark
Description
________________________________
Each data point represents each workload runtime percent compared with the previous week. Different lines represents different workloads running on spark yarn-client mode.
Hardware
________________________________
CPU type: Intel(r) Xeon(r) CPU E5-2697 v2 @ 2.70GHz
Memory: 128GB
NIC: 10GbE
Disk(s): 8 x 1TB SATA HDD
Software
________________________________
JAVA version: 1.8.0_25
Hadoop version: 2.5.0-CDH5.3.2
HiBench version: 4.0
Spark on yarn-client mode
Cluster
________________________________
1 node for Master
10 nodes for Slave
Regular
Summary
The lower percent the better performance.
________________________________
Group

ww19

ww20

ww22

ww23

ww24

ww25

ww26

HiBench

9.1%

6.6%

6.0%

7.9%

-6.5%

-3.1%

-2.1%

spark-perf

4.1%

4.4%

-1.8%

4.1%

-4.7%

-4.6%

-5.4%


[http://01org.github.io/sparkscore/image/plaf1.time/overall.png]
Y-Axis: normalized completion time; X-Axis: Work Week.
The commit number can be found in the result table.
The performance score for each workload is normalized based on the elapsed time for 1.2 release. The lower the better.

Detail
________________________________
HiBench
________________________________
JOB

ww19

ww20

ww22

ww23

ww24

ww25

ww26

commit

489700c8

8e3822a0

530efe3e

90c60692

db81b9d8

4eb48ed1

32e3cdaa

sleep

%

%

-2.1%

-2.9%

-4.1%

12.8%

-5.1%

wordcount

17.6%

11.4%

8.0%

8.3%

-18.6%

-10.9%

6.9%

kmeans

92.1%

61.5%

72.1%

92.9%

86.9%

95.8%

123.3%

scan

-4.9%

-7.2%

%

-1.1%

-25.5%

-21.0%

-12.4%

bayes

-24.3%

-20.1%

-18.3%

-11.1%

-29.7%

-31.3%

-30.9%

aggregation

5.6%

10.5%

%

9.2%

-15.3%

-15.0%

-37.6%

join

4.5%

1.2%

%

1.0%

-12.7%

-13.9%

-16.4%

sort

-3.3%

-0.5%

-11.9%

-12.5%

-17.5%

-17.3%

-20.7%

pagerank

2.2%

3.2%

4.0%

2.9%

-11.4%

-13.0%

-11.4%

terasort

-7.1%

-0.2%

-9.5%

-7.3%

-16.7%

-17.0%

-16.3%


Comments: null means no such workload running or workload failed in this time.
[http://01org.github.io/sparkscore/image/plaf1.time/HiBench_workloads.png]
Y-Axis: normalized completion time; X-Axis: Work Week.
The commit number can be found in the result table.
The performance score for each workload is normalized based on the elapsed time for 1.2 release.The lower the better.
spark-perf
________________________________
JOB

ww19

ww20

ww22

ww23

ww24

ww25

ww26

commit

489700c8

8e3822a0

530efe3e

90c60692

db81b9d8

4eb48ed1

32e3cdaa

agg

13.2%

7.0%

%

18.3%

5.2%

2.5%

1.1%

agg-int

16.4%

21.2%

%

9.6%

4.0%

8.2%

7.0%

agg-naive

4.3%

-2.4%

%

-0.8%

-6.7%

-6.8%

-8.5%

scheduling

-6.1%

-8.9%

-14.5%

-2.1%

-6.4%

-6.5%

-5.7%

count-filter

4.1%

1.0%

6.6%

6.8%

-10.2%

-10.4%

-9.8%

count

4.8%

4.6%

6.7%

8.0%

-7.3%

-7.0%

-8.0%

sort

-8.1%

-2.5%

-6.2%

-7.0%

-14.6%

-14.4%

-13.9%

sort-int

4.5%

15.3%

-1.6%

-0.1%

-1.5%

-2.2%

-5.3%


Comments: null means no such workload running or workload failed in this time.
[http://01org.github.io/sparkscore/image/plaf1.time/spark-perf_workloads.png]
Y-Axis: normalized completion time; X-Axis: Work Week.
The commit number can be found in the result table.
The performance score for each workload is normalized based on the elapsed time for 1.2 release. The lower the better.
Release
Summary
The lower percent the better performance.
________________________________
Group

1.2.1

1.3.0

1.3.1

1.4.0

HiBench

-1.0%

10.5%

8.4%

8.6%

spark-perf

3.2%

0.9%

1.9%

1.3%


[http://01org.github.io/sparkscore/image/plaf1.release/overall.png]
Y-Axis: normalized completion time; X-Axis: Release.
The performance score for each workload is normalized based on the elapsed time for 1.2 release. The lower the better.

Detail
________________________________
HiBench
________________________________
JOB

1.2.1

1.3.0

1.3.1

1.4.0

sleep

%

%

%

-0.5%

wordcount

3.5%

5.4%

5.1%

8.7%

kmeans

6.0%

72.6%

82.7%

100.7%

scan

-0.7%

-3.2%

-1.9%

-4.4%

bayes

-19.7%

7.7%

-24.5%

-14.4%

aggregation

4.6%

7.1%

9.9%

9.3%

join

0.7%

4.0%

8.6%

1.3%

sort

-1.0%

2.1%

-1.8%

-10.4%

pagerank

1.5%

2.2%

1.3%

5.4%

terasort

-3.7%

-3.3%

-3.7%

-9.5%


Comments: null means no such workload running or workload failed in this time.
[http://01org.github.io/sparkscore/image/plaf1.release/HiBench_workloads.png]
Y-Axis: normalized completion time; X-Axis: Release.
The commit number can be found in the result table.
The performance score for each workload is normalized based on the elapsed time for 1.2 release. The lower the better.
spark-perf
________________________________
JOB

1.2.1

1.3.0

1.3.1

1.4.0

agg

1.9%

3.1%

6.2%

5.0%

agg-int

6.4%

17.1%

18.0%

24.2%

agg-naive

-2.6%

-3.2%

-1.8%

-5.2%

scheduling

8.2%

-16.8%

-14.4%

-19.1%

count-filter

-0.4%

0.3%

-0.5%

0.4%

count

0.6%

-0.3%

0.4%

0.9%

sort

1.2%

-3.3%

-5.3%

-1.9%

sort-int

10.1%

10.0%

12.3%

6.0%


Comments: null means no such workload running or workload failed in this time.
[http://01org.github.io/sparkscore/image/plaf1.release/spark-perf_workloads.png]
Y-Axis: normalized completion time; X-Axis: Release.
The commit number can be found in the result table.
The performance score for each workload is normalized based on the elapsed time for 1.2 release. The lower the better.
________________________________
Copyright (c) 2015 Intel Corporation. All rights reserved. *Other names and brands may be claimed as the property of others.
Project Email: sparkscore@lists.01.org<mailto:sparkscore@lists.01.org> Please subscribe to the list at: https://lists.01.org/mailman/listinfo/sparkscore
"
Peter Rudenko <petro.rudenko@gmail.com>,"Fri, 26 Jun 2015 14:38:32 +0300",Re: [SQL] codegen on wide dataset throws StackOverflow,Josh Rosen <rosenville@gmail.com>,"I'm using spark-1.4.0. Sure will try to make steps to reproduce and file 
a JIRA ticket.

Thanks,
Peter Rudenko


"
Nan Zhu <zhunanmcgill@gmail.com>,"Fri, 26 Jun 2015 07:59:25 -0400",Re: [SparkScore]Performance portal for Apache Spark - WW26,=?utf-8?Q?Huang=2C_Jie?= <jie.huang@intel.com>,"Hi, Jie,  

Thank you very much for this work! Very helpful!

I just would like to confirm that I understand the numbers correctly: if we take the running time of 1.2 release as 100s

9.1% - means the running time is 109.1 s?

-4% - means it comes 96s?

If thatâ€™s the true meaning of the numbers, what happened to k-means in HiBench?

Best,  

--  
Nan Zhu
http://codingcat.me





"
"""Huang, Jie"" <jie.huang@intel.com>","Fri, 26 Jun 2015 12:17:44 +0000",RE: [SparkScore]Performance portal for Apache Spark - WW26,Nan Zhu <zhunanmcgill@gmail.com>,"Correct. Your calculation is right!

We have been aware of that kmeans performance drop also. According to our observation, it is caused by some unbalanced executions among different tasks. Even we used the same test data between different versions (i.e., not caused by the data skew).

And the corresponding run time information has been shared with Xiangrui. Now he is also helping to identify the root cause altogether.

Thank you && Best Regards,
Grace ï¼ˆHuang Jie)

From: Nan Zhu [mailto:zhunanmcgill@gmail.com]
Sent: Friday, June 26, 2015 7:59 PM
To: Huang, Jie
Cc: user@spark.apache.org; dev@spark.apache.org
Subject: Re: [SparkScore]Performance portal for Apache Spark - WW26

Hi, Jie,

Thank you very much for this work! Very helpful!

I just would like to confirm that I understand the numbers correctly: if we take the running time of 1.2 release as 100s

9.1% - means the running time is 109.1 s?

-4% - means it comes 96s?

If thatâ€™s the true meaning of the numbers, what happened to k-means in HiBench?

Best,

--
Nan Zhu
http://codingcat.me


On Friday, June 26, 2015 at 7:24 AM, Huang, Jie wrote:
IntelÂ® XeonÂ® CPU E5-2697

"
Nan Zhu <zhunanmcgill@gmail.com>,"Fri, 26 Jun 2015 08:21:11 -0400",Re: [SparkScore]Performance portal for Apache Spark - WW26,=?utf-8?Q?Huang=2C_Jie?= <jie.huang@intel.com>,"Thank you, Jie! Very nice work!

--  
Nan Zhu
http://codingcat.me



ur observation, it is caused by some unbalanced executions among different tasks. Even we used the same test data between different versions (i.e., not caused by the data skew).
i. Now he is also helping to identify the root cause altogether.  
rk.apache.org (mailto:dev@spark.apache.org)
  
f we take the running time of 1.2 release as 100s
eans in HiBench?


"
"""Huang, Jie"" <jie.huang@intel.com>","Fri, 26 Jun 2015 12:28:17 +0000",RE: [SparkScore]Performance portal for Apache Spark - WW26,Nan Zhu <zhunanmcgill@gmail.com>,"Thanks. In general, we can see a stable trend in Spark master branch and latest release.

And we are also considering to add more benchmarks/workloads into this automation perf tool. Any comment and feedback is warmly welcomed.

Thank you && Best Regards,
Grace ï¼ˆHuang Jie)

From: Nan Zhu [mailto:zhunanmcgill@gmail.com]
Sent: Friday, June 26, 2015 8:21 PM
To: Huang, Jie
Cc: user@spark.apache.org; dev@spark.apache.org
Subject: Re: [SparkScore]Performance portal for Apache Spark - WW26

Thank you, Jie! Very nice work!

--
Nan Zhu
http://codingcat.me

On Friday, June 26, 2015 at 8:17 AM, Huang, Jie wrote:

Correct. Your calculation is right!



We have been aware of that kmeans performance drop also. According to our observation, it is caused by some unbalanced executions among different tasks. Even we used the same test data between different versions (i.e., not caused by the data skew).



And the corresponding run time information has been shared with Xiangrui. Now he is also helping to identify the root cause altogether.



Thank you && Best Regards,

Grace ï¼ˆHuang Jie)



From: Nan Zhu [mailto:zhunanmcgill@gmail.com]
Sent: Friday, June 26, 2015 7:59 PM
To: Huang, Jie
Cc: user@spark.apache.org<mailto:user@spark.apache.org>; dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: [SparkScore]Performance portal for Apache Spark - WW26



Hi, Jie,



Thank you very much for this work! Very helpful!



I just would like to confirm that I understand the numbers correctly: if we take the running time of 1.2 release as 100s



9.1% - means the running time is 109.1 s?



-4% - means it comes 96s?



If thatâ€™s the true meaning of the numbers, what happened to k-means in HiBench?



Best,



--

Nan Zhu

http://codingcat.me



On Friday, June 26, 2015 at 7:24 AM, Huang, Jie wrote:

IntelÂ® XeonÂ® CPU E5-2697



"
"""=?gb18030?B?U2Vh?="" <261810726@qq.com>","Fri, 26 Jun 2015 20:59:47 +0800","=?gb18030?B?u9i4tKO6IFRpbWUgaXMgdWdseSBpbiBTcGFyayBT?=
 =?gb18030?B?dHJlYW1pbmcuLi4u?=","""=?gb18030?B?R2VyYXJkIE1hYXM=?="" <gerard.maas@gmail.com>","Yes, I make it.




------------------ Ô­Ê¼ÓÊ¼þ ------------------
·¢¼þÈË: ""Gerard Maas"";<gerard.maas@gmail.com>;
·¢ËÍÊ±¼ä: 2015Äê6ÔÂ26ÈÕ(ÐÇÆÚÎå) ÏÂÎç5:40
ÊÕ¼þÈË: ""Sea""<261810726@qq.com>; 
³­ËÍ: ""user""<user@spark.apache.org>; ""dev""<dev@spark.apache.org>; 
Ö÷Ìâ: Re: Time is ugly in Spark Streaming....



Are you sharing the SimpleDateFormat instance? This looks a lot more like the non-thread-safe behaviour of SimpleDateFormat (that has claimed many unsuspecting victims over the years), than any 'ugly' Spark Streaming. Try writing the timestamps in millis to Kafka and compare.

-kr, Gerard.


On Fri, Jun 26, 2015 at 11:06 AM, Sea <261810726@qq.com> wrote:
Hi, all


I find a problem in spark streaming, when I use the time in function foreachRDD... I find the time is very interesting.
val messages = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, topicsSet)
dataStream.map(x => createGroup(x._2, dimensions)).groupByKey().foreachRDD((rdd, time) => {
  try {
    if (!rdd.partitions.isEmpty) {
      rdd.foreachPartition(partition => {
        handlePartition(partition, timeType, time, dimensions, outputTopic, brokerList)
      })
    }
  } catch {
    case e: Exception => e.printStackTrace()
  }
})


val dateFormat = new SimpleDateFormat(""yyyy-MM-dd'T'HHilliseconds))

Then I insert the 'date' into Kafka , but I found .....


{""timestamp"":""2015-06-00T16:50:02"",""status"":""3"",""type"":""1"",""waittime"":""0"",""count"":17}
{""timestamp"":""2015-06-26T16:51:13"",""status"":""1"",""type"":""1"",""waittime"":""0"",""count"":34}
{""timestamp"":""2015-06-00T16:50:02"",""status"":""4"",""type"":""0"",""waittime"":""0"",""count"":279}
{""timestamp"":""2015-06-26T16:52:00"",""status"":""11"",""type"":""1"",""waittime"":""0"",""count"":9}
{""timestamp"":""0020-06-26T16:50:36"",""status"":""7"",""type"":""0"",""waittime"":""0"",""count"":1722}
{""timestamp"":""2015-06-10T16:51:17"",""status"":""0"",""type"":""0"",""waittime"":""0"",""count"":2958}
{""timestamp"":""2015-06-26T16:52:00"",""status"":""0"",""type"":""1"",""waittime"":""0"",""count"":114}
{""timestamp"":""2015-06-10T16:51:17"",""status"":""11"",""type"":""0"",""waittime"":""0"",""count"":2066}
{""timestamp"":""2015-06-26T16:52:00"",""status"":""1"",""type"":""0"",""waittime"":""0"",""count"":1539}"
Tom Graves <tgraves_cs@yahoo.com.INVALID>,"Fri, 26 Jun 2015 14:10:15 +0000 (UTC)",Re: [VOTE] Release Apache Spark 1.4.1,"Andrew Ash <andrew@andrewash.com>, Sean Owen <sowen@cloudera.com>","So is this open for vote then or are we waiting on other things?
Tom 


   

 I would guess that many tickets targeted at 1.4.1 were set that way during the tail end of the 1.4.0 voting process as people realized they wouldn't make the .0 release in time.Â  In that case, they were likely aiming for a 1.4.x release, not necessarily 1.4.1 specifically.Â  Maybe creating a ""1.4.x"" target in Jira in addition to 1.4.0, 1.4.1, 1.4.2, etc would make it more clear that these tickets are targeted at ""some 1.4 update release"" rather than specifically ""the 1.4.1 update"".

That makes sense to me -- there's an urgent fix to get out. I missed
that part. Not that it really matters but was that expressed
elsewhere?

I know we tend to start the RC process even when a few more changes
are still in progress, to get a first wave or two of testing done
early, knowing that the RC won't be the final one. It makes sense for
some issues for X to be open when an RC is cut, if they are actually
truly intended for X.

44 seems like a lot, and I don't think it's good practice just because
that's how it's happened before. It looks like half of them weren't
actually important for 1.4.x as we're now down to 21. I don't disagree
with the idea that only ""most"" of the issues targeted for version X
will be in version X; the target expresses a ""stretch goal"". Given the
fast pace of change that's probably the only practical view.

I think we're just missing a step then: before RC of X, ask people to
review and update the target of JIRAs for X? In this case, it was a
good point to untarget stuff from 1.4.x entirely; I suspect everything
else should then be targeted at 1.4.2 by default with the exception of
a handful that people really do intend to work in for 1.4.1 before its
final release.

I know it sounds like pencil-pushing, but it's a cheap way to bring
some additional focus to release planning. RC time has felt like a
last-call to *begin* changes ad-hoc when it would go faster if it were
more intentional and constrained. Meaning faster RCs, meaning getting
back to a 3-month release cycle or less, and meaning less rush to push
stuff into a .0 release and less frequent need for a maintenance .1
version.

So what happens if all 1.4.1-targeted JIRAs are targeted to 1.4.2?
would that miss something that is definitely being worked on for
1.4.1?

:
ote:
n 1.4.1!
e:

---------------------------------------------------------------------





  "
Patrick Wendell <pwendell@gmail.com>,"Fri, 26 Jun 2015 09:22:06 -0700",Re: [VOTE] Release Apache Spark 1.4.1,Tom Graves <tgraves_cs@yahoo.com>,"Hey Tom - no one voted on this yet, so I need to keep it open until
people vote. But I'm not aware of specific things we are waiting for.
Anyone else?

- Patrick


---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Fri, 26 Jun 2015 13:27:49 -0700",Re: [VOTE] Release Apache Spark 1.4.1,Patrick Wendell <pwendell@gmail.com>,"I got the following when running test suite:

[INFO] compiler plugin:
BasicArtifact(org.scalamacros,paradise_2.10.4,2.0.1,null)
^[[0m[^[[0minfo^[[0m] ^[[0mCompiling 2 Scala sources and 1 Java source to
/home/hbase/spark-1.4.1/streaming/target/scala-2.10/test-classes...^[[0m
^[[0m[^[[31merror^[[0m]
^[[0m/home/hbase/spark-1.4.1/streaming/src/test/scala/org/apache/spark/streaming/DStreamClosureSuite.scala:82:
not found: type TestException^[[0m
^[[0m[^[[31merror^[[0m] ^[[0m        throw new TestException(^[[0m
^[[0m[^[[31merror^[[0m] ^[[0m                  ^^[[0m
^[[0m[^[[31merror^[[0m]
^[[0m/home/hbase/spark-1.4.1/streaming/src/test/scala/org/apache/spark/streaming/scheduler/JobGeneratorSuite.scala:73:
not found: type TestReceiver^[[0m
^[[0m[^[[31merror^[[0m] ^[[0m      val inputStream = ssc.receiverStream(new
TestReceiver)^[[0m
^[[0m[^[[31merror^[[0m] ^[[0m
^^[[0m
^[[0m[^[[31merror^[[0m] ^[[0mtwo errors found^[[0m
^[[0m[^[[31merror^[[0m] ^[[0mCompile failed at Jun 25, 2015 5:12:24 PM
[1.492s]^[[0m

Has anyone else seen similar error ?

Thanks


"
Ted Yu <yuzhihong@gmail.com>,"Fri, 26 Jun 2015 13:38:51 -0700",Re: [VOTE] Release Apache Spark 1.4.1,Patrick Wendell <pwendell@gmail.com>,"Pardon.
During earlier test run, I got:

^[[32mStreamingContextSuite:^[[0m
^[[32m- from no conf constructor^[[0m
^[[32m- from no conf + spark home^[[0m
^[[32m- from no conf + spark home + env^[[0m
^[[32m- from conf with settings^[[0m
^[[32m- from existing SparkContext^[[0m
^[[32m- from existing SparkContext with settings^[[0m
^[[31m*** RUN ABORTED ***^[[0m
^[[31m  java.lang.NoSuchMethodError:
org.apache.spark.ui.JettyUtils$.createStaticHandler(Ljava/lang/String;Ljava/lang/String;)Lorg/eclipse/jetty/servlet/ServletContextHandler;^[[0m
^[[31m  at
org.apache.spark.streaming.ui.StreamingTab.attach(StreamingTab.scala:49)^[[0m
^[[31m  at
org.apache.spark.streaming.StreamingContext$$anonfun$start$2.apply(StreamingContext.scala:601)^[[0m
^[[31m  at
org.apache.spark.streaming.StreamingContext$$anonfun$start$2.apply(StreamingContext.scala:601)^[[0m
^[[31m  at scala.Option.foreach(Option.scala:236)^[[0m
^[[31m  at
org.apache.spark.streaming.StreamingContext.start(StreamingContext.scala:601)^[[0m
^[[31m  at
org.apache.spark.streaming.StreamingContextSuite$$anonfun$8.apply$mcV$sp(StreamingContextSuite.scala:101)^[[0m
^[[31m  at
org.apache.spark.streaming.StreamingContextSuite$$anonfun$8.apply(StreamingContextSuite.scala:96)^[[0m
^[[31m  at
org.apache.spark.streaming.StreamingContextSuite$$anonfun$8.apply(StreamingContextSuite.scala:96)^[[0m
^[[31m  at
org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)^[[0m
^[[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)^[[0m

The error from previous email was due to absence
of StreamingContextSuite.scala


"
"""Vasili I. Galchin"" <vigalchin@gmail.com>","Fri, 26 Jun 2015 15:19:21 -0700",R - Scala interface used in Spark?,dev@spark.apache.org,"A friend sent the below:

        http://cran.r-project.org/web/packages/rscala/index.html

Is this the ""glue"" between R and Scala that is used in Spark?

Vasili
"
"""Emrehan =?UTF-8?Q?T=C3=BCz=C3=BCn?="" <emrehan.tuzun@gmail.com>","Fri, 26 Jun 2015 15:57:02 -0700 (PDT)",Re: Time is ugly in Spark Streaming....,"""Sea"" <261810726@qq.com>","


foreachRDD... I find the time is very interesting.
StringDecoder, StringDecoder](ssc, kafkaParams, topicsSet)
foreachRDD((rdd, time) => {
outputTopic, brokerList)
""type"":""1"",""waittime"":""0"",""count"":17}
""type"":""1"",""waittime"":""0"",""count"":34}
""type"":""0"",""waittime"":""0"",""count"":279}
""type"":""1"",""waittime"":""0"",""count"":9}
""type"":""0"",""waittime"":""0"",""count"":1722}
""type"":""0"",""waittime"":""0"",""count"":2958}
""type"":""1"",""waittime"":""0"",""count"":114}
""type"":""0"",""waittime"":""0"",""count"":2066}
""type"":""0"",""waittime"":""0"",""count"":1539}"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Fri, 26 Jun 2015 17:03:05 -0700",Re: R - Scala interface used in Spark?,"""Vasili I. Galchin"" <vigalchin@gmail.com>","We don't use the rscala package in SparkR -- We have an in built R-JVM
bridge that is customized to work with various deployment modes. You can
find more details in my Spark Summit 2015 talk.

Thanks
Shivaram


"
"""Vasili I. Galchin"" <vigalchin@gmail.com>","Fri, 26 Jun 2015 17:19:29 -0700",Re: R - Scala interface used in Spark?,"""shivaram@eecs.berkeley.edu"" <shivaram@eecs.berkeley.edu>","Url plese !!  URL. Please of ypur work.


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Fri, 26 Jun 2015 17:21:00 -0700",Re: R - Scala interface used in Spark?,"""Vasili I. Galchin"" <vigalchin@gmail.com>","You can see the slides, video at
https://spark-summit.org/2015/events/sparkr-the-past-the-present-and-the-future/


"
"""Vasili I. Galchin"" <vigalchin@gmail.com>","Fri, 26 Jun 2015 17:21:34 -0700",Re: R - Scala interface used in Spark?,"""shivaram@eecs.berkeley.edu"" <shivaram@eecs.berkeley.edu>","How about Python??


"
Reynold Xin <rxin@databricks.com>,"Fri, 26 Jun 2015 18:06:04 -0700",Re: R - Scala interface used in Spark?,"""Vasili I. Galchin"" <vigalchin@gmail.com>","You doing something for Haskell??


"
Reynold Xin <rxin@databricks.com>,"Fri, 26 Jun 2015 19:03:16 -0700",Re: R - Scala interface used in Spark?,"""Vasili I. Galchin"" <vigalchin@gmail.com>","Take a look at this for Python:

https://cwiki.apache.org/confluence/display/SPARK/PySpark+Internals



"
"""Vasili I. Galchin"" <vigalchin@gmail.com>","Fri, 26 Jun 2015 19:04:53 -0700",Re: R - Scala interface used in Spark?,Reynold Xin <rxin@databricks.com>,"thx Reynold!

Vasya


"
Tathagata Das <tdas@databricks.com>,"Sat, 27 Jun 2015 00:32:42 -0700",Re: Time is ugly in Spark Streaming....,=?UTF-8?B?RW1yZWhhbiBUw7x6w7xu?= <emrehan.tuzun@gmail.com>,"Could you print the ""time"" on the driver (that is, in foreachRDD but before
RDD.foreachPartition) and see if it is behaving weird?

TD

il.com>

eachRDD...
oder, StringDecoder](ssc, kafkaParams, topicsSet)
hRDD((rdd, time) => {
erList)
"",""count"":17}
"",""count"":34}
"",""count"":279}
0"",""count"":9}
"",""count"":2958}
"",""count"":114}
0"",""count"":2066}
"",""count"":1539}
"
Dumas Hwang <dumas.hwang@gmail.com>,"Sat, 27 Jun 2015 08:16:22 -0400",Re: Time is ugly in Spark Streaming....,Tathagata Das <tdas@databricks.com>,"Java's SimpleDateFormat is not thread safe.  You can consider using
DateTimeFormatter if you are using Java 8 or Joda-time


mail.com>
reachRDD...
coder, StringDecoder](ssc, kafkaParams, topicsSet)
chRDD((rdd, time) => {
kerList)
0"",""count"":17}
0"",""count"":34}
0"",""count"":279}
""0"",""count"":9}
0"",""count"":2958}
0"",""count"":114}
""0"",""count"":2066}
0"",""count"":1539}
"
"""=?gb18030?B?U2Vh?="" <261810726@qq.com>","Sat, 27 Jun 2015 22:36:10 +0800","=?gb18030?B?u9i4tKO6IFRpbWUgaXMgdWdseSBpbiBTcGFyayBT?=
 =?gb18030?B?dHJlYW1pbmcuLi4u?=","""=?gb18030?B?RHVtYXMgSHdhbmc=?="" <dumas.hwang@gmail.com>, ""=?gb18030?B?VGF0aGFnYXRhIERhcw==?="" <tdas@databricks.com>","Yes , things go well now.  It is a problem of SimpleDateFormat. Thank you all.




------------------ Ô­Ê¼ÓÊ¼þ ------------------
·¢¼þÈË: ""Dumas Hwang"";<dumas.hwang@gmail.com>;
·¢ËÍÊ±¼ä: 2015Äê6ÔÂ27ÈÕ(ÐÇÆÚÁù) ÍíÉÏ8:16
ÊÕ¼þÈË: ""Tathagata Das""<tdas@databricks.com>; 
³­ËÍ: ""Emrehan T¨¹z¨¹n""<emrehan.tuzun@gmail.com>; ""Sea""<261810726@qq.com>; ""dev""<dev@spark.apache.org>; ""user""<user@spark.apache.org>; 
Ö÷Ìâ: Re: Time is ugly in Spark Streaming....



Java's SimpleDateFormat is not thread safe.  You can consider using DateTimeFormatter if you are using Java 8 or Joda-time

On Sat, Jun 27, 2015 at 3:32 AM, Tathagata Das <tdas@databricks.com> wrote:
Could you print the ""time"" on the driver (that is, in foreachRDD but before RDD.foreachPartition) and see if it is behaving weird?

TD


On Fri, Jun 26, 2015 at 3:57 PM, Emrehan T¨¹z¨¹n <emrehan.tuzun@gmail.com> wrote:
 





On Fri, Jun 26, 2015 at 12:30 PM, Sea <261810726@qq.com> wrote:

 Hi, all
 

 I find a problem in spark streaming, when I use the time in function foreachRDD... I find the time is very interesting. 
 val messages = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, topicsSet)
 dataStream.map(x => createGroup(x._2, dimensions)).groupByKey().foreachRDD((rdd, time) => {
try {
if (!rdd.partitions.isEmpty) {
      rdd.foreachPartition(partition => {
handlePartition(partition, timeType, time, dimensions, outputTopic, brokerList)
      })
    }
  } catch {
case e: Exception => e.printStackTrace()
  }
})
 

 val dateFormat = new SimpleDateFormat(""yyyy-MM-dd'T'HH:mm:ss"")
  var date = dateFormat.format(new Date(time.milliseconds)) 
 
 Then I insert the 'date' into Kafka , but I found .....
  

 {""timestamp"":""2015-06-00T16:50:02"",""status"":""3"",""type"":""1"",""waittime"":""0"",""count"":17}
 {""timestamp"":""2015-06-26T16:51:13"",""status"":""1"",""type"":""1"",""waittime"":""0"",""count"":34}
 {""timestamp"":""2015-06-00T16:50:02"",""status"":""4"",""type"":""0"",""waittime"":""0"",""count"":279}
 {""timestamp"":""2015-06-26T16:52:00"",""status"":""11"",""type"":""1"",""waittime"":""0"",""count"":9}
 {""timestamp"":""0020-06-26T16:50:36"",""status"":""7"",""type"":""0"",""waittime"":""0"",""count"":1722}
 {""timestamp"":""2015-06-10T16:51:17"",""status"":""0"",""type"":""0"",""waittime"":""0"",""count"":2958}
 {""timestamp"":""2015-06-26T16:52:00"",""status"":""0"",""type"":""1"",""waittime"":""0"",""count"":114}
 {""timestamp"":""2015-06-10T16:51:17"",""status"":""11"",""type"":""0"",""waittime"":""0"",""count"":2066}
 {""timestamp"":""2015-06-26T16:52:00"",""status"":""1"",""type"":""0"",""waittime"":""0"",""count"":1539}"
Sean Owen <sowen@cloudera.com>,"Sun, 28 Jun 2015 11:27:11 +0100",Unable to add to roles in JIRA,dev <dev@spark.apache.org>,"In case you've tried and failed to add a person to a role in JIRA...
https://issues.apache.org/jira/browse/INFRA-9891

---------------------------------------------------------------------


"
Dogtail Ray <spark.rui92@gmail.com>,"Sun, 28 Jun 2015 11:32:40 -0400",Question about Spark process and thread,dev@spark.apache.org,"Hi,

I was looking at Spark source code, and I found that when launching a
Executor, actually Spark is launching a threadpool; each time the scheduler
launches a task, the executor will launch a thread within the threadpool.

However, I also found that the Spark process always has approximately 40
threads running regardless of my configuration (SPARK_WORKER_CORES,
SPARK_WORKER_INSTANCES, --executor-cores, --total-executor-cores, etc.).
Does it mean Spark will pre-launch 40 threads even before the tasks are
launched? Great thanks!

Best,
Ray
"
Krishna Sankar <ksankar42@gmail.com>,"Sun, 28 Jun 2015 12:14:48 -0700",Re: [VOTE] Release Apache Spark 1.4.1,Patrick Wendell <pwendell@gmail.com>,"Patrick,
   Haven't seen any replies on test results. I will byte ;o) - Should I
test this version or is another one in the wings ?
Cheers
<k/>


"
Patrick Wendell <pwendell@gmail.com>,"Sun, 28 Jun 2015 12:21:41 -0700",Re: [VOTE] Release Apache Spark 1.4.1,Krishna Sankar <ksankar42@gmail.com>,"Hey Krishna - this is still the current release candidate.

- Patrick


---------------------------------------------------------------------


"
Alessandro Baretta <alexbaretta@gmail.com>,"Sun, 28 Jun 2015 18:02:14 -0700",Spark 1.5.0-SNAPSHOT broken with Scala 2.11,"""dev@spark.apache.org"" <dev@spark.apache.org>","I am building the current master branch with Scala 2.11 following these
instructions:

Building for Scala 2.11

To produce a Spark package compiled with Scala 2.11, use the -Dscala-2.11
 property:

dev/change-version-to-2.11.sh
mvn -Pyarn -Phadoop-2.4 -Dscala-2.11 -DskipTests clean package


Here's what I'm seeing:

log4j:WARN No appenders could be found for logger
(org.apache.hadoop.security.Groups).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for
more info.
Using Spark's repl log4j profile:
org/apache/spark/log4j-defaults-repl.properties
To adjust logging level use sc.setLogLevel(""INFO"")
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.5.0-SNAPSHOT
      /_/

Using Scala version 2.10.4 (OpenJDK 64-Bit Server VM, Java 1.7.0_79)
Type in expressions to have them evaluated.
Type :help for more information.
15/06/29 00:42:20 ERROR ActorSystemImpl: Uncaught fatal error from thread
[sparkDriver-akka.remote.default-remote-dispatcher-6] shutting down
ActorSystem [sparkDriver]
java.lang.VerifyError: class akka.remote.WireFormats$AkkaControlMessage
overrides final method
getUnknownFields.()Lcom/google/protobuf/UnknownFieldSet;
at java.lang.ClassLoader.defineClass1(Native Method)
at java.lang.ClassLoader.defineClass(ClassLoader.java:800)
at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
at java.net.URLClassLoader.defineClass(URLClassLoader.java:449)
at java.net.URLClassLoader.access$100(URLClassLoader.java:71)
at java.net.URLClassLoader$1.run(URLClassLoader.java:361)
at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
at java.security.AccessController.doPrivileged(Native Method)
at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
at
akka.remote.transport.AkkaPduProtobufCodec$.constructControlMessagePdu(AkkaPduCodec.scala:231)
at
akka.remote.transport.AkkaPduProtobufCodec$.<init>(AkkaPduCodec.scala:153)
at akka.remote.transport.AkkaPduProtobufCodec$.<clinit>(AkkaPduCodec.scala)
at akka.remote.EndpointManager$$anonfun$9.apply(Remoting.scala:733)
at akka.remote.EndpointManager$$anonfun$9.apply(Remoting.scala:703)

What am I doing wrong?
"
Ted Yu <yuzhihong@gmail.com>,"Sun, 28 Jun 2015 18:30:07 -0700",Re: Spark 1.5.0-SNAPSHOT broken with Scala 2.11,Alessandro Baretta <alexbaretta@gmail.com>,"Spark-Master-Scala211-Compile build is green.

However it is not clear what the actual command is:

[EnvInject] - Variables injected successfully.
[Spark-Master-Scala211-Compile] $ /bin/bash /tmp/hudson8945334776362889961.sh


FYI



"
Josh Rosen <rosenville@gmail.com>,"Sun, 28 Jun 2015 18:34:13 -0700",Re: Spark 1.5.0-SNAPSHOT broken with Scala 2.11,Ted Yu <yuzhihong@gmail.com>,"The 2.11 compile build is going to be green because this is an issue with
tests, not compilation.


"
Debasish Das <debasish.das83@gmail.com>,"Sun, 28 Jun 2015 18:42:38 -0700",Gossip protocol in Master selection,dev <dev@spark.apache.org>,"Hi,

Akka cluster uses gossip protocol for Master election. The approach in
Spark right now is to use Zookeeper for high availability.

Interestingly Cassandra and Redis clusters are both using Gossip protocol.

I am not sure what is the default behavior right now. If the master dies
and zookeeper selects a new master, the whole depedency graph will be
re-executed or only the unfinished stages will be restarted ?

Also why the zookeeper based HA was preferred in Spark ? I was wondering if
there is JIRA to add gossip protocol for Spark Master election ?

In the code I see zookeeper, filesystem, custom and default is
MonarchyLeader. So looks like Spark is designed to add new
leaderElectionAgent.

Thanks.
Deb
"
dobashim <dobashim@oss.nttdata.co.jp>,"Sun, 28 Jun 2015 21:33:56 -0700 (MST)",Re: UnusedStubClass in 1.3.0-rc1,dev@spark.apache.org,"Hi, all

I found the same situation in Spark Streaming + Kafka of 1.4.0.
Are there any progress about this discussion?
(I cannot find JIRA issue about this.)

By the way, I can avoid this kind of error by using ""exclude"" in SBT
configuration of my application
like the following.

::

 libraryDependencies += (""org.apache.spark"" %% ""spark-streaming-kafka"" %
sparkVersion).exclude(""org.spark-project.spark"", ""unused"")

I'm not sure whether this solution is preferred way for all users.



--

---------------------------------------------------------------------


"
Shixiong Zhu <zsxwing@gmail.com>,"Mon, 29 Jun 2015 14:03:55 +0800",Re: Spark 1.5.0-SNAPSHOT broken with Scala 2.11,Alessandro Baretta <alexbaretta@gmail.com>,"Could you update your maven to 3.3.3? I'm not sure if this is a known issue
but the exception message looks same. See
https://github.com/apache/spark/pull/6770

Best Regards,
Shixiong Zhu

2015-06-29 9:02 GMT+08:00 Alessandro Baretta <alexbaretta@gmail.com>:

"
Reynold Xin <rxin@databricks.com>,"Sun, 28 Jun 2015 23:29:46 -0700",Re: Question about Spark process and thread,Dogtail Ray <spark.rui92@gmail.com>,"Most of those threads are not for task execution. They are for RPC,
scheduling, ...



"
Andrew Vykhodtsev <yozhyg@gmail.com>,"Mon, 29 Jun 2015 08:57:20 +0200",Dataframes filter by count fails with python API,dev@spark.apache.org,"Dear developers,

I found the following behaviour that I think is a minor bug.

If I apply groupBy and count in python API,  the resulting data frame has
grouped columns and the field named ""count"". Filtering by that field does
not work because it thinks it is a key word:

x = sc.parallelize(zip(xrange(1000),xrange(1000)))
df = sqlContext.createDataFrame(x)

df.groupBy(""_1"").count().printSchema()

root
 |-- _1: long (nullable = true)
 |-- count: long (nullable = false)


df.groupBy(""_1"").count().filter(""count > 1"")

gives

: java.lang.RuntimeException: [1.7] failure: ``('' expected but `>' found

count > 1
      ^
	at scala.sys.package$.error(package.scala:27)



the following syntax works :

f = df.groupBy(""_1"").count()
n = f.filter(f[""count""] > 1)

In Scala, referring to $""count"" column works as well.

please let me know if I should submit a JIRA for this.
"
Tathagata Das <tathagata.das1565@gmail.com>,"Mon, 29 Jun 2015 00:04:10 -0700",Re: [VOTE] Release Apache Spark 1.4.1,Patrick Wendell <pwendell@gmail.com>,"@Ted, could you elaborate more on what was the test command that you ran?
What profiles, using SBT or Maven?

TD


"
Reynold Xin <rxin@databricks.com>,"Mon, 29 Jun 2015 00:04:52 -0700",Re: Dataframes filter by count fails with python API,Andrew Vykhodtsev <yozhyg@gmail.com>,"Hi Andrew,

Thanks for the email. This is a known bug with the expression parser. We
will hopefully fix this in 1.5.

There are more keywords with the expression parser, and we have already got
rid of most of them. Count is still there due to the handling of count
distinct, but we plan to get rid of that as well.




"
"""Vasili I. Galchin"" <vigalchin@gmail.com>","Mon, 29 Jun 2015 01:33:37 -0700","Re: how can I write a language ""wrapper""?",shivaram@eecs.berkeley.edu,"Shivaram,

    Vis-a-vis Haskell support, I am reading DataFrame.R,
SparkRBackend*, context.R, et. al., am I headed in the correct
direction?/ Yes or no, please give more guidance. Thank you.

Kind regards,

Vasili




---------------------------------------------------------------------


"
=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"Mon, 29 Jun 2015 12:27:23 +0200",Re: Spark 1.5.0-SNAPSHOT broken with Scala 2.11,Alessandro Baretta <alexbaretta@gmail.com>,"

Something is deeply wrong with your build.

iulian





-- 

--
Iulian Dragos

------
Reactive Apps on the JVM
www.typesafe.com
"
Daniel Darabos <daniel.darabos@lynxanalytics.com>,"Mon, 29 Jun 2015 12:38:29 +0200","Re: how can I write a language ""wrapper""?","""Vasili I. Galchin"" <vigalchin@gmail.com>","Hi Vasili,
It so happens that the entire SparkR code was merged to Apache Spark in a
single pull request. So you can see at once all the required changes in
https://github.com/apache/spark/pull/5096. It's 12,043 lines and took more
than 20 people about a year to write as I understand it.


"
Tom Graves <tgraves_cs@yahoo.com.INVALID>,"Mon, 29 Jun 2015 13:18:50 +0000 (UTC)",Re: [VOTE] Release Apache Spark 1.4.1,"Tathagata Das <tathagata.das1565@gmail.com>, 
	Patrick Wendell <pwendell@gmail.com>","+1. Tested on yarn on hadoop 2.6 cluster
Tom 


   

 @Ted, could you elaborate more on what was the test command that you ran? What profiles, using SBT or Maven?Â 
TD
e:

Hey Krishna - this is still the current release candidate.

- Patrick

e:
Should I "
Sean Owen <sowen@cloudera.com>,"Mon, 29 Jun 2015 14:51:19 +0100",Re: [VOTE] Release Apache Spark 1.4.1,Patrick Wendell <pwendell@gmail.com>,"+1 sigs, license, etc check out.

All tests pass for me in the Hadoop 2.6 + Hive configuration on Ubuntu.
(I still get those pesky cosmetic UDF test failures in Java 8, but
they are clearly just test issues.)

I'll follow up on retargeting 1.4.1 issues af"
Steve Loughran <stevel@hortonworks.com>,"Mon, 29 Jun 2015 14:37:17 +0000",Re: Spark 1.5.0-SNAPSHOT broken with Scala 2.11,"=?utf-8?B?SXVsaWFuIERyYWdvyJk=?= <iulian.dragos@typesafe.com>, ""Alessandro
 Baretta"" <alexbaretta@gmail.com>","
On 29 Jun 2015, at 11:27, Iulian DragoÈ™ <iulian.dragos@typesafe.com<mailto:iulian.dragos@typesafe.com>> wrote:



On Mon, Jun 29, 2015 at 3:02 AM, Alessandro Baretta <alexbaretta@gmail.com<mailto:alexbaretta@gmail.com>> wrote:
I am building the current master branch with Scala 2.11 following these instructions:



Type :help for more information.
15/06/29 00:42:20 ERROR ActorSystemImpl: Uncaught fatal error from thread [sparkDriver-akka.remote.default-remote-dispatcher-6] shutting down ActorSystem [sparkDriver]
java.lang.VerifyError: class akka.remote.WireFormats$AkkaControlMessage overrides final method getUnknownFields.()Lcom/google/protobuf/UnknownFieldSet;
at java.lang.ClassLoader.defineClass1(Native Method)
at java.lang.ClassLoader.defineClass(ClassLoader.java:800)
at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
at java.net.URLClassLoader.defineClass(URLClassLoader.java:449)
at java.net.URLClassLoader.access$100(URLClassLoader.java:71)
at java.net.URLClassLoader$1.run(URLClassLoader.java:361)
at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
at java.security.AccessController.doPrivileged(Native Method)
at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
at akka.remote.transport.AkkaPduProtobufCodec$.constructControlMessagePdu(AkkaPduCodec.scala:231)
at akka.remote.transport.AkkaPduProtobufCodec$.<init>(AkkaPduCodec.scala:153)
at akka.remote.transport.AkkaPduProtobufCodec$.<clinit>(AkkaPduCodec.scala)
at akka.remote.EndpointManager$$anonfun$9.apply(Remoting.scala:733)
at akka.remote.EndpointManager$$anonfun$9.apply(Remoting.scala:703)

What am I doing wrong?




oh, that's just the version of the protoc protobuf compiler generating code that implementation classes aren't compatible with, and/or the version of probuf.jar on the classpath. Google's libraries are turning out to be surprisingly brittle that way.

when you type  protoc --version on the command line, you should expect to see, ""libprotoc 2.5.0'; and have protobuf-2.5.0 on the classpath. If neither of those conditions are met: fix them

-Steve

"
Yin Huai <yhuai@databricks.com>,"Mon, 29 Jun 2015 11:39:06 -0700",Re: [VOTE] Release Apache Spark 1.4.1,Sean Owen <sowen@cloudera.com>,"+1. I tested those SQL blocker bugs in my laptop and they have been fixed.


"
Ted Yu <yuzhihong@gmail.com>,"Mon, 29 Jun 2015 11:41:27 -0700",Re: [VOTE] Release Apache Spark 1.4.1,Tathagata Das <tathagata.das1565@gmail.com>,"Here is the command I used:
mvn -Phadoop-2.4 -Dhadoop.version=2.7.0 -Pyarn -Phive package

Java: 1.8.0_45

OS:
Linux x.com 2.6.32-504.el6.x86_64 #1 SMP Wed Oct 15 04:27:16 UTC 2014
x86_64 x86_64 x86_64 GNU/Linux

Cheers


"
Krishna Sankar <ksankar42@gmail.com>,"Mon, 29 Jun 2015 13:05:18 -0700",Re: [VOTE] Release Apache Spark 1.4.1,Patrick Wendell <pwendell@gmail.com>,"+1 (non-binding, of course)

1. Compiled OSX 10.10 (Yosemite) OK Total time: 13:26 min
     mvn clean package -Pyarn -Phadoop-2.6 -DskipTests
2. Tested pyspark, mllib
2.1. statistics (min,max,mean,Pearson,Spearman) OK
2.2. Linear/Ridge/Laso Regression OK
"
Tathagata Das <tathagata.das1565@gmail.com>,"Mon, 29 Jun 2015 14:01:32 -0700",Re: [VOTE] Release Apache Spark 1.4.1,Krishna Sankar <ksankar42@gmail.com>,"@Ted, I ran the following two commands.

mvn -Phadoop-2.4 -Dhadoop.version=2.7.0 -Pyarn -Phive -DskipTests clean
package
mvn -Phadoop-2.4 -Dhadoop.version=2.7.0 -Pyarn -Phive
-DwildcardSuites=org.apache.spark.streaming.StreamingContextSuite test

Using Ja"
Ted Yu <yuzhihong@gmail.com>,"Mon, 29 Jun 2015 14:29:23 -0700",Re: [VOTE] Release Apache Spark 1.4.1,Tathagata Das <tathagata.das1565@gmail.com>,"The test passes when run alone on my machine as well.

Please run test suite.

Thanks


"
Andrew Or <andrew@databricks.com>,"Mon, 29 Jun 2015 15:33:13 -0700",Re: [VOTE] Release Apache Spark 1.4.1,Ted Yu <yuzhihong@gmail.com>,"Hi Ted,

We haven't observed a StreamingContextSuite failure on our test
infrastructure recently. Given that we cannot reproduce it even locally it
is unlikely that this uncovers a real bug. Even if it does I would not
block the release on it because many in the community are waiting for a few
important fixes. In general, there will always be outstanding issues in
Spark that we cannot address in every release.

-Andrew

2015-06-29 14:29 GMT-07:00 Ted Yu <yuzhihong@gmail.com>:

"
Ted Yu <yuzhihong@gmail.com>,"Mon, 29 Jun 2015 15:35:21 -0700",Re: [VOTE] Release Apache Spark 1.4.1,Andrew Or <andrew@databricks.com>,"Andrew:
I agree with your assessment.

Cheers


"
Alessandro Baretta <alexbaretta@gmail.com>,"Mon, 29 Jun 2015 15:41:27 -0700",Re: Spark 1.5.0-SNAPSHOT broken with Scala 2.11,Steve Loughran <stevel@hortonworks.com>,"Steve,

It was indeed a protocol buffers issue. I am able to build spark now.
Thanks.


m
d
2)
kkaPduCodec.scala:231)
3)
on
"
Justin Uang <justin.uang@gmail.com>,"Tue, 30 Jun 2015 02:04:45 +0000","Re: how can I write a language ""wrapper""?","Daniel Darabos <daniel.darabos@lynxanalytics.com>, 
	""Vasili I. Galchin"" <vigalchin@gmail.com>","My guess is that if you are just wrapping the spark sql APIs, you can get
away with not having to reimplement a lot of the complexities in Pyspark
like storing everything in RDDs as pickled byte arrays, pipelining RDDs,
doing aggregations and joins in the python interpreters, etc.

Since the canonical representation of objects in Spark SQL is in scala/jvm,
you're effectively just proxying calls to the java side. The only tricky
thing is UDFs, which naturally need to run in an interpreter of the wrapper
language. I'm currently thinking of redesigning the UDFs to be sent in a
language agnostic data format like protobufs or msgpack, so that all
language wrappers just need to implement the simple protocol of reading
those in, transforming it, then outputting it back as that language
agnostic format.


"
"""York, Brennon"" <Brennon.York@capitalone.com>","Tue, 30 Jun 2015 10:51:16 -0400",Re: [VOTE] Release Apache Spark 1.4.1,"Patrick Wendell <pwendell@gmail.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","+1 (non-binding)

* built Spark on OSX10.10 w/java version 1.8.0_45 via `dev/run-tests`
  * all tests ran successfully
* ran spark-on-YARN core ETL pipeline (success)
  * no regression issues / performance issues
* ran spark-on-YARN MLLib ALS recommendati"
=?UTF-8?Q?Zolt=C3=A1n_Zvara?= <zoltan.zvara@gmail.com>,"Tue, 30 Jun 2015 14:59:21 +0000",DStream.reduce,"""dev@spark.apache.org"" <dev@spark.apache.org>","Why is reduce in DStream implemented with a map, reduceByKey and another
map, given that we have an RDD.reduce?
"
rake <randy@randykerber.com>,"Tue, 30 Jun 2015 08:10:16 -0700 (MST)",Re: [DataFrame] partitionBy issues,dev@spark.apache.org,"I ran into a similar problem, reading a csv file into a DataFrame and saving
to Parquet with 'partitionBy', and getting OutOfMemory error even though
it's not a large data file.

I discovered that by default Spark appears to be allocating a block of 128MB
in memory for each output Parquet partition, controlled by a
""parquet.block.size"" parameter.  So if there are lots of Parquet partitions,
it's easy to quickly run out of memory even if the actual amount of data is
small.

I tried repeatedly to override the 128MB default by setting properties with
names like ""block.size"", ""parquet.block.size"",
""spark.sql.parquet.block.size"", etc., to SparkContext, SQLContext, and as an
""option()"" to DataFrameWriter when calling DataFrameWriter.parquet.  None of
those worked.  No matter what, my setting was ignored and the default value
of 128MB was used (according the log from using spark-shell).

Eventually I found a pointer to sparkContext.hadoopConfiguration. For
example, to set the block size to 16MB try:

    sparkContext.hadoopConfiguration.setInt( ""dfs.blocksize"", 1024 * 1024 *
16 )
    sparkContext.hadoopConfiguration.setInt( ""parquet.block.size"", 1024 *
1024 * 16 )

That worked.  Though it seems like some of the other options I tried (e.g.,
""spark.sql.parquet.block.size"") ought to be supported.

-- Randy

Randy Kerber
Data Science Consultant


vladio wrote
y
t
f
r
â€)
ble.csv>  





--
3.nabble.com/DataFrame-partitionBy-issues-tp12838p12970.html
om.

---------------------------------------------------------------------


"
RJ Nowling <rnowling@gmail.com>,"Tue, 30 Jun 2015 13:01:57 -0500",Grouping runs of elements in a RDD,"""dev@spark.apache.org"" <dev@spark.apache.org>, ""user@spark.apache.org"" <user@spark.apache.org>","Hi all,

I have a problem where I have a RDD of elements:

Item1 Item2 Item3 Item4 Item5 Item6 ...

and I want to run a function over them to decide which runs of elements to
group together:

[Item1 Item2] [Item3] [Item4 Item5 Item6] ...

Technically, I could use aggregate to do this, but I would have to use a
List of List of T which would produce a very large collection in memory.

Is there an easy way to accomplish this?  e.g.,, it would be nice to have a
version of aggregate where the combination function can return a complete
group that is added to the new RDD and an incomplete group which is passed
to the next call of the reduce function.

Thanks,
RJ
"
Reynold Xin <rxin@databricks.com>,"Tue, 30 Jun 2015 11:03:21 -0700",Re: Grouping runs of elements in a RDD,RJ Nowling <rnowling@gmail.com>,"Try mapPartitions, which gives you an iterator, and you can produce an
iterator back.



"
vladio <vladio@palantir.com>,"Tue, 30 Jun 2015 11:19:15 -0700 (MST)",Re: [DataFrame] partitionBy issues,dev@spark.apache.org,"https://issues.apache.org/jira/browse/SPARK-8597

A JIRA ticket discussing the same problem (with more insights than here)!



--

---------------------------------------------------------------------


"
RJ Nowling <rnowling@gmail.com>,"Tue, 30 Jun 2015 14:00:38 -0500",Re: Grouping runs of elements in a RDD,Reynold Xin <rxin@databricks.com>,"Thanks, Reynold.  I still need to handle incomplete groups that fall
between partition boundaries. So, I need a two-pass approach. I came up
with a somewhat hacky way to handle those using the partition indices and
key-value pairs as a second pass after the first.

OCaml's std library provides a function called group() that takes a break
function that operators on pairs of successive elements.  It seems a
similar approach could be used in Spark and would be more efficient than my
approach with key-value pairs since you know the ordering of the partitions.

Has this need been expressed by others?


"
"""Abhishek R. Singh"" <abhishsi@tetrationanalytics.com>","Tue, 30 Jun 2015 12:07:20 -0700",Re: Grouping runs of elements in a RDD,RJ Nowling <rnowling@gmail.com>,"could you use a custom partitioner to preserve boundaries such that all related tuples end up on the same partition?


between partition boundaries. So, I need a two-pass approach. I came up with a somewhat hacky way to handle those using the partition indices and key-value pairs as a second pass after the first.
break function that operators on pairs of successive elements.  It seems a similar approach could be used in Spark and would be more efficient than my approach with key-value pairs since you know the ordering of the partitions.
iterator back.
elements to group together:
a List of List of T which would produce a very large collection in memory.
have a version of aggregate where the combination function can return a complete group that is added to the new RDD and an incomplete group which is passed to the next call of the reduce function.

"
RJ Nowling <rnowling@gmail.com>,"Tue, 30 Jun 2015 14:21:14 -0500",Re: Grouping runs of elements in a RDD,"""Abhishek R. Singh"" <abhishsi@tetrationanalytics.com>","That's an interesting idea!  I hadn't considered that.  However, looking at
the Partitioner interface, I would need to know from looking at a single
key which doesn't fit my case, unfortunately.  For my case, I need to
compare successive pairs of keys.  (I'm trying to re-join lines that were
split prematurely.)


"
