forough <foroughi.forough@gmail.com>,"Sat, 1 Nov 2014 01:53:11 -0700 (PDT)",deterministic D-Stream mode,dev@spark.incubator.apache.org,"Hi all, 
I have 2 questions:

1. I found out the meaning of ""deterministic"" in D-Stream model is :
given a particular input, will always produce the same output that is
necessary for recomputation in lineage.
Is it right? 

2. Why spark stream needs to replicate input data for making deterministic
state? what is the relation between deterministic tasks or operations and
replication?

Thanks.



--

---------------------------------------------------------------------


"
"""Arthur.hk.chan@gmail.com"" <arthur.hk.chan@gmail.com>","Sat, 1 Nov 2014 17:00:21 +0800",Re: Surprising Spark SQL benchmark,Kay Ousterhout <keo@eecs.berkeley.edu>,"Hi Key,

Thank you so much for your update!!
Look forward to the shared code from AMPLab.  As a member of the Spark community, I really hope that I could help to run TPC-DS on SparkSQL.  At the moment, I am trying TPC-H 22 queries on SparkSQL 1.1.0 +Hive 0.12, and Hive 0.13.1 respectively (waiting Spark 1.2).

Arthur  


something
that
the
to
this is
laid
this:
allow
I
since
behind
this
Steve Nunez<snunez@hortonworks.com>ë‹˜ì´ ì‘ì„±í•œ ë©”ì‹œì§€:
for example
https://databricks.com/blog/2014/10/10/spark-petabyte-sort.html
future
reproducible
(didn't
Patrick Wendell<pwendell@gmail.com <javascript:;>>ë‹˜ì´
SQL
improvement.
does
broadcast
you
to
posts
in
entity
confidential,
reader
notified


---------------------------------------------------------------------


"
RJ Nowling <rnowling@gmail.com>,"Sat, 1 Nov 2014 09:57:03 -0400",Re: Surprising Spark SQL benchmark,Nicholas Chammas <nicholas.chammas@gmail.com>,"Two thoughts here:

1. The real flaw with the sort benchmark was that Hadoop wasn't run on the
same hardware. Given the advances in networking (availabIlity of
10GB Ethernet) and disks (SSDs) since the Hadoop benchmarks it was compared
to, it's an apples to oranges comparison. Without that, it doesn't tell me
whether the improvement is due to Spark or just hardware.

To me, that's the biggest flaw -- not the reproducibility of it. As you
say, most people won't have the financial means to access those resources
to reproduce it.

And that's the same sort of flaw every other marketing benchmark has --
apples to oranges comparisons.

2. The BDD benchmark is hard to run outside of EC2 and I and other users
were not able to access all of the data via S3.  I could reproduce some of
the data using HiBench but not the web corpus sub sample. As a result, for
all the hard work put into documenting it, it's still hard to reproduce :(


is
d
e
Nunez<snunez@hortonworks.com <javascript:;>>ë‹˜ì´
xample
e
rick Wendell<pwendell@gmail.com <javascript:;>
L
ou
s
n
y
er


-- 
em rnowling@gmail.com
c 954.496.2314
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sat, 1 Nov 2014 13:48:36 -0400",Re: Surprising Spark SQL benchmark,RJ Nowling <rnowling@gmail.com>,"Good points raised. Some comments.

Re: #1

It seems like there is a misunderstanding of the purpose of the Daytona
Gray benchmark. The purpose of the benchmark is to see how fast you can
sort 100 TB of data (technically, your sort rate during the operation)
using *any* hardware or software config, within the common rules laid out
at http://sortbenchmark.org

Though people will naturally want to compare one benchmarked system to
another, the Gray benchmark does not control the hardware to make such a
comparison useful. So you're right that it's apples to oranges to compare
Databricks's Spark run to Yahoo's Hadoop run in this type of benchmark, but
that's just inherent to the definition of the benchmark. I wouldn't fault
Databricks or Yahoo for this.

That said, it's nice that Databricks went with a public cloud to do this
benchmark, which makes it more likely that future benchmarks done on the
same cloud can be compared meaningfully. The same can't be said of Yahoo's
benchmark, for example, which was done in private datacenter.

Re: #2

EC2 is a good place to run a reproducible benchmark since it's publicly
accessible and the instance types are well defined. If you had trouble
reproducing the AMPLab benchmark there, I would raise that with the AMPLab
team. I'd assume they would be interested in correcting any problems with
reproducing it, as it definitely detracts from the value of the benchmark.

Nick


2014ë…„ 11ì›” 1ì¼ í† ìš”ì¼, RJ Nowling<rnowling@gmail.com>ë‹˜ì´ ì‘ì„±í•œ ë©”ì‹œì§€:

e
ed
e
f
r
(
id
:
ce
 Nunez<snunez@hortonworks.com>ë‹˜ì´ ì‘ì„±í•œ ë©”ì‹œì§€:
example
l
re
trick Wendell<pwendell@gmail.com
QL
.
s
t
ts
in
,
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sat, 1 Nov 2014 13:50:47 -0400",Re: Surprising Spark SQL benchmark,Kay Ousterhout <keo@eecs.berkeley.edu>,"Kay,

Is this effort related to the existing AMPLab Big Data benchmark that
covers Spark, Redshift, Tez, and Impala?

Nick


2014ë…„ 10ì›” 31ì¼ ê¸ˆìš”ì¼, Kay Ousterhout<keo@eecs.berkeley.edu>ë‹˜ì´ ì‘ì„±í•œ ë©”ì‹œì§€:

ng
t
id
:
ce
 Nunez<snunez@hortonworks.com
´ ì‘ì„±í•œ ë©”ì‹œì§€:
example
l
re
trick Wendell<pwendell@gmail.com
‹˜ì´
QL
.
s
t
ts
in
,
"
Kay Ousterhout <keo@eecs.berkeley.edu>,"Sat, 1 Nov 2014 12:32:09 -0700",Re: Surprising Spark SQL benchmark,Nicholas Chammas <nicholas.chammas@gmail.com>,"Hi Nick,

No -- we're doing a much more constrained thing of just trying to get
things set up to easily run TPC-DS on SparkSQL (which involves generating
the data, storing it in HDFS, getting all the queries in the right format,
etc.).
Cloudera does have a repo here: https://github.com/cloudera/impala-tpcds-kit
that we've found helpful in running TPC-DS on Hive (you should also be able
to use that repo to run TPC-DS on Impala, although we haven't actually done
this).

-Kay


sterhout<keo@eecs.berkeley.edu>ë‹˜ì´ ì‘ì„±í•œ ë©”ì‹œì§€:
ing
at
s
s:
w
e Nunez<snunez@hortonworks.com>ë‹˜ì´ ì‘ì„±í•œ ë©”ì‹œì§€:
 example
t
atrick Wendell<pwendell@gmail.com
t.
es
st
o
l,
d
"
Patrick Wendell <pwendell@gmail.com>,"Sat, 1 Nov 2014 13:17:10 -0700",Changes to Spark's networking subsystem,"""dev@spark.apache.org"" <dev@spark.apache.org>","== Short version ==
A recent commit replaces Spark's networking subsystem with one based
on Netty rather than raw sockets. Users running off of master can
disable this change by setting
""spark.shuffle.blockTransferService=nio"". We will be testing with thi"
Sean Owen <sowen@cloudera.com>,"Sun, 2 Nov 2014 18:34:49 +0100",OOM when making bins in BinaryClassificationMetrics ?,dev <dev@spark.apache.org>,"This might be a question for Xiangrui. Recently I was using
BinaryClassificationMetrics to build an AUC curve for a classifier
over a reasonably large number of points (~12M). The scores were all
probabilities, so tended to be almost entirely unique.

The computation does some operations by key, and this ran out of
memory. It's something you can solve with more than the default amount
of memory, but in this case, it seemed unuseful to create an AUC curve
with such fine-grained resolution.

I ended up just binning the scores so there were ~1000 unique values
and then it was fine.

Does that sound generally useful as some kind of parameter? or am I
missing a trick here.

Sean

---------------------------------------------------------------------


"
Xiangrui Meng <mengxr@gmail.com>,"Sun, 2 Nov 2014 10:44:25 -0800",Re: OOM when making bins in BinaryClassificationMetrics ?,Sean Owen <sowen@cloudera.com>,"Yes, if there are many distinct values, we need binning to compute the
AUC curve. Usually, the scores are not evenly distribution, we cannot
simply truncate the digits. Estimating the quantiles for binning is
necessary, similar to RangePartitioner:
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/Partitioner.scala#L104
. Limiting the number of bins is definitely useful. Do you have time
to work on it? -Xiangrui


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Sun, 2 Nov 2014 19:45:47 +0100",Re: OOM when making bins in BinaryClassificationMetrics ?,Xiangrui Meng <mengxr@gmail.com>,"Agree, just rounding only makes sense if the values are sort of evenly
distributed -- in my case they were in 0,1. I will put it on my to-do
list to look at, yes. Thanks for the confirmation.


---------------------------------------------------------------------


"
Imran Rashid <imran@therashids.com>,"Sun, 2 Nov 2014 21:25:46 -0600",sbt scala compiler crashes on spark-sql,dev@spark.incubator.apache.org,"I'm finding the scala compiler crashes when I compile the spark-sql project
in sbt.  This happens in both the 1.1 branch and master (full error
below).  The other projects build fine in sbt, and everything builds fine
in maven.  is there some sbt option I'm forgetting?  Any one else
experiencing this?

Also, are there up-to-date instructions on how to do common dev tasks in
both sbt & maven?  I have only found these instructions on building with
maven:

http://spark.apache.org/docs/latest/building-with-maven.html

and some general info here:

https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark

but I think this doesn't walk through a lot of the steps of a typical dev
cycle, eg, continuous compilation, running one test, running one main
class, etc.  (especially since it seems like people still favor sbt for
dev.)  If it doesn't already exist somewhere, I could try to put together a
brief doc for how to do the basics.  (I'm returning to spark dev after a
little hiatus myself, and I'm hitting some stumbling blocks that are
probably common knowledge to everyone still dealing with it all the time.)

thanks,
Imran

------------------------------
full crash info from sbt:

[info] Set current project to spark-sql (in build
file:/Users/imran/spark/spark/)
[info] Compiling 62 Scala sources to
/Users/imran/spark/spark/sql/catalyst/target/scala-2.10/classes...
[info] Compiling 45 Scala sources and 39 Java sources to
/Users/imran/spark/spark/sql/core/target/scala-2.10/classes...
[error]
[error]      while compiling:
/Users/imran/spark/spark/sql/core/src/main/scala/org/apache/spark/sql/types/util/DataTypeConversions.scala
[error]         during phase: jvm
[error]      library version: version 2.10.4
[error]     compiler version: version 2.10.4
[error]   reconstructed args: -classpath
/Users/imran/spark/spark/sql/core/target/scala-2.10/classes:/Users/imran/spark/spark/core/target/scala-2.10/classes:/Users/imran/spark/spark/sql/catalyst/target/scala-2.10/classes:/Users/imran/spark/spark/lib_managed/jars/hadoop-client-1.0.4.jar:/Users/imran/spark/spark/lib_managed/jars/hadoop-core-1.0.4.jar:/Users/imran/spark/spark/lib_managed/jars/xmlenc-0.52.jar:/Users/imran/spark/spark/lib_managed/jars/commons-math-2.1.jar:/Users/imran/spark/spark/lib_managed/jars/commons-configuration-1.6.jar:/Users/imran/spark/spark/lib_managed/jars/commons-collections-3.2.1.jar:/Users/imran/spark/spark/lib_managed/jars/commons-lang-2.4.jar:/Users/imran/spark/spark/lib_managed/jars/commons-logging-1.1.1.jar:/Users/imran/spark/spark/lib_managed/jars/commons-digester-1.8.jar:/Users/imran/spark/spark/lib_managed/jars/commons-beanutils-1.7.0.jar:/Users/imran/spark/spark/lib_managed/jars/commons-beanutils-core-1.8.0.jar:/Users/imran/spark/spark/lib_managed/jars/commons-net-2.2.jar:/Users/imran/spark/spark/lib_managed/jars/commons-el-1.0.jar:/Users/imran/spark/spark/lib_managed/jars/hsqldb-1.8.0.10.jar:/Users/imran/spark/spark/lib_managed/jars/oro-2.0.8.jar:/Users/imran/spark/spark/lib_managed/jars/jets3t-0.7.1.jar:/Users/imran/spark/spark/lib_managed/jars/commons-httpclient-3.1.jar:/Users/imran/spark/spark/lib_managed/bundles/curator-recipes-2.4.0.jar:/Users/imran/spark/spark/lib_managed/bundles/curator-framework-2.4.0.jar:/Users/imran/spark/spark/lib_managed/bundles/curator-client-2.4.0.jar:/Users/imran/spark/spark/lib_managed/jars/zookeeper-3.4.5.jar:/Users/imran/spark/spark/lib_managed/jars/slf4j-log4j12-1.7.5.jar:/Users/imran/spark/spark/lib_managed/bundles/log4j-1.2.17.jar:/Users/imran/spark/spark/lib_managed/jars/jline-0.9.94.jar:/Users/imran/spark/spark/lib_managed/bundles/guava-14.0.1.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-plus-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/orbits/javax.transaction-1.1.1.v201105210645.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-webapp-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-xml-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-util-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-servlet-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-security-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-server-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/orbits/javax.servlet-3.0.0.v201112011016.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-continuation-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-http-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-io-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-jndi-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/orbits/javax.mail.glassfish-1.4.1.v201005082020.jar:/Users/imran/spark/spark/lib_managed/orbits/javax.activation-1.1.0.v201105071233.jar:/Users/imran/spark/spark/lib_managed/jars/commons-lang3-3.3.2.jar:/Users/imran/spark/spark/lib_managed/jars/jsr305-1.3.9.jar:/Users/imran/spark/spark/lib_managed/jars/slf4j-api-1.7.5.jar:/Users/imran/spark/spark/lib_managed/jars/jul-to-slf4j-1.7.5.jar:/Users/imran/spark/spark/lib_managed/jars/jcl-over-slf4j-1.7.5.jar:/Users/imran/spark/spark/lib_managed/bundles/compress-lzf-1.0.0.jar:/Users/imran/spark/spark/lib_managed/bundles/snappy-java-1.0.5.3.jar:/Users/imran/spark/spark/lib_managed/jars/lz4-1.2.0.jar:/Users/imran/spark/spark/lib_managed/jars/chill_2.10-0.3.6.jar:/Users/imran/spark/spark/lib_managed/jars/chill-java-0.3.6.jar:/Users/imran/spark/spark/lib_managed/bundles/kryo-2.21.jar:/Users/imran/spark/spark/lib_managed/jars/reflectasm-1.07-shaded.jar:/Users/imran/spark/spark/lib_managed/jars/minlog-1.2.jar:/Users/imran/spark/spark/lib_managed/jars/objenesis-1.2.jar:/Users/imran/spark/spark/lib_managed/bundles/akka-remote_2.10-2.2.3-shaded-protobuf.jar:/Users/imran/spark/spark/lib_managed/jars/akka-actor_2.10-2.2.3-shaded-protobuf.jar:/Users/imran/spark/spark/lib_managed/bundles/config-1.0.2.jar:/Users/imran/spark/spark/lib_managed/bundles/netty-3.6.6.Final.jar:/Users/imran/spark/spark/lib_managed/jars/protobuf-java-2.4.1-shaded.jar:/Users/imran/spark/spark/lib_managed/jars/uncommons-maths-1.2.2a.jar:/Users/imran/spark/spark/lib_managed/bundles/akka-slf4j_2.10-2.2.3-shaded-protobuf.jar:/Users/imran/spark/spark/lib_managed/jars/json4s-jackson_2.10-3.2.10.jar:/Users/imran/spark/spark/lib_managed/jars/json4s-core_2.10-3.2.10.jar:/Users/imran/spark/spark/lib_managed/jars/json4s-ast_2.10-3.2.10.jar:/Users/imran/spark/spark/lib_managed/jars/paranamer-2.6.jar:/Users/imran/spark/spark/lib_managed/jars/scalap-2.10.0.jar:/Users/imran/spark/spark/lib_managed/bundles/jackson-databind-2.3.1.jar:/Users/imran/spark/spark/lib_managed/bundles/jackson-annotations-2.3.0.jar:/Users/imran/spark/spark/lib_managed/bundles/jackson-core-2.3.1.jar:/Users/imran/spark/spark/lib_managed/jars/colt-1.2.0.jar:/Users/imran/spark/spark/lib_managed/jars/concurrent-1.3.4.jar:/Users/imran/spark/spark/lib_managed/jars/mesos-0.18.1-shaded-protobuf.jar:/Users/imran/spark/spark/lib_managed/jars/netty-all-4.0.23.Final.jar:/Users/imran/spark/spark/lib_managed/jars/stream-2.7.0.jar:/Users/imran/spark/spark/lib_managed/bundles/metrics-core-3.0.0.jar:/Users/imran/spark/spark/lib_managed/bundles/metrics-jvm-3.0.0.jar:/Users/imran/spark/spark/lib_managed/bundles/metrics-json-3.0.0.jar:/Users/imran/spark/spark/lib_managed/bundles/metrics-graphite-3.0.0.jar:/Users/imran/spark/spark/lib_managed/jars/tachyon-client-0.5.0.jar:/Users/imran/spark/spark/lib_managed/jars/tachyon-0.5.0.jar:/Users/imran/spark/spark/lib_managed/jars/commons-io-2.4.jar:/Users/imran/spark/spark/lib_managed/jars/pyrolite-2.0.1.jar:/Users/imran/spark/spark/lib_managed/jars/py4j-0.8.2.1.jar:/Users/imran/.sbt/boot/scala-2.10.4/lib/scala-compiler.jar:/Users/imran/.sbt/boot/scala-2.10.4/lib/scala-reflect.jar:/Users/imran/spark/spark/lib_managed/jars/quasiquotes_2.10-2.0.1.jar:/Users/imran/spark/spark/lib_managed/jars/parquet-column-1.4.3.jar:/Users/imran/spark/spark/lib_managed/jars/parquet-common-1.4.3.jar:/Users/imran/spark/spark/lib_managed/jars/parquet-encoding-1.4.3.jar:/Users/imran/spark/spark/lib_managed/jars/parquet-generator-1.4.3.jar:/Users/imran/spark/spark/lib_managed/jars/commons-codec-1.5.jar:/Users/imran/spark/spark/lib_managed/jars/parquet-hadoop-1.4.3.jar:/Users/imran/spark/spark/lib_managed/jars/parquet-format-2.0.0.jar:/Users/imran/spark/spark/lib_managed/jars/parquet-jackson-1.4.3.jar:/Users/imran/spark/spark/lib_managed/jars/jackson-mapper-asl-1.9.11
.jar:/Users/imran/spark/spark/lib_managed/jars/jackson-core-asl-1.9.11.jar
-deprecation -feature
-P:genjavadoc:out=/Users/imran/spark/spark/sql/core/target/java
-Xplugin:/Users/imran/spark/spark/lib_managed/jars/genjavadoc-plugin_2.10.4-0.7.jar
-bootclasspath
/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/resources.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/sunrsasign.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/classes:/Users/imran/.sbt/boot/scala-2.10.4/lib/scala-library.jar
-unchecked -language:postfixOps
[error]
[error]   last tree to typer:
Literal(Constant(org.apache.spark.sql.catalyst.types.PrimitiveType))
[error]               symbol: null
[error]    symbol definition: null
[error]                  tpe:
Class(classOf[org.apache.spark.sql.catalyst.types.PrimitiveType])
[error]        symbol owners:
[error]       context owners: anonymous class anonfun$asScalaDataType$1 ->
package util
[error]
[error] == Enclosing template or block ==
[error]
[error] Template( // val <local $anonfun>: <notype>,
tree.tpe=org.apache.spark.sql.types.util.anonfun$asScalaDataType$1
[error]   ""scala.runtime.AbstractFunction1"", ""scala.Serializable"" // parents
[error]   ValDef(
[error]     private
[error]     ""_""
[error]     <tpt>
[error]     <empty>
[error]   )
[error]   // 3 statements
[error]   DefDef( // final def apply(javaStructField:
org.apache.spark.sql.api.java.StructField):
org.apache.spark.sql.catalyst.types.StructField
[error]     <method> final <triedcooking>
[error]     ""apply""
[error]     []
[error]     // 1 parameter list
[error]     ValDef( // javaStructField:
org.apache.spark.sql.api.java.StructField
[error]       <param> <synthetic> <triedcooking>
[error]       ""javaStructField""
[error]       <tpt> // tree.tpe=org.apache.spark.sql.api.java.StructField
[error]       <empty>
[error]     )
[error]     <tpt> //
tree.tpe=org.apache.spark.sql.catalyst.types.StructField
[error]     Apply( // def asScalaStructField(javaStructField:
org.apache.spark.sql.api.java.StructField):
org.apache.spark.sql.catalyst.types.StructField in object
DataTypeConversions,
tree.tpe=org.apache.spark.sql.catalyst.types.StructField
[error]       DataTypeConversions.this.""asScalaStructField"" // def
asScalaStructField(javaStructField:
org.apache.spark.sql.api.java.StructField):
org.apache.spark.sql.catalyst.types.StructField in object
DataTypeConversions, tree.tpe=(javaStructField:
org.apache.spark.sql.api.java.StructField)org.apache.spark.sql.catalyst.types.StructField
[error]       ""javaStructField"" // javaStructField:
org.apache.spark.sql.api.java.StructField,
tree.tpe=org.apache.spark.sql.api.java.StructField
[error]     )
[error]   )
[error]   DefDef( // final def apply(v1: Object): Object
[error]     <method> final <bridge>
[error]     ""apply""
[error]     []
[error]     // 1 parameter list
[error]     ValDef( // v1: Object
[error]       <param> <triedcooking>
[error]       ""v1""
[error]       <tpt> // tree.tpe=Object
[error]       <empty>
[error]     )
[error]     <tpt> // tree.tpe=Object
[error]     Apply( // final def apply(javaStructField:
org.apache.spark.sql.api.java.StructField):
org.apache.spark.sql.catalyst.types.StructField,
tree.tpe=org.apache.spark.sql.catalyst.types.StructField
[error]       DataTypeConversions$$anonfun$asScalaDataType$1.this.""apply""
// final def apply(javaStructField:
org.apache.spark.sql.api.java.StructField):
org.apache.spark.sql.catalyst.types.StructField, tree.tpe=(javaStructField:
org.apache.spark.sql.api.java.StructField)org.apache.spark.sql.catalyst.types.StructField
[error]       Apply( // final def $asInstanceOf[T0 >: ? <: ?](): T0 in
class Object, tree.tpe=org.apache.spark.sql.api.java.StructField
[error]         TypeApply( // final def $asInstanceOf[T0 >: ? <: ?](): T0
in class Object, tree.tpe=()org.apache.spark.sql.api.java.StructField
[error]           ""v1"".""$asInstanceOf"" // final def $asInstanceOf[T0 >: ?
<: ?](): T0 in class Object, tree.tpe=[T0 >: ? <: ?]()T0
[error]           <tpt> //
tree.tpe=org.apache.spark.sql.api.java.StructField
[error]         )
[error]         Nil
[error]       )
[error]     )
[error]   )
[error]   DefDef( // def <init>():
org.apache.spark.sql.types.util.anonfun$asScalaDataType$1
[error]     <method> <triedcooking>
[error]     ""<init>""
[error]     []
[error]     List(Nil)
[error]     <tpt> //
tree.tpe=org.apache.spark.sql.types.util.anonfun$asScalaDataType$1
[error]     Block( // tree.tpe=Unit
[error]       Apply( // def <init>(): scala.runtime.AbstractFunction1 in
class AbstractFunction1, tree.tpe=scala.runtime.AbstractFunction1
[error]
DataTypeConversions$$anonfun$asScalaDataType$1.super.""<init>"" // def
<init>(): scala.runtime.AbstractFunction1 in class AbstractFunction1,
tree.tpe=()scala.runtime.AbstractFunction1
[error]         Nil
[error]       )
[error]       ()
[error]     )
[error]   )
[error] )
[error]
[error] == Expanded type of tree ==
[error]
[error] ConstantType(
[error]   value =
Constant(org.apache.spark.sql.catalyst.types.PrimitiveType)
[error] )
[error]
[error] uncaught exception during compilation: java.lang.AssertionError
[trace] Stack trace suppressed: run last sql/compile:compile for the full
output.
[error] (sql/compile:compile) java.lang.AssertionError: assertion failed:
List(object package$DebugNode, object package$DebugNode)
[error] Total time: 23 s, completed Nov 2, 2014 1:00:37 PM
"
Patrick Wendell <pwendell@gmail.com>,"Sun, 2 Nov 2014 19:28:07 -0800",Re: sbt scala compiler crashes on spark-sql,Imran Rashid <imran@therashids.com>,"Does this happen if you clean and recompile? I've seen failures on and
off, but haven't been able to find one that I could reproduce from a
clean build such that we could hand it to the scala team.

- Patrick

ct
 a
)
es/util/DataTypeConversions.scala
spark/spark/core/target/scala-2.10/classes:/Users/imran/spark/spark/sql/catalyst/target/scala-2.10/classes:/Users/imran/spark/spark/lib_managed/jars/hadoop-client-1.0.4.jar:/Users/imran/spark/spark/lib_managed/jars/hadoop-core-1.0.4.jar:/Users/imran/spark/spark/lib_managed/jars/xmlenc-0.52.jar:/Users/imran/spark/spark/lib_managed/jars/commons-math-2.1.jar:/Users/imran/spark/spark/lib_managed/jars/commons-configuration-1.6.jar:/Users/imran/spark/spark/lib_managed/jars/commons-collections-3.2.1.jar:/Users/imran/spark/spark/lib_managed/jars/commons-lang-2.4.jar:/Users/imran/spark/spark/lib_managed/jars/commons-logging-1.1.1.jar:/Users/imran/spark/spark/lib_managed/jars/commons-digester-1.8.jar:/Users/imran/spark/spark/lib_managed/jars/commons-beanutils-1.7.0.jar:/Users/imran/spark/spark/lib_managed/jars/commons-beanutils-core-1.8.0.jar:/Users/imran/spark/spark/lib_managed/jars/commons-net-2.2.jar:/Users/imran/spark/spark/lib_managed/jars/commons-el-1.0.jar:/Users/imran/spark/spark/lib_managed/jars/hsqldb-1.8.0.10.jar:/Users/imran/spark/spark/lib_managed/jars/oro-2.0.8.jar:/Users/imran/spark/spark/lib_managed/jars/jets3t-0.7.1.jar:/Users/imran/spark/spark/lib_managed/jars/commons-httpclient-3.1.jar:/Users/imran/spark/spark/lib_managed/bundles/curator-recipes-2.4.0.jar:/Users/imran/spark/spark/lib_managed/bundles/curator-framework-2.4.0.jar:/Users/imran/spark/spark/lib_managed/bundles/curator-client-2.4.0.jar:/Users/imran/spark/spark/lib_managed/jars/zookeeper-3.4.5.jar:/Users/imran/spark/spark/lib_managed/jars/slf4j-log4j12-1.7.5.jar:/Users/imran/spark/spark/lib_managed/bundles/log4j-1.2.17.jar:/Users/imran/spark/spark/lib_managed/jars/jline-0.9.94.jar:/Users/imran/spark/spark/lib_managed/bundles/guava-14.0.1.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-plus-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/orbits/javax.transaction-1.1.1.v201105210645.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-webapp-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-xml-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-util-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-servlet-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-security-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-server-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/orbits/javax.servlet-3.0.0.v201112011016.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-continuation-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-http-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-io-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-jndi-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/orbits/javax.mail.glassfish-1.4.1.v201005082020.jar:/Users/imran/spark/spark/lib_managed/orbits/javax.activation-1.1.0.v201105071233.jar:/Users/imran/spark/spark/lib_managed/jars/commons-lang3-3.3.2.jar:/Users/imran/spark/spark/lib_managed/jars/jsr305-1.3.9.jar:/Users/imran/spark/spark/lib_managed/jars/slf4j-api-1.7.5.jar:/Users/imran/spark/spark/lib_managed/jars/jul-to-slf4j-1.7.5.jar:/Users/imran/spark/spark/lib_managed/jars/jcl-over-slf4j-1.7.5.jar:/Users/imran/spark/spark/lib_managed/bundles/compress-lzf-1.0.0.jar:/Users/imran/spark/spark/lib_managed/bundles/snappy-java-1.0.5.3.jar:/Users/imran/spark/spark/lib_managed/jars/lz4-1.2.0.jar:/Users/imran/spark/spark/lib_managed/jars/chill_2.10-0.3.6.jar:/Users/imran/spark/spark/lib_managed/jars/chill-java-0.3.6.jar:/Users/imran/spark/spark/lib_managed/bundles/kryo-2.21.jar:/Users/imran/spark/spark/lib_managed/jars/reflectasm-1.07-shaded.jar:/Users/imran/spark/spark/lib_managed/jars/minlog-1.2.jar:/Users/imran/spark/spark/lib_managed/jars/objenesis-1.2.jar:/Users/imran/spark/spark/lib_managed/bundles/akka-remote_2.10-2.2.3-shaded-protobuf.jar:/Users/imran/spark/spark/lib_managed/jars/akka-actor_2.10-2.2.3-shaded-protobuf.jar:/Users/imran/spark/spark/lib_managed/bundles/config-1.0.2.jar:/Users/imran/spark/spark/lib_managed/bundles/netty-3.6.6.Final.jar:/Users/imran/spark/spark/lib_managed/jars/protobuf-java-2.4.1-shaded.jar:/Users/imran/spark/spark/lib_managed/jars/uncommons-maths-1.2.2a.jar:/Users/imran/spark/spark/lib_managed/bundles/akka-slf4j_2.10-2.2.3-shaded-protobuf.jar:/Users/imran/spark/spark/lib_managed/jars/json4s-jackson_2.10-3.2.10.jar:/Users/imran/spark/spark/lib_managed/jars/json4s-core_2.10-3.2.10.jar:/Users/imran/spark/spark/lib_managed/jars/json4s-ast_2.10-3.2.10.jar:/Users/imran/spark/spark/lib_managed/jars/paranamer-2.6.jar:/Users/imran/spark/spark/lib_managed/jars/scalap-2.10.0.jar:/Users/imran/spark/spark/lib_managed/bundles/jackson-databind-2.3.1.jar:/Users/imran/spark/spark/lib_managed/bundles/jackson-annotations-2.3.0.jar:/Users/imran/spark/spark/lib_managed/bundles/jackson-core-2.3.1.jar:/Users/imran/spark/spark/lib_managed/jars/colt-1.2.0.jar:/Users/imran/spark/spark/lib_managed/jars/concurrent-1.3.4.jar:/Users/imran/spark/spark/lib_managed/jars/mesos-0.18.1-shaded-protobuf.jar:/Users/imran/spark/spark/lib_managed/jars/netty-all-4.0.23.Final.jar:/Users/imran/spark/spark/lib_managed/jars/stream-2.7.0.jar:/Users/imran/spark/spark/lib_managed/bundles/metrics-core-3.0.0.jar:/Users/imran/spark/spark/lib_managed/bundles/metrics-jvm-3.0.0.jar:/Users/imran/spark/spark/lib_managed/bundles/metrics-json-3.0.0.jar:/Users/imran/spark/spark/lib_managed/bundles/metrics-graphite-3.0.0.jar:/Users/imran/spark/spark/lib_managed/jars/tachyon-client-0.5.0.jar:/Users/imran/spark/spark/lib_managed/jars/tachyon-0.5.0.jar:/Users/imran/spark/spark/lib_managed/jars/commons-io-2.4.jar:/Users/imran/spark/spark/lib_managed/jars/pyrolite-2.0.1.jar:/Users/imran/spark/spark/lib_managed/jars/py4j-0.8.2.1.jar:/Users/imran/.sbt/boot/scala-2.10.4/lib/scala-compiler.jar:/Users/imran/.sbt/boot/scala-2.10.4/lib/scala-reflect.jar:/Users/imran/spark/spark/lib_managed/jars/quasiquotes_2.10-2.0.1.jar:/Users/imran/spark/spark/lib_managed/jars/parquet-column-1.4.3.jar:/Users/imran/spark/spark/lib_managed/jars/parquet-common-1.4.3.jar:/Users/imran/spark/spark/lib_managed/jars/parquet-encoding-1.4.3.jar:/Users/imran/spark/spark/lib_managed/jars/parquet-generator-1.4.3.jar:/Users/imran/spark/spark/lib_managed/jars/commons-codec-1.5.jar:/Users/imran/spark/spark/lib_managed/jars/parquet-hadoop-1.4.3.jar:/Users/imran/spark/spark/lib_managed/jars/parquet-format-2.0.0.jar:/Users/imran/spark/spark/lib_managed/jars/parquet-jackson-1.4.3.jar:/Users/imran/spark/spark/lib_managed/jars/jackson-mapper-asl-1.9.11
r
.4-0.7.jar
esources.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/sunrsasign.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/classes:/Users/imran/.sbt/boot/scala-2.10.4/lib/scala-library.jar
nts
ld
ypes.StructField
eld:
ypes.StructField

---------------------------------------------------------------------


"
Cheng Lian <lian.cs.zju@gmail.com>,"Mon, 3 Nov 2014 11:37:26 +0800",Re: sbt scala compiler crashes on spark-sql,Patrick Wendell <pwendell@gmail.com>,"I often see this when I first build the whole Spark project with SBT, then
modify some code and tries to build and debug within IDEA, or vice versa. A
clean rebuild can always solve this.

:

:
ne
n
h
ev
a
es/util/DataTypeConversions.scala
spark/spark/core/target/scala-2.10/classes:/Users/imran/spark/spark/sql/catalyst/target/scala-2.10/classes:/Users/imran/spark/spark/lib_managed/jars/hadoop-client-1.0.4.jar:/Users/imran/spark/spark/lib_managed/jars/hadoop-core-1.0.4.jar:/Users/imran/spark/spark/lib_managed/jars/xmlenc-0.52.jar:/Users/imran/spark/spark/lib_managed/jars/commons-math-2.1.jar:/Users/imran/spark/spark/lib_managed/jars/commons-configuration-1.6.jar:/Users/imran/spark/spark/lib_managed/jars/commons-collections-3.2.1.jar:/Users/imran/spark/spark/lib_managed/jars/commons-lang-2.4.jar:/Users/imran/spark/spark/lib_managed/jars/commons-logging-1.1.1.jar:/Users/imran/spark/spark/lib_managed/jars/commons-digester-1.8.jar:/Users/imran/spark/spark/lib_managed/jars/commons-beanutils-1.7.0.jar:/Users/imran/spark/spark/lib_managed/jars/commons-beanutils-core-1.8.0.jar:/Users/imran/spark/spark/lib_managed/jars/commons-net-2.2.jar:/Users/imran/spark/spark/lib_managed/jars/commons-el-1.0.jar:/Users/imran/spark/spark/lib_managed/jars/hsqldb-1.8.0.10.jar:/Users/imran/spark/spark/lib_managed/jars/oro-2.0.8.jar:/Users/imran/spark/spark/lib_managed/jars/jets3t-0.7.1.jar:/Users/imran/spark/spark/lib_managed/jars/commons-httpclient-3.1.jar:/Users/imran/spark/spark/lib_managed/bundles/curator-recipes-2.4.0.jar:/Users/imran/spark/spark/lib_managed/bundles/curator-framework-2.4.0.jar:/Users/imran/spark/spark/lib_managed/bundles/curator-client-2.4.0.jar:/Users/imran/spark/spark/lib_managed/jars/zookeeper-3.4.5.jar:/Users/imran/spark/spark/lib_managed/jars/slf4j-log4j12-1.7.5.jar:/Users/imran/spark/spark/lib_managed/bundles/log4j-1.2.17.jar:/Users/imran/spark/spark/lib_managed/jars/jline-0.9.94.jar:/Users/imran/spark/spark/lib_managed/bundles/guava-14.0.1.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-plus-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/orbits/javax.transaction-1.1.1.v201105210645.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-webapp-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-xml-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-util-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-servlet-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-security-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-server-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/orbits/javax.servlet-3.0.0.v201112011016.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-continuation-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-http-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-io-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-jndi-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/orbits/javax.mail.glassfish-1.4.1.v201005082020.jar:/Users/imran/spark/spark/lib_managed/orbits/javax.activation-1.1.0.v201105071233.jar:/Users/imran/spark/spark/lib_managed/jars/commons-lang3-3.3.2.jar:/Users/imran/spark/spark/lib_managed/jars/jsr305-1.3.9.jar:/Users/imran/spark/spark/lib_managed/jars/slf4j-api-1.7.5.jar:/Users/imran/spark/spark/lib_managed/jars/jul-to-slf4j-1.7.5.jar:/Users/imran/spark/spark/lib_managed/jars/jcl-over-slf4j-1.7.5.jar:/Users/imran/spark/spark/lib_managed/bundles/compress-lzf-1.0.0.jar:/Users/imran/spark/spark/lib_managed/bundles/snappy-java-1.0.5.3.jar:/Users/imran/spark/spark/lib_managed/jars/lz4-1.2.0.jar:/Users/imran/spark/spark/lib_managed/jars/chill_2.10-0.3.6.jar:/Users/imran/spark/spark/lib_managed/jars/chill-java-0.3.6.jar:/Users/imran/spark/spark/lib_managed/bundles/kryo-2.21.jar:/Users/imran/spark/spark/lib_managed/jars/reflectasm-1.07-shaded.jar:/Users/imran/spark/spark/lib_managed/jars/minlog-1.2.jar:/Users/imran/spark/spark/lib_managed/jars/objenesis-1.2.jar:/Users/imran/spark/spark/lib_managed/bundles/akka-remote_2.10-2.2.3-shaded-protobuf.jar:/Users/imran/spark/spark/lib_managed/jars/akka-actor_2.10-2.2.3-shaded-protobuf.jar:/Users/imran/spark/spark/lib_managed/bundles/config-1.0.2.jar:/Users/imran/spark/spark/lib_managed/bundles/netty-3.6.6.Final.jar:/Users/imran/spark/spark/lib_managed/jars/protobuf-java-2.4.1-shaded.jar:/Users/imran/spark/spark/lib_managed/jars/uncommons-maths-1.2.2a.jar:/Users/imran/spark/spark/lib_managed/bundles/akka-slf4j_2.10-2.2.3-shaded-protobuf.jar:/Users/imran/spark/spark/lib_managed/jars/json4s-jackson_2.10-3.2.10.jar:/Users/imran/spark/spark/lib_managed/jars/json4s-core_2.10-3.2.10.jar:/Users/imran/spark/spark/lib_managed/jars/json4s-ast_2.10-3.2.10.jar:/Users/imran/spark/spark/lib_managed/jars/paranamer-2.6.jar:/Users/imran/spark/spark/lib_managed/jars/scalap-2.10.0.jar:/Users/imran/spark/spark/lib_managed/bundles/jackson-databind-2.3.1.jar:/Users/imran/spark/spark/lib_managed/bundles/jackson-annotations-2.3.0.jar:/Users/imran/spark/spark/lib_managed/bundles/jackson-core-2.3.1.jar:/Users/imran/spark/spark/lib_managed/jars/colt-1.2.0.jar:/Users/imran/spark/spark/lib_managed/jars/concurrent-1.3.4.jar:/Users/imran/spark/spark/lib_managed/jars/mesos-0.18.1-shaded-protobuf.jar:/Users/imran/spark/spark/lib_managed/jars/netty-all-4.0.23.Final.jar:/Users/imran/spark/spark/lib_managed/jars/stream-2.7.0.jar:/Users/imran/spark/spark/lib_managed/bundles/metrics-core-3.0.0.jar:/Users/imran/spark/spark/lib_managed/bundles/metrics-jvm-3.0.0.jar:/Users/imran/spark/spark/lib_managed/bundles/metrics-json-3.0.0.jar:/Users/imran/spark/spark/lib_managed/bundles/metrics-graphite-3.0.0.jar:/Users/imran/spark/spark/lib_managed/jars/tachyon-client-0.5.0.jar:/Users/imran/spark/spark/lib_managed/jars/tachyon-0.5.0.jar:/Users/imran/spark/spark/lib_managed/jars/commons-io-2.4.jar:/Users/imran/spark/spark/lib_managed/jars/pyrolite-2.0.1.jar:/Users/imran/spark/spark/lib_managed/jars/py4j-0.8.2.1.jar:/Users/imran/.sbt/boot/scala-2.10.4/lib/scala-compiler.jar:/Users/imran/.sbt/boot/scala-2.10.4/lib/scala-reflect.jar:/Users/imran/spark/spark/lib_managed/jars/quasiquotes_2.10-2.0.1.jar:/Users/imran/spark/spark/lib_managed/jars/parquet-column-1.4.3.jar:/Users/imran/spark/spark/lib_managed/jars/parquet-common-1.4.3.jar:/Users/imran/spark/spark/lib_managed/jars/parquet-encoding-1.4.3.jar:/Users/imran/spark/spark/lib_managed/jars/parquet-generator-1.4.3.jar:/Users/imran/spark/spark/lib_managed/jars/commons-codec-1.5.jar:/Users/imran/spark/spark/lib_managed/jars/parquet-hadoop-1.4.3.jar:/Users/imran/spark/spark/lib_managed/jars/parquet-format-2.0.0.jar:/Users/imran/spark/spark/lib_managed/jars/parquet-jackson-1.4.3.jar:/Users/imran/spark/spark/lib_managed/jars/jackson-mapper-asl-1.9.11
r
.4-0.7.jar
esources.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/sunrsasign.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/classes:/Users/imran/.sbt/boot/scala-2.10.4/lib/scala-library.jar
ield
ypes.StructField
y""
ypes.StructField
T0
 ?
n
ll
d:
"
Stephen Boesch <javadba@gmail.com>,"Sun, 2 Nov 2014 19:48:47 -0800",Re: sbt scala compiler crashes on spark-sql,Cheng Lian <lian.cs.zju@gmail.com>,"Yes I have seen this same error - and for team members as well - repeatedly
since June. A Patrick and Cheng mentioned, the next step is to do an sbt
clean

2014-11-02 19:37 GMT-08:00 Cheng Lian <lian.cs.zju@gmail.com>:

n
 A
or
r
es/util/DataTypeConversions.scala
spark/spark/core/target/scala-2.10/classes:/Users/imran/spark/spark/sql/catalyst/target/scala-2.10/classes:/Users/imran/spark/spark/lib_managed/jars/hadoop-client-1.0.4.jar:/Users/imran/spark/spark/lib_managed/jars/hadoop-core-1.0.4.jar:/Users/imran/spark/spark/lib_managed/jars/xmlenc-0.52.jar:/Users/imran/spark/spark/lib_managed/jars/commons-math-2.1.jar:/Users/imran/spark/spark/lib_managed/jars/commons-configuration-1.6.jar:/Users/imran/spark/spark/lib_managed/jars/commons-collections-3.2.1.jar:/Users/imran/spark/spark/lib_managed/jars/commons-lang-2.4.jar:/Users/imran/spark/spark/lib_managed/jars/commons-logging-1.1.1.jar:/Users/imran/spark/spark/lib_managed/jars/commons-digester-1.8.jar:/Users/imran/spark/spark/lib_managed/jars/commons-beanutils-1.7.0.jar:/Users/imran/spark/spark/lib_managed/jars/commons-beanutils-core-1.8.0.jar:/Users/imran/spark/spark/lib_managed/jars/commons-net-2.2.jar:/Users/imran/spark/spark/lib_managed/jars/commons-el-1.0.jar:/Users/imran/spark/spark/lib_managed/jars/hsqldb-1.8.0.10.jar:/Users/imran/spark/spark/lib_managed/jars/oro-2.0.8.jar:/Users/imran/spark/spark/lib_managed/jars/jets3t-0.7.1.jar:/Users/imran/spark/spark/lib_managed/jars/commons-httpclient-3.1.jar:/Users/imran/spark/spark/lib_managed/bundles/curator-recipes-2.4.0.jar:/Users/imran/spark/spark/lib_managed/bundles/curator-framework-2.4.0.jar:/Users/imran/spark/spark/lib_managed/bundles/curator-client-2.4.0.jar:/Users/imran/spark/spark/lib_managed/jars/zookeeper-3.4.5.jar:/Users/imran/spark/spark/lib_managed/jars/slf4j-log4j12-1.7.5.jar:/Users/imran/spark/spark/lib_managed/bundles/log4j-1.2.17.jar:/Users/imran/spark/spark/lib_managed/jars/jline-0.9.94.jar:/Users/imran/spark/spark/lib_managed/bundles/guava-14.0.1.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-plus-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/orbits/javax.transaction-1.1.1.v201105210645.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-webapp-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-xml-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-util-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-servlet-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-security-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-server-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/orbits/javax.servlet-3.0.0.v201112011016.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-continuation-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-http-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-io-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-jndi-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/orbits/javax.mail.glassfish-1.4.1.v201005082020.jar:/Users/imran/spark/spark/lib_managed/orbits/javax.activation-1.1.0.v201105071233.jar:/Users/imran/spark/spark/lib_managed/jars/commons-lang3-3.3.2.jar:/Users/imran/spark/spark/lib_managed/jars/jsr305-1.3.9.jar:/Users/imran/spark/spark/lib_managed/jars/slf4j-api-1.7.5.jar:/Users/imran/spark/spark/lib_managed/jars/jul-to-slf4j-1.7.5.jar:/Users/imran/spark/spark/lib_managed/jars/jcl-over-slf4j-1.7.5.jar:/Users/imran/spark/spark/lib_managed/bundles/compress-lzf-1.0.0.jar:/Users/imran/spark/spark/lib_managed/bundles/snappy-java-1.0.5.3.jar:/Users/imran/spark/spark/lib_managed/jars/lz4-1.2.0.jar:/Users/imran/spark/spark/lib_managed/jars/chill_2.10-0.3.6.jar:/Users/imran/spark/spark/lib_managed/jars/chill-java-0.3.6.jar:/Users/imran/spark/spark/lib_managed/bundles/kryo-2.21.jar:/Users/imran/spark/spark/lib_managed/jars/reflectasm-1.07-shaded.jar:/Users/imran/spark/spark/lib_managed/jars/minlog-1.2.jar:/Users/imran/spark/spark/lib_managed/jars/objenesis-1.2.jar:/Users/imran/spark/spark/lib_managed/bundles/akka-remote_2.10-2.2.3-shaded-protobuf.jar:/Users/imran/spark/spark/lib_managed/jars/akka-actor_2.10-2.2.3-shaded-protobuf.jar:/Users/imran/spark/spark/lib_managed/bundles/config-1.0.2.jar:/Users/imran/spark/spark/lib_managed/bundles/netty-3.6.6.Final.jar:/Users/imran/spark/spark/lib_managed/jars/protobuf-java-2.4.1-shaded.jar:/Users/imran/spark/spark/lib_managed/jars/uncommons-maths-1.2.2a.jar:/Users/imran/spark/spark/lib_managed/bundles/akka-slf4j_2.10-2.2.3-shaded-protobuf.jar:/Users/imran/spark/spark/lib_managed/jars/json4s-jackson_2.10-3.2.10.jar:/Users/imran/spark/spark/lib_managed/jars/json4s-core_2.10-3.2.10.jar:/Users/imran/spark/spark/lib_managed/jars/json4s-ast_2.10-3.2.10.jar:/Users/imran/spark/spark/lib_managed/jars/paranamer-2.6.jar:/Users/imran/spark/spark/lib_managed/jars/scalap-2.10.0.jar:/Users/imran/spark/spark/lib_managed/bundles/jackson-databind-2.3.1.jar:/Users/imran/spark/spark/lib_managed/bundles/jackson-annotations-2.3.0.jar:/Users/imran/spark/spark/lib_managed/bundles/jackson-core-2.3.1.jar:/Users/imran/spark/spark/lib_managed/jars/colt-1.2.0.jar:/Users/imran/spark/spark/lib_managed/jars/concurrent-1.3.4.jar:/Users/imran/spark/spark/lib_managed/jars/mesos-0.18.1-shaded-protobuf.jar:/Users/imran/spark/spark/lib_managed/jars/netty-all-4.0.23.Final.jar:/Users/imran/spark/spark/lib_managed/jars/stream-2.7.0.jar:/Users/imran/spark/spark/lib_managed/bundles/metrics-core-3.0.0.jar:/Users/imran/spark/spark/lib_managed/bundles/metrics-jvm-3.0.0.jar:/Users/imran/spark/spark/lib_managed/bundles/metrics-json-3.0.0.jar:/Users/imran/spark/spark/lib_managed/bundles/metrics-graphite-3.0.0.jar:/Users/imran/spark/spark/lib_managed/jars/tachyon-client-0.5.0.jar:/Users/imran/spark/spark/lib_managed/jars/tachyon-0.5.0.jar:/Users/imran/spark/spark/lib_managed/jars/commons-io-2.4.jar:/Users/imran/spark/spark/lib_managed/jars/pyrolite-2.0.1.jar:/Users/imran/spark/spark/lib_managed/jars/py4j-0.8.2.1.jar:/Users/imran/.sbt/boot/scala-2.10.4/lib/scala-compiler.jar:/Users/imran/.sbt/boot/scala-2.10.4/lib/scala-reflect.jar:/Users/imran/spark/spark/lib_managed/jars/quasiquotes_2.10-2.0.1.jar:/Users/imran/spark/spark/lib_managed/jars/parquet-column-1.4.3.jar:/Users/imran/spark/spark/lib_managed/jars/parquet-common-1.4.3.jar:/Users/imran/spark/spark/lib_managed/jars/parquet-encoding-1.4.3.jar:/Users/imran/spark/spark/lib_managed/jars/parquet-generator-1.4.3.jar:/Users/imran/spark/spark/lib_managed/jars/commons-codec-1.5.jar:/Users/imran/spark/spark/lib_managed/jars/parquet-hadoop-1.4.3.jar:/Users/imran/spark/spark/lib_managed/jars/parquet-format-2.0.0.jar:/Users/imran/spark/spark/lib_managed/jars/parquet-jackson-1.4.3.jar:/Users/imran/spark/spark/lib_managed/jars/jackson-mapper-asl-1.9.11
r
.4-0.7.jar
esources.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/sunrsasign.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/classes:/Users/imran/.sbt/boot/scala-2.10.4/lib/scala-library.jar
$1
ypes.StructField
ypes.StructField
n
:
ld
or
"
Patrick Wendell <pwendell@gmail.com>,"Sun, 2 Nov 2014 20:20:36 -0800",Re: sbt scala compiler crashes on spark-sql,Stephen Boesch <javadba@gmail.com>,"By the way - we can report issues to the Scala/Typesafe team if we
have a way to reproduce this. I just haven't found a reliable
reproduction yet.

- Patrick

ly
en
.
s
ark
l
n
er
types/util/DataTypeConversions.scala
an/spark/spark/core/target/scala-2.10/classes:/Users/imran/spark/spark/sql/catalyst/target/scala-2.10/classes:/Users/imran/spark/spark/lib_managed/jars/hadoop-client-1.0.4.jar:/Users/imran/spark/spark/lib_managed/jars/hadoop-core-1.0.4.jar:/Users/imran/spark/spark/lib_managed/jars/xmlenc-0.52.jar:/Users/imran/spark/spark/lib_managed/jars/commons-math-2.1.jar:/Users/imran/spark/spark/lib_managed/jars/commons-configuration-1.6.jar:/Users/imran/spark/spark/lib_managed/jars/commons-collections-3.2.1.jar:/Users/imran/spark/spark/lib_managed/jars/commons-lang-2.4.jar:/Users/imran/spark/spark/lib_managed/jars/commons-logging-1.1.1.jar:/Users/imran/spark/spark/lib_managed/jars/commons-digester-1.8.jar:/Users/imran/spark/spark/lib_managed/jars/commons-beanutils-1.7.0.jar:/Users/imran/spark/spark/lib_managed/jars/commons-beanutils-core-1.8.0.jar:/Users/imran/spark/spark/lib_managed/jars/commons-net-2.2.jar:/Users/imran/spark/spark/lib_managed/jars/commons-el-1.0.jar:/Users/imran/spark/spark/lib_managed/jars/hsqldb-1.8.0.10.jar:/Users/imran/spark/spark/lib_managed/jars/oro-2.0.8.jar:/Users/imran/spark/spark/lib_managed/jars/jets3t-0.7.1.jar:/Users/imran/spark/spark/lib_managed/jars/commons-httpclient-3.1.jar:/Users/imran/spark/spark/lib_managed/bundles/curator-recipes-2.4.0.jar:/Users/imran/spark/spark/lib_managed/bundles/curator-framework-2.4.0.jar:/Users/imran/spark/spark/lib_managed/bundles/curator-client-2.4.0.jar:/Users/imran/spark/spark/lib_managed/jars/zookeeper-3.4.5.jar:/Users/imran/spark/spark/lib_managed/jars/slf4j-log4j12-1.7.5.jar:/Users/imran/spark/spark/lib_managed/bundles/log4j-1.2.17.jar:/Users/imran/spark/spark/lib_managed/jars/jline-0.9.94.jar:/Users/imran/spark/spark/lib_managed/bundles/guava-14.0.1.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-plus-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/orbits/javax.transaction-1.1.1.v201105210645.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-webapp-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-xml-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-util-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-servlet-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-security-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-server-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/orbits/javax.servlet-3.0.0.v201112011016.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-continuation-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-http-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-io-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-jndi-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/orbits/javax.mail.glassfish-1.4.1.v201005082020.jar:/Users/imran/spark/spark/lib_managed/orbits/javax.activation-1.1.0.v201105071233.jar:/Users/imran/spark/spark/lib_managed/jars/commons-lang3-3.3.2.jar:/Users/imran/spark/spark/lib_managed/jars/jsr305-1.3.9.jar:/Users/imran/spark/spark/lib_managed/jars/slf4j-api-1.7.5.jar:/Users/imran/spark/spark/lib_managed/jars/jul-to-slf4j-1.7.5.jar:/Users/imran/spark/spark/lib_managed/jars/jcl-over-slf4j-1.7.5.jar:/Users/imran/spark/spark/lib_managed/bundles/compress-lzf-1.0.0.jar:/Users/imran/spark/spark/lib_managed/bundles/snappy-java-1.0.5.3.jar:/Users/imran/spark/spark/lib_managed/jars/lz4-1.2.0.jar:/Users/imran/spark/spark/lib_managed/jars/chill_2.10-0.3.6.jar:/Users/imran/spark/spark/lib_managed/jars/chill-java-0.3.6.jar:/Users/imran/spark/spark/lib_managed/bundles/kryo-2.21.jar:/Users/imran/spark/spark/lib_managed/jars/reflectasm-1.07-shaded.jar:/Users/imran/spark/spark/lib_managed/jars/minlog-1.2.jar:/Users/imran/spark/spark/lib_managed/jars/objenesis-1.2.jar:/Users/imran/spark/spark/lib_managed/bundles/akka-remote_2.10-2.2.3-shaded-protobuf.jar:/Users/imran/spark/spark/lib_managed/jars/akka-actor_2.10-2.2.3-shaded-protobuf.jar:/Users/imran/spark/spark/lib_managed/bundles/config-1.0.2.jar:/Users/imran/spark/spark/lib_managed/bundles/netty-3.6.6.Final.jar:/Users/imran/spark/spark/lib_managed/jars/protobuf-java-2.4.1-shaded.jar:/Users/imran/spark/spark/lib_managed/jars/uncommons-maths-1.2.2a.jar:/Users/imran/spark/spark/lib_managed/bundles/akka-slf4j_2.10-2.2.3-shaded-protobuf.jar:/Users/imran/spark/spark/lib_managed/jars/json4s-jackson_2.10-3.2.10.jar:/Users/imran/spark/spark/lib_managed/jars/json4s-core_2.10-3.2.10.jar:/Users/imran/spark/spark/lib_managed/jars/json4s-ast_2.10-3.2.10.jar:/Users/imran/spark/spark/lib_managed/jars/paranamer-2.6.jar:/Users/imran/spark/spark/lib_managed/jars/scalap-2.10.0.jar:/Users/imran/spark/spark/lib_managed/bundles/jackson-databind-2.3.1.jar:/Users/imran/spark/spark/lib_managed/bundles/jackson-annotations-2.3.0.jar:/Users/imran/spark/spark/lib_managed/bundles/jackson-core-2.3.1.jar:/Users/imran/spark/spark/lib_managed/jars/colt-1.2.0.jar:/Users/imran/spark/spark/lib_managed/jars/concurrent-1.3.4.jar:/Users/imran/spark/spark/lib_managed/jars/mesos-0.18.1-shaded-protobuf.jar:/Users/imran/spark/spark/lib_managed/jars/netty-all-4.0.23.Final.jar:/Users/imran/spark/spark/lib_managed/jars/stream-2.7.0.jar:/Users/imran/spark/spark/lib_managed/bundles/metrics-core-3.0.0.jar:/Users/imran/spark/spark/lib_managed/bundles/metrics-jvm-3.0.0.jar:/Users/imran/spark/spark/lib_managed/bundles/metrics-json-3.0.0.jar:/Users/imran/spark/spark/lib_managed/bundles/metrics-graphite-3.0.0.jar:/Users/imran/spark/spark/lib_managed/jars/tachyon-client-0.5.0.jar:/Users/imran/spark/spark/lib_managed/jars/tachyon-0.5.0.jar:/Users/imran/spark/spark/lib_managed/jars/commons-io-2.4.jar:/Users/imran/spark/spark/lib_managed/jars/pyrolite-2.0.1.jar:/Users/imran/spark/spark/lib_managed/jars/py4j-0.8.2.1.jar:/Users/imran/.sbt/boot/scala-2.10.4/lib/scala-compiler.jar:/Users/imran/.sbt/boot/scala-2.10.4/lib/scala-reflect.jar:/Users/imran/spark/spark/lib_managed/jars/quasiquotes_2.10-2.0.1.jar:/Users/imran/spark/spark/lib_managed/jars/parquet-column-1.4.3.jar:/Users/imran/spark/spark/lib_managed/jars/parquet-common-1.4.3.jar:/Users/imran/spark/spark/lib_managed/jars/parquet-encoding-1.4.3.jar:/Users/imran/spark/spark/lib_managed/jars/parquet-generator-1.4.3.jar:/Users/imran/spark/spark/lib_managed/jars/commons-codec-1.5.jar:/Users/imran/spark/spark/lib_managed/jars/parquet-hadoop-1.4.3.jar:/Users/imran/spark/spark/lib_managed/jars/parquet-format-2.0.0.jar:/Users/imran/spark/spark/lib_managed/jars/parquet-jackson-1.4.3.jar:/Users/imran/spark/spark/lib_managed/jars/jackson-mapper-asl-1.9.11
.jar
.10.4-0.7.jar
b/resources.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/sunrsasign.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/classes:/Users/imran/.sbt/boot/scala-2.10.4/lib/scala-library.jar
t.types.StructField
t.types.StructField
in
):
eld
1
,

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Mon, 3 Nov 2014 00:55:20 -0800",branch-1.2 has been cut,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi All,

I've just cut the release branch for Spark 1.2, consistent with then
end of the scheduled feature window for the release. New commits to
master will need to be explicitly merged into branch-1.2 in order to
be in the release.

This begins the transition into a QA period for Spark 1.2, with a
focus on testing and fixes. A few smaller features may still go in as
folks wrap up loose ends in the next 48 hours (or for developments in
alpha components).

To help with QA, I'll try to package up a SNAPSHOT release soon for
community testing; this worked well when testing Spark 1.1 before
official votes started. I might give it a few days to allow committers
to merge in back-logged fixes and other patches that were punted to
after the feature freeze.

Thanks to everyone who helped author and review patches over the last few weeks!

- Patrick

---------------------------------------------------------------------


"
Imran Rashid <imran@therashids.com>,"Mon, 3 Nov 2014 07:39:30 -0600",Re: sbt scala compiler crashes on spark-sql,Patrick Wendell <pwendell@gmail.com>,"thanks everyone, that worked.  I had been just cleaning the ""sql"" project,
which wasn't enough, but a full clean of everything and its happy now.

just in case this helps anybody else come up with steps to reproduce, for
me the error was always in DataTypeConversions.scala, and I think it
*might* have started after I did a maven build as well.
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 3 Nov 2014 12:32:28 -0500",Re: branch-1.2 has been cut,Patrick Wendell <pwendell@gmail.com>,"Minor question, but when would be the right time to update the default
Spark version
<https://github.com/apache/spark/blob/76386e1a23c55a58c0aeea67820aab2bac71b24b/ec2/spark_ec2.py#L42>
in the EC2 script?


"
ozgun <ozgun@citusdata.com>,"Mon, 3 Nov 2014 15:01:28 -0800 (PST)",Re: Surprising Spark SQL benchmark,dev@spark.incubator.apache.org,"Hey Patrick,

It's Ozgun from Citus Data. We'd like to make these benchmark results fair,
and have tried different config settings for SparkSQL over the past month.
We picked the best config settings we could find, and also contacted the
Spark users list about running TPC-H numbers.

http://goo.gl/IU5Hw0
http://goo.gl/WQ1kML
http://goo.gl/ihLzgh

We also received advice at the Spark Summit '14 to wait until v1.1, and
Marco and Samay from our team have much more context, and I'll let them
answer your questions on the different settings we tried.

that front, we used publicly available documentation and user lists, and
spent about a month trying to get the best Spark performance results. If
there are specific optimizations we should have applied and missed, we'd
love to be involved with the community in re-running the numbers.

Is this email thread the best place to continue the conversation?

Best,
Ozgun



--

---------------------------------------------------------------------


"
Debasish Das <debasish.das83@gmail.com>,"Mon, 3 Nov 2014 15:20:30 -0800",Re: matrix factorization cross validation,Sean Owen <sowen@cloudera.com>,"I added the drivers for precisionAt(k: Int) driver for the movielens
test-cases...Although I am a bit confused on precisionAt(k: Int) code from
RankingMetrics.scala...

While cross validating, I am really not sure how to set K...

if (labSet.nonEmpty) { val n = math.min(pred.length, k) ... }

If I make k as a function of pred.length val n = math.min(pred.length,
k*pred.length) then I can vary k between 0 and 1 and choose the sweet spot
for K on a given dataset but I am not sure if it is a measure that makes
sense for recommendation...

MAP is something that makes sense as it is average over all test set...



"
Debasish Das <debasish.das83@gmail.com>,"Mon, 3 Nov 2014 17:25:14 -0800","MatrixFactorizationModel predict(Int, Int) API",dev <dev@spark.apache.org>,"Hi,

I am testing MatrixFactorizationModel.predict(user: Int, product: Int) but
the code fails on userFeatures.lookup(user).head

In computeRmse MatrixFactorizationModel.predict(RDD[(Int, Int)]) has been
called and in all the test-cases that API has been used...

I can perhaps refactor my code to do the same but I was wondering whether
people test the lookup(user) version of the code..

Do I need to cache the model to make it work ? I think right now default is
STORAGE_AND_DISK...

Thanks.
Deb
"
Matt Cheah <mcheah@palantir.com>,"Tue, 4 Nov 2014 01:26:03 +0000",Spark shuffle consolidateFiles performance degradation numbers,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi everyone,

I'm running into more and more cases where too many files are opened when
spark.shuffle.consolidateFiles is turned off.

I was wondering if this is a common scenario among the rest of the
community, and if so, if it is worth considering the setting to be turned on
by default. From the documentation, it seems like the performance could be
hurt on ext3 file systems. However, what are the concrete numbers of
performance degradation that is seen typically? A 2x slowdown in the average
job? 3x? Also, what cause the performance degradation on ext3 file systems
specifically?

Thanks,

-Matt Cheah




"
Matt Cheah <mcheah@palantir.com>,"Tue, 4 Nov 2014 02:05:06 +0000","Spark shuffle consolidateFiles performance degradation
 quantification","""dev@spark.apache.org"" <dev@spark.apache.org>","Hi everyone,

I'm running into more and more cases where too many files are opened when
spark.shuffle.consolidateFiles is turned off.

I was wondering if this is a common scenario among the rest of the
community, and if so, if it is worth considering the setting to be turned on
by default. From the documentation, it seems like the performance could be
hurt on ext3 file systems. However, what are the concrete numbers of
performance degradation that is seen typically? A 2x slowdown in the average
job? 3x? Also, what cause the performance degradation on ext3 file systems
specifically?

Thanks,

-Matt Cheah


"
Andrew Or <andrew@databricks.com>,"Mon, 3 Nov 2014 18:12:34 -0800",Re: Spark shuffle consolidateFiles performance degradation numbers,Matt Cheah <mcheah@palantir.com>,"Hey Matt,

There's some prior work that compares consolidation performance on some
medium-scale workload:
http://www.cs.berkeley.edu/~kubitron/courses/cs262a-F13/projects/reports/project16_report.pdf

There we noticed about 2x performance degradation in the reduce phase on
ext3. I am not aware of any other concrete numbers. Maybe others have more
experiences to add.

-Andrew

2014-11-03 17:26 GMT-08:00 Matt Cheah <mcheah@palantir.com>:

"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 3 Nov 2014 18:28:09 -0800",Re: Spark shuffle consolidateFiles performance degradation numbers,Andrew Or <andrew@databricks.com>,"In Spark 1.1, the sort-based shuffle (spark.shuffle.manager=sort) will have better performance while creating fewer files. So I'd suggest trying that too.

Matei

some
http://www.cs.berkeley.edu/~kubitron/courses/cs262a-F13/projects/reports/project16_report.pdf
on
more
when
turned
performance could
of
file


---------------------------------------------------------------------


"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 3 Nov 2014 18:28:47 -0800",Re: Spark shuffle consolidateFiles performance degradation numbers,Andrew Or <andrew@databricks.com>,"(BTW this had a bug with negative hash codes in 1.1.0 so you should try branch-1.1 for it).

Matei

will have better performance while creating fewer files. So I'd suggest trying that too.
some
http://www.cs.berkeley.edu/~kubitron/courses/cs262a-F13/projects/reports/project16_report.pdf
on
more
when
turned
performance could
of
ext3 file


---------------------------------------------------------------------


"
Zach Fry <zfry@palantir.com>,"Tue, 4 Nov 2014 02:34:41 +0000",Re: Spark shuffle consolidateFiles performance degradation numbers,"Matei Zaharia <matei.zaharia@gmail.com>, Andrew Or <andrew@databricks.com>","Hey Andrew, Matei,

Thanks for responding.

For some more context, we were running into ""Too many open files"" issues
where we were seeing this happen immediately after the Collect phase
(about 30 seconds into a run) on a decently sized dataset (14 MM rows).
The ulimit set in the spark-env was 256,000 which we believe should have
been enough, but even with it set at that number, we were still seeing
issues. 
Can you comment on what a ""good"" ulimit should be in these cases?

We believe what might have caused this is  some process got orphaned
without cleaning up its open file handles.
However, other than anecdotal evidence and some speculation, we don't have
much evidence to expand on this further.

We were wondering if we could get some more information about how many
files get opened during a shuffle.
We discussed that it is going to be around N x M, where N is the number of
Tasks and M is the number of Reducers.
Does this sound about right?


Are there any other considerations we should be aware of when setting
consolidateFiles to True?

Thanks, 
Zach Fry
Palantir | Developer Support Engineer
zfry@palantir.com <mailto:email@palantir.com> | 650.226.6338



On 11/3/14 6:28 09PM, ""Matei Zaharia"" <matei.zaharia@gmail.com> wrote:

>In Spark 1.1, the sort-based shuffle (spark.shuffle.manager=sort) will
>have better performance while creating fewer files. So I'd suggest trying
>that too.
>
>Matei
>
>> On Nov 3, 2014, at 6:12 PM, Andrew Or <andrew@databricks.com> wrote:
>> 
>> Hey Matt,
>> 
>> There's some prior work that compares consolidation performance on some
>> medium-scale workload:
>> 
>>https://urldefense.proofpoint.com/v2/url?u=http-3A__www.cs.berkeley.edu_-
>>7Ekubitron_courses_cs262a-2DF13_projects_reports_project16-5Freport.pdf&d
>>=AAIFAg&c=izlc9mHr637UR4lpLEZLFFS3Vn2UXBrZ4tFb6oOnmz8&r=0Yj0NJdi423O9rGnW
>>Dox5yE_2OXftYbKeoFygDwj99U&m=fQgGKwxzg3lfq5XUaEZy674jjtWDSrFOHIrIDFEGpQc&
>>s=ukSpYSbxzzrYdHJEXPMx3gGsErP2vA2PMdBVsY3EOnA&e=
>> 
>> There we noticed about 2x performance degradation in the reduce phase on
>> ext3. I am not aware of any other concrete numbers. Maybe others have
>>more
>> experiences to add.
>> 
>> -Andrew
>> 
>> 2014-11-03 17:26 GMT-08:00 Matt Cheah <mcheah@palantir.com>:
>> 
>>> Hi everyone,
>>> 
>>> I'm running into more and more cases where too many files are opened
>>>when
>>> spark.shuffle.consolidateFiles is turned off.
>>> 
>>> I was wondering if this is a common scenario among the rest of the
>>> community, and if so, if it is worth considering the setting to be
>>>turned
>>> on by default. From the documentation, it seems like the performance
>>>could
>>> be hurt on ext3 file systems. However, what are the concrete numbers of
>>> performance degradation that is seen typically? A 2x slowdown in the
>>> average job? 3x? Also, what cause the performance degradation on ext3
>>>file
>>> systems specifically?
>>> 
>>> Thanks,
>>> 
>>> -Matt Cheah
>>> 
>>> 
>>> 
>


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org
"
Xiangrui Meng <mengxr@gmail.com>,"Mon, 3 Nov 2014 21:24:09 -0800","Re: MatrixFactorizationModel predict(Int, Int) API",Debasish Das <debasish.das83@gmail.com>,"Was ""user"" presented in training? We can put a check there and return
NaN if the user is not included in the model. -Xiangrui


---------------------------------------------------------------------


"
Ashutosh <ashutosh.trivedi@iiitb.org>,"Tue, 4 Nov 2014 07:22:21 -0800 (PST)",Re: [MLlib] Contributing Algorithm for Outlier Detection,dev@spark.incubator.apache.org,"Anant,

I got rid of those increment/ decrements functions and now code is much cleaner. Please check. All your comments have been looked after.

https://github.com/codeAshu/Outlier-Detection-with-AVF-Spark/blob/master/OutlierWithAVFModel.scala


_Ashu

[https://avatars3.githubusercontent.com/u/5406975?v=2&s=400]<https://github.com/codeAshu/Outlier-Detection-with-AVF-Spark/blob/master/OutlierWithAVFModel.scala>

Outlier-Detection-with-AVF-Spark/OutlierWithAVFModel.scala at master Â· codeAshu/Outlier-Detection-with-AVF-Spark Â· GitHub
Contribute to Outlier-Detection-with-AVF-Spark development by creating an account on GitHub.
Read more...<https://github.com/codeAshu/Outlier-Detection-with-AVF-Spark/blob/master/OutlierWithAVFModel.scala>


________________________________
From: slcclimber [via Apache Spark Developers List] <ml-node+s1001551n9037h23@n3.nabble.com>
Sent: Friday, October 31, 2014 10:09 AM
To: Ashutosh Trivedi (MT2013030)
Subject: Re: [MLlib] Contributing Algorithm for Outlier Detection


You should create a jira ticket to go with it as well.
Thanks


?Okay. I'll try it and post it soon with test case. After that I think we can go ahead with the PR.

________________________________
From: slcclimber [via Apache Spark Developers List] <ml-node+[hidden email]<http://user/SendEmail.jtp?type=node&node=9036&i=0>>
Sent: Friday, October 31, 2014 10:03 AM
To: Ashutosh Trivedi (MT2013030)
Subject: Re: [MLlib] Contributing Algorithm for Outlier Detection


Ashutosh,
A vector would be a good idea vectors are used very frequently.
Test data is usually stored in the spark/data/mllib folder

idden email]<http://user/SendEmail.jtp?type=node&node=9035&i=0>> wrote:
Hi Anant,
sorry for my late reply. Thank you for taking time and reviewing it.

I have few comments on first issue.

You are correct on the string (csv) part. But we can not take input of type you mentioned. We calculate frequency in our function. Otherwise user has to do all this computation. I realize that taking a RDD[Vector] would be general enough for all. What do you say?

I agree on rest all the issues. I will correct them soon and post it.
I have a doubt on test cases. Where should I put data while giving test scripts? or should i generate synthetic data for testing with in the scripts, how does this work?

Regards,
Ashutosh

________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/MLlib-Contributing-Algorithm-for-Outlier-Detection-tp8880p9034.html
lick here.
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>


________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/MLlib-Contributing-Algorithm-for-Outlier-Detection-tp8880p9035.html
lick here.
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>


________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/MLlib-Contributing-Algorithm-for-Outlier-Detection-tp8880p9036.html
lick here.
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>


________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/MLlib-Contributing-Algorithm-for-Outlier-Detection-tp8880p9037.html
lick here<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=8880&code=YXNodXRvc2gudHJpdmVkaUBpaWl0Yi5vcmd8ODg4MHwtMzkzMzE5NzYx>.
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>




--
3.nabble.com/MLlib-Contributing-Algorithm-for-Outlier-Detection-tp8880p9083.html
om."
Cody Koeninger <cody@koeninger.org>,"Tue, 4 Nov 2014 11:34:30 -0600",Hadoop configuration for checkpointing,"""dev@spark.apache.org"" <dev@spark.apache.org>","3 quick questions, then some background:

1.  Is there a reason not to document the fact that spark.hadoop.* is
copied from spark config into hadoop config?

2.  Is there a reason StreamingContext.getOrCreate defaults to a blank
hadoop configuration rather than
org.apache.spark.deploy.SparkHadoopUtil.get.conf,
which would pull values from spark config?

3.  If I submit a PR to address those issues, is it likely to be lost in
the 1.2 scramble :)


Background:

I have a streaming job that is not recoverable from checkpoint, because the
s3 credentials were originally set using
sparkContext.hadoopConfiguration.set.

Checkpointing saves the spark config, but not the transient spark context,
so does not save the s3 credentials unless they were originally present in
the spark config.

Providing a hadoop config to getOrCreate only uses that hadoop config for
CheckpointReader's initial load of the checkpoint file.  It does not copy
the hadoop config into the newly created spark context, and so the
immediately following attempt to restore DStreamCheckpointData fails for
lack of credentials.

I think the cleanest way to handle this would be to encourage people to set
hadoop configuration in the spark config, and for
StreamingContext.getOrCreate to use SparkHadoopUtil rather than a blank
config.


Relevant stack trace:

14/11/04 15:37:30 INFO CheckpointReader: Checkpoint files found: s3n://XXX

14/11/04 15:37:30 INFO CheckpointReader: Attempting to load checkpoint from
file s3n://XXX

14/11/04 15:37:30 INFO NativeS3FileSystem: Opening 's3n://XXX

14/11/04 15:37:31 INFO Checkpoint: Checkpoint for time 1415114220000 ms
validated

14/11/04 15:37:31 INFO CheckpointReader: Checkpoint successfully loaded
from file s3n://XXX

14/11/04 15:37:31 INFO CheckpointReader: Checkpoint was generated at time
1415114220000 ms

14/11/04 15:37:33 INFO DStreamGraph: Restoring checkpoint data

14/11/04 15:37:33 INFO ForEachDStream: Restoring checkpoint data

14/11/04 15:37:33 INFO StateDStream: Restoring checkpoint data

14/11/04 15:37:33 INFO DStreamCheckpointData: Restoring checkpointed RDD
for time 1415097420000 ms from file 's3n://XXX

Exception in thread ""main"" java.lang.IllegalArgumentException: AWS Access
Key ID and Secret Access Key must be specified as

the username or password (respectively) of a s3n URL, or by setting the
fs.s3n.awsAccessKeyId or fs.s3n.awsSecretAccessKey properties
(respectively).

at org.apache.hadoop.fs.s3.S3Credentials.initialize(S3Credentials.java:70)

at
org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.initialize(Jets3tNativeFileSystemStore.java:73)

at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)

at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

at java.lang.reflect.Method.invoke(Method.java:606)

at
org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)

at
org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)

at org.apache.hadoop.fs.s3native.$Proxy8.initialize(Unknown Source)

at
org.apache.hadoop.fs.s3native.NativeS3FileSystem.initialize(NativeS3FileSystem.java:272)

at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2397)

at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:89)

at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2431)

at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2413)

at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:368)

at org.apache.hadoop.fs.Path.getFileSystem(Path.java:296)

at org.apache.spark.rdd.CheckpointRDD.<init>(CheckpointRDD.scala:42)

at org.apache.spark.SparkContext.checkpointFile(SparkContext.scala:824)

at
org.apache.spark.streaming.dstream.DStreamCheckpointData$$anonfun$restore$1.apply(DStreamCheckpointData.scala:112)

at
org.apache.spark.streaming.dstream.DStreamCheckpointData$$anonfun$restore$1.apply(DStreamCheckpointData.scala:109)

at
scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)

at
scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)

at
scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)

at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)

at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)

at
org.apache.spark.streaming.dstream.DStreamCheckpointData.restore(DStreamCheckpointData.scala:109)

at
org.apache.spark.streaming.dstream.DStream.restoreCheckpointData(DStream.scala:397)

at
org.apache.spark.streaming.dstream.DStream$$anonfun$restoreCheckpointData$2.apply(DStream.scala:398)

at
org.apache.spark.streaming.dstream.DStream$$anonfun$restoreCheckpointData$2.apply(DStream.scala:398)

at scala.collection.immutable.List.foreach(List.scala:318)

at
org.apache.spark.streaming.dstream.DStream.restoreCheckpointData(DStream.scala:398)

at
org.apache.spark.streaming.DStreamGraph$$anonfun$restoreCheckpointData$2.apply(DStreamGraph.scala:149)

at
org.apache.spark.streaming.DStreamGraph$$anonfun$restoreCheckpointData$2.apply(DStreamGraph.scala:149)

at
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)

at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)

at
org.apache.spark.streaming.DStreamGraph.restoreCheckpointData(DStreamGraph.scala:149)

at
org.apache.spark.streaming.StreamingContext.<init>(StreamingContext.scala:131)

at
org.apache.spark.streaming.StreamingContext$$anonfun$getOrCreate$1.apply(StreamingContext.scala:552)

at
org.apache.spark.streaming.StreamingContext$$anonfun$getOrCreate$1.apply(StreamingContext.scala:552)

at scala.Option.map(Option.scala:145)

at
org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:552)
"
Marcelo Vanzin <vanzin@cloudera.com>,"Tue, 4 Nov 2014 09:48:01 -0800",Re: Hadoop configuration for checkpointing,Cody Koeninger <cody@koeninger.org>,"
This is probably something I overlooked when I changed the rest of the
code to use SparkHadoopUtil.get.conf. Feel free to send a PR for it,
we should get it into 1.2 so that all code creates configuration
objects in a consistent way.

-- 
Marcelo

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Tue, 4 Nov 2014 18:49:13 +0000",Re: Hadoop configuration for checkpointing,Marcelo Vanzin <vanzin@cloudera.com>,"Let me crash this thread to suggest this *might* be related to this
problem I'm trying to solve:
https://issues.apache.org/jira/browse/SPARK-4196

Basically the question there is: this blank Configuration object gets
made on the driver in the saveAsNewAPIHadoopFiles call, and seems to
need to be serialized to use it in foreachRDD. This fails for me and
at least 2 other users I know. But I feel like I am missing something.

If you're investigating handling of Configuration when enabling
checkpointing with the getOrCreate method, have a look and see if you
have any comments vis-a-vis this JIRA.



---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Tue, 4 Nov 2014 11:05:50 -0800",Re: Surprising Spark SQL benchmark,"ozgun <ozgun@citusdata.com>, Patrick Wendell <patrick@databricks.com>","dev to bcc.

Thanks for reaching out, Ozgun.  Let's discuss if there were any missing
optimizations off list.  We'll make sure to report back or add any findings
to the tuning guide.


"
Otis Gospodnetic <otis.gospodnetic@gmail.com>,"Tue, 4 Nov 2014 15:12:52 -0500",[ANN] Spark resources searchable,"""user@spark.apache.org"" <user@spark.apache.org>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Hi everyone,

We've recently added indexing of all Spark resources to
http://search-hadoop.com/spark .

Everything is nicely searchable:
* user & dev mailing lists
* JIRA issues
* web site
* wiki
* source code
* javadoc.

Maybe it's worth adding to http://spark.apache.org/community.html ?

Enjoy!

Otis
--
Monitoring * Alerting * Anomaly Detection * Centralized Log Management
Solr & Elasticsearch Support * http://sematext.com/
"
Alessandro Baretta <alexbaretta@gmail.com>,"Tue, 4 Nov 2014 14:08:23 -0800",Build fails on master (f90ad5d),"""dev@spark.apache.org"" <dev@spark.apache.org>","Fellow Sparkers,

I am new here and still trying to learn to crawl. Please, bear with me.

I just pulled f90ad5d from https://github.com/apache/spark.git and am
running the compile command in the sbt shell. This is the error I'm seeing:

[error]
/home/alex/git/spark/mllib/src/main/scala/org/apache/spark/mllib/linalg/Vectors.scala:32:
object sql is not a member of package org.apache.spark
[error] import org.apache.spark.sql.catalyst.types._
[error]                         ^

Am I doing something obscenely stupid is the build genuinely broken?

Alex
"
Ted Yu <yuzhihong@gmail.com>,"Tue, 4 Nov 2014 14:11:41 -0800",Re: Build fails on master (f90ad5d),Alessandro Baretta <alexbaretta@gmail.com>,"I built based on this commit today and the build was successful.

What command did you use ?

Cheers


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 4 Nov 2014 18:11:44 -0500",Re: Build fails on master (f90ad5d),Ted Yu <yuzhihong@gmail.com>,"FWIW, the ""official"" build instructions are here:
https://github.com/apache/spark#building-spark


"
"""Hari Shreedharan"" <hshreedharan@cloudera.com>","Tue, 04 Nov 2014 15:14:33 -0800 (PST)",Re: Build fails on master (f90ad5d),"""Nicholas Chammas"" <nicholas.chammas@gmail.com>","I have seen this on sbt sometimes. I usually do an sbt clean and that fixes it.


Thanks,
Hari


com>
.
Vectors.scala:32:
broken?"
Debasish Das <debasish.das83@gmail.com>,"Tue, 4 Nov 2014 16:42:04 -0800",Issues with AbstractParams,dev <dev@spark.apache.org>,"Hi,

I build the master today and I was testing IR statistics on movielens
dataset (open up a PR in a bit)...

Right now in the master examples.MovieLensALS, case class Params extends
AbstractParam[Params]


./bin/spark-submit --master spark://
tusca09lmlvt00c.uswin.ad.vzwcorp.com:7077 --jars
/Users/v606014/.m2/repository/com/github/scopt/scopt_2.10/3.2.0/scopt_2.10-3.2.0.jar
--total-executor-cores 4 --executor-memory 4g --driver-memory 1g --class
org.apache.spark.examples.mllib.MovieLensALS
./examples/target/spark-examples_2.10-1.2.0-SNAPSHOT.jar --kryo --lambda
0.065 hdfs://localhost:8020/sandbox/movielens/

2014-11-04 16:00:18.691 java[1811:1903] Unable to load realm mapping info
from SCDynamicStore

14/11/04 16:00:18 WARN NativeCodeLoader: Unable to load native-hadoop
library for your platform... using builtin-java classes where applicable

14/11/04 16:00:21 WARN TaskSetManager: Lost task 1.0 in stage 0.0 (TID 1,
tusca09lmlvt00c.uswin.ad.vzwcorp.com): java.io.InvalidClassException:
org.apache.spark.examples.mllib.MovieLensALS$Params; no valid constructor


java.io.ObjectStreamClass$ExceptionInfo.newInvalidClassException(ObjectStreamClass.java:150)


java.io.ObjectStreamClass.checkDeserialize(ObjectStreamClass.java:768)


java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1772)

        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)


java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)


java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)


java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)

        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)


java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)


java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)


java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)

        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)


java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)


java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)


java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)

        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)

        java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)


org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:62)


org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:87)

        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:57)

        org.apache.spark.scheduler.Task.run(Task.scala:56)


org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:186)


java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)


java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

        java.lang.Thread.run(Thread.java:745)
If I remove AbstractParams from examples.MovieLensALS and recompile then
code runs fine:

./bin/spark-submit --master spark://
tusca09lmlvt00c.uswin.ad.vzwcorp.com:7077 --jars
/Users/v606014/.m2/repository/com/github/scopt/scopt_2.10/3.2.0/scopt_2.10-3.2.0.jar
--total-executor-cores 4 --executor-memory 4g --driver-memory 1g --class
org.apache.spark.examples.mllib.MovieLensALS
./examples/target/spark-examples_2.10-1.2.0-SNAPSHOT.jar --kryo --lambda
0.065 hdfs://localhost:8020/sandbox/movielens/

2014-11-04 16:26:25.359 java[2892:1903] Unable to load realm mapping info
from SCDynamicStore

14/11/04 16:26:25 WARN NativeCodeLoader: Unable to load native-hadoop
library for your platform... using builtin-java classes where applicable

Got 1000209 ratings from 6040 users on 3706 movies.

Training: 800650, test: 199559.

Test RMSE = 0.8525220763317215.

14/11/04 16:26:41 ERROR ConnectionManager: Corresponding SendingConnection
to ConnectionManagerId(tusca09lmlvt00c.uswin.ad.vzwcorp.com,50749) not found

14/11/04 16:26:42 WARN ConnectionManager: All connections not cleaned up
Is this a known issue for mllib examples ? If there is no open JIRA for
this, I can open it up...

Thanks.
Deb
"
Alessandro Baretta <alexbaretta@gmail.com>,"Tue, 4 Nov 2014 17:03:52 -0800",Re: Build fails on master (f90ad5d),Nicholas Chammas <nicholas.chammas@gmail.com>,"Nicholas,

Yes, I saw them, but they refer to maven, and I'm under the impression that
sbt is the preferred way of building spark. Is indeed maven the ""right
way""? Anyway, as per your advice I ctrl-d'ed my sbt shell and have ran `mvn
-DskipTests clean package`, which completed successfully. So, indeed, in
trying to use sbt I was on a wild goose chase.

Here's a couple of glitches I'm seeing. First of all many warnings such as
the following:

[WARNING]     assert(windowedStream2.generatedRDDs.contains(Time(10000)))
[WARNING]                            ^
[WARNING]
/home/alex/git/spark/streaming/src/test/scala/org/apache/spark/streaming/BasicOperationsSuite.scala:454:
inferred existential type
scala.collection.mutable.HashMap[org.apache.spark.streaming.Time,org.apache.spark.rdd.RDD[_$2]]
forSome { type _$2 }, which cannot be expressed by wildcards,  should be
enabled
by making the implicit value scala.language.existentials visible.

[WARNING]
/home/alex/git/spark/sql/hive/src/main/scala/org/apache/spark/sql/hive/parquet/FakeParquetSerDe.scala:34:
@deprecated now takes two arguments; see the scaladoc.
[WARNING] @deprecated(""No code should depend on FakeParquetHiveSerDe as it
is only intended as a "" +
[WARNING]  ^

[WARNING]
/home/alex/git/spark/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMetastoreCatalog.scala:435:
trait Deserializer in package serde2 is deprecated: see corresponding
Javadoc for more information.
[WARNING]
Utils.getContextOrSparkClassLoader).asInstanceOf[Class[Deserializer]],
[WARNING]                                                        ^

[WARNING]
/home/alex/git/spark/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingKMeans.scala:22:
imported `StreamingKMeans' is permanently hidden by definition of object
StreamingKMeans in package mllib
[WARNING] import org.apache.spark.mllib.clustering.StreamingKMeans
Are they expected?

Also, mvn complains about not having zinc. Is this a problem?

[WARNING] Zinc server is not available at port 3030 - reverting to normal
incremental compile

Alex


"
slcclimber <anant.asty@gmail.com>,"Tue, 4 Nov 2014 17:15:16 -0800 (PST)",Re: [MLlib] Contributing Algorithm for Outlier Detection,dev@spark.incubator.apache.org,"Ashutosh,
I still see a few issues.
a RDD the counter will cause issues. Also that is not good functional style
to use a filter function with a side effect.
You could use randomSplit instead. This does not the same thing without the
side effect.
2. Similar shared usage of j in line 102 is going to be an issue as well.
also hash seed does not need to be sequential it could be randomly
generated or hashed on the values.
3. The compute function and trim scores still runs on a comma separeated
RDD. We should take a vector instead giving the user flexibility to decide
data source/ type. what if we want data from hive tables or parquet or JSON
or avro formats. This is a very restrictive format. With vectors the user
has the choice of taking in whatever data format and converting them to
vectors insteda of reading json files creating a csv file and then workig
on that.
4. Similar use of counters in 54 and 65 is an issue.
Basically the shared state counters is a huge issue that does not scale.
Since the processing of RDD's is distributed and the value j lives on the
master.

Anant




OutlierWithAVFModel.scala
/OutlierWithAVFModel.scala>
Â·
n
/OutlierWithAVFModel.scala>
[hidden
I think
ser
ld
uting-Algorithm-for-Outlier-Detection-tp8880p9034.html
lServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
ting-Algorithm-for-Outlier-Detection-tp8880p9035.html
Servlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
ting-Algorithm-for-Outlier-Detection-tp8880p9036.html
Servlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
ing-Algorithm-for-Outlier-Detection-tp8880p9037.html
, click
ervlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
ing-Algorithm-for-Outlier-Detection-tp8880p9083.html
, click
ervlet.jtp?macro=unsubscribe_by_code&node=8880&code=YW5hbnQuYXN0eUBnbWFpbC5jb218ODg4MHwxOTU2OTQ5NjMy>
ervlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>




--
3.nabble.com/MLlib-Contributing-Algorithm-for-Outlier-Detection-tp8880p9095.html
om."
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 4 Nov 2014 20:42:09 -0500",Re: Build fails on master (f90ad5d),Alessandro Baretta <alexbaretta@gmail.com>,"Zinc, I believe, is something you can install and run to speed up your
Maven builds. It's not required.

I get a bunch of warnings when compiling with Maven, too. Dunno if they are
expected or not, but things work fine from there on.

Many people do indeed use sbt. I don't know where we have documentation on
how to use sbt (we recently removed it from the README), but sbt/sbt clean
followed by sbt/sbt assembly should work fine.

Maven is indeed the ""proper"" way to build Spark, but building with sbt is
supported too and most Spark devs I believe use it because it's faster than
Maven.

Nick


"
Alessandro Baretta <alexbaretta@gmail.com>,"Tue, 4 Nov 2014 17:53:03 -0800",Re: Build fails on master (f90ad5d),Nicholas Chammas <nicholas.chammas@gmail.com>,"Nicholas,

Indeed, I was trying to use sbt to speed up the build. My initial
experiments with the maven process took over 50 minutes, which on a 4-core
2014 MacBookPro seems obscene. Then again, after the failed attempt with
sbt, mvn clean package took only 13 minutes, leading me to think that most
of the time was somehow being spent in downloading and building
dependencies.

Anyway, if sbt is supported it would be great to add docs about somewhere,
especially since, as you point out, most devs are using it.

Thanks for your help.

Alex


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 4 Nov 2014 21:00:03 -0500",Re: Build fails on master (f90ad5d),Alessandro Baretta <alexbaretta@gmail.com>,"Ah, found it:
https://github.com/apache/spark/blob/master/docs/building-spark.md#building-with-sbt

This version of the docs should be published once 1.2.0 is released.

Nick


"
Ted Yu <yuzhihong@gmail.com>,"Tue, 4 Nov 2014 18:14:46 -0800",Re: Build fails on master (f90ad5d),Alessandro Baretta <alexbaretta@gmail.com>,"
Here is the tail of mvn build output:

[INFO] Spark Project External Flume ...................... SUCCESS [7.368s]
[INFO] Spark Project External ZeroMQ ..................... SUCCESS [9.153s]
[INFO] Spark Project External MQTT ....................... SUCCESS [5.233s]
[INFO] Spark Project Examples ............................ SUCCESS [49.011s]
[INFO]
------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO]
------------------------------------------------------------------------
[INFO] Total time: 7:42.208s
[INFO] Finished at: Tue Nov 04 18:10:44 PST 2014
[INFO] Final Memory: 48M/500M

FYI


"
Qiuzhuang Lian <qiuzhuang.lian@gmail.com>,"Wed, 5 Nov 2014 11:13:34 +0800",src/main/resources/kv1.txt not found in example of HiveFromSpark,dev@spark.apache.org,"When running HiveFromSpark example via run-example shell, I got error,

FAILED: SemanticException Line 1:23 Invalid path
''src/main/resources/kv1.txt'': No files matching path
file:/home/kand/javaprojects/spark/src/main/resources/kv1.txt

======================
END HIVE FAILURE OUTPUT
======================

Exception in thread ""main""
org.apache.spark.sql.execution.QueryExecutionException: FAILED:
SemanticException Line 1:23 Invalid path ''src/main/resources/kv1.txt'': No
files matching path
file:/home/kand/javaprojects/spark/src/main/resources/kv1.txt

I think the hardcode dir should be

examples/src/main/resources/kv1.txt

instead of

src/main/resources/kv1.txt in that class.

Thanks,
Qiuzhuang
"
Cody Koeninger <cody@koeninger.org>,"Tue, 4 Nov 2014 21:40:34 -0600",Re: Hadoop configuration for checkpointing,Marcelo Vanzin <vanzin@cloudera.com>,"Opened
https://issues.apache.org/jira/browse/SPARK-4229

Sent a PR
https://github.com/apache/spark/pull/3102


"
Joseph Bradley <joseph@databricks.com>,"Tue, 4 Nov 2014 22:49:56 -0800",Re: Issues with AbstractParams,Debasish Das <debasish.das83@gmail.com>,"Hi Deb,
Thanks for pointing it out!  I don't know of a JIRA for it now, so it would
be great if you could open one.  I'm looking into the bug...
Joseph


"
Sean Owen <sowen@cloudera.com>,"Wed, 5 Nov 2014 06:55:01 +0000",Re: Issues with AbstractParams,Debasish Das <debasish.das83@gmail.com>,"I don't think it's anything to do with AbstractParams. The problem is
MovieLensALS$Params, which is a case class without default
constructor. It is not Serializable.

However you can see it gets used in an RDD function:

val ratings = sc.textFile(params.input).map { line =>
  val fields = line.split(""::"")
  if (params.implicitPrefs) {

It it just a matter of rejiggering the code to not pass params.
Have at it; I'm happy to do it too.


---------------------------------------------------------------------


"
Marco Slot <marco@citusdata.com>,"Wed, 5 Nov 2014 12:45:00 +0100",Re: Surprising Spark SQL benchmark,dev@spark.apache.org,"Hi Patrick,

We left the details of the configuration of Spark that we used out of the
blog post for brevity, but we're happy to share them. We've done quite a
bit of tuning to find the configuration settings that gave us the best
query times and run the most queries. I think there might still be a few
improvements that we could make. We spent the majority of our time
optimizing the 20-node (in-memory) case. For documentation purposes, I'm
including a summary of the work we've done so far below. We are also
looking forward to working with the SparkSQL team to look into any further
optimizations.

Initially, we started off with Spark 1.0.2, so we wrote Java programs to
run the queries. We compared SQLContext to HiveContext and found the latter
to be faster and it was recommended to us on the mailing list for having
better SQL support http://goo.gl/IU5Hw0. With Spark 1.1.0, which is the
version that we used to get the benchmark numbers, we just used the
spark-sql command-line tool, which uses HiveContext by default.

We pretty quickly ran into the issue that query times were highly variable
(from minutes to hours). If I understand correctly, Spark's MemoryStore
uses a FIFO caching policy, which means it will remove the oldest block
first rather than the least recently-used one. At the start, the oldest
data is the actual table data which will be reused many times. In the
20-node benchmark, we found that the query times became more stable with
cache, because it pins the table data in memory and also uses the optimized
in-memory format. We did not see any difference between text and parquet
tables in the in-memory case after caching. However, we did not use cache
in the 4-node benchmark, because we saw better query times without it, and
used parquet files generated by inserting into a parquet-backed Hive
metastore table that used ParquetHiveSerDe.

A problem we ran into at the start is that the table data wouldn't fit in
memory when spark.storage.memoryFraction was set to the default value of
0.6, so we increased it to 0.65. We also confirmed that the partitions are
fitting in memory by looking at the storage tab of the Spark Web UI. We
also increased spark.shuffle.memoryFraction from 0.2 to 0.25. This avoided
some premature eviction problems. We ran the benchmark on machines with
122GB memory. We set spark.executor.memory to 110000m to use most of memory
available. Increasing this value gave more stable query times and fewer
failures.

We set spark.serializer to org.apache.spark.serializer.KryoSerializer as
recommended by the Spark docs, and also because it gave us better query
times. We also set spark.sql.inMemoryColumnarStorage.compressed to true.

TPC-H has some very large intermediate jobs and result sets. We found that
a number of timeouts in Spark trigger too early during queries. We
eventually increased spark.worker.timeout, spark.akka.timeout, and
spark.storage.blockManagerSlaveTimeoutMs
to 10 minutes to avoid these issues.

We also tried different block sizes. Some queries run slight faster, but
others a lot slower when using a bigger block size (e.g., 512MB), but
eventually found 128MB gave us most stable and overall lowest query times.
We also experimented a bit with spark.sql.shuffle.partitions. We eventually
set it to the number of vCPUs in the cluster, which was 320. I should note
that one thing that we've noticed in all of our benchmarks was that when
running TPC-H on EC2, there is not that much benefit of using 16 vCPUs over
8. This is because the r3.4xlarge and i2.4xlarge machines have 8 physical
cores, which each have hyper-threading to give 16 vCPUs, but the benefit of
hyper-threading isn't huge in this case, meaning that the cores are (in the
worst case) only half as fast when using all 16.

There are a few more settings we experimented with further with mixed, but
overall not hugely significant results. We tried
increasing spark.shuffle.file.buffer.kb, spark.akka.framesize. We increased
spark.akka.threads from 4 to 8.

We tried compute analyze <table name> compute statistics, but that failed
with the following errors:

java.lang.Throwable: Child Error
        at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:271)
Caused by: java.io.IOException: Task process exit with nonzero status of 1.
        at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:258)

We also tried running compute analyze <table name> compute statistics
noscan. However, when using cache <table name>, it failed with the
following error:

14/11/03 15:59:17 ERROR CliDriver: scala.NotImplementedError: Analyze has
only implemented for Hive tables, but customer is a Subquery
        at
org.apache.spark.sql.hive.HiveContext.analyze(HiveContext.scala:189)

Without cache, the noscan command completes successfully. However, we have
not seen any performance benefit from and still see queries like Q5 failing
(after a very long time). We found using cache to be preferable over using
analyze in the in-memory case.

Besides these, we also experimented with several different settings but
didn't find them to have a particular impact on query times. We are now
looking forward to working together with SparkSQL developers, and
re-running the numbers with proposed optimizations.

regards,
Marco
"
"""Ganelin, Ilya"" <Ilya.Ganelin@capitalone.com>","Wed, 5 Nov 2014 11:02:35 -0500",Appropriate way to add a debug flag,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hello all – I am working on https://issues.apache.org/jira/browse/SPARK-3694 and would like to understand the appropriate mechanism by which to check for a debug flag before printing a graph traversal of dependencies of an RDD or Task. I understand that I can use the logging utility and use logDebug to actually print the output but the graph traversal should not be executed unless the debug output is enabled. The code changes I will be making will be in the DAGScheduler and TaskSetManager classes.

Modifying the function interfaces does not seem like the appropriate approach . Is there an existing debug flag that is set somehow within the spark config?
________________________________________________________

The information contained in this e-mail is confidential and/or proprietary is intended only for use by the individual or entity to which it is addressed.  If the reader of this message is not the intended recipient, you are hereby notified that any review, retransmission, dissemination, distribution, copying or other use of, or taking of any action in reliance upon this information is strictly prohibited. If you have received this communication in error, please contact the sender and delete the material from your computer.
"
Marcelo Vanzin <vanzin@cloudera.com>,"Wed, 5 Nov 2014 09:58:27 -0800",Re: src/main/resources/kv1.txt not found in example of HiveFromSpark,"Qiuzhuang Lian <qiuzhuang.lian@gmail.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Yeah, the code looks for the file in the source location, not in the
packaged location. It's in the root of the examples jar; you can
extract it to ""src/main/resources/
kv1.txt"" in the local directory (creating the subdirs) and then you
can run the example.

Probably should be fixed though (bonus if both styles work after the fix).




-- 
Marcelo

---------------------------------------------------------------------


"
Joseph Bradley <joseph@databricks.com>,"Wed, 5 Nov 2014 11:33:42 -0800",Re: Issues with AbstractParams,Sean Owen <sowen@cloudera.com>,"I'm making a JIRA and will then do a quick PR for it.  (Thanks both for
pointing out the bug & fix!)


"
Reynold Xin <rxin@databricks.com>,"Wed, 5 Nov 2014 15:11:42 -0800",Re: Breaking the previous large-scale sort record with Spark,"user <user@spark.apache.org>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,

We are excited to announce that the benchmark entry has been reviewed by
the Sort Benchmark committee and Spark has officially won the Daytona
GraySort contest in sorting 100TB of data.

Our entry tied with a UCSD research team building high performance systems
and we jointly set a new world record. This is an important milestone for
the project, as it validates the amount of engineering work put into Spark
by the community.

As Matei said, ""For an engine to scale from these multi-hour petabyte batch
jobs down to 100-millisecond streaming and interactive queries is quite
uncommon, and it's thanks to all of you folks that we are able to make this
happen.""

Updated blog post:
http://databricks.com/blog/2014/11/05/spark-officially-sets-a-new-record-in-large-scale-sorting.html





"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 5 Nov 2014 18:43:42 -0500",Re: Surprising Spark SQL benchmark,Steve Nunez <snunez@hortonworks.com>,"
I believe that benchmark has a pending certification on it. See
Regarding this comment, Reynold has just announced that this benchmark is
now certified.

   - Announcement:
   http://databricks.com/blog/2014/11/05/spark-officially-sets-a-new-record-in-large-scale-sorting.html
   - Updated benchmark results page: http://sortbenchmark.org/
   - Paper detailing Spark cluster configuration for the benchmark:
   http://sortbenchmark.org/ApacheSpark2014.pdf

Nick
â€‹
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 5 Nov 2014 18:56:38 -0500",Re: Surprising Spark SQL benchmark,Steve Nunez <snunez@hortonworks.com>,"Steve Nunez, I believe the information behind the links below should
address your concerns earlier about Databricks's submission to the Daytona
Gray benchmark.

m

rd-in-large-scale-sorting.html
"
Matei Zaharia <matei.zaharia@gmail.com>,"Wed, 5 Nov 2014 16:02:51 -0800",Re: Breaking the previous large-scale sort record with Spark,Reynold Xin <rxin@databricks.com>,"Congrats to everyone who helped make this happen. And if anyone has even more machines they'd like us to run on next year, let us know :).

Matei

by
systems
for
Spark
batch
quite
this
http://databricks.com/blog/2014/11/05/spark-officially-sets-a-new-record-in-large-scale-sorting.html
<matei.zaharia@gmail.com>
some
use
3x
http://databricks.com/blog/2014/10/10/spark-breaks-previous-large-scale-sort-record.html.
by
minutes on
Ali
would of
testing
to
and


---------------------------------------------------------------------


"
Steve Nunez <snunez@hortonworks.com>,"Wed, 05 Nov 2014 16:07:05 -0800",Re: Surprising Spark SQL benchmark,Nicholas Chammas <nicholas.chammas@gmail.com>,"Nicholas,

I never doubted the authenticity of the benchmark, nor the results. What I
think could be better is an objective analysis of the results. That post
neglected to point out the significant differences in hardware those two
benchmarks were run on. It is bit like bragging you broke the world record
at the NÃ¼rburgring in a 2014 1000hp LaFerrari and somehow forgetting to
mention that the last record was held by a 2001 Toyota Celica.

- Steve


From:  Nicholas Chammas <nicholas.chammas@gmail.com>
Date:  Wednesday, November 5, 2014 at 15:56
To:  Steve Nunez <snunez@hortonworks.com>
Cc:  Patrick Wendell <pwendell@gmail.com>, dev <dev@spark.apache.org>
Subject:  Re: Surprising Spark SQL benchmark

ess
com>
s now
-in-l



-- 
CONFIDENTIALITY NOTICE
NOTICE: This message is intended for the use of the individual or entity to 
which it is addressed and may contain information that is confidential, 
privileged and exempt from disclosure under applicable law. If the reader 
of this message is not the intended recipient, you are hereby notified that 
any printing, copying, dissemination, distribution, disclosure or 
forwarding of this communication is strictly prohibited. If you have 
received this communication in error, please contact the sender immediately 
and delete it from your system. Thank You.
"
Reynold Xin <rxin@databricks.com>,"Wed, 5 Nov 2014 16:10:53 -0800",Re: Surprising Spark SQL benchmark,Steve Nunez <snunez@hortonworks.com>,"Steve,

I wouldn't say Hadoop MR is a 2001 Toyota Celica :) In either case, I
updated the blog post to actually include CPU / disk / network measures.
You should see that in any measure that matters to this benchmark, the old
2100 node cluster is vastly superior. The data even fit in memory!




I
d
 to
in-l
to
at
ly
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 5 Nov 2014 20:03:04 -0500",Re: Surprising Spark SQL benchmark,Reynold Xin <rxin@databricks.com>,"Steve,

Your original comment was about the *reproducibility* of the benchmark,
which I was responding to. No one is suggesting you doubt the authenticity
or results of the benchmark.

For which no details or code have been released to allow others to

to avoid the stigma of vendor reported benchmarks and publish enough

information and code to let others repeat the exercise easily.


So to reiterate, the results and paper that Databricks published should let
other people reproduce their submission to the Daytona Gray benchmark. This
addresses your original concern quoted above.

Nick



d
 I
rd
g to
y
k
-in-l
r
"
Matei Zaharia <matei.zaharia@gmail.com>,"Wed, 5 Nov 2014 17:31:58 -0800",[VOTE] Designating maintainers for some Spark components,dev <dev@spark.apache.org>,"Hi all,

I wanted to share a discussion we've been having on the PMC list, as well as call for an official vote on it on a public list. Basically, as the Spark project scales up, we need to define a model to make sure there is still great oversight of key components (in particular internal architecture and public APIs), and to this end I've proposed implementing a maintainer model for some of these components, similar to other large projects.

As background on this, Spark has grown a lot since joining Apache. We've had over 80 contributors/month for the past 3 months, which I believe makes us the most active project in contributors/month at Apache, as well as over 500 patches/month. The codebase has also grown significantly, with new libraries for SQL, ML, graphs and more.

In this kind of large project, one common way to scale development is to assign ""maintainers"" to oversee key components, where each patch to that component needs to get sign-off from at least one of its maintainers. Most existing large projects do this -- at Apache, some large ones with this model are CloudStack (the second-most active project overall), Subversion, and Kafka, and other examples include Linux and Python. This is also by-and-large how Spark operates today -- most components have a de-facto maintainer.

IMO, adopting this model would have two benefits:

1) Consistent oversight of design for that component, especially regarding architecture and API. This process would ensure that the component's maintainers see all proposed changes and consider them to fit together in a good way.

2) More structure for new contributors and committers -- in particular, it would be easy to look up whoâ€™s responsible for each module and ask them for reviews, etc, rather than having patches slip between the cracks.

We'd like to start with in a light-weight manner, where the model only applies to certain key components (e.g. scheduler, shuffle) and user-facing APIs (MLlib, GraphX, etc). Over time, as the project grows, we can expand it if we deem it useful. The specific mechanics would be as follows:

- Some components in Spark will have maintainers assigned to them, where one of the maintainers needs to sign off on each patch to the component.
- Each component with maintainers will have at least 2 maintainers.
- Maintainers will be assigned from the most active and knowledgeable committers on that component by the PMC. The PMC can vote to add / remove maintainers, and maintained components, through consensus.
- Maintainers are expected to be active in responding to patches for their components, though they do not need to be the main reviewers for them (e.g. they might just sign off on architecture / API). To prevent inactive maintainers from blocking the project, if a maintainer isn't responding in a reasonable time period (say 2 weeks), other committers can merge the patch, and the PMC will want to discuss adding another maintainer.

If you'd like to see examples for this model, check out the following projects:
- CloudStack: https://cwiki.apache.org/confluence/display/CLOUDSTACK/CloudStack+Maintainers+Guide <https://cwiki.apache.org/confluence/display/CLOUDSTACK/CloudStack+Maintainers+Guide> 
- Subversion: https://subversion.apache.org/docs/community-guide/roles.html <https://subversion.apache.org/docs/community-guide/roles.html>

Finally, I wanted to list our current proposal for initial components and maintainers. It would be good to get feedback on other components we might add, but please note that personnel discussions (e.g. ""I don't think Matei should maintain *that* component) should only happen on the private list. The initial components were chosen to include all public APIs and the main core components, and the maintainers were chosen from the most active contributors to those modules.

- Spark core public API: Matei, Patrick, Reynold
- Job scheduler: Matei, Kay, Patrick
- Shuffle and network: Reynold, Aaron, Matei
- Block manager: Reynold, Aaron
- YARN: Tom, Andrew Or
- Python: Josh, Matei
- MLlib: Xiangrui, Matei
- SQL: Michael, Reynold
- Streaming: TD, Matei
- GraphX: Ankur, Joey, Reynold

I'd like to formally call a [VOTE] on this model, to last 72 hours. The [VOTE] will end on Nov 8, 2014 at 6 PM PST.

Matei"
Matei Zaharia <matei.zaharia@gmail.com>,"Wed, 5 Nov 2014 17:33:25 -0800",Re: [VOTE] Designating maintainers for some Spark components,dev <dev@spark.apache.org>,"BTW, my own vote is obviously +1 (binding).

Matei

well as call for an official vote on it on a public list. Basically, as the Spark project scales up, we need to define a model to make sure there is still great oversight of key components (in particular internal architecture and public APIs), and to this end I've proposed implementing a maintainer model for some of these components, similar to other large projects.
We've had over 80 contributors/month for the past 3 months, which I believe makes us the most active project in contributors/month at Apache, as well as over 500 patches/month. The codebase has also grown significantly, with new libraries for SQL, ML, graphs and more.
to assign ""maintainers"" to oversee key components, where each patch to that component needs to get sign-off from at least one of its maintainers. Most existing large projects do this -- at Apache, some large ones with this model are CloudStack (the second-most active project overall), Subversion, and Kafka, and other examples include Linux and Python. This is also by-and-large how Spark operates today -- most components have a de-facto maintainer.
regarding architecture and API. This process would ensure that the component's maintainers see all proposed changes and consider them to fit together in a good way.
particular, it would be easy to look up whoâ€™s responsible for each module and ask them for reviews, etc, rather than having patches slip between the cracks.
applies to certain key components (e.g. scheduler, shuffle) and user-facing APIs (MLlib, GraphX, etc). Over time, as the project grows, we can expand it if we deem it useful. The specific mechanics would be as follows:
where one of the maintainers needs to sign off on each patch to the component.
committers on that component by the PMC. The PMC can vote to add / remove maintainers, and maintained components, through consensus.
their components, though they do not need to be the main reviewers for them (e.g. they might just sign off on architecture / API). To prevent inactive maintainers from blocking the project, if a maintainer isn't responding in a reasonable time period (say 2 weeks), other committers can merge the patch, and the PMC will want to discuss adding another maintainer.
projects:
https://cwiki.apache.org/confluence/display/CLOUDSTACK/CloudStack+Maintainers+Guide <https://cwiki.apache.org/confluence/display/CLOUDSTACK/CloudStack+Maintainers+Guide> 
https://subversion.apache.org/docs/community-guide/roles.html <https://subversion.apache.org/docs/community-guide/roles.html>
and maintainers. It would be good to get feedback on other components we might add, but please note that personnel discussions (e.g. ""I don't think Matei should maintain *that* component) should only happen on the private list. The initial components were chosen to include all public APIs and the main core components, and the maintainers were chosen from the most active contributors to those modules.
The [VOTE] will end on Nov 8, 2014 at 6 PM PST.

"
Timothy Chen <tnachen@gmail.com>,"Wed, 5 Nov 2014 17:35:38 -0800",Re: [VOTE] Designating maintainers for some Spark components,Matei Zaharia <matei.zaharia@gmail.com>,"Hi Matei,

Definitely in favor of moving into this model for exactly the reasons
you mentioned.

and is not listed is the Mesos integration piece.

I believe we also need a maintainer for Mesos, and I wonder if there
is someone that can be added to that?

Tim

te:
 as call for an official vote on it on a public list. Basically, as the Spark project scales up, we need to define a model to make sure there is still great oversight of key components (in particular internal architecture and public APIs), and to this end I've proposed implementing a maintainer model for some of these components, similar to other large projects.
had over 80 contributors/month for the past 3 months, which I believe makes us the most active project in contributors/month at Apache, as well as over 500 patches/month. The codebase has also grown significantly, with new libraries for SQL, ML, graphs and more.
assign ""maintainers"" to oversee key components, where each patch to that component needs to get sign-off from at least one of its maintainers. Most existing large projects do this -- at Apache, some large ones with this model are CloudStack (the second-most active project overall), Subversion, and Kafka, and other examples include Linux and Python. This is also by-and-large how Spark operates today -- most components have a de-facto maintainer.
g architecture and API. This process would ensure that the component's maintainers see all proposed changes and consider them to fit together in a good way.
t would be easy to look up whoâ€™s responsible for each module and ask them for reviews, etc, rather than having patches slip between the cracks.
plies to certain key components (e.g. scheduler, shuffle) and user-facing APIs (MLlib, GraphX, etc). Over time, as the project grows, we can expand it if we deem it useful. The specific mechanics would be as follows:
one of the maintainers needs to sign off on each patch to the component.
mitters on that component by the PMC. The PMC can vote to add / remove maintainers, and maintained components, through consensus.
r components, though they do not need to be the main reviewers for them (e.g. they might just sign off on architecture / API). To prevent inactive maintainers from blocking the project, if a maintainer isn't responding in a reasonable time period (say 2 weeks), other committers can merge the patch, and the PMC will want to discuss adding another maintainer.
jects:
dStack+Maintainers+Guide <https://cwiki.apache.org/confluence/display/CLOUDSTACK/CloudStack+Maintainers+Guide>
ml <https://subversion.apache.org/docs/community-guide/roles.html>
 maintainers. It would be good to get feedback on other components we might add, but please note that personnel discussions (e.g. ""I don't think Matei should maintain *that* component) should only happen on the private list. The initial components were chosen to include all public APIs and the main core components, and the maintainers were chosen from the most active contributors to those modules.
VOTE] will end on Nov 8, 2014 at 6 PM PST.

---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Wed, 5 Nov 2014 17:36:40 -0800",Re: [VOTE] Designating maintainers for some Spark components,Matei Zaharia <matei.zaharia@gmail.com>,"+1 (binding)


he
 a
e
es
er
o
t
,
 ask them
ng
e
em
e
n
ners+Guide
ners+Guide
k
he
e
"
Reynold Xin <rxin@databricks.com>,"Wed, 5 Nov 2014 17:38:31 -0800",Re: [VOTE] Designating maintainers for some Spark components,Matei Zaharia <matei.zaharia@gmail.com>,"+1 (binding)

We are already doing this implicitly. In my experience, this can create
longer term personal commitment, which usually leads to better design
decisions if somebody knows they would need to look after something for a
while.


he
 a
e
es
er
o
"
Nan Zhu <zhunanmcgill@gmail.com>,"Wed, 5 Nov 2014 20:55:59 -0500",Re: [VOTE] Designating maintainers for some Spark components,Matei Zaharia <matei.zaharia@gmail.com>,"+1, with a question

Will these maintainers have a cleanup for those pending PRs upon we start to apply this model? there are some patches always being there but havenâ€™t been  merged, some of which are periodically maintained (rebase, ping , etcâ€¦.), t"
Matei Zaharia <matei.zaharia@gmail.com>,"Wed, 5 Nov 2014 17:50:16 -0800",Re: [VOTE] Designating maintainers for some Spark components,Timothy Chen <tnachen@gmail.com>,"Hi Tim,

We can definitely add one for that if the component grows larger or becomes harder to maintain. The main reason I didn't propose one is that the Mesos integration is actually a lot simpler than YARN at the moment, partly because we support several YARN versions that have incompatible APIs. But so far our modus operandi has been to ask Mesos contributors to review patches that touch it.

We didn't want to add a lot of components at the beginning partly to minimize overhead, but we can revisit it later. It would definitely be bad if we break Mesos support.

Matei

with
well as call for an official vote on it on a public list. Basically, as the Spark project scales up, we need to define a model to make sure there is still great oversight of key components (in particular internal architecture and public APIs), and to this end I've proposed implementing a maintainer model for some of these components, similar to other large projects.
We've had over 80 contributors/month for the past 3 months, which I believe makes us the most active project in contributors/month at Apache, as well as over 500 patches/month. The codebase has also grown significantly, with new libraries for SQL, ML, graphs and more.
to assign ""maintainers"" to oversee key components, where each patch to that component needs to get sign-off from at least one of its maintainers. Most existing large projects do this -- at Apache, some large ones with this model are CloudStack (the second-most active project overall), Subversion, and Kafka, and other examples include Linux and Python. This is also by-and-large how Spark operates today -- most components have a de-facto maintainer.
regarding architecture and API. This process would ensure that the component's maintainers see all proposed changes and consider them to fit together in a good way.
particular, it would be easy to look up whoâ€™s responsible for each module and ask them for reviews, etc, rather than having patches slip between the cracks.
only applies to certain key components (e.g. scheduler, shuffle) and user-facing APIs (MLlib, GraphX, etc). Over time, as the project grows, we can expand it if we deem it useful. The specific mechanics would be as follows:
where one of the maintainers needs to sign off on each patch to the component.
committers on that component by the PMC. The PMC can vote to add / remove maintainers, and maintained components, through consensus.
their components, though they do not need to be the main reviewers for them (e.g. they might just sign off on architecture / API). To prevent inactive maintainers from blocking the project, if a maintainer isn't responding in a reasonable time period (say 2 weeks), other committers can merge the patch, and the PMC will want to discuss adding another maintainer.
projects:
https://cwiki.apache.org/confluence/display/CLOUDSTACK/CloudStack+Maintainers+Guide <https://cwiki.apache.org/confluence/display/CLOUDSTACK/CloudStack+Maintainers+Guide>
https://subversion.apache.org/docs/community-guide/roles.html <https://subversion.apache.org/docs/community-guide/roles.html>
and maintainers. It would be good to get feedback on other components we might add, but please note that personnel discussions (e.g. ""I don't think Matei should maintain *that* component) should only happen on the private list. The initial components were chosen to include all public APIs and the main core components, and the maintainers were chosen from the most active contributors to those modules.
The [VOTE] will end on Nov 8, 2014 at 6 PM PST.


---------------------------------------------------------------------


"
Sandy Ryza <sandy.ryza@cloudera.com>,"Wed, 5 Nov 2014 17:57:52 -0800",Re: [VOTE] Designating maintainers for some Spark components,Michael Armbrust <michael@databricks.com>,"This seems like a good idea.

An area that wasn't listed, but that I think could strongly benefit from
maintainers, is the build.  Having consistent oversight over Maven, SBT,
and dependencies would allow us to avoid subtle breakages.

Component maintainers have come up several times within the Hadoop project,
and I think one of the main reasons the proposals have been rejected is
that, structurally, its effect is to slow down development.  As you
mention, this is somewhat mitigated if being a maintainer leads committers
to take on more responsibility, but it might be worthwhile to draw up more
specific ideas on how to combat this?  E.g. do obvious changes, doc fixes,
test fixes, etc. always require a maintainer?

-Sandy


s
t
o
it
r,
nd ask
y
nd
.
ve
ners+Guide
ners+Guide
e
te
he
"
Patrick Wendell <pwendell@gmail.com>,"Wed, 5 Nov 2014 18:08:36 -0800",Re: [VOTE] Designating maintainers for some Spark components,Sandy Ryza <sandy.ryza@cloudera.com>,"I'm a +1 on this as well, I think it will be a useful model as we
scale the project in the future and recognizes some informal process
we have now.

To respond to Sandy's comment: for changes that fall in between the
component boundaries or are straightforward, my understanding of this
model is you wouldn't need an explicit sign off. I think this is why
unlike some other projects, we wouldn't e.g. lock down permissions to
portions of the source tree. If some obvious fix needs to go in,
people should just merge it.

- Patrick


---------------------------------------------------------------------


"
Andrew Or <andrew@databricks.com>,"Wed, 5 Nov 2014 18:16:42 -0800",Re: [VOTE] Designating maintainers for some Spark components,Patrick Wendell <pwendell@gmail.com>,"+1

2014-11-05 18:08 GMT-08:00 Patrick Wendell <pwendell@gmail.com>:

"
Prashant Sharma <scrapcodes@gmail.com>,"Thu, 6 Nov 2014 07:50:20 +0530",Re: [VOTE] Designating maintainers for some Spark components,Matei Zaharia <matei.zaharia@gmail.com>,"+1, Sounds good.

Now I know whom to ping for what, even if I did not follow the whole
history of the project very carefully.

Prashant Sharma




 a
es
er
t
,
g
 a
t
k them for
ng
r
g.
n
ners+Guide
ners+Guide
t
i
n
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 5 Nov 2014 21:29:34 -0500",Re: [VOTE] Designating maintainers for some Spark components,Nan Zhu <zhunanmcgill@gmail.com>,"+1 on this proposal.




I second Nan's question. I would like to see this initiative drive a
reduction in the number of stale PRs we have out there. We're approaching
300 open PRs again.

Nick
"
Matei Zaharia <matei.zaharia@gmail.com>,"Wed, 5 Nov 2014 18:34:34 -0800",Re: Surprising Spark SQL benchmark,Reynold Xin <rxin@databricks.com>,"Yup, the Hadoop nodes were from 2013, each with 64 GB RAM, 12 cores, 10 Gbps Ethernet and 12 disks. For 100 TB of data, the intermediate data could fit in memory on this cluster, which can make shuffle much faster than with intermediate data on SSDs. You can find the specs in http://sortbenchmark.org/Yahoo2013Sort.pdf. It just takes effort to utilize modern machines fully -- for instance the Yahoo! cluster had 1 TB/s network bandwidth, but only sorted data at 0.02 TB/s. Systems optimized for sorting, like TritonSort (which also won this year's benchmark), get much closer to full utilization.

Matei

measures.
old
What I
post
two
record
forgetting to
Gray
benchmark
http://databricks.com/blog/2014/11/05/spark-officially-sets-a-new-record-in-l
entity to
confidential,
reader
notified that
immediately


---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 5 Nov 2014 22:36:46 -0500",create_image.sh contains broken hadoop web link,dev <dev@spark.apache.org>,"As part of my work for SPARK-3821
<https://issues.apache.org/jira/browse/SPARK-3821>, I tried building an AMI
today using create_image.sh.

This line
<https://github.com/mesos/spark-ec2/blob/f6773584dd71afc49f1225be48439653313c0341/create_image.sh#L68>
appears to be broken now (it wasnâ€™t a week or so ago).

This link appears to be broken:
http://apache.mirrors.tds.net/hadoop/common/hadoop-2.4.1/hadoop-2.4.1-src.tar.gz

Is this temporary? Should we update this to something else?

Nick
â€‹
"
Ted Yu <yuzhihong@gmail.com>,"Wed, 5 Nov 2014 19:43:01 -0800",Re: create_image.sh contains broken hadoop web link,Nicholas Chammas <nicholas.chammas@gmail.com>,"Have you seen this thread ?

http://search-hadoop.com/m/LgpTk2Pnw6O/andrew+apache+mirror&subj=Re+All+mirrored+download+links+from+the+Apache+Hadoop+site+are+broken

Cheers

m

13c0341/create_image.sh#L68
.tar.gz
"
Mark Hamstra <mark@clearstorydata.com>,"Wed, 5 Nov 2014 19:52:10 -0800",Re: [VOTE] Designating maintainers for some Spark components,dev <dev@spark.apache.org>,"+1 (binding)


"
Matt Cheah <mcheah@palantir.com>,"Thu, 6 Nov 2014 03:52:55 +0000",[Classloading] Strange class loading issue,"""dev@spark.apache.org"" <dev@spark.apache.org>, Patrick Wendell
	<pwendell@gmail.com>, Matei Zaharia <matei.zaharia@gmail.com>","Hi everyone,

I¹m running into a strange class loading issue when running a Spark job,
using Spark 1.0.2.

I¹m running a process where some Java code is compiled dynamically into a
jar and added to the Spark context via addJar(). It is also added to the
class loader of the thread that created the Spark context. When I try to run
any job that only references the dynamically-compiled class on the workers,
and then convert them to some other value (say integers) before collecting
the result at the driver, the job completes successfully.

We¹re using Kryo serialization. When I try to run a similar workflow but
request for objects containing fields of the type of the dynamically
compiled class at the driver (say by collect()) the job breaks with the
following exception:

""Caused by: java.lang.RuntimeException:
java.security.PrivilegedActionException: org.apache.spark.SparkException:
Job aborted due to stage failure: Exception while getting task result:
com.esotericsoftware.kryo.KryoException: Unable to find class: TBEjava1""

Where ³TBEjava1² is the name of the dynamically compiled class.

Here is what I can deduce from my debugging:
1. The class is loaded on the thread that launches the Spark context and
calls reduce(). To check, I put a breakpoint right before my reduce() call
and used Class.forName(""TBEjava1"", false,
Thread.currentThread().getContextClassLoader()); and got back a valid class
object without ClassNotFoundException being thrown.
2. The worker threads can also refer to the class. I put breakpoints in the
worker methods (using local[N] mode for the context for now) and they
complete the mapping and reducing functions successfully.
3. The Kryo serializer calls readClass() and then calls Class.forName(Š)
inside Kryo, using the class loader in that Kryo object. The class not found
exception is thrown there, however the stack trace doesn¹t appear as such.
I¹m wondering if we might be running into
https://issues.apache.org/jira/browse/SPARK-3046 or something. I looked a
bit at the Spark code, and from what I understand, the thread pool created
for task result getter does not inherit the context class loader of the
thread that created the Spark Context, which would explain why the task
result getter threads can¹t find classes even though they are available via
addJar().

Any suggestions for a workaround? Feel free to correct me on any incorrect
observations I¹ve made as well.

Thanks,

-Matt Cheah


"
Xiangrui Meng <mengxr@gmail.com>,"Wed, 5 Nov 2014 20:04:16 -0800",Re: [VOTE] Designating maintainers for some Spark components,Mark Hamstra <mark@clearstorydata.com>,"+1 (binding)


---------------------------------------------------------------------


"
Denny Lee <denny.g.lee@gmail.com>,"Thu, 06 Nov 2014 04:05:51 +0000",Re: [VOTE] Designating maintainers for some Spark components,"Xiangrui Meng <mengxr@gmail.com>, Mark Hamstra <mark@clearstorydata.com>","+1 great idea.

"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 5 Nov 2014 23:07:40 -0500",Re: create_image.sh contains broken hadoop web link,Ted Yu <yuzhihong@gmail.com>,"Nope, thanks for pointing me to it.

Doesn't look like there is a resolution to the issue. Also, the like you
pointed to also appears to be broken now:
http://apache.mesi.com.ar/hadoop/common/

Nick


+mirrored+download+links+from+the+Apache+Hadoop+site+are+broken
313c0341/create_image.sh#L68
c.tar.gz
"
Ted Yu <yuzhihong@gmail.com>,"Wed, 5 Nov 2014 20:13:48 -0800",Re: create_image.sh contains broken hadoop web link,Nicholas Chammas <nicholas.chammas@gmail.com>,"The artifacts are in archive:
http://archive.apache.org/dist/hadoop/common/hadoop-2.4.1/

Cheers

ote:

ointed to also appears to be broken now: http://apache.mesi.com.ar/hadoop/common/
+mirrored+download+links+from+the+Apache+Hadoop+site+are+broken
MI
3313c0341/create_image.sh#L68>
c.tar.gz
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 5 Nov 2014 23:16:29 -0500",Re: create_image.sh contains broken hadoop web link,Ted Yu <yuzhihong@gmail.com>,"Yup, I just stumbled on that. I'll submit a PR to fix that link. Thanks Ted.


l+mirrored+download+links+from+the+Apache+Hadoop+site+are+broken
3313c0341/create_image.sh#L68
rc.tar.gz
"
"""Wangfei (X)"" <wangfei1@huawei.com>","Thu, 6 Nov 2014 04:40:05 +0000",Re: [VOTE] Designating maintainers for some Spark components,Denny Lee <denny.g.lee@gmail.com>,"+1

·¢×ÔÎÒµÄ iPhone

> ÔÚ 2014Äê11ÔÂ5ÈÕ£¬20:06£¬""Denny Lee"" <denny.g.lee@gmail.com> Ğ´µÀ£º
> 
> +1 great idea.
>> On Wed, Nov 5, 2014 at 20:04 Xiangrui Meng <mengxr@gmail.com> wrote:
>> 
>> +1 (binding)
>> 
>> On Wed, Nov 5, 2014 at 7:52 PM, Ma"
Cheng Lian <lian.cs.zju@gmail.com>,"Thu, 6 Nov 2014 12:46:15 +0800",Re: [VOTE] Designating maintainers for some Spark components,dev <dev@spark.apache.org>,"+1 since this is already the de facto model we are using.


ny Lee"" <denny.g.lee@gmail.com> å†™é“ï¼š
"
Timothy Chen <tnachen@gmail.com>,"Wed, 5 Nov 2014 20:48:42 -0800",Re: [VOTE] Designating maintainers for some Spark components,Cheng Lian <lian.cs.zju@gmail.com>,"Matei that makes sense, +1 (non-binding)

Tim

nny Lee"" <denny.g.lee@gmail.com> å†™é“ï¼š
e
a

---------------------------------------------------------------------


"
Jeremy Freeman <freeman.jeremy@gmail.com>,"Wed, 5 Nov 2014 23:53:38 -0500",Re: [VOTE] Designating maintainers for some Spark components,Timothy Chen <tnachen@gmail.com>,"Great idea! +1

â€” Jeremy

-------------------------
jeremyfreeman.net
@thefreemanlab


enny Lee"" <denny.g.lee@gmail.com> å†™é“ï¼š
<mark@clearstorydata.com>
<zhunanmcgill@gmail.com>
upon we
drive a
---------------------------------------------------------------------
---------------------------------------------------------------------

"
"""Cheng, Hao"" <hao.cheng@intel.com>","Thu, 6 Nov 2014 04:54:33 +0000",RE: [VOTE] Designating maintainers for some Spark components,dev <dev@spark.apache.org>,"+1, that definitely will speeds up the PR reviewing / merging.

-----Original Message-----
From: Cheng Lian [mailto:lian.cs.zju@gmail.com] 
Sent: Thursday, November 6, 2014 12:46 PM
To: dev
Subject: Re: [VOTE] Designating maintainers for some Spark "
jackylk <jacky.likun@huawei.com>,"Wed, 5 Nov 2014 21:11:03 -0800 (PST)",Re: [VOTE] Designating maintainers for some Spark components,dev@spark.incubator.apache.org,"+1 Great idea!



--

---------------------------------------------------------------------


"
Kousuke Saruta <sarutak@oss.nttdata.co.jp>,"Wed, 05 Nov 2014 21:19:32 -0800",Re: [VOTE] Designating maintainers for some Spark components,"Matei Zaharia <matei.zaharia@gmail.com>, dev@spark.apache.org","+1, It makes sense!

- Kousuke



---------------------------------------------------------------------


"
Reza Zadeh <reza@databricks.com>,"Wed, 5 Nov 2014 21:41:49 -0800",Re: [VOTE] Designating maintainers for some Spark components,Kousuke Saruta <sarutak@oss.nttdata.co.jp>,"+1, sounds good.


l
g a
kes
ver
st
n,
t
d ask them
ing
d
e
hem
ve
in
d
ht
ei
.
in
"
Xuefeng Wu <benewu@gmail.com>,"Thu, 6 Nov 2014 14:22:29 +0800",Re: [VOTE] Designating maintainers for some Spark components,Matei Zaharia <matei.zaharia@gmail.com>,"+1  it make more focus and more consistence. 
 

Yours, Xuefeng Wu ÎâÑ©·å ¾´ÉÏ

s call for an official vote on it on a public list. Basically, as the Spark project scales up, we need to define a model to make sure there is still great oversight of key com"
Sean Owen <sowen@cloudera.com>,"Thu, 6 Nov 2014 06:46:39 +0000",Re: [VOTE] Designating maintainers for some Spark components,Matei Zaharia <matei.zaharia@gmail.com>,"Naturally, this sounds great. FWIW my only but significant worry about
Spark is scaling up to meet unprecedented demand in the form of
questions and contribution. Clarifying responsibility and ownership
helps more than it hurts by adding process.

This is related but different topic, but, I wonder out loud what this
can do to help clear the backlog -- ~*1200* open JIRAs and ~300 open
PRs, most of which have de facto already fallen between some cracks.
This harms the usefulness of these tools and processes.

I'd love to see this translate into triage / closing of most of it by
maintainers, and new actions and strategies for increasing
'throughput' in review and/or helping people make better contributions
in the first place.

te:
 as call for an official vote on it on a public list. Basically, as the Spark project scales up, we need to define a model to make sure there is still great oversight of key components (in particular internal architecture and public APIs), and to this end I've proposed implementing a maintainer model for some of these components, similar to other large projects.

---------------------------------------------------------------------


"
Matei Zaharia <matei.zaharia@gmail.com>,"Wed, 5 Nov 2014 23:21:00 -0800",Re: [VOTE] Designating maintainers for some Spark components,Sean Owen <sowen@cloudera.com>,"Several people asked about having maintainers review the PR queue for their modules regularly, and I like that idea. We have a new tool now to help with that in https://spark-prs.appspot.com.

In terms of the set of open PRs itself, it is large but note that there are also 2800 *closed* PRs, which means we close the majority of PRs (and I don't know the exact stats but I'd guess that 90% of those are accepted and merged). I think one problem is that with GitHub, people often develop something as a PR and have a lot of discussion on there (including whether we even want the feature). I recently updated our ""how to contribute"" page to encourage opening a JIRA and having discussions on the dev list first, but I do think we need to be faster with closing ones that we don't have a plan to merge. Note that Hadoop, Hive, HBase, etc also have about 300 issues each in the ""patch available"" state, so this is some kind of universal constant :P.

Matei


well as call for an official vote on it on a public list. Basically, as the Spark project scales up, we need to define a model to make sure there is still great oversight of key components (in particular internal architecture and public APIs), and to this end I've proposed implementing a maintainer model for some of these components, similar to other large projects.


---------------------------------------------------------------------


"
"""=?gb18030?B?d2l0Z28=?="" <witgo@qq.com>","Thu, 6 Nov 2014 15:30:44 +0800","=?gb18030?B?u9i4tKO6IFtWT1RFXSBEZXNpZ25hdGluZyBtYWlu?=
 =?gb18030?B?dGFpbmVycyBmb3Igc29tZSBTcGFyayBjb21wb25l?=
 =?gb18030?B?bnRz?=","""=?gb18030?B?TWF0ZWkgWmFoYXJpYQ==?="" <matei.zaharia@gmail.com>, ""=?gb18030?B?U2VhbiBPd2Vu?="" <sowen@cloudera.com>","+1




------------------ Ô­Ê¼ÓÊ¼ş ------------------
·¢¼şÈË: ""Matei Zaharia""<matei.zaharia@gmail.com>; 
·¢ËÍÊ±¼ä: 2014Äê11ÔÂ6ÈÕ(ĞÇÆÚËÄ) ÏÂÎç3:21
ÊÕ¼şÈË: ""Sean Owen""<sowen@cloudera.com>; 
³­ËÍ: ""dev""<dev@spark.apache.org>; 
Ö÷Ìâ: Re: [VOTE] Desi"
Manoj Babu <manoj444@gmail.com>,"Thu, 6 Nov 2014 13:02:22 +0530",Re: [VOTE] Designating maintainers for some Spark components,Matei Zaharia <matei.zaharia@gmail.com>,"+1

Cheers!
Manoj.


"
"""=?gb18030?B?d2l0Z28=?="" <witgo@qq.com>","Thu, 6 Nov 2014 15:30:44 +0800","=?gb18030?B?u9i4tKO6IFtWT1RFXSBEZXNpZ25hdGluZyBtYWlu?=
 =?gb18030?B?dGFpbmVycyBmb3Igc29tZSBTcGFyayBjb21wb25l?=
 =?gb18030?B?bnRz?=","""=?gb18030?B?TWF0ZWkgWmFoYXJpYQ==?="" <matei.zaharia@gmail.com>, ""=?gb18030?B?U2VhbiBPd2Vu?="" <sowen@cloudera.com>","+1




------------------ Ô­Ê¼ÓÊ¼ş ------------------
·¢¼şÈË: ""Matei Zaharia""<matei.zaharia@gmail.com>; 
·¢ËÍÊ±¼ä: 2014Äê11ÔÂ6ÈÕ(ĞÇÆÚËÄ) ÏÂÎç3:21
ÊÕ¼şÈË: ""Sean Owen""<sowen@cloudera.com>; 
³­ËÍ: ""dev""<dev@spark.apache.org>; 
Ö÷Ìâ: Re: [VOTE] Desi"
Liquan Pei <liquanpei@gmail.com>,"Wed, 5 Nov 2014 23:48:21 -0800",Re: [VOTE] Designating maintainers for some Spark components,Manoj Babu <manoj444@gmail.com>,"+1

Liquan





-- 
Liquan Pei
Department of Physics
University of Massachusetts Amherst
"
Ravindra pesala <ravindra.pesala@huawei.com>,"Thu, 6 Nov 2014 07:46:05 +0000",RE: [VOTE] Designating maintainers for some Spark components,"Manoj Babu <manoj444@gmail.com>, Matei Zaharia <matei.zaharia@gmail.com>","+1

Thanks & Regards,
Ravindra Pesala

________________________________________
From: Manoj Babu [manoj444@gmail.com]
Sent: Thursday, November 06, 2014 1:02 PM
To: Matei Zaharia
Cc: Sean Owen; dev
Subject: Re: [VOTE] Designating maintainers for some Spark"
Ankur Dave <ankurdave@gmail.com>,"Thu, 6 Nov 2014 00:02:51 -0800",Re: [VOTE] Designating maintainers for some Spark components,dev <dev@spark.apache.org>,"+1 (binding)

Ankur <http://www.ankurdave.com/>


"
Kushal Datta <kushal.datta@gmail.com>,"Thu, 6 Nov 2014 00:30:17 -0800",Re: [VOTE] Designating maintainers for some Spark components,Ankur Dave <ankurdave@gmail.com>,"+1 (binding)

For tickets which span across multiple components, will it need to be
approved by all maintainers? For example, I'm working on the Python
bindings of GraphX where code is added to both Python and GraphX modules.

Thanks,
-Kushal.


"
Shixiong Zhu <zsxwing@gmail.com>,"Thu, 6 Nov 2014 19:12:37 +0800",About implicit rddToPairRDDFunctions,dev@spark.apache.org,"I saw many people asked how to convert a RDD to a PairRDDFunctions. I would
like to ask a question about it. Why not put the following implicit into
""pacakge object rdd"" or ""object rdd""?

  implicit def rddToPairRDDFunctions[K, V](rdd: RDD[(K, V)])
      (implicit kt: ClassTag[K], vt: ClassTag[V], ord: Ordering[K] = null)
= {
    new PairRDDFunctions(rdd)
  }

If so, the converting will be automatic and not need to
import org.apache.spark.SparkContext._

I tried to search some discussion but found nothing.

Best Regards,
Shixiong Zhu
"
Sean Owen <sowen@cloudera.com>,"Thu, 6 Nov 2014 12:13:01 +0000",JIRA + PR backlog,Matei Zaharia <matei.zaharia@gmail.com>,"(Different topic, indulge me one more reply --)

Yes the number of JIRAs/PRs closed is unprecedented too and that
deserves big praise. The project has stuck to making all changes and
discussion in this public process, which is so powerful. Adjusted for
the sheer inbound volume, Spark is doing a much better job than other
projects; I would not hold them up as a benchmark of 'good enough', to
be honest.

JIRA is usually under-managed and it's a pet issue of mine. My motive
is that core contributor / committer time is very valuable and in
changes and fix bugs in the core that only the very experienced can.
build a business, etc.

So I harp on JIRA management as a way to save time:
- Merging PRs sooner means less rebasing / retesting
- Bouncing back bad PRs/JIRAs early teaches everyone what's acceptable
as a good PR/JIRA and prevents the noise in the first place
- Resolving issues soon prevents duplicates from being filed
- Recording 'WontFix' resolutions early heads off repeated
discussion/work on out of scope topics

I have more concrete ideas about managing this but it's not for now.
For now, thanks for zapping some old JIRAs this morning and for
endorsing the idea of staying on top of the issue list in general. As
a long-time fan I hope I can help from the sidelines by also closing
JIRAs I'm all but certain are stale, and review minor PRs to clear the
way for maintainers to take on the more important work.


te:
ir modules regularly, and I like that idea. We have a new tool now to help with that in https://spark-prs.appspot.com.
re also 2800 *closed* PRs, which means we close the majority of PRs (and I don't know the exact stats but I'd guess that 90% of those are accepted and merged). I think one problem is that with GitHub, people often develop something as a PR and have a lot of discussion on there (including whether we even want the feature). I recently updated our ""how to contribute"" page to encourage opening a JIRA and having discussions on the dev list first, but I do think we need to be faster with closing ones that we don't have a plan to merge. Note that Hadoop, Hive, HBase, etc also have about 300 issues each in the ""patch available"" state, so this is some kind of universal constant :P.

---------------------------------------------------------------------


"
Imran Rashid <imran@therashids.com>,"Thu, 6 Nov 2014 07:39:40 -0600",Re: [VOTE] Designating maintainers for some Spark components,dev <dev@spark.apache.org>,"+1 overall

also +1 to Sandy's suggestion to getting build maintainers as well.


t,
s
e
,
s
as
s
w
is
is
 and ask
le
r
r
ng
e
ng
ners+Guide
ners+Guide
ts
nd
"
Jason Dai <jason.dai@gmail.com>,"Thu, 6 Nov 2014 21:48:53 +0800",Re: [VOTE] Designating maintainers for some Spark components,Ankur Dave <ankurdave@gmail.com>,"+1 (binding)


"
RJ Nowling <rnowling@gmail.com>,"Thu, 6 Nov 2014 09:06:55 -0500",Re: [VOTE] Designating maintainers for some Spark components,"""dev@spark.apache.org"" <dev@spark.apache.org>","Matei,

I saw that you're listed as a maintainer for ~6 different subcomponents,
and on over half of those, you're only the 2nd person.  My concern is that
you would be stretched thin and maybe wouldn't be able to work as a ""back
up"" on all of those subcomponents.  Are you planning on adding more
maintainers for each subcomponent?  I think it would be good to have 2
regulars + backups for each.

RJ





-- 
em rnowling@gmail.com
c 954.496.2314
"
Tom Graves <tgraves_cs@yahoo.com.INVALID>,"Thu, 6 Nov 2014 14:52:08 +0000 (UTC)",Re: [VOTE] Designating maintainers for some Spark components,"Matei Zaharia <matei.zaharia@gmail.com>, dev <dev@spark.apache.org>","+1.
Tom 

   

 BTW, my own vote is obviously +1 (binding).

Matei

:
 as call for an official vote on it on a public list. Basically, as the Spark project scales up, we need to define a model to make sure there is still great oversight of key components "
Sean McNamara <Sean.McNamara@Webtrends.com>,"Thu, 6 Nov 2014 16:47:34 +0000",Re: [VOTE] Designating maintainers for some Spark components,Matei Zaharia <matei.zaharia@gmail.com>,"+1

Sean


 as call for an official vote on it on a public list. Basically, as the Spark project scales up, we need to define a model to make sure there is still great oversight of key components (in particular internal architecture and public APIs), and "
Debasish Das <debasish.das83@gmail.com>,"Thu, 6 Nov 2014 08:52:23 -0800",Re: [VOTE] Designating maintainers for some Spark components,Sean McNamara <Sean.McNamara@webtrends.com>,"+1

The app to track PRs based on component is a great idea...


:
he
 a
e
es
er
o
t
,
 ask them
ng
e
em
e
n
ners+Guide
ners+Guide
k
he
e
"
"""Nick Pentreath"" <nick.pentreath@gmail.com>","Thu, 06 Nov 2014 09:28:14 -0800 (PST)",Re: [VOTE] Designating maintainers for some Spark components,dev@spark.apache.org,"+1 (binding)

â€”
Sent from Mailbox


com>
the
is
implementing a
We've
makes
over
to
that
Most
Subversion,
de-facto
fit
,
and ask them
only
user-facing
expand
where

remove
them
inactive
in
iners+Guide
iners+Guide
we
think
private
the
active
The"
Josh Rosen <rosenville@gmail.com>,"Thu, 6 Nov 2014 10:00:41 -0800",Re: [VOTE] Designating maintainers for some Spark components,dev <dev@spark.apache.org>,"+1 (binding).

(our pull request browsing tool is open-source, by the way; contributions
welcome: https://github.com/databricks/spark-pr-dashboard)


s
is
s
at
s
to
and ask
ly
t.
e
g
g
ners+Guide
ners+Guide
s
we
d
"
bc Wong <bcwalrus@cloudera.com>,"Thu, 6 Nov 2014 10:53:12 -0800",Re: [VOTE] Designating maintainers for some Spark components,Matei Zaharia <matei.zaharia@gmail.com>,"Hi Matei,

Good call on scaling the project itself. Identifying domain experts in
different areas is a good thing. But I have some questions about the
implementation. Here's my understanding of the proposal:

(1) The PMC votes on a list of components and their maintainers. Changes to
that list requires PMC approval.
(2) No committer shall commit changes to a component without a +1 from a
maintainer of that component.

I see good reasons for #1, to help people navigate the project and identify
expertise. For #2, I'd like to understand what problem it's trying to
solve. Do we have rogue committers committing to areas that they don't know
much about? If that's the case, we should address it directly, instead of
adding new processes.

To point out the obvious, it completely changes what ""committers"" means in
Spark. Do we have clear promotion criteria from ""committer"" to
""maintainer""? Is there a max number of maintainers per area Currently, as
committers gains expertise in new areas, they could start reviewing code in
those areas and give +1. This encourages more contributions and
cross-component knowledge sharing. Under the new proposal, they now have to
be promoted to ""maintainers"" first. That reduces our review bandwidth.

Again, if there is a quality issue with code reviews, let's talk to those
committers and help them do better. There are non-process ways to solve the
problem.

So I think we shouldn't require ""maintainer +1"". I do like the idea of
having explicit maintainers on a volunteer basis. These maintainers should
watch their jira and PR traffic, and be very active in design & API
discussions. That leads to better consistency and long-term design choices.

Cheers,
bc


 a
es
er
t
,
g
 a
t
k them for
ng
r
g.
n
ners+Guide
ners+Guide
t
i
n
"
Matei Zaharia <matei.zaharia@gmail.com>,"Thu, 6 Nov 2014 11:25:14 -0800",Re: [VOTE] Designating maintainers for some Spark components,bc Wong <bcwalrus@cloudera.com>,"Hi BC,

The point is exactly to ensure that the maintainers have looked at each patch to that component and consider it to fit consistently into its architecture. The issue is not about ""rogue"" committers, it's about making sure that changes don't accidentally sneak in that we want to roll back, particularly because we have frequent releases and we guarantee API stability. This process is meant to ensure that whichever committer reviews a patch also forwards it to its maintainers.

Note that any committer is able to review patches in any component. The maintainer sign-off is just a second requirement for some core components (central parts of the system and public APIs). But I expect that most maintainers will let others do the bulk of the reviewing and focus only on changes to the architecture or API.

Ultimately, the core motivation is that the project has grown to the point where it's hard to expect every committer to have full understanding of every component. Some committers know a ton about systems but little about machine learning, some are algorithmic whizzes but may not realize the implications of changing something on the Python API, etc. This is just a way to make sure that a domain expert has looked at the areas where it is most likely for something to go wrong.

Matei

different areas is a good thing. But I have some questions about the implementation. Here's my understanding of the proposal:
Changes to that list requires PMC approval.
a maintainer of that component.
identify expertise. For #2, I'd like to understand what problem it's trying to solve. Do we have rogue committers committing to areas that they don't know much about? If that's the case, we should address it directly, instead of adding new processes.
means in Spark. Do we have clear promotion criteria from ""committer"" to ""maintainer""? Is there a max number of maintainers per area Currently, as committers gains expertise in new areas, they could start reviewing code in those areas and give +1. This encourages more contributions and cross-component knowledge sharing. Under the new proposal, they now have to be promoted to ""maintainers"" first. That reduces our review bandwidth.
those committers and help them do better. There are non-process ways to solve the problem.
having explicit maintainers on a volunteer basis. These maintainers should watch their jira and PR traffic, and be very active in design & API discussions. That leads to better consistency and long-term design choices.
well as call for an official vote on it on a public list. Basically, as the Spark project scales up, we need to define a model to make sure there is still great oversight of key components (in particular internal architecture and public APIs), and to this end I've proposed implementing a maintainer model for some of these components, similar to other large projects.
We've had over 80 contributors/month for the past 3 months, which I believe makes us the most active project in contributors/month at Apache, as well as over 500 patches/month. The codebase has also grown significantly, with new libraries for SQL, ML, graphs and more.
to assign ""maintainers"" to oversee key components, where each patch to that component needs to get sign-off from at least one of its maintainers. Most existing large projects do this -- at Apache, some large ones with this model are CloudStack (the second-most active project overall), Subversion, and Kafka, and other examples include Linux and Python. This is also by-and-large how Spark operates today -- most components have a de-facto maintainer.
regarding architecture and API. This process would ensure that the component's maintainers see all proposed changes and consider them to fit together in a good way.
particular, it would be easy to look up whoâ€™s responsible for each module and ask them for reviews, etc, rather than having patches slip between the cracks.
applies to certain key components (e.g. scheduler, shuffle) and user-facing APIs (MLlib, GraphX, etc). Over time, as the project grows, we can expand it if we deem it useful. The specific mechanics would be as follows:
where one of the maintainers needs to sign off on each patch to the component.
committers on that component by the PMC. The PMC can vote to add / remove maintainers, and maintained components, through consensus.
their components, though they do not need to be the main reviewers for them (e.g. they might just sign off on architecture / API). To prevent inactive maintainers from blocking the project, if a maintainer isn't responding in a reasonable time period (say 2 weeks), other committers can merge the patch, and the PMC will want to discuss adding another maintainer.
projects:
https://cwiki.apache.org/confluence/display/CLOUDSTACK/CloudStack+Maintainers+Guide <https://cwiki.apache.org/confluence/display/CLOUDSTACK/CloudStack+Maintainers+Guide><https://cwiki.apache.org/confluence/display/CLOUDSTACK/CloudStack+Maintainers+Guide <https://cwiki.apache.org/confluence/display/CLOUDSTACK/CloudStack+Maintainers+Guide>>
https://subversion.apache.org/docs/community-guide/roles.html <https://subversion.apache.org/docs/community-guide/roles.html><https://subversion.apache.org/docs/community-guide/roles.html <https://subversion.apache.org/docs/community-guide/roles.html>>
and maintainers. It would be good to get feedback on other components we might add, but please note that personnel discussions (e.g. ""I don't think Matei should maintain *that* component) should only happen on the private list. The initial components were chosen to include all public APIs and the main core components, and the maintainers were chosen from the most active contributors to those modules.
The [VOTE] will end on Nov 8, 2014 at 6 PM PST.

"
"""York, Brennon"" <Brennon.York@capitalone.com>","Thu, 6 Nov 2014 14:34:33 -0500",Implementing TinkerPop on top of GraphX,"""dev@spark.apache.org"" <dev@spark.apache.org>","All, was wondering if there had been any discussion around this topic yet?
TinkerPop <https://github.com/tinkerpop> is a great abstraction for graph
databases and has been implemented across various graph database backends
/ gaining traction. Has anyone thought about integrating the TinkerPop
framework with GraphX to enable GraphX as another backend? Not sure if
this has been brought up or not, but would certainly volunteer to
spearhead this effort if the community thinks it to be a good idea!

As an aside, wasn¹t sure if this discussion should happen on the board
here or on JIRA, but a made a ticket as well for reference:
https://issues.apache.org/jira/browse/SPARK-4279

________________________________________________________

The information contained in this e-mail is confidential and/or proprietary is intended only for use by the individual or entity to which it is addressed.  If the reader of this message is not the intended recipient, you are hereby notified that any review, retransmission, dissemination, distribution, copying or other use of, or taking of any action in reliance upon this information is strictly prohibited. If you have received this communication in error, please contact the sender and delete the material from your computer.


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Thu, 6 Nov 2014 11:42:49 -0800",Re: Implementing TinkerPop on top of GraphX,"""York, Brennon"" <Brennon.York@capitalone.com>","cc Matthias

In the past we talked with Matthias and there were some discussions about
this.


?
rd
"
Kyle Ellrott <kellrott@soe.ucsc.edu>,"Thu, 6 Nov 2014 11:48:35 -0800",Re: Implementing TinkerPop on top of GraphX,Reynold Xin <rxin@databricks.com>,"I've taken a crack at implementing the TinkerPop Blueprints API in GraphX (
https://github.com/kellrott/sparkgraph ). I've also implemented portions of
the Gremlin Search Language and a Parquet based graph store.
I've been working out finalize some code details and putting together
better code examples and documentation before I started telling people
about it.
But if you want to start looking at the code, I can answer any questions
you have. And if you would like to contribute, I would really appreciate
the help.

Kyle



ds
oard
ty
r
"
Kushal Datta <kushal.datta@gmail.com>,"Thu, 6 Nov 2014 11:55:21 -0800",Re: Implementing TinkerPop on top of GraphX,Kyle Ellrott <kellrott@soe.ucsc.edu>,"What do you guys think about the Tinkerpop3 Gremlin interface?
It has MapReduce to run Gremlin operators in a distributed manner and
Giraph to execute vertex programs.

The Tinkpop3 is better suited for GraphX.

:

 (
ut
p
f
 board
"
andy petrella <andy.petrella@gmail.com>,"Thu, 6 Nov 2014 20:55:49 +0100",Re: Implementing TinkerPop on top of GraphX,Kyle Ellrott <kellrott@soe.ucsc.edu>,"Great stuffs!
I've got some thoughts about that, and I was wondering if it would be first
interesting to have something like for spark-core (let's say):
0/ Core API offering basic (or advanced â†’ HeLP) primitives
1/ catalyst optimizer for a text base system (SPARQL, Cypher, custom SQL3,
whatnot)
2/ adequate DSL layer on top (Ã  la LinQ)

my2Â¢


aâ„•dy â„™etrella
about.me/noootsab
[image: aâ„•dy â„™etrella on about.me]

<http://about.me/noootsab>


 (
ut
p
f
 board
"
Kyle Ellrott <kellrott@soe.ucsc.edu>,"Thu, 6 Nov 2014 12:10:02 -0800",Re: Implementing TinkerPop on top of GraphX,Kushal Datta <kushal.datta@gmail.com>,"I still have to dig into the Tinkerpop3 internals (I started my work long
before it had been released), but I can say that to get the Tinerpop2
Gremlin pipeline to work in the GraphX was a bit of a hack. The
whole Tinkerpop2 Gremlin design was based around streaming pipes of
data, rather then large distributed map-reduce operations. I had to hack
the pipes to aggregate all of the data and pass a single object wrapping
the GraphX RDDs down the pipes in a single go, rather then streaming it
element by element.
Just based on their description, Tinkerpop3 may be more amenable to the
Spark platform.

Kyle



X
:
c
op
if
e board
,
-
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 6 Nov 2014 16:49:43 -0500",Re: JIRA + PR backlog,Sean Owen <sowen@cloudera.com>,"I think better tooling will make it much easier for committers to trim the
list of stale JIRA issues and PRs. Convenience enables action.

   - Spark PR Dashboard <https://spark-prs.appspot.com/>: Additional
   filters for stale PRs
   <https://github.com/databricks/spark-pr-dashboard/issues/1> or PRs
   waiting on committer response would be great.
   - Stale Spark JIRA issues
   <https://issues.apache.org/jira/issues/?filter=12329614&jql=project%20%3D%20SPARK%20AND%20resolution%20%3D%20Unresolved%20AND%20updated%20%3C%3D%20-90d%20ORDER%20BY%20updated%20ASC>:
   This filter is sorted by the least recently updated issues first and can be
   filtered additionally by component. There are many, many easy wins in this
   filter.

Nick
â€‹


 I
nd
r
e
a
"
Gordon Benjamin <gordon.benjamin65@gmail.com>,"Thu, 6 Nov 2014 22:01:49 +0000",Using partitioning to speed up queries in Shark,dev@spark.apache.org,"Hi All,

I'm using Spark/Shark as the foundation for some reporting that I'm doing
and have a customers table with approximately 3 million rows that I've
cached in memory.

I've also created a partitioned table that I've also cached in memory on a
per day basis

FROM
customers_cached
INSERT OVERWRITE TABLE
part_customers_cached
PARTITION(createday)
SELECT id,email,dt_cr, to_date(dt_cr) as createday where
dt_cr>unix_timestamp('2013-01-01 00:00:00') and
dt_cr<unix_timestamp('2013-12-31 23:59:59');
set exec.dynamic.partition=true;

set exec.dynamic.partition.mode=nonstrict;

however when I run the following basic tests I get this type of performance

[localhost:10000] shark> select count(*) from part_customers_cached where
 createday >= '2014-08-01' and createday <= '2014-12-06';
37204
Time taken (including network latency): 3.131 seconds

[localhost:10000] shark>  SELECT count(*) from customers_cached where
dt_cr>unix_timestamp('2013-08-01 00:00:00') and
dt_cr<unix_timestamp('2013-12-06 23:59:59');
37204
Time taken (including network latency): 1.538 seconds

I'm running this on a cluster with one master and two slaves and was hoping
that the partitioned table would be noticeably faster but it looks as
though the partitioning has slowed things down... Is this the case, or is
there some additional configuration that I need to do to speed things up?

Best Wishes,

Gordon
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 6 Nov 2014 17:07:45 -0500",Re: Using partitioning to speed up queries in Shark,Gordon Benjamin <gordon.benjamin65@gmail.com>,"Did you mean to send this to the user list?

This is the dev list, where we discuss things related to development on
Spark itself.


"
Gary Malouf <malouf.gary@gmail.com>,"Thu, 6 Nov 2014 17:10:21 -0500",Wrong temp directory when compressing before sending text file to S3,"""dev@spark.apache.org"" <dev@spark.apache.org>","We have some data that we are exporting from our HDFS cluster to S3 with
some help from Spark.  The final RDD command we run is:

csvData.saveAsTextFile(""s3n://data/mess/2014/11/dump-oct-30-to-nov-5-gzip"",
classOf[GzipCodec])


We have our 'spark.local.dir' set to our large ephemeral partition on
each slave (on EC2), but with compression on an intermediate format
seems to be written to /tmp/hadoop-root/s3.  Is this a bug in Spark or
are we missing a configuration property?


It's a problem for us because the root disks on EC2 xls are small (~ 5GB).
"
bc Wong <bcwalrus@cloudera.com>,"Thu, 6 Nov 2014 14:18:04 -0800",Re: [VOTE] Designating maintainers for some Spark components,Matei Zaharia <matei.zaharia@gmail.com>,"â€‹<snip>

t
t

â€‹Hi Matei,

I understand where you're coming from. My suggestion is to solve this
without adding a new process. In the example above, those ""algo whizzes""
committers should realize that they're touching the Python API, and loop in
some Python maintainersâ€‹. Those Python maintainers would then respond and
help move the PR along. This is good hygiene and should already be
happening. For example, HDFS committers have commit rights to all of
Hadoop. But none of them would check in YARN code without getting agreement
from the YARN folks.

I think the majority of the effort here will be education and building the
convention. We have to ask committers to watch out for API changes, know
their own limits, and involve the component domain experts. We need that
anyways, which btw also seems to solve the problem. It's not clear what the
new process would add.

It'd be good to know the details, too. What are the exact criteria for a
committer to get promoted to be a maintainer? How often does the PMC
re-evaluate the list of maintainers? Is there an upper bound on the number
of maintainers for a component? Can we have an automatic rule for a
maintainer promotion after X patches or Y lines of code in that area?

Cheers,
bc

ng
ad
n
in
to
he
d
s.
l
g a
kes
ver
st
n,
t
d ask them
ing
d
e
hem
ve
in
iners+Guide
iners+Guide
d
ht
ei
.
in
"
Matei Zaharia <matei.zaharia@gmail.com>,"Thu, 6 Nov 2014 14:51:14 -0800",Re: [VOTE] Designating maintainers for some Spark components,bc Wong <bcwalrus@cloudera.com>,"I think you're misunderstanding the idea of ""process"" here. The point of process is to make sure something happens automatically, which is useful to ensure a certain level of quality. For example, all our patches go through Jenkins, and nobody will make the mistake of merging them if they fail tests, or RAT checks, or API compatibility checks. The idea is to get the same kind of automation for design on these components. This is a very common process for large software projects, and it's essentially what we had already, but formalizing it will make clear that this is the process we want. It's important to do it early in order to be able to refine the process as the project grows.

In terms of scope, again, the maintainers are *not* going to be the only reviewers for that component, they are just a second level of sign-off required for architecture and API. Being a maintainer is also not a ""promotion"", it's a responsibility. Since we don't have much experience yet with this model, I didn't propose automatic rules beyond that the PMC can add / remove maintainers -- presumably the PMC is in the best position to know what the project needs. I think automatic rules are exactly the kind of ""process"" you're arguing against. The ""process"" here is about ensuring certain checks are made for every code change, not about automating personnel and development decisions.

In any case, I appreciate your input on this, and we're going to evaluate the model to see how it goes. It might be that we decide we don't want it at all. However, from what I've seen of other projects (not Hadoop but projects with an order of magnitude more contributors, like Python or Linux), this is one of the best ways to have consistently great releases with a large contributor base and little room for error. With all due respect to what Hadoop's accomplished, I wouldn't use Hadoop as the best example to strive for; in my experience there I've seen patches reverted because of architectural disagreements, new APIs released and abandoned, and generally an experience that's been painful for users. A lot of the decisions we've made in Spark (e.g. time-based release cycle, built-in libraries, API stability rules, etc) were based on lessons learned there, in an attempt to define a better model.

Matei


point where it's hard to expect every committer to have full understanding of every component. Some committers know a ton about systems but little about machine learning, some are algorithmic whizzes but may not realize the implications of changing something on the Python API, etc. This is just a way to make sure that a domain expert has looked at the areas where it is most likely for something to go wrong.
without adding a new process. In the example above, those ""algo whizzes"" committers should realize that they're touching the Python API, and loop in some Python maintainersâ€‹. Those Python maintainers would then respond and help move the PR along. This is good hygiene and should already be happening. For example, HDFS committers have commit rights to all of Hadoop. But none of them would check in YARN code without getting agreement from the YARN folks.
the convention. We have to ask committers to watch out for API changes, know their own limits, and involve the component domain experts. We need that anyways, which btw also seems to solve the problem. It's not clear what the new process would add.
a committer to get promoted to be a maintainer? How often does the PMC re-evaluate the list of maintainers? Is there an upper bound on the number of maintainers for a component? Can we have an automatic rule for a maintainer promotion after X patches or Y lines of code in that area?
in different areas is a good thing. But I have some questions about the implementation. Here's my understanding of the proposal:
Changes to that list requires PMC approval.
from a maintainer of that component.
identify expertise. For #2, I'd like to understand what problem it's trying to solve. Do we have rogue committers committing to areas that they don't know much about? If that's the case, we should address it directly, instead of adding new processes.
means in Spark. Do we have clear promotion criteria from ""committer"" to ""maintainer""? Is there a max number of maintainers per area Currently, as committers gains expertise in new areas, they could start reviewing code in those areas and give +1. This encourages more contributions and cross-component knowledge sharing. Under the new proposal, they now have to be promoted to ""maintainers"" first. That reduces our review bandwidth.
those committers and help them do better. There are non-process ways to solve the problem.
of having explicit maintainers on a volunteer basis. These maintainers should watch their jira and PR traffic, and be very active in design & API discussions. That leads to better consistency and long-term design choices.
well as call for an official vote on it on a public list. Basically, as the Spark project scales up, we need to define a model to make sure there is still great oversight of key components (in particular internal architecture and public APIs), and to this end I've proposed implementing a maintainer model for some of these components, similar to other large projects.
We've had over 80 contributors/month for the past 3 months, which I believe makes us the most active project in contributors/month at Apache, as well as over 500 patches/month. The codebase has also grown significantly, with new libraries for SQL, ML, graphs and more.
to assign ""maintainers"" to oversee key components, where each patch to that component needs to get sign-off from at least one of its maintainers. Most existing large projects do this -- at Apache, some large ones with this model are CloudStack (the second-most active project overall), Subversion, and Kafka, and other examples include Linux and Python. This is also by-and-large how Spark operates today -- most components have a de-facto maintainer.
regarding architecture and API. This process would ensure that the component's maintainers see all proposed changes and consider them to fit together in a good way.
particular, it would be easy to look up whoâ€™s responsible for each module and ask them for reviews, etc, rather than having patches slip between the cracks.
only applies to certain key components (e.g. scheduler, shuffle) and user-facing APIs (MLlib, GraphX, etc). Over time, as the project grows, we can expand it if we deem it useful. The specific mechanics would be as follows:
where one of the maintainers needs to sign off on each patch to the component.
committers on that component by the PMC. The PMC can vote to add / remove maintainers, and maintained components, through consensus.
their components, though they do not need to be the main reviewers for them (e.g. they might just sign off on architecture / API). To prevent inactive maintainers from blocking the project, if a maintainer isn't responding in a reasonable time period (say 2 weeks), other committers can merge the patch, and the PMC will want to discuss adding another maintainer.
projects:
https://cwiki.apache.org/confluence/display/CLOUDSTACK/CloudStack+Maintainers+Guide <https://cwiki.apache.org/confluence/display/CLOUDSTACK/CloudStack+Maintainers+Guide><https://cwiki.apache.org/confluence/display/CLOUDSTACK/CloudStack+Maintainers+Guide <https://cwiki.apache.org/confluence/display/CLOUDSTACK/CloudStack+Maintainers+Guide>>
https://subversion.apache.org/docs/community-guide/roles.html <https://subversion.apache.org/docs/community-guide/roles.html><https://subversion.apache.org/docs/community-guide/roles.html <https://subversion.apache.org/docs/community-guide/roles.html>>
and maintainers. It would be good to get feedback on other components we might add, but please note that personnel discussions (e.g. ""I don't think Matei should maintain *that* component) should only happen on the private list. The initial components were chosen to include all public APIs and the main core components, and the maintainers were chosen from the most active contributors to those modules.
The [VOTE] will end on Nov 8, 2014 at 6 PM PST.

"
catchmonster <skacanski@gmail.com>,"Thu, 6 Nov 2014 15:01:04 -0800 (PST)",Python3 and spark 1.1.0,dev@spark.incubator.apache.org,"Hi,
I am interested in py3 with spark! Simply everything that I am developing in
py is happening on the py3 side.
is there plan to integrate spark 1.1.0 or UP with py3...
it seems that is not supported in current latest version ...





--

---------------------------------------------------------------------


"
Jeremy Freeman <freeman.jeremy@gmail.com>,"Thu, 6 Nov 2014 18:14:59 -0500",Re: Python3 and spark 1.1.0,catchmonster <skacanski@gmail.com>,"Currently, Spark 1.1.0 works with Python 2.6 or higher, but not Python 3. There does seem to be interest, see also this post (http://apache-spark-user-list.1001560.n3.nabble.com/pyspark-on-python-3-td15706.html).

I believe Ariel Rokem (cced) has been trying to get it work and might be working on a PR. It would probably be good to create a JIRA ticket for this.

— Jeremy

-------------------------
jeremyfreeman.net
@thefreemanlab


developing in
http://apache-spark-developers-list.1001551.n3.nabble.com/Python3-and-spark-1-1-0-tp9180.html
Nabble.com.

"
"""York, Brennon"" <Brennon.York@capitalone.com>","Thu, 6 Nov 2014 18:25:16 -0500",Re: Implementing TinkerPop on top of GraphX,"Kyle Ellrott <kellrott@soe.ucsc.edu>, Kushal Datta
	<kushal.datta@gmail.com>","This was my thought exactly with the TinkerPop3 release. Looks like, to move this forward, we¡¯d need to implement gremlin-core per <http://www.tinkerpop.com/docs/3.0.0.M1/#_implementing_gremlin_core>. The real question lies in whether GraphX can only support the OLTP functionality, or if we can bake into it the OLAP requirements as well. At a first glance I believe we could create an entire OLAP system. If so, I believe we could do this in a set of parallel subtasks, those being the implementation of each of the individual API¡¯s (Structure, Process, and, if OLAP, GraphComputer) necessary for gremlin-core. Thoughts?


From: Kyle Ellrott <kellrott@soe.ucsc.edu<mailto:kellrott@soe.ucsc.edu>>
Date: Thursday, November 6, 2014 at 12:10 PM
To: Kushal Datta <kushal.datta@gmail.com<mailto:kushal.datta@gmail.com>>
Cc: Reynold Xin <rxin@databricknon.york@capitalone.com<mailto:brennon.york@capitalone.com>>, ""dev@spark.apache.org<mailto:dev@spark.apache.org>"" <dev@spark.apache.org<mailto:dev@spark.apache.org>>, Matthias Broecheler <matthias@thinkaurelius.com<mailto:matthias@thinkaurelius.com>>
Subject: Re: Implementing TinkerPop on top of GraphX

I still have to dig into the Tinkerpop3 internals (I started my work long before it had been released), but I can say that to get the Tinerpop2 Gremlin pipeline to work in the GraphX was a bit of a hack. The whole Tinkerpop2 Gremlin design was based around streaming pipes of data, rather then large distributed map-reduce operations. I had to hack the pipes to aggregate all of the data and pass a single object wrapping the GraphX RDDs down the pipes in a single go, rather then streaming it element by element.
Just based on their description, Tinkerpop3 may be more amenable to the Spark platform.

Kyle


On Thu, Nov 6, 2014 at 11:55 AM, Kushal Datta <kushal.datta@gmail.com<mailto:kushal.datta@gmail.com>> wrote:
What do you guys think about the Tinkerpop3 Gremlin interface?
It has MapReduce to run Gremlin operators in a distributed manner and Giraph to execute vertex programs.

The Tinkpop3 is better suited for GraphX.

On Thu, Nov 6, 2014 at 11:48 AM, Kyle Ellrott <kellrott@soe.ucsc.edu<mailto:kellrott@soe.ucsc.edu>> wrote:
I've taken a crack at implementing the TinkerPop Blueprints API in GraphX (
https://github.com/kellrott/sparkgraph ). I've also implemented portions of
the Gremlin Search Language and a Parquet based graph store.
I've been working out finalize some code details and putting together
better code examples and documentation before I started telling people
about it.
But if you want to start looking at the code, I can answer any questions
you have. And if you would like to contribute, I would really appreciate
the help.

Kyle


On Thu, Nov 6, 2014 at 11:42 AM, Reynold Xin <rxin@databricks.com<mailto:rxin@databricks.com>> wrote:

> cc Matthias
>
> In the past we talked with Matthias and there were some discussions about
> this.
>
> On Thu, Nov 6, 2014 at 11:34 AM, York, Brennon <
> Brennon.York@capitalone.com<mailto:Brennon.York@capitalone.com>>
> wrote:
>
> > All, was wondering if there had been any discussion around this topic
> yet?
> > TinkerPop <https://github.com/tinkerpop> is a great abstraction for
> graph
> > databases and has been implemented across various graph database backends
> > / gaining traction. Has anyone thought about integrating the TinkerPop
> > framework with GraphX to enable GraphX as another backend? Not sure if
> > this has been brought up or not, but would certainly volunteer to
> > spearhead this effort if the community thinks it to be a good idea!
> >
> > As an aside, wasn©öt sure if this discussion should happen on the board
> > here or on JIRA, but a made a ticket as well for reference:
> > https://issues.apache.org/jira/browse/SPARK-4279
> >
> > ________________________________________________________
> >
> > The information contained in this e-mail is confidential and/or
> > proprietary to Capital One and/or its affiliates. The information
> > transmitted herewith is intended only for use by the individual or entity
> > to which it is addressed.  If the reader of this message is not the
> > intended recipient, you are hereby notified that any review,
> > retransmission, dissemination, distribution, copying or other use of, or
> > taking of any action in reliance upon this information is strictly
> > prohibited. If you have received this communication in error, please
> > contact the sender and delete the material from your computer.
> >
> >
> > ---------------------------------------------------------------------
> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org>
> > For additional commands, e-mail: dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>
> >
> >
>


________________________________________________________

The information contained in this e-mail is confidential and/or proprietary to Capital One and/or its affiliates. The information transmitted herewith is intended only for use by the individual or entity to which it is addressed.  If the reader of this message is not the intended recipient, you are hereby notified that any review, retransmission, dissemination, distribution, copying or other use of, or taking of any action in reliance upon this information is strictly prohibited. If you have received this communication in error, please contact the sender and delete the material from your computer.
"
"""Hari Shreedharan"" <hshreedharan@cloudera.com>","Thu, 06 Nov 2014 15:29:11 -0800 (PST)",Re: [VOTE] Designating maintainers for some Spark components,"""Matei Zaharia"" <matei.zaharia@gmail.com>","How would this model work with a new committer who gets voted in? Does it mean that a new committer would be a maintainer for at least one area â€” else we could end up having committers who really canâ€™t merge anything significant until he becomes a maintainer.Â 


Thanks,
Hari


 of process is to make sure something happens automatically, which is useful to ensure a certain level of quality. For example, all our patches go through Jenkins, and nobody will make the mistake of merging them if they fail tests, or RAT checks, or API compatibility checks. The idea is to get the same kind of automation for design on these components. This is a very common process for large software projects, and it's essentially what we had already, but formalizing it will make clear that this is the process we want. It's important to do it early in order to be able to refine the process as the project grows.
reviewers for that component, they are just a second level of sign-off required for architecture and API. Being a maintainer is also not a ""promotion"", it's a responsibility. Since we don't have much experience yet with this model, I didn't propose automatic rules beyond that the PMC can add / remove maintainers -- presumably the PMC is in the best position to know what the project needs. I think automatic rules are exactly the kind of ""process"" you're arguing against. The ""process"" here is about ensuring certain checks are made for every code change, not about automating personnel and development decisions.
 the model to see how it goes. It might be that we decide we don't want it at all. However, from what I've seen of other projects (not Hadoop but projects with an order of magnitude more contributors, like Python or Linux), this is one of the best ways to have consistently great releases with a large contributor base and little room for error. With all due respect to what Hadoop's accomplished, I wouldn't use Hadoop as the best example to strive for; in my experience there I've seen patches reverted because of architectural disagreements, new APIs released and abandoned, and generally an experience that's been painful for users. A lot of the decisions we've made in Spark (e.g. time-based release cycle, built-in libraries, API stability rules, etc) were based on lessons learned there, in an attempt to define a better model.
point where it's hard to expect every committer to have full understanding of every component. Some committers know a ton about systems but little about machine learning, some are algorithmic whizzes but may not realize the implications of changing something on the Python API, etc. This is just a way to make sure that a domain expert has looked at the areas where it is most likely for something to go wrong.
without adding a new process. In the example above, those ""algo whizzes"" committers should realize that they're touching the Python API, and loop in some Python maintainersâ€‹. Those Python maintainers would then respond and help move the PR along. This is good hygiene and should already be happening. For example, HDFS committers have commit rights to all of Hadoop. But none of them would check in YARN code without getting agreement from the YARN folks.
the convention. We have to ask committers to watch out for API changes, know their own limits, and involve the component domain experts. We need that anyways, which btw also seems to solve the problem. It's not clear what the new process would add.
 committer to get promoted to be a maintainer? How often does the PMC re-evaluate the list of maintainers? Is there an upper bound on the number of maintainers for a component? Can we have an automatic rule for a maintainer promotion after X patches or Y lines of code in that area?
different areas is a good thing. But I have some questions about the implementation. Here's my understanding of the proposal:
Changes to that list requires PMC approval.
a maintainer of that component.
identify expertise. For #2, I'd like to understand what problem it's trying to solve. Do we have rogue committers committing to areas that they don't know much about? If that's the case, we should address it directly, instead of adding new processes.
means in Spark. Do we have clear promotion criteria from ""committer"" to ""maintainer""? Is there a max number of maintainers per area Currently, as committers gains expertise in new areas, they could start reviewing code in those areas and give +1. This encourages more contributions and cross-component knowledge sharing. Under the new proposal, they now have to be promoted to ""maintainers"" first. That reduces our review bandwidth.
those committers and help them do better. There are non-process ways to solve the problem.
 of having explicit maintainers on a volunteer basis. These maintainers should watch their jira and PR traffic, and be very active in design & API discussions. That leads to better consistency and long-term design choices.
well as call for an official vote on it on a public list. Basically, as the Spark project scales up, we need to define a model to make sure there is still great oversight of key components (in particular internal architecture and public APIs), and to this end I've proposed implementing a maintainer model for some of these components, similar to other large projects.
We've had over 80 contributors/month for the past 3 months, which I believe makes us the most active project in contributors/month at Apache, as well as over 500 patches/month. The codebase has also grown significantly, with new libraries for SQL, ML, graphs and more.
to assign ""maintainers"" to oversee key components, where each patch to that component needs to get sign-off from at least one of its maintainers. Most existing large projects do this -- at Apache, some large ones with this model are CloudStack (the second-most active project overall), Subversion, and Kafka, and other examples include Linux and Python. This is also by-and-large how Spark operates today -- most components have a de-facto maintainer.
regarding architecture and API. This process would ensure that the component's maintainers see all proposed changes and consider them to fit together in a good way.
 it would be easy to look up whoâ€™s responsible for each module and ask them for reviews, etc, rather than having patches slip between the cracks.
applies to certain key components (e.g. scheduler, shuffle) and user-facing APIs (MLlib, GraphX, etc). Over time, as the project grows, we can expand it if we deem it useful. The specific mechanics would be as follows:
where one of the maintainers needs to sign off on each patch to the component.
committers on that component by the PMC. The PMC can vote to add / remove maintainers, and maintained components, through consensus.
their components, though they do not need to be the main reviewers for them (e.g. they might just sign off on architecture / API). To prevent inactive maintainers from blocking the project, if a maintainer isn't responding in a reasonable time period (say 2 weeks), other committers can merge the patch, and the PMC will want to discuss adding another maintainer.
projects:
oudStack+Maintainers+Guide <https://cwiki.apache.org/confluence/display/CLOUDSTACK/CloudStack+Maintainers+Guide><https://cwiki.apache.org/confluence/display/CLOUDSTACK/CloudStack+Maintainers+Guide <https://cwiki.apache.org/confluence/display/CLOUDSTACK/CloudStack+Maintainers+Guide>>
html <https://subversion.apache.org/docs/community-guide/roles.html><https://subversion.apache.org/docs/community-guide/roles.html <https://subversion.apache.org/docs/community-guide/roles.html>>
and maintainers. It would be good to get feedback on other components we might add, but please note that personnel discussions (e.g. ""I don't think Matei should maintain *that* component) should only happen on the private list. The initial components were chosen to include all public APIs and the main core components, and the maintainers were chosen from the most active contributors to those modules.
 [VOTE] will end on Nov 8, 2014 at 6 PM PST."
Kyle Ellrott <kellrott@soe.ucsc.edu>,"Thu, 6 Nov 2014 15:38:19 -0800",Re: Implementing TinkerPop on top of GraphX,"""York, Brennon"" <Brennon.York@capitalone.com>","I think I've already done most of the work for the OLTP objects (Graph,
Element, Vertex, Edge, Properties) when implementing Tinkerpop2. Singleton
write operations, like addVertex/deleteEdge, were cached locally until a
read operation was requested, then the set of build operations where
parallelized into an RDD and merged with the existing graph.
Its not efficient for large numbers of operations, but it passes unit tests
and works for small graph tweaking.

OLAP stuff looks completely new, but considering they have a Giraph
implementation, it should be pretty straight forward.

Kyle



t
s, and, if
g>,
s
e
ic
he
e
--
"
Patrick Wendell <pwendell@gmail.com>,"Thu, 6 Nov 2014 15:45:29 -0800",Re: [VOTE] Designating maintainers for some Spark components,Hari Shreedharan <hshreedharan@cloudera.com>,"I think new committers might or might not be maintainers (it would
depend on the PMC vote). I don't think it would affect what you could
merge, you can merge in any part of the source tree, you just need to
get sign off if you want to touch a public API or make major
architectural changes. Most projects already require code review from
other committers before you commit something, so it's just a version
of that where you have specific people appointed to specific
components for review.

If you look, most large software projects have a maintainer model,
both in Apache and outside of it. Cloudstack is probably the best
example in Apache since they are the second most active project
(roughly) after Spark. They have two levels of maintainers and much
strong language - their language: ""In general, maintainers only have
commit rights on the module for which they are responsible."".

I'd like us to start with something simpler and lightweight as
proposed here. Really the proposal on the table is just to codify the
current de-facto process to make sure we stick by it as we scale. If
we want to add more formality to it or strictness, we can do it later.

- Patrick

 mean that a new committer would be a maintainer for at least one area -- else we could end up having committers who really can't merge anything significant until he becomes a maintainer.
 process is to make sure something happens automatically, which is useful to ensure a certain level of quality. For example, all our patches go through Jenkins, and nobody will make the mistake of merging them if they fail tests, or RAT checks, or API compatibility checks. The idea is to get the same kind of automation for design on these components. This is a very common process for large software projects, and it's essentially what we had already, but formalizing it will make clear that this is the process we want. It's important to do it early in order to be able to refine the process as the project grows.
 reviewers for that component, they are just a second level of sign-off required for architecture and API. Being a maintainer is also not a ""promotion"", it's a responsibility. Since we don't have much experience yet with this model, I didn't propose automatic rules beyond that the PMC can add / remove maintainers -- presumably the PMC is in the best position to know what the project needs. I think automatic rules are exactly the kind of ""process"" you're arguing against. The ""process"" here is about ensuring certain checks are made for every code change, not about automating personnel and development decisions.
e the model to see how it goes. It might be that we decide we don't want it at all. However, from what I've seen of other projects (not Hadoop but projects with an order of magnitude more contributors, like Python or Linux), this is one of the best ways to have consistently great releases with a large contributor base and little room for error. With all due respect to what Hadoop's accomplished, I wouldn't use Hadoop as the best example to strive for; in my experience there I've seen patches reverted because of architectural disagreements, new APIs released and abandoned, and generally an experience that's been painful for users. A lot of the decisions we've made in Spark (e.g. time-based release cycle, built-in libraries, API stability rules, etc) were based on lessons learned there, in an attempt to define a better model.
int where it's hard to expect every committer to have full understanding of every component. Some committers know a ton about systems but little about machine learning, some are algorithmic whizzes but may not realize the implications of changing something on the Python API, etc. This is just a way to make sure that a domain expert has looked at the areas where it is most likely for something to go wrong.
ithout adding a new process. In the example above, those ""algo whizzes"" committers should realize that they're touching the Python API, and loop in some Python maintainers. Those Python maintainers would then respond and help move the PR along. This is good hygiene and should already be happening. For example, HDFS committers have commit rights to all of Hadoop. But none of them would check in YARN code without getting agreement from the YARN folks.
the convention. We have to ask committers to watch out for API changes, know their own limits, and involve the component domain experts. We need that anyways, which btw also seems to solve the problem. It's not clear what the new process would add.
a committer to get promoted to be a maintainer? How often does the PMC re-evaluate the list of maintainers? Is there an upper bound on the number of maintainers for a component? Can we have an automatic rule for a maintainer promotion after X patches or Y lines of code in that area?
 different areas is a good thing. But I have some questions about the implementation. Here's my understanding of the proposal:
es to that list requires PMC approval.
 a maintainer of that component.
ntify expertise. For #2, I'd like to understand what problem it's trying to solve. Do we have rogue committers committing to areas that they don't know much about? If that's the case, we should address it directly, instead of adding new processes.
s in Spark. Do we have clear promotion criteria from ""committer"" to ""maintainer""? Is there a max number of maintainers per area Currently, as committers gains expertise in new areas, they could start reviewing code in those areas and give +1. This encourages more contributions and cross-component knowledge sharing. Under the new proposal, they now have to be promoted to ""maintainers"" first. That reduces our review bandwidth.
ose committers and help them do better. There are non-process ways to solve the problem.
 having explicit maintainers on a volunteer basis. These maintainers should watch their jira and PR traffic, and be very active in design & API discussions. That leads to better consistency and long-term design choices.
ell as call for an official vote on it on a public list. Basically, as the Spark project scales up, we need to define a model to make sure there is still great oversight of key components (in particular internal architecture and public APIs), and to this end I've proposed implementing a maintainer model for some of these components, similar to other large projects.
ve had over 80 contributors/month for the past 3 months, which I believe makes us the most active project in contributors/month at Apache, as well as over 500 patches/month. The codebase has also grown significantly, with new libraries for SQL, ML, graphs and more.
to assign ""maintainers"" to oversee key components, where each patch to that component needs to get sign-off from at least one of its maintainers. Most existing large projects do this -- at Apache, some large ones with this model are CloudStack (the second-most active project overall), Subversion, and Kafka, and other examples include Linux and Python. This is also by-and-large how Spark operates today -- most components have a de-facto maintainer.
ding architecture and API. This process would ensure that the component's maintainers see all proposed changes and consider them to fit together in a good way.
, it would be easy to look up who's responsible for each module and ask them for reviews, etc, rather than having patches slip between the cracks.
 applies to certain key components (e.g. scheduler, shuffle) and user-facing APIs (MLlib, GraphX, etc). Over time, as the project grows, we can expand it if we deem it useful. The specific mechanics would be as follows:
re one of the maintainers needs to sign off on each patch to the component.
committers on that component by the PMC. The PMC can vote to add / remove maintainers, and maintained components, through consensus.
heir components, though they do not need to be the main reviewers for them (e.g. they might just sign off on architecture / API). To prevent inactive maintainers from blocking the project, if a maintainer isn't responding in a reasonable time period (say 2 weeks), other committers can merge the patch, and the PMC will want to discuss adding another maintainer.
projects:
loudStack+Maintainers+Guide <https://cwiki.apache.org/confluence/display/CLOUDSTACK/CloudStack+Maintainers+Guide><https://cwiki.apache.org/confluence/display/CLOUDSTACK/CloudStack+Maintainers+Guide <https://cwiki.apache.org/confluence/display/CLOUDSTACK/CloudStack+Maintainers+Guide>>
.html <https://subversion.apache.org/docs/community-guide/roles.html><https://subversion.apache.org/docs/community-guide/roles.html <https://subversion.apache.org/docs/community-guide/roles.html>>
and maintainers. It would be good to get feedback on other components we might add, but please note that personnel discussions (e.g. ""I don't think Matei should maintain *that* component) should only happen on the private list. The initial components were chosen to include all public APIs and the main core components, and the maintainers were chosen from the most active contributors to those modules.
e [VOTE] will end on Nov 8, 2014 at 6 PM PST.

---------------------------------------------------------------------


"
Kushal Datta <kushal.datta@gmail.com>,"Thu, 6 Nov 2014 16:00:48 -0800",Re: Implementing TinkerPop on top of GraphX,"""York, Brennon"" <Brennon.York@capitalone.com>","Before we dive into the implementation details, what are the high level
thoughts on Gremlin/GraphX? Scala already provides the procedural way to
query graphs in GraphX today. So, today I can run
g.vertices().filter().join() queries as OLAP in GraphX just like Tinkerpop3
Gremlin, of course sans the useful operators that Gremlin offers such as
outE, inE, loop, as, dedup, etc. In that case is mapping Gremlin operators
to GraphX api's a better approach or should we extend the existing set of
transformations/actions that GraphX already offers with the useful
operators from Gremlin? For example, we add as(), loop() and dedup()
methods in VertexRDD and EdgeRDD.

Either way we get a desperately needed graph query interface in GraphX.


t
s, and, if
g>,
s
e
ic
he
e
--
"
Greg Stein <gstein@gmail.com>,"Fri, 7 Nov 2014 00:18:40 +0000",Re: [VOTE] Designating maintainers for some Spark components,Matei Zaharia <matei.zaharia@gmail.com>,"-1 (non-binding)

This is an idea that runs COMPLETELY counter to the Apache Way, and is
to be severely frowned up. This creates *unequal* ownership of the
codebase.

Each Member of the PMC should have *equal* rights to all areas of the
codebase until the"
"""Hari Shreedharan"" <hshreedharan@cloudera.com>","Thu, 06 Nov 2014 16:18:20 -0800 (PST)",Re: [VOTE] Designating maintainers for some Spark components,"""Patrick Wendell"" <pwendell@gmail.com>","In Cloudstack, I believe one becomes a maintainer first for a subset of modules, before he/she becomes a proven maintainter who has commit rights on the entire source tree.Â 




So would it make sense to go that route, and have committers voted in as maintainers for certain parts of the codebase and then eventually become proven maintainers (though this might have be honor code based, since I donâ€™t think git allows per module commit rights).


Thanks,
Hari


 it mean that a new committer would be a maintainer for at least one area -- else we could end up having committers who really can't merge anything significant until he becomes a maintainer.
point of process is to make sure something happens automatically, which is useful to ensure a certain level of quality. For example, all our patches go through Jenkins, and nobody will make the mistake of merging them if they fail tests, or RAT checks, or API compatibility checks. The idea is to get the same kind of automation for design on these components. This is a very common process for large software projects, and it's essentially what we had already, but formalizing it will make clear that this is the process we want. It's important to do it early in order to be able to refine the process as the project grows.
only reviewers for that component, they are just a second level of sign-off required for architecture and API. Being a maintainer is also not a ""promotion"", it's a responsibility. Since we don't have much experience yet with this model, I didn't propose automatic rules beyond that the PMC can add / remove maintainers -- presumably the PMC is in the best position to know what the project needs. I think automatic rules are exactly the kind of ""process"" you're arguing against. The ""process"" here is about ensuring certain checks are made for every code change, not about automating personnel and development decisions.
evaluate the model to see how it goes. It might be that we decide we don't want it at all. However, from what I've seen of other projects (not Hadoop but projects with an order of magnitude more contributors, like Python or Linux), this is one of the best ways to have consistently great releases with a large contributor base and little room for error. With all due respect to what Hadoop's accomplished, I wouldn't use Hadoop as the best example to strive for; in my experience there I've seen patches reverted because of architectural disagreements, new APIs released and abandoned, and generally an experience that's been painful for users. A lot of the decisions we've made in Spark (e.g. time-based release cycle, built-in libraries, API stability rules, etc) were based on lessons learned there, in an attempt to define a better model.
point where it's hard to expect every committer to have full understanding of every component. Some committers know a ton about systems but little about machine learning, some are algorithmic whizzes but may not realize the implications of changing something on the Python API, etc. This is just a way to make sure that a domain expert has looked at the areas where it is most likely for something to go wrong.
without adding a new process. In the example above, those ""algo whizzes"" committers should realize that they're touching the Python API, and loop in some Python maintainers. Those Python maintainers would then respond and help move the PR along. This is good hygiene and should already be happening. For example, HDFS committers have commit rights to all of Hadoop. But none of them would check in YARN code without getting agreement from the YARN folks.
 the convention. We have to ask committers to watch out for API changes, know their own limits, and involve the component domain experts. We need that anyways, which btw also seems to solve the problem. It's not clear what the new process would add.
 a committer to get promoted to be a maintainer? How often does the PMC re-evaluate the list of maintainers? Is there an upper bound on the number of maintainers for a component? Can we have an automatic rule for a maintainer promotion after X patches or Y lines of code in that area?
in different areas is a good thing. But I have some questions about the implementation. Here's my understanding of the proposal:
Changes to that list requires PMC approval.
from a maintainer of that component.
identify expertise. For #2, I'd like to understand what problem it's trying to solve. Do we have rogue committers committing to areas that they don't know much about? If that's the case, we should address it directly, instead of adding new processes.
 means in Spark. Do we have clear promotion criteria from ""committer"" to ""maintainer""? Is there a max number of maintainers per area Currently, as committers gains expertise in new areas, they could start reviewing code in those areas and give +1. This encourages more contributions and cross-component knowledge sharing. Under the new proposal, they now have to be promoted to ""maintainers"" first. That reduces our review bandwidth.
those committers and help them do better. There are non-process ways to solve the problem.
idea of having explicit maintainers on a volunteer basis. These maintainers should watch their jira and PR traffic, and be very active in design & API discussions. That leads to better consistency and long-term design choices.
well as call for an official vote on it on a public list. Basically, as the Spark project scales up, we need to define a model to make sure there is still great oversight of key components (in particular internal architecture and public APIs), and to this end I've proposed implementing a maintainer model for some of these components, similar to other large projects.
We've had over 80 contributors/month for the past 3 months, which I believe makes us the most active project in contributors/month at Apache, as well as over 500 patches/month. The codebase has also grown significantly, with new libraries for SQL, ML, graphs and more.
 to assign ""maintainers"" to oversee key components, where each patch to that component needs to get sign-off from at least one of its maintainers. Most existing large projects do this -- at Apache, some large ones with this model are CloudStack (the second-most active project overall), Subversion, and Kafka, and other examples include Linux and Python. This is also by-and-large how Spark operates today -- most components have a de-facto maintainer.
regarding architecture and API. This process would ensure that the component's maintainers see all proposed changes and consider them to fit together in a good way.
particular, it would be easy to look up who's responsible for each module and ask them for reviews, etc, rather than having patches slip between the cracks.
only applies to certain key components (e.g. scheduler, shuffle) and user-facing APIs (MLlib, GraphX, etc). Over time, as the project grows, we can expand it if we deem it useful. The specific mechanics would be as follows:
where one of the maintainers needs to sign off on each patch to the component.
 committers on that component by the PMC. The PMC can vote to add / remove maintainers, and maintained components, through consensus.
their components, though they do not need to be the main reviewers for them (e.g. they might just sign off on architecture / API). To prevent inactive maintainers from blocking the project, if a maintainer isn't responding in a reasonable time period (say 2 weeks), other committers can merge the patch, and the PMC will want to discuss adding another maintainer.
 projects:
CloudStack+Maintainers+Guide <https://cwiki.apache.org/confluence/display/CLOUDSTACK/CloudStack+Maintainers+Guide><https://cwiki.apache.org/confluence/display/CLOUDSTACK/CloudStack+Maintainers+Guide <https://cwiki.apache.org/confluence/display/CLOUDSTACK/CloudStack+Maintainers+Guide>>
s.html <https://subversion.apache.org/docs/community-guide/roles.html><https://subversion.apache.org/docs/community-guide/roles.html <https://subversion.apache.org/docs/community-guide/roles.html>>
 and maintainers. It would be good to get feedback on other components we might add, but please note that personnel discussions (e.g. ""I don't think Matei should maintain *that* component) should only happen on the private list. The initial components were chosen to include all public APIs and the main core components, and the maintainers were chosen from the most active contributors to those modules.
The [VOTE] will end on Nov 8, 2014 at 6 PM PST."
Patrick Wendell <pwendell@gmail.com>,"Thu, 6 Nov 2014 16:26:56 -0800",Re: [VOTE] Designating maintainers for some Spark components,Greg Stein <gstein@gmail.com>,"Hey Greg,

Regarding subversion - I think the reference is to partial vs full
committers here:
https://subversion.apache.org/docs/community-guide/roles.html

- Patrick

l as call for an official vote on it on a public list. Basically, as the Spark project scales up, we need to define a model to make sure there is still great oversight of key components (in particular internal architecture and public APIs), and to this end I've proposed implementing a maintainer model for some of these components, similar to other large projects.
 had over 80 contributors/month for the past 3 months, which I believe makes us the most active project in contributors/month at Apache, as well as over 500 patches/month. The codebase has also grown significantly, with new libraries for SQL, ML, graphs and more.
 assign ""maintainers"" to oversee key components, where each patch to that component needs to get sign-off from at least one of its maintainers. Most existing large projects do this -- at Apache, some large ones with this model are CloudStack (the second-most active project overall), Subversion, and Kafka, and other examples include Linux and Python. This is also by-and-large how Spark operates today -- most components have a de-facto maintainer.
ng architecture and API. This process would ensure that the component's maintainers see all proposed changes and consider them to fit together in a good way.
it would be easy to look up who's responsible for each module and ask them for reviews, etc, rather than having patches slip between the cracks.
pplies to certain key components (e.g. scheduler, shuffle) and user-facing APIs (MLlib, GraphX, etc). Over time, as the project grows, we can expand it if we deem it useful. The specific mechanics would be as follows:
 one of the maintainers needs to sign off on each patch to the component.
mmitters on that component by the PMC. The PMC can vote to add / remove maintainers, and maintained components, through consensus.
ir components, though they do not need to be the main reviewers for them (e.g. they might just sign off on architecture / API). To prevent inactive maintainers from blocking the project, if a maintainer isn't responding in a reasonable time period (say 2 weeks), other committers can merge the patch, and the PMC will want to discuss adding another maintainer.
ojects:
udStack+Maintainers+Guide <https://cwiki.apache.org/confluence/display/CLOUDSTACK/CloudStack+Maintainers+Guide>
tml <https://subversion.apache.org/docs/community-guide/roles.html>
d maintainers. It would be good to get feedback on other components we might add, but please note that personnel discussions (e.g. ""I don't think Matei should maintain *that* component) should only happen on the private list. The initial components were chosen to include all public APIs and the main core components, and the maintainers were chosen from the most active contributors to those modules.
[VOTE] will end on Nov 8, 2014 at 6 PM PST.

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Thu, 6 Nov 2014 16:28:54 -0800",Re: [VOTE] Designating maintainers for some Spark components,Greg Stein <gstein@gmail.com>,"In fact, if you look at the subversion commiter list, the majority of
people here have commit access only for particular areas of the
project:

http://svn.apache.org/repos/asf/subversion/trunk/COMMITTERS

ll as call for an official vote on it on a public list. Basically, as the Spark project scales up, we need to define a model to make sure there is still great oversight of key components (in particular internal architecture and public APIs), and to this end I've proposed implementing a maintainer model for some of these components, similar to other large projects.
e had over 80 contributors/month for the past 3 months, which I believe makes us the most active project in contributors/month at Apache, as well as over 500 patches/month. The codebase has also grown significantly, with new libraries for SQL, ML, graphs and more.
o assign ""maintainers"" to oversee key components, where each patch to that component needs to get sign-off from at least one of its maintainers. Most existing large projects do this -- at Apache, some large ones with this model are CloudStack (the second-most active project overall), Subversion, and Kafka, and other examples include Linux and Python. This is also by-and-large how Spark operates today -- most components have a de-facto maintainer.
ing architecture and API. This process would ensure that the component's maintainers see all proposed changes and consider them to fit together in a good way.
 it would be easy to look up who's responsible for each module and ask them for reviews, etc, rather than having patches slip between the cracks.
applies to certain key components (e.g. scheduler, shuffle) and user-facing APIs (MLlib, GraphX, etc). Over time, as the project grows, we can expand it if we deem it useful. The specific mechanics would be as follows:
e one of the maintainers needs to sign off on each patch to the component.
ommitters on that component by the PMC. The PMC can vote to add / remove maintainers, and maintained components, through consensus.
eir components, though they do not need to be the main reviewers for them (e.g. they might just sign off on architecture / API). To prevent inactive maintainers from blocking the project, if a maintainer isn't responding in a reasonable time period (say 2 weeks), other committers can merge the patch, and the PMC will want to discuss adding another maintainer.
rojects:
oudStack+Maintainers+Guide <https://cwiki.apache.org/confluence/display/CLOUDSTACK/CloudStack+Maintainers+Guide>
html <https://subversion.apache.org/docs/community-guide/roles.html>
nd maintainers. It would be good to get feedback on other components we might add, but please note that personnel discussions (e.g. ""I don't think Matei should maintain *that* component) should only happen on the private list. The initial components were chosen to include all public APIs and the main core components, and the maintainers were chosen from the most active contributors to those modules.
 [VOTE] will end on Nov 8, 2014 at 6 PM PST.

---------------------------------------------------------------------


"
"""York, Brennon"" <Brennon.York@capitalone.com>","Thu, 6 Nov 2014 19:39:13 -0500",Re: Implementing TinkerPop on top of GraphX,Kushal Datta <kushal.datta@gmail.com>,"My personal 2c is that, since GraphX is just beginning to provide a full featured graph API, I think it would be better to align with the TinkerPop group rather than roll our own. In my mind the benefits out way the detriments as follows:

Benefits:
* GraphX gains the ability to become another core tenant within the TinkerPop community allowing a more diverse group of users into the Spark ecosystem.
* TinkerPop can continue to maintain and own a solid / feature-rich graph API that has already been accepted by a wide audience, relieving the pressure of ¡°one off¡± API additions from the GraphX team.
* GraphX can demonstrate its ability to be a key player in the GraphDB space sitting inline with other major distributions (Neo4j, Titan, etc.).
* Allows for the abstract graph traversal logic (query API) to be owned and maintained by a group already proven on the topic.

Drawbacks:
* GraphX doesn¡¯t own the API for its graph query capability. This could be seen as good or bad, but it might make GraphX-specific implementation additions more tricky (possibly). Also, GraphX will need to maintain the features described within the TinkerPop API as that might change in the future.

From: Kushal Datta <kushal.datta@gmail.com<mailto:kushal.datta@gmail.com>>
Date: Thursday, November 6, 2014 at 4:00 PM
To: ""York, Brennon"" <brennon.york@capitalone.com<mailto:brennon.york@capitalone.com>>
Cc: Kyle Ellrott <kellrott@soe.ucsc.edu<mailto:kellrott@soe.ucsc.edu>>, Reynold Xin <rxin@databricks.com<mailto:rxin@databricks.com>>, ""dev@spark.apache.org<mailto:dev@spark.apache.org>"" <dev@spark.apache.org<mailto:dev@spark.apache.org>>, Matthias Broecheler <matthias@thinkaurelius.com<mailto:matthias@thinkaurelius.com>>
Subject: Re: Implementing TinkerPop on top of GraphX

Before we dive into the implementation details, what are the high level thoughts on Gremlin/GraphX? Scala already provides the procedural way to query graphs in GraphX today. So, today I can run g.vertices().filter().join() queries as OLAP in GraphX just like Tinkerpop3 Gremlin, of course sans the useful operators that Gremlin offers such as outE, inE, loop, as, dedup, etc. In that case is mapping Gremlin operators to GraphX api's a better approach or should we extend the existing set of transformations/actions that GraphX already offers with the useful operators from Gremlin? For example, we add as(), loop() and dedup() methods in VertexRDD and EdgeRDD.

Either way we get a desperately needed graph query interface in GraphX.

On Thu, Nov 6, 2014 at 3:25 PM, York, Brennon <Brennon.York@capitalone.com<mailto:Brennon.York@capitalone.com>> wrote:
This was my thought exactly with the TinkerPop3 release. Looks like, to move this forward, we¡¯d need to implement gremlin-core per <http://www.tinkerpop.com/docs/3.0.0.M1/#_implementing_gremlin_core>. The real question lies in whether GraphX can only support the OLTP functionality, or if we can bake into it the OLAP requirements as well. At a first glance I believe we could create an entire OLAP system. If so, I believe we could do this in a set of parallel subtasks, those being the implementation of each of the individual API¡¯s (Structure, Process, and, if OLAP, GraphComputer) necessary for gremlin-core. Thoughts?


From: Kyle Ellrott <kellrott@soe.ucsc.edu<mailto:kellrott@soe.ucsc.edu>>
Date: Thursday, November 6, 2014 at 12:10 PM
To: Kushal Datta <kushal.datta@gmail.com<mailto:kushal.datta@gmail.com>>
Cc: Reynold Xin <rxin@databricks.com<mailto:rxin@databricks.com>>, ""York, Brennon"" <brennon.york@capitalone.com<mailto:brennon.york@capitalone.com>>, ""dev@spark.apache.org<mailto:dev@spark.apache.org>"" <dev@spark.apache.org<mailto:dev@spark.apache.org>>, Matthias Broecheler <matthias@thinkaurelius.com<mailto:matthias@thinkaurelius.com>>
Subject: Re: Implementing TinkerPop on top of GraphX

I still have to dig into the Tinkerpop3 internals (I started my work long before it had been released), but I can say that to get the Tinerpop2 Gremlin pipeline to work in the GraphX was a bit of a hack. The whole Tinkerpop2 Gremlin design was based around streaming pipes of data, rather then large distributed map-reduce operations. I had to hack the pipes to aggregate all of the data and pass a single object wrapping the GraphX RDDs down the pipes in a single go, rather then streaming it element by element.
Just based on their description, Tinkerpop3 may be more amenable to the Spark platform.

Kyle


On Thu, Nov 6, 2014 at 11:55 AM, Kushal Datta <kushal.datta@gmail.com<mailto:kushal.datta@gmail.com>> wrote:
What do you guys think about the Tinkerpop3 Gremlin interface?
It has MapReduce to run Gremlin operators in a distributed manner and Giraph to execute vertex programs.

The Tinkpop3 is better suited for GraphX.

On Thu, Nov 6, 2014 at 11:48 AM, Kyle Ellrott <kellrott@soe.ucsc.edu<mailto:kellrott@soe.ucsc.edu>> wrote:
I've taken a crack at implementing the TinkerPop Blueprints API in GraphX (
https://github.com/kellrott/sparkgraph ). I've also implemented portions of
the Gremlin Search Language and a Parquet based graph store.
I've been working out finalize some code details and putting together
better code examples and documentation before I started telling people
about it.
But if you want to start looking at the code, I can answer any questions
you have. And if you would like to contribute, I would really appreciate
the help.

Kyle


On Thu, Nov 6, 2014 at 11:42 AM, Reynold Xin <rxin@databricks.com<> In the past we talked with Matthias and there were some discussions about
> this.
>
> On Thu, Nov 6, 2014 at 11:34 AM, York, Brennon <
> Brennon.York@capitalone.com<mailto:Brennon.York@capitalone.com>>
> wrote:
>
> > All, was wondering if there had been any discussion around this topic
> yet?
> > TinkerPop <https://github.com/tinkerpop> is a great abstraction for
> graph
> > databases and has been implemented across various graph database backends
> > / gaining traction. Has anyone thought about integrating the TinkerPop
> > framework with GraphX to enable GraphX as another backend? Not sure if
> > this has been brought up or not, but would certainly volunteer to
> > spearhead this effort if the community thinks it to be a good idea!
> >
> > As an aside, wasn©öt sure if this discussion should happen on the board
> > here or on JIRA, but a made a ticket as well for reference:
> > https://issues.apache.org/jira/browse/SPARK-4279
> >
> > ________________________________________________________
> >
> > The information contained in this e-mail is confidential and/or
> > proprietary to Capital One and/or its affiliates. The information
> > transmitted herewith is intended only for use by the individual or entity
> > to which it is addressed.  If the reader of this message is not the
> > intended recipient, you are hereby notified that any review,
> > retransmission, dissemination, distribution, copying or other use of, or
> > taking of any action in reliance upon this information is strictly
> > prohibited. If you have received this communication in error, please
> > contact the sender and delete the material from your computer.
> >
> >
> > ---------------------------------------------------------------------
> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org>
> > For additional commands, e-mail: dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>
> >
> >
>



________________________________

The information contained in this e-mail is confidential and/or proprietary to Capital One and/or its affiliates. The information transmitted herewith is intended only for use by the individual or entity to which it is addressed.  If the reader of this message is not the intended recipient, you are hereby notified that any review, retransmission, dissemination, distribution, copying or other use of, or taking of any action in reliance upon this information is strictly prohibited. If you have received this communication in error, please contact the sender and delete the material from your computer.

________________________________________________________

The information contained in this e-mail is confidential and/or proprietary to Capital One and/or its affiliates. The information transmitted herewith is intended only for use by the individual or entity to which it is addressed.  If the reader of this message is not the intended recipient, you are hereby notified that any review, retransmission, dissemination, distribution, copying or other use of, or taking of any action in reliance upon this information is strictly prohibited. If you have received this communication in error, please contact the sender and delete the material from your computer.
"
Debasish Das <debasish.das83@gmail.com>,"Thu, 6 Nov 2014 16:39:00 -0800","Re: MatrixFactorizationModel predict(Int, Int) API",Xiangrui Meng <mengxr@gmail.com>,"I reproduced the problem in mllib tests ALSSuite.scala using the following
functions:

        val arrayPredict = userProductsRDD.map{case(user,product) =>

         val recommendedProducts = model.recommendProducts(user, products)

         val productScore = recommendedProducts.find{x=>x.product == product
}

          require(productScore != None)

          productScore.get

        }.collect

        arrayPredict.foreach { elem =>

          if (allRatings.get(elem.user, elem.product) != elem.rating)

          fail(""Prediction APIs don't match"")

        }

If the usage of model.recommendProducts is correct, the test fails with the
same error I sent before...

org.apache.spark.SparkException: Job aborted due to stage failure: Task 0
in stage 316.0 failed 1 times, most recent failure: Lost task 0.0 in stage
316.0 (TID 79, localhost): scala.MatchError: null

org.apache.spark.rdd.PairRDDFunctions.lookup(PairRDDFunctions.scala:825)
 org.apache.spark.mllib.recommendation.MatrixFactorizationModel.recommendProducts(MatrixFactorizationModel.scala:81)

It is a blocker for me and I am debugging it. I will open up a JIRA if this
is indeed a bug...

Do I have to cache the models to make userFeatures.lookup(user).head to
work ?


"
Xiangrui Meng <mengxr@gmail.com>,"Thu, 6 Nov 2014 16:41:22 -0800","Re: MatrixFactorizationModel predict(Int, Int) API",Debasish Das <debasish.das83@gmail.com>,"ALS model contains RDDs. So you cannot put `model.recommendProducts`
inside a RDD closure `userProductsRDD.map`. -Xiangrui


---------------------------------------------------------------------


"
Greg Stein <gstein@gmail.com>,"Thu, 6 Nov 2014 18:43:56 -0600",Re: [VOTE] Designating maintainers for some Spark components,Patrick Wendell <pwendell@gmail.com>,"Partial committers are people invited to work on a particular area, and
they do not require sign-off to work on that area. They can get a sign-off
and commit outside that area. That approach doesn't compare to this
proposal.

Full committers are PMC members. As each PMC member is responsible for
*every* line of code, then every PMC member should have complete rights to
every line of code. Creating disparity flies in the face of a PMC member's
responsibility. If I am a Spark PMC member, then I have responsibility for
GraphX code, whether my name is Ankur, Joey, Reynold, or Greg. And
interposing a barrier inhibits my responsibility to ensure GraphX is
designed, maintained, and delivered to the Public.

Cheers,
-g

(and yes, I'm aware of COMMITTERS; I've been changing that file for the
past 12 years :-) )


"
Corey Nolet <cjnolet@gmail.com>,"Thu, 6 Nov 2014 19:44:44 -0500",Re: [VOTE] Designating maintainers for some Spark components,Patrick Wendell <pwendell@gmail.com>,"+1 (non-binding) [for original process proposal]

Greg, the first time I've seen the word ""ownership"" on this thread is in
your message. The first time the word ""lead"" has appeared in this thread is
in your message as well. I don't think that was the inte"
Corey Nolet <cjnolet@gmail.com>,"Thu, 6 Nov 2014 19:44:44 -0500",Re: [VOTE] Designating maintainers for some Spark components,Patrick Wendell <pwendell@gmail.com>,"+1 (non-binding) [for original process proposal]

Greg, the first time I've seen the word ""ownership"" on this thread is in
your message. The first time the word ""lead"" has appeared in this thread is
in your message as well. I don't think that was the inte"
Matei Zaharia <matei.zaharia@gmail.com>,"Thu, 6 Nov 2014 16:49:12 -0800",Re: [VOTE] Designating maintainers for some Spark components,Greg Stein <gstein@gmail.com>,"So I don't understand, Greg, are the partial committers committers, or are they not? Spark also has a PMC, but our PMC currently consists of all committers (we decided not to have a differentiation when we left the incubator). I see the Subversion partial committers listed as ""committers"" on https://people.apache.org/committers-by-project.html#subversion, so I assume they are committers. As far as I can see, CloudStack is similar.

Matei

and they do not require sign-off to work on that area. They can get a sign-off and commit outside that area. That approach doesn't compare to this proposal.
*every* line of code, then every PMC member should have complete rights to every line of code. Creating disparity flies in the face of a PMC member's responsibility. If I am a Spark PMC member, then I have responsibility for GraphX code, whether my name is Ankur, Joey, Reynold, or Greg. And interposing a barrier inhibits my responsibility to ensure GraphX is designed, maintained, and delivered to the Public.
the past 12 years :-) )
<http://svn.apache.org/repos/asf/subversion/trunk/COMMITTERS>
<https://subversion.apache.org/docs/community-guide/roles.html>
is
the
ago,
is
people
and
as well as call for an official vote on it on a public list. Basically, as the Spark project scales up, we need to define a model to make sure there is still great oversight of key components (in particular internal architecture and public APIs), and to this end I've proposed implementing a maintainer model for some of these components, similar to other large projects.
We've had over 80 contributors/month for the past 3 months, which I believe makes us the most active project in contributors/month at Apache, as well as over 500 patches/month. The codebase has also grown significantly, with new libraries for SQL, ML, graphs and more.
is to assign ""maintainers"" to oversee key components, where each patch to that component needs to get sign-off from at least one of its maintainers. Most existing large projects do this -- at Apache, some large ones with this model are CloudStack (the second-most active project overall), Subversion, and Kafka, and other examples include Linux and Python. This is also by-and-large how Spark operates today -- most components have a de-facto maintainer.
regarding architecture and API. This process would ensure that the component's maintainers see all proposed changes and consider them to fit together in a good way.
particular, it would be easy to look up who's responsible for each module and ask them for reviews, etc, rather than having patches slip between the cracks.
only applies to certain key components (e.g. scheduler, shuffle) and user-facing APIs (MLlib, GraphX, etc). Over time, as the project grows, we can expand it if we deem it useful. The specific mechanics would be as follows:
where one of the maintainers needs to sign off on each patch to the component.
maintainers.
knowledgeable committers on that component by the PMC. The PMC can vote to add / remove maintainers, and maintained components, through consensus.
for their components, though they do not need to be the main reviewers for them (e.g. they might just sign off on architecture / API). To prevent inactive maintainers from blocking the project, if a maintainer isn't responding in a reasonable time period (say 2 weeks), other committers can merge the patch, and the PMC will want to discuss adding another maintainer.
following projects:
https://cwiki.apache.org/confluence/display/CLOUDSTACK/CloudStack+Maintainers+Guide <https://cwiki.apache.org/confluence/display/CLOUDSTACK/CloudStack+Maintainers+Guide> <https://cwiki.apache.org/confluence/display/CLOUDSTACK/CloudStack+Maintainers+Guide <https://cwiki.apache.org/confluence/display/CLOUDSTACK/CloudStack+Maintainers+Guide>>
https://subversion.apache.org/docs/community-guide/roles.html <https://subversion.apache.org/docs/community-guide/roles.html> <https://subversion.apache.org/docs/community-guide/roles.html <https://subversion.apache.org/docs/community-guide/roles.html>>
components and maintainers. It would be good to get feedback on other components we might add, but please note that personnel discussions (e.g. ""I don't think Matei should maintain *that* component) should only happen on the private list. The initial components were chosen to include all public APIs and the main core components, and the maintainers were chosen from the most active contributors to those modules.
hours. The [VOTE] will end on Nov 8, 2014 at 6 PM PST.
---------------------------------------------------------------------
<mailto:dev-unsubscribe@spark.apache.org>
<mailto:dev-help@spark.apache.org>

"
Debasish Das <debasish.das83@gmail.com>,"Thu, 6 Nov 2014 16:51:39 -0800","Re: MatrixFactorizationModel predict(Int, Int) API",Xiangrui Meng <mengxr@gmail.com>,"model.recommendProducts can only be called from the master then ? I have a
set of 20% users on whom I am performing the test...the 20% users are in a
RDD...if I have to collect them all to master node and then call
model.recommendProducts, that's a issue...

Any idea how to optimize this so that we can calculate MAP statistics on
large samples of data ?



"
Xiangrui Meng <mengxr@gmail.com>,"Thu, 6 Nov 2014 17:07:21 -0800","Re: MatrixFactorizationModel predict(Int, Int) API",Debasish Das <debasish.das83@gmail.com>,"There is a JIRA for it: https://issues.apache.org/jira/browse/SPARK-3066

The easiest case is when one side is small. If both sides are large,
this is a super-expensive operation. We can do block-wise cross
product and then find top-k for each user.

Best,
Xiangrui


---------------------------------------------------------------------


"
Corey Nolet <cjnolet@gmail.com>,"Thu, 6 Nov 2014 20:17:43 -0500",Re: [VOTE] Designating maintainers for some Spark components,Matei Zaharia <matei.zaharia@gmail.com>,"PMC [1] is responsible for oversight and does not designate partial or full
committer. There are projects where all committers become PMC and others
where PMC is reserved for committers with the most merit (and willingness
to take on the responsibility of project oversight, releases, etc...).
Community maintains the codebase through committers. Committers to mentor,
roll in patches, and spread the project throughout other communities.

Adding someone's name to a list as a ""maintainer"" is not a barrier. With a
community as large as Spark's, and myself not being a committer on this
project, I see it as a welcome opportunity to find a mentor in the areas in
which I'm interested in contributing. We'd expect the list of names to grow
as more volunteers gain more interest, correct? To me, that seems quite
contrary to a ""barrier"".

[1] http://www.apache.org/dev/pmc.html



"
Sandy Ryza <sandy.ryza@cloudera.com>,"Thu, 6 Nov 2014 17:28:52 -0800",Re: [VOTE] Designating maintainers for some Spark components,Corey Nolet <cjnolet@gmail.com>,"It looks like the difference between the proposed Spark model and the
CloudStack / SVN model is:
* In the former, maintainers / partial committers are a way of centralizing
oversight over particular components among committers
* In the latter, maintainers / partial committers are a way of giving
non-committers some power to make changes

-Sandy


"
Kyle Ellrott <kellrott@soe.ucsc.edu>,"Thu, 6 Nov 2014 17:42:12 -0800",Re: Implementing TinkerPop on top of GraphX,"""York, Brennon"" <Brennon.York@capitalone.com>","I think its best to look to existing standard rather then try to make your
own. Of course small additions would need to be added to make it valuable
for the Spark community, like a method similar to Gremlin's 'table'
function, that produces an RDD instead.
But there may be a lot of extra code and data structures that would need to
be added to make it work, and those may not be directly applicable to all
GraphX users. I think it would be best run as a separate module/project
that builds directly on top of GraphX.

Kyle




p
 could be
m>,
p3
s
m
At
ss, and, if
g
ns
te
r
e
!
the
e
se
y
"
Arun C Murthy <acm@hortonworks.com>,"Thu, 6 Nov 2014 17:46:20 -0800",Re: [VOTE] Designating maintainers for some Spark components,dev@spark.apache.org,"With my ASF Member hat on, I fully agree with Greg.

As he points out, this is an anti-pattern in the ASF and is severely frowned upon.

We, in Hadoop, had a similar trajectory where we had were politely told to go away from having sub-project committers (HDFS, MapReduce etc.) to a common list of committers. There were some concerns initially, but we have successfully managed to work together and build a more healthy community as a result of following the advice on the ASF Way.

I do have sympathy for good oversight etc. as the project grows and attracts many contributors - it's essentially the need to have smaller, well-knit Ps  (e.g. Spark, MLLIB, GraphX) with separate committer lists for each representing the appropriate community. Hadoop went a similar route where we had Pig, Hive, HBase etc. as sub-projects initially and then split them into TLPs with more focussed communities to the benefit of everyone. Maybe you guys want to try this too?

----

Few more observations:
# In general, *discussions* on project directions (such as new concept of *maintainers*) should happen first on the public lists *before* voting, not in the private PMC list.
# If you chose to go this route in spite of this advice, seems to me Spark would be better of having more maintainers per component (at least 4-5), probably with a lot more diversity in terms of affiliations. Not sure if that is a concern - do you have good diversity in the proposed list? This will ensure that there are no concerns about a dominant employer controlling a project.

----

Hope this helps - we've gone through similar journey, got through similar issues and fully embraced the Apache Way (™) as Greg points out to our benefit.

thanks,
Arun



l as call for an official vote on it on a public list. Basically, as the Spark project scales up, we need to define a model to make sure there is still great oversight of key components (in particular internal architecture and public APIs), and to this end I've proposed implementing a maintainer model for some of these components, similar to other large projects.
 had over 80 contributors/month for the past 3 months, which I believe makes us the most active project in contributors/month at Apache, as well as over 500 patches/month. The codebase has also grown significantly, with new libraries for SQL, ML, graphs and more.
 assign ""maintainers"" to oversee key components, where each patch to that component needs to get sign-off from at least one of its maintainers. Most existing large projects do this -- at Apache, some large ones with this model are CloudStack (the second-most active project overall), Subversion, and Kafka, and other examples include Linux and Python. This is also by-and-large how Spark operates today -- most components have a de-facto maintainer.
ng architecture and API. This process would ensure that the component's maintainers see all proposed changes and consider them to fit together in a good way.
it would be easy to look up who’s responsible for each module and ask them for reviews, etc, rather than having patches slip between the cracks.
pplies to certain key components (e.g. scheduler, shuffle) and user-facing APIs (MLlib, GraphX, etc). Over time, as the project grows, we can expand it if we deem it useful. The specific mechanics would be as follows:
 one of the maintainers needs to sign off on each patch to the component.
mmitters on that component by the PMC. The PMC can vote to add / remove maintainers, and maintained components, through consensus.
ir components, though they do not need to be the main reviewers for them (e.g. they might just sign off on architecture / API). To prevent inactive maintainers from blocking the project, if a maintainer isn't responding in a reasonable time period (say 2 weeks), other committers can merge the patch, and the PMC will want to discuss adding another maintainer.
ojects:
udStack+Maintainers+Guide <https://cwiki.apache.org/confluence/display/CLOUDSTACK/CloudStack+Maintainers+Guide> 
tml <https://subversion.apache.org/docs/community-guide/roles.html>
d maintainers. It would be good to get feedback on other components we might add, but please note that personnel discussions (e.g. ""I don't think Matei should maintain *that* component) should only happen on the private list. The initial components were chosen to include all public APIs and the main core components, and the maintainers were chosen from the most active contributors to those modules.
[VOTE] will end on Nov 8, 2014 at 6 PM PST.



-- 
CONFIDENTIALITY NOTICE
NOTICE: This message is intended for the use of the individual or entity to 
which it is addressed and may contain information that is confidential, 
privileged and exempt from disclosure under applicable law. If the reader 
of this message is not the intended recipient, you are hereby notified that 
any printing, copying, dissemination, distribution, disclosure or 
forwarding of this communication is strictly prohibited. If you have 
received this communication in error, please contact the sender immediately 
and delete it from your system. Thank You.

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Thu, 6 Nov 2014 19:24:30 -0800",Re: Implementing TinkerPop on top of GraphX,Kyle Ellrott <kellrott@soe.ucsc.edu>,"Some form of graph querying support would be great to have. This can be a
great community project hosted outside of Spark initially, both due to the
maturity of the component itself as well as the maturity of query language
standards (there isn't really a dominant standard for graph ql).

provide more primitives in order to support the new ql implementation.
There might also be inherent mismatches in the way the external API is
defined vs what GraphX can support. We should discuss those on a
case-by-case basis.



r
ct
m
op
k
h
.
.
s could
op3
rs
f
 At
I
ess, and, if
op2
k
g
e
a!
 the
r
he
y
ty
r
y
"
Cody Koeninger <cody@koeninger.org>,"Thu, 6 Nov 2014 22:14:50 -0600",Re: [VOTE] Designating maintainers for some Spark components,,"My 2 cents:

Spark since pre-Apache days has been the most friendly and welcoming open
source project I've seen, and that's reflected in its success.

It seems pretty obvious to me that, for example, Michael should be looking
at major changes to the SQL codebase.  I trust him to do that in a way
that's technically and socially appropriate.

What Matei is saying makes sense, regardless of whether it gets codified in
a process.




o
e
as
en
ot
k
o our
he
 a
ve
h
at
t
,
,
 ask them
ng
em
e
n
ners+Guide
ners+Guide
k
he
e
e
to
at
ly
"
Corey Nolet <cjnolet@gmail.com>,"Thu, 6 Nov 2014 23:47:51 -0500",Re: [VOTE] Designating maintainers for some Spark components,Arun C Murthy <acm@hortonworks.com>,"I'm actually going to change my non-binding to +0 for the proposal as-is.

I overlooked some parts of the original proposal that, when reading over
them again, do not sit well with me. ""one of the maintainers needs to sign
off on each patch to the component"", as Greg has pointed out, does seem to
imply that there are committers with more power than others with regards to
specific components- which does imply ownership.

My thinking would be to re-work in some way as to take out the accent on
ownership. I would maybe focus on things such as:

1) Other committers and contributors being forced to consult with
maintainers of modules before patches can get rolled in.
2) Maintainers being assigned specifically from PMC.
3) Oversight to have more accent on keeping the community happy in a
specific area of interest vice being a consultant for the design of a
specific piece.


o
e
as
en
ot
k
o our
he
 a
ve
h
at
t
,
,
 ask them
ng
em
e
n
ners+Guide
ners+Guide
k
he
e
e
to
at
ly
"
Greg Stein <gstein@gmail.com>,"Thu, 6 Nov 2014 23:08:22 -0600",Re: [VOTE] Designating maintainers for some Spark components,Corey Nolet <cjnolet@gmail.com>,"[ I'm going to try and pull a couple thread directions into this one, to
avoid explosion :-) ]


Note: I'm going to use ""you"" generically; I understand you [Corey] are not
a PMC member, at this time.

+1 (non-binding) [for original process proposal]

The word ""ownership"" is there, but with a different term. If you are a PMC
member, and *cannot* alter a line of code without another's consent, then
you don't ""own"" that code. Your ownership is subservient to another. You
are not a *peer*, but a second-class citizen at this point.

The term ""maintainer"" in this context is being used as a word for ""lead"".
The maintainers are a *gate* for any change. That is not consensus. The
proposal attempts to soften that, and turn it into an oligarchy of several
maintainers. But the simple fact is that you have ""some"" with the ability
to set direction, and those who do not. They are called ""leaders"" in most
contexts, but however you want to slice it... the dynamic creates people
with unequal commit ability.

But as the PMC member you *are* responsible for it. That is the very basic
definition of being a PMC member. You are responsible for ""all things
Spark"".

responsibility to the community to make sure that their patches are being

""where each patch to that component needs to get sign-off from at least one
of its maintainers""

That establishes two types of PMC members: those who require sign-off, and
those who don't. Apache is intended to be a group of peers, none ""more
equal"" than others.

That said, we *do* recognize various levels of merit. This is where you see
differences between committers, their range of access, and PMC members. But
when you hit the *PMC member* role, then you are talking about a legal
construct established by the Foundation. You move outside of community
norms, and into how the umbrella of the Foundation operates. PMC members
are individually responsible for all of the code under their purview, which
is then at the direction of the Foundation itself. I'll skip that
conversation, and leave it with the simple phrase: as a PMC member, you're
responsible for the whole codebase.

So following from that, anything that *restricts* your ability to work on
that code, is a problem.

In fact, I'll go as far as to say that since Apache is a meritocracy, the

ends. You're a PMC member, and that is all there is to it. Just because
Jane commits 1000 times per month, makes her no better than John who
commits 10/month. They are peers on the PMC and have equal rights and
responsibility to the codebase.

Historically, some PMCs have attempted to create variant levels within the
PMC, or create different groups and rights, or different partitions over
the code, and ... again, historically: it has failed. This is why Apache
stresses consensus. The failure modes are crazy and numerous when moving
away from that, into silos.



PMC members are responsible for the code. They provide the oversight,
direction, and management. (they're also responsible for the community, but
that distinction isn't relevant in this contrasting example)

Committers can make changes to the code, with the
acknowledgement/agreement/direction of the PMC.

When these groups are equal, like Spark, then things are pretty simple.

But many communities in Apache define them as disparate. Committers may
work on the code (a single area, or all of it), but don't have any direct
input into its direction (ie. they're not on the PMC).

Within Subversion, we give people commit rights to areas, and let them go
wild. But they aren't part of the *whole* project's direction. Maybe just
the SWIG bindings, or a migration tool, or a supplemental administrative
tool. These are ""partial committers"", in Subversion's parlance. PMC
members, on the other hand, are known historically as ""full committers"" and
have rights over the whole codebase. There are no partitions. There are no
component maintainers.

Many projects at Apache provide whole-project commit access to people, but
don't give them PMC rights. I find that strange, but it is quite common.
The trust level is ""code"" rather than ""direction"". Subversion trusts people
to limited areas, or to the whole project, and (thus) to the project's
direction.

...

Within this context of those who are responsible and involved with the
project, I find it very disconcerting to partition things and tell people
""you cannot make any change [even though you're on the PMC, and responsible
for it] unless John says it is okay.""

Historically, Apache allowed fine-grained access control lists over
who-could-change-what. Most of those have been removed, as we learned how
dangerous they were to a community. How they set up cliques, and leads, and
killed the peer relationships. As we've reviewed the relationships and
oversight needs, to create a proper legal umbrella for all of our
committers, the relationship of PMC members to its codebase has become much
more clear.

Unfortunately, much of this is historical knowledge, rather than written
down. But writing it down makes it sounds like ""rules"", and proscriptions
just never seem to work out right. It is a very hard problem, to share what
works (or not) across the communities here at the Foundation.

Cheers,
-g
"
Greg Stein <gstein@gmail.com>,"Thu, 6 Nov 2014 23:27:59 -0600",Re: [VOTE] Designating maintainers for some Spark components,Sandy Ryza <sandy.ryza@cloudera.com>,"

I can't speak for CloudStack, but for Subversion: yes, you're exactly
right, Sandy.

We use the ""partial committer"" role as a way to bring in new committers.
""Great idea, go work >there<, and have fun"". Any PMC member can give a
single +1, and that new (partial) committer gets and account/access, and is
off and running. We don't even ask for a PMC vote (though, we almost always
have a brief discussion).

The ""svnrdump"" tool was written by a *Git* Google Summer of Code student.
He wanted a quick way to get a Subversion dumpfile from a remote
repository, in order to drop that into Git. We gave him commit access
directly into trunk/svnrdump, and he wrote the tool. Technically, he could
commit anywhere in our tree, but we just asked him not to, without a +1
from a PMC member.

Partial committers are a way to *include* people into the [coding]
community. And hopefully, over time, they grow into something more.

""Maintainers"" are a way (IMO) to *exclude* people from certain commit
activity. (or more precisely: limit/restrict, rather than exclude)

You can see why it concerns me :-)

Cheers,
-g
"
Matei Zaharia <matei.zaharia@gmail.com>,"Thu, 6 Nov 2014 21:38:26 -0800",Re: [VOTE] Designating maintainers for some Spark components,Greg Stein <gstein@gmail.com>,"Alright, Greg, I think I understand how Subversion's model is different, which is that the PMC members are all full committers. However, I still think that the model proposed here is purely organizational (how the PMC and committers organize themselves), and in no way changes peoples' ownership or rights. Certainly the reason I proposed it was organizational, to make sure patches get seen by the right people. I believe that every PMC member still has the same responsibility for two reasons:

1) The PMC is actually what selects the maintainers, so basically this mechanism is a way for the PMC to make sure certain people review each patch.

2) Code changes are all still made by consensus, where any individual has veto power over the code. The maintainer model mentioned here is only meant to make sure that the ""experts"" in an area get to see each patch *before* it is merged, and choose whether to exercise their veto power.

Let me give a simple example, which is a patch to the Spark core public API. Say I'm a maintainer in this API. Without the maintainer model, the decision on the patch would be made as follows:

- Any committer could review the patch and merge it
- At any point during this process, I (as the main expert on this) could come in and -1 it, or give feedback
- In addition, any other committer beyond me is allowed to -1 this patch

With the maintainer model, the process is as follows:

- Any committer could review the patch and merge it, but they would need to forward it to me (or another core API maintainer) to make sure we also approve
- At any point during this process, I could come in and -1 it, or give feedback
- In addition, any other committer beyond me is still allowed to -1 this patch

The only change in this model is that committers are responsible to forward patches in these areas to certain other committers. If every committer had perfect oversight of the project, they could have also seen every patch to their component on their own, but this list ensures that they see it even if they somehow overlooked it.

It's true that technically this model might ""gate"" development in the sense of adding some latency, but it doesn't ""gate"" it any more than consensus as a whole does, where any committer (not even PMC member) can -1 any code change. In fact I believe this will speed development by motivating the maintainers to be active in reviewing their areas and by reducing the chance that mistakes happen that require a revert.

I apologize if this wasn't clear in any way, but I do think it's pretty clear in the original wording of the proposal. The sign-off by a maintainer is simply an extra step in the merge process, it does *not* mean that other committers can't -1 a patch, or that the maintainers get to review all patches, or that they somehow have more ""ownership"" of the component (since they already had the ability to -1). I also wanted to clarify another thing -- it seems there is a misunderstanding that only PMC members can be maintainers, but this was not the point; the PMC *assigns* maintainers but they can do it out of the whole committer pool (and if we move to separating the PMC from the committers, I fully expect some non-PMC committers to be made maintainers).

I hope this clarifies where we're coming from, and why we believe that this still conforms fully with the spirit of Apache (collaborative, open development that anyone can participate in, and meritocracy for project governance). There were some comments made about the maintainers being only some kind of list of people without a requirement to review stuff, but as you can see it's the requirement to review that is the main reason I'm proposing this, to ensure we have an automated process for patches to certain components to be seen. If it helps we may be able to change the wording to something like ""it is every committer's responsibility to forward patches for a maintained component to that component's maintainer"", or something like that, instead of using ""sign off"". If we don't do this, I'd actually be against any measure that lists some component ""maintainers"" without them having a specific responsibility. Apache is not a place for people to gain kudos by having fancier titles given on a website, it's a place for building great communities and software.

Matei



committers.
and is
always
student.
could
+1


---------------------------------------------------------------------


"
Greg Stein <gstein@gmail.com>,"Fri, 7 Nov 2014 01:27:18 -0600",Re: [VOTE] Designating maintainers for some Spark components,Matei Zaharia <matei.zaharia@gmail.com>,"[last reply for tonite; let others read; and after the next drink or three,
I shouldn't be replying...]




That was not my impression, when your proposal said that maintainers need
to provide ""sign-off"".

Okay. Now my next item of feedback starts here:



... and ends here.

All of that text is about a process for applying Vetoes. ... That is just
the wrong focus (IMO).

Back around 2000, in httpd, we ran into vetoes. It was horrible. The
community suffered. We actually had a face-to-face at one point, flying in
people from around the US, gathering a bunch of the httpd committers to
work through some basic problems. The vetoes were flying fast and furious,
and it was just the wrong dynamic. Discussion and consensus had been thrown
aside. Trust was absent. Peer relationships were ruined. (tho thankfully,
our personal relationships never suffered, and that basis helped us pull it
back together)

Contrast that with Subversion. We've had some vetoes, yes. But invariably,
MOST of them would really be considered ""woah. -1 on that. let's talk"".
few, a -1 was always about opening a discussion to fix a particular commit.

It looks like you are creating a process to apply vetoes. That seems
backwards.

It seems like you want a process to ensure that reviews are performed. IMO,
all committers/PMC members should begin as *trusted*. Why not? You've
already voted them in as committers/PMCers. So trust them. Trust.

And that leads to ""trust, but verify"". The review process. So how about
creating a workflow that is focused on ""what needs to be reviewed"" rather
than ""nobody can make changes, unless John says so"". ??




As I've mentioned ""sign off"" is a term for unequal rights. So yes...
finding a modification would be great. But honestly, I think it would be
nice to find a workflow that establishes the *reviews* that you're seeking.

[ Subversion revision props could be used to tag if/when somebody has
reviewed a particular revision; I dunno if you guys are using svn or git ]




Be careful here. ""Responsibility"" is pretty much a taboo word. All of
Apache is a group of volunteers. People can disappear at any point, which
is why you need multiple (as my fellow Director warned, on your private
list). And multiple people can disappear.

This is the primary reason why Apache prefers lazy consensus. Participants
may disappear, so everybody should be aware of that, and be aware that lazy
consensus is in operation. Placing a *volunteer* in the path of forward
progress is fraught with disaster, in the long run :-)


Cheers,
-g
"
Reynold Xin <rxin@databricks.com>,"Thu, 6 Nov 2014 23:37:33 -0800",Re: [VOTE] Designating maintainers for some Spark components,Greg Stein <gstein@gmail.com>,"Greg,

Thanks a lot for commenting on this, but I feel we are splitting hairs
here. Matei did mention -1, followed by ""or give feedback"". The original
process outlined by Matei was exactly about review, rather than fighting.
Nobody wants to spend their energy fighting.  Everybody is doing it to
improve the project.


In particular, quoting you in your email

""Be careful here. ""Responsibility"" is pretty much a taboo word. All of
Apache is a group of volunteers. People can disappear at any point, which
is why you need multiple (as my fellow Director warned, on your private
list). And multiple people can disappear.""

Take a look at this page: http://www.apache.org/dev/pmc.html

This Project Management Committee Guide outlines the general
***responsibilities*** of PMC members in managing their projects.

Are you suggesting the wording used by the PMC guideline itself is taboo?






"
Vinod Kumar Vavilapalli <vinodkv@apache.org>,"Thu, 6 Nov 2014 23:43:25 -0800",Re: [VOTE] Designating maintainers for some Spark components,Matei Zaharia <matei.zaharia@gmail.com>,"need to forward it to me (or another core API maintainer) to make sure we also approve
feedback
this patch
forward patches in these areas to certain other committers. If every committer had perfect oversight of the project, they could have also seen every patch to their component on their own, but this list ensures that they see it even if they somehow overlooked it.


Having done the job of playing an informal 'maintainer' of a project myself, this is what I think you really need:

The so called 'maintainers' do one of the below
 - Actively poll the lists and watch over contributions. And follow what is repeated often around here: Trust but verify.
 - Setup automated mechanisms to send all bug-tracker updates of a specific component to a list that people can subscribe to

And/or
 - Individual contributors send review requests to unofficial 'maintainers' over dev-lists or through tools. Like many projects do with review boards and other tools.

Note that none of the above is a required step. It must not be, that's the point. But once set as a convention, they will all help you address your concerns with project scalability.

Anything else that you add is bestowing privileges to a select few and forming dictatorships. And contrary to what the proposal claims, this is neither scalable nor confirming to Apache governance rules.

+Vinod
"
Sandy Ryza <sandy.ryza@cloudera.com>,"Fri, 7 Nov 2014 01:05:34 -0800",proposal / discuss: multiple Serializers within a SparkContext?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey all,

Was messing around with Spark and Google FlatBuffers for fun, and it got me
thinking about Spark and serialization.  I know there's been work / talk
about in-memory columnar formats Spark SQL, so maybe there are ways to
provide this flexibility already that I've missed?  Either way, my thoughts:

Java and Kryo serialization are really nice in that they require almost no
extra work on the part of the user.  They can also represent complex object
graphs with cycles etc.

There are situations where other serialization frameworks are more
efficient:
* A Hadoop Writable style format that delineates key-value boundaries and
allows for raw comparisons can greatly speed up some shuffle operations by
entirely avoiding deserialization until the object hits user code.
Writables also probably ser / deser faster than Kryo.
* ""No-deserialization"" formats like FlatBuffers and Cap'n Proto address the
tradeoff between (1) Java objects that offer fast access but take lots of
space and stress GC and (2) Kryo-serialized buffers that are more compact
but take time to deserialize.

The drawbacks of these frameworks are that they require more work from the
user to define types.  And that they're more restrictive in the reference
graphs they can represent.

In large applications, there are probably a few points where a
""specialized"" serialization format is useful. But requiring Writables
everywhere because they're needed in a particularly intense shuffle is
cumbersome.

In light of that, would it make sense to enable varying Serializers within
an app? It could make sense to choose a serialization framework both based
on the objects being serialized and what they're being serialized for
(caching vs. shuffle).  It might be possible to implement this underneath
the Serializer interface with some sort of multiplexing serializer that
chooses between subserializers.

Nothing urgent here, but curious to hear other's opinions.

-Sandy
"
Reynold Xin <rxin@databricks.com>,"Fri, 7 Nov 2014 01:09:18 -0800",Re: proposal / discuss: multiple Serializers within a SparkContext?,Sandy Ryza <sandy.ryza@cloudera.com>,"Technically you can already do custom serializer for each shuffle operation
(it is part of the ShuffledRDD). I've seen Matei suggesting on jira issues
(or github) in the past a ""storage policy"" in which you can specify how
data should be stored. I think that would be a great API to have in the
long run. Designing it won't be trivial though.



"
Matei Zaharia <matei.zaharia@gmail.com>,"Fri, 7 Nov 2014 01:19:27 -0800",Re: proposal / discuss: multiple Serializers within a SparkContext?,Reynold Xin <rxin@databricks.com>,"Yup, the JIRA for this was https://issues.apache.org/jira/browse/SPARK-540 (one of our older JIRAs). I think it would be interesting to explore this further. Basically the way to add it into the API would be to add a version of persist() that takes another class than StorageLevel, say StorageStrategy, which allows specifying a custom serializer or perhaps even a transformation to turn each partition into another representation before saving it. It would also be interesting if this could work directly on an InputStream or ByteBuffer to deal with off-heap data.

is that a lot of type information is lost when you pass data to it, so the serializers spend a fair bit of time figuring out what class each object written is. With this model, it would be possible for a serializer to know that all its data is of one type, which is pretty cool, but we might also consider ways of expanding the current Serializer interface to take more info.

Matei

operation
issues
how
the
got me
talk
to
almost no
object
and
operations by
address the
lots of
compact
from the
reference
is
within
based
underneath
that


---------------------------------------------------------------------


"
rapelly kartheek <kartheek.mbms@gmail.com>,"Fri, 7 Nov 2014 15:41:18 +0530","How spark/*/Storage/BlockManagerMaster.askDriverWithReply() responds
 to various query messages",dev@spark.apache.org," Hi,

I am trying to understand how the
/spark/*/Storage/BlockManagerMaster.askDriverWithReply() works.

def getPeers(blockManagerId: BlockManagerId, numPeers: Int):
Seq[BlockManagerId] = {

val result =
askDriverWithReply[Seq[BlockManagerId]](GetPeers(blockManagerId, numPeers))

if (result.length != numPeers) {

throw new SparkException(

""Error getting peers, only got "" + result.size + "" instead of "" + numPeers)

}

result
 }

Here, getPeers calls askDriverWithReply().

 private def askDriverWithReply[T](message: Any): T = {

// TODO: Consider removing multiple attempts

if (driverActor == null) {

throw new SparkException(""Error sending message to BlockManager as
driverActor is null "" +

""[message = "" + message + ""]"")

}

var attempts = 0

var lastException: Exception = null

while (attempts < AKKA_RETRY_ATTEMPTS) {

attempts += 1

try {

val future = driverActor.ask(message)(timeout)

val result = Await.result(future, timeout)

if (result == null) {

throw new SparkException(""BlockManagerMaster returned null"")

}

 return result.asInstanceOf[T]

} catch {

case ie: InterruptedException => throw ie

case e: Exception =>

lastException = e

logWarning(""Error sending message to BlockManagerMaster in "" + attempts + ""
attempts"", e)

}

Thread.sleep(AKKA_RETRY_INTERVAL_MS)

}

throw new SparkException(""Error sending message to BlockManagerMaster
[message = "" + message + ""]"", lastException)
 }

Here, getPeers method calls askDriverWithReply() with message ""GetPeers()"".
The Driver returns the BlockManagerId's.

val future = driverActor.ask(message)(timeout)

val result = Await.result(future, timeout)
Here, we obtain ""result"". But, I couldn't find definition of ask() that
processes message GetPeers(). Can someone please tell me how/where the
'result' is being constructed??

Thank you!!
Karthik
"
Imran Rashid <imran@therashids.com>,"Fri, 7 Nov 2014 08:34:17 -0600","Re: How spark/*/Storage/BlockManagerMaster.askDriverWithReply()
 responds to various query messages",rapelly kartheek <kartheek.mbms@gmail.com>,"ask() is a method on every Actor.  It comes from the akka library, which
spark uses for a lot of the communication between various components.

There is some documentation on ask() here (go to the section on ""Send
messages""):
http://doc.akka.io/docs/akka/2.2.3/scala/actors.html

though if you are totally new to it, you might want to work through a
simple akka tutorial first, before diving into the docs.


"
Josh Rosen <rosenville@gmail.com>,"Fri, 7 Nov 2014 09:51:09 -0800",Re: Appropriate way to add a debug flag,"""Ganelin, Ilya"" <Ilya.Ganelin@capitalone.com>, dev <dev@spark.apache.org>","(Whoops, forgot to copy dev@ in my original reply; adding it back)

Yeah, the GraphViz part was mostly for fun and for understanding cyclic
object graphs.  In general, an object graph might contain cycles, so for
understanding the overall structure it's handy to have a picture.  The
GraphViz thing is actually pretty fun to play with in an interactive
notebook environment, since even fairly simple programs can produce really
interesting object graphs.

For the purposes of debugging serialization errors, though, I guess you
only need to know about some path of non-transient fields that leads from
the target object to an unserializable object.  For that case, you might be
able to add a try-catch block that performs an object graph traversal to
find a path to a non-serializable object if a serialization error occurs.
Would logging this path be sufficient to debug the most common
serialization issues, such as unexpected over-capture of non-serializable
objects in closures?

I guess that some users might also want a more general object graph printer
/ debugger to help debug performance issues related to over-captures that
do not lead to errors, but that might be a lower priority / could happen in
a separate PR.

Another option would be to do something like
http://blog.crazybob.org/2007/02/debugging-serialization.html to print a
trace from the serializer's point of view, but the output from that might
be hard to understand since it could obscure the chain of references /
fields that led to the error.

- Josh


 in RDDs
wanted in
ng
ree?
stuff in
h of
y
e
ed
"
"""Ganelin, Ilya"" <Ilya.Ganelin@capitalone.com>","Fri, 7 Nov 2014 13:04:58 -0500",RE: Appropriate way to add a debug flag,"'Josh Rosen' <rosenville@gmail.com>, 'dev' <dev@spark.apache.org>","That perspective - identifying the non serializable components - is actually very helpful. I'll look into figuring out a way to do this. Thanks !

-----Original Message-ville@gmail.com>]
Sent: Friday, November 07, 2014 12:51 PM Eastern Standard Time
To: Ganelin, Ilya; dev
Subject: Re: Appropriate way to add a debug flag

(Whoops, forgot to copy dev@ in my original reply; adding it back)

Yeah, the GraphViz part was mostly for fun and for understanding cyclic object graphs.  In general, an object graph might contain cycles, so for understanding the overall structure it's handy to have a picture.  The GraphViz thing is actually pretty fun to play with in an interactive notebook environment, since even fairly simple programs can produce really interesting object graphs.

For the purposes of debugging serialization errors, though, I guess you only need to know about some path of non-transient fields that leads from the target object to an unserializable object.  For that case, you might be able to add a try-catch block that performs an object graph traversal to find a path to a non-serializable object if a serialization error occurs.  Would logging this path be sufficient to debug the most common serialization issues, such as unexpected over-capture of non-serializable objects in closures?

I guess that some users might also want a more general object graph printer / debugger to help debug performance issues related to over-captures that do not lead to errors, but that might be a lower priority / could happen in a separate PR.

Another option would be to do something like http://blog.crazybob.org/2007/02/debugging-serialization.html to print a trace from the serializer's point of view, but the output from that might be hard to understand since it could obscure the chain of references / fields that led to the error.

- Josh

On Thu, Nov 6, 2014 at 12:01 PM, Ganelin, Ilya <Ilya.Ganelin@capitalone.com<mailto:Ilya.Ganelin@capitalone.com>> wrote:
Hi Josh â€“ I think this could be useful for visualizing references in RDDs but I actually wasnâ€™t sure that this was that the original issue wanted in terms of a solution. I assumed the the more useful output would be a string output. E.g.

RDD
 - Child 1
  â€” Child 1.1
  â€” Child 1.2
 - Child 2
 - Child 3

So that itâ€™s readily integrated with the Spark logs. Would you agree?

I like the SparkConf idea, I will look into that.
From: Josh Rosen <rosenville@gmail.com<mailto:rosenville@gmail.com>>
Date: Thursday, November 6, 2014 at 2:42 PM
To: ""Ganelin, Ilya"" <ilya.ganelin@capitalone.com<mailto:ilya.ganelin@capitalone.com>>
Subject: Re: Appropriate way to add a debug flag

This is timely, since Iâ€™ve actually been hacking on some related stuff in order to debug whether unexpected objects are being pulled into closures.  Hereâ€™s some code to print a graphviz DOT file that shows the graph of non-transient, non-primitive objects reachable from a given object: https://gist.github.com/JoshRosen/d6a8972c99992e97d040

For enabling / disabling automatic logging of this, I suppose that you could add a configuration option to SparkConf.


On November 5, 2014 at 8:02:35 AM, Ganelin, Ilya (ilya.ganelin@capitalone.com<mailto:ilya.ganelin@capitalone.com>) wrote:

Hello all â€“ I am working on https://issues.apache.org/jira/browse/SPARK-3694 and would like to understand the appropriate mechanism by which to check for a debug flag before printing a graph traversal of dependencies of an RDD or Task. I understand that I can use the logging utility and use logDebug to actually print the output but the graph traversal should not be executed unless the debug output is enabled. The code changes I will be making will be in the DAGScheduler and TaskSetManager classes.

Modifying the function interfaces does not seem like the appropriate approach . Is there an existing debug flag that is set somehow within the spark config?
________________________________________________________

The information contained in this e-mail is confidential and/or proprietary to Capital One and/or its affiliates. The information transmitted herewith is intended only for use by the individual or entity to which it is addressed. If the reader of this message is not the intended recipient, you are hereby notified that any review, retransmission, dissemination, distribution, copying or other use of, or taking of any action in reliance upon this information is strictly prohibited. If you have received this communication in error, please contact the sender and delete the material from your computer.

________________________________

The information contained in this e-mail is confidential and/or proprietary to Capital One and/or its affiliates. The information transmitted herewith is intended only for use by the individual or entity to which it is addressed.  If the reader of this message is not the intended recipient, you are hereby notified that any review, retransmission, dissemination, distribution, copying or other use of, or taking of any action in reliance upon this information is strictly prohibited. If you have received this communication in error, please contact the sender and delete the material from your computer.

________________________________________________________

The information contained in this e-mail is confidential and/or proprietary to Capital One and/or its affiliates. The information transmitted herewith is intended only for use by the individual or entity to which it is addressed.  If the reader of this message is not the intended recipient, you are hereby notified that any review, retransmission, dissemination, distribution, copying or other use of, or taking of any action in reliance upon this information is strictly prohibited. If you have received this communication in error, please contact the sender and delete the material from your computer.
"
Kay Ousterhout <keo@eecs.berkeley.edu>,"Fri, 7 Nov 2014 10:48:33 -0800",Re: [VOTE] Designating maintainers for some Spark components,"""dev@spark.apache.org"" <dev@spark.apache.org>","+1 (binding)

I see this as a way to increase transparency and efficiency around a
process that already informally exists, with benefits to both new
contributors and committers.  For new contributors, it makes clear who they
should ping about a pending pa"
Kyle Ellrott <kellrott@soe.ucsc.edu>,"Fri, 7 Nov 2014 10:59:59 -0800",Re: Implementing TinkerPop on top of GraphX,Reynold Xin <rxin@databricks.com>,"Who here would be interested in helping to work on an implementation of the
Tikerpop3 Gremlin API for Spark? Is this something that should continue in
the Spark discussion group, or should it migrate to the Gremlin message
group?

Reynold is right that there will be inherent mismatches in the APIs, and
there will need to be some discussions with the GraphX group about the best
explicit edges ids, while Gremlin has both. Edge ids could be put into the
attr field, but then that means the user would have to explicitly subclass
their edge attribute to the edge attribute interface. Is that worth doing,
versus adding an id to everyones's edges?

Kyle



e
e
ect
l
Pop
rk
the
m.
).
is could
n
e
o
pop3
s
ors
of
o
. At
 I
e
cess, and, if
pop2
ck
ng
t
e
r
le
s
o
n the
n
e
ly
-
ity
or
ty
r
"
"""York, Brennon"" <Brennon.York@capitalone.com>","Fri, 7 Nov 2014 14:11:36 -0500",Re: Implementing TinkerPop on top of GraphX,"Kyle Ellrott <kellrott@soe.ucsc.edu>, Reynold Xin <rxin@databricks.com>","I¡¯m definitely onboard to help / take a portion of this work. I too am wondering what the proper discussion venue should be moving forward given Reynold¡¯s remarks on a community project hosted outside Spark. If I¡¯m understanding correctly my take would be:

1. to find a core group of developers to take on this work (Kyle, myself, ???)
2. build an initial implementation
3. iterate / discuss with the Spark community as we find discrepancies between GraphX and the Gremlin3 API¡¯s
4. contribute back to the Spark community when complete

Does that seem like a sound plan or am I way off base here? Itching to work on this :)

From: Kyle Ellrott <kellrott@soe.ucsc.edu<mailto:kellrott@soe.ucsc.edu>>
Date: Friday, November 7, 2014 at 10:59 AM
To: Reynold Xin <rxin@databricks.com<mailto:rxin@databricks.com>>
Cc: ""York, Brennon"" <brennon.york@capitalone.com<mailto:brennon.york@capitalone.com>>, Kushal Datta <kushal.datta@gmail.com<mailto:kushal.datta@gmail.com>>, ""dev@spark.apache.org<mailto:dev@spark.apache.org>"" <dev@spark.apache.org<mailto:dev@spark.apache.org>>, Matthias Broecheler <matthias@thinkaurelius.com<mailto:matthias@thinkaurelius.com>>
Subject: Re: Implementing TinkerPop on top of GraphX

Who here would be interested in helping to work on an implementation of the Tikerpop3 Gremlin API for Spark? Is this something that should continue in the Spark discussion group, or should it migrate to the Gremlin message group?

Reynold is right that there will be inherent mismatches in the APIs, and there will need to be some discussions with the GraphX group about the best way to go. One example would be edge ids. GraphX has vertex ids, but no explicit edges ids, while Gremlin has both. Edge ids could be put into the attr field, but then that means the user would have to explicitly subclass their edge attribute to the edge attribute interface. Is that worth doing, versus adding an id to everyones's edges?

Kyle


On Thu, Nov 6, 2014 at 7:24 PM, Reynold Xin <rxin@databricks.com<mailto:rxin@databricks.com>> wrote:
Some form of graph querying support would be great to have. This can be a great community project hosted outside of Spark initially, both due to the maturity of the component itself as well as the maturity of query language standards (there isn't really a dominant standard for graph ql).

One thing is that GraphX API will need to evolve and probably need to provide more primitives in order to support the new ql implementation. There might also be inherent mismatches in the way the external API is defined vs what GraphX can support. We should discuss those on a case-by-case basis.


On Thu, Nov 6, 2014 at 5:42 PM, Kyle Ellrott <kellrott@soe.ucsc.edu<mailto:kellrott@soe.ucsc.edu>> wrote:
I think its best to look to existing standard rather then try to make your own. Of course small additions would need to be added to make it valuable for the Spark community, like a method similar to Gremlin's 'table' function, that produces an RDD instead.
But there may be a lot of extra code and data structures that would need to be added to make it work, and those may not be directly applicable to all GraphX users. I think it would be best run as a separate module/project that builds directly on top of GraphX.

Kyle



On Thu, Nov 6, 2014 at 4:39 PM, York, Brennon <Brennon.York@capitalone.com<mailto:Brennon.York@capitalone.com>> wrote:
My personal 2c is that, since GraphX is just beginning to provide a full featured graph API, I think it would be better to align with the TinkerPop group rather than roll our own. In my mind the benefits out way the detriments as follows:

Benefits:
* GraphX gains the ability to become another core tenant within the TinkerPop community allowing a more diverse group of users into the Spark ecosystem.
* TinkerPop can continue to maintain and own a solid / feature-rich graph API that has already been accepted by a wide audience, relieving the pressure of ¡°one off¡± API additions from the GraphX team.
* GraphX can demonstrate its ability to be a key player in the GraphDB space sitting inline with other major distributions (Neo4j, Titan, etc.).
* Allows for the abstract graph traversal logic (query API) to be owned and maintained by a group already proven on the topic.

Drawbacks:
* GraphX doesn¡¯t own the API for its graph query capability. This could be seen as good or bad, but it might make GraphX-specific implementation additions more tricky (possibly). Also, GraphX will need to maintain the features described within the TinkerPop API as that might change in the future.

From: Kushal Datta <kushal.datta@gmail.com<mailto:kushal.datta@gmail.com>>
Date: Thursday, November 6, 2014 at 4:00 PM
To: ""York, Brennon"" <brennon.york@capitalone.com<mailto:brennon.york@capitalone.com>>
Cc: Kyle Ellrott <kellrott@soe.ucsc.edu<mailto:kellrott@soe.ucsc.edu>>, Rey>, ""dev@spark.apache.org<mailto:dev@spark.apache.org>"" <dev@spark.apache.org<mailto:dev@spark.apache.org>>, Matthias Broecheler <matthias@thinkaurelius.com<mailto:matthias@thinkaurelius.com>>

Subject: Re: Implementing TinkerPop on top of GraphX

Before we dive into the implementation details, what are the high level thoughts on Gremlin/GraphX? Scala already provides the procedural way to query graphs in GraphX today. So, today I can run g.vertices().filter().join() queries as OLAP in GraphX just like Tinkerpop3 Gremlin, of course sans the useful operators that Gremlin offers such as outE, inE, loop, as, dedup, etc. In that case is mapping Gremlin operators to GraphX api's a better approach or should we extend the existing set of transformations/actions that GraphX already offers with the useful operators from Gremlin? For example, we add as(), loop() and dedup() methods in VertexRDD and EdgeRDD.

Either way we get a desperately needed graph query interface in GraphX.

On Thu, Nov 6, 2014 at 3:25 PM, York, Brennon <Brennon.York@capitalone.com<mailto:Brennon.York@capitalone.com>> wrote:
This was my thought exactly with the TinkerPop3 release. Looks like, to move this forward, we¡¯d need to implement gremlin-core per <http://www.tinkerpop.com/docs/3.0.0.M1/#_implementing_gremlin_core>. The real question lies in whether GraphX can only support the OLTP functionality, or if we can bake into it the OLAP requirements as well. At a first glance I believe we could create an entire OLAP system. If so, I believe we could do this in a set of parallel subtasks, those being the implementation of each of the individual API¡¯s (Structure, Process, and, if OLAP, GraphComputer) necessary for gremlin-core. Thoughts?


From: Kyle Ellrott <kellrott@soe.ucsc.edu<mailto:kellrott@soe.ucsc.edu>>
Date: Thursday, November 6, 2014 at 12:10 PM
To: Kushal Datta <kushal.datta@gmail.com<mailto:kushal.datta@gmail.com>>
Cc: Reynold Xin <rxin@databricks.com<mailto:rxin@databricks.com>>, ""York, Brennon"" <brennon.york@capitalone.com<mailto:brennon.york@capitalone.com>>, ""dev@spark.apache.org<mailto:dev@spark.apache.org>"" <dev@spark.apache.org<mailto:dev@spark.apache.org>>, Matthias Broecheler <matthias@thinkaurelius.com<mailto:matthias@thinkaurelius.com>>
Subject: Re: Implementing TinkerPop on top of GraphX

I still have to dig into the Tinkerpop3 internals (I started my work long before it had been released), but I can say that to get the Tinerpop2 Gremlin pipeline to work in the GraphX was a bit of a hack. The whole Tinkerpop2 Gremlin design was based around streaming pipes of data, rather then large distributed map-reduce operations. I had to hack the pipes to aggregate all of the data and pass a single object wrapping the GraphX RDDs down the pipes in a single go, rather then streaming it element by element.
Just based on their description, Tinkerpop3 may be more amenable to the Spark platform.

Kyle


On Thu, Nov 6, 2014 at 11:55 AM, Kushal Datta <kushal.datta@gmail.com<mailto:kushal.datta@gmail.com>> wrote:
What do you guys think about the Tinkerpop3 Gremlin interface?
It has MapReduce to run Gremlin operators in a distributed manner and Giraph to execute vertex programs.

The Tinkpop3 is better suited for GraphX.

On Thu, Nov 6, 2014 at 11:48 AM, Kyle Ellrott <kellrott@soe.ucsc.edu<mailto:kellrott@soe.ucsc.edu>> wrote:
I've taken a crack at implementing the TinkerPop Blueprints API in GraphX (
https://github.com/kellrott/sparkgraph ). I've also implemented portions of
the Gremlin Search Language and a Parquet based graph store.
I've been working out finalize some code details and putting together
better code examples and documentation before I started telling people
about it.
But if you want to start looking at the code, I can answer any questions
you have. And if you would like to contribute, I would really appreciate
the help.

Kyle


On Thu, Nov 6, 2014 at 11:42 AM, Reynold Xin <rxin@databricks.com<> In the past we talked with Matthias and there were some discussions about
> this.
>
> On Thu, Nov 6, 2014 at 11:34 AM, York, Brennon <
> Brennon.York@capitalone.com<mailto:Brennon.York@capitalone.com>>
> wrote:
>
> > All, was wondering if there had been any discussion around this topic
> yet?
> > TinkerPop <https://github.com/tinkerpop> is a great abstraction for
> graph
> > databases and has been implemented across various graph database backends
> > / gaining traction. Has anyone thought about integrating the TinkerPop
> > framework with GraphX to enable GraphX as another backend? Not sure if
> > this has been brought up or not, but would certainly volunteer to
> > spearhead this effort if the community thinks it to be a good idea!
> >
> > As an aside, wasn©öt sure if this discussion should happen on the board
> > here or on JIRA, but a made a ticket as well for reference:
> > https://issues.apache.org/jira/browse/SPARK-4279
> >
> > ________________________________________________________
> >
> > The information contained in this e-mail is confidential and/or
> > proprietary to Capital One and/or its affiliates. The information
> > transmitted herewith is intended only for use by the individual or entity
> > to which it is addressed.  If the reader of this message is not the
> > intended recipient, you are hereby notified that any review,
> > retransmission, dissemination, distribution, copying or other use of, or
> > taking of any action in reliance upon this information is strictly
> > prohibited. If you have received this communication in error, please
> > contact the sender and delete the material from your computer.
> >
> >
> > ---------------------------------------------------------------------
> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org>
> > For additional commands, e-mail: dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>
> >
> >
>



________________________________

The information contained in this e-mail is confidential and/or proprietary to Capital One and/or its affiliates. The information transmitted herewith is intended only for use by the individual or entity to which it is addressed.  If the reader of this message is not the intended recipient, you are hereby notified that any review, retransmission, dissemination, distribution, copying or other use of, or taking of any action in reliance upon this information is strictly prohibited. If you have received this communication in error, please contact the sender and delete the material from your computer.


________________________________

The information contained in this e-mail is confidential and/or proprietary to Capital One and/or its affiliates. The information transmitted herewith is intended only for use by the individual or entity to which it is addressed.  If the reader of this message is not the intended recipient, you are hereby notified that any review, retransmission, dissemination, distribution, copying or other use of, or taking of any action in reliance upon this information is strictly prohibited. If you have received this communication in error, please contact the sender and delete the material from your computer.



________________________________________________________

The information contained in this e-mail is confidential and/or proprietary to Capital One and/or its affiliates. The information transmitted herewith is intended only for use by the individual or entity to which it is addressed.  If the reader of this message is not the intended recipient, you are hereby notified that any review, retransmission, dissemination, distribution, copying or other use of, or taking of any action in reliance upon this information is strictly prohibited. If you have received this communication in error, please contact the sender and delete the material from your computer.
"
Kushal Datta <kushal.datta@gmail.com>,"Fri, 7 Nov 2014 11:17:12 -0800",Re: Implementing TinkerPop on top of GraphX,Kyle Ellrott <kellrott@soe.ucsc.edu>,"I think if we are going to use GraphX as the query engine in Tinkerpop3,
then the Tinkerpop3 community is the right platform to further the
discussion.

The reason I asked the question on improving APIs in GraphX is because why
only Gremlin, any graph DSL can exploit the GraphX APIs. Cypher has some
good subgraph matching query interfaces which I believe can be distributed
using GraphX apis.

An edge ID is an internal attribute of the edge generated automatically,
mostly hidden from the user. That's why adding it as an edge property might
not be a good idea. There are several little differences like this. E.g. in
Tinkerpop3 Gremlin implementation for Giraph, only vertex programs are
executed in Giraph directly. The side-effect operators are mapped to
Map-Reduce functions. In the implementation we are talking about, all of
these operations can be done within GraphX. I will be interested to
co-develop the query engine.

@Reynold, I agree. And as I said earlier, the apis should be designed in
such a way that it can be used in any Graph DSL.

:

ue
e
st
e
s
,
a
he
ge
d
o
ject
way
ark
 the
am.
.).
d
his could
on
he
e
l
to
rpop3
as
tors
 of
.
 <
l. At
, I
he
ocess, and, if
m
rpop2
ack
ing
it
d
er
e
to
on the
on
--
tity
 or
ity
or
"
Jeniba Johnson <Jeniba.Johnson@lntinfotech.com>,"Fri, 7 Nov 2014 17:34:26 +0530",Bind exception while running FlumeEventCount,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

I have installed spark-1.1.0 and  apache flume 1.4 for running  streaming example FlumeEventCount. Previously the code was working fine. Now Iam facing with the below mentioned issues. My flume is running properly it is able to write the file.

The command I use is

bin/run-example org.apache.spark.examples.streaming.FlumeEventCount 172.29.17.178  65001


14/11/07 23:19:23 INFO receiver.ReceiverSupervisorImpl: Stopping receiver with message: Error starting receiver 0: org.jboss.netty.channel.ChannelException: Failed to bind to: /172.29.17.178:65001
14/11/07 23:19:23 INFO flume.FlumeReceiver: Flume receiver stopped
14/11/07 23:19:23 INFO receiver.ReceiverSupervisorImpl: Called receiver onStop
14/11/07 23:19:23 INFO receiver.ReceiverSupervisorImpl: Deregistering receiver 0
14/11/07 23:19:23 ERROR scheduler.ReceiverTracker: Deregistered receiver for stream 0: Error starting receiver 0 - org.jboss.netty.channel.ChannelException: Failed to bind to: /172.29.17.178:65001
        at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272)
        at org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:106)
        at org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:119)
        at org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:74)
        at org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:68)
        at org.apache.spark.streaming.flume.FlumeReceiver.initServer(FlumeInputDStream.scala:164)
        at org.apache.spark.streaming.flume.FlumeReceiver.onStart(FlumeInputDStream.scala:171)
        at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:121)
        at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:106)
        at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverLauncher$$anonfun$9.apply(ReceiverTracker.scala:264)
        at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverLauncher$$anonfun$9.apply(ReceiverTracker.scala:257)
        at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1121)
        at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1121)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
        at org.apache.spark.scheduler.Task.run(Task.scala:54)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
Caused by: java.net.BindException: Address already in use
        at sun.nio.ch.Net.bind0(Native Method)
        at sun.nio.ch.Net.bind(Net.java:344)
        at sun.nio.ch.Net.bind(Net.java:336)
        at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:199)
        at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
        at org.jboss.netty.channel.socket.nio.NioServerBoss$RegisterTask.run(NioServerBoss.java:193)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.processTaskQueue(AbstractNioSelector.java:366)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:290)
        at org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)
        ... 3 more

14/11/07 23:19:23 INFO receiver.ReceiverSupervisorImpl: Stopped receiver 0
14/11/07 23:19:23 INFO receiver.BlockGenerator: Stopping BlockGenerator
14/11/07 23:19:23 INFO util.RecurringTimer: Stopped timer for BlockGenerator after time 1415382563200
14/11/07 23:19:23 INFO receiver.BlockGenerator: Waiting for block pushing thread
14/11/07 23:19:23 INFO receiver.BlockGenerator: Pushing out the last 0 blocks
14/11/07 23:19:23 INFO receiver.BlockGenerator: Stopped block pushing thread
14/11/07 23:19:23 INFO receiver.BlockGenerator: Stopped BlockGenerator
14/11/07 23:19:23 INFO receiver.ReceiverSupervisorImpl: Waiting for executor stop is over
14/11/07 23:19:23 ERROR receiver.ReceiverSupervisorImpl: Stopped executor with error: org.jboss.netty.channel.ChannelException: Failed to bind to: /172.29.17.178:65001
14/11/07 23:19:23 ERROR executor.Executor: Exception in task 0.0 in stage 0.0 (TID 0)
org.jboss.netty.channel.ChannelException: Failed to bind to: /172.29.17.178:65001
        at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272)
        at org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:106)
        at org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:119)
        at org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:74)
        at org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:68)
        at org.apache.spark.streaming.flume.FlumeReceiver.initServer(FlumeInputDStream.scala:164)
        at org.apache.spark.streaming.flume.FlumeReceiver.onStart(FlumeInputDStream.scala:171)
        at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:121)
        at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:106)
        at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverLauncher$$anonfun$9.apply(ReceiverTracker.scala:264)
        at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverLauncher$$anonfun$9.apply(ReceiverTracker.scala:257)
        at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1121)
        at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1121)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
        at org.apache.spark.scheduler.Task.run(Task.scala:54)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
Caused by: java.net.BindException: Address already in use
        at sun.nio.ch.Net.bind0(Native Method)
        at sun.nio.ch.Net.bind(Net.java:344)
        at sun.nio.ch.Net.bind(Net.java:336)
        at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:199)
        at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
        at org.jboss.netty.channel.socket.nio.NioServerBoss$RegisterTask.run(NioServerBoss.java:193)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.processTaskQueue(AbstractNioSelector.java:366)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:290)
        at org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)
        ... 3 more
14/11/07 23:19:23 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, localhost): org.jboss.netty.channel.ChannelException: Failed to bind to: /172.29.17.178:65001
        org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272)
        org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:106)
        org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:119)
        org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:74)
        org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:68)
        org.apache.spark.streaming.flume.FlumeReceiver.initServer(FlumeInputDStream.scala:164)
        org.apache.spark.streaming.flume.FlumeReceiver.onStart(FlumeInputDStream.scala:171)
        org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:121)
        org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:106)
        org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverLauncher$$anonfun$9.apply(ReceiverTracker.scala:264)
        org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverLauncher$$anonfun$9.apply(ReceiverTracker.scala:257)
        org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1121)
        org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1121)
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
        org.apache.spark.scheduler.Task.run(Task.scala:54)
       org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:722)
14/11/07 23:19:23 ERROR scheduler.TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job
14/11/07 23:19:23 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
14/11/07 23:19:23 INFO scheduler.TaskSchedulerImpl: Cancelling stage 0
14/11/07 23:19:23 INFO scheduler.DAGScheduler: Failed to run runJob at ReceiverTracker.scala:275
Exception in thread ""Thread-28"" org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): org.jboss.netty.channel.ChannelException: Failed to bind to: /172.29.17.178:65001
        org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272)
        org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:106)
        org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:119)
        org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:74)
        org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:68)
        org.apache.spark.streaming.flume.FlumeReceiver.initServer(FlumeInputDStream.scala:164)
        org.apache.spark.streaming.flume.FlumeReceiver.onStart(FlumeInputDStream.scala:171)
        org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:121)
        org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:106)
        org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverLauncher$$anonfun$9.apply(ReceiverTracker.scala:264)
        org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverLauncher$$anonfun$9.apply(ReceiverTracker.scala:257)
        org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1121)
        org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1121)
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
        org.apache.spark.scheduler.Task.run(Task.scala:54)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:722)
Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1185)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1174)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1173)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1173)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:688)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:688)
        at scala.Option.foreach(Option.scala:236)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:688)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1391)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
        at akka.actor.ActorCell.invoke(ActorCell.scala:456)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
        at akka.dispatch.Mailbox.run(Mailbox.scala:219)
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
        at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
        at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
        at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
        at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

Regards,
Jeniba Johnson


________________________________
The contents of this e-mail and any attachment(s) may contain confidential or privileged information for the intended recipient(s). Unintended recipients are prohibited from taking action on the basis of information in this e-mail and using or disseminating the information, and must notify the sender and delete it from their system. L&T Infotech will not accept responsibility or liability for the accuracy or completeness of, or the presence of any virus or disabling code in this e-mail""
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 7 Nov 2014 18:05:27 -0500",Replacing Spark's native scheduler with Sparrow,dev <dev@spark.apache.org>,"I just watched Kay's talk from 2013 on Sparrow
<https://www.youtube.com/watch?v=ayjH_bG-RC0>. Is replacing Spark's native
scheduler with Sparrow still on the books?

The Sparrow repo <https://github.com/radlab/sparrow> hasn't been updated
recently, and I don't see any JIRA issues about it.

It would be good to at least have a JIRA issue to track progress on this if
it's a long-term goal.

Nick
"
Davies Liu <davies@databricks.com>,"Fri, 7 Nov 2014 15:18:38 -0800",Re: [VOTE] Designating maintainers for some Spark components,Vinod Kumar Vavilapalli <vinodkv@apache.org>,"-1 (not binding, +1 for maintainer, -1 for sign off)

Agree with Greg and Vinod. In the beginning, everything is better
(more efficient, more focus), but after some time, fighting begins.

Code style is the most hot topic to fight (we already saw it in so"
Kay Ousterhout <keo@eecs.berkeley.edu>,"Fri, 7 Nov 2014 16:42:16 -0800",Re: Replacing Spark's native scheduler with Sparrow,Nicholas Chammas <nicholas.chammas@gmail.com>,"Hi Nick,

This hasn't yet been directly supported by Spark because of a lack of
demand.  The last time I ran a throughput test on the default Spark
scheduler (~1 year ago, so this may have changed), it could launch
approximately 1500 tasks / second.  If, for example, you have a cluster of
100 machines, this means the scheduler can launch 150 tasks per machine per
second.  I don't know of any existing Spark clusters that have a large
enough number of machines or short enough tasks to justify the added
complexity of distributing the scheduler.  Eventually I hope to see Spark
used on much larger clusters, such that Sparrow will be necessary!

-Kay


"
Tathagata Das <tathagata.das1565@gmail.com>,"Fri, 7 Nov 2014 17:53:08 -0800",Re: [VOTE] Designating maintainers for some Spark components,Davies Liu <davies@databricks.com>,"+1 (binding)

I agree with the proposal that it just formalizes what we have been
doing till now, and will increase the efficiency and focus of the
review process.

To address Davies' concern, I agree coding style is often a hot topic
of contention. But t"
Davies Liu <davies@databricks.com>,"Fri, 7 Nov 2014 17:56:23 -0800",Re: [VOTE] Designating maintainers for some Spark components,Vinod Kumar Vavilapalli <vinodkv@apache.org>,"Sorry for my last email, I misunderstood the proposal here, all the
committer still have equal -1 to all the code changes.

Also, as mentioned in the proposal, the sign off only happens to
public API and architect, something like discussion about code style
things are still the same.

So, I'd revert my vote to +1. Sorry for this.

Davies


d to forward it to me (or another core API maintainer) to make sure we also approve
feedback
s patch
ward patches in these areas to certain other committers. If every committer had perfect oversight of the project, they could have also seen every patch to their component on their own, but this list ensures that they see it even if they somehow overlooked it.
elf, this is what I think you really need:
 is repeated often around here: Trust but verify.
fic component to a list that people can subscribe to
rs' over dev-lists or through tools. Like many projects do with review boards and other tools.
he point. But once set as a convention, they will all help you address your concerns with project scalability.
orming dictatorships. And contrary to what the proposal claims, this is neither scalable nor confirming to Apache governance rules.

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 7 Nov 2014 21:20:11 -0500",Re: Replacing Spark's native scheduler with Sparrow,Kay Ousterhout <keo@eecs.berkeley.edu>,"

Did you mean 15 tasks per machine per second here? Or alternatively, 10
machines?

I don't know of any existing Spark clusters that have a large enough number


Actually, this was the reason I took interest in Sparrow--specifically, the
idea of a Spark cluster handling many very short (<< 50 ms) tasks.

At the recent Spark Committer Night
<http://www.meetup.com/Spark-NYC/events/209271842/> in NYC, I asked Michael
if he thought that Spark SQL could eventually completely fill the need for
very low latency queries currently served by MPP databases like Redshift or
Vertica. If I recall correctly, he said that the main obstacle to that was
simply task startup time, which is on the order of 100 ms.

Is there interest in (or perhaps an existing initiative related to)
improving task startup times to the point where one could legitimately look
at Spark SQL as a low latency database that can serve many users or
applications at once? That would probably make a good use case for Sparrow,
no?

Nick
"
Kay Ousterhout <keo@eecs.berkeley.edu>,"Fri, 7 Nov 2014 18:35:04 -0800",Re: Replacing Spark's native scheduler with Sparrow,"Nicholas Chammas <nicholas.chammas@gmail.com>, Evan Sparks <sparks@cs.berkeley.edu>","
Yes -- sorry for the terrible math there!


Shorter tasks would indeed be a good use case for Sparrow, and was the
motivation behind the Sparrow work.  When evaluating Sparrow, we focused on
running SQL workloads where tasks were in the 50-100ms range (detailed in
the paper <http://people.csail.mit.edu/matei/papers/2013/sosp_sparrow.pdf>).

I know Evan, who I added here, has been looking at task startup times in
the context of ML workloads; this motivated some recent work (e.g.,
https://issues.apache.org/jira/browse/SPARK-3984) to improve metrics shown
in the UI to describe task launch overhead.  For jobs we've looked at, task
startup time was at most tens of milliseconds (I also remember this being
the case when we ran short tasks on Sparrow).  Decreasing this seems like
it would be widely beneficial, especially if there are cases where it's
more like 100ms, as Michael alluded.  Hopefully some of the improved UI
reporting will help to understand the degree to which this is (or is not)
an issue.  I'm not sure how much Evan is attempting to quantify the
overhead versus fix it -- so I'll let him chime in here.


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 7 Nov 2014 23:04:05 -0500",Re: Replacing Spark's native scheduler with Sparrow,Kay Ousterhout <keo@eecs.berkeley.edu>,"Sounds good. I'm looking forward to tracking improvements in this area.

Also, just to connect some more dots here, I just remembered that there is
currently an initiative to add an IndexedRDD
<https://issues.apache.org/jira/browse/SPARK-2365> interface. Some
interesting use cases mentioned there include (emphasis added):

To address these problems, we propose IndexedRDD, an efficient key-value


GraphX would be the first user of IndexedRDD, since it currently implements


Maybe some day we'll have Spark clusters directly serving up point lookups
or updates. I imagine the tasks running on clusters like that would be tiny
and would benefit from very low task startup times and scheduling latency.
Am I painting that picture correctly?

Anyway, thanks for explaining the current status of Sparrow.

Nick
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Fri, 7 Nov 2014 20:15:52 -0800",Re: Replacing Spark's native scheduler with Sparrow,Nicholas Chammas <nicholas.chammas@gmail.com>,"
Case for Tiny Tasks in Compute Clusters""
http://shivaram.org/publications/tinytasks-hotos13.pdf

"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sat, 8 Nov 2014 00:23:52 -0500",Re: Replacing Spark's native scheduler with Sparrow,Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Hmm, relevant quote from section 3.3:

newer frameworks like Spark [35] reduce the overhead to 5ms. To support
l
rks easily
Î¼s
 s task
m


So it looks like I misunderstood the current cost of task initialization.
It's already as low as 5ms (and not 100ms)?

Nick


is
ng
ps
y.
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Fri, 7 Nov 2014 21:43:00 -0800",Re: Replacing Spark's native scheduler with Sparrow,Nicholas Chammas <nicholas.chammas@gmail.com>,"I think Kay might be able to give a better answer. The most recent
benchmark I remember had the number at at somewhere between 8.6ms and
14.6ms depending on the Spark version (
https://github.com/apache/spark/pull/2030#issuecomment-52715181). Another
point to note is that this is the total time to run a null job, so this
includes scheduling + task launch + time to send back results etc.

Shivaram

m

ll
orks easily
0Î¼s
,
¼ s task
om
e
"
vaquar khan <vaquar.khan@gmail.com>,"Sat, 8 Nov 2014 11:29:43 +0530",Re: [VOTE] Designating maintainers for some Spark components,Davies Liu <davies@databricks.com>,"+1 (binding)

"
Kay Ousterhout <keo@eecs.berkeley.edu>,"Fri, 7 Nov 2014 22:22:52 -0800",Re: Replacing Spark's native scheduler with Sparrow,Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"I don't have much more info than what Shivaram said.  My sense is that,
over time, task launch overhead with Spark has slowly grown as Spark
supports more and more functionality.  However, I haven't seen it be as
high as the 100ms Michael quoted (maybe this was for jobs with tasks that
have much larger objects that take a long time to deserialize?).
Fortunately, the UI now quantifies this: if you click ""Show Additional
Metrics"", the scheduler delay (which basically represents the overhead of
shipping the task to the worker and getting the result back), the task
deserialization time, and the result serialization time all represent parts
of the task launch overhead.  So, you can use the UI to get a sense of what
this overhead is for the workload you're considering and whether it's worth
optimizing.

-Kay


g
all
works easily
10Î¼s
g,
¼ s task
rom
.
.
e
ue
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sat, 8 Nov 2014 01:37:11 -0500",Re: Replacing Spark's native scheduler with Sparrow,Kay Ousterhout <keo@eecs.berkeley.edu>,"Yeah, perhaps I misunderstood what Michael was saying. But thanks for
pointing out the relevant UI functionality.


ts
at
th
so
ng
d
call
tworks easily
 10Î¼s
ng,
¼ s task
from
.
a.
e
"
Mayur Rustagi <mayur.rustagi@gmail.com>,"Sat, 8 Nov 2014 12:48:21 +0530",Re: [MLlib] Contributing Algorithm for Outlier Detection,slcclimber <anant.asty@gmail.com>,"
What do you mean by vector datatype exactly?

Mayur Rustagi
Ph: +1 (760) 203 3257
http://www.sigmoidanalytics.com
@mayur_rustagi <https://twitter.com/mayur_rustagi>



n
le
he
e
ON
t]
OutlierWithAVFModel.scala
OutlierWithAVFModel.scala
Â·
OutlierWithAVFModel.scala
:
t I think
]""
=0>>
f
st
ing-Algorithm-for-Outlier-Detection-tp8880p9034.html
rvlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml
ing-Algorithm-for-Outlier-Detection-tp8880p9035.html
rvlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml
ing-Algorithm-for-Outlier-Detection-tp8880p9036.html
rvlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml
on
ing-Algorithm-for-Outlier-Detection-tp8880p9037.html
rvlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml
on
ing-Algorithm-for-Outlier-Detection-tp8880p9083.html
rvlet.jtp?macro=unsubscribe_by_code&node=8880&code=YW5hbnQuYXN0eUBnbWFpbC5jb218ODg4MHwxOTU2OTQ5NjMy
rvlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml
ing-Algorithm-for-Outlier-Detection-tp8880p9095.html
"
Sean Owen <sowen@cloudera.com>,"Sat, 8 Nov 2014 07:43:03 +0000","Should new YARN shuffle service work with ""yarn-alpha""?",dev <dev@spark.apache.org>,"I noticed that this doesn't compile:

mvn -Pyarn-alpha -Phadoop-0.23 -Dhadoop.version=0.23.7 -DskipTests clean package

[error] warning: [options] bootstrap class path not set in conjunction
with -source 1.6
[error] /Users/srowen/Documents/spark/network/yarn/src/main/java/org/apache/spark/network/yarn/YarnShuffleService.java:26:
error: cannot find symbol
[error] import org.apache.hadoop.yarn.server.api.AuxiliaryService;
[error]                                         ^
[error]   symbol:   class AuxiliaryService
[error]   location: package org.apache.hadoop.yarn.server.api
[error] /Users/srowen/Documents/spark/network/yarn/src/main/java/org/apache/spark/network/yarn/YarnShuffleService.java:27:
error: cannot find symbol
[error] import org.apache.hadoop.yarn.server.api.ApplicationInitializationContext;
[error]                                         ^
...

Should it work? if not shall I propose to enable the service only with -Pyarn?

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Fri, 7 Nov 2014 23:45:00 -0800","Re: Should new YARN shuffle service work with ""yarn-alpha""?",Sean Owen <sowen@cloudera.com>,"I bet it doesn't work. +1 on isolating it's inclusion to only the
newer YARN API's.

- Patrick


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Sat, 8 Nov 2014 07:50:12 +0000","Re: Should new YARN shuffle service work with ""yarn-alpha""?",Patrick Wendell <pwendell@gmail.com>,"Hm. Problem is, core depends directly on it:

[error] /Users/srowen/Documents/spark/core/src/main/scala/org/apache/spark/SecurityManager.scala:25:
object sasl is not a member of package org.apache.spark.network
[error] import org.apache.spark.network.sasl.SecretKeyHolder
[error]                                 ^
[error] /Users/srowen/Documents/spark/core/src/main/scala/org/apache/spark/SecurityManager.scala:147:
not found: type SecretKeyHolder
[error] private[spark] class SecurityManager(sparkConf: SparkConf)
extends Logging with SecretKeyHolder {
[error]
                 ^
[error] /Users/srowen/Documents/spark/core/src/main/scala/org/apache/spark/network/netty/NettyBlockTransferService.scala:29:
object RetryingBlockFetcher is not a member of package
org.apache.spark.network.shuffle
[error] import org.apache.spark.network.shuffle.{RetryingBlockFetcher,
[error]        ^
[error] /Users/srowen/Documents/spark/core/src/main/scala/org/apache/spark/deploy/worker/StandaloneWorkerShuffleService.scala:23:
object sasl is not a member of package org.apache.spark.network
[error] import org.apache.spark.network.sasl.SaslRpcHandler
[error]

...

[error] /Users/srowen/Documents/spark/core/src/main/scala/org/apache/spark/storage/BlockManager.scala:124:
too many arguments for constructor ExternalShuffleClient: (x$1:
org.apache.spark.network.util.TransportConf, x$2:
String)org.apache.spark.network.shuffle.ExternalShuffleClient
[error]     new
ExternalShuffleClient(SparkTransportConf.fromSparkConf(conf),
securityManager,
[error]     ^
[error] /Users/srowen/Documents/spark/core/src/main/scala/org/apache/spark/storage/BlockManager.scala:39:
object protocol is not a member of package
org.apache.spark.network.shuffle
[error] import org.apache.spark.network.shuffle.protocol.ExecutorShuffleInfo
[error]                                         ^
[error] /Users/srowen/Documents/spark/core/src/main/scala/org/apache/spark/storage/BlockManager.scala:214:
not found: type ExecutorShuffleInfo
[error]     val shuffleConfig = new ExecutorShuffleInfo(
[error]
...


More refactoring needed? Either to support YARN alpha as a separate
shuffle module, or sever this dependency?

Of course this goes away when yarn-alpha goes away too.



---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Sat, 8 Nov 2014 00:23:42 -0800","Re: Should new YARN shuffle service work with ""yarn-alpha""?",Sean Owen <sowen@cloudera.com>,"I think you might be conflating two things. The first error you posted
was because YARN didn't standardize the shuffle API in alpha versions
so our spark-network-yarn module won't compile. We should just disable
that module if yarn alpha is used. spark-network-yarn is a leaf in the
intra-module dependency graph, and core doesn't depend on it.

This second error is something else. Maybe you are excluding
network-shuffle instead of spark-network-yarn?




---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sat, 8 Nov 2014 03:38:16 -0500",Re: EC2 clusters ready in launch time + 30 seconds,David Rowe <davidrowe@gmail.com>,"I've posted
<https://issues.apache.org/jira/browse/SPARK-3821?focusedCommentId=14203280&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14203280>
an initial proposal and implementation of using Packer to automate
generating Spark AMIs to SPARK-3821
<https://issues.apache.org/jira/browse/SPARK-3821>.


"
Sean Owen <sowen@cloudera.com>,"Sat, 8 Nov 2014 08:47:18 +0000","Re: Should new YARN shuffle service work with ""yarn-alpha""?",Patrick Wendell <pwendell@gmail.com>,"Oops, that was my mistake. I moved network/shuffle into yarn, when
it's just that network/yarn should be removed from yarn-alpha. That
makes yarn-alpha work. I'll run tests and open a quick JIRA / PR for
the change.


---------------------------------------------------------------------


"
Manu Kaul <manohar.kaul@gmail.com>,"Sat, 8 Nov 2014 10:37:01 +0100",MLlib related query,dev@spark.apache.org,"Hi All,
I would like to contribute code to the MLlib library with some other ML
algorithms, but I was
wondering if there were any research papers that led to the development of
these libraries
using Breeze? I see papers for Apache Spark, but not for MLlib.

Thanks,
Manu

-- 

The greater danger for most of us lies not in setting our aim too high and
falling short; but in setting our aim too low, and achieving our mark.
- Michelangelo
"
Patrick Wendell <pwendell@gmail.com>,"Sat, 8 Nov 2014 09:59:54 -0800","Re: Should new YARN shuffle service work with ""yarn-alpha""?",Sean Owen <sowen@cloudera.com>,"Great - I think that should work, but if there are any issues we can
definitely fix them up.


---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Sat, 8 Nov 2014 10:26:51 -0800",Re: Replacing Spark's native scheduler with Sparrow,Kay Ousterhout <keo@eecs.berkeley.edu>,"
I was thinking more about the average end-to-end latency for launching a
query that has 100s of partitions. Its also quite possible that SQLs task
launch overhead is higher since we have never profiled how much is getting
pulled into the closures.
"
Sandy Ryza <sandy.ryza@cloudera.com>,"Sat, 8 Nov 2014 15:07:04 -0800",Re: proposal / discuss: multiple Serializers within a SparkContext?,Matei Zaharia <matei.zaharia@gmail.com>,"Ah awesome.  Passing customer serializers when persisting an RDD is exactly
one of the things I was thinking of.

-Sandy


"
Matei Zaharia <matei.zaharia@gmail.com>,"Sat, 8 Nov 2014 19:28:20 -0800",[RESULT] [VOTE] Designating maintainers for some Spark components,dev <dev@spark.apache.org>,"Thanks everyone for voting on this. With all of the PMC votes being for, the vote passes, but there were some concerns that I wanted to address for everyone who brought them up, as well as in the wording we will use for this policy.

First, like every Apache project, Spark follows the Apache voting process (http://www.apache.org/foundation/voting.html), wherein all code changes are done by consensus. This means that any PMC member can block a code change on technical grounds, and thus that there is consensus when something goes in. It's absolutely true that every PMC member is responsible for the whole codebase, as Greg said (not least due to legal reasons, e.g. making sure it complies to licensing rules), and this idea will not change that. To make this clear, I will include that in the wording on the project page, to make sure new committers and other community members are all aware of it.

What the maintainer model does, instead, is to change the review process, by having a required review from some people on some types of code changes (assuming those people respond in time). Projects can have their own diverse review processes (e.g. some do commit-then-review and others do review-then-commit, some point people to specific reviewers, etc). This kind of process seems useful to try (and to refine) as the project grows. We will of course evaluate how it goes and respond to any problems.

So to summarize,

- Every committer is responsible for, and more than welcome to review and vote on, every code change. In fact all community members are welcome to do this, and lots are doing it.
- Everyone has the same voting rights on these code changes (namely consensus as described at http://www.apache.org/foundation/voting.html)
- Committers will be asked to run patches that are making architectural and API changes by the maintainers before merging.

In practice, none of this matters too much because we are not exactly a hot-well of discord ;), and even in the case of discord, the point of the ASF voting process is to create consensus. The goal is just to have a better structure for reviewing and minimize the chance of errors.

Here is a tally of the votes:

Binding votes (from PMC): 17 +1, no 0 or -1

Matei Zaharia
Michael Armbrust
Reynold Xin
Patrick Wendell
Andrew Or
Prashant Sharma
Mark Hamstra
Xiangrui Meng
Ankur Dave
Imran Rashid
Jason Dai
Tom Graves
Sean McNamara
Nick Pentreath
Josh Rosen
Kay Ousterhout
Tathagata Das

Non-binding votes: 18 +1, one +0, one -1

+1:
Nan Zhu
Nicholas Chammas
Denny Lee
Cheng Lian
Timothy Chen
Jeremy Freeman
Cheng Hao
Jackylk Likun
Kousuke Saruta
Reza Zadeh
Xuefeng Wu
Witgo
Manoj Babu
Ravindra Pesala
Liquan Pei
Kushal Datta
Davies Liu
Vaquar Khan

+0: Corey Nolet

-1: Greg Stein

I'll send another email when I have a more detailed writeup of this on the website.

Matei
---------------------------------------------------------------------


"
Tathagata Das <tathagata.das1565@gmail.com>,"Sat, 8 Nov 2014 22:51:36 -0800",Re: Replacing Spark's native scheduler with Sparrow,Michael Armbrust <michael@databricks.com>,"Let me chime in on the discussion as well. Spark Streaming is another
usecase where the scheduler's task-launching throughput and
task-latency can limit the batch interval and the overall latencies
achievable by Spark Streaming. Lets say we want to do batches of 20 ms
(for achieve end-to-end latencies < 50ms) with 100 receivers. If each
receiver chunks received data into 10 ms blocks (required for making
batches every 10 ms), then launches tasks to process those blocks, it
means 100 blocks / second X 100 receivers = 10000 tasks per seconds.
This causes a scalability vs. latency tradeoff - if your limit is 1000
tasks per second (simplifying from 1500), you could either configure
it to use 100 receivers at 100 ms batches (10 blocks/sec), or 1000
receivers at 1 second batches.

Note that I did the calculation without considering processing power
of the worker nodes, task closure size, workload characteristics, etc
and other bottlenecks in the system. This calculation therefore does
not reflect the current performance limits of Spark Streaming, which
are most likely much lower than above due to other overheads. However,
as we optimize things further, I know that the scheduler is going to
be one of the biggest hurdles, and only out-of-the-box approaches like
Sparrow can solve it.

TD


---------------------------------------------------------------------


"
Jeniba Johnson <Jeniba.Johnson@lntinfotech.com>,"Mon, 10 Nov 2014 14:07:48 +0530",Bind exception while running FlumeEventCount,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

I have installed spark-1.1.0 and  apache flume 1.4 for running  streaming example FlumeEventCount. Previously the code was working fine. Now Iam facing with the below mentioned issues. My flume is running properly it is able to write the file.

The command I use is

bin/run-example org.apache.spark.examples.streaming.FlumeEventCount 172.29.17.178  65001


14/11/07 23:19:23 INFO receiver.ReceiverSupervisorImpl: Stopping receiver with message: Error starting receiver 0: org.jboss.netty.channel.ChannelException: Failed to bind to: /172.29.17.178:65001
14/11/07 23:19:23 INFO flume.FlumeReceiver: Flume receiver stopped
14/11/07 23:19:23 INFO receiver.ReceiverSupervisorImpl: Called receiver onStop
14/11/07 23:19:23 INFO receiver.ReceiverSupervisorImpl: Deregistering receiver 0
14/11/07 23:19:23 ERROR scheduler.ReceiverTracker: Deregistered receiver for stream 0: Error starting receiver 0 - org.jboss.netty.channel.ChannelException: Failed to bind to: /172.29.17.178:65001
        at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272)
        at org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:106)
        at org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:119)
        at org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:74)
        at org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:68)
        at org.apache.spark.streaming.flume.FlumeReceiver.initServer(FlumeInputDStream.scala:164)
        at org.apache.spark.streaming.flume.FlumeReceiver.onStart(FlumeInputDStream.scala:171)
        at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:121)
        at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:106)
        at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverLauncher$$anonfun$9.apply(ReceiverTracker.scala:264)
        at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverLauncher$$anonfun$9.apply(ReceiverTracker.scala:257)
        at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1121)
        at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1121)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
        at org.apache.spark.scheduler.Task.run(Task.scala:54)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
Caused by: java.net.BindException: Address already in use
        at sun.nio.ch.Net.bind0(Native Method)
        at sun.nio.ch.Net.bind(Net.java:344)
        at sun.nio.ch.Net.bind(Net.java:336)
        at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:199)
        at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
        at org.jboss.netty.channel.socket.nio.NioServerBoss$RegisterTask.run(NioServerBoss.java:193)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.processTaskQueue(AbstractNioSelector.java:366)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:290)
        at org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)
        ... 3 more

14/11/07 23:19:23 INFO receiver.ReceiverSupervisorImpl: Stopped receiver 0
14/11/07 23:19:23 INFO receiver.BlockGenerator: Stopping BlockGenerator
14/11/07 23:19:23 INFO util.RecurringTimer: Stopped timer for BlockGenerator after time 1415382563200
14/11/07 23:19:23 INFO receiver.BlockGenerator: Waiting for block pushing thread
14/11/07 23:19:23 INFO receiver.BlockGenerator: Pushing out the last 0 blocks
14/11/07 23:19:23 INFO receiver.BlockGenerator: Stopped block pushing thread
14/11/07 23:19:23 INFO receiver.BlockGenerator: Stopped BlockGenerator
14/11/07 23:19:23 INFO receiver.ReceiverSupervisorImpl: Waiting for executor stop is over
14/11/07 23:19:23 ERROR receiver.ReceiverSupervisorImpl: Stopped executor with error: org.jboss.netty.channel.ChannelException: Failed to bind to: /172.29.17.178:65001
14/11/07 23:19:23 ERROR executor.Executor: Exception in task 0.0 in stage 0.0 (TID 0)
org.jboss.netty.channel.ChannelException: Failed to bind to: /172.29.17.178:65001
        at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272)
        at org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:106)
        at org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:119)
        at org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:74)
        at org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:68)
        at org.apache.spark.streaming.flume.FlumeReceiver.initServer(FlumeInputDStream.scala:164)
        at org.apache.spark.streaming.flume.FlumeReceiver.onStart(FlumeInputDStream.scala:171)
        at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:121)
        at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:106)
        at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverLauncher$$anonfun$9.apply(ReceiverTracker.scala:264)
        at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverLauncher$$anonfun$9.apply(ReceiverTracker.scala:257)
        at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1121)
        at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1121)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
        at org.apache.spark.scheduler.Task.run(Task.scala:54)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
Caused by: java.net.BindException: Address already in use
        at sun.nio.ch.Net.bind0(Native Method)
        at sun.nio.ch.Net.bind(Net.java:344)
        at sun.nio.ch.Net.bind(Net.java:336)
        at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:199)
        at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
        at org.jboss.netty.channel.socket.nio.NioServerBoss$RegisterTask.run(NioServerBoss.java:193)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.processTaskQueue(AbstractNioSelector.java:366)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:290)
        at org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)
        ... 3 more
14/11/07 23:19:23 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, localhost): org.jboss.netty.channel.ChannelException: Failed to bind to: /172.29.17.178:65001
        org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272)
        org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:106)
        org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:119)
        org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:74)
        org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:68)
        org.apache.spark.streaming.flume.FlumeReceiver.initServer(FlumeInputDStream.scala:164)
        org.apache.spark.streaming.flume.FlumeReceiver.onStart(FlumeInputDStream.scala:171)
        org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:121)
        org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:106)
        org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverLauncher$$anonfun$9.apply(ReceiverTracker.scala:264)
        org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverLauncher$$anonfun$9.apply(ReceiverTracker.scala:257)
        org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1121)
        org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1121)
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
        org.apache.spark.scheduler.Task.run(Task.scala:54)
       org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:722)
14/11/07 23:19:23 ERROR scheduler.TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job
14/11/07 23:19:23 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
14/11/07 23:19:23 INFO scheduler.TaskSchedulerImpl: Cancelling stage 0
14/11/07 23:19:23 INFO scheduler.DAGScheduler: Failed to run runJob at ReceiverTracker.scala:275
Exception in thread ""Thread-28"" org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): org.jboss.netty.channel.ChannelException: Failed to bind to: /172.29.17.178:65001
        org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272)
        org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:106)
        org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:119)
        org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:74)
        org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:68)
        org.apache.spark.streaming.flume.FlumeReceiver.initServer(FlumeInputDStream.scala:164)
        org.apache.spark.streaming.flume.FlumeReceiver.onStart(FlumeInputDStream.scala:171)
        org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:121)
        org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:106)
        org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverLauncher$$anonfun$9.apply(ReceiverTracker.scala:264)
        org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverLauncher$$anonfun$9.apply(ReceiverTracker.scala:257)
        org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1121)
        org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1121)
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
        org.apache.spark.scheduler.Task.run(Task.scala:54)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:722)
Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1185)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1174)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1173)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1173)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:688)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:688)
        at scala.Option.foreach(Option.scala:236)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:688)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1391)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
        at akka.actor.ActorCell.invoke(ActorCell.scala:456)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
        at akka.dispatch.Mailbox.run(Mailbox.scala:219)
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
        at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
        at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
        at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
        at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)


Regards,
Jeniba Johnson


________________________________
The contents of this e-mail and any attachment(s) may contain confidential or privileged information for the intended recipient(s). Unintended recipients are prohibited from taking action on the basis of information in this e-mail and using or disseminating the information, and must notify the sender and delete it from their system. L&T Infotech will not accept responsibility or liability for the accuracy or completeness of, or the presence of any virus or disabling code in this e-mail""
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 10 Nov 2014 12:56:28 -0500",Re: Replacing Spark's native scheduler with Sparrow,Tathagata Das <tathagata.das1565@gmail.com>,"

This raises an interesting question, TD.

Do we have a benchmark for Spark Streaming that tests it at the extreme for
some key metric, perhaps processed messages per second per node? Something
that would press Spark's ability to process tasks quickly enough.

Given such a benchmark, it would probably be interesting to see how -- if
at all -- Sparrow has an impact on Spark Streaming's performance.

Nick
"
Tathagata Das <tathagata.das1565@gmail.com>,"Mon, 10 Nov 2014 12:58:57 -0800",Re: Replacing Spark's native scheduler with Sparrow,Nicholas Chammas <nicholas.chammas@gmail.com>,"Too bad Nick, I dont have anything immediately ready that tests Spark
Streaming with those extreme settings. :)


---------------------------------------------------------------------


"
Sadhan Sood <sadhan.sood@gmail.com>,"Mon, 10 Nov 2014 16:42:29 -0500",getting exception when trying to build spark from master,dev@spark.apache.org,"Getting an exception while trying to build spark in spark-core:

[ERROR]

     while compiling:
/Users/dev/tellapart_spark/core/src/main/scala/org/apache/spark/ui/JettyUtils.scala

        during phase: typer

     library version: version 2.10.4

    compiler version: version 2.10.4

  reconstructed args: -deprecation -feature -classpath


  last tree to typer: Ident(enumDispatcher)

              symbol: value enumDispatcher (flags: <triedcooking>)

   symbol definition: val enumDispatcher:
java.util.EnumSet[javax.servlet.DispatcherType]

                 tpe: java.util.EnumSet[javax.servlet.DispatcherType]

       symbol owners: value enumDispatcher -> value $anonfun -> method
addFilters -> object JettyUtils -> package ui

      context owners: value $anonfun -> value $anonfun -> method addFilters
-> object JettyUtils -> package ui


== Enclosing template or block ==


Block(

  ValDef( // val filters: Array[String]

    <triedcooking>

    ""filters""

    AppliedTypeTree(

      ""Array""

      ""String""

    )

    Apply(

      conf.get(""spark.ui.filters"", """").split(',').""map""

      Function( // val $anonfun: <notype>, tree.tpe=String => String

        ValDef( // x$1: String

          <param> <synthetic> <triedcooking>

          ""x$1""

          <tpt> // tree.tpe=String

          <empty>

        )

        Apply( // def trim(): String in class String, tree.tpe=String

          ""x$1"".""trim"" // def trim(): String in class String,
tree.tpe=()String

          Nil

        )

      )

    )

  )

  Apply(

    ""filters"".""foreach""

    Match(

      <empty>

      CaseDef(

        Bind( // val filter: String

          ""filter""

          Typed(

            ""_"" // tree.tpe=String

            ""String""

          )

        )

        If(

          ""filter"".""isEmpty"".""unary_$bang""

          Block(

            // 7 statements

            Apply(

              ""logInfo""

              Apply( // final def +(x$1: Any): String in class String,
tree.tpe=String

                ""Adding filter: "".""$plus"" // final def +(x$1: Any): String
in class String, tree.tpe=(x$1: Any)String

                ""filter"" // val filter: String, tree.tpe=String

              )

            )

            ValDef( // val holder: org.eclipse.jetty.servlet.FilterHolder

              <triedcooking>

              ""holder""

              ""FilterHolder""

              Apply(

                new FilterHolder.""<init>""

                Nil

              )

            )

            Apply( // def setClassName(x$1: String): Unit in class Holder,
tree.tpe=Unit

              ""holder"".""setClassName"" // def setClassName(x$1: String):
Unit in class Holder, tree.tpe=(x$1: String)Unit

              ""filter"" // val filter: String, tree.tpe=String

            )

            Apply(

              conf.get(""spark."".+(filter).+("".params""),
"""").split(',').map(((x$2: String) => x$2.trim())).""toSet"".""foreach""

              Function( // val $anonfun: <notype>

                ValDef( // param: String

                  <param> <triedcooking>

                  ""param""

                  ""String""

                  <empty>

                )

                If(

                  ""param"".""isEmpty"".""unary_$bang""

                  Block(

                    ValDef( // val parts: Array[String]

                      <triedcooking>

                      ""parts""

                      <tpt> // tree.tpe=Array[String]

                      Apply( // def split(x$1: String): Array[String] in
class String, tree.tpe=Array[String]

                        ""param"".""split"" // def split(x$1: String):
Array[String] in class String, tree.tpe=(x$1: String)Array[String]

                        ""=""

                      )

                    )

                    If(

                      Apply( // def ==(x: Int): Boolean in class Int,
tree.tpe=Boolean

                        ""parts"".""length"".""$eq$eq"" // def ==(x: Int):
Boolean in class Int, tree.tpe=(x: Int)Boolean

                        2

                      )

                      Apply( // def setInitParameter(x$1: String,x$2:
String): Unit in class Holder

                        ""holder"".""setInitParameter"" // def
setInitParameter(x$1: String,x$2: String): Unit in class Holder,
tree.tpe=(x$1: String, x$2: String)Unit

                        // 2 arguments

                        Apply( // val parts: Array[String]

                          ""parts"" // val parts: Array[String],
tree.tpe=parts.type

                          0

                        )

                        Apply( // val parts: Array[String]

                          ""parts"" // val parts: Array[String],
tree.tpe=parts.type

                          1

                        )

                      )

                      ()

                    )

                  )

                  ()

                )

              )

            )

            ValDef( // val prefix: String

              <triedcooking>

              ""prefix""

              <tpt> // tree.tpe=String

              Apply(

                StringContext(""spark."", "".param."").""s""

                ""filter"" // val filter: String, tree.tpe=String

              )

            )

            Apply(

              conf.getAll.filter(<empty> match {

  case scala.Tuple2((k @ _), (v @ _)) =>
k.length().>(prefix.length()).&&(k.startsWith(prefix))

}).""foreach""

              Match(

                <empty>

                CaseDef(

                  Apply( // object Tuple2 in package scala

                    ""scala"".""Tuple2"" // object Tuple2 in package scala,
tree.tpe=Tuple2.type

                    // 2 arguments

                    Bind( // val k: String, tree.tpe=String

                      ""k""

                      ""_"" // tree.tpe=String

                    )

                    Bind( // val v: String, tree.tpe=String

                      ""v""

                      ""_"" // tree.tpe=String

                    )

                  )

                  Apply( // def setInitParameter(x$1: String,x$2: String):
Unit in class Holder, tree.tpe=Unit

                    ""holder"".""setInitParameter"" // def
setInitParameter(x$1: String,x$2: String): Unit in class Holder,
tree.tpe=(x$1: String, x$2: String)Unit

                    // 2 arguments

                    Apply( // def substring(x$1: Int): String in class
String, tree.tpe=String

                      ""k"".""substring"" // def substring(x$1: Int): String in
class String, tree.tpe=(x$1: Int)String

                      Apply( // def length(): Int in class String,
tree.tpe=Int

                        ""prefix"".""length"" // def length(): Int in class
String, tree.tpe=()Int

                        Nil

                      )

                    )

                    ""v"" // val v: String, tree.tpe=String

                  )

                )

              )

            )

            ValDef( // val enumDispatcher:
java.util.EnumSet[javax.servlet.DispatcherType]

              <triedcooking>

              ""enumDispatcher""

              <tpt> //
tree.tpe=java.util.EnumSet[javax.servlet.DispatcherType]

              Apply( // def of[E <: Enum[E]](x$1: E,x$2: E,x$3: E,x$4:
E,x$5: E): java.util.EnumSet[E] in object EnumSet

                ""java"".""util"".""EnumSet"".""of"" // def of[E <: Enum[E]](x$1:
E,x$2: E,x$3: E,x$4: E,x$5: E): java.util.EnumSet[E] in object EnumSet,
tree.tpe=[E <: Enum[E]](x$1: E, x$2: E, x$3: E, x$4: E, x$5:
E)java.util.EnumSet[E]

                // 5 arguments

                ""DispatcherType"".""ASYNC""

                ""DispatcherType"".""ERROR""

                ""DispatcherType"".""FORWARD""

                ""DispatcherType"".""INCLUDE""

                ""DispatcherType"".""REQUEST""

              )

            )

            Apply( // def foreach[U](f: A => U): Unit in trait IterableLike

              ""handlers"".""foreach"" // def foreach[U](f: A => U): Unit in
trait IterableLike, tree.tpe=[U](f:
org.eclipse.jetty.servlet.ServletContextHandler => U)Unit

              Match(

                <empty>

                CaseDef(

                  Bind( // val handler:
org.eclipse.jetty.servlet.ServletContextHandler,
tree.tpe=org.eclipse.jetty.servlet.ServletContextHandler

                    ""handler""

                    ""_"" //
tree.tpe=org.eclipse.jetty.servlet.ServletContextHandler

                  )

                  Apply( // val addFilter: (x$1: String, x$2: String, x$3:
Int)org.eclipse.jetty.servlet.FilterHolder <and> (x$1: Class[_ <:
javax.servlet.Filter], x$2: String, x$3:
Int)org.eclipse.jetty.servlet.FilterHolder <and> (x$1:
org.eclipse.jetty.servlet.FilterHolder, x$2: String, x$3: Int)Unit <and>
(x$1: String, x$2: String, x$3:
java.util.EnumSet[org.eclipse.jetty.server.DispatcherType])org.eclipse.jetty.servlet.FilterHolder
<and> (x$1: Class[_ <: javax.servlet.Filter], x$2: String, x$3:
java.util.EnumSet[org.eclipse.jetty.server.DispatcherType])org.eclipse.jetty.servlet.FilterHolder
<and> (x$1: org.eclipse.jetty.servlet.FilterHolder, x$2: String, x$3:
java.util.EnumSet[org.eclipse.jetty.server.DispatcherType])Unit in class
ServletContextHandler

                    ""handler"".""addFilter"" // val addFilter: (x$1: String,
x$2: String, x$3: Int)org.eclipse.jetty.servlet.FilterHolder <and> (x$1:
Class[_ <: javax.servlet.Filter], x$2: String, x$3:
Int)org.eclipse.jetty.servlet.FilterHolder <and> (x$1:
org.eclipse.jetty.servlet.FilterHolder, x$2: String, x$3: Int)Unit <and>
(x$1: String, x$2: String, x$3:
java.util.EnumSet[org.eclipse.jetty.server.DispatcherType])org.eclipse.jetty.servlet.FilterHolder
<and> (x$1: Class[_ <: javax.servlet.Filter], x$2: String, x$3:
java.util.EnumSet[org.eclipse.jetty.server.DispatcherType])org.eclipse.jetty.servlet.FilterHolder
<and> (x$1: org.eclipse.jetty.servlet.FilterHolder, x$2: String, x$3:
java.util.EnumSet[org.eclipse.jetty.server.DispatcherType])Unit in class
ServletContextHandler, tree.tpe=(x$1: String, x$2: String, x$3:
Int)org.eclipse.jetty.servlet.FilterHolder <and> (x$1: Class[_ <:
javax.servlet.Filter], x$2: String, x$3:
Int)org.eclipse.jetty.servlet.FilterHolder <and> (x$1:
org.eclipse.jetty.servlet.FilterHolder, x$2: String, x$3: Int)Unit <and>
(x$1: String, x$2: String, x$3:
java.util.EnumSet[org.eclipse.jetty.server.DispatcherType])org.eclipse.jetty.servlet.FilterHolder
<and> (x$1: Class[_ <: javax.servlet.Filter], x$2: String, x$3:
java.util.EnumSet[org.eclipse.jetty.server.DispatcherType])org.eclipse.jetty.servlet.FilterHolder
<and> (x$1: org.eclipse.jetty.servlet.FilterHolder, x$2: String, x$3:
java.util.EnumSet[org.eclipse.jetty.server.DispatcherType])Unit

                    // 3 arguments

                    ""holder"" // val holder:
org.eclipse.jetty.servlet.FilterHolder,
tree.tpe=org.eclipse.jetty.servlet.FilterHolder

                    ""/*""

                    ""enumDispatcher"" // val enumDispatcher:
java.util.EnumSet[javax.servlet.DispatcherType],
tree.tpe=java.util.EnumSet[javax.servlet.DispatcherType]

                  )

                )

              )

            )

          )

          ()

        )

      )

    )

  )

)


== Expanded type of tree ==


TypeRef(

  TypeSymbol(

    abstract class EnumSet[E <: Enum[E]] extends AbstractSet[E] with
Cloneable with Serializable



  )

  args = List(

    TypeRef(

      TypeSymbol(

        final sealed abstract class DispatcherType extends
Enum[javax.servlet.DispatcherType]



      )

    )

  )

)


uncaught exception during compilation: java.lang.AssertionError

Exception in thread ""main"" java.lang.AssertionError: assertion failed:
org.eclipse.jetty.server.DispatcherType

at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1212)

at
scala.reflect.internal.Types$ClassTypeRef$class.baseType(Types.scala:2186)

at scala.reflect.internal.Types$TypeRef$$anon$6.baseType(Types.scala:2544)

at scala.reflect.internal.Types$class.firstTry$1(Types.scala:6043)

at scala.reflect.internal.Types$class.isSubType2(Types.scala:6207)

at scala.reflect.internal.Types$class.isSubType(Types.scala:5816)

at scala.reflect.internal.SymbolTable.isSubType(SymbolTable.scala:13)

at scala.reflect.internal.Types$class.isSubArg$1(Types.scala:6005)

at scala.reflect.internal.Types$$anonfun$isSubArgs$2.apply(Types.scala:6007)

at scala.reflect.internal.Types$$anonfun$isSubArgs$2.apply(Types.scala:6007)

at
scala.reflect.internal.util.Collections$class.corresponds3(Collections.scala:23)

at scala.reflect.internal.SymbolTable.corresponds3(SymbolTable.scala:13)

at scala.reflect.internal.Types$class.isSubArgs(Types.scala:6007)

at scala.reflect.internal.SymbolTable.isSubArgs(SymbolTable.scala:13)

at scala.reflect.internal.Types$class.firstTry$1(Types.scala:6040)

at scala.reflect.internal.Types$class.isSubType2(Types.scala:6207)

at scala.reflect.internal.Types$class.isSubType(Types.scala:5816)

at scala.reflect.internal.SymbolTable.isSubType(SymbolTable.scala:13)

at scala.reflect.internal.Types$class.isSubType(Types.scala:5774)

at scala.reflect.internal.SymbolTable.isSubType(SymbolTable.scala:13)

at scala.reflect.internal.Types$class.isWeakSubType(Types.scala:6754)

at scala.reflect.internal.SymbolTable.isWeakSubType(SymbolTable.scala:13)

at scala.reflect.internal.Types$Type.weak_$less$colon$less(Types.scala:914)

at
scala.tools.nsc.typechecker.Infer$Inferencer.scala$tools$nsc$typechecker$Infer$Inferencer$$isCompatible(Infer.scala:425)

at
scala.tools.nsc.typechecker.Infer$Inferencer$$anonfun$isCompatibleArgs$1.apply(Infer.scala:428)

at
scala.tools.nsc.typechecker.Infer$Inferencer$$anonfun$isCompatibleArgs$1.apply(Infer.scala:428)

at scala.collection.LinearSeqLike$class.corresponds(LinearSeqLike.scala:76)

at scala.collection.immutable.List.corresponds(List.scala:84)

at
scala.tools.nsc.typechecker.Infer$Inferencer.isCompatibleArgs(Infer.scala:428)

at
scala.tools.nsc.typechecker.Infer$Inferencer.typesCompatible$1(Infer.scala:814)

at
scala.tools.nsc.typechecker.Infer$Inferencer.scala$tools$nsc$typechecker$Infer$Inferencer$$isApplicable(Infer.scala:833)

at
scala.tools.nsc.typechecker.Infer$Inferencer$$anonfun$inferMethodAlternative$1$$anonfun$38$$anonfun$apply$1.apply$mcZ$sp(Infer.scala:1655)

at
scala.tools.nsc.typechecker.Infer$Inferencer$$anonfun$inferMethodAlternative$1$$anonfun$38.apply(Infer.scala:1655)

at
scala.tools.nsc.typechecker.Infer$Inferencer$$anonfun$inferMethodAlternative$1$$anonfun$38.apply(Infer.scala:1653)

at
scala.collection.TraversableLike$$anonfun$filter$1.apply(TraversableLike.scala:264)

at scala.collection.immutable.List.foreach(List.scala:318)

at scala.collection.TraversableLike$class.filter(TraversableLike.scala:263)

at scala.collection.AbstractTraversable.filter(Traversable.scala:105)

at
scala.tools.nsc.typechecker.Infer$Inferencer$$anonfun$inferMethodAlternative$1.apply(Infer.scala:1653)

at
scala.tools.nsc.typechecker.Infer$Inferencer$$anonfun$inferMethodAlternative$1.apply(Infer.scala:1645)

at
scala.tools.nsc.typechecker.Infer$Inferencer$$anonfun$tryTwice$1.apply(Infer.scala:1700)

at
scala.tools.nsc.typechecker.Infer$Inferencer$$anonfun$tryTwice$1.apply(Infer.scala:1700)

at
scala.tools.nsc.typechecker.Contexts$Context.withImplicitsDisabled(Contexts.scala:220)

at scala.tools.nsc.typechecker.Infer$Inferencer.tryTwice(Infer.scala:1700)

at
scala.tools.nsc.typechecker.Infer$Inferencer.inferMethodAlternative(Infer.scala:1645)

at
scala.tools.nsc.typechecker.Typers$Typer.handleOverloaded$1(Typers.scala:3190)

at scala.tools.nsc.typechecker.Typers$Typer.doTypedApply(Typers.scala:3194)

at
scala.tools.nsc.typechecker.Typers$Typer$$anonfun$92.apply(Typers.scala:4568)

at
scala.tools.nsc.typechecker.Typers$Typer$$anonfun$92.apply(Typers.scala:4568)

at scala.tools.nsc.typechecker.Typers$Typer.silent(Typers.scala:727)

at
scala.tools.nsc.typechecker.Typers$Typer.tryTypedApply$1(Typers.scala:4568)

at
scala.tools.nsc.typechecker.Typers$Typer.normalTypedApply$1(Typers.scala:4626)

at scala.tools.nsc.typechecker.Typers$Typer.typedApply$1(Typers.scala:4659)

at scala.tools.nsc.typechecker.Typers$Typer.typed1(Typers.scala:5564)

at org.scalamacros.paradise.typechecker.Analyzer$$anon$1.org
$scalamacros$paradise$typechecker$Typers$ParadiseTyper$$super$typed1(Analyzer.scala:19)

at
org.scalamacros.paradise.typechecker.Typers$ParadiseTyper$class.typed1(Typers.scala:44)

at
org.scalamacros.paradise.typechecker.Analyzer$$anon$1.typed1(Analyzer.scala:19)

at scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5642)

at scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5721)

at scala.tools.nsc.typechecker.Typers$Typer.typedCase(Typers.scala:2473)

at
scala.tools.nsc.typechecker.Typers$Typer$$anonfun$typedCases$1.apply(Typers.scala:2503)

at
scala.tools.nsc.typechecker.Typers$Typer$$anonfun$typedCases$1.apply(Typers.scala:2502)

at scala.collection.immutable.List.loop$1(List.scala:170)

at scala.collection.immutable.List.mapConserve(List.scala:186)

at scala.tools.nsc.typechecker.Typers$Typer.typedCases(Typers.scala:2502)

at scala.tools.nsc.typechecker.Typers$Typer.typedMatch(Typers.scala:2515)

at
scala.tools.nsc.typechecker.Typers$Typer.typedVirtualizedMatch$1(Typers.scala:4363)

at scala.tools.nsc.typechecker.Typers$Typer.typed1(Typers.scala:5576)

at org.scalamacros.paradise.typechecker.Analyzer$$anon$1.org
$scalamacros$paradise$typechecker$Typers$ParadiseTyper$$super$typed1(Analyzer.scala:19)

at
org.scalamacros.paradise.typechecker.Typers$ParadiseTyper$class.typed1(Typers.scala:44)

at
org.scalamacros.paradise.typechecker.Analyzer$$anon$1.typed1(Analyzer.scala:19)

at scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5642)

at scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5721)

at
scala.tools.nsc.typechecker.Typers$Typer.scala$tools$nsc$typechecker$Typers$Typer$$typedFunction(Typers.scala:2853)

at
scala.tools.nsc.typechecker.Typers$Typer.typedFunction$1(Typers.scala:5556)

at scala.tools.nsc.typechecker.Typers$Typer.typed1(Typers.scala:5575)

at org.scalamacros.paradise.typechecker.Analyzer$$anon$1.org
$scalamacros$paradise$typechecker$Typers$ParadiseTyper$$super$typed1(Analyzer.scala:19)

at
org.scalamacros.paradise.typechecker.Typers$ParadiseTyper$class.typed1(Typers.scala:44)

at
org.scalamacros.paradise.typechecker.Analyzer$$anon$1.typed1(Analyzer.scala:19)

at
scala.tools.nsc.typechecker.Typers$Typer.typedVirtualizedMatch$1(Typers.scala:4360)

at scala.tools.nsc.typechecker.Typers$Typer.typed1(Typers.scala:5576)

at org.scalamacros.paradise.typechecker.Analyzer$$anon$1.org
$scalamacros$paradise$typechecker$Typers$ParadiseTyper$$super$typed1(Analyzer.scala:19)

at
org.scalamacros.paradise.typechecker.Typers$ParadiseTyper$class.typed1(Typers.scala:44)

at
org.scalamacros.paradise.typechecker.Analyzer$$anon$1.typed1(Analyzer.scala:19)

at scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5642)

at scala.tools.nsc.typechecker.Typers$Typer.typedArg(Typers.scala:3042)

at
scala.tools.nsc.typechecker.Typers$Typer.scala$tools$nsc$typechecker$Typers$Typer$$typedArgToPoly$1(Typers.scala:3418)

at
scala.tools.nsc.typechecker.Typers$Typer$$anonfun$70.apply(Typers.scala:3426)

at
scala.tools.nsc.typechecker.Typers$Typer$$anonfun$70.apply(Typers.scala:3426)

at scala.reflect.internal.util.Collections$class.map2(Collections.scala:51)

at scala.reflect.internal.SymbolTable.map2(SymbolTable.scala:13)

at
scala.tools.nsc.typechecker.Typers$Typer.handlePolymorphicCall$1(Typers.scala:3426)

at scala.tools.nsc.typechecker.Typers$Typer.doTypedApply(Typers.scala:3438)

at
scala.tools.nsc.typechecker.Typers$Typer.normalTypedApply$1(Typers.scala:4627)

at scala.tools.nsc.typechecker.Typers$Typer.typedApply$1(Typers.scala:4659)

at scala.tools.nsc.typechecker.Typers$Typer.typed1(Typers.scala:5564)

at org.scalamacros.paradise.typechecker.Analyzer$$anon$1.org
$scalamacros$paradise$typechecker$Typers$ParadiseTyper$$super$typed1(Analyzer.scala:19)

at
org.scalamacros.paradise.typechecker.Typers$ParadiseTyper$class.typed1(Typers.scala:44)

at
org.scalamacros.paradise.typechecker.Analyzer$$anon$1.typed1(Analyzer.scala:19)

at scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5642)

at scala.tools.nsc.typechecker.Typers$Typer.typedBlock(Typers.scala:2433)

at org.scalamacros.paradise.typechecker.Analyzer$$anon$1.org
$scalamacros$paradise$typechecker$Typers$ParadiseTyper$$super$typedBlock(Analyzer.scala:19)

at
org.scalamacros.paradise.typechecker.Typers$ParadiseTyper$class.typedBlock(Typers.scala:58)

at
org.scalamacros.paradise.typechecker.Analyzer$$anon$1.typedBlock(Analyzer.scala:19)

at scala.tools.nsc.typechecker.Typers$Typer.typed1(Typers.scala:5570)

at org.scalamacros.paradise.typechecker.Analyzer$$anon$1.org
$scalamacros$paradise$typechecker$Typers$ParadiseTyper$$super$typed1(Analyzer.scala:19)

at
org.scalamacros.paradise.typechecker.Typers$ParadiseTyper$class.typed1(Typers.scala:44)

at
org.scalamacros.paradise.typechecker.Analyzer$$anon$1.typed1(Analyzer.scala:19)

at scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5642)

at scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5721)

at scala.tools.nsc.typechecker.Typers$Typer.typedIf$1(Typers.scala:4315)

at scala.tools.nsc.typechecker.Typers$Typer.typed1(Typers.scala:5571)

at org.scalamacros.paradise.typechecker.Analyzer$$anon$1.org
$scalamacros$paradise$typechecker$Typers$ParadiseTyper$$super$typed1(Analyzer.scala:19)

at
org.scalamacros.paradise.typechecker.Typers$ParadiseTyper$class.typed1(Typers.scala:44)

at
org.scalamacros.paradise.typechecker.Analyzer$$anon$1.typed1(Analyzer.scala:19)

at scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5642)

at scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5721)

at scala.tools.nsc.typechecker.Typers$Typer.typedCase(Typers.scala:2473)

at
scala.tools.nsc.typechecker.Typers$Typer$$anonfun$typedCases$1.apply(Typers.scala:2503)

at
scala.tools.nsc.typechecker.Typers$Typer$$anonfun$typedCases$1.apply(Typers.scala:2502)

at scala.collection.immutable.List.loop$1(List.scala:170)

at scala.collection.immutable.List.mapConserve(List.scala:186)

at scala.tools.nsc.typechecker.Typers$Typer.typedCases(Typers.scala:2502)

at scala.tools.nsc.typechecker.Typers$Typer.typedMatch(Typers.scala:2515)

at
scala.tools.nsc.typechecker.Typers$Typer.typedVirtualizedMatch$1(Typers.scala:4363)

at scala.tools.nsc.typechecker.Typers$Typer.typed1(Typers.scala:5576)

at org.scalamacros.paradise.typechecker.Analyzer$$anon$1.org
$scalamacros$paradise$typechecker$Typers$ParadiseTyper$$super$typed1(Analyzer.scala:19)

at
org.scalamacros.paradise.typechecker.Typers$ParadiseTyper$class.typed1(Typers.scala:44)

at
org.scalamacros.paradise.typechecker.Analyzer$$anon$1.typed1(Analyzer.scala:19)

at scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5642)

at scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5721)

at
scala.tools.nsc.typechecker.Typers$Typer.scala$tools$nsc$typechecker$Typers$Typer$$typedFunction(Typers.scala:2853)

at
scala.tools.nsc.typechecker.Typers$Typer.typedFunction$1(Typers.scala:5556)

at scala.tools.nsc.typechecker.Typers$Typer.typed1(Typers.scala:5575)

at org.scalamacros.paradise.typechecker.Analyzer$$anon$1.org
$scalamacros$paradise$typechecker$Typers$ParadiseTyper$$super$typed1(Analyzer.scala:19)

at
org.scalamacros.paradise.typechecker.Typers$ParadiseTyper$class.typed1(Typers.scala:44)

at
org.scalamacros.paradise.typechecker.Analyzer$$anon$1.typed1(Analyzer.scala:19)

at
scala.tools.nsc.typechecker.Typers$Typer.typedVirtualizedMatch$1(Typers.scala:4360)

at scala.tools.nsc.typechecker.Typers$Typer.typed1(Typers.scala:5576)

at org.scalamacros.paradise.typechecker.Analyzer$$anon$1.org
$scalamacros$paradise$typechecker$Typers$ParadiseTyper$$super$typed1(Analyzer.scala:19)

at
org.scalamacros.paradise.typechecker.Typers$ParadiseTyper$class.typed1(Typers.scala:44)

at
org.scalamacros.paradise.typechecker.Analyzer$$anon$1.typed1(Analyzer.scala:19)

at scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5642)

at scala.tools.nsc.typechecker.Typers$Typer.typedArg(Typers.scala:3042)

at
scala.tools.nsc.typechecker.Typers$Typer.scala$tools$nsc$typechecker$Typers$Typer$$typedArgToPoly$1(Typers.scala:3418)

at
scala.tools.nsc.typechecker.Typers$Typer$$anonfun$70.apply(Typers.scala:3426)

at
scala.tools.nsc.typechecker.Typers$Typer$$anonfun$70.apply(Typers.scala:3426)

at scala.reflect.internal.util.Collections$class.map2(Collections.scala:51)

at scala.reflect.internal.SymbolTable.map2(SymbolTable.scala:13)

at
scala.tools.nsc.typechecker.Typers$Typer.handlePolymorphicCall$1(Typers.scala:3426)

at scala.tools.nsc.typechecker.Typers$Typer.doTypedApply(Typers.scala:3438)

at
scala.tools.nsc.typechecker.Typers$Typer.normalTypedApply$1(Typers.scala:4627)

at scala.tools.nsc.typechecker.Typers$Typer.typedApply$1(Typers.scala:4659)

at scala.tools.nsc.typechecker.Typers$Typer.typed1(Typers.scala:5564)

at org.scalamacros.paradise.typechecker.Analyzer$$anon$1.org
$scalamacros$paradise$typechecker$Typers$ParadiseTyper$$super$typed1(Analyzer.scala:19)

at
org.scalamacros.paradise.typechecker.Typers$ParadiseTyper$class.typed1(Typers.scala:44)

at
org.scalamacros.paradise.typechecker.Analyzer$$anon$1.typed1(Analyzer.scala:19)

at scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5642)

at scala.tools.nsc.typechecker.Typers$Typer.typedBlock(Typers.scala:2433)

at org.scalamacros.paradise.typechecker.Analyzer$$anon$1.org
$scalamacros$paradise$typechecker$Typers$ParadiseTyper$$super$typedBlock(Analyzer.scala:19)

at
org.scalamacros.paradise.typechecker.Typers$ParadiseTyper$class.typedBlock(Typers.scala:58)

at
org.scalamacros.paradise.typechecker.Analyzer$$anon$1.typedBlock(Analyzer.scala:19)

at scala.tools.nsc.typechecker.Typers$Typer.typed1(Typers.scala:5570)

at org.scalamacros.paradise.typechecker.Analyzer$$anon$1.org
$scalamacros$paradise$typechecker$Typers$ParadiseTyper$$super$typed1(Analyzer.scala:19)

at
org.scalamacros.paradise.typechecker.Typers$ParadiseTyper$class.typed1(Typers.scala:44)

at
org.scalamacros.paradise.typechecker.Analyzer$$anon$1.typed1(Analyzer.scala:19)

at scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5642)

at
scala.tools.nsc.typechecker.Typers$Typer.transformedOrTyped(Typers.scala:5845)

at scala.tools.nsc.typechecker.Typers$Typer.typedDefDef(Typers.scala:2257)

at scala.tools.nsc.typechecker.Typers$Typer.typed1(Typers.scala:5569)

at org.scalamacros.paradise.typechecker.Analyzer$$anon$1.org
$scalamacros$paradise$typechecker$Typers$ParadiseTyper$$super$typed1(Analyzer.scala:19)

at
org.scalamacros.paradise.typechecker.Typers$ParadiseTyper$class.typed1(Typers.scala:44)

at
org.scalamacros.paradise.typechecker.Analyzer$$anon$1.typed1(Analyzer.scala:19)

at scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5642)

at
scala.tools.nsc.typechecker.Typers$Typer.scala$tools$nsc$typechecker$Typers$Typer$$typedStat$1(Typers.scala:2928)

at
scala.tools.nsc.typechecker.Typers$Typer$$anonfun$61.apply(Typers.scala:3032)

at
scala.tools.nsc.typechecker.Typers$Typer$$anonfun$61.apply(Typers.scala:3032)

at scala.collection.immutable.List.loop$1(List.scala:170)

at scala.collection.immutable.List.mapConserve(List.scala:186)

at scala.tools.nsc.typechecker.Typers$Typer.typedStats(Typers.scala:3032)

at scala.tools.nsc.typechecker.Typers$Typer.typedTemplate(Typers.scala:1919)

at org.scalamacros.paradise.typechecker.Analyzer$$anon$1.org
$scalamacros$paradise$typechecker$Typers$ParadiseTyper$$super$typedTemplate(Analyzer.scala:19)

at
org.scalamacros.paradise.typechecker.Typers$ParadiseTyper$class.typedTemplate(Typers.scala:51)

at
org.scalamacros.paradise.typechecker.Analyzer$$anon$1.typedTemplate(Analyzer.scala:19)

at
scala.tools.nsc.typechecker.Typers$Typer.typedModuleDef(Typers.scala:1800)

at scala.tools.nsc.typechecker.Typers$Typer.typed1(Typers.scala:5584)

at org.scalamacros.paradise.typechecker.Analyzer$$anon$1.org
$scalamacros$paradise$typechecker$Typers$ParadiseTyper$$super$typed1(Analyzer.scala:19)

at
org.scalamacros.paradise.typechecker.Typers$ParadiseTyper$class.typed1(Typers.scala:44)

at
org.scalamacros.paradise.typechecker.Analyzer$$anon$1.typed1(Analyzer.scala:19)

at scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5642)

at
scala.tools.nsc.typechecker.Typers$Typer.scala$tools$nsc$typechecker$Typers$Typer$$typedStat$1(Typers.scala:2928)

at
scala.tools.nsc.typechecker.Typers$Typer$$anonfun$61.apply(Typers.scala:3032)

at
scala.tools.nsc.typechecker.Typers$Typer$$anonfun$61.apply(Typers.scala:3032)

at scala.collection.immutable.List.loop$1(List.scala:170)

at scala.collection.immutable.List.mapConserve(List.scala:186)

at scala.tools.nsc.typechecker.Typers$Typer.typedStats(Typers.scala:3032)

at
scala.tools.nsc.typechecker.Typers$Typer.typedPackageDef$1(Typers.scala:5301)

at scala.tools.nsc.typechecker.Typers$Typer.typed1(Typers.scala:5587)

at org.scalamacros.paradise.typechecker.Analyzer$$anon$1.org
$scalamacros$paradise$typechecker$Typers$ParadiseTyper$$super$typed1(Analyzer.scala:19)

at
org.scalamacros.paradise.typechecker.Typers$ParadiseTyper$class.typedPackageDef$1(Typers.scala:35)

at
org.scalamacros.paradise.typechecker.Typers$ParadiseTyper$class.typed1(Typers.scala:43)

at
org.scalamacros.paradise.typechecker.Analyzer$$anon$1.typed1(Analyzer.scala:19)

at scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5642)

at scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5704)

at
scala.tools.nsc.typechecker.Analyzer$typerFactory$$anon$3.apply(Analyzer.scala:99)

at scala.tools.nsc.Global$GlobalPhase.applyPhase(Global.scala:464)

at
scala.tools.nsc.typechecker.Analyzer$typerFactory$$anon$3$$anonfun$run$1.apply(Analyzer.scala:91)

at
scala.tools.nsc.typechecker.Analyzer$typerFactory$$anon$3$$anonfun$run$1.apply(Analyzer.scala:91)

at scala.collection.Iterator$class.foreach(Iterator.scala:727)

at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)

at
scala.tools.nsc.typechecker.Analyzer$typerFactory$$anon$3.run(Analyzer.scala:91)

at scala.tools.nsc.Global$Run.compileUnitsInternal(Global.scala:1583)

at scala.tools.nsc.Global$Run.compileUnits(Global.scala:1557)

at scala.tools.nsc.Global$Run.compileSources(Global.scala:1553)

at scala.tools.nsc.Global$Run.compile(Global.scala:1662)

at xsbt.CachedCompiler0.run(CompilerInterface.scala:123)

at xsbt.CachedCompiler0.run(CompilerInterface.scala:99)

at xsbt.CompilerInterface.run(CompilerInterface.scala:27)

at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

at java.lang.reflect.Method.invoke(Method.java:483)

at sbt.compiler.AnalyzingCompiler.call(AnalyzingCompiler.scala:102)

at sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala:48)

at sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala:41)

at
sbt.compiler.AggressiveCompile$$anonfun$3$$anonfun$compileScala$1$1.apply$mcV$sp(AggressiveCompile.scala:99)

at
sbt.compiler.AggressiveCompile$$anonfun$3$$anonfun$compileScala$1$1.apply(AggressiveCompile.scala:99)

at
sbt.compiler.AggressiveCompile$$anonfun$3$$anonfun$compileScala$1$1.apply(AggressiveCompile.scala:99)

at
sbt.compiler.AggressiveCompile.sbt$compiler$AggressiveCompile$$timed(AggressiveCompile.scala:166)

at
sbt.compiler.AggressiveCompile$$anonfun$3.compileScala$1(AggressiveCompile.scala:98)

at
sbt.compiler.AggressiveCompile$$anonfun$3.apply(AggressiveCompile.scala:143)

at
sbt.compiler.AggressiveCompile$$anonfun$3.apply(AggressiveCompile.scala:87)

at sbt.inc.IncrementalCompile$$anonfun$doCompile$1.apply(Compile.scala:39)

at sbt.inc.IncrementalCompile$$anonfun$doCompile$1.apply(Compile.scala:37)

at sbt.inc.IncrementalCommon.cycle(Incremental.scala:99)

at sbt.inc.Incremental$$anonfun$1.apply(Incremental.scala:38)

at sbt.inc.Incremental$$anonfun$1.apply(Incremental.scala:37)

at sbt.inc.Incremental$.manageClassfiles(Incremental.scala:65)

at sbt.inc.Incremental$.compile(Incremental.scala:37)

at sbt.inc.IncrementalCompile$.apply(Compile.scala:27)

at sbt.compiler.AggressiveCompile.compile2(AggressiveCompile.scala:157)

at sbt.compiler.AggressiveCompile.compile1(AggressiveCompile.scala:71)

at com.typesafe.zinc.Compiler.compile(Compiler.scala:184)

at com.typesafe.zinc.Compiler.compile(Compiler.scala:164)

at sbt_inc.SbtIncrementalCompiler.compile(SbtIncrementalCompiler.java:92)

at
scala_maven.ScalaCompilerSupport.incrementalCompile(ScalaCompilerSupport.java:303)

at scala_maven.ScalaCompilerSupport.compile(ScalaCompilerSupport.java:119)

at scala_maven.ScalaCompilerSupport.doExecute(ScalaCompilerSupport.java:9"
"
Josh Rosen <rosenville@gmail.com>,Mon"," 10 Nov 2014 13:45:32 -0800""",Re: getting exception when trying to build spark from master,Sadhan Sood <sadhan.sood@gmail.com>,"It looks like the Jenkins maven builds are broken, too.  Based on the
Jenkins logs, I think that this pull request may have broken things
(although I'm not sure why):

https://github.com/apache/spark/pull/3030#issuecomment-62436181


"
Cody Koeninger <cody@koeninger.org>,"Mon, 10 Nov 2014 16:05:30 -0600",Http client dependency conflict when using AWS,"""dev@spark.apache.org"" <dev@spark.apache.org>","I'm wondering why

https://issues.apache.org/jira/browse/SPARK-3638

only updated the version of http client for the kinesis-asl profile and
left the base dependencies unchanged.

Spark built without that profile still has the same

java.lang.NoSuchMethodError:
org.apache.http.impl.conn.DefaultClientConnectionOperator.<init>(Lorg/apache/http/conn/scheme/SchemeRegistry;Lorg/apache/http/conn/DnsResolver;)V


when using aws components in general, not just kinesis (e.g.
AmazonCloudWatchAsyncClient)


Re-Reading the ""Dependency Hell in Spark applications"" thread from
september didn't shed any light on the subject.  As noted in that thread,
userClassPathFirst doesn't help.


Is there a reason not to depend on an updated version of httpclient for all
spark builds, as opposed to just kinesis-asl?
"
Andrew Or <andrew@databricks.com>,"Mon, 10 Nov 2014 14:17:10 -0800",Spark 1.1.1 release,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi everyone,

I am the release manager for 1.1.1, and I am preparing to cut a release
tonight at midnight. 1.1.1 is a maintenance release which will ship several
important bug fixes to users of Spark 1.1. Many users are waiting for
these fixes so I would like to release it as soon as possible.

At this point, I believe we have already back ported all critical fixes
from master other than a few known ones. Below is a list of issues that
have been back ported into 1.1.1. If there are other critical fixes in the
master branch that are not in this list, please let me know and I will take
a look.

https://issues.apache.org/jira/browse/SPARK-3653?jql=project%20%3D%20SPARK%20AND%20fixVersion%20%3D%201.1.1%20AND%20fixVersion%20%3D%201.2.0

Best,
- Andrew
"
Andrew Or <andrew@databricks.com>,"Mon, 10 Nov 2014 14:18:40 -0800",Re: Spark 1.1.1 release,"""dev@spark.apache.org"" <dev@spark.apache.org>","(Tonight at midnight being in PST 12am on 11/11)

2014-11-10 14:17 GMT-08:00 Andrew Or <andrew@databricks.com>:

"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 10 Nov 2014 14:32:16 -0800",Re: Spark 1.1.1 release,Andrew Or <andrew@databricks.com>,"Hey Andrew, your JIRA search link seems wrong, it's probably supposed to be this:

https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20resolution%20%3D%20Fixed%20AND%20fixVersion%20%3D%201.1.1%20ORDER%20BY%20priority%20DESC

Matei

release
several
fixes
that
in the
will take
https://issues.apache.org/jira/browse/SPARK-3653?jql=project%20%3D%20SPARK%20AND%20fixVersion%20%3D%201.1.1%20AND%20fixVersion%20%3D%201.2.0


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Mon, 10 Nov 2014 15:00:29 -0800",Re: getting exception when trying to build spark from master,Josh Rosen <rosenville@gmail.com>,"I reverted that patch to see if it fixes it.


---------------------------------------------------------------------


"
"""Hari Shreedharan"" <hshreedharan@cloudera.com>","Mon, 10 Nov 2014 15:11:14 -0800 (PST)",Re: Bind exception while running FlumeEventCount,"""Jeniba Johnson"" <jeniba.johnson@lntinfotech.com>","Looks like that port is not available because another app is using that port. Can you take a look at netstat -a and use a port that is free?


Thanks,
Hari


 example FlumeEventCount. Previously the code was working fine. Now Iam facing with the below mentioned issues. My flume is running properly it is able to write the file.
29.17.178  65001
 with message: Error starting receiver 0: org.jboss.netty.channel.ChannelException: Failed to bind to: /172.29.17.178:65001
onStop
receiver 0
for stream 0: Error starting receiver 0 - org.jboss.netty.channel.ChannelException: Failed to bind to: /172.29.17.178:65001
.java:272)
initServer(FlumeInputDStream.scala:164)
onStart(FlumeInputDStream.scala:171)
startReceiver(ReceiverSupervisor.scala:121)
start(ReceiverSupervisor.scala:106)
auncher$$anonfun$9.apply(ReceiverTracker.scala:264)
auncher$$anonfun$9.apply(ReceiverTracker.scala:257)
apply(SparkContext.scala:1121)
apply(SparkContext.scala:1121)
scala:62)
scala:177)
runWorker(ThreadPoolExecutor.java:1145)
run(ThreadPoolExecutor.java:615)
l.java:199)
java:74)
run(NioServerBoss.java:193)
processTaskQueue(AbstractNioSelector.java:366)
run(AbstractNioSelector.java:290)
run(NioServerBoss.java:42)
0
BlockGenerator after time 1415382563200
 thread
blocks
thread
executor stop is over
 with error: org.jboss.netty.channel.ChannelException: Failed to bind to: /172.29.17.178:65001
 0.0 (TID 0)
178:65001
.java:272)
initServer(FlumeInputDStream.scala:164)
onStart(FlumeInputDStream.scala:171)
startReceiver(ReceiverSupervisor.scala:121)
start(ReceiverSupervisor.scala:106)
auncher$$anonfun$9.apply(ReceiverTracker.scala:264)
auncher$$anonfun$9.apply(ReceiverTracker.scala:257)
apply(SparkContext.scala:1121)
apply(SparkContext.scala:1121)
scala:62)
scala:177)
runWorker(ThreadPoolExecutor.java:1145)
run(ThreadPoolExecutor.java:615)
l.java:199)
java:74)
run(NioServerBoss.java:193)
processTaskQueue(AbstractNioSelector.java:366)
run(AbstractNioSelector.java:290)
run(NioServerBoss.java:42)
.0 (TID 0, localhost): org.jboss.netty.channel.ChannelException: Failed to bind to: /172.29.17.178:65001
java:272)
initServer(FlumeInputDStream.scala:164)
onStart(FlumeInputDStream.scala:171)
startReceiver(ReceiverSupervisor.scala:121)
start(ReceiverSupervisor.scala:106)
cher$$anonfun$9.apply(ReceiverTracker.scala:264)
cher$$anonfun$9.apply(ReceiverTracker.scala:257)
apply(SparkContext.scala:1121)
apply(SparkContext.scala:1121)
scala:62)
scala:177)
tor.java:1145)
run(ThreadPoolExecutor.java:615)
failed 1 times; aborting job
whose tasks have all completed, from pool
ReceiverTracker.scala:275
aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): org.jboss.netty.channel.ChannelException: Failed to bind to: /172.29.17.178:65001
java:272)
initServer(FlumeInputDStream.scala:164)
onStart(FlumeInputDStream.scala:171)
startReceiver(ReceiverSupervisor.scala:121)
start(ReceiverSupervisor.scala:106)
cher$$anonfun$9.apply(ReceiverTracker.scala:264)
cher$$anonfun$9.apply(ReceiverTracker.scala:257)
apply(SparkContext.scala:1121)
apply(SparkContext.scala:1121)
scala:62)
scala:177)
tor.java:1145)
run(ThreadPoolExecutor.java:615)
org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1185)
apply(DAGScheduler.scala:1174)
apply(DAGScheduler.scala:1173)
foreach(ResizableArray.scala:59)
scala:47)
abortStage(DAGScheduler.scala:1173)
Failed$1.apply(DAGScheduler.scala:688)
Failed$1.apply(DAGScheduler.scala:688)
handleTaskSetFailed(DAGScheduler.scala:688)
fun$receive$2.applyOrElse(DAGScheduler.scala:1391)
exec(AbstractDispatcher.scala:386)
java:260)
runTask(ForkJoinPool.java:1339)
java:1979)
run(ForkJoinWorkerThread.java:107)
________
confidential or privileged information for the intended recipient(s). Unintended recipients are prohibited from taking action on the basis of information in this e-mail and using or disseminating the information, and must notify the sender and delete it from their system. L&T Infotech will not accept responsibility or liability for the accuracy or completeness of, or the presence of any virus or disabling code in this e-mail"""
Sadhan Sood <sadhan.sood@gmail.com>,"Mon, 10 Nov 2014 18:36:36 -0500",Re: getting exception when trying to build spark from master,Patrick Wendell <pwendell@gmail.com>,"I reverted the patch locally, seems to be working for me.


"
Debasish Das <debasish.das83@gmail.com>,"Mon, 10 Nov 2014 16:05:58 -0800","Re: MatrixFactorizationModel predict(Int, Int) API",Xiangrui Meng <mengxr@gmail.com>,"I tested 2 different implementations to generate the predicted ranked
list...The first version uses a cartesian of user and product features and
then generates a predicted value for each (user,product) key...

The second version does a collect on the skinny matrix (most likely
products) and then broadcasts it to every node which computes the predicted
value...

cartesian is slower than the broadcast version...but in the broadcast
version also the shuffle time is significant..Bottleneck is the groupBy on
(user,product) composite key followed by local sort to generate topK...

The third version I thought of was to use topK predict API but this works
only if topK is bounded by a small number...If topK is large (say 100K) it
does not work since then it is bounded by master memory...

The block-wise cross product idea will optimize the groupBy right ? we
break user and feature matrices into blocks (re-use ALS partitioning) and
then in place of using (user,product) as a key use (userBlock,
productBlock) as key...Does this help improve in shuffle size ?



"
Sadhan Sood <sadhan.sood@gmail.com>,"Mon, 10 Nov 2014 19:29:27 -0500",thrift jdbc server probably running queries as hive query,"user <user@spark.apache.org>, dev@spark.incubator.apache.org","I was testing out the spark thrift jdbc server by running a simple query in
the beeline client. The spark itself is running on a yarn cluster.

However, when I run a query in beeline -> I see no running jobs in the
spark UI(completely empty) and the yarn UI seem to indicate that the
submitted query is being run as a map reduce job. This is probably also
being indicated from the spark logs but I am not completely sure:

2014-11-11 00:19:00,492 INFO  ql.Context
(Context.java:getMRScratchDir(267)) - New scratch dir is
hdfs://xxxxxxxx:9000/tmp/hive-ubuntu/hive_2014-11-11_00-19-00_367_3847629323646885865-1

2014-11-11 00:19:00,877 INFO  ql.Context
(Context.java:getMRScratchDir(267)) - New scratch dir is
hdfs://xxxxxxxx:9000/tmp/hive-ubuntu/hive_2014-11-11_00-19-00_367_3847629323646885865-2

2014-11-11 00:19:04,152 INFO  ql.Context
(Context.java:getMRScratchDir(267)) - New scratch dir is
hdfs://xxxxxxxx:9000/tmp/hive-ubuntu/hive_2014-11-11_00-19-00_367_3847629323646885865-2

2014-11-11 00:19:04,425 INFO  Configuration.deprecation
is deprecated. Instead, use mapreduce.client.submit.file.replication

2014-11-11 00:19:04,516 INFO  client.RMProxy
(RMProxy.java:createRMProxy(92)) - Connecting to ResourceManager
at xxxxxxxx:8032

2014-11-11 00:19:04,607 INFO  client.RMProxy
(RMProxy.java:createRMProxy(92)) - Connecting to ResourceManager
at xxxxxxxx:8032

2014-11-11 00:19:04,639 WARN  mapreduce.JobSubmitter
(JobSubmitter.java:copyAndConfigureFiles(150)) - Hadoop command-line option
parsing not performed. Implement the Tool interface and execute your
application with ToolRunner to remedy this

2014-11-11 00:00:08,806 INFO  input.FileInputFormat
(FileInputFormat.java:listStatus(287)) - Total input paths to process :
14912

2014-11-11 00:00:08,864 INFO  lzo.GPLNativeCodeLoader
(GPLNativeCodeLoader.java:<clinit>(34)) - Loaded native gpl library

2014-11-11 00:00:08,866 INFO  lzo.LzoCodec (LzoCodec.java:<clinit>(76)) -
Successfully loaded & initialized native-lzo library [hadoop-lzo rev
8e266e052e423af592871e2dfe09d54c03f6a0e8]

2014-11-11 00:00:09,873 INFO  input.CombineFileInputFormat
(CombineFileInputFormat.java:createSplits(413)) - DEBUG: Terminated node
allocation with : CompletedNodes: 1, size left: 194541317

2014-11-11 00:00:10,017 INFO  mapreduce.JobSubmitter
(JobSubmitter.java:submitJobInternal(396)) - number of splits:615

2014-11-11 00:00:10,095 INFO  mapreduce.JobSubmitter
(JobSubmitter.java:printTokens(479)) - Submitting tokens for job:
job_1414084656759_0115

2014-11-11 00:00:10,241 INFO  impl.YarnClientImpl
(YarnClientImpl.java:submitApplication(167)) - Submitted application
application_1414084656759_0115


It seems like the query is being run as a hive query instead of spark
query. The same query works fine when run from spark-sql cli.
"
scwf <wangfei1@huawei.com>,"Mon, 10 Nov 2014 18:21:27 -0700 (MST)",Re: thrift jdbc server probably running queries as hive query,dev@spark.incubator.apache.org,"The sql run successfully? and what sql you running?



--

---------------------------------------------------------------------


"
Xu Lijie <lijie.xlj@gmail.com>,"Tue, 11 Nov 2014 10:19:03 +0800",Checkpoint bugs in GraphX,"user@spark.apache.org, dev@spark.apache.org","Hi, all. I'm not sure whether someone has reported this bug:


There should be a checkpoint() method in EdgeRDD and VertexRDD as follows:

override def checkpoint(): Unit = { partitionsRDD.checkpoint() }


Current EdgeRDD and VertexRDD use *RDD.checkpoint()*, which only checkpoint
the edges/vertices but not the critical partitionsRDD.


Also, the variables (partitionsRDD and targetStroageLevel) in EdgeRDD and
VertexRDD should be transient.

class EdgeRDD[@specialized ED: ClassTag, VD: ClassTag]( @transient val
partitionsRDD: RDD[(PartitionID, EdgePartition[ED, VD])], @transient val
targetStorageLevel: StorageLevel = StorageLevel.MEMORY_ONLY) extends
RDD[Edge[ED]](partitionsRDD.context, List(new


class VertexRDD[@specialized VD: ClassTag]( @transient val partitionsRDD:
RDD[ShippableVertexPartition[VD]], @transient val targetStorageLevel:
StorageLevel = StorageLevel.MEMORY_ONLY) extends RDD[(VertexId,


These two bugs usually lead to stackoverflow error in iterative application
written by GraphX.
"
Cheng Lian <lian.cs.zju@gmail.com>,"Tue, 11 Nov 2014 10:59:55 +0800",Re: thrift jdbc server probably running queries as hive query,"Sadhan Sood <sadhan.sood@gmail.com>, user <user@spark.apache.org>, 
 dev@spark.incubator.apache.org","Hey Sadhan,

I really don't think this is Spark log... Unlike Shark, Spark SQL 
doesn't even provide a Hive mode to let you execute queries against 
Hive. Would you please check whether there is an existing HiveServer2 
running there? Spark SQL HiveThriftServer2 is just a Spark port of 
HiveServer2, and they share the same default listening port. I guess the 
Thrift server didn't start successfully because the HiveServer2 occupied 
the port, and your Beeline session was probably linked against HiveServer2.

Cheng


"
"""=?utf-8?B?R3VvUWlhbmcgTGk=?="" <witgo@qq.com>","Tue, 11 Nov 2014 11:16:23 +0800",Re: Checkpoint bugs in GraphX,"""=?utf-8?B?WHUgTGlqaWU=?="" <lijie.xlj@gmail.com>, ""=?utf-8?B?dXNlcg==?="" <user@spark.apache.org>, ""=?utf-8?B?ZGV2?="" <dev@spark.apache.org>","I have been trying to fix this bug.â€
The related PR: 
https://github.com/apache/spark/pull/2631â€


------------------ Original ------------------
From:  ""Xu Lijie"";<lijie.xlj@gmail.com>;
Date:  Tue, Nov 11, 2014 10:19 AM
To:  ""user""<user@spark.apache.org>; ""dev""<dev@spark.apache.org>; 

Subject:  Checkpoint bugs in GraphX



Hi, all. I'm not sure whether someone has reported this bug:


There should be a checkpoint() method in EdgeRDD and VertexRDD as follows:

override def checkpoint(): Unit = { partitionsRDD.checkpoint() }


Current EdgeRDD and VertexRDD use *RDD.checkpoint()*, which only checkpoint
the edges/vertices but not the critical partitionsRDD.


Also, the variables (partitionsRDD and targetStroageLevel) in EdgeRDD and
VertexRDD should be transient.

class EdgeRDD[@specialized ED: ClassTag, VD: ClassTag]( @transient val
partitionsRDD: RDD[(PartitionID, EdgePartition[ED, VD])], @transient val
targetStorageLevel: StorageLevel = StorageLevel.MEMORY_ONLY) extends
RDD[Edge[ED]](partitionsRDD.context, List(new
OneToOneDependency(partitionsRDD))) {


class VertexRDD[@specialized VD: ClassTag]( @transient val partitionsRDD:
RDD[ShippableVertexPartition[VD]], @transient val targetStorageLevel:
StorageLevel = StorageLevel.MEMORY_ONLY) extends RDD[(VertexId,
VD)](partitionsRDD.context, List(new OneToOneDependency(partitionsRDD))) {


These two bugs usually lead to stackoverflow error in iterative application
written by GraphX."
Xu Lijie <lijie.xlj@gmail.com>,"Tue, 11 Nov 2014 11:32:12 +0800",Discuss how to do checkpoint more efficently,"user@spark.apache.org, dev@spark.apache.org","Hi, all. I want to seek suggestions on how to do checkpoint more
efficiently, especially for iterative applications written by GraphX.


For iterative applications, the lineage of a job can be very long, which is
easy to cause statckoverflow error. A solution is to do checkpoint.
However, checkpoint is time-consuming and not easy for ordinary users to
perform (e.g., which RDDs need checkpoint and when to checkpoint them).
Moreover, to shorten the linage, iterative applications need to do
checkpoint frequently (e.g., every 10 iterations). As a result, checkpoint
is too heavy for iterative applications especially written by GraphX.

I'm wondering if there is an elegant way to solve the problem: shortening
the lineage and also saving the intermediate data/results in a lightweight
way.

Maybe we can develop a new API like checkpoint(StorageLevel), which has the
feature of both cache() and current checkpoint().




Examples:

The lineage is very long without checkpoint even in the first iteration in
GraphX job.

[Iter 1][DEBUG] (2) EdgeRDD[33] at RDD at EdgeRDD.scala:35
 |  EdgeRDD ZippedPartitionsRDD2[32] at zipPartitions at
ReplicatedVertexView.scala:114
 |  EdgeRDD MapPartitionsRDD[12] at mapPartitionsWithIndex at
EdgeRDD.scala:169
 |  MappedRDD[11] at map at Graph.scala:392
 |  MappedRDD[10] at distinct at KCoreCommonDebug.scala:115
 |  ShuffledRDD[9] at distinct at KCoreCommonDebug.scala:115
 +-(2) MappedRDD[8] at distinct at KCoreCommonDebug.scala:115
    |  FilteredRDD[7] at filter at KCoreCommonDebug.scala:112
    |  MappedRDD[6] at map at KCoreCommonDebug.scala:102
    |  MappedRDD[5] at repartition at KCoreCommonDebug.scala:101
    |  CoalescedRDD[4] at repartition at KCoreCommonDebug.scala:101
    |  ShuffledRDD[3] at repartition at KCoreCommonDebug.scala:101
    +-(2) MapPartitionsRDD[2] at repartition at KCoreCommonDebug.scala:101
       |  D:\graphData\verylarge.txt MappedRDD[1] at textFile at
KCoreCommonDebug.scala:100
       |  D:\graphData\verylarge.txt HadoopRDD[0] at textFile at
KCoreCommonDebug.scala:100
 |  ShuffledRDD[31] at partitionBy at ReplicatedVertexView.scala:112
 +-(2) ReplicatedVertexView.updateVertices - shippedVerts false false
(broadcast) MapPartitionsRDD[30] at mapPartitions at VertexRDD.scala:347
    |  VertexRDD ZippedPartitionsRDD2[28] at zipPartitions at
VertexRDD.scala:174
    |  VertexRDD, VertexRDD MapPartitionsRDD[18] at mapPartitions at
VertexRDD.scala:441
    |  MapPartitionsRDD[17] at mapPartitions at VertexRDD.scala:457
    |  ShuffledRDD[16] at ShuffledRDD at RoutingTablePartition.scala:36
    +-(2) VertexRDD.createRoutingTables - vid2pid (aggregation)
MapPartitionsRDD[15] at mapPartitions at VertexRDD.scala:452
       |  EdgeRDD MapPartitionsRDD[12] at mapPartitionsWithIndex at
EdgeRDD.scala:169
       |  MappedRDD[11] at map at Graph.scala:392
       |  MappedRDD[10] at distinct at KCoreCommonDebug.scala:115
       |  ShuffledRDD[9] at distinct at KCoreCommonDebug.scala:115
       +-(2) MappedRDD[8] at distinct at KCoreCommonDebug.scala:115
          |  FilteredRDD[7] at filter at KCoreCommonDebug.scala:112
          |  MappedRDD[6] at map at KCoreCommonDebug.scala:102
          |  MappedRDD[5] at repartition at KCoreCommonDebug.scala:101
          |  CoalescedRDD[4] at repartition at KCoreCommonDebug.scala:101
          |  ShuffledRDD[3] at repartition at KCoreCommonDebug.scala:101
          +-(2) MapPartitionsRDD[2] at repartition at
KCoreCommonDebug.scala:101
             |  D:\graphData\verylarge.txt MappedRDD[1] at textFile at
KCoreCommonDebug.scala:100
             |  D:\graphData\verylarge.txt HadoopRDD[0] at textFile at
KCoreCommonDebug.scala:100
    |  VertexRDD ZippedPartitionsRDD2[26] at zipPartitions at
VertexRDD.scala:200
    |  VertexRDD, VertexRDD MapPartitionsRDD[18] at mapPartitions at
VertexRDD.scala:441
    |  MapPartitionsRDD[17] at mapPartitions at VertexRDD.scala:457
    |  ShuffledRDD[16] at ShuffledRDD at RoutingTablePartition.scala:36
    +-(2) VertexRDD.createRoutingTables - vid2pid (aggregation)
MapPartitionsRDD[15] at mapPartitions at VertexRDD.scala:452
       |  EdgeRDD MapPartitionsRDD[12] at mapPartitionsWithIndex at
EdgeRDD.scala:169
       |  MappedRDD[11] at map at Graph.scala:392
       |  MappedRDD[10] at distinct at KCoreCommonDebug.scala:115
       |  ShuffledRDD[9] at distinct at KCoreCommonDebug.scala:115
       +-(2) MappedRDD[8] at distinct at KCoreCommonDebug.scala:115
          |  FilteredRDD[7] at filter at KCoreCommonDebug.scala:112
          |  MappedRDD[6] at map at KCoreCommonDebug.scala:102
          |  MappedRDD[5] at repartition at KCoreCommonDebug.scala:101
          |  CoalescedRDD[4] at repartition at KCoreCommonDebug.scala:101
          |  ShuffledRDD[3] at repartition at KCoreCommonDebug.scala:101
          +-(2) MapPartitionsRDD[2] at repartition at
KCoreCommonDebug.scala:101
             |  D:\graphData\verylarge.txt MappedRDD[1] at textFile at
KCoreCommonDebug.scala:100
             |  D:\graphData\verylarge.txt HadoopRDD[0] at textFile at
KCoreCommonDebug.scala:100
    |  VertexRDD, GraphOps.degrees ZippedPartitionsRDD2[24] at
zipPartitions at VertexRDD.scala:301
    |  VertexRDD, VertexRDD MapPartitionsRDD[18] at mapPartitions at
VertexRDD.scala:441
    |  MapPartitionsRDD[17] at mapPartitions at VertexRDD.scala:457
    |  ShuffledRDD[16] at ShuffledRDD at RoutingTablePartition.scala:36
    +-(2) VertexRDD.createRoutingTables - vid2pid (aggregation)
MapPartitionsRDD[15] at mapPartitions at VertexRDD.scala:452
       |  EdgeRDD MapPartitionsRDD[12] at mapPartitionsWithIndex at
EdgeRDD.scala:169
       |  MappedRDD[11] at map at Graph.scala:392
       |  MappedRDD[10] at distinct at KCoreCommonDebug.scala:115
       |  ShuffledRDD[9] at distinct at KCoreCommonDebug.scala:115
       +-(2) MappedRDD[8] at distinct at KCoreCommonDebug.scala:115
          |  FilteredRDD[7] at filter at KCoreCommonDebug.scala:112
          |  MappedRDD[6] at map at KCoreCommonDebug.scala:102
          |  MappedRDD[5] at repartition at KCoreCommonDebug.scala:101
          |  CoalescedRDD[4] at repartition at KCoreCommonDebug.scala:101
          |  ShuffledRDD[3] at repartition at KCoreCommonDebug.scala:101
          +-(2) MapPartitionsRDD[2] at repartition at
KCoreCommonDebug.scala:101
             |  D:\graphData\verylarge.txt MappedRDD[1] at textFile at
KCoreCommonDebug.scala:100
             |  D:\graphData\verylarge.txt HadoopRDD[0] at textFile at
KCoreCommonDebug.scala:100
    |  ShuffledRDD[23] at ShuffledRDD at MessageToPartition.scala:31
    +-(2) GraphImpl.mapReduceTriplets - preAgg MapPartitionsRDD[22] at
mapPartitions at GraphImpl.scala:192
       |  EdgeRDD MapPartitionsRDD[12] at mapPartitionsWithIndex at
EdgeRDD.scala:169
       |  MappedRDD[11] at map at Graph.scala:392
       |  MappedRDD[10] at distinct at KCoreCommonDebug.scala:115
       |  ShuffledRDD[9] at distinct at KCoreCommonDebug.scala:115
       +-(2) MappedRDD[8] at distinct at KCoreCommonDebug.scala:115
          |  FilteredRDD[7] at filter at KCoreCommonDebug.scala:112
          |  MappedRDD[6] at map at KCoreCommonDebug.scala:102
          |  MappedRDD[5] at repartition at KCoreCommonDebug.scala:101
          |  CoalescedRDD[4] at repartition at KCoreCommonDebug.scala:101
          |  ShuffledRDD[3] at repartition at KCoreCommonDebug.scala:101
          +-(2) MapPartitionsRDD[2] at repartition at
KCoreCommonDebug.scala:101
             |  D:\graphData\verylarge.txt MappedRDD[1] at textFile at
KCoreCommonDebug.scala:100
             |  D:\graphData\verylarge.txt HadoopRDD[0] at textFile at
KCoreCommonDebug.scala:100

Checkpoint can shorten the lineage. However, the lineage is still very long
even with checkpoint. For example,


[Iter 3][DEBUG] (2) VertexRDD[71] at RDD at VertexRDD.scala:58
 |  VertexRDD ZippedPartitionsRDD2[70] at zipPartitions at
VertexRDD.scala:200
 |  VertexRDD MapPartitionsRDD[61] at mapPartitions at VertexRDD.scala:127
 |  VertexRDD ZippedPartitionsRDD2[47] at zipPartitions at
VertexRDD.scala:200
 |  CheckpointRDD[56] at apply at List.scala:318
 |  VertexRDD, GraphOps.degrees ZippedPartitionsRDD2[68] at zipPartitions
at VertexRDD.scala:301
 |  VertexRDD MapPartitionsRDD[61] at mapPartitions at VertexRDD.scala:127
 |  VertexRDD ZippedPartitionsRDD2[47] at zipPartitions at
VertexRDD.scala:200
 |  CheckpointRDD[56] at apply at List.scala:318
 |  ShuffledRDD[67] at ShuffledRDD at MessageToPartition.scala:31
 +-(2) GraphImpl.mapReduceTriplets - preAgg MapPartitionsRDD[66] at
mapPartitions at GraphImpl.scala:192
    |  EdgeRDD MapPartitionsRDD[63] at mapPartitions at EdgeRDD.scala:85
    |  EdgeRDD ZippedPartitionsRDD2[53] at zipPartitions at
ReplicatedVertexView.scala:114
    |  CheckpointRDD[57] at apply at List.scala:318


[Iter 4][DEBUG] (2) VertexRDD[92] at RDD at VertexRDD.scala:58
 |  VertexRDD ZippedPartitionsRDD2[91] at zipPartitions at
VertexRDD.scala:200
 |  VertexRDD MapPartitionsRDD[82] at mapPartitions at VertexRDD.scala:127
 |  VertexRDD ZippedPartitionsRDD2[70] at zipPartitions at
VertexRDD.scala:200
 |  VertexRDD MapPartitionsRDD[61] at mapPartitions at VertexRDD.scala:127
 |  VertexRDD ZippedPartitionsRDD2[47] at zipPartitions at
VertexRDD.scala:200
 |  CheckpointRDD[56] at apply at List.scala:318
 |  VertexRDD, GraphOps.degrees ZippedPartitionsRDD2[68] at zipPartitions
at VertexRDD.scala:301
 |  VertexRDD MapPartitionsRDD[61] at mapPartitions at VertexRDD.scala:127
 |  VertexRDD ZippedPartitionsRDD2[47] at zipPartitions at
VertexRDD.scala:200
 |  CheckpointRDD[56] at apply at List.scala:318
 |  ShuffledRDD[67] at ShuffledRDD at MessageToPartition.scala:31
 +-(2) GraphImpl.mapReduceTriplets - preAgg MapPartitionsRDD[66] at
mapPartitions at GraphImpl.scala:192
    |  EdgeRDD MapPartitionsRDD[63] at mapPartitions at EdgeRDD.scala:85
    |  EdgeRDD ZippedPartitionsRDD2[53] at zipPartitions at
ReplicatedVertexView.scala:114
    |  CheckpointRDD[57] at apply at List.scala:318
 |  VertexRDD, GraphOps.degrees ZippedPartitionsRDD2[89] at zipPartitions
at VertexRDD.scala:301
 |  VertexRDD MapPartitionsRDD[82] at mapPartitions at VertexRDD.scala:127
 |  VertexRDD ZippedPartitionsRDD2[70] at zipPartitions at
VertexRDD.scala:200
 |  VertexRDD MapPartitionsRDD[61] at mapPartitions at VertexRDD.scala:127
 |  VertexRDD ZippedPartitionsRDD2[47] at zipPartitions at
VertexRDD.scala:200
 |  CheckpointRDD[56] at apply at List.scala:318
 |  VertexRDD, GraphOps.degrees ZippedPartitionsRDD2[68] at zipPartitions
at VertexRDD.scala:301
 |  VertexRDD MapPartitionsRDD[61] at mapPartitions at VertexRDD.scala:127
 |  VertexRDD ZippedPartitionsRDD2[47] at zipPartitions at
VertexRDD.scala:200
 |  CheckpointRDD[56] at apply at List.scala:318
 |  ShuffledRDD[67] at ShuffledRDD at MessageToPartition.scala:31
 +-(2) GraphImpl.mapReduceTriplets - preAgg MapPartitionsRDD[66] at
mapPartitions at GraphImpl.scala:192
    |  EdgeRDD MapPartitionsRDD[63] at mapPartitions at EdgeRDD.scala:85
    |  EdgeRDD ZippedPartitionsRDD2[53] at zipPartitions at
ReplicatedVertexView.scala:114
    |  CheckpointRDD[57] at apply at List.scala:318
 |  ShuffledRDD[88] at ShuffledRDD at MessageToPartition.scala:31
 +-(2) GraphImpl.mapReduceTriplets - preAgg MapPartitionsRDD[87] at
mapPartitions at GraphImpl.scala:192
    |  EdgeRDD MapPartitionsRDD[84] at mapPartitions at EdgeRDD.scala:85
    |  EdgeRDD ZippedPartitionsRDD2[76] at zipPartitions at
ReplicatedVertexView.scala:114
    |  EdgeRDD MapPartitionsRDD[63] at mapPartitions at EdgeRDD.scala:85
    |  EdgeRDD ZippedPartitionsRDD2[53] at zipPartitions at
ReplicatedVertexView.scala:114
    |  CheckpointRDD[57] at apply at List.scala:318
    |  ShuffledRDD[75] at partitionBy at ReplicatedVertexView.scala:112
    +-(2) ReplicatedVertexView.updateVertices - shippedVerts true true
(broadcast) MapPartitionsRDD[74] at mapPartitions at VertexRDD.scala:347
       |  VertexRDD ZippedPartitionsRDD2[72] at zipPartitions at
VertexRDD.scala:174
       |  VertexRDD MapPartitionsRDD[61] at mapPartitions at
VertexRDD.scala:127
       |  VertexRDD ZippedPartitionsRDD2[47] at zipPartitions at
VertexRDD.scala:200
       |  CheckpointRDD[56] at apply at List.scala:318
       |  VertexRDD ZippedPartitionsRDD2[70] at zipPartitions at
VertexRDD.scala:200
       |  VertexRDD MapPartitionsRDD[61] at mapPartitions at
VertexRDD.scala:127
       |  VertexRDD ZippedPartitionsRDD2[47] at zipPartitions at
VertexRDD.scala:200
       |  CheckpointRDD[56] at apply at List.scala:318
       |  VertexRDD, GraphOps.degrees ZippedPartitionsRDD2[68] at
zipPartitions at VertexRDD.scala:301
       |  VertexRDD MapPartitionsRDD[61] at mapPartitions at
VertexRDD.scala:127
       |  VertexRDD ZippedPartitionsRDD2[47] at zipPartitions at
VertexRDD.scala:200
       |  CheckpointRDD[56] at apply at List.scala:318
       |  ShuffledRDD[67] at ShuffledRDD at MessageToPartition.scala:31
       +-(2) GraphImpl.mapReduceTriplets - preAgg MapPartitionsRDD[66] at
mapPartitions at GraphImpl.scala:192
          |  EdgeRDD MapPartitionsRDD[63] at mapPartitions at
EdgeRDD.scala:85
          |  EdgeRDD ZippedPartitionsRDD2[53] at zipPartitions at
ReplicatedVertexView.scala:114
          |  CheckpointRDD[57] at apply at List.scala:318


[Iter 5][DEBUG] (2) VertexRDD[113] at RDD at VertexRDD.scala:58
 |  VertexRDD ZippedPartitionsRDD2[112] at zipPartitions at
VertexRDD.scala:200
 |  VertexRDD MapPartitionsRDD[103] at mapPartitions at VertexRDD.scala:127
 |  VertexRDD ZippedPartitionsRDD2[91] at zipPartitions at
VertexRDD.scala:200
 |  VertexRDD MapPartitionsRDD[82] at mapPartitions at VertexRDD.scala:127
 |  VertexRDD ZippedPartitionsRDD2[70] at zipPartitions at
VertexRDD.scala:200
 |  VertexRDD MapPartitionsRDD[61] at mapPartitions at VertexRDD.scala:127
 |  VertexRDD ZippedPartitionsRDD2[47] at zipPartitions at
VertexRDD.scala:200
 |  CheckpointRDD[56] at apply at List.scala:318
 |  VertexRDD, GraphOps.degrees ZippedPartitionsRDD2[68] at zipPartitions
at VertexRDD.scala:301
 |  VertexRDD MapPartitionsRDD[61] at mapPartitions at VertexRDD.scala:127
 |  VertexRDD ZippedPartitionsRDD2[47] at zipPartitions at
VertexRDD.scala:200
 |  CheckpointRDD[56] at apply at List.scala:318
 |  ShuffledRDD[67] at ShuffledRDD at MessageToPartition.scala:31
 +-(2) GraphImpl.mapReduceTriplets - preAgg MapPartitionsRDD[66] at
mapPartitions at GraphImpl.scala:192
    |  EdgeRDD MapPartitionsRDD[63] at mapPartitions at EdgeRDD.scala:85
    |  EdgeRDD ZippedPartitionsRDD2[53] at zipPartitions at
ReplicatedVertexView.scala:114
    |  CheckpointRDD[57] at apply at List.scala:318
 |  VertexRDD, GraphOps.degrees ZippedPartitionsRDD2[89] at zipPartitions
at VertexRDD.scala:301
 |  VertexRDD MapPartitionsRDD[82] at mapPartitions at VertexRDD.scala:127
 |  VertexRDD ZippedPartitionsRDD2[70] at zipPartitions at
VertexRDD.scala:200
 |  VertexRDD MapPartitionsRDD[61] at mapPartitions at VertexRDD.scala:127
 |  VertexRDD ZippedPartitionsRDD2[47] at zipPartitions at
VertexRDD.scala:200
 |  CheckpointRDD[56] at apply at List.scala:318
 |  VertexRDD, GraphOps.degrees ZippedPartitionsRDD2[68] at zipPartitions
at VertexRDD.scala:301
 |  VertexRDD MapPartitionsRDD[61] at mapPartitions at VertexRDD.scala:127
 |  VertexRDD ZippedPartitionsRDD2[47] at zipPartitions at
VertexRDD.scala:200
 |  CheckpointRDD[56] at apply at List.scala:318
 |  ShuffledRDD[67] at ShuffledRDD at MessageToPartition.scala:31
 +-(2) GraphImpl.mapReduceTriplets - preAgg MapPartitionsRDD[66] at
mapPartitions at GraphImpl.scala:192
    |  EdgeRDD MapPartitionsRDD[63] at mapPartitions at EdgeRDD.scala:85
    |  EdgeRDD ZippedPartitionsRDD2[53] at zipPartitions at
ReplicatedVertexView.scala:114
    |  CheckpointRDD[57] at apply at List.scala:318
 |  ShuffledRDD[88] at ShuffledRDD at MessageToPartition.scala:31
 +-(2) GraphImpl.mapReduceTriplets - preAgg MapPartitionsRDD[87] at
mapPartitions at GraphImpl.scala:192
    |  EdgeRDD MapPartitionsRDD[84] at mapPartitions at EdgeRDD.scala:85
    |  EdgeRDD ZippedPartitionsRDD2[76] at zipPartitions at
ReplicatedVertexView.scala:114
    |  EdgeRDD MapPartitionsRDD[63] at mapPartitions at EdgeRDD.scala:85
    |  EdgeRDD ZippedPartitionsRDD2[53] at zipPartitions at
ReplicatedVertexView.scala:114
    |  CheckpointRDD[57] at apply at List.scala:318
    |  ShuffledRDD[75] at partitionBy at ReplicatedVertexView.scala:112
    +-(2) ReplicatedVertexView.updateVertices - shippedVerts true true
(broadcast) MapPartitionsRDD[74] at mapPartitions at VertexRDD.scala:347
       |  VertexRDD ZippedPartitionsRDD2[72] at zipPartitions at
VertexRDD.scala:174
       |  VertexRDD MapPartitionsRDD[61] at mapPartitions at
VertexRDD.scala:127
       |  VertexRDD ZippedPartitionsRDD2[47] at zipPartitions at
VertexRDD.scala:200
       |  CheckpointRDD[56] at apply at List.scala:318
       |  VertexRDD ZippedPartitionsRDD2[70] at zipPartitions at
VertexRDD.scala:200
       |  VertexRDD MapPartitionsRDD[61] at mapPartitions at
VertexRDD.scala:127
       |  VertexRDD ZippedPartitionsRDD2[47] at zipPartitions at
VertexRDD.scala:200
       |  CheckpointRDD[56] at apply at List.scala:318
       |  VertexRDD, GraphOps.degrees ZippedPartitionsRDD2[68] at
zipPartitions at VertexRDD.scala:301
       |  VertexRDD MapPartitionsRDD[61] at mapPartitions at
VertexRDD.scala:127
       |  VertexRDD ZippedPartitionsRDD2[47] at zipPartitions at
VertexRDD.scala:200
       |  CheckpointRDD[56] at apply at List.scala:318
       |  ShuffledRDD[67] at ShuffledRDD at MessageToPartition.scala:31
       +-(2) GraphImpl.mapReduceTriplets - preAgg MapPartitionsRDD[66] at
mapPartitions at GraphImpl.scala:192
          |  EdgeRDD MapPartitionsRDD[63] at mapPartitions at
EdgeRDD.scala:85
          |  EdgeRDD ZippedPartitionsRDD2[53] at zipPartitions at
ReplicatedVertexView.scala:114
          |  CheckpointRDD[57] at apply at List.scala:318
 |  VertexRDD, GraphOps.degrees ZippedPartitionsRDD2[110] at zipPartitions
at VertexRDD.scala:301
 |  VertexRDD MapPartitionsRDD[103] at mapPartitions at VertexRDD.scala:127
 |  VertexRDD ZippedPartitionsRDD2[91] at zipPartitions at
VertexRDD.scala:200
 |  VertexRDD MapPartitionsRDD[82] at mapPartitions at VertexRDD.scala:127
 |  VertexRDD ZippedPartitionsRDD2[70] at zipPartitions at
VertexRDD.scala:200
 |  VertexRDD MapPartitionsRDD[61] at mapPartitions at VertexRDD.scala:127
 |  VertexRDD ZippedPartitionsRDD2[47] at zipPartitions at
VertexRDD.scala:200
 |  CheckpointRDD[56] at apply at List.scala:318
 |  VertexRDD, GraphOps.degrees ZippedPartitionsRDD2[68] at zipPartitions
at VertexRDD.scala:301
 |  VertexRDD MapPartitionsRDD[61] at mapPartitions at VertexRDD.scala:127
 |  VertexRDD ZippedPartitionsRDD2[47] at zipPartitions at
VertexRDD.scala:200
 |  CheckpointRDD[56] at apply at List.scala:318
 |  ShuffledRDD[67] at ShuffledRDD at MessageToPartition.scala:31
 +-(2) GraphImpl.mapReduceTriplets - preAgg MapPartitionsRDD[66] at
mapPartitions at GraphImpl.scala:192
    |  EdgeRDD MapPartitionsRDD[63] at mapPartitions at EdgeRDD.scala:85
    |  EdgeRDD ZippedPartitionsRDD2[53] at zipPartitions at
ReplicatedVertexView.scala:114
    |  CheckpointRDD[57] at apply at List.scala:318
 |  VertexRDD, GraphOps.degrees ZippedPartitionsRDD2[89] at zipPartitions
at VertexRDD.scala:301
 |  VertexRDD MapPartitionsRDD[82] at mapPartitions at VertexRDD.scala:127
 |  VertexRDD ZippedPartitionsRDD2[70] at zipPartitions at
VertexRDD.scala:200
 |  VertexRDD MapPartitionsRDD[61] at mapPartitions at VertexRDD.scala:127
 |  VertexRDD ZippedPartitionsRDD2[47] at zipPartitions at
VertexRDD.scala:200
 |  CheckpointRDD[56] at apply at List.scala:318
 |  VertexRDD, GraphOps.degrees ZippedPartitionsRDD2[68] at zipPartitions
at VertexRDD.scala:301
 |  VertexRDD MapPartitionsRDD[61] at mapPartitions at VertexRDD.scala:127
 |  VertexRDD ZippedPartitionsRDD2[47] at zipPartitions at
VertexRDD.scala:200
 |  CheckpointRDD[56] at apply at List.scala:318
 |  ShuffledRDD[67] at ShuffledRDD at MessageToPartition.scala:31
 +-(2) GraphImpl.mapReduceTriplets - preAgg MapPartitionsRDD[66] at
mapPartitions at GraphImpl.scala:192
    |  EdgeRDD MapPartitionsRDD[63] at mapPartitions at EdgeRDD.scala:85
    |  EdgeRDD ZippedPartitionsRDD2[53] at zipPartitions at
ReplicatedVertexView.scala:114
    |  CheckpointRDD[57] at apply at List.scala:318
 |  ShuffledRDD[88] at ShuffledRDD at MessageToPartition.scala:31
 +-(2) GraphImpl.mapReduceTriplets - preAgg MapPartitionsRDD[87] at
mapPartitions at GraphImpl.scala:192
    |  EdgeRDD MapPartitionsRDD[84] at mapPartitions at EdgeRDD.scala:85
    |  EdgeRDD ZippedPartitionsRDD2[76] at zipPartitions at
ReplicatedVertexView.scala:114
    |  EdgeRDD MapPartitionsRDD[63] at mapPartitions at EdgeRDD.scala:85
    |  EdgeRDD ZippedPartitionsRDD2[53] at zipPartitions at
ReplicatedVertexView.scala:114
    |  CheckpointRDD[57] at apply at List.scala:318
    |  ShuffledRDD[75] at partitionBy at ReplicatedVertexView.scala:112
    +-(2) ReplicatedVertexView.updateVertices - shippedVerts true true
(broadcast) MapPartitionsRDD[74] at mapPartitions at VertexRDD.scala:347
       |  VertexRDD ZippedPartitionsRDD2[72] at zipPartitions at
VertexRDD.scala:174
       |  VertexRDD MapPartitionsRDD[61] at mapPartitions at
VertexRDD.scala:127
       |  VertexRDD ZippedPartitionsRDD2[47] at zipPartitions at
VertexRDD.scala:200
       |  CheckpointRDD[56] at apply at List.scala:318
       |  VertexRDD ZippedPartitionsRDD2[70] at zipPartitions at
VertexRDD.scala:200
       |  VertexRDD MapPartitionsRDD[61] at mapPartitions at
VertexRDD.scala:127
       |  VertexRDD ZippedPartitionsRDD2[47] at zipPartitions at
VertexRDD.scala:200
       |  CheckpointRDD[56] at apply at List.scala:318
       |  VertexRDD, GraphOps.degrees ZippedPartitionsRDD2[68] at
zipPartitions at VertexRDD.scala:301
       |  VertexRDD MapPartitionsRDD[61] at mapPartitions at
VertexRDD.scala:127
       |  VertexRDD ZippedPartitionsRDD2[47] at zipPartitions at
VertexRDD.scala:200
       |  CheckpointRDD[56] at apply at List.scala:318
       |  ShuffledRDD[67] at ShuffledRDD at MessageToPartition.scala:31
       +-(2) GraphImpl.mapReduceTriplets - preAgg MapPartitionsRDD[66] at
mapPartitions at GraphImpl.scala:192
          |  EdgeRDD MapPartitionsRDD[63] at mapPartitions at
EdgeRDD.scala:85
          |  EdgeRDD ZippedPartitionsRDD2[53] at zipPartitions at
ReplicatedVertexView.scala:114
          |  CheckpointRDD[57] at apply at List.scala:318
 |  ShuffledRDD[109] at ShuffledRDD at MessageToPartition.scala:31
 +-(2) GraphImpl.mapReduceTriplets - preAgg MapPartitionsRDD[108] at
mapPartitions at GraphImpl.scala:192
    |  EdgeRDD MapPartitionsRDD[105] at mapPartitions at EdgeRDD.scala:85
    |  EdgeRDD ZippedPartitionsRDD2[97] at zipPartitions at
ReplicatedVertexView.scala:114
    |  EdgeRDD MapPartitionsRDD[84] at mapPartitions at EdgeRDD.scala:85
    |  EdgeRDD ZippedPartitionsRDD2[76] at zipPartitions at
ReplicatedVertexView.scala:114
    |  EdgeRDD MapPartitionsRDD[63] at mapPartitions at EdgeRDD.scala:85
    |  EdgeRDD ZippedPartitionsRDD2[53] at zipPartitions at
ReplicatedVertexView.scala:114
    |  CheckpointRDD[57] at apply at List.scala:318
    |  ShuffledRDD[75] at partitionBy at ReplicatedVertexView.scala:112
    +-(2) ReplicatedVertexView.updateVertices - shippedVerts true true
(broadcast) MapPartitionsRDD[74] at mapPartitions at VertexRDD.scala:347
       |  VertexRDD ZippedPartitionsRDD2[72] at zipPartitions at
VertexRDD.scala:174
       |  VertexRDD MapPartitionsRDD[61] at mapPartitions at
VertexRDD.scala:127
       |  VertexRDD ZippedPartitionsRDD2[47] at zipPartitions at
VertexRDD.scala:200
       |  CheckpointRDD[56] at apply at List.scala:318
       |  VertexRDD ZippedPartitionsRDD2[70] at zipPartitions at
VertexRDD.scala:200
       |  VertexRDD MapPartitionsRDD[61] at mapPartitions at
VertexRDD.scala:127
       |  VertexRDD ZippedPartitionsRDD2[47] at zipPartitions at
VertexRDD.scala:200
       |  CheckpointRDD[56] at apply at List.scala:318
       |  VertexRDD, GraphOps.degrees ZippedPartitionsRDD2[68] at
zipPartitions at VertexRDD.scala:301
       |  VertexRDD MapPartitionsRDD[61] at mapPartitions at
VertexRDD.scala:127
       |  VertexRDD ZippedPartitionsRDD2[47] at zipPartitions at
VertexRDD.scala:200
       |  CheckpointRDD[56] at apply at List.scala:318
       |  ShuffledRDD[67] at ShuffledRDD at MessageToPartition.scala:31
       +-(2) GraphImpl.mapReduceTriplets - preAgg MapPartitionsRDD[66] at
mapPartitions at GraphImpl.scala:192
          |  EdgeRDD MapPartitionsRDD[63] at mapPartitions at
EdgeRDD.scala:85
          |  EdgeRDD ZippedPartitionsRDD2[53] at zipPartitions at
ReplicatedVertexView.scala:114
          |  CheckpointRDD[57] at apply at List.scala:318
    |  ShuffledRDD[96] at partitionBy at ReplicatedVertexView.scala:112
    +-(2) ReplicatedVertexView.updateVertices - shippedVerts true true
(broadcast) MapPartitionsRDD[95] at mapPartitions at VertexRDD.scala:347
       |  VertexRDD ZippedPartitionsRDD2[93] at zipPartitions at
VertexRDD.scala:174
       |  VertexRDD MapPartitionsRDD[82] at mapPartitions at
VertexRDD.scala:127
       |  VertexRDD ZippedPartitionsRDD2[70] at zipPartitions at
VertexRDD.scala:200
       |  VertexRDD MapPartitionsRDD[61] at mapPartitions at
VertexRDD.scala:127
       |  VertexRDD ZippedPartitionsRDD2[47] at zipPartitions at
VertexRDD.scala:200
       |  CheckpointRDD[56] at apply at List.scala:318
       |  VertexRDD, GraphOps.degrees ZippedPartitionsRDD2[68] at
zipPartitions at VertexRDD.scala:301
       |  VertexRDD MapPartitionsRDD[61] at mapPartitions at
VertexRDD.scala:127
       |  VertexRDD ZippedPartitionsRDD2[47] at zipPartitions at
VertexRDD.scala:200
       |  CheckpointRDD[56] at apply at List.scala:318
       |  ShuffledRDD[67] at ShuffledRDD at MessageToPartition.scala:31
       +-(2) GraphImpl.mapReduceTriplets - preAgg MapPartitionsRDD[66] at
mapPartitions at GraphImpl.scala:192
          |  EdgeRDD MapPartitionsRDD[63] at mapPartitions at
EdgeRDD.scala:85
          |  EdgeRDD ZippedPartitionsRDD2[53] at zipPartitions at
ReplicatedVertexView.scala:114
          |  CheckpointRDD[57] at apply at List.scala:318
       |  VertexRDD ZippedPartitionsRDD2[91] at zipPartitions at
VertexRDD.scala:200
       |  VertexRDD MapPartitionsRDD[82] at mapPartitions at
VertexRDD.scala:127
       |  VertexRDD ZippedPartitionsRDD2[70] at zipPartitions at
VertexRDD.scala:200
       |  VertexRDD MapPartitionsRDD[61] at mapPartitions at
VertexRDD.scala:127
       |  VertexRDD ZippedPartitionsRDD2[47] at zipPartitions at
VertexRDD.scala:200
       |  CheckpointRDD[56] at apply at List.scala:318
       |  VertexRDD, GraphOps.degrees ZippedPartitionsRDD2[68] at
zipPartitions at VertexRDD.scala:301
       |  VertexRDD MapPartitionsRDD[61] at mapPartitions at
VertexRDD.scala:127
       |  VertexRDD ZippedPartitionsRDD2[47] at zipPartitions at
VertexRDD.scala:200
       |  CheckpointRDD[56] at apply at List.scala:318
       |  ShuffledRDD[67] at ShuffledRDD at MessageToPartition.scala:31
       +-(2) GraphImpl.mapReduceTriplets - preAgg MapPartitionsRDD[66] at
mapPartitions at GraphImpl.scala:192
          |  EdgeRDD MapPartitionsRDD[63] at mapPartitions at
EdgeRDD.scala:85
          |  EdgeRDD ZippedPartitionsRDD2[53] at zipPartitions at
ReplicatedVertexView.scala:114
          |  CheckpointRDD[57] at apply at List.scala:318
       |  VertexRDD, GraphOps.degrees ZippedPartitionsRDD2[89] at
zipPartitions at VertexRDD.scala:301
       |  VertexRDD MapPartitionsRDD[82] at mapPartitions at
VertexRDD.scala:127
       |  VertexRDD ZippedPartitionsRDD2[70] at zipPartitions at
VertexRDD.scala:200
       |  VertexRDD MapPartitionsRDD[61] at mapPartitions at
VertexRDD.scala:127
       |  VertexRDD ZippedPartitionsRDD2[47] at zipPartitions at
VertexRDD.scala:200
       |  CheckpointRDD[56] at apply at List.scala:318
       |  VertexRDD, GraphOps.degrees ZippedPartitionsRDD2[68] at
zipPartitions at VertexRDD.scala:301
       |  VertexRDD MapPartitionsRDD[61] at mapPartitions at
VertexRDD.scala:127
       |  VertexRDD ZippedPartitionsRDD2[47] at zipPartitions at
VertexRDD.scala:200
       |  CheckpointRDD[56] at apply at List.scala:318
       |  ShuffledRDD[67] at ShuffledRDD at MessageToPartition.scala:31
       +-(2) GraphImpl.mapReduceTriplets - preAgg MapPartitionsRDD[66] at
mapPartitions at GraphImpl.scala:192
          |  EdgeRDD MapPartitionsRDD[63] at mapPartitions at
EdgeRDD.scala:85
          |  EdgeRDD ZippedPartitionsRDD2[53] at zipPartitions at
ReplicatedVertexView.scala:114
          |  CheckpointRDD[57] at apply at List.scala:318
       |  ShuffledRDD[88] at ShuffledRDD at MessageToPartition.scala:31
       +-(2) GraphImpl.mapReduceTriplets - preAgg MapPartitionsRDD[87] at
mapPartitions at GraphImpl.scala:192
          |  EdgeRDD MapPartitionsRDD[84] at mapPartitions at
EdgeRDD.scala:85
          |  EdgeRDD ZippedPartitionsRDD2[76] at zipPartitions at
ReplicatedVertexView.scala:114
          |  EdgeRDD MapPartitionsRDD[63] at mapPartitions at
EdgeRDD.scala:85
          |  EdgeRDD ZippedPartitionsRDD2[53] at zipPartitions at
ReplicatedVertexView.scala:114
          |  CheckpointRDD[57] at apply at List.scala:318
          |  ShuffledRDD[75] at partitionBy at
ReplicatedVertexView.scala:112
          +-(2) ReplicatedVertexView.updateVertices - shippedVerts true
true (broadcast) MapPartitionsRDD[74] at mapPartitions at
VertexRDD.scala:347
             |  VertexRDD ZippedPartitionsRDD2[72] at zipPartitions at
VertexRDD.scala:174
             |  VertexRDD MapPartitionsRDD[61] at mapPartitions at
VertexRDD.scala:127
             |  VertexRDD ZippedPartitionsRDD2[47] at zipPartitions at
VertexRDD.scala:200
             |  CheckpointRDD[56] at apply at List.scala:318
             |  VertexRDD ZippedPartitionsRDD2[70] at zipPartitions at
VertexRDD.scala:200
             |  VertexRDD MapPartitionsRDD[61] at mapPartitions at
VertexRDD.scala:127
             |  VertexRDD ZippedPartitionsRDD2[47] at zipPartitions at
VertexRDD.scala:200
             |  CheckpointRDD[56] at apply at List.scala:318
             |  VertexRDD, GraphOps.degrees ZippedPartitionsRDD2[68] at
zipPartitions at VertexRDD.scala:301
             |  VertexRDD MapPartitionsRDD[61] at mapPartitions at
VertexRDD.scala:127
             |  VertexRDD ZippedPartitionsRDD2[47] at zipPartitions at
VertexRDD.scala:200
             |  CheckpointRDD[56] at apply at List.scala:318
             |  ShuffledRDD[67] at ShuffledRDD at
MessageToPartition.scala:31
             +-(2) GraphImpl.mapReduceTriplets - preAgg
MapPartitionsRDD[66] at mapPartitions at GraphImpl.scala:192
                |  EdgeRDD MapPartitionsRDD[63] at mapPartitions at
EdgeRDD.scala:85
                |  EdgeRDD ZippedPartitionsRDD2[53] at zipPartitions at
ReplicatedVertexView.scala:114
                |  CheckpointRDD[57] at apply at List.scala:318
"
Xu Lijie <lijie.xlj@gmail.com>,"Tue, 11 Nov 2014 11:40:48 +0800",Re: Checkpoint bugs in GraphX,GuoQiang Li <witgo@qq.com>,"Nice, we currently encounter a stackoverflow error caused by this bug.

We also found that ""val partitionsRDD: RDD[(PartitionID, EdgePartition[ED,
VD])],
val targetStorageLevel: StorageLevel = StorageLevel.MEMORY_ONLY)"" will not
be serialized even without adding @transient.

However, transient can affect the JVM stack. Our guess is that:

If we do not add @transient, the pointers of ""partitionsRDD"" and
""targetStorageLevel""
will be kept in the stack.
Or else, the stack will not keep any information of the two variables
during serialization/deserialization.

I'm wondering whether the guess is right.

2014-11-11 11:16 GMT+08:00 GuoQiang Li <witgo@qq.com>:

:
nt
{
on
"
Jeniba Johnson <Jeniba.Johnson@lntinfotech.com>,"Tue, 11 Nov 2014 10:59:41 +0530",RE: Bind exception while running FlumeEventCount,Hari Shreedharan <hshreedharan@cloudera.com>,"Hi Hari

Thanks for your kind reply

Even after killing the process id  of the specific port. Still Iam facing with the similar error.
The command I use is

sudo lsof -i -P | grep -i ""listen""

Kill -9 PID

However If I try to work with the port which is available, still the error remains the same.


Regards,
Jeniba Johnson

From: Hari Shreedharan [mailto:hshreedharan@cloudera.com]
Sent: Tuesday, November 11, 2014 4:41 AM
To: Jeniba Johnson
Cc: dev@spark.apache.org
Subject: Re: Bind exception while running FlumeEventCount

Looks like that port is not available because another app is using that port. Can you take a look at netstat -a and use a port that is free?

Thanks,
Hari


On Fri, Nov 7, 2014 at 2:05 PM, Jeniba Johnson <Jeniba.Johnson@lntinfotech.com<mailto:Jeniba.Johnson@lntinfotech.com>> wrote:

Hi,

I have installed spark-1.1.0 and apache flume 1.4 for running streaming example FlumeEventCount. Previously the code was working fine. Now Iam facing with the below mentioned issues. My flume is running properly it is able to write the file.

The command I use is

bin/run-example org.apache.spark.examples.streaming.FlumeEventCount 172.29.17.178 65001


14/11/07 23:19:23 INFO receiver.ReceiverSupervisorImpl: Stopping receiver with message: Error starting receiver 0: org.jboss.netty.channel.ChannelException: Failed to bind to: /172.29.17.178:65001
14/11/07 23:19:23 INFO flume.FlumeReceiver: Flume receiver stopped
14/11/07 23:19:23 INFO receiver.ReceiverSupervisorImpl: Called receiver onStop
14/11/07 23:19:23 INFO receiver.ReceiverSupervisorImpl: Deregistering receiver 0
14/11/07 23:19:23 ERROR scheduler.ReceiverTracker: Deregistered receiver for stream 0: Error starting receiver 0 - org.jboss.netty.channel.ChannelException: Failed to bind to: /172.29.17.178:65001
at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272)
at org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:106)
at org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:119)
at org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:74)
at org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:68)
at org.apache.spark.streaming.flume.FlumeReceiver.initServer(FlumeInputDStream.scala:164)
at org.apache.spark.streaming.flume.FlumeReceiver.onStart(FlumeInputDStream.scala:171)
at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:121)
at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:106)
at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverLauncher$$anonfun$9.apply(ReceiverTracker.scala:264)
at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverLauncher$$anonfun$9.apply(ReceiverTracker.scala:257)
at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1121)
at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1121)
at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
at org.apache.spark.scheduler.Task.run(Task.scala:54)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:722)
Caused by: java.net.BindException: Address already in use
at sun.nio.ch.Net.bind0(Native Method)
at sun.nio.ch.Net.bind(Net.java:344)
at sun.nio.ch.Net.bind(Net.java:336)
at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:199)
at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
at org.jboss.netty.channel.socket.nio.NioServerBoss$RegisterTask.run(NioServerBoss.java:193)
at org.jboss.netty.channel.socket.nio.AbstractNioSelector.processTaskQueue(AbstractNioSelector.java:366)
at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:290)
at org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)
... 3 more

14/11/07 23:19:23 INFO receiver.ReceiverSupervisorImpl: Stopped receiver 0
14/11/07 23:19:23 INFO receiver.BlockGenerator: Stopping BlockGenerator
14/11/07 23:19:23 INFO util.RecurringTimer: Stopped timer for BlockGenerator after time 1415382563200
14/11/07 23:19:23 INFO receiver.BlockGenerator: Waiting for block pushing thread
14/11/07 23:19:23 INFO receiver.BlockGenerator: Pushing out the last 0 blocks
14/11/07 23:19:23 INFO receiver.BlockGenerator: Stopped block pushing thread
14/11/07 23:19:23 INFO receiver.BlockGenerator: Stopped BlockGenerator
14/11/07 23:19:23 INFO receiver.ReceiverSupervisorImpl: Waiting for executor stop is over
14/11/07 23:19:23 ERROR receiver.ReceiverSupervisorImpl: Stopped executor with error: org.jboss.netty.channel.ChannelException: Failed to bind to: /172.29.17.178:65001
14/11/07 23:19:23 ERROR executor.Executor: Exception in task 0.0 in stage 0.0 (TID 0)
org.jboss.netty.channel.ChannelException: Failed to bind to: /172.29.17.178:65001
at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272)
at org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:106)
at org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:119)
at org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:74)
at org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:68)
at org.apache.spark.streaming.flume.FlumeReceiver.initServer(FlumeInputDStream.scala:164)
at org.apache.spark.streaming.flume.FlumeReceiver.onStart(FlumeInputDStream.scala:171)
at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:121)
at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:106)
at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverLauncher$$anonfun$9.apply(ReceiverTracker.scala:264)
at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverLauncher$$anonfun$9.apply(ReceiverTracker.scala:257)
at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1121)
at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1121)
at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
at org.apache.spark.scheduler.Task.run(Task.scala:54)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:722)
Caused by: java.net.BindException: Address already in use
at sun.nio.ch.Net.bind0(Native Method)
at sun.nio.ch.Net.bind(Net.java:344)
at sun.nio.ch.Net.bind(Net.java:336)
at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:199)
at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
at org.jboss.netty.channel.socket.nio.NioServerBoss$RegisterTask.run(NioServerBoss.java:193)
at org.jboss.netty.channel.socket.nio.AbstractNioSelector.processTaskQueue(AbstractNioSelector.java:366)
at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:290)
at org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)
... 3 more
14/11/07 23:19:23 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, localhost): org.jboss.netty.channel.ChannelException: Failed to bind to: /172.29.17.178:65001
org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272)
org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:106)
org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:119)
org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:74)
org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:68)
org.apache.spark.streaming.flume.FlumeReceiver.initServer(FlumeInputDStream.scala:164)
org.apache.spark.streaming.flume.FlumeReceiver.onStart(FlumeInputDStream.scala:171)
org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:121)
org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:106)
org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverLauncher$$anonfun$9.apply(ReceiverTracker.scala:264)
org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverLauncher$$anonfun$9.apply(ReceiverTracker.scala:257)
org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1121)
org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1121)
org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
org.apache.spark.scheduler.Task.run(Task.scala:54)
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
java.lang.Thread.run(Thread.java:722)
14/11/07 23:19:23 ERROR scheduler.TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job
14/11/07 23:19:23 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
14/11/07 23:19:23 INFO scheduler.TaskSchedulerImpl: Cancelling stage 0
14/11/07 23:19:23 INFO scheduler.DAGScheduler: Failed to run runJob at ReceiverTracker.scala:275
Exception in thread ""Thread-28"" org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): org.jboss.netty.channel.ChannelException: Failed to bind to: /172.29.17.178:65001
org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272)
org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:106)
org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:119)
org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:74)
org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:68)
org.apache.spark.streaming.flume.FlumeReceiver.initServer(FlumeInputDStream.scala:164)
org.apache.spark.streaming.flume.FlumeReceiver.onStart(FlumeInputDStream.scala:171)
org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:121)
org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:106)
org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverLauncher$$anonfun$9.apply(ReceiverTracker.scala:264)
org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverLauncher$$anonfun$9.apply(ReceiverTracker.scala:257)
org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1121)
org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1121)
org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
org.apache.spark.scheduler.Task.run(Task.scala:54)
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
java.lang.Thread.run(Thread.java:722)
Driver stacktrace:
at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1185)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1174)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1173)
at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1173)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:688)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:688)
at scala.Option.foreach(Option.scala:236)
at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:688)
at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1391)
at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
at akka.actor.ActorCell.invoke(ActorCell.scala:456)
at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
at akka.dispatch.Mailbox.run(Mailbox.scala:219)
at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

Regards,
Jeniba Johnson


________________________________
The contents of this e-mail and any attachment(s) may contain confidential or privileged information for the intended recipient(s). Unintended recipients are prohibited from taking action on the basis of information in this e-mail and using or disseminating the information, and must notify the sender and delete it from their system. L&T Infotech will not accept responsibility or liability for the accuracy or completeness of, or the presence of any virus or disabling code in this e-mail""

"
"""Hari Shreedharan"" <hshreedharan@cloudera.com>","Mon, 10 Nov 2014 21:34:10 -0800 (PST)",RE: Bind exception while running FlumeEventCount,"""Jeniba Johnson"" <jeniba.johnson@lntinfotech.com>","The socket may have been in TIME_WAIT. Can you try after a bit? The error message definitely suggests that some other app is listening on that port.


Thanks,
Hari


 with the similar error.
error remains the same.
port. Can you take a look at netstat -a and use a port that is free?
example FlumeEventCount. Previously the code was working fine. Now Iam facing with the below mentioned issues. My flume is running properly it is able to write the file.
29.17.178 65001
 with message: Error starting receiver 0: org.jboss.netty.channel.ChannelException: Failed to bind to: /172.29.17.178:65001
onStop
receiver 0
for stream 0: Error starting receiver 0 - org.jboss.netty.channel.ChannelException: Failed to bind to: /172.29.17.178:65001
java:272)
initServer(FlumeInputDStream.scala:164)
onStart(FlumeInputDStream.scala:171)
startReceiver(ReceiverSupervisor.scala:121)
start(ReceiverSupervisor.scala:106)
$anonfun$9.apply(ReceiverTracker.scala:264)
$anonfun$9.apply(ReceiverTracker.scala:257)
scala:1121)
scala:1121)
java:1145)
java:615)
java:199)
run(NioServerBoss.java:193)
processTaskQueue(AbstractNioSelector.java:366)
run(AbstractNioSelector.java:290)
java:42)
0
BlockGenerator after time 1415382563200
 thread
blocks
thread
executor stop is over
 with error: org.jboss.netty.channel.ChannelException: Failed to bind to: /172.29.17.178:65001
 0.0 (TID 0)
178:65001
java:272)
initServer(FlumeInputDStream.scala:164)
onStart(FlumeInputDStream.scala:171)
startReceiver(ReceiverSupervisor.scala:121)
start(ReceiverSupervisor.scala:106)
$anonfun$9.apply(ReceiverTracker.scala:264)
$anonfun$9.apply(ReceiverTracker.scala:257)
scala:1121)
scala:1121)
java:1145)
java:615)
java:199)
run(NioServerBoss.java:193)
processTaskQueue(AbstractNioSelector.java:366)
run(AbstractNioSelector.java:290)
java:42)
.0 (TID 0, localhost): org.jboss.netty.channel.ChannelException: Failed to bind to: /172.29.17.178:65001
am.scala:164)
scala:171)
startReceiver(ReceiverSupervisor.scala:121)
start(ReceiverSupervisor.scala:106)
onfun$9.apply(ReceiverTracker.scala:264)
onfun$9.apply(ReceiverTracker.scala:257)
scala:1121)
scala:1121)
java:1145)
java:615)
failed 1 times; aborting job
whose tasks have all completed, from pool
ReceiverTracker.scala:275
aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): org.jboss.netty.channel.ChannelException: Failed to bind to: /172.29.17.178:65001
am.scala:164)
scala:171)
startReceiver(ReceiverSupervisor.scala:121)
start(ReceiverSupervisor.scala:106)
onfun$9.apply(ReceiverTracker.scala:264)
onfun$9.apply(ReceiverTracker.scala:257)
scala:1121)
scala:1121)
java:1145)
java:615)
Scheduler$$failJobAndIndependentStages(DAGScheduler.scala:1185)
apply(DAGScheduler.scala:1174)
apply(DAGScheduler.scala:1173)
scala:59)
scala:1173)
.apply(DAGScheduler.scala:688)
.apply(DAGScheduler.scala:688)
er.scala:688)
ive$2.applyOrElse(DAGScheduler.scala:1391)
exec(AbstractDispatcher.scala:386)
java:1339)
java:1979)
run(ForkJoinWorkerThread.java:107)
________
confidential or privileged information for the intended recipient(s). Unintended recipients are prohibited from taking action on the basis of information in this e-mail and using or disseminating the information, and must notify the sender and delete it from their system. L&T Infotech will not accept responsibility or liability for the accuracy or completeness of, or the presence of any virus or disabling code in this e-mail"""
"""=?utf-8?B?R3VvUWlhbmcgTGk=?="" <witgo@qq.com>","Tue, 11 Nov 2014 13:56:50 +0800",Re:  Checkpoint bugs in GraphX,"""=?utf-8?B?WHUgTGlqaWU=?="" <lijie.xlj@gmail.com>","Many methods are not required serialization EdgeRDD or VertexRDD(eg: graph.edges.â€â€countâ€), moreover , partitionsRDD(or targetStorageLevelâ€) need only in the driver. partitionsRDD (or targetStorageLevel) â€is not serialized no effect.
â€




------------------ Original ------------------
From:  ""Xu Lijie"";<lijie.xlj@gmail.com>;
Date:  Tue, Nov 11, 2014 11:40 AM
To:  ""GuoQiang Li""<witgo@qq.com>; 
Cc:  ""user""<user@spark.apache.org>; ""dev""<dev@spark.apache.org>; 
Subject:  Re: Checkpoint bugs in GraphX



Nice, we currently encounter a stackoverflow error caused by this bug.

We also found that ""val partitionsRDD: RDD[(PartitionID, EdgePartition[ED,
VD])],
val targetStorageLevel: StorageLevel = StorageLevel.MEMORY_ONLY)"" will not
be serialized even without adding @transient.

However, transient can affect the JVM stack. Our guess is that:

If we do not add @transient, the pointers of ""partitionsRDD"" and
""targetStorageLevel""
will be kept in the stack.
Or else, the stack will not keep any information of the two variables
during serialization/deserialization.

I'm wondering whether the guess is right.

2014-11-11 11:16 GMT+08:00 GuoQiang Li <witgo@qq.com>:

> I have been trying to fix this bug.â€
> The related PR:
> https://github.com/apache/spark/pull/2631â€
>
> ------------------ Original ------------------
> *From: * ""Xu Lijie"";<lijie.xlj@gmail.com>;
> *Date: * Tue, Nov 11, 2014 10:19 AM
> *To: * ""user""<user@spark.apache.org>; ""dev""<dev@spark.apache.org>;
> *Subject: * Checkpoint bugs in GraphX
>
> Hi, all. I'm not sure whether someone has reported this bug:
>
>
> There should be a checkpoint() method in EdgeRDD and VertexRDD as follows:
>
> override def checkpoint(): Unit = { partitionsRDD.checkpoint() }
>
>
> Current EdgeRDD and VertexRDD use *RDD.checkpoint()*, which only checkpoint
> the edges/vertices but not the critical partitionsRDD.
>
>
> Also, the variables (partitionsRDD and targetStroageLevel) in EdgeRDD and
> VertexRDD should be transient.
>
> class EdgeRDD[@specialized ED: ClassTag, VD: ClassTag]( @transient val
> partitionsRDD: RDD[(PartitionID, EdgePartition[ED, VD])], @transient val
> targetStorageLevel: StorageLevel = StorageLevel.MEMORY_ONLY) extends
> RDD[Edge[ED]](partitionsRDD.context, List(new
> OneToOneDependency(partitionsRDD))) {
>
>
> class VertexRDD[@specialized VD: ClassTag]( @transient val partitionsRDD:
> RDD[ShippableVertexPartition[VD]], @transient val targetStorageLevel:
> StorageLevel = StorageLevel.MEMORY_ONLY) extends RDD[(VertexId,
> VD)](partitionsRDD.context, List(new OneToOneDependency(partitionsRDD))) {
>
>
> These two bugs usually lead to stackoverflow error in iterative application
> written by GraphX.
>
>"
Jeniba Johnson <Jeniba.Johnson@lntinfotech.com>,"Tue, 11 Nov 2014 12:37:05 +0530",RE: Bind exception while running FlumeEventCount,Hari Shreedharan <hshreedharan@cloudera.com>,"Hi Hari

Just to give you a background , I had  installed spark-1.1.0 and apache flume 1.4 with basic configurations as needed. I just wanted to know that
Is this the correct way for running Spark streaming examples with Flume.

So  As you had mentioned about the TIME_WAIT parameter, did not get exactly.. Iam attaching the screenshot ,so that you can help me with it
The screenshot specify the ports listening after the program is executed


Regards,
Jeniba Johnson

-----Original Message-----
From: Hari Shreedharan [mailto:hshreedharan@cloudera.com]
Sent: Tuesday, November 11, 2014 11:04 AM
To: Jeniba Johnson
Cc: dev@spark.apache.org
Subject: RE: Bind exception while running FlumeEventCount

The socket may have been in TIME_WAIT. Can you try after a bit? The error message definitely suggests that some other app is listening on that port.


Thanks,
Hari

On Mon, Nov 10, 2014 at 9:30 PM, Jeniba Johnson <Jeniba.Johnson@lntinfotech.com> wrote:

> Hi Hari
> Thanks for your kind reply
> Even after killing the process id  of the specific port. Still Iam facing with the similar error.
> The command I use is
> sudo lsof -i -P | grep -i ""listen""
> Kill -9 PID
> However If I try to work with the port which is available, still the error remains the same.
> Regards,
> Jeniba Johnson
> From: Hari Shreedharan [mailto:hshreedharan@cloudera.com]
> Sent: Tuesday, November 11, 2014 4:41 AM
> To: Jeniba Johnson
> Cc: dev@spark.apache.org
> Subject: Re: Bind exception while running FlumeEventCount Looks like
> that port is not available because another app is using that port. Can you take a look at netstat -a and use a port that is free?
> Thanks,
> Hari
> On Fri, Nov 7, 2014 at 2:05 PM, Jeniba Johnson <Jeniba.Johnson@lntinfotech.com<mailto:Jeniba.Johnson@lntinfotech.com>> wrote:
> Hi,
> I have installed spark-1.1.0 and apache flume 1.4 for running streaming example FlumeEventCount. Previously the code was working fine. Now Iam facing with the below mentioned issues. My flume is running properly it is able to write the file.
> The command I use is
> bin/run-example org.apache.spark.examples.streaming.FlumeEventCount
> 172.29.17.178 65001
> 14/11/07 23:19:23 INFO receiver.ReceiverSupervisorImpl: Stopping
> receiver with message: Error starting receiver 0:
> org.jboss.netty.channel.ChannelException: Failed to bind to:
> /172.29.17.178:65001
> 14/11/07 23:19:23 INFO flume.FlumeReceiver: Flume receiver stopped
> 14/11/07 23:19:23 INFO receiver.ReceiverSupervisorImpl: Called
> receiver onStop
> 14/11/07 23:19:23 INFO receiver.ReceiverSupervisorImpl: Deregistering
> receiver 0
> 14/11/07 23:19:23 ERROR scheduler.ReceiverTracker: Deregistered
> receiver for stream 0: Error starting receiver 0 -
> org.jboss.netty.channel.ChannelException: Failed to bind to:
> /172.29.17.178:65001 at
> org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:27
> 2) at org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:106)
> at org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:119)
> at org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:74)
> at org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:68)
> at
> org.apache.spark.streaming.flume.FlumeReceiver.initServer(FlumeInputDS
> tream.scala:164) at
> org.apache.spark.streaming.flume.FlumeReceiver.onStart(FlumeInputDStre
> am.scala:171) at
> org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(R
> eceiverSupervisor.scala:121) at
> org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverS
> upervisor.scala:106) at
> org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverLauncher$
> $anonfun$9.apply(ReceiverTracker.scala:264)
> at
> org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverLauncher$
> $anonfun$9.apply(ReceiverTracker.scala:257)
> at
> org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.sca
> la:1121) at
> org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.sca
> la:1121) at
> org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
> at org.apache.spark.scheduler.Task.run(Task.scala:54)
> at
> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
> at
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.j
> ava:1145) at
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.
> java:615) at java.lang.Thread.run(Thread.java:722)
> Caused by: java.net.BindException: Address already in use at
> sun.nio.ch.Net.bind0(Native Method) at
> sun.nio.ch.Net.bind(Net.java:344) at sun.nio.ch.Net.bind(Net.java:336)
> at
> sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:1
> 99) at
> sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
> at
> org.jboss.netty.channel.socket.nio.NioServerBoss$RegisterTask.run(NioS
> erverBoss.java:193) at
> org.jboss.netty.channel.socket.nio.AbstractNioSelector.processTaskQueu
> e(AbstractNioSelector.java:366) at
> org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNio
> Selector.java:290) at
> org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.jav
> a:42)
> ... 3 more
> 14/11/07 23:19:23 INFO receiver.ReceiverSupervisorImpl: Stopped
> receiver 0
> 14/11/07 23:19:23 INFO receiver.BlockGenerator: Stopping
> BlockGenerator
> 14/11/07 23:19:23 INFO util.RecurringTimer: Stopped timer for
> BlockGenerator after time 1415382563200
> 14/11/07 23:19:23 INFO receiver.BlockGenerator: Waiting for block
> pushing thread
> 14/11/07 23:19:23 INFO receiver.BlockGenerator: Pushing out the last 0
> blocks
> 14/11/07 23:19:23 INFO receiver.BlockGenerator: Stopped block pushing
> thread
> 14/11/07 23:19:23 INFO receiver.BlockGenerator: Stopped BlockGenerator
> 14/11/07 23:19:23 INFO receiver.ReceiverSupervisorImpl: Waiting for
> executor stop is over
> 14/11/07 23:19:23 ERROR receiver.ReceiverSupervisorImpl: Stopped
> executor with error: org.jboss.netty.channel.ChannelException: Failed
> to bind to: /172.29.17.178:65001
> 14/11/07 23:19:23 ERROR executor.Executor: Exception in task 0.0 in
> stage 0.0 (TID 0)
> org.jboss.netty.channel.ChannelException: Failed to bind to:
> /172.29.17.178:65001 at
> org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:27
> 2) at org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:106)
> at org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:119)
> at org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:74)
> at org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:68)
> at
> org.apache.spark.streaming.flume.FlumeReceiver.initServer(FlumeInputDS
> tream.scala:164) at
> org.apache.spark.streaming.flume.FlumeReceiver.onStart(FlumeInputDStre
> am.scala:171) at
> org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(R
> eceiverSupervisor.scala:121) at
> org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverS
> upervisor.scala:106) at
> org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverLauncher$
> $anonfun$9.apply(ReceiverTracker.scala:264)
> at
> org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverLauncher$
> $anonfun$9.apply(ReceiverTracker.scala:257)
> at
> org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.sca
> la:1121) at
> org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.sca
> la:1121) at
> org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
> at org.apache.spark.scheduler.Task.run(Task.scala:54)
> at
> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
> at
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.j
> ava:1145) at
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.
> java:615) at java.lang.Thread.run(Thread.java:722)
> Caused by: java.net.BindException: Address already in use at
> sun.nio.ch.Net.bind0(Native Method) at
> sun.nio.ch.Net.bind(Net.java:344) at sun.nio.ch.Net.bind(Net.java:336)
> at
> sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:1
> 99) at
> sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
> at
> org.jboss.netty.channel.socket.nio.NioServerBoss$RegisterTask.run(NioS
> erverBoss.java:193) at
> org.jboss.netty.channel.socket.nio.AbstractNioSelector.processTaskQueu
> e(AbstractNioSelector.java:366) at
> org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNio
> Selector.java:290) at
> org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.jav
> a:42)
> ... 3 more
> 14/11/07 23:19:23 WARN scheduler.TaskSetManager: Lost task 0.0 in
> stage 0.0 (TID 0, localhost):
> org.jboss.netty.channel.ChannelException: Failed to bind to:
> /172.29.17.178:65001
> org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:27
> 2)
> org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:106)
> org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:119)
> org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:74)
> org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:68)
> org.apache.spark.streaming.flume.FlumeReceiver.initServer(FlumeInputDS
> tream.scala:164)
> org.apache.spark.streaming.flume.FlumeReceiver.onStart(FlumeInputDStre
> am.scala:171)
> org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(R
> eceiverSupervisor.scala:121)
> org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverS
> upervisor.scala:106)
> org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverLauncher$
> $anonfun$9.apply(ReceiverTracker.scala:264)
> org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverLauncher$
> $anonfun$9.apply(ReceiverTracker.scala:257)
> org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.sca
> la:1121)
> org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.sca
> la:1121)
> org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
> org.apache.spark.scheduler.Task.run(Task.scala:54)
> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.j
> ava:1145)
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.
> java:615)
> java.lang.Thread.run(Thread.java:722)
> 14/11/07 23:19:23 ERROR scheduler.TaskSetManager: Task 0 in stage 0.0
> failed 1 times; aborting job
> 14/11/07 23:19:23 INFO scheduler.TaskSchedulerImpl: Removed TaskSet
> 0.0, whose tasks have all completed, from pool
> 14/11/07 23:19:23 INFO scheduler.TaskSchedulerImpl: Cancelling stage 0
> 14/11/07 23:19:23 INFO scheduler.DAGScheduler: Failed to run runJob at
> ReceiverTracker.scala:275 Exception in thread ""Thread-28""
> org.apache.spark.SparkException: Job aborted due to stage failure:
> Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0
> in stage 0.0 (TID 0, localhost):
> org.jboss.netty.channel.ChannelException: Failed to bind to:
> /172.29.17.178:65001
> org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:27
> 2)
> org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:106)
> org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:119)
> org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:74)
> org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:68)
> org.apache.spark.streaming.flume.FlumeReceiver.initServer(FlumeInputDS
> tream.scala:164)
> org.apache.spark.streaming.flume.FlumeReceiver.onStart(FlumeInputDStre
> am.scala:171)
> org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(R
> eceiverSupervisor.scala:121)
> org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverS
> upervisor.scala:106)
> org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverLauncher$
> $anonfun$9.apply(ReceiverTracker.scala:264)
> org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverLauncher$
> $anonfun$9.apply(ReceiverTracker.scala:257)
> org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.sca
> la:1121)
> org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.sca
> la:1121)
> org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
> org.apache.spark.scheduler.Task.run(Task.scala:54)
> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.j
> ava:1145)
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.
> java:615)
> java.lang.Thread.run(Thread.java:722)
> Driver stacktrace:
> at
> org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAG
> Scheduler$$failJobAndIndependentStages(DAGScheduler.scala:1185)
> at
> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DA
> GScheduler.scala:1174) at
> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DA
> GScheduler.scala:1173) at
> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.s
> cala:59) at
> scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
> at
> org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:
> 1173) at
> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1
> .apply(DAGScheduler.scala:688) at
> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1
> .apply(DAGScheduler.scala:688) at
> scala.Option.foreach(Option.scala:236)
> at
> org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGSchedul
> er.scala:688) at
> org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$rece
> ive$2.applyOrElse(DAGScheduler.scala:1391)
> at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
> at akka.actor.ActorCell.invoke(ActorCell.scala:456)
> at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
> at akka.dispatch.Mailbox.run(Mailbox.scala:219)
> at
> akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(Abstr
> actDispatcher.scala:386) at
> scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
> at
> scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.
> java:1339) at
> scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:197
> 9) at
> scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThrea
> d.java:107)
> Regards,
> Jeniba Johnson
> ________________________________
> The contents of this e-mail and any attachment(s) may contain confidential or privileged information for the intended recipient(s). Unintended recipients are prohibited from taking action on the basis of information in this e-mail and using or disseminating the information, and must notify the sender and delete it from their system. L&T Infotech will not accept responsibility or liability for the accuracy or completeness of, or the presence of any virus or disabling code in this e-mail""

---------------------------------------------------------------------"
"""Hari Shreedharan"" <hshreedharan@cloudera.com>","Mon, 10 Nov 2014 23:11:22 -0800 (PST)",RE: Bind exception while running FlumeEventCount,"""Jeniba Johnson"" <jeniba.johnson@lntinfotech.com>","First, can you try a different port?




TIME_WAIT is basically a timeout for a socket to be completely a few minutes and if you still see a startup issue, can you also send the error logs? From what I can see, the port seems to be in use.


Thanks,
Hari"
"""Hari Shreedharan"" <hshreedharan@cloudera.com>","Mon, 10 Nov 2014 23:36:29 -0800 (PST)",Re: Bind exception while running FlumeEventCount,"""Jeniba Johnson"" <jeniba.johnson@lntinfotech.com>","Did you start a Flume agent to push data to the relevant port?


Thanks,
Hari


 example FlumeEventCount. Previously the code was working fine. Now Iam facing with the below mentioned issues. My flume is running properly it is able to write the file.
29.17.178  65001
 with message: Error starting receiver 0: org.jboss.netty.channel.ChannelException: Failed to bind to: /172.29.17.178:65001
onStop
receiver 0
for stream 0: Error starting receiver 0 - org.jboss.netty.channel.ChannelException: Failed to bind to: /172.29.17.178:65001
.java:272)
initServer(FlumeInputDStream.scala:164)
onStart(FlumeInputDStream.scala:171)
startReceiver(ReceiverSupervisor.scala:121)
start(ReceiverSupervisor.scala:106)
auncher$$anonfun$9.apply(ReceiverTracker.scala:264)
auncher$$anonfun$9.apply(ReceiverTracker.scala:257)
apply(SparkContext.scala:1121)
apply(SparkContext.scala:1121)
scala:62)
scala:177)
runWorker(ThreadPoolExecutor.java:1145)
run(ThreadPoolExecutor.java:615)
l.java:199)
java:74)
run(NioServerBoss.java:193)
processTaskQueue(AbstractNioSelector.java:366)
run(AbstractNioSelector.java:290)
run(NioServerBoss.java:42)
0
BlockGenerator after time 1415382563200
 thread
blocks
thread
executor stop is over
 with error: org.jboss.netty.channel.ChannelException: Failed to bind to: /172.29.17.178:65001
 0.0 (TID 0)
178:65001
.java:272)
initServer(FlumeInputDStream.scala:164)
onStart(FlumeInputDStream.scala:171)
startReceiver(ReceiverSupervisor.scala:121)
start(ReceiverSupervisor.scala:106)
auncher$$anonfun$9.apply(ReceiverTracker.scala:264)
auncher$$anonfun$9.apply(ReceiverTracker.scala:257)
apply(SparkContext.scala:1121)
apply(SparkContext.scala:1121)
scala:62)
scala:177)
runWorker(ThreadPoolExecutor.java:1145)
run(ThreadPoolExecutor.java:615)
l.java:199)
java:74)
run(NioServerBoss.java:193)
processTaskQueue(AbstractNioSelector.java:366)
run(AbstractNioSelector.java:290)
run(NioServerBoss.java:42)
.0 (TID 0, localhost): org.jboss.netty.channel.ChannelException: Failed to bind to: /172.29.17.178:65001
java:272)
initServer(FlumeInputDStream.scala:164)
onStart(FlumeInputDStream.scala:171)
startReceiver(ReceiverSupervisor.scala:121)
start(ReceiverSupervisor.scala:106)
cher$$anonfun$9.apply(ReceiverTracker.scala:264)
cher$$anonfun$9.apply(ReceiverTracker.scala:257)
apply(SparkContext.scala:1121)
apply(SparkContext.scala:1121)
scala:62)
scala:177)
tor.java:1145)
run(ThreadPoolExecutor.java:615)
failed 1 times; aborting job
whose tasks have all completed, from pool
ReceiverTracker.scala:275
aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): org.jboss.netty.channel.ChannelException: Failed to bind to: /172.29.17.178:65001
java:272)
initServer(FlumeInputDStream.scala:164)
onStart(FlumeInputDStream.scala:171)
startReceiver(ReceiverSupervisor.scala:121)
start(ReceiverSupervisor.scala:106)
cher$$anonfun$9.apply(ReceiverTracker.scala:264)
cher$$anonfun$9.apply(ReceiverTracker.scala:257)
apply(SparkContext.scala:1121)
apply(SparkContext.scala:1121)
scala:62)
scala:177)
tor.java:1145)
run(ThreadPoolExecutor.java:615)
org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1185)
apply(DAGScheduler.scala:1174)
apply(DAGScheduler.scala:1173)
foreach(ResizableArray.scala:59)
scala:47)
abortStage(DAGScheduler.scala:1173)
Failed$1.apply(DAGScheduler.scala:688)
Failed$1.apply(DAGScheduler.scala:688)
handleTaskSetFailed(DAGScheduler.scala:688)
fun$receive$2.applyOrElse(DAGScheduler.scala:1391)
exec(AbstractDispatcher.scala:386)
java:260)
runTask(ForkJoinPool.java:1339)
java:1979)
run(ForkJoinWorkerThread.java:107)
________
confidential or privileged information for the intended recipient(s). Unintended recipients are prohibited from taking action on the basis of information in this e-mail and using or disseminating the information, and must notify the sender and delete it from their system. L&T Infotech will not accept responsibility or liability for the accuracy or completeness of, or the presence of any virus or disabling code in this e-mail"""
Jeniba Johnson <Jeniba.Johnson@lntinfotech.com>,"Tue, 11 Nov 2014 13:02:59 +0530",RE: Bind exception while running FlumeEventCount,Hari Shreedharan <hshreedharan@cloudera.com>,"Hi Hari

Meanwhile Iam  trying out with different port. I need to confirm with you about the installation for Spark and Flume.
For installation, I have  just unzipped spark-1.1.0-bin-hadoop1.tar.gz and  apache-flume-1.4.0-bin.tar.gz for running spark streaming examples.
Is this the correct way or else Is there any other way, then just let me know.

Awaiting for your kind reply.

Regards,
Jeniba Johnson
From: Hari Shreedharan [mailto:hshreedharan@cloudera.com]
Sent: Tuesday, November 11, 2014 12:41 PM
To: Jeniba Johnson
Cc: dev@spark.apache.org
Subject: RE: Bind exception while running FlumeEventCount

First, can you try a different port?

TIME_WAIT is basically a timeout for a socket to be completely decommissioned for the port to be available for binding. Once you wait for a few minutes and if you still see a startup issue, can you also send the error logs? From what I can see, the port seems to be in use.

Thanks,
Hari


On Mon, Nov 10, 2014 at 11:07 PM, Jeniba Johnson <Jeniba.Johnson@lntinfotech.com<mailto:Jeniba.Johnson@lntinfotech.com>> wrote:

Hi Hari

Just to give you a background , I had installed spark-1.1.0 and apache flume 1.4 with basic configurations as needed. I just wanted to know that
Is this the correct way for running Spark streaming examples with Flume.

So As you had mentioned about the TIME_WAIT parameter, did not get exactly.. Iam attaching the screenshot ,so that you can help me with it
The screenshot specify the ports listening after the program is executed


Regards,
Jeniba Johnson

-----Original Message-----
From: Hari Shreedharan [mailto:hshreedharan@cloudera.com]
Sent: Tuesday, November 11, 2014 11:04 AM
To: Jeniba Johnson
Cc: dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: RE: Bind exception while running FlumeEventCount

The socket may have been in TIME_WAIT. Can you try after a bit? The error message definitely suggests that some other app is listening on that port.


Thanks,
Hari

On Mon, Nov 10, 2014 at 9:30 PM, Jeniba Johnson <Jeniba.Johnson@lntinfotech.com<mailto:Jeniba.Johnson@lntinfotech.com>> wrote:

> Hi Hari
> Thanks for your kind reply
> Even after killing the process id of the specific port. Still Iam facing with the similar error.
> The command I use is
> sudo lsof -i -P | grep -i ""listen""
> Kill -9 PID
> However If I try to work with the port which is available, still the error remains the same.
> Regards,
> Jeniba Johnson
> From: Hari Shreedharan [mailto:hshreedharan@cloudera.com]
> Sent: Tuesday, November 11, 2014 4:41 AM
> To: Jeniba Johnson
> Cc: dev@spark.apache.org<mailto:dev@spark.apache.org>
> Subject: Re: Bind exception while running FlumeEventCount Looks like
> that port is not available because another app is using that port. Can you take a look at netstat -a and use a port that is free?
> Thanks,
> Hari
> On Fri, Nov 7, 2014 at 2:05 PM, Jeniba Johnson <Jeniba.Johnson@lntinfotech.com<mailto:Jeniba.Johnson@lntinfotech.com<mailto:Jeniba.Johnson@lntinfotech.com%3cmailto:Jeniba.Johnson@lntinfotech.com>>> wrote:
> Hi,
> I have installed spark-1.1.0 and apache flume 1.4 for running streaming example FlumeEventCount. Previously the code was working fine. Now Iam facing with the below mentioned issues. My flume is running properly it is able to write the file.
> The command I use is
> bin/run-example org.apache.spark.examples.streaming.FlumeEventCount
> 172.29.17.178 65001
> 14/11/07 23:19:23 INFO receiver.ReceiverSupervisorImpl: Stopping
> receiver with message: Error starting receiver 0:
> org.jboss.netty.channel.ChannelException: Failed to bind to:
> /172.29.17.178:65001
> 14/11/07 23:19:23 INFO flume.FlumeReceiver: Flume receiver stopped
> 14/11/07 23:19:23 INFO receiver.ReceiverSupervisorImpl: Called
> receiver onStop
> 14/11/07 23:19:23 INFO receiver.ReceiverSupervisorImpl: Deregistering
> receiver 0
> 14/11/07 23:19:23 ERROR scheduler.ReceiverTracker: Deregistered
> receiver for stream 0: Error starting receiver 0 -
> org.jboss.netty.channel.ChannelException: Failed to bind to:
> /172.29.17.178:65001 at
> org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:27
> 2) at org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:106)
> at org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:119)
> at org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:74)
> at org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:68)
> at
> org.apache.spark.streaming.flume.FlumeReceiver.initServer(FlumeInputDS
> tream.scala:164) at
> org.apache.spark.streaming.flume.FlumeReceiver.onStart(FlumeInputDStre
> am.scala:171) at
> org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(R
> eceiverSupervisor.scala:121) at
> org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverS
> upervisor.scala:106) at
> org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverLauncher$
> $anonfun$9.apply(ReceiverTracker.scala:264)
> at
> org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverLauncher$
> $anonfun$9.apply(ReceiverTracker.scala:257)
> at
> org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.sca
> la:1121) at
> org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.sca
> la:1121) at
> org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
> at org.apache.spark.scheduler.Task.run(Task.scala:54)
> at
> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
> at
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.j
> ava:1145) at
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.
> java:615) at java.lang.Thread.run(Thread.java:722)
> Caused by: java.net.BindException: Address already in use at
> sun.nio.ch.Net.bind0(Native Method) at
> sun.nio.ch.Net.bind(Net.java:344) at sun.nio.ch.Net.bind(Net.java:336)
> at
> sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:1
> 99) at
> sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
> at
> org.jboss.netty.channel.socket.nio.NioServerBoss$RegisterTask.run(NioS
> erverBoss.java:193) at
> org.jboss.netty.channel.socket.nio.AbstractNioSelector.processTaskQueu
> e(AbstractNioSelector.java:366) at
> org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNio
> Selector.java:290) at
> org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.jav
> a:42)
> ... 3 more
> 14/11/07 23:19:23 INFO receiver.ReceiverSupervisorImpl: Stopped
> receiver 0
> 14/11/07 23:19:23 INFO receiver.BlockGenerator: Stopping
> BlockGenerator
> 14/11/07 23:19:23 INFO util.RecurringTimer: Stopped timer for
> BlockGenerator after time 1415382563200
> 14/11/07 23:19:23 INFO receiver.BlockGenerator: Waiting for block
> pushing thread
> 14/11/07 23:19:23 INFO receiver.BlockGenerator: Pushing out the last 0
> blocks
> 14/11/07 23:19:23 INFO receiver.BlockGenerator: Stopped block pushing
> thread
> 14/11/07 23:19:23 INFO receiver.BlockGenerator: Stopped BlockGenerator
> 14/11/07 23:19:23 INFO receiver.ReceiverSupervisorImpl: Waiting for
> executor stop is over
> 14/11/07 23:19:23 ERROR receiver.ReceiverSupervisorImpl: Stopped
> executor with error: org.jboss.netty.channel.ChannelException: Failed
> to bind to: /172.29.17.178:65001
> 14/11/07 23:19:23 ERROR executor.Executor: Exception in task 0.0 in
> stage 0.0 (TID 0)
> org.jboss.netty.channel.ChannelException: Failed to bind to:
> /172.29.17.178:65001 at
> org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:27
> 2) at org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:106)
> at org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:119)
> at org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:74)
> at org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:68)
> at
> org.apache.spark.streaming.flume.FlumeReceiver.initServer(FlumeInputDS
> tream.scala:164) at
> org.apache.spark.streaming.flume.FlumeReceiver.onStart(FlumeInputDStre
> am.scala:171) at
> org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(R
> eceiverSupervisor.scala:121) at
> org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverS
> upervisor.scala:106) at
> org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverLauncher$
> $anonfun$9.apply(ReceiverTracker.scala:264)
> at
> org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverLauncher$
> $anonfun$9.apply(ReceiverTracker.scala:257)
> at
> org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.sca
> la:1121) at
> org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.sca
> la:1121) at
> org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
> at org.apache.spark.scheduler.Task.run(Task.scala:54)
> at
> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
> at
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.j
> ava:1145) at
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.
> java:615) at java.lang.Thread.run(Thread.java:722)
> Caused by: java.net.BindException: Address already in use at
> sun.nio.ch.Net.bind0(Native Method) at
> sun.nio.ch.Net.bind(Net.java:344) at sun.nio.ch.Net.bind(Net.java:336)
> at
> sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:1
> 99) at
> sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
> at
> org.jboss.netty.channel.socket.nio.NioServerBoss$RegisterTask.run(NioS
> erverBoss.java:193) at
> org.jboss.netty.channel.socket.nio.AbstractNioSelector.processTaskQueu
> e(AbstractNioSelector.java:366) at
> org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNio
> Selector.java:290) at
> org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.jav
> a:42)
> ... 3 more
> 14/11/07 23:19:23 WARN scheduler.TaskSetManager: Lost task 0.0 in
> stage 0.0 (TID 0, localhost):
> org.jboss.netty.channel.ChannelException: Failed to bind to:
> /172.29.17.178:65001
> org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:27
> 2)
> org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:106)
> org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:119)
> org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:74)
> org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:68)
> org.apache.spark.streaming.flume.FlumeReceiver.initServer(FlumeInputDS
> tream.scala:164)
> org.apache.spark.streaming.flume.FlumeReceiver.onStart(FlumeInputDStre
> am.scala:171)
> org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(R
> eceiverSupervisor.scala:121)
> org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverS
> upervisor.scala:106)
> org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverLauncher$
> $anonfun$9.apply(ReceiverTracker.scala:264)
> org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverLauncher$
> $anonfun$9.apply(ReceiverTracker.scala:257)
> org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.sca
> la:1121)
> org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.sca
> la:1121)
> org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
> org.apache.spark.scheduler.Task.run(Task.scala:54)
> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.j
> ava:1145)
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.
> java:615)
> java.lang.Thread.run(Thread.java:722)
> 14/11/07 23:19:23 ERROR scheduler.TaskSetManager: Task 0 in stage 0.0
> failed 1 times; aborting job
> 14/11/07 23:19:23 INFO scheduler.TaskSchedulerImpl: Removed TaskSet
> 0.0, whose tasks have all completed, from pool
> 14/11/07 23:19:23 INFO scheduler.TaskSchedulerImpl: Cancelling stage 0
> 14/11/07 23:19:23 INFO scheduler.DAGScheduler: Failed to run runJob at
> ReceiverTracker.scala:275 Exception in thread ""Thread-28""
> org.apache.spark.SparkException: Job aborted due to stage failure:
> Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0
> in stage 0.0 (TID 0, localhost):
> org.jboss.netty.channel.ChannelException: Failed to bind to:
> /172.29.17.178:65001
> org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:27
> 2)
> org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:106)
> org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:119)
> org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:74)
> org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:68)
> org.apache.spark.streaming.flume.FlumeReceiver.initServer(FlumeInputDS
> tream.scala:164)
> org.apache.spark.streaming.flume.FlumeReceiver.onStart(FlumeInputDStre
> am.scala:171)
> org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(R
> eceiverSupervisor.scala:121)
> org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverS
> upervisor.scala:106)
> org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverLauncher$
> $anonfun$9.apply(ReceiverTracker.scala:264)
> org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverLauncher$
> $anonfun$9.apply(ReceiverTracker.scala:257)
> org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.sca
> la:1121)
> org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.sca
> la:1121)
> org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
> org.apache.spark.scheduler.Task.run(Task.scala:54)
> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.j
> ava:1145)
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.
> java:615)
> java.lang.Thread.run(Thread.java:722)
> Driver stacktrace:
> at
> org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAG
> Scheduler$$failJobAndIndependentStages(DAGScheduler.scala:1185)
> at
> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DA
> GScheduler.scala:1174) at
> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DA
> GScheduler.scala:1173) at
> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.s
> cala:59) at
> scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
> at
> org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:
> 1173) at
> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1
> .apply(DAGScheduler.scala:688) at
> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1
> .apply(DAGScheduler.scala:688) at
> scala.Option.foreach(Option.scala:236)
> at
> org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGSchedul
> er.scala:688) at
> org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$rece
> ive$2.applyOrElse(DAGScheduler.scala:1391)
> at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
> at akka.actor.ActorCell.invoke(ActorCell.scala:456)
> at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
> at akka.dispatch.Mailbox.run(Mailbox.scala:219)
> at
> akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(Abstr
> actDispatcher.scala:386) at
> scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
> at
> scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.
> java:1339) at
> scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:197
> 9) at
> scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThrea
> d.java:107)
> Regards,
> Jeniba Johnson
> ________________________________
> The contents of this e-mail and any attachment(s) may contain confidential or privileged information for the intended recipient(s). Unintended recipients are prohibited from taking action on the basis of information in this e-mail and using or disseminating the information, and must notify the sender and delete it from their system. L&T Infotech will not accept responsibility or liability for the accuracy or completeness of, or the presence of any virus or disabling code in this e-mail""
<port_status.png><Time_status.png><screenshot3.png>
<port_status.png><Time_status.png><screenshot3.png>

"
Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"Tue, 11 Nov 2014 01:16:27 -0700 (MST)",Re: [VOTE] Designating maintainers for some Spark components,dev@spark.incubator.apache.org,"+1 (binding) 


 
e 
o 
 
t 
 
, 
 
 
 
 ask them 
 
 
e 
 
 
 
e 
n 
 
ners+Guide
ners+Guide
 
 
k 
 
e 
 



-----
-- Yu Ishikawa
--
3.nabble.com/VOTE-Designating-maintainers-for-some-Spark-components-tp9115p9281.html
om.

------------------------------"
Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"Tue, 11 Nov 2014 01:26:11 -0700 (MST)",Re: JIRA + PR backlog,dev@spark.incubator.apache.org,"Great jobs!
I didn't know ""Spark PR Dashboard.""

Thanks
Yu Ishikawa



-----
-- Yu Ishikawa
--

---------------------------------------------------------------------


"
Jeniba Johnson <Jeniba.Johnson@lntinfotech.com>,"Tue, 11 Nov 2014 14:46:47 +0530",RE: Bind exception while running FlumeEventCount,Hari Shreedharan <hshreedharan@cloudera.com>,"Hi Hari

Yes I started Flume agent to push data to the relevant port. Below  mentioned are the conf files for flume configurations

Test21.conf

# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = avro
a1.sources.r1.bind = localhost
a1.sources.r1.port = 2323

# Describe the sink
a1.sinks.k1.type = logger

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1

Command used is
bin/flume-ng agent -n a1 -c conf -f conf/test21.conf -Dflume.root.logger=INFO,console

Test12.conf

agent1.sources = seqGenSrc
agent1.sinks = avrosink
agent1.channels = memoryChannel

agent1.sources.seqGenSrc.type = exec
agent1.sources.seqGenSrc.command = tail -f /home/huser/access.log
agent1.sources.seqGenSrc.batch-size = 1

agent1.sinks.avrosink.type = avro
agent1.sinks.avrosink.hostname = localhost
agent1.sinks.avrosink.port = 2323
agent1.sinks.arvosink.batch-size = 100
agent1.sinks.arvosink.connect-timeout = 60000
agent1.sinks.avrosink.request-timeout = 60000

agent1.channels.memoryChannel.type = memory
agent1.channels.memoryChannel.capacity = 1000
agent1.channels.memoryChannel.transactionCapacity = 100

agent1.sources.seqGenSrc.channels = memoryChannel
agent1.sinks.avrosink.channel = memoryChannel


Command used is
bin/flume-ng agent -n agent1 -c conf -f conf/test12.conf -Dflume.root.logger=DEBUG,console

Even after changing the port several times, still Iam facing with the same issues,
Kindly look into my conf file and just let me know the steps.


Regards,
Jeniba Johnson
From: Hari Shreedharan [mailto:hshreedharan@cloudera.com]
Sent: Tuesday, November 11, 2014 1:06 PM
To: Jeniba Johnson
Cc: dev@spark.apache.org
Subject: Re: Bind exception while running FlumeEventCount

Did you start a Flume agent to push data to the relevant port?

Thanks,
Hari


On Fri, Nov 7, 2014 at 2:05 PM, Jeniba Johnson <Jeniba.Johnson@lntinfotech.com<mailto:Jeniba.Johnson@lntinfotech.com>> wrote:

Hi,

I have installed spark-1.1.0 and apache flume 1.4 for running streaming example FlumeEventCount. Previously the code was working fine. Now Iam facing with the below mentioned issues. My flume is running properly it is able to write the file.

The command I use is

bin/run-example org.apache.spark.examples.streaming.FlumeEventCount 172.29.17.178 65001


14/11/07 23:19:23 INFO receiver.ReceiverSupervisorImpl: Stopping receiver with message: Error starting receiver 0: org.jboss.netty.channel.ChannelException: Failed to bind to: /172.29.17.178:65001
14/11/07 23:19:23 INFO flume.FlumeReceiver: Flume receiver stopped
14/11/07 23:19:23 INFO receiver.ReceiverSupervisorImpl: Called receiver onStop
14/11/07 23:19:23 INFO receiver.ReceiverSupervisorImpl: Deregistering receiver 0
14/11/07 23:19:23 ERROR scheduler.ReceiverTracker: Deregistered receiver for stream 0: Error starting receiver 0 - org.jboss.netty.channel.ChannelException: Failed to bind to: /172.29.17.178:65001
at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272)
at org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:106)
at org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:119)
at org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:74)
at org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:68)
at org.apache.spark.streaming.flume.FlumeReceiver.initServer(FlumeInputDStream.scala:164)
at org.apache.spark.streaming.flume.FlumeReceiver.onStart(FlumeInputDStream.scala:171)
at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:121)
at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:106)
at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverLauncher$$anonfun$9.apply(ReceiverTracker.scala:264)
at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverLauncher$$anonfun$9.apply(ReceiverTracker.scala:257)
at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1121)
at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1121)
at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
at org.apache.spark.scheduler.Task.run(Task.scala:54)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:722)
Caused by: java.net.BindException: Address already in use
at sun.nio.ch.Net.bind0(Native Method)
at sun.nio.ch.Net.bind(Net.java:344)
at sun.nio.ch.Net.bind(Net.java:336)
at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:199)
at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
at org.jboss.netty.channel.socket.nio.NioServerBoss$RegisterTask.run(NioServerBoss.java:193)
at org.jboss.netty.channel.socket.nio.AbstractNioSelector.processTaskQueue(AbstractNioSelector.java:366)
at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:290)
at org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)
... 3 more

14/11/07 23:19:23 INFO receiver.ReceiverSupervisorImpl: Stopped receiver 0
14/11/07 23:19:23 INFO receiver.BlockGenerator: Stopping BlockGenerator
14/11/07 23:19:23 INFO util.RecurringTimer: Stopped timer for BlockGenerator after time 1415382563200
14/11/07 23:19:23 INFO receiver.BlockGenerator: Waiting for block pushing thread
14/11/07 23:19:23 INFO receiver.BlockGenerator: Pushing out the last 0 blocks
14/11/07 23:19:23 INFO receiver.BlockGenerator: Stopped block pushing thread
14/11/07 23:19:23 INFO receiver.BlockGenerator: Stopped BlockGenerator
14/11/07 23:19:23 INFO receiver.ReceiverSupervisorImpl: Waiting for executor stop is over
14/11/07 23:19:23 ERROR receiver.ReceiverSupervisorImpl: Stopped executor with error: org.jboss.netty.channel.ChannelException: Failed to bind to: /172.29.17.178:65001
14/11/07 23:19:23 ERROR executor.Executor: Exception in task 0.0 in stage 0.0 (TID 0)
org.jboss.netty.channel.ChannelException: Failed to bind to: /172.29.17.178:65001
at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272)
at org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:106)
at org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:119)
at org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:74)
at org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:68)
at org.apache.spark.streaming.flume.FlumeReceiver.initServer(FlumeInputDStream.scala:164)
at org.apache.spark.streaming.flume.FlumeReceiver.onStart(FlumeInputDStream.scala:171)
at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:121)
at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:106)
at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverLauncher$$anonfun$9.apply(ReceiverTracker.scala:264)
at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverLauncher$$anonfun$9.apply(ReceiverTracker.scala:257)
at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1121)
at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1121)
at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
at org.apache.spark.scheduler.Task.run(Task.scala:54)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:722)
Caused by: java.net.BindException: Address already in use
at sun.nio.ch.Net.bind0(Native Method)
at sun.nio.ch.Net.bind(Net.java:344)
at sun.nio.ch.Net.bind(Net.java:336)
at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:199)
at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
at org.jboss.netty.channel.socket.nio.NioServerBoss$RegisterTask.run(NioServerBoss.java:193)
at org.jboss.netty.channel.socket.nio.AbstractNioSelector.processTaskQueue(AbstractNioSelector.java:366)
at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:290)
at org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)
... 3 more
14/11/07 23:19:23 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, localhost): org.jboss.netty.channel.ChannelException: Failed to bind to: /172.29.17.178:65001
org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272)
org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:106)
org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:119)
org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:74)
org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:68)
org.apache.spark.streaming.flume.FlumeReceiver.initServer(FlumeInputDStream.scala:164)
org.apache.spark.streaming.flume.FlumeReceiver.onStart(FlumeInputDStream.scala:171)
org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:121)
org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:106)
org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverLauncher$$anonfun$9.apply(ReceiverTracker.scala:264)
org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverLauncher$$anonfun$9.apply(ReceiverTracker.scala:257)
org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1121)
org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1121)
org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
org.apache.spark.scheduler.Task.run(Task.scala:54)
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
java.lang.Thread.run(Thread.java:722)
14/11/07 23:19:23 ERROR scheduler.TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job
14/11/07 23:19:23 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
14/11/07 23:19:23 INFO scheduler.TaskSchedulerImpl: Cancelling stage 0
14/11/07 23:19:23 INFO scheduler.DAGScheduler: Failed to run runJob at ReceiverTracker.scala:275
Exception in thread ""Thread-28"" org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): org.jboss.netty.channel.ChannelException: Failed to bind to: /172.29.17.178:65001
org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272)
org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:106)
org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:119)
org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:74)
org.apache.avro.ipc.NettyServer.<init>(NettyServer.java:68)
org.apache.spark.streaming.flume.FlumeReceiver.initServer(FlumeInputDStream.scala:164)
org.apache.spark.streaming.flume.FlumeReceiver.onStart(FlumeInputDStream.scala:171)
org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:121)
org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:106)
org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverLauncher$$anonfun$9.apply(ReceiverTracker.scala:264)
org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverLauncher$$anonfun$9.apply(ReceiverTracker.scala:257)
org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1121)
org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1121)
org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
org.apache.spark.scheduler.Task.run(Task.scala:54)
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
java.lang.Thread.run(Thread.java:722)
Driver stacktrace:
at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1185)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1174)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1173)
at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1173)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:688)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:688)
at scala.Option.foreach(Option.scala:236)
at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:688)
at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1391)
at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
at akka.actor.ActorCell.invoke(ActorCell.scala:456)
at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
at akka.dispatch.Mailbox.run(Mailbox.scala:219)
at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

Regards,
Jeniba Johnson


________________________________
The contents of this e-mail and any attachment(s) may contain confidential or privileged information for the intended recipient(s). Unintended recipients are prohibited from taking action on the basis of information in this e-mail and using or disseminating the information, and must notify the sender and delete it from their system. L&T Infotech will not accept responsibility or liability for the accuracy or completeness of, or the presence of any virus or disabling code in this e-mail""

"
Ewan Higgs <ewan.higgs@ugent.be>,"Tue, 11 Nov 2014 14:03:40 +0100",Terasort example,dev@spark.incubator.apache.org,"Hi all,
I saw that Reynold Xin had a Terasort example PR on Github[1]. It didn't 
appear to be similar to the Hadoop Terasort example, so I've tried to 
brush it into shape so it can generate Terasort files (teragen), sort 
the files (terasort) and validate the files (teravalidate). My branch is 
available here:

https://github.com/ehiggs/spark/tree/terasort

With this code, you can run the following:

# Generate 1M 100 byte records:
  ./bin/run-example terasort.TeraGen 100M ~/data/terasort_in

# Sort the file:
MASTER=local[4] ./bin/run-example terasort.TeraSort ~/data/terasort_in  
~/data/terasort_out

# Validate the file
MASTER=local[4] ./bin/run-example terasort.TeraValidate 
~/data/terasort_out  ~/data/terasort_validate

# Validate that an unsorted file is indeed not correctly sorted:

MASTER=local[4] ./bin/run-example terasort.TeraValidate 
~/data/terasort_in  ~/data/terasort_validate_bad

This matches the interface for the Hadoop version of Terasort, except I 
added the ability to use K,M,G,T for record sizes in TeraGen. This code 
therefore makes a good example of how to use Spark, how to read and 
write Hadoop files, and also a way to test some of the performance 
claims of Spark.

 > That's great, but why is this on the mailing list and not submitted 
as a PR?

I suspect there are some rough edges and I'd really appreciate reviews. 
I would also like to know if others can try it out on clusters and tell 
me if it's performing as it should.

For example, I find it runs fine on my local machine, but when I try to 
sort 100G of data on a cluster of 16 nodes, I get >2900 file splits. 
This really eats into the sort time.

Another issue is that in TeraValidate, to work around SPARK-1018 I had 
to clone each element. Does this /really/ need to be done? It's pretty lame.

In any event, I know the Spark 1.2 merge window closed on Friday but as 
this is only for the examples directory maybe we can slip it in if we 
can bash it into shape quickly enough?

Anyway, thanks to everyone on #apache-spark and #scala who helped me get 
through learning some rudimentary Scala to get this far.

Yours,
Ewan Higgs

[1] https://github.com/apache/spark/pull/1242

---------------------------------------------------------------------


"
Sadhan Sood <sadhan.sood@gmail.com>,"Tue, 11 Nov 2014 11:34:02 -0500",Re: thrift jdbc server probably running queries as hive query,Cheng Lian <lian.cs.zju@gmail.com>,"Hi Cheng,

I made sure the only hive server running on the machine is
hivethriftserver2.

/usr/lib/jvm/default-java/bin/java -cp
/usr/lib/hadoop/lib/hadoop-lzo.jar::/mnt/sadhan/spark-3/sbin/../conf:/mnt/sadhan/spark-3/spark-assembly-1.2.0-SNAPSHOT-hadoop2.3.0-cdh5.0.2.jar:/etc/hadoop/conf
-Xms512m -Xmx512m org.apache.spark.deploy.SparkSubmit --class
org.apache.spark.sql.hive.thriftserver.HiveThriftServer2 --master yarn
--jars reporting.jar spark-internal

The query I am running is a simple count(*): ""select count(*) from Xyz
where date_prefix=20141031"" and pretty sure it's submitting a map reduce
job based on the spark logs:

TakesRest=false

Total jobs = 1

Launching Job 1 out of 1

Number of reduce tasks determined at compile time: 1

In order to change the average load for a reducer (in bytes):

  set hive.exec.reducers.bytes.per.reducer=<number>

In order to limit the maximum number of reducers:

  set hive.exec.reducers.max=<number>

In order to set a constant number of reducers:

  set mapreduce.job.reduces=<number>

14/11/11 16:23:17 INFO ql.Context: New scratch dir is
hdfs://fdsfdsfsdfsdf:9000/tmp/hive-ubuntu/hive_2014-11-11_16-23-17_333_5669798325805509526-2

Starting Job = job_1414084656759_0142, Tracking URL =
http://xxxxxxx:8100/proxy/application_1414084656759_0142/
<http://t.signauxdix.com/e1t/c/5/f18dQhb0S7lC8dDMPbW2n0x6l2B9nMJW7t5XYg2zGvG-W8rBGxP1p8d-TW64zBkx56dS1Dd58vwq02?t=http%3A%2F%2Fec2-54-83-34-89.compute-1.amazonaws.com%3A8100%2Fproxy%2Fapplication_1414084656759_0142%2F&si=6222577584832512&pi=626685a9-b628-43cc-91a1-93636171ce77>

Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1414084656759_0142


ou
k
323646885865-1
323646885865-2
323646885865-2
on
on
"
Ashutosh <ashutosh.trivedi@iiitb.org>,"Tue, 11 Nov 2014 11:06:01 -0700 (MST)",Re: [MLlib] Contributing Algorithm for Outlier Detection,dev@spark.incubator.apache.org,"Hi Mayur,

Vector data types are implemented using breeze library, it is presented at

.../org/apache/spark/mllib/linalg


Anant,

ly restrict the user.

What are you thoughts on LibSVM format?

Thanks for the comments, I was just trying to get away from those increment /decrement functions, they look ugly. Points are noted. I'll try to fix them soon. Tests are also required for the code.


Regards,

Ashutosh


________________________________
From: Mayur Rustagi [via Apache Spark Developers List] <ml-node+s1001551n9239h85@n3.nabble.com>
Sent: Saturday, November 8, 2014 12:52 PM
To: Ashutosh Trivedi (MT2013030)
Subject: Re: [MLlib] Contributing Algorithm for Outlier Detection


What do you mean by vector datatype exactly?

Mayur Rustagi
Ph: +1 (760) 203 3257
http://www.sigmoidanalytics.com
@mayur_rustagi <https://twitter.com/mayur_rustagi>



n
le
he
e
ON
t]
:
OutlierWithAVFModel.scala
OutlierWithAVFModel.scala
Â·
OutlierWithAVFModel.scala
:
t I think
]""
=0>>
f
st
ing-Algorithm-for-Outlier-Detection-tp8880p9034.html
rvlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml
ing-Algorithm-for-Outlier-Detection-tp8880p9035.html
rvlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml
ing-Algorithm-for-Outlier-Detection-tp8880p9036.html
rvlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml
on
ing-Algorithm-for-Outlier-Detection-tp8880p9037.html
rvlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml
on
ing-Algorithm-for-Outlier-Detection-tp8880p9083.html
rvlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml
ing-Algorithm-for-Outlier-Detection-tp8880p9095.html


________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/MLlib-Contributing-Algorithm-for-Outlier-Detection-tp8880p9239.html
lick here<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=8880&code=YXNodXRvc2gudHJpdmVkaUBpaWl0Yi5vcmd8ODg4MHwtMzkzMzE5NzYx>.
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>




--
3.nabble.com/MLlib-Contributing-Algorithm-for-Outlier-Detection-tp8880p9286.html
om."
slcclimber <anant.asty@gmail.com>,"Tue, 11 Nov 2014 11:16:04 -0700 (MST)",Re: [MLlib] Contributing Algorithm for Outlier Detection,dev@spark.incubator.apache.org,"Mayur,
Libsvm format sounds good to me. I could work on writing the tests if that
helps you?
Anant

t
y
d
.
1>>
OutlierWithAVFModel.scala
OutlierWithAVFModel.scala
r
OutlierWithAVFModel.scala
n
te:
hat I
=0>>
.
e
ing-Algorithm-for-Outlier-Detection-tp8880p9034.html
rvlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml
ing-Algorithm-for-Outlier-Detection-tp8880p9035.html
rvlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml
ing-Algorithm-for-Outlier-Detection-tp8880p9036.html
rvlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml
ing-Algorithm-for-Outlier-Detection-tp8880p9037.html
rvlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml
ing-Algorithm-for-Outlier-Detection-tp8880p9083.html
rvlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml
ing-Algorithm-for-Outlier-Detection-tp8880p9095.html
ing-Algorithm-for-Outlier-Detection-tp8880p9239.html
, click
ervlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
ing-Algorithm-for-Outlier-Detection-tp8880p9286.html
, click
ervlet.jtp?macro=unsubscribe_by_code&node=8880&code=YW5hbnQuYXN0eUBnbWFpbC5jb218ODg4MHwxOTU2OTQ5NjMy>
ervlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>




--
3.nabble.com/MLlib-Contributing-Algorithm-for-Outlier-Detection-tp8880p9287.html
om."
Xiangrui Meng <mengxr@gmail.com>,"Tue, 11 Nov 2014 10:16:06 -0800",Re: MLlib related query,Manu Kaul <manohar.kaul@gmail.com>,"Searched MLlib on Google Scholar and didn't find any:) MLlib
implements well-recognized algorithms. Each of which may correspond to
a paper or serveral papers. Please find the reference in the code if
you are interested. -Xiangrui


---------------------------------------------------------------------


"
Ashutosh <ashutosh.trivedi@iiitb.org>,"Tue, 11 Nov 2014 11:20:55 -0700 (MST)",Re: [MLlib] Contributing Algorithm for Outlier Detection,dev@spark.incubator.apache.org,"sure you are welcome.

Let me fix the issues you have pointed out. I'll update you soon by this weekend.


_Ashutosh

________________________________
From: slcclimber [via Apache Spark Developers List] <ml-node+s1001551n9287h73@n3.nabble.com>
Sent: Tuesday, November 11, 2014 11:46 PM
To: Ashutosh Trivedi (MT2013030)
Subject: Re: [MLlib] Contributing Algorithm for Outlier Detection


Mayur,
Libsvm format sounds good to me. I could work on writing the tests if that helps you?
Anant


Hi Mayur,

Vector data types are implemented using breeze library, it is presented at

.../org/apache/spark/mllib/linalg


Anant,

ly restrict the user.

What are you thoughts on LibSVM format?

Thanks for the comments, I was just trying to get away from those increment /decrement functions, they look ugly. Points are noted. I'll try to fix them soon. Tests are also required for the code.


Regards,

Ashutosh


________________________________
From: Mayur Rustagi [via Apache Spark Developers List] <ml-node+[hidden email]<http://user/SendEmail.jtp?type=node&node=9286&i=0>>
Sent: Saturday, November 8, 2014 12:52 PM
To: Ashutosh Trivedi (MT2013030)
Subject: Re: [MLlib] Contributing Algorithm for Outlier Detection


What do you mean by vector datatype exactly?

Mayur Rustagi
Ph: <a href=""tel:%2B1%20%28760%29%20203%203257"" value=""+17602033257"" target=""_blank"">+1 (760) 203 3257
http://www.sigmoidanalytics.com
@mayur_rustagi <https://twitter.com/mayur_rustagi>



n
le
he
e
ON
t]
OutlierWithAVFModel.scala
OutlierWithAVFModel.scala
Â·
OutlierWithAVFModel.scala
:
t I think
]""
=0>>
f
st
ing-Algorithm-for-Outlier-Detection-tp8880p9034.html
rvlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml
ing-Algorithm-for-Outlier-Detection-tp8880p9035.html
rvlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml
ing-Algorithm-for-Outlier-Detection-tp8880p9036.html
rvlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml
on
ing-Algorithm-for-Outlier-Detection-tp8880p9037.html
rvlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml
on
ing-Algorithm-for-Outlier-Detection-tp8880p9083.html
rvlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml
ing-Algorithm-for-Outlier-Detection-tp8880p9095.html


________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/MLlib-Contributing-Algorithm-for-Outlier-Detection-tp8880p9239.html
lick here.
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>


________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/MLlib-Contributing-Algorithm-for-Outlier-Detection-tp8880p9286.html
lick here.
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>


________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/MLlib-Contributing-Algorithm-for-Outlier-Detection-tp8880p9287.html
lick here<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=8880&code=YXNodXRvc2gudHJpdmVkaUBpaWl0Yi5vcmd8ODg4MHwtMzkzMzE5NzYx>.
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>




--
3.nabble.com/MLlib-Contributing-Algorithm-for-Outlier-Detection-tp8880p9289.html
om."
Reynold Xin <rxin@databricks.com>,"Tue, 11 Nov 2014 10:58:49 -0800",Re: Terasort example,Ewan Higgs <ewan.higgs@ugent.be>,"This is great. I think the consensus from last time was that we would put
performance stuff into spark-perf, so it is easy to test different Spark
versions.



"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 11 Nov 2014 15:23:00 -0500",Re: JIRA + PR backlog,Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"Yeah, kudos to Josh for putting that together.


"
Ewan Higgs <ewan.higgs@ugent.be>,"Tue, 11 Nov 2014 21:51:59 +0100",Re: Terasort example,Reynold Xin <rxin@databricks.com>,"Shall I move the code to spark-perf then and submit a PR? Or shall I 
submit a PR to spark where it can remain an idiomatic example and we can 
clone it in spark-perf where it can potentially evolve non-idiomatic 
optimizations?

Yours,
Ewan


"
Patrick Wendell <pwendell@gmail.com>,"Tue, 11 Nov 2014 13:12:33 -0800",Re: JIRA + PR backlog,Nicholas Chammas <nicholas.chammas@gmail.com>,"I wonder if we should be linking to that dashboard somewhere from our
official docs or the wiki...


---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 11 Nov 2014 16:32:43 -0500",Re: JIRA + PR backlog,Patrick Wendell <pwendell@gmail.com>,"That's a good idea.

To encourage and leverage the community to review PRs and JIRA issues, we
should link to both the PR dashboard <https://spark-prs.appspot.com/> and
the stale JIRA filter
<https://issues.apache.org/jira/browse/SPARK-560?filter=12329614> (or
whatever JIRA filter or dashboard we want) from someplace prominent,
perhaps from the CONTRIBUTING
<https://github.com/apache/spark/blob/master/CONTRIBUTING.md> file.

Nick


"
Josh Rosen <rosenville@gmail.com>,"Tue, 11 Nov 2014 13:39:37 -0800",Re: Terasort example,"Ewan Higgs <ewan.higgs@ugent.be>, Reynold Xin <rxin@databricks.com>","For now, Iâ€™d recommend opening a PR against spark-perf. Â It would be great to try to integrate this into the spark-perf harness so that I can run it automatically as part of Spark 1.2.0 release testing. Â If you open a rough WIP PR over there, Iâ€™ll be able to provide some feedback to help you get it integrated into our benchmarking harness.

rote:

Shall I move the code to spark-perf then and submit a PR? Or shall I  
submit a PR to spark where it can remain an idiomatic example and we can  
clone it in spark-perf where it can potentially evolve non-idiomatic  
optimizations?  

Yours,  
Ewan  

 

"
Josh Rosen <rosenville@gmail.com>,"Tue, 11 Nov 2014 13:45:41 -0800",Re: JIRA + PR backlog,"Nicholas Chammas <nicholas.chammas@gmail.com>, Patrick Wendell
 <pwendell@gmail.com>","If possible, it would be great if we could set up a DNS alias, such as prs.spark.apache.org, to point to this app. Â I think the appspot domain is being blocked by some usersâ€™ firewalls, preventing them from accessing the site.


That's a good idea.  

To encourage and leverage the community to review PRs and JIRA issues, we  
should link to both the PR dashboard <https://spark-prs.appspot.com/> and  
the stale JIRA filter  
<https://issues.apache.org/jira/browse/SPARK-560?filter=12329614> (or  
whatever JIRA filter or dashboard we want) from someplace prominent,  
perhaps from the CONTRIBUTING  
<https://github.com/apache/spark/blob/master/CONTRIBUTING.md> file.  

Nick  

rote:  

og-tp9157p9282.html  
-  
"
Sadhan Sood <sadhan.sood@gmail.com>,"Tue, 11 Nov 2014 18:38:10 -0500",Partition caching taking too long,"user <user@spark.apache.org>, dev@spark.incubator.apache.org","While testing SparkSQL on top of our Hive metastore, we were trying to
cache the data for one partition of the table in memory like this:

CACHE TABLE xyz_20141029 AS SELECT * FROM xyz where date_prefix = 20141029

Table xyz is a hive table which is partitioned with date_prefix. The data
is date_prefix = 20141029 directory is one parquet file:

hdfs dfs -ls /event_logs/xyz/20141029

Found 1 items

-rw-r--r--   3 ubuntu hadoop  854521061 2014-11-11 22:20
/event_logs/xyz/20141029/part-01493178cd7f2-31eb-3f9d-b004-149a97ac4d79-r-01493.lzo.parquet

The file size is no more than 800 MB but still the cache command is taking
longer than an hour and is reading data > multiple Gigs from what seems
like from the UI with multiple failures.

Stage 0(mapPartition) which took longest was running as (from UI):

0

mapPartitions at Exchange.scala:86 +details

RDD: HiveTableScan [tid#46,compact#47,date_prefix#45], (MetastoreRelation
default, bid, None), Some((CAST(date_prefix#45, DoubleType) = 2.0141029E7))

org.apache.spark.rdd.RDD.mapPartitions(RDD.scala:602)

org.apache.spark.sql.execution.Exchange$$anonfun$execute$1.apply(Exchange.scala:86)

org.apache.spark.sql.execution.Exchange$$anonfun$execute$1.apply(Exchange.scala:45)

org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:46)

org.apache.spark.sql.execution.Exchange.execute(Exchange.scala:44)

org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1.apply(Aggregate.scala:128)

org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1.apply(Aggregate.scala:127)

org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:46)

org.apache.spark.sql.execution.Aggregate.execute(Aggregate.scala:126)

org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:84)

org.apache.spark.sql.SchemaRDD.collect(SchemaRDD.scala:423)

org.apache.spark.sql.SchemaRDD.count(SchemaRDD.scala:343)

org.apache.spark.sql.execution.CacheTableCommand.sideEffectResult$lzycompute(commands.scala:168)

org.apache.spark.sql.execution.CacheTableCommand.sideEffectResult(commands.scala:159)

org.apache.spark.sql.execution.Command$class.execute(commands.scala:46)

org.apache.spark.sql.execution.CacheTableCommand.execute(commands.scala:153)

org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:425)

org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:425)

org.apache.spark.sql.SchemaRDDLike$class.$init$(SchemaRDDLike.scala:58)

org.apache.spark.sql.SchemaRDD.<init>(SchemaRDD.scala:105)

2014/11/11 22:28:47 40 min(Duration)


19546/19546(Tasks: Succeeded/Total) 201.1 GB(input) 973.5 KB(Shuffle Write)


I need help understanding what is going on and how we can optimize the
caching.

---------------------------------------------------------------------"
Cheng Lian <lian.cs.zju@gmail.com>,"Wed, 12 Nov 2014 10:02:46 +0800",Re: thrift jdbc server probably running queries as hive query,Sadhan Sood <sadhan.sood@gmail.com>,"Hey Sadhan,

Sorry for my previous abrupt reply. Submitting a MR job is definitely 
wrong here, I'm investigating. Would you mind to provide the 
Spark/Hive/Hadoop versions you are using? If you're using most recent 
master branch, a concrete commit sha1 would be very helpful.

Thanks!
Cheng



"
Patrick Wendell <pwendell@gmail.com>,"Tue, 11 Nov 2014 21:47:09 -0800",[NOTICE] [BUILD] Minor changes to Spark's build,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey All,

I've just merged a patch that adds support for Scala 2.11 which will
have some minor implications for the build. These are due to the
complexities of supporting two versions of Scala in a single project.

1. The JDBC server will now require a special flag to build
-Phive-thriftserver on top of the existing flag -Phive. This is
because some build permutations (only in Scala 2.11) won't support the
JDBC server yet due to transitive dependency conflicts.

2. The build now uses non-standard source layouts in a few additional
places (we already did this for the Hive project) - the repl and the
examples modules. This is just fine for maven/sbt, but it may affect
users who import the build in IDE's that are using these projects and
want to build Spark from the IDE. I'm going to update our wiki to
include full instructions for making this work well in IntelliJ.

If there are any other build related issues please respond to this
thread and we'll make sure they get sorted out. Thanks to Prashant
Sharma who is the author of this feature!

- Patrick

---------------------------------------------------------------------


"
Jeniba Johnson <Jeniba.Johnson@lntinfotech.com>,"Wed, 12 Nov 2014 12:39:05 +0530",Spark-Submit issues,Hari Shreedharan <hshreedharan@cloudera.com>,"Hi Hari,

Now Iam trying out the same FlumeEventCount example running with spark-submit Instead of run example. The steps I followed is that I have exported the JavaFlumeEventCount.java into jar.

The command used is
./bin/spark-submit --jars lib/spark-examples-1.1.0-hadoop1.0.4.jar --master local --class org.JavaFlumeEventCount  bin/flumeeventcnt2.jar  localhost 2323

The output is
14/11/12 17:55:02 INFO scheduler.ReceiverTracker: Stream 0 received 1 blocks
14/11/12 17:55:02 INFO scheduler.JobScheduler: Added jobs for time 1415795102000

If I use this command
 ./bin/spark-submit --master local --class org.JavaFlumeEventCount bin/flumeeventcnt2.jar  localhost 2323

Then I get an error
Spark assembly has been built with Hive, including Datanucleus jars on classpath
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/spark/examples/streaming/StreamingExamples
        at org.JavaFlumeEventCount.main(JavaFlumeEventCount.java:22)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:601)
        at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:328)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.examples.streaming.StreamingExamples
        at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:423)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:356)
        ... 8 more


I Just wanted to ask is  that it is able to  find spark-assembly.jar but why not spark-example.jar.
The next doubt is  while running FlumeEventCount example through runexample

I get an output as
Received 4 flume events.

14/11/12 18:30:14 INFO scheduler.JobScheduler: Finished job streaming job 1415797214000 ms.0 from job set of time 1415797214000 ms
14/11/12 18:30:14 INFO rdd.MappedRDD: Removing RDD 70 from persistence list

But If I run the same program through Spark-Submit

I get an output as
14/11/12 17:55:02 INFO scheduler.ReceiverTracker: Stream 0 received 1 blocks
14/11/12 17:55:02 INFO scheduler.JobScheduler: Added jobs for time 1415795102000

So I need a clarification, since in the program the printing statement is written as "" Received n flume events."" So how come Iam able to see as ""Stream 0 received n blocks"".
And what is the difference of running the program through spark-submit and run-example.

Awaiting for your kind reply

Regards,
Jeniba Johnson


________________________________
The contents of this e-mail and any attachment(s) may contain confidential or privileged information for the intended recipient(s). Unintended recipients are prohibited from taking action on the basis of information in this e-mail and using or disseminating the information, and must notify the sender and delete it from their system. L&T Infotech will not accept responsibility or liability for the accuracy or completeness of, or the presence of any virus or disabling code in this e-mail""
"
Sean Owen <sowen@cloudera.com>,"Wed, 12 Nov 2014 10:23:51 +0000",Re: [NOTICE] [BUILD] Minor changes to Spark's build,"""dev@spark.apache.org"" <dev@spark.apache.org>","- Tip: when you rebase, IntelliJ will temporarily think things like the
Kafka module are being removed. Say 'no' when it asks if you want to remove
them.
- Can we go straight to Scala 2.11.4?


"
Ted Malaska <ted.malaska@cloudera.com>,"Wed, 12 Nov 2014 06:23:23 -0500",Re: Spark-Submit issues,Jeniba Johnson <Jeniba.Johnson@lntinfotech.com>,"Hey this is Ted

Are you using Shade when you build your jar and are you using the bigger
jar?  Looks like classes are not included in you jar.


"
Sadhan Sood <sadhan.sood@gmail.com>,"Wed, 12 Nov 2014 12:31:12 -0500",Too many failed collects when trying to cache a table in SparkSQL,"dev@spark.incubator.apache.org, user <user@spark.apache.org>, 
	dev <dev@spark.apache.org>","We are running spark on yarn with combined memory > 1TB and when trying to
cache a table partition(which is < 100G), seeing a lot of failed collect
stages in the UI and this never succeeds. Because of the failed collect, it
seems like the mapPartitions keep getting resubmitted. We have more than
enough memory so its surprising we are seeing this issue. Can someone
please help. Thanks!

The stack trace of the failed collect from UI is:

org.apache.spark.shuffle.MetadataFetchFailedException: Missing an
output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$1.apply(MapOutputTracker.scala:386)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$1.apply(MapOutputTracker.scala:383)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:108)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:382)
	at org.apache.spark.MapOutputTracker.getServerStatuses(MapOutputTracker.scala:178)
	at org.apache.spark.shuffle.hash.BlockStoreShuffleFetcher$.fetch(BlockStoreShuffleFetcher.scala:42)
	at org.apache.spark.shuffle.hash.HashShuffleReader.read(HashShuffleReader.scala:40)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:92)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:195)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
"
Sadhan Sood <sadhan.sood@gmail.com>,"Wed, 12 Nov 2014 12:31:12 -0500",Too many failed collects when trying to cache a table in SparkSQL,"dev@spark.incubator.apache.org, user <user@spark.apache.org>, 
	dev <dev@spark.apache.org>","We are running spark on yarn with combined memory > 1TB and when trying to
cache a table partition(which is < 100G), seeing a lot of failed collect
stages in the UI and this never succeeds. Because of the failed collect, it
seems like the mapPartitions keep getting resubmitted. We have more than
enough memory so its surprising we are seeing this issue. Can someone
please help. Thanks!

The stack trace of the failed collect from UI is:

org.apache.spark.shuffle.MetadataFetchFailedException: Missing an
output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$1.apply(MapOutputTracker.scala:386)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$1.apply(MapOutputTracker.scala:383)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:108)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:382)
	at org.apache.spark.MapOutputTracker.getServerStatuses(MapOutputTracker.scala:178)
	at org.apache.spark.shuffle.hash.BlockStoreShuffleFetcher$.fetch(BlockStoreShuffleFetcher.scala:42)
	at org.apache.spark.shuffle.hash.HashShuffleReader.read(HashShuffleReader.scala:40)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:92)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:195)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
"
Sadhan Sood <sadhan.sood@gmail.com>,"Wed, 12 Nov 2014 14:50:47 -0500",Re: Too many failed collects when trying to cache a table in SparkSQL,"dev@spark.incubator.apache.org, user <user@spark.apache.org>, 
	dev <dev@spark.apache.org>","This is the log output:

2014-11-12 19:07:16,561 INFO  thriftserver.SparkExecuteStatementOperation
(Logging.scala:logInfo(59)) - Running query 'CACHE TABLE xyz_cached AS
SELECT * FROM xyz where date_prefix = 20141112'

2014-11-12 19:07:17,455 INFO  Configuration.deprecation
deprecated. Instead, use mapreduce.job.maps

2014-11-12 19:07:17,756 INFO  spark.SparkContext
(Logging.scala:logInfo(59)) - Created broadcast 0 from broadcast at
TableReader.scala:68

2014-11-12 19:07:18,292 INFO  spark.SparkContext
(Logging.scala:logInfo(59)) - Starting job: collect at SparkPlan.scala:84

2014-11-12 19:07:22,801 INFO  mapred.FileInputFormat
(FileInputFormat.java:listStatus(253)) - Total input paths to process : 200

2014-11-12 19:07:22,835 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - Registering RDD 12 (mapPartitions at
Exchange.scala:86)

2014-11-12 19:07:22,837 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - Got job 0 (collect at SparkPlan.scala:84)
with 1 output partitions (allowLocal=false)

2014-11-12 19:07:22,838 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - Final stage: Stage 1(collect at
SparkPlan.scala:84)

2014-11-12 19:07:22,838 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - Parents of final stage: List(Stage 0)

2014-11-12 19:07:22,842 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - Missing parents: List(Stage 0)

2014-11-12 19:07:22,871 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - Submitting Stage 0 (MapPartitionsRDD[12] at
mapPartitions at Exchange.scala:86), which has no missing parents

2014-11-12 19:07:22,916 INFO  spark.SparkContext
(Logging.scala:logInfo(59)) - Created broadcast 1 from broadcast at
DAGScheduler.scala:838

2014-11-12 19:07:22,963 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - Submitting 461 missing tasks from Stage 0
(MapPartitionsRDD[12] at mapPartitions at Exchange.scala:86)

2014-11-12 19:10:04,088 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - Stage 0 (mapPartitions at Exchange.scala:86)
finished in 161.113 s

2014-11-12 19:10:04,089 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - looking for newly runnable stages

2014-11-12 19:10:04,089 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - running: Set()

2014-11-12 19:10:04,090 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - waiting: Set(Stage 1)

2014-11-12 19:10:04,090 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - failed: Set()

2014-11-12 19:10:04,094 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - Missing parents for Stage 1: List()

2014-11-12 19:10:04,097 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - Submitting Stage 1 (MappedRDD[16] at map at
SparkPlan.scala:84), which is now runnable

2014-11-12 19:10:04,112 INFO  spark.SparkContext
(Logging.scala:logInfo(59)) - Created broadcast 2 from broadcast at
DAGScheduler.scala:838

2014-11-12 19:10:04,115 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - Submitting 1 missing tasks from Stage 1
(MappedRDD[16] at map at SparkPlan.scala:84)

2014-11-12 19:10:08,541 ERROR cluster.YarnClientClusterScheduler
(Logging.scala:logError(75)) - Lost executor 52 on
ip-10-61-175-167.ec2.internal: remote Akka client disassociated

2014-11-12 19:10:08,543 WARN  remote.ReliableDeliverySupervisor
(Slf4jLogger.scala:apply$mcV$sp(71)) - Association with remote system
[akka.tcp://sparkExecutor@ip-10-61-175-167.ec2.internal:50918] has failed,
address is now gated for [5000] ms. Reason is: [Disassociated].

2014-11-12 19:10:08,548 ERROR cluster.YarnClientSchedulerBackend
(Logging.scala:logError(75)) - Asked to remove non-existent executor 52

2014-11-12 19:10:08,550 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - Executor lost: 52 (epoch 1)

2014-11-12 19:10:08,555 INFO  scheduler.Stage (Logging.scala:logInfo(59)) -
Stage 0 is now unavailable on executor 52 (460/461, false)

2014-11-12 19:10:08,686 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - Marking Stage 1 (collect at
SparkPlan.scala:84) as failed due to a fetch failure from Stage 0
(mapPartitions at Exchange.scala:86)

2014-11-12 19:10:08,686 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - Stage 1 (collect at SparkPlan.scala:84)
failed in 4.571 s

2014-11-12 19:10:08,687 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - Resubmitting Stage 0 (mapPartitions at
Exchange.scala:86) and Stage 1 (collect at SparkPlan.scala:84) due to fetch
failure

2014-11-12 19:10:08,908 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - Resubmitting failed stages

2014-11-12 19:10:08,974 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - Submitting Stage 0 (MapPartitionsRDD[12] at
mapPartitions at Exchange.scala:86), which has no missing parents

2014-11-12 19:10:08,989 INFO  spark.SparkContext
(Logging.scala:logInfo(59)) - Created broadcast 3 from broadcast at
DAGScheduler.scala:838

2014-11-12 19:10:08,990 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - Submitting 1 missing tasks from Stage 0
(MapPartitionsRDD[12] at mapPartitions at Exchange.scala:86)

2014-11-12 19:11:15,465 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - Stage 0 (mapPartitions at Exchange.scala:86)
finished in 66.475 s

2014-11-12 19:11:15,465 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - looking for newly runnable stages

2014-11-12 19:11:15,465 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - running: Set()

2014-11-12 19:11:15,465 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - waiting: Set(Stage 1)

2014-11-12 19:11:15,465 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - failed: Set()

2014-11-12 19:11:15,466 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - Missing parents for Stage 1: List()

2014-11-12 19:11:15,466 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - Submitting Stage 1 (MappedRDD[16] at map at
SparkPlan.scala:84), which is now runnable

2014-11-12 19:11:15,482 INFO  spark.SparkContext
(Logging.scala:logInfo(59)) - Created broadcast 4 from broadcast at
DAGScheduler.scala:838

2014-11-12 19:11:15,482 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - Submitting 1 missing tasks from Stage 1
(MappedRDD[16] at map at SparkPlan.scala:84)

2014-11-12 19:11:21,655 ERROR cluster.YarnClientClusterScheduler
(Logging.scala:logError(75)) - Lost executor 372 on
ip-10-95-163-84.ec2.internal: remote Akka client disassociated

2014-11-12 19:11:21,655 WARN  remote.ReliableDeliverySupervisor
(Slf4jLogger.scala:apply$mcV$sp(71)) - Association with remote system
[akka.tcp://sparkExecutor@ip-10-95-163-84.ec2.internal:20998] has failed,
address is now gated for [5000] ms. Reason is: [Disassociated].

2014-11-12 19:11:21,655 ERROR cluster.YarnClientSchedulerBackend
(Logging.scala:logError(75)) - Asked to remove non-existent executor 372

2014-11-12 19:11:21,655 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - Executor lost: 372 (epoch 3)





"
Sadhan Sood <sadhan.sood@gmail.com>,"Wed, 12 Nov 2014 14:50:47 -0500",Re: Too many failed collects when trying to cache a table in SparkSQL,"dev@spark.incubator.apache.org, user <user@spark.apache.org>, 
	dev <dev@spark.apache.org>","This is the log output:

2014-11-12 19:07:16,561 INFO  thriftserver.SparkExecuteStatementOperation
(Logging.scala:logInfo(59)) - Running query 'CACHE TABLE xyz_cached AS
SELECT * FROM xyz where date_prefix = 20141112'

2014-11-12 19:07:17,455 INFO  Configuration.deprecation
deprecated. Instead, use mapreduce.job.maps

2014-11-12 19:07:17,756 INFO  spark.SparkContext
(Logging.scala:logInfo(59)) - Created broadcast 0 from broadcast at
TableReader.scala:68

2014-11-12 19:07:18,292 INFO  spark.SparkContext
(Logging.scala:logInfo(59)) - Starting job: collect at SparkPlan.scala:84

2014-11-12 19:07:22,801 INFO  mapred.FileInputFormat
(FileInputFormat.java:listStatus(253)) - Total input paths to process : 200

2014-11-12 19:07:22,835 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - Registering RDD 12 (mapPartitions at
Exchange.scala:86)

2014-11-12 19:07:22,837 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - Got job 0 (collect at SparkPlan.scala:84)
with 1 output partitions (allowLocal=false)

2014-11-12 19:07:22,838 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - Final stage: Stage 1(collect at
SparkPlan.scala:84)

2014-11-12 19:07:22,838 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - Parents of final stage: List(Stage 0)

2014-11-12 19:07:22,842 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - Missing parents: List(Stage 0)

2014-11-12 19:07:22,871 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - Submitting Stage 0 (MapPartitionsRDD[12] at
mapPartitions at Exchange.scala:86), which has no missing parents

2014-11-12 19:07:22,916 INFO  spark.SparkContext
(Logging.scala:logInfo(59)) - Created broadcast 1 from broadcast at
DAGScheduler.scala:838

2014-11-12 19:07:22,963 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - Submitting 461 missing tasks from Stage 0
(MapPartitionsRDD[12] at mapPartitions at Exchange.scala:86)

2014-11-12 19:10:04,088 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - Stage 0 (mapPartitions at Exchange.scala:86)
finished in 161.113 s

2014-11-12 19:10:04,089 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - looking for newly runnable stages

2014-11-12 19:10:04,089 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - running: Set()

2014-11-12 19:10:04,090 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - waiting: Set(Stage 1)

2014-11-12 19:10:04,090 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - failed: Set()

2014-11-12 19:10:04,094 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - Missing parents for Stage 1: List()

2014-11-12 19:10:04,097 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - Submitting Stage 1 (MappedRDD[16] at map at
SparkPlan.scala:84), which is now runnable

2014-11-12 19:10:04,112 INFO  spark.SparkContext
(Logging.scala:logInfo(59)) - Created broadcast 2 from broadcast at
DAGScheduler.scala:838

2014-11-12 19:10:04,115 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - Submitting 1 missing tasks from Stage 1
(MappedRDD[16] at map at SparkPlan.scala:84)

2014-11-12 19:10:08,541 ERROR cluster.YarnClientClusterScheduler
(Logging.scala:logError(75)) - Lost executor 52 on
ip-10-61-175-167.ec2.internal: remote Akka client disassociated

2014-11-12 19:10:08,543 WARN  remote.ReliableDeliverySupervisor
(Slf4jLogger.scala:apply$mcV$sp(71)) - Association with remote system
[akka.tcp://sparkExecutor@ip-10-61-175-167.ec2.internal:50918] has failed,
address is now gated for [5000] ms. Reason is: [Disassociated].

2014-11-12 19:10:08,548 ERROR cluster.YarnClientSchedulerBackend
(Logging.scala:logError(75)) - Asked to remove non-existent executor 52

2014-11-12 19:10:08,550 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - Executor lost: 52 (epoch 1)

2014-11-12 19:10:08,555 INFO  scheduler.Stage (Logging.scala:logInfo(59)) -
Stage 0 is now unavailable on executor 52 (460/461, false)

2014-11-12 19:10:08,686 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - Marking Stage 1 (collect at
SparkPlan.scala:84) as failed due to a fetch failure from Stage 0
(mapPartitions at Exchange.scala:86)

2014-11-12 19:10:08,686 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - Stage 1 (collect at SparkPlan.scala:84)
failed in 4.571 s

2014-11-12 19:10:08,687 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - Resubmitting Stage 0 (mapPartitions at
Exchange.scala:86) and Stage 1 (collect at SparkPlan.scala:84) due to fetch
failure

2014-11-12 19:10:08,908 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - Resubmitting failed stages

2014-11-12 19:10:08,974 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - Submitting Stage 0 (MapPartitionsRDD[12] at
mapPartitions at Exchange.scala:86), which has no missing parents

2014-11-12 19:10:08,989 INFO  spark.SparkContext
(Logging.scala:logInfo(59)) - Created broadcast 3 from broadcast at
DAGScheduler.scala:838

2014-11-12 19:10:08,990 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - Submitting 1 missing tasks from Stage 0
(MapPartitionsRDD[12] at mapPartitions at Exchange.scala:86)

2014-11-12 19:11:15,465 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - Stage 0 (mapPartitions at Exchange.scala:86)
finished in 66.475 s

2014-11-12 19:11:15,465 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - looking for newly runnable stages

2014-11-12 19:11:15,465 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - running: Set()

2014-11-12 19:11:15,465 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - waiting: Set(Stage 1)

2014-11-12 19:11:15,465 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - failed: Set()

2014-11-12 19:11:15,466 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - Missing parents for Stage 1: List()

2014-11-12 19:11:15,466 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - Submitting Stage 1 (MappedRDD[16] at map at
SparkPlan.scala:84), which is now runnable

2014-11-12 19:11:15,482 INFO  spark.SparkContext
(Logging.scala:logInfo(59)) - Created broadcast 4 from broadcast at
DAGScheduler.scala:838

2014-11-12 19:11:15,482 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - Submitting 1 missing tasks from Stage 1
(MappedRDD[16] at map at SparkPlan.scala:84)

2014-11-12 19:11:21,655 ERROR cluster.YarnClientClusterScheduler
(Logging.scala:logError(75)) - Lost executor 372 on
ip-10-95-163-84.ec2.internal: remote Akka client disassociated

2014-11-12 19:11:21,655 WARN  remote.ReliableDeliverySupervisor
(Slf4jLogger.scala:apply$mcV$sp(71)) - Association with remote system
[akka.tcp://sparkExecutor@ip-10-95-163-84.ec2.internal:20998] has failed,
address is now gated for [5000] ms. Reason is: [Disassociated].

2014-11-12 19:11:21,655 ERROR cluster.YarnClientSchedulerBackend
(Logging.scala:logError(75)) - Asked to remove non-existent executor 372

2014-11-12 19:11:21,655 INFO  scheduler.DAGScheduler
(Logging.scala:logInfo(59)) - Executor lost: 372 (epoch 3)





"
Josh Rosen <rosenville@gmail.com>,"Wed, 12 Nov 2014 13:14:21 -0800",Re: Wrong temp directory when compressing before sending text file to S3,Gary Malouf <malouf.gary@gmail.com>,"Hi Gary,

Could you create a Spark JIRA ticket for this so that it doesn't fall
through the cracks?  Thanks!


"
"""Hari Shreedharan"" <hshreedharan@cloudera.com>","Wed, 12 Nov 2014 13:25:34 -0800 (PST)",Re: Spark-Submit issues,"""Ted Malaska"" <ted.malaska@cloudera.com>","Yep, youâ€™d need to shade jars to ensure all your dependencies are in the classpath.


Thanks,
Hari


have
java:57)
Impl.java:43)
scala:75)
but
runexample
job
list
is
as
and
________
confidential
 in
the"
Ted Malaska <ted.malaska@cloudera.com>,"Wed, 12 Nov 2014 16:26:56 -0500",Re: Spark-Submit issues,Hari Shreedharan <hshreedharan@cloudera.com>,"Other wish include them at the time of execution.  here is an example.

spark-submit --jars
/opt/cloudera/parcels/CDH/lib/zookeeper/zookeeper-3.4.5-cdh5.1.0.jar,/opt/cloudera/parcels/CDH/lib/hbase/lib/guava-12.0.1.jar,/opt/cloudera/parcels/CDH/lib/hbase/lib/protobuf-java-2.5.0.jar,/opt/cloudera/parcels/CDH/lib/hbase/hbase-protocol.jar,/opt/cloudera/parcels/CDH/lib/hbase/hbase-client.jar,/opt/cloudera/parcels/CDH/lib/hbase/hbase-common.jar,/opt/cloudera/parcels/CDH/lib/hbase/hbase-hadoop2-compat.jar,/opt/cloudera/parcels/CDH/lib/hbase/hbase-hadoop-compat.jar,/opt/cloudera/parcels/CDH/lib/hbase/hbase-server.jar,/opt/cloudera/parcels/CDH/lib/hbase/lib/htrace-core.jar
--class org.apache.spark.hbase.example.HBaseBulkDeleteExample --master yarn
--deploy-mode client --executor-memory 512M --num-executors 4
--driver-java-options
-Dspark.executor.extraClassPath=/opt/cloudera/parcels/CDH/lib/hbase/lib/*
SparkHBase.jar t1 c

m

 in the
e
va:57)
rImpl.java:43)
t
s
and
ll
 of,
"
Sadhan Sood <sadhan.sood@gmail.com>,"Wed, 12 Nov 2014 17:58:15 -0500",Re: Too many failed collects when trying to cache a table in SparkSQL,"dev@spark.incubator.apache.org, user <user@spark.apache.org>, 
	dev <dev@spark.apache.org>","collect(stage 1) fails it always leads to mapPartition(stage 0) for one
partition to be re-run. This can be seen from the collect log as well on
the container log:

rg.apache.spark.shuffle.MetadataFetchFailedException: Missing an
output location for shuffle 0

The data is lzo compressed sequence file with compressed size ~ 26G. Is
there a way to understand why shuffle keeps failing for one partition. I
believe we have enough memory to store the uncompressed data in memory.


"
Sadhan Sood <sadhan.sood@gmail.com>,"Wed, 12 Nov 2014 17:58:15 -0500",Re: Too many failed collects when trying to cache a table in SparkSQL,"dev@spark.incubator.apache.org, user <user@spark.apache.org>, 
	dev <dev@spark.apache.org>","collect(stage 1) fails it always leads to mapPartition(stage 0) for one
partition to be re-run. This can be seen from the collect log as well on
the container log:

rg.apache.spark.shuffle.MetadataFetchFailedException: Missing an
output location for shuffle 0

The data is lzo compressed sequence file with compressed size ~ 26G. Is
there a way to understand why shuffle keeps failing for one partition. I
believe we have enough memory to store the uncompressed data in memory.


"
Sadhan Sood <sadhan.sood@gmail.com>,"Wed, 12 Nov 2014 18:16:32 -0500",Cache sparkSql data without uncompressing it in memory,"user <user@spark.apache.org>, dev@spark.incubator.apache.org, 
	dev <dev@spark.apache.org>","We noticed while caching data from our hive tables which contain data in
compressed sequence file format that it gets uncompressed in memory when
getting cached. Is there a way to turn this off and cache the compressed
data as is ?
"
Sadhan Sood <sadhan.sood@gmail.com>,"Wed, 12 Nov 2014 18:16:32 -0500",Cache sparkSql data without uncompressing it in memory,"user <user@spark.apache.org>, dev@spark.incubator.apache.org, 
	dev <dev@spark.apache.org>","We noticed while caching data from our hive tables which contain data in
compressed sequence file format that it gets uncompressed in memory when
getting cached. Is there a way to turn this off and cache the compressed
data as is ?
"
Cheng Lian <lian.cs.zju@gmail.com>,"Thu, 13 Nov 2014 11:05:00 +0800",Re: Cache sparkSql data without uncompressing it in memory,"Sadhan Sood <sadhan.sood@gmail.com>, user <user@spark.apache.org>, 
 dev@spark.incubator.apache.org, dev <dev@spark.apache.org>","Currently thereâ€™s no way to cache the compressed sequence file directly. 
Spark SQL uses in-memory columnar format while caching table rows, so we 
must read all the raw data and convert them into columnar format. 
However, you can enable in-memory columnar compression by setting 
|spark.sql.inMemoryColumnarStorage.compressed| to |true|. This property 
is already set to true by default in master branch and branch-1.2.



â€‹
"
Cheng Lian <lian.cs.zju@gmail.com>,"Thu, 13 Nov 2014 11:05:00 +0800",Re: Cache sparkSql data without uncompressing it in memory,"Sadhan Sood <sadhan.sood@gmail.com>, user <user@spark.apache.org>, 
 dev@spark.incubator.apache.org, dev <dev@spark.apache.org>","Currently thereâ€™s no way to cache the compressed sequence file directly. 
Spark SQL uses in-memory columnar format while caching table rows, so we 
must read all the raw data and convert them into columnar format. 
However, you can enable in-memory columnar compression by setting 
|spark.sql.inMemoryColumnarStorage.compressed| to |true|. This property 
is already set to true by default in master branch and branch-1.2.



â€‹
"
Andrew Or <andrew@databricks.com>,"Wed, 12 Nov 2014 20:34:47 -0800",[VOTE] Release Apache Spark 1.1.1 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version 1
.1.1.

This release fixes a number of bugs in Spark 1.1.0. Some of the notable
ones are
- [SPARK-3426] Sort-based shuffle compression settings are incompatible
- [SPARK-3948] Stream corruption issues in sort-based shuffle
- [SPARK-4107] Incorrect handling of Channel.read() led to data truncation
The full list is at http://s.apache.org/z9h and in the CHANGES.txt attached.

The tag to be voted on is v1.1.1-rc1 (commit 72a4fdbe):
http://s.apache.org/cZC

The release files, including signatures, digests, etc can be found at:
http://people.apache.org/~andrewor14/spark-1.1.1-rc1/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/andrewor14.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1034/

The documentation corresponding to this release can be found at:
http://people.apache.org/~andrewor14/spark-1.1.1-rc1-docs/

Please vote on releasing this package as Apache Spark 1.1.1!

The vote is open until Sunday, November 16, at 04:30 UTC and passes if
a majority of at least 3 +1 PMC votes are cast.
[ ] +1 Release this package as Apache Spark 1.1.1
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

Cheers,
Andrew
Spark Change Log
----------------

Release 1.1.1

  [SPARK-4295][External]Fix exception in SparkSinkSuite
  maji2014 <maji3@asiainfo.com>
  2014-11-11 02:18:27 -0800
  Commit: bf867c3, github.com/apache/spark/pull/3177

  [branch-1.1][SPARK-3990] add a note on ALS usage
  Xiangrui Meng <meng@databricks.com>
  2014-11-10 22:39:09 -0800
  Commit: b2cb357, github.com/apache/spark/pull/3190

  [BRANCH-1.1][SPARK-2652] change the default spark.serializer in pyspark back to Kryo
  Xiangrui Meng <meng@databricks.com>
  2014-11-10 22:21:14 -0800
  Commit: 11798d0, github.com/apache/spark/pull/3187

  [SPARK-4330][Doc] Link to proper URL for YARN overview
  Kousuke Saruta <sarutak@oss.nttdata.co.jp>
  2014-11-10 22:18:00 -0800
  Commit: d313be8, github.com/apache/spark/pull/3196

  [SQL] Backport backtick and smallint JDBC fixes to 1.1
  Michael Armbrust <michael@databricks.com>, ravipesala <ravindra.pesala@huawei.com>, scwf <wangfei1@huawei.com>
  2014-11-10 19:51:07 -0800
  Commit: 8a1d818, github.com/apache/spark/pull/3199

  Update versions for 1.1.1 release
  Andrew Or <andrew@databricks.com>
  2014-11-10 18:40:34 -0800
  Commit: 01d233e

  [SPARK-3495][SPARK-3496] Backporting block replication fixes made in master to branch 1.1
  Tathagata Das <tathagata.das1565@gmail.com>
  2014-11-10 18:23:02 -0800
  Commit: be0cc99, github.com/apache/spark/pull/3191

  [SPARK-3954][Streaming] Optimization to FileInputDStream
  surq <surq@asiainfo.com>
  2014-11-10 17:37:16 -0800
  Commit: 3d889df, github.com/apache/spark/pull/2811

  [SPARK-3971][SQL] Backport #2843 to branch-1.1
  Cheng Lian <lian@databricks.com>, Cheng Lian <lian.cs.zju@gmail.com>, Michael Armbrust <michael@databricks.com>
  2014-11-10 17:04:10 -0800
  Commit: 64945f8, github.com/apache/spark/pull/3113

  [SPARK-4308][SQL] Follow up of #3175 for branch 1.1
  Cheng Lian <lian@databricks.com>
  2014-11-10 16:57:34 -0800
  Commit: b3ef06b, github.com/apache/spark/pull/3176

  [SPARK-2548][HOTFIX][Streaming] Removed use of o.a.s.streaming.Durations in branch 1.1
  Tathagata Das <tathagata.das1565@gmail.com>
  2014-11-10 14:13:42 -0800
  Commit: 86b1bd0, github.com/apache/spark/pull/3188

  Update RecoverableNetworkWordCount.scala
  comcmipi <pitonak@fns.uniba.sk>
  2014-11-10 12:33:48 -0800
  Commit: 254b135, github.com/apache/spark/pull/2735

  SPARK-2548 [STREAMING] JavaRecoverableWordCount is missing
  Sean Owen <sowen@cloudera.com>
  2014-11-10 11:47:27 -0800
  Commit: cdcf546, github.com/apache/spark/pull/2564

  [SPARK-4169] [Core] Accommodate non-English Locales in unit tests
  Niklas Wilcke <1wilcke@informatik.uni-hamburg.de>
  2014-11-10 11:37:38 -0800
  Commit: dc38def, github.com/apache/spark/pull/3036

  [SPARK-4301] StreamingContext should not allow start() to be called after calling stop()
  Josh Rosen <joshrosen@databricks.com>
  2014-11-08 18:10:23 -0800
  Commit: 78cd3ab, github.com/apache/spark/pull/3160

  [SPARK-4304] [PySpark] Fix sort on empty RDD
  Davies Liu <davies@databricks.com>
  2014-11-07 20:53:03 -0800
  Commit: 4895f65, github.com/apache/spark/pull/3162

  Update JavaCustomReceiver.java
  xiao321 <1042460381@qq.com>
  2014-11-07 12:56:49 -0800
  Commit: 4fb26df, github.com/apache/spark/pull/3153

  [SPARK-4249][GraphX]fix a problem of EdgePartitionBuilder in Graphx
  lianhuiwang <lianhuiwang09@gmail.com>
  2014-11-06 10:46:45 -0800
  Commit: 0a40eac, github.com/apache/spark/pull/3138

  [SPARK-4158] Fix for missing resources.
  Brenden Matthews <brenden@diddyinc.com>
  2014-11-05 16:02:44 -0800
  Commit: c58c1bb, github.com/apache/spark/pull/3024

  SPARK-3223 runAsSparkUser cannot change HDFS write permission properly i...
  Jongyoul Lee <jongyoul@gmail.com>
  2014-11-05 15:49:42 -0800
  Commit: 590a943, github.com/apache/spark/pull/3034

  [branch-1.1][SPARK-4148][PySpark] fix seed distribution and add some tests for rdd.sample
  Xiangrui Meng <meng@databricks.com>
  2014-11-05 10:30:10 -0800
  Commit: 44751af, github.com/apache/spark/pull/3104

  [SPARK-4115][GraphX] Add overrided count for edge counting of EdgeRDD.
  luluorta <luluorta@gmail.com>
  2014-11-01 01:22:46 -0700
  Commit: 1b282cd, github.com/apache/spark/pull/2975

  [SPARK-4097] Fix the race condition of 'thread'
  zsxwing <zsxwing@gmail.com>
  2014-10-29 14:42:50 -0700
  Commit: abdb90b, github.com/apache/spark/pull/2957

  [SPARK-4065] Add check for IPython on Windows
  Michael Griffiths <msjgriffiths@gmail.com>
  2014-10-28 12:47:21 -0700
  Commit: f0c5717, github.com/apache/spark/pull/2910

  [SPARK-4107] Fix incorrect handling of read() and skip() return values (branch-1.1 backport)
  Josh Rosen <joshrosen@databricks.com>
  2014-10-28 12:30:12 -0700
  Commit: 286f1ef, github.com/apache/spark/pull/2974

  [SPARK-4110] Wrong comments about default settings in spark-daemon.sh
  Kousuke Saruta <sarutak@oss.nttdata.co.jp>
  2014-10-28 12:29:01 -0700
  Commit: dee3317, github.com/apache/spark/pull/2972

  [MLlib] SPARK-3987: add test case on objective value for NNLS
  coderxiang <shuoxiangpub@gmail.com>
  2014-10-27 19:43:39 -0700
  Commit: 2ef2f5a, github.com/apache/spark/pull/2965

  Fix build breakage introduced by 6c10c2770c718287f9cc2af4109b701fa1057b70
  Josh Rosen <joshrosen@databricks.com>
  2014-10-25 20:33:17 -0700
  Commit: 2eb9d7c

  Revert ""[SPARK-4056] Upgrade snappy-java to 1.1.1.5""
  Josh Rosen <joshrosen@databricks.com>
  2014-10-25 17:09:01 -0700
  Commit: c1989aa

  [SPARK-4056] Upgrade snappy-java to 1.1.1.5
  Josh Rosen <rosenville@gmail.com>, Josh Rosen <joshrosen@databricks.com>
  2014-10-24 17:21:08 -0700
  Commit: b7541ae, github.com/apache/spark/pull/2911

  [SPARK-4080] Only throw IOException from [write|read][Object|External]
  Josh Rosen <joshrosen@databricks.com>
  2014-10-24 15:06:15 -0700
  Commit: 6c10c27, github.com/apache/spark/pull/2932

  [SPARK-4006] In long running contexts, we encountered the situation of d...
  Tal Sliwowicz <tal.s@taboola.com>
  2014-10-24 13:51:25 -0700
  Commit: 59297e9, github.com/apache/spark/pull/2915

  [SPARK-4075] [Deploy] Jar url validation is not enough for Jar file
  Kousuke Saruta <sarutak@oss.nttdata.co.jp>
  2014-10-24 13:08:21 -0700
  Commit: 80dde80, github.com/apache/spark/pull/2925

  [SPARK-4076] Parameter expansion in spark-config is wrong
  Kousuke Saruta <sarutak@oss.nttdata.co.jp>
  2014-10-24 13:04:35 -0700
  Commit: 386fc46, github.com/apache/spark/pull/2930

  [SPARK-2652] [PySpark] donot use KyroSerializer as default serializer
  Davies Liu <davies@databricks.com>
  2014-10-23 23:58:00 -0700
  Commit: 926f8ca, github.com/apache/spark/pull/2916

  [SPARK-3426] Fix sort-based shuffle error when spark.shuffle.compress and spark.shuffle.spill.compress settings are different
  Josh Rosen <joshrosen@databricks.com>
  2014-10-22 14:49:58 -0700
  Commit: 5e191fa, github.com/apache/spark/pull/2890

  [SPARK-3877][YARN] Throw an exception when application is not successful so that the exit code wil be set to 1 (for branch-1.1)
  zsxwing <zsxwing@gmail.com>
  2014-10-22 15:08:28 -0700
  Commit: eb62094, github.com/apache/spark/pull/2748

  [SPARK-4010][Web UI]Spark UI returns 500 in yarn-client mode
  GuoQiang Li <witgo@qq.com>
  2014-10-20 11:01:26 -0700
  Commit: 457ef59, github.com/apache/spark/pull/2858

  [SPARK-3948][Shuffle]Fix stream corruption bug in sort-based shuffle
  jerryshao <saisai.shao@intel.com>
  2014-10-20 10:20:21 -0700
  Commit: 12a61d8, github.com/apache/spark/pull/2824

  [SPARK-2546] Clone JobConf for each task (branch-1.0 / 1.1 backport)
  Josh Rosen <joshrosen@apache.org>
  2014-10-19 00:31:06 -0700
  Commit: 2cd40db, github.com/apache/spark/pull/2684

  SPARK-3926 [CORE] Result of JavaRDD.collectAsMap() is not Serializable
  Sean Owen <sowen@cloudera.com>
  2014-10-18 12:33:20 -0700
  Commit: 327404d, github.com/apache/spark/pull/2805

  [SPARK-3606] [yarn] Correctly configure AmIpFilter for Yarn HA (1.1 vers...
  Marcelo Vanzin <vanzin@cloudera.com>
  2014-10-17 00:53:15 -0700
  Commit: 0d958f1, github.com/apache/spark/pull/2497

  [SPARK-3067] JobProgressPage could not show Fair Scheduler Pools section sometimes
  yantangzhai <tyz0303@163.com>, YanTangZhai <hakeemzhai@tencent.com>
  2014-10-16 19:25:37 -0700
  Commit: 35875e9, github.com/apache/spark/pull/1966

  [SPARK-3890][Docs]remove redundant spark.executor.memory in doc
  WangTaoTheTonic <barneystinson@aliyun.com>, WangTao <barneystinson@aliyun.com>
  2014-10-16 19:12:39 -0700
  Commit: 2c41170, github.com/apache/spark/pull/2745

  [SQL]typo in HiveFromSpark
  Kun Li <jacky.likun@gmail.com>
  2014-10-16 19:00:10 -0700
  Commit: 61e5903, github.com/apache/spark/pull/2809

  SPARK-3807: SparkSql does not work for tables created using custom serde
  chirag <chirag.aggarwal@guavus.com>
  2014-10-13 13:47:26 -0700
  Commit: 925e22d, github.com/apache/spark/pull/2674

  [SPARK-3899][Doc]fix wrong links in streaming doc
  w00228970 <wangfei1@huawei.com>, wangfei <wangfei1@huawei.com>
  2014-10-12 23:35:50 -0700
  Commit: 4fc6638, github.com/apache/spark/pull/2749

  [SPARK-3905][Web UI]The keys for sorting the columns of Executor page ,Stage page Storage page are incorrect
  GuoQiang Li <witgo@qq.com>
  2014-10-12 22:48:54 -0700
  Commit: a36116c, github.com/apache/spark/pull/2763

  [SPARK-3121] Wrong implementation of implicit bytesWritableConverter
  Jakub DubovskÃ½ <james64@inMail.sk>, Dubovsky Jakub <dubovsky@avast.com>
  2014-10-12 22:03:26 -0700
  Commit: 0e32579, github.com/apache/spark/pull/2712

  SPARK-3716 [GraphX] Update Analytics.scala for partitionStrategy assignment
  NamelessAnalyst <NamelessAnalyst@users.noreply.github.com>
  2014-10-12 14:18:55 -0700
  Commit: 5a21e3e, github.com/apache/spark/pull/2569

  [SPARK-3711][SQL] Optimize where in clause filter queries
  Yash Datta <Yash.Datta@guavus.com>
  2014-10-09 12:59:14 -0700
  Commit: 18ef22a, github.com/apache/spark/pull/2561

  [SPARK-3844][UI] Truncate appName in WebUI if it is too long
  Xiangrui Meng <meng@databricks.com>
  2014-10-09 00:00:24 -0700
  Commit: 09d6a81, github.com/apache/spark/pull/2707

  [SPARK-3788] [yarn] Fix compareFs to do the right thing for HDFS namespaces (1.1 version).
  Marcelo Vanzin <vanzin@cloudera.com>
  2014-10-08 08:51:17 -0500
  Commit: a44af73, github.com/apache/spark/pull/2650

  [SPARK-3829] Make Spark logo image on the header of HistoryPage as a link to HistoryPage's page #1
  Kousuke Saruta <sarutak@oss.nttdata.co.jp>
  2014-10-07 16:54:49 -0700
  Commit: a1f833f, github.com/apache/spark/pull/2690

  [SPARK-3777] Display ""Executor ID"" for Tasks in Stage page
  zsxwing <zsxwing@gmail.com>
  2014-10-07 16:00:22 -0700
  Commit: e8afb73, github.com/apache/spark/pull/2642

  [SPARK-3731] [PySpark] fix memory leak in PythonRDD
  Davies Liu <davies.liu@gmail.com>
  2014-10-07 12:20:12 -0700
  Commit: 5531830, github.com/apache/spark/pull/2668

  [SPARK-3825] Log more detail when unrolling a block fails
  Andrew Or <andrewor14@gmail.com>
  2014-10-07 12:52:10 -0700
  Commit: 267c7be, github.com/apache/spark/pull/2688

  [SPARK-3808] PySpark fails to start in Windows
  Masayoshi TSUZUKI <tsudukim@oss.nttdata.co.jp>
  2014-10-07 11:53:22 -0700
  Commit: 3a7875d, github.com/apache/spark/pull/2669

  [SPARK-3827] Very long RDD names are not rendered properly in web UI
  Hossein <hossein@databricks.com>
  2014-10-07 11:46:26 -0700
  Commit: 82ab4a7, github.com/apache/spark/pull/2687

  [SPARK-3792][SQL] Enable JavaHiveQLSuite
  scwf <wangfei1@huawei.com>
  2014-10-05 17:47:20 -0700
  Commit: 964e3aa, github.com/apache/spark/pull/2652

  SPARK-1656: Fix potential resource leaks
  zsxwing <zsxwing@gmail.com>
  2014-10-05 09:55:17 -0700
  Commit: c068d90, github.com/apache/spark/pull/577

  [SPARK-3597][Mesos] Implement `killTask`.
  Brenden Matthews <brenden@diddyinc.com>
  2014-10-05 09:49:24 -0700
  Commit: d9cf4d0, github.com/apache/spark/pull/2453

  [SPARK-3774] typo comment in bin/utils.sh
  Masayoshi TSUZUKI <tsudukim@oss.nttdata.co.jp>
  2014-10-03 13:12:37 -0700
  Commit: e4ddede, github.com/apache/spark/pull/2639

  [SPARK-3775] Not suitable error message in spark-shell.cmd
  Masayoshi TSUZUKI <tsudukim@oss.nttdata.co.jp>
  2014-10-03 13:09:48 -0700
  Commit: f130256, github.com/apache/spark/pull/2640

  [SPARK-3535][Mesos] Fix resource handling.
  Brenden Matthews <brenden@diddyinc.com>
  2014-10-03 12:58:04 -0700
  Commit: 6f15097, github.com/apache/spark/pull/2401

  [SPARK-3696]Do not override the user-difined conf_dir
  WangTaoTheTonic <barneystinson@aliyun.com>
  2014-10-03 10:42:41 -0700
  Commit: d5af9e1, github.com/apache/spark/pull/2541

  SPARK-2058: Overriding SPARK_HOME/conf with SPARK_CONF_DIR
  EugenCepoi <cepoi.eugen@gmail.com>
  2014-10-03 10:03:15 -0700
  Commit: 5d991db, github.com/apache/spark/pull/2481

  [DEPLOY] SPARK-3759: Return the exit code of the driver process
  Eric Eijkelenboom <ee@userreport.com>
  2014-10-02 18:04:38 -0700
  Commit: 699af62, github.com/apache/spark/pull/2628

  [SPARK-3755][Core] avoid trying privileged port when request a non-privileged port
  scwf <wangfei1@huawei.com>
  2014-10-02 17:47:56 -0700
  Commit: 16789f6, github.com/apache/spark/pull/2623

  [SQL][Docs] Update the output of printSchema and fix a typo in SQL programming guide.
  Yin Huai <huai@cse.ohio-state.edu>
  2014-10-02 11:37:24 -0700
  Commit: 6869351, github.com/apache/spark/pull/2630

  SPARK-3638 | Forced a compatible version of http client in kinesis-asl profile
  aniketbhatnagar <aniket.bhatnagar@gmail.com>
  2014-10-01 18:31:18 -0700
  Commit: c52c231, github.com/apache/spark/pull/2535

  Typo error in KafkaWordCount example
  Gaspar Munoz <munozs.88@gmail.com>
  2014-10-01 13:47:22 -0700
  Commit: 24ee616, github.com/apache/spark/pull/2614

  [SPARK-3756] [Core]check exception is caused by an address-port collision properly
  scwf <wangfei1@huawei.com>
  2014-10-01 11:51:30 -0700
  Commit: b4f690d, github.com/apache/spark/pull/2611

  SPARK-2626 [DOCS] Stop SparkContext in all examples
  Sean Owen <sowen@cloudera.com>
  2014-10-01 11:28:22 -0700
  Commit: 13f33cf, github.com/apache/spark/pull/2575

  [SPARK-3755][Core] Do not bind port 1 - 1024 to server in spark
  scwf <wangfei1@huawei.com>
  2014-10-01 11:30:29 -0700
  Commit: c8c3b49, github.com/apache/spark/pull/2610

  [SPARK-3747] TaskResultGetter could incorrectly abort a stage if it cannot get result for a specific task
  Reynold Xin <rxin@apache.org>
  2014-10-01 00:29:14 -0700
  Commit: a7d2df4, github.com/apache/spark/pull/2599

  SPARK-3745 - fix check-license to properly download and check jar
  shane knapp <incomplete@gmail.com>
  2014-09-30 13:11:25 -0700
  Commit: 06b96d4, github.com/apache/spark/pull/2596

  [SPARK-3709] Executors don't always report broadcast block removal properly back to the driver (for branch-1.1)
  Reynold Xin <rxin@apache.org>
  2014-09-30 12:24:58 -0700
  Commit: a8c6e82, github.com/apache/spark/pull/2591

  [SPARK-3734] DriverRunner should not read SPARK_HOME from submitter's environment
  Josh Rosen <joshrosen@apache.org>
  2014-09-29 23:36:10 -0700
  Commit: 48be657, github.com/apache/spark/pull/2586

  Fixed the condition in StronglyConnectedComponents Issue: SPARK-3635
  oded <oded@HP-DV6.c4internal.c4-security.com>
  2014-09-29 18:05:53 -0700
  Commit: 85dd513, github.com/apache/spark/pull/2486

  [graphX] GraphOps: random pick vertex bug
  yingjieMiao <yingjie@42go.com>
  2014-09-29 18:01:27 -0700
  Commit: e5ab113, github.com/apache/spark/pull/2553

  [SPARK-3032][Shuffle] Fix key comparison integer overflow introduced sorting exception
  jerryshao <saisai.shao@intel.com>
  2014-09-29 11:25:32 -0700
  Commit: df5a62f, github.com/apache/spark/pull/2514

  [CORE] Bugfix: LogErr format in DAGScheduler.scala
  Zhang, Liye <liye.zhang@intel.com>
  2014-09-29 01:13:15 -0700
  Commit: 7d88471, github.com/apache/spark/pull/2572

  [SPARK-3715][Docs]minor typo
  WangTaoTheTonic <barneystinson@aliyun.com>
  2014-09-28 18:30:13 -0700
  Commit: 004b6fa, github.com/apache/spark/pull/2567

  Docs : use ""--total-executor-cores"" rather than ""--cores"" after spark-shell
  CrazyJvm <crazyjvm@gmail.com>
  2014-09-27 09:41:04 -0700
  Commit: d9d94e0, github.com/apache/spark/pull/2540

  SPARK-3639 | Removed settings master in examples
  aniketbhatnagar <aniket.bhatnagar@gmail.com>
  2014-09-26 09:47:58 -0700
  Commit: d6ed5ab, github.com/apache/spark/pull/2536

  [SPARK-1853] Show Streaming application code context (file, line number) in Spark Stages UI
  Mubarak Seyed <mubarak.seyed@gmail.com>, Tathagata Das <tathagata.das1565@gmail.com>
  2014-09-23 15:09:12 -0700
  Commit: 505ed6b, github.com/apache/spark/pull/2464

  [SPARK-3653] Respect SPARK_*_MEMORY for cluster mode
  Andrew Or <andrewor14@gmail.com>
  2014-09-23 14:00:33 -0700
  Commit: 5bbc621, github.com/apache/spark/pull/2500

  SPARK-3612. Executor shouldn't quit if heartbeat message fails to reach ...
  Sandy Ryza <sandy@cloudera.com>
  2014-09-23 13:44:18 -0700
  Commit: ffd97be, github.com/apache/spark/pull/2487

  Update docs to use jsonRDD instead of wrong jsonRdd.
  Grega Kespret <grega.kespret@gmail.com>
  2014-09-22 10:13:44 -0700
  Commit: aab0a1d, github.com/apache/spark/pull/2479

  [MLLib] Fix example code variable name misspelling in MLLib Feature Extraction guide
  RJ Nowling <rnowling@gmail.com>
  2014-09-22 09:10:41 -0700
  Commit: 32bb97f, github.com/apache/spark/pull/2459

  Revert ""[SPARK-3595] Respect configured OutputCommitters when calling saveAsHadoopFile""
  Patrick Wendell <pwendell@gmail.com>
  2014-09-21 13:07:20 -0700
  Commit: f5bf7de

  [SPARK-3595] Respect configured OutputCommitters when calling saveAsHadoopFile
  Ian Hummel <ian@themodernlife.net>
  2014-09-21 13:04:36 -0700
  Commit: 7a76657, github.com/apache/spark/pull/2450

  [Docs] Fix outdated docs for standalone cluster
  andrewor14 <andrewor14@gmail.com>, Andrew Or <andrewor14@gmail.com>
  2014-09-19 16:02:38 -0700
  Commit: fd88353, github.com/apache/spark/pull/2461

  [SPARK-2062][GraphX] VertexRDD.apply does not use the mergeFunc
  Larry Xiao <xiaodi@sjtu.edu.cn>, Blie Arkansol <xiaodi@sjtu.edu.cn>, Ankur Dave <ankurdave@gmail.com>
  2014-09-18 23:32:32 -0700
  Commit: 1687d6b, github.com/apache/spark/pull/1903

  [Minor Hot Fix] Move a line in SparkSubmit to the right place
  Andrew Or <andrewor14@gmail.com>
  2014-09-18 17:49:28 -0700
  Commit: cf15b22, github.com/apache/spark/pull/2452

  [SPARK-3560] Fixed setting spark.jars system property in yarn-cluster mode
  Victsm <victor.nju@gmail.com>, Min Shen <mshen@linkedin.com>
  2014-09-18 15:58:14 -0700
  Commit: 832dff6, github.com/apache/spark/pull/2449

  [SPARK-3589][Minor]remove redundant code
  WangTaoTheTonic <barneystinson@aliyun.com>
  2014-09-18 12:07:24 -0700
  Commit: 2b28692, github.com/apache/spark/pull/2445

  [SPARK-3565]Fix configuration item not consistent with document
  WangTaoTheTonic <barneystinson@aliyun.com>
  2014-09-17 21:59:23 -0700
  Commit: 32f2222, github.com/apache/spark/pull/2427

  [SPARK-3564][WebUI] Display App ID on HistoryPage
  Kousuke Saruta <sarutak@oss.nttdata.co.jp>
  2014-09-17 16:31:58 -0700
  Commit: 3f1f974, github.com/apache/spark/pull/2424

  Docs: move HA subsections to a deeper indentation level
  Andrew Ash <andrew@andrewash.com>
  2014-09-17 15:07:57 -0700
  Commit: 0690410, github.com/apache/spark/pull/2402

  [SQL][DOCS] Improve table caching section
  Michael Armbrust <michael@databricks.com>
  2014-09-17 12:41:49 -0700
  Commit: 85e7c52, github.com/apache/spark/pull/2434

  [SPARK-3490] Disable SparkUI for tests (backport into 1.1)
  Andrew Or <andrewor14@gmail.com>
  2014-09-16 18:23:28 -0700
  Commit: 937de93, github.com/apache/spark/pull/2415

  [SPARK-3555] Fix UISuite race condition
  Andrew Or <andrewor14@gmail.com>
  2014-09-16 16:03:20 -0700
  Commit: 856156b, github.com/apache/spark/pull/2418

  [SQL][DOCS] Improve section on thrift-server
  Michael Armbrust <michael@databricks.com>
  2014-09-16 11:51:46 -0700
  Commit: 75158a7, github.com/apache/spark/pull/2384

  [SPARK-3518] Remove wasted statement in JsonProtocol
  Kousuke Saruta <sarutak@oss.nttdata.co.jp>
  2014-09-15 16:11:41 -0700
  Commit: 99a6c5e, github.com/apache/spark/pull/2380

  SPARK-3039: Allow spark to be built using avro-mapred for hadoop2
  Bertrand Bossy <bertrandbossy@gmail.com>
  2014-09-14 21:10:17 -0700
  Commit: 78887f9, github.com/apache/spark/pull/1945

  [SQL] [Docs] typo fixes
  Nicholas Chammas <nicholas.chammas@gmail.com>
  2014-09-13 12:34:20 -0700
  Commit: 70f93d5, github.com/apache/spark/pull/2367

  [SPARK-3515][SQL] Moves test suite setup code to beforeAll rather than in constructor
  Cheng Lian <lian.cs.zju@gmail.com>
  2014-09-12 20:14:09 -0700
  Commit: 44e534e, github.com/apache/spark/pull/2375

  [SPARK-3500] [SQL] use JavaSchemaRDD as SchemaRDD._jschema_rdd
  Davies Liu <davies.liu@gmail.com>
  2014-09-12 19:05:39 -0700
  Commit: 9c06c72, github.com/apache/spark/pull/2369

  [SPARK-3481] [SQL] Eliminate the error log in local Hive comparison test
  Cheng Hao <hao.cheng@intel.com>
  2014-09-12 11:29:30 -0700
  Commit: 6cbf83c, github.com/apache/spark/pull/2352

  Revert ""[Spark-3490] Disable SparkUI for tests""
  Andrew Or <andrewor14@gmail.com>
  2014-09-12 10:40:03 -0700
  Commit: f17b795

  [SPARK-3465] fix task metrics aggregation in local mode
  Davies Liu <davies.liu@gmail.com>
  2014-09-11 18:53:26 -0700
  Commit: e69deb8, github.com/apache/spark/pull/2338

  [SPARK-3429] Don't include the empty string """" as a defaultAclUser
  Andrew Ash <andrew@andrewash.com>
  2014-09-11 17:28:36 -0700
  Commit: 4245404, github.com/apache/spark/pull/2286

  [Spark-3490] Disable SparkUI for tests
  Andrew Or <andrewor14@gmail.com>
  2014-09-11 17:18:46 -0700
  Commit: 2ffc798, github.com/apache/spark/pull/2363

  [SPARK-2140] Updating heap memory calculation for YARN stable and alpha.
  Chris Cope <ccope@resilientscience.com>
  2014-09-11 08:13:07 -0500
  Commit: 06fb2d0, github.com/apache/spark/pull/2253

  HOTFIX: Changing color on doc menu
  Patrick Wendell <pwendell@gmail.com>
  2014-09-10 22:14:55 -0700
  Commit: e51ce9a

  [SPARK-1919] Fix Windows spark-shell --jars
  Andrew Or <andrewor14@gmail.com>
  2014-09-02 10:47:05 -0700
  Commit: 359cd59, github.com/apache/spark/pull/2211

  [SPARK-3061] Fix Maven build under Windows
  Josh Rosen <joshrosen@apache.org>, Josh Rosen <rosenville@gmail.com>, Josh Rosen <joshrosen@databricks.com>
  2014-09-02 10:45:14 -0700
  Commit: 23fd3e8, github.com/apache/spark/pull/2165

  [SPARK-3345] Do correct parameters for ShuffleFileGroup
  Liang-Chi Hsieh <viirya@gmail.com>
  2014-09-03 17:04:53 -0700
  Commit: e5f77ae, github.com/apache/spark/pull/2235

  [SPARK-3193]output errer info when Process exit code is not zero in test suite
  scwf <wangfei1@huawei.com>
  2014-09-09 11:57:01 -0700
  Commit: 2426268, github.com/apache/spark/pull/2108

  SPARK-2425 Don't kill a still-running Application because of some misbehaving Executors
  Mark Hamstra <markhamstra@gmail.com>
  2014-09-08 20:51:56 -0700
  Commit: e884805, github.com/apache/spark/pull/1360

  [SQL] Minor edits to sql programming guide.
  Henry Cook <hcook@eecs.berkeley.edu>
  2014-09-08 14:56:37 -0700
  Commit: 7a236dc, github.com/apache/spark/pull/2316

  [SPARK-938][doc] Add OpenStack Swift support
  Reynold Xin <rxin@apache.org>, Gil Vernik <gilv@il.ibm.com>
  2014-09-07 20:56:04 -0700
  Commit: 8c6306a, github.com/apache/spark/pull/is

  Fixed typos in make-distribution.sh
  Cheng Lian <lian.cs.zju@gmail.com>
  2014-09-07 20:38:32 -0700
  Commit: e45bfa8, github.com/apache/spark/pull/2121

  [SPARK-3408] Fixed Limit operator so it works with sort-based shuffle.
  Reynold Xin <rxin@apache.org>
  2014-09-07 18:42:24 -0700
  Commit: d555c2e, github.com/apache/spark/pull/2281

  [SQL] Update SQL Programming Guide
  Michael Armbrust <michael@databricks.com>, Yin Huai <huai@cse.ohio-state.edu>
  2014-09-07 21:34:46 -0400
  Commit: 65dae63, github.com/apache/spark/pull/2258

  [SPARK-3394] [SQL] Fix crash in TakeOrdered when limit is 0
  Eric Liang <ekl@google.com>
  2014-09-07 17:57:59 -0700
  Commit: c5d8d82, github.com/apache/spark/pull/2264

  [SPARK-2419][Streaming][Docs] More updates to the streaming programming guide
  Tathagata Das <tathagata.das1565@gmail.com>, Chris Fregly <chris@fregly.com>
  2014-09-06 14:46:43 -0700
  Commit: ce4053c, github.com/apache/spark/pull/2307

  SPARK-3211 .take() is OOM-prone with empty partitions
  Andrew Ash <andrew@andrewash.com>
  2014-09-05 18:52:05 -0700
  Commit: 28ce67b, github.com/apache/spark/pull/2117

  [Docs] fix minor MLlib case typo
  Nicholas Chammas <nicholas.chammas@gmail.com>
  2014-09-04 23:37:06 -0700
  Commit: 6b128be, github.com/apache/spark/pull/2278

  [SPARK-3401][PySpark] Wrong usage of tee command in python/run-tests
  Kousuke Saruta <sarutak@oss.nttdata.co.jp>
  2014-09-04 10:29:11 -0700
  Commit: dbf8120, github.com/apache/spark/pull/2272

  [HOTFIX] [SPARK-3400] Revert 9b225ac ""fix GraphX EdgeRDD zipPartitions""
  Ankur Dave <ankurdave@gmail.com>
  2014-09-03 23:49:47 -0700
  Commit: 8c40ab5, github.com/apache/spark/pull/2271

  [SPARK-3372] [MLlib] MLlib doesn't pass maven build / checkstyle due to multi-byte character contained in Gradient.scala
  Kousuke Saruta <sarutak@oss.nttdata.co.jp>
  2014-09-03 20:47:00 -0700
  Commit: f41c45a, github.com/apache/spark/pull/2248

  [SPARK-2419][Streaming][Docs] Updates to the streaming programming guide
  Tathagata Das <tathagata.das1565@gmail.com>, Jacek Laskowski <jacek@japila.pl>
  2014-09-03 17:38:01 -0700
  Commit: 3111501, github.com/apache/spark/pull/2254


Release 1.1.0

  [SPARK-3320][SQL] Made batched in-memory column buffer building work for SchemaRDDs with empty partitions
  Cheng Lian <lian.cs.zju@gmail.com>
  2014-08-29 18:16:47 -0700
  Commit: aa9364a, github.com/apache/spark/pull/2213

  [SPARK-3296][mllib] spark-example should be run-example in head notation of DenseKMeans and SparseNaiveBayes
  wangfei <wangfei_hello@126.com>
  2014-08-29 17:37:15 -0700
  Commit: b0facb5, github.com/apache/spark/pull/2193

  [SPARK-3291][SQL]TestcaseName in createQueryTest should not contain "":""
  qiping.lqp <qiping.lqp@alibaba-inc.com>
  2014-08-29 15:37:43 -0700
  Commit: c1333b8, github.com/apache/spark/pull/2191

  [SPARK-3269][SQL] Decreases initial buffer size for row set to prevent OOM
  Cheng Lian <lian.cs.zju@gmail.com>
  2014-08-29 15:36:04 -0700
  Commit: 9bae345, github.com/apache/spark/pull/2171

  [SPARK-3234][Build] Fixed environment variables that rely on deprecated command line options in make-distribution.sh
  Cheng Lian <lian.cs.zju@gmail.com>
  2014-08-29 15:29:43 -0700
  Commit: cf049ef, github.com/apache/spark/pull/2208

  [Docs] SQL doc formatting and typo fixes
  Nicholas Chammas <nicholas.chammas@gmail.com>, nchammas <nicholas.chammas@gmail.com>
  2014-08-29 15:23:32 -0700
  Commit: bfa2dc9, github.com/apache/spark/pull/2201

  [SPARK-3307] [PySpark] Fix doc string of SparkContext.broadcast()
  Davies Liu <davies.liu@gmail.com>
  2014-08-29 11:47:49 -0700
  Commit: 98d0716, github.com/apache/spark/pull/2202

  HOTFIX: Bump spark-ec2 version to 1.1.0
  Patrick Wendell <pwendell@gmail.com>
  2014-08-29 11:20:45 -0700
  Commit: c71b5c6

  Adding new CHANGES.txt
  Patrick Wendell <pwendell@gmail.com>
  2014-08-28 17:17:30 -0700
  Commit: 7db87b3

  [SPARK-3277] Fix external spilling with LZ4 assertion error
  Andrew Or <andrewor14@gmail.com>, Patrick Wendell <pwendell@gmail.com>
  2014-08-28 17:05:21 -0700
  Commit: fe4df34, github.com/apache/spark/pull/2187

  SPARK-3082. yarn.Client.logClusterResourceDetails throws NPE if requeste...
  Sandy Ryza <sandy@cloudera.com>
  2014-08-28 16:18:50 -0700
  Commit: f4cbf5e, github.com/apache/spark/pull/1984

  [SPARK-3190] Avoid overflow in VertexRDD.count()
  Ankur Dave <ankurdave@gmail.com>
  2014-08-28 15:17:01 -0700
  Commit: 0b9718a, github.com/apache/spark/pull/2106

  [SPARK-3264] Allow users to set executor Spark home in Mesos
  Andrew Or <andrewor14@gmail.com>
  2014-08-28 11:05:44 -0700
  Commit: 069ecfe, github.com/apache/spark/pull/2166

  [SPARK-3150] Fix NullPointerException in in Spark recovery: Add initializing default values in DriverInfo.init()
  Tatiana Borisova <tanyatik@yandex.ru>
  2014-08-28 10:36:36 -0700
  Commit: fd98020, github.com/apache/spark/pull/2062

  Additional CHANGES.txt
  Patrick Wendell <pwendell@gmail.com>
  2014-08-28 00:19:03 -0700
  Commit: a9df703

  [SPARK-3230][SQL] Fix udfs that return structs
  Michael Armbrust <michael@databricks.com>
  2014-08-28 00:15:23 -0700
  Commit: 2e8ad99, github.com/apache/spark/pull/2133

  [SQL] Fixed 2 comment typos in SQLConf
  Cheng Lian <lian.cs.zju@gmail.com>
  2014-08-28 00:08:09 -0700
  Commit: c0e3bc1, github.com/apache/spark/pull/2172

  HOTFIX: Don't build with YARN support for Mapr3
  Patrick Wendell <pwendell@gmail.com>
  2014-08-27 15:40:40 -0700
  Commit: ad0fab2

  [HOTFIX][SQL] Remove cleaning of UDFs
  Michael Armbrust <michael@databricks.com>
  2014-08-27 23:05:34 -0700
  Commit: 233c283, github.com/apache/spark/pull/2174

  [HOTFIX] Wait for EOF only for the PySpark shell
  Andrew Or <andrewor14@gmail.com>
  2014-08-27 23:03:46 -0700
  Commit: 54ccd93, github.com/apache/spark/pull/2170

  BUILD: Updating CHANGES.txt for Spark 1.1
  Patrick Wendell <pwendell@gmail.com>
  2014-08-27 15:55:59 -0700
  Commit: 8597e9c

  Add line continuation for script to work w/ py2.7.5
  Matthew Farrellee <matt@redhat.com>
  2014-08-27 15:50:30 -0700
  Commit: d4cf7a0, github.com/apache/spark/pull/2139

  [SPARK-3235][SQL] Ensure in-memory tables don't always broadcast.
  Michael Armbrust <michael@databricks.com>
  2014-08-27 15:14:08 -0700
  Commit: 9a62cf3, github.com/apache/spark/pull/2147

  [SPARK-3065][SQL] Add locale setting to fix results do not match for udf_unix_timestamp format ""yyyy MMM dd h:mm:ss a"" run with not ""America/Los_Angeles"" TimeZone in HiveCompatibilitySuite
  luogankun <luogankun@gmail.com>
  2014-08-27 15:08:22 -0700
  Commit: 5ea260e, github.com/apache/spark/pull/1968

  [SQL] [SPARK-3236] Reading Parquet tables from Metastore mangles location
  Aaron Davidson <aaron@databricks.com>
  2014-08-27 15:05:47 -0700
  Commit: 7711687, github.com/apache/spark/pull/2150

  [SPARK-3252][SQL] Add missing condition for test
  viirya <viirya@gmail.com>
  2014-08-27 14:55:05 -0700
  Commit: b3d763b, github.com/apache/spark/pull/2159

  [SPARK-3243] Don't use stale spark-driver.* system properties
  Andrew Or <andrewor14@gmail.com>
  2014-08-27 14:46:56 -0700
  Commit: c1ffa3e, github.com/apache/spark/pull/2154

  Spark-3213 Fixes issue with spark-ec2 not detecting slaves created with ""Launch More like this""
  Vida Ha <vida@databricks.com>
  2014-08-27 14:26:06 -0700
  Commit: 3cb4e17, github.com/apache/spark/pull/2163

  [SPARK-3138][SQL] sqlContext.parquetFile should be able to take a single file as parameter
  chutium <teng.qiu@gmail.com>
  2014-08-27 13:13:04 -0700
  Commit: 90f8f3e, github.com/apache/spark/pull/2044

  [SPARK-3197] [SQL] Reduce the Expression tree object creations for aggregation function (min/max)
  Cheng Hao <hao.cheng@intel.com>
  2014-08-27 12:50:47 -0700
  Commit: 4c7f082, github.com/apache/spark/pull/2113

  [SPARK-3118][SQL]add ""SHOW TBLPROPERTIES tblname;"" and ""SHOW COLUMNS (FROM|IN) table_name [(FROM|IN) db_name]"" support"
Andrew Or <andrew@databricks.com>,"Wed, 12 Nov 2014 20:35:10 -0800",Re: [VOTE] Release Apache Spark 1.1.1 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","I will start the vote with a +1

2014-11-12 20:34 GMT-08:00 Andrew Or <andrew@databricks.com>:

"
Prashant Sharma <scrapcodes@gmail.com>,"Thu, 13 Nov 2014 11:59:21 +0530",Re: [NOTICE] [BUILD] Minor changes to Spark's build,Sean Owen <sowen@cloudera.com>,"Thanks Patrick, I have one suggestion that we should make passing
-Pscala-2.10 mandatory for maven users. I am sorry for not mentioning this
before. There is no way around not passing that option for maven
users(only). However, this is unnecessary for sbt users because it is added
automatically if -Pscala-2.11 is absent.


Prashant Sharma




"
Prashant Sharma <scrapcodes@gmail.com>,"Thu, 13 Nov 2014 12:03:15 +0530",Re: [NOTICE] [BUILD] Minor changes to Spark's build,Sean Owen <sowen@cloudera.com>,"For scala 2.11.4, there are minor changes needed in repl code. I can do
that if that is a high priority.

Prashant Sharma




"
Patrick Wendell <pwendell@gmail.com>,"Wed, 12 Nov 2014 22:35:01 -0800",Re: [NOTICE] [BUILD] Minor changes to Spark's build,Prashant Sharma <scrapcodes@gmail.com>,"Yeah Sandy and I were chatting about this today and din't realize
-Pscala-2.10 was mandatory. This is a fairly invasive change, so I was
thinking maybe we could try to remove that. Also if someone doesn't
give -Pscala-2.10 it fails in a way that is initially silent, which is
bad because most people won't know to do this.

https://issues.apache.org/jira/browse/SPARK-4375


---------------------------------------------------------------------


"
Prashant Sharma <scrapcodes@gmail.com>,"Thu, 13 Nov 2014 12:07:01 +0530",Re: [NOTICE] [BUILD] Minor changes to Spark's build,Patrick Wendell <pwendell@gmail.com>,"about how this can be done, but since now I can write groovy inside maven
build so we have more control. (Yay!!)

Prashant Sharma




"
Patrick Wendell <pwendell@gmail.com>,"Wed, 12 Nov 2014 22:59:55 -0800",Re: [NOTICE] [BUILD] Minor changes to Spark's build,Prashant Sharma <scrapcodes@gmail.com>,"I think printing an error that says ""-Pscala-2.10 must be enabled"" is
probably okay. It's a slight regression but it's super obvious to
users. That could be a more elegant solution than the somewhat
complicated monstrosity I proposed on the JIRA.


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Wed, 12 Nov 2014 23:14:30 -0800",Re: [NOTICE] [BUILD] Minor changes to Spark's build,Sandy Ryza <sandy.ryza@cloudera.com>,"I actually do agree with this - let's see if we can find a solution
that doesn't regress this behavior. Maybe we can simply move the one
kafka example into its own project instead of having it in the
examples project.


---------------------------------------------------------------------


"
Sandy Ryza <sandy.ryza@cloudera.com>,"Wed, 12 Nov 2014 23:07:32 -0800",Re: [NOTICE] [BUILD] Minor changes to Spark's build,Patrick Wendell <pwendell@gmail.com>,"Currently there are no mandatory profiles required to build Spark.  I.e.
""mvn package"" just works.  It seems sad that we would need to break this.


"
MaChong <machongmc@sina.com>,"Thu, 13 Nov 2014 15:55:37 +0800",Problems with spark.locality.wait,dev <dev@spark.apache.org>,"Hi,

We are running a time sensitive application with 70 partition and 800MB each parition size. The application first load data from database in different cluster, then apply a filter, cache the filted data, then apply a map and a reduce, finally collect results.
The application will be finished in 20 seconds if we set spark.locality.wait to a large value (30 minutes). And it will use 100 seconds, if we set spark.locality.wait a small value(less than 10 seconds) 
We have analysed the driver log and found lot of NODE_LOCAL and RACK_LOCAL level tasks, normally a PROCESS_LOCAL task only takes 15 seconds, but NODE_LOCAL or RACK_LOCAL tasks will take 70 seconds.

So I think we'd better set spark.locality.wait to a large value(30 minutes), until we meet this problem: 

Now our application will load data from hdfs in the same spark cluster, it will get NODE_LOCAL and RACK_LOCAL level tasks during loading stage, if the tasks in loading stage have same locality level, ether NODE_LOCAL or RACK_LOCAL it works fine. 
But if the tasks in loading stage get mixed locality level, such as 3 NODE_LOCAL tasks, and 2 RACK_LOCAL tasks, then the TaskSetManager of loading stage will submit the 3 NODE_LOCAL tasks as soon as resources were offered, then wait for spark.locality.wait.node, which was setted to 30 minutes, the 2 RACK_LOCAL tasks will wait 30 minutes even though resources are avaliable.


Does any one have met this problem? Do you have a nice solution?


Thanks




Ma chong
"
Krishna Sankar <ksankar42@gmail.com>,"Thu, 13 Nov 2014 00:18:21 -0800",Re: [VOTE] Release Apache Spark 1.1.1 (RC1),Andrew Or <andrew@databricks.com>,"+1
1. Compiled OSX 10.10 (Yosemite) mvn -Pyarn -Phadoop-2.4
-Dhadoop.version=2.4.0 -DskipTests clean package 10:49 min
2. Tested pyspark, mlib
2.1. statistics OK
2.2. Linear/Ridge/Laso Regression OK
2.3. Decision Tree, Naive Bayes OK
2.4. KMeans OK
2.5. r"
Sean Owen <sowen@cloudera.com>,"Thu, 13 Nov 2014 10:05:59 +0000",Re: [VOTE] Release Apache Spark 1.1.1 (RC1),Andrew Or <andrew@databricks.com>,"LICENSE and NOTICE are fine. Signature and checksum is fine. I
unzipped and built the plain source distribution, which built.

However I am seeing a consistent test failure with ""mvn -DskipTests
clean package; mvn test"". In the Hive module:

- SET commands semantics for a HiveContext *** FAILED ***
  Expected Array(""spark.sql.key.usedfortestonly=test.val.0"",
""spark.sql.key.usedfortestonlyspark.sql.key.usedfortestonly=test.val.0test.val.0""),
but got Array(""spark.sql.key.usedfortestonlyspark.sql.key.usedfortestonly=test.val.0test.val.0"",
""spark.sql.key.usedfortestonly=test.val.0"") (HiveQuerySuite.scala:544)

Anyone else seeing this?



---------------------------------------------------------------------


"
=?UTF-8?B?5aSP5L+K6bi+?= <xiajunluan@gmail.com>,"Thu, 13 Nov 2014 21:07:02 +0800",Join operator in PySpark,dev@spark.apache.org,"Hi all

    I have noticed that â€œJoinâ€ operator has been transferred to union and
groupByKey operator instead of cogroup operator in PySpark, this change
will probably generate more shuffle stage, for example

    rdd1 = sc.makeRDD(...).partitionBy(2)
    rdd2 = sc.makeRDD(...).partitionBy(2)
    rdd3 = rdd1.join(rdd2).collect()

    Above code implemented with scala will generate 2 shuffle, but will
generate 3 shuffle with python. what is initial design motivation of join
operator in PySpark? Any idea to improve join performance in PySpark?

Andrew
"
Sadhan Sood <sadhan.sood@gmail.com>,"Thu, 13 Nov 2014 12:27:45 -0500",Re: Cache sparkSql data without uncompressing it in memory,Cheng Lian <lian.cs.zju@gmail.com>,"Thanks Chneg, Just one more question - does that mean that we still need
enough memory in the cluster to uncompress the data before it can be
compressed again or does that just read the raw data as is?


irectly.
,
en
"
Sadhan Sood <sadhan.sood@gmail.com>,"Thu, 13 Nov 2014 12:27:45 -0500",Re: Cache sparkSql data without uncompressing it in memory,Cheng Lian <lian.cs.zju@gmail.com>,"Thanks Chneg, Just one more question - does that mean that we still need
enough memory in the cluster to uncompress the data before it can be
compressed again or does that just read the raw data as is?


irectly.
,
en
"
Marcelo Vanzin <vanzin@cloudera.com>,"Thu, 13 Nov 2014 10:05:17 -0800",Re: [NOTICE] [BUILD] Minor changes to Spark's build,Patrick Wendell <pwendell@gmail.com>,"Hello there,

So I just took a quick look at the pom and I see two problems with it.

- ""activatedByDefault"" does not work like you think it does. It only
""activates by default"" if you do not explicitly activate other
profiles. So if you do ""mvn package"", scala-2.10 will be activated;
but if you do ""mvn -Pyarn package"", it will not.

- you need to duplicate the ""activation"" stuff everywhere where the
profile is declared, not just in the root pom. (I spent quite some
time yesterday fighting a similar issue...)

My suggestion here is to change the activation of scala-2.10 to look like this:

<activation>
  <property>
    <name>!scala-2.11</name>
  </property>
</activation>

And change the scala-2.11 profile to do this:

<properties>
  <scala-2.11>true</scala-2.11>
</properties>

I haven't tested, but in my experience this will activate the
scala-2.10 profile by default, unless you explicitly activate the 2.11
profile, in which case that property will be set and scala-2.10 will
not activate. If you look at examples/pom.xml, that's the same
strategy used to choose which hbase profile to activate.

Ah, and just to reinforce, the activation logic needs to be copied to
other places (e.g. examples/pom.xml, repl/pom.xml, and any other place
that has scala-2.x profiles).






-- 
Marcelo

---------------------------------------------------------------------


"
Sandy Ryza <sandy.ryza@cloudera.com>,"Thu, 13 Nov 2014 10:15:24 -0800",Re: [NOTICE] [BUILD] Minor changes to Spark's build,Marcelo Vanzin <vanzin@cloudera.com>,"https://github.com/apache/spark/pull/3239 addresses this


"
Ashutosh <ashutosh.trivedi@iiitb.org>,"Thu, 13 Nov 2014 11:31:11 -0700 (MST)",Re: [MLlib] Contributing Algorithm for Outlier Detection,dev@spark.incubator.apache.org,"Hi Anant,

Please see the changes.

https://github.com/codeAshu/Outlier-Detection-with-AVF-Spark/blob/master/OutlierWithAVFModel.scala


I have changed the input format to Vector of String. I think we can also make it generic.


Line 59 & 72 : that counter will not affect in parallelism, Since it only work on one datapoint. It  only                         does the Indexing of the column.


Rest all side effects have been removed.

â€‹

Thanks,

Ashutosh




________________________________
From: slcclimber [via Apache Spark Developers List] <ml-node+s1001551n9287h73@n3.nabble.com>
Sent: Tuesday, November 11, 2014 11:46 PM
To: Ashutosh Trivedi (MT2013030)
Subject: Re: [MLlib] Contributing Algorithm for Outlier Detection


Mayur,
Libsvm format sounds good to me. I could work on writing the tests if that helps you?
Anant


Hi Mayur,

Vector data types are implemented using breeze library, it is presented at

.../org/apache/spark/mllib/linalg


Anant,

ly restrict the user.

What are you thoughts on LibSVM format?

Thanks for the comments, I was just trying to get away from those increment /decrement functions, they look ugly. Points are noted. I'll try to fix them soon. Tests are also required for the code.


Regards,

Ashutosh


________________________________
From: Mayur Rustagi [via Apache Spark Developers List] <ml-node+[hidden email]<http://user/SendEmail.jtp?type=node&node=9286&i=0>>
Sent: Saturday, November 8, 2014 12:52 PM
To: Ashutosh Trivedi (MT2013030)
Subject: Re: [MLlib] Contributing Algorithm for Outlier Detection


What do you mean by vector datatype exactly?

Mayur Rustagi
Ph: <a href=""tel:%2B1%20%28760%29%20203%203257"" value=""+17602033257"" target=""_blank"">+1 (760) 203 3257
http://www.sigmoidanalytics.com
@mayur_rustagi <https://twitter.com/mayur_rustagi>



n
le
he
e
ON
t]
OutlierWithAVFModel.scala
OutlierWithAVFModel.scala
Â·
OutlierWithAVFModel.scala
:
t I think
]""
=0>>
f
st
ing-Algorithm-for-Outlier-Detection-tp8880p9034.html
rvlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml
ing-Algorithm-for-Outlier-Detection-tp8880p9035.html
rvlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml
ing-Algorithm-for-Outlier-Detection-tp8880p9036.html
rvlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml
on
ing-Algorithm-for-Outlier-Detection-tp8880p9037.html
rvlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml
on
ing-Algorithm-for-Outlier-Detection-tp8880p9083.html
rvlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml
ing-Algorithm-for-Outlier-Detection-tp8880p9095.html


________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/MLlib-Contributing-Algorithm-for-Outlier-Detection-tp8880p9239.html
lick here.
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>


________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/MLlib-Contributing-Algorithm-for-Outlier-Detection-tp8880p9286.html
lick here.
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>


________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/MLlib-Contributing-Algorithm-for-Outlier-Detection-tp8880p9287.html
lick here<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=8880&code=YXNodXRvc2gudHJpdmVkaUBpaWl0Yi5vcmd8ODg4MHwtMzkzMzE5NzYx>.
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>




--
3.nabble.com/MLlib-Contributing-Algorithm-for-Outlier-Detection-tp8880p9327.html
om."
Patrick Wendell <pwendell@gmail.com>,"Thu, 13 Nov 2014 10:49:04 -0800",Re: [NOTICE] [BUILD] Minor changes to Spark's build,Sandy Ryza <sandy.ryza@cloudera.com>,"Hey Marcelo,

I'm not sure chaining activation works like that. At least in my
experience activation based on properties only works for properties
explicitly specified at the command line rather than declared
elsewhere in the pom.

https://gist.github.com/pwendell/6834223e68f254e6945e

I any case, I think Prashant just didn't document that his patch
required -Pscala-2.10 explicitly, which is what he said further up in
the thread. And Sandy has a solution that has better behavior than
that, which is nice.

- Patrick


---------------------------------------------------------------------


"
Marcelo Vanzin <vanzin@cloudera.com>,"Thu, 13 Nov 2014 10:53:27 -0800",Re: [NOTICE] [BUILD] Minor changes to Spark's build,Patrick Wendell <pwendell@gmail.com>,"Hey Patrick,


That's true, but note the code I posted activates a profile based on
the lack of a property being set, which is why it works. Granted, I
did not test that if you activate the other profile, the one with the
property check will be disabled.


Yeah, I saw Sandy's patch now and it's probably a better solution
(since it doesn't abuse the sort of tricky maven profile stuff as
much).

-- 
Marcelo

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Thu, 13 Nov 2014 10:58:37 -0800",Re: [NOTICE] [BUILD] Minor changes to Spark's build,Marcelo Vanzin <vanzin@cloudera.com>,"
Ah yeah good call - I so then we'd trigger 2.11-vs-not based on the
presence of -Dscala-2.11.

Would that fix this issue then? It might be a simpler fix to merge
into the 1.2 branch than Sandy's patch since we're pretty late in the
game (though that patch does other things separately that I'd like to
see end up in Spark soon).

---------------------------------------------------------------------


"
Marcelo Vanzin <vanzin@cloudera.com>,"Thu, 13 Nov 2014 11:03:04 -0800",Re: [NOTICE] [BUILD] Minor changes to Spark's build,Patrick Wendell <pwendell@gmail.com>,"
Yeah, that's the idea. As for simplicity, I think Sandy's patch would
be just as simple if it avoided all the changes to isolate the
examples / external stuff into different profiles.

-- 
Marcelo

---------------------------------------------------------------------


"
Kay Ousterhout <keo@eecs.berkeley.edu>,"Thu, 13 Nov 2014 11:20:40 -0800",Re: Problems with spark.locality.wait,MaChong <machongmc@sina.com>,"Hi,

Shivaram and I stumbled across this problem a few weeks ago, and AFAIK
there is no nice solution.  We worked around it by avoiding jobs with tasks
that have tasks with two locality levels.

To fix this problem, we really need to fix the underlying problem in the
scheduling code, which currently tries to schedule all tasks at the minimum
locality of any of the tasks in the job.  Unfortunately, this involves
adding a bunch of complexity to the scheduling code.

Patrick had previously convinced us that we were the only ones running into
this problem, so it wasn't worth fixing (especially because we found an OK
workaround for our case).  It would be useful to hear if any other folks
have run into this problem -- it sounds like maybe we should go ahead and
fix the scheduling code.

I've filed a JIRA to track this:
https://issues.apache.org/jira/browse/SPARK-4383

-Kay


"
Michael Armbrust <michael@databricks.com>,"Thu, 13 Nov 2014 11:26:14 -0800",Re: [VOTE] Release Apache Spark 1.1.1 (RC1),Sean Owen <sowen@cloudera.com>,"Hey Sean,

Thanks for pointing this out.  Looks like a bad test where we should be
doing Set comparison instead of Array.

Michael


"
Reynold Xin <rxin@databricks.com>,"Thu, 13 Nov 2014 11:57:33 -0800",Re: About implicit rddToPairRDDFunctions,Shixiong Zhu <zsxwing@gmail.com>,"Do people usually important o.a.spark.rdd._ ?

Also in order to maintain source and binary compatibility, we would need to
keep both right?



"
Andrew Or <andrew@databricks.com>,"Thu, 13 Nov 2014 12:01:01 -0800",Re: [VOTE] Release Apache Spark 1.1.1 (RC1),Michael Armbrust <michael@databricks.com>,"Yeah, this seems to be somewhat environment specific too. The same test has
been passing here for a while:
https://amplab.cs.berkeley.edu/jenkins/job/Spark-1.1-Maven-pre-YARN/hadoop.version=1.0.4,label=centos/lastBuild/consoleFull

2014-11-13 11:26 GMT-08:00 Michael Armbrust <michael@databricks.com>:

"
Josh Rosen <rosenville@gmail.com>,"Thu, 13 Nov 2014 12:03:39 -0800",Re: Join operator in PySpark,"=?utf-8?Q?=E5=A4=8F=E4=BF=8A=E9=B8=BE?= <xiajunluan@gmail.com>, 
 dev@spark.apache.org","We should implement this using cogroup(); it will just require some tracking to map Python partitioners into dummy Java ones so that Java Sparkâ€™s cogroup() operator respects Pythonâ€™s partitioning. Â Iâ€™m sure that there are some other subtleties, particularly if we mix datasets that use different serializers / Java object representations.

Thereâ€™s a longstanding JIRA to fix this:Â https://issues.apache.org/jira/browse/SPARK-655


Hi all  

I have noticed that â€œJoinâ€ operator has been transferred to union and  
groupByKey operator instead of cogroup operator in PySpark, this change  
will probably generate more shuffle stage, for example  

rdd1 = sc.makeRDD(...).partitionBy(2)  
rdd2 = sc.makeRDD(...).partitionBy(2)  
rdd3 = rdd1.join(rdd2).collect()  

Above code implemented with scala will generate 2 shuffle, but will  
generate 3 shuffle with python. what is initial design motivation of join  
operator in PySpark? Any idea to improve join performance in PySpark?  

Andrew  
"
Mridul Muralidharan <mridul@gmail.com>,"Fri, 14 Nov 2014 01:41:05 +0530",Re: Problems with spark.locality.wait,MaChong <machongmc@sina.com>,"Instead of setting spark.locality.wait, try setting individual
locality waits specifically.

Namely, spark.locality.wait.PROCESS_LOCAL to high value (so that
process local tasks are always scheduled in case the task set has
process local tasks).
Set spark.locality.wait.NODE_LOCAL and spark.locality.wait.RACK_LOCAL
to low value - so that in case task set has no process local tasks,
both node local and rack local tasks are scheduled asap.



Kay's comment, IMO, is slightly general in nature - and I suspect
unless we overhaul how preferred locality is specified, and allow for
taskset specific hints for schedule, we cant resolve that IMO.


Regards,
Mridul



ach parition size. The application first load data from database in different cluster, then apply a filter, cache the filted data, then apply a map and a reduce, finally collect results.
ait to a large value (30 minutes). And it will use 100 seconds, if we set spark.locality.wait a small value(less than 10 seconds)
L level tasks, normally a PROCESS_LOCAL task only takes 15 seconds, but NODE_LOCAL or RACK_LOCAL tasks will take 70 seconds.
s), until we meet this problem:
t will get NODE_LOCAL and RACK_LOCAL level tasks during loading stage, if the tasks in loading stage have same locality level, ether NODE_LOCAL or RACK_LOCAL it works fine.
E_LOCAL tasks, and 2 RACK_LOCAL tasks, then the TaskSetManager of loading stage will submit the 3 NODE_LOCAL tasks as soon as resources were offered, then wait for spark.locality.wait.node, which was setted to 30 minutes, the 2 RACK_LOCAL tasks will wait 30 minutes even though resources are avaliable.

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Thu, 13 Nov 2014 20:10:33 +0000",Re: [VOTE] Release Apache Spark 1.1.1 (RC1),Andrew Or <andrew@databricks.com>,"Ah right. This is because I'm running Java 8. This was fixed in
SPARK-3329 (https://github.com/apache/spark/commit/2b7ab814f9bde65ebc57ebd04386e56c97f06f4a#diff-7bfd8d7c8cbb02aa0023e4c3497ee832).
Consider back-porting it if other reasons arise, but this is specific
to tests and to Java 8.


---------------------------------------------------------------------


"
Kay Ousterhout <keo@eecs.berkeley.edu>,"Thu, 13 Nov 2014 12:35:59 -0800",Re: Problems with spark.locality.wait,Mridul Muralidharan <mridul@gmail.com>,"Hi Mridul,

In the case Shivaram and I saw, and based on my understanding of Ma chong's
description, I don't think that completely fixes the problem.

To be very concrete, suppose your job has two tasks, t1 and t2, and they
each have input data (in HDFS) on h1 and h2, respectively, and that h1 and
h2 are on the same rack. Suppose your Spark job gets allocated two
executors, one on h1 and another on h3 (a different host with no input
data).  When the job gets submitted to the task set manager (TSM),
TSM.computeValidLocalityLevels will determine that the valid levels are
NODE_LOCAL (because t1 could be run on the NODE_LOCAL executor on h1),
RACK_LOCAL, ANY.  As a result, the TSM will not schedule t2 until
spark.locality.wait.NODE_LOCAL expires, even though t2 has no hope of being
scheduled on a NODE_LOCAL machine (because the job wasn't given any
executors on h2).  You could set spark.locality.wait.NODE_LOCAL to be low,
but then it might cause t1 (or more generally, in a larger job, other tasks
that have NODE_LOCAL executors where they can be scheduled) to get
scheduled on h3 (and not on h1).

Is there a way you were thinking of configuring things that avoids this
problem?

I'm pretty sure we could fix this problem by tracking more information
about each task in the TSM -- for example, the TSM has enough information
to know that there are no NODE_LOCAL executors where t2 could be scheduled
in the above example (and that the best possible locality level for t2 is
RACK_LOCAL), and could schedule t2 right away on a RACK_LOCAL machine.  Of
course, this would add a bunch of complexity to the TSM, hence the earlier
decision that the added complexity may not be worth it.

-Kay


"
Mridul Muralidharan <mridul@gmail.com>,"Fri, 14 Nov 2014 02:23:41 +0530",Re: Problems with spark.locality.wait,Kay Ousterhout <keo@eecs.berkeley.edu>,"In the specific example stated, the user had two taskset if I
understood right ... the first taskset reads off db (dfs in your
example), and does some filter, etc and caches it.
Second which works off the cached data (which is, now, process local
locality level aware) to do map, group, etc.

The taskset(s) which work off the cached data would be sensitive to
PROCESS_LOCAL locality level.
But for the initial taskset (which loaded off hdfs/database, etc) no
tasks can be process local - since we do not have a way to specify
that in spark (which, imo, is a limitation).

Given this, the requirement seemed to be to relax locality level for
initial load taskset - since not scheduling on rack local or other
nodes seems to be hurting utilization and latency when no node local
executors are available.
But for tasksets which have process local tasks, user wants to ensure
that node/rack local schedule does not happen (based on the timeouts
and perf numbers).

Hence my suggestion on setting the individual locality level timeouts
- ofcourse, my suggestion was highly specific to the problem as stated
:-)
It is, by no means, a generalization - and I do agree we definitely do
need to address the larger scheduling issue.

Regards,
Mridul




---------------------------------------------------------------------


"
Debasish Das <debasish.das83@gmail.com>,"Thu, 13 Nov 2014 16:19:30 -0800",TimSort in 1.2,dev <dev@spark.apache.org>,"Hi,

I am noticing the first step for Spark jobs does a TimSort in 1.2
branch...and there is some time spent doing the TimSort...Is this assigning
the RDD blocks to different nodes based on a sort order ?

Could someone please point to a JIRA about this change so that I can read
more about it ?

Thanks.
Deb
"
"""Ganelin, Ilya"" <Ilya.Ganelin@capitalone.com>","Thu, 13 Nov 2014 19:33:53 -0500",RE: Spark- How can I run MapReduce only on one partition in an RDD?,'Tim Chou' <timchou.hit@gmail.com>,"For testing purposes you can take a sample of your data with take() and then transform that smaller dataset into an rdd.

-----Original Message-----
From: Tim Chou [timchou.hit@gmail.com<mailto:timchou.hit@gmail.com>]
Sent: Thursday, November 13, 2014 06:41 PM Eastern Standard Time
To: Ganelin, Ilya
Subject: Re: Spark- How can I run MapReduce only on one partition in an RDD?

Hi Ganelin,
Thank you for your reply. I can actually get partitions information with partitions(). But I cannot change partition to a new RDD which I want to use.

I know it doesn't make sense if I only want to use one partition but create a large RDD.
I just want to map each partition one by one. So I can quickly get the early map result from the RDD.

That's why I want to read a file on HDFS to create multiple RDDs.

Any suggestions?

Thanks,
Tim

2014-11-13 17:05 GMT-06:00 Ganelin, Ilya <Ilya.Ganelin@capitalone.com<mailto:Ilya.Ganelin@capitalone.com>>:
Why do you only want the third partition? You can access individual partitions using the partitions() function. You can also filter your data using the filter() function to only contain the data you care about. Moreover, when you create your RDDs unless you define a custom partitioner you have no way of controlling what data is in partition #3. Therefore, there is almost no reason to want to operate on an individual partition.


-----Original Message-----
From: Tim Chou [timchou.hit@gmail.com<mailto:timchou.hit@gmail.com>]
Sent: Thursday, November 13, 2014 06:01 PM Eastern Standard Time
To: user@spark.apache.org<mailto:user@spark.apache.org>
Subject: Spark- How can I run MapReduce only on one partition in an RDD?

Hi All,

I use textFile to create a RDD. However, I don't want to handle the whole data in this RDD. For example, maybe I only want to solve the data in 3rd partition of the RDD.

How can I do it? Here are some possible solutions that I'm thinking:
1. Create multiple RDDs when reading the file
2.  Run MapReduce functions with the specific partition for an RDD.

However, I cannot find any appropriate function.

Thank you and wait for your suggestions.

Best,
Tim

________________________________

The information contained in this e-mail is confidential and/or proprietary to Capital One and/or its affiliates. The information transmitted herewith is intended only for use by the individual or entity to which it is addressed.  If the reader of this message is not the intended recipient, you are hereby notified that any review, retransmission, dissemination, distribution, copying or other use of, or taking of any action in reliance upon this information is strictly prohibited. If you have received this communication in error, please contact the sender and delete the material from your computer.

________________________________________________________

The information contained in this e-mail is confidential and/or proprietary to Capital One and/or its affiliates. The information transmitted herewith is intended only for use by the individual or entity to which it is addressed.  If the reader of this message is not the intended recipient, you are hereby notified that any review, retransmission, dissemination, distribution, copying or other use of, or taking of any action in reliance upon this information is strictly prohibited. If you have received this communication in error, please contact the sender and delete the material from your computer.
"
Reza Zadeh <reza@databricks.com>,"Thu, 13 Nov 2014 16:56:59 -0800",Re: TimSort in 1.2,Debasish Das <debasish.das83@gmail.com>,"See https://issues.apache.org/jira/browse/SPARK-2045
and https://issues.apache.org/jira/browse/SPARK-3280


"
Nathan Kronenfeld <nkronenfeld@oculusinfo.com>,"Thu, 13 Nov 2014 21:27:23 -0500",Re: Problems with spark.locality.wait,dev <dev@spark.apache.org>,"This sounds like it may be exactly the problem we've been having (and about
which I recently  posted on the user list).

Is there any way of monitoring it's attempts to wait, giving up, and trying
another level?

In general, I'm trying to figure out why we can have repeated identical
jobs, the first of which will have all PROCESS_LOCAL, and the next will
have 95% PROCESS_LOCAL, and 5% ANY.






-- 
Nathan Kronenfeld
Senior Visualization Developer
Oculus Info Inc
2 Berkeley Street, Suite 600,
Phone:  +1-416-203-3003 x 238
Email:  nkronenfeld@oculusinfo.com
"
Shixiong Zhu <zsxwing@gmail.com>,"Fri, 14 Nov 2014 11:13:47 +0800",Re: About implicit rddToPairRDDFunctions,Reynold Xin <rxin@databricks.com>,"If we put the `implicit` into ""pacakge object rdd"" or ""object rdd"", when we
write `rdd.groupbykey()`, because rdd is an object of RDD, Scala compiler
will search `object rdd`(companion object) and `package object rdd`(pacakge
object) by default. We don't need to import them explicitly. Here is a post
about the implicit search logic:
http://eed3si9n.com/revisiting-implicits-without-import-tax

To maintain the compatibility, we can keep `rddToPairRDDFunctions` in the
SparkContext but remove `implicit`. The disadvantage is there are two
copies of same codes.




Best Regards,
Shixiong Zhu

2014-11-14 3:57 GMT+08:00 Reynold Xin <rxin@databricks.com>:

"
Cheng Lian <lian.cs.zju@gmail.com>,"Fri, 14 Nov 2014 11:50:36 +0800",Re: Cache sparkSql data without uncompressing it in memory,Sadhan Sood <sadhan.sood@gmail.com>,"No, the columnar buffer is built in a small batching manner, the batch 
size is controlled by the |spark.sql.inMemoryColumnarStorage.batchSize| 
property. The default value for this in master and branch-1.2 is 10,000 
rows per batch.


â€‹
"
Cheng Lian <lian.cs.zju@gmail.com>,"Fri, 14 Nov 2014 11:50:36 +0800",Re: Cache sparkSql data without uncompressing it in memory,Sadhan Sood <sadhan.sood@gmail.com>,"No, the columnar buffer is built in a small batching manner, the batch 
size is controlled by the |spark.sql.inMemoryColumnarStorage.batchSize| 
property. The default value for this in master and branch-1.2 is 10,000 
rows per batch.


â€‹
"
MaChong <machongmc@sina.com>,"Fri, 14 Nov 2014 11:50:56 +0800",Re: Re: Problems with spark.locality.wait,"""Mridul Muralidharan"" <mridul@gmail.com>, 
	""Kay Ousterhout"" <keo@eecs.berkeley.edu>","In the specific example stated, the user had two taskset if I
understood right ... the first taskset reads off db (dfs in your
example), and does some filter, etc and caches it.
Second which works off the cached data (which is, now, process local
locality level aware) to do map, group, etc.

The taskset(s) which work off the cached data would be sensitive to
PROCESS_LOCAL locality level.
But for the initial taskset (which loaded off hdfs/database, etc) no
tasks can be process local - since we do not have a way to specify
that in spark (which, imo, is a limitation).

Given this, the requirement seemed to be to relax locality level for
initial load taskset - since not scheduling on rack local or other
nodes seems to be hurting utilization and latency when no node local
executors are available.
But for tasksets which have process local tasks, user wants to ensure
that node/rack local schedule does not happen (based on the timeouts
and perf numbers).

Hence my suggestion on setting the individual locality level timeouts
- ofcourse, my suggestion was highly specific to the problem as stated
:-)
It is, by no means, a generalization - and I do agree we definitely do
need to address the larger scheduling issue.

Regards,
Mridul



On Fri, Nov 14, 2014 at 2:05 AM, Kay Ousterhout <keo@eecs.berkeley.edu> wrote:
> Hi Mridul,
>
> In the case Shivaram and I saw, and based on my understanding of Ma chong's
> description, I don't think that completely fixes the problem.
>
> To be very concrete, suppose your job has two tasks, t1 and t2, and they
> each have input data (in HDFS) on h1 and h2, respectively, and that h1 and
> h2 are on the same rack. Suppose your Spark job gets allocated two
> executors, one on h1 and another on h3 (a different host with no input
> data).  When the job gets submitted to the task set manager (TSM),
> TSM.computeValidLocalityLevels will determine that the valid levels are
> NODE_LOCAL (because t1 could be run on the NODE_LOCAL executor on h1),
> RACK_LOCAL, ANY.  As a result, the TSM will not schedule t2 until
> spark.locality.wait.NODE_LOCAL expires, even though t2 has no hope of being
> scheduled on a NODE_LOCAL machine (because the job wasn't given any
> executors on h2).  You could set spark.locality.wait.NODE_LOCAL to be low,
> but then it might cause t1 (or more generally, in a larger job, other tasks
> that have NODE_LOCAL executors where they can be scheduled) to get scheduled
> on h3 (and not on h1).
>
> Is there a way you were thinking of configuring things that avoids this
> problem?
>
> I'm pretty sure we could fix this problem by tracking more information about
> each task in the TSM -- for example, the TSM has enough information to know
> that there are no NODE_LOCAL executors where t2 could be scheduled in the
> above example (and that the best possible locality level for t2 is
> RACK_LOCAL), and could schedule t2 right away on a RACK_LOCAL machine.  Of
> course, this would add a bunch of complexity to the TSM, hence the earlier
> decision that the added complexity may not be worth it.
>
> -Kay
>
> On Thu, Nov 13, 2014 at 12:11 PM, Mridul Muralidharan <mridul@gmail.com>
> wrote:
>>
>> Instead of setting spark.locality.wait, try setting individual
>> locality waits specifically.
>>
>> Namely, spark.locality.wait.PROCESS_LOCAL to high value (so that
>> process local tasks are always scheduled in case the task set has
>> process local tasks).
>> Set spark.locality.wait.NODE_LOCAL and spark.locality.wait.RACK_LOCAL
>> to low value - so that in case task set has no process local tasks,
>> both node local and rack local tasks are scheduled asap.
>>
>> From your description, this will alleviate the problem you mentioned.
>>
>>
>> Kay's comment, IMO, is slightly general in nature - and I suspect
>> unless we overhaul how preferred locality is specified, and allow for
>> taskset specific hints for schedule, we cant resolve that IMO.
>>
>>
>> Regards,
>> Mridul
>>
>>
>>
>> On Thu, Nov 13, 2014 at 1:25 PM, MaChong <machongmc@sina.com> wrote:
>> > Hi,
>> >
>> > We are running a time sensitive application with 70 partition and 800MB
>> > each parition size. The application first load data from database in
>> > different cluster, then apply a filter, cache the filted data, then apply a
>> > map and a reduce, finally collect results.
>> > The application will be finished in 20 seconds if we set
>> > spark.locality.wait to a large value (30 minutes). And it will use 100
>> > seconds, if we set spark.locality.wait a small value(less than 10 seconds)
>> > We have analysed the driver log and found lot of NODE_LOCAL and
>> > RACK_LOCAL level tasks, normally a PROCESS_LOCAL task only takes 15 seconds,
>> > but NODE_LOCAL or RACK_LOCAL tasks will take 70 seconds.
>> >
>> > So I think we'd better set spark.locality.wait to a large value(30
>> > minutes), until we meet this problem:
>> >
>> > Now our application will load data from hdfs in the same spark cluster,
>> > it will get NODE_LOCAL and RACK_LOCAL level tasks during loading stage, if
>> > the tasks in loading stage have same locality level, ether NODE_LOCAL or
>> > RACK_LOCAL it works fine.
>> > But if the tasks in loading stage get mixed locality level, such as 3
>> > NODE_LOCAL tasks, and 2 RACK_LOCAL tasks, then the TaskSetManager of loading
>> > stage will submit the 3 NODE_LOCAL tasks as soon as resources were offered,
>> > then wait for spark.locality.wait.node, which was setted to 30 minutes, the
>> > 2 RACK_LOCAL tasks will wait 30 minutes even though resources are avaliable.
>> >
>> >
>> > Does any one have met this problem? Do you have a nice solution?
>> >
>> >
>> > Thanks
>> >
>> >
>> >
>> >
>> > Ma chong
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


"
Meethu Mathew <meethu.mathew@flytxt.com>,"Fri, 14 Nov 2014 10:02:55 +0530",Re: [MLlib] Contributing Algorithm for Outlier Detection,"Ashutosh <ashutosh.trivedi@iiitb.org>, 
 ""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi Ashutosh,

Please edit the README file.I think the following function call is 
changed now.

|model = OutlierWithAVFModel.outliers(master:String, input dir:String , percentage:Double||)
|

Regards,

*Meethu Mathew*

*Engineer*

*Flytxt*

_<http://www.linkedin.com/home?trk=hb_tab_home_top>_


"
Reynold Xin <rxin@databricks.com>,"Thu, 13 Nov 2014 20:34:00 -0800",Re: About implicit rddToPairRDDFunctions,Shixiong Zhu <zsxwing@gmail.com>,"That seems like a great idea. Can you submit a pull request?



"
Shixiong Zhu <zsxwing@gmail.com>,"Fri, 14 Nov 2014 13:20:29 +0800",Re: About implicit rddToPairRDDFunctions,Reynold Xin <rxin@databricks.com>,"OK. I'll take it.

Best Regards,
Shixiong Zhu

2014-11-14 12:34 GMT+08:00 Reynold Xin <rxin@databricks.com>:

"
"""MaChong"" <machongmc@sina.com>","Fri, 14 Nov 2014 13:26:38 +0800 ",=?GBK?B?u9i4tKO6UmU6IFByb2JsZW1zIHdpdGggc3BhcmsubG9jYWxpdHkud2FpdA==?=,"""Mridul Muralidharan"" <mridul@gmail.com>,
  ""Kay Ousterhout"" <keo@eecs.berkeley.edu>,","
Hi Mridul, 
I have tried your method, it works fine for this case. I set locality.process to 30 minutes, locality.node and locality.rack to 3 seconds. I got loading stage's RACK level tasks submitted after 3 seconds wait, and only PROCESS tasks after loading stage.

Hi Kay, 
Our case is exactly same as what you say. I do agree with you that this is an issue need to be fixed. In my opinion, locality.wait is a simple way to handle locality problem, but not good enough. In fact I always disable the timeout in my application to get better performance. 
Every time I set the timeout to lower value, It will run into a loop that: some tasks slow -> locality wait timeout -> run tasks in poor locality -> network and memory overload-> more tasks become slow.
So I think may be a better locality strategy is needed, Such as determined by cache size or actual useable locality. 


Thanks for your guy's help.

--Ma Chong


--------------------------------


----- Ô­Ê¼ÓÊ¼ş -----
·¢¼şÈË£ºMridul Muralidharan <mridul@gmail.com>
ÊÕ¼şÈË£ºKay Ousterhout <keo@eecs.berkeley.edu>
³­ËÍÈË£ºMaChong <machongmc@sina.com>, dev <dev@spark.apache.org>
Ö÷Ìâ£ºRe: Problems with spark.locality.wait
ÈÕÆÚ£º2014Äê11ÔÂ14ÈÕ 04µã53·Ö

"
Meethu Mathew <meethu.mathew@flytxt.com>,"Fri, 14 Nov 2014 11:42:02 +0530",Re: [MLlib] Contributing Algorithm for Outlier Detection,"Ashutosh <ashutosh.trivedi@iiitb.org>, 
 ""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","
Hi,

I have a doubt regarding the input to your algorithm.
_<http://www.linkedin.com/home?trk=hb_tab_home_top>_

val model = OutlierWithAVFModel.outliers(data :RDD[Vector[String]], 
percent : Double, sc :SparkContext)


Here our input  data is an RDD[Vector[String]]. How we can create this 
RDD from a file? sc.textFile will simply give us an RDD, how to make it 
a Vector[String]?


Could you plz share any code snippet of this conversion if you have..


Regards,
Meethu Mathew


"
Ashutosh <ashutosh.trivedi@iiitb.org>,"Thu, 13 Nov 2014 23:23:20 -0700 (MST)",Re: [MLlib] Contributing Algorithm for Outlier Detection,dev@spark.incubator.apache.org,"Please use the following snippet. I am still working on to make it a generic vector, so that input
should not Vector[String] always. But String will work fine for now.


def main(args:Array[String])
 {
  val sc = new SparkContext(""local"", ""OutlierDetection"")
  val dir = ""hdfs://localhost:54310/train3""      <your file path>

   val data = sc.textFile(dir).map(word => word.split("","").toVector)
   val model = OutlierWithAVFModel.outliers(data,20,sc)

   model.score.saveAsTextFile(""../scores"")
   model.trimmed_data.saveAsTextFile("".../trimmed"")
 }


________________________________
From: Meethu Mathew-2 [via Apache Spark Developers List] <ml-node+s1001551n9352h88@n3.nabble.com>
Sent: Friday, November 14, 2014 11:42 AM
To: Ashutosh Trivedi (MT2013030)
Subject: Re: [MLlib] Contributing Algorithm for Outlier Detection


Hi,

I have a doubt regarding the input to your algorithm.
_<http://www.linkedin.com/home?trk=hb_tab_home_top>_

val model = OutlierWithAVFModel.outliers(data :RDD[Vector[String]],
percent : Double, sc :SparkContext)


Here our input  data is an RDD[Vector[String]]. How we can create this
RDD from a file? sc.textFile will simply give us an RDD, how to make it
a Vector[String]?


Could you plz share any code snippet of this conversion if you have..


Regards,
Meethu Mathew


 percentage:Double||)
/OutlierWithAVFModel.scala
 make it generic.
y work on one datapoint. It  only                         does the Indexing of the column.
r/SendEmail.jtp?type=node&node=9352&i=0>>
at helps you?
at
ually restrict the user.
ent /decrement functions, they look ugly. Points are noted. I'll try to fix them soon. Tests are also required for the code.
email]<http://user/SendEmail.jtp?type=node&node=9286&i=0>>
 target=""_blank"">+1 (760) 203 3257
 in
tyle
 the
l.
d
ide
JSON
er
ig
.
he
ist]
h
r/OutlierWithAVFModel.scala
r/OutlierWithAVFModel.scala
er Â·
ing
r/OutlierWithAVFModel.scala
""
e:
that I think
n
ist]""
=0>>
of
e
.
est
uting-Algorithm-for-Outlier-Detection-tp8880p9034.html
Servlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml
uting-Algorithm-for-Outlier-Detection-tp8880p9035.html
Servlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml
uting-Algorithm-for-Outlier-Detection-tp8880p9036.html
Servlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml
ssion
uting-Algorithm-for-Outlier-Detection-tp8880p9037.html
Servlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml
ssion
uting-Algorithm-for-Outlier-Detection-tp8880p9083.html
Servlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml
uting-Algorithm-for-Outlier-Detection-tp8880p9095.html
 below:
ting-Algorithm-for-Outlier-Detection-tp8880p9239.html
, click here.
NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
 below:
ting-Algorithm-for-Outlier-Detection-tp8880p9286.html
, click here.
NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
 below:
ting-Algorithm-for-Outlier-Detection-tp8880p9287.html
, click here<
NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
1.n3.nabble.com/MLlib-Contributing-Algorithm-for-Outlier-Detection-tp8880p9327.html
e.com.



________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/MLlib-Contributing-Algorithm-for-Outlier-Detection-tp8880p9352.html
lick here<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=8880&code=YXNodXRvc2gudHJpdmVkaUBpaWl0Yi5vcmd8ODg4MHwtMzkzMzE5NzYx>.
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>




--
3.nabble.com/MLlib-Contributing-Algorithm-for-Outlier-Detection-tp8880p9353.html
om."
Priya Ch <learnings.chitturi@gmail.com>,"Fri, 14 Nov 2014 17:17:26 +0530",1gb file processing...task doesn't launch on all the node...Unseen exception,"user@spark.apache.org, dev@spark.apache.org","Hi All,

  We have set up 2 node cluster (NODE-DSRV05 and NODE-DSRV02) each is
having 32gb RAM and 1 TB hard disk capacity and 8 cores of cpu. We have set
up hdfs which has 2 TB capacity and the block size is 256 mb   When we try
to process 1 gb file on spark, we see the following exception

14/11/14 17:01:42 INFO scheduler.TaskSetManager: Starting task 0.0 in stage
0.0 (TID 0, NODE-DSRV05.impetus.co.in, NODE_LOCAL, 1667 bytes)
14/11/14 17:01:42 INFO scheduler.TaskSetManager: Starting task 1.0 in stage
0.0 (TID 1, NODE-DSRV05.impetus.co.in, NODE_LOCAL, 1667 bytes)
14/11/14 17:01:42 INFO scheduler.TaskSetManager: Starting task 2.0 in stage
0.0 (TID 2, NODE-DSRV05.impetus.co.in, NODE_LOCAL, 1667 bytes)
14/11/14 17:01:43 INFO cluster.SparkDeploySchedulerBackend: Registered
executor: Actor[akka.tcp://sparkExecutor@IMPETUS-DSRV02:41124/user/Executor#539551156]
with ID 0
14/11/14 17:01:43 INFO storage.BlockManagerMasterActor: Registering block
manager NODE-DSRV05.impetus.co.in:60432 with 2.1 GB RAM
14/11/14 17:01:43 INFO storage.BlockManagerMasterActor: Registering block
manager NODE-DSRV02:47844 with 2.1 GB RAM
14/11/14 17:01:43 INFO network.ConnectionManager: Accepted connection from [
NODE-DSRV05.impetus.co.in/192.168.145.195:51447]
14/11/14 17:01:43 INFO network.SendingConnection: Initiating connection to [
NODE-DSRV05.impetus.co.in/192.168.145.195:60432]
14/11/14 17:01:43 INFO network.SendingConnection: Connected to [
NODE-DSRV05.impetus.co.in/192.168.145.195:60432], 1 messages pending
14/11/14 17:01:43 INFO storage.BlockManagerInfo: Added broadcast_1_piece0
in memory on NODE-DSRV05.impetus.co.in:60432 (size: 17.1 KB, free: 2.1 GB)
14/11/14 17:01:43 INFO storage.BlockManagerInfo: Added broadcast_0_piece0
in memory on NODE-DSRV05.impetus.co.in:60432 (size: 14.1 KB, free: 2.1 GB)
14/11/14 17:01:44 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 0.0
(TID 0, NODE-DSRV05.impetus.co.in): java.lang.NullPointerException:
        org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:609)
        org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:609)

org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
        org.apache.spark.scheduler.Task.run(Task.scala:54)

org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)

java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)

java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        java.lang.Thread.run(Thread.java:722)
14/11/14 17:01:44 INFO scheduler.TaskSetManager: Starting task 0.1 in stage
0.0 (TID 3, NODE-DSRV05.impetus.co.in, NODE_LOCAL, 1667 bytes)
14/11/14 17:01:44 INFO scheduler.TaskSetManager: Lost task 1.0 in stage 0.0
(TID 1) on executor NODE-DSRV05.impetus.co.in:
java.lang.NullPointerException (null) [duplicate 1]
14/11/14 17:01:44 INFO scheduler.TaskSetManager: Lost task 2.0 in stage 0.0
(TID 2) on executor NODE-DSRV05.impetus.co.in:
java.lang.NullPointerException (null) [duplicate 2]
14/11/14 17:01:44 INFO scheduler.TaskSetManager: Starting task 2.1 in stage
0.0 (TID 4, NODE-DSRV05.impetus.co.in, NODE_LOCAL, 1667 bytes)
14/11/14 17:01:44 INFO scheduler.TaskSetManager: Starting task 1.1 in stage
0.0 (TID 5, NODE-DSRV02, NODE_LOCAL, 1667 bytes)
14/11/14 17:01:44 INFO scheduler.TaskSetManager: Lost task 0.1 in stage 0.0
(TID 3) on executor NODE-DSRV05.impetus.co.in:
java.lang.NullPointerException (null) [duplicate 3]
14/11/14 17:01:44 INFO scheduler.TaskSetManager: Starting task 0.2 in stage
0.0 (TID 6, NODE-DSRV02, NODE_LOCAL, 1667 bytes)
14/11/14 17:01:44 INFO scheduler.TaskSetManager: Lost task 2.1 in stage 0.0
(TID 4) on executor NODE-DSRV05.impetus.co.in:
java.lang.NullPointerException (null) [duplicate 4]
14/11/14 17:01:44 INFO scheduler.TaskSetManager: Starting task 2.2 in stage
0.0 (TID 7, NODE-DSRV02, NODE_LOCAL, 1667 bytes)


What I see is, it couldnt launch tasks on NODE-DSRV05 and processing it on
single node i.e NODE-DSRV02. When we tried with 360 MB of data, I dont see
any exception but the entire processing is done by only one node. I couldnt
figure out where the issue lies.

Any suggestions on what kind of situations might cause such issue ?

Thanks,
Padma Ch
"
Akhil Das <akhil@sigmoidanalytics.com>,"Fri, 14 Nov 2014 19:40:03 +0530","Re: 1gb file processing...task doesn't launch on all the
 node...Unseen exception",Priya Ch <learnings.chitturi@gmail.com>,"It shows nullPointerException, your data could be corrupted? Try putting a
try catch inside the operation that you are doing, Are you running the
worker process on the master node also? If not, then only 1 node will be
doing the processing. If yes, then try setting the level of parallelism and
number of partitions while creating/transforming the RDD.

Thanks
Best Regards


"
Qiuzhuang Lian <qiuzhuang.lian@gmail.com>,"Fri, 14 Nov 2014 23:28:35 +0800",Skipping Bad Records in Spark,dev@spark.apache.org,"Hi,

MapReduce has the feature of skipping bad records. Is there any equivalent
in Spark? Should I use filter API to do this?

Thanks,
Qiuzhuang
"
"""Ganelin, Ilya"" <Ilya.Ganelin@capitalone.com>","Fri, 14 Nov 2014 10:38:24 -0500",Re: Skipping Bad Records in Spark,"Qiuzhuang Lian <qiuzhuang.lian@gmail.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","Hi Quizhuang - you have two options:
1) Within the map step define a validation function that will be executed
on every record.
2) Use the filter function to create a filtered dataset prior to
processing. 



________________________________________________________

The information contained in this e-mail is confidential and/or proprietary is intended only for use by the individual or entity to which it is addressed.  If the reader of this message is not the intended recipient, you are hereby notified that any review, retransmission, dissemination, distribution, copying or other use of, or taking of any action in reliance upon this information is strictly prohibited. If you have received this communication in error, please contact the sender and delete the material from your computer.


---------------------------------------------------------------------


"
Corey Nolet <cjnolet@gmail.com>,"Fri, 14 Nov 2014 10:43:49 -0500",Spark & Hadoop 2.5.1,dev <dev@spark.apache.org>,"I noticed Spark 1.2.0-SNAPSHOT still has 2.4.x in the pom. Since 2.5.x is
the current stable Hadoop 2.x, would it make sense for us to update the
poms?
"
Sean Owen <sowen@cloudera.com>,"Fri, 14 Nov 2014 15:46:09 +0000",Re: Spark & Hadoop 2.5.1,Corey Nolet <cjnolet@gmail.com>,"I don't think it's necessary. You're looking at the hadoop-2.4
profile, which works with anything >= 2.4. AFAIK there is no further
specialization needed beyond that. The profile sets hadoop.version to
2.4.0 by default, but this can be overridden.


---------------------------------------------------------------------


"
Corey Nolet <cjnolet@gmail.com>,"Fri, 14 Nov 2014 11:48:26 -0500",Re: Spark & Hadoop 2.5.1,Sean Owen <sowen@cloudera.com>,"In the past, I've built it by providing -Dhadoop.version=2.5.1 exactly like
you've mentioned. What prompted me to write this email was that I did not
see any documentation that told me Hadoop 2.5.1 was officially supported by
Spark (i.e. community has been using it, any bugs are being fixed, etc...).
It builds, tests pass, etc... but there could be other implications that I
have not run into based on my own use of the framework.

If we are saying that the standard procedure is to build with the
hadoop-2.4 profile and override the -Dhadoop.version property, should we
provide that on the build instructions [1] at least?

[1] http://spark.apache.org/docs/latest/building-with-maven.html


"
sandy.ryza@cloudera.com,"Fri, 14 Nov 2014 08:57:59 -0800",Re: Spark & Hadoop 2.5.1,Corey Nolet <cjnolet@gmail.com>,"You're the second person to request this today. Planning to include this in my PR for Spark-4338.

-Sandy

ike
y
.

s

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Fri, 14 Nov 2014 17:02:57 +0000",Re: Spark & Hadoop 2.5.1,Corey Nolet <cjnolet@gmail.com>,"Yeah I think someone even just suggested that today in a separate
thread? couldn't hurt to just add an example.


---------------------------------------------------------------------


"
Andrew Or <andrew@databricks.com>,"Fri, 14 Nov 2014 10:45:55 -0800",Re: [VOTE] Release Apache Spark 1.1.1 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all, since the vote ends on a Sunday, please let me know if you would
like to extend the deadline to allow more time for testing.

2014-11-13 12:10 GMT-08:00 Sean Owen <sowen@cloudera.com>:

"
Matei Zaharia <matei.zaharia@gmail.com>,"Fri, 14 Nov 2014 11:18:07 -0800",Re: [VOTE] Release Apache Spark 1.1.1 (RC1),Andrew Or <andrew@databricks.com>,"+1

Tested on Mac OS X, and verified that sort-based shuffle bug is fixed.

Matei

would
https://github.com/apache/spark/commit/2b7ab814f9bde65ebc57ebd04386e56c97f06f4a#diff-7bfd8d7c8cbb02aa0023e4c3497ee832
test
https://amplab.cs.berkeley.edu/jenkins/job/"
Patrick Wendell <pwendell@gmail.com>,"Fri, 14 Nov 2014 12:17:54 -0800",Has anyone else observed this build break?,"""dev@spark.apache.org"" <dev@spark.apache.org>","A recent patch broke clean builds for me, I am trying to see how
widespread this issue is and whether we need to revert the patch.

The error I've seen is this when building the examples project:

spark-examples_2.10: Could not resolve dependencies for project
org.apache.spark:spark-examples_2.10:jar:1.2.0-SNAPSHOT: Could not
find artifact jdk.tools:jdk.tools:jar:1.7 at specified path
/System/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Home/../lib/tools.jar

The reason for this error is that hbase-annotations is using a
""system"" scoped dependency in their hbase-annotations pom, and this
doesn't work with certain JDK layouts such as that provided on Mac OS:

http://central.maven.org/maven2/org/apache/hbase/hbase-annotations/0.98.7-hadoop2/hbase-annotations-0.98.7-hadoop2.pom

Has anyone else seen this or is it just me?

- Patrick

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Fri, 14 Nov 2014 12:19:13 -0800",Re: Has anyone else observed this build break?,"""dev@spark.apache.org"" <dev@spark.apache.org>","A work around for this fix is identified here:
http://dbknickerbocker.blogspot.com/2013/04/simple-fix-to-missing-toolsjar-in-jdk.html

However, if this affects more users I'd prefer to just fix it properly
in our build.


---------------------------------------------------------------------


"
"""Hari Shreedharan"" <hshreedharan@cloudera.com>","Fri, 14 Nov 2014 12:24:43 -0800 (PST)",Re: Has anyone else observed this build break?,"""Patrick Wendell"" <pwendell@gmail.com>","Seems like a comment on that page mentions a fix, which would add yet another profile though â€” specifically telling mvn that if it is an apple jdk, use the classes.jar as the tools.jar as well, since Apple-packaged JDK 6 bundled them together.




Link: http://permalink.gmane.org/gmane.comp.java.maven-plugins.mojo.user/4320


I didnâ€™t test it, but maybe this can fix it?


Thanks,
Hari


r-in-jdk.html
/lib/tools.jar
7-hadoop2/hbase-annotations-0.98.7-hadoop2.pom
org"
Patrick Wendell <pwendell@gmail.com>,"Fri, 14 Nov 2014 12:32:29 -0800",Re: Has anyone else observed this build break?,Hari Shreedharan <hshreedharan@cloudera.com>,"I think in this case we can probably just drop that dependency, so
there is a simpler fix. But mostly I'm curious whether anyone else has
observed this.


---------------------------------------------------------------------


"
Zach Fry <zfry@palantir.com>,"Fri, 14 Nov 2014 17:25:10 -0700 (MST)",Re: [VOTE] Release Apache Spark 1.1.1 (RC1),dev@spark.incubator.apache.org,"+0

I expect to start testing on Monday but won't have enough results to change
my vote from +0
until Monday night or Tuesday morning.

Thanks,
Zach 



--

---------------------------------------------------------------------


"
Cheng Lian <lian.cs.zju@gmail.com>,"Sat, 15 Nov 2014 11:28:34 +0800",Re: [VOTE] Release Apache Spark 1.1.1 (RC1),dev@spark.incubator.apache.org,"+1

Tested HiveThriftServer2 against Hive 0.12.0 on Mac OS X. Known issues 
are fixed. Hive version inspection works as expected.



---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Sat, 15 Nov 2014 14:38:32 +0000",Re: Has anyone else observed this build break?,Patrick Wendell <pwendell@gmail.com>,"FWIW I do not see this on master with ""mvn -DskipTests clean package"".
I'm on OS X 10.10 and I build with Java 8 by default.


---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Sat, 15 Nov 2014 06:56:30 -0800",Re: Has anyone else observed this build break?,Patrick Wendell <pwendell@gmail.com>,"Sorry for the late reply.

I tested my patch on Mac with the following JDK:

java version ""1.7.0_60""
Java(TM) SE Runtime Environment (build 1.7.0_60-b19)
Java HotSpot(TM) 64-Bit Server VM (build 24.60-b09, mixed mode)

Let me see if the problem can be solved upstream in HBase hbase-annotations
module.

Cheers


"
Ted Yu <yuzhihong@gmail.com>,"Sat, 15 Nov 2014 07:27:21 -0800",Re: Has anyone else observed this build break?,Patrick Wendell <pwendell@gmail.com>,"I couldn't reproduce the problem using:

java version ""1.6.0_65""
Java(TM) SE Runtime Environment (build 1.6.0_65-b14-462-11M4609)
Java HotSpot(TM) 64-Bit Server VM (build 20.65-b04-462, mixed mode)

Since hbase-annotations is a transitive dependency, I created the following
pull request to exclude it from various hbase modules:
https://github.com/apache/spark/pull/3286

Cheers

https://github.com/apache/spark/pull/3286


"
Patrick Wendell <pwendell@gmail.com>,"Sat, 15 Nov 2014 10:30:11 -0800",Re: Has anyone else observed this build break?,Ted Yu <yuzhihong@gmail.com>,"Sounds like this is pretty specific to my environment so not a big
deal then. However, if we can safely exclude those packages it's worth
doing.


---------------------------------------------------------------------


"
"""Yiming \(John\) Zhang"" <sdiris@gmail.com>","Sun, 16 Nov 2014 09:41:41 +0800",mvn or sbt for studying and developing Spark?,<dev@spark.apache.org>,"Hi,

 

I am new in developing Spark and my current focus is about co-scheduling of
spark tasks. However, I am confused with the building tools: sometimes the
documentation uses mvn but sometimes uses sbt. 

 

So, my question is that which one is the preferred tool of Spark community?
And what's the technical difference between them? Thank you!

 

Cheers,

Yiming

"
"""Dinesh J. Weerakkody"" <dineshjweerakkody@gmail.com>","Sun, 16 Nov 2014 08:27:40 +0530",Re: mvn or sbt for studying and developing Spark?,sdiris@gmail.com,"Hi Yiming,

I believe that both SBT and MVN is supported in SPARK, but SBT is preferred
(I'm not 100% sure about this :) ). When I'm using MVN I got some build
failures. After that used SBT and works fine.

You can go through these discussions regarding SBT vs MVN and learn pros
and cons of both [1] [2].

[1]
http://apache-spark-developers-list.1001551.n3.nabble.com/DISCUSS-Necessity-of-Maven-and-SBT-Build-in-Spark-td2315.html

[2]
https://groups.google.com/forum/#!msg/spark-developers/OxL268v0-Qs/fBeBY8zmh3oJ

Thanks,




-- 
Thanks & Best Regards,

*Dinesh J. Weerakkody*
"
Sean Owen <sowen@cloudera.com>,"Sun, 16 Nov 2014 06:58:34 +0000",Re: mvn or sbt for studying and developing Spark?,"""Dinesh J. Weerakkody"" <dineshjweerakkody@gmail.com>","No, the Maven build is the main one.  I would use it unless you have a need
to use the SBT build in particular.

"
Vibhanshu Prasad <vibhanshugsoc2@gmail.com>,"Sun, 16 Nov 2014 16:52:35 +0530",Regarding RecordReader of spark,dev@spark.apache.org,"Hello Everyone,

I am going through the source code of rdd and Record readers
There are found 2 classes

1. WholeTextFileRecordReader
2. WholeCombineFileRecordReader  ( extends CombineFileRecordReader )

The description of both the classes is perfectly similar.

I am not able to understand why we have 2 classes. Is
CombineFileRecordReader providing some extra advantage?

Regards
Vibhanshu
"
"""Dinesh J. Weerakkody"" <dineshjweerakkody@gmail.com>","Sun, 16 Nov 2014 16:57:40 +0530",Re: mvn or sbt for studying and developing Spark?,Sean Owen <sowen@cloudera.com>,"Hi Stephen and Sean,

Thanks for correction.




-- 
Thanks & Best Regards,

*Dinesh J. Weerakkody*
"
scwf <wangfei1@huawei.com>,"Sun, 16 Nov 2014 05:24:57 -0700 (MST)",send currentJars and currentFiles to exetutor with actor?,dev@spark.incubator.apache.org,"I notice that spark serialize each task with the dependencies (files and JARs
added to the SparkContext) , 
  def serializeWithDependencies(
      task: Task[_],
      currentFiles: HashMap[String, Long],
      currentJars: HashMap[String, Long],
      serializer: SerializerInstance)
    : ByteBuffer = {

    val out = new ByteArrayOutputStream(4096)
    val dataOut = new DataOutputStream(out)

    // Write currentFiles
    dataOut.writeInt(currentFiles.size)
    for ((name, timestamp) <- currentFiles) {
      dataOut.writeUTF(name)
      dataOut.writeLong(timestamp)
    }

    // Write currentJars
    dataOut.writeInt(currentJars.size)
    for ((name, timestamp) <- currentJars) {
      dataOut.writeUTF(name)
      dataOut.writeLong(timestamp)
    }

    // Write the task itself and finish
    dataOut.flush()
    val taskBytes = serializer.serialize(task).array()
    out.write(taskBytes)
    ByteBuffer.wrap(out.toByteArray)
  }

Why not send currentJars and currentFiles to exetutor using actor? I think
it's not necessary to serialize them for each task. 



--

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Sun, 16 Nov 2014 14:12:27 +0000","If first batch fails, does Streaming JobGenerator.stop() hang?","dev <dev@spark.apache.org>, Tathagata Das <tathagata.das1565@gmail.com>","I thought I'd ask first since there's a good chance this isn't a
problem, but, I'm having a problem wherein the first batch that Spark
Streaming processes fails (due to an app problem), but then, stop()
blocks a very long time.

This bit of JobGenerator.stop() executes, since the message appears in the logs:


def haveAllBatchesBeenProcessed = {
  lastProcessedBatch != null && lastProcessedBatch.milliseconds == stopTime
}
logInfo(""Waiting for jobs to be processed and checkpoints to be written"")
while (!hasTimedOut && !haveAllBatchesBeenProcessed) {
  Thread.sleep(pollTime)
}

// ... 10x batch duration wait here, before seeing the next line log:

logInfo(""Waited for jobs to be processed and checkpoints to be written"")


I think that lastProcessedBatch is always null since no batch ever
succeeds. Of course, for all this code knows, the next batch might
succeed and so is there waiting for it. But it should proceed after
one more batch completes, even if it failed?

JobGenerator.onBatchCompleted is only called for a successful batch.
Can it be called if it fails too? I think that would fix it.

Should the condition also not be lastProcessedBatch.milliseconds <=
stopTime instead of == ?

Thanks for any pointers.

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Sun, 16 Nov 2014 12:28:06 -0800",Re: send currentJars and currentFiles to exetutor with actor?,scwf <wangfei1@huawei.com>,"The current design is not ideal, but the size of dependencies should be
fairly small since we only send the path and timestamp, not the jars
themselves.

Executors can come and go. This is essentially a state replication problem
that you gotta be very careful with consistency.


"
Michael Armbrust <michael@databricks.com>,"Sun, 16 Nov 2014 13:23:45 -0800",Re: mvn or sbt for studying and developing Spark?,"""Dinesh J. Weerakkody"" <dineshjweerakkody@gmail.com>","I'm going to have to disagree here.  If you are building a release
distribution or integrating with legacy systems then maven is probably the
correct choice.  However most of the core developers that I know use sbt,
and I think its a better choice for exploration and development overall.
That said, this probably falls into the category of a religious argument so
you might want to look at both options and decide for yourself.

In my experience the SBT build is significantly faster with less effort
(and I think sbt is still faster even if you go through the extra effort of
installing zinc) and easier to read.  The console mode of sbt (just run
sbt/sbt and then a long running console session is started that will accept
further commands) is great for building individual subprojects or running
single test suites.  In addition to being faster since its a long running
JVM, its got a lot of nice features like tab-completion for test case names.

For example, if I wanted to see what test cases are available in the SQL
subproject you can do the following:

[marmbrus@michaels-mbp spark (tpcds)]$ sbt/sbt
[info] Loading project definition from
/Users/marmbrus/workspace/spark/project/project
[info] Loading project definition from
/Users/marmbrus/.sbt/0.13/staging/ad8e8574a5bcb2d22d23/sbt-pom-reader/project
[info] Set current project to spark-parent (in build
file:/Users/marmbrus/workspace/spark/)
--
 org.apache.spark.sql.CachedTableSuite
org.apache.spark.sql.DataTypeSuite
 org.apache.spark.sql.DslQuerySuite
org.apache.spark.sql.InsertIntoSuite
...

Another very useful feature is the development console, which starts an
interactive REPL including the most recent version of the code and a lot of
useful imports for some subprojects.  For example in the hive subproject it
automatically sets up a temporary database with a bunch of test data
pre-loaded:

$ sbt/sbt hive/console
...
import org.apache.spark.sql.hive._
import org.apache.spark.sql.hive.test.TestHive._
import org.apache.spark.sql.parquet.ParquetTestData
Welcome to Scala version 2.10.4 (Java HotSpot(TM) 64-Bit Server VM, Java
1.7.0_45).
Type in expressions to have them evaluated.
Type :help for more information.

scala> sql(""SELECT * FROM src"").take(2)
res0: Array[org.apache.spark.sql.Row] = Array([238,val_238], [86,val_86])

Michael


"
Sean Owen <sowen@cloudera.com>,"Sun, 16 Nov 2014 21:36:27 +0000",Re: mvn or sbt for studying and developing Spark?,Michael Armbrust <michael@databricks.com>,"Yeah, my comment was mostly reflecting the fact that mvn is what
creates the releases and is the 'build of reference', from which the
SBT build is generated. The docs were recently changed to suggest that
Maven is the default build and SBT is for advanced users. I find Maven
plays nicer with IDEs, or at least, IntelliJ.

SBT is faster for incremental compilation and better for anyone who
knows and can leverage SBT's model.

If someone's new to it all, I dunno, they're likelier to have fewer
problems using Maven to start? YMMV.


---------------------------------------------------------------------


"
Stephen Boesch <javadba@gmail.com>,"Sun, 16 Nov 2014 13:47:18 -0800",Re: mvn or sbt for studying and developing Spark?,Michael Armbrust <michael@databricks.com>,"HI Michael,
 That insight is useful.   Some thoughts:

* I moved from sbt to maven in June specifically due to Andrew Or's
describing mvn as the default build tool.  Developers should keep in mind
that jenkins uses mvn so we need to run mvn before submitting PR's - even
if sbt were used for day to day dev work
*  In addition, as Sean has alluded to, the Intellij seems to comprehend
the maven builds a bit more readily than sbt
* But for command line and day to day dev purposes:  sbt sounds great to
use  Those sound bites you provided about exposing built-in test databases
for hive and for displaying available testcases are sweet.  Any
easy/convenient way to see ""more of "" those kinds of facilities available
through sbt ?


2014-11-16 13:23 GMT-08:00 Michael Armbrust <michael@databricks.com>:

"
Mark Hamstra <mark@clearstorydata.com>,"Sun, 16 Nov 2014 13:47:38 -0800",Re: mvn or sbt for studying and developing Spark?,Michael Armbrust <michael@databricks.com>,"

We include the scala-maven-plugin in spark/pom.xml, so equivalent
functionality is available using Maven.  You can start a console session
with `mvn scala:console`.



"
Patrick Wendell <pwendell@gmail.com>,"Sun, 16 Nov 2014 13:50:04 -0800",Re: mvn or sbt for studying and developing Spark?,Mark Hamstra <mark@clearstorydata.com>,"Neither is strictly optimal which is why we ended up supporting both.
Our reference build for packaging is Maven so you are less likely to
run into unexpected dependency issues, etc. Many developers use sbt as
well. It's somewhat religion and the best thing might be to try both
and see which you prefer.

- Patrick


---------------------------------------------------------------------


"
Mark Hamstra <mark@clearstorydata.com>,"Sun, 16 Nov 2014 13:53:58 -0800",Re: mvn or sbt for studying and developing Spark?,Michael Armbrust <michael@databricks.com>,"Ok, strictly speaking, that's equivalent to your second class of
examples, ""development
console"", not the first ""sbt console""


"
jay vyas <jayunit100.apache@gmail.com>,"Sun, 16 Nov 2014 19:12:21 -0500",Is there a way for scala compiler to catch unserializable app code?,"""dev@spark.apache.org"" <dev@spark.apache.org>","This is more a curiosity than an immediate problem.

Here is my question: I ran into this easily solved issue
http://stackoverflow.com/questions/22592811/task-not-serializable-java-io-notserializableexception-when-calling-function-ou
recently.  The solution was to replace my ""class"" with a scala singleton,
which i guess is readily serializable.

So its clear that spark needs to serialize objects which carry the driver
methods for an app, in order to run... but I'm wondering,,, maybe there is
a way to change or update the spark API to catch unserializable spark apps
at compile time?


-- 
jay vyas
"
Reynold Xin <rxin@databricks.com>,"Sun, 16 Nov 2014 16:37:13 -0800",Re: Is there a way for scala compiler to catch unserializable app code?,jay vyas <jayunit100.apache@gmail.com>,"That's a great idea and it is also a pain point for some users. However, it
is not possible to solve this problem at compile time, because the content
of serialization can only be determined at runtime.

example project that is more researchy is Spore:
http://docs.scala-lang.org/sips/pending/spores.html




"
Andrew Ash <andrew@andrewash.com>,"Sun, 16 Nov 2014 16:44:34 -0800",Re: Is there a way for scala compiler to catch unserializable app code?,Reynold Xin <rxin@databricks.com>,"Hi Jay,

I just came across SPARK-720 Statically guarantee serialization will succeed
<https://issues.apache.org/jira/browse/SPARK-720> which sounds like exactly
what you're referring to.  Like Reynold I think it's not possible at this
time but it would be good to get your feedback on that ticket.

Andrew



"
Reynold Xin <rxin@databricks.com>,"Sun, 16 Nov 2014 16:49:31 -0800",Re: Regarding RecordReader of spark,Vibhanshu Prasad <vibhanshugsoc2@gmail.com>,"I don't think the code is immediately obvious.

Davies - I think you added the code, and Josh reviewed it. Can you guys
explain and maybe submit a patch to add more documentation on the whole
thing?

Thanks.



"
Josh Rosen <rosenville@gmail.com>,"Sun, 16 Nov 2014 17:08:40 -0800",Re: [VOTE] Release Apache Spark 1.1.1 (RC1),"""Spark Dev (Apache Incubator)"" <dev@spark.incubator.apache.org>","-1

I found a potential regression in 1.1.1 related to spark-submit and cluster
deploy mode: https://issues.apache.org/jira/browse/SPARK-4434

I think that this is worth fixing.


"
Andrew Ash <andrew@andrewash.com>,"Sun, 16 Nov 2014 17:27:37 -0800",Re: Regarding RecordReader of spark,Reynold Xin <rxin@databricks.com>,"Filed as https://issues.apache.org/jira/browse/SPARK-4437


"
Kousuke Saruta <sarutak@oss.nttdata.co.jp>,"Sun, 16 Nov 2014 17:29:53 -0800",Re: [VOTE] Release Apache Spark 1.1.1 (RC1),"Josh Rosen <rosenville@gmail.com>, 
 ""Spark Dev (Apache Incubator)"" <dev@spark.incubator.apache.org>","Now I've finished to revert for SPARK-4434 and opened PR.



---------------------------------------------------------------------


"
"""Yiming \(John\) Zhang"" <sdiris@gmail.com>","Mon, 17 Nov 2014 10:11:18 +0800",re: mvn or sbt for studying and developing Spark?,"""'Dinesh J. Weerakkody'"" <dineshjweerakkody@gmail.com>","Hi Dinesh, Sean, Michael, Stephen, Mark, and Patrick

 

Thank you for your reply and discussions. So the conclusion is that mvn is preferred when packaging and distribution, while sbt is better for development. This also explains why the compilation tool of make-distribution.sh changed from sbt (in spark-0.9) to mvn(in spark-1.0).

 

Cheers,

Yiming

 

å‘ä»¶äºº: Dinesh J. Weerakkody [mailto:dineshjweerakkody@gmail.com] 
å‘é€æ—¶é—´: 2014å¹´11æœˆ16æ—¥ 10:58
æ”¶ä»¶äºº: sdiris@gmail.com
æŠ„é€: dev@spark.apache.org
ä¸»é¢˜: Re: mvn or sbt for studying and developing Spark?

 

Hi Yiming,

I believe that both SBT and MVN is supported in SPARK, but SBT is preferred (I'm not 100% sure about this :) ). When I'm using MVN I got some build failures. After that used SBT and works fine.

You can go through these discussions regarding SBT vs MVN and learn pros and cons of both [1] [2].

[1] http://apache-spark-developers-list.1001551.n3.nabble.com/DISCUSS-Necessity-of-Maven-and-SBT-Build-in-Spark-td2315.html

[2] https://groups.google.com/forum/#!msg/spark-developers/OxL268v0-Qs/fBeBY8zmh3oJ

 

Thanks,

 


Hi,



I am new in developing Spark and my current focus is about co-scheduling of
spark tasks. However, I am confused with the building tools: sometimes the
documentation uses mvn but sometimes uses sbt.



So, my question is that which one is the preferred tool of Spark community?
And what's the technical difference between them? Thank you!



Cheers,

Yiming




-- 

Thanks & Best Regards,

Dinesh J. Weerakkody

"
Mark Hamstra <mark@clearstorydata.com>,"Sun, 16 Nov 2014 19:29:48 -0800",Re: mvn or sbt for studying and developing Spark?,yiming zhang <sdiris@gmail.com>,"More or less correct, but I'd add that there are an awful lot of software
systems out there that use Maven.  Integrating with those systems is
generally easier if you are also working with Spark in Maven.  (And I
wouldn't classify all of those Maven-built systems as ""legacy"", Michael :)
 What that ends up meaning is that if you are working *on* Spark, then SBT
can be more convenient and productive; but if you are working *with* Spark
along with other significant pieces of software, then using Maven can be
the better approach.


s
.
dy@gmail.com]
 10:58
me
ty-of-Maven-and-SBT-Build-in-Spark-td2315.html
zmh3oJ
of
e
y?
"
slcclimber <anant.asty@gmail.com>,"Sun, 16 Nov 2014 22:15:18 -0700 (MST)",Re: [MLlib] Contributing Algorithm for Outlier Detection,dev@spark.incubator.apache.org,"Ashutosh,
The counter will certainly be an parellization issue when multiple nodes are
used specially over massive datasets.
A better approach would be to use some thing along these lines:

    val index = sc.parallelize(Range.Long(0, rdd.count, 1),
rdd.partitions.size)
    val rddWithIndex = rdd.zip(index)
Which zips the two RDD's in a parallelizable fashion.




--

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Mon, 17 Nov 2014 01:42:47 -0800",[ANNOUNCE] Spark 1.2.0 Release Preview Posted,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi All,

I've just posted a preview of the Spark 1.2.0. release for community
regression testing.

Issues reported now will get close attention, so please help us test!
You can help by running an existing Spark 1.X workload on this and
reporting any regressions. As we start voting, etc, the bar for
reported issues to hold the release will get higher and higher, so
test early!

The tag to be is v1.2.0-snapshot1 (commit 38c1fbd96)

The release files, including signatures, digests, etc can be found at:
http://people.apache.org/~pwendell/spark-1.2.0-snapshot1

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1038/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-1.2.0-snapshot1-docs/

== Notes ==
- Maven artifacts are published for both Scala 2.10 and 2.11. Binary
distributions are not posted for Scala 2.11 yet, but will be posted
soon.

- There are two significant config default changes that users may want
to revert if doing A:B testing against older versions.

""spark.shuffle.manager"" default has changed to ""sort"" (was ""hash"")
""spark.shuffle.blockTransferService"" default has changed to ""netty"" (was ""nio"")

- This release contains a shuffle service for YARN. This jar is
present in all Hadoop 2.X binary packages in
""lib/spark-1.2.0-yarn-shuffle.jar""

Cheers,
Patrick

---------------------------------------------------------------------


"
Andrew Or <andrew@databricks.com>,"Mon, 17 Nov 2014 10:33:13 -0800",Re: [VOTE] Release Apache Spark 1.1.1 (RC1),Kousuke Saruta <sarutak@oss.nttdata.co.jp>,"This seems like a legitimate blocker. We will cut another RC to include the
revert.

2014-11-16 17:29 GMT-08:00 Kousuke Saruta <sarutak@oss.nttdata.co.jp>:

"
Debasish Das <debasish.das83@gmail.com>,"Mon, 17 Nov 2014 10:42:18 -0800",Re: [VOTE] Release Apache Spark 1.1.1 (RC1),Andrew Or <andrew@databricks.com>,"Andrew,

I put up 1.1.1 branch and I am getting shuffle failures while doing flatMap
followed by groupBy...My cluster memory is less than the memory I need and
therefore flatMap does around 400 GB of shuffle...memory is around 120 GB...

14/11/13 23:10:49 WARN TaskSetManager: Lost task 22.1 in stage 191.0 (TID
4084, istgbd020.hadoop.istg.verizon.com): FetchFailed(null, shuffleId=4,
mapId=-1, reduceId=22)

I searched on user-list and this issue has been found over there:

http://apache-spark-user-list.1001560.n3.nabble.com/Issues-with-partitionBy-FetchFailed-td14760.html

I wanted to make sure whether 1.1.1 does not have the same bug...-1 from me
till we figure out the root cause...

Thanks.

Deb


"
Alessandro Baretta <alexbaretta@gmail.com>,"Mon, 17 Nov 2014 11:11:01 -0800",Quantile regression in tree models,"""dev@spark.apache.org"" <dev@spark.apache.org>","I see that, as of v. 1.1, MLLib supports regression and classification tree
models. I assume this means that it uses a squared-error loss function for
the first and logistic cost function for the second. I don't see support
for quantile regression via an absolute error cost function. Or am I
missing something?

If, as it seems, this is missing, how do you recommend to implement it?

Alex
"
Michael Armbrust <michael@databricks.com>,"Mon, 17 Nov 2014 11:47:45 -0800",Re: mvn or sbt for studying and developing Spark?,Stephen Boesch <javadba@gmail.com>,"
To be clear, I think that the PR builder actually uses sbt
<https://github.com/apache/spark/blob/master/dev/run-tests#L198> currently,
but there are master builds that make sure maven doesn't break (amongst
other things).



Yeah, this is a very good point.  I have used `sbt/sbt gen-idea` in the
past, but I'm currently using the maven integration of inteliJ since it
seems more stable.



The Spark SQL developer readme
<https://github.com/apache/spark/tree/master/sql> has a little bit of this,
but we really should have some documentation on using SBT as well.

 Integrating with those systems is generally easier if you are also working


Also a good point, though I've seen some pretty clever uses of sbt's
external project references to link spark into other projects.  I'll
certainly admit I have a bias towards new shiny things in general though,
so my definition of legacy is probably skewed :)
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 17 Nov 2014 20:39:02 +0000",Re: mvn or sbt for studying and developing Spark?,"Michael Armbrust <michael@databricks.com>, Stephen Boesch <javadba@gmail.com>","The docs on using sbt are here:
https://github.com/apache/spark/blob/master/docs/building-spark.md#building-with-sbt

They'll be published with 1.2.0 presumably.
2:49 Michael Armbrust <michael@databricks.com>

nd
en
d
o
le
ng
"
Kevin Markey <kevin.markey@oracle.com>,"Mon, 17 Nov 2014 15:04:39 -0700",Re: [VOTE] Release Apache Spark 1.1.1 (RC1),dev@spark.apache.org,"+0 (non-binding)

Compiled Spark, recompiled and ran application with 1.1.1 RC1 with Yarn, 
plain-vanilla Hadoop 2.3.0. No regressions.

However, 12% to 22% increase in run time relative to 1.0.0 release.  (No 
other environment or configuration changes.)"
Patrick Wendell <pwendell@gmail.com>,"Mon, 17 Nov 2014 14:22:55 -0800",Re: [VOTE] Release Apache Spark 1.1.1 (RC1),Kevin Markey <kevin.markey@oracle.com>,"Hey Kevin,

If you are upgrading from 1.0.X to 1.1.X checkout the upgrade notes
here [1] - it could be that default changes caused a regression for
your workload. Do you still see a regression if you restore the
configuration changes?

It's great to hear specifically about issues like this, so please fork
a new thread and describe your workload if you see a regression. The
main focus of a patch release vote like this is to test regressions
against the previous release on the same line (e.g. 1.1.1 vs 1.1.0)
though of course we still want to be cognizant of 1.0-to-1.1
regressions and make sure we can address them down the road.

[1] https://spark.apache.org/releases/spark-release-1-1-0.html


---------------------------------------------------------------------


"
Manish Amde <manish9ue@gmail.com>,"Mon, 17 Nov 2014 14:24:07 -0800",Re: Quantile regression in tree models,Alessandro Baretta <alexbaretta@gmail.com>,"Hi Alessandro,

MLlib v1.1 supports variance for regression and gini impurity and entropy
for classification.
http://spark.apache.org/docs/latest/mllib-decision-tree.html

If the information gain calculation can be performed by distributed
aggregation then it might be possible to plug it into the existing
implementation. We want to perform such calculations (for e.g. median) for
the gradient boosting models (coming up in the 1.2 release) using absolute
error and deviance as loss functions but I don't think anyone is planning
to work on it yet. :-)

-Manish


"
Andrew Or <andrew@databricks.com>,"Mon, 17 Nov 2014 14:41:13 -0800",[VOTE][RESULT] Release Apache Spark 1.1.1 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","This is canceled in favor of RC2 with the following blockers:

https://issues.apache.org/jira/browse/SPARK-4434
https://issues.apache.org/jira/browse/SPARK-3633

The latter one involves a regression from 1.0.2 to 1.1.0, NOT from 1.1.0 to
1.1.1. For this reason, we are currently investigating this issue but may
not necessarily block on this to release 1.1.1.


2014-11-17 10:42 GMT-08:00 Debasish Das <debasish.das83@gmail.com>:

Andrew,
"
Alessandro Baretta <alexbaretta@gmail.com>,"Mon, 17 Nov 2014 15:24:30 -0800",Re: Quantile regression in tree models,Manish Amde <manish9ue@gmail.com>,"Manish,

Thanks for pointing me to the relevant docs. It is unfortunate that
absolute error is not supported yet. I can't seem to find a Jira for it.

Now, here's the what the comments say in the current master branch:
/**
 * :: Experimental ::
 * A class that implements Stochastic Gradient Boosting
 * for regression and binary classification problems.
 *
 * The implementation is based upon:
 *   J.H. Friedman.  ""Stochastic Gradient Boosting.""  1999.
 *
 * Notes:
 *  - This currently can be run with several loss functions.  However, only
SquaredError is
 *    fully supported.  Specifically, the loss function should be used to
compute the gradient
 *    (to re-label training instances on each iteration) and to weight weak
hypotheses.
 *    Currently, gradients are computed correctly for the available loss
functions,
 *    but weak hypothesis weights are not computed correctly for LogLoss or
AbsoluteError.
 *    Running with those losses will likely behave reasonably, but lacks
the same guarantees.
...
*/

By the looks of it, the GradientBoosting API would support an absolute
error type loss function to perform quantile regression, except for ""weak
hypothesis weights"". Does this refer to the weights of the leaves of the
trees?

Alex


"
Debasish Das <debasish.das83@gmail.com>,"Mon, 17 Nov 2014 19:32:13 -0800",Using sampleByKey,dev <dev@spark.apache.org>,"Hi,

I have a rdd whose key is a userId and value is (movieId, rating)...

I want to sample 80% of the (movieId,rating) that each userId has seen for
train, rest is for test...

val indexedRating = sc.textFile(...).map{x=> Rating(x(0), x(1), x(2))

val keyedRatings = indexedRating.map{x => (x.product, (x.user, x.rating))}

val keyedTraining = keyedRatings.sample(true, 0.8, 1L)

val keyedTest = keyedRatings.subtract(keyedTraining)

blocks = sc.maxParallelism

println(s""Rating keys ${keyedRatings.groupByKey(blocks).count()}"")

println(s""Training keys ${keyedTraining.groupByKey(blocks).count()}"")

println(s""Test keys ${keyedTest.groupByKey(blocks).count()}"")

My expectation was that the println will produce exact number of keys for
keyedRatings, keyedTraining and keyedTest but this is not the case...


Rating keys 3706

Training keys 3676

Test keys 3470

I also tried sampleByKey as follows:

val keyedRatings = indexedRating.map{x => (x.product, (x.user, x.rating))}

val fractions = keyedRatings.map{x=> (x._1, 0.8)}.collect.toMap

val keyedTraining = keyedRatings.sampleByKey(false, fractions, 1L)

val keyedTest = keyedRatings.subtract(keyedTraining)

Still I get the results as:

Rating keys 3706

Training keys 3682

Test keys 3459

Any idea what's is wrong here...

Are my assumptions about behavior of sample/sampleByKey on a key-value RDD
correct ? If this is a bug I can dig deeper...

Thanks.

Deb
"
liaoyuxi <liaoyuxi@huawei.com>,"Tue, 18 Nov 2014 03:24:20 +0000",matrix computation in spark,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hi,
Matrix computation is critical for algorithm efficiency like least square, Kalman filter and so on.
For now, the mllib module offers limited linear algebra on matrix, especially for distributed matrix.

We have been working on establishing distributed matrix computation APIs based on data structures in MLlib.
The main idea is to partition the matrix into sub-blocks, based on the strategy in the following paper.
http://www.cs.berkeley.edu/~odedsc/papers/bfsdfs-mm-ipdps13.pdf
In our experiment, it's communication-optimal.
But operations like factorization may not be appropriate to carry out in blocks.

Any suggestions and guidance are welcome.

Thanks,
Yuxi

"
Zongheng Yang <zongheng.y@gmail.com>,"Tue, 18 Nov 2014 03:36:54 +0000",Re: matrix computation in spark,"liaoyuxi <liaoyuxi@huawei.com>, 
	""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","There's been some work at the AMPLab on a distributed matrix library on top
of Spark; see here [1]. In particular, the repo contains a couple
factorization algorithms.

[1] https://github.com/amplab/ml-matrix

Zongheng


"
Manish Amde <manish9ue@gmail.com>,"Mon, 17 Nov 2014 20:25:09 -0800",Re: Quantile regression in tree models,Alessandro Baretta <alexbaretta@gmail.com>,"Hi Alessandro,

I think absolute error as splitting criterion might be feasible with the
current architecture -- I think the sufficient statistics we collect
currently might be able to support this. Could you let us know scenarios
where absolute error has significantly outperformed squared error for
regression trees? Also, what's your use case that makes squared error
undesirable.

For gradient boosting, you are correct. The weak hypothesis weights refer
to tree predictions in each of the branches. We plan to explain this in the
1.2 documentation and may be add some more clarifications to the Javadoc.

I will try to search for JIRAs or create new ones and update this thread.

-Manish


"
=?UTF-8?B?6aG+6I2j?= <gurongwalker@gmail.com>,"Tue, 18 Nov 2014 13:49:25 +0800",Re: matrix computation in spark,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","Hey Yuxi,

We also have implemented a distributed matrix multiplication library in
PasaLab. The repo is host on here https://github.com/PasaLab/marlin . We
implemented three distributed matrix multiplication algorithms on Spark. As
we see, communication-optimal does not always means the total-optimal.
Thus, besides the CARMA matrix multiplication you mentioned, we also
implemented the Block-splitting matrix multiplication and Broadcast matrix
multiplication. They are more efficient than the CARMA matrix
multiplication for some situations, for example a large matrix multiplies a
small matrix.

Actually, We have shared the work on Spark Meetup@Beijing on October 26th.(
http://www.meetup.com/spark-user-beijing-Meetup/events/210422112/ ). The
slide can be download from the archive here
http://pan.baidu.com/s/1dDoyHX3#path=%252Fmeetup-3rd

Best,
Rong

2014-11-18 13:11 GMT+08:00 é¡¾è£ <gurongwalker@gmail.com>:

As
x
 a
Is
in



-- 
------------------
Rong Gu
Department of Computer Science and Technology
State Key Laboratory for Novel Software Technology
Nanjing University
Phone: +86 15850682791
Email: gurongwalker@gmail.com
Homepage: http://pasa-bigdata.nju.edu.cn/people/ronggu/
"
liaoyuxi <liaoyuxi@huawei.com>,"Tue, 18 Nov 2014 06:50:23 +0000",=?utf-8?B?562U5aSNOiBtYXRyaXggY29tcHV0YXRpb24gaW4gc3Bhcms=?=,"Zongheng Yang <zongheng.y@gmail.com>,
        ""dev@spark.incubator.apache.org""
	<dev@spark.incubator.apache.org>","Hi,
I checked the work of ml-matrix. For now, it doesnâ€™t include matrix multiply and LU decomposition. Whatâ€™s your plan? Can we contribute our work to these parts?
Otherwise, the block number of row/column is decided manually, As we mentioned, the CARMA method in paper is communication-optimal.

å‘ä»¶äºº: Zongheng Yang [mailto:zongheng.y@gmail.com]
å‘é€æ—¶é—´: 2014å¹´11æœˆ18æ—¥ 11:37
æ”¶ä»¶äºº: liaoyuxi; dev@spark.incubator.apache.org
æŠ„é€: Shivaram Venkataraman
ä¸»é¢˜: Re: matrix computation in spark

There's been some work at the AMPLab on a distributed matrix library on top of Spark; see here [1]. In particular, the repo contains a couple factorization algorithms.

[1] https://github.com/amplab/ml-matrix

Zongheng

On Mon Nov 17 2014 at 7:34:17 PM liaoyuxi <liaoyuxi@huawei.com<mailto:liaoyuxi@huawei.com>> wrote:
Hi,
Matrix computation is critical for algorithm efficiency like least square, Kalman filter and so on.
For now, the mllib module offers limited linear algebra on matrix, especially for distributed matrix.

We have been working on establishing distributed matrix computation APIs based on data structures in MLlib.
The main idea is to partition the matrix into sub-blocks, based on the strategy in the following paper.
http://www.cs.berkeley.edu/~odedsc/papers/bfsdfs-mm-ipdps13.pdf
In our experiment, it's communication-optimal.
But operations like factorization may not be appropriate to carry out in blocks.

Any suggestions and guidance are welcome.

Thanks,
Yuxi
"
Reza Zadeh <reza@databricks.com>,"Mon, 17 Nov 2014 23:11:11 -0800",Re: matrix computation in spark,=?UTF-8?B?6aG+6I2j?= <gurongwalker@gmail.com>,"Hi Yuxi,

We are integrating the ml-matrix from the AMPlab repo into MLlib, tracked
by this JIRA: https://issues.apache.org/jira/browse/SPARK-3434

We already have matrix multiply, but are missing LU decomposition. Could
you please track that JIRA, once the initial design is in, we can sync on
how to contribute LU decomposition.

Let's move the discussion to the JIRA.

Thanks!


As
x
 a
e
.
/
n
he
t
"
Sean Owen <sowen@cloudera.com>,"Tue, 18 Nov 2014 10:34:08 +0100",Re: Using sampleByKey,Debasish Das <debasish.das83@gmail.com>,"I use randomSplit to make a train/CV/test set in one go. It definitely
produces disjoint data sets and is efficient. The problem is you can't
do it by key.

I am not sure why your subtract does not work. I suspect it is because
the values do not partition the same way, or they don't evaluate
equality in the expected way, but I don't see any reason why. Tuples
work as expected here.


---------------------------------------------------------------------


"
Ashutosh <ashutosh.trivedi@iiitb.org>,"Tue, 18 Nov 2014 03:40:14 -0700 (MST)",Re: [MLlib] Contributing Algorithm for Outlier Detection,dev@spark.incubator.apache.org,"Hi Anant,


I have removed the counter and all possible side effects. Now I think we can go ahead with the testing. I have created another folder for testing. I will add you as a collaborator in github .


_Ashutosh

________________________________
From: slcclimber [via Apache Spark Developers List] <ml-node+s1001551n9399h45@n3.nabble.com>
Sent: Monday, November 17, 2014 10:45 AM
To: Ashutosh Trivedi (MT2013030)
Subject: Re: [MLlib] Contributing Algorithm for Outlier Detection

Ashutosh,
The counter will certainly be an parellization issue when multiple nodes are used specially over massive datasets.
A better approach would be to use some thing along these lines:

    val index = sc.parallelize(Range.Long(0, rdd.count, 1), rdd.partitions.size)
    val rddWithIndex = rdd.zip(index)
Which zips the two RDD's in a parallelizable fashion.


________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/MLlib-Contributing-Algorithm-for-Outlier-Detection-tp8880p9399.html
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>




--"
Debasish Das <debasish.das83@gmail.com>,"Tue, 18 Nov 2014 06:59:34 -0800",Re: Using sampleByKey,Sean Owen <sowen@cloudera.com>,"Sean,

I thought sampleByKey (stratified sampling) in 1.1 was designed to solve
the problem that randomSplit can't sample by key...

Xiangrui,

What's the expected behavior of sampleByKey ? In the dataset sampled using
sampleByKey the keys should match the input dataset keys right ? If it is a
bug, I can open up a JIRA and look into it...

Thanks.
Deb


"
Alessandro Baretta <alexbaretta@gmail.com>,"Tue, 18 Nov 2014 10:07:54 -0800",Re: Quantile regression in tree models,Manish Amde <manish9ue@gmail.com>,"Manish,

My use case for (asymmetric) absolute error is quite trivially quantile
regression. In other words, I want to use Spark to learn conditional
cumulative distribution functions. See R's GBM quantile regression option.

If you either find or create a Jira ticket, I would be happy to give it a
shot. Is there a design doc explaining how the gradient boosting algorithm
is laid out in MLLib? I tried reading the code, but without a ""Rosetta
stone"" it's impossible to make sense of it.

Alex


"
Xiangrui Meng <mengxr@gmail.com>,"Tue, 18 Nov 2014 10:31:04 -0800",Re: Using sampleByKey,Debasish Das <debasish.das83@gmail.com>,"`sampleByKey` with the same fraction per stratum acts the same as
`sample`. The operation you want is perhaps `sampleByKeyExact` here.
However, when you use stratified sampling, there should not be many
strata. My question is why we need to split on each user's ratings. If
a user is missing in training and appears in test, we can simply
ignore it. -Xiangrui


---------------------------------------------------------------------


"
Debasish Das <debasish.das83@gmail.com>,"Tue, 18 Nov 2014 11:12:03 -0800",Re: Using sampleByKey,Xiangrui Meng <mengxr@gmail.com>,"For mllib PR, I will add this logic: ""If a user is missing in training and
appears in test, we can simply ignore it.""

I was struggling since users appear in test on which the model was not
trained on...

For our internal tests we want to cross validate on every product / user as
all of them are equally important and so I have to come up with a sampling
strategy for every user/product...

In general for stratified sampling what's the bound on strata ? Like number
of classes in a labeled dataset ~ 100 ?


"
Xiangrui Meng <mengxr@gmail.com>,"Tue, 18 Nov 2014 11:53:18 -0800",Re: Using sampleByKey,Debasish Das <debasish.das83@gmail.com>,"If all users are equally important, then the average score should be
representative. You shouldn't worry about missing one or two. For
stratified sampling, wikipedia has a paragraph about its disadvantage:

http://en.wikipedia.org/wiki/Stratified_sampling#Disadvantages

It depends on the size of the population. For example, in the US
Census survey sampling design, there are many (>> 100) strata:

https://www.census.gov/acs/www/Downloads/survey_methodology/Chapter_4_RevisedDec2010.pdf

If you indeed want to do the split per user, you should use groupByKey
and apply reservoir sampling for ratings from each user.

-Xiangrui


---------------------------------------------------------------------


"
Kyle Ellrott <kellrott@soe.ucsc.edu>,"Tue, 18 Nov 2014 14:23:07 -0800",Re: Implementing TinkerPop on top of GraphX,Kushal Datta <kushal.datta@gmail.com>,"The new Tinkerpop3 API was different enough from V2, that it was worth
starting a new implementation rather then trying to completely refactor my
old code.
I've started a new project: https://github.com/kellrott/spark-gremlin which
compiles and runs the first set of unit tests (which it completely fails).
Most of the classes are structured in the same way they are in the Gigraph
implementation. There isn't much actual GraphX code in the project yet,
just a framework to start working in.
Hopefully this will keep the conversation going.

Kyle


y
d
ht
in
nue
ge
est
he
ss
g,
o
).
able
 way
park
g the
eam.
B
c.).
This
ed to
t
al way
erpop3
 as
ators
t of
X.
r <
ll. At
o, I
the
rocess, and, if
erpop2
hack
ping
 it
m
u
s
 on
r
l
t
---
ntity
, or
tity
 or
"
Manish Amde <manish9ue@gmail.com>,"Tue, 18 Nov 2014 19:29:41 -0800",Re: Quantile regression in tree models,Alessandro Baretta <alexbaretta@gmail.com>,"Hi Alex,

Here is the ticket for refining tree predictions. Let's discuss this
further on the JIRA.
https://issues.apache.org/jira/browse/SPARK-4240

There is no ticket yet for quantile regression. It will be great if you
could create one and note down the corresponding loss function and gradient
calculations. There is a design doc that Joseph Bradley wrote for
supporting boosting algorithms with generic weak learners but it doesn't
include implementation details. I can definitely help you understand the
existing code if you decide to work on it. However, let's discuss the
relevance of the algorithm to MLlib on the JIRA. It seems like a nice
addition though I am not sure about the implementation complexity. I will
be great to see what others think.

-Manish


"
"""Yiming \(John\) Zhang"" <sdiris@gmail.com>","Wed, 19 Nov 2014 12:00:18 +0800",Intro to using IntelliJ to debug SPARK-1.1 Apps with mvn/sbt (for beginners),<dev@spark.apache.org>,"Hi,

 

I noticed it is hard to find a thorough introduction to using IntelliJ to
debug SPARK-1.1 Apps with mvn/sbt, which is not straightforward for
beginners. So I spent several days to figure it out and hope that it would
be helpful for beginners like me and that professionals can help me improve
it. (The intro with figures can be found at:
http://kylinx.com/spark/Debug-Spark-in-IntelliJ.htm)

 

(1) Install the Scala plugin

 

(2) Download, unzip and open spark-1.1.0 in IntelliJ 

a) mvn: File -> Open. 

    Select the Spark source folder (e.g., /root/spark-1.1.0). Maybe it will
take a long time to download and compile a lot of things

b) sbt: File -> Import Project. 

    Select ""Import project from external model"", then choose SBT project,
click Next. Input the Spark source path (e.g., /root/spark-1.1.0) for ""SBT
project"", and select Use auto-import.

 

(3) First compile and run spark examples in the console to ensure everything
OK

# mvn -Phadoop-2.2 -Dhadoop.version=2.2.0 -DskipTests clean package

# ./sbt/sbt assembly -Phadoop-2.2 -Dhadoop.version=2.2.0

 

(4) Add the compiled spark-hadoop library (spark-assembly-1.1.0-hadoop2.2.0)
to ""Libraries"" (File -> Project Structure. -> Libraries -> green +). And
choose modules that use it (right-click the library and click ""Add to
Modules""). It seems only spark-examples need it.

 

(5) In the ""Dependencies"" page of the modules using this library, ensure
that the ""Scope"" of this library is ""Compile"" (File -> Project Structure. ->
Modules)

(6) For sbt, it seems that we have to label the scope of all other hadoop
dependencies (SBT: org.apache.hadoop.hadoop-*) as ""Test"" (due to poor
Internet connection?) And this has to be done every time opening IntelliJ
(due to a bug?)

 

(7) Configure debug environment (using LogQuery as an example). Run -> Edit
Configurations.

Main class: org.apache.spark.examples.LogQuery

VM options: -Dspark.master=local

Working directory: /root/spark-1.1.0

Use classpath of module: spark-examples_2.10

Before launch: External tool: mvn

    Program: /root/Programs/apache-maven-3.2.1/bin/mvn

    Parameters: -Phadoop-2.2 -Dhadoop.version=2.2.0 -DskipTests package

    Working directory: /root/spark-1.1.0

Before launch: External tool: sbt

    Program: /root/spark-1.1.0/sbt/sbt

    Parameters: -Phadoop-2.2 -Dhadoop.version=2.2.0 assembly 

    Working directory: /root/spark-1.1.0

 

(8) Click Run -> Debug 'LogQuery' to start debugging

 

 

Cheers,

Yiming

"
Chen He <airbots@gmail.com>,"Tue, 18 Nov 2014 20:26:56 -0800",Re: Intro to using IntelliJ to debug SPARK-1.1 Apps with mvn/sbt (for beginners),sdiris@gmail.com,"Thank you Yiming. It is helpful.

Regards!

Chen


"
"""Chester @work"" <chester@alpinenow.com>","Tue, 18 Nov 2014 20:59:57 -0800",Re: Intro to using IntelliJ to debug SPARK-1.1 Apps with mvn/sbt (for beginners),Chen He <airbots@gmail.com>,"For sbt
You can simplify run
sbt/sbt gen-idea 

To generate the IntelliJ idea project module for you. You can the just open the generated project, which includes all the needed dependencies 

Sent from my iPhone


d
ve
l
T



it

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Tue, 18 Nov 2014 22:24:55 -0800",Apache infra github sync down,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey All,

The Apache-->github mirroring is not working right now and hasn't been
working fo more than 24 hours. This means that pull requests will not
appear as closed even though they have been merged. It also causes
diffs to display incorrectly in some cases. If you'd like to follow
progress by Apache infra on this issue you can watch this JIRA:

https://issues.apache.org/jira/browse/INFRA-8654

- Patrick

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Tue, 18 Nov 2014 22:31:20 -0800",Re: Apache infra github sync down,Patrick Wendell <pwendell@gmail.com>,"This basically stops us from merging patches. I'm wondering if it is
possible for ASF to give some Spark committers write permission to github
repo. In that case, if the sync tool is down, we can manually push
periodically.


"
"""Yiming \(John\) Zhang"" <sdiris@gmail.com>","Wed, 19 Nov 2014 15:12:59 +0800",re: Intro to using IntelliJ to debug SPARK-1.1 Apps with mvn/sbt (for beginners),"""'Chester @work'"" <chester@alpinenow.com>","Hi Chester, thank you for your reply. But I tried this approach and it
failed. It seems that there are more difficulty using sbt in IntelliJ than
expected.

And according to some references ""# sbt/sbt gen-idea"" is not necessary
(after Spark-1.0.0?), you can simply import the spark project and IntelliJ
will automatically generate the dependencies (but as described here, with
some possible mistakes that may fail the compilation).

Cheers,
Yiming

-----ÓÊ¼şÔ­¼ş-----
·¢¼şÈË: Chester @work [mailto:chester@alpinenow.com] 
·¢ËÍÊ±¼ä: 2014Äê11ÔÂ19ÈÕ 13:00
ÊÕ¼şÈË: Chen He
³­ËÍ: sdiris@gmail.com; dev@spark.apache.org
Ö÷Ìâ: Re: Intro to using IntelliJ to debug SPARK-1.1 Apps with mvn/sbt (for
beginners)

For sbt
You can simplify run
sbt/sbt gen-idea 

To generate the IntelliJ idea project module for you. You can the just open
the generated project, which includes all the needed dependencies 

Sent from my iPhone

found at:

Structure.


---------------------------------------------------------------------


"
madhu phatak <phatak.dev@gmail.com>,"Wed, 19 Nov 2014 16:27:32 +0530",Help needed to publish SizeEstimator as separate library,dev@spark.apache.org,"Hi,
 As I was going through spark source code, SizeEstimator
<https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/SizeEstimator.scala>
caught my eye. It's a very useful tool to do the size estimations on JVM
which helps in use cases like memory bounded cache.

It will be useful to have this as separate library, which can be used in
the other projects too. There was a discussion
<https://spark-project.atlassian.net/browse/SPARK-383> long back, but i
don't see any updates on it.

I have extracted the code and packaged as separate project on github
<https://github.com/phatak-dev/java-sizeof>. I have simplified the code to
remove dependencies from google-guava and OpenHashSet which leads to a
small compromise in accuracy in big arrays. But at same time, it greatly
simplifies the code base and dependency graph. I want to publish it to
maven central so it can be added as dependency.

Though I have published code under my package ""com.madhu"" with keeping
license information, I am not sure is it the right way to do. So it will be
great if someone can guide me on package naming and attribution.

-- 
Regards,
Madhukara Phatak
http://www.madhukaraphatak.com
"
Chester At Work <chester@alpinenow.com>,"Wed, 19 Nov 2014 06:52:21 -0800",Re: Intro to using IntelliJ to debug SPARK-1.1 Apps with mvn/sbt (for beginners),"""<sdiris@gmail.com>"" <sdiris@gmail.com>","gen-idea should work.  I use it all the time. But use the approach that works for you



Sent from my iPad

te:



/sbt (for
n

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Wed, 19 Nov 2014 14:09:29 -0800",Build break,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey All,

Just a heads up. I merged this patch last night which caused the Spark
build to break:

https://github.com/apache/spark/commit/397d3aae5bde96b01b4968dde048b6898bb6c914

The patch itself was fine and previously had passed on Jenkins. The
issue was that other intermediate changes merged since it last passed,
and the combination of those changes with the patch caused an issue
with our binary compatibility tests. This kind of race condition can
happen from time to time.

I've merged in a hot fix that should resolve this:
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=0df02ca463a4126e5437b37114c6759a57ab71ee

We'll keep an eye this and make sure future builds are passing.

- Patrick

---------------------------------------------------------------------


"
Andrew Or <andrew@databricks.com>,"Wed, 19 Nov 2014 14:51:55 -0800",Re: [VOTE] Release Apache Spark 1.1.1 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","I will start with a +1

2014-11-19 14:51 GMT-08:00 Andrew Or <andrew@databricks.com>:

"
Andrew Or <andrew@databricks.com>,"Wed, 19 Nov 2014 14:51:39 -0800",[VOTE] Release Apache Spark 1.1.1 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version 1
.1.1.

This release fixes a number of bugs in Spark 1.1.0. Some of the notable
ones are
- [SPARK-3426] Sort-based shuffle compression settings are incompatible
- [SPARK-3948] Stream corruption issues in sort-based shuffle
- [SPARK-4107] Incorrect handling of Channel.read() led to data truncation
The full list is at http://s.apache.org/z9h and in the CHANGES.txt attached.

Additionally, this candidate fixes two blockers from the previous RC:
- [SPARK-4434] Cluster mode jar URLs are broken
- [SPARK-4480][SPARK-4467] Too many open files exception from shuffle spills

The tag to be voted on is v1.1.1-rc2 (commit 3693ae5d):
http://s.apache.org/p8

The release files, including signatures, digests, etc can be found at:
http://people.apache.org/~andrewor14/spark-1.1.1-rc2/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/andrewor14.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1043/

The documentation corresponding to this release can be found at:
http://people.apache.org/~andrewor14/spark-1.1.1-rc2-docs/

Please vote on releasing this package as Apache Spark 1.1.1!

The vote is open until Saturday, November 22, at 23:00 UTC and passes if
a majority of at least 3 +1 PMC votes are cast.
[ ] +1 Release this package as Apache Spark 1.1.1
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

Cheers,
Andrew
Spark Change Log
----------------

Release 1.1.1

  [SPARK-4480] Avoid many small spills in external data structures (1.1)
  Andrew Or <andrew@databricks.com>
  2014-11-19 10:45:42 -0800
  Commit: 16bf5f3, github.com/apache/spark/pull/3354

  [SPARK-4380] Log more precise number of bytes spilled (1.1)
  Andrew Or <andrew@databricks.com>
  2014-11-18 20:15:00 -0800
  Commit: e22a759, github.com/apache/spark/pull/3355

  [SPARK-4468][SQL] Backports #3334 to branch-1.1
  Cheng Lian <lian@databricks.com>
  2014-11-18 17:40:24 -0800
  Commit: f9739b9, github.com/apache/spark/pull/3338

  [SPARK-4433] fix a racing condition in zipWithIndex
  Xiangrui Meng <meng@databricks.com>
  2014-11-18 16:25:44 -0800
  Commit: ae9b1f6, github.com/apache/spark/pull/3291

  [SPARK-4393] Fix memory leak in ConnectionManager ACK timeout TimerTasks; use HashedWheelTimer (For branch-1.1)
  Kousuke Saruta <sarutak@oss.nttdata.co.jp>
  2014-11-18 12:09:18 -0800
  Commit: 91b5fa8, github.com/apache/spark/pull/3321

  [SPARK-4467] Partial fix for fetch failure in sort-based shuffle (1.1)
  Andrew Or <andrew@databricks.com>
  2014-11-17 18:10:49 -0800
  Commit: aa9ebda, github.com/apache/spark/pull/3330

  Revert ""[SPARK-4075] [Deploy] Jar url validation is not enough for Jar file""
  Andrew Or <andrew@databricks.com>
  2014-11-17 11:25:38 -0800
  Commit: b528367

  [branch-1.1][SPARK-4355] OnlineSummarizer doesn't merge mean correctly
  Xiangrui Meng <meng@databricks.com>
  2014-11-13 15:36:03 -0800
  Commit: 4b1c77c, github.com/apache/spark/pull/3251

  [Release] Correct make-distribution.sh log path
  Andrew Or <andrew@databricks.com>
  2014-11-12 13:46:26 -0800
  Commit: ba6d81d

  [Release] Bring audit scripts up-to-date
  Andrew Or <andrewor14@gmail.com>
  2014-11-13 00:30:58 +0000
  Commit: 88bc482

  [Release] Log build output for each distribution
  Andrew Or <andrew@databricks.com>
  2014-11-11 18:02:59 -0800
  Commit: e3a5ee9

  Revert ""SPARK-3039: Allow spark to be built using avro-mapred for hadoop2""
  Andrew Or <andrew@databricks.com>
  2014-11-12 00:04:30 -0800
  Commit: 45a01b6

  Update CHANGES.txt
  Andrew Or <andrewor14@gmail.com>
  2014-11-11 23:11:32 +0000
  Commit: 131c626

  [SPARK-4295][External]Fix exception in SparkSinkSuite
  maji2014 <maji3@asiainfo.com>
  2014-11-11 02:18:27 -0800
  Commit: bf867c3, github.com/apache/spark/pull/3177

  [branch-1.1][SPARK-3990] add a note on ALS usage
  Xiangrui Meng <meng@databricks.com>
  2014-11-10 22:39:09 -0800
  Commit: b2cb357, github.com/apache/spark/pull/3190

  [BRANCH-1.1][SPARK-2652] change the default spark.serializer in pyspark back to Kryo
  Xiangrui Meng <meng@databricks.com>
  2014-11-10 22:21:14 -0800
  Commit: 11798d0, github.com/apache/spark/pull/3187

  [SPARK-4330][Doc] Link to proper URL for YARN overview
  Kousuke Saruta <sarutak@oss.nttdata.co.jp>
  2014-11-10 22:18:00 -0800
  Commit: d313be8, github.com/apache/spark/pull/3196

  [SQL] Backport backtick and smallint JDBC fixes to 1.1
  Michael Armbrust <michael@databricks.com>, ravipesala <ravindra.pesala@huawei.com>, scwf <wangfei1@huawei.com>
  2014-11-10 19:51:07 -0800
  Commit: 8a1d818, github.com/apache/spark/pull/3199

  Update versions for 1.1.1 release
  Andrew Or <andrew@databricks.com>
  2014-11-10 18:40:34 -0800
  Commit: 01d233e

  [SPARK-3495][SPARK-3496] Backporting block replication fixes made in master to branch 1.1
  Tathagata Das <tathagata.das1565@gmail.com>
  2014-11-10 18:23:02 -0800
  Commit: be0cc99, github.com/apache/spark/pull/3191

  [SPARK-3954][Streaming] Optimization to FileInputDStream
  surq <surq@asiainfo.com>
  2014-11-10 17:37:16 -0800
  Commit: 3d889df, github.com/apache/spark/pull/2811

  [SPARK-3971][SQL] Backport #2843 to branch-1.1
  Cheng Lian <lian@databricks.com>, Cheng Lian <lian.cs.zju@gmail.com>, Michael Armbrust <michael@databricks.com>
  2014-11-10 17:04:10 -0800
  Commit: 64945f8, github.com/apache/spark/pull/3113

  [SPARK-4308][SQL] Follow up of #3175 for branch 1.1
  Cheng Lian <lian@databricks.com>
  2014-11-10 16:57:34 -0800
  Commit: b3ef06b, github.com/apache/spark/pull/3176

  [SPARK-2548][HOTFIX][Streaming] Removed use of o.a.s.streaming.Durations in branch 1.1
  Tathagata Das <tathagata.das1565@gmail.com>
  2014-11-10 14:13:42 -0800
  Commit: 86b1bd0, github.com/apache/spark/pull/3188

  Update RecoverableNetworkWordCount.scala
  comcmipi <pitonak@fns.uniba.sk>
  2014-11-10 12:33:48 -0800
  Commit: 254b135, github.com/apache/spark/pull/2735

  SPARK-2548 [STREAMING] JavaRecoverableWordCount is missing
  Sean Owen <sowen@cloudera.com>
  2014-11-10 11:47:27 -0800
  Commit: cdcf546, github.com/apache/spark/pull/2564

  [SPARK-4169] [Core] Accommodate non-English Locales in unit tests
  Niklas Wilcke <1wilcke@informatik.uni-hamburg.de>
  2014-11-10 11:37:38 -0800
  Commit: dc38def, github.com/apache/spark/pull/3036

  [SPARK-4301] StreamingContext should not allow start() to be called after calling stop()
  Josh Rosen <joshrosen@databricks.com>
  2014-11-08 18:10:23 -0800
  Commit: 78cd3ab, github.com/apache/spark/pull/3160

  [SPARK-4304] [PySpark] Fix sort on empty RDD
  Davies Liu <davies@databricks.com>
  2014-11-07 20:53:03 -0800
  Commit: 4895f65, github.com/apache/spark/pull/3162

  Update JavaCustomReceiver.java
  xiao321 <1042460381@qq.com>
  2014-11-07 12:56:49 -0800
  Commit: 4fb26df, github.com/apache/spark/pull/3153

  [SPARK-4249][GraphX]fix a problem of EdgePartitionBuilder in Graphx
  lianhuiwang <lianhuiwang09@gmail.com>
  2014-11-06 10:46:45 -0800
  Commit: 0a40eac, github.com/apache/spark/pull/3138

  [SPARK-4158] Fix for missing resources.
  Brenden Matthews <brenden@diddyinc.com>
  2014-11-05 16:02:44 -0800
  Commit: c58c1bb, github.com/apache/spark/pull/3024

  SPARK-3223 runAsSparkUser cannot change HDFS write permission properly i...
  Jongyoul Lee <jongyoul@gmail.com>
  2014-11-05 15:49:42 -0800
  Commit: 590a943, github.com/apache/spark/pull/3034

  [branch-1.1][SPARK-4148][PySpark] fix seed distribution and add some tests for rdd.sample
  Xiangrui Meng <meng@databricks.com>
  2014-11-05 10:30:10 -0800
  Commit: 44751af, github.com/apache/spark/pull/3104

  [SPARK-4115][GraphX] Add overrided count for edge counting of EdgeRDD.
  luluorta <luluorta@gmail.com>
  2014-11-01 01:22:46 -0700
  Commit: 1b282cd, github.com/apache/spark/pull/2975

  [SPARK-4097] Fix the race condition of 'thread'
  zsxwing <zsxwing@gmail.com>
  2014-10-29 14:42:50 -0700
  Commit: abdb90b, github.com/apache/spark/pull/2957

  [SPARK-4065] Add check for IPython on Windows
  Michael Griffiths <msjgriffiths@gmail.com>
  2014-10-28 12:47:21 -0700
  Commit: f0c5717, github.com/apache/spark/pull/2910

  [SPARK-4107] Fix incorrect handling of read() and skip() return values (branch-1.1 backport)
  Josh Rosen <joshrosen@databricks.com>
  2014-10-28 12:30:12 -0700
  Commit: 286f1ef, github.com/apache/spark/pull/2974

  [SPARK-4110] Wrong comments about default settings in spark-daemon.sh
  Kousuke Saruta <sarutak@oss.nttdata.co.jp>
  2014-10-28 12:29:01 -0700
  Commit: dee3317, github.com/apache/spark/pull/2972

  [MLlib] SPARK-3987: add test case on objective value for NNLS
  coderxiang <shuoxiangpub@gmail.com>
  2014-10-27 19:43:39 -0700
  Commit: 2ef2f5a, github.com/apache/spark/pull/2965

  Fix build breakage introduced by 6c10c2770c718287f9cc2af4109b701fa1057b70
  Josh Rosen <joshrosen@databricks.com>
  2014-10-25 20:33:17 -0700
  Commit: 2eb9d7c

  Revert ""[SPARK-4056] Upgrade snappy-java to 1.1.1.5""
  Josh Rosen <joshrosen@databricks.com>
  2014-10-25 17:09:01 -0700
  Commit: c1989aa

  [SPARK-4056] Upgrade snappy-java to 1.1.1.5
  Josh Rosen <rosenville@gmail.com>, Josh Rosen <joshrosen@databricks.com>
  2014-10-24 17:21:08 -0700
  Commit: b7541ae, github.com/apache/spark/pull/2911

  [SPARK-4080] Only throw IOException from [write|read][Object|External]
  Josh Rosen <joshrosen@databricks.com>
  2014-10-24 15:06:15 -0700
  Commit: 6c10c27, github.com/apache/spark/pull/2932

  [SPARK-4006] In long running contexts, we encountered the situation of d...
  Tal Sliwowicz <tal.s@taboola.com>
  2014-10-24 13:51:25 -0700
  Commit: 59297e9, github.com/apache/spark/pull/2915

  [SPARK-4075] [Deploy] Jar url validation is not enough for Jar file
  Kousuke Saruta <sarutak@oss.nttdata.co.jp>
  2014-10-24 13:08:21 -0700
  Commit: 80dde80, github.com/apache/spark/pull/2925

  [SPARK-4076] Parameter expansion in spark-config is wrong
  Kousuke Saruta <sarutak@oss.nttdata.co.jp>
  2014-10-24 13:04:35 -0700
  Commit: 386fc46, github.com/apache/spark/pull/2930

  [SPARK-2652] [PySpark] donot use KyroSerializer as default serializer
  Davies Liu <davies@databricks.com>
  2014-10-23 23:58:00 -0700
  Commit: 926f8ca, github.com/apache/spark/pull/2916

  [SPARK-3426] Fix sort-based shuffle error when spark.shuffle.compress and spark.shuffle.spill.compress settings are different
  Josh Rosen <joshrosen@databricks.com>
  2014-10-22 14:49:58 -0700
  Commit: 5e191fa, github.com/apache/spark/pull/2890

  [SPARK-3877][YARN] Throw an exception when application is not successful so that the exit code wil be set to 1 (for branch-1.1)
  zsxwing <zsxwing@gmail.com>
  2014-10-22 15:08:28 -0700
  Commit: eb62094, github.com/apache/spark/pull/2748

  [SPARK-4010][Web UI]Spark UI returns 500 in yarn-client mode
  GuoQiang Li <witgo@qq.com>
  2014-10-20 11:01:26 -0700
  Commit: 457ef59, github.com/apache/spark/pull/2858

  [SPARK-3948][Shuffle]Fix stream corruption bug in sort-based shuffle
  jerryshao <saisai.shao@intel.com>
  2014-10-20 10:20:21 -0700
  Commit: 12a61d8, github.com/apache/spark/pull/2824

  [SPARK-2546] Clone JobConf for each task (branch-1.0 / 1.1 backport)
  Josh Rosen <joshrosen@apache.org>
  2014-10-19 00:31:06 -0700
  Commit: 2cd40db, github.com/apache/spark/pull/2684

  SPARK-3926 [CORE] Result of JavaRDD.collectAsMap() is not Serializable
  Sean Owen <sowen@cloudera.com>
  2014-10-18 12:33:20 -0700
  Commit: 327404d, github.com/apache/spark/pull/2805

  [SPARK-3606] [yarn] Correctly configure AmIpFilter for Yarn HA (1.1 vers...
  Marcelo Vanzin <vanzin@cloudera.com>
  2014-10-17 00:53:15 -0700
  Commit: 0d958f1, github.com/apache/spark/pull/2497

  [SPARK-3067] JobProgressPage could not show Fair Scheduler Pools section sometimes
  yantangzhai <tyz0303@163.com>, YanTangZhai <hakeemzhai@tencent.com>
  2014-10-16 19:25:37 -0700
  Commit: 35875e9, github.com/apache/spark/pull/1966

  [SPARK-3890][Docs]remove redundant spark.executor.memory in doc
  WangTaoTheTonic <barneystinson@aliyun.com>, WangTao <barneystinson@aliyun.com>
  2014-10-16 19:12:39 -0700
  Commit: 2c41170, github.com/apache/spark/pull/2745

  [SQL]typo in HiveFromSpark
  Kun Li <jacky.likun@gmail.com>
  2014-10-16 19:00:10 -0700
  Commit: 61e5903, github.com/apache/spark/pull/2809

  SPARK-3807: SparkSql does not work for tables created using custom serde
  chirag <chirag.aggarwal@guavus.com>
  2014-10-13 13:47:26 -0700
  Commit: 925e22d, github.com/apache/spark/pull/2674

  [SPARK-3899][Doc]fix wrong links in streaming doc
  w00228970 <wangfei1@huawei.com>, wangfei <wangfei1@huawei.com>
  2014-10-12 23:35:50 -0700
  Commit: 4fc6638, github.com/apache/spark/pull/2749

  [SPARK-3905][Web UI]The keys for sorting the columns of Executor page ,Stage page Storage page are incorrect
  GuoQiang Li <witgo@qq.com>
  2014-10-12 22:48:54 -0700
  Commit: a36116c, github.com/apache/spark/pull/2763

  [SPARK-3121] Wrong implementation of implicit bytesWritableConverter
  Jakub DubovskÃ½ <james64@inMail.sk>, Dubovsky Jakub <dubovsky@avast.com>
  2014-10-12 22:03:26 -0700
  Commit: 0e32579, github.com/apache/spark/pull/2712

  SPARK-3716 [GraphX] Update Analytics.scala for partitionStrategy assignment
  NamelessAnalyst <NamelessAnalyst@users.noreply.github.com>
  2014-10-12 14:18:55 -0700
  Commit: 5a21e3e, github.com/apache/spark/pull/2569

  [SPARK-3711][SQL] Optimize where in clause filter queries
  Yash Datta <Yash.Datta@guavus.com>
  2014-10-09 12:59:14 -0700
  Commit: 18ef22a, github.com/apache/spark/pull/2561

  [SPARK-3844][UI] Truncate appName in WebUI if it is too long
  Xiangrui Meng <meng@databricks.com>
  2014-10-09 00:00:24 -0700
  Commit: 09d6a81, github.com/apache/spark/pull/2707

  [SPARK-3788] [yarn] Fix compareFs to do the right thing for HDFS namespaces (1.1 version).
  Marcelo Vanzin <vanzin@cloudera.com>
  2014-10-08 08:51:17 -0500
  Commit: a44af73, github.com/apache/spark/pull/2650

  [SPARK-3829] Make Spark logo image on the header of HistoryPage as a link to HistoryPage's page #1
  Kousuke Saruta <sarutak@oss.nttdata.co.jp>
  2014-10-07 16:54:49 -0700
  Commit: a1f833f, github.com/apache/spark/pull/2690

  [SPARK-3777] Display ""Executor ID"" for Tasks in Stage page
  zsxwing <zsxwing@gmail.com>
  2014-10-07 16:00:22 -0700
  Commit: e8afb73, github.com/apache/spark/pull/2642

  [SPARK-3731] [PySpark] fix memory leak in PythonRDD
  Davies Liu <davies.liu@gmail.com>
  2014-10-07 12:20:12 -0700
  Commit: 5531830, github.com/apache/spark/pull/2668

  [SPARK-3825] Log more detail when unrolling a block fails
  Andrew Or <andrewor14@gmail.com>
  2014-10-07 12:52:10 -0700
  Commit: 267c7be, github.com/apache/spark/pull/2688

  [SPARK-3808] PySpark fails to start in Windows
  Masayoshi TSUZUKI <tsudukim@oss.nttdata.co.jp>
  2014-10-07 11:53:22 -0700
  Commit: 3a7875d, github.com/apache/spark/pull/2669

  [SPARK-3827] Very long RDD names are not rendered properly in web UI
  Hossein <hossein@databricks.com>
  2014-10-07 11:46:26 -0700
  Commit: 82ab4a7, github.com/apache/spark/pull/2687

  [SPARK-3792][SQL] Enable JavaHiveQLSuite
  scwf <wangfei1@huawei.com>
  2014-10-05 17:47:20 -0700
  Commit: 964e3aa, github.com/apache/spark/pull/2652

  SPARK-1656: Fix potential resource leaks
  zsxwing <zsxwing@gmail.com>
  2014-10-05 09:55:17 -0700
  Commit: c068d90, github.com/apache/spark/pull/577

  [SPARK-3597][Mesos] Implement `killTask`.
  Brenden Matthews <brenden@diddyinc.com>
  2014-10-05 09:49:24 -0700
  Commit: d9cf4d0, github.com/apache/spark/pull/2453

  [SPARK-3774] typo comment in bin/utils.sh
  Masayoshi TSUZUKI <tsudukim@oss.nttdata.co.jp>
  2014-10-03 13:12:37 -0700
  Commit: e4ddede, github.com/apache/spark/pull/2639

  [SPARK-3775] Not suitable error message in spark-shell.cmd
  Masayoshi TSUZUKI <tsudukim@oss.nttdata.co.jp>
  2014-10-03 13:09:48 -0700
  Commit: f130256, github.com/apache/spark/pull/2640

  [SPARK-3535][Mesos] Fix resource handling.
  Brenden Matthews <brenden@diddyinc.com>
  2014-10-03 12:58:04 -0700
  Commit: 6f15097, github.com/apache/spark/pull/2401

  [SPARK-3696]Do not override the user-difined conf_dir
  WangTaoTheTonic <barneystinson@aliyun.com>
  2014-10-03 10:42:41 -0700
  Commit: d5af9e1, github.com/apache/spark/pull/2541

  SPARK-2058: Overriding SPARK_HOME/conf with SPARK_CONF_DIR
  EugenCepoi <cepoi.eugen@gmail.com>
  2014-10-03 10:03:15 -0700
  Commit: 5d991db, github.com/apache/spark/pull/2481

  [DEPLOY] SPARK-3759: Return the exit code of the driver process
  Eric Eijkelenboom <ee@userreport.com>
  2014-10-02 18:04:38 -0700
  Commit: 699af62, github.com/apache/spark/pull/2628

  [SPARK-3755][Core] avoid trying privileged port when request a non-privileged port
  scwf <wangfei1@huawei.com>
  2014-10-02 17:47:56 -0700
  Commit: 16789f6, github.com/apache/spark/pull/2623

  [SQL][Docs] Update the output of printSchema and fix a typo in SQL programming guide.
  Yin Huai <huai@cse.ohio-state.edu>
  2014-10-02 11:37:24 -0700
  Commit: 6869351, github.com/apache/spark/pull/2630

  SPARK-3638 | Forced a compatible version of http client in kinesis-asl profile
  aniketbhatnagar <aniket.bhatnagar@gmail.com>
  2014-10-01 18:31:18 -0700
  Commit: c52c231, github.com/apache/spark/pull/2535

  Typo error in KafkaWordCount example
  Gaspar Munoz <munozs.88@gmail.com>
  2014-10-01 13:47:22 -0700
  Commit: 24ee616, github.com/apache/spark/pull/2614

  [SPARK-3756] [Core]check exception is caused by an address-port collision properly
  scwf <wangfei1@huawei.com>
  2014-10-01 11:51:30 -0700
  Commit: b4f690d, github.com/apache/spark/pull/2611

  SPARK-2626 [DOCS] Stop SparkContext in all examples
  Sean Owen <sowen@cloudera.com>
  2014-10-01 11:28:22 -0700
  Commit: 13f33cf, github.com/apache/spark/pull/2575

  [SPARK-3755][Core] Do not bind port 1 - 1024 to server in spark
  scwf <wangfei1@huawei.com>
  2014-10-01 11:30:29 -0700
  Commit: c8c3b49, github.com/apache/spark/pull/2610

  [SPARK-3747] TaskResultGetter could incorrectly abort a stage if it cannot get result for a specific task
  Reynold Xin <rxin@apache.org>
  2014-10-01 00:29:14 -0700
  Commit: a7d2df4, github.com/apache/spark/pull/2599

  SPARK-3745 - fix check-license to properly download and check jar
  shane knapp <incomplete@gmail.com>
  2014-09-30 13:11:25 -0700
  Commit: 06b96d4, github.com/apache/spark/pull/2596

  [SPARK-3709] Executors don't always report broadcast block removal properly back to the driver (for branch-1.1)
  Reynold Xin <rxin@apache.org>
  2014-09-30 12:24:58 -0700
  Commit: a8c6e82, github.com/apache/spark/pull/2591

  [SPARK-3734] DriverRunner should not read SPARK_HOME from submitter's environment
  Josh Rosen <joshrosen@apache.org>
  2014-09-29 23:36:10 -0700
  Commit: 48be657, github.com/apache/spark/pull/2586

  Fixed the condition in StronglyConnectedComponents Issue: SPARK-3635
  oded <oded@HP-DV6.c4internal.c4-security.com>
  2014-09-29 18:05:53 -0700
  Commit: 85dd513, github.com/apache/spark/pull/2486

  [graphX] GraphOps: random pick vertex bug
  yingjieMiao <yingjie@42go.com>
  2014-09-29 18:01:27 -0700
  Commit: e5ab113, github.com/apache/spark/pull/2553

  [SPARK-3032][Shuffle] Fix key comparison integer overflow introduced sorting exception
  jerryshao <saisai.shao@intel.com>
  2014-09-29 11:25:32 -0700
  Commit: df5a62f, github.com/apache/spark/pull/2514

  [CORE] Bugfix: LogErr format in DAGScheduler.scala
  Zhang, Liye <liye.zhang@intel.com>
  2014-09-29 01:13:15 -0700
  Commit: 7d88471, github.com/apache/spark/pull/2572

  [SPARK-3715][Docs]minor typo
  WangTaoTheTonic <barneystinson@aliyun.com>
  2014-09-28 18:30:13 -0700
  Commit: 004b6fa, github.com/apache/spark/pull/2567

  Docs : use ""--total-executor-cores"" rather than ""--cores"" after spark-shell
  CrazyJvm <crazyjvm@gmail.com>
  2014-09-27 09:41:04 -0700
  Commit: d9d94e0, github.com/apache/spark/pull/2540

  SPARK-3639 | Removed settings master in examples
  aniketbhatnagar <aniket.bhatnagar@gmail.com>
  2014-09-26 09:47:58 -0700
  Commit: d6ed5ab, github.com/apache/spark/pull/2536

  [SPARK-1853] Show Streaming application code context (file, line number) in Spark Stages UI
  Mubarak Seyed <mubarak.seyed@gmail.com>, Tathagata Das <tathagata.das1565@gmail.com>
  2014-09-23 15:09:12 -0700
  Commit: 505ed6b, github.com/apache/spark/pull/2464

  [SPARK-3653] Respect SPARK_*_MEMORY for cluster mode
  Andrew Or <andrewor14@gmail.com>
  2014-09-23 14:00:33 -0700
  Commit: 5bbc621, github.com/apache/spark/pull/2500

  SPARK-3612. Executor shouldn't quit if heartbeat message fails to reach ...
  Sandy Ryza <sandy@cloudera.com>
  2014-09-23 13:44:18 -0700
  Commit: ffd97be, github.com/apache/spark/pull/2487

  Update docs to use jsonRDD instead of wrong jsonRdd.
  Grega Kespret <grega.kespret@gmail.com>
  2014-09-22 10:13:44 -0700
  Commit: aab0a1d, github.com/apache/spark/pull/2479

  [MLLib] Fix example code variable name misspelling in MLLib Feature Extraction guide
  RJ Nowling <rnowling@gmail.com>
  2014-09-22 09:10:41 -0700
  Commit: 32bb97f, github.com/apache/spark/pull/2459

  Revert ""[SPARK-3595] Respect configured OutputCommitters when calling saveAsHadoopFile""
  Patrick Wendell <pwendell@gmail.com>
  2014-09-21 13:07:20 -0700
  Commit: f5bf7de

  [SPARK-3595] Respect configured OutputCommitters when calling saveAsHadoopFile
  Ian Hummel <ian@themodernlife.net>
  2014-09-21 13:04:36 -0700
  Commit: 7a76657, github.com/apache/spark/pull/2450

  [Docs] Fix outdated docs for standalone cluster
  andrewor14 <andrewor14@gmail.com>, Andrew Or <andrewor14@gmail.com>
  2014-09-19 16:02:38 -0700
  Commit: fd88353, github.com/apache/spark/pull/2461

  [SPARK-2062][GraphX] VertexRDD.apply does not use the mergeFunc
  Larry Xiao <xiaodi@sjtu.edu.cn>, Blie Arkansol <xiaodi@sjtu.edu.cn>, Ankur Dave <ankurdave@gmail.com>
  2014-09-18 23:32:32 -0700
  Commit: 1687d6b, github.com/apache/spark/pull/1903

  [Minor Hot Fix] Move a line in SparkSubmit to the right place
  Andrew Or <andrewor14@gmail.com>
  2014-09-18 17:49:28 -0700
  Commit: cf15b22, github.com/apache/spark/pull/2452

  [SPARK-3560] Fixed setting spark.jars system property in yarn-cluster mode
  Victsm <victor.nju@gmail.com>, Min Shen <mshen@linkedin.com>
  2014-09-18 15:58:14 -0700
  Commit: 832dff6, github.com/apache/spark/pull/2449

  [SPARK-3589][Minor]remove redundant code
  WangTaoTheTonic <barneystinson@aliyun.com>
  2014-09-18 12:07:24 -0700
  Commit: 2b28692, github.com/apache/spark/pull/2445

  [SPARK-3565]Fix configuration item not consistent with document
  WangTaoTheTonic <barneystinson@aliyun.com>
  2014-09-17 21:59:23 -0700
  Commit: 32f2222, github.com/apache/spark/pull/2427

  [SPARK-3564][WebUI] Display App ID on HistoryPage
  Kousuke Saruta <sarutak@oss.nttdata.co.jp>
  2014-09-17 16:31:58 -0700
  Commit: 3f1f974, github.com/apache/spark/pull/2424

  Docs: move HA subsections to a deeper indentation level
  Andrew Ash <andrew@andrewash.com>
  2014-09-17 15:07:57 -0700
  Commit: 0690410, github.com/apache/spark/pull/2402

  [SQL][DOCS] Improve table caching section
  Michael Armbrust <michael@databricks.com>
  2014-09-17 12:41:49 -0700
  Commit: 85e7c52, github.com/apache/spark/pull/2434

  [SPARK-3490] Disable SparkUI for tests (backport into 1.1)
  Andrew Or <andrewor14@gmail.com>
  2014-09-16 18:23:28 -0700
  Commit: 937de93, github.com/apache/spark/pull/2415

  [SPARK-3555] Fix UISuite race condition
  Andrew Or <andrewor14@gmail.com>
  2014-09-16 16:03:20 -0700
  Commit: 856156b, github.com/apache/spark/pull/2418

  [SQL][DOCS] Improve section on thrift-server
  Michael Armbrust <michael@databricks.com>
  2014-09-16 11:51:46 -0700
  Commit: 75158a7, github.com/apache/spark/pull/2384

  [SPARK-3518] Remove wasted statement in JsonProtocol
  Kousuke Saruta <sarutak@oss.nttdata.co.jp>
  2014-09-15 16:11:41 -0700
  Commit: 99a6c5e, github.com/apache/spark/pull/2380

  SPARK-3039: Allow spark to be built using avro-mapred for hadoop2
  Bertrand Bossy <bertrandbossy@gmail.com>
  2014-09-14 21:10:17 -0700
  Commit: 78887f9, github.com/apache/spark/pull/1945

  [SQL] [Docs] typo fixes
  Nicholas Chammas <nicholas.chammas@gmail.com>
  2014-09-13 12:34:20 -0700
  Commit: 70f93d5, github.com/apache/spark/pull/2367

  [SPARK-3515][SQL] Moves test suite setup code to beforeAll rather than in constructor
  Cheng Lian <lian.cs.zju@gmail.com>
  2014-09-12 20:14:09 -0700
  Commit: 44e534e, github.com/apache/spark/pull/2375

  [SPARK-3500] [SQL] use JavaSchemaRDD as SchemaRDD._jschema_rdd
  Davies Liu <davies.liu@gmail.com>
  2014-09-12 19:05:39 -0700
  Commit: 9c06c72, github.com/apache/spark/pull/2369

  [SPARK-3481] [SQL] Eliminate the error log in local Hive comparison test
  Cheng Hao <hao.cheng@intel.com>
  2014-09-12 11:29:30 -0700
  Commit: 6cbf83c, github.com/apache/spark/pull/2352

  Revert ""[Spark-3490] Disable SparkUI for tests""
  Andrew Or <andrewor14@gmail.com>
  2014-09-12 10:40:03 -0700
  Commit: f17b795

  [SPARK-3465] fix task metrics aggregation in local mode
  Davies Liu <davies.liu@gmail.com>
  2014-09-11 18:53:26 -0700
  Commit: e69deb8, github.com/apache/spark/pull/2338

  [SPARK-3429] Don't include the empty string """" as a defaultAclUser
  Andrew Ash <andrew@andrewash.com>
  2014-09-11 17:28:36 -0700
  Commit: 4245404, github.com/apache/spark/pull/2286

  [Spark-3490] Disable SparkUI for tests
  Andrew Or <andrewor14@gmail.com>
  2014-09-11 17:18:46 -0700
  Commit: 2ffc798, github.com/apache/spark/pull/2363

  [SPARK-2140] Updating heap memory calculation for YARN stable and alpha.
  Chris Cope <ccope@resilientscience.com>
  2014-09-11 08:13:07 -0500
  Commit: 06fb2d0, github.com/apache/spark/pull/2253

  HOTFIX: Changing color on doc menu
  Patrick Wendell <pwendell@gmail.com>
  2014-09-10 22:14:55 -0700
  Commit: e51ce9a

  [SPARK-1919] Fix Windows spark-shell --jars
  Andrew Or <andrewor14@gmail.com>
  2014-09-02 10:47:05 -0700
  Commit: 359cd59, github.com/apache/spark/pull/2211

  [SPARK-3061] Fix Maven build under Windows
  Josh Rosen <joshrosen@apache.org>, Josh Rosen <rosenville@gmail.com>, Josh Rosen <joshrosen@databricks.com>
  2014-09-02 10:45:14 -0700
  Commit: 23fd3e8, github.com/apache/spark/pull/2165

  [SPARK-3345] Do correct parameters for ShuffleFileGroup
  Liang-Chi Hsieh <viirya@gmail.com>
  2014-09-03 17:04:53 -0700
  Commit: e5f77ae, github.com/apache/spark/pull/2235

  [SPARK-3193]output errer info when Process exit code is not zero in test suite
  scwf <wangfei1@huawei.com>
  2014-09-09 11:57:01 -0700
  Commit: 2426268, github.com/apache/spark/pull/2108

  SPARK-2425 Don't kill a still-running Application because of some misbehaving Executors
  Mark Hamstra <markhamstra@gmail.com>
  2014-09-08 20:51:56 -0700
  Commit: e884805, github.com/apache/spark/pull/1360

  [SQL] Minor edits to sql programming guide.
  Henry Cook <hcook@eecs.berkeley.edu>
  2014-09-08 14:56:37 -0700
  Commit: 7a236dc, github.com/apache/spark/pull/2316

  [SPARK-938][doc] Add OpenStack Swift support
  Reynold Xin <rxin@apache.org>, Gil Vernik <gilv@il.ibm.com>
  2014-09-07 20:56:04 -0700
  Commit: 8c6306a, github.com/apache/spark/pull/is

  Fixed typos in make-distribution.sh
  Cheng Lian <lian.cs.zju@gmail.com>
  2014-09-07 20:38:32 -0700
  Commit: e45bfa8, github.com/apache/spark/pull/2121

  [SPARK-3408] Fixed Limit operator so it works with sort-based shuffle.
  Reynold Xin <rxin@apache.org>
  2014-09-07 18:42:24 -0700
  Commit: d555c2e, github.com/apache/spark/pull/2281

  [SQL] Update SQL Programming Guide
  Michael Armbrust <michael@databricks.com>, Yin Huai <huai@cse.ohio-state.edu>
  2014-09-07 21:34:46 -0400
  Commit: 65dae63, github.com/apache/spark/pull/2258

  [SPARK-3394] [SQL] Fix crash in TakeOrdered when limit is 0
  Eric Liang <ekl@google.com>
  2014-09-07 17:57:59 -0700
  Commit: c5d8d82, github.com/apache/spark/pull/2264

  [SPARK-2419][Streaming][Docs] More updates to the streaming programming guide
  Tathagata Das <tathagata.das1565@gmail.com>, Chris Fregly <chris@fregly.com>
  2014-09-06 14:46:43 -0700
  Commit: ce4053c, github.com/apache/spark/pull/2307

  SPARK-3211 .take() is OOM-prone with empty partitions
  Andrew Ash <andrew@andrewash.com>
  2014-09-05 18:52:05 -0700
  Commit: 28ce67b, github.com/apache/spark/pull/2117

  [Docs] fix minor MLlib case typo
  Nicholas Chammas <nicholas.chammas@gmail.com>
  2014-09-04 23:37:06 -0700
  Commit: 6b128be, github.com/apache/spark/pull/2278

  [SPARK-3401][PySpark] Wrong usage of tee command in python/run-tests
  Kousuke Saruta <sarutak@oss.nttdata.co.jp>
  2014-09-04 10:29:11 -0700
  Commit: dbf8120, github.com/apache/spark/pull/2272

  [HOTFIX] [SPARK-3400] Revert 9b225ac ""fix GraphX EdgeRDD zipPartitions""
  Ankur Dave <ankurdave@gmail.com>
  2014-09-03 23:49:47 -0700
  Commit: 8c40ab5, github.com/apache/spark/pull/2271

  [SPARK-3372] [MLlib] MLlib doesn't pass maven build / checkstyle due to multi-byte character contained in Gradient.scala
  Kousuke Saruta <sarutak@oss.nttdata.co.jp>
  2014-09-03 20:47:00 -0700
  Commit: f41c45a, github.com/apache/spark/pull/2248

  [SPARK-2419][Streaming][Docs] Updates to the streaming programming guide
  Tathagata Das <tathagata.das1565@gmail.com>, Jacek Laskowski <jacek@japila.pl>
  2014-09-03 17:38:01 -0700
  Commit: 3111501, github.com/apache/spark/pull/2254


Release 1.1.0

  [SPARK-3320][SQL] Made batched in-memory column buffer building work for SchemaRDDs with empty partitions
  Cheng Lian <lian.cs.zju@gmail.com>
  2014-08-29 18:16:47 -0700
  Commit: aa9364a, github.com/apache/spark/pull/2213

  [SPARK-3296][mllib] spark-example should be run-example in head notation of DenseKMeans and SparseNaiveBayes
  wangfei <wangfei_hello@126.com>
  2014-08-29 17:37:15 -0700
  Commit: b0facb5, github.com/apache/spark/pull/2193

  [SPARK-3291][SQL]TestcaseName in createQueryTest should not contain "":""
  qiping.lqp <qiping.lqp@alibaba-inc.com>
  2014-08-29 15:37:43 -0700
  Commit: c1333b8, github.com/apache/spark/pull/2191

  [SPARK-3269][SQL] Decreases initial buffer size for row set to prevent OOM
  Cheng Lian <lian.cs.zju@gmail.com>
  2014-08-29 15:36:04 -0700
  Commit: 9bae345, github.com/apache/spark/pull/2171

  [SPARK-3234][Build] Fixed environment variables that rely on deprecated command line options in make-distribution.sh
  Cheng Lian <lian.cs.zju@gmail.com>
  2014-08-29 15:29:43 -0700
  Commit: cf049ef, github.com/apache/spark/pull/2208

  [Docs] SQL doc formatting and typo fixes
  Nicholas Chammas <nicholas.chammas@gmail.com>, nchammas <nicholas.chammas@gmail.com>
  2014-08-29 15:23:32 -0700
  Commit: bfa2dc9, github.com/apache/spark/pull/2201

  [SPARK-3307] [PySpark] Fix doc string of SparkContext.broadcast()
  Davies Liu <davies.liu@gmail.com>
  2014-08-29 11:47:49 -0700
  Commit: 98d0716, github.com/apache/spark/pull/2202

  HOTFIX: Bump spark-ec2 version to 1.1.0
  Patrick Wendell <pwendell@gmail.com>
  2014-08-29 11:20:45 -0700
  Commit: c71b5c6

  Adding new CHANGES.txt
  Patrick Wendell <pwendell@gmail.com>
  2014-08-28 17:17:30 -0700
  Commit: 7db87b3

  [SPARK-3277] Fix external spilling with LZ4 assertion error
  Andrew Or <andrewor14@gmail.com>, Patrick Wendell <pwendell@gmail.com>
  2014-08-28 17:05:21 -0700
  Commit: fe4df34, github.com/apache/spark/pull/2187

  SPARK-3082. yarn.Client.logClusterResourceDetails throws NPE if requeste...
  Sandy Ryza <sandy@cloudera.com>
  2014-08-28 16:18:50 -0700
  Commit: f4cbf5e, github.com/apache/spark/pull/1984

  [SPARK-3190] Avoid overflow in VertexRDD.count()
  Ankur Dave <ankurdave@gmail.com>
  2014-08-28 15:17:01 -0700
  Commit: 0b9718a, github.com/apache/spark/pull/2106

  [SPARK-3264] Allow users to set executor Spark home in Mesos
  Andrew Or <andrewor14@gmail.com>
  2014-08-28 11:05:44 -0700
  Commit: 069ecfe, github.com/apache/spark/pull/2166

  [SPARK-3150] Fix NullPointerException in in Spark recovery: Add initializing default values in DriverInfo.init()
  Tatiana Borisova <tanyatik@yandex.ru>
  2014-08-28 10:36:36 -0700
  Commit: fd98020, github.com/apache/spark/pull/2062

  Additional CHANGES.txt
  Patrick Wendell <pwendell@gmail.com>
  2014-08-28 00:19:03 -0700
  Commit: a9df703

  [SPARK-3230][SQL] Fix udfs that return structs
  Michael Armbrust <michael@databricks.com>
  2014-08-28 00:15:23 -0700
  Commit: 2e8ad99, github.com/apache/spark/pull/2133

  [SQL] Fixed 2 comment typos in SQLConf
  Cheng Lian <lian.cs.zju@gmail.com>
  2014-08-28 00:08:09 -0700
  Commit: c0e3bc1, github.com/apache/spark/pull/2172

  HOTFIX: Don't build with YARN support for Mapr3
  Patrick Wendell <pwendell@gmail.com>
  2014-08-27 15:40:40 -0700
  Commit: ad0fab2

  [HOTFIX][SQL] Remove cleaning of UDFs
  Michael Armbrust <michael@databricks.com"
Xiangrui Meng <mengxr@gmail.com>,"Wed, 19 Nov 2014 18:00:00 -0800",Re: [VOTE] Release Apache Spark 1.1.1 (RC2),Andrew Or <andrew@databricks.com>,"+1. Checked version numbers and doc. Tested a few ML examples with
Java 6 and verified some recently merged bug fixes. -Xiangrui


---------------------------------------------------------------------


"
slcclimber <anant.asty@gmail.com>,"Wed, 19 Nov 2014 19:19:13 -0700 (MST)",Re: [MLlib] Contributing Algorithm for Outlier Detection,dev@spark.incubator.apache.org,"You could also use rdd.zipWithIndex() to create indexes.
Anant



--

---------------------------------------------------------------------


"
Krishna Sankar <ksankar42@gmail.com>,"Wed, 19 Nov 2014 19:44:45 -0800",Re: [VOTE] Release Apache Spark 1.1.1 (RC2),Xiangrui Meng <mengxr@gmail.com>,"+1
1. Compiled OSX 10.10 (Yosemite) mvn -Pyarn -Phadoop-2.4
-Dhadoop.version=2.4.0 -DskipTests clean package 10:49 min
2. Tested pyspark, mlib
2.1. statistics OK
2.2. Linear/Ridge/Laso Regression OK
2.3. Decision Tree, Naive Bayes OK
2.4. KMeans OK
2.5. r"
Qiuzhuang Lian <qiuzhuang.lian@gmail.com>,"Thu, 20 Nov 2014 12:15:24 +0800",Too many open files error,dev@spark.apache.org,"Hi All,

While doing some ETL, I  run into error of 'Too many open files' as
following logs,

Thanks,
Qiuzhuang

in-memory map of 100.8 KB to disk (953 times so far)
14/11/20 20:12:02 ERROR storage.DiskBlockObjectWriter: Uncaught exception
while reverting partial writes to file
/tmp/spark-local-20141120200455-4137/2f/temp_local_f83cbf2f-60a4-4fbd-b5d2-32a0c569311b
java.io.FileNotFoundException:
/tmp/spark-local-20141120200455-4137/2f/temp_local_f83cbf2f-60a4-4fbd-b5d2-32a0c569311b
(Too many open files)
        at java.io.FileOutputStream.open(Native Method)
        at java.io.FileOutputStream.<init>(FileOutputStream.java:221)
        at
org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(BlockObjectWriter.scala:178)
        at
        at
        at
org.apache.spark.util.collection.Spillable$class.maybeSpill(Spillable.scala:77)
        at
        at
        at
org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
        at
org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
        at
scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
        at
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        at
scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        at
scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
        at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
        at
org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
        at
org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
        at org.apache.spark.rdd.FilteredRDD.compute(FilteredRDD.scala:34)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
        at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
        at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:61)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:228)
        at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
        at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
        at
org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
        at org.apache.spark.scheduler.Task.run(Task.scala:56)
        at
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
        at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
14/11/20 20:12:02 ERROR executor.Executor: Exception in task 0.0 in stage
36.0 (TID 20)
java.io.FileNotFoundException:
/tmp/spark-local-20141120200455-4137/2f/temp_local_f83cbf2f-60a4-4fbd-b5d2-32a0c569311b
(Too many open files)
        at java.io.FileOutputStream.open(Native Method)
        at java.io.FileOutputStream.<init>(FileOutputStream.java:221)
        at
org.apache.spark.storage.DiskBlockObjectWriter.open(BlockObjectWriter.scala:123)
        at
org.apache.spark.storage.DiskBlockObjectWriter.write(BlockObjectWriter.scala:192)
        at
        at
        at
org.apache.spark.util.collection.Spillable$class.maybeSpill(Spillable.scala:77)
        at
        at
"
Ashutosh <ashutosh.trivedi@iiitb.org>,"Wed, 19 Nov 2014 22:59:16 -0700 (MST)",Re: [MLlib] Contributing Algorithm for Outlier Detection,dev@spark.incubator.apache.org,"Done. Thanks. Added you as a collaborator. So that you can add code in it.


Thanks,

Ashutosh

________________________________
From: slcclimber [via Apache Spark Developers List] <ml-node+s1001551n9441h47@n3.nabble.com>
Sent: Thursday, November 20, 2014 7:49 AM
To: Ashutosh Trivedi (MT2013030)
Subject: Re: [MLlib] Contributing Algorithm for Outlier Detection

You could also use rdd.zipWithIndex() to create indexes.
Anant

________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/MLlib-Contributing-Algorithm-for-Outlier-Detection-tp8880p9441.html
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>




--"
"""Dinesh J. Weerakkody"" <dineshjweerakkody@gmail.com>","Thu, 20 Nov 2014 11:40:04 +0530",Re: Too many open files error,Qiuzhuang Lian <qiuzhuang.lian@gmail.com>,"Hi Qiuzhuang,

This is a linux related issue. Please go through this [1] and change the
limits. hope this will solve your problem.

[1] https://rtcamp.com/tutorials/linux/increase-open-files-limit/





-- 
Thanks & Best Regards,

*Dinesh J. Weerakkody*
"
Sandy Ryza <sandy.ryza@cloudera.com>,"Wed, 19 Nov 2014 22:18:15 -0800",Re: Too many open files error,"""Dinesh J. Weerakkody"" <dineshjweerakkody@gmail.com>","Quizhang,

in certain situations. SPARK-4452 aims to deal with this issue, but we
haven't finalized a solution yet.

Dinesh's solution should help as a workaround, but you'll likely experience
suboptimal performance when trying to merge tons of small files from disk.

-Sandy


"
Matei Zaharia <matei.zaharia@gmail.com>,"Wed, 19 Nov 2014 23:11:25 -0800",Re: [VOTE] Release Apache Spark 1.1.1 (RC2),Krishna Sankar <ksankar42@gmail.com>,"+1

Tested on Mac OS X, checked that bugs with too many small files being spilled are fixed.

Matei

hang
itertools,
medium
notable
incompatible
RC:
shuffle
at:
https://repository.apache.org/content/repositories/orgapachespark-1043/
passes if


----------"
Sean Owen <sowen@cloudera.com>,"Thu, 20 Nov 2014 14:00:04 +0100",Re: [VOTE] Release Apache Spark 1.1.1 (RC2),Andrew Or <andrew@databricks.com>,"+1 (non binding)

Signatures and license looks good. I built the plain-vanilla
distribution and ran tests. While I still see the Java 8 + Hive test
failure, I think we've established this is ignorable.


---------------------------------------------------"
Madhu <madhu@madhu.com>,"Thu, 20 Nov 2014 07:39:00 -0700 (MST)",Re: [ANNOUNCE] Spark 1.2.0 Release Preview Posted,dev@spark.incubator.apache.org,"Thanks Patrick.

I've been testing some 1.2 features, looks good so far.
I have some example code that I think will be helpful for certain MR-style
use cases (secondary sort).
Can I still add that to the 1.2 documentation, or is that frozen at this
point?



-----
--
Madhu
https://www.linkedin.com/in/msiddalingaiah
--

---------------------------------------------------------------------


"
Corey Nolet <cjnolet@gmail.com>,"Thu, 20 Nov 2014 10:17:00 -0500",Re: [ANNOUNCE] Spark 1.2.0 Release Preview Posted,Madhu <madhu@madhu.com>,"I was actually about to post this myself- I have a complex join that could
benefit from something like a GroupComparator vs having to do multiple
grouyBy operations. This is probably the wrong thread for a full discussion
on this but I didn't see a JIRA ticket for this or anything similar- any
reasons why this would not make sense given Spark's design?


"
Nan Zhu <zhunanmcgill@gmail.com>,"Thu, 20 Nov 2014 10:42:57 -0500",Re: [ANNOUNCE] Spark 1.2.0 Release Preview Posted,Corey Nolet <cjnolet@gmail.com>,"BTW, this PR https://github.com/apache/spark/pull/2524 is related to a blocker level bug, 

and this is actually close to be merged (have been reviewed for several rounds)

I would appreciated if anyone can continue the process, 

@mateiz 

-- 
Nan Zhu
http://codingcat.me





"
slcclimber <anant.asty@gmail.com>,"Thu, 20 Nov 2014 10:03:25 -0700 (MST)",Re: [VOTE] Release Apache Spark 1.1.1 (RC2),dev@spark.incubator.apache.org,"+1
Built successfully and ran the 
python examples.



--

---------------------------------------------------------------------


"
Marcelo Vanzin <vanzin@cloudera.com>,"Thu, 20 Nov 2014 10:21:26 -0800",Re: [VOTE] Release Apache Spark 1.1.1 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1 (non-binding)

. ran simple things on spark-shell
. ran jobs in yarn client & cluster modes, and standalone cluster mode




-- 
Marcelo

---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Thu, 20 Nov 2014 10:21:01 -0800",[important] jenkins down,"dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>","i noticed that there were no builds, and noticed that it's throwing a bunch
of exceptions in the log file.

i'm looking in to this right now and will update when i get things rolling
again.

sorry for the inconvenience,

shane
"
Hector Yee <hector.yee@gmail.com>,"Thu, 20 Nov 2014 10:24:55 -0800",Re: [ANNOUNCE] Spark 1.2.0 Release Preview Posted,Nan Zhu <zhunanmcgill@gmail.com>,"I'm getting a lot of task lost with this build in a large mesos cluster.
Happens with both hash and sort shuffles.

14/11/20 18:08:38 WARN TaskSetManager: Lost task 9.1 in stage 1.0 (TID 897,
i-d4d6553a.inst.aws.airbnb.com): FetchFailed(null, shuffleId=1, mapId=-1,
reduceId=9, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output
location for shuffle 1
        at
org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$1.apply(MapOutputTracker.scala:386)
        at
org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$1.apply(MapOutputTracker.scala:383)
        at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at
scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
        at
scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
        at
scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
        at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:108)
        at
org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:382)
        at
org.apache.spark.MapOutputTracker.getServerStatuses(MapOutputTracker.scala:178)
        at
org.apache.spark.shuffle.hash.BlockStoreShuffleFetcher$.fetch(BlockStoreShuffleFetcher.scala:42)
        at
org.apache.spark.shuffle.hash.HashShuffleReader.read(HashShuffleReader.scala:40)
        at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:92)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)





-- 
Yee Yang Li Hector <http://google.com/+HectorYee>
*google.com/+HectorYee <http://google.com/+HectorYee>*
"
shane knapp <sknapp@berkeley.edu>,"Thu, 20 Nov 2014 10:35:30 -0800",Re: [important] jenkins down,"dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>","ok, we're back up and building now...  looks like there was a seriously bad
git (or github) plugin update that caused all sorts of unintended
consequences, mostly with cron stacktracing.

i'll take a closer look and see if i can find out exactly what happened,
but suffice to say, we'll be really cautious when updating even recommended
plugins.

sorry for the disruption!

shane


"
Hector Yee <hector.yee@gmail.com>,"Thu, 20 Nov 2014 10:44:53 -0800",Re: [VOTE] Release Apache Spark 1.1.1 (RC2),Marcelo Vanzin <vanzin@cloudera.com>,"I'm still seeing the fetch failed error and updated
https://issues.apache.org/jira/browse/SPARK-3633




-- 
Yee Yang Li Hector <http://google.com/+HectorYee>
*google.com/+HectorYee <http://google.com/+HectorYee>*
"
Kushal Datta <kushal.datta@gmail.com>,"Thu, 20 Nov 2014 10:59:02 -0800",Re: Implementing TinkerPop on top of GraphX,Kyle Ellrott <kellrott@soe.ucsc.edu>,"I have also added a graphx-gremlin module in the Tinkerpop3 codebase. Right
now a GraphX graph can be instantiated from the Gremlin command line (in a
similar manner a Giraph graph is instantiated) and the g.V().count()
function calls the count() method on RDDs.
Please check out the code in:
https://github.com/kdatta/tinkerpop3/tree/graphx-gremlin

@Kyle, I'm off for a few days till Thanksgiving. After that I'll try the
EdgeIterator in this code.

Thanks,
-Kushal.

:

y
e
t
ght
 in
inue
age
d
best
the
ass
ng,
:
e
to
l).
cable
e
t way
Spark
ng the
team.
Titan,
 This
eed to
ht
ral way
kerpop3
h as
rators
et of
,
er <
.
ell. At
so, I
 the
Process, and, if
k
nerpop2
 hack
pping
g it
n
t
r
n on
or
.
----
entity
f, or
e
ntity
, or
"
Matei Zaharia <matei.zaharia@gmail.com>,"Thu, 20 Nov 2014 11:20:43 -0800",Re: [ANNOUNCE] Spark 1.2.0 Release Preview Posted,Madhu <madhu@madhu.com>,"You can still send patches for docs until the release goes out -- please do if you see stuff.

Matei

MR-style
this
http://apache-spark-developers-list.1001551.n3.nabble.com/ANNOUNCE-Spark-1-2-0-Release-Preview-Posted-tp9400p9449.html
Nabble.com.


---------------------------------------------------------------------


"
Nishkam Ravi <nravi@cloudera.com>,"Thu, 20 Nov 2014 11:37:56 -0800",Re: [ANNOUNCE] Spark 1.2.0 Release Preview Posted,Matei Zaharia <matei.zaharia@gmail.com>,"Seeing issues with sort-based shuffle (OOM errors and memory leak):
https://issues.apache.org/jira/browse/SPARK-4515.

Good performance gains for TeraSort as compared to hash (as expected).

Thanks,
Nishkam



"
Hector Yee <hector.yee@gmail.com>,"Thu, 20 Nov 2014 11:39:40 -0800",Re: [VOTE] Release Apache Spark 1.1.1 (RC2),Marcelo Vanzin <vanzin@cloudera.com>,"I think it is a race condition caused by netty deactivating a channel while
it is active.
Switched to nio and it works fine
--conf spark.shuffle.blockTransferService=nio





-- 
Yee Yang Li Hector <http://google.com/+HectorYee>
*google.com/+HectorYee <http://google.com/+HectorYee>*
"
Matei Zaharia <matei.zaharia@gmail.com>,"Thu, 20 Nov 2014 11:48:01 -0800",Re: [VOTE] Release Apache Spark 1.1.1 (RC2),Hector Yee <hector.yee@gmail.com>,"Hector, is this a comment on 1.1.1 or on the 1.2 preview?

Matei



---------------------------------------------------------------------


"
Hector Yee <hector.yee@gmail.com>,"Thu, 20 Nov 2014 11:50:43 -0800",Re: [VOTE] Release Apache Spark 1.1.1 (RC2),Matei Zaharia <matei.zaharia@gmail.com>,"This is whatever was in http://people.apache.org/~andrewor14/spark-1
.1.1-rc2/




-- 
Yee Yang Li Hector <http://google.com/+HectorYee>
*google.com/+HectorYee <http://google.com/+HectorYee>*
"
Matei Zaharia <matei.zaharia@gmail.com>,"Thu, 20 Nov 2014 11:59:15 -0800",Re: [VOTE] Release Apache Spark 1.1.1 (RC2),Hector Yee <hector.yee@gmail.com>,"Ah, I see. But the spark.shuffle.blockTransferService property doesn't exist in 1.1 (AFAIK) -- what exactly are you doing to get this problem?

Matei

http://people.apache.org/~andrewor14/spark-1.1.1-rc2/ <http://people.apache.org/~andrewor14/spark-1.1.1-rc2/>
channel while
<https://issues.apache.org/jira/browse/SPARK-3633>
<vanzin@cloudera.com <mailto:vanzin@cloudera.com>>
mode
version
notable
incompatible
<http://s.apache.org/z9h> and in the CHANGES.txt
RC:
shuffle
found at:
<http://people.apache.org/~andrewor14/spark-1.1.1-rc2/>
<https://people.apache.org/keys/committer/andrewor14.asc>
https://repository.apache.org/content/repositories/orgapachespark-1043/ <https://repository.apache.org/content/repositories/orgapachespark-1043/>
<http://people.apache.org/~andrewor14/spark-1.1.1-rc2-docs/>
passes if
---------------------------------------------------------------------
<mailto:dev-unsubscribe@spark.apache.org>
<mailto:dev-help@spark.apache.org>
---------------------------------------------------------------------
<mailto:dev-unsubscribe@spark.apache.org>
<mailto:dev-help@spark.apache.org>
<http://google.com/+HectorYee>>
<http://google.com/+HectorYee <http://google.com/+HectorYee>>*
<http://google.com/+HectorYee>>
<http://google.com/+HectorYee <http://google.com/+HectorYee>>*

"
Hector Yee <hector.yee@gmail.com>,"Thu, 20 Nov 2014 12:16:46 -0800",Re: [VOTE] Release Apache Spark 1.1.1 (RC2),Matei Zaharia <matei.zaharia@gmail.com>,"Whoops I must have used the 1.2 preview and mixed them up.

spark-shell -version shows  version 1.2.0

Will update the bug https://issues.apache.org/jira/browse/SPARK-4516 to 1.2




-- 
Yee Yang Li Hector <http://google.com/+HectorYee>
*google.com/+HectorYee <http://google.com/+HectorYee>*
"
Gerard Maas <gerard.maas@gmail.com>,"Thu, 20 Nov 2014 21:25:58 +0100",Spark Streaming Metrics,"spark users <user@spark.apache.org>, dev@spark.apache.org","As the Spark Streaming tuning guide indicates, the key indicators of a
healthy streaming job are:
- Processing Time
- Total Delay

The Spark UI page for the Streaming job [1] shows these two indicators but
the metrics source for Spark Streaming (StreamingSource.scala)  [2] does
not.

Any reasons for that? I would like to monitor job performance through an
external monitor (Ganglia in our case) and I've connected already the
currently published metrics.

-kr,  Gerard.


[1]
https://github.com/apache/spark/blob/master/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingPage.scala#L127

[2]
https://github.com/apache/spark/blob/master/streaming/src/main/scala/org/apache/spark/streaming/StreamingSource.scala
"
Joseph Bradley <joseph@databricks.com>,"Thu, 20 Nov 2014 14:51:29 -0800",Re: [MLlib] Contributing Algorithm for Outlier Detection,Ashutosh <ashutosh.trivedi@iiitb.org>,"Could we move discussion of the design and implementation to the JIRA
and/or a work-in-progress PR (tagged with [WIP])?  That will help leave a
record for the future.
Thanks!
Joseph


.
ing-Algorithm-for-Outlier-Detection-tp8880p9441.html
rvlet.jtp?macro=unsubscribe_by_code&node=8880&code=YXNodXRvc2gudHJpdmVkaUBpaWl0Yi5vcmd8ODg4MHwtMzkzMzE5NzYx
rvlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml
ing-Algorithm-for-Outlier-Detection-tp8880p9444.html
"
slcclimber <anant.asty@gmail.com>,"Thu, 20 Nov 2014 15:54:58 -0700 (MST)",Re: [MLlib] Contributing Algorithm for Outlier Detection,dev@spark.incubator.apache.org,"That would be a very wise decision.





--"
Shixiong Zhu <zsxwing@gmail.com>,"Fri, 21 Nov 2014 11:14:09 +0800",Re: Eliminate copy while sending data : any Akka experts here ?,Reynold Xin <rxin@databricks.com>,"Is it possible that Spark buffers the messages
of mapOutputStatuses(Array[Byte]) according to the size
of mapOutputStatuses which have already sent but not yet ACKed? The buffer
will be cheap since the mapOutputStatuses messages are same and the memory
cost is only a few pointers.

Best Regards,
Shixiong Zhu

2014-09-20 16:24 GMT+08:00 Reynold Xin <rxin@databricks.com>:

"
Reynold Xin <rxin@databricks.com>,"Thu, 20 Nov 2014 20:28:50 -0800",Re: Eliminate copy while sending data : any Akka experts here ?,Shixiong Zhu <zsxwing@gmail.com>,"Can you elaborate? Not 100% sure if I understand what you mean.


"
pedrorodriguez <ski.rodriguez@gmail.com>,"Thu, 20 Nov 2014 20:53:47 -0700 (MST)","sbt publish-local fails, missing spark-network-common",dev@spark.incubator.apache.org,"I am developing an application which calls into Spark MLlib I am working on (LDA). To do so, I am linking Spark locally in the application and using sbt assembly/publish-local in the spark directory.Â 

When I run sbt assembly in my application I get the following error:Â 
$ sbt assemblyÂ 
[info] Loading global plugins from /Users/pedro/.sbt/0.13/pluginsÂ 
[info] Loading project definition from /Users/pedro/Documents/Code/nips-lda/projectÂ 
[info] Set current project to nips-lda (in build file:/Users/pedro/Documents/Code/nips-lda/)Â 
[info] Updating {file:/Users/pedro/Documents/Code/nips-lda/}nips-lda...Â 
[info] Resolving org.apache.spark#spark-network-common_2.10;1.3.0-SNAPSHOT ...Â 
[warn] module not found: org.apache.spark#spark-network-common_2.10;1.3.0-SNAPSHOTÂ 
[warn] ==== local: triedÂ 
[warn] Â  /Users/pedro/.ivy2/local/org.apache.spark/spark-network-common_2.10/1.3.0-SNAPSHOT/ivys/ivy.xmlÂ 
[warn] ==== public: triedÂ 
[warn] Â Â https://repo1.maven.org/maven2/org/apache/spark/spark-network-common_2.10/1.3.0-SNAPSHOT/spark-network-common_2.10-1.3.0-SNAPSHOT.pom
[warn] ==== Typesafe: triedÂ 
[warn] Â Â http://repo.typesafe.com/typesafe/releases/org/apache/spark/spark-network-common_2.10/1.3.0-SNAPSHOT/spark-network-common_2.10-1.3.0-SNAPSHOT.pom
[warn] ==== Spray: triedÂ 
[warn] Â Â http://repo.spray.cc/org/apache/spark/spark-network-common_2.10/1.3.0-SNAPSHOT/spark-network-common_2.10-1.3.0-SNAPSHOT.pom
[info] Resolving org.fusesource.jansi#jansi;1.4 ...Â 
[warn] ::::::::::::::::::::::::::::::::::::::::::::::Â 
[warn] :: Â  Â  Â  Â  Â UNRESOLVED DEPENDENCIES Â  Â  Â  Â  ::Â 
[warn] ::::::::::::::::::::::::::::::::::::::::::::::Â 
[warn] :: org.apache.spark#spark-network-common_2.10;1.3.0-SNAPSHOT: not foundÂ 
[warn] ::::::::::::::::::::::::::::::::::::::::::::::Â 
[warn]Â 
[warn] Note: Unresolved dependencies path:Â 
[warn] org.apache.spark:spark-network-common_2.10:1.3.0-SNAPSHOTÂ 
[warn] Â +- org.apache.spark:spark-network-shuffle_2.10:1.3.0-SNAPSHOTÂ 
[warn] Â +- org.apache.spark:spark-core_2.10:1.3.0-SNAPSHOT (/Users/pedro/Documents/Code/nips-lda/build.sbt#L15-26)Â 
[warn] Â +- org.apache.spark:spark-catalyst_2.10:1.3.0-SNAPSHOTÂ 
[warn] Â +- org.apache.spark:spark-sql_2.10:1.3.0-SNAPSHOTÂ 
[warn] Â +- org.apache.spark:spark-mllib_2.10:1.3.0-SNAPSHOT (/Users/pedro/Documents/Code/nips-lda/build.sbt#L15-26)Â 
[warn] Â +- edu.berkeley.cs.amplab:nips-lda_2.10:0.1Â 
sbt.ResolveException: unresolved dependency: org.apache.spark#spark-network-common_2.10;1.3.0-SNAPSHOT: not foundÂ 
Â  Â  Â  Â  at sbt.IvyActions$.sbt$IvyActions$$resolve(IvyActions.scala:243)Â 
Â  Â  Â  Â  at sbt.IvyActions$$anonfun$updateEither$1.apply(IvyActions.scala:158)Â 

Inspecting the sbt publish-local logs and ~/.ivy2/local/org.apache.spark it looks like spark-network-common is not getting published, which causes this error in build time. I am rebased on upstream/master recently and it looks like this was recently added. Inspecting the pom.xml in spark, it looks like submodules look in order, same with project/SparkBuild.scala.Â 

Any suggestions on how to fix this?Â 

P.S. is javadoc compilation broken atm?

--Â 
Pedro



--
3.nabble.com/sbt-publish-local-fails-missing-spark-network-common-tp9471.html
om."
Patrick Wendell <pwendell@gmail.com>,"Thu, 20 Nov 2014 23:42:42 -0800",Spark development with IntelliJ,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi All,

I noticed people sometimes struggle to get Spark set up in IntelliJ.
I'd like to maintain comprehensive instructions on our Wiki to make
this seamless for future developers. Due to some nuances of our build,
getting to the point where you can build + test every module from
within the IDE is not trivial. I created a reference here:

https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools#UsefulDeveloperTools-BuildingSparkinIntelliJIDEA

I'd love people to independently test this and/or share potential improvements.

- Patrick

---------------------------------------------------------------------


"
Gerard Maas <gerard.maas@gmail.com>,"Fri, 21 Nov 2014 11:14:46 +0100",Re: Spark Streaming Metrics,"spark users <user@spark.apache.org>, dev@spark.apache.org","Looks like metrics are not a hot topic to discuss - yet so important to
sleep well when jobs are running in production.

I've created Spark-4537 <https://issues.apache.org/jira/browse/SPARK-4537>
to track this issue.

-kr, Gerard.


"
PierreB <pierre.borckmans@realimpactanalytics.com>,"Fri, 21 Nov 2014 03:24:31 -0700 (MST)","Re: sbt publish-local fails, missing spark-network-common",dev@spark.incubator.apache.org,"Hi Pedro,

Exact same issue here!

Have you found a workaround?

Thanks

P.



--

---------------------------------------------------------------------


"
Xuelin Cao <xuelincao@yahoo.com>,"Fri, 21 Nov 2014 07:12:13 -0700 (MST)",Why Executor Deserialize Time takes more than 300ms?,dev@spark.incubator.apache.org,"
In our experimental cluster (1 driver, 5 workers), we tried the simplest
example:   sc.parallelize(Range(0, 100), 2).count

In the event log, we found the executor takes too much time on
deserialization, about 300 ~ 500ms, and the execution time is only 1ms.

Our servers are with 2.3G Hz CPU * 24 cores.  And, we have set the
serializer to org.apache.spark.serializer.KryoSerializer .

The question is, is it normal that the executor takes 300~500ms on
deserialization?  If not, any clue for the performance tuning?






--

---------------------------------------------------------------------


"
andy petrella <andy.petrella@gmail.com>,"Fri, 21 Nov 2014 15:39:32 +0000",Re: Spark Streaming Metrics,"Gerard Maas <gerard.maas@gmail.com>, spark users <user@spark.apache.org>, dev@spark.apache.org","Yo,

I've discussed with some guyz from cloudera that are working (only oO) on
spark-core and streaming.
The streaming was telling me the same thing about the scheduling part.

Do you have some nice screenshots and info about stages running, task time,
akka health and things like these -- I said the guy that I might poke him
today with more materials.

Btw, how're you?

Tchuss man
andy

PS: did you tried the recent events thingy?



"
pedrorodriguez <ski.rodriguez@gmail.com>,"Fri, 21 Nov 2014 10:26:50 -0700 (MST)","Re: sbt publish-local fails, missing spark-network-common",dev@spark.incubator.apache.org,"Haven't found one yet, but work in AMPlab/at ampcamp so I will see if I can
find someone who would know more about this (maybe reynold since he rolled
out networking improvements for the PB sort). Good to have confirmation at
least one other person is having problems with this rather than something
isolated.

-pedro



--

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Fri, 21 Nov 2014 12:24:29 -0800",Automated github closing of issues is not working,"""dev@spark.apache.org"" <dev@spark.apache.org>","After we merge pull requests in Spark they are closed via a special
message we put in each commit description (""Closes #XXX""). This
feature stopped working around 21 hours ago causing already-merged
pull requests to display as open.

I've contacted Github support with the issue. No word from them yet.

It is not clear whether this relates to recently delays syncing with Github.

- Patrick

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 21 Nov 2014 21:50:20 +0000",Troubleshooting JVM OOM during Spark Unit Tests,dev <dev@spark.apache.org>,"Howdy folks,

Iâ€™m trying to understand why Iâ€™m getting â€œinsufficient memoryâ€ errors when
trying to run Spark Units tests within a CentOS Docker container.

Iâ€™m building Spark and running the tests as follows:

# build
sbt/sbt -Pyarn -Phadoop-2.3 -Dhadoop.version=2.3.0 -Pkinesis-asl
-Phive -Phive-thriftserver package assembly/assembly

# Scala unit tests
sbt/sbt -Pyarn -Phadoop-2.3 -Dhadoop.version=2.3.0 -Pkinesis-asl
-Phive -Phive-thriftserver catalyst/test sql/test hive/test mllib/test

The build completes successfully. After humming along for many minutes, the
unit tests fail with this:

OpenJDK 64-Bit Server VM warning: INFO:
os::commit_memory(0x000000074a580000, 30932992, 0) failed;
error='Cannot allocate memory' (errno=12)
#
# There is insufficient memory for the Java Runtime Environment to continue.
# Native memory allocation (malloc) failed to allocate 30932992 bytes
for committing reserved memory.
# An error report file with more information is saved as:
# /tmp/jvm-21940/hs_error.log
Exception in thread ""Thread-20"" Exception in thread ""Thread-16""
java.io.EOFException
    at java.io.ObjectInputStream$BlockDataInputStream.peekByte(ObjectInputStream.java:2598)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1318)
    at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
    at org.scalatest.tools.Framework$ScalaTestRunner$Skeleton$1$React.react(Framework.scala:945)
    at org.scalatest.tools.Framework$ScalaTestRunner$Skeleton$1.run(Framework.scala:934)
    at java.lang.Thread.run(Thread.java:745)
java.net.SocketException: Connection reset
    at java.net.SocketInputStream.read(SocketInputStream.java:196)
    at java.net.SocketInputStream.read(SocketInputStream.java:122)
    at java.net.SocketInputStream.read(SocketInputStream.java:210)
    at java.io.ObjectInputStream$PeekInputStream.peek(ObjectInputStream.java:2293)
    at java.io.ObjectInputStream$BlockDataInputStream.peek(ObjectInputStream.java:2586)
    at java.io.ObjectInputStream$BlockDataInputStream.peekByte(ObjectInputStream.java:2596)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1318)
    at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
    at sbt.React.react(ForkTests.scala:114)
    at sbt.ForkTests$$anonfun$mainTestTask$1$Acceptor$2$.run(ForkTests.scala:74)
    at java.lang.Thread.run(Thread.java:745)

Here are some (I think) relevant environment variables I have set:

export JAVA_HOME=""/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.71-2.5.3.1.el7_0.x86_64""
export JAVA_OPTS=""-Xms128m -Xmx1g -XX:MaxPermSize=128m""
export MAVEN_OPTS=""-Xmx512m -XX:MaxPermSize=128m""

How do I narrow down why this is happening? I know that running this thing
within a Docker container may be playing a role here, but before poking
around with Docker configs I want to make an effort at getting the Java
setup right within the container.

Iâ€™ve already tried giving the container 2GB of memory, so I donâ€™t think at
this point itâ€™s a restriction on the container.

Any pointers on how to narrow the problem down?

Nick

P.S. If youâ€™re wondering why Iâ€™m trying to run unit tests within a Docker
container, Iâ€™m exploring a different angle on SPARK-3431
<https://issues.apache.org/jira/browse/SPARK-3431>.
â€‹
"
Markus Weimer <markus@weimo.de>,"Fri, 21 Nov 2014 14:19:07 -0800",Re: Automated github closing of issues is not working,dev@spark.apache.org,"-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Hi,

FWIW, we see quite large latencies (>8h) between commits to Apache git
and the sync to GitHub in the REEF project as well.

Markus


-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQIcBAEBAgAGBQJUb7pbA"
Zhan Zhang <zhazhan@gmail.com>,"Fri, 21 Nov 2014 15:51:23 -0700 (MST)",How spark and hive integrate in long term?,dev@spark.incubator.apache.org,"Now Spark and hive integration is a very nice feature. But I am wondering
what the long term roadmap is for spark integration with hive. Both of these
two projects are undergoing fast improvement and changes. Currently, my
understanding is that spark hive sql part relies on hive meta store and
basic parser to operate, and the thrift-server intercept hive query and
replace it with its own engine.

With every release of hive, there need a significant effort on spark part to
support it. 

For the metastore part, we may possibly replace it with hcatalog. But given
the dependency of other parts on hive, e.g., metastore, thriftserver,
hcatlog may not be able to help much. 

Does anyone have any insight or idea in mind?

Thanks.

Zhan Zhang 



--

---------------------------------------------------------------------


"
Dean Wampler <deanwampler@gmail.com>,"Fri, 21 Nov 2014 15:12:18 -0800",Re: How spark and hive integrate in long term?,Zhan Zhang <zhazhan@gmail.com>,"I can't comment on plans for Spark SQL's support for Hive, but several
companies are porting Hive itself onto Spark:

http://blog.cloudera.com/blog/2014/11/apache-hive-on-apache-spark-the-first-demo/

I'm not sure if they are leveraging the old Shark code base or not, but it
appears to be a fresh effort.

dean

Dean Wampler, Ph.D.
Author: Programming Scala, 2nd Edition
<http://shop.oreilly.com/product/0636920033073.do> (O'Reilly)
Typesafe <http://typesafe.com>
@deanwampler <http://twitter.com/deanwampler>
http://polyglotprogramming.com


"
Zhan Zhang <zzhang@hortonworks.com>,"Fri, 21 Nov 2014 15:45:24 -0800",Re: How spark and hive integrate in long term?,Dean Wampler <deanwampler@gmail.com>,"Thanks Dean, for the information.

Hive-on-spark is nice. Spark sql has the advantage to take the full advantage of spark and allows user to manipulate the table as RDD through native spark support.

When I tried to upgrade the current hive-0.13.1 support to hive-0.14.0. I found the hive parser is not compatible any more. In the meantime, those new feature introduced in hive-0.14.1, e.g, ACID, etc, is not there yet. In the meantime, spark-0.12 also
has some nice feature added which is supported by thrift-server too, e.g., hive-0.13, table cache, etc. 

Given that both have more and more features added, it would be great if user can take advantage of both. Current, spark sql give us such benefits partially, but I am wondering how to keep such integration in long term.

Thanks.

Zhan Zhang




-- 
CONFIDENTIALITY NOTICE
NOTICE: This message is intended for the use of the individual or entity to 
which it is addressed and may contain information that is confidential, 
privileged and exempt from disclosure under applicable law. If the reader 
of this message is not the intended recipient, you are hereby notified that 
any printing, copying, dissemination, distribution, disclosure or 
forwarding of this communication is strictly prohibited. If you have 
received this communication in error, please contact the sender immediately 
and delete it from your system. Thank You.

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Fri, 21 Nov 2014 16:02:54 -0800",Re: How spark and hive integrate in long term?,Zhan Zhang <zzhang@hortonworks.com>,"bq. spark-0.12 also has some nice feature added

Minor correction: you meant Spark 1.2.0 I guess

Cheers


"
Reynold Xin <rxin@databricks.com>,"Sat, 22 Nov 2014 00:09:34 -0800",Re: Troubleshooting JVM OOM during Spark Unit Tests,Nicholas Chammas <nicholas.chammas@gmail.com>,"What does /tmp/jvm-21940/hs_error.log tell you? It might give hints to what
threads are allocating the extra off-heap memory.



cient memoryâ€ errors when
he
.java:2598)
ework.scala:945)
ala:934)
3)
a:2586)
.java:2596)
4""
g
â€™t think at
 within a Docker
"
Xuelin Cao <xuelincao@yahoo.com.INVALID>,"Sat, 22 Nov 2014 12:52:06 +0000 (UTC)",Why Executor Deserialize Time takes more than 300ms?,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","
In our experimental cluster (1 driver, 5 workers), we tried the simplest example: Â  sc.parallelize(Range(0, 100), 2).countÂ 

In the event log, we found the executor takes too much time on deserialization, about 300 ~ 500ms, and the execution time is only 1ms.Â 

Our servers are with 2.3G Hz CPU * 24 cores. Â And, we have set the serializer to org.apache.spark.serializer.KryoSerializer .Â 

The question is, is it normal that the executor takes 300~500ms on deserialization? Â If not, any clue for the performance tuning?Â 


"
Imran Rashid <imran@therashids.com>,"Sat, 22 Nov 2014 07:30:24 -0600",Re: Why Executor Deserialize Time takes more than 300ms?,Xuelin Cao <xuelincao@yahoo.com>,"Hi Xuelin,

this type of question is probably better asked on the spark-user mailing
list, user@spark.apache.org
<http://apache-spark-user-list.1001560.n3.nabble.com>

Do you mean the very first set of tasks take 300 - 500 ms to deserialize?
That is most likely because of the time taken to ship the jars from the
driver to the executors.  You should only pay this cost once per spark
context (assuming you are not adding more jars later on).  You could try
simply running the same task again, from the same spark context, and see
whether it still takes that much time to deserialize the tasks.

If you really want to eliminate that initial time to send the jars, you
could ensure that the jars are already on the executors, so they don't need
to get sent at all by spark.   (Of course, this makes it harder to deploy
new code; you'd still need to update those jars *somehow* when you do.)

hope this helps,
Imran



"
Xuelin Cao <xuelincao@yahoo.com>,"Sat, 22 Nov 2014 06:42:50 -0700 (MST)",Re: Why Executor Deserialize Time takes more than 300ms?,dev@spark.incubator.apache.org,"
Thanks Imran,

     The problems is, *every time* I run the same task, the deserialization
time is around 300~500ms. I don't know if this is a normal case.





--

---------------------------------------------------------------------


"
rzykov <rzykov@gmail.com>,"Sat, 22 Nov 2014 07:23:41 -0700 (MST)",java.lang.OutOfMemoryError at simple local test,dev@spark.incubator.apache.org,"Dear all, 

Unfortunately I've not got ant respond in users forum. That's why I decided
to publish this question here.
We encountered problems of failed jobs with huge amount of data. For
example, an application works perfectly with relative small sized data, but
when it grows in 2 times this  application fails.

A simple local test was prepared for this question at
https://gist.github.com/copy-of-rezo/6a137e13a1e4f841e7eb
It generates 2 sets of key-value pairs, join them, selects distinct values
and counts data finally. 

object Spill { 
  def generate = { 
    for{ 
      j <- 1 to 10 
      i <- 1 to 200 
    } yield(j, i) 
  } 
  
  def main(args: Array[String]) { 
    val conf = new SparkConf().setAppName(getClass.getSimpleName) 
    conf.set(""spark.shuffle.spill"", ""true"") 
    conf.set(""spark.serializer"",
""org.apache.spark.serializer.KryoSerializer"") 
    val sc = new SparkContext(conf) 
    println(generate) 
  
    val dataA = sc.parallelize(generate) 
    val dataB = sc.parallelize(generate) 
    val dst = dataA.join(dataB).distinct().count() 
    println(dst) 
  } 
} 

We compiled it locally and run 3 times with different settings of memory: 
1) --executor-memory 10M --driver-memory 10M --num-executors 1
--executor-cores 1
It fails wtih ""java.lang.OutOfMemoryError: GC overhead limit exceeded"" at 
..... 

2) --executor-memory 20M --driver-memory 20M --num-executors 1
--executor-cores 1
It works OK 

3)  --executor-memory 10M --driver-memory 10M --num-executors 1
--executor-cores 1 But let's make less data for i from 200 to 100. It
reduces input data in 2 times and joined data in 4 times 

  def generate = { 
    for{ 
      j <- 1 to 10 
      i <- 1 to 100   // previous value was 200 
    } yield(j, i) 
  } 
This code works OK. 

We don't understand why 10M is not enough for such simple operation with
32000 bytes of ints (2 * 10 * 200 * 2 * 4) approximately? 10M of RAM works
if we change the data volume in 2 times (2000 of records of (int, int)).   
Why spilling to disk doesn't cover this case? 



--

---------------------------------------------------------------------


"
Cheng Lian <lian.cs.zju@gmail.com>,"Sat, 22 Nov 2014 23:05:00 +0800",Re: How spark and hive integrate in long term?,"Zhan Zhang <zhazhan@gmail.com>, dev@spark.incubator.apache.org","Hey Zhan,

This is a great question. We are also seeking for a stable API/protocol 
that works with multiple Hive versions (esp. 0.12+). SPARK-4114 
<https://issues.apache.org/jira/browse/SPARK-4114> was opened for this. 
Did some research into HCatalog recently, but I must confess that Iâ€™m 
not an expert on HCatalog, actually spent only 1 day on exploring it. So 
please donâ€™t hesitate to correct me if I was wrong about the conclusions 
I made below.

First, although HCatalog API is more pleasant to work with, itâ€™s 
unfortunately feature incomplete. It only provides a subset of most 
commonly used operations. For example, |HCatCreateTableDesc| maps only a 
subset of |CreateTableDesc|, properties like |storeAsSubDirectories|, 
|skewedColNames| and |skewedColValues| are missing. Itâ€™s also impossible 
to alter table properties via HCatalog API (Spark SQL uses this to 
implement the |ANALYZE| command). The |hcat| CLI tool provides all those 
features missing in HCatalog API via raw Metastore API, and is 
structurally similar to the old Hive CLI.

Second, HCatalog API itself doesnâ€™t ensure compatibility, itâ€™s the 
Thrift protocol that matters. HCatalog is directly built upon raw 
Metastore API, and talks the same Metastore Thrift protocol. The problem 
we encountered in Spark SQL is that, usually we deploy Spark SQL Hive 
support with embedded mode (for testing) or local mode Metastore, and 
this makes us suffer from things like Metastore database schema changes. 
If Hive Metastore Thrift protocol is guaranteed to be downward 
compatible, then hopefully we can resort to remote mode Metastore and 
always depend on most recent Hive APIs. I had a glance of Thrift 
protocol version handling code in Hive, it seems that downward 
compatibility is not an issue. However I didnâ€™t find any official 
documents about Thrift protocol compatibility.

That said, in the future, hopefully we can only depend on most recent 
Hive dependencies and remove the Hive shim layer introduced in branch 
1.2. For users who use exactly the same version of Hive as Spark SQL, 
they can use either remote or local/embedded Metastore; while for users 
who want to interact with existing legacy Hive clusters, they have to 
setup a remote Metastore and let the Thrift protocol to handle 
compatibility.

â€” Cheng


â€‹
"
Cheng Lian <lian.cs.zju@gmail.com>,"Sat, 22 Nov 2014 23:14:33 +0800",Re: How spark and hive integrate in long term?,"Zhan Zhang <zhazhan@gmail.com>, dev@spark.incubator.apache.org","Should emphasize that this is still a quick and rough conclusion, will 
investigate this in more detail after 1.2.0 release. Anyway we really 
like to provide Hive support in Spark SQL as smooth and clean as 
possible for both developers and end users.


"
Sean Owen <sowen@cloudera.com>,"Sat, 22 Nov 2014 17:37:58 +0100",Re: java.lang.OutOfMemoryError at simple local test,rzykov <rzykov@gmail.com>,"10M is tiny compared to all of the overhead of running a lot complex Scala
based app in a JVM. I think you may be bumping up against practical minimum
sizes and that you may find it is not really the data size? I don't think
it really scales down this far.

"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sat, 22 Nov 2014 17:18:18 +0000",Re: Troubleshooting JVM OOM during Spark Unit Tests,Reynold Xin <rxin@databricks.com>,"Hereâ€™s that log file <https://gist.github.com/nchammas/08d3a3a02486cf602ceb>
from a different run of the unit tests that also failed. Iâ€™m not sure what
to look for.

If it matters any, I also changed JAVA_OPTS as follows for this run:

export JAVA_OPTS=""-Xms512m -Xmx1024m -XX:PermSize=64m
-XX:MaxPermSize=128m -Xss512k""

Nick


What does /tmp/jvm-21940/hs_error.log tell you? It might give hints to what
icient memoryâ€ errors when
m.java:2598)
)
mework.scala:945)
cala:934)
93)
va:2586)
m.java:2596)
)
64""
ng
â€™t think at
s within a Docker
"
Zhan Zhang <zzhang@hortonworks.com>,"Sat, 22 Nov 2014 10:01:02 -0800",Re: How spark and hive integrate in long term?,Cheng Lian <lian.cs.zju@gmail.com>,"Thanks Cheng for the insights. 

Regarding the HCatalog, I did some initial investigation too and agree with you. As of now, it seems not a good solution. I will try to talk to Hive people to see whether there is such guarantee for downward compatibility for thrift protocol. By the way, I tried some basic functions using hive-0.13 connect to hive-0.14 metastore, and it looks like they are compatible. 

Thanks.

Zhan Zhang



vestigate this in more detail after 1.2.0 release. Anyway we really like to provide Hive support in Spark SQL as smooth and clean as possible for both developers and end users.
that works with multiple Hive versions (esp. 0.12+). SPARK-4114 <https://issues.apache.org/jira/browse/SPARK-4114> was opened for this. Did some research into HCatalog recently, but I must confess that Iâ€™m not an expert on HCatalog, actually spent only 1 day on exploring it. So please donâ€™t hesitate to correct me if I was wrong about the conclusions I made below.
 unfortunately feature incomplete. It only provides a subset of most commonly used operations. For example, |HCatCreateTableDesc| maps only a subset of |CreateTableDesc|, properties like |storeAsSubDirectories|, |skewedColNames| and |skewedColValues| are missing. Itâ€™s also impossible to alter table properties via HCatalog API (Spark SQL uses this to implement the |ANALYZE| command). The |hcat| CLI tool provides all those features missing in HCatalog API via raw Metastore API, and is structurally similar to the old Hive CLI.
€™s the Thrift protocol that matters. HCatalog is directly built upon raw Metastore API, and talks the same Metastore Thrift protocol. The problem we encountered in Spark SQL is that, usually we deploy Spark SQL Hive support with embedded mode (for testing) or local mode Metastore, and this makes us suffer from things like Metastore database schema changes. If Hive Metastore Thrift protocol is guaranteed to be downward compatible, then hopefully we can resort to remote mode Metastore and always depend on most recent Hive APIs. I had a glance of Thrift protocol version handling code in Hive, it seems that downward compatibility is not an issue. However I didnâ€™t find any official documents about Thrift protocol compatibility.
ve dependencies and remove the Hive shim layer introduced in branch 1.2. For users who use exactly the same version of Hive as Spark SQL, they can use either remote or local/embedded Metastore; while for users who want to interact with existing legacy Hive clusters, they have to setup a remote Metastore and let the Thrift protocol to handle compatibility.
ng
these
rt to
iven
1.n3.nabble.com/How-spark-and-hive-integrate-in-long-term-tp9482.html
le.com.


-- 
CONFIDENTIALITY NOTICE
NOTICE: This message is intended for the use of the individual or entity to 
which it is addressed and may contain information that is confidential, 
privileged and exempt from disclosure under applicable law. If the reader 
of this message is not the intended recipient, you are hereby notified that 
any printing, copying, dissemination, distribution, disclosure or 
forwarding of this communication is strictly prohibited. If you have 
received this communication in error, please contact the sender immediately 
and delete it from your system. Thank You.

---------------------------------------------------------------------


"
Prashant Sharma <scrapcodes@gmail.com>,"Sun, 23 Nov 2014 06:43:05 +0530","Re: sbt publish-local fails, missing spark-network-common",pedrorodriguez <ski.rodriguez@gmail.com>,"Can you update to latest master and see if this issue exists.

"
Patrick Wendell <pwendell@gmail.com>,"Sat, 22 Nov 2014 17:56:09 -0800",Re: How spark and hive integrate in long term?,Zhan Zhang <zzhang@hortonworks.com>,"There are two distinct topics when it comes to hive integration. Part
of the 1.3 roadmap will likely be better defining the plan for Hive
integration as Hive adds future versions.

1. Ability to interact with Hive metastore's from different versions
==> I.e. if a user has a metastore, can Spark SQL read the data? This
one we want need to solve by asking Hive for a stable metastore thrift
API, or adding sufficient features to the HCatalog API so we can use
that.

2. Compatibility with HQL over time as Hive adds new features.
==> This relates to how often we update our internal library
dependency on Hive and/or build support for new Hive features
internally.

:
th you. As of now, it seems not a good solution. I will try to talk to Hive people to see whether there is such guarantee for downward compatibility for thrift protocol. By the way, I tried some basic functions using hive-0.13 connect to hive-0.14 metastore, and it looks like they are compatible.
nvestigate this in more detail after 1.2.0 release. Anyway we really like to provide Hive support in Spark SQL as smooth and clean as possible for both developers and end users.
 that works with multiple Hive versions (esp. 0.12+). SPARK-4114 <https://issues.apache.org/jira/browse/SPARK-4114> was opened for this. Did some research into HCatalog recently, but I must confess that I'm not an expert on HCatalog, actually spent only 1 day on exploring it. So please don't hesitate to correct me if I was wrong about the conclusions I made below.
unately feature incomplete. It only provides a subset of most commonly used operations. For example, |HCatCreateTableDesc| maps only a subset of |CreateTableDesc|, properties like |storeAsSubDirectories|, |skewedColNames| and |skewedColValues| are missing. It's also impossible to alter table properties via HCatalog API (Spark SQL uses this to implement the |ANALYZE| command). The |hcat| CLI tool provides all those features missing in HCatalog API via raw Metastore API, and is structurally similar to the old Hive CLI.
ft protocol that matters. HCatalog is directly built upon raw Metastore API, and talks the same Metastore Thrift protocol. The problem we encountered in Spark SQL is that, usually we deploy Spark SQL Hive support with embedded mode (for testing) or local mode Metastore, and this makes us suffer from things like Metastore database schema changes. If Hive Metastore Thrift protocol is guaranteed to be downward compatible, then hopefully we can resort to remote mode Metastore and always depend on most recent Hive APIs. I had a glance of Thrift protocol version handling code in Hive, it seems that downward compatibility is not an issue. However I didn't find any official documents about Thrift protocol compatibility.
ive dependencies and remove the Hive shim layer introduced in branch 1.2. For users who use exactly the same version of Hive as Spark SQL, they can use either remote or local/embedded Metastore; while for users who want to interact with existing legacy Hive clusters, they have to setup a remote Metastore and let the Thrift protocol to handle compatibility.
ing
 these
y
d
d
art to
given
51.n3.nabble.com/How-spark-and-hive-integrate-in-long-term-tp9482.html
ble.com.
to
at
ly

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Sat, 22 Nov 2014 18:47:02 -0800",Re: Apache infra github sync down,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi All,

Unfortunately this went back down again. I've opened a new JIRA to track it:

https://issues.apache.org/jira/browse/INFRA-8688

- Patrick


---------------------------------------------------------------------


"
"""Evan R. Sparks"" <evan.sparks@gmail.com>","Sun, 23 Nov 2014 08:55:45 -0800",Notes on writing complex spark applications,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,

Shivaram Venkataraman, Joseph Gonzalez, Tomer Kaftan, and I have been
working on a short document about writing high performance Spark
applications based on our experience developing MLlib, GraphX, ml-matrix,
pipelines, etc. It may be a useful document both for users and new Spark
developers - perhaps it should go on the wiki?

The document itself is here:
https://docs.google.com/document/d/1gEIawzRsOwksV_bq4je3ofnd-7Xu-u409mdW-RXTDnQ/edit?usp=sharing
and I've created SPARK-4565
<https://issues.apache.org/jira/browse/SPARK-4565> to track this.

- Evan
"
andy petrella <andy.petrella@gmail.com>,"Sun, 23 Nov 2014 17:00:14 +0000",Re: Notes on writing complex spark applications,"""Evan R. Sparks"" <evan.sparks@gmail.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Cool!


"
Sam Bessalah <samkiller.oss@gmail.com>,"Sun, 23 Nov 2014 18:17:28 +0100",Re: Notes on writing complex spark applications,"""Evan R. Sparks"" <evan.sparks@gmail.com>","Thanks Evan, this is great.

"
Debasish Das <debasish.das83@gmail.com>,"Sun, 23 Nov 2014 09:50:29 -0800",Re: [VOTE] Release Apache Spark 1.1.1 (RC2),Hector Yee <hector.yee@gmail.com>,"-1 from me...same FetchFailed issue as what Hector saw...

I am running Netflix dataset and dumping out recommendation for all users.
It shuffles around 100 GB data on disk to run a reduceByKey per user on
utils.BoundedPriorityQueue...The code runs fine w"
Patrick Wendell <pwendell@gmail.com>,"Sun, 23 Nov 2014 15:39:50 -0800",Re: [VOTE] Release Apache Spark 1.1.1 (RC2),Debasish Das <debasish.das83@gmail.com>,"+1 (binding).

Don't see any evidence of regressions at this point. The issue
reported by Hector was not related to this rlease.


---------------------------------------------------------------------


"
Stephen Haberman <stephen.haberman@gmail.com>,"Mon, 24 Nov 2014 03:54:23 +0000",Re: [VOTE] Release Apache Spark 1.1.1 (RC2),"Andrew Or <andrew@databricks.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

I wanted to try 1.1.1-rc2 because we're running into SPARK-3633, but
the""rc"" releases not being tagged with ""-rcX"" means the pre-built artifacts
are basically useless to me.

(Pedantically, to test a release, I have to upload it into our internal
repo, to compile jobs, start clusters, etc. Invariably when an rcX artifact
ends up not being final, then I'm screwed, because I would have to clear
the local cache of any of our machines, dev/Jenkins/etc., that ever
downloaded the ""formerly known as 1.1.1 but not really"" rc artifacts.)

What's frustrating is that I know other Apache projects do rc releases, and
even get them into Maven central, e.g.:

http://search.maven.org/#search%7Cgav%7C1%7Cg%3A%22org.apache.tapestry%22%20AND%20a%3A%22tapestry-ioc%22

So, I apologize for the distraction from getting real work done, but
perhaps you guys could find a creative way to work around the
well-intentioned mandate on artifact voting?

(E.g. perhaps have multiple votes, one for each successive rc (with -rcX
suffix), then, once blessed, another one on the actually-final/no-rcX
artifact (built from the last rc's tag); or publish no-rcX artifacts for
official voting, as today, but then, at the same time, add -rcX artifacts
to Maven central for non-binding/3rd party testing, etc.)

Thanks,
Stephen
"
Matei Zaharia <matei.zaharia@gmail.com>,"Sun, 23 Nov 2014 20:11:11 -0800",Re: [VOTE] Release Apache Spark 1.1.1 (RC2),Stephen Haberman <stephen.haberman@gmail.com>,"Interesting, perhaps we could publish each one with two IDs, of which the rc one is unofficial. The problem is indeed that you have to vote on a hash for a potentially final artifact.

Matei

artifacts
internal
artifact
clear
releases, and
http://search.maven.org/#search%7Cgav%7C1%7Cg%3A%22org.apache.tapestry%22%20AND%20a%3A%22tapestry-ioc%22
-rcX
for
artifacts


---------------------------------------------------------------------


"
Inkyu Lee <gofiri@gmail.com>,"Mon, 24 Nov 2014 13:27:21 +0900",Re: Notes on writing complex spark applications,Sam Bessalah <samkiller.oss@gmail.com>,"Very helpful!!

thank you very much!

2014-11-24 2:17 GMT+09:00 Sam Bessalah <samkiller.oss@gmail.com>:

"
Patrick Wendell <pwendell@gmail.com>,"Sun, 23 Nov 2014 21:01:09 -0800",Re: [VOTE] Release Apache Spark 1.1.1 (RC2),Matei Zaharia <matei.zaharia@gmail.com>,"Hey Stephen,

Thanks for bringing this up. Technically when we call a release vote
it needs to be on the exact commit that will be the final release.
However, one thing I've thought of doing for a while would be to
publish the maven artifacts using a version tag with $VERSION-rcX even
if the underlying commit has $VERSION in the pom files. Some recent
changes I've made to the way we do publishing in branch 1.2 should
make this pretty easy - it wasn't very easy before because we used
maven's publishing plugin which makes modifying the published version
tricky. Our current approach is, indeed, problematic because maven
artifacts are supposed to be immutable once they have a specific
version identifier.

I created SPARK-4568 to track this:
https://issues.apache.org/jira/browse/SPARK-4568

- Patrick


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Sun, 23 Nov 2014 21:02:49 -0800",Re: Notes on writing complex spark applications,Inkyu Lee <gofiri@gmail.com>,"Hey Evan,

It might be nice to merge this into existing documentation. In
particular, a lot of this could serve to update the current tuning
section and programming guides.

It could also work to paste this wholesale as a reference for Spark
users, but in that case it's less likely to get updated when other
things change, or be found by users reading through the spark docs.

- Patrick


---------------------------------------------------------------------


"
tian zhang <tzhang101@yahoo.com.INVALID>,"Mon, 24 Nov 2014 05:31:28 +0000 (UTC)",2 spark streaming questions,"""user@spark.incubator.apache.org"" <user@spark.incubator.apache.org>, 
	Dev <dev@spark.apache.org>, User <user@spark.apache.org>, 
	Tathagata Das <tdas@databricks.com>","
Hi, Dear Spark Streaming Developers and Users,
We are prototyping using spark streaming and hit the following 2 issues thatI would like to seek your expertise.
1) We have a spark streaming application in scala, that reads Â data from Kafka intoa DStream, does some processing and output a transformed DStream. If for some reasonthe Kafka connection is not available or timed out, the spark streaming job will startto send empty RDD afterwards. The log is clean w/o any ERROR indicator.Â I googled Â around and this seems to be a known issue.We believe thatÂ spark streaming infrastructure should either retry or return error/exception.Can you share how you handle this case?
2) We would like implement a spark streaming job that join an 1 minute Â duration DStream of real time eventswith a metadata RDD that was read from a database. The metadata only changes slightly each day in the database.So what is the best practice of refresh the RDD daily keep the streaming join job running? Is this do-able as of spark 1.1.0?
Thanks.
Tian

"
Stephen Haberman <stephen.haberman@gmail.com>,"Mon, 24 Nov 2014 05:37:40 +0000",Re: [VOTE] Release Apache Spark 1.1.1 (RC2),"Patrick Wendell <pwendell@gmail.com>, Matei Zaharia <matei.zaharia@gmail.com>","Awesome, sounds great, guys; thanks for understanding.

Depending on how badly I need 1.1.1-rc2 (I'll check my jobs tomorrow) I'll
just build a local version for now. Should be easy, it's just been awhile.
:-)

Thanks,
Stephen



"
Sean Owen <sowen@cloudera.com>,"Mon, 24 Nov 2014 05:46:32 +0000",Re: [VOTE] Release Apache Spark 1.1.1 (RC2),Stephen Haberman <stephen.haberman@gmail.com>,"Stephen you can publish the artifact to your repo under a different
name, right? IIRC Maven will take care of the pom change along the
way. Yes you would not ever want to mess with changing an artifact
after it's published.

http://maven.apache.org/plugins/maven-install-plugin/examples/specific-local-repo.html

That's how I thought people did this, if they needed to do more than
test a tarball.


---------------------------------------------------------------------


"
Stephen Haberman <stephen.haberman@gmail.com>,"Mon, 24 Nov 2014 06:13:30 +0000",Re: [VOTE] Release Apache Spark 1.1.1 (RC2),Sean Owen <sowen@cloudera.com>,"

Hm, I didn't know about that plugin--assuming it does all of the
jar/pom/sources/etc., then, yes, that could work...

At first glance, I'm not sure it'll bring over the pom with all of the
transitive dependencies (because at least this invocation is only pointing
at a raw jar file to import), but I'll try it out tomorrow.

That is a good point though; if either you or other people have already
been solving this ""import a renamed-to-rc artifact"" in terribly
simple/obvious ways that I've just missed, I'm happy to be enlightened. :-)

Thanks,
Stephen
"
Shixiong Zhu <zsxwing@gmail.com>,"Mon, 24 Nov 2014 16:12:10 +0800",Re: Eliminate copy while sending data : any Akka experts here ?,Reynold Xin <rxin@databricks.com>,"MapOutputTrackerMasterActor sends the `mapOutputStatuses` to a buffer at
first. The messages in this buffer will be sent by some background threads.
In these threads, they will check if there are already too many messages
sent to Akka. If so, they will wait until there is enough memory.

I put a commit for this idea here:
https://github.com/zsxwing/spark/commit/c998856cdf747aa0452d030e58c3c2dd4ef7f97d

Best Regards,
Shixiong Zhu

2014-11-21 12:28 GMT+08:00 Reynold Xin <rxin@databricks.com>:

"
"""York, Brennon"" <Brennon.York@capitalone.com>","Mon, 24 Nov 2014 09:31:22 -0500",Time taken to merge Spark PR's?,"""dev@spark.apache.org"" <dev@spark.apache.org>","All, I just finished the SPARK-3182 feature and, for me, its raised a larger question of how to ensure patches that are awaiting review get noted / tagged upstream. Since I don’t have access writes to assign the above issue to myself I can’t tag it as “In Progress” like Matei mentioned so, at this rate, its just going to sit in the queue. Did I miss something on the “Contributing to Spark” page, is there a ‘tribal-knowledge’ way to let a set of commiters know that patches are ready, or is it that everyone is already too slammed and we’re all waiting diligently? :) Just trying to get some clarity on this topic, thanks!
________________________________________________________

The information contained in this e-mail is confidential and/or proprietary is intended only for use by the individual or entity to which it is addressed.  If the reader of this message is not the intended recipient, you are hereby notified that any review, retransmission, dissemination, distribution, copying or other use of, or taking of any action in reliance upon this information is strictly prohibited. If you have received this communication in error, please contact the sender and delete the material from your computer.
"
Debasish Das <debasish.das83@gmail.com>,"Mon, 24 Nov 2014 09:06:18 -0800",Re: [VOTE] Release Apache Spark 1.1.1 (RC2),Hector Yee <hector.yee@gmail.com>,"Actually +1 from me...

This is a recommendAll feature we are testing which is really compute
intensive...

For ranking metric calculation I was trying to run through the Netflix
matrix and generate a ranked list of recommendation for all 17K products
and perhaps it needs more compute than what is needed. I was running 6
nodes, 120 cores, 240 GB...It needed to shuffle around 100 GB over 6
nodes...

A version with topK runs fine where K = (some multipler on number of movies
each user saw and we cross validate on that)

Running the following JIRA on Netflix dataset (the dataset is distributed
with Jellyfish code http://i.stanford.edu/hazy/victor/Hogwild/), will
reproduce the failure...

https://issues.apache.org/jira/browse/SPARK-4231

The failed job I will debug more and figure out the real cause. If needed I
will open up new JIRAs.


"
vaquar khan <vaquar.khan@gmail.com>,"Mon, 24 Nov 2014 22:54:13 +0530",Re: [VOTE] Release Apache Spark 1.1.1 (RC2),Andrew Or <andrew@databricks.com>,"+1 Release this package as Apache Spark 1.1.1

"
Andrew Or <andrew@databricks.com>,"Mon, 24 Nov 2014 11:22:59 -0800",[VOTE][RESULT] Release Apache Spark 1.1.1 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","The vote passes unanimously with 4 binding +1 votes, 5 non-binding +1
votes, and no +0 or -1 votes. The final release will be posted in the next
48 hours. Thanks to everyone who voted.

-Andrew

+1:

Andrew Or*
Xiangrui Meng*
Krishna Sankar
Matei Zaharia*
Sean Owen
Anant Asthana
Marcelo Vanzin
Patrick Wendell*
Debasish Das

+0:

-1:

*binding
"
"""Evan R. Sparks"" <evan.sparks@gmail.com>","Mon, 24 Nov 2014 12:08:41 -0800",Re: Notes on writing complex spark applications,Patrick Wendell <pwendell@gmail.com>,"Thanks Patrick,

You raise a good point - for this to be useful it's imperative that it is
updated with new versions of spark.

My thought with putting it on the wiki was that it's lower friction for
community members to edit, but it likely won't have the same level of
quality control as the existing documentation.

At a higher level - some of these tips are best practices for writing
applications that depend on Spark. I'm wondering if a new document is in
order for things like ""this is how you set up a project skeleton to link
against spark,"" and ""this is how you handle external libraries,"" - etc.? I
know that in the past I've run into stumbling blocks on things like getting
classpaths correct, trying to link against a different version of akka, and
so on that would be useful to have in such a document, in addition to some
of the application architecture suggestions we propose in *this* document.

- Evan


"
pedrorodriguez <ski.rodriguez@gmail.com>,"Mon, 24 Nov 2014 13:54:31 -0700 (MST)","Re: sbt publish-local fails, missing spark-network-common",dev@spark.incubator.apache.org,"My fork is (still) even with the master branch, so there is nothing to
update. Talking about this here: https://github.com/apache/spark/pull/3405

If anyone knows how to configure the build process on handling
warnings/errors from javadoc, that would be very helpful.



--

---------------------------------------------------------------------


"
EarthsonLu <Earthson.Lu@gmail.com>,"Mon, 24 Nov 2014 19:08:11 -0700 (MST)",[SparkSQL] Why this AttributeReference.exprId is not setted?,dev@spark.incubator.apache.org,"https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/Aggregate.scala#L85

I can't understand this code, it seems to be a ""bug"", but group by of
SparkSQL just works fine.

with code below, some expressions are mapping to AttributeReferences, then
""bindReference"" method will find there references. But resultAttribute's
exprId is new, I don't think it can find the true reference(But it does,
Why? How?)


  private[this] val resultExpressions = aggregateExpressions.map { agg =>
    agg.transform {
      case e: Expression if resultMap.contains(e) => resultMap(e)
    }
  }

I'm trying to write a DSL with Catalyst





--

---------------------------------------------------------------------


"
EarthsonLu <Earthson.Lu@gmail.com>,"Mon, 24 Nov 2014 19:31:04 -0700 (MST)","Re: [SparkSQL][Solved] Why this AttributeReference.exprId is not
 setted?",dev@spark.incubator.apache.org,"Got it. 


private[this] val computedSchema = computedAggregates.map(_.resultAttribute) 



--

---------------------------------------------------------------------


"
xukun <xukun.xu@huawei.com>,"Tue, 25 Nov 2014 21:43:11 +0800",java.io.IOException: sendMessageReliably failed without being ACK'd,<dev@spark.apache.org>,"data size: text file, 315G
cmd:
./spark-submit --class com.spark.test.JavaWordCountWithSave --num-executors 7 --executor-memory 60g --driver-memory 2g --executor-cores 32
--master yarn-client /home/cjs/spark-test.jar hdfs://wordcount/input hdfs://wordcount/output

code of JavaWordCountWithSave:
```
public final class JavaWordCountWithSave {
    private static final Pattern SPACE = Pattern.compile("" "");

    public static void main(String[] args) throws Exception {

        if (args.length < 2) {
            System.err.println(""Usage: JavaWordCount <file>"");
            System.exit(1);
        }

        SparkConf sparkConf = new SparkConf().setAppName(""JavaWordCountWithSave"");
        JavaSparkContext ctx = new JavaSparkContext(sparkConf);
        JavaRDD<String> lines = ctx.textFile(args[0], 1);

        JavaRDD<String> words = lines.flatMap(new FlatMapFunction<String, String>() {
            @Override
            public Iterable<String> call(String s) {
                return Arrays.asList(SPACE.split(s));
            }
        });

        JavaPairRDD<String, Integer> ones = words.mapToPair(new PairFunction<String, String, Integer>() {
            @Override
            public Tuple2<String, Integer> call(String s) {
                return new Tuple2<String, Integer>(s, 1);
            }
        });

        JavaPairRDD<String, Integer> counts = ones.reduceByKey(new Function2<Integer, Integer, Integer>() {
            @Override
            public Integer call(Integer i1, Integer i2) {
                return i1 + i2;
            }
        });

        counts.saveAsTextFile(args[1]);

        ctx.stop();
    }
}
```


log of driver---
14/11/20 14:57:46 WARN TaskSetManager: Lost task 167.0 in stage 1.0 (TID 5207, linux-171): ExecutorLostFailure (executor lost)
14/11/20 14:57:46 WARN TaskSetManager: Lost task 41.0 in stage 1.0 (TID 5081, linux-171): ExecutorLostFailure (executor lost)
14/11/20 14:57:46 WARN TaskSetManager: Lost task 104.0 in stage 1.0 (TID 5144, linux-171): ExecutorLostFailure (executor lost)
14/11/20 14:57:46 WARN TaskSetManager: Lost task 62.0 in stage 1.0 (TID 5102, linux-171): ExecutorLostFailure (executor lost)
14/11/20 14:57:46 WARN TaskSetManager: Lost task 20.0 in stage 1.0 (TID 5060, linux-171): ExecutorLostFailure (executor lost)
14/11/20 14:57:46 ERROR YarnClientSchedulerBackend: Asked to remove non existant executor 5
14/11/20 14:57:46 INFO DAGScheduler: Executor lost: 5 (epoch 1)
14/11/20 14:57:46 ERROR YarnClientSchedulerBackend: Asked to remove non existant executor 5
14/11/20 14:57:46 INFO BlockManagerMasterActor: Trying to remove executor 5 from BlockManagerMaster.
14/11/20 14:57:46 INFO BlockManagerMaster: Removed 5 successfully in removeExecutor
14/11/20 14:57:46 ERROR YarnClientSchedulerBackend: Asked to remove non existant executor 5
14/11/20 14:57:46 ERROR YarnClientSchedulerBackend: Asked to remove non existant executor 5

log of executor---
2014-11-20 14:57:46,879 | INFO  | [connection-manager-thread] | key already cancelled ? sun.nio.ch.SelectionKeyImpl@a6b0591 | org.apache.spark.Logging$class.logInfo(Logging.scala:80)
java.nio.channels.CancelledKeyException
	at org.apache.spark.network.nio.ConnectionManager.run(ConnectionManager.scala:379)
	at org.apache.spark.network.nio.ConnectionManager$$anon$4.run(ConnectionManager.scala:132)
2014-11-20 14:57:46,958 | INFO  | [handle-read-write-executor-3] | Removing SendingConnection to ConnectionManagerId(172.168.xxx.16,2267) | org.apache.spark.Logging$class.logInfo(Logging.scala:59)
2014-11-20 14:57:46,963 | INFO  | [handle-read-write-executor-3] | Notifying org.apache.spark.network.nio.ConnectionManager$MessageStatus@272b8b5a | org.apache.spark.Logging$class.logInfo(Logging.scala:59)
2014-11-20 14:57:46,963 | INFO  | [handle-read-write-executor-3] | Notifying org.apache.spark.network.nio.ConnectionManager$MessageStatus@1bc9d5cd | org.apache.spark.Logging$class.logInfo(Logging.scala:59)
2014-11-20 14:57:47,107 | ERROR | [Connection manager future execution context-2] | Failed to get block(s) from 172.168.xxx.16:2267 | org.apache.spark.Logging$class.logError(Logging.scala:96)
java.io.IOException: sendMessageReliably failed without being ACK'd
	at org.apache.spark.network.nio.ConnectionManager$$anonfun$14.apply(ConnectionManager.scala:822)
	at org.apache.spark.network.nio.ConnectionManager$$anonfun$14.apply(ConnectionManager.scala:818)
	at org.apache.spark.network.nio.ConnectionManager$MessageStatus.markDone(ConnectionManager.scala:61)
	at org.apache.spark.network.nio.ConnectionManager$$anonfun$removeConnection$3.apply(ConnectionManager.scala:451)
	at org.apache.spark.network.nio.ConnectionManager$$anonfun$removeConnection$3.apply(ConnectionManager.scala:449)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.network.nio.ConnectionManager.removeConnection(ConnectionManager.scala:449)
	at org.apache.spark.network.nio.ConnectionManager$$anonfun$addListeners$3.apply(ConnectionManager.scala:428)
	at org.apache.spark.network.nio.ConnectionManager$$anonfun$addListeners$3.apply(ConnectionManager.scala:428)
	at org.apache.spark.network.nio.Connection.close(Connection.scala:124)
	at org.apache.spark.network.nio.SendingConnection.read(Connection.scala:414)
	at org.apache.spark.network.nio.ConnectionManager$$anon$7.run(ConnectionManager.scala:192)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
2014-11-20 14:57:47,148 | INFO  | [handle-read-write-executor-3] | Notifying org.apache.spark.network.nio.ConnectionManager$MessageStatus@5f59c2f6 | org.apache.spark.Logging$class.logInfo(Logging.scala:59)
2014-11-20 14:57:47,149 | ERROR | [Connection manager future execution context-4] | Failed to get block(s) from 172.168.xxx.16:2267 | org.apache.spark.Logging$class.logError(Logging.scala:96)
java.io.IOException: sendMessageReliably failed without being ACK'd


---------------------------------------------------------------------


"
xukun <xukun.xu@huawei.com>,"Tue, 25 Nov 2014 22:00:41 +0800","java.util.concurrent.TimeoutException: Futures timed out after [10000
 milliseconds]",<dev@spark.apache.org>,"submit 12 spark applications in the same time. yarn web page shows: two task fail.

the cmd:
./spark-submit    --class org.apache.spark.examples.JavaWordCount   --master yarn-cluster   ---executor-memory 2g   ../lib/spark-examples_2.10-1.1.0.jar     hdfs://hacluster/bigData

driver log of one fail task:
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
Exception in thread ""Driver"" java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
	at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)
	at scala.concurrent.Await$.result(package.scala:107)
	at akka.remote.Remoting.start(Remoting.scala:173)
	at akka.remote.RemoteActorRefProvider.init(RemoteActorRefProvider.scala:184)
	at akka.actor.ActorSystemImpl._start$lzycompute(ActorSystem.scala:579)
	at akka.actor.ActorSystemImpl._start(ActorSystem.scala:577)
	at akka.actor.ActorSystemImpl.start(ActorSystem.scala:588)
	at akka.actor.ActorSystem$.apply(ActorSystem.scala:111)
	at akka.actor.ActorSystem$.apply(ActorSystem.scala:104)
	at org.apache.spark.util.AkkaUtils$.org$apache$spark$util$AkkaUtils$$doCreateActorSystem(AkkaUtils.scala:121)
	at org.apache.spark.util.AkkaUtils$$anonfun$1.apply(AkkaUtils.scala:54)
	at org.apache.spark.util.AkkaUtils$$anonfun$1.apply(AkkaUtils.scala:53)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.AkkaUtils$.createActorSystem(AkkaUtils.scala:56)
	at org.apache.spark.SparkEnv$.create(SparkEnv.scala:161)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:213)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:56)
2014-11-23 18:41:19,010 | INFO  | [main] | Registered signal handlers for [TERM, HUP, INT] | org.apache.spark.util.SignalLogger$.register(SignalLogger.scala:47)
2014-11-23 18:41:54,403 | WARN  | [main] | Unable to load native-hadoop library for your platform... using builtin-java classes where applicable | org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62)
2014-11-23 18:42:10,319 | INFO  | [main] | ApplicationAttemptId: appattempt_1416732306135_0043_000001 | org.apache.spark.Logging$class.logInfo(Logging.scala:59)
2014-11-23 18:42:12,213 | INFO  | [main] | Changing view acls to: omm,spark | org.apache.spark.Logging$class.logInfo(Logging.scala:59)
2014-11-23 18:42:12,280 | INFO  | [main] | Changing modify acls to: omm,spark | org.apache.spark.Logging$class.logInfo(Logging.scala:59)
2014-11-23 18:42:12,300 | INFO  | [main] | SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(omm, spark); users with modify permissions: Set(omm, spark) | org.apache.spark.Logging$class.logInfo(Logging.scala:59)
2014-11-23 18:42:18,597 | INFO  | [main] | Starting the user JAR in a separate Thread | org.apache.spark.Logging$class.logInfo(Logging.scala:59)
2014-11-23 18:42:18,787 | INFO  | [main] | Waiting for spark context initialization | org.apache.spark.Logging$class.logInfo(Logging.scala:59)
2014-11-23 18:42:18,788 | INFO  | [main] | Waiting for spark context initialization ... 0 | org.apache.spark.Logging$class.logInfo(Logging.scala:59)
2014-11-23 18:42:19,801 | WARN  | [Driver] | In Spark 1.0 and later spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone and LOCAL_DIRS in YARN). | org.apache.spark.Logging$class.logWarning(Logging.scala:71)
2014-11-23 18:42:22,495 | INFO  | [Driver] | Changing view acls to: omm,spark | org.apache.spark.Logging$class.logInfo(Logging.scala:59)
2014-11-23 18:42:22,521 | INFO  | [Driver] | Changing modify acls to: omm,spark | org.apache.spark.Logging$class.logInfo(Logging.scala:59)
2014-11-23 18:42:22,521 | INFO  | [Driver] | SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(omm, spark); users with modify permissions: Set(omm, spark) | org.apache.spark.Logging$class.logInfo(Logging.scala:59)
2014-11-23 18:42:28,823 | INFO  | [main] | Waiting for spark context initialization ... 1 | org.apache.spark.Logging$class.logInfo(Logging.scala:59)
2014-11-23 18:42:38,896 | INFO  | [main] | Waiting for spark context initialization ... 2 | org.apache.spark.Logging$class.logInfo(Logging.scala:59)
2014-11-23 18:42:47,737 | INFO  | [sparkDriver-akka.actor.default-dispatcher-3] | Slf4jLogger started | akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:80)
2014-11-23 18:42:48,938 | INFO  | [main] | Waiting for spark context initialization ... 3 | org.apache.spark.Logging$class.logInfo(Logging.scala:59)
2014-11-23 18:42:53,192 | INFO  | [sparkDriver-akka.actor.default-dispatcher-4] | Starting remoting | akka.event.slf4j.Slf4jLogger$$anonfun$receive$1$$anonfun$applyOrElse$3.apply$mcV$sp(Slf4jLogger.scala:74)
2014-11-23 18:42:58,969 | INFO  | [main] | Waiting for spark context initialization ... 4 | org.apache.spark.Logging$class.logInfo(Logging.scala:59)
2014-11-23 18:43:04,721 | ERROR | [sparkDriver-akka.actor.default-dispatcher-3] | Remoting error: [Startup timed out] [
akka.remote.RemoteTransportException: Startup timed out
	at akka.remote.Remoting.akka$remote$Remoting$$notifyError(Remoting.scala:129)
	at akka.remote.Remoting.start(Remoting.scala:191)
	at akka.remote.RemoteActorRefProvider.init(RemoteActorRefProvider.scala:184)
	at akka.actor.ActorSystemImpl._start$lzycompute(ActorSystem.scala:579)
	at akka.actor.ActorSystemImpl._start(ActorSystem.scala:577)
	at akka.actor.ActorSystemImpl.start(ActorSystem.scala:588)
	at akka.actor.ActorSystem$.apply(ActorSystem.scala:111)
	at akka.actor.ActorSystem$.apply(ActorSystem.scala:104)
	at org.apache.spark.util.AkkaUtils$.org$apache$spark$util$AkkaUtils$$doCreateActorSystem(AkkaUtils.scala:121)
	at org.apache.spark.util.AkkaUtils$$anonfun$1.apply(AkkaUtils.scala:54)
	at org.apache.spark.util.AkkaUtils$$anonfun$1.apply(AkkaUtils.scala:53)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.AkkaUtils$.createActorSystem(AkkaUtils.scala:56)
	at org.apache.spark.SparkEnv$.create(SparkEnv.scala:161)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:213)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:56)
	at org.apache.spark.examples.JavaWordCount.main(JavaWordCount.java:44)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:460)
Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
	at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)
	at scala.concurrent.Await$.result(package.scala:107)
	at akka.remote.Remoting.start(Remoting.scala:173)
	... 22 more



driver log of other fail task:

 2014-11-23 18:49:15,962 | INFO  | [main] | Waiting for spark context initialization ... 9 | org.apache.spark.Logging$class.logInfo(Logging.scala:59)
2014-11-23 18:49:17,188 | INFO  | [Driver] | Successfully started service 'SparkUI' on port 23702. | org.apache.spark.Logging$class.logInfo(Logging.scala:59)
2014-11-23 18:49:17,246 | INFO  | [Driver] | Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter | org.apache.spark.Logging$class.logInfo(Logging.scala:59)
2014-11-23 18:49:17,482 | INFO  | [Driver] | Started SparkUI at http://vm-181:23702 | org.apache.spark.Logging$class.logInfo(Logging.scala:59)
2014-11-23 18:49:20,526 | INFO  | [Driver] | Created YarnClusterScheduler | org.apache.spark.Logging$class.logInfo(Logging.scala:59)
2014-11-23 18:49:26,121 | ERROR | [main] | SparkContext did not initialize after waiting for 100000 ms. Please check earlier log output for errors. Failing the application. | org.apache.spark.Logging$class.logError(Logging.scala:75)
2014-11-23 18:49:27,082 | INFO  | [main] | Final app status: FAILED, exitCode: 13, (reason: Timed out waiting for SparkContext.) | org.apache.spark.Logging$class.logInfo(Logging.scala:59)
2014-11-23 18:49:27,482 | INFO  | [main] | In securityManager checkExit, exit code: 13 | org.apache.spark.Logging$class.logInfo(Logging.scala:59)
2014-11-23 18:49:28,699 | INFO  | [Thread-3] | Unregistering ApplicationMaster with FAILED (diag message: Timed out waiting for SparkContext.) | org.apache.spark.Logging$class.logInfo(Logging.scala:59)
2014-11-23 18:50:22,133 | INFO  | [Thread-3] | Deleting staging directory .sparkStaging/application_1416732306135_0043 | org.apache.spark.Logging$class.logInfo(Logging.scala:59)
2014-11-23 18:50:36,274 | WARN  | [Driver] | interrupted waiting to send rpc request to server | org.apache.hadoop.ipc.Client.call(Client.java:1388)
java.lang.InterruptedException
	at java.util.concurrent.FutureTask.awaitDone(FutureTask.java:400)
	at java.util.concurrent.FutureTask.get(FutureTask.java:187)
	at org.apache.hadoop.ipc.Client$Connection.sendRpcRequest(Client.java:1029)
	at org.apache.hadoop.ipc.Client.call(Client.java:1383)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy16.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:701)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy17.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1780)
	at org.apache.hadoop.hdfs.DistributedFileSystem$18.doCall(DistributedFileSystem.java:1165)
	at org.apache.hadoop.hdfs.DistributedFileSystem$18.doCall(DistributedFileSystem.java:1161)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1161)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1398)
	at org.apache.spark.util.FileLogger.createLogDir(FileLogger.scala:108)
	at org.apache.spark.util.FileLogger.start(FileLogger.scala:100)
	at org.apache.spark.scheduler.EventLoggingListener.start(EventLoggingListener.scala:74)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:323)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:56)
	at org.apache.spark.examples.JavaWordCount.main(JavaWordCount.java:44)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:460)






---------------------------------------------------------------------


"
"""York, Brennon"" <Brennon.York@capitalone.com>","Tue, 25 Nov 2014 14:12:23 -0500",How to resolve Spark site issues?,"""dev@spark.apache.org"" <dev@spark.apache.org>","For JIRA tickets like SPARK-4046<https://issues.apache.org/jira/browse/SPARK-4046> (Incorrect Java example on site) is there a way to go about fixing those things? Its a trivial fix, but I’m not seeing that code in the codebase anywhere. Is this something the admins are going to have to take care of? Just want to clarify before I let it go and let the example sit on the site :)
________________________________________________________

The information contained in this e-mail is confidential and/or proprietary is intended only for use by the individual or entity to which it is addressed.  If the reader of this message is not the intended recipient, you are hereby notified that any review, retransmission, dissemination, distribution, copying or other use of, or taking of any action in reliance upon this information is strictly prohibited. If you have received this communication in error, please contact the sender and delete the material from your computer.
"
Reynold Xin <rxin@databricks.com>,"Tue, 25 Nov 2014 11:30:28 -0800",Re: How to resolve Spark site issues?,"""York, Brennon"" <Brennon.York@capitalone.com>","The website is hosted on some svn server by ASF and unfortunately it
doesn't have a github mirror, so we will have to manually patch it ...


m

x,
mething
 I
"
Sean Owen <sowen@cloudera.com>,"Tue, 25 Nov 2014 19:51:55 +0000",Re: How to resolve Spark site issues?,Reynold Xin <rxin@databricks.com>,"For the interested, the SVN repo for the site is viewable at
http://svn.apache.org/viewvc/spark/site/ and to check it out, you can
""svn co https://svn.apache.org/repos/asf/spark/site""

I assume the best process is to make a diff and attach it to the JIRA.
How old school.

com
e
ix,
omething
e I
y

---------------------------------------------------------------------


"
Jianshi Huang <jianshi.huang@gmail.com>,"Wed, 26 Nov 2014 14:13:55 +0800",Re: How to do broadcast join in SparkSQL,Michael Armbrust <michael@databricks.com>,"Hi,

Looks like the latest SparkSQL with Hive 0.12 has a bug in Parquet support.
I got the following exceptions:

org.apache.hadoop.hive.ql.parse.SemanticException: Output Format must
implement HiveOutputFormat, otherwise it should be either
IgnoreKeyTextOutputFormat or SequenceFileOutputFormat
        at
org.apache.hadoop.hive.ql.plan.CreateTableDesc.validate(CreateTableDesc.java:431)
        at
org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeCreateTable(SemanticAnalyzer.java:9964)
        at
org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:9180)
        at
org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:327)

Using the same DDL and Analyze script above.

Jianshi






-- 
Jianshi Huang

LinkedIn: jianshi
Twitter: @jshuang
Github & Blog: http://huangjs.github.com/
"
Jianshi Huang <jianshi.huang@gmail.com>,"Wed, 26 Nov 2014 14:26:49 +0800",Re: How to do broadcast join in SparkSQL,Michael Armbrust <michael@databricks.com>,"Oh, I found a explanation from
http://cmenguy.github.io/blog/2013/10/30/using-hive-with-parquet-format-in-cdh-4-dot-3/

The error here is a bit misleading, what it really means is that the class
parquet.hive.DeprecatedParquetOutputFormat isnâ€™t in the classpath for Hive.
Sure enough, doing a ls /usr/lib/hive/lib doesnâ€™t show any of the parquet
jars, but ls /usr/lib/impala/lib shows the jar weâ€™re looking for as
parquet-hive-1.0.jar
Is it removed from latest Spark?

Jianshi



ava:431)
ticAnalyzer.java:9964)
Analyzer.java:9180)
Analyzer.java:327)
for
ng
n to
e033990ab5
t weird
:
n



-- 
Jianshi Huang

LinkedIn: jianshi
Twitter: @jshuang
Github & Blog: http://huangjs.github.com/
"
Yanbo Liang <yanbohappy@gmail.com>,"Wed, 26 Nov 2014 17:39:06 +0800","[mllib] useFeatureScaling likes hardcode in LogisticRegressionWithLBFGS
 and is not comprehensive for users.","""dev@spark.apache.org"" <dev@spark.apache.org>, Xiangrui Meng <mengxr@gmail.com>","Hi All,

LogisticRegressionWithLBFGS set useFeatureScaling to true default which can
improve the convergence during optimization.
However, other model training method such as LogisticRegressionWithSGD does
not set useFeatureScaling to true by default and the corresponding set
function is private in mllib scope which can not be set by users.

The default configuration will cause mismatch training and prediction.
Suppose that users prepare input data for training set and predict set with
the same format, then run model training with LogisticRegressionWithLBFGS
and prediction.
But they do not know that it contains feature scaling in training step but
w/o it in prediction step.
When prediction step, it will apply model on dataset whose extent or scope
is not consistent with training step.

Should we make setFeatureScaling function to public and change default
value to false?
I think it is more clear and comprehensive to make feature scale and
normalization in preprocessing step of the machine learning pipeline.
If this proposal is OK, I will file a JIRA to track.
"
rzykov <rzykov@gmail.com>,"Wed, 26 Nov 2014 02:49:24 -0700 (MST)",Re: java.lang.OutOfMemoryError at simple local test,dev@spark.incubator.apache.org,"We made some changes in code (it generates 1000 * 1000 elements) and memory
limits up to 100M:

def generate = {
  for{
    j <- 1 to 10
    i <- 1 to 1000
  } yield(j, i)
}

~/soft/spark-1.1.0-bin-hadoop2.3/bin/spark-submit --master local
--executor-memory 100M --driver-memory 100M --class Spill --num-executors 1
--executor-cores 1 target/scala-2.10/Spill-assembly-1.0.jar

The result of this: 
14/11/24 14:57:40 ERROR ExecutorUncaughtExceptionHandler: Uncaught exception
in thread Thread[Executor task launch worker-0,5,main]
java.lang.OutOfMemoryError: GC overhead limit exceeded

We decided to check this one by profiler and took this screenshot:
<http://apache-spark-developers-list.1001551.n3.nabble.com/file/n9532/%D0%A1%D0%BA%D1%80%D0%B8%D0%BD%D1%88%D0%BE%D1%82_2014-11-26_11.png> 

Each element of collection takes 48 bytes. Each element  = scala.Tuple2 of 2
java.lang.Integer.
But Scala  supports ""@specialized""
<https://github.com/scala/scala/blob/v2.10.4/src/library/scala/Tuple2.scala#L19>   
unboxed primitive type of Int. Which takes 4 bytes only.
So from this point of view this collection would take about 1000 * 1000 * 2
* 4 = 8 Mb + some overheads.
This number in 5 times less then current result of memory consumpution.
Why Spark didn't use primitive (@specialized) types in this case?





--

---------------------------------------------------------------------


"
"""York, Brennon"" <Brennon.York@capitalone.com>","Wed, 26 Nov 2014 11:17:16 -0500",Re: How to resolve Spark site issues?,"Sean Owen <sowen@cloudera.com>, Reynold Xin <rxin@databricks.com>","A diff you say?! Done and done. If someone with privileges to push to the
SVN site repo could check it out I think we¹d be good to go. @Sean, thanks
for the repo URL!



________________________________________________________

The information contained in this e-mail is confidential and/or proprietary is intended only for use by the individual or entity to which it is addressed.  If the reader of this message is not the intended recipient, you are hereby notified that any review, retransmission, dissemination, distribution, copying or other use of, or taking of any action in reliance upon this information is strictly prohibited. If you have received this communication in error, please contact the sender and delete the material from your computer.


---------------------------------------------------------------------


"
Xiangrui Meng <mengxr@gmail.com>,"Wed, 26 Nov 2014 12:06:57 -0800","Re: [mllib] useFeatureScaling likes hardcode in LogisticRegressionWithLBFGS
 and is not comprehensive for users.",Yanbo Liang <yanbohappy@gmail.com>,"Hi Yanbo,

We scale the model coefficients back after training. So scaling in
prediction is not necessary.

We had some discussion about this. I'd like to treat feature scaling
as part of the feature transformation, and recommend users to apply
feature scaling before training. It is a cleaner solution to me, and
this is easy with the new pipeline API. DB (cc'ed) recommends
embedding feature scaling in linear methods, because it generally
leads better conditioning, which is also valid. Feel free to create a
JIRA and we can have the discussion there.

Best,
Xiangrui


---------------------------------------------------------------------


"
DB Tsai <dbtsai@dbtsai.com>,"Wed, 26 Nov 2014 16:05:49 -0800","Re: [mllib] useFeatureScaling likes hardcode in LogisticRegressionWithLBFGS
 and is not comprehensive for users.",Xiangrui Meng <mengxr@gmail.com>,"Hi Yanbo,

As Xiangrui said, the feature scaling in training step is transparent
to users, and
in theory, with/without feature scaling, the optimization should
converge to the same
solution after transforming to the original space.

In short, we do the training in the scaled space, and get the weights
in the scaled space.
Then we transform the weights to the original space so it's
transparent to users.

GLMNET package in R does the same thing, and I think we should do it
instead of asking
users to do it using pipeline API since not all the users know this stuff.

Also, in GLMNET package, there are different strategies to do feature
scalling for linear regression
and logistic regression; as a result, we don't want to make it public
api naively without addressing
different use-case.

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Wed, 26 Nov 2014 16:36:39 -0800",Re: How to resolve Spark site issues?,"""York, Brennon"" <Brennon.York@capitalone.com>","Thanks, Brennon. I pushed the change and updated the website.


thanks
:
l
"
Shaocun Tian <tianshaocun@gmail.com>,"Wed, 26 Nov 2014 19:59:33 -0700 (MST)","Re: [mllib] useFeatureScaling likes hardcode in
 LogisticRegressionWithLBFGS and is not comprehensive for users.",dev@spark.incubator.apache.org,"Hi, all

As I understand, with feature scaling the optimization algorithm will
converge faster.  Here I have a question about doing scaling multi times. I
know that doing more standard scaling will cause no difference. But if I
want to try MinMax scaling, would it be weird to using standard scaling
again before training?

Another small issue we met in Spark 1.0 is that loss computation is
LogisticGradient might overflow (e.g. log1p(exp(margin)) might be
infinity). This issue disappears when feature is scaled. So if we decide to
accept unscaled input data, it should be handled properly. We have been
using softmax instead log1p to fix it.

btw, I can contribute our MinMaxScaler implementation if it is useful for
others.

Best,
Shaocun Tian






--"
Praveen Sripati <praveensripati@gmail.com>,"Thu, 27 Nov 2014 17:17:24 +0530",Standalone scheduling - document inconsistent,dev@spark.apache.org,"Hi,

There is a bit of inconsistent in the document. Which is the correct
statement?

`http://spark.apache.org/docs/latest/spark-standalone.html` says

The standalone cluster mode currently only supports a simple FIFO scheduler
across applications.

while `http://spark.apache.org/docs/latest/job-scheduling.html` says

Starting in Spark 0.8, it is also possible to configure fair sharing
between jobs.

Thanks,
Praveen
"
Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"Thu, 27 Nov 2014 07:41:58 -0700 (MST)",[mllib] Which is the correct package to add a new algorithm?,dev@spark.incubator.apache.org,"Hi all, 

Spark ML alpha version exists in the current master branch on Github.
If we want to add new machine learning algorithms or to modify algorithms
which already exists, 
which package should we implement them at org.apache.spark.mllib or
org.apache.spark.ml?

thanks,
Yu



-----
-- Yu Ishikawa
--

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 27 Nov 2014 19:58:06 +0000",Re: Time taken to merge Spark PR's?,"""York, Brennon"" <Brennon.York@capitalone.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","1.1.1 was just released, and 1.2 is close to a release. That, plus
Thanksgiving in the US (where most Spark committers AFAIK are located),
probably means a temporary lull in committer activity on non-critical items
should be expected.


ed
 above
ike Matei mentioned so, at
e
knowledgeâ€™ way to let a
ing to get
"
Reynold Xin <rxin@databricks.com>,"Thu, 27 Nov 2014 12:47:20 -0800",Re: Standalone scheduling - document inconsistent,Praveen Sripati <praveensripati@gmail.com>,"The 1st was referring to different Spark applications connecting to the
standalone cluster manager, and the 2nd one was referring to within a
single Spark application, the jobs can be scheduled using a fair scheduler.



"
Niranda Perera <niranda@wso2.com>,"Fri, 28 Nov 2014 12:01:13 +0530",Creating a SchemaRDD from an existing API,"dev@spark.apache.org, user@spark.apache.org","Hi,

I am evaluating Spark for an analytic component where we do batch
processing of data using SQL.

So, I am particularly interested in Spark SQL and in creating a SchemaRDD
from an existing API [1].

This API exposes elements in a database as datasources. Using the methods
allowed by this data source, we can access and edit data.

So, I want to create a custom SchemaRDD using the methods and provisions of
this API. I tried going through Spark documentation and the Java Docs, but
unfortunately, I was unable to come to a final conclusion if this was
actually possible.

I would like to ask the Spark Devs,
1. As of the current Spark release, can we make a custom SchemaRDD?
2. What is the extension point to a custom SchemaRDD? or are there
particular interfaces?
3. Could you please point me the specific docs regarding this matter?

Your help in this regard is highly appreciated.

Cheers

[1]
https://github.com/wso2-dev/carbon-analytics/tree/master/components/xanalytics

-- 
*Niranda Perera*
Software Engineer, WSO2 Inc.
Mobile: +94-71-554-8430
Twitter: @n1r44 <https://twitter.com/N1R44>
"
Michael Armbrust <michael@databricks.com>,"Fri, 28 Nov 2014 11:27:22 -0800",Re: Creating a SchemaRDD from an existing API,Niranda Perera <niranda@wso2.com>,"You probably don't need to create a new kind of SchemaRDD.  Instead I'd
suggest taking a look at the data sources API that we are adding in Spark
1.2.  There is not a ton of documentation, but the test cases show how to
implement the various interfaces
<https://github.com/apache/spark/tree/master/sql/core/src/test/scala/org/apache/spark/sql/sources>,
and there is an example library for reading Avro data
<https://github.com/databricks/spark-avro>.


"
Joseph Bradley <joseph@databricks.com>,"Fri, 28 Nov 2014 12:15:55 -0800",Re: [mllib] Which is the correct package to add a new algorithm?,Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"Hi Yu,
Thanks for bringing it up for clarification.  Here's a rough draft of a
section for the soon-to-be-updated programming guide, which will have more
info on the spark.ml package.
Joseph

## spark.mllib vs. spark.ml

Spark 1.2 will include a new machine learning package called spark.ml,
currently an alpha component but potentially a successor to spark.mllib.
The spark.ml package aims to replace the old APIs with a cleaner, more
uniform set of APIs which will help users create full machine learning
pipelines.

(More info about pipelines will be included in the updated programming
guide for Spark 1.2.)

### Development plan

With Spark 1.2, spark.mllib is still the primary machine learning package,
and spark.ml is an alpha component for testing the new API.  The primary
parts of this API are:
* the Pipeline concept for constructing complicated ML workflows consisting
of Estimators and Transformers,
* SchemaRDD as an ML dataset,
* and constructs for specifying parameters for algorithms and pipelines.

If all goes well, spark.ml will become the primary ML package at the time
of the Spark 1.3 release.  Initially, simple wrappers will be used to port
algorithms to spark.ml, but eventually, code will be moved to spark.ml and
spark.mllib will be deprecated.

### Advice to developers

During the next development cycle, new algorithms should be contributed to
spark.mllib.  Optionally, wrappers for new (and old) algorithms can be
contributed to spark.ml.

Users will be able to use algorithms from either of the two packages; the
only difficulty will be the differences in APIs between the two packages.



"
Patrick Wendell <pwendell@gmail.com>,"Sat, 29 Nov 2014 00:16:56 -0500",[VOTE] Release Apache Spark 1.2.0 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version 1.2.0!

The tag to be voted on is v1.2.0-rc1 (commit 1056e9ec1):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=1056e9ec13203d0c51564265e94d77a054498fdb

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-1.2.0-rc1/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1048/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-1.2.0-rc1-docs/

Please vote on releasing this package as Apache Spark 1.2.0!

The vote is open until Tuesday, December 02, at 05:15 UTC and passes
if a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.1.0
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

== What justifies a -1 vote for this release? ==
This vote is happening very late into the QA period compared with
previous votes, so -1 votes should only occur for significant
regressions from 1.0.2. Bugs already present in 1.1.X, minor
regressions, or bugs related to new features will not block this
release.

== What default changes should I be aware of? ==
1. The default value of ""spark.shuffle.blockTransferService"" has been
changed to ""netty""
--> Old behavior can be restored by switching to ""nio""

2. The default value of ""spark.shuffle.manager"" has been changed to ""sort"".
--> Old behavior can be restored by setting ""spark.shuffle.manager"" to ""hash"".

== Other notes ==
Because this vote is occurring over a weekend, I will likely extend
the vote if this RC survives until the end of the vote period.

- Patrick

---------------------------------------------------------------------


"
Krishna Sankar <ksankar42@gmail.com>,"Fri, 28 Nov 2014 21:55:37 -0800",Re: [VOTE] Release Apache Spark 1.2.0 (RC1),Patrick Wendell <pwendell@gmail.com>,"Looks like the documentation hasn't caught up with the new features.
RandomForest, gbtree and so forth. Is a refresh of the documentation
planned ?
Am happy to see these capabilities, but these would need good explanations
as well, especially the new thinking around the ml ... pipelines,
transformations et al.
IMHO, the documentation is a -1.
Will check out the compilation, mlib et al

Cheers
<k/>


"
Reynold Xin <rxin@databricks.com>,"Fri, 28 Nov 2014 21:58:27 -0800",Re: [VOTE] Release Apache Spark 1.2.0 (RC1),Krishna Sankar <ksankar42@gmail.com>,"Krishna,

Docs don't block the rc voting because docs can be updated in parallel with
release candidates, until the point a release is made.



"
Matei Zaharia <matei.zaharia@gmail.com>,"Fri, 28 Nov 2014 22:26:37 -0800",Re: [VOTE] Release Apache Spark 1.2.0 (RC1),Patrick Wendell <pwendell@gmail.com>,"Hey Patrick, unfortunately you got some of the text here wrong, saying 1.1.0 instead of 1.2.0. Not sure it will matter since there can well be another RC after testing, but we should be careful.

Matei

version 1.2.0!
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=1056e9ec13203d0c51564265e94d77a054498fdb
at:
https://repository.apache.org/content/repositories/orgapachespark-1048/
""sort"".
""hash"".


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Sat, 29 Nov 2014 14:08:17 +0000",Re: [VOTE] Release Apache Spark 1.2.0 (RC1),Patrick Wendell <pwendell@gmail.com>,"+1 non-binding

I downloaded the plain source artifact and verified signatures.
Licesning and artifact looks OK. All tests pass with a plain
package/test run, and also with -Phive -Pyarn -Dhadop.version=2.5.2,
which covers two fairly different scenarios.
"
Patrick Wendell <pwendell@gmail.com>,"Sat, 29 Nov 2014 11:05:31 -0500",Re: [VOTE] Release Apache Spark 1.2.0 (RC1),Matei Zaharia <matei.zaharia@gmail.com>,"Thanks for pointing this out, Matei. I don't think a minor typo like
this is a big deal. Hopefully it's clear to everyone this is the 1.2.0
release vote, as indicated by the subject and all of the artifacts.


---------------------------------------------------------------------


"
slcclimber <anant.asty@gmail.com>,"Sat, 29 Nov 2014 09:06:56 -0700 (MST)",Re: [VOTE] Release Apache Spark 1.2.0 (RC1),dev@spark.incubator.apache.org,"+1
1> Compiled binaries
2> All Tests Pass
3> Ran python and scala examples for spark and Mllib on local and master + 4
workers




--

---------------------------------------------------------------------


"
Krishna Sankar <ksankar42@gmail.com>,"Sat, 29 Nov 2014 14:49:15 -0800",Re: [VOTE] Release Apache Spark 1.2.0 (RC1),Patrick Wendell <pwendell@gmail.com>,"+1
1. Compiled OSX 10.10 (Yosemite) mvn -Pyarn -Phadoop-2.4
-Dhadoop.version=2.4.0 -DskipTests clean package 16:46 min (slightly slower
connection)
2. Tested pyspark, mlib - running as well as compare esults with 1.1.x
2.1. statistics OK
2.2. Linear/Ridge"
"""Ganelin, Ilya"" <Ilya.Ganelin@capitalone.com>","Sat, 29 Nov 2014 22:29:42 -0500",Trouble testing after updating to latest master,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all – I’ve just merged in the latest changes from the Spark master branch to my local branch. I am able to build just fine with
mvm clean package
However, when I attempt to run dev/run-tests, I get the following error:

Using /Library/Java/JavaVirtualMachines/jdk1.8.0_20.jdk/Contents/Home as default JAVA_HOME.
Note, this will be overridden by -java-home if it is set.
Error: Invalid or corrupt jarfile sbt/sbt-launch-0.13.6.jar
[error] Got a return code of 1 on line 163 of the run-tests script.

With an individual test I get the same error. I have tried downloading a new copy of SBT 0.13.6 but it has not helped. Does anyone have any suggestions for getting this running? Things worked fine before updating Spark.
________________________________________________________

The information contained in this e-mail is confidential and/or proprietary is intended only for use by the individual or entity to which it is addressed.  If the reader of this message is not the intended recipient, you are hereby notified that any review, retransmission, dissemination, distribution, copying or other use of, or taking of any action in reliance upon this information is strictly prohibited. If you have received this communication in error, please contact the sender and delete the material from your computer.
"
vaquar khan <vaquar.khan@gmail.com>,"Sun, 30 Nov 2014 09:13:37 +0530",Re: [VOTE] Release Apache Spark 1.2.0 (RC1),Krishna Sankar <ksankar42@gmail.com>,"+1
1> Compiled binaries
2> All Tests Pass

Regards,
Vaquar khan

"
Patrick Wendell <pwendell@gmail.com>,"Sat, 29 Nov 2014 22:52:29 -0500",Re: Trouble testing after updating to latest master,"""Ganelin, Ilya"" <Ilya.Ganelin@capitalone.com>","to make sure you have a totally clean working space (""git clean -fdx""
will blow away any differences you have from the repo, of course only
do that if you don't have other files around). Can you reproduce this
if you just run ""sbt/sbt compile""? Also, if you can, can you reproduce
it if you checkout only the spark master branch and not merged with
your own code? Finally, if you can reproduce it on master, can you
perform a bisection to find out which commit caused it?

- Patrick

nch to my local branch. I am able to build just fine with
default JAVA_HOME.
new copy of SBT 0.13.6 but it has not helped. Does anyone have any suggestions for getting this running? Things worked fine before updating Spark.
th is intended only for use by the individual or entity to which it is addressed.  If the reader of this message is not the intended recipient, you are hereby notified that any review, retransmission, dissemination, distribution, copying or other use of, or taking of any action in reliance upon this information is strictly prohibited. If you have received this communication in error, please contact the sender and delete the material from your computer.

---------------------------------------------------------------------


"
"""Ganelin, Ilya"" <Ilya.Ganelin@capitalone.com>","Sat, 29 Nov 2014 23:16:08 -0500",Re: Trouble testing after updating to latest master,Patrick Wendell <pwendell@gmail.com>,"I am able to successfully run sbt/sbt-compile and run the tests after
running git clean -fdx. I¹m guessing network issues wound up corrupting
some of the files that had been downloaded. Thanks, Patrick!




________________________________________________________

The information contained in this e-mail is confidential and/or proprietary is intended only for use by the individual or entity to which it is addressed.  If the reader of this message is not the intended recipient, you are hereby notified that any review, retransmission, dissemination, distribution, copying or other use of, or taking of any action in reliance upon this information is strictly prohibited. If you have received this communication in error, please contact the sender and delete the material from your computer.


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Sat, 29 Nov 2014 23:20:57 -0500",Re: Trouble testing after updating to latest master,"""Ganelin, Ilya"" <Ilya.Ganelin@capitalone.com>","Sounds good. Glad you got it working.

:
th is intended only for use by the individual or entity to which it is addressed.  If the reader of this message is not the intended recipient, you are hereby notified that any review, retransmission, dissemination, distribution, copying or other use of, or taking of any action in reliance upon this information is strictly prohibited. If you have received this communication in error, please contact the sender and delete the material from your computer.

---------------------------------------------------------------------


"
"""=?utf-8?B?R3VvUWlhbmcgTGk=?="" <witgo@qq.com>","Sun, 30 Nov 2014 17:10:58 +0800",Re: [VOTE] Release Apache Spark 1.2.0 (RC1),"""=?utf-8?B?UGF0cmljayBXZW5kZWxs?="" <pwendell@gmail.com>, ""=?utf-8?B?ZGV2?="" <dev@spark.apache.org>","+1 (non-bindingâ€)




------------------ Original ------------------
From:  ""Patrick Wendell"";<pwendell@gmail.com>;
Date:  Sat, Nov 29, 2014 01:16 PM
To:  ""dev@spark.apache.org""<dev@spark.apache.org>; 

Subject:  [VOTE] Release Apache Spark 1."
Ryan Williams <ryan.blake.williams@gmail.com>,"Sun, 30 Nov 2014 22:39:28 +0000","Spurious test failures, testing best practices",dev@spark.apache.org,"In the course of trying to make contributions to Spark, I have had a lot of
trouble running Spark's tests successfully. The main pain points I've
experienced are:

    1) frequent, spurious test failures
    2) high latency of running tests
    3) difficulty running specific tests in an iterative fashion

Here is an example series of failures that I encountered this weekend
(along with footnote links to the console output from each and
approximately how long each took):

- `./dev/run-tests` [1]: failure in BroadcastSuite that I've not seen
before.
- `mvn '-Dsuites=*BroadcastSuite*' test` [2]: same failure.
- `mvn '-Dsuites=*BroadcastSuite* Unpersisting' test` [3]: BroadcastSuite
passed, but scala compiler crashed on the ""catalyst"" project.
- `mvn clean`: some attempts to run earlier commands (that previously
didn't crash the compiler) all result in the same compiler crash. Previous
discussion on this list implies this can only be solved by a `mvn clean`
[4].
- `mvn '-Dsuites=*BroadcastSuite*' test` [5]: immediately post-clean,
BroadcastSuite can't run because assembly is not built.
- `./dev/run-tests` again [6]: pyspark tests fail, some messages about
version mismatches and python 2.6. The machine this ran on has python 2.7,
so I don't know what that's about.
- `./dev/run-tests` again [7]: ""too many open files"" errors in several
tests. `ulimit -a` shows a maximum of 4864 open files. Apparently this is
not enough, but only some of the time? I increased it to 8192 and tried
again.
- `./dev/run-tests` again [8]: same pyspark errors as before. This seems to
be the issue from SPARK-3867 [9], which was supposedly fixed on October 14;
not sure how I'm seeing it now. In any case, switched to Python 2.6 and
installed unittest2, and python/run-tests seems to be unblocked.
- `./dev/run-tests` again [10]: finally passes!

This was on a spark checkout at ceb6281 (ToT Friday), with a few trivial
changes added on (that I wanted to test before sending out a PR), on a
macbook running OSX Yosemite (10.10.1), java 1.8 and mvn 3.2.3 [11].

Meanwhile, on a linux 2.6.32 / CentOS 6.4 machine, I tried similar commands
from the same repo state:

- `./dev/run-tests` [12]: YarnClusterSuite failure.
- `./dev/run-tests` [13]: same YarnClusterSuite failure. I know I've seen
this one before on this machine and am guessing it actually occurs every
time.
- `./dev/run-tests` [14]: to be sure, I reverted my changes, ran one more
time from ceb6281, and saw the same failure.

This was with java 1.7 and maven 3.2.3 [15]. In one final attempt to narrow
down the linux YarnClusterSuite failure, I ran `./dev/run-tests` on my mac,
from ceb6281, with java 1.7 (instead of 1.8, which the previous runs used),
and it passed [16], so the failure seems specific to my linux machine/arch.

At this point I believe that my changes don't break any tests (the
YarnClusterSuite failure on my linux presumably not being... ""real""), and I
am ready to send out a PR. Whew!

However, reflecting on the 5 or 6 distinct failure-modes represented above:

hopefully) fix once and for all. It cost me an ~hour this time (approximate
time of running ./dev/run-tests) and a few hours other times when I didn't
fully understand/fix it. It doesn't happen deterministically (why?), but
does happen somewhat frequently to people, having been discussed on the
user list multiple times [17] and on SO [18]. Maybe some note in the
documentation advising people to check their ulimit makes sense?
fixed upstream of the commits I tested here; I don't know why I'm still
running into it. This cost me a few hours of running `./dev/run-tests`
multiple times to see if it was transient, plus some time researching and
working around it.
- The original BroadcastSuite failure cost me a few hours and went away
before I'd even run `mvn clean`.
- A new incarnation of the sbt-compiler-crash phenomenon cost me a few
hours of running `./dev/run-tests` in different ways before deciding that,
as usual, there was no way around it and that I'd need to run `mvn clean`
and start running tests from scratch.
- The YarnClusterSuite failures on my linux box have cost me hours of
trying to figure out whether they're my fault. I've seen them many times
over the past weeks/months, plus or minus other failures that have come and
gone, and was especially befuddled by them when I was seeing a disjoint set
of reproducible failures on my mac [19] (the triaging of which involved
dozens of runs of `./dev/run-tests`).

While I'm interested in digging into each of these issues, I also want to
discuss the frequency with which I've run into issues like these. This is
unfortunately not the first time in recent months that I've spent days
playing spurious-test-failure whack-a-mole with a 60-90min dev/run-tests
iteration time, which is no fun! So I am wondering/thinking:

- Do other people experience this level of flakiness from spark tests?
- Do other people bother running dev/run-tests locally, or just let Jenkins
do it during the CR process?
- Needing to run a full assembly post-clean just to continue running one
specific test case feels especially wasteful, and the failure output when
naively attempting to run a specific test without having built an assembly
jar is not always clear about what the issue is or how to fix it; even the
fact that certain tests require ""building the world"" is not something I
would have expected, and has cost me hours of confusion.
    - Should a person running spark tests assume that they must build an
assembly JAR before running anything?
    - Are there some proper ""unit"" tests that are actually self-contained /
able to be run without building an assembly jar?
    - Can we better document/demarcate which tests have which dependencies?
    - Is there something finer-grained than building an assembly JAR that
is sufficient in some cases?
        - If so, can we document that?
        - If not, can we move to a world of finer-grained dependencies for
some of these?
- Leaving all of these spurious failures aside, the process of assembling
and testing a new JAR is not a quick one (40 and 60 mins for me typically,
respectively). I would guess that there are dozens (hundreds?) of people
who build a Spark assembly from various ToTs on any given day, and who all
wait on the exact same compilation / assembly steps to occur. Expanding on
the recent work to publish nightly snapshots [20], can we do a better job
caching/sharing compilation artifacts at a more granular level (pre-built
assembly JARs at each SHA? pre-built JARs per-maven-module, per-SHA? more
granular maven modules, plus the previous two?), or otherwise save some of
the considerable amount of redundant compilation work that I had to do over
the course of my odyssey this weekend?

Ramping up on most projects involves some amount of supplementing the
documentation with trial and error to figure out what to run, which
""errors"" are real errors and which can be ignored, etc., but navigating
that minefield on Spark has proved especially challenging and
time-consuming for me. Some of that comes directly from scala's relatively
slow compilation times and immature build-tooling ecosystem, but that is
the world we live in and it would be nice if Spark took the alleviation of
the resulting pain more seriously, as one of the more interesting and
well-known large scala projects around right now. The official
documentation around how to build different subsets of the codebase is
somewhat sparse [21], and there have been many mixed [22] accounts [23] on
this mailing list about preferred ways to build on mvn vs. sbt (none of
which has made it into official documentation, as far as I've seen).
Expecting new contributors to piece together all of this received
folk-wisdom about how to build/test in a sane way by trawling mailing list
archives seems suboptimal.

Thanks for reading, looking forward to hearing your ideas!

-Ryan

P.S. Is ""best practice"" for emailing this list to not incorporate any HTML
in the body? It seems like all of the archives I've seen strip it out, but
other people have used it and gmail displays it.


[1]
https://gist.githubusercontent.com/ryan-williams/8a162367c4dc157d2479/raw/484c2fb8bc0efa0e39d142087eefa9c3d5292ea3/dev%20run-tests:%20fail
(57 mins)
[2]
https://gist.githubusercontent.com/ryan-williams/8a162367c4dc157d2479/raw/ce264e469be3641f061eabd10beb1d71ac243991/mvn%20test:%20fail
(6 mins)
[3]
https://gist.githubusercontent.com/ryan-williams/8a162367c4dc157d2479/raw/6bc76c67aeef9c57ddd9fb2ba260fb4189dbb927/mvn%20test%20case:%20pass%20test,%20fail%20subsequent%20compile
(4 mins)
[4]
https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&ved=0CCUQFjAB&url=http%3A%2F%2Fapache-spark-user-list.1001560.n3.nabble.com%2Fscalac-crash-when-compiling-DataTypeConversions-scala-td17083.html&ei=aRF6VJrpNKr-iAKDgYGYBQ&usg=AFQjCNHjM9m__Hrumh-ecOsSE00-JkjKBQ&sig2=zDeSqOgs02AXJXj78w5I9g&bvm=bv.80642063,d.cGE&cad=rja
[5]
https://gist.githubusercontent.com/ryan-williams/8a162367c4dc157d2479/raw/4ab0bd6e76d9fc5745eb4b45cdf13195d10efaa2/mvn%20test,%20post%20clean,%20need%20dependencies%20built
[6]
https://gist.githubusercontent.com/ryan-williams/8a162367c4dc157d2479/raw/f4c7e6fc8c301f869b00598c7b541dac243fb51e/dev%20run-tests,%20post%20clean
(50 mins)
[7]
https://gist.github.com/ryan-williams/57f8bfc9328447fc5b97#file-dev-run-tests-failure-too-many-files-open-then-hang-L5260
(1hr)
[8] https://gist.github.com/ryan-williams/d0164194ad5de03f6e3f (1hr)
[9] https://issues.apache.org/jira/browse/SPARK-3867
[10] https://gist.github.com/ryan-williams/735adf543124c99647cc
[11] https://gist.github.com/ryan-williams/8d149bbcd0c6689ad564
[12]
https://gist.github.com/ryan-williams/07df5c583c9481fe1c14#file-gistfile1-txt-L853
(~90 mins)
[13]
https://gist.github.com/ryan-williams/718f6324af358819b496#file-gistfile1-txt-L852
(91 mins)
[14]
https://gist.github.com/ryan-williams/c06c1f4aa0b16f160965#file-gistfile1-txt-L854
[15] https://gist.github.com/ryan-williams/f8d410b5b9f082039c73
[16] https://gist.github.com/ryan-williams/2e94f55c9287938cf745
[17]
http://apache-spark-user-list.1001560.n3.nabble.com/quot-Too-many-open-files-quot-exception-on-reduceByKey-td2462.html
[18]
http://stackoverflow.com/questions/25707629/why-does-spark-job-fail-with-too-many-open-files
[19] https://issues.apache.org/jira/browse/SPARK-4002
[20] https://issues.apache.org/jira/browse/SPARK-4542
[21]
https://spark.apache.org/docs/latest/building-with-maven.html#spark-tests-in-maven
[22] https://www.mail-archive.com/dev@spark.apache.org/msg06443.html
[23]
http://mail-archives.apache.org/mod_mbox/spark-dev/201410.mbox/%3CCAOhmDzeUNhuCr41B7KRPTEwMn4cga_2TNpZrWqQB8REekokxzg@mail.gmail.com%3E
"
"""York, Brennon"" <Brennon.York@capitalone.com>","Sun, 30 Nov 2014 18:03:05 -0500","Re: Spurious test failures, testing best practices","Ryan Williams <ryan.blake.williams@gmail.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","+1, you aren¹t alone in this. I certainly would like some clarity in these
things well, but, as its been said on this listserv a few times (and you
noted), most developers use `sbt` for their day-to-day compilations to
greatly speed up the iterative testi"
Matei Zaharia <matei.zaharia@gmail.com>,"Sun, 30 Nov 2014 15:06:37 -0800","Re: Spurious test failures, testing best practices",Ryan Williams <ryan.blake.williams@gmail.com>,"Hi Ryan,

As a tip (and maybe this isn't documented well), I normally use SBT for development to avoid the slow build process, and use its interactive console to run only specific tests. The nice advantage is that SBT can keep the Scala compiler loaded and JITed across builds, making it faster to iterate. To use it, you can do the following:

- Start the SBT interactive console with sbt/sbt
- Build your assembly by running the ""assembly"" target in the assembly project: assembly/assembly
- Run all the tests in one module: core/test
- Run a specific suite: core/test-only org.apache.spark.rdd.RDDSuite (this also supports tab completion)

Running all the tests does take a while, and I usually just rely on Jenkins for that once I've run the tests for the things I believed my patch could break. But this is because some of them are integration tests (e.g. DistributedSuite, which creates multi-process mini-clusters). Many of the individual suites run fast without requiring this, however, so you can pick the ones you want. Perhaps we should find a way to tag them so people  can do a ""quick-test"" that skips the integration ones.

The assembly builds are annoying but they only take about a minute for me on a MacBook Pro with SBT warmed up. The assembly is actually only required for some of the ""integration"" tests (which launch new processes), but I'd recommend doing it all the time anyway since it would be very confusing to run those with an old assembly. The Scala compiler crash issue can also be a problem, but I don't see it very often with SBT. If it happens, I exit SBT and do sbt clean.

Anyway, this is useful feedback and I think we should try to improve some of these suites, but hopefully you can also try the faster SBT process. At the end of the day, if we want integration tests, the whole test process will take an hour, but most of the developers I know leave that to Jenkins and only run individual tests locally before submitting a patch.

Matei


lot of
BroadcastSuite
Previous
clean`
post-clean,
2.7,
is
tried
seems to
October 14;
and
trivial
commands
seen
every
more
narrow
mac,
used),
machine/arch.
and I
above:
(approximate
didn't
but
the
supposedly
still
and
away
that,
clean`
times
come and
disjoint set
involved
to
is
dev/run-tests
Jenkins
one
when
assembly
the
I
an
self-contained /
dependencies?
that
for
assembling
typically,
people
all
Expanding on
job
(pre-built
more
some of
over
navigating
relatively
is
alleviation of
[23] on
of
list
HTML
but
https://gist.githubusercontent.com/ryan-williams/8a162367c4dc157d2479/raw/484c2fb8bc0efa0e39d142087eefa9c3d5292ea3/dev%20run-tests:%20fail
https://gist.githubusercontent.com/ryan-williams/8a162367c4dc157d2479/raw/ce264e469be3641f061eabd10beb1d71ac243991/mvn%20test:%20fail
https://gist.githubusercontent.com/ryan-williams/8a162367c4dc157d2479/raw/6bc76c67aeef9c57ddd9fb2ba260fb4189dbb927/mvn%20test%20case:%20pass%20test,%20fail%20subsequent%20compile
https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&ved=0CCUQFjAB&url=http%3A%2F%2Fapache-spark-user-list.1001560.n3.nabble.com%2Fscalac-crash-when-compiling-DataTypeConversions-scala-td17083.html&ei=aRF6VJrpNKr-iAKDgYGYBQ&usg=AFQjCNHjM9m__Hrumh-ecOsSE00-JkjKBQ&sig2=zDeSqOgs02AXJXj78w5I9g&bvm=bv.80642063,d.cGE&cad=rja
https://gist.githubusercontent.com/ryan-williams/8a162367c4dc157d2479/raw/4ab0bd6e76d9fc5745eb4b45cdf13195d10efaa2/mvn%20test,%20post%20clean,%20need%20dependencies%20built
https://gist.githubusercontent.com/ryan-williams/8a162367c4dc157d2479/raw/f4c7e6fc8c301f869b00598c7b541dac243fb51e/dev%20run-tests,%20post%20clean
https://gist.github.com/ryan-williams/57f8bfc9328447fc5b97#file-dev-run-tests-failure-too-many-files-open-then-hang-L5260
https://gist.github.com/ryan-williams/07df5c583c9481fe1c14#file-gistfile1-txt-L853
https://gist.github.com/ryan-williams/718f6324af358819b496#file-gistfile1-txt-L852
https://gist.github.com/ryan-williams/c06c1f4aa0b16f160965#file-gistfile1-txt-L854
http://apache-spark-user-list.1001560.n3.nabble.com/quot-Too-many-open-files-quot-exception-on-reduceByKey-td2462.html
http://stackoverflow.com/questions/25707629/why-does-spark-job-fail-with-too-many-open-files
https://spark.apache.org/docs/latest/building-with-maven.html#spark-tests-in-maven
http://mail-archives.apache.org/mod_mbox/spark-dev/201410.mbox/%3CCAOhmDzeUNhuCr41B7KRPTEwMn4cga_2TNpZrWqQB8REekokxzg@mail.gmail.com%3E


---------------------------------------------------------------------


"
Ryan Williams <ryan.blake.williams@gmail.com>,"Sun, 30 Nov 2014 23:35:06 +0000","Re: Spurious test failures, testing best practices",Matei Zaharia <matei.zaharia@gmail.com>,"thanks for the info, Matei and Brennon. I will try to switch my workflow to
using sbt. Other potential action items:

- currently the docs only contain information about building with maven,
and even then don't cover many important cases, as I described in my
previous email. If SBT is as much better as you've described then that
should be made much more obvious. Wasn't it the case recently that there
was only a page about building with SBT, and not one about building with
maven? Clearer messaging around this needs to exist in the documentation,
not just on the mailing list, imho.

- +1 to better distinguishing between unit and integration tests, having
separate scripts for each, improving documentation around common workflows,
expectations of brittleness with each kind of test, advisability of just
relying on Jenkins for certain kinds of tests to not waste too much time,
etc. Things like the compiler crash should be discussed in the
documentation, not just in the mailing list archives, if new contributors
are likely to run into them through no fault of their own.

- What is the algorithm you use to decide what tests you might have broken?
Can we codify it in some scripts that other people can use?




"
Patrick Wendell <pwendell@gmail.com>,"Sun, 30 Nov 2014 15:49:45 -0800","Re: Spurious test failures, testing best practices",Ryan Williams <ryan.blake.williams@gmail.com>,"Hey Ryan,

A few more things here. You should feel free to send patches to
Jenkins to test them, since this is the reference environment in which
we regularly run tests. This is the normal workflow for most
developers and we spend a lot of effort provisioning/maintaining a
very large jenkins cluster to allow developers access this resource. A
common development approach is to locally run tests that you've added
in a patch, then send it to jenkins for the full run, and then try to
debug locally if you see specific unanticipated test failures.

Java versions, Python versions, ulimits, etc. there is a combinatorial
number of environments in which tests could be run. It is very hard in
some cases to figure out post-hoc why a given test is not working in a
specific environment. I think a good solution here would be to use a
standardized docker container for running Spark tests and asking folks
to use that locally if they are trying to run all of the hundreds of
Spark tests.

Another solution would be to mock out every system interaction in
Spark's tests including e.g. filesystem interactions to try and reduce
variance across environments. However, that seems difficult.

As the number of developers of Spark increases, it's definitely a good
idea for us to invest in developer infrastructure including things
like snapshot releases, better documentation, etc. Thanks for bringing
this up as a pain point.

- Patrick



---------------------------------------------------------------------


"
