Chester Chen <chester@alpinenow.com>,"Mon, 31 Aug 2015 18:40:35 -0700",Re: [VOTE] Release Apache Spark 1.5.0 (RC2),Reynold Xin <rxin@databricks.com>,"Seems that Github branch-1.5 already changing the version to
1.5.1-SNAPSHOT,

I am a bit confused are we still on 1.5.0 RC3 or we are in 1.5.1 ?

Chester


n
t)
ER
A'"")
ork. But
 and
a5fc27a
:
e
/
/
/
================
================
n
=========================
=========================
eady
l not
========================================
========================================
o
===========================
===========================
e
gacy
UI
and
"
Sean Owen <sowen@cloudera.com>,"Tue, 1 Sep 2015 09:52:05 +0100",Re: [VOTE] Release Apache Spark 1.5.0 (RC2),Chester Chen <chester@alpinenow.com>,"That's correct for the 1.5 branch, right? this doesn't mean that the
next RC would have this value. You choose the release version during
the release process.

OT,
a
en
NER
WA'"")
work. But
C and
aa5fc27a
t:
/
be
1/
0/
s/
================
================
an
=========================
=========================
ready
ll not
========================================
========================================
to
===========================
===========================
he
egacy
 and

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Mon, 31 Aug 2015 23:11:12 -1000",Re: Tungsten off heap memory access for C++ libraries,Paul Weiss <paulweiss.dev@gmail.com>,"Please do. Thanks.


"
chester@alpinenow.com,"Tue, 1 Sep 2015 06:48:26 -0700",Re: [VOTE] Release Apache Spark 1.5.0 (RC2),Sean Owen <sowen@cloudera.com>,"Sorry, I am still not follow. I assume the release would build from 1.5.0 before moving to 1.5.1. Are you saying the 1.5.0 rc3 could build from 1.5.1 snapshot during release ? Or 1.5.0 rc3 would build from the last commit of 1.5.0 (before changing to 1.5.1 snapshot) ?



Sent from my iPad

e:
OT,
:

en
NER
WA'"")
k. But
C and

aa5fc27a
t:
/
e
1/
0/
s/
================
================
n
=========================
=========================
ready
ll not
========================================
========================================
to

===========================
===========================
he

egacy

 and

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Tue, 1 Sep 2015 14:57:13 +0100",Re: [VOTE] Release Apache Spark 1.5.0 (RC2),Chester Chen <chester@alpinenow.com>,"The head of branch 1.5 will always be a ""1.5.x-SNAPSHOT"" version. Yeah
technically you would expect it to be 1.5.0-SNAPSHOT until 1.5.0 is
released. In practice I think it's simpler to follow the defaults of
the Maven release plugin, which will set this to 1.5.1-SNAPSHOT after
any 1.5.0-rc is released. It doesn't affect later RCs. This has
nothing to do with what commits go into 1.5.0; it's an ignorable
detail of the version in POMs in the source tree, which don't mean
much anyway as the source tree itself is not a released version.

 before moving to 1.5.1. Are you saying the 1.5.0 rc3 could build from 1.5.1 snapshot during release ? Or 1.5.0 rc3 would build from the last commit of 1.5.0 (before changing to 1.5.1 snapshot) ?
ote:
SHOT,
te:
s
h a
been
INNER
 'WA'"")
t work. But
UTC and
g/
5aaa5fc27a
 at:
in/
n be
141/
140/
ocs/
=================
=================
g an
==========================
==========================
already
will not
=========================================
=========================================
into
he
============================
============================
 the
s)
 legacy
ls
r, and
c

---------------------------------------------------------------------


"
Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"Tue, 1 Sep 2015 07:09:01 -0700 (MST)",[SparkR] lint script for SpakrR,dev@spark.apache.org,"Hi all,

Shivaram and I added a lint script for SparkR which is `dev/lint-r`. And
it's been already running on Jenkins. If there are any validation problems
in your patch, Jenkins will fail. 
Could you please make sure that your patch don't have any validation
problems on your local machine before sending a PR.
https://github.com/apache/spark/blob/master/dev/lint-r

And we could also discuss the validation rules. I think there is still room
for improvement. 
If you have any idea, please join the discussion about SparkR style guide.
https://issues.apache.org/jira/browse/SPARK-6813

Thanks Shivaram and Josh, I couldn't have done it without you.

Thanks
Yu



-----
-- Yu Ishikawa
--

---------------------------------------------------------------------


"
chester@alpinenow.com,"Tue, 1 Sep 2015 07:13:47 -0700",Re: [VOTE] Release Apache Spark 1.5.0 (RC2),Sean Owen <sowen@cloudera.com>,"Thanks for the explanation. Since 1.5.0 rc3 is not yet released, I assume it would cut from 1.5 branch, doesn't that bring 1.5.1 snapshot code ? 

The reason I am asking these questions is that I would like to know If I want build 1.5.0  myself, which commit should I use ? 

Sent from my iPad

 before moving to 1.5.1. Are you saying the 1.5.0 rc3 could build from 1.5.1 snapshot during release ? Or 1.5.0 rc3 would build from the last commit of 1.5.0 (before changing to 1.5.1 snapshot) ?
ote:
SHOT,
te:
s
h a
een

NNER
 'WA'"")
ork. But
TC and
g/
5aaa5fc27a
 at:
in/
n be
141/

140/
ocs/
================
================
g an

=========================
=========================
lready
ill not
========================================
========================================
nto
he
===========================
===========================
 the


s)
 legacy

ls
r, and
c


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Tue, 1 Sep 2015 15:17:00 +0100",Re: [VOTE] Release Apache Spark 1.5.0 (RC2),Chester Chen <chester@alpinenow.com>,"Any 1.5 RC comes from the latest state of the 1.5 branch at some point
in time. The next RC will be cut from whatever the latest commit is.
You can see the tags in git for the specific commits for each RC.
There's no such thing as ""1.5.1 SNAPSHOT"" commits, just commits to
branch 1.5. I would ignore the ""SNAPSHOT"" version for your purpose.

You can always build from the exact commit that an RC did by looking
at tags. There is no 1.5.0 yet so you can't build that, but once it's
released, you would be able to find its tag as well. You can always
build the latest 1.5.x branch by building from HEAD of that branch.

 it would cut from 1.5 branch, doesn't that bring 1.5.1 snapshot code ?
want build 1.5.0  myself, which commit should I use ?
.0 before moving to 1.5.1. Are you saying the 1.5.0 rc3 could build from 1.5.1 snapshot during release ? Or 1.5.0 rc3 would build from the last commit of 1.5.0 (before changing to 1.5.1 snapshot) ?
APSHOT,
rote:
 is
ith a
y been
m>
s INNER
= 'WA'"")
™t work. But
0 UTC and
org/
7f5aaa5fc27a
nd at:
-bin/
can be
-1141/
be
-1140/
-docs/
==================
==================
ing an
en
===========================
===========================
1
s already
s will not
==========================================
==========================================
o into
 the
e
=============================
=============================
or the
n
y
on
N,
c
ons)
r
rd legacy
fy
t
ails
r
ver, and
nic
ds

---------------------------------------------------------------------


"
Chester Chen <chester@alpinenow.com>,"Tue, 1 Sep 2015 07:22:35 -0700",Re: [VOTE] Release Apache Spark 1.5.0 (RC2),Sean Owen <sowen@cloudera.com>,"Thanks Sean, that make it clear.


de
I
om
e,
K
 =
€™t
m
k
c27a
)
n
:
==================
==================
===========================
===========================
==========================================
==========================================
=============================
=============================
+
th
e
s,
e
Ms
r
"
anshu shukla <anshushukla0@gmail.com>,"Tue, 1 Sep 2015 23:25:01 +0530",Resource allocation in SPARK streaming,"dev@spark.apache.org, user <user@spark.apache.org>, 
	Tathagata Das <tdas@databricks.com>","I am not much clear about  resource allocation (CPU/CORE/Thread  level
allocation)  as per the parallelism by  setting  number of cores in  spark
 standalone mode .

Any guidelines for that .

-- 
Thanks & Regards,
Anshu Shukla
"
Reynold Xin <rxin@databricks.com>,"Tue, 1 Sep 2015 10:41:46 -1000",[VOTE] Release Apache Spark 1.5.0 (RC3),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version
1.5.0. The vote is open until Friday, Sep 4, 2015 at 21:00 UTC and passes
if a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.5.0
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see http://spark.apache.org/


The tag to be voted on is v1.5.0-rc3:
https://github.com/apache/spark/commit/908e37bcc10132bb2aa7f80ae694a9df6e40f31a

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.5.0-rc3-bin/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release (published as 1.5.0-rc3) can be
found at:
https://repository.apache.org/content/repositories/orgapachespark-1143/

The staging repository for this release (published as 1.5.0) can be found
at:
https://repository.apache.org/content/repositories/orgapachespark-1142/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.5.0-rc3-docs/


=======================================
How can I help test this release?
=======================================
If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions.


================================================
What justifies a -1 vote for this release?
================================================
This vote is happening towards the end of the 1.5 QA period, so -1 votes
should only occur for significant regressions from 1.4. Bugs already
present in 1.4, minor regressions, or bugs related to new features will not
block this release.


===============================================================
What should happen to JIRA tickets still targeting 1.5.0?
===============================================================
1. It is OK for documentation patches to target 1.5.0 and still go into
branch-1.5, since documentations will be packaged separately from the
release.
2. New features for non-alpha-modules should target 1.6+.
3. Non-blocker bug fixes should target 1.5.1 or 1.6.0, or drop the target
version.


==================================================
Major changes to help you focus your testing
==================================================

As of today, Spark 1.5 contains more than 1000 commits from 220+
contributors. I've curated a list of important changes for 1.5. For the
complete list, please refer to Apache JIRA changelog.

RDD/DataFrame/SQL APIs

- New UDAF interface
- DataFrame hints for broadcast join
- expr function for turning a SQL expression into DataFrame column
- Improved support for NaN values
- StructType now supports ordering
- TimestampType precision is reduced to 1us
- 100 new built-in expressions, including date/time, string, math
- memory and local disk only checkpointing

DataFrame/SQL Backend Execution

- Code generation on by default
- Improved join, aggregation, shuffle, sorting with cache friendly
algorithms and external algorithms
- Improved window function performance
- Better metrics instrumentation and reporting for DF/SQL execution plans

Data Sources, Hive, Hadoop, Mesos and Cluster Management

- Dynamic allocation support in all resource managers (Mesos, YARN,
Standalone)
- Improved Mesos support (framework authentication, roles, dynamic
allocation, constraints)
- Improved YARN support (dynamic allocation with preferred locations)
- Improved Hive support (metastore partition pruning, metastore
connectivity to 0.13 to 1.2, internal Hive upgrade to 1.2)
- Support persisting data in Hive compatible format in metastore
- Support data partitioning for JSON data sources
- Parquet improvements (upgrade to 1.7, predicate pushdown, faster metadata
discovery and schema merging, support reading non-standard legacy Parquet
files generated by other libraries)
- Faster and more robust dynamic partition insert
- DataSourceRegister interface for external data sources to specify short
names

SparkR

- YARN cluster mode in R
- GLMs with R formula, binomial/Gaussian families, and elastic-net
regularization
- Improved error messages
- Aliases to make DataFrame functions more R-like

Streaming

- Backpressure for handling bursty input streams.
- Improved Python support for streaming sources (Kafka offsets, Kinesis,
MQTT, Flume)
- Improved Python streaming machine learning algorithms (K-Means, linear
regression, logistic regression)
- Native reliable Kinesis stream support
- Input metadata like Kafka offsets made visible in the batch details UI
- Better load balancing and scheduling of receivers across cluster
- Include streaming storage in web UI

Machine Learning and Advanced Analytics

- Feature transformers: CountVectorizer, Discrete Cosine transformation,
MinMaxScaler, NGram, PCA, RFormula, StopWordsRemover, and VectorSlicer.
- Estimators under pipeline APIs: naive Bayes, k-means, and isotonic
regression.
- Algorithms: multilayer perceptron classifier, PrefixSpan for sequential
pattern mining, association rule generation, 1-sample Kolmogorov-Smirnov
test.
- Improvements to existing algorithms: LDA, trees/ensembles, GMMs
- More efficient Pregel API implementation for GraphX
- Model summary for linear and logistic regression.
- Python API: distributed matrices, streaming k-means and linear models,
LDA, power iteration clustering, etc.
- Tuning and evaluation: train-validation split and multiclass
classification evaluator.
- Documentation: document the release version of public API methods
"
Paul Wais <paulwais@gmail.com>,"Tue, 1 Sep 2015 14:31:58 -0700 (MST)",Re: Tungsten off heap memory access for C++ libraries,dev@spark.apache.org,"Paul: I've worked on running C++ code on Spark at scale before (via JNA, ~200
cores) and am working on something more contribution-oriented now (via JNI). 
A few comments:
 * If you need something *today*, try JNA.  It can be slow (e.g. a short
native function in a tight loop) but works if you have an existing C
library.
 * If you want true zero-copy nested data structures (with explicit schema),
you probably want to look at Google Flatbuffers or Captain Proto.  Protobuf
does copies; not sure about Avro.  However, if instances of your nested
messages fit completely in CPU cache, there might not be much benefit to
zero-copy.
 * Tungsten numeric arrays and UTF-8 strings should be portable but likely
need some special handling.  (A major benefit of Protobuf, Avro,
Flatbuffers, Capnp, etc., is these libraries already handle endianness and
UTF8 for C++).  
 * NB: Don't try to dive into messing with (standard) Java String <->
std::string using JNI.  It's a very messy problem :)

Was there indeed a JIRA started to track this issue?  Can't find it at the
moment ...



--

---------------------------------------------------------------------


"
"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Tue, 1 Sep 2015 23:08:25 +0000",Use of UnsafeRow,"""dev@spark.apache.org"" <dev@spark.apache.org>","Dear Spark developers,

Could you suggest what is the intended use of UnsafeRow (except for Tungsten groupBy and sort) and give an example how to use it?
1)Is it intended to be instantiated as the copy of the Row in order to perform in-place modifications of it?
2)Can I create a new UnsafeRow given the types of columns I want it to have (so it will allocate a memory and point to it)?

Best regards, Alexander
"
Paul Weiss <paulweiss.dev@gmail.com>,"Tue, 1 Sep 2015 19:57:26 -0400",Re: Tungsten off heap memory access for C++ libraries,Paul Wais <paulwais@gmail.com>,"https://issues.apache.org/jira/browse/SPARK-10399

Is the jira to track.

"
"""Wangchangchun (A)"" <wangchangchun88@huawei.com>","Wed, 2 Sep 2015 04:23:41 +0000","[ compress in-memory column storage used in sparksql cache table
     ]","""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,  I have an idea, can someone give me some advice?



I want to compress data in in-memory column storage which is used by cache table in spark. This will make cache table use less memory.



I will set an conf to this function, so if anyone want to use this function, he can set this conf to true.



Compress algorithom I want to use Dictionary Encoding.



Do you think this method worth a try ?

"
Niranda Perera <niranda.perera@gmail.com>,"Wed, 2 Sep 2015 10:05:13 +0530",taking an n number of rows from and RDD starting from an index,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,

I have a large set of data which would not fit into the memory. So, I wan
to take n number of data from the RDD given a particular index. for an
example, take 1000 rows starting from the index 1001.

I see that there is a  take(num: Int): Array[T] method in the RDD, but it
only returns the 'first n number of rows'.

the simplest use case of this, requirement is, say, I write a custom
relation provider with a custom relation extending the InsertableRelation.

say I submit this query,
""insert into table abc select * from xyz sort by x asc""

in my custom relation, I have implemented the def insert(data: DataFrame,
overwrite: Boolean): Unit
method. here, since the data is large, I can not call methods such as
DataFrame.collect(). Instead, I could do, DataFrame.foreachpartition(...).
As you could see, the resultant DF from the ""select * from xyz sort by x
asc"" is sorted, and if I sun, foreachpartition on that DF and implement the
insert method, this sorted order would be affected, since the inserting
operation would be done in parallel in each partition.

in order to handle this, my initial idea was to take rows from the RDD in
batches and do the insert operation, and for that I was looking for a
method to take n number of rows starting from a given index.

is there any better way to handle this, in RDDs?

your assistance in this regard is highly appreciated.

cheers

-- 
Niranda
@n1r44 <https://twitter.com/N1R44>
https://pythagoreanscript.wordpress.com/
"
ankit tyagi <ankittyagi.mnnit@gmail.com>,"Wed, 2 Sep 2015 11:33:50 +0530",OOM in spark driver,dev@spark.apache.org,"Hi All,

I am using spark-sql 1.3.1 with hadoop 2.4.0 version.  I am running sql
query against parquet files and wanted to save result on s3 but looks like
https://issues.apache.org/jira/browse/SPARK-2984 problem still coming while
saving data to s3.

Hence Now i am saving result on hdfs and with the help
of JavaSparkListener, copying file from hdfs to s3 with hadoop fileUtil
in onApplicationEnd method. But  my job is getting failed with OOM in spark
driver.

*5/09/02 04:17:57 INFO cluster.YarnClusterSchedulerBackend: Asking each
executor to shut down*
*15/09/02 04:17:59 INFO
scheduler.OutputCommitCoordinator$OutputCommitCoordinatorActor:
OutputCommitCoordinator stopped!*
*Exception in thread ""Reporter"" *
*Exception: java.lang.OutOfMemoryError thrown from the
UncaughtExceptionHandler in thread ""Reporter""*
*Exception in thread ""SparkListenerBus"" *
*Exception: java.lang.OutOfMemoryError thrown from the
UncaughtExceptionHandler in thread ""SparkListenerBus""*
*Exception in thread ""Driver"" *
*Exception: java.lang.OutOfMemoryError thrown from the
UncaughtExceptionHandler in thread ""Driver""*


Strage part is, result is getting saved on HDFS but while copying file job
is getting failed. size of file is under 1MB.

Any help or leads would be appreciated.
"
Hemant Bhanawat <hemant9379@gmail.com>,"Wed, 2 Sep 2015 12:22:24 +0530",Re: taking an n number of rows from and RDD starting from an index,Niranda Perera <niranda.perera@gmail.com>,"I think rdd.toLocalIterator is what you want. But it will keep one
partition's data in-memory.


"
=?UTF-8?B?SnVhbiBSb2Ryw61ndWV6IEhvcnRhbMOh?= <juan.rodriguez.hortala@gmail.com>,"Wed, 2 Sep 2015 09:03:01 +0200",Re: taking an n number of rows from and RDD starting from an index,Hemant Bhanawat <hemant9379@gmail.com>,"Hi,

Maybe you could use zipWithIndex and filter to skip the first elements. For
example starting from

scala> sc.parallelize(100 to 120, 4).zipWithIndex.collect
res12: Array[(Int, Long)] = Array((100,0), (101,1), (102,2), (103,3),
(104,4), (105,5), (106,6), (107,7), (108,8), (109,9), (110,10), (111,11),
(112,12), (113,13), (114,14), (115,15), (116,16), (117,17), (118,18),
(119,19), (120,20))

we can get the 3 first elements starting from the 4th (counting from 0) as

scala> sc.parallelize(100 to 120, 4).zipWithIndex.filter(_._2 >=4).take(3)
res14: Array[(Int, Long)] = Array((104,4), (105,5), (106,6))

Hope that helps


2015-09-02 8:52 GMT+02:00 Hemant Bhanawat <hemant9379@gmail.com>:

"
Nitin Goyal <nitin2goyal@gmail.com>,"Wed, 2 Sep 2015 00:58:38 -0700 (MST)","Re: [ compress in-memory column storage used in sparksql cache
 table ]",dev@spark.apache.org,"I think spark sql's in-memory columnar cache already does compression. Check
out classes in following path :-

https://github.com/apache/spark/tree/master/sql/core/src/main/scala/org/apache/spark/sql/columnar/compression

Although compression ratio is not as good as Parquet.

Thanks
-Nitin



--

---------------------------------------------------------------------


"
Cheng Lian <lian.cs.zju@gmail.com>,"Wed, 2 Sep 2015 16:44:08 +0800","Re: [ compress in-memory column storage used in sparksql cache table
 ]","Nitin Goyal <nitin2goyal@gmail.com>, dev@spark.apache.org","Yeah, two of the reasons why the built-in in-memory columnar storage 
doesn't achieve comparable compression ratio as Parquet are:

1. The in-memory columnar representation doesn't handle nested types. So 
array/map/struct values are not compressed.
2. Parquet may use more than one kind of compression methods to compress 
a single column. For example, dictionary  + RLE.

Cheng



---------------------------------------------------------------------


"
Pavel Gladkov <gladkov.p@gmail.com>,"Wed, 2 Sep 2015 16:54:51 +0300",Harmonic centrality in GraphX,dev@spark.apache.org,"Hi,

What do you think about this algorithm
https://github.com/webgeist/spark-centrality/blob/master/src/main/scala/cc/p2k/spark/graphx/lib/HarmonicCentrality.scala

This is an implementation of the Harmonic Centrality algorithm
http://infoscience.epfl.ch/record/200525/files/%5BEN%5DASNA09.pdf.

Should it be in GraphX lib?

-- 
Pavel Gladkov
"
Mike Hynes <91mbbh@gmail.com>,"Wed, 2 Sep 2015 14:27:59 -0400",Re: OOM in spark driver,ankit tyagi <ankittyagi.mnnit@gmail.com>,"Just a thought; this has worked for me before on standalone client
with a similar OOM error in a driver thread. Try setting:
export SPARK_DAEMON_MEMORY=4G #or whatever size you can afford on your machine
in your environment/spark-env.sh before running spark-submit.
Mike



-- 
Thanks,
Mike

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Wed, 2 Sep 2015 23:34:08 +0100",Re: [VOTE] Release Apache Spark 1.5.0 (RC3),Reynold Xin <rxin@databricks.com>,"- As usual the license and signatures are OK
- No blockers, check
- 9 ""Critical"" bugs for 1.5.0 are listed below just for everyone's
reference (48 total issues still targeted for 1.5.0)
- Under Java 7 + Ubuntu 15, I only had one consistent test failure,
b"
lankaz <sasikanth.lanka@zomato.com>,"Wed, 2 Sep 2015 21:33:04 -0700 (MST)",[HELP] Spark 1.4.1 tasks take ridiculously long time to complete,dev@spark.apache.org,"Hi this is a image of the screent shot some take seconds to execute some take
hours.
<http://apache-spark-developers-list.1001551.n3.nabble.com/file/n13942/Screen_Shot_2015-09-02_at_8.png> 



--

---------------------------------------------------------------------


"
Niranda Perera <niranda.perera@gmail.com>,"Thu, 3 Sep 2015 10:16:23 +0530",Re: taking an n number of rows from and RDD starting from an index,=?UTF-8?B?SnVhbiBSb2Ryw61ndWV6IEhvcnRhbMOh?= <juan.rodriguez.hortala@gmail.com>,"Hi all,

thank you for your response.

after taking a look at the implementations of rdd.collect(), I thought of
using the rdd.runJob(...) method .

for (int i = 0; i < dataFrame.rdd().partitions().length; i++) {
                dataFrame.sqlContext().sparkContext().runJob(data.rdd(),
some function, { i } , false, ClassTag$.MODULE$.Unit());
            }

this iterates through the partitions of the dataframe.

I would like to know if this is an accepted way of iterating through
dataFrame partitions while conserving the order of rows encapsulated by the
dataframe?

cheers



s
(3)
m
 an
on.
.).
x
 the
a


-- 
Niranda
@n1r44 <https://twitter.com/N1R44>
https://pythagoreanscript.wordpress.com/
"
Niranda Perera <niranda.perera@gmail.com>,"Thu, 3 Sep 2015 10:49:59 +0530",Spark SQL sort by and collect by in multiple partitions,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,

I have been using sort by and order by in spark sql and I observed the
following

when using SORT BY and collect results, the results are getting sorted
partition by partition.
example:
if we have 1, 2, ... , 12 and 4 partitions and I want to sort it in
descending order,
partition 0 (p0) would have 12, 8, 4
p1 = 11, 7, 3
p2 = 10, 6, 2
p3 = 9, 5, 1

so collect() would return 12, 8, 4, 11, 7, 3, 10, 6, 2, 9, 5, 1

BUT when I use ORDER BY and collect results
p0 = 12, 11, 10
p1 =  9, 8, 7
.....
so collect() would return 12, 11, .., 1 which is the desirable result.

is this the intended behavior of SORT BY and ORDER BY or is there something
I'm missing?

cheers

-- 
Niranda
@n1r44 <https://twitter.com/N1R44>
https://pythagoreanscript.wordpress.com/
"
Vishnu Kumar <mr.viskumar@gmail.com>,"Thu, 3 Sep 2015 11:54:04 +0530",Re: Spark SQL sort by and collect by in multiple partitions,Niranda Perera <niranda.perera@gmail.com>,"Hi,

Yes this is intended behavior. ""ORDER BY"" guarantees the total order in
output while  ""SORT BY"" guarantees the order within a partition.


Vishnu


"
robineast <robin.east@xense.co.uk>,"Thu, 3 Sep 2015 03:12:54 -0700 (MST)","Re: [HELP] Spark 1.4.1 tasks take ridiculously long time to
 complete",dev@spark.apache.org,"I would suggest you move this to the Spark User list, this is the development
list for discussion on development of Spark. It would help if you could give
some more information about what you are trying to do e.g. what code you are
running, how you submitted the job (spark-shell, spark-submit) and what sort
of cluster (standalone, Yarn, Mesos)





--

---------------------------------------------------------------------


"
Tom Graves <tgraves_cs@yahoo.com.INVALID>,"Thu, 3 Sep 2015 14:49:27 +0000 (UTC)",Re: [VOTE] Release Apache Spark 1.5.0 (RC3),"Reynold Xin <rxin@databricks.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","+1. Tested on Yarn with Hadoop 2.6.Â 
A few of the things tested: pyspark, hive integration, aux shuffle handler, history server, basic submit cli behavior, distributed cache behavior, cluster and client mode...
Tom 


   

 Please vote on releasing the f"
mkhaitman <mark.khaitman@chango.com>,"Thu, 3 Sep 2015 08:56:00 -0700 (MST)",Re: [VOTE] Release Apache Spark 1.5.0 (RC3),dev@spark.apache.org,"Built and tested on CentOS 7, Hadoop 2.7.1 (Built for 2.6 profile),
Standalone without any problems. Re-tested dynamic allocation specifically. 

""Lost executor"" messages are still an annoyance since they're expected to
occur with dynamic allocation, and shouldn't WARN/ERROR as they do now,
however there's already a JIRA ticket for it:
https://issues.apache.org/jira/browse/SPARK-4134 . Will probably have to
filter these messages out in log4j properties for this release!

Mark.



--

---------------------------------------------------------------------


"
<andrew.rowson@thomsonreuters.com>,"Thu, 3 Sep 2015 16:32:47 +0000",EOFException on History server reading in progress lz4,<dev@spark.apache.org>,"I'm trying to solve a problem of the history server spamming my logs with
EOFExceptions when it tries to read a history file from HDFS that is both
lz4 compressed and incomplete. The actual exception is:

java.io.EOFException: Stream ended prematurely
	at
net.jpountz.lz4.LZ4BlockInputStream.readFully(LZ4BlockInputStream.java:218)
	at
net.jpountz.lz4.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:192)
	at
net.jpountz.lz4.LZ4BlockInputStream.read(LZ4BlockInputStream.java:117)
	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	at java.io.InputStreamReader.read(InputStreamReader.java:184)
	at java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.io.BufferedReader.readLine(BufferedReader.java:324)
	at java.io.BufferedReader.readLine(BufferedReader.java:389)
	at
scala.io.BufferedSource$BufferedLineIterator.hasNext(BufferedSource.scala:67
)
	at
org.apache.spark.scheduler.ReplayListenerBus.replay(ReplayListenerBus.scala:
55)
	at
org.apache.spark.deploy.history.FsHistoryProvider.org$apache$spark$deploy$hi
story$FsHistoryProvider$$replay(FsHistoryProvider.scala:443)
	at
org.apache.spark.deploy.history.FsHistoryProvider$$anonfun$10.apply(FsHistor
yProvider.scala:278)
	at
org.apache.spark.deploy.history.FsHistoryProvider$$anonfun$10.apply(FsHistor
yProvider.scala:275)
	at
scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.sc
ala:251)
	at
scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.sc
ala:251)
	at
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:5
9)
	at
scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at
scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251)
	at
scala.collection.AbstractTraversable.flatMap(Traversable.scala:105)
	at
org.apache.spark.deploy.history.FsHistoryProvider.org$apache$spark$deploy$hi
story$FsHistoryProvider$$mergeApplicationListing(FsHistoryProvider.scala:275
)
	at
org.apache.spark.deploy.history.FsHistoryProvider$$anonfun$checkForLogs$1$$a
non$2.run(FsHistoryProvider.scala:209)
	at
java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:11
42)
	at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:6
17)
	at java.lang.Thread.run(Thread.java:745)

The bit I'm struggling with is handling this in ReplayListenerBus.scala - I
tried adding the following to the try/catch:

case eof: java.io.EOFException =>
    logWarning(s""EOFException (probably due to incomplete lz4) at
$sourceName"", eof)

but this never seems to get triggered - it still dumps the whole exception
out to the log.

I feel like there's something basic I'm missing for the exception not to be
caught by the try/catch in ReplayListenerBus. Can anyone point me in the
right direction?

Thanks,

Andrew
"
Burak Yavuz <brkyvz@gmail.com>,"Thu, 3 Sep 2015 09:57:45 -0700",Re: [VOTE] Release Apache Spark 1.5.0 (RC3),mkhaitman <mark.khaitman@chango.com>,"+1. Tested complex R package support (Scala + R code), BLAS and DataFrame
fixes good.

Burak


"
Michael Armbrust <michael@databricks.com>,"Thu, 3 Sep 2015 10:05:19 -0700",Re: [VOTE] Release Apache Spark 1.5.0 (RC3),Burak Yavuz <brkyvz@gmail.com>,"+1 Ran TPC-DS and ported several jobs over to 1.5


"
Davies Liu <davies@databricks.com>,"Thu, 3 Sep 2015 10:26:11 -0700",Re: [VOTE] Release Apache Spark 1.5.0 (RC3),Michael Armbrust <michael@databricks.com>,"+1, built 1.5 from source and ran TPC-DS locally and clusters, ran
performance benchmark for aggregation and join with difference scales,
all worked well.


---------------------------------------------------------------------


"
saurfang <forest.fang@outlook.com>,"Thu, 3 Sep 2015 11:24:34 -0700 (MST)",Re: [VOTE] Release Apache Spark 1.5.0 (RC3),dev@spark.apache.org,"+1. Compiled on Windows with YARN and Hive. Tested Tungsten aggregation and
observed similar (good) performance comparing to 1.4 with unsafe on. Ran a
few workloads and tested SparkSQL thrift server



--

-------------------------------------------------"
Reynold Xin <rxin@databricks.com>,"Thu, 3 Sep 2015 10:37:17 -1000",Re: Code generation for GPU,"""dev@spark.apache.org"" <dev@spark.apache.org>","See responses inline.


the apply method is called in the operators.

E.g. GenerateUnsafeProjection returns a Projection class (which is just a
class with an apply method), and TungstenProject calls that class.



This is future work. You'd need to create batches of rows or columns. This
is a pretty major refactoring though.


changes to internals every release. If I were you, I'd use either master or
branch-1.5 for your prototyping.


"
Krishna Sankar <ksankar42@gmail.com>,"Thu, 3 Sep 2015 20:20:54 -0700",Re: [VOTE] Release Apache Spark 1.5.0 (RC3),Reynold Xin <rxin@databricks.com>,"+?

1. Compiled OSX 10.10 (Yosemite) OK Total time: 26:09 min
     mvn clean package -Pyarn -Phadoop-2.6 -DskipTests
2. Tested pyspark, mllib
2.1. statistics (min,max,mean,Pearson,Spearman) OK
2.2. Linear/Ridge/Laso Regression OK
2.3. Decision Tree, Naive"
Denny Lee <denny.g.lee@gmail.com>,"Fri, 04 Sep 2015 05:13:31 +0000",Re: [VOTE] Release Apache Spark 1.5.0 (RC3),"Krishna Sankar <ksankar42@gmail.com>, Reynold Xin <rxin@databricks.com>","+1

Distinct count test is blazing fast - awesome!,


"") OK
k. But
Price)â€™;
€™; previously 'AVG(Total)').
4194304
h
s
e40f31a
d
===============
===============
========================
========================
not
======================================="
Akhil Das <akhil@sigmoidanalytics.com>,"Fri, 4 Sep 2015 14:57:21 +0530",Re: OOM in spark driver,Mike Hynes <91mbbh@gmail.com>,"Or you can increase the driver heap space (export _JAVA_OPTIONS=""-Xmx5g"")

Thanks
Best Regards


"
Tom Graves <tgraves_cs@yahoo.com.INVALID>,"Fri, 4 Sep 2015 14:30:08 +0000 (UTC)",Re: [VOTE] Release Apache Spark 1.5.0 (RC3),"Krishna Sankar <ksankar42@gmail.com>, Reynold Xin <rxin@databricks.com>","The upper/lower case thing is known. Â https://issues.apache.org/jira/browse/SPARK-9550I assume it was decided to be ok and its going to be in the release notes Â but Reynold or Josh can probably speak to it more.
Tom 


   

 +?Â 
1. Compiled OSX 10.10 (Yosemite) OK Total time: 26:09 minÂ Â  Â  Â mvn clean package -Pyarn -Phadoop-2.6 -DskipTests2. Tested pyspark, mllib2.1. statistics (min,max,mean,Pearson,Spearman) OK2.2. Linear/Ridge/Laso Regression OKÂ 2.3. Decision Tree, Naive Bayes OK2.4. KMeans OKÂ  Â  Â  Â Center And Scale OK2.5. RDD operations OKÂ  Â  Â  State of the Union Texts - MapReduce, Filter,sortByKey (word count)2.6. Recommendation (Movielens medium dataset ~1 M ratings) OKÂ  Â  Â  Â Model evaluation/optimization (rank, numIter, lambda) with itertools OK3. Scala - MLlib3.1. statistics (min,max,mean,Pearson,Spearman) OK3.2. LinearRegressionWithSGD OK3.3. Decision Tree OK3.4. KMeans OK3.5. Recommendation (Movielens medium dataset ~1 M ratings) OK3.6. saveAsParquetFile OK3.7. Read and verify the 4.3 save(above) - sqlContext.parquetFile, registerTempTable, sql OK3.8. result = sqlContext.sql(""SELECT OrderDetails.OrderID,ShipCountry,UnitPrice,Qty,Discount FROM Orders INNER JOIN OrderDetails ON Orders.OrderID = OrderDetails.OrderID"") OK4.0. Spark SQL from Python OK4.1. result = sqlContext.sql(""SELECT * from people WHERE State = 'WA'"") OK5.0. Packages5.1. com.databricks.spark.csv - read/write OK(--packages com.databricks:spark-csv_2.11:1.2.0-s_2.11 didnâ€™t work. But com.databricks:spark-csv_2.11:1.2.0 worked)6.0. DataFramesÂ 6.1. cast,dtypes OK6.2. groupBy,avg,crosstab,corr,isNull,na.drop OK6.3. All joins,sql,set operations,udf OK
Two Problems:
1. The synthetic column names are lowercase ( i.e. now â€˜sum(OrderPrice)â€™; previously â€˜SUM(OrderPrice)â€™, now â€˜avg(Total)â€™; previously 'AVG(Total)'). So programs that depend on the case of the synthetic column names would fail.2. orders_3.groupBy(""Year"",""Month"").sum('Total').show()Â  Â  fails with the error â€˜java.io.IOException: Unable to acquire 4194304 bytes of memoryâ€™Â  Â  orders_3.groupBy(""CustomerID"",""Year"").sum('Total').show() - fails with the same errorÂ  Â  Is this a known bug ?Cheers<k/>P.S: Sorry for the spam, forgot Reply AllÂ 

Please vote on releasing the following candidate as Apache Spark version 1.5.0. The vote is open until Friday, Sep 4, 2015 at 21:00 UTC and passes if a majority of at least 3 +1 PMC votes are cast.
[ ] +1 Release this package as Apache Spark 1.5.0[ ] -1 Do not release this package because ...
To learn more about Apache Spark, please see http://spark.apache.org/

The tag to be voted on is v1.5.0-rc3:https://github.com/apache/spark/commit/908e37bcc10132bb2aa7f80ae694a9df6e40f31a
The release files, including signatures, digests, etc. can be found at:http://people.apache.org/~pwendell/spark-releases/spark-1.5.0-rc3-bin/
Release artifacts are signed with the following key:https://people.apache.org/keys/committer/pwendell.asc
The staging repository for this release (published as 1.5.0-rc3) can be found at:https://repository.apache.org/content/repositories/orgapachespark-1143/
The staging repository for this release (published as 1.5.0) can be found at:https://repository.apache.org/content/repositories/orgapachespark-1142/
The documentation corresponding to this release can be found at:http://people.apache.org/~pwendell/spark-releases/spark-1.5.0-rc3-docs/

=======================================How can I help test this release?=======================================If you are a Spark user, you can help us test this release by taking an existing Spark workload and running on this release candidate, then reporting any regressions.

================================================What justifies a -1 vote for this release?================================================This vote is happening towards the end of the 1.5 QA period, so -1 votes should only occur for significant regressions from 1.4. Bugs already present in 1.4, minor regressions, or bugs related to new features will not block this release.

===============================================================What should happen to JIRA tickets still targeting 1.5.0?===============================================================1. It is OK for documentation patches to target 1.5.0 and still go into branch-1.5, since documentations will be packaged separately from the release.2. New features for non-alpha-modules should target 1.6+.3. Non-blocker bug fixes should target 1.5.1 or 1.6.0, or drop the target version.

==================================================Major changes to help you focus your testing==================================================
As of today, Spark 1.5 contains more than 1000 commits from 220+ contributors. I've curated a list of important changes for 1.5. For the complete list, please refer to Apache JIRA changelog.
RDD/DataFrame/SQL APIs
- New UDAF interface- DataFrame hints for broadcast join- expr function for turning a SQL expression into DataFrame column- Improved support for NaN values- StructType now supports ordering- TimestampType precision is reduced to 1us- 100 new built-in expressions, including date/time, string, math- memory and local disk only checkpointing
DataFrame/SQL Backend Execution
- Code generation on by default- Improved join, aggregation, shuffle, sorting with cache friendly algorithms and external algorithms- Improved window function performance- Better metrics instrumentation and reporting for DF/SQL execution plans
Data Sources, Hive, Hadoop, Mesos and Cluster Management
- Dynamic allocation support in all resource managers (Mesos, YARN, Standalone)- Improved Mesos support (framework authentication, roles, dynamic allocation, constraints)- Improved YARN support (dynamic allocation with preferred locations)- Improved Hive support (metastore partition pruning, metastore connectivity to 0.13 to 1.2, internal Hive upgrade to 1.2)- Support persisting data in Hive compatible format in metastore- Support data partitioning for JSON data sources- Parquet improvements (upgrade to 1.7, predicate pushdown, faster metadata discovery and schema merging, support reading non-standard legacy Parquet files generated by other libraries)- Faster and more robust dynamic partition insert- DataSourceRegister interface for external data sources to specify short names
SparkR
- YARN cluster mode in R- GLMs with R formula, binomial/Gaussian families, and elastic-net regularization- Improved error messages- Aliases to make DataFrame functions more R-like
Streaming
- Backpressure for handling bursty input streams.- Improved Python support for streaming sources (Kafka offsets, Kinesis, MQTT, Flume)- Improved Python streaming machine learning algorithms (K-Means, linear regression, logistic regression)- Native reliable Kinesis stream support- Input metadata like Kafka offsets made visible in the batch details UI- Better load balancing and scheduling of receivers across cluster- Include streaming storage in web UI
Machine Learning and Advanced Analytics
- Feature transformers: CountVectorizer, Discrete Cosine transformation, MinMaxScaler, NGram, PCA, RFormula, StopWordsRemover, and VectorSlicer.- Estimators under pipeline APIs: naive Bayes, k-means, and isotonic regression.- Algorithms: multilayer perceptron classifier, PrefixSpan for sequential pattern mining, association rule generation, 1-sample Kolmogorov-Smirnov test.- Improvements to existing algorithms: LDA, trees/ensembles, GMMs- More efficient Pregel API implementation for GraphX- Model summary for linear and logistic regression.- Python API: distributed matrices, streaming k-means and linear models, LDA, power iteration clustering, etc.- Tuning and evaluation: train-validation split and multiclass classification evaluator.- Documentation: document the release version of public API methods





  "
Krishna Sankar <ksankar42@gmail.com>,"Fri, 4 Sep 2015 08:00:47 -0700",Re: [VOTE] Release Apache Spark 1.5.0 (RC3),Tom Graves <tgraves_cs@yahoo.com>,"Thanks Tom.  Interestingly it happened between RC2 and RC3.
Now my vote is +1/2 unless the memory error is known and has a workaround.

Cheers
<k/>



"") OK
k. But
Price)â€™;
€™; previously 'AVG(Total)').
4194304
h
40f31a
===============
===============
========================
========================
ot
=======================================
=======================================
==========================
==========================
y
"
Yin Huai <yhuai@databricks.com>,"Fri, 4 Sep 2015 09:58:30 -0700",Re: [VOTE] Release Apache Spark 1.5.0 (RC3),Krishna Sankar <ksankar42@gmail.com>,"Hi Krishna,

Can you share your code to reproduce the memory allocation issue?

Thanks,

Yin


.
s
)
R
'"") OK
rk. But
rPrice)â€™;
€™; previously 'AVG(Total)').
 4194304
s
e40f31a
d
===============
===============
========================
========================
not
=======================================
=======================================
t
==========================
==========================
s
cy
t
l
"
Eron Wright <ewright@live.com>,"Fri, 4 Sep 2015 10:08:48 -0700",(Spark SQL) partition-scoped UDF,"""dev@spark.apache.org"" <dev@spark.apache.org>","Transformers in Spark ML typically operate on a per-row basis, based on callUDF. For a new transformer that I'm developing, I have a need to transform an entire partition with a function, as opposed to transforming each row separately.   The reason is that, in my case, rows must be transformed in batch for efficiency to amortize some overhead.   How may I accomplish this?
RDD that is then converted back to a DataFrame.   Unsure about the viability or consequences of that.
Thanks!Eron Wright 		 	   		  "
Cheolsoo Park <piaozhexiu@gmail.com>,"Fri, 4 Sep 2015 11:09:15 -0700",Flaky test in DAGSchedulerSuite?,Dev <dev@spark.apache.org>,"Hi devs,

I noticed this test case fails intermittently in Jenkins.

For eg, see the following builds-
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/41991/
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/41999/

The test failed in different PRs, and the failure looks unrelated to
changes in the PRs. Looks like the test was added by the following commit-

commit 80e2568b25780a7094199239da8ad6cfb6efc9f7
Author: Imran Rashid <irashid@cloudera.com>
Date:   Mon Jul 20 10:28:32 2015 -0700
    [SPARK-8103][core] DAGScheduler should not submit multiple concurrent
attempts for a stag

Thanks!
Cheolsoo
"
Reynold Xin <rxin@databricks.com>,"Fri, 4 Sep 2015 08:32:03 -1000",Re: [VOTE] Release Apache Spark 1.5.0 (RC3),Krishna Sankar <ksankar42@gmail.com>,"Krishna - I think the rename happened before rc1 actually. Was done couple
months ago.


.
s
)
R
'"") OK
rk. But
rPrice)â€™;
€™; previously 'AVG(Total)').
 4194304
s
e40f31a
d
===============
===============
========================
========================
not
=======================================
=======================================
t
==========================
==========================
s
cy
t
l
"
Pete Robbins <robbinspg@gmail.com>,"Fri, 4 Sep 2015 21:24:42 +0100",Re: Flaky test in DAGSchedulerSuite?,Cheolsoo Park <piaozhexiu@gmail.com>,"I've also just hit this and was about to raise a JIRA for this if there
isn't one already. I have a simple fix.


"
Pete Robbins <robbinspg@gmail.com>,"Fri, 4 Sep 2015 21:40:02 +0100",Re: Flaky test in DAGSchedulerSuite?,Cheolsoo Park <piaozhexiu@gmail.com>,"raised https://issues.apache.org/jira/browse/SPARK-10454 and PR


"
Krishna Sankar <ksankar42@gmail.com>,"Fri, 4 Sep 2015 14:22:49 -0700",Re: [VOTE] Release Apache Spark 1.5.0 (RC3),Yin Huai <yhuai@databricks.com>,"Yin,
   It is the
https://github.com/xsankar/global-bd-conf/blob/master/004-Orders.ipynb.
Cheers
<k/>


d.
t)
ER
A'"")
ork. But
â€™, now â€˜avg(Total)â€™;
e 4194304
:
n
es
6e40f31a
================
================
=========================
=========================
s
 not
========================================
========================================
===========================
===========================
ns
acy
,
r
I
,
,
"
shane knapp <sknapp@berkeley.edu>,"Fri, 4 Sep 2015 14:45:22 -0700",[build system] java package updates on the amplab jenkins workers,"dev <dev@spark.apache.org>, Josh Rosen <joshrosen@databricks.com>, 
	Patrick Wendell <patrick@databricks.com>","i've installed the latest java 7 and 8 packages on all of the jenkins workers!

i haven't updated the /usr/java/latest and /usr/java/default symlinks
to point to the new java 7 package, as i'd like to wait for downtime
when no builds are running.  switching java versions mid-build might
be fun, but also might cause failures and weird behavior.  this
downtime will happen once voting is done for spark 1.5 and the release
is cut.

however, java 8 is ready to go!  tests can be configured to point
JAVA_HOME at /usr/java/jdk1.8.0_60

related JIRAs:
https://issues.apache.org/jira/browse/SPARK-10455
https://issues.apache.org/jira/browse/SPARK-10456

---------------------------------------------------------------------


"
Cheolsoo Park <piaozhexiu@gmail.com>,"Fri, 4 Sep 2015 15:58:34 -0700",Re: Flaky test in DAGSchedulerSuite?,Pete Robbins <robbinspg@gmail.com>,"Thank you Pete!


"
Andrew Or <andrew@databricks.com>,"Fri, 4 Sep 2015 16:46:58 -0700",Re: Flaky test in DAGSchedulerSuite?,Cheolsoo Park <piaozhexiu@gmail.com>,"(merge into master, thanks for the quick fix Pete).

2015-09-04 15:58 GMT-07:00 Cheolsoo Park <piaozhexiu@gmail.com>:

"
Reynold Xin <rxin@databricks.com>,"Fri, 4 Sep 2015 14:19:57 -1000",Re: (Spark SQL) partition-scoped UDF,Eron Wright <ewright@live.com>,"Can you say more about your transformer?

This is a good idea, and indeed we are doing it for R already (the latest
way to run UDFs in R is to pass the entire partition as a local R dataframe
for users to run on). However, what works for R for simple data processing
might not work for your high performance transformer, etc.



"
Davies Liu <davies@databricks.com>,"Fri, 4 Sep 2015 21:57:34 -0700",Re: [VOTE] Release Apache Spark 1.5.0 (RC3),Krishna Sankar <ksankar42@gmail.com>,"Could you update the notebook to use builtin SQL function month and year,
instead of Python UDF? (they are introduced in 1.5).


:
NER
WA'"")
work. But
â€™, now â€˜avg(Total)â€™;
re 4194304
e:
on
ses if
f6e40f31a
:
e
/
/
/
================
================
n
=========================
=========================
es
resent
ock
========================================
========================================
o
===========================
===========================
e
gacy
s,
ar
UI
n,
.
s,

---------------------------------------------------------------------


"
Krishna Sankar <ksankar42@gmail.com>,"Fri, 4 Sep 2015 23:08:37 -0700",Re: [VOTE] Release Apache Spark 1.5.0 (RC3),Davies Liu <davies@databricks.com>,"Excellent & Thanks Davies. Yep, now runs fine and takes 1/2 the time !
This was exactly why I had put in the elapsed time calculations.
And thanks for the new pyspark.sql.functions.

+1 from my side for 1.5.0 RC3.
Cheers
<k/>


t work.
e)â€™, now â€˜avg(Total)â€™;
uire
s
/
40f31a
=================
=================
==========================
==========================
=========================================
=========================================
e
============================
============================
)
s
"
Reynold Xin <rxin@databricks.com>,"Fri, 4 Sep 2015 21:29:56 -1000",Re: [VOTE] Release Apache Spark 1.5.0 (RC3),Krishna Sankar <ksankar42@gmail.com>,"Thanks, Krishna, for the report. We should fix your problem using the
Python UDFs in 1.6 too.

I'm going to close this vote now. Thanks everybody for voting. This vote
passes with 8 +1 votes (3 binding) and no 0 or -1 votes.

+1:
Reynold Xin*
Tom Graves*
Burak Yavuz
Michael Armbrust*
Davies Liu
Forest Fang
Krishna Sankar
Denny Lee

0:

-1:


I will work on packaging this release in the next few days.




,
.
=
™t work.
ce)â€™, now â€˜avg(Total)â€™;
e
quire
ls
e40f31a
n
=================
=================
g
==========================
==========================
y
=========================================
=========================================
he
============================
============================
s)
c
"
Madawa Soysa <madawa.11@cse.mrt.ac.lk>,"Sat, 5 Sep 2015 19:17:19 +0530",Exception in saving MatrixFactorizationModel,dev@spark.apache.org,"Hi All,

I'm getting an error when trying to save a ALS MatrixFactorizationModel.
I'm using following method to save the model.

*model.save(sc, outPath)*

I'm getting the following exception when saving the model. I have attached
the full stack trace. Any help would be appreciated to resolve this issue.

org.apache.spark.SparkException: Job aborted.
        at
org.apache.spark.sql.sources.InsertIntoHadoopFsRelation.insert(commands.scala:166)
        at
org.apache.spark.sql.sources.InsertIntoHadoopFsRelation.run(commands.scala:139)
        at
org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:57)
        at
org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:57)
        at
org.apache.spark.sql.execution.ExecutedCommand.doExecute(commands.scala:68)
        at
org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:88)
        at
org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:88)
        at
org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
        at
org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:87)
        at
org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:950)
        at
org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:950)
        at
org.apache.spark.sql.sources.ResolvedDataSource$.apply(ddl.scala:336)
        at
org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:144)
        at
org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:135)
        at
org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:281)
        at
org.apache.spark.mllib.recommendation.MatrixFactorizationModel$SaveLoadV1_0$.save(MatrixFactorizationModel.scala:284)
        at
org.apache.spark.mllib.recommendation.MatrixFactorizationModel.save(MatrixFactorizationModel.scala:141)


Thanks,
Madawa
org.apache.spark.SparkException: Job aborted.
        at org.apache.spark.sql.sources.InsertIntoHadoopFsRelation.insert(commands.scala:166)
        at org.apache.spark.sql.sources.InsertIntoHadoopFsRelation.run(commands.scala:139)
        at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:57)
        at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:57)
        at org.apache.spark.sql.execution.ExecutedCommand.doExecute(commands.scala:68)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:88)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:88)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:87)
        at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:950)
        at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:950)
        at org.apache.spark.sql.sources.ResolvedDataSource$.apply(ddl.scala:336)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:144)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:135)
        at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:281)
        at org.apache.spark.mllib.recommendation.MatrixFactorizationModel$SaveLoadV1_0$.save(MatrixFactorizationModel.scala:284)
        at org.apache.spark.mllib.recommendation.MatrixFactorizationModel.save(MatrixFactorizationModel.scala:141)

org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 234.0 failed 1 times, most recent failure: Lost task 0.0 in stage 234.0 (TID 141, localhost): java.lang.NullPointerException
        at parquet.hadoop.InternalParquetRecordWriter.flushRowGroupToStore(InternalParquetRecordWriter.java:147)
        at parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:113)
        at parquet.hadoop.ParquetRecordWriter.close(ParquetRecordWriter.java:112)
        at org.apache.spark.sql.parquet.ParquetOutputWriter.close(newParquet.scala:88)
        at org.apache.spark.sql.sources.DefaultWriterContainer.abortTask(commands.scala:491)
        at org.apache.spark.sql.sources.InsertIntoHadoopFsRelation.org$apache$spark$sql$sources$InsertIntoHadoopFsRelation$$writeRows$1(commands.scala:190)
        at org.apache.spark.sql.sources.InsertIntoHadoopFsRelation$$anonfun$insert$1.apply(commands.scala:160)
        at org.apache.spark.sql.sources.InsertIntoHadoopFsRelation$$anonfun$insert$1.apply(commands.scala:160)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)
        at org.apache.spark.scheduler.Task.run(Task.scala:70)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)

Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)
        at scala.Option.foreach(Option.scala:236)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)

---------------------------------------------------------------------"
Eron Wright <ewright@live.com>,"Sat, 5 Sep 2015 13:55:34 -0700",Re: (Spark SQL) partition-scoped UDF,Reynold Xin <rxin@databricks.com>,"The transformer is a classification model produced by the NeuralNetClassification estimator of dl4j-spark-ml.  Source code here.  The neural net operates most efficiently when many examples are classified in batch.  I imagine overriding `transform` rather than `predictRaw`.   Does anyone know of a solution compatible with Spark 1.4 or 1.5?

Thanks again!

From:  Reynold Xin
Date:  Friday, September 4, 2015 at 5:19 PM
To:  Eron Wright
Cc:  ""dev@spark.apache.org""
Subject:  Re: (Spark SQL) partition-scoped UDF

Can you say more about your transformer?

This is a good idea, and indeed we are doing it for R already (the latest way to run UDFs in R is to pass the entire partition as a local R dataframe for users to run on). However, what works for R for simple data processing might not work for your high performance transformer, etc.


Transformers in Spark ML typically operate on a per-row basis, based on callUDF. For a new transformer that I'm developing, I have a need to transform an entire partition with a function, as opposed to transforming each row separately.   The reason is that, in my case, rows must be transformed in batch for efficiency to amortize some overhead.   How may I accomplish this?


Thanks!
Eron Wright
       


"
Yanbo Liang <ybliang8@gmail.com>,"Sun, 6 Sep 2015 11:23:12 +0800",Re: Exception in saving MatrixFactorizationModel,Madawa Soysa <madawa.11@cse.mrt.ac.lk>,"Please check the ""outPath"" and verify whether the saving succeed.
Which version did you use?
You may hit this issue <https://issues.apache.org/jira/browse/SPARK-7837> which
is resolved at version 1.5.

2015-09-05 21:47 GMT+08:00 Madawa Soysa <madawa.11@cse.mrt.ac.lk>:

"
Madawa Soysa <madawa.11@cse.mrt.ac.lk>,"Sun, 6 Sep 2015 11:46:41 +0530",Re: Exception in saving MatrixFactorizationModel,Yanbo Liang <ybliang8@gmail.com>,"outPath is correct. In the path, there are two directories data and
metadata. In the data directory, following data structure is there.

|-data
|----user
|--------_temporary
|------------ 0
|----------------_temporary

But nothing is written inside the folders. I'm using spark 1.4.1.




--
"
Madhu <madhu@madhu.com>,"Sun, 6 Sep 2015 08:23:54 -0700 (MST)",Detecting configuration problems,dev@spark.apache.org,"I'm not sure if this has been discussed already, if so, please point me to
the thread and/or related JIRA.

I have been running with about 1TB volume on a 20 node D2 cluster (255
GiB/node).
I have uniformly distributed data, so skew is not a problem.

I found that default settings (or wrong setting) for driver and executor
memory caused out of memory exceptions during shuffle (subtractByKey to be
exact). This was not easy to track down, for me at least.

3000 partitions, shuffle worked quite well (12 mins for subtractByKey). I'm
sure there are more improvement to made, but it's a lot better than heap
space exceptions!

similar disk backed collection.
I have some familiarity with that code based on previous work with external
sorting.

Is it possible to detect misconfiguration that leads to these OOMs and
produce a more meaningful error messages? I think that would really help
users who might not understand all the inner workings and configuration of
Spark (myself included). As it is, heap space issues are a challenge and
does not present Spark in a positive light.

I can help with that effort if someone is willing to point me to the precise
location of memory pressure during shuffle.

Thanks!



-----
--
Madhu
https://www.linkedin.com/in/msiddalingaiah
--

---------------------------------------------------------------------


"
Ranjana Rajendran <ranjana.rajendran@gmail.com>,"Sun, 6 Sep 2015 10:09:55 -0700",Re: Exception in saving MatrixFactorizationModel,Madawa Soysa <madawa.11@cse.mrt.ac.lk>,"It looks like you hit https://issues.apache.org/jira/browse/SPARK-7837 .
As I understand this occurs if there is skew in unpartitioned data.

Can you try partitioning model before saving it ?


"
Madawa Soysa <madawa.11@cse.mrt.ac.lk>,"Mon, 7 Sep 2015 07:14:59 +0530",Re: Exception in saving MatrixFactorizationModel,Ranjana Rajendran <ranjana.rajendran@gmail.com>,"Hi,

I'll try partitioning.

I have another question, after creating the MatrixFactorizationModel
through spark, can it be serialized as a Java object without any problem?


"
"""Cheng, Hao"" <hao.cheng@intel.com>","Mon, 7 Sep 2015 02:09:04 +0000",RE: [VOTE] Release Apache Spark 1.5.0 (RC3),"Reynold Xin <rxin@databricks.com>, Krishna Sankar <ksankar42@gmail.com>","Not sure if itâ€™s too late, but we found a critical bug at https://issues.apache.org/jira/browse/SPARK-10466
UnsafeRow ser/de will cause assert error, particularly for sort-based shuffle with data spill, this is not acceptable as itâ€™s very common in a large table joins.

From: RSeptember 5, 2015 3:30 PM
To: Krishna Sankar
Cc: Davies Liu; Yin Huai; Tom Graves; dev@spark.apache.org
Subject: Re: [VOTE] Release Apache Spark 1.5.0 (RC3)

Thanks, Krishna, for the report. We should fix your problem using the Python UDFs in 1.6 too.

I'm going to close this vote now. Thanks everybody for voting. This vote passes with 8 +1 votes (3 binding) and no 0 or -1 votes.

+1:
Reynold Xin*
Tom Graves*
Burak Yavuz
Michael Armbrust*
Davies Liu
Forest Fang
Krishna Sankar
Denny Lee

0:

-1:


I will work on packaging this release in the next few days.



On Fri, Sep 4, 2015 at 8:08 PM, Krishna Sankar <ksankar42@gmail.com<mailto:ksankar42@gmail.com>> wrote:
Excellent & Thanks Davies. Yep, now runs fine and takes 1/2 the time !
This was exactly why I had put in the elapsed time calculations.
And thanks for the new pyspark.sql.functions.

+1 from my side for 1.5.0 RC3.
Cheers
<k/>

On Fri, Sep 4, 2015 at 9:57 PM, Davies Liu <davies@databricks.com<mailto:davies@databricks.com>> wrote:
Could you update the notebook to use builtin SQL function month and year,
instead of Python UDF? (they are introduced in 1.5).

Once remove those two udfs, it runs successfully, also much faster.

On Fri, Sep 4, 2015 at 2:22 PM, Krishna Sankar <ksankar42@gmail.com<mailto:ksankar42@gmail.com>> wrote:
> Yin,
>    It is the
> https://github.com/xsankar/global-bd-conf/blob/master/004-Orders.ipynb.
> Cheers
> <k/>
>
> On Fri, Sep 4, uai@databricks.com>> wrote:
>>
>> Hi Krishna,
>>
>> Can you share your code to reproduce the memory allocation issue?
>>
>> Thanks,
>>
>> Yin
>>
>> On Fri, Sep 4, 2015 at 8:00 AM, Krishna Sankar <ksankar42@gmail.com<mailto:ksankar42@gmail.com>>
>> wrote:
>>>
>>> Thanks Tom.  Interestingly it happened between RC2 and RC3.
>>> Now my vote is +1/2 unless the memory error is known and has a
>>> workaround.
>>>
>>> Cheers
>>> <k/>
>>>
>>>
>>> On Fri, Sep 4, 2015 at 7:30 AM, Tom Graves <tgrave
>>>> The upper/lower case thing is known.
>>>> https://issues.apache.org/jira/browse/SPARK-9550
>>>> I assume it was decided to be ok and its going to be in the release
>>>> notes  but Reynold or Josh can probably speak to it more.
>>>>
>>>> Tom
>>>>
>>>>
>>>>
>>>> On Thursday, September 3, 2015 10:21 PM, Krishna Sankar
>>>> <ksankar42@gmail.com<mailto:ksankar42@gmail.com>> wrote:
>>>>
>>>>
>>>> +?
>>>>
>>>> 1. Compiled OSX 10.10 (Yosemite) OK Total time: 26:09 min
>>>>      mvn clean package -Pyarn -Phadoop-2.6 -DskipTests
>>>> 2. Tested pyspark, mllib
>>>> 2.1. statistics (min,max,mean,Pearson,Spearman) OK
>>>> 2.2. Linear/Ridge/Laso Regression OK
>>>> 2.3. Decision Tree, Naive Bayes OK
>>>> 2.4. KMeans OK
>>>>        Center And Scale OK
>>>> 2.5. RDD operations OK
>>>>       State of the Union Texts - MapReduce, Filter,sortByKey (word
>>>> count)
>>>> 2.6. Recommendation (Movielens medium dataset ~1 M ratings) OK
>>>>        Model evaluation/optimization (rank, numIter, lambda) with
>>>> itertools OK
>>>> 3. Scala - MLlib
>>>> 3.1. statistics (min,max,mean,Pearson,Spearman) OK
>>>> 3.2. LinearRegressionWithSGD OK
>>>> 3.3. Decision Tree OK
>>>> 3.4. KMeans OK
>>>> 3.5. Recommendation (Movielens medium dataset ~1 M ratings) OK
>>>> 3.6. saveAsParquetFile OK
>>>> 3.7. Read and verify the 4.3 save(above) - sqlContext.parquetFile,
>>>> registerTempTable, sql OK
>>>> 3.8. result = sqlContext.sql(""SELECT
>>>> OrderDetails.OrderID,ShipCountry,UnitPrice,Qty,Discount FROM Orders INNER
>>>> JOIN OrderDetails ON Orders.OrderID = OrderDetails.OrderID"") OK
>>>> 4.0. Spark SQL from Python OK
>>>> 4.1. result = sqlContext.sql(""SELECT * from people WHERE State = 'WA'"")
>>>> OK
>>>> 5.0. Packages
>>>> 5.1. com.databricks.spark.csv - read/write OK
>>>> (--packaork. But
>>>> com.databricks:spark-csv_2.11:1.2.0 worked)
>>>> 6.0. DataFrames
>>>> 6.1. cast,dtypes OK
>>>> 6.2. groupBy,avg,crosstab,corr,isNull,na.drop OK
>>>> 6.3. All joins,sql,set operations,udf OK
>>>>
>>>> Two Problems:
>>>>
>>>> 1. The synthetic column names are lowercase ( i.e. now
>>>> â€˜sum(OrderPrice)â€™; previously â€˜SUM(OrderPrice)â€™, now â€˜avg(Total)â€™;
>>>> previously 'AVG(Total)'). So programs that depend on the case of the
>>>> synthetic column names would fail.
>>>> 2. orders_3.groupBy(""Year"",""Month"").sum('Total').show()
>>>>     fails with the error â€˜java.io.IOException: Unable to acquire 4194304
>>>> bytes of memoryâ€™
>>>>     orders_3.groupBy(""CustomerID"",""Year"").sum('Total').show() - fails
>>>> with the same error
>>>>     Is this a known bug ?
>>>> Cheers
>>>> <k/>
>>>> P.S: Sorry for the spam, forgot Reply All
>>>>
>>>> On Tue, Sep 1, 2015 at abricks.com>> wrote:
>>>>
>>>> Please vote on releasing the following candidate as Apache Spark version
>>>> 1.5.0. The vote is open until Friday, Sep 4, 2015 at 21:00 UTC and passes if
>>>> a majority of at least 3 +1 PMC votes are cast.
>>>>
>>>> [ ] +1 Release this package as Apache Spark 1.5.0
>>>> [ ] -1 Do not release this package because ...
>>>>
>>>> To learn more about Apache Spark, please see http://spark.apache.org/
>>>>
>>>>
>>>> The tag to be voted on is v1.5.0-rc3:
>>>>
>>>> https://github.com/apache/spark/commit/908e37bcc10132bb2aa7f80ae694a9df6e40f31a
>>>>
>>>> The release files, including signatures, digests, etc. can be found at:
>>>> http://people.apache.org/~pwendell/spark-releases/spark-1.5.0-rc3-bin/
>>>>
>>>> Release artifacts are signed with the following key:
>>>> https://people.apache.org/keys/committer/pwendell.asc
>>>>
>>>> The staging repository for this release (published as 1.5.0-rc3) can be
>>>> found at:
>>>> https://repository.apache.org/content/repositories/orgapachespark-1143/
>>>>
>>>> The staging repository for this release (published as 1.5.0) can be
>>>> found at:
>>>> https://repository.apache.org/content/repositories/orgapachespark-1142/
>>>>
>>>> The documentation corresponding to this release can be found at:
>>>> http://people.apache.org/~pwendell/spark-releases/spark-1.5.0-rc3-docs/
>>>>
>>>>
>>>> =======================================
>>>> How can I help test this release?
>>>> =======================================
>>>> If you are a Spark user, you can help us test this release by taking an
>>>> existing Spark workload and running on this release candidate, then
>>>> reporting any regressions.
>>>>
>>>>
>>>> ================================================
>>>> What justifies a -1 vote for this release?
>>>> ================================================
>>>> This vote is happening towards the end of the 1.5 QA period, so -1 votes
>>>> should only occur for significant regressions from 1.4. Bugs already present
>>>> in 1.4, minor regressions, or bugs related to new features will not block
>>>> this release.
>>>>
>>>>
>>>> ===============================================================
>>>> What should happen to JIRA tickets still targeting 1.5.0?
>>>> ===============================================================
>>>> 1. It is OK for documentation patches to target 1.5.0 and still go into
>>>> branch-1.5, since documentations will be packaged separately from the
>>>> release.
>>>> 2. New features for non-alpha-modules should target 1.6+.
>>>> 3. Non-blocker bug fixes should target 1.5.1 or 1.6.0, or drop the
>>>> target version.
>>>>
>>>>
>>>> ==================================================
>>>> Major changes to help you focus your testing
>>>> ==================================================
>>>>
>>>> As of today, Spark 1.5 contains more than 1000 commits from 220+
>>>> contributors. I've curated a list of important changes for 1.5. For the
>>>> complete list, please refer to Apache JIRA changelog.
>>>>
>>>> RDD/DataFrame/SQL APIs
>>>>
>>>> - New UDAF interface
>>>> - DataFrame hints for broadcast join
>>>> - expr function for turning a SQL expression into DataFrame column
>>>> - Improved support for NaN values
>>>> - StructType now supports ordering
>>>> - TimestampType precision is reduced to 1us
>>>> - 100 new built-in expressions, including date/time, string, math
>>>> - memory and local disk only checkpointing
>>>>
>>>> DataFrame/SQL Backend Execution
>>>>
>>>> - Code generation on by default
>>>> - Improved join, aggregation, shuffle, sorting with cache friendly
>>>> algorithms and external algorithms
>>>> - Improved window function performance
>>>> - Better metrics instrumentation and reporting for DF/SQL execution
>>>> plans
>>>>
>>>> Data Sources, Hive, Hadoop, Mesos and Cluster Management
>>>>
>>>> - Dynamic allocation support in all resource managers (Mesos, YARN,
>>>> Standalone)
>>>> - Improved Mesos support (framework authentication, roles, dynamic
>>>> allocation, constraints)
>>>> - Improved YARN support (dynamic allocation with preferred locations)
>>>> - Improved Hive support (metastore partition pruning, metastore
>>>> connectivity to 0.13 to 1.2, internal Hive upgrade to 1.2)
>>>> - Support persisting data in Hive compatible format in metastore
>>>> - Support data partitioning for JSON data sources
>>>> - Parquet improvements (upgrade to 1.7, predicate pushdown, faster
>>>> metadata discovery and schema merging, support reading non-standard legacy
>>>> Parquet files generated by other libraries)
>>>> - Faster and more robust dynamic partition insert
>>>> - DataSourceRegister interface for external data sources to specify
>>>> short names
>>>>
>>>> SparkR
>>>>
>>>> - YARN cluster mode in R
>>>> - GLMs with R formula, binomial/Gaussian families, and elastic-net
>>>> regularization
>>>> - Improved error messages
>>>> - Aliases to make DataFrame functions more R-like
>>>>
>>>> Streaming
>>>>
>>>> - Backpressure for handling bursty input streams.
>>>> - Improved Python support for streaming sources (Kafka offsets, Kinesis,
>>>> MQTT, Flume)
>>>> - Improved Python streaming machine learning algorithms (K-Means, linear
>>>> regression, logistic regression)
>>>> - Native reliable Kinesis stream support
>>>> - Input metadata like Kafka offsets made visible in the batch details UI
>>>> - Better load balancing and scheduling of receivers across cluster
>>>> - Include streaming storage in web UI
>>>>
>>>> Machine Learning and Advanced Analytics
>>>>
>>>> - Feature transformers: CountVectorizer, Discrete Cosine transformation,
>>>> MinMaxScaler, NGram, PCA, RFormula, StopWordsRemover, and VectorSlicer.
>>>> - Estimators under pipeline APIs: naive Bayes, k-means, and isotonic
>>>> regression.
>>>> - Algorithms: multilayer perceptron classifier, PrefixSpan for
>>>> sequential pattern mining, association rule generation, 1-sample
>>>> Kolmogorov-Smirnov test.
>>>> - Improvements to existing algorithms: LDA, trees/ensembles, GMMs
>>>> - More efficient Pregel API implementation for GraphX
>>>> - Model summary for linear and logistic regression.
>>>> - Python API: distributed matrices, streaming k-means and linear models,
>>>> LDA, power iteration clustering, etc.
>>>> - Tuning and evaluation: train-validation split and multiclass
>>>> classification evaluator.
>>>> - Documentation: document the release version of public API methods
>>>>
>>>>
>>>>
>>>>
>>>>
>>>
>>
>


"
james <yiazhou@gmail.com>,"Sun, 6 Sep 2015 20:25:06 -0700 (MST)",Re: [VOTE] Release Apache Spark 1.5.0 (RC3),dev@spark.apache.org,"I saw a new ""spark.shuffle.manager=tungsten-sort"" implemented in
https://issues.apache.org/jira/browse/SPARK-7081, but it can't be found its
corresponding description in
http://people.apache.org/~pwendell/spark-releases/spark-1.5.0-rc3-docs/configuration.html(Currenlty
there are only 'sort' and 'hash' two options).



--

---------------------------------------------------------------------


"
kaklakariada <christoph.pirkl@gmail.com>,"Mon, 7 Sep 2015 01:02:18 -0700 (MST)",groupByKey() and keys with many values,dev@spark.apache.org,"Hi,

I already posted this question on the users mailing list
(http://apache-spark-user-list.1001560.n3.nabble.com/Using-groupByKey-with-many-values-per-key-td24538.html)
but did not get a reply. Maybe this is the correct forum to ask.

My problem is, that doing groupByKey().mapToPair() loads all values for a
key into memory which is a problem when the values don't fit into memory.
This was not a problem with Hadoop map/reduce, as the Iterable passed to the
reducer read from disk.

In Spark, the Iterable passed to mapToPair() is backed by a CompactBuffer
containing all values.

Is it possible to change this behavior without modifying Spark, or is there
a plan to change this?

Thank you very much for your help!
Christoph.



--

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Mon, 7 Sep 2015 09:31:17 +0100",Re: groupByKey() and keys with many values,kaklakariada <christoph.pirkl@gmail.com>,"That's how it's intended to work; if it's a problem, you probably need
to re-design your computation to not use groupByKey. Usually you can
do so.


---------------------------------------------------------------------


"
james <yiazhou@gmail.com>,"Mon, 7 Sep 2015 08:35:55 -0700 (MST)",Re: [VOTE] Release Apache Spark 1.5.0 (RC3),dev@spark.apache.org,"add a critical bug https://issues.apache.org/jira/browse/SPARK-10474
(Aggregation failed with unable to acquire memory)



--

---------------------------------------------------------------------


"
Antonio Piccolboni <antonio@piccolboni.info>,"Mon, 07 Sep 2015 19:11:40 +0000",Re: groupByKey() and keys with many values,"Sean Owen <sowen@cloudera.com>, kaklakariada <christoph.pirkl@gmail.com>","To expand on what Sean said, I would look into replacing groupByKey with
reduceByKey. Also take a look at this doc
<http://databricks.gitbooks.io/databricks-spark-knowledge-base/content/best_practices/prefer_reducebykey_over_groupbykey.html>.
I happen to have designed a library that was subject to the same criticism
when compared to the java mapreduce API wrt the use of iterables, but
neither we nor the critics could ever find a natural example of a problem
when you can express a computation as a single pass through each group
while using a constant amount of memory  that could not be converted to
using a combiner (mapreduce jargon, called a reduce in Spark and most
functional circles). If  you found such an example, while an obstacle for
you,  it would be of some  interest to know what it is.



"
lonikar <lonikar@gmail.com>,"Mon, 7 Sep 2015 12:44:14 -0700 (MST)",Re: Code generation for GPU,dev@spark.apache.org,"Hi Reynold,

Thanks for responding. I was waiting for this on the spark user group and my
own email id since I had not posted this on spark dev. Just saw your reply.

1. I figured the various code generation classes have either *apply* or
*eval* method depending on whether it computes something or uses expression
as filter. And the code that executes this generated code is in
sql.execution.basicOperators.scala.

2. If the vectorization is difficult or a major effort, I am not sure how I
am going to implement even a glimpse of changes I would like to. I think I
will have to satisfied with only a partial effort. Batching rows defeats the
purpose as I have found that it consumes a considerable amount of CPU cycles
and producing one row at a time also takes away the performance benefit.
Whats really required is to access a large partition and produce the result
partition in one shot. 

I think I will have to severely limit the scope of my talk in that case. Or
re-orient it to propose the changes instead of presenting the results of
execution on GPU. Please suggest since you seem to have selected the talk.

3. I agree, its pretty high paced development. I have started working on
1.5.1 spapshot.

4. How do I tune the batch size (number of rows in the ByteBuffer)? Is it
through the property spark.sql.inMemoryColumnarStorage.batchSize?

-Kiran



--

---------------------------------------------------------------------


"
Justin Uang <justin.uang@gmail.com>,"Tue, 08 Sep 2015 04:02:11 +0000",Fast Iteration while developing,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

What is the normal workflow for the core devs?

- Do we need to build the assembly jar to be able to run it from the spark
repo?
- Do you use sbt or maven to do the build?
- Is zinc only usuable for maven?

I'm asking because the current process I have right now is to do sbt build,
which means I'm stuck with about a 3-5 minute iteration cycle.

Thanks!

Justin
"
Reynold Xin <rxin@databricks.com>,"Mon, 7 Sep 2015 18:03:52 -1000",Re: Fast Iteration while developing,Justin Uang <justin.uang@gmail.com>,"I usually write a test case for what I want to test, and then run

sbt/sbt ""~module/test:test-only *MyTestSuite""




"
kaklakariada <christoph.pirkl@gmail.com>,"Mon, 7 Sep 2015 23:20:25 -0700 (MST)",Re: groupByKey() and keys with many values,dev@spark.apache.org,"Hi Antonio!

Thank you very much for your answer!
You are right in that in my case the computation could be replaced by a
reduceByKey. The thing is that my computation also involves database
queries:

1. Fetch key-specific data from database into memory. This is expensive and
I only want to do this once for a key.
2. Process each value using this data and update the common data
3. Store modified data to database. Here it is important to write all data
for a key in one go.

Is there a pattern how to implement something like this with reduceByKey?

Out of curiosity: I understand why you want to discourage people from using
groupByKey. But is there a technical reason why the Iterable is implemented
the way it is?

Kind regards,
Christoph.



--

---------------------------------------------------------------------


"
Niranda Perera <niranda.perera@gmail.com>,"Tue, 8 Sep 2015 12:33:19 +0530",adding jars to the classpath with the relative path to spark home,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

is it possible to add jars to the spark executor/  driver classpath with
the relative path of the jar (relative to the spark home)?
I need to set the following settings in the spark conf
- spark.driver.extraClassPath
- spark.executor.extraClassPath

the reason why I need to use the relative path is, if not, if we have a
spark cluster, all the jars needs to be kept in the same folder path.

I know we can pass the jars using the --jars options. but I'd rather prefer
this option.

cheers
-- 
Niranda
@n1r44 <https://twitter.com/N1R44>
https://pythagoreanscript.wordpress.com/
"
Sean Owen <sowen@cloudera.com>,"Tue, 8 Sep 2015 08:53:00 +0100",Re: groupByKey() and keys with many values,kaklakariada <christoph.pirkl@gmail.com>,"I think groupByKey is intended for cases where you do want the values
in memory; for one-pass use cases, it's more efficient to use
reduceByKey, or aggregateByKey if lower-level operations are needed.

For your case, you probably want to do you reduceByKey, then perform
the expensive per-key lookups once per key. You also probably want to
do this in foreachPartition, not foreach, in order to pay DB
connection costs just once per partition.


---------------------------------------------------------------------


"
"""Prabeesh K."" <prabsmails@gmail.com>","Tue, 8 Sep 2015 12:45:03 +0400",Pyspark DataFrame TypeError,dev <dev@spark.apache.org>,"I am trying to run the code RandomForestClassifier example in the PySpark
1.4.1 documentation,
https://spark.apache.org/docs/1.4.1/api/python/pyspark.ml.html#pyspark.ml.classification.RandomForestClassifier
.

Below is screen shot of ipython notebook



â€‹But for df.columns. It shows following error.


TypeError                                 Traceback (most recent call
last)<ipython-input-79-6a4642092433> in <module>()----> 1 df.columns
/home/datasci/src/spark/python/pyspark/sql/dataframe.pyc in
columns(self)    484         ['age', 'name']    485         """"""--> 486
        return [f.name for f in self.schema.fields]    487     488
@ignore_unicode_prefix
/home/datasci/src/spark/python/pyspark/sql/dataframe.pyc in
schema(self)    194         """"""    195         if self._schema is
None:--> 196             self._schema =
_parse_datatype_json_string(self._jdf.schema().json())    197
return self._schema    198
/home/datasci/src/spark/python/pyspark/sql/types.pyc in
_parse_datatype_json_string(json_string)    519     >>>
check_datatype(structtype_with_udt)    520     """"""--> 521     return
_parse_datatype_json_value(json.loads(json_string))    522     523
/home/datasci/src/spark/python/pyspark/sql/types.pyc in
_parse_datatype_json_value(json_value)    539         tpe =
json_value[""type""]    540         if tpe in _all_complex_types:--> 541
            return _all_complex_types[tpe].fromJson(json_value)    542
        elif tpe == 'udt':    543             return
UserDefinedType.fromJson(json_value)
/home/datasci/src/spark/python/pyspark/sql/types.pyc in fromJson(cls,
json)    386     @classmethod    387     def fromJson(cls, json):-->
388         return StructType([StructField.fromJson(f) for f in
json[""fields""]])    389     390
/home/datasci/src/spark/python/pyspark/sql/types.pyc in fromJson(cls,
json)    347     def fromJson(cls, json):    348         return
StructField(json[""name""],--> 349
_parse_datatype_json_value(json[""type""]),    350
     json[""nullable""],    351
json[""metadata""])
/home/datasci/src/spark/python/pyspark/sql/types.pyc in
_parse_datatype_json_value(json_value)    541             return
_all_complex_types[tpe].fromJson(json_value)    542         elif tpe
== 'udt':--> 543             return
UserDefinedType.fromJson(json_value)    544         else:    545
      raise ValueError(""not supported type: %s"" % tpe)
/home/datasci/src/spark/python/pyspark/sql/types.pyc in fromJson(cls,
json)    453         pyModule = pyUDT[:split]    454         pyClass =
pyUDT[split+1:]--> 455         m = __import__(pyModule, globals(),
locals(), [pyClass])    456         UDT = getattr(m, pyClass)    457
      return UDT()
TypeError: Item in ``from list'' not a string
"
Akhil Das <akhil@sigmoidanalytics.com>,"Tue, 8 Sep 2015 15:38:46 +0530",Re: Detecting configuration problems,Madhu <madhu@madhu.com>,"I found an old JIRA referring the same.
https://issues.apache.org/jira/browse/SPARK-5421

Thanks
Best Regards


"
Steve Loughran <stevel@hortonworks.com>,"Tue, 8 Sep 2015 11:27:15 +0000",Re: Code generation for GPU,lonikar <lonikar@gmail.com>,"


2. If the vectorization is difficult or a major effort, I am not sure how I
am going to implement even a glimpse of changes I would like to. I think I
will have to satisfied with only a partial effort. Batching rows defeats the
purpose as I have found that it consumes a considerable amount of CPU cycles
and producing one row at a time also takes away the performance benefit.
Whats really required is to access a large partition and produce the result
partition in one shot.


why not look at the dataframes APIs and the back-end implementations of things which support it?  The data sources which are columnized from the outset (ORC, parquet) are the ones where vector operations work well : you can read at of columns, perform a parallel operation, then repeat.

If you can hook up to a column structure you may get that speedup.


I think I will have to severely limit the scope of my talk in that case. Or
re-orient it to propose the changes instead of presenting the results of
execution on GPU. Please suggest since you seem to have selected the talk.

It is always essential to have the core of your talk ready before you propose the talk -its something reviewers (nothing to do with me here) mostly expect. Otherwise you are left in a panic three days before trying to do bash together some slides you will have to present to an audience that may include people that know the code better than you. I've been there -and fear I will be there again in 3 weeks time.

Some general suggestions

  1.  assume the audience knows spark, but not how to code for GPUs: intro that on a slide or two
  2.  cover the bandwidth problem: how much computation is needed before working with the GPU is justified
  3.  Look at the body of work of Hadoop MapReduce & GPUs and the limitations (IO bandwidth, intermediate stage B/W) as well as benefits (perf on CPU workloads, power budget)
  4.  Cover how that's changing: SDDs, in-memory filesystems, whether infiniband would help.
  5.  Try to demo something. It's always nice to show something working at a talk, even if its just your laptop


"
Madhu <madhu@madhu.com>,"Tue, 8 Sep 2015 05:53:57 -0700 (MST)",Re: Detecting configuration problems,dev@spark.apache.org,"Thanks Akhil!

I suspect the root cause of the shuffle OOM I was seeing (and probably many
that users might see) is due to individual partitions on the reduce side not
fitting in memory. As a guideline, I was thinking of something like ""be sure
that your largest partitions occupy no more then 1% of executor memory"" or
something to that effect. I can add that documentation to the tuning page if
someone can suggest the the best wording and numbers. I can also add a
simple Spark shell example to estimate largest partition size to determine
executor memory and number of partitions.

ShuffleManager, but that seems to be on the reduce side. Where is the code
driving the map side writes and reduce reads? I think it is possible to add
up reduce side volume for a key (they are byte reads at some point) and
raise an alarm if it's getting too high. Even a warning on the console would
be better than a catastrophic OOM.



-----
--
Madhu
https://www.linkedin.com/in/msiddalingaiah
--

---------------------------------------------------------------------


"
Madhusudanan Kandasamy <madhusudanan@in.ibm.com>,"Tue, 8 Sep 2015 20:30:37 +0530",Question on DAGScheduler.getMissingParentStages(),dev@spark.apache.org,"

Hi,

I'm new to SPARK, trying to understand the DAGScheduler code flow. As per
my understanding it looks like getMissingParentStages() doing a redundant
job of re-calculating stage dependencies. When the first stage is created
all of its dependent/parent stages would be recursively calculated and
stored in stage.parents member. Whenever any given stage needs to be
submitted, it would call getMissingParentStages() to get list of all
un-computed parent stages.

I've expected that getMissingParentStages() would go through stage.parents
and retrieve information about whether they are already computed or not.
However, this function does another graph traversal from the stage.rdd
which seems unnecessary. Is there any specific reason to design like that?
If not, I would like to redesign getMissingParentStages() avoiding the
graph traversal.

Thanks,
Madhu."
Antonio Piccolboni <antonio@piccolboni.info>,"Tue, 08 Sep 2015 16:51:55 +0000",Re: groupByKey() and keys with many values,"Sean Owen <sowen@cloudera.com>, kaklakariada <christoph.pirkl@gmail.com>","You may also consider selecting distinct keys and fetching from database
first, then join on key with values. This in case Sean's approach is not
viable -- in case you need to have the DB data before the first reduce
call. By not revealing your problem, you are forcing us to make guesses,
which are less useful. Imagine you want to compute a binning of the values
on a per key basis. The bin definitions are in the database. Then the
reduce would be updating counts per bin.  You could let the reduce
initialize the bin counts from DB when empty. This will result in multiple
database accesses and connections per key, and the higher the degree of
parallelism, the bigger the cost (see this
<https://gist.github.com/tdhopper/0e5b53b5692f1e371534> elementary
example), which is something you should avoid if you want to write code
with some durability to it. If you use the join approach, you can select
the keys, unique them and perform data base access to obtain bin defs. Now
join the data file with the bin file on key. Then pass this through a
reduceByKey to update the bin counts. Different application, you want to
compute max min values per key and want to compare with previously recored
max min, then store the overall max min. Then you don't need the data based
values during the reduce. You just fetch them in the foreachPartition,
before each write.

As far as the DB writes,  remember spark can retry a computation, so your
writes have to be idempotent (see this thread
<https://groups.google.com/forum/#!topic/spark-users/oM-IzQs0Z2s>, in which
Reynold is a bit optimistic about failures than I am comfortable with, but
who am I to question Reynold?)







"
Davies Liu <davies@databricks.com>,"Tue, 8 Sep 2015 11:38:06 -0700",Re: Pyspark DataFrame TypeError,"""Prabeesh K."" <prabsmails@gmail.com>","I tried with Python 2.7/3.4 and Spark 1.4.1/1.5-RC3, they all work as expected:

```
+-----+---------+
|label| featuers|
+-----+---------+
|  1.0|    [1.0]|
|  0.0|(1,[],[])|
+-----+---------+

['label', 'featuers']
```


---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Tue, 8 Sep 2015 11:47:28 -0700",Re: Fast Iteration while developing,Reynold Xin <rxin@databricks.com>,"+1 to reynolds suggestion.  This is probably the fastest way to iterate.

Another option for more ad-hoc debugging is `sbt/sbt sparkShell` which is
similar to bin/spark-shell but doesn't require you to rebuild the assembly
jar.


"
Kevin Chen <kchen@palantir.com>,"Tue, 8 Sep 2015 19:46:46 +0000",Deserializing JSON into Scala objects in Java code,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hello Spark Devs,

 I am trying to use the new Spark API json endpoints at /api/v1/[path]
(added in SPARK-3454).

 In order to minimize maintenance on our end, I would like to use
Retrofit/Jackson to parse the json directly into the Scala classes in
org/apache/spark/status/api/v1/api.scala (ApplicationInfo,
ApplicationAttemptInfo, etcâ€¦). However, Jackson does not seem to know how to
handle Scala Seqs, and will throw an error when trying to parse the
attempts: Seq[ApplicationAttemptInfo] field of ApplicationInfo. Our codebase
is in Java.

 My questions are:
1. Do you have any recommendations on how to easily deserialize Scala
objects from json? For example, do you have any current usage examples of
SPARK-3454 with Java?
2. Alternatively, are you committed to the json formats of /api/v1/path? I
would guess so, because of the â€˜v1â€™, but wanted to confirm. If so, I could
deserialize the json into instances of my own Java classes instead, without
worrying about changing the class structure later due to changes in the
Spark API.
Some further information:
* The error I am getting with Jackson when trying to deserialize the json
into ApplicationInfo is Caused by:
com.fasterxml.jackson.databind.JsonMappingException: Can not construct
instance of scala.collection.Seq, problem: abstract types either need to be
mapped to concrete types, have custom deserializer, or be instantiated with
additional type information
* I tried using Jacksonâ€™s DefaultScalaModule, which seems to have support
for Scala Seqs, but got no luck.
* Deserialization works if the Scala class does not have any Seq fields, and
works if the fields are Java Lists instead of Seqs.
Thanks very much for your help!
Kevin Chen



"
Marcelo Vanzin <vanzin@cloudera.com>,"Tue, 8 Sep 2015 12:55:09 -0700",Re: Deserializing JSON into Scala objects in Java code,Kevin Chen <kchen@palantir.com>,"Hi Kevin,

How did you try to use the Scala module? Spark has this code when
setting up the ObjectMapper used to generate the output:

  mapper.registerModule(com.fasterxml.jackson.module.scala.DefaultScalaModule)

As for supporting direct serialization to Java objects, I don't think
that was the goal of the API. The Scala API classes are public mostly
so that API compatibility checks are performed against them. If you
don't mind the duplication, you could write your own Java POJOs that
mirror the Scala API, and use them to deserialize the JSON.


know how to
ase
s
454
m. If so, I could
ut
nto
be
th
upport for
d



-- 
Marcelo

---------------------------------------------------------------------


"
Kevin Chen <kchen@palantir.com>,"Tue, 8 Sep 2015 20:27:29 +0000",Re: Deserializing JSON into Scala objects in Java code,Marcelo Vanzin <vanzin@cloudera.com>,"Hi Marcelo,

 Thanks for the quick response. I understand that I can just write my own
Java classes (I will use that as a fallback option), but in order to avoid
code duplication and further possible changes, I was hoping there would be
a way to use the Spark API classes directly, since it seems there should
be.

 I registered the Scala module in the same way (except in Java instead of
Scala),

mapper.registerModule(new DefaultScalaModule());

But I donâ€™t think the module is being used/registered properly? Do you
happen to know whether the above line should work in Java?




t
"
Marcelo Vanzin <vanzin@cloudera.com>,"Tue, 8 Sep 2015 13:43:10 -0700",Re: Deserializing JSON into Scala objects in Java code,Kevin Chen <kchen@palantir.com>,"Hi Kevin,

This code works fine for me (output is ""List(1, 2)""):

import org.apache.spark.status.api.v1.RDDPartitionInfo;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.module.scala.DefaultScalaModule;

class jackson { public static void main(String[] args) throws Exception {
  ObjectMapper mapper = new ObjectMapper();
  mapper.registerModule(new DefaultScalaModule());

  String json = ""{ \""blockName\"" : \""name\"", \""executors\"" : [ \""1\"",
\""2\"" ] }"";
  RDDPartitionInfo info = mapper.readValue(json, RDDPartitionInfo.class);
  System.out.println(info.executors());
} }


d
e
o you
l
o know
irm. If so, I
 support



-- 
Marcelo

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Tue, 8 Sep 2015 10:58:18 -1000",Re: groupByKey() and keys with many values,Antonio Piccolboni <antonio@piccolboni.info>,"

I'm wrong all the time so please do question me :)

to enforce idempotency. Maybe that's some API we can provide in Spark
itself to make it easier to write applications.
"
"""Prabeesh K."" <prabsmails@gmail.com>","Wed, 9 Sep 2015 10:05:07 +0400",Re: Pyspark DataFrame TypeError,Davies Liu <davies@databricks.com>,"Thanks for the reply. after rebuild now it looks good.


"
Maandy <dymczyk@gmail.com>,"Wed, 9 Sep 2015 02:01:12 -0700 (MST)",[MLlib] Extensibility of MLlib classes (Word2VecModel etc.),dev@spark.apache.org,"Hey,

I'm trying to implement doc2vec
(http://cs.stanford.edu/~quocle/paragraph_vector.pdf, mainly for
sport/research purpose due to all it's limitations so I would probably not
even try to PR it into MLlib itself) but to do that it would be highly
useful to have access to MLlib's Word2VecModel class, which is mostly
private. Is there any reason (i.e. some Spark/MLlib guidelines) for that or
would it be ok to refactor the code and make a PR? I've found a similar JIRA
issue which was posted almost a year ago but for some reason it got closed:
https://issues.apache.org/jira/browse/SPARK-4101.

Mateusz



--

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Wed, 9 Sep 2015 10:42:51 +0100",Did the 1.5 release complete?,dev <dev@spark.apache.org>,"I saw the end of the RC3 vote:
https://mail-archives.apache.org/mod_mbox/spark-dev/201509.mbox/%3CCAPh_B%3DbQWf_vVuPs_eRpvnNSj8fbULX4kULnbs6MCAA10ZQ9eQ%40mail.gmail.com%3E

but there are no artifacts for it in Maven?
http://search.maven.org/#search%7Cgav%7C1%7Cg%3A%22org.apache.spark%22%20AND%20a%3A%22spark-parent_2.10%22

and I don't see any announcement at dev@
https://mail-archives.apache.org/mod_mbox/spark-dev/201509.mbox/browser

But it was announced here just now:
https://databricks.com/blog/2015/09/09/announcing-spark-1-5.html

Did I miss something?

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Wed, 9 Sep 2015 02:47:30 -0700",[ANNOUNCE] Announcing Spark 1.5.0,"""dev@spark.apache.org"" <dev@spark.apache.org>, user <user@spark.apache.org>","Hi All,

Spark 1.5.0 is the sixth release on the 1.x line. This release represents
1400+ patches from 230+ contributors and 80+ institutions. To download
Spark 1.5.0 visit the downloads page.

A huge thanks go to all of the individuals and organizations involved in
development and testing of this release.

Visit the release notes [1] to read about the new features, or download [2]
the release today.

For errata in the contributions or release notes, please e-mail me
*directly* (not on-list).

Thanks to everyone who helped work on this release!

[1] http://spark.apache.org/releases/spark-release-1-5-0.html
[2] http://spark.apache.org/downloads.html
"
Reynold Xin <rxin@databricks.com>,"Wed, 9 Sep 2015 02:49:48 -0700",Re: Did the 1.5 release complete?,Sean Owen <sowen@cloudera.com>,"Dev/user announcement was made just now.

For Maven, I did publish it this afternoon (so it's been a few hours). If
it is still not there tomorrow morning, I will look into it.




"
Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"Wed, 9 Sep 2015 06:25:23 -0700 (MST)",Re: [ANNOUNCE] Announcing Spark 1.5.0,dev@spark.apache.org,"Great work, everyone!



-----
-- Yu Ishikawa
--

---------------------------------------------------------------------


"
Dimitris Kouzis - Loukas <lookfwd@gmail.com>,"Wed, 9 Sep 2015 14:30:06 +0100",Re: [ANNOUNCE] Announcing Spark 1.5.0,,"Yeii!


"
Mohammed Guller <mohammed@glassbeam.com>,"Wed, 9 Sep 2015 15:36:00 +0000",looking for a technical reviewer to review a book on Spark,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Spark developers,

I am writing a book on Spark. The publisher of the book is looking for a technical reviewer. You will be compensated for your time. The publisher will pay a flat rate per page for the review.

I spoke with Matei Zaharia about this and he suggested that I send an email to the dev mailing list.

The book covers Spark core and the Spark libraries, including Spark SQL, Spark Streaming, MLlib, Spark ML, and GraphX. It also covers operational aspects such as deployment with different cluster managers and monitoring.

Please let me know if you are interested and I will connect you with the publisher.

Thanks,
Mohammed

Principal Architect, Glassbeam Inc, www.glassbeam.com<http://www.glassbeam.com/>,
5201 Great America Parkway, Suite 360, Santa Clara, CA 95054
p: +1.408.740.4610<tel:%2B1.408.740.4615>, m: +1.925.786.7521<tel:%2B1.650.804.5065>, f: +1.408.740.4601<tel:%2B1.408.740.4601>, skype : mguller

"
Gurumurthy Yeleswarapu <guruyvs@yahoo.com.INVALID>,"Wed, 9 Sep 2015 15:50:08 +0000 (UTC)",Re: looking for a technical reviewer to review a book on Spark,"Mohammed Guller <mohammed@glassbeam.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Mohammad:
I'm interested.
ThanksGuru Yeleswarapu
      From: Mohammed Guller <mohammed@glassbeam.com>
 To: ""dev@spark.apache.org"" <dev@spark.apache.org> 
 Sent: Wednesday, September 9, 2015 8:36 AM
 Subject: looking for a technical reviewer to review a book on Spark
   
 <!--#yiv4992658051 _filtered #yiv4992658051 {font-family:Calibri;panose-1:2 15 5 2 2 2 4 3 2 4;}#yiv4992658051 #yiv4992658051 p.yiv4992658051MsoNormal, #yiv4992658051 li.yiv4992658051MsoNormal, #yiv4992658051 div.yiv4992658051MsoNormal {margin:0in;margin-bottom:.0001pt;font-size:11.0pt;font-family:""Calibri"", ""sans-serif"";}#yiv4992658051 a:link, #yiv4992658051 span.yiv4992658051MsoHyperlink {color:blue;text-decoration:underline;}#yiv4992658051 a:visited, #yiv4992658051 span.yiv4992658051MsoHyperlinkFollowed {color:purple;text-decoration:underline;}#yiv4992658051 span.yiv4992658051EmailStyle17 {font-family:""Calibri"", ""sans-serif"";color:windowtext;}#yiv4992658051 .yiv4992658051MsoChpDefault {font-family:""Calibri"", ""sans-serif"";} _filtered #yiv4992658051 {margin:1.0in 1.0in 1.0in 1.0in;}#yiv4992658051 div.yiv4992658051WordSection1 {}-->Hi Spark developers,  Â  I am writing a book on Spark. The publisher of the book is looking for a technical reviewer. You will be compensated for your time. The publisher will pay a flat rate per page for the review.  Â  I spoke with Matei Zaharia about this and he suggested that I send an email to the dev mailing list.  Â  The book covers Spark core and the Spark libraries, including Spark SQL, Spark Streaming, MLlib, Spark ML, and GraphX. It also covers operational aspects such as deployment with different cluster managers and monitoring.  Â  Please let me know if you are interested and I will connect you with the publisher.  Â  Thanks, Mohammed   Â  Principal Architect, Glassbeam Inc,Â www.glassbeam.com, 5201 Great America Parkway, Suite 360, Santa Clara, CA 95054Â 
p:Â +1.408.740.4610,Â m:Â +1.925.786.7521,Â f:Â +1.408.740.4601,skypeÂ : mguller  Â  

  "
Gurumurthy Yeleswarapu <guruyvs@yahoo.com.INVALID>,"Wed, 9 Sep 2015 16:07:09 +0000 (UTC)",Re: looking for a technical reviewer to review a book on Spark,"Gurumurthy Yeleswarapu <guruyvs@yahoo.com>, 
	Mohammed Guller <mohammed@glassbeam.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","My Apologies for broadcast! That email was meant for Mohammad.
      From: Gurumurthy Yeleswarapu <guruyvs@yahoo.com.INVALID>
 To: Mohammed Guller <mohammed@glassbeam.com>; ""dev@spark.apache.org"" <dev@spark.apache.org> 
 Sent: Wednesday, September 9, 2015 8:50 AM
 Subject: Re: looking for a technical reviewer to review a book on Spark
   
Hi Mohammad:
I'm interested.
ThanksGuru Yeleswarapu
 

     From: Mohammed Guller <mohammed@glassbeam.com>
 To: ""dev@spark.apache.org"" <dev@spark.apache.org> 
 Sent: Wednesday, September 9, 2015 8:36 AM
 Subject: looking for a technical reviewer to review a book on Spark
   
 #yiv1554291100 #yiv1554291100 -- filtered {font-family:Calibri;panose-1:2 15 5 2 2 2 4 3 2 4;}#yiv1554291100 p.yiv1554291100MsoNormal, #yiv1554291100 li.yiv1554291100MsoNormal, #yiv1554291100 div.yiv1554291100MsoNormal {margin:0in;margin-bottom:.0001pt;font-size:11.0pt;}#yiv1554291100 a:link, #yiv1554291100 span.yiv1554291100MsoHyperlink {color:blue;text-decoration:underline;}#yiv1554291100 a:visited, #yiv1554291100 span.yiv1554291100MsoHyperlinkFollowed {color:purple;text-decoration:underline;}#yiv1554291100 span.yiv1554291100EmailStyle17 {color:windowtext;}#yiv1554291100 .yiv1554291100MsoChpDefault {}#yiv1554291100 filtered {margin:1.0in 1.0in 1.0in 1.0in;}#yiv1554291100 div.yiv1554291100WordSection1 {}#yiv1554291100 Hi Spark developers,  Â  I am writing a book on Spark. The publisher of the book is looking for a technical reviewer. You will be compensated for your time. The publisher will pay a flat rate per page for the review.  Â  I spoke with Matei Zaharia about this and he suggested that I send an email to the dev mailing list.  Â  The book covers Spark core and the Spark libraries, including Spark SQL, Spark Streaming, MLlib, Spark ML, and GraphX. It also covers operational aspects such as deployment with different cluster managers and monitoring.  Â  Please let me know if you are interested and I will connect you with the publisher.  Â  Thanks, Mohammed   Â  Principal Architect, Glassbeam Inc,Â www.glassbeam.com, 5201 Great America Parkway, Suite 360, Santa Clara, CA 95054Â 
p:Â +1.408.740.4610,Â m:Â +1.925.786.7521,Â f:Â +1.408.740.4601,skypeÂ : mguller  Â  

   

  "
Eron Wright <ewright@live.com>,"Wed, 9 Sep 2015 09:30:29 -0700",RE: (Spark SQL) partition-scoped UDF,Reynold Xin <rxin@databricks.com>,"Follow-up:  solved this problem by overriding the model's `transform` method, and using `mapPartitions` to produce a new DataFrame rather than using `udf`.   
Source code:https://github.com/deeplearning4j/deeplearning4j/blob/135d3b25b96c21349abf488a44f59bb37a2a5930/deeplearning4j-scaleout/spark/dl4j-spark-ml/src/main/scala/org/deeplearning4j/spark/ml/classification/MultiLayerNetworkClassification.scala#L143
Thanks Reynold for your time.
-Eron
Date: Sat, 5 Sep 2015 13:55:34 -0700
Subject: Re: (Spark SQL) partition-scoped UDF
From: ewright@live.com
To: rxin@databricks.com
CC: dev@spark.apache.org

The transformer is a classification model produced by the NeuralNetClassification estimator of dl4j-spark-ml.  Source code here.  The neural net operates most efficiently when many examples are classified in batch.  I imagine overriding `transform` rather than `predictRaw`.   Does anyone know of a solution compatible with Spark 1.4 or 1.5?
Thanks again!
From:  Reynold Xin
Date:  Friday, September 4, 2015 at 5:19 PM
To:  Eron Wright
Cc:  ""dev@spark.apache.org""
Subject:  Re: (Spark SQL) partition-scoped UDF

Can you say more about your transformer?
This is a good idea, and indeed we are doing it for R already (the latest way to run UDFs in R is to pass the entire partition as a local R dataframe for users to run on). However, what works for R for simple data processing might not work for your high performance transformer, etc.

Transformers in Spark ML typically operate on a per-row basis, based on callUDF. For a new transformer that I'm developing, I have a need to transform an entire partition with a function, as opposed to transforming each row separately.   The reason is that, in my case, rows must be transformed in batch for efficiency to amortize some overhead.   How may I accomplish this?
RDD that is then converted back to a DataFrame.   Unsure about the viability or consequences of that.
Thanks!Eron Wright 		 	   		  
 		 	   		  "
Jerry Lam <chilinglam@gmail.com>,"Wed, 9 Sep 2015 12:35:35 -0400",Re: [ANNOUNCE] Announcing Spark 1.5.0,,"Hi Spark Developers,

I'm eager to try it out! However, I got problems in resolving dependencies:
[warn] [NOT FOUND  ]
org.apache.spark#spark-core_2.10;1.5.0!spark-core_2.10.jar (0ms)
[warn] ==== jcenter: tried

When the package will be available?

Best Regards,

Jerry



"
Ted Yu <yuzhihong@gmail.com>,"Wed, 9 Sep 2015 09:39:21 -0700",Re: [ANNOUNCE] Announcing Spark 1.5.0,Jerry Lam <chilinglam@gmail.com>,"Jerry:
I just tried building hbase-spark module with 1.5.0 and I see:

ls -l ~/.m2/repository/org/apache/spark/spark-core_2.10/1.5.0
total 21712
-rw-r--r--  1 tyu  staff       196 Sep  9 09:37 _maven.repositories
-rw-r--r--  1 tyu  staff  11081542 Sep  9 09:37 spark-core_2.10-1.5.0.jar
-rw-r--r--  1 tyu  staff        41 Sep  9 09:37
spark-core_2.10-1.5.0.jar.sha1
-rw-r--r--  1 tyu  staff     19816 Sep  9 09:37 spark-core_2.10-1.5.0.pom
-rw-r--r--  1 tyu  staff        41 Sep  9 09:37
spark-core_2.10-1.5.0.pom.sha1

FYI


"
andy petrella <andy.petrella@gmail.com>,"Wed, 09 Sep 2015 16:40:37 +0000",Re: [ANNOUNCE] Announcing Spark 1.5.0,"Ted Yu <yuzhihong@gmail.com>, Jerry Lam <chilinglam@gmail.com>","You can try it out really quickly by ""building"" a Spark Notebook from
http://spark-notebook.io/.

Just choose the master branch and 1.5.0, a correct hadoop version (default
to 2.2.0 though) and there you go :-)



andy
"
lonikar <lonikar@gmail.com>,"Wed, 9 Sep 2015 12:18:24 -0700 (MST)",Re: Code generation for GPU,dev@spark.apache.org,"I am already looking at the dataframes APIs and the implementation. In fact,
the columnar representation
https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/columnar/ColumnType.scala
is what gave me the idea of my talk proposal. It is ideally suited for
computation on GPU. But from what Reynold said, it appears that the columnar
structure is not exploited for computation like expressions. It appears that
the columnar structure is used only for space efficient in memory storage
and not for computations. Even the TungstenProject invokes the operations on
a row by row basis. The UnsafeRow is optimized in the sense that it is only
a logical row as opposed to the InternalRow which has physical copies of the
values. But the computation is still on a per row basis rather than batches
of rows stored in columnar structure.

Thanks for some concrete suggestions on presentation. I do have the core
idea or theme of my talk ready in mind, but I will now present on the lines
you suggest. I wasn't really thinking of a demo, but now I will do that. I
was actually hoping to be able to contribute to spark code and show results
on those changes rather than offline changes. I will still try to do that by
hooking to the columnar structure, but it may not be in a shape that can go
in the spark code. Thats what I meant by severely limiting the scope of my
talk.

I have seen a perf improvement of 5-10 times on expression evaluation even
on ""ordinary"" laptop GPUs. Thus, it will be a good demo along with some
concrete proposals for vectorization. As you said, I will have to hook up to
a column structure and perform computation and let the existing spark
computation also proceed and compare the performance.

I will focus on the slides early (7th Oct is deadline), and then continue
the work for another 3 weeks till the summit. It still gives me enough time
to do considerable work. Hope your fear does not come true.






--

---------------------------------------------------------------------


"
lonikar <lonikar@gmail.com>,"Wed, 9 Sep 2015 12:31:29 -0700 (MST)","Spark 1.5: How to trigger expression execution through
 UnsafeRow/TungstenProject",dev@spark.apache.org,"The tungsten, cogegen etc options are enabled by default. But I am not able
to get the execution through the UnsafeRow/TungstenProject. It still
executes using InternalRow/Project.

I see this in the SparkStrategies.scala: If unsafe mode is enabled and we
support these data types in Unsafe, use the tungsten project. Otherwise use
the normal project.

Can someone give an example code on what can trigger this? I tried some of
the primitive types but did not work.



--

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Wed, 9 Sep 2015 14:18:06 -0700",Re: Spark 1.5: How to trigger expression execution through UnsafeRow/TungstenProject,lonikar <lonikar@gmail.com>,"Here is the example from Reynold (
http://search-hadoop.com/m/q3RTtfvs1P1YDK8d) :

scala> val data = sc.parallelize(1 to size, 5).map(x =>
(util.Random.nextInt(size /
repetitions),util.Random.nextDouble)).toDF(""key"", ""value"")
data: org.apache.spark.sql.DataFrame = [key: int, value: double]

scala> data.explain
== Physical Plan ==
TungstenProject [_1#0 AS key#2,_2#1 AS value#3]
 Scan PhysicalRDD[_1#0,_2#1]

...
scala> val res = df.groupBy(""key"").agg(sum(""value""))
res: org.apache.spark.sql.DataFrame = [key: int, sum(value): double]

scala> res.explain
15/09/09 14:17:26 INFO MemoryStore: ensureFreeSpace(88456) called with
curMem=84037, maxMem=556038881
15/09/09 14:17:26 INFO MemoryStore: Block broadcast_2 stored as values in
memory (estimated size 86.4 KB, free 530.1 MB)
15/09/09 14:17:26 INFO MemoryStore: ensureFreeSpace(19788) called with
curMem=172493, maxMem=556038881
15/09/09 14:17:26 INFO MemoryStore: Block broadcast_2_piece0 stored as
bytes in memory (estimated size 19.3 KB, free 530.1 MB)
15/09/09 14:17:26 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory
on localhost:42098 (size: 19.3 KB, free: 530.2 MB)
15/09/09 14:17:26 INFO SparkContext: Created broadcast 2 from explain at
<console>:27
== Physical Plan ==
TungstenAggregate(key=[key#19],
functions=[(sum(value#20),mode=Final,isDistinct=false)],
output=[key#19,sum(value)#21])
 TungstenExchange hashpartitioning(key#19)
  TungstenAggregate(key=[key#19],
functions=[(sum(value#20),mode=Partial,isDistinct=false)],
output=[key#19,currentSum#25])
   Scan ParquetRelation[file:/tmp/data][key#19,value#20]

FYI


"
Kevin Chen <kchen@palantir.com>,"Wed, 9 Sep 2015 22:31:06 +0000",Re: Deserializing JSON into Scala objects in Java code,"Christopher Currie <christopher@currie.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>, Marcelo Vanzin <vanzin@cloudera.com>","Marcelo and Christopher,

 Thanks for your help! The problem turned out to arise from a different part
of the code (we have multiple ObjectMappers), but because I am not very
familiar with Jackson I had thought there was a problem with the Scala
module.

Thank you again,
Kevin

From:  Christopher Currie <christopher@currie.com>
Date:  Wednesday, September 9, 2015 at 10:17 AM
To:  Kevin Chen <kchen@palantir.com>, ""dev@spark.apache.org""
<dev@spark.apache.org>
Cc:  Matt Cheah <mcheah@palantir.com>, Mingyu Kim <mkim@palantir.com>
Subject:  Fwd: Deserializing JSON into Scala objects in Java code

Kevin,

I'm not a Spark dev, but I maintain the Scala module for Jackson. If you're
continuing to have issues with parsing JSON using the Spark Scala datatypes,
let me know or chime in on the jackson mailing list
(jackson-user@googlegroups.com) and I'll see what I can do to help.

Christopher Currie

---------- Forwarded message ----------
From: Paul Brown <prb@mult.ifario.us>
Date: Tue, Sep 8, 2015 at 8:58 PM
Subject: Fwd: Deserializing JSON into Scala objects in Java code
To: Christopher Currie <christopher@currie.com>


Passing along. 

---------- Forwarded message ----------
From: Kevin Chen <kchen@palantir.com>
Date: Tuesday, September 8, 2015
Subject: Deserializing JSON into Scala objects in Java code
To: ""dev@spark.apache.org"" <dev@spark.apache.org>
Cc: Matt Cheah <mcheah@palantir.com>, Mingyu Kim <mkim@palantir.com>


Hello Spark Devs,

 I am trying to use the new Spark API json endpoints at /api/v1/[path]
(added in SPARK-3454).

 In order to minimize maintenance on our end, I would like to use
Retrofit/Jackson to parse the json directly into the Scala classes in
org/apache/spark/status/api/v1/api.scala (ApplicationInfo,
ApplicationAttemptInfo, etcâ€¦). However, Jackson does not seem to know how to
handle Scala Seqs, and will throw an error when trying to parse the
attempts: Seq[ApplicationAttemptInfo] field of ApplicationInfo. Our codebase
is in Java.

 My questions are:
1. Do you have any recommendations on how to easily deserialize Scala
objects from json? For example, do you have any current usage examples of
SPARK-3454 with Java?
2. Alternatively, are you committed to the json formats of /api/v1/path? I
would guess so, because of the â€˜v1â€™, but wanted to confirm. If so, I could
deserialize the json into instances of my own Java classes instead, without
worrying about changing the class structure later due to changes in the
Spark API.
Some further information:
* The error I am getting with Jackson when trying to deserialize the json
into ApplicationInfo is Caused by:
com.fasterxml.jackson.databind.JsonMappingException: Can not construct
instance of scala.collection.Seq, problem: abstract types either need to be
mapped to concrete types, have custom deserializer, or be instantiated with
additional type information
* I tried using Jacksonâ€™s DefaultScalaModule, which seems to have support
for Scala Seqs, but got no luck.
* Deserialization works if the Scala class does not have any Seq fields, and
works if the fields are Java Lists instead of Seqs.
Thanks very much for your help!
Kevin Chen




-- 
(Sent from mobile. Pardon brevity.)



"
StanZhai <mail@zhaishidan.cn>,"Wed, 9 Sep 2015 23:11:35 -0700 (MST)",[SparkSQL]Could not alter table in Spark 1.5 use HiveContext,dev@spark.apache.org,"After upgrade spark from 1.4.1 to 1.5.0, I encountered the following
exception when use alter table statement in HiveContext:

The sql is: ALTER TABLE a RENAME TO b

The exception is:

FAILED: Execution Error, return code 1 from
org.apache.hadoop.hive.ql.exec.DDLTask. Unable to alter table. Invalid
method name: 'alter_table_with_cascade'
msg: org.apache.spark.sql.execution.QueryExecutionException: FAILED:
Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask.
Unable to alter table. Invalid method name: 'alter_table_with_cascade'
	at
org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$runHive$1.apply(ClientWrapper.scala:433)
	at
org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$runHive$1.apply(ClientWrapper.scala:418)
	at
org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$withHiveState$1.apply(ClientWrapper.scala:256)
	at
org.apache.spark.sql.hive.client.ClientWrapper.retryLocked(ClientWrapper.scala:211)
	at
org.apache.spark.sql.hive.client.ClientWrapper.withHiveState(ClientWrapper.scala:248)
	at
org.apache.spark.sql.hive.client.ClientWrapper.runHive(ClientWrapper.scala:418)
	at
org.apache.spark.sql.hive.client.ClientWrapper.runSqlHive(ClientWrapper.scala:408)
	at org.apache.spark.sql.hive.HiveContext.runSqlHive(HiveContext.scala:558)
	at
org.apache.spark.sql.hive.execution.HiveNativeCommand.run(HiveNativeCommand.scala:33)
	at
org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:57)
	at
org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:57)
	at
org.apache.spark.sql.execution.ExecutedCommand.doExecute(commands.scala:69)
	at
org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:140)
	at
org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:138)
	at
org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:138)
	at
org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:927)
	at
org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:927)
	at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:144)
	at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:129)
	at org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:51)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:719)
	at test.service.QueryService.query(QueryService.scala:28)
	at test.api.DatabaseApi$$anonfun$query$1.apply(DatabaseApi.scala:39)
	at test.api.DatabaseApi$$anonfun$query$1.apply(DatabaseApi.scala:30)
	at test.web.JettyUtils$$anon$1.getOrPost(JettyUtils.scala:81)
	at test.web.JettyUtils$$anon$1.doPost(JettyUtils.scala:119)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:755)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:848)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)
	at
org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:501)
	at
org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)
	at
org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:428)
	at
org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)
	at
org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
	at org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:52)
	at
org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
	at org.eclipse.jetty.server.Server.handle(Server.java:370)
	at
org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)
	at
org.eclipse.jetty.server.AbstractHttpConnection.content(AbstractHttpConnection.java:982)
	at
org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:1043)
	at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:865)
	at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:240)
	at
org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
	at
org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:667)
	at
org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)
	at
org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at
org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:745)

The sql can be run both at Spark 1.4.1 and Hive, I think this should be a
bug of Spark 1.5, Any suggestion? 

Best, Stan



--

---------------------------------------------------------------------


"
Paul Wais <paulwais@gmail.com>,"Thu, 10 Sep 2015 00:42:40 -0700 (MST)",Re: Code generation for GPU,dev@spark.apache.org,"In order to get a major speedup from applying *single-pass* map/filter/reduce
operations on an array in GPU memory, wouldn't you need to stream the
columnar data directly into GPU memory somehow?  You might find in your
experiments that GPU memory allocation is a bottleneck.  See e.g. John
Canny's paper here (Section 1.1 paragraph 2):
http://www.cs.berkeley.edu/~jfc/papers/13/BIDMach.pdf    If the per-item
operation is very non-trivial, though, a dramatic GPU speedup may be more
likely.

Something related (and perhaps easier to contribute to Spark) might be a
GPU-accelerated sorter for sorting Unsafe records.  Especially since that
stuff is already broken out somewhat well-- e.g. `UnsafeInMemorySorter`. 
Spark appears to use (single-threaded) Timsort for sorting Unsafe records,
so I imagine a multi-thread/multi-core GPU solution could handily beat that.



--

---------------------------------------------------------------------


"
Steve Loughran <stevel@hortonworks.com>,"Thu, 10 Sep 2015 08:38:08 +0000",Re: Code generation for GPU,lonikar <lonikar@gmail.com>,"
n
 to

you might also be interested to know that there's now a YARN JIRA on making GPU another resource you can ask for
https://issues.apache.org/jira/browse/YARN-4122

if implemented, it'd let you submit work into the cluster asking for GPUs, and get allocated containers on servers with the GPU capacity you need. This'd allow you to share GPUs with other code (including your own containers)

me

good luck. And the fear is about my talk at apachecon on the Hadoop stack & Kerberos


---------------------------------------------------------------------


"
lonikar <lonikar@gmail.com>,"Thu, 10 Sep 2015 04:10:12 -0700 (MST)",Spark 1.5.x: Java files in src/main/scala and vice versa,dev@spark.apache.org,"I found these files:
spark-1.5.0/sql/catalyst/*src/main/scala*/org/apache/spark/sql/types/*SQLUserDefinedType.java*

spark-1.5.0/core/src/main/java/org/apache/spark/api/java/function/package.scala
and several java files in spark-1.5.0/core/src/main/scala/.

Is this intentional or inadvertant?



--

---------------------------------------------------------------------


"
Nitay Joffe <nitay@actioniq.co>,"Thu, 10 Sep 2015 09:16:03 -0400",DF.intersection issue in 1.5,dev@spark.apache.org,"The following fails for me in Spark 1.5:
https://gist.github.com/nitay/d08cb294ccf00b80c49a
Specifically, it returns 1 instead of 100 (in both versions).
When I print out the contents (i.e. collect()) I see all 100 items, yet the
count returns 1.

This works in 1.3 and 1.4.

Any ideas what's going on?
"
Sean Owen <sowen@cloudera.com>,"Thu, 10 Sep 2015 15:38:39 +0200",Re: Spark 1.5.x: Java files in src/main/scala and vice versa,"lonikar <lonikar@gmail.com>, Reynold Xin <rxin@databricks.com>","I feel like I knew the answer to this but have forgotten. Reynold do
you know about this file? looks like you added it.


---------------------------------------------------------------------


"
Olivier Toupin <olivier.toupin@gmail.com>,"Thu, 10 Sep 2015 09:09:18 -0700 (MST)",Concurrency issue in SQLExecution.withNewExecutionId,dev@spark.apache.org,"Look at this code:

https://github.com/apache/spark/blob/branch-1.5/sql/core/src/main/scala/org/apache/spark/sql/execution/SQLExecution.scala#L42

and 

https://github.com/apache/spark/blob/branch-1.5/sql/core/src/main/scala/org/apache/spark/sql/execution/SQLExecution.scala#L87

This exception is there to prevent ""nested `withNewExecutionId`"" but what if
there is two concurrent commands that happens to run on the same thread?
Then the thread local getLocalProperty will returns an execution id,
triggering that exception.

This is not hypothetical,  one of our spark job crash randomly with the
following stack trace (Using Spark 1.5, it ran without problem in Spark
1.4.1):

java.lang.IllegalArgumentException: spark.sql.execution.id is already set
	at
org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:87)
	at org.apache.spark.sql.DataFrame.withNewExecutionId(DataFrame.scala:1904)
	at org.apache.spark.sql.DataFrame.collect(DataFrame.scala:1385)


Also imagine the following:

future {Â df1.count() }
future {Â df2.count() }

Could we double check this if this an issue?




--
3.nabble.com/Concurrency-issue-in-SQLExecution-withNewExecutionId-tp14035.html
om.

---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Thu, 10 Sep 2015 10:38:16 -0700",Re: DF.intersection issue in 1.5,Nitay Joffe <nitay@actioniq.co>,"Thanks for pointing this out.
https://issues.apache.org/jira/browse/SPARK-10539

We will fix this for Spark 1.5.1.


"
Michael Armbrust <michael@databricks.com>,"Thu, 10 Sep 2015 10:48:15 -0700",Re: [SparkSQL]Could not alter table in Spark 1.5 use HiveContext,StanZhai <mail@zhaishidan.cn>,"Can you open a JIRA?


"
Yin Huai <yhuai@databricks.com>,"Thu, 10 Sep 2015 11:58:56 -0700",Re: [SparkSQL]Could not alter table in Spark 1.5 use HiveContext,Michael Armbrust <michael@databricks.com>,"What is the Hive version of your metastore server?

Looks like you are using a Hive 1.2's metastore client talking to your
existing Hive 0.13.1 metastore server?


"
Andrew Or <andrew@databricks.com>,"Thu, 10 Sep 2015 13:01:19 -0700",Re: Concurrency issue in SQLExecution.withNewExecutionId,Olivier Toupin <olivier.toupin@gmail.com>,"Thanks for reporting this, I have filed
https://issues.apache.org/jira/browse/SPARK-10548.

2015-09-10 9:09 GMT-07:00 Olivier Toupin <olivier.toupin@gmail.com>:

"
Andrew Or <andrew@databricks.com>,"Thu, 10 Sep 2015 15:25:38 -0700",Re: Concurrency issue in SQLExecution.withNewExecutionId,Olivier Toupin <olivier.toupin@gmail.com>,"@Olivier, did you use scala's parallel collections by any chance? If not,
what form of concurrency were you using?

2015-09-10 13:01 GMT-07:00 Andrew Or <andrew@databricks.com>:

"
Reynold Xin <rxin@databricks.com>,"Thu, 10 Sep 2015 15:28:52 -0700",Re: ClassCastException using DataFrame only when num-executors > 2 ...,Olivier Girardot <ssaboum@gmail.com>,"Does this still happen on 1.5.0 release?


:

tLong(rows.scala:41)
.scala:220)
la:85)
rojection.apply(Unknown
a:325)
a:252)
1
g
t
eduler.scala:1267)
cheduler.scala:1255)
cheduler.scala:1254)
la:59)
54)
pply(DAGScheduler.scala:684)
pply(DAGScheduler.scala:684)
.scala:684)
cheduler.scala:1480)
eduler.scala:1442)
eduler.scala:1431)
)
a:147)
a:108)
nge.scala:156)
nge.scala:141)
8)
t
etLong(rows.scala:41)
s.scala:220)
ala:85)
Projection.apply(Unknown
la:325)
la:252)
ndow.scala:265)
cala:272)
y(RDD.scala:706)
y(RDD.scala:706)
nsWithPreparationRDD.scala:46)
nsWithPreparationRDD.scala:46)
ala:88)
nsWithPreparationRDD.scala:46)
3)
1)
a:1145)
va:615)
e
la:325)
"
Reynold Xin <rxin@databricks.com>,"Thu, 10 Sep 2015 15:43:28 -0700",Re: Spark 1.5.x: Java files in src/main/scala and vice versa,Sean Owen <sowen@cloudera.com>,"There isn't really any difference I think where you put them. Did you run
into a problem?



"
StanZhai <mail@zhaishidan.cn>,"Thu, 10 Sep 2015 19:45:11 -0700 (MST)",Re: [SparkSQL]Could not alter table in Spark 1.5 use HiveContext,dev@spark.apache.org,"Thank you for the swift reply!

The version of my hive metastore server is 0.13.1, I've build spark use sbt
like this:
build/sbt -Pyarn -Phadoop-2.4 -Phive -Phive-thriftserver assembly

Is spark 1.5 bind the hive client version of 1.2 by default?



--

---------------------------------------------------------------------


"
Yin Huai <yhuai@databricks.com>,"Thu, 10 Sep 2015 21:29:55 -0700",Re: [SparkSQL]Could not alter table in Spark 1.5 use HiveContext,StanZhai <mail@zhaishidan.cn>,"Yes, Spark 1.5 use Hive 1.2's metastore client by default. You can change
it by putting the following settings in your spark conf.

spark.sql.hive.metastore.version = 0.13.1
spark.sql.hive.metastore.jars = maven or the path of your hive 0.13 jars
and hadoop jars

For spark.sql.hive.metastore.jars, basically, it tells spark sql where to
find metastore client's classes of Hive 0.13.1. If you set it to maven, we
will download needed jars directly (it is an easy way to do testing work).


"
Sean Owen <sowen@cloudera.com>,"Fri, 11 Sep 2015 06:30:27 +0200",Re: Spark 1.5.x: Java files in src/main/scala and vice versa,Reynold Xin <rxin@databricks.com>,"This is probably  true as the scala plugin actually compiles both
.scala and .java files. Still it seems like the wrong place just as a
matter of style. Can we try moving it and verify it's still OK?


---------------------------------------------------------------------


"
StanZhai <mail@zhaishidan.cn>,"Thu, 10 Sep 2015 23:29:25 -0700 (MST)",Re: [SparkSQL]Could not alter table in Spark 1.5 use HiveContext,dev@spark.apache.org,"Thanks a lot! I've fixed this issue by set: 
spark.sql.hive.metastore.version = 0.13.1
spark.sql.hive.metastore.jars = maven


Yin Huai-2 wrote











--

---------------------------------------------------------------------


"
Sandeep Giri <sandeep@knowbigdata.com>,"Fri, 11 Sep 2015 09:48:56 +0000",Re: MongoDB and Spark,"""Mishra, Abhishek"" <Abhishek.Mishra@xerox.com>, 
	""user@spark.apache.org"" <user@spark.apache.org>, ""dev@spark.apache.org"" <dev@spark.apache.org>","use map-reduce.


"
Sandeep Giri <sandeep@knowbigdata.com>,"Fri, 11 Sep 2015 19:04:35 +0530",Re: MongoDB and Spark,"""Mishra, Abhishek"" <Abhishek.Mishra@xerox.com>","I think it should be possible by loading collections as RDD and then doing
a union on them.

Regards,
Sandeep Giri,
+1 347 781 4573 (US)
+91-953-899-8962 (IN)

www.KnowBigData.com. <http://KnowBigData.com.>
Phone: +1-253-397-1945 (Office)

[image: linkedin icon] <https://linkedin.com/company/knowbigdata> [image:
other site icon] <http://knowbigdata.com>  [image: facebook icon]
<https://facebook.com/knowbigdata> [image: twitter icon]
<https://twitter.com/IKnowBigData> <https://twitter.com/IKnowBigData>


m

p
"
lonikar <lonikar@gmail.com>,"Fri, 11 Sep 2015 07:10:04 -0700 (MST)",Re: Spark 1.5.x: Java files in src/main/scala and vice versa,dev@spark.apache.org,"It does not cause any problem when building using maven. But when doing
eclipse:eclipse, the generated .classpath files contained only
<classpathentry kind=""src"" including=""**/*.java"" path=""src""/>. This caused
all the .scala sources to be ignored and caused all kinds of eclipse build
errors. It resolved only when I added prebuild jars in the java build path,
and it also prevented me from debugging spark code.

I understand eclipse:eclipse is not recommended way of creating eclipse
projects, but thats how I noticed this issue.

As sean said, its a matter of code organization, and its confusing to find
java files in src/main/scala. In my env, I moved the files and did not see
notice any issues. Unless there is any specific purpose, it will be better
if the code is reorganized.



--

---------------------------------------------------------------------


"
lonikar <lonikar@gmail.com>,"Fri, 11 Sep 2015 07:19:18 -0700 (MST)","Re: Spark 1.5: How to trigger expression execution through
 UnsafeRow/TungstenProject",dev@spark.apache.org,"thanks that worked 



--

---------------------------------------------------------------------


"
Corey Nolet <cjnolet@gmail.com>,"Fri, 11 Sep 2015 10:27:40 -0400",Re: MongoDB and Spark,Sandeep Giri <sandeep@knowbigdata.com>,"Unfortunately, MongoDB does not directly expose its locality via its client
API so the problem with trying to schedule Spark tasks against it is that
the tasks themselves cannot be scheduled locally on nodes containing query
results- which means you can only assume most results will be sent over the
network to the task that needs to process it. This is bad. The other reason
(which is also related to the issue of locality) is that I'm not sure if
there's an easy way to spread the results of a query over multiple
different clients- thus you'd probably have to start your Spark RDD with a
single partition and then repartition. What you've done at that point is
you've taken data from multiple mongodb nodes and you've collected them on
a single node just to re-partition them, again across the network, onto
multiple nodes. This is also bad.

I think this is the reason it was recommended to use MongoDB's mapreduce
because they can use their locality information internally. I had this same
issue w/ Couchbase a couple years back- it's unfortunate but it's the
reality.





g
lp
"
lonikar <lonikar@gmail.com>,"Fri, 11 Sep 2015 09:54:59 -0700 (MST)",Spark 1.5.0: setting up debug env,dev@spark.apache.org,"I have setup spark debug env on windows and mac, and thought its worth
sharing given some of the issues I encountered and the  instructions given
here
<https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools#UsefulDeveloperTools-Eclipse>  
did not work for *eclipse* (possibly outdated now). The first step ""sbt/sbt""
or ""build/sbt"" hangs in downloading sbt with the message ""Getting
org.scala-sbt sbt 0.13.7 ..."". I tried the alternative ""build/mvn
eclipse:eclipse"", but that too failed as the generated .classpath files
contained classpathentry only for java files.

1. Build spark using maven on command line. This will download all the
necessary jars from maven repos and speed up eclipse build. Maven 3.3.3 is
required. Spark ships with it. Just use build/mvn and ensure that there is
no ""mvn"" command in PATH (build/mvn -Pyarn -Phadoop-2.6
-Dhadoop.version=2.6.0 -DskipTests clean package).
2. Download latest scala-ide (4.1.1 as of now) from http://scala-ide.org
3. Check if the eclipse scala maven plugin is installed. If not, install it:
Help --> Install New Software -->
http://alchim31.free.fr/m2e-scala/update-site/ which is sourced from
https://github.com/sonatype/m2eclipse-scala.
4. If using scala 2.10, add installation 2.10.4. If you build spark using
steps in  described here
<http://spark.apache.org/docs/latest/building-spark.html>  , (build/mvn
-Pyarn -Phadoop-2.6 -Dhadoop.version=2.6.0 -DskipTests clean package), it
gets installed in build/scala-2.10.4. In Eclipse Preferences -> Scala ->
Installations -> Add, specify the <spark-dir>/build/scala-2.10.4/lib.
5. In Eclipse -> Project, disable Build Automatically. This is to avoid
building projects till all projects are imported and some settings are
changed. Otherwise, eclipse takes up hours building projects while in half
baked state.
6. In Eclipse -> Preferences -> Java -> Compiler -> Errors/Warnings -->
Deprecated and Restricted API, change the setting to Warning from earlier
Error. This is to take care of Unsafe classes for project tungsten.
7. Import maven projects: In eclipse, File --> Import --> Maven --> Existing
Maven Projects (*not General --> Existing projects in workspace*).
8. After the projects are completely imported, select all projects except
java8-tests_2.10, spark-assembly_2.10, spark-parent_2.10, right click and
choose Scala -> Set the Scala Installation. Choose 2.10.4. This step is also 
described here
<https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools#UsefulDeveloperTools-Eclipse> 
. It does not work for some projects. Right click on each project,
Properties -> Scala Compiler -> Check Use Project Settings, Select Scala
Installation as scala 2.10.4 and click OK.
9. Some projects will give error ""Plugin execution not covered by lifecycle
configuration"" when building. The issue is  described here
<http://stackoverflow.com/questions/6352208/how-to-solve-plugin-execution-not-covered-by-lifecycle-configuration-for-sprin> 
. The pom.xml of those projects will need <pluginManagement> ...
</pluginManagement> around the <plugins> like below:

 The projects which need this change are spark-streaming-flume-sink_2.10
(external/flume-sink/pom.xml), spark-repl_2.10 (repl/pom.xml),
spark-sql_2.10 (sql/pom.xml), spark-hive_2.10 (sql/hive/pom.xml),
spark-hive-thriftserver_2.10 (sql/hive-thriftserver_2.10/pom.xml),
spark-unsafe_2.10 (unsafe/pom.xml).
10. Right click on project spark-streaming-flume-sink_2.10, Properties ->
Java Build Path -> Source -> Add Folder. Navigate to target -> scala-2.10 ->
src_managed -> main -> compiled_avro. Check the checkbox and click OK.
11. Now enable Project -> Build Automatically. Sit back and relax. If build
fails for some projects (SBT crashes sometimes), just select those, Project
-> Clean -> Clean selected projects.
12. After the build completes (hopefully without any errors), run/debug an
example from spark-examples_2.10. You should be able to put breakpoints in
spark code and debug. You may have to change source of examples to add
*/.setMaster(""local"")/* on the */val sparkConf/* line. After this minor
change, it will work. Also, the first time you debug, it will ask you
specify source path. Just select Add -> Java Project -> select all spark
projects. Let the first debugging session complete as will not show any
spark code. You may disable breakpoints in this session to let it go.
Subsequent sessions allow you to walk through step by step in spark code.
Enjoy  

You may not have to go through all this if using scala 2.11 or IntelliJ. But
if you are like me, who uses eclipse and also the spark's current scala
2.10.4, you will find this useful and avoid a lot of googling 

The one issue I encountered is debugging/setting breakpoints in expression
generated java code. This code generated as string in spark-catalyst_2.10
--> org.apache.spark.sql.catalyst.expressions and
org.apache.spark.sql.catalyst.expressions.codegen. If anyone has figured out
how to do it, please update on this thread.



--

---------------------------------------------------------------------


"
Ryan Williams <ryan.blake.williams@gmail.com>,"Fri, 11 Sep 2015 17:21:49 +0000",Re: [ANNOUNCE] Announcing Spark 1.5.0,,"Any idea why 1.5.0 is not in Maven central yet
<http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22org.apache.spark%22>? Is
that a separate release process?


"
Ted Yu <yuzhihong@gmail.com>,"Fri, 11 Sep 2015 10:23:58 -0700",Re: [ANNOUNCE] Announcing Spark 1.5.0,Ryan Williams <ryan.blake.williams@gmail.com>,"This is related:
https://issues.apache.org/jira/browse/SPARK-10557


"
Reynold Xin <rxin@databricks.com>,"Fri, 11 Sep 2015 10:24:56 -0700",Re: [ANNOUNCE] Announcing Spark 1.5.0,Ryan Williams <ryan.blake.williams@gmail.com>,"It is already there, but the search is not updated. Not sure what's going
on with maven central search.


http://repo1.maven.org/maven2/org/apache/spark/spark-parent_2.10/1.5.0/




"
Renyi Xiong <renyixiong0@gmail.com>,"Fri, 11 Sep 2015 10:54:33 -0700",Re: SparkR driver side JNI,shivaram@eecs.berkeley.edu,"forgot to reply all.

I see. but what prevents e.g. R driver getting those command line arguments
from spark-submit and setting them with SparkConf to R diver's
in-process JVM through JNI?


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Fri, 11 Sep 2015 11:10:40 -0700",Re: SparkR driver side JNI,Renyi Xiong <renyixiong0@gmail.com>,"Its possible -- in the sense that a lot of designs are possible. But
AFAIK there are no clean interfaces for getting all the arguments /
SparkConf options from spark-submit and its all the more tricker to
handle scenarios where the first JVM has already created a
SparkContext that you want to use from R. The inter-process
communication is cleaner, pretty lightweight and handles all the
scenarios.

Thanks
Shivaram


---------------------------------------------------------------------


"
Jonathan Kelly <jonathakamzn@gmail.com>,"Fri, 11 Sep 2015 11:16:35 -0700",Re: [ANNOUNCE] Announcing Spark 1.5.0,Reynold Xin <rxin@databricks.com>,"I just clicked the
http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22org.apache.spark%22 link
provided above by Ryan, and I see 1.5.0. Was this just fixed within the
past hour, or is some caching causing some people not to see it?


"
Ryan Williams <ryan.blake.williams@gmail.com>,"Fri, 11 Sep 2015 18:27:17 +0000",Re: [ANNOUNCE] Announcing Spark 1.5.0,"Jonathan Kelly <jonathakamzn@gmail.com>, Reynold Xin <rxin@databricks.com>","yea, wfm now, thanks!


"
Kevin Chen <kchen@palantir.com>,"Fri, 11 Sep 2015 18:30:31 +0000",New Spark json endpoints,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hello Spark Devs,

 I noticed that [SPARK-3454], which introduces new json endpoints at
/api/v1/[path] for information previously only shown on the web UI, does not
expose several useful properties about Spark jobs that are exposed on the
web UI and on the unofficial /json endpoint.

 Specific examples include the maximum number of allotted cores per
application, amount of memory allotted to each slave, and number of cores
used by each worker. These are provided at â€˜app.cores, app.memoryperslave,
and worker.coresusedâ€™ in the /json endpoint, and also all appear on the web
UI page.

 Is there any specific reason that these fields are not exposed in the
public API? If not, would it be reasonable to add them to the json blobs,
possibly in a future /api/v2 API?

Thank you,
Kevin Chen



"
Renyi Xiong <renyixiong0@gmail.com>,"Fri, 11 Sep 2015 12:06:02 -0700",Re: SparkR driver side JNI,shivaram@eecs.berkeley.edu,"got it! thanks a lot.


"
Rachana Srivastava <Rachana.Srivastava@markmonitor.com>,"Fri, 11 Sep 2015 19:09:56 +0000",New JavaRDD Inside JavaPairDStream,"""user@spark.apache.org"" <user@spark.apache.org>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","Hello all,

Can we invoke JavaRDD while processing stream from Kafka for example.  Following code is throwing some serialization exception.  Not sure if this is feasible.

  JavaStreamingContext jssc = new JavaStreamingContext(jsc, Durations.seconds(5));
    JavaPairReceiverInputDStream<String, String> messages = KafkaUtils.createStream(jssc, zkQuorum, group, topicMap);
    JavaDStream<String> lines = messages.map(new Function<Tuple2<String, String>, String>() {
      public String call(Tuple2<String, String> tuple2) { return tuple2._2();
      }
    });
    JavaPairDStream<String, String> wordCounts = lines.mapToPair( new PairFunction<String, String, String>() {
        public Tuple2<String, String> call(String urlString) {
                        String propertiesFile = ""/home/cloudera/Desktop/sample/input/featurelist.properties"";
                        JavaRDD<String> propertiesFileRDD = jsc.textFile(propertiesFile);
                          JavaPairRDD<String, String> featureKeyClassPair = propertiesFileRDD.mapToPair(
                                      new PairFunction<String, String, String>() {
                                                  public Tuple2<String, String> call(String property) {
                                                    return new Tuple2(property.split(""="")[0], property.split(""="")[1]);
                                                  }
                                     });
                                    featureKeyClassPair.count();
          return new Tuple2<String, String>(urlString,  featureScore);
        }
      });

"
Cody Koeninger <cody@koeninger.org>,"Fri, 11 Sep 2015 14:43:14 -0500",Re: New JavaRDD Inside JavaPairDStream,Rachana Srivastava <Rachana.Srivastava@markmonitor.com>,"No, in general you can't make new RDDs in code running on the executors.

It looks like your properties file is a constant, why not process it at the
beginning of the job and broadcast the result?


"
Olivier Toupin <olivier.toupin@gmail.com>,"Fri, 11 Sep 2015 14:42:09 -0700 (MST)",Re: Concurrency issue in SQLExecution.withNewExecutionId,dev@spark.apache.org,"@Andrew_Or-2 I am using Scala futures.



--

---------------------------------------------------------------------


"
"""Varadhan, Jawahar"" <varadhan@yahoo.com.INVALID>","Fri, 11 Sep 2015 22:02:21 +0000 (UTC)","SIGTERM 15 Issue  : Spark Streaming for ingesting huge text files
 using custom Receiver","Dev <dev@spark.apache.org>, User <user@spark.apache.org>","Hi all,Â  Â I have a coded a custom receiver which receives kafka messages. These Kafka messages have FTP server credentials in them. The receiver then opens the message and uses the ftp credentials in it Â to connect to the ftp server. It then streams this huge text file (3.3G) . Finally this stream it read line by line using buffered reader and pushed to the spark streaming via the receiver's ""store"" method. Spark streaming process receives all these lines and stores it in hdfs.
With this process I could ingest small files (50 mb) but cant ingest this 3.3gb file. Â I get a YARN exception of SIGTERM 15 in spark streaming process. Also, I tried going to that 3.3GB file directly (without custom receiver) in spark streaming using ssc.textFileStream Â and everything works fine and that file ends in HDFS
Please let me know what I might have to do to get this working with receiver. I know there are better ways to ingest the file but we need to use Spark streaming in our case.
Thanks."
Rachana Srivastava <Rachana.Srivastava@markmonitor.com>,"Sat, 12 Sep 2015 03:07:28 +0000",Multithreaded vs Spark Executor,"""user@spark.apache.org"" <user@spark.apache.org>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","Hello all,

We are getting stream of input data from a Kafka queue using Spark Streaming API.  For each data element we want to run parallel threads to process a set of feature lists (nearly 100 feature or more).    Since feature lists creation is independent of each other we would like to execute these feature lists in parallel on the input data that we get from the Kafka queue.

Question is

1. Should we write thread pool and manage these features execution on diffeconfined to the node that is assigned to the input data from the Kafka stream we cannot leverage distributed nodes for processing of these features for a single input data.

2.  Or since we are using JavaRDD as a feature list, these feature execution will be managed internally by Spark executors.

Thanks,

Rachana
"
=?UTF-8?B?SsO2cm4gRnJhbmtl?= <jornfranke@gmail.com>,"Sat, 12 Sep 2015 07:32:57 +0000","Re: SIGTERM 15 Issue : Spark Streaming for ingesting huge text files
 using custom Receiver","""Varadhan, Jawahar"" <varadhan@yahoo.com>, Dev <dev@spark.apache.org>, 
	User <user@spark.apache.org>","I am not sure what are you trying to achieve here. Have you thought about
using flume? Additionally maybe something like rsync?

Le sam. 12 sept. 2015 Ã  0:02, Varadhan, Jawahar <varadhan@yahoo.com.invalid>
a Ã©crit :

ns
it
ks
"
Nick Pentreath <nick.pentreath@gmail.com>,"Sat, 12 Sep 2015 10:07:28 +0200",Re: HyperLogLogUDT,"""dev@spark.apache.org"" <dev@spark.apache.org>","Inspired by this post:
http://eugenezhulenev.com/blog/2015/07/15/interactive-audience-analytics-with-spark-and-hyperloglog/,
I've started putting together something based on the Spark 1.5 UDAF
interface: https://gist.github.com/MLnick/eca566604f2e4e3c6141

Some questions -

1. How do I get the UDAF to accept input arguments of different type? We
can hash anything basically for HLL - Int, Long, String, Object, raw bytes
etc. Right now it seems we'd need to build a new UDAF for each input type,
which seems strange - I should be able to use one UDAF that can handle raw
input of different types, as well as handle existing HLLs that can be
merged/aggregated (e.g. for grouped data)
2. @Reynold, how would I ensure this works for Tungsten (ie against raw
bytes in memory)? Or does the new Aggregate2 stuff automatically do that?
Where should I look for examples on how this works internally?
3. I've based this on the Sum and Avg examples for the new UDAF interface -
any suggestions or issue please advise. Is the intermediate buffer
efficient?
4. The current HyperLogLogUDT is private - so I've had to make my own one
which is a bit pointless as it's copy-pasted. Any thoughts on exposing that
type? Or I need to make the package spark.sql ...

Nick


er
ache/spark/rdd/RDD.scala#L1153> and
m
 to
e
y
ble to
 adapt
as
"" for
"
=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@questtec.nl>,"Sat, 12 Sep 2015 10:45:38 +0200",Re: HyperLogLogUDT,Nick Pentreath <nick.pentreath@gmail.com>,"Hello Nick,

I have been working on a (UDT-less) implementation of HLL++. You can find
the PR here: https://github.com/apache/spark/pull/8362. This current
implements the dense version of HLL++, which is a further development of
HLL. It returns a Long, but it shouldn't be to hard to return a Row
containing the cardinality and/or the HLL registers (the binary data).

I am curious what the stance is on using UDTs in the new UDAF interface. Is
this still viable? This wouldn't work with UnsafeRow for instance. The
OpenHashSetUDT for instance would be a nice building block for CollectSet
and all Distinct Aggregate operators. Are there any opinions on this?

Kind regards,

Herman van HÃ¶vell tot Westerflier

QuestTec B.V.
Torenwacht 98
2353 DC Leiderdorp
hvanhovell@questtec.nl
+599 9 521 4402


2015-09-12 10:07 GMT+02:00 Nick Pentreath <nick.pentreath@gmail.com>:

with-spark-and-hyperloglog/,
s
,
w
at
f
her
l
m
pache/spark/rdd/RDD.scala#L1153> and
y to
ue
ry
able to
e adapt
 as
y
d"" for
"
Nick Pentreath <nick.pentreath@gmail.com>,"Sat, 12 Sep 2015 11:05:01 +0200",Re: HyperLogLogUDT,"""dev@spark.apache.org"" <dev@spark.apache.org>","Can I ask why you've done this as a custom implementation rather than using
StreamLib, which is already implemented and widely used? It seems more
portable to me to use a library - for example, I'd like to export the
grouped data with raw HLLs to say Elasticsearch, and then do further
on-demand aggregation in ES and visualization in Kibana etc.

Others may want to do something similar into Hive, Cassandra, HBase or
whatever they are using. In this case they'd need to use this particular
implementation from Spark which may be tricky to include in a dependency
etc.

If there are enhancements, does it not make sense to do a PR to StreamLib?
Or does this interact in some better way with Tungsten?

I am unclear on how the interop with Tungsten raw memory works - some
pointers on that and where to look in the Spark code would be helpful.


-with-spark-and-hyperloglog/,
es
e,
aw
?
e
e
hat
tion
s.
apache/spark/rdd/RDD.scala#L1153> and
g
ty to
que
ery
 able to
e
I
ne adapt
d as
ay
ld"" for
"
Fengdong Yu <fengdongy@everstring.com>,"Sat, 12 Sep 2015 17:05:43 +0800","=?utf-8?Q?spark_dataframe_transform_JSON_to_ORC_meet_=E2=80=9Cco?=
 =?utf-8?Q?lumn_ambigous_exception=E2=80=9D?=",dev@spark.apache.org,"Hi,

I am using spark1.4.1 data frame, read JSON data, then save it to orc. the code is very simple:

DataFrame json = sqlContext.read().json(input);

json.write().format(""orc"").save(output);

the job failed. what's wrong with this exception? Thanks.

Exception in thread ""main"" org.apache.spark.sql.AnalysisException: Reference 'Canonical_URL' is ambiguous, could be: Canonical_URL#960, Canonical_URL#1010.; at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolve(LogicalPlan.scala:279) at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveChildren(LogicalPlan.scala:116) at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$8$$anonfun$applyOrElse$4$$anonfun$16.apply(Analyzer.scala:350) at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$8$$anonfun$applyOrElse$4$$anonfun$16.apply(Analyzer.scala:350) at org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:48) at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$8$$anonfun$applyOrElse$4.applyOrElse(Analyzer.scala:350) at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$8$$anonfun$applyOrElse$4.applyOrElse(Analyzer.scala:341) at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:286) at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:286) at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51) at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:285) at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$transformExpressionUp$1(QueryPlan.scala:108) at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2$$anonfun$apply$2.apply(QueryPlan.scala:123) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244) at scala.collection.immutable.List.foreach(List.scala:318) at scala.collection.TraversableLike$class.map(TraversableLike.scala:244) at scala.collection.AbstractTraversable.map(Traversable.scala:105) at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:122) at scala.collection.Iterator$$anon$11.next(Iterator.scala:328) at scala.collection.Iterator$class.foreach(Iterator.scala:727) at scala.collection.AbstractIterator.foreach(Iterator.scala:1157) at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48) at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103) at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47) at scala.collection.AbstractIterator.to(Iterator.scala:1157) at  at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157) at at scala.collection.AbstractIterator.toArray(Iterator.scala:1157) at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:127) at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$8.applyOrElse(Analyzer.scala:341) at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$8.applyOrElse(Analyzer.scala:243) at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:286) at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:286) at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51) at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:285) at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:243) at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:242) at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:61) at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:59) at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:111) at scala.collection.immutable.List.foldLeft(List.scala:84) at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:59) at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:51) at scala.collection.immutable.List.foreach(List.scala:318) at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:51) at org.apache.spark.sql.SQLContext$QueryExecution.analyzed$lzycompute(SQLContext.scala:933) at org.apache.spark.sql.SQLContext$QueryExecution.analyzed(SQLContext.scala:933) at org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:931) at org.apache.spark.sql.DataFrame.(DataFrame.scala:131) at org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:51) at org.apache.spark.sql.sources.InsertIntoHadoopFsRelation.run(commands.scala:132) at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:57) at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:57) at org.apache.spark.sql.execution.ExecutedCommand.doExecute(commands.scala:68) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:88) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:88) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147) at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:87) at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:950) at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:950) at org.apache.spark.sql.sources.ResolvedDataSource$.apply(ddl.scala:336) at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:144) at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:135) at com.es.infrastructure.spark.orc.transformer.JsonTransformer.run(JsonTransformer.java:22) at Main.main(Main.java:70) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665) at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170) at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193) at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112) at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)


"
Nick Pentreath <nick.pentreath@gmail.com>,"Sat, 12 Sep 2015 11:06:30 +0200",Re: HyperLogLogUDT,"""dev@spark.apache.org"" <dev@spark.apache.org>","I should add that surely the idea behind UDT is exactly that it can (a) fit
automatically into DFs and Tungsten and (b) that it can be used efficiently
in writing ones own UDTs and UDAFs?



e
?
<
d
e
t
s-with-spark-and-hyperloglog/,
e
tes
pe,
raw
t?
ing
:
ution
ss.
a
/apache/spark/rdd/RDD.scala#L1153> and
ity to
ique
very
e able to
he
d one
 a field
y play
l
eld"" for
d
"
Sean Owen <sowen@cloudera.com>,"Sat, 12 Sep 2015 10:42:32 +0100",Re: Spark 1.5.x: Java files in src/main/scala and vice versa,lonikar <lonikar@gmail.com>,"There are actually 33 instances of a Java file in src/main/scala -- I
opened https://issues.apache.org/jira/browse/SPARK-10576 to track a
discussion and decision.


---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Sat, 12 Sep 2015 02:52:03 -0700","=?UTF-8?Q?Re=3A_spark_dataframe_transform_JSON_to_ORC_meet_=E2=80=9Cco?=
	=?UTF-8?Q?lumn_ambigous_exception=E2=80=9D?=",Fengdong Yu <fengdongy@everstring.com>,"Is it possible that Canonical_URL occurs more than once in your json ?

Can you check your json input ?

Thanks


"
Fengdong Yu <fengdongy@everstring.com>,"Sat, 12 Sep 2015 18:40:23 +0800","=?UTF-8?Q?Re=3A_spark_dataframe_transform_JSON_to_ORC_meet_=E2=80=9Cco?=
	=?UTF-8?Q?lumn_ambigous_exception=E2=80=9D?=",Ted Yu <yuzhihong@gmail.com>,"Hi Ted,
I checked the JSON, there aren't duplicated key in JSON.


Azuryy Yu
Sr. Infrastructure Engineer

cel: 158-0164-9103
wetchat: azuryy



"
kiran lonikar <lonikar@gmail.com>,"Sat, 12 Sep 2015 18:29:54 +0530",Re: Code generation for GPU,Paul Wais <paulwais@gmail.com>,"Thanks. Yes thats exactly what i would like to do: copy large amounts of
data to GPU RAM, perform computation and get bulk rows back for map/filter
or reduce result. It is true that non trivial operations benefit more. Even
streaming data to GPU RAM and interleaving computation with data transfer
works but it complicates the design and doing it in spark would be even
more so.

Thanks for bringing out the sorting. Its a good idea since its already
isolated as you pointed out. I was looking at the terasort effort and
something I always wanted to take up. But somehow thought expression would
be easier to deal with in a short term. Would love to work on that after
this especially because unsafe is for primitive types and suited for GPUs
computation model. It would be exciting to better the terasort record too.

Kiran

"
kiran lonikar <lonikar@gmail.com>,"Sat, 12 Sep 2015 18:36:06 +0530",Re: Code generation for GPU,Steve Loughran <stevel@hortonworks.com>,"Thanks for pointing to the yarn JIRA. For now, it would be good for my talk
since it brings out that hadoop and big data community is already aware of
the GPUs and making effort to exploit it.

Good luck for your talk. That fear is lurking in my mind too :)

"
Ted Yu <yuzhihong@gmail.com>,"Sat, 12 Sep 2015 07:19:21 -0700","=?UTF-8?Q?Re=3A_spark_dataframe_transform_JSON_to_ORC_meet_=E2=80=9Cco?=
	=?UTF-8?Q?lumn_ambigous_exception=E2=80=9D?=",Fengdong Yu <fengdongy@everstring.com>,"Can you take a look at SPARK-5278 where ambiguity is shown between field
names which differ only by case ?

Cheers


"
Cazen Lee <cazen.lee@gmail.com>,"Sun, 13 Sep 2015 00:07:01 +0900",[Question] ORC - EMRFS Problem,dev@spark.apache.org,"Good Day!

I think there are some problems between ORC and AWS EMRFS.

When I was trying to read ""upper 150M"" ORC files from S3, ArrayOutOfIndex Exception occured.

I'm sure that it's AWS side issue because there was no exception when trying from HDFS or S3NativeFileSystem.

Parquet runs ordinarily but it's inconvenience(Almost our system runs based on ORC)

Does anybody knows about this issue?

I've tried spark 1.4.1(EMR 4.0.0) and there are no 1.5 patch-note about this

Thank You
---------------------------------------------------------------------


"
=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@questtec.nl>,"Sat, 12 Sep 2015 18:06:06 +0200",Re: HyperLogLogUDT,Nick Pentreath <nick.pentreath@gmail.com>,"I am typically all for code re-use. The reason for writing this is to
prevent the indirection of a UDT and work directly against memory. A UDT
will work fine at the moment because we still use
GenericMutableRow/SpecificMutableRow as aggregation buffers. However if you
would use an UnsafeRow as an AggregationBuffer (which is attractive when
you have a lot of groups during aggregation) the use of an UDT is either
impossible or it would become very slow because it would require us to
deserialize/serialize a UDT on every update.

As for compatibility, the implementation produces exactly the same results
as the ClearSpring implementation. You could easily export the HLL++
register values to the current ClearSpring implementation and export those.

Met vriendelijke groet/Kind regards,

Herman van HÃ¶vell tot Westerflier

QuestTec B.V.
Torenwacht 98
2353 DC Leiderdorp
hvanhovell@questtec.nl
+599 9 521 4402


2015-09-12 11:06 GMT+02:00 Nick Pentreath <nick.pentreath@gmail.com>:

m
he
 <
.
he
et
cs-with-spark-and-hyperloglog/,
w
nput
hat
w
at?
e
sing
n
y
cution
ess.
t
g/apache/spark/rdd/RDD.scala#L1153> and
lity to
nique
 very
be able to
the
?
ld one
s a field
ly play
ll
ield"" for
.
"
Priya Ch <learnings.chitturi@gmail.com>,"Sat, 12 Sep 2015 23:04:33 +0530",Spark Streaming..Exception,"""user@spark.apache.org"" <user@spark.apache.org>, dev@spark.apache.org","Hello All,

 When I push messages into kafka and read into streaming application, I see
the following exception-
 I am running the application on YARN and no where broadcasting the message
within the application. Just simply reading message, parsing it and
populating fields in a class and then printing the dstream (using
DStream.print).

 Have no clue if this is cluster issue or spark version issue or node
issue. The strange part is, sometimes the message is processed but
sometimes I see the below exception -

java.io.IOException: org.apache.spark.SparkException: Failed to get
broadcast_5_piece0 of broadcast_5
        at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1155)
        at
org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:164)
        at
org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:64)
        at
org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:64)
        at
org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:87)
        at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)
        at
org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:58)
        at org.apache.spark.scheduler.Task.run(Task.scala:64)
        at
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
        at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Failed to get
broadcast_5_piece0 of broadcast_5
        at
org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1$$anonfun$2.apply(TorrentBroadcast.scala:137)
        at
org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1$$anonfun$2.apply(TorrentBroadcast.scala:137)
        at scala.Option.getOrElse(Option.scala:120)
        at
org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply$mcVI$sp(TorrentBroadcast.scala:136)
        at
org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:119)
        at
org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:119)
        at scala.collection.immutable.List.foreach(List.scala:318)
        at org.apache.spark.broadcast.TorrentBroadcast.org
<http://org.apache.spark.broadcast.torrentbroadcast.org/>
$apache$spark$broadcast$TorrentBroadcast$$readBlocks(TorrentBroadcast.scala:119)
        at
org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:174)
        at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1152)


I would be glad if someone can throw some light on this.

Thanks,
Padma Ch
"
Nick Pentreath <nick.pentreath@gmail.com>,"Sat, 12 Sep 2015 19:40:12 +0200",Re: HyperLogLogUDT,"""dev@spark.apache.org"" <dev@spark.apache.org>","Ok, that makes sense. So this is (a) more efficient, since as far as I can
see it is updating the HLL registers directly in the buffer for each value,
and (b) would be ""Tungsten-compatible"" as it can work against UnsafeRow? Is
it currently possible to specify an UnsafeRow as a buffer in a UDAF?

So is extending AggregateFunction2 the preferred approach over the
UserDefinedAggregationFunction interface? Or it is that internal only?

I see one of the main use cases for things like HLL / CMS and other
approximate data structure being the fact that you can store them as
columns representing distinct counts in an aggregation. And then do further
arbitrary aggregations on that data as required. e.g. store hourly
aggregate data, and compute daily or monthly aggregates from that, while
still keeping the ability to have distinct counts on certain fields.

So exposing the serialized HLL as Array[Byte] say, so that it can be
further aggregated in a later DF operation, or saved to an external data
source, would be super useful.




ou
s
e.
the
r
y
r <
ck
nions
ics-with-spark-and-hyperloglog/,
aw
input
that
do
te
osing
 in
sten
memory
e
rg/apache/spark/rdd/RDD.scala#L1153> and
ility to
y
unique
o very
 be able to
 the
 so could
HLL as a
matically
hen call
field"" for
..
"
Yin Huai <yhuai@databricks.com>,"Sat, 12 Sep 2015 15:09:24 -0700",Re: HyperLogLogUDT,Nick Pentreath <nick.pentreath@gmail.com>,"Hi Nick,

The buffer exposed to UDAF interface is just a view of underlying buffer
(this underlying buffer is shared by different aggregate functions and
every function takes one or multiple slots). If you need a UDAF, extending
UserDefinedAggregationFunction is the preferred
approach. AggregateFunction2 is used for built-in aggregate function.

Thanks,

Yin


n
e,
Is
er
you
rt
s
 the
ar
cy
er <
r
ock
inions
tics-with-spark-and-hyperloglog/,
?
raw
 input
n
 that
 do
ate
n
posing
s in
gsten
 memory
org/apache/spark/rdd/RDD.scala#L1153> and
o
bility to
say unique
so very
l be able to
taining the
- so could
 HLL as a
omatically
then call
 field"" for
"
Reynold Xin <rxin@databricks.com>,"Sat, 12 Sep 2015 21:45:03 -0700",Re: Spark 1.5.x: Java files in src/main/scala and vice versa,Sean Owen <sowen@cloudera.com>,"Most these files are just package-info.java there for having a good package
index for JavaDoc. If we move them, we will need to create a folder in the
java one for each package that exposes any documentation. And it is very
likely we will forget to update package-info.java when we update
package.scala if the two files are far apart.

Doesn't seem that big of a deal to have them in Scala folder, and the
benefits ain't that big either.



"
"""Nick Pentreath"" <nick.pentreath@gmail.com>","Sat, 12 Sep 2015 23:01:14 -0700 (PDT)",Re: HyperLogLogUDT,"""dev"" <dev@spark.apache.org>","Thanks Yin




So how does one ensure a UDAF works with Tungsten and UnsafeRow buffers? Or is this something that will be included in the UDAF interface in future?Â 




Is there a performance difference between Extending UDAF vs Aggregate2?




It's also not clear to me how to handle inputs of different types? What if my UDAF can handle String and Long for example? Do I need to specify AnyType or is there a way to specify multiple types possible for a single input column?




If no performance difference and UDAF can work with Tungsten, then Herman does it perhaps make sense to use UDAF (but without a UDT as you've done for performance)? As it would then be easy to extend that UDAF and adjust the output types as needed. It also provides a really nice example of how to use the interface for something advanced and high performance.



â€”
Sent from Mailbox


extending
com>
can
value,
UnsafeRow? Is
only?
further
while
data
<
UDT
 you
when
either
export
(a)
than
seems
export the
or
particular
dependency
Tungsten?
some
.
Westerflier <
can
to
(the
 for
block
opinions
com>:
ytics-with-spark-and-hyperloglog/,
UDAF
type?
 raw
each input
can
HLLs that
against
automatically do
internally?
intermediate
own
exposing
bytes in
Tungsten
some memory
about
could
0/core/src/main/scala/org/apache/spark/rdd/RDD.scala#L1153> and
is
data
do
ability to
grouped
 say unique
also very
still be able to
field containing the
SerDe - so could
serialized HLL as a
automatically
 then call
 ""default field"" for
approximate"
=?GBK?B?va/B1g==?= <yellowrosehao321@163.com>,"Mon, 14 Sep 2015 09:43:45 +0800 (CST)",(send this email to subscribe),dev@spark.apache.org,"Hi,I need subscribe email list,please send me,thank you"
Ted Yu <yuzhihong@gmail.com>,"Sun, 13 Sep 2015 18:57:38 -0700",Re: (send this email to subscribe),=?utf-8?B?6JKL5p6X?= <yellowrosehao321@163.com>,"See first section of http://spark.apache.org/community.html

Cheers



"
Yin Huai <yhuai@databricks.com>,"Sun, 13 Sep 2015 21:13:09 -0700",Re: HyperLogLogUDT,Nick Pentreath <nick.pentreath@gmail.com>,"The user implementing a UDAF does not need to consider what is the
underlying buffer. Our aggregate operator will figure out if the buffer
data types of all aggregate functions used by a query are supported by the
UnsafeRow. If so, we will use the UnsafeRow as the buffer.

Regarding the performance, UDAF is not as efficient as out built-in
aggregate functions mainly because (1) users implement UDAFs with JVM data
types not SQL data types (e.g. in a UDAF you will use String not
UTF8String, which is our SQL data type) (2) UDAF does not support
code-generation.

For handling different data types for an argument, having multiple UDAF
classes is the way for now. We will consider what will be the right way to
support specifying multiple possible data types for an argument.

Thanks,

Yin


e?
f
ng
h
 in
ther
e
a
 <
DT
f you
en
er
port
ed
n
ems
rt the
ticular
ency
e
l.
lier <
n
to
he
for
block
opinions
lytics-with-spark-and-hyperloglog/,
F
Object,
r each
 that can
Ls that
t
ly do
y?
diate
 on
tes in
ungsten
me memory
t
d
a/org/apache/spark/rdd/RDD.scala#L1153> and
s
ne to do
 ability to
d
f say unique
also very
ill be able to
ontaining the
e - so could
ed HLL as a
utomatically
o then call
lt field"" for
e
"
Prashant Sharma <scrapcodes@gmail.com>,"Mon, 14 Sep 2015 11:48:36 +0530",An alternate UI for Spark.,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,

TLDR;
Some of my colleagues at Imaginea are interested in building an alternate
UI for Spark. Basically allow people or groups to build an alternate UI for
Spark.

More Details:
Looking at feasibility, it feels definitely possible to do. But we need a
consensus on a public(can be experimental initially ) interface which would
give access to UI in core. Given this is done, their job will be easy.

Infact, it opens up a lot of possibilities for alternate UI for Apache
spark. Also considering a pluggable UI - where alternate UI can just be a
plugin. Ofcourse, implementing later can be a long term goal. Elasticsearch
is a good example of the later approach.

My knowledge on this is certainly limited. Comments and criticism
appreciated.

Thanks,
Prashant
"
Ryan Williams <ryan.blake.williams@gmail.com>,"Mon, 14 Sep 2015 06:23:51 +0000",Re: An alternate UI for Spark.,"Prashant Sharma <scrapcodes@gmail.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","You can check out Spree <https://github.com/hammerlab/spree> for one data
point about how this can be done; it is a near-clone of the Spark web UI
that updates in real-time.

It uses JsonRelay <https://github.com/hammerlab/spark-json-relay>, a
SparkListener that sends events as JSON over the network; it receives those
events, aggregates stats similar to the JobProgressListener and writes
those to Mongo in slim <https://github.com/hammerlab/slim>, and then Spree
uses Meteor to display a real-time web UI based on the data in Mongo.


"
Priya Ch <learnings.chitturi@gmail.com>,"Mon, 14 Sep 2015 14:28:45 +0530",Re: Spark Streaming..Exception,"""user@spark.apache.org"" <user@spark.apache.org>, dev@spark.apache.org","Hi All,

 I came across the related old conversation on the above issue (
https://issues.apache.org/jira/browse/SPARK-5594. ) Is the issue fixed? I
tried different values for spark.cleaner.ttl  -> 0sec, -1sec,
2000sec,..none of them worked. I also tried setting
spark.streaming.unpersist -> true. What is the possible solution for this ?
Is this a bug in Spark 1.3.0? Changing the scheduling mode to Stand-alone
or Mesos mode would work fine ??

Please someone share your views on this.


"
Pete Robbins <robbinspg@gmail.com>,"Mon, 14 Sep 2015 12:16:11 +0100",Unable to acquire memory errors in HiveCompatibilitySuite,Dev <dev@spark.apache.org>,"I keep hitting errors running the tests on 1.5 such as


- join31 *** FAILED ***
  Failed to execute query using catalyst:
  Error: Job aborted due to stage failure: Task 9 in stage 3653.0 failed 1
times, most recent failure: Lost task 9.0 in stage 3653.0 (TID 123363,
localhost): java.io.IOException: Unable to acquire 4194304 bytes of memory
      at
org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPage(UnsafeExternalSorter.java:368)


This is using the command
build/mvn -Pyarn -Phadoop-2.2 -Phive -Phive-thriftserver  test


I don't see these errors in any of the amplab jenkins builds. Do those
builds have any configuration/environment that I may be missing? My build
is running with whatever defaults are in the top level pom.xml, eg -Xmx3G.

I can make these tests pass by setting spark.shuffle.memoryFraction=0.6 in
the HiveCompatibilitySuite rather than the default 0.2 value.

Trying to analyze what is going on with the test it is related to the
number of active tasks, which seems to rise to 32, and so the
ShuffleMemoryManager allows less memory per task even though most of those
tasks do not have any memory allocated to them.

Has anyone seen issues like this before?
"
Akhil Das <akhil@sigmoidanalytics.com>,"Mon, 14 Sep 2015 17:00:36 +0530",Re: Spark Streaming..Exception,Priya Ch <learnings.chitturi@gmail.com>,"You should consider upgrading your spark from 1.3.0 to a higher version.

Thanks
Best Regards


"
Rachana Srivastava <Rachana.Srivastava@markmonitor.com>,"Mon, 14 Sep 2015 16:54:25 +0000",JavaRDD using Reflection,"""user@spark.apache.org"" <user@spark.apache.org>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","Hello all,

I am working a problem that requires us to create different set of JavaRDD based on different input arguments.  We are getting following error when we try to use a factory to create JavaRDD.  Error message is clear but I am wondering is there any workaround.

Question:
How to create different set of JavaRDD based on different input arguments dynamically.  Trying to implement something like factory pattern.

Error Message:
RDD transformations and actions can only be invoked by the driver, not inside of other transformations; for example, rdd1.map(x => rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063.

Thanks,

Rachana
"
Joseph Bradley <joseph@databricks.com>,"Mon, 14 Sep 2015 09:58:38 -0700",Re: [MLlib] Extensibility of MLlib classes (Word2VecModel etc.),Maandy <dymczyk@gmail.com>,"We tend to resist opening up APIs unless there's a strong reason to and we
feel reasonably confident that the API will remain stable.  That allows us
to make fixes if we realize there are issues with those APIs.  But if you
have an important use case, I'd recommend opening up a JIRA to discuss it.
Joseph


"
"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Mon, 14 Sep 2015 17:50:29 +0000",Data frame with one column,"""dev@spark.apache.org"" <dev@spark.apache.org>","Dear Spark developers,

I would like to create a dataframe with one column. However, the createDataFrame method accepts at least a Product:

val data = Seq(1.0, 2.0)
val rdd = sc.parallelize(data, 2)
val df = sqlContext.createDataFrame(rdd)
[fail]<console>:25: error: overloaded method value createDataFrame with alternatives:
 [A <: Product](data: Seq[A])(implicit evidence$2: reflect.runtime.universe.TypeTag[A])org.apache.spark.sql.DataFrame <and>
  [A <: Product](rdd: org.apache.spark.rdd.RDD[A])(implicit evidence$1: reflect.runtime.universe.TypeTag[A])org.apache.spark.sql.DataFrame
cannot be applied to (org.apache.spark.rdd.RDD[Double])
       val df = sqlContext.createDataFrame(rdd)

So, if I zip rdd with index, then it is OK:
val df = sqlContext.createDataFrame(rdd.zipWithIndex)
[success]df: org.apache.spark.sql.DataFrame = [_1: double, _2: bigint]

Also, if I use the case class, it also seems to work:
case class Hack(x: Double)
val caseRDD = rdd.map( x => Hack(x))
val df = sqlContext.createDataFrame(caseRDD)
[success]df: org.apache.spark.sql.DataFrame = [x: double]

What is the recommended way of creating a dataframe with one column?

Best regards, Alexander
"
Feynman Liang <fliang@databricks.com>,"Mon, 14 Sep 2015 11:04:54 -0700",Re: Data frame with one column,"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","For an example, see the ml-feature word2vec user guide
<https://spark.apache.org/docs/latest/ml-features.html#word2vec>


"
Feynman Liang <fliang@databricks.com>,"Mon, 14 Sep 2015 11:03:24 -0700",Re: Data frame with one column,"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","You could use `Tuple1(x)` instead of `Hack`


"
"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Mon, 14 Sep 2015 18:35:07 +0000",RE: Data frame with one column,Feynman Liang <fliang@databricks.com>,"Thank you for quick response! Iâ€™ll use Tuple1

From: Feynman Liang [mailto:fliang@databricks.com]
Sent: Monday, September 14, 2015 11:05 AM
To: Ulanov, Alexander
Cc: dev@spark.apache.org
Subject: Re: Data frame with one column

For an example, see the ml-feature word2vec user guide<https://spark.apache.org/docs/latest/ml-features.html#word2vec>

On Mon, Sep 14, 2015 at 11:03 AM, Feynman Liang <fliang@databricks.com<mailto:fliang@databricks.com>> wrote:
You could use `Tuple1(x)` instead of `Hack`

On Mon, Sep 14, 2015 at 10:50 AM, Ulanov, Alexander <alexander.ulanov@hpe.com<mailto:alexander.ulanov@hpe.com>> wrote:
Dear Spark developers,

I would like to create a dataframe with one column. However, the createDataFrame method accepts at least a Product:

val data = Seq(1.0, 2.0)
val rdd = sc.parallelize(data, 2)
val df = sqlContext.createDataFrame(rdd)
[fail]<console>:25: error: overloaded method value createDataFrame with alternatives:
 [A <: Product](data: Seq[A])(implicit evidence$2: reflect.runtime.universe.TypeTag[A])org.apache.spark.sql.DataFrame <and>
  [A <: Product](rdd: org.apache.spark.rdd.RDD[A])(implicit evidence$1: reflect.runtime.universe.TypeTag[A])org.apache.spark.sql.DataFrame
cannot be applied to (org.apache.spark.rdd.RDD[Double])
       val df = sqlContext.createDataFrame(rdd)

So, if I zip rdd with index, then it is OK:
val df = sqlContext.createDataFrame(rdd.zipWithIndex)
[success]df: org.apache.spark.sql.DataFrame = [_1: double, _2: bigint]

Also, if I use the case class, it also seems to work:
case class Hack(x: Double)
val caseRDD = rdd.map( x => Hack(x))
val df = sqlContext.createDataFrame(caseRDD)
[success]df: org.apache.spark.sql.DataFrame = [x: double]

What is the recommended way of creating a dataframe with one column?

Best regards, Alexander


"
<Saif.A.Ellafi@wellsfargo.com>,"Mon, 14 Sep 2015 18:48:53 +0000",ML: embed a transformer,<dev@spark.apache.org>,"Hi all, I'm very new to spark and looking forward to get deep into the topic.

Right now I am trying to inherit my own transformer, by what I am reading so far, it is not very public that we can apply to this practice as ""users"".
I am defining my transformer based on the Binarizer, but simply failing to retrieve the private package ml.param.shared._ to use common traits. I've read it should be public since 1.4.x but not sure.

Thanks!
Saif

"
=?UTF-8?B?SnVhbiBSb2Ryw61ndWV6IEhvcnRhbMOh?= <juan.rodriguez.hortala@gmail.com>,"Mon, 14 Sep 2015 20:51:24 +0200",Fwd: JobScheduler: Error generating jobs for time for custom InputDStream,dev <dev@spark.apache.org>,"Hi,

I sent this message to the user list a few weeks ago with no luck, so I'm
forwarding it to the dev list in case someone could give a hand with this.
Thanks a lot in advance

I've developed a ScalaCheck property for testing Spark Streaming
transformations. To do that I had to develop a custom InputDStream, which
is very similar to QueueInputDStream but has a method for adding new test
cases for dstreams, which are objects of type Seq[Seq[A]], to the DStream.
You can see the code at
https://github.com/juanrh/sscheck/blob/32c2bff66aa5500182e0162a24ecca6d47707c42/src/main/scala/org/apache/spark/streaming/dstream/DynSeqQueueInputDStream.scala.
I have developed a few properties that run in local mode
https://github.com/juanrh/sscheck/blob/32c2bff66aa5500182e0162a24ecca6d47707c42/src/test/scala/es/ucm/fdi/sscheck/spark/streaming/ScalaCheckStreamingTest.scala.
The problem is that when the batch interval is too small, and the machine
cannot complete the batches fast enough, I get the following exceptions in
the Spark log

15/08/26 11:22:02 ERROR JobScheduler: Error generating jobs for time
1440580922500 ms
java.lang.NullPointerException
    at
org.apache.spark.streaming.dstream.DStream$$anonfun$count$1$$anonfun$apply$18.apply(DStream.scala:587)
    at
org.apache.spark.streaming.dstream.DStream$$anonfun$count$1$$anonfun$apply$18.apply(DStream.scala:587)
    at
org.apache.spark.streaming.dstream.DStream$$anonfun$transform$1$$anonfun$apply$21.apply(DStream.scala:654)
    at
org.apache.spark.streaming.dstream.DStream$$anonfun$transform$1$$anonfun$apply$21.apply(DStream.scala:654)
    at
org.apache.spark.streaming.dstream.DStream$$anonfun$transform$2$$anonfun$5.apply(DStream.scala:668)
    at
org.apache.spark.streaming.dstream.DStream$$anonfun$transform$2$$anonfun$5.apply(DStream.scala:666)
    at
org.apache.spark.streaming.dstream.TransformedDStream.compute(TransformedDStream.scala:41)
    at
org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:350)
    at
org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:350)
    at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
    at
org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:349)
    at
org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:349)
    at
org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:399)
    at
org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:344)
    at
org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:342)
    at scala.Option.orElse(Option.scala:257)
    at
org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:339)
    at
org.apache.spark.streaming.dstream.ShuffledDStream.compute(ShuffledDStream.scala:41)
    at
org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:350)
    at
org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:350)
    at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
    at
org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:349)
    at
org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:349)
    at
org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:399)
    at
org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:344)
    at
org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:342)
    at scala.Option.orElse(Option.scala:257)
    at
org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:339)
    at
org.apache.spark.streaming.dstream.MappedDStream.compute(MappedDStream.scala:35)
    at
org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:350)
    at
org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:350)
    at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
    at
org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:349)
    at
org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:349)
    at
org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:399)
    at
org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:344)
    at
org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:342)
    at scala.Option.orElse(Option.scala:257)
    at
org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:339)
    at
org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:38)
    at
org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:120)
    at
org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:120)
    at
scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
    at
scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
    at
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
    at
scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251)
    at scala.collection.AbstractTraversable.flatMap(Traversable.scala:105)
    at
org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:120)
    at
org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$2.apply(JobGenerator.scala:243)
    at
org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$2.apply(JobGenerator.scala:241)
    at scala.util.Try$.apply(Try.scala:161)
    at
org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:241)
    at org.apache.spark.streaming.scheduler.JobGenerator.org
$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:177)
    at
org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:83)
    at
org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:82)
    at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
15/08/26 11:22:02 ERROR JobScheduler: Error generating jobs for time
1440580922600 ms

Sometimes test cases finish correctly anyway when this happens, but I'm a
bit concerned and wanted to check that my custom InputDStream is ok. In a
previous topic
http://apache-spark-user-list.1001560.n3.nabble.com/NullPointerException-from-count-foreachRDD-Resolved-td2066.html
the suggested solution was to return Some of an empty RDD on compute() when
the batch is empty. But that solution doesn't work for me because when I do
 that then batches are mixed up (sometimes two consecutive batches are
fused in a single batch, leaving empty one of the batches), so the
integrity of the test case generated by ScalaCheck is not preserved.
Besides, QueueuInputDStream returns None when there is no batch. I would
like to understand why Option[RDD[T]] is the returning type of
DStream.compute(), and check with the list if my custom InputDStream is ok

Thanks a lot for your help.

Greetings,

Juan
"
Feynman Liang <fliang@databricks.com>,"Mon, 14 Sep 2015 12:08:09 -0700",Re: ML: embed a transformer,Saif.A.Ellafi@wellsfargo.com,"Where did you read that it should be public? The traits in ml.param.shared
are meant to be used across internal spark.ml transformer implementations.

If your transformer could be included in spark.ml, then I would recommend
implementing it there so these package private traits can be reused.
Otherwise, you can re-use Param and Transformer which are both public.


o the
€œusersâ€.
o
€™ve
"
<Saif.A.Ellafi@wellsfargo.com>,"Mon, 14 Sep 2015 19:12:50 +0000",RE: ML: embed a transformer,<fliang@databricks.com>,"Thank you, I will do as you suggested.
Ps: I read that in this random user archive I found: http://mail-archives.us.apache.org/mod_mbox/spark-user/201506.mbox/%3C55709F7B.2090906@gmail.com%3E

Saif

From: Feynman Liang [mailto:fliang@databricks.com]
Sent: Monday, September 14, 2015 4:08 PM
To: Ellafi, Saif A.
Cc: dev
Subject: Re: ML: embed a transformer

Where did you read that it should be public? The traits in ml.param.shared are meant to be used across internal spark.ml<http://spark.ml> transformer implementations.

If your transformer could be included in spark.ml<http://spark.ml>, then I would recommend implementing it there so these package private traits can be reused. Otherwise, you can re-use Param and Transformer which are both public.

On Mon, Sep 14, 2015 at 11:48 AM, <Saif.A.Ellafi@wellsfargo.com<mailto:Saif.A.Ellafi@wellsfargo.com>> wrote:
Hi all, Iâ€™m very new to spark and looking forward to get deep into the topic.

Right now I am trying to inherit my own transformer, by what I am reading so far, it is not very public that we can apply to this practice as â€œusersâ€.
I am defining my transformer based on the Binarizer, but simply failing to retrieve the private package ml.param.shared._ to use common traits. Iâ€™ve read it should be public since 1.4.x but not sure.

Thanks!
Saif


"
Ajay Singal <asingal11@gmail.com>,"Mon, 14 Sep 2015 15:19:53 -0400",Re: JavaRDD using Reflection,"Rachana Srivastava <Rachana.Srivastava@markmonitor.com>, 
	Ankur Srivastava <ankur.srivastava@gmail.com>","Hello Rachana,

The easiest way would be to start with creating a 'parent' JavaRDD and run
different filters (based on different input arguments) to create respective
'child' JavaRDDs dynamically.

Notice that the creation of these children RDDs is handled by the
application driver.

Hope this helps!
Ajay


"
Reynold Xin <rxin@databricks.com>,"Mon, 14 Sep 2015 13:07:35 -0700",Spark 1.5.1 release,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi devs,

FYI - we have already accumulated an ""interesting"" list of issues found
with the 1.5.0 release. I will work on an RC in the next week or two,
depending on how many blocker/critical issues are fixed.

https://issues.apache.org/jira/issues/?filter=12333321
"
Luciano Resende <luckbr1975@gmail.com>,"Mon, 14 Sep 2015 13:34:40 -0700",JDBC Dialect tests,dev <dev@spark.apache.org>,"I was looking for the code mentioned in SPARK-9818 and SPARK-6136 that
supposedly is testing MySQL and PostgreSQL using Docker and it seems that
this code has been removed. Could anyone provide me a pointer on where are
these tests actually located at the moment, and how they are integrated
with the Spark build ?

My goal is to integrate some DB2 JDBC Dialect tests as mentioned in
SPARK-10521



[1] https://issues.apache.org/jira/browse/SPARK-9818
[2] https://issues.apache.org/jira/browse/SPARK-6136
[3] https://issues.apache.org/jira/browse/SPARK-10521

-- 
Luciano Resende
http://people.apache.org/~lresende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
Ankur Srivastava <ankur.srivastava@gmail.com>,"Mon, 14 Sep 2015 13:37:53 -0700",Re: JavaRDD using Reflection,Rachana.Srivastava@thomsonreuters.com,"It is not reflection that is the issue here but use of an RDD
transformation ""featureKeyClassPair.map"" inside ""lines.mapToPair"".

getFeatureScore(id,data)
invokes executeFeedFeatures, but if that is the case it is not very obvious
that â€œdataâ€ is a supposed to be huge and thus need to be  PairRDD and if it
is not you do not need to use the JavaPairRDD<String, String>, instead use
a Map<String, String> and return a List<Double>.

If it data is huge and has to be PairRDD pull out the logic to build the
data PairRDD and then invoke map function on that RDD.

- Ankur


new
E
avaPairRDD<String,
n
ve
D
we
or
"
Dirceu Semighini Filho <dirceu.semighini@gmail.com>,"Mon, 14 Sep 2015 17:42:07 -0300",Null Value in DecimalType column of DataFrame,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,
I'm moving from spark 1.4 to 1.5, and one of my tests is failing.
It seems that there was some changes in org.apache.spark.sql.types.
DecimalType

This ugly code is a little sample to reproduce the error, don't use it into
your project.

test(""spark test"") {
  val file = context.sparkContext().textFile(s""${defaultFilePath}Test.csv"").map(f
=> Row.fromSeq({
    val values = f.split("","")
    Seq(values.head.toString.toInt,values.tail.head.toString.toInt,BigDecimal(values.tail.tail.head),
    values.tail.tail.tail.head)}))

  val structType = StructType(Seq(StructField(""id"", IntegerType, false),
    StructField(""int2"", IntegerType, false), StructField(""double"",

 DecimalType(10,10), false),


    StructField(""str2"", StringType, false)))

  val df = context.sqlContext.createDataFrame(file,structType)
  df.first
}

The content of the file is:

1,5,10.5,va
2,1,0.1,vb
3,8,10.0,vc

The problem resides in DecimalType, before 1.5 the scala wasn't required.
Now when using  DecimalType(12,10) it works fine, but using
DecimalType(10,10) the Decimal values
10.5 became null, and the 0.1 works.

Is there anybody working with DecimalType for 1.5.1?

Regards,
Dirceu
"
Reynold Xin <rxin@databricks.com>,"Mon, 14 Sep 2015 13:47:52 -0700",Re: JDBC Dialect tests,Luciano Resende <luckbr1975@gmail.com>,"SPARK-9818 you link to actually links to a pull request trying to bring
them back.



"
Yin Huai <yhuai@databricks.com>,"Mon, 14 Sep 2015 14:54:14 -0700",Re: Null Value in DecimalType column of DataFrame,Dirceu Semighini Filho <dirceu.semighini@gmail.com>,"A scale of 10 means that there are 10 digits at the right of the decimal
point. If you also have precision 10, the range of your data will be [0, 1)
and casting ""10.5"" to DecimalType(10, 10) will return null, which is
expected.


"
Reynold Xin <rxin@databricks.com>,"Mon, 14 Sep 2015 16:32:23 -0700",Re: Unable to acquire memory errors in HiveCompatibilitySuite,Pete Robbins <robbinspg@gmail.com>,"Is this on latest master / branch-1.5?

out of the box we reserve only 16% (0.2 * 0.8) of the memory for execution
(e.g. aggregate, join) / shuffle sorting. With a 3GB heap, that's 480MB. So
each task gets 480MB / 32 = 15MB, and each operator reserves at least one
page for execution. If your page size is 4MB, it only takes 3 operators to
use up its memory.

The thing is page size is dynamically determined -- and in your case it
should be smaller than 4MB.
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/shuffle/ShuffleMemoryManager.scala#L174

Maybe there is a place that in the maven tests that we explicitly set the
page size (spark.buffer.pageSize) to 4MB? If yes, we need to find it and
just remove it.



"
"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Mon, 14 Sep 2015 23:31:14 +0000",Enum parameter in ML,"""dev@spark.apache.org"" <dev@spark.apache.org>","Dear Spark developers,

I am currently implementing the Estimator in ML that has a parameter that can take several different values that are mutually exclusive. The most appropriate type seems to be Scala Enum (http://www.scala-lang.org/api/current/index.html#scala.Enumeration). However, the current ML API has the following parameter types:
BooleanParam, DoubleArrayParam, DoubleParam, FloatParam, IntArrayParam, IntParam, LongParam, StringArrayParam

Should I introduce a new parameter type in ML API that is based on Scala Enum?

Best regards, Alexander
"
sim <sim@swoop.com>,"Mon, 14 Sep 2015 16:36:05 -0700 (MST)",RDD API patterns,dev@spark.apache.org,"I'd like to get some feedback on an API design issue pertaining to RDDs. 

The design goal to avoid RDD nesting, which I agree with, leads the methods
operating on subsets of an RDD (not necessarily partitions) to use Iterable
as an abstraction. The mapPartitions and groupBy* family of methods are good
examples. The problem with that API choice is that developers often very
quickly run out of the benefits of the RDD API, independent of partitioning. 

Consider two very simple problems that demonstrate the issue. The input is
the same for all: an RDD of integers that has been grouped into odd and
even.

1. Sample the odds at 10% and the evens at 20%. Trivial, as stratified
sampling (sampleByKey) is built into PairRDDFunctions.

2. Sample at 10% if there are more than 1,000 elements in a group and at 20%
otherwise. Suddenly, the problem becomes a lot less easy. The sub-groups are
no longer RDDs and we can't use the RDD sampling API.

Note that the only reason the first problem is easy is because it was part
of Spark. If that hadn't happened, implementing it with the higher-level API
abstractions wouldn't have been easy. As more an more people use Spark for
ever more diverse sets of problems the likelihood that the RDD APIs provide
pre-existing high-level abstractions will diminish. 

How do you feel about this? Do you think it is desirable to lose all
high-level RDD API abstractions the very moment we group an RDD or call
mapPartitions? Does the goal of no nested RDDs mean there are absolutely no
high-level abstractions that we can expose via the Iterables borne of RDDs?

I'd love your thoughts.

/Sim
http://linkedin.com/in/simeons <http://linkedin.com/in/simeons>  



--

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Mon, 14 Sep 2015 16:40:55 -0700",Re: Unable to acquire memory errors in HiveCompatibilitySuite,Pete Robbins <robbinspg@gmail.com>,"Pete - can you do me a favor?

https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/shuffle/ShuffleMemoryManager.scala#L174

Print the parameters that are passed into the getPageSize function, and
check their values.


"
Feynman Liang <fliang@databricks.com>,"Mon, 14 Sep 2015 17:26:34 -0700",Re: Enum parameter in ML,"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Since PipelineStages are serializable, the params must also be
serializable. We also have to keep the Java API in mind. Introducing a new
enum Param type may work, but we will have to ensure that Java users can
use it without dealing with ClassTags (I believe Scala will create new
types for each possible value in the Enum) and that it can be serialized.


"
"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Tue, 15 Sep 2015 00:43:23 +0000",RE: Enum parameter in ML,Feynman Liang <fliang@databricks.com>,"Hi Feynman,

Thank you for suggestion. How can I ensure that there will be no problems for Java users? (I only use Scala API)

Best regards, Alexander

From: Feynman Liang [mailto:fliang@databricks.com]
Sent: Monday, September 14, 2015 5:27 PM
To: Ulanov, Alexander
Cc: dev@spark.apache.org
Subject: Re: Enum parameter in ML

Since PipelineStages are serializable, the params must also be serializable. We also have to keep the Java API in mind. Introducing a new enum Param type may work, but we will have to ensure that Java users can use it without dealing with ClassTags (I believe Scala will create new types for each possible value in the Enum) and that it can be serialized.

On Mon, Sep 14, 2015 at 4:31 PM, Ulanov, Alexander <alexander.ulanov@hpe.com<mailto:alexander.ulanov@hpe.com>> wrote:
Dear Spark developers,

I am currently implementing the Estimator in ML that has a parameter that can take several different values that are mutually exclusive. The most appropriate type seems to be Scala Enum (http://www.scala-lang.org/api/current/index.html#scala.Enumeration). However, the current ML API has the following parameter types:
BooleanParam, DoubleArrayParam, DoubleParam, FloatParam, IntArrayParam, IntParam, LongParam, StringArrayParam

Should I introduce a new parameter type in ML API that is based on Scala Enum?

Best regards, Alexander

"
Feynman Liang <fliang@databricks.com>,"Mon, 14 Sep 2015 18:07:50 -0700",Re: Enum parameter in ML,"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","We usually write a Java test suite which exercises the public API (e.g. DCT
<https://github.com/apache/spark/blob/master/mllib/src/test/java/org/apache/spark/ml/feature/JavaDCTSuite.java#L71>
).

It may be possible to create a sealed trait with singleton concrete
instances inside of a serializable companion object, the just introduce a
Param[SealedTrait] to the model (e.g. StreamingDecay PR
<https://github.com/apache/spark/pull/8022/files#diff-cea0bec4853b1b2748ec006682218894R99>).
However, this would require Java users to use
CompanionObject$.ConcreteInstanceName to access enum values which isn't the
prettiest syntax.

Another option would just be to use Strings, which although is not type
safe does simplify implementation.


"
Zack Sampson <zsampson@palantir.com>,"Tue, 15 Sep 2015 04:12:08 +0000",And.eval short circuiting,"""dev@spark.apache.org"" <dev@spark.apache.org>","It seems like And.eval can avoid calculating right.eval if left.eval returns null. Is there a reason it's written like it is?


override def eval(input: Row): Any = {
  val l = left.eval(input)
  if (l == false) {
    false
  } else {
    val r = right.eval(input)
    if (r == false) {
      false
    } else {
      if (l != null && r != null) {
        true
      } else {
        null
      }
    }
  }
}
"
Reynold Xin <rxin@databricks.com>,"Mon, 14 Sep 2015 22:14:46 -0700",Re: And.eval short circuiting,Zack Sampson <zsampson@palantir.com>,"rxin=# select null and true;
 ?column?
----------

(1 row)

rxin=# select null and false;
 ?column?
----------
 f
(1 row)


null and false should return false.



"
Pete Robbins <robbinspg@gmail.com>,"Tue, 15 Sep 2015 07:06:04 +0100",Re: Unable to acquire memory errors in HiveCompatibilitySuite,Reynold Xin <rxin@databricks.com>,"Reynold, thanks for replying.

getPageSize parameters: maxMemory=515396075, numCores=0
Calculated values: cores=8, default=4194304

So am I getting a large page size as I only have 8 cores?


"
Reynold Xin <rxin@databricks.com>,"Mon, 14 Sep 2015 23:22:17 -0700",Re: Unable to acquire memory errors in HiveCompatibilitySuite,Pete Robbins <robbinspg@gmail.com>,"Yea I think this is where the heuristics is failing -- it uses 8 cores to
approximate the number of active tasks, but the tests somehow is using 32
(maybe because it explicitly sets it to that, or you set it yourself? I'm
not sure which one)


"
Pete Robbins <robbinspg@gmail.com>,"Tue, 15 Sep 2015 08:50:45 +0100",Re: Unable to acquire memory errors in HiveCompatibilitySuite,Reynold Xin <rxin@databricks.com>,"Ok so it looks like the max number of active tasks reaches 30. I'm not
setting anything as it is a clean environment with clean spark code
checkout. I'll dig further to see why so many tasks are active.

Cheers,


"
Pete Robbins <robbinspg@gmail.com>,"Tue, 15 Sep 2015 11:37:54 +0100",Re: Unable to acquire memory errors in HiveCompatibilitySuite,Reynold Xin <rxin@databricks.com>,"This is the culprit:

https://issues.apache.org/jira/browse/SPARK-8406

""2.  Make `TestHive` use a local mode `SparkContext` with 32 threads to
increase parallelism

    The major reason for this is that, the original parallelism of 2 is too
low to reproduce
the data loss issue.  Also, higher concurrency may potentially caught more
concurrency bugs
during testing phase. (It did help us spotted SPARK-8501.)""

Specific change:

http://git-wip-us.apache.org/repos/asf/spark/blob/0818fdec/sql/hive/src/main/scala/org/apache/spark/sql/hive/test/TestHive.scala
----------------------------------------------------------------------
diff --git
a/sql/hive/src/main/scala/org/apache/spark/sql/hive/test/TestHive.scala
b/sql/hive/src/main/scala/org/apache/spark/sql/hive/test/TestHive.scala
index f901bd8..ea325cc 100644
--- a/sql/hive/src/main/scala/org/apache/spark/sql/hive/test/TestHive.scala
+++ b/sql/hive/src/main/scala/org/apache/spark/sql/hive/test/TestHive.scala
@@ -49,7 +49,7 @@ import scala.collection.JavaConversions._
 object TestHive
   extends TestHiveContext(
     new SparkContext(
-      System.getProperty(""spark.sql.test.master"", ""local[2]""),
+      System.getProperty(""spark.sql.test.master"", ""local[32]""),
       ""TestSQLContext"",
       new SparkConf()
         .set(""spark.sql.test"", """")



Setting that to local[8] to match my cores the HiveCompatibilitySuite
passes (and runs so much faster!) so maybe that should be changed to limit
threads to num cores?

Cheers,


"
Dirceu Semighini Filho <dirceu.semighini@gmail.com>,"Tue, 15 Sep 2015 10:39:57 -0300",Fwd: Null Value in DecimalType column of DataFrame,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Yin, posted here because I think it's a bug.
So, it will return null and I can get a nullpointerexception, as I was
getting. Is this really the expected behavior? Never seen something
returning null in other Scala tools that I used.

Regards,


2015-09-14 18:54 GMT-03:00 Yin Huai <yhuai@databricks.com>:

"
Marcelo Vanzin <vanzin@cloudera.com>,"Tue, 15 Sep 2015 09:40:41 -0700",Re: Unable to acquire memory errors in HiveCompatibilitySuite,Reynold Xin <rxin@databricks.com>,"That test explicitly sets the number of executor cores to 32.

object TestHive
  extends TestHiveContext(
    new SparkContext(
      System.getProperty(""spark.sql.test.master"", ""local[32]""),





-- 
Marcelo

---------------------------------------------------------------------


"
Pete Robbins <robbinspg@gmail.com>,"Tue, 15 Sep 2015 18:28:20 +0100",Re: Unable to acquire memory errors in HiveCompatibilitySuite,Marcelo Vanzin <vanzin@cloudera.com>,"Yes and at least there is an override by setting  spark.sql.test.master to
local[8] , in fact local[16] worked on my 8 core box.

I'm happy to use this as a workaround but the 32 hard-coded will fail
running build/tests on a clean checkout if you only have 8 cores.


"
Ravi Ravi <i.am.ravi.ravi@gmail.com>,"Tue, 15 Sep 2015 23:02:55 +0530",Predicate push-down bug?,dev@spark.apache.org,"Turning on predicate pushdown for ORC datasources results in a
NoSuchElementException:

scala> val df = sqlContext.sql(""SELECT name FROM people WHERE age < 15"")
df: org.apache.spark.sql.DataFrame = [name: string]

scala> sqlContext.setConf(""spark.sql.orc.filterPushdown"", ""*true*"")

scala> df.explain
== Physical Plan ==
*java.util.NoSuchElementException*

Disabling the pushdown makes things work again:

scala> sqlContext.setConf(""spark.sql.orc.filterPushdown"", ""*false*"")

scala> df.explain
== Physical Plan ==
Project [name#6]
 Filter (age#7 < 15)
  Scan
OrcRelation[file:/home/mydir/spark-1.5.0-SNAPSHOT/test/people][name#6,age#7]

Have any of you run into this problem before? Is a fix available?

Thanks,
Ravi
"
Ram Sriharsha <sriharsha.ram@gmail.com>,"Tue, 15 Sep 2015 10:47:20 -0700",Re: Predicate push-down bug?,Ravi Ravi <i.am.ravi.ravi@gmail.com>,"Hi Ravi

Can you share more details? What Spark version are you running?

Ram


"
Reynold Xin <rxin@databricks.com>,"Tue, 15 Sep 2015 10:54:42 -0700",Re: Unable to acquire memory errors in HiveCompatibilitySuite,Pete Robbins <robbinspg@gmail.com>,"Maybe we can change the heuristics in memory calculation to use
SparkContext.defaultParallelism if it is local mode.



"
Ram Sriharsha <sriharsha.ram@gmail.com>,"Tue, 15 Sep 2015 12:04:31 -0700",Re: Predicate push-down bug?,Ravi Ravi <i.am.ravi.ravi@gmail.com>,"Hi Ravi

This does look like a bug.. I have created a JIRA to track it here:

https://issues.apache.org/jira/browse/SPARK-10623

Ram


"
Zack Sampson <zsampson@palantir.com>,"Tue, 15 Sep 2015 20:09:43 +0000",RE: And.eval short circuiting,Reynold Xin <rxin@databricks.com>,"I see. We're having problems with code like this (forgive my noob scala):

val df = Seq((""moose"",""ice""), (null,""fire"")).toDF(""animals"", ""elements"")
df
  .filter($""animals"".rlike("".*""))
  .filter(callUDF({(value: String) => value.length > 2}, BooleanType, $""animals""))
.collect()

This code throws a NPE because:
* Catalyst combines the filters with an AND
* the first filter passes returns null on the first input
* the second filter tries to read the length of that null

This feels weird. Reading that code, I wouldn't expect null to be passed to the second filter. Even weirder is that if you call collect() after the first filter you won't see nulls, and if you write the data to disk and reread it, the NPE won't happen.

It's bewildering! Is this the intended behavior?
________________________________
From: Reynold Xin [rxin@databricks.com]
Sent: Monday, September 14, 2015 10:14 PM
To: Zack Sampson
Cc: dev@spark.apache.org
Subject: Re: And.eval short circuiting

rxin=# select null and true;
 ?column?
----------

(1 row)

rxin=# select null and false;
 ?column?
----------
 f
(1 row)


null and false should return false.


It seems like And.eval can avoid calculating right.eval if left.eval returns null. Is there a reason it's written like it is?


override def eval(input: Row): Any = {
  val l = left.eval(input)
  if (l == false) {
    false
  } else {
    val r = right.eval(input)
    if (r == false) {
      false
    } else {
      if (l != null && r != null) {
        true
      } else {
        null
      }
    }
  }
}

"
Renyi Xiong <renyixiong0@gmail.com>,"Tue, 15 Sep 2015 13:46:13 -0700",pyspark streaming DStream compute,dev@spark.apache.org,"Can anybody help understand why pyspark streaming uses py4j callback to
execute python code while pyspark batch uses worker.py?

regarding pyspark streaming, is py4j callback only used for
DStream, worker.py still used for RDD?

thanks,
Renyi.
"
Davies Liu <davies@databricks.com>,"Tue, 15 Sep 2015 18:44:48 -0700",Re: pyspark streaming DStream compute,Renyi Xiong <renyixiong0@gmail.com>,"
There are two kind of callback in pyspark streaming:
1) one operate on RDDs, it take an RDD and return an new RDD, uses
py4j callback,
because SparkContext and RDDs are not accessible in worker.py
2) operate on records of RDD, it take an record and return new
records, uses worker.py


Yes.


---------------------------------------------------------------------


"
Pete Robbins <robbinspg@gmail.com>,"Wed, 16 Sep 2015 06:46:15 +0100",Re: Unable to acquire memory errors in HiveCompatibilitySuite,Reynold Xin <rxin@databricks.com>,"The page size calculation is the issue here as there is plenty of free
memory, although there is maybe a fair bit of wasted space in some pages.
It is that when we have a lot of tasks each is only allowed to reach 1/n of
the available memory and several of the tasks bump in to that limit. With
tasks 4 times the number of cores there will be some contention and so they
remain active for longer.

So I think this is a test case issue configuring the number of executors
too high.


"
Pete Robbins <robbinspg@gmail.com>,"Wed, 16 Sep 2015 06:47:00 +0100",Re: Unable to acquire memory errors in HiveCompatibilitySuite,Reynold Xin <rxin@databricks.com>,"Oops... I meant to say ""The page size calculation is NOT the issue here""


"
Reynold Xin <rxin@databricks.com>,"Tue, 15 Sep 2015 22:53:20 -0700",Re: Unable to acquire memory errors in HiveCompatibilitySuite,Pete Robbins <robbinspg@gmail.com>,"It is exactly the issue here, isn't it?

We are using memory / N, where N should be the maximum number of active
tasks. In the current master, we use the number of cores to approximate the
number of tasks -- but it turned out to be a bad approximation in tests
because it is set to 32 to increase concurrency.



"
=?UTF-8?B?SnVhbiBSb2Ryw61ndWV6IEhvcnRhbMOh?= <juan.rodriguez.hortala@gmail.com>,"Wed, 16 Sep 2015 08:51:30 +0200",JobScheduler: Error generating jobs for time for custom InputDStream,dev <dev@spark.apache.org>,"Hi,

Sorry to insist, anyone has any thoughts on this? Or at least someone can
point me to a documentation of DStream.compute() so I can understand when I
should return None for a batch?

Thanks

Juan


2015-09-14 20:51 GMT+02:00 Juan RodrÃ­guez HortalÃ¡ <
juan.rodriguez.hortala@gmail.com>:

.
.
707c42/src/main/scala/org/apache/spark/streaming/dstream/DynSeqQueueInputDStream.scala.
707c42/src/test/scala/es/ucm/fdi/sscheck/spark/streaming/ScalaCheckStreamingTest.scala.
n
y$18.apply(DStream.scala:587)
y$18.apply(DStream.scala:587)
apply$21.apply(DStream.scala:654)
apply$21.apply(DStream.scala:654)
5.apply(DStream.scala:668)
5.apply(DStream.scala:666)
DStream.scala:41)
un$1$$anonfun$apply$7.apply(DStream.scala:350)
un$1$$anonfun$apply$7.apply(DStream.scala:350)
un$1.apply(DStream.scala:349)
un$1.apply(DStream.scala:349)
Stream.scala:399)
DStream.scala:344)
DStream.scala:342)
)
m.scala:41)
un$1$$anonfun$apply$7.apply(DStream.scala:350)
un$1$$anonfun$apply$7.apply(DStream.scala:350)
un$1.apply(DStream.scala:349)
un$1.apply(DStream.scala:349)
Stream.scala:399)
DStream.scala:344)
DStream.scala:342)
)
ala:35)
un$1$$anonfun$apply$7.apply(DStream.scala:350)
un$1$$anonfun$apply$7.apply(DStream.scala:350)
un$1.apply(DStream.scala:349)
un$1.apply(DStream.scala:349)
Stream.scala:399)
DStream.scala:344)
DStream.scala:342)
)
eam.scala:38)
la:120)
la:120)
.scala:251)
.scala:251)
a:59)
)
20)
erator.scala:243)
erator.scala:241)
or.scala:241)
.scala:177)
nerator.scala:83)
nerator.scala:82)
from-count-foreachRDD-Resolved-td2066.html
en
do
k
"
Pete Robbins <robbinspg@gmail.com>,"Wed, 16 Sep 2015 09:07:01 +0100",Re: Unable to acquire memory errors in HiveCompatibilitySuite,Reynold Xin <rxin@databricks.com>,"ok so let me try again ;-)

I don't think that the page size calculation matters apart from hitting the
allocation limit earlier if the page size is too large.

If a task is going to need X bytes, it is going to need X bytes. In this
case, for at least one of the tasks, X > maxmemory/no_active_tasks at some
point during execution. A smaller page size may use the memory more
efficiently but would not necessarily avoid this issue.

The next question would be: Is the memory limit per task of
max_memory/no_active_tasks reasonable? It seems fair but if this limit is
reached currently an exception is thrown, maybe the task could wait for
no_active_tasks to decrease?

I think what causes my test issue is that the 32 tasks don't execute as
quickly on my 8 core box so more are active at any one time.

I will experiment with the page size calculation to see what effect it has.

Cheers,




"
Reynold Xin <rxin@databricks.com>,"Wed, 16 Sep 2015 01:30:47 -0700",Re: Unable to acquire memory errors in HiveCompatibilitySuite,Pete Robbins <robbinspg@gmail.com>,"Can you paste the entire stacktrace of the error? In your original email
you only included the last function call.

Maybe I'm missing something here, but I still think the bad heuristics is
the issue.

Some operators pre-reserve memory before running anything in order to avoid
starvation. For example, imagine we have an aggregate followed by a sort.
If the aggregate is very high cardinality, and uses up all the memory and
even starts spilling (falling back to sort-based aggregate), there isn't
memory available at all for the sort operator to use. To work around this,
each operator reserves a page of memory before they process any data.

Page size is computed by Spark using:

the total amount of execution memory / (maximum number of active tasks * 16)

and then rounded to the next power of 2, and cap between 1MB and 64MB.

That is to say, in the worst case, we should be able to reserve at least 8
pages (16 rounded up to the next power of 2).

However, in your case, the max number of active tasks is 32 (set by test
env), while the page size is determined using # cores (8 in your case). So
it is off by a factor of 4. As a result, with this page size, we can only
reserve at least 2 pages. That is to say, if you have more than 3 operators
that need page reservation (e.g. an aggregate followed by a join on the
group by key followed by a shuffle - which seems to be the case of
join31.q), the task can fail to reserve memory before running anything.


There is a 2nd problem (maybe this is the one you were trying to point
out?) that is tasks running at the same time can be competing for memory
with each other.  Spark allows each task to claim up to 2/N share of
memory, where N is the number of active tasks. If a task is launched before
others and hogs a lot of memory quickly, the other tasks that are launched
after it might not be able to get enough memory allocation, and thus will
fail. This is not super ideal, but probably fine because tasks can be
retried, and can succeed in retries.



"
Reynold Xin <rxin@databricks.com>,"Wed, 16 Sep 2015 01:34:39 -0700",Re: RDD API patterns,sim <sim@swoop.com>,"I'm not sure what we can do here. Nested RDDs are a pain to implement,
support, and explain. The programming model is not well explored.

Maybe a UDAF interface that allows going through the data twice?



"
Pete Robbins <robbinspg@gmail.com>,"Wed, 16 Sep 2015 10:17:06 +0100",Re: Unable to acquire memory errors in HiveCompatibilitySuite,Reynold Xin <rxin@databricks.com>,"I see what you are saying. Full stack trace:

java.io.IOException: Unable to acquire 4194304 bytes of memory
      at
org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPage(UnsafeExternalSorter.java:368)
      at
org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:349)
      at
org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertKVRecord(UnsafeExternalSorter.java:478)
      at
org.apache.spark.sql.execution.UnsafeKVExternalSorter.insertKV(UnsafeKVExternalSorter.java:138)
      at
org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.switchToSortBasedAggregation(TungstenAggregationIterator.scala:489)
      at
org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:379)
      at
org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.start(TungstenAggregationIterator.scala:622)
      at
org.apache.spark.sql.execution.aggregate.TungstenAggregate$$anonfun$doExecute$
1.org
$apache$spark$sql$execution$aggregate$TungstenAggregate$$anonfun$$executePartition$1(TungstenAggregate.scala:110)
      at
org.apache.spark.sql.execution.aggregate.TungstenAggregate$$anonfun$doExecute$1$$anonfun$2.apply(TungstenAggregate.scala:119)
      at
org.apache.spark.sql.execution.aggregate.TungstenAggregate$$anonfun$doExecute$1$$anonfun$2.apply(TungstenAggregate.scala:119)
      at
org.apache.spark.rdd.MapPartitionsWithPreparationRDD.compute(MapPartitionsWithPreparationRDD.scala:64)
      at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
      at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
      at
org.apache.spark.rdd.MapPartitionsWithPreparationRDD.compute(MapPartitionsWithPreparationRDD.scala:63)
      at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
      at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
      at
org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:99)
      at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
      at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
      at
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
      at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
      at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
      at
org.apache.spark.rdd.MapPartitionsWithPreparationRDD.compute(MapPartitionsWithPreparationRDD.scala:63)
      at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
      at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
      at
org.apache.spark.rdd.MapPartitionsWithPreparationRDD.compute(MapPartitionsWithPreparationRDD.scala:63)
      at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
      at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
      at
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
      at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
      at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
      at
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
      at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
      at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
      at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
      at org.apache.spark.scheduler.Task.run(Task.scala:88)
      at
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
      at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1153)
      at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
      at java.lang.Thread.run(Thread.java:785)


"
Aniket <aniket.bhatnagar@gmail.com>,"Wed, 16 Sep 2015 02:51:44 -0700 (MST)",Re: RDD API patterns,dev@spark.apache.org,"I agree that this in issue but I am afraid supporting RDD nesting would be
hard and perhaps would need rearchitecting Spark. For now, you may to use
workarounds like storing each group in a separate file, process each file
as separate RDD and finally merge results in a single RDD.

I know its painful and I share the pain :)

Thanks,
Aniket






--"
=?UTF-8?B?SnVhbiBSb2Ryw61ndWV6IEhvcnRhbMOh?= <juan.rodriguez.hortala@gmail.com>,"Wed, 16 Sep 2015 12:06:46 +0200",Re: RDD API patterns,,"Hi,

That reminds me to a previous discussion about splitting an RDD into
several RDDs
http://apache-spark-developers-list.1001551.n3.nabble.com/RDD-split-into-multiple-RDDs-td11877.html.
There you can see a simple code to convert RDD[(K, V)] into Map[K, RDD[V]]
abstraction that simulates nested RDDs, as a proof of concepts, forgetting
for now about performance. But the main problem I've found is that the
Spark scheduler gets stuck when you have a huge amount of very small RDDs,
or at least that is what happened several versions ago
http://mail-archives.us.apache.org/mod_mbox/spark-user/201502.mbox/%3CCAMAsSdJ+bzV++Cr44eDV-CPChr-1X-A+y2vmtUgwc0uX91fvKg@mail.gmail.com%3E

Just my two cents





2015-09-16 11:51 GMT+02:00 Aniket <aniket.bhatnagar@gmail.com>:

e
dden
:
se
nd
hat
 no
Ds?
ns-tp14116.html
Servlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
ns-tp14116p14146.html>
"
robineast <robin.east@xense.co.uk>,"Wed, 16 Sep 2015 06:39:08 -0700 (MST)",Re: RDD API patterns,dev@spark.apache.org,"I'm not sure the problem is quite as bad as you state. Both sampleByKey and
sampleByKeyExact are implemented using a function from
StratifiedSamplingUtils which does one of two things depending on whether
the exact implementation is needed. The exact version requires double the
number of lines of code (17) than the non-exact and has to do extra passes
over the data to get, for example, the counts per key.

As far as I can see your problem 2 and sampleByKeyExact are very similar and
could be solved the same way. It has been decided that sampleByKeyExact is a
widely useful function and so is provided out of the box as part of the
PairRDD API. I don't see any reason why your problem 2 couldn't be provided
in the same way as part of the API if there was the demand for it. 

An alternative design would perhaps be something like an extension to
PairRDD, let's call it TwoPassPairRDD, where certain information for the key
could be provided along with an Iterable e.g. the counts for the key. Both
sampleByKeyExact and your problem 2 could be implemented in a few less lines
of code.



--

---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Wed, 16 Sep 2015 08:40:24 -0700","JENKINS: downtime next week, wed and thurs mornings (9-23 and 9-24)","dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>, 
	Jon Kuroda <jkuroda@eecs.berkeley.edu>","good morning, denizens of the aether!

your hard working build system (and some associated infrastructure)
has been in need of some updates and housecleaning for quite a while
now.  we will be splitting the maintenance over two mornings to
minimize impact.

here's the plan:

7am-9am wednesday, 9-24-15  (or 24-9-15 for those not in amurrica):
* firewall taken offline for system and firewall updates
* expected downtime:  maybe an hour, but we'll say two just in case
* this will be done by jkuroda (CCed on this message)

630am-10am thursday, 9-24-15:
* jenknins update to 1.629 (we're a few months behind in versions, and
some big bugs have been fixed)
* jenkins master and worker system package updates
* all systems get a reboot (lots of hanging java processes have been
building up over the months)
* builds will stop being accepted ~630am, and i'll kill any hangers-on
at 730am, and retrigger once we're done
* expected downtime:  3.5 hours or so
* i will also be testing out some of my shiny new ansible playbooks
for the system updates!


please let me know if you have any questions, or requests to postpone
this maintenance.  thanks in advance!

shane & jon

---------------------------------------------------------------------


"
Pete Robbins <robbinspg@gmail.com>,"Wed, 16 Sep 2015 16:57:39 +0100",Re: Unable to acquire memory errors in HiveCompatibilitySuite,Reynold Xin <rxin@databricks.com>,"so forcing the ShuffleMemoryManager to assume 32 cores and therefore
calculate a pagesize of 1MB passes the tests.

How can we determine the correct value to use in getPageSize rather than
Runtime.getRuntime.availableProcessors()?


"
Reynold Xin <rxin@databricks.com>,"Wed, 16 Sep 2015 09:23:47 -0700","Re: JENKINS: downtime next week, wed and thurs mornings (9-23 and 9-24)",shane knapp <sknapp@berkeley.edu>,"Thanks Shane and Jon for the heads up.


"
Reynold Xin <rxin@databricks.com>,"Wed, 16 Sep 2015 09:27:30 -0700",Re: Unable to acquire memory errors in HiveCompatibilitySuite,Pete Robbins <robbinspg@gmail.com>,"SparkEnv for the driver was created in SparkContext. The default
parallelism field is set to the number of slots (max number of active
tasks). Maybe we can just use the default parallelism to compute that in
local mode.


"
Renyi Xiong <renyixiong0@gmail.com>,"Wed, 16 Sep 2015 09:52:57 -0700",SparkR streaming source code,dev@spark.apache.org,"SparkR streaming is mentioned at about page 17 in below pdf, can anyone
share source code? (could not find it on GitHub)


https://spark-summit.org/2015-east/wp-content/uploads/2015/03/SSE15-19-Hao-Lin-Haichuan-Wang.pdf


Thanks,

Renyi.
"
Reynold Xin <rxin@databricks.com>,"Wed, 16 Sep 2015 10:06:40 -0700",Re: SparkR streaming source code,Renyi Xiong <renyixiong0@gmail.com>,"You should reach out to the speakers directly.



"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Wed, 16 Sep 2015 10:14:14 -0700",Re: SparkR streaming source code,Reynold Xin <rxin@databricks.com>,"I think Hao posted a link to the source code in the description of
https://issues.apache.org/jira/browse/SPARK-6803


---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Wed, 16 Sep 2015 12:15:41 -0700","Re: JENKINS: downtime next week, wed and thurs mornings (9-23 and 9-24)","dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>, 
	Jon Kuroda <jkuroda@eecs.berkeley.edu>","i forgot one thing:

* moving default system java for builds from jdk1.7.0_71 to jdk1.7.0_79

---------------------------------------------------------------------


"
Renyi Xiong <renyixiong0@gmail.com>,"Wed, 16 Sep 2015 12:36:22 -0700",Re: SparkR streaming source code,shivaram@eecs.berkeley.edu,"got it, thanks a lot!


"
Muhammad Haseeb Javed <11besemjaved@seecs.edu.pk>,"Thu, 17 Sep 2015 00:44:51 +0500",Communication between executors and drivers,dev <dev@spark.apache.org>,"How do executors communicate with the driver in Spark ? I understand that
it s done using Akka actors and messages are exchanged as
CoarseGrainedSchedulerMessage, but I'd really appreciate if someone could
explain the entire process in a bit detail.
"
Renyi Xiong <renyixiong0@gmail.com>,"Wed, 16 Sep 2015 12:47:21 -0700",Spark streaming DStream state on worker,dev@spark.apache.org,"Hi,

I want to do temporal join operation on DStream across RDDs, my question
is: Are RDDs from same DStream always computed on same worker (except
failover) ?

thanks,
Renyi.
"
Reynold Xin <rxin@databricks.com>,"Wed, 16 Sep 2015 13:17:17 -0700",Re: And.eval short circuiting,Zack Sampson <zsampson@palantir.com>,"This is ""expected"" in the sense that DataFrame operations can get
re-ordered under the hood by the optimizer. For example, if the optimizer
deems it is cheaper to apply the 2nd filter first, it might re-arrange the
filters. In reality, it doesn't do that. I think this is too confusing and
violates principle of least astonishment, so we should fix it.

I discussed more with Michael offline, and think we can add a rule for the
physical filter operator to replace the general AND/OR/equality/etc with a
special version that treats null as false. This rule needs to be carefully
written because it should only apply to subtrees of AND/OR/equality/etc
(e.g. it shouldn't rewrite children of isnull).



"
Joseph Bradley <joseph@databricks.com>,"Wed, 16 Sep 2015 17:35:22 -0700",Re: Enum parameter in ML,Feynman Liang <fliang@databricks.com>,"I've tended to use Strings.  Params can be created with a validator
(isValid) which can ensure users get an immediate error if they try to pass
an unsupported String.  Not as nice as compile-time errors, but easier on
the APIs.


"
"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Thu, 17 Sep 2015 00:43:14 +0000",RE: Enum parameter in ML,"Joseph Bradley <joseph@databricks.com>, Feynman Liang
	<fliang@databricks.com>","Hi Joseph,

Strings sounds reasonable. However, there is no StringParam (only StringArrayParam). Should I create a new param type? Also, how can the user get all possible values of String parameter?

Best regards, Alexander

From: Joseph Bradley [mailto:joseph@databricks.com]
Sent: Wednesday, September 16, 2015 5:35 PM
To: Feynman Liang
Cc: Ulanov, Alexander; dev@spark.apache.org
Subject: Re: Enum parameter in ML

I've tended to use Strings.  Params can be created with a validator (isValid) which can ensure users get an immediate error if they try to pass an unsupported String.  Not as nice as compile-time errors, but easier on the APIs.

On Mon, Sep 14, 2015 at 6:07 PM, Feynman Liang <fliang@databricks.com<mailto:fliang@databricks.com>> wrote:
We usually write a Java test suite which exercises the public API (e.g. DCT<https://github.com/apache/spark/blob/master/mllib/src/test/java/org/apache/spark/ml/feature/JavaDCTSuite.java#L71>).

It may be possible to create a sealed trait with singleton concrete instances inside of a serializable companion object, the just introduce a Param[SealedTrait] to the model (e.g. StreamingDecay PR<https://github.com/apache/spark/pull/8022/files#diff-cea0bec4853b1b2748ec006682218894R99>). However, this would require Java users to use CompanionObject$.ConcreteInstanceName to access enum values which isn't the prettiest syntax.

Another option would just be to use Strings, which although is not type safe does simplify implementation.

On Mon, Sep 14, 2015 at 5:43 PM, Ulanov, Alexander <alexander.ulanov@hpe.com<mailto:alexander.ulanov@hpe.com>> wrote:
Hi Feynman,

Thank you for suggestion. How can I ensure that there will be no problems for Java users? (I only use Scala API)

Best regards, Alexander

From: Feynman Liang [mailto:fliang@databricks.com<mailto:fliang@databricks.com>]
Sent: Monday, September 14, 2015 5:27 PM
To: Ulanov, Alexander
Cc: dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: Enum parameter in ML

Since PipelineStages are serializable, the params must also be serializable. We also have to keep the Java API in mind. Introducing a new enum Param type may work, but we will have to ensure that Java users can use it without dealing with ClassTags (I believe Scala will create new types for each possible value in the Enum) and that it can be serialized.

On Mon, Sep 14, 2015 at 4:31 PM, Ulanov, Alexander <alexander.ulanov@hpe.com<mailto:alexander.ulanov@hpe.com>> wrote:
Dear Spark developers,

I am currently implementing the Estimator in ML that has a parameter that can take several different values that are mutually exclusive. The most appropriate type seems to be Scala Enum (http://www.scala-lang.org/api/current/index.html#scala.Enumeration). However, the current ML API has the following parameter types:
BooleanParam, DoubleArrayParam, DoubleParam, FloatParam, IntArrayParam, IntParam, LongParam, StringArrayParam

Should I introduce a new parameter type in ML API that is based on Scala Enum?

Best regards, Alexander



"
Stephen Boesch <javadba@gmail.com>,"Wed, 16 Sep 2015 18:00:36 -0700",Re: Enum parameter in ML,"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","There was a long thread about enum's initiated by Xiangrui several months
back in which the final consensus was to use java enum's.  Is that
discussion (/decision) applicable here?

2015-09-16 17:43 GMT-07:00 Ulanov, Alexander <alexander.ulanov@hpe.com>:

"
Joseph Bradley <joseph@databricks.com>,"Wed, 16 Sep 2015 18:23:06 -0700",Re: Enum parameter in ML,Stephen Boesch <javadba@gmail.com>,"@Alexander  It's worked for us to use Param[String] directly.  (I think
it's b/c String is exactly java.lang.String, rather than a Scala version of
it, so it's still Java-friendly.)  In other classes, I've added a static
list (e.g., NaiveBayes.supportedMo"
Kevin Chen <kchen@palantir.com>,"Thu, 17 Sep 2015 02:10:39 +0000",Re: New Spark json endpoints,"Kevin Chen <kchen@palantir.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","Just wanted to bring this email up again in case there were any thoughts.
Having all the information from the web UI accessible through a supported
json API is very important to us; are there any objections to us adding a v2
API to Spark?

Thanks!

From:  Kevin Chen <kchen@palantir.com>
Date:  Friday, September 11, 2015 at 11:30 AM
To:  ""dev@spark.apache.org"" <dev@spark.apache.org>
Cc:  Matt Cheah <mcheah@palantir.com>, Mingyu Kim <mkim@palantir.com>
Subject:  New Spark json endpoints

Hello Spark Devs,

 I noticed that [SPARK-3454], which introduces new json endpoints at
/api/v1/[path] for information previously only shown on the web UI, does not
expose several useful properties about Spark jobs that are exposed on the
web UI and on the unofficial /json endpoint.

 Specific examples include the maximum number of allotted cores per
application, amount of memory allotted to each slave, and number of cores
used by each worker. These are provided at â€˜app.cores, app.memoryperslave,
and worker.coresusedâ€™ in the /json endpoint, and also all appear on the web
UI page.

 Is there any specific reason that these fields are not exposed in the
public API? If not, would it be reasonable to add them to the json blobs,
possibly in a future /api/v2 API?

Thank you,
Kevin Chen



"
"""Cheng, Hao"" <hao.cheng@intel.com>","Thu, 17 Sep 2015 02:27:53 +0000",RE: Unable to acquire memory errors in HiveCompatibilitySuite,"Reynold Xin <rxin@databricks.com>, Pete Robbins <robbinspg@gmail.com>","We actually meet the similiar problem in a real case, see https://issues.apache.org/jira/browse/SPARK-10474

After checking the source code, the external sort memory management strategy seems the root cause of the issue.

Currently, we allocate the 4MB (page size) buffer as initial in the beginning of the sorting, and during the processing of each input record, we possible run into the cycle of spill => de-allocate buffer => try allocate a buffer with size x2. I know this strategy is quite flexible in some cases. However, for example in a data skew case, says 2 tasks with large amount of records runs at a single executor, the keep growing buffer strategy will eventually eat out the pre-set offheap memory threshold, and then exception thrown like what weâ€™ve seen.

I mean can we just take a simple memory management strategy for external sorter, like:
Step 1) Allocate a fixed size  buffer for the current task (maybe: MAX_MEMORY_THRESHOLD/(2 * PARALLEL_TASKS_PER_EXECUTOR))
Step 2) for (record in the input) { if (hasMemoryForRecord(record)) insert(record) else spill(buffer); insert(record); }
Step 3) Deallocate(buffer)

Probably weâ€™d better to move the discussion in ji: Thursday, September 17, 2015 12:28 AM
To: Pete Robbins
Cc: Dev
Subject: Re: Unable to acquire memory errors in HiveCompatibilitySuite

SparkEnv for the driver was created in SparkContext. The default parallelism field is set to the number of slots (max number of active tasks). Maybe we can just use the default parallelism to compute that in local mode.

On Wednesday, September 16, 2015, Pete Robbins <robbinspg@gmail.com<mailto:robbinspg@gmail.com>> wrote:
so forcing the ShuffleMemoryManager to assume 32 cores and therefore calculate a pagesize of 1MB passes the tests.
How can we determine the correct value to use in getPageSize rather than Runtime.getRuntime.availableProcessors()?

On 16 September 2015 at 10:17, Pete Robbins <robbinspg@gmail.com<javascript:_e(%7B%7D,'cvml','robbinspg@gmail.com');>> wrote:
I see what you are saying. Full stack trace:

java.io.IOException: Unable to acquire 4194304 bytes of memory
      at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPage(UnsafeExternalSorter.java:368)
      at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:349)
      at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertKVRecord(UnsafeExternalSorter.java:478)
      at org.apache.spark.sql.execution.UnsafeKVExternalSorter.insertKV(UnsafeKVExternalSorter.java:138)
      at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.switchToSortBasedAggregation(TungstenAggregationIterator.scala:489)
      at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:379)
      at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.start(TungstenAggregationIterator.scala:622)
      at org.apache.spark.sql.execution.aggregate.TungstenAggregate$$anonfun$doExecute$1.org<http://1.org>$apache$spark$sql$execution$aggregate$TungstenAggregate$$anonfun$$executePartition$1(TungstenAggregate.scala:110)
      at org.apache.spark.sql.execution.aggregate.TungstenAggregate$$anonfun$doExecute$1$$anonfun$2.apply(TungstenAggregate.scala:119)
      at org.apache.spark.sql.execution.aggregate.TungstenAggregate$$anonfun$doExecute$1$$anonfun$2.apply(TungstenAggregate.scala:119)
      at org.apache.spark.rdd.MapPartitionsWithPreparationRDD.compute(MapPartitionsWithPreparationRDD.scala:64)
      at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
      at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
      at org.apache.spark.rdd.MapPartitionsWithPreparationRDD.compute(MapPartitionsWithPreparationRDD.scala:63)
      at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
      at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
      at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:99)
      at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
      at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
      at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
      at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
      at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
      at org.apache.spark.rdd.MapPartitionsWithPreparationRDD.compute(MapPartitionsWithPreparationRDD.scala:63)
      at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
      at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
      at org.apache.spark.rdd.MapPartitionsWithPreparationRDD.compute(MapPartitionsWithPreparationRDD.scala:63)
      at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
      at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
      at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
      at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
      at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
      at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
      at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
      at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
      at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
      at org.apache.spark.scheduler.Task.run(Task.scala:88)
      at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
      at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1153)
      at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
      at java.lang.Thread.run(Thread.java:785)

On 16 September 2015 at 09:30, Reynold Xin <rxin@databricks.com<javascript:_e(%7B%7D,'cvml','rxin@databricks.com');>> wrote:
Can you paste the entire stacktrace of the error? In your original email you only included the last function call.

Maybe I'm missing something here, but I still think the bad heuristics is the issue.

Some operators pre-reserve memory before running anything in order to avoid starvation. For example, imagine we have an aggregate followed by a sort. If the aggregate is very high cardinality, and uses up all the memory and even starts spilling (falling back to sort-based aggregate), there isn't memory available at all for the sort operator to use. To work around this, each operator reserves a page of memory before they process any data.

Page size is computed by Spark using:

the total amount of execution memory / (maximum number of active tasks * 16)

and then rounded to the next power of 2, and cap between 1MB and 64MB.

That is to say, in the worst case, we should be able to reserve at least 8 pages (16 rounded up to the next power of 2).

However, in your case, the max number of active tasks is 32 (set by test env), while the page size is determined using # cores (8 in your case). So it is off by a factor of 4. As a result, with this page size, we can only reserve at least 2 pages. That is to say, if you have more than 3 operators that need page reservation (e.g. an aggregate followed by a join on the group by key followed by a shuffle - which seems to be the case of join31.q), the task can fail to reserve memory before running anything.


There is a 2nd problem (maybe this is the one you were trying to point out?) that is tasks running at the same time can be competing for memory with each other.  Spark allows each task to claim up to 2/N share of memory, where N is the number of active tasks. If a task is launched before others and hogs a lot of memory quickly, the other tasks that are launched after it might not be able to get enough memory allocation, and thus will fail. This is not super ideal, but probably fine because tasks can be retried, and can succeed in retries.


On Wed, Sep 16, 2015 at 1:07 AM, Pete Robbins <robbinspg@gmail.com<javascript:_e(%7B%7D,'cvml','robbinspg@gmail.com');>> wrote:
ok so let me try again ;-)
I don't think that the page size calculation matters apart from hitting the allocation limit earlier if the page size is too large.

If a task is going to need X bytes, it is going to need X bytes. In this case, for at least one of the tasks, X > maxmemory/no_active_tasks at some point during execution. A smaller page size may use the memory more efficiently but would not necessarily avoid this issue.
The next question would be: Is the memory limit per task of max_memory/no_active_tasks reasonable? It seems fair but if this limit is reached currently an exception is thrown, maybe the task could wait for no_active_tasks to decrease?
I think what causes my test issue is that the 32 tasks don't execute as quickly on my 8 core box so more are active at any one time.
I will experiment with the page size calculation to see what effect it has.

Cheers,


On 16 September 2015 at 06:53, Reynold Xin <rxin@databricks.com<javascript:_e(%7B%7D,'cvml','rxin@databricks.com');>> wrote:
It is exactly the issue here, isn't it?

We are using memory / N, where N should be the maximum number of active tasks. In the current master, we use the number of cores to approximate the number of tasks -- but it turned out to be a bad approximation in tests because it is set to 32 to increase concurrency.


On Tue, Sep 15, 2015 at 10:47 PM, Pete Robbins <robbinspg@gmail.com<javascript:_e(%7B%7D,'cvml','robbinspg@gmail.com');>> wrote:
Oops... I meant to say ""The page size calculation is NOT the issue here""

On 16 September 2015 at 06:46, Pete Robbins <robbinspg@gmail.com<javascript:_e(%7B%7D,'cvml','robbinspg@gmail.com');>> wrote:
The page size calculation is the issue here as there is plenty of free memory, although there is maybe a fair bit of wasted space in some pages. It is that when we have a lot of tasks each is only allowed to reach 1/n of the available memory and several of the tasks bump in to that limit. With tasks 4 times the number of cores there will be some contention and so they remain active for longer.

So I think this is a test case issue configuring the number of executors too high.

On 15 September 2015 at 18:54, Reynold Xin <rxin@databricks.com<javascript:_e(%7B%7D,'cvml','rxin@databricks.com');>> wrote:
Maybe we can change the heuristics in memory calculation to use SparkContext.defaultParallelism if it is local mode.


On Tue, Sep 15, 2015 at 10:28 AM, Pete Robbins <robbinspg@gmail.com<javascript:_e(%7B%7D,'cvml','robbinspg@gmail.com');>> wrote:
Yes and at least there is an override by setting  spark.sql.test.master to local[8] , in fact local[16] worked on my 8 core box.

I'm happy to use this as a workaround but the 32 hard-coded will fail running build/tests on a clean checkout if you only have 8 cores.

On 15 September 2015 at 17:40, Marcelo Vanzin <vanzin@cloudera.com<javascript:_e(%7B%7D,'cvml','vanzin@cloudera.com');>> wrote:
That test explicitly sets the number of executor cores to 32.

object TestHive
  extends TestHiveContext(
    new SparkContext(
      System.getProperty(""spark.sql.test.master"", ""local[32]""),

On Mon, Sep 14, 2015 at 11:22 PM, Reynold Xin <rxin@databricks.com<javascript:_e(%7B%7D,'cvml','rxin@databricks.com');>> wrote:
> Yea I think this is where the heuristics is failing -- it uses 8 cores to
> approximate the number of active tasks, but the tests somehow is using 32
> (maybe because it explicitly sets it to that, or you set it yourself? I'm
> not sure which one)
>
> On Mon, Sep 14, 2015 at 11:06 PM, Pete Robbins <robbinspg@gmail.com<javascript:_e(%7B%7D,'cvml','robbinspg@gmail.com');>> wrote:
>>
>> Reynold, thanks for replying.
>>
>> getPageSize parameters: maxMemory=515396075, numCores=0
>> Calculated values: cores=8, default=4194304
>>
>> So am I getting a large page size as I only have 8 cores?
>>
>> On 15 September 2015 at 00:40, Reynold Xin <rxin@databricks.com<javascript:_e(%7B%7D,'cvml','rxin@databricks.com');>> wrote:
>>>
>>> Pete - can you do me a favor?
>>>
>>>
>>> https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/shuffle/ShuffleMemoryManager.scala#L174
>>>
>>> Print the parameters that are passed into the getPageSize function, and
>>> check their values.
>>>
>>> On Mon, Sep 14, 2015 at 4:32 PM, Reynold Xin <rxin@databricks.com<javascript:_e(%7B%7D,'cvml','rxin@databricks.com');>> wrote:
>>>>
>>>> Is this on latest master / branch-1.5?
>>>>
>>>> out of the box we reserve only 16% (0.2 * 0.8) of the memory for
>>>> execution (e.g. aggregate, join) / shuffle sorting. With a 3GB heap, that's
>>>> 480MB. So each task gets 480MB / 32 = 15MB, and each operator reserves at
>>>> least one page for execution. If your page size is 4MB, it only takes 3
>>>> operators to use up its memory.
>>>>
>>>> The thing is page size is dynamically determined -- and in your case it
>>>> should be smaller than 4MB.
>>>> https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/shuffle/ShuffleMemoryManager.scala#L174
>>>>
>>>> Maybe there is a place that in the maven tests that we explicitly set
>>>> the page size (spark.buffer.pageSize) to 4MB? If yes, we need to find it and
>>>> just remove it.
>>>>
>>>>
>>>> On Mon, Sep 14, 2015 at 4:16 AM, Pete Robbins <robbinspg@gmail.com<javascript:_e(%7B%7D,'cvml','robbinspg@gmail.com');>>
>>>> wrote:
>>>>>
>>>>> I keep hitting errors running the tests on 1.5 such as
>>>>>
>>>>>
>>>>> - join31 *** FAILED ***
>>>>>   Failed to execute query using catalyst:
>>>>>   Error: Job aborted due to stage failure: Task 9 in stage 3653.0
>>>>> failed 1 times, most recent failure: Lost task 9.0 in stage 3653.0 (TID
>>>>> 123363, localhost): java.io.IOException: Unable to acquire 4194304 bytes of
>>>>> memory
>>>>>       at
>>>>> org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPage(UnsafeExternalSorter.java:368)
>>>>>
>>>>>
>>>>> This is using the command
>>>>> build/mvn -Pyarn -Phadoop-2.2 -Phive -Phive-thriftserver  test
>>>>>
>>>>>
>>>>> I don't see these errors in any of the amplab jenkins builds. Do those
>>>>> builds have any configuration/environment that I may be missing? My build is
>>>>> running with whatever defaults are in the top level pom.xml, eg -Xmx3G.
>>>>>
>>>>> I can make these tests pass by setting spark.shuffle.memoryFraction=0.6
>>>>> in the HiveCompatibilitySuite rather than the default 0.2 value.
>>>>>
>>>>> Trying to analyze what is going on with the test it is related to the
>>>>> number of active tasks, which seems to rise to 32, and so the
>>>>> ShuffleMemoryManager allows less memory per task even though most of those
>>>>> tasks do not have any memory allocated to them.
>>>>>
>>>>> Has anyone seen issues like this before?
>>>>
>>>>
>>>
>>
>


--
Marcelo









"
Reynold Xin <rxin@databricks.com>,"Wed, 16 Sep 2015 22:17:53 -0700",Re: New Spark json endpoints,Kevin Chen <kchen@palantir.com>,"Do we need to increment the version number if it is just strict additions?



perslave,
on the web
"
Gil Vernik <GILV@il.ibm.com>,"Thu, 17 Sep 2015 10:07:50 +0300",how to send additional configuration to the RDD after it was lazily created,Dev <dev@spark.apache.org>,"Hi,

I have the following case, which i am not sure how to resolve.

My code uses HadoopRDD and creates various RDDs on top of it 
(MapPartitionsRDD, and so on ) 
After all RDDs were lazily created, my code ""knows"" some new information 
and i want that ""compute"" method of the HadoopRDD will be aware of it (at 
the point when ""compute"" method will be called). 
What is the possible way 'to send' some additional information to the 
compute method of the HadoopRDD after this RDD is lazily created?
I tried to play with configuration, like to perform set(""test"",""111"") in 
the code and modify the compute method of HadoopRDD with get(""test"") - but 
of it's not working,  since SparkContext has only clone of the of the 
configuration and it can't be modified in run time.

Any thoughts how can i make it? 

Thanks
Gil."
Huangguowei <huangguowei@huawei.com>,"Thu, 17 Sep 2015 07:46:20 +0000","bug in Worker.scala, ExecutorRunner is not serializable",Dev <dev@spark.apache.org>,"
In Worker.scala line 480:

    case RequestWorkerState =>
      sender ! WorkerStateResponse(host, port, workerId, executors.values.toList,
        finishedExecutors.values.toList, drivers.values.toList,
        finishedDrivers.values.toList, activeMasterUrl, cores, memory,
        coresUsed, memoryUsed, activeMasterWebUiUrl)

The executors's type is:
val executors = new HashMap[String, ExecutorRunner]

but ExecutorRunner cannot be Serialized, so if ask RequestWorkerState will cause java.io.NotSerializableException.


"
Sean Owen <sowen@cloudera.com>,"Thu, 17 Sep 2015 07:53:55 +0000","Re: bug in Worker.scala, ExecutorRunner is not serializable","Huangguowei <huangguowei@huawei.com>, Dev <dev@spark.apache.org>","Did this cause an error for you?


l
"
Huangguowei <huangguowei@huawei.com>,"Thu, 17 Sep 2015 08:26:20 +0000","=?utf-8?B?562U5aSNOiBidWcgaW4gV29ya2VyLnNjYWxhLCBFeGVjdXRvclJ1bm5lciBp?=
 =?utf-8?Q?s_not_serializable?=","Sean Owen <sowen@cloudera.com>, Dev <dev@spark.apache.org>","Not error in normal case.

But if I want to ask Worker through akkaUrl to get executors status, it will cause Ex.com]
å‘é€æ—¶é—´: 2015å¹´9æœˆ17æ—¥ 15:54
æ”¶ä»¶äºº: Huangguowei; Dev
ä¸»é¢˜: Re: bug in Worker.scala, ExecutorRunner is not serializable


Did this cause an error for you?

On Thu, Sep 17, 2015, 8:51 AM Huangguowei <huangguowei@huawei.com<mailto:huangguowei@huawei.com>> wrote:

In Worker.scala line 480:

    case RequestWorkerState =>
      sender ! WorkerStateResponse(host, port, workerId, executors.values.toList,
        finishedExecutors.values.toList, drivers.values.toList,
        finishedDrivers.values.toList, activeMasterUrl, cores, memory,
        coresUsed, memoryUsed, activeMasterWebUiUrl)

The executorsâ€™s type is:
val executors = new HashMap[String, ExecutorRunner]

but ExecutorRunner cannot be Serialized, so if ask RequestWorkerState will cause java.io.NotSerializableException.


"
Huangguowei <huangguowei@huawei.com>,"Thu, 17 Sep 2015 08:36:45 +0000","re: bug in Worker.scala, ExecutorRunner is not serializable","Sean Owen <sowen@cloudera.com>, Dev <dev@spark.apache.org>","
Is it possible to get Executors status when running an application?

å‘ä»¶äºº: Sean Owen [mailto:sowen@cloudera.com]
å‘é€æ—¶é—´: 2015å¹´9æœˆ17æ—¥ 15:54
æ”¶ä»¶äºº: Huangguowei; Dev
ä¸»é¢˜: Re: bug in Worker.scala, ExecutorRunner is not serializable


Did this cause an error for you?

On Thu, Sep 17, 2015, 8:51 AM Huangguowei <huangguowei@huawei.com<mailto:huangguowei@huawei.com>> wrote:

In Worker.scala line 480:

    case RequestWorkerState =>
      sender ! WorkerStateResponse(host, port, workerId, executors.values.toList,
        finishedExecutors.values.toList, drivers.values.toList,
        finishedDrivers.values.toList, activeMasterUrl, cores, memory,
        coresUsed, memoryUsed, activeMasterWebUiUrl)

The executorsâ€™s type is:
val executors = new HashMap[String, ExecutorRunner]

but ExecutorRunner cannot be Serialized, so if ask RequestWorkerState will cause java.io.NotSerializableException.


"
Bin Wang <wbin00@gmail.com>,"Thu, 17 Sep 2015 08:50:25 +0000",QueueStream doesn't support checkpoint makes it difficult to do unit test,"""dev@spark.apache.org"" <dev@spark.apache.org>","I'm using spark streaming and use updateStateByKey, which forced to use
checkpoint. In my unit test, I create a queueStream to test. But in spark
1.5, QueueStream will throw an exception while use it with checkpoint, it
makes difficult to do unit test. Is there an option to disable this? Though
I know it will fail to recover from checkpoint but since it is a test I
don't care it.

I've found the git commit here
https://mail-archives.apache.org/mod_mbox/incubator-spark-commits/201506.mbox/%3C8efccd27016447fb8d1e0b3d9582ba78@git.apache.org%3E
"
Shixiong Zhu <zsxwing@gmail.com>,"Thu, 17 Sep 2015 17:22:40 +0800","Re: bug in Worker.scala, ExecutorRunner is not serializable","Huangguowei <huangguowei@huawei.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","RequestWorkerState is an internal message between Worker and WorkerWebUI.
Since they are in the same process, that's fine. Actually, these are not
public APIs. Could you elaborate your use case?

Best Regards,
Shixiong Zhu

2015-09-17 16:36 GMT+08:00 Huangguowei <huangguowei@huawei.com>:

¥ 15:54
alizable
l
"
Bin Wang <wbin00@gmail.com>,"Thu, 17 Sep 2015 09:40:09 +0000","Re: QueueStream doesn't support checkpoint makes it difficult to do
 unit test","""dev@spark.apache.org"" <dev@spark.apache.org>","Never mind. I've found a PR and it merged:
https://github.com/apache/spark/pull/8624/commits

Bin Wang <wbin00@gmail.com>äºŽ2015å¹´9æœˆ17æ—¥å‘¨å›› ä¸‹åˆ4:50å†™é“ï¼š

gh
mbox/%3C8efccd27016447fb8d1e0b3d9582ba78@git.apache.org%3E
"
Huangguowei <huangguowei@huawei.com>,"Thu, 17 Sep 2015 11:08:28 +0000","=?utf-8?B?562U5aSNOiBidWcgaW4gV29ya2VyLnNjYWxhLCBFeGVjdXRvclJ1bm5lciBp?=
 =?utf-8?Q?s_not_serializable?=","Shixiong Zhu <zsxwing@gmail.com>,
        ""dev@spark.apache.org""
	<dev@spark.apache.org>","Thanks for your reply. I just want to do some monitors, nil.com]
å‘é€æ—¶é—´: 2015å¹´9æœˆ17æ—¥ 17:23
æ”¶ä»¶äºº: Huangguowei; dev@spark.apache.org
ä¸»é¢˜: Re: bug in Worker.scala, ExecutorRunner is not serializable

RequestWorkerState is an internal message between Worker and WorkerWebUI. Since they are in the same process, that's fine. Actually, these are not public APIs. Could you elaborate your use case?


Best Regards,
Shixiong Zhu

2015-09-17 16:36 GMT+08:00 Huangguowei <huangguowei@huawei.com<mailto:huangguowei@huawei.com>>:

Is it possible to get Executors status when running an application?

å‘ä»¶äºº: Sean Owen [mailto:sowen@cloudera.com<mailto:sowen@cloudera.com>]
å‘é€æ—¶é—´: 2015å¹´9æœˆ17æ—¥ 15:54
æ”¶ä»¶äºº: Huangguowei; Dev
ä¸»é¢˜: Re: bug in Worker.scala, ExecutorRunner is not serializable


Did this cause an error for you?

On Thu, Sep 17, 2015, 8:51 AM Huangguowei <huangguowei@huawei.com<mailto:huangguowei@huawei.com>> wrote:

In Worker.scala line 480:

    case RequestWorkerState =>
      sender ! WorkerStateResponse(host, port, workerId, executors.values.toList,
        finishedExecutors.values.toList, drivers.values.toList,
        finishedDrivers.values.toList, activeMasterUrl, cores, memory,
        coresUsed, memoryUsed, activeMasterWebUiUrl)

The executorsâ€™s type is:
val executors = new HashMap[String, ExecutorRunner]

but ExecutorRunner cannot be Serialized, so if ask RequestWorkerState will cause java.io.NotSerializableException.



"
gsvic <victorasgs@gmail.com>,"Thu, 17 Sep 2015 05:23:05 -0700 (MST)",RDD: Execution and Scheduling,dev@spark.apache.org,"After reading some parts of Spark source code I would like to make some
questions about RDD execution and scheduling.

At first, please correct me if I am wrong at the following:
1) The number of partitions equals to the number of tasks will be executed
in parallel (e.g. , when an RDD is repartitioned in 30 partitions, a count
aggregate will be executed in 30 tasks distributed in the cluster)

2) A  task
<https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/Task.scala>  
concerns only one partition (partitionId: Int) and this partition maps to an
RDD block.

3) If and RDD is cached, then the preferred location for execution of this
Partition and the corresponding RDD block will be the node the data is
cached in.

The questions are the following:

I run some SQL aggregate functions on a TPCH dataset. The cluster is
consisted of 7 executors (and one driver) each one contains 8 GB RAM and 4
VCPUs. The dataset is in Parquet file format in an external Hadoop Cluster,
that is, Spark workers and Hadoop DataNodes are running on different VMs.

1) For a count aggregate, I repartitioned the DataFrame into 24 partitions
and each executor took 2 partitions(tasks) for execution. Is that always
happens the same way (the number of tasks per node is equal to
#Partitions/#Workers) ?

2) How Spark chooses the executor for each task if the data is not cached?
It's clear what happens if the data is cached in  DAGScheduler.scala
<https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L1541> 
, but what if is not? Is it possible to determine that before execution?

3) In the case of an SQL Join operation, is it possible to determine how
many tasks/partitions will be generated and in which worker each task be
submitted?



--

---------------------------------------------------------------------


"
Imran Rashid <irashid@cloudera.com>,"Thu, 17 Sep 2015 09:30:15 -0500",Re: New Spark json endpoints,Kevin Chen <kchen@palantir.com>,"Hi Kevin,

I think it would be great if you added this.  It never got added in the
first place b/c the original PR was already pretty bloated, and just never
got back to this.  I agree with Reynold -- you shouldn't need to increase
the version for just adding new endpoints (or even adding new fields to
existing endpoints).  See the guarantees we make here:

http://spark.apache.org/docs/latest/monitoring.html#rest-api

(Though if you think we should make different guarantees around versions,
that would be worth discussing as well.)

Can you file a jira, and we move discussion there?  Please cc me, and maybe
also Josh Rosen (I'm not sure if he has cycles now but he's been very
helpful on these issues in the past).

thanks,
Imran



perslave,
on the web
"
Mark Hamstra <mark@clearstorydata.com>,"Thu, 17 Sep 2015 08:00:06 -0700",Re: New Spark json endpoints,Imran Rashid <irashid@cloudera.com>,"While we're at it, adding endpoints that get results by jobGroup (cf.
SparkContext#setJobGroup) instead of just for a single Job would also be
very useful to some of us.


r
ry
.
d
a
n
s
yperslave,
 on the web
,
"
Hao Ren <invkrh@gmail.com>,"Thu, 17 Sep 2015 17:07:10 +0200",[MLlib] BinaryLogisticRegressionSummary on test set,dev@spark.apache.org,"Working on spark.ml.classification.LogisticRegression.scala (spark 1.5),

It might be useful if we can create a summary for any given dataset, not
just training set.
Actually, BinaryLogisticRegressionTrainingSummary  is only created when
model is computed based on training set.
As usual, we need to summary test set to know about the model performance.
However, we can not create our own BinaryLogisticRegressionSummary for
other date set (of type DataFrame), because the Summary class is ""private""
in classification package.

Would it be better to remove the ""private"" access modifier and allow the
following code on user side:

val lr = new LogisticRegression()

val model = lr.fit(trainingSet)

val binarySummary =
  new BinaryLogisticRegressionSummary(
    model.transform(testSet),
    lr.probabilityCol,
    lr.labelCol
  )

binarySummary.roc


Thus, we can use the model to summary any data set we want.

If there is a way to summary test set, please let me know. I have browsed
LogisticRegression.scala, but failed to find one.

Thx.

-- 
Hao Ren

Data Engineer @ leboncoin

Paris, France
"
Feynman Liang <fliang@databricks.com>,"Thu, 17 Sep 2015 09:44:16 -0700",Re: [MLlib] BinaryLogisticRegressionSummary on test set,Hao Ren <invkrh@gmail.com>,"We have kept that private because we need to decide on a name for the
method which evaluates on a test set (see the TODO comment
<https://github.com/apache/spark/pull/7099/files#diff-668c79317c51f40df870d3404d8a731fR272>);
perhaps you could push for this to happen by creating a Jira and pinging
jkbradley and mengxr. Thanks!


"
Kevin Chen <kchen@palantir.com>,"Thu, 17 Sep 2015 17:04:37 +0000",Re: New Spark json endpoints,"Mark Hamstra <mark@clearstorydata.com>, Imran Rashid
	<irashid@cloudera.com>","Thank you all for the feedback. Iâ€™ve created a corresponding JIRA ticket at
https://issues.apache.org/jira/browse/SPARK-10565, updated with a summary of
this thread.

From:  Mark Hamstra <mark@clearstorydata.com>
Date:  Thursday, September 17, 2015 at 8:00 AM
To:  Imran Rashid <irashid@cloudera.com>
Cc:  Kevin Chen <kchen@palantir.com>, ""dev@spark.apache.org""
<dev@spark.apache.org>, Matt Cheah <mcheah@palantir.com>, Mingyu Kim
<mkim@palantir.com>
Subject:  Re: New Spark json endpoints

While we're at it, adding endpoints that get results by jobGroup (cf.
SparkContext#setJobGroup) instead of just for a single Job would also be
very useful to some of us.

irst
back
ion
s_lat
BrZ4t
vb-5y
 that
be
pful
.
d
a v2
 not
e web
s
ave,
e web
ublic
bly



"
Reynold Xin <rxin@databricks.com>,"Thu, 17 Sep 2015 10:10:39 -0700",Re: RDD: Execution and Scheduling,gsvic <victorasgs@gmail.com>,"Your understanding is mostly correct. Replies inline.



No that is not always true. If a node is slower than others, less tasks
will get scheduled there. Or if a node is busy running some other thing,
maybe no tasks will be scheduled there.



RDD itself still has preferred locations.




Not sure what you mean - right now for shuffle join it is hard coded by to
200 partitions, and the scheduler randomly chooses the executors to do
joins.

For broadcast join, there is no shuffle. Tasks are scheduled based on the
locality of the large fact table.
"
Debasish Das <debasish.das83@gmail.com>,"Thu, 17 Sep 2015 10:47:14 -0700",Re: RDD API patterns,Robin East <robin.east@xense.co.uk>,"Rdd nesting can lead to recursive nesting...i would like to know the
usecase and why join can't support it...you can always expose an api over a
rdd and access that in another rdd mappartition...use a external data
source like hbase cassandra redis to support the api...

For ur case group by and then pass the logic...collect each group sample in
a seq and then lookup if u r doing one at a time...if doing all try joining
it...pattern is common if every key is a iid and you a cross validating a
model for each key on 80% train 20% test...

We are looking to fit it in pipeline flow...with minor mods it will fit..

"
Luciano Resende <luckbr1975@gmail.com>,"Thu, 17 Sep 2015 12:42:06 -0700",Re: JDBC Dialect tests,Reynold Xin <rxin@databricks.com>,"Thanks Reynold,

Also, what is the status of the associated PR are we planning to merge it
soon ? This will help me with the Db2 dialect test framework using Docker.

Thanks

[1] https://github.com/apache/spark/pull/8101




-- 
Luciano Resende
http://people.apache.org/~lresende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
gsvic <victorasgs@gmail.com>,"Thu, 17 Sep 2015 13:52:01 -0700 (MST)",Re: RDD: Execution and Scheduling,dev@spark.apache.org,"Concerning answers 1 and 2:

1) How Spark determines a node as a ""slow node"" and how slow is that?

2) How an RDD choose a location as a preferred location and with which
criteria?

Could you please also include the links of the source files for the two
questions above?



--

---------------------------------------------------------------------


"
Mingyu Kim <mkim@palantir.com>,"Fri, 18 Sep 2015 06:20:34 +0000",Re: And.eval short circuiting,"Reynold Xin <rxin@databricks.com>, Zack Sampson <zsampson@palantir.com>","That sounds good. I think the optimizer should not change the behavior of execution and reordering the filters can easily result in errors as exemplified below. I agree that the optimizer should not reorder the filters for correctness. Please correct me if I have an incorrect assumption about the guarantees of the optimizer.

Is there a bug filed that tracks the change you suggested below, btw? Iâ€™d like to follow the issue, if thereâ€™s one.

Thanks,
Mingyu

From:  Reynold Xin
Date:  Wednesday, September 16, 2015 at 1:17 PM
To:  Zack Sampson
Cc:  ""dev@spark.apache.org"", Mingyu Kim, Peter Faiman, Matt Cheah, Michael Armbrust
Subject:  Re: And.eval short circuiting

This is ""expected"" in the sense that DataFrame operations can get re-ordered under the hood by the optimizer. For example, if the optimizer deems it is cheaper to apply the 2nd filter first, it might re-arrange the filters. In reality, it doesn't do that. I think this is too confusing and violates principle of least astonishment, so we should fix it. 

I discussed more with Michael offline, and think we can add a rule for the physical filter operator to replace the general AND/OR/equality/etc with a special version that treats null as false. This rule needs to be carefully written because it should only apply to subtrees of AND/OR/equality/etc (e.g. it shouldn't rewrite children of isnull).


:
I see. We're having problems with code like this (forgive my noob scala):
val df = Seq((""moose"",""ice""), (null,""fire"")).toDF(""animals"", ""elements"")
df
  .filter($""animals"".rlike("".*""))
  .filter(callUDF({(value: String) => value.length > 2}, BooleanType, $""animals""))
.collect()
This code throws a NPE because:
* Catalyst combines the filters with an AND
* the first filter passes returns null on the first input
* the second filter tries to read the length of that null

This feels weird. Reading that code, I wouldn't expect null to be passed to the second filter. Even weirder is that if you call collect() after the first filter you won't see nulls, and if you write the data to disk and reread it, the NPE won't happen.

It's bewildering! Is this the intended behavior?
From: Reynold Xin [rxin@databricks.com]
Sent: Monday, September 14, 2015 10:14 PM
To: Zack Sampson
Cc: dev@spark.apache.org
Subject: Re: And.eval short circuiting

rxin=# select null and true;
 ?column? 
----------
 
(1 row)

rxin=# select null and false;
 ?column? 
----------
 f
(1 row)


null and false should return false.


:
It seems like And.eval can avoid calculating right.eval if left.eval returns null. Is there a reason it's written like it is? 

override def eval(input: Row): Any = {
  val l = left.eval(input)
  if (l == false) {
    false
  } else {
    val r = right.eval(input)
    if (r == false) {
      false
    } else {
      if (l != null && r != null) {
        true
      } else {
        null
      }
    }
  }
}



"
Reynold Xin <rxin@databricks.com>,"Thu, 17 Sep 2015 23:22:02 -0700",Re: And.eval short circuiting,Mingyu Kim <mkim@palantir.com>,"Please file a ticket and cc me. Thanks.



â€™d
e
d
e
a
y
:
"")
$""animals""))
he
"
Reynold Xin <rxin@databricks.com>,"Fri, 18 Sep 2015 00:10:37 -0700","=?UTF-8?Q?Re=3A_=E7=AD=94=E5=A4=8D=3A_bug_in_Worker=2Escala=2C_ExecutorRunner_is_n?=
	=?UTF-8?Q?ot_serializable?=",Huangguowei <huangguowei@huawei.com>,"Maybe we should add some inline comment explaining why it is ok for that
message to be not serializable.


:

¥ 17:23
alizable
¥ 15:54
alizable
l
"
Hao Ren <invkrh@gmail.com>,"Fri, 18 Sep 2015 11:47:53 +0200",Re: [MLlib] BinaryLogisticRegressionSummary on test set,Feynman Liang <fliang@databricks.com>,"Thank you for the reply.

I have created a jira issue and pinged mengxr.

Here is the link: https://issues.apache.org/jira/browse/SPARK-10691

I did not find jkbradley on jira. I saw he is on github.

BTW, should I create a pull request on removing the private modifier for
further discussion ?

Thx.




-- 
Hao Ren

Data Engineer @ leboncoin

Paris, France
"
sim <sim@swoop.com>,"Fri, 18 Sep 2015 05:27:34 -0700 (MST)",Re: RDD API patterns,dev@spark.apache.org,"Thanks everyone for the comments! I waited for more replies to come before I
responded as I was interested in the community's opinion. 

The thread I'm noticing in this thread (pun intended) is that most responses
focus on the nested RDD issue. I think we all agree that it is problematic
for many reasons, including not just implementation complexity but also
user-facing complexity (programming model, processing patterns, debugging,
etc.).

What about a much simpler approach? Rather than producing RDDs with
Iterable[T], why not produce RDDs with SparkIterable[T]? Then, let's look at
the *RDD APIs and decide which methods would be useful to have there. The
simplest rule I can think of is anything that does not involve context, job
or partitioning in any form, therefore implicitly protecting the
RDD/partitioning abstractions underneath. Instead of returning RDDs these
functions in SparkIterable will produce SparkIterables. 

The benefits are significant: API consistency, programming model
simplicity/consistency, greater leverage of non-trivial community code such
as sampling, approximate counting, reuse of user code for reducers, etc. The
cost is only the implementation effort of the new methods. No change to the
process model. No nested RDDs.

Here is a quick list of the methods we can do this for. Not all need to be
available at once: this is directional. This list is alphabetical, /not/ in
priority order of value.

*From RDD*
aggregate
count
countApprox
countApproxDistinct
countByValue
countByValueApprox
pipe
randomSplit
sample
sortBy
takeOrdered
takeSample
treeAggregate
treeReduce
union
zipWithUniqueId
aggregateByKey

*From PairRDDFunctions*
combineByKey
countApproxDistinctByKey
countByKey
flatMapValues
foldByKey
groupByKey
keys
lookup
mapValues
reduceByKey
sampleByKey
sampleByKeyExact
values

Here is another way to look at this: I am not sure why these methods, whose
signatures have nothing to do with partitions or partitioning, were defined
directly on RDDs as opposed to in some abstract trait. How a method is
implemented is a separate concern from how APIs should be designed. Had
these methods been put in a trait early on in the life of Spark, it would
have been natural to expose them to Spark-backed Iterables. Since this was
not done, we look at them and tell ourselves ""we can't do this because we
can't have RDD nesting"" which is not the real issue as the implementations
of these methods in a SparkIterable don't need access to any RDD APIs. In
fact, many implementations would be one-liners using the underlying Scala
Iterable API:

// Purely for illustration
implicit class PairSparkIterableFunctions[K, V](self: SparkIterable[(K, V)])
extends SparkIterable[(K, V)] {
  def groupByKey() = groupBy(_._1).map { case (k, valuePairs) => (k,
valuePairs.map(_._2)) }
}

The Spark community has decided that the RDD methods are important for large
scale data processing so let's make them available to all data Spark touches
while avoiding the nested RDD mess. 

What do you think about this approach?

P.S. In a previous life I built developer tools, APIs and standards used by
simple, high-level APIs based on consistent patterns substantially
accelerate the growth of communities. Conversely, lack of either high-level
abstractions or consistency introduces friction. Because of the iterative
nature of development, even small amounts of friction meaningfully slow down
adoption. Further, simplicity of high-level APIs and consistency always beat
capability & performance in terms of how the mass of developers make
technology choices. I have found no exceptions to this, which is why I
wanted to bring the issue with the RDD API up here.




--

---------------------------------------------------------------------


"
sim <sim@swoop.com>,"Fri, 18 Sep 2015 05:28:54 -0700 (MST)",Re: RDD API patterns,dev@spark.apache.org,"Aniket, yes, I've done the separate file trick. :) Still, I think we can
solve this problem without nested RDDs. 



--

---------------------------------------------------------------------


"
sim <sim@swoop.com>,"Fri, 18 Sep 2015 05:38:01 -0700 (MST)",Re: RDD API patterns,dev@spark.apache.org,"Juan, thanks for sharing this. I am facing what looks like a similar issue
having to do with variable grouped upsampling (sampling some groups at
different rates, sometimes > 100%). I will study the approach you took.

As for the topic of this thread, I think it is important to separate two
issues:

- Logical RDD-style operations on Iterables
- Physical RDD-style operations on partitioned data

Issues related to nested RDDs, jobs and the scheduler only apply to the
latter unless we want to heavily optimize the performance of the former. I
wouldn't do that until we see enough usage of the former to know what's
worth optimizing.

Thanks,
Sim



--

---------------------------------------------------------------------


"
sim <sim@swoop.com>,"Fri, 18 Sep 2015 05:45:58 -0700 (MST)",Re: RDD API patterns,dev@spark.apache.org,"Robin, my point exactly. When an API is valuable, let's expose it in a way
that it may be used easily for all data Spark touches. It should not require
much development work to implement the sampling logic to work for an
Iterable as opposed to an RDD.



--

---------------------------------------------------------------------


"
sim <sim@swoop.com>,"Fri, 18 Sep 2015 05:55:26 -0700 (MST)",Re: RDD API patterns,dev@spark.apache.org,"@debasish83, yes, there are many ways to optimize and work around the
limitation of no nested RDDs. The point of this thread is to discuss the API
patterns of Spark in order to make the platform more accessible to lots of
developers solving interesting pr"
Shixiong Zhu <zsxwing@gmail.com>,"Fri, 18 Sep 2015 23:50:55 +0800","=?UTF-8?Q?Re=3A_=E7=AD=94=E5=A4=8D=3A_bug_in_Worker=2Escala=2C_ExecutorRunner_is_n?=
	=?UTF-8?Q?ot_serializable?=",Reynold Xin <rxin@databricks.com>,"I'm wondering if we should create a tag trait (e.g., LocalMessage) for
messages like this and add the comment in the trait. Looks better than
adding inline comments for all these messages.

Best Regards,
Shixiong Zhu

2015-09-18 15:10 GMT+08:00 Reynold Xin <rxin@databricks.com>:

¥ 17:23
ializable
.
¥ 15:54
ializable
:
"
Feynman Liang <fliang@databricks.com>,"Fri, 18 Sep 2015 10:04:45 -0700",Re: [MLlib] BinaryLogisticRegressionSummary on test set,Hao Ren <invkrh@gmail.com>,"If you have the time, submitting a PR for it would be awesome! However, our
review bandwidth is limited so you should not expect it to get immediately
reviewed. Let's continue discussion of the name on JIRA


"
Reynold Xin <rxin@databricks.com>,"Fri, 18 Sep 2015 10:51:39 -0700","=?UTF-8?Q?Re=3A_=E7=AD=94=E5=A4=8D=3A_bug_in_Worker=2Escala=2C_ExecutorRunner_is_n?=
	=?UTF-8?Q?ot_serializable?=",Shixiong Zhu <zsxwing@gmail.com>,"Sounds good.



¥ 17:23
rializable
lly,
¥ 15:54
rializable
"
Mingyu Kim <mkim@palantir.com>,"Fri, 18 Sep 2015 18:18:10 +0000",Re: And.eval short circuiting,Reynold Xin <rxin@databricks.com>,"I filed SPARK-10703. Thanks!

Mingyu

From:  Reynold Xin
Date:  Thursday, September 17, 2015 at 11:22 PM
To:  Mingyu Kim
Cc:  Zack Sampson, ""dev@spark.apache.org"", Peter Faiman, Matt Cheah, Michael Armbrust
Subject:  Re: And.eval short circuiting

Please file a ticket and cc me. Thanks. 


That sounds good. I think the optimizer should not change the behavior of execution and reordering the filters can easily result in errors as exemplified below. I agree that the optimizer should not reorder the filters for correctness. Please correct me if I have an incorrect assumption about the guarantees of the optimizer.

Is there a bug filed that tracks the change you suggested below, btw? Iâ€™d like to follow the issue, if thereâ€™s one.

Thanks,
Mingyu

From: Reynold Xin
Date: Wednesday, September 16, 2015 at 1:17 PM
To: Zack Sampson
Cc: ""dev@spark.apache.org"", Mingyu Kim, Peter Faiman, Matt Cheah, Michael Armbrust 

Subject: Re: And.eval short circuiting

This is ""expected"" in the sense that DataFrame operations can get re-ordered under the hood by the optimizer. For example, if the optimizer deems it is cheaper to apply the 2nd filter first, it might re-arrange the filters. In reality, it doesn't do that. I think this is too confusing and violates principle of least astonishment, so we should fix it. 

I discussed more with Michael offline, and think we can add a rule for the physical filter operator to replace the general AND/OR/equality/etc with a special version that treats null as false. This rule needs to be carefully written because it should only apply to subtrees of AND/OR/equality/etc (e.g. it shouldn't rewrite children of isnull).


:
I see. We're having problems with code like this (forgive my noob scala):
val df = Seq((""moose"",""ice""), (null,""fire"")).toDF(""animals"", ""elements"")
df
  .filter($""animals"".rlike("".*""))
  .filter(callUDF({(value: String) => value.length > 2}, BooleanType, $""animals""))
.collect()
This code throws a NPE because:
* Catalyst combines the filters with an AND
* the first filter passes returns null on the first input
* the second filter tries to read the length of that null

This feels weird. Reading that code, I wouldn't expect null to be passed to the second filter. Even weirder is that if you call collect() after the first filter you won't see nulls, and if you write the data to disk and reread it, the NPE won't happen.

It's bewildering! Is this the intended behavior?
From: Reynold Xin [rxin@databricks.com]
Sent: Monday, September 14, 2015 10:14 PM
To: Zack Sampson
Cc: dev@spark.apache.org
Subject: Re: And.eval short circuiting

rxin=# select null and true;
 ?column? 
----------
 
(1 row)

rxin=# select null and false;
 ?column? 
----------
 f
(1 row)


null and false should return false.


:
It seems like And.eval can avoid calculating right.eval if left.eval returns null. Is there a reason it's written like it is? 

override def eval(input: Row): Any = {
  val l = left.eval(input)
  if (l == false) {
    false
  } else {
    val r = right.eval(input)
    if (r == false) {
      false
    } else {
      if (l != null && r != null) {
        true
      } else {
        null
      }
    }
  }
}




"
Josh Rosen <joshrosen@databricks.com>,"Fri, 18 Sep 2015 13:17:14 -0700",Does anyone use ShuffleDependency directly?,"Dev <dev@spark.apache.org>, user <user@spark.apache.org>","Does anyone use ShuffleDependency
<https://github.com/apache/spark/blob/00a2911c5bea67a1a4796fb1d6fd5d0a8ee79001/core/src/main/scala/org/apache/spark/Dependency.scala#L70>
directly in their Spark code or libraries? If so, how do you use it?

Similarly, does anyone use ShuffleHandle
<https://github.com/apache/spark/blob/00a2911c5bea67a1a4796fb1d6fd5d0a8ee79001/core/src/main/scala/org/apache/spark/shuffle/ShuffleHandle.scala#L27>
directly?
"
"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Fri, 18 Sep 2015 22:57:44 +0000",,"""dev@spark.apache.org"" <dev@spark.apache.org>","Dear Spark developers,

Is it possible (and how to do it if possible) to pick one element per physical node from an RDD? Let's say the first element of any partition on that node. The result would be an RDD[element], the count of elements is equal to the N of nodes that has partitions of the initial RDD.

Best regards, Alexander
"
Feynman Liang <fliang@databricks.com>,"Fri, 18 Sep 2015 16:06:01 -0700",,"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","rdd.mapPartitions(x => new Iterator(x.head))

m

tition on
"
"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Fri, 18 Sep 2015 23:09:30 +0000",,Feynman Liang <fliang@databricks.com>,"Thank you! How can I guarantee that I have only one element per executor (per worker, or per physical node)?

From: Feynman Liang [mailto:fliang@databricks.com]
Sent: Friday, September 18, 2015 4:06 PM
To: Ulanov, Alexander
Cc: dev@spark.apache.org
Subject: Re: One element per node

rdd.mapPartitions(x => new Iterator(x.head))

On Fri, Sep 18, 2015 at 3:57 PM, Ulanov, Alexander <alexander.ulanov@hpe.com<mailto:alexander.ulanov@hpe.com>> wrote:
Dear Spark developers,

Is it possible (and how to do it if possible) to pick one element per physical node from an RDD? Letâ€™s say the first element of any partition on that node. The result would be an RDD[element], the count of elements is equal to the N of nodes that has partitions of the initial RDD.

Best regards, Alexander

"
Reynold Xin <rxin@databricks.com>,"Fri, 18 Sep 2015 16:37:06 -0700",,"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Use a global atomic boolean and return nothing from that partition if the
boolean is true.

Note that your result won't be deterministic.


Thank you! How can I guarantee that I have only one element per executor
(per worker, or per physical node)?



*From:* Feynman Liang [mailto:fliang@databricks.com <fliang@databricks.com>]

*Sent:* Friday, September 18, 2015 4:06 PM
*To:* Ulanov, Alexander
*Cc:* dev@spark.apache.org



rdd.mapPartitions(x => new Iterator(x.head))



m>

Dear Spark developers,



Is it possible (and how to do it if possible) to pick one element per
physical node from an RDD? Letâ€™s say the first element of any partition on
that node. The result would be an RDD[element], the count of elements is
equal to the N of nodes that has partitions of the initial RDD.



Best regards, Alexander
"
Feynman Liang <fliang@databricks.com>,"Fri, 18 Sep 2015 16:51:25 -0700",,"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","AFAIK the physical distribution is not exposed in the public API; the
closest I can think of is
`rdd.coalesce(numPhysicalNodes).mapPartitions(...` but this assumes that
one partition exists per node

m

tition on
"
"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Sat, 19 Sep 2015 00:53:29 +0000",,Reynold Xin <rxin@databricks.com>,"Sounds interesting! Is it possible to make it deterministic by using global long value and get the element on partition only if someFunction(partitionId, globalLong)==true? Or by using some specific partitioner that creates such partitionIds that can be decomposed into nodeId and number of partitions per node?

From: Reynold Xin [mailto:rxin@databricks.com]
Sent: Friday, September 18, 2015 4:37 PM
To: Ulanov, Alexander
Cc: Feynman Liang; dev@spark.apache.org
Subject: Re: One element per node

Use a global atomic boolean and return nothing from that partition if the boolean is true.

Note that your result won't be deterministic.

On Sep 18, 2015, at 4:11 PM, Ulanov, Alexander <alexander.ulanov@hpe.com<mailto:alexander.ulanov@hpe.com>> wrote:
Thank you! How can I guarantee that I have only one element per executor (per worker, or per physical node)?

From: Feynman Liang [mailto:fliang@databricks.com]
Sent: Friday, September 18, 2015 4:06 PM
To: Ulanov, Alexander
Cc: dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: One element per node

rdd.mapPartitions(x => new Iterator(x.head))

On Fri, Sep 18, 2015 at 3:57 PM, Ulanov, Alexander <alexander.ulanov@hpe.com<mailto:alexander.ulanov@hpe.com>> wrote:
Dear Spark developers,

Is it possible (and how to do it if possible) to pick one element per physical node from an RDD? Letâ€™s say the first element of any partition on that node. The result would be an RDD[element], the count of elements is equal to the N of nodes that has partitions of the initial RDD.

Best regards, Alexander

"
Reynold Xin <rxin@databricks.com>,"Fri, 18 Sep 2015 18:57:08 -0700",,"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","The reason it is nondeterministic is because tasks are not always scheduled
to the same nodes -- so I don't think you can make this deterministic.

If you assume no failures and tasks take a while to run (so it runs slower
than the scheduler can schedule them), then I think you can make it
deterministic by setting spark.locality.wait to a really high number, and
coalescing everything into just N partitions, where N = the number of
machines.




m

c
tition on
"
shane knapp <sknapp@berkeley.edu>,"Sat, 19 Sep 2015 00:28:56 -0700","BUILD SYSTEM: fire and power event at UC berkeley's IST colo, jenkins offline","dev <dev@spark.apache.org>, Jon Kuroda <jkuroda@eecs.berkeley.edu>, 
	Matthew L Massie <massie@berkeley.edu>","TL; DR:  jenkins is currently down and will probably not be brought
back up until monday morning.

a machine caught fire in the colo this evening, and this tripped the
halon, and now IST is overheating...  it looks like it may have been
one of our servers that popped and caused the event, and thankfully no
one was hurt.

http://ucbsystems.org/

amplab jenkins is currently down.  some ot her university services are
also down as well.

jon is currently at the colo unplugging the remaining machines of the
type that caught fire and we've reached out to the vendor who supplied
them to see about an investigation.

IST staff will be starting their investigation tomorrow morning, and
jon or i will post some updates as soon as we get them.

sorry for the inconvenience,

shane

---------------------------------------------------------------------


"
Steve Loughran <stevel@hortonworks.com>,"Sat, 19 Sep 2015 10:58:32 +0000","Re: BUILD SYSTEM: fire and power event at UC berkeley's IST colo,
 jenkins offline",shane knapp <sknapp@berkeley.edu>,"


hope things recover: once a rack has overheated you are in trouble.

I know some clusters that keep the ToR switches in middle of the racks for this reason: its less exposed to the hot air near the ceiling, so the most valuable H/W on the rack gets more protection.

As an added benefit: your ether cables are shorter, which, when you go to 4x1 bonded, makes a big difference in cost.



---------------------------------------------------------------------


"
StanZhai <mail@zhaishidan.cn>,"Sat, 19 Sep 2015 04:43:28 -0700 (MST)",[SparkSQL]How does spark handle a parquet file in parallel?,dev@spark.apache.org,"Hi all,

I'm using Spark (1.4.1) + Hive (0.13.1), I found that a large number of
network IO appeared when query a parquet table *with only one part file* use
SparkSQL. 

The SQL is: SELECT concat(year(fkbb5855f0), ""-"", month(fkbb5855f0), ""-"",
day(fkbb5855f0), "" 00:00:00""),COUNT(fk919b1d80) FROM test WHERE fkbb5855f0
&gt;= '2015-08-02&nbsp;00:00:00' AND fkbb5855f0 &lt;
'2015-09-01&nbsp;00:00:00' AND fk418c5509 IN ('add_summary') AND (fkbb5855f0
!= '' AND fkbb5855f0 is not NULL) GROUP BY year(fkbb5855f0),
month(fkbb5855f0), day(fkbb5855f0)

The SQL's query explain is:

== Parsed Logical Plan ==
'Limit 10000
&nbsp;'Aggregate ['year('fkbb5855f0),'month('fkbb5855f0),'day('fkbb5855f0)],
['concat('year('fkbb5855f0),-,'month('fkbb5855f0),-,'day('fkbb5855f0),
00:00:00) AS _c0#14,COUNT('fk919b1d80) AS _c1#15]
&nbsp; 'Filter (((('fkbb5855f0 &gt;= 2015-08-02 00:00:00) &amp;&amp;
('fkbb5855f0 &lt; 2015-09-01 00:00:00)) &amp;&amp; 'fk418c5509 IN
(add_summary)) &amp;&amp; (NOT ('fkbb5855f0 = ) &amp;&amp; IS NOT NULL
'fkbb5855f0))
&nbsp; &nbsp;'UnresolvedRelation [test], None

== Analyzed Logical Plan ==
_c0: string, _c1: bigint
Limit 10000
&nbsp;Aggregate
[HiveSimpleUdf#org.apache.hadoop.hive.ql.udf.UDFYear(fkbb5855f0#36),HiveSimpleUdf#org.apache.hadoop.hive.ql.udf.UDFMonth(fkbb5855f0#36),HiveSimpleUdf#org.apache.hadoop.hive.ql.udf.UDFDayOfMonth(fkbb5855f0#36)],
[HiveGenericUdf#org.apache.hadoop.hive.ql.udf.generic.GenericUDFConcat(HiveSimpleUdf#org.apache.hadoop.hive.ql.udf.UDFYear(fkbb5855f0#36),-,HiveSimpleUdf#org.apache.hadoop.hive.ql.udf.UDFMonth(fkbb5855f0#36),-,HiveSimpleUdf#org.apache.hadoop.hive.ql.udf.UDFDayOfMonth(fkbb5855f0#36),
00:00:00) AS _c0#14,COUNT(fk919b1d80#34) AS _c1#15L]
&nbsp; Filter ((((fkbb5855f0#36 &gt;= 2015-08-02 00:00:00) &amp;&amp;
(fkbb5855f0#36 &lt; 2015-09-01 00:00:00)) &amp;&amp; fk418c5509#35 IN
(add_summary)) &amp;&amp; (NOT (fkbb5855f0#36 = ) &amp;&amp; IS NOT NULL
fkbb5855f0#36))
&nbsp; &nbsp;Subquery test
&nbsp; &nbsp;
Relation[fkb80bb774#33,fk919b1d80#34,fk418c5509#35,fkbb5855f0#36]
org.apache.spark.sql.parquet.ParquetRelation2@5a271032

== Optimized Logical Plan ==
Limit 10000
&nbsp;Aggregate
[HiveSimpleUdf#org.apache.hadoop.hive.ql.udf.UDFYear(fkbb5855f0#36),HiveSimpleUdf#org.apache.hadoop.hive.ql.udf.UDFMonth(fkbb5855f0#36),HiveSimpleUdf#org.apache.hadoop.hive.ql.udf.UDFDayOfMonth(fkbb5855f0#36)],
[HiveGenericUdf#org.apache.hadoop.hive.ql.udf.generic.GenericUDFConcat(HiveSimpleUdf#org.apache.hadoop.hive.ql.udf.UDFYear(fkbb5855f0#36),-,HiveSimpleUdf#org.apache.hadoop.hive.ql.udf.UDFMonth(fkbb5855f0#36),-,HiveSimpleUdf#org.apache.hadoop.hive.ql.udf.UDFDayOfMonth(fkbb5855f0#36),
00:00:00) AS _c0#14,COUNT(fk919b1d80#34) AS _c1#15L]
&nbsp; Project [fkbb5855f0#36,fk919b1d80#34]
&nbsp; &nbsp;Filter ((((fkbb5855f0#36 &gt;= 2015-08-02 00:00:00) &amp;&amp;
(fkbb5855f0#36 &lt; 2015-09-01 00:00:00)) &amp;&amp; fk418c5509#35 INSET
(add_summary)) &amp;&amp; (NOT (fkbb5855f0#36 = ) &amp;&amp; IS NOT NULL
fkbb5855f0#36))
&nbsp; &nbsp;
Relation[fkb80bb774#33,fk919b1d80#34,fk418c5509#35,fkbb5855f0#36]
org.apache.spark.sql.parquet.ParquetRelation2@5a271032

== Physical Plan ==
Limit 10000
&nbsp;Aggregate false, [PartialGroup#42,PartialGroup#43,PartialGroup#44],
[HiveGenericUdf#org.apache.hadoop.hive.ql.udf.generic.GenericUDFConcat(PartialGroup#42,-,PartialGroup#43,-,PartialGroup#44,
00:00:00) AS _c0#14,Coalesce(SUM(PartialCount#41L),0) AS _c1#15L]
&nbsp; Exchange (HashPartitioning 200)
&nbsp; &nbsp;Aggregate true,
[HiveSimpleUdf#org.apache.hadoop.hive.ql.udf.UDFYear(fkbb5855f0#36),HiveSimpleUdf#org.apache.hadoop.hive.ql.udf.UDFMonth(fkbb5855f0#36),HiveSimpleUdf#org.apache.hadoop.hive.ql.udf.UDFDayOfMonth(fkbb5855f0#36)],
[HiveSimpleUdf#org.apache.hadoop.hive.ql.udf.UDFYear(fkbb5855f0#36) AS
PartialGroup#42,HiveSimpleUdf#org.apache.hadoop.hive.ql.udf.UDFMonth(fkbb5855f0#36)
AS
PartialGroup#43,HiveSimpleUdf#org.apache.hadoop.hive.ql.udf.UDFDayOfMonth(fkbb5855f0#36)
AS PartialGroup#44,COUNT(fk919b1d80#34) AS PartialCount#41L]
&nbsp; &nbsp; Project [fkbb5855f0#36,fk919b1d80#34]
&nbsp; &nbsp; &nbsp;Filter (((((fkbb5855f0#36 &gt;= 2015-08-02 00:00:00)
&amp;&amp; (fkbb5855f0#36 &lt; 2015-09-01 00:00:00)) &amp;&amp;
fk418c5509#35 INSET (add_summary)) &amp;&amp; NOT (fkbb5855f0#36 = ))
&amp;&amp; IS NOT NULL fkbb5855f0#36)
&nbsp; &nbsp; &nbsp; PhysicalRDD
[fkbb5855f0#36,fk919b1d80#34,fk418c5509#35], MapPartitionsRDD[5] at

Code Generation: false
== RDD ==

The size of the `test` table is 3.3GB, I have 5 nodes in the Hadoop cluster,
and Spark use the same cluster. There are 3 replications of test table and
the block size is 64MB. 

The task count of the first stage is 54 when SparkSQL execute the SQL, the
Locality Level of all task is NODE_LOCAL.&nbsp;I use dstat monitoring a node
of the cluster, there are a large number of network IO:

----total-cpu-usage---- -dsk/total- -net/total- ---paging-- ---system--
usr sys idl wai hiq siq| read &nbsp;writ| recv &nbsp;send| &nbsp;in &nbsp;
out | int &nbsp; csw
&nbsp; 1 &nbsp; 0 &nbsp;99 &nbsp; 0 &nbsp; 0 &nbsp; 0| 107k &nbsp;389k|
&nbsp; 0 &nbsp; &nbsp; 0 | 193B 1097B|2408 &nbsp;5443
&nbsp; 0 &nbsp; 0 100 &nbsp; 0 &nbsp; 0 &nbsp; 0| &nbsp; 0 &nbsp; &nbsp; 0
|5709B 3285B| &nbsp; 0 &nbsp; &nbsp; 0 |1921 &nbsp;4282
&nbsp; 0 &nbsp; 0 100 &nbsp; 0 &nbsp; 0 &nbsp; 0|1936k &nbsp; &nbsp;0 |3761B
1251B| &nbsp; 0 &nbsp; &nbsp; 0 |1907 &nbsp;4197
&nbsp; 0 &nbsp; 0 100 &nbsp; 0 &nbsp; 0 &nbsp; 0| &nbsp; 0 &nbsp; 584k|3399B
1539B| &nbsp; 0 &nbsp; &nbsp; 0 |1903 &nbsp;4338
&nbsp; 0 &nbsp; 0 &nbsp;99 &nbsp; 0 &nbsp; 0 &nbsp; 0|1936k &nbsp; &nbsp;0
|4332B 1447B| &nbsp; 0 &nbsp; &nbsp; 0 |2070 &nbsp;4448
&nbsp; 4 &nbsp; 1 &nbsp;93 &nbsp; 3 &nbsp; 0 &nbsp; 0| &nbsp;16M &nbsp;
&nbsp;0 |5117B 1439B| &nbsp; 0 &nbsp; &nbsp; 0 |9177 &nbsp; &nbsp;11k
&nbsp; 1 &nbsp; 1 &nbsp;77 &nbsp;21 &nbsp; 0 &nbsp; 0| 215M &nbsp; &nbsp;0 |
921k &nbsp; 68M| &nbsp; 0 &nbsp; &nbsp; 0 | &nbsp;21k &nbsp; 10k
&nbsp; 6 &nbsp; 1 &nbsp;79 &nbsp;14 &nbsp; 0 &nbsp; 0| 175M &nbsp; &nbsp;0 |
327k &nbsp; 19M| &nbsp; 0 &nbsp; &nbsp; 0 | &nbsp;10k 8207
&nbsp;32 &nbsp; 1 &nbsp;68 &nbsp; 0 &nbsp; 0 &nbsp; 0|3308k &nbsp; &nbsp;0 |
237k &nbsp; 14M| &nbsp; 0 &nbsp; &nbsp; 0 | &nbsp;16k 6536
&nbsp;22 &nbsp; 0 &nbsp;78 &nbsp; 0 &nbsp; 0 &nbsp; 0|2048k &nbsp; &nbsp;0 |
&nbsp;98k 5733k| &nbsp; 0 &nbsp; &nbsp; 0 |9190 &nbsp;5823
&nbsp;30 &nbsp; 0 &nbsp;69 &nbsp; 0 &nbsp; 0 &nbsp; 0|8080k 8192B| 175k
&nbsp; 11M| &nbsp; 0 &nbsp; &nbsp; 0 | &nbsp;18k 6950
&nbsp;23 &nbsp; 0 &nbsp;77 &nbsp; 0 &nbsp; 0 &nbsp; 0| &nbsp;18M &nbsp;
&nbsp;0 | 727k &nbsp; 52M| &nbsp; 0 &nbsp; &nbsp; 0 | &nbsp;25k 8648
&nbsp;22 &nbsp; 0 &nbsp;78 &nbsp; 0 &nbsp; 0 &nbsp; 0| &nbsp;28M &nbsp;
&nbsp;0 | 920k &nbsp; 96M| &nbsp; 0 &nbsp; &nbsp; 0 | &nbsp;26k &nbsp; 11k
&nbsp;22 &nbsp; 0 &nbsp;78 &nbsp; 0 &nbsp; 0 &nbsp; 0| &nbsp;31M &nbsp;
&nbsp;0 |1003k &nbsp;114M| &nbsp; 0 &nbsp; &nbsp; 0 | &nbsp;25k &nbsp; 10k
&nbsp;22 &nbsp; 0 &nbsp;78 &nbsp; 0 &nbsp; 0 &nbsp; 0|9372k &nbsp; &nbsp;0 |
487k &nbsp; 49M| &nbsp; 0 &nbsp; &nbsp; 0 |9935 &nbsp;5599
&nbsp;18 &nbsp; 1 &nbsp;81 &nbsp; 0 &nbsp; 0 &nbsp; 0| &nbsp; 0 &nbsp; 125k|
&nbsp;47k 2027k| &nbsp; 0 &nbsp; &nbsp; 0 |8820 &nbsp;5358
&nbsp; 4 &nbsp; 1 &nbsp;95 &nbsp; 0 &nbsp; 0 &nbsp; 0| &nbsp;28M &nbsp;450k|
289k &nbsp; 23M| &nbsp; 0 &nbsp; &nbsp; 0 | &nbsp;16k 7992
&nbsp; 0 &nbsp; 0 &nbsp;99 &nbsp; 0 &nbsp; 0 &nbsp; 0| &nbsp;10M &nbsp;
&nbsp;0 | 446k &nbsp; 42M| &nbsp; 0 &nbsp; &nbsp; 0 |3765 &nbsp;5262
&nbsp; 0 &nbsp; 0 100 &nbsp; 0 &nbsp; 0 &nbsp; 0|1944k &nbsp; &nbsp;0 |8540B
1364k| &nbsp; 0 &nbsp; &nbsp; 0 |1943 &nbsp;4378
&nbsp; 0 &nbsp; 0 &nbsp;99 &nbsp; 0 &nbsp; 0 &nbsp; 0| &nbsp; 0 &nbsp; 426k|
&nbsp;11k 2469B| &nbsp; 0 &nbsp; &nbsp; 0 |2008 &nbsp;4476
&nbsp; 1 &nbsp; 0 &nbsp;99 &nbsp; 0 &nbsp; 0 &nbsp; 0|1852k &nbsp;368k|
&nbsp;16k 1687B| &nbsp; 0 &nbsp; &nbsp; 0 |2111 &nbsp;4509

But the sql's result size is only 3KB, the network IO confused me. There are
also something confused me too, I found *half of the task's `Input
Size/Records` is `64.0 MB (hadoop) / 0`. *&nbsp;I think Spark dispatch jobs
in a wrong way.

I split the `test` table into different count of part files to valid my
thought. The data is:

part files, single file lengthï¼Œquery elsï¼Œnetwork IO
1, 3.34G, 12s, ~200MB/node
5, 800MB, 10sï¼Œ~100MB/node
64, 74MB, 8s, ~50MB/node
74, 64MB, 7s, only a little
150, 32MB, 6s, only a little
297ï¼Œ20MB, 5s, &nbsp;only a little
I also set the replication of `test` table to 5, I found no network IO
appeared, but the query cost 11s! So, I think the network IO should be spark
fetching data from a remote node when a parquet file large than the block
size of Hadoop (64MB).

I want to know:
1. How spark dispatch jobs when a parquet file only have one part file which
large than the block size of Hadoop?
2. In which condition task's `Input Size/Records` is `64.0 MB (hadoop) / 0`
or `5.2 KB (hadoop) / 0`&nbsp;or `0.0 B (hadoop) / 50`?

Any ideas? 
Thanks




--
3.nabble.com/SparkSQL-How-does-spark-handle-a-parquet-file-in-parallel-tp14210.html
om."
shane knapp <sknapp@berkeley.edu>,"Sat, 19 Sep 2015 07:35:28 -0700","Re: BUILD SYSTEM: fire and power event at UC berkeley's IST colo,
 jenkins offline","dev <dev@spark.apache.org>, Jon Kuroda <jkuroda@eecs.berkeley.edu>, 
	Matthew L Massie <massie@berkeley.edu>","it was definitely one of our servers...  we have no ETA on when
jenkins will be back online.  we will need to inspect the rack closely
before we plug in and turn everything on.

---------------------------------------------------------------------"
Madhu <madhu@madhu.com>,"Sat, 19 Sep 2015 09:14:12 -0700 (MST)",spark-shell 1.5 doesn't seem to work in local mode,dev@spark.apache.org,"I downloaded spark-1.5.0-bin-hadoop2.6.tgz recently and installed on CentOS.
All my local Spark code works fine locally.

For some odd reason, spark-shell doesn't work in local mode.
It looks like it want's to connect to HDFS, even if I use --master local or
specify local mode in the conf.
Even sc.textFile(...) is trying to connect to HDFS.
Here's the conf, which clearly says spark.master is local:

scala> sc.getConf.getAll.foreach(println)
(spark.repl.class.uri,http://192.168.2.133:60639)
(spark.app.name,Spark shell)
(spark.driver.port,57705)
(spark.fileserver.uri,http://192.168.2.133:38494)
(spark.app.id,local-1442679054864)
(spark.driver.host,192.168.2.133)
(spark.jars,)
(spark.externalBlockStore.folderName,spark-34654a51-3461-4851-91be-0b78dd4b4bd6)
(spark.master,local[*])
(spark.executor.id,driver)
(spark.submit.deployMode,client)

Just to check my environment, I downloaded spark-1.4.1-bin-hadoop2.6.tgz,
and spark-shell behaves normally. I can access local files, everything works
as expected, no exceptions.

Here's the stack trace when I run spark-shell with Spark 1.5:

java.lang.RuntimeException: java.net.ConnectException: Call From
ltree1/127.0.0.1 to localhost:9000 failed on connection exception:
java.net.ConnectException: Connection refused; For more details see: 
http://wiki.apache.org/hadoop/ConnectionRefused
	at
org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:522)
	at
org.apache.spark.sql.hive.client.ClientWrapper.<init>(ClientWrapper.scala:171)
	at
org.apache.spark.sql.hive.HiveContext.executionHive$lzycompute(HiveContext.scala:163)
	at
org.apache.spark.sql.hive.HiveContext.executionHive(HiveContext.scala:161)
	at org.apache.spark.sql.hive.HiveContext.<init>(HiveContext.scala:168)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at
sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at
sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.spark.repl.SparkILoop.createSQLContext(SparkILoop.scala:1028)
	at $iwC$$iwC.<init>(<console>:9)
	at $iwC.<init>(<console>:18)
	at <init>(<console>:20)
	at .<init>(<console>:24)
	at .<clinit>(<console>)
	at .<init>(<console>:7)
	at .<clinit>(<console>)
	at $print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at
org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)
	at
org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1340)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:857)
	at
org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:902)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:814)
	at
org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:132)
	at
org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:124)
	at org.apache.spark.repl.SparkIMain.beQuietDuring(SparkIMain.scala:324)
	at
org.apache.spark.repl.SparkILoopInit$class.initializeSpark(SparkILoopInit.scala:124)
	at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:64)
	at
org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1$$anonfun$apply$mcZ$sp$5.apply$mcV$sp(SparkILoop.scala:974)
	at
org.apache.spark.repl.SparkILoopInit$class.runThunks(SparkILoopInit.scala:159)
	at org.apache.spark.repl.SparkILoop.runThunks(SparkILoop.scala:64)
	at
org.apache.spark.repl.SparkILoopInit$class.postInitialization(SparkILoopInit.scala:108)
	at org.apache.spark.repl.SparkILoop.postInitialization(SparkILoop.scala:64)
	at
org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply$mcZ$sp(SparkILoop.scala:991)
	at
org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)
	at
org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)
	at
scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at
org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$process(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1059)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at
org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.net.ConnectException: Call From ltree1/127.0.0.1 to
localhost:9000 failed on connection exception: java.net.ConnectException:
Connection refused; For more details see: 
http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at
sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at
sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at
org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy21.getFileInfo(Unknown Source)
	at
org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:752)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at
org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at
org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy22.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1988)
	at
org.apache.hadoop.hdfs.DistributedFileSystem$18.doCall(DistributedFileSystem.java:1118)
	at
org.apache.hadoop.hdfs.DistributedFileSystem$18.doCall(DistributedFileSystem.java:1114)
	at
org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at
org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1114)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1400)
	at
org.apache.hadoop.hive.ql.session.SessionState.createRootHDFSDir(SessionState.java:596)
	at
org.apache.hadoop.hive.ql.session.SessionState.createSessionDirs(SessionState.java:554)
	at
org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:508)
	... 56 more
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at
org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 76 more

<console>:10: error: not found: value sqlContext
       import sqlContext.implicits._
              ^
<console>:10: error: not found: value sqlContext
       import sqlContext.sql
              ^




-----
--
Madhu
https://www.linkedin.com/in/msiddalingaiah
--

---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Sat, 19 Sep 2015 10:29:14 -0700","Re: BUILD SYSTEM: fire and power event at UC berkeley's IST colo,
 jenkins offline","dev <dev@spark.apache.org>, Jon Kuroda <jkuroda@eecs.berkeley.edu>, 
	Matthew L Massie <massie@berkeley.edu>","we're up and building!  time for breakfast...  :)

https://amplab.cs.berkeley.edu/jenkins/


---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Sat, 19 Sep 2015 10:29:36 -0700","Re: AMP JENKINS - unplanned outage at 1845, ongoing",amp-infra <amp-infra@googlegroups.com>,"we're up and building!  time for breakfast...  :)

https://amplab.cs.berkeley.edu/jenkins/


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Sat, 19 Sep 2015 11:30:54 -0700",Re: spark-shell 1.5 doesn't seem to work in local mode,Madhu <madhu@madhu.com>,"Maybe you have a hdfs-site.xml lying around somewhere?



"
Sean Owen <sowen@cloudera.com>,"Sat, 19 Sep 2015 19:44:03 +0100",Re: spark-shell 1.5 doesn't seem to work in local mode,Madhu <madhu@madhu.com>,"It sounds a lot like you have some local Hadoop config pointing to a
cluster, and you're picking that up when you run the shell. Look for
HADOOP_* env variables and clear them, and use --master local[*]


---------------------------------------------------------------------


"
Madhu <madhu@madhu.com>,"Sat, 19 Sep 2015 12:14:31 -0700 (MST)",Re: spark-shell 1.5 doesn't seem to work in local mode,dev@spark.apache.org,"Thanks guys.

I do have HADOOP_INSTALL set, but Spark 1.4.1 did not seem to mind.
Seems like there's a difference in behavior between 1.5.0 and 1.4.1 for some
reason.

To the best of my knowledge, I just downloaded each tgz and untarred them in
/opt
I adjusted my PATH to point to one or the other, but that should be about
it.

Does 1.5.0 pick up HADOOP_INSTALL?
Wouldn't spark-shell --master local override that?
1.5 seemed to completely ignore --master local



-----
--
Madhu
https://www.linkedin.com/in/msiddalingaiah
--

---------------------------------------------------------------------


"
Devl Devel <devl.development@gmail.com>,"Sat, 19 Sep 2015 20:30:16 +0100",SparkR installation not working,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi All,

I've built spark 1.5.0 with hadoop 2.6 with a fresh download :

build/mvn  -Phadoop-2.6 -Dhadoop.version=2.6.0 -DskipTests clean package

I try to run SparkR it launches the normal R without the spark addons:

./bin/sparkR --master local[*]
Picked up JAVA_TOOL_OPTIONS: -javaagent:/usr/share/java/jayatanaag.jar

R version 3.1.2 (2014-10-31) -- ""Pumpkin Helmet""
Copyright (C) 2014 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.


With no ""Welcome to SparkR""

also

Error: could not find function ""sparkR.init""
Error: could not find function ""sparkRSQL.init""

Spark-shell and other components are fine. Using scala 2.10.6 and Java
1.8_45, Ubuntu 15.0.4. Please can anyone give me any pointers? Is there a
spark maven profile I need to enable?

Thanks
Devl
"
Ted Yu <yuzhihong@gmail.com>,"Sat, 19 Sep 2015 12:37:58 -0700",Re: SparkR installation not working,Devl Devel <devl.development@gmail.com>,"Looks like you didn't specify sparkr profile when building.

Cheers


"
=?UTF-8?B?SnVhbiBSb2Ryw61ndWV6IEhvcnRhbMOh?= <juan.rodriguez.hortala@gmail.com>,"Sat, 19 Sep 2015 22:11:33 +0200",Re: RDD API patterns,sim <sim@swoop.com>,"Hi Sim,

I understand that what you propose is defining a trait SparkIterable (and
also PairSparkIterable for RDDs of pairs) that encapsulates the methods in
RDDs, and then program using that trait instead of RDD. That is similar to
programming using scala.collection.GenSeq to abstract from using a
sequential or parallel Seq. This new trait SparkIterable would be needed to
cover methods in RDDs that are not present in GenSeq and other standard
traits. I understand you suggest implementing it using wrapper classes and
implicit conversions, like in PairRDDFunctions, in order to see both RDD,
Iterable and other classes as SparkIterable. That reminds me of type
classes
http://danielwestheide.com/blog/2013/02/06/the-neophytes-guide-to-scala-part-12-type-classes.html,
which could be a similar approach. I think it would be interesting to know
if some standard type classes like for example those in
https://non.github.io/cats//typeclasses.html could be of use here.

A downside I find in this approach is that it would be more difficult to
reason about the performance of programs, and to write them to obtain the
best performance, if we don't know whether a SparkIterable in a distributed
RDD or a node local collection, that for example might even be indexed. Or
we might avoid accessing a SparkIterable from a closure in a map because we
don't know if we are in the driver or in a worker. That could difficult the
development of efficient programs, but this is not very surprising because
the trade off because abstraction level and performance is always there in
programming anyway.

Anyway I find your idea very interesting, I think it could be developed
into a nice library

Greetings,

Juan




2015-09-18 14:55 GMT+02:00 sim <sim@swoop.com>:

"
Reynold Xin <rxin@databricks.com>,"Sat, 19 Sep 2015 14:18:56 -0700","Re: BUILD SYSTEM: fire and power event at UC berkeley's IST colo,
 jenkins offline",shane knapp <sknapp@berkeley.edu>,"Great!

Jon / Shane: Thanks for handling this.


"
sim <sim@swoop.com>,"Sat, 19 Sep 2015 17:03:24 -0700 (MST)",Re: RDD API patterns,dev@spark.apache.org,"Juan, I wouldn't go as far as suggesting we switch from programming using
RDDs to using SparkIterable. For example, all methods involving context,
jobs or partitions should only be part of the RDD API and not part of
SparkIterable. That said, the Spark community would benefit from a
consistent set of APIs for both RDDs and Iterables inside RDDs.

You raise an important point about performance analysis & guarantees.
Reasoning about performance should not be any more complicated than
reasoning about the performance of the code that works with Iterables
generated by mapPartitions or groupByKey today. 

However, it is important to not confuse users about what object they are
working with: an RDD, which supports the SparkIterable API, vs. an iterable
part of an RDD, which also supports the SparkIterable API (e.g., one that
mapPartitions generates). Therefore, RDD transformation APIs should continue
to return RDDs, as they do today.

Thank you for your implementation pointers. The Scala type system is
certainly flexible enough to support SparkIterable. If we get more consensus
that this is a good direction, I'd love to do a Skype session with you to
evaluate implementation options.

Best,
Sim



--

---------------------------------------------------------------------


"
Zhan Zhang <zzhang@hortonworks.com>,"Sun, 20 Sep 2015 03:29:22 +0000",Re: spark-shell 1.5 doesn't seem to work in local mode,"Madhu <madhu@madhu.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","It does not matter whether you start your spark with local or other mode. If you have hdfs-site.xml somewhere and spark configuration pointing to that config, you will read/write to HDFS.

Thanks.

Zhan Zhang

________________________________________
From: Madhu <madhu@madhu.com>
Sent: Saturday, September 19, 2015 12:14 PM
To: dev@spark.apache.org
Subject: Re: spark-shell 1.5 doesn't seem to work in local mode

Thanks guys.

I do have HADOOP_INSTALL set, but Spark 1.4.1 did not seem to mind.
Seems like there's a difference in behavior between 1.5.0 and 1.4.1 for some
reason.

To the best of my knowledge, I just downloaded each tgz and untarred them in
/opt
I adjusted my PATH to point to one or the other, but that should be about
it.

Does 1.5.0 pick up HADOOP_INSTALL?
Wouldn't spark-shell --master local override that?
1.5 seemed to completely ignore --master local



-----
--
Madhu
https://www.linkedin.com/in/msiddalingaiah
--
3.nabble.com/spark-shell-1-5-doesn-t-seem-to-work-in-local-mode-tp14212p14217.html
om.

---------------------------------------------------------------------



---------------------------------------------------------------------


"
Stephen Boesch <javadba@gmail.com>,"Sun, 20 Sep 2015 06:18:35 -0700",Using scala-2.11 when making changes to spark source,"""dev@spark.apache.org"" <dev@spark.apache.org>","The dev/change-scala-version.sh [2.11]  script modifies in-place  the
pom.xml files across all of the modules.  This is a git-visible change.  So
if we wish to make changes to spark source in our own fork's - while
developing with scala 2.11 - we would end up conflating those updates with
our own.

A possible scenario would be to update .gitignore - by adding pom.xml.
However I can not get that to work: .gitignore is tricky.

Suggestions appreciated.
"
Ted Yu <yuzhihong@gmail.com>,"Sun, 20 Sep 2015 09:12:22 -0700",Re: Using scala-2.11 when making changes to spark source,Stephen Boesch <javadba@gmail.com>,"Maybe the following can be used for changing Scala version:
http://maven.apache.org/archetype/maven-archetype-plugin/

I played with it a little bit but didn't get far.

FYI


"
gsvic <victorasgs@gmail.com>,"Sun, 20 Sep 2015 15:58:30 -0700 (MST)",Re: RDD: Execution and Scheduling,dev@spark.apache.org,"Concerning answers 1 and 2: 

1) How Spark determines a node as a ""slow node"" and how slow is that? 

2) How an RDD chooses a location as a preferred location and with which
criteria? 

Could you please also include the links of the source files for the two
questions above?



--

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Sun, 20 Sep 2015 18:01:34 -0700",Re: RDD: Execution and Scheduling,gsvic <victorasgs@gmail.com>,"

There are two cases here:

1. If a node is busy (e.g. all slots are already occupied), the scheduler
cannot schedule anything on it. See ""Delay Scheduling: A Simple Technique
for Achieving
Locality and Fairness in Cluster Scheduling"" paper for how locality
scheduling is done.

2. Within the same stage, if a task is slower than other tasks, a copy of
it can be launched speculatively in order to mitigate stragglers. Search
for speculation in the code base to find out more.




This is part of the RDD definition. The RDD interface itself defines
locality. The Spark NSDI paper already talks about this.

Why don't you just do a little bit of code reading yourself?



"
guoxu1231 <guoxu1231@gmail.com>,"Sun, 20 Sep 2015 23:47:56 -0700 (MST)",Join operation on DStreams,dev@spark.apache.org,"Hi Spark Experts, 

I'm trying to use join(otherStream, [numTasks]) on DStreams,  and it
requires called on two DStreams of (K, V) and (K, W) pairs,

Usually in common RDD, we could use keyBy(f) to build the (K, V) pair,
however I could not find it in DStream. 

My question is:
What is the expected way to build (K, V) pair in DStream?


Thanks
Shawn




--

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Sun, 20 Sep 2015 23:49:18 -0700",Re: Join operation on DStreams,guoxu1231 <guoxu1231@gmail.com>,"stream.map(record => (keyFunction(record), record))


For future reference, this question should go to the user list, not dev
list.



"
Siva <sbhavanari@gmail.com>,"Mon, 21 Sep 2015 00:46:11 -0700",Hbase Spark streaming issue.,"dev@spark.apache.org, user@spark.apache.org","Hi,

I m seeing some strange error while inserting data from spark streaming to
hbase.

I can able to write the data from spark (without streaming) to hbase
successfully, but when i use the same code to write dstream I m seeing the
below error.

I tried setting the below parameters, still didnt help. Did any face the
similar issue?

conf.set(""hbase.defaults.for.version.skip"", ""true"")
conf.set(""hbase.defaults.for.version"", ""0.98.4.2.2.4.2-2-hadoop2"")

15/09/20 22:39:10 ERROR Executor: Exception in task 0.0 in stage 14.0 (TID
16)
java.lang.RuntimeException: hbase-default.xml file seems to be for and old
version of HBase (null), this version is 0.98.4.2.2.4.2-2-hadoop2
        at
org.apache.hadoop.hbase.HBaseConfiguration.checkDefaultsVersion(HBaseConfiguration.java:73)
        at
org.apache.hadoop.hbase.HBaseConfiguration.addHbaseResources(HBaseConfiguration.java:105)
        at
org.apache.hadoop.hbase.HBaseConfiguration.create(HBaseConfiguration.java:116)
        at
org.apache.hadoop.hbase.HBaseConfiguration.create(HBaseConfiguration.java:125)
        at
$line51.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$HBaseConn$.hbaseConnection(<console>:49)
        at
$line52.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$TestHbaseSpark$$anonfun$run$1$$anonfun$apply$1.apply(<console>:73)
        at
$line52.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$TestHbaseSpark$$anonfun$run$1$$anonfun$apply$1.apply(<console>:73)
        at scala.collection.Iterator$class.foreach(Iterator.scala:727)
        at
org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
        at org.apache.spark.rdd.RDD$$anonfun$foreach$1.apply(RDD.scala:782)
        at org.apache.spark.rdd.RDD$$anonfun$foreach$1.apply(RDD.scala:782)
        at
org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1353)
        at
org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1353)
        at
org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
        at org.apache.spark.scheduler.Task.run(Task.scala:56)
        at
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:200)
        at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
15/09/20 22:39:10 WARN TaskSetManager: Lost task 0.0 in stage 14.0 (TID 16,
localhost): java.lang.RuntimeException: hbase-default.xml file seems to be
for and old version of HBase (null), this version is
0.98.4.2.2.4.2-2-hadoop2


Thanks,
Siva.
"
guoxu1231 <guoxu1231@gmail.com>,"Mon, 21 Sep 2015 01:13:57 -0700 (MST)",Re: Join operation on DStreams,dev@spark.apache.org,"Thanks for the prompt reply. 

May I ask why the keyBy(f) is not supported in DStreams? any particular
reason?
or is it possible to add it in future release since that ""stream.map(record
=> (keyFunction(record), record))"" looks tedious.

I checked the python source code, KeyBy looks like a ""shortcut"" method. 
maybe people are more familiar with it.

def keyBy(self, f):
        """"""
        Creates tuples of the elements in this RDD by applying C{f}.
        >>> x = sc.parallelize(range(0,3)).keyBy(lambda x: x*x)
        >>> y = sc.parallelize(zip(range(0,5), range(0,5)))
        >>> [(x, list(map(list, y))) for x, y in
sorted(x.cogroup(y).collect())]
        [(0, [[0], [0]]), (1, [[1], [1]]), (2, [[], [2]]), (3, [[], [3]]),
(4, [[2], [4]])]
        """"""
        return self.map(lambda x: (f(x), x))



--

---------------------------------------------------------------------


"
Romi Kuntsman <romi@totango.com>,"Mon, 21 Sep 2015 12:38:11 +0300","Re: how to send additional configuration to the RDD after it was
 lazily created",Gil Vernik <GILV@il.ibm.com>,"What new information do you know after creating the RDD, that you didn't
know at the time of it's creation?
I think the whole point is that RDD is immutable, you can't change it once
it was created.
Perhaps you need to refactor your logic to know the parameters earlier, or
create a whole new RDD again.

*Romi Kuntsman*, *Big Data Engineer*
http://www.totango.com


"
Priya Ch <learnings.chitturi@gmail.com>,"Mon, 21 Sep 2015 16:57:10 +0530",Re: passing SparkContext as parameter,"Petr Novak <oss.mlists@gmail.com>, ""user@spark.apache.org"" <user@spark.apache.org>, dev@spark.apache.org, 
	spark-connector-user@lists.datastax.com","can i use this sparkContext on executors ??
In my application, i have scenario of reading from db for certain records
in rdd. Hence I need sparkContext to read from DB (cassandra in our case),

If sparkContext couldn't be sent to executors , what is the workaround for
this ??????


"
Romi Kuntsman <romi@totango.com>,"Mon, 21 Sep 2015 14:37:43 +0300",Re: passing SparkContext as parameter,Priya Ch <learnings.chitturi@gmail.com>,"sparkConext is available on the driver, not on executors.

To read from Cassandra, you can use something like this:
https://github.com/datastax/spark-cassandra-connector/blob/master/doc/2_loading.md

*Romi Kuntsman*, *Big Data Engineer*
http://www.totango.com


"
Ted Yu <yuzhihong@gmail.com>,"Mon, 21 Sep 2015 04:40:04 -0700",Re: passing SparkContext as parameter,Priya Ch <learnings.chitturi@gmail.com>,"You can use broadcast variable for passing connection information. 

Cheers

:
n rdd. Hence I need sparkContext to read from DB (cassandra in our case),
 this ??????

 Because passing sparkContext is giving me TaskNotSerializable Exception.
"
Mohamed Baddar <mohamed.baddar@badrit.com>,"Mon, 21 Sep 2015 13:47:25 +0200",Forecasting Library For Apache Spark,dev@spark.apache.org,"Hello everybody , this my first mail in the List , and i would like to
introduce my self first :)
My Name is Mohamed baddar , I work as Big Data and Analytics Software
Engieer at BADRIT (http://badrit.com/) , a software Startup with focus in
Big Data , also i have been working for 6+ years at IBM R&D Egypt , in HPC
, Big Data and Analytics Are

I just have a question , i can't find supported Apache Spark library for
forecasting using ARIMA , ETS , Bayesian model or any method , is there any
plans for such a development , as i can't find any issue talking about it ,
is any one interested to have/develop a related module , as i find it a
critical feature to be added to SPARK

Thanks
"
Corey Nolet <cjnolet@gmail.com>,"Mon, 21 Sep 2015 08:43:28 -0400",Re: Forecasting Library For Apache Spark,Mohamed Baddar <mohamed.baddar@badrit.com>,"Mohamed,

Have you checked out the Spark Timeseries [1] project? Non-seasonal ARIMA
was added to this recently and seasonal ARIMA should be following shortly.

[1] https://github.com/cloudera/spark-timeseries


"
Mohamed Baddar <mohamed.baddar@badrit.com>,"Mon, 21 Sep 2015 14:49:20 +0200",Re: Forecasting Library For Apache Spark,Corey Nolet <cjnolet@gmail.com>,"Thanks Corey for the suggestion , will check it


"
Adam Roberts <AROBERTS@uk.ibm.com>,"Mon, 21 Sep 2015 16:21:15 +0100",Test workflow - blacklist entire suites and run any independently,dev@spark.apache.org,"Hi, is there an existing way to blacklist any test suite?

Ideally we'd have a text file with a series of names (let's say comma 
separated) and if a name matches with the fully qualified class name for a 
suite, this suite will be skipped.

Perhaps we can achieve this via ScalaTest or Maven?

Currently if a number of suites are failing we're required to comment 
these out, commit and push this change then kick off a Jenkins job 
(perhaps building a custom branch) - not ideal when working with Jenkins, 
would be quicker to use such a mechanism as described above as opposed to 
having a few branches that are a little different from others.

Also, how can we quickly only run any one suite within, say, sql/hive? -f 
sql/hive/pom.xml with -nsu results in compile failures each time.
Unless stated otherwise above:
IBM United Kingdom Limited - Registered in England and Wales with number 
741598. 
Registered office: PO Box 41, North Harbour, Portsmouth, Hampshire PO6 3AU
"
Jerry Lam <chilinglam@gmail.com>,"Mon, 21 Sep 2015 11:56:15 -0400",Spark SQL DataFrame 1.5.0 is extremely slow for take(1) or head() or first(),"""user@spark.apache.org"" <user@spark.apache.org>, Spark dev list <dev@spark.apache.org>","Hi Spark Developers,

I just ran some very simple operations on a dataset. I was surprise by the
execution plan of take(1), head() or first().

For your reference, this is what I did in pyspark 1.5:
df=sqlContext.read.parquet(""someparquetfiles"")
df.head()

The above lines take over 15 minutes. I was frustrated because I can do
better without using spark :) Since I like spark, so I tried to figure out
why. It seems the dataframe requires 3 stages to give me the first row. It
reads all data (which is about 1 billion rows) and run Limit twice.

Instead of head(), show(1) runs much faster. Not to mention that if I do:

df.rdd.take(1) //runs much faster.

Is this expected? Why head/first/take is so slow for dataframe? Is it a bug
in the optimizer? or I did something wrong?

Best Regards,

Jerry
"
Yin Huai <yhuai@databricks.com>,"Mon, 21 Sep 2015 10:01:01 -0700","Re: Spark SQL DataFrame 1.5.0 is extremely slow for take(1) or head()
 or first()",Jerry Lam <chilinglam@gmail.com>,"Hi Jerry,

Looks like it is a Python-specific issue. Can you create a JIRA?

Thanks,

Yin


"
Yin Huai <yhuai@databricks.com>,"Mon, 21 Sep 2015 10:01:55 -0700","Re: Spark SQL DataFrame 1.5.0 is extremely slow for take(1) or head()
 or first()",Jerry Lam <chilinglam@gmail.com>,"btw, does 1.4 has the same problem?


"
shane knapp <sknapp@berkeley.edu>,"Mon, 21 Sep 2015 10:11:32 -0700","Re: JENKINS: downtime next week, wed and thurs mornings (9-23 and 9-24)","dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>, 
	Jon Kuroda <jkuroda@eecs.berkeley.edu>","quick update:  we actually did some of the maintenance on our systems
after the berkeley-wide outage caused by one of our (non-jenkins)
servers halting and catching fire.

we'll still have some downtime early wednesday, but tomorrow's will be
cancelled.  i'll send out another update real soon now with what we'll
be covering on wednesday once we get our current situation more under
control.  :)


---------------------------------------------------------------------


"
Dulaj Viduranga <vidura.me@icloud.com>,"Mon, 21 Sep 2015 22:45:58 +0530",Unsubscribe,dev@spark.apache.org,"Unsubscribe

---------------------------------------------------------------------


"
Richard Hillegas <rhilleg@us.ibm.com>,"Mon, 21 Sep 2015 10:19:59 -0700",Re: Unsubscribe,Dulaj Viduranga <vidura.me@icloud.com>,"
dev-unsubscribe@spark.apache.org as described here:
http://spark.apache.org/community.html#mailing-lists.

Thanks,
-Rick

Dulaj Viduranga <vidura.me@icloud.com> wrote on 09/21/2015 10:15:58 AM:


"
Yin Huai <yhuai@databricks.com>,"Mon, 21 Sep 2015 10:40:52 -0700","Re: Spark SQL DataFrame 1.5.0 is extremely slow for take(1) or head()
 or first()",Jerry Lam <chilinglam@gmail.com>,"Seems 1.4 has the same issue.


"
Jerry Lam <chilinglam@gmail.com>,"Mon, 21 Sep 2015 13:43:22 -0400","Re: Spark SQL DataFrame 1.5.0 is extremely slow for take(1) or head()
 or first()",Yin Huai <yhuai@databricks.com>,"Hi Yin,

You are right! I just tried the scala version with the above lines, it
works as expected.
I'm not sure if it happens also in 1.4 for pyspark but I thought the
pyspark code just calls the scala code via py4j. I didn't expect that this
bug is pyspark specific. That surprises me actually a bit. I created a
ticket for this (SPARK-10731
<https://issues.apache.org/jira/browse/SPARK-10731>).

Best Regards,

Jerry



"
Jerry Lam <chilinglam@gmail.com>,"Mon, 21 Sep 2015 13:44:08 -0400","Re: Spark SQL DataFrame 1.5.0 is extremely slow for take(1) or head()
 or first()",Yin Huai <yhuai@databricks.com>,"I just noticed you found 1.4 has the same issue. I added that as well in
the ticket.


"
Yin Huai <yhuai@databricks.com>,"Mon, 21 Sep 2015 10:59:57 -0700","Re: Spark SQL DataFrame 1.5.0 is extremely slow for take(1) or head()
 or first()",Jerry Lam <chilinglam@gmail.com>,"Looks like the problem is df.rdd does not work very well with limit. In
scala, df.limit(1).rdd will also trigger the issue you observed. I will add
this in the jira.


"
Josh Rosen <rosenville@gmail.com>,"Mon, 21 Sep 2015 11:19:07 -0700",Re: Test workflow - blacklist entire suites and run any independently,Adam Roberts <AROBERTS@uk.ibm.com>,"For quickly running individual suites:
https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools#UsefulDeveloperTools-RunningIndividualTests


"
Reynold Xin <rxin@databricks.com>,"Mon, 21 Sep 2015 13:12:32 -0700",Re: Null Value in DecimalType column of DataFrame,Dirceu Semighini Filho <dirceu.semighini@gmail.com>,"+dev list

Hi Dirceu,

The answer to whether throwing an exception is better or null is better
depends on your use case. If you are debugging and want to find bugs with
your program, you might prefer throwing an exception. However, if you are
running on a"
Dogtail Ray <spark.rui92@gmail.com>,"Mon, 21 Sep 2015 16:20:06 -0400",How to modify Hadoop APIs used by Spark?,dev@spark.apache.org,"Hi all,

I find that Spark uses some Hadoop APIs such as InputFormat, InputSplit,
etc., and I want to modify these Hadoop APIs. Do you know how can I
integrate my modified Hadoop code into Spark? Great thanks!
"
Ted Yu <yuzhihong@gmail.com>,"Mon, 21 Sep 2015 13:23:23 -0700",Re: How to modify Hadoop APIs used by Spark?,Dogtail Ray <spark.rui92@gmail.com>,"Can you clarify what you want to do:
If you modify existing hadoop InputFormat, etc, it would be a matter of
rebuilding hadoop and build Spark using the custom built hadoop as
dependency.

Do you introduce new InputFormat ?

Cheers


"
Adam Roberts <AROBERTS@uk.ibm.com>,"Mon, 21 Sep 2015 21:59:03 +0100",Re: Test workflow - blacklist entire suites and run any independently,Josh Rosen <rosenville@gmail.com>,"Thanks Josh, I should have added that we've tried with -DwildcardSuites 
and Maven and we use this helpful feature regularly (although this does 
result in building plenty of tests and running other tests in other 
modules too), so wondering if there's a more ""streamlined"" way - e.g. with 
junit and eclipse we'd just right click one individual unit test and 
that'd be run - without building again AFAIK

Unfortunately using sbt causes a lot of pain, such as...

[error]
[error]   last tree to typer: 
Literal(Constant(org.apache.spark.sql.test.ExamplePoint))
[error]               symbol: null
[error]    symbol definition: null
[error]                  tpe: 
Class(classOf[org.apache.spark.sql.test.ExamplePoint])
[error]        symbol owners:
[error]       context owners: class ExamplePointUDT -> package test
[error]

and then an awfully long stacktrace with plenty of errors. Must be an 
easier way...



From:   Josh Rosen <rosenville@gmail.com>
To:     Adam Roberts/UK/IBM@IBMGB
Cc:     dev <dev@spark.apache.org>
Date:   21/09/2015 19:19
Subject:        Re: Test workflow - blacklist entire suites and run any 
independently



For quickly running individual suites: 
https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools#UsefulDeveloperTools-RunningIndividualTests

Hi, is there an existing way to blacklist any test suite? 

Ideally we'd have a text file with a series of names (let's say comma 
separated) and if a name matches with the fully qualified class name for a 
suite, this suite will be skipped. 

Perhaps we can achieve this via ScalaTest or Maven? 

Currently if a number of suites are failing we're required to comment 
these out, commit and push this change then kick off a Jenkins job 
(perhaps building a custom branch) - not ideal when working with Jenkins, 
would be quicker to use such a mechanism as described above as opposed to 
having a few branches that are a little different from others. 

Also, how can we quickly only run any one suite within, say, sql/hive? -f 
sql/hive/pom.xml with -nsu results in compile failures each time.
Unless stated otherwise above:
IBM United Kingdom Limited - Registered in England and Wales with number 
741598. 
Registered office: PO Box 41, North Harbour, Portsmouth, Hampshire PO6 3AU


Unless stated otherwise above:
IBM United Kingdom Limited - Registered in England and Wales with number 
741598. 
Registered office: PO Box 41, North Harbour, Portsmouth, Hampshire PO6 3AU
"
Dogtail L <spark.rui92@gmail.com>,"Mon, 21 Sep 2015 19:17:27 -0400",Re: How to modify Hadoop APIs used by Spark?,Ted Yu <yuzhihong@gmail.com>,"Oh, I want to modify existing Hadoop InputFormat.


"
Matt Cheah <mcheah@palantir.com>,"Tue, 22 Sep 2015 00:34:39 +0000",DataFrames Aggregate does not spill?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi everyone,

I¹m debugging some slowness and apparent memory pressure + GC issues after I
ported some workflows from raw RDDs to Data Frames. In particular, I¹m
looking into an aggregation workflow that computes many aggregations per key
at once.

My workflow before was doing a fairly straightforward combineByKey call
where the aggregation would build up non-trivially sized objects in memory ­
I was computing numerous sums over various fields of the data at a time. In
particular, I noticed that there was spilling to disk on the map side of the
aggregation.

When I switched to using DataFrames aggregation ­ particularly
DataFrame.groupBy(some list of keys).agg(exprs) where I passed a large
number of ³Sum² exprs - the execution began to choke. I saw one of my
executors had a long GC pause and my job isn¹t able to recover. However when
I reduced the number of Sum expressions being computed in the aggregation,
the workflow started to work normally.

I have a hypothesis that I would like to run by everyone. In
org.apache.spark.sql.execution.Aggregate.scala, branch-1.5, I¹m looking at
the execution of Aggregation
<https://github.com/apache/spark/blob/branch-1.5/sql/core/src/main/scala/org
/apache/spark/sql/execution/Aggregate.scala> , which appears to build the
aggregation result in memory via updating a HashMap and iterating over the
rows. However this appears to be less robust than what would happen if
PairRDDFunctions.combineByKey were to be used. If combineByKey were used,
then instead of using two mapPartitions calls (assuming the aggregation is
partially-computable, as Sum is), it would use the ExternalSorter and
the aggregated result to grow large as some of the aggregated result could
be spilled to disk. This especially seems bad if the aggregation reduction
factor is low; that is, if there are many unique keys in the dataset. In
particular, the Hash Map is holding O(# of keys * number of aggregated
results per key) items in memory at a time.

I was wondering what everyone¹s thought on this problem is. Did we
consciously think about the robustness implications when choosing to use an
in memory Hash Map to compute the aggregation? Is this an inherent
limitation of the aggregation implementation in Data Frames?

Thanks,

-Matt Cheah







"
Reynold Xin <rxin@databricks.com>,"Mon, 21 Sep 2015 17:36:24 -0700",Re: DataFrames Aggregate does not spill?,Matt Cheah <mcheah@palantir.com>,"What's the plan if you run explain?

In 1.5 the default should be TungstenAggregate, which does spill (switching
from hash-based aggregation to sort-based aggregation).


ues after
€™m
y
t a time.
of
aw one of my
owever
ooking at
org/apache/spark/sql/execution/Aggregate.scala>,
st
If
se
y
e
an
"
Hossein <falaki@gmail.com>,"Mon, 21 Sep 2015 18:18:50 -0700",SparkR package path,dev@spark.apache.org,"Hi dev list,

SparkR backend assumes SparkR source files are located under
""SPARK_HOME/R/lib/."" This directory is created by running R/install-dev.sh.
This setting makes sense for Spark developers, but if an R user downloads
and installs SparkR source package, the source files are going to be in
placed different locations.

In the R runtime it is easy to find location of package files using
path.package(""SparkR""). But we need to make some changes to R backend
and/or spark-submit so that, JVM process learns the location of worker.R
and daemon.R and shell.R from the R runtime.

Do you think this change is feasible?

Thanks,
--Hossein
"
Matt Cheah <mcheah@palantir.com>,"Tue, 22 Sep 2015 02:21:03 +0000",Re: DataFrames Aggregate does not spill?,Reynold Xin <rxin@databricks.com>,"I was executing on Spark 1.4 so I didn¹t notice the Tungsten option would
make spilling happen in 1.5. I¹ll upgrade to 1.5 and see how that turns out.
Thanks!

From:  Reynold Xin <rxin@databricks.com>
Date:  Monday, September 21, 2015 at 5:36 PM
To:  Matt Cheah <mcheah@palantir.com>
Cc:  ""dev@spark.apache.org"" <dev@spark.apache.org>, Mingyu Kim
<mkim@palantir.com>, Peter Faiman <peterfaiman@palantir.com>
Subject:  Re: DataFrames Aggregate does not spill?

What's the plan if you run explain?

In 1.5 the default should be TungstenAggregate, which does spill (switching
from hash-based aggregation to sort-based aggregation).

r I
oking
nce.
here
was
the
mber
had a
e
t the
park_
Aggre
NQ9E9
Zfff8
ild
r the
 then
w the
actor
lar,
y)
ously
 Hash



"
"""Sun, Rui"" <rui.sun@intel.com>","Tue, 22 Sep 2015 03:15:29 +0000",RE: SparkR package path,"Hossein <falaki@gmail.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Hossein,

Any strong reason to download and install SparkR source package separately from the Spark distribution?
An R user can simply download the spark distribution, which contains SparkR source and binary package, and directly use sparkR. No need to install SparkR package at all.

From: Hossein [mailto:falaki@gmail.com]
Sent: Tuesday, September 22, 2015 9:19 AM
To: dev@spark.apache.org
Subject: SparkR package path

Hi dev list,

SparkR backend assumes SparkR source files are located under ""SPARK_HOME/R/lib/."" This directory is created by running R/install-dev.sh. This setting makes sense for Spark developers, but if an R user downloads and installs SparkR source package, the source files are going to be in placed different locations.

In the R runtime it is easy to find location of package files using path.package(""SparkR""). But we need to make some changes to R backend and/or spark-submit so that, JVM process learns the location of worker.R and daemon.R and shell.R from the R runtime.

Do you think this change is feasible?

Thanks,
--Hossein
"
Bin Wang <wbin00@gmail.com>,"Tue, 22 Sep 2015 04:51:04 +0000",Why there is no snapshots for 1.5 branch?,"""dev@spark.apache.org"" <dev@spark.apache.org>","I'd like to use some important bug fixes in 1.5 branch and I look for the
apache maven host, but don't find any snapshot for 1.5 branch.
https://repository.apache.org/content/groups/snapshots/org/apache/spark/spark-core_2.10/1.5.0-SNAPSHOT/

I can find 1.4.X and 1.6.0 versions, why there is no snapshot for 1.5.X?
"
Mark Hamstra <mark@clearstorydata.com>,"Mon, 21 Sep 2015 21:55:13 -0700",Re: Why there is no snapshots for 1.5 branch?,Bin Wang <wbin00@gmail.com>,"There is no 1.5.0-SNAPSHOT because 1.5.0 has already been released.  The
current head of branch-1.5 is 1.5.1-SNAPSHOT -- soon to be 1.5.1 release
candidates and then the 1.5.1 release.


"
Bin Wang <wbin00@gmail.com>,"Tue, 22 Sep 2015 04:57:39 +0000",Re: Why there is no snapshots for 1.5 branch?,Mark Hamstra <mark@clearstorydata.com>,"But I cannot find 1.5.1-SNAPSHOT either at
https://repository.apache.org/content/groups/snapshots/org/apache/spark/spark-core_2.10/

Mark Hamstra <mark@clearstorydata.com>äºŽ2015å¹´9æœˆ22æ—¥å‘¨äºŒ ä¸‹åˆ12:55å†™é“ï¼š

e
spark-core_2.10/1.5.0-SNAPSHOT/
"
Mark Hamstra <mark@clearstorydata.com>,"Mon, 21 Sep 2015 22:25:07 -0700",Re: Why there is no snapshots for 1.5 branch?,Bin Wang <wbin00@gmail.com>,"Yeah, whoever is maintaining the scripts and snapshot builds has fallen
down on the job -- but there is nothing preventing you from checking out
branch-1.5 and creating your own build, which is arguably a smarter thing
to do anyway.  If I'm going to use a non-release build, then I want the
full git commit history of exactly what is in that build readily available,
not just somewhat arbitrary JARs.


park-core_2.10/
æ—¥å‘¨äºŒ ä¸‹åˆ12:55å†™é“ï¼š
/spark-core_2.10/1.5.0-SNAPSHOT/
?
"
Bin Wang <wbin00@gmail.com>,"Tue, 22 Sep 2015 05:26:24 +0000",Re: Why there is no snapshots for 1.5 branch?,Mark Hamstra <mark@clearstorydata.com>,"My project is using sbt (or maven), which need to download dependency from
a maven repo. I have my own private maven repo with nexus but I don't know
how to push my own build to it, can you give me a hint?

Mark Hamstra <mark@clearstorydata.com>äºŽ2015å¹´9æœˆ22æ—¥å‘¨äºŒ ä¸‹åˆ1:25å†™é“ï¼š

e,
spark-core_2.10/
æ—¥å‘¨äºŒ ä¸‹åˆ12:55å†™é“ï¼š
e
e
k/spark-core_2.10/1.5.0-SNAPSHOT/
X?
"
Fengdong Yu <fengdongy@everstring.com>,"Tue, 22 Sep 2015 13:29:23 +0800",Re: Why there is no snapshots for 1.5 branch?,Bin Wang <wbin00@gmail.com>,"Do you mean you want to publish the artifact to your private repository?

if so, please using â€˜sbt publishâ€™

add the following in your build.sb:

publishTo := {
  val nexus = ""https://YOUR_PRIVATE_REPO_HOSTS/""
  if (version.value.endsWith(""SNAPSHOT""))
    Some(""snapshots"" at nexus + ""content/repositories/snapshots"")
  else
    Some(""releases""  at nexus + ""content/repositories/releases"")

}



from a maven repo. I have my own private maven repo with nexus but I don't know how to push my own build to it, can you give me a hint?
<mailto:mark@clearstorydata.com>>äºŽ2015å¹´9æœˆ22æ—¥å‘¨äºŒ ä¸‹åˆ1:25å†™é“ï¼š
fallen down on the job -- but there is nothing preventing you from checking out branch-1.5 and creating your own build, which is arguably a smarter thing to do anyway.  If I'm going to use a non-release build, then I want the full git commit history of exactly what is in that build readily available, not just somewhat arbitrary JARs.
https://repository.apache.org/content/groups/snapshots/org/apache/spark/spark-core_2.10/ <https://repository.apache.org/content/groups/snapshots/org/apache/spark/spark-core_2.10/>
<mailto:mark@clearstorydata.com>>äºŽ2015å¹´9æœˆ22æ—¥å‘¨äºŒ ä¸‹åˆ12:55å†™é“ï¼š
The current head of branch-1.5 is 1.5.1-SNAPSHOT -- soon to be 1.5.1 release candidates and then the 1.5.1 release.
the apache maven host, but don't find any snapshot for 1.5 branch. https://repository.apache.org/content/groups/snapshots/org/apache/spark/spark-core_2.10/1.5.0-SNAPSHOT/ <https://repository.apache.org/content/groups/snapshots/org/apache/spark/spark-core_2.10/1.5.0-SNAPSHOT/>
1.5.X?

"
Bin Wang <wbin00@gmail.com>,"Tue, 22 Sep 2015 05:34:31 +0000",Re: Why there is no snapshots for 1.5 branch?,Fengdong Yu <fengdongy@everstring.com>,"No, I mean push spark to my private repository. Spark don't have a
build.sbt as far as I see.

Fengdong Yu <fengdongy@everstring.com>äºŽ2015å¹´9æœˆ22æ—¥å‘¨äºŒ ä¸‹åˆ1:29å†™é“ï¼š

m
w
æ—¥å‘¨äºŒ ä¸‹åˆ1:25å†™é“ï¼š
g
le,
/spark-core_2.10/
2æ—¥å‘¨äºŒ ä¸‹åˆ12:55å†™é“ï¼š
rk/spark-core_2.10/1.5.0-SNAPSHOT/
"
Bin Wang <wbin00@gmail.com>,"Tue, 22 Sep 2015 05:36:49 +0000",Re: Why there is no snapshots for 1.5 branch?,Fengdong Yu <fengdongy@everstring.com>,"However I find some scripts in dev/audit-release, can I use them?

Bin Wang <wbin00@gmail.com>äºŽ2015å¹´9æœˆ22æ—¥å‘¨äºŒ ä¸‹åˆ1:34å†™é“ï¼š

æ—¥å‘¨äºŒ ä¸‹åˆ1:29å†™é“ï¼š
't
æ—¥å‘¨äºŒ ä¸‹åˆ1:25å†™é“ï¼š
t
ng
ble,
k/spark-core_2.10/
22æ—¥å‘¨äºŒ ä¸‹åˆ12:55å†™é“ï¼š
r
ark/spark-core_2.10/1.5.0-SNAPSHOT/
"
Patrick Wendell <pwendell@gmail.com>,"Mon, 21 Sep 2015 23:04:15 -0700",Re: Why there is no snapshots for 1.5 branch?,Bin Wang <wbin00@gmail.com>,"I just added snapshot builds for 1.5. They will take a few hours to
build, but once we get them working should publish every few hours.

https://amplab.cs.berkeley.edu/jenkins/view/Spark-Packaging

- Patrick

‘¨äºŒ ä¸‹åˆ1:34å†™é“ï¼š
æ—¥å‘¨äºŒ ä¸‹åˆ1:29å†™é“ï¼š
?
n't
2æ—¥å‘¨äºŒ ä¸‹åˆ1:25å†™é“ï¼š
n
ut
ing to
ull
 not
rk/spark-core_2.10/
ˆ22æ—¥å‘¨äºŒ ä¸‹åˆ12:55å†™é“ï¼š
 release
or
park/spark-core_2.10/1.5.0-SNAPSHOT/

---------------------------------------------------------------------


"
Fengdong Yu <fengdongy@everstring.com>,"Tue, 22 Sep 2015 14:07:57 +0800",Re: Why there is no snapshots for 1.5 branch?,Bin Wang <wbin00@gmail.com>,"basically, you can build snapshot by yourself.

just clone the source code, and then 'mvn package/deploy/installâ€¦..â€™


Azuryy Yu



<mailto:wbin00@gmail.com>>äºŽ2015å¹´9æœˆ22æ—¥å‘¨äºŒ ä¸‹åˆ1:34å†™é“ï¼š
build.sbt as far as I see.
<mailto:fengdongy@everstring.com>>äºŽ2015å¹´9æœˆ22æ—¥å‘¨äºŒ ä¸‹åˆ1:29å†™é“ï¼š
repository?
<https://your_private_repo_hosts/>""
from a maven repo. I have my own private maven repo with nexus but I don't know how to push my own build to it, can you give me a hint?
<mailto:mark@clearstorydata.com>>äºŽ2015å¹´9æœˆ22æ—¥å‘¨äºŒ ä¸‹åˆ1:25å†™é“ï¼š
fallen down on the job -- but there is nothing preventing you from checking out branch-1.5 and creating your own build, which is arguably a smarter thing to do anyway.  If I'm going to use a non-release build, then I want the full git commit history of exactly what is in that build readily available, not just somewhat arbitrary JARs.
https://repository.apache.org/content/groups/snapshots/org/apache/spark/spark-core_2.10/ <https://repository.apache.org/content/groups/snapshots/org/apache/spark/spark-core_2.10/>
<mailto:mark@clearstorydata.com>>äºŽ2015å¹´9æœˆ22æ—¥å‘¨äºŒ ä¸‹åˆ12:55å†™é“ï¼š
The current head of branch-1.5 is 1.5.1-SNAPSHOT -- soon to be 1.5.1 release candidates and then the 1.5.1 release.
the apache maven host, but don't find any snapshot for 1.5 branch. https://repository.apache.org/content/groups/snapshots/org/apache/spark/spark-core_2.10/1.5.0-SNAPSHOT/ <https://repository.apache.org/content/groups/snapshots/org/apache/spark/spark-core_2.10/1.5.0-SNAPSHOT/>
1.5.X?

"
Bin Wang <wbin00@gmail.com>,"Tue, 22 Sep 2015 07:33:23 +0000",Re: Why there is no snapshots for 1.5 branch?,Fengdong Yu <fengdongy@everstring.com>,"Thanks. I've solved it. I modified pom.xml and add my own repo into it,
then use ""mvn deploy"".

Fengdong Yu <fengdongy@everstring.com>äºŽ2015å¹´9æœˆ22æ—¥å‘¨äºŒ ä¸‹åˆ2:08å†™é“ï¼š

..â€™
‘¨äºŒ ä¸‹åˆ1:34å†™é“ï¼š
æ—¥å‘¨äºŒ ä¸‹åˆ1:29å†™é“ï¼š
?
n't
2æ—¥å‘¨äºŒ ä¸‹åˆ1:25å†™é“ï¼š
n
ut
ing
e
able,
rk/spark-core_2.10/
ˆ22æ—¥å‘¨äºŒ ä¸‹åˆ12:55å†™é“ï¼š
ch.
park/spark-core_2.10/1.5.0-SNAPSHOT/
"
<Saif.A.Ellafi@wellsfargo.com>,"Tue, 22 Sep 2015 15:27:08 +0000","RowMatrix tallSkinnyQR - ERROR: Second call to constructor of
 static parser",<dev@spark.apache.org>,"Hi all,

wondering if any could make the new 1.5.0 stallSkinnyQR to work.
Follows my output, which is a big loop of the same errors until the shell dies.
I am curious since im failing to load any implementations from BLAS, LAPACK, etc.

scala> mat.tallSkinnyQR(false)
15/09/22 10:18:11 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK
15/09/22 10:18:11 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK
ERROR: Second call to constructor of static parser.  You must
ERROR: Second call to constructor of static parser.  You must
       either use ReInit() or set the JavaCC option STATIC to false
ERROR: Second call to constructor of static parser.  You must
       either use ReInit() or set the JavaCC option STATIC to false
       during parser generation.
ERROR: Second call to constructor of static parser.  You must
       either use ReInit() or set the JavaCC option STATIC to false
       during parser generation.
...
ERROR: Second call to constructor of static parser.  You must
       either use ReInit() or set the JavaCC option STATIC to false
       during parser generation.
15/09/22 10:18:11 ERROR Executor: Exception in task 6.0 in stage 3.0 (TID 31)
java.lang.Error
        at org.j_paine.formatter.FormatParser.<init>(FormatParser.java:353)
        at org.j_paine.formatter.FormatParser.<init>(FormatParser.java:346)
        at org.j_paine.formatter.Parsers.<init>(Formatter.java:1748)
        at org.j_paine.formatter.Parsers.theParsers(Formatter.java:1739)
        at org.j_paine.formatter.Format.<init>(Formatter.java:177)

"
Pedro Rodriguez <ski.rodriguez@gmail.com>,"Tue, 22 Sep 2015 09:50:26 -0600",Open Issues for Contributors,dev@spark.apache.org,"Where is the best place to look at open issues that haven't been
assigned/started for the next release? I am interested in working on
something, but I don't know what issues are higher priority for the next
release.

the next release (be it 1.5.1 or 1.6) with some parent issues along with
smaller child issues to work on (like the built ins ticket from 1.5)?

Thanks,
-- 
Pedro Rodriguez
PhD Student in Distributed Machine Learning | CU Boulder
UC Berkeley AMPLab Alumni

ski.rodriguez@gmail.com | pedrorodriguez.io | 208-340-1703
Github: github.com/EntilZha | LinkedIn:
https://www.linkedin.com/in/pedrorodriguezscience
"
shane knapp <sknapp@berkeley.edu>,"Tue, 22 Sep 2015 10:07:56 -0700","Re: JENKINS: downtime next week, wed and thurs mornings (9-23 and 9-24)","dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>, 
	Jon Kuroda <jkuroda@eecs.berkeley.edu>","ok, here's the updated downtime schedule for this week:

wednesday, sept 23rd:

firewall maintenance cancelled, as jon took care of the update
saturday morning while we were bringing jenkins back up after the colo
fire

thursday, sept 24th:

jenkins maintenance is still scheduled, but abbreviated as some of the
maintenance was performed saturday morning as well
* new builds will stop being accepted ~630am PDT
  - i'll kill any hangers-on at 730am, and after maintenance is done,
i will retrigger any killed jobs
* jenkins worker system package updates
  - amp-jenkins-master was completed on saturday
  - this will NOT include kernel updates as moving to
2.6.32-573.3.1.el6 bricked amp-jenkins-master
* moving default system java for builds from jdk1.7.0_71 to jdk1.7.0_79
* all systems get a reboot
* expected downtime:  3.5 hours or so

i'll post updates as i progress.

also, i'll post a copy of our post-mortem once the dust settles.  it's
been, shall we say, a pretty crazy few days.

http://news.berkeley.edu/2015/09/19/campus-network-outage/

:)


---------------------------------------------------------------------


"
Luciano Resende <luckbr1975@gmail.com>,"Tue, 22 Sep 2015 10:10:17 -0700",Re: Open Issues for Contributors,Pedro Rodriguez <ski.rodriguez@gmail.com>,"You can use Jira filters to narrow down the scope of issues you want to
possible address, for instance, I use this filter to look into open issues,
that are unassigned :

https://issues.apache.org/jira/issues/?filter=12333428

For a specific release, you can also filter the release, and I Reynold had
sent this a few days ago for 1.5.1

https://issues.apache.org/jira/issues/?filter=12333321





-- 
Luciano Resende
http://people.apache.org/~lresende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
Pedro Rodriguez <ski.rodriguez@gmail.com>,"Tue, 22 Sep 2015 11:13:39 -0600",Re: Open Issues for Contributors,Luciano Resende <luckbr1975@gmail.com>,"Thanks for the links (first one is broken or private).

I think the main mistake I was making was looking at fix version instead of
target version (JIRA homepage with listings of versions links to fix
versions).

For anyone else interested in MLlib things, I am looking at this to see
what goals are: https://issues.apache.org/jira/browse/SPARK-10324





-- 
Pedro Rodriguez
PhD Student in Distributed Machine Learning | CU Boulder
UC Berkeley AMPLab Alumni

ski.rodriguez@gmail.com | pedrorodriguez.io | 208-340-1703
Github: github.com/EntilZha | LinkedIn:
https://www.linkedin.com/in/pedrorodriguezscience
"
Richard Hillegas <rhilleg@us.ibm.com>,"Tue, 22 Sep 2015 10:53:23 -0700",column identifiers in Spark SQL,Dev <dev@spark.apache.org>,"

I am puzzled by the behavior of column identifiers in Spark SQL. I don't
find any guidance in the ""Spark SQL and DataFrame Guide"" at
http://spark.apache.org/docs/latest/sql-programming-guide.html. I am seeing
odd behavior related to case-sensitivity and to delimited (quoted)
identifiers.

Consider the following declaration of a table in the Derby relational
database, whose dialect hews closely to the SQL Standard:

   create table app.t( a int, ""b"" int, ""c""""d"" int );

Now let's load that table into Spark like this:

  import org.apache.spark.sql._
  import org.apache.spark.sql.types._

  val df = sqlContext.read.format(""jdbc"").options(
    Map(""url"" -> ""jdbc:derby:/Users/rhillegas/derby/databases/derby1"",
    ""dbtable"" -> ""app.t"")).load()
  df.registerTempTable(""test_data"")

The following query runs fine because the column name matches the
normalized form in which it is stored in the metadata catalogs of the
relational database:

  // normalized column names are recognized
  sqlContext.sql(s""""""select A from test_data"""""").show

But the following query fails during name resolution. This puzzles me
because non-delimited identifiers are case-insensitive in the ANSI/ISO
Standard. They are also supposed to be case-insensitive in HiveQL, at least
according to section 2.3.1 of the QuotedIdentifier.html webpage attached to
https://issues.apache.org/jira/browse/HIVE-6013:

  // ...unnormalized column names raise this error:
org.apache.spark.sql.AnalysisException: cannot resolve 'a' given input
columns A, b, c""d;
  sqlContext.sql(""""""select a from test_data"""""").show

Delimited (quoted) identifiers are treated as string literals. Again,
non-Standard behavior:

  // this returns rows consisting of the string literal ""b""
  sqlContext.sql(""""""select ""b"" from test_data"""""").show

Embedded quotes in delimited identifiers won't even parse:

  // embedded quotes raise this error: java.lang.RuntimeException: [1.11]
failure: ``union'' expected but ""d"" found
  sqlContext.sql(""""""select ""c""""d"" from test_data"""""").show

This behavior is non-Standard and it strikes me as hard to describe to
users concisely. Would the community support an effort to bring the
handling of column identifiers into closer conformance with the Standard?
Would backward compatibility concerns even allow us to do that?

Thanks,
-Rick"
Michael Armbrust <michael@databricks.com>,"Tue, 22 Sep 2015 10:58:36 -0700",Re: column identifiers in Spark SQL,Richard Hillegas <rhilleg@us.ibm.com>,"Are you using a SQLContext or a HiveContext?  The programming guide
suggests the latter, as the former is really only there because some
applications may have conflicts with Hive dependencies.  SQLContext is case
sensitive by default where as the HiveContext is not.  The parser in
HiveContext is also a lot better.


"
Richard Hillegas <rhilleg@us.ibm.com>,"Tue, 22 Sep 2015 13:06:50 -0700",Re: column identifiers in Spark SQL,Dev <dev@spark.apache.org>,"
Thanks for that tip, Michael. I think that my sqlContext was a raw
SQLContext originally. I have rebuilt Spark like so...

  sbt/sbt -Phive assembly/assembly

Now I see that my sqlContext is a HiveContext. That fixes one of the
queries. Now unnormalized column names work:

  // ...unnormalized column names work now
  sqlContext.sql(""""""select a from test_data"""""").show

However, quoted identifiers are still treated as string literals:

  // this still returns rows consisting of the string literal ""b""
  sqlContext.sql(""""""select ""b"" from test_data"""""").show

And embedded quotes inside quoted identifiers are swallowed up:

  // this now returns rows consisting of the string literal ""cd""
  sqlContext.sql(""""""select ""c""""d"" from test_data"""""").show

Thanks,
-Rick

Michael Armbrust <michael@databricks.com> wrote on 09/22/2015 10:58:36 AM:



m>
by1"",
hat?
"
Michael Armbrust <michael@databricks.com>,"Tue, 22 Sep 2015 13:16:12 -0700",Re: column identifiers in Spark SQL,Richard Hillegas <rhilleg@us.ibm.com>,"HiveQL uses `backticks` for quoted identifiers.


"
Richard Hillegas <rhilleg@us.ibm.com>,"Tue, 22 Sep 2015 13:28:29 -0700",Derby version in Spark,Dev <dev@spark.apache.org>,"

I see that lib_managed/jars holds these old Derby versions:

  lib_managed/jars/derby-10.10.1.1.jar
  lib_managed/jars/derby-10.10.2.0.jar

The Derby 10.10 release family supports some ancient JVMs: Java SE 5 and
Java ME CDC/Foundation Profile 1.1. It's hard to imagine anyone running
Spark on the resource-constrained Java ME platform. Is Spark really
deployed on Java SE 5? Is there some other reason that Spark uses the 10.10
Derby family?

If no-one needs those ancient JVMs, maybe we could consider changing the
Derby version to 10.11.1.1 or even to the upcoming 10.12.1.1 release (both
run on Java 6 and up).

Thanks,
-Rick"
Ted Yu <yuzhihong@gmail.com>,"Tue, 22 Sep 2015 13:32:39 -0700",Re: Derby version in Spark,Richard Hillegas <rhilleg@us.ibm.com>,"Which Spark release are you building ?

For master branch, I get the following:

lib_managed/jars/datanucleus-api-jdo-3.2.6.jar
 lib_managed/jars/datanucleus-core-3.2.10.jar
 lib_managed/jars/datanucleus-rdbms-3.2.9.jar

FYI


"
gsvic <victorasgs@gmail.com>,"Tue, 22 Sep 2015 14:13:15 -0700 (MST)",Re: RDD: Execution and Scheduling,dev@spark.apache.org,"I already have but I needed some clarifications. Thanks for all your help!



--

---------------------------------------------------------------------


"
Richard Hillegas <rhilleg@us.ibm.com>,"Tue, 22 Sep 2015 15:21:57 -0700",Re: Derby version in Spark,Dev <dev@spark.apache.org>,"
Thanks, Ted. I'm working on my master branch. The lib_managed/jars
directory has a lot of jarballs, including hadoop and hive. Maybe these
were faulted in when I built with the following command?

  sbt/sbt -Phive assembly/assembly

The Derby jars seem to be used in order to manage the metastore_db
database. Maybe my question should be directed to the Hive community?

Thanks,
-Rick

Here are the gory details:

bash-3.2$ ls lib_managed/jars
FastInfoset-1.2.12.jar				curator-test-2.4.0.jar
	jersey-test-framework-grizzly2-1.9.jar
parquet-format-2.3.0-incubating.jar
JavaEWAH-0.3.2.jar				datanucleus-api-jdo-3.2.6.jar
	jets3t-0.7.1.jar				parquet-generator-1.7.0.jar
ST4-4.0.4.jar					datanucleus-core-3.2.10.jar
	jetty-continuation-8.1.14.v20131031.jar
parquet-hadoop-1.7.0.jar
activation-1.1.jar				datanucleus-rdbms-3.2.9.jar
	jetty-http-8.1.14.v20131031.jar
parquet-hadoop-bundle-1.6.0.jar
akka-actor_2.10-2.3.11.jar			derby-10.10.1.1.jar
	jetty-io-8.1.14.v20131031.jar			parquet-jackson-1.7.0.jar
akka-remote_2.10-2.3.11.jar			derby-10.10.2.0.jar
	jetty-jndi-8.1.14.v20131031.jar			platform-3.4.0.jar
akka-slf4j_2.10-2.3.11.jar
genjavadoc-plugin_2.10.4-0.9-spark0.jar
jetty-plus-8.1.14.v20131031.jar			pmml-agent-1.1.15.jar
akka-testkit_2.10-2.3.11.jar			groovy-all-2.1.6.jar
	jetty-security-8.1.14.v20131031.jar		pmml-model-1.1.15.jar
antlr-2.7.7.jar					guava-11.0.2.jar
jetty-server-8.1.14.v20131031.jar		pmml-schema-1.1.15.jar
antlr-runtime-3.4.jar				guice-3.0.jar
	jetty-servlet-8.1.14.v20131031.jar
postgresql-9.3-1102-jdbc41.jar
aopalliance-1.0.jar				h2-1.4.183.jar
	jetty-util-6.1.26.jar				py4j-0.8.2.1.jar
arpack_combined_all-0.1-javadoc.jar		hadoop-annotations-2.2.0.jar
	jetty-util-8.1.14.v20131031.jar			pyrolite-4.4.jar
arpack_combined_all-0.1.jar			hadoop-auth-2.2.0.jar
	jetty-webapp-8.1.14.v20131031.jar		quasiquotes_2.10-2.0.0.jar
asm-3.2.jar					hadoop-client-2.2.0.jar
jetty-websocket-8.1.14.v20131031.jar		reflectasm-1.07-shaded.jar
avro-1.7.4.jar					hadoop-common-2.2.0.jar
	jetty-xml-8.1.14.v20131031.jar			sac-1.3.jar
avro-1.7.7.jar					hadoop-hdfs-2.2.0.jar
	jline-0.9.94.jar				scala-compiler-2.10.0.jar
avro-ipc-1.7.7-tests.jar
hadoop-mapreduce-client-app-2.2.0.jar		jline-2.10.4.jar
	scala-compiler-2.10.4.jar
avro-ipc-1.7.7.jar
hadoop-mapreduce-client-common-2.2.0.jar	jline-2.12.jar
	scala-library-2.10.4.jar
avro-mapred-1.7.7-hadoop2.jar
hadoop-mapreduce-client-core-2.2.0.jar		jna-3.4.0.jar
		scala-reflect-2.10.4.jar
breeze-macros_2.10-0.11.2.jar
hadoop-mapreduce-client-jobclient-2.2.0.jar	joda-time-2.5.jar
	scalacheck_2.10-1.11.3.jar
breeze_2.10-0.11.2.jar
hadoop-mapreduce-client-shuffle-2.2.0.jar	jodd-core-3.5.2.jar
	scalap-2.10.0.jar
calcite-avatica-1.2.0-incubating.jar		hadoop-yarn-api-2.2.0.jar
		json-20080701.jar				selenium-api-2.42.2.jar
calcite-core-1.2.0-incubating.jar		hadoop-yarn-client-2.2.0.jar
	json-20090211.jar				selenium-chrome-driver-2.42.2.jar
calcite-linq4j-1.2.0-incubating.jar		hadoop-yarn-common-2.2.0.jar
	json4s-ast_2.10-3.2.10.jar
selenium-firefox-driver-2.42.2.jar
cglib-2.2.1-v20090111.jar
hadoop-yarn-server-common-2.2.0.jar		json4s-core_2.10-3.2.10.jar
	selenium-htmlunit-driver-2.42.2.jar
cglib-nodep-2.1_3.jar
hadoop-yarn-server-nodemanager-2.2.0.jar	json4s-jackson_2.10-3.2.10.jar
		selenium-ie-driver-2.42.2.jar
chill-java-0.5.0.jar				hamcrest-core-1.1.jar
	jsr173_api-1.0.jar				selenium-java-2.42.2.jar
chill_2.10-0.5.0.jar				hamcrest-core-1.3.jar
	jsr305-1.3.9.jar				selenium-remote-driver-2.42.2.jar
commons-beanutils-1.7.0.jar			hamcrest-library-1.3.jar
	jsr305-2.0.1.jar				selenium-safari-driver-2.42.2.jar
commons-beanutils-core-1.8.0.jar		hive-exec-1.2.1.spark.jar
	jta-1.1.jar					selenium-support-2.42.2.jar
commons-cli-1.2.jar				hive-metastore-1.2.1.spark.jar
		jtransforms-2.4.0.jar				serializer-2.7.1.jar
commons-codec-1.10.jar				htmlunit-2.14.jar
jul-to-slf4j-1.7.10.jar				slf4j-api-1.7.10.jar
commons-codec-1.4.jar				htmlunit-core-js-2.14.jar
	junit-4.10.jar					slf4j-log4j12-1.7.10.jar
commons-codec-1.5.jar				httpclient-4.3.2.jar
	junit-dep-4.10.jar				snappy-0.2.jar
commons-codec-1.9.jar				httpcore-4.3.1.jar
	junit-dep-4.8.2.jar				spire-macros_2.10-0.7.4.jar
commons-collections-3.2.1.jar			httpmime-4.3.2.jar
	junit-interface-0.10.jar			spire_2.10-0.7.4.jar
commons-compiler-2.7.8.jar			istack-commons-runtime-2.16.jar
		junit-interface-0.9.jar				stax-api-1.0.1.jar
commons-compress-1.4.1.jar			ivy-2.4.0.jar
	libfb303-0.9.2.jar				stream-2.7.0.jar
commons-configuration-1.6.jar			jackson-core-asl-1.8.8.jar
	libthrift-0.9.2.jar				stringtemplate-3.2.1.jar
commons-dbcp-1.4.jar				jackson-core-asl-1.9.13.jar
	lz4-1.3.0.jar					tachyon-client-0.7.1.jar
commons-digester-1.8.jar			jackson-jaxrs-1.8.8.jar
	mesos-0.21.1-shaded-protobuf.jar
tachyon-underfs-hdfs-0.7.1.jar
commons-exec-1.1.jar				jackson-mapper-asl-1.9.13.jar
	minlog-1.2.jar
tachyon-underfs-local-0.7.1.jar
commons-httpclient-3.1.jar			jackson-xc-1.8.8.jar
	mockito-core-1.9.5.jar				test-interface-0.5.jar
commons-io-2.1.jar				janino-2.7.8.jar
mysql-connector-java-5.1.34.jar			test-interface-1.0.jar
commons-io-2.4.jar				jansi-1.4.jar
	nekohtml-1.9.20.jar				uncommons-maths-1.2.2a.jar
commons-lang-2.5.jar				javassist-3.15.0-GA.jar
	netty-all-4.0.29.Final.jar			unused-1.0.0.jar
commons-lang-2.6.jar				javax.inject-1.jar
	objenesis-1.0.jar				webbit-0.4.14.jar
commons-lang3-3.3.2.jar				jaxb-api-2.2.2.jar
	objenesis-1.2.jar				xalan-2.7.1.jar
commons-logging-1.1.3.jar			jaxb-api-2.2.7.jar
	opencsv-2.3.jar					xercesImpl-2.11.0.jar
commons-math-2.1.jar				jaxb-core-2.2.7.jar
	oro-2.0.8.jar					xml-apis-1.4.01.jar
commons-math-2.2.jar				jaxb-impl-2.2.3-1.jar
	paranamer-2.3.jar				xmlenc-0.52.jar
commons-math3-3.4.1.jar				jaxb-impl-2.2.7.jar
	paranamer-2.6.jar				xz-1.0.jar
commons-net-3.1.jar				jblas-1.2.4.jar
	parquet-avro-1.7.0.jar				zookeeper-3.4.5.jar
commons-pool-1.5.4.jar				jcl-over-slf4j-1.7.10.jar
	parquet-column-1.7.0.jar
core-1.1.2.jar					jdo-api-3.0.1.jar
parquet-common-1.7.0.jar
cssparser-0.9.13.jar				jersey-guice-1.9.jar
	parquet-encoding-1.7.0.jar

Ted Yu <yuzhihong@gmail.com> wrote on 09/22/2015 01:32:39 PM:

.9.jar
"
Ted Yu <yuzhihong@gmail.com>,"Tue, 22 Sep 2015 15:29:38 -0700",Re: Derby version in Spark,Richard Hillegas <rhilleg@us.ibm.com>,"I see.
I use maven to build so I observe different contents under lib_managed
directory.

Here is snippet of dependency tree:

[INFO] |  +- org.spark-project.hive:hive-metastore:jar:1.2.1.spark:compile
[INFO] |  |  +- com.jolbox:bonecp:jar:0.8.0.RELEASE:compile
[INFO] |  |  +- org.apache.derby:derby:jar:10.10.1.1:compile


"
Ted Yu <yuzhihong@gmail.com>,"Tue, 22 Sep 2015 15:41:12 -0700",Re: Derby version in Spark,Richard Hillegas <rhilleg@us.ibm.com>,"I cloned Hive 1.2 code base and saw:

    <derby.version>10.10.2.0</derby.version>

So the version used by Spark is quite close to what Hive uses.


"
Richard Hillegas <rhilleg@us.ibm.com>,"Tue, 22 Sep 2015 15:48:14 -0700",Re: Derby version in Spark,Dev <dev@spark.apache.org>,"
Thanks, Ted. I'll follow up with the Hive folks.

Cheers,
-Rick

Ted Yu <yuzhihong@gmail.com> wrote on 09/22/2015 03:41:12 PM:

org.spark-project.hive:hive-metastore:jar:1.2.1.spark:compile

.jar
snappy-0.2.jar
stream-2.7.0.jar
webbit-0.4.14.jar
xalan-2.7.1.jar
xml-apis-1.4.01.jar
xmlenc-0.52.jar
jar
zookeeper-3.4.5.jar
.jar

lib_managed/jars/datanucleus-rdbms-3.2.9.jar
om
5

k

g
"
Richard Hillegas <rhilleg@us.ibm.com>,"Tue, 22 Sep 2015 16:05:14 -0700",Re: column identifiers in Spark SQL,Dev <dev@spark.apache.org>,"
Thanks for that additional tip, Michael. Backticks fix the problem query in
which an identifier was transformed into a string literal. So this works
now...

  // now correctly resolves the unnormalized column id
  sqlContext.sql(""""""select `b` from test_data"""""").show

Any suggestion about how to escape an embedded double quote?

  // java.sql.SQLSyntaxErrorException: Syntax error: Encountered ""\"""" at
line 1, column 12.
  sqlContext.sql(""""""select `c""d` from test_data"""""").show

  // org.apache.spark.sql.AnalysisException: cannot resolve 'c\""d' given
input columns A, b, c""d; line 1 pos 7
  sqlContext.sql(""""""select `c\""d` from test_data"""""").show

Thanks,
-Rick

Michael Armbrust <michael@databricks.com> wrote on 09/22/2015 01:16:12 PM:

6
AM:
de
e
xt
e
com
m
:
erby1"",

/
:



that?
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Tue, 22 Sep 2015 19:21:36 -0700",Re: SparkR package path,"""Sun, Rui"" <rui.sun@intel.com>","As Rui says it would be good to understand the use case we want to
support (supporting CRAN installs could be one for example). I don't
think it should be very hard to do as the RBackend itself doesn't use
the R source files. The RRDD does use it and the value comes from
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/api/r/RUtils.scala#L29
AFAIK -- So we could introduce a new config flag that can be used for
this new mode.

Thanks
Shivaram


---------------------------------------------------------------------


"
Andy Huang <andy.huang@servian.com.au>,"Wed, 23 Sep 2015 15:03:39 +1000",Fwd: Parallel collection in driver programs,dev@spark.apache.org,"Hi Devs,

Hopefully one of you know more on this?

Thanks

Andy
---------- Forwarded message ----------
From: Andy Huang <andy.huang@servian.com.au>
Date: Wed, Sep 23, 2015 at 12:39 PM
Subject: Parallel collection in driver programs
To: user@spark.apache.org


Hi All,

Would like know if anyone has experienced with parallel collection in the
driver program. And, if there is actual advantage/disadvantage of doing so.

E.g. With a collection of Jdbc connections and tables

We have adapted our non-spark code which utilize parallel collection to the
spark code and it seems to work fine.

val conf = List(
  (""tbl1"",""dbo.tbl1::tb1_id::0::127::128""),
  (""tbl2"",""dbo.tbl2::tb2_id::0::31::32""),
  (""tbl3"",""dbo.tbl3::tb3_id::0::63::64"")
)

val _JDBC_DEFAULT = ""jdbc:sqlserver://192.168.52.1;database=TestSource""
val _STORE_DEFAULT = ""hdfs://192.168.52.132:9000/""

val prop = new Properties()
prop.setProperty(""user"",""sa"")
prop.setProperty(""password"",""password"")

conf.par.map(pair=>{

  val qry = pair._2.split(""::"")(0)
  val pCol = pair._2.split(""::"")(1)
  val lo = pair._2.split(""::"")(2).toInt
  val hi = pair._2.split(""::"")(3).toInt
  val part = pair._2.split(""::"")(4).toInt

  //create dataframe from jdbc table
  val jdbcDF = sqlContext.read.jdbc(
    _JDBC_DEFAULT,
    ""(""+qry+"") a"",
    pCol,
    lo, //lower bound
    hi, //upper bound
    part, //number of partitions
    prop //java.utils.Properties - key value pair
  )

  //save to parquet
  jdbcDF.write.mode(""overwrite"").parquet(_STORE_DEFAULT+pair._1+"".parquet"")

})


Thanks.
-- 
Andy Huang | Managing Consultant | Servian Pty Ltd | t: 02 9376 0700 |
f: 02 9376 0730| m: 0433221979



-- 
Andy Huang | Managing Consultant | Servian Pty Ltd | t: 02 9376 0700 |
f: 02 9376 0730| m: 0433221979
"
qiuhai <986775883@qq.com>,"Tue, 22 Sep 2015 22:57:33 -0700 (MST)",Why Filter return a DataFrame object in DataFrame.scala?,dev@spark.apache.org,"Hi,
  Recently,I am reading source codeï¼ˆ1.5 versionï¼‰ about sparksql .
   ã€€In DataFrame.scala, there is a funtion named filter inã€€the 737 row 

        *def filter(condition: Column): DataFrame = Filter(condition.expr,
logicalPlan)*

  The fucntion return  a Filter objectï¼Œbut it require a DataFrame object.


  thanks.



--
3.nabble.com/Why-Filter-return-a-DataFrame-object-in-DataFrame-scala-tp14295.html
om.

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Tue, 22 Sep 2015 22:59:42 -0700",Re: Why Filter return a DataFrame object in DataFrame.scala?,qiuhai <986775883@qq.com>,"There is an implicit conversion in scope

https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/DataFrame.scala#L153


  /**
   * An implicit conversion function internal to this class for us to avoid
doing
   * ""new DataFrame(...)"" everywhere.
   */
  @inline private implicit def logicalPlanToDataFrame(logicalPlan:
LogicalPlan): DataFrame = {
    new DataFrame(sqlContext, logicalPlan)
  }



arksql .
pr,
 object.
rn-a-DataFrame-object-in-DataFrame-scala-tp14295.html
"
Reynold Xin <rxin@databricks.com>,"Tue, 22 Sep 2015 23:00:31 -0700",Re: Why Filter return a DataFrame object in DataFrame.scala?,qiuhai <986775883@qq.com>,"There is an implicit conversion in scope

https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/DataFrame.scala#L153


  /**
   * An implicit conversion function internal to this class for us to avoid
doing
   * ""new DataFrame(...)"" everywhere.
   */
  @inline private implicit def logicalPlanToDataFrame(logicalPlan:
LogicalPlan): DataFrame = {
    new DataFrame(sqlContext, logicalPlan)
  }



arksql .
pr,
 object.
rn-a-DataFrame-object-in-DataFrame-scala-tp14295.html
"
qiuhai <986775883@qq.com>,"Tue, 22 Sep 2015 23:03:49 -0700 (MST)",Re: Why Filter return a DataFrame object in DataFrame.scala?,dev@spark.apache.org,"Thank you very much



--

---------------------------------------------------------------------


"
Steve Loughran <stevel@hortonworks.com>,"Wed, 23 Sep 2015 09:05:22 +0000",using Codahale counters in source,dev <dev@spark.apache.org>,"


Quick question: is it OK to use Codahale Metric classes (e.g. Counter) in source as generic thread-safe counters, with the option of hooking them to a Codahale metrics registry if there is one in the spark context?

The Counter class does extend LongAdder, which is by Doug Lea and promises to be a better performing long counter when update contention is low â€”such as when precisely one thread is doing the updates.

I've done that in other projects, and it works relatively well, in that you don't have to add extra code for metrics, you just have to make sure you implement counters that are relevant, and, when registring them, given them a useful name.

-Steve"
Bin Wang <wbin00@gmail.com>,"Wed, 23 Sep 2015 10:58:34 +0000",Checkpoint directory structure,"""dev@spark.apache.org"" <dev@spark.apache.org>","I find the checkpoint directory structure is like this:

-rw-r--r--   1 root root     134820 2015-09-23 16:55
/user/root/checkpoint/checkpoint-1442998500000
-rw-r--r--   1 root root     134768 2015-09-23 17:00
/user/root/checkpoint/checkpoint-1442998800000
-rw-r--r--   1 root root     134895 2015-09-23 17:05
/user/root/checkpoint/checkpoint-1442999100000
-rw-r--r--   1 root root     134899 2015-09-23 17:10
/user/root/checkpoint/checkpoint-1442999400000
-rw-r--r--   1 root root     134913 2015-09-23 17:15
/user/root/checkpoint/checkpoint-1442999700000
-rw-r--r--   1 root root     134928 2015-09-23 17:20
/user/root/checkpoint/checkpoint-1443000000000
-rw-r--r--   1 root root     134987 2015-09-23 17:25
/user/root/checkpoint/checkpoint-1443000300000
-rw-r--r--   1 root root     134944 2015-09-23 17:30
/user/root/checkpoint/checkpoint-1443000600000
-rw-r--r--   1 root root     134956 2015-09-23 17:35
/user/root/checkpoint/checkpoint-1443000900000
-rw-r--r--   1 root root     135244 2015-09-23 17:40
/user/root/checkpoint/checkpoint-1443001200000
drwxr-xr-x   - root root          0 2015-09-23 18:48
/user/root/checkpoint/d3714249-e03a-45c7-a0d5-1dc870b7d9f2
drwxr-xr-x   - root root          0 2015-09-23 17:44
/user/root/checkpoint/receivedBlockMetadata


I restart spark and it reads from
/user/root/checkpoint/d3714249-e03a-45c7-a0d5-1dc870b7d9f2. But it seems
that the data in it lost some rdds so it is not able to recovery. While I
find other directories in checkpoint/, like
 /user/root/checkpoint/checkpoint-1443001200000.  What does it used for?
Can I recovery my data from that?
"
Hossein <falaki@gmail.com>,"Wed, 23 Sep 2015 10:41:58 -0700",Re: SparkR package path,shivaram@eecs.berkeley.edu,"Yes, I think exposing SparkR in CRAN can significantly expand the reach of
both SparkR and Spark itself to a larger community of data scientists (and
statisticians).

I have been getting questions on how to use SparkR in RStudio. Most of
these folks have a Spark Cluster and wish to talk to it from RStudio. While
that is a bigger task, for now, first step could be not requiring them to
download Spark source and run a script that is named install-dev.sh. I
filed SPARK-10776 to track this.


--Hossein


"
Marcelo Vanzin <vanzin@cloudera.com>,"Wed, 23 Sep 2015 15:13:07 -0700",RFC: packaging Spark without assemblies,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey all,

This is something that we've discussed several times internally, but
never really had much time to look into; but as time passes by, it's
increasingly becoming an issue for us and I'd like to throw some ideas
around about how to fix it.

So, without further ado:
https://github.com/vanzin/spark/pull/2/files

(You can comment there or click ""View"" to read the formatted document.
I thought that would be easier than sharing on Google Drive or Box or
something.)

It would be great to get people's feedback, especially if there are
strong reasons for the assemblies that I'm not aware of.


-- 
Marcelo

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Wed, 23 Sep 2015 16:43:26 -0700",Re: RFC: packaging Spark without assemblies,Marcelo Vanzin <vanzin@cloudera.com>,"I think it would be a big improvement to get rid of it. It's not how
jars are supposed to be packaged and it has caused problems in many
different context over the years.

For me a key step in moving away would be to fully audit/understand
all compatibility implications of removing it. If other people are
supportive of this plan I can offer to help spend some time thinking
about any potential corner cases, etc.

- Patrick


---------------------------------------------------------------------


"
Bin Wang <wbin00@gmail.com>,"Thu, 24 Sep 2015 01:32:01 +0000",Re: Checkpoint directory structure,"""dev@spark.apache.org"" <dev@spark.apache.org>","BTW, I just kill the application and restart it. Then the application
cannot recover from checkpoint because of some lost of RDD. So I'm wonder,
if there are some failure in the application, won't it possible not be able
to recovery from checkpoint?

Bin Wang <wbin00@gmail.com>äºŽ2015å¹´9æœˆ23æ—¥å‘¨ä¸‰ ä¸‹åˆ6:58å†™é“ï¼š

"
Tathagata Das <tathagata.das1565@gmail.com>,"Wed, 23 Sep 2015 18:44:50 -0700",Re: Checkpoint directory structure,Bin Wang <wbin00@gmail.com>,"Could you provide the logs on when and how you are seeing this error?


,
le
‘¨ä¸‰ ä¸‹åˆ6:58å†™é“ï¼š
I
"
"""Sun, Rui"" <rui.sun@intel.com>","Thu, 24 Sep 2015 02:28:42 +0000",RE: SparkR package path,"Hossein <falaki@gmail.com>, ""shivaram@eecs.berkeley.edu""
	<shivaram@eecs.berkeley.edu>","SparkR package is not a standalone R package, as it is actually R API of Spark and needs to co-operate with a matching version of Spark, so exposing it in CRAN does not ease use of R users as they need to download matching Spark distribution, unless we expose a bundled SparkR package to CRAN (packageing with Spark), is this desirable? Actually, for normal users who are not developers, they are not required to download Spark source, build and install SparkR package. They just need to download a Spark distribution, and then use SparkR.

For using SparkR in Rstudio, there is a documentation at https://github.com/apache/spark/tree/master/R



From: Hossein [mailto:falaki@gmail.com]
Sent: Thursday, September 24, 2015 1:42 AM
To: shivaram@eecs.berkeley.edu
Cc: Sun, Rui; dev@spark.apache.org
Subject: Re: SparkR package path

Yes, I think exposing SparkR in CRAN can significantly expand the reach of both SparkR and Spark itself to a larger community of data scientists (and statisticians).

I have been getting questions on how to use SparkR in RStudio. Most of these folks have a Spark Cluster and wish to talk to it from RStudio. While that is a bigger task, for now, first step could be not requiring them to download Spark source and run a script that is named install-dev.sh. I filed SPARK-10776 to track this.


--Hossein

On Tue, Sep 22, 2015 at 7:21 PM, Shivaram Venkataraman <shivaram@eecs.bs Rui says it would be good to understand the use case we want to
support (supporting CRAN installs could be one for example). I don't
think it should be very hard to do as the RBackend itself doesn't use
the R source files. The RRDD does use it and the value comes from
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/api/r/RUtils.scala#L29
AFAIK -- So we could introduce a new config flag that can be used for
this new mode.

Thanks
Shivaram

On Mon, Sep 21, 2015 at 8:15 PM, Sun, Rui <rui.sun@intel.com<mailto:rui.sun@intel.com>> wrote:
> Hossein,
>
>
>
> Any strong reason to download and install SparkR source package separately
> from the Spark distribution?
>
> An R user can simply download the spark distribution, which contains SparkR
> source and binary package, and directly use sparkR. No need to install
> SparkR package at all.
>
>
>
> From: Hossein [mailto:falaki@gmail.com<mailto:falaki@gmail.com>]
> Sent: Tuesday, September 22, 2015 9:19 AM
> To: dev@spark.apache.org<mailto:dev@spark.apache.org>
> Subject: SparkR package path
>
>
>
> Hi dev list,
>
>
>
> SparkR backend assumes SparkR source files are located under
> ""SPARK_HOME/R/lib/."" This directory is created by running R/install-dev.sh.
> This setting makes sense for Spark developers, but if an R user downloads
> and installs SparkR source package, the source files are going to be in
> placed different locations.
>
>
>
> In the R runtime it is easy to find location of package files using
> path.package(""SparkR""). But we need to make some changes to R backend and/or
> spark-submit so that, JVM process learns the location of worker.R and
> daemon.R and shell.R from the R runtime.
>
>
>
> Do you think this change is feasible?
>
>
>
> Thanks,
>
> --Hossein

"
Bin Wang <wbin00@gmail.com>,"Thu, 24 Sep 2015 02:33:17 +0000",Re: Checkpoint directory structure,Tathagata Das <tathagata.das1565@gmail.com>,"I've attached the full log. The error is like this:

15/09/23 17:47:39 ERROR yarn.ApplicationMaster: User class threw exception:
java.lang.IllegalArgumentException: requirement failed: Checkpoint
directory does not exist: hdfs://
szq2.appadhoc.com:8020/user/root/checkpoint/d3714249-e03a-45c7-a0d5-1dc870b7d9f2/rdd-26909
java.lang.IllegalArgumentException: requirement failed: Checkpoint
directory does not exist: hdfs://
szq2.appadhoc.com:8020/user/root/checkpoint/d3714249-e03a-45c7-a0d5-1dc870b7d9f2/rdd-26909
at scala.Predef$.require(Predef.scala:233)
at
org.apache.spark.rdd.ReliableCheckpointRDD.<init>(ReliableCheckpointRDD.scala:45)
at
org.apache.spark.SparkContext$$anonfun$checkpointFile$1.apply(SparkContext.scala:1227)
at
org.apache.spark.SparkContext$$anonfun$checkpointFile$1.apply(SparkContext.scala:1227)
at
org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
at
org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
at org.apache.spark.SparkContext.withScope(SparkContext.scala:709)
at org.apache.spark.SparkContext.checkpointFile(SparkContext.scala:1226)
at
org.apache.spark.streaming.dstream.DStreamCheckpointData$$anonfun$restore$1.apply(DStreamCheckpointData.scala:112)
at
org.apache.spark.streaming.dstream.DStreamCheckpointData$$anonfun$restore$1.apply(DStreamCheckpointData.scala:109)
at
scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
at
scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
at
scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
at
org.apache.spark.streaming.dstream.DStreamCheckpointData.restore(DStreamCheckpointData.scala:109)
at
org.apache.spark.streaming.dstream.DStream.restoreCheckpointData(DStream.scala:487)
at
org.apache.spark.streaming.dstream.DStream$$anonfun$restoreCheckpointData$2.apply(DStream.scala:488)
at
org.apache.spark.streaming.dstream.DStream$$anonfun$restoreCheckpointData$2.apply(DStream.scala:488)
at scala.collection.immutable.List.foreach(List.scala:318)
at
org.apache.spark.streaming.dstream.DStream.restoreCheckpointData(DStream.scala:488)
at
org.apache.spark.streaming.dstream.DStream$$anonfun$restoreCheckpointData$2.apply(DStream.scala:488)
at
org.apache.spark.streaming.dstream.DStream$$anonfun$restoreCheckpointData$2.apply(DStream.scala:488)
at scala.collection.immutable.List.foreach(List.scala:318)
at
org.apache.spark.streaming.dstream.DStream.restoreCheckpointData(DStream.scala:488)
at
org.apache.spark.streaming.dstream.DStream$$anonfun$restoreCheckpointData$2.apply(DStream.scala:488)
at
org.apache.spark.streaming.dstream.DStream$$anonfun$restoreCheckpointData$2.apply(DStream.scala:488)
at scala.collection.immutable.List.foreach(List.scala:318)
at
org.apache.spark.streaming.dstream.DStream.restoreCheckpointData(DStream.scala:488)
at
org.apache.spark.streaming.DStreamGraph$$anonfun$restoreCheckpointData$2.apply(DStreamGraph.scala:153)
at
org.apache.spark.streaming.DStreamGraph$$anonfun$restoreCheckpointData$2.apply(DStreamGraph.scala:153)
at
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
at
org.apache.spark.streaming.DStreamGraph.restoreCheckpointData(DStreamGraph.scala:153)
at
org.apache.spark.streaming.StreamingContext.<init>(StreamingContext.scala:158)
at
org.apache.spark.streaming.StreamingContext$$anonfun$getOrCreate$1.apply(StreamingContext.scala:837)
at
org.apache.spark.streaming.StreamingContext$$anonfun$getOrCreate$1.apply(StreamingContext.scala:837)
at scala.Option.map(Option.scala:145)
at
org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:837)
at com.appadhoc.data.main.StatCounter$.main(StatCounter.scala:51)
at com.appadhoc.data.main.StatCounter.main(StatCounter.scala)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at
org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:525)
15/09/23 17:47:39 INFO yarn.ApplicationMaster: Final app status: FAILED,
exitCode: 15, (reason: User class threw exception:
java.lang.IllegalArgumentException: requirement failed: Checkpoint
directory does not exist: hdfs://
szq2.appadhoc.com:8020/user/root/checkpoint/d3714249-e03a-45c7-a0d5-1dc870b7d9f2/rdd-26909
)
15/09/23 17:47:39 INFO spark.SparkContext: Invoking stop() from shutdown
hook


Tathagata Das <tathagata.das1565@gmail.com>äºŽ2015å¹´9æœˆ24æ—¥å‘¨å›› ä¸Šåˆ9:45å†™é“ï¼š

r,
ble
å‘¨ä¸‰ ä¸‹åˆ6:58å†™é“ï¼š
s
 I
?
Log Type: stderr
Log Upload Time: Wed Sep 23 17:47:51 +0800 2015
Log Length: 55303
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/yarn/nm/usercache/root/filecache/6753/spark-assembly-1.5.1-SNAPSHOT-hadoop2.6.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-5.4.1-1.cdh5.4.1.p0.6/jars/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
15/09/23 17:47:28 INFO yarn.ApplicationMaster: Registered signal handlers for [TERM, HUP, INT]
15/09/23 17:47:31 INFO yarn.ApplicationMaster: ApplicationAttemptId: appattempt_1440495451668_0297_000001
15/09/23 17:47:31 INFO spark.SecurityManager: Changing view acls to: yarn,root
15/09/23 17:47:31 INFO spark.SecurityManager: Changing modify acls to: yarn,root
15/09/23 17:47:31 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(yarn, root); users with modify permissions: Set(yarn, root)
15/09/23 17:47:32 INFO yarn.ApplicationMaster: Starting the user application in a separate Thread
15/09/23 17:47:32 INFO yarn.ApplicationMaster: Waiting for spark context initialization
15/09/23 17:47:32 INFO yarn.ApplicationMaster: Waiting for spark context initialization ... 
15/09/23 17:47:32 INFO streaming.CheckpointReader: Checkpoint files found: hdfs://szq2.appadhoc.com:8020/user/root/checkpoint/checkpoint-1443001200000,hdfs://szq2.appadhoc.com:8020/user/root/checkpoint/checkpoint-1443000900000,hdfs://szq2.appadhoc.com:8020/user/root/checkpoint/checkpoint-1443000600000,hdfs://szq2.appadhoc.com:8020/user/root/checkpoint/checkpoint-1443000300000,hdfs://szq2.appadhoc.com:8020/user/root/checkpoint/checkpoint-1443000000000,hdfs://szq2.appadhoc.com:8020/user/root/checkpoint/checkpoint-1442999700000,hdfs://szq2.appadhoc.com:8020/user/root/checkpoint/checkpoint-1442999400000,hdfs://szq2.appadhoc.com:8020/user/root/checkpoint/checkpoint-1442999100000,hdfs://szq2.appadhoc.com:8020/user/root/checkpoint/checkpoint-1442998800000,hdfs://szq2.appadhoc.com:8020/user/root/checkpoint/checkpoint-1442998500000
15/09/23 17:47:32 INFO streaming.CheckpointReader: Attempting to load checkpoint from file hdfs://szq2.appadhoc.com:8020/user/root/checkpoint/checkpoint-1443001200000
15/09/23 17:47:33 INFO streaming.Checkpoint: Checkpoint for time 1443001200000 ms validated
15/09/23 17:47:33 INFO streaming.CheckpointReader: Checkpoint successfully loaded from file hdfs://szq2.appadhoc.com:8020/user/root/checkpoint/checkpoint-1443001200000
15/09/23 17:47:33 INFO streaming.CheckpointReader: Checkpoint was generated at time 1443001200000 ms
15/09/23 17:47:33 INFO spark.SparkContext: Running Spark version 1.5.0
15/09/23 17:47:33 INFO spark.SecurityManager: Changing view acls to: yarn,root
15/09/23 17:47:33 INFO spark.SecurityManager: Changing modify acls to: yarn,root
15/09/23 17:47:33 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(yarn, root); users with modify permissions: Set(yarn, root)
15/09/23 17:47:33 INFO slf4j.Slf4jLogger: Slf4jLogger started
15/09/23 17:47:33 INFO Remoting: Starting remoting
15/09/23 17:47:33 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.1.1:60297]
15/09/23 17:47:33 INFO util.Utils: Successfully started service 'sparkDriver' on port 60297.
15/09/23 17:47:33 INFO spark.SparkEnv: Registering MapOutputTracker
15/09/23 17:47:33 INFO spark.SparkEnv: Registering BlockManagerMaster
15/09/23 17:47:33 INFO storage.DiskBlockManager: Created local directory at /yarn/nm/usercache/root/appcache/application_1440495451668_0297/blockmgr-879fbea0-e88f-4da9-87d5-5901fb589848
15/09/23 17:47:33 INFO storage.MemoryStore: MemoryStore started with capacity 520.8 MB
15/09/23 17:47:33 INFO spark.HttpFileServer: HTTP File server directory is /yarn/nm/usercache/root/appcache/application_1440495451668_0297/spark-26c88c3e-55d5-42db-8672-5cb418213119/httpd-2acfcba6-6d7b-4c03-ba56-972b48371de3
15/09/23 17:47:33 INFO spark.HttpServer: Starting HTTP Server
15/09/23 17:47:33 INFO server.Server: jetty-8.y.z-SNAPSHOT
15/09/23 17:47:33 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:11387
15/09/23 17:47:33 INFO util.Utils: Successfully started service 'HTTP file server' on port 11387.
15/09/23 17:47:34 INFO spark.SparkEnv: Registering OutputCommitCoordinator
15/09/23 17:47:34 INFO ui.JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
15/09/23 17:47:34 INFO server.Server: jetty-8.y.z-SNAPSHOT
15/09/23 17:47:34 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:35721
15/09/23 17:47:34 INFO util.Utils: Successfully started service 'SparkUI' on port 35721.
15/09/23 17:47:34 INFO ui.SparkUI: Started SparkUI at http://192.168.1.1:35721
15/09/23 17:47:34 INFO cluster.YarnClusterScheduler: Created YarnClusterScheduler
15/09/23 17:47:34 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33428.
15/09/23 17:47:34 INFO netty.NettyBlockTransferService: Server created on 33428
15/09/23 17:47:34 INFO storage.BlockManagerMaster: Trying to register BlockManager
15/09/23 17:47:34 INFO storage.BlockManagerMasterEndpoint: Registering block manager 192.168.1.1:33428 with 520.8 MB RAM, BlockManagerId(driver, 192.168.1.1, 33428)
15/09/23 17:47:34 INFO storage.BlockManagerMaster: Registered BlockManager
15/09/23 17:47:34 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as AkkaRpcEndpointRef(Actor[akka://sparkDriver/user/YarnAM#-1356902315])
15/09/23 17:47:34 INFO client.RMProxy: Connecting to ResourceManager at szq2.appadhoc.com/192.168.1.2:8030
15/09/23 17:47:34 INFO yarn.YarnRMClient: Registering the ApplicationMaster
15/09/23 17:47:34 INFO yarn.YarnAllocator: Will request 1 executor containers, each with 4 cores and 33792 MB memory including 3072 MB overhead
15/09/23 17:47:34 INFO yarn.YarnAllocator: Container request (host: Any, capability: <memory:33792, vCores:4>)
15/09/23 17:47:34 INFO yarn.ApplicationMaster: Started progress reporter thread with (heartbeat : 3000, initial allocation : 200) intervals
15/09/23 17:47:35 INFO impl.AMRMClientImpl: Received new token for : szq1.appadhoc.com:8041
15/09/23 17:47:36 INFO yarn.YarnAllocator: Launching container container_1440495451668_0297_01_000002 for on host szq1.appadhoc.com
15/09/23 17:47:36 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@192.168.1.1:60297/user/CoarseGrainedScheduler,  executorHostname: szq1.appadhoc.com
15/09/23 17:47:36 INFO yarn.YarnAllocator: Received 1 containers from YARN, launching executors on 1 of them.
15/09/23 17:47:36 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/23 17:47:36 INFO impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
15/09/23 17:47:36 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/23 17:47:36 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/23 17:47:36 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource { scheme: ""hdfs"" host: ""szq2.appadhoc.com"" port: 8020 file: ""/user/root/.sparkStaging/application_1440495451668_0297/adhoc-data-assembly-1.0.jar"" } size: 87320747 timestamp: 1443001639377 type: FILE visibility: PRIVATE, __spark__.jar -> resource { scheme: ""hdfs"" host: ""szq2.appadhoc.com"" port: 8020 file: ""/user/root/.sparkStaging/application_1440495451668_0297/spark-assembly-1.5.1-SNAPSHOT-hadoop2.6.0.jar"" } size: 142738268 timestamp: 1443001631742 type: FILE visibility: PRIVATE)
15/09/23 17:47:36 INFO yarn.ExecutorRunnable: 
===============================================================================
YARN executor launch context:
  env:
    CLASSPATH -> {{PWD}}<CPS>{{PWD}}/__spark__.jar<CPS>$HADOOP_CLIENT_CONF_DIR<CPS>$HADOOP_CONF_DIR<CPS>$HADOOP_COMMON_HOME/*<CPS>$HADOOP_COMMON_HOME/lib/*<CPS>$HADOOP_HDFS_HOME/*<CPS>$HADOOP_HDFS_HOME/lib/*<CPS>$HADOOP_YARN_HOME/*<CPS>$HADOOP_YARN_HOME/lib/*<CPS>$HADOOP_MAPRED_HOME/*<CPS>$HADOOP_MAPRED_HOME/lib/*<CPS>$MR2_CLASSPATH
    SPARK_LOG_URL_STDERR -> http://szq1.appadhoc.com:8042/node/containerlogs/container_1440495451668_0297_01_000002/root/stderr?start=-4096
    SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1440495451668_0297
    SPARK_YARN_CACHE_FILES_FILE_SIZES -> 142738268,87320747
    SPARK_USER -> root
    SPARK_YARN_CACHE_FILES_VISIBILITIES -> PRIVATE,PRIVATE
    SPARK_YARN_MODE -> true
    SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1443001631742,1443001639377
    SPARK_LOG_URL_STDOUT -> http://szq1.appadhoc.com:8042/node/containerlogs/container_1440495451668_0297_01_000002/root/stdout?start=-4096
    SPARK_YARN_CACHE_FILES -> hdfs://szq2.appadhoc.com:8020/user/root/.sparkStaging/application_1440495451668_0297/spark-assembly-1.5.1-SNAPSHOT-hadoop2.6.0.jar#__spark__.jar,hdfs://szq2.appadhoc.com:8020/user/root/.sparkStaging/application_1440495451668_0297/adhoc-data-assembly-1.0.jar#__app__.jar

  command:
    {{JAVA_HOME}}/bin/java -server -XX:OnOutOfMemoryError='kill %p' -Xms30720m -Xmx30720m '-Dconfig.resource=prod.conf' -Djava.io.tmpdir={{PWD}}/tmp '-Dspark.driver.port=60297' '-Dspark.ui.port=0' -Dspark.yarn.app.container.log.dir=<LOG_DIR> org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url akka.tcp://sparkDriver@192.168.1.1:60297/user/CoarseGrainedScheduler --executor-id 1 --hostname szq1.appadhoc.com --cores 4 --app-id application_1440495451668_0297 --user-class-path file:$PWD/__app__.jar 1> <LOG_DIR>/stdout 2> <LOG_DIR>/stderr
===============================================================================
      
15/09/23 17:47:36 INFO impl.ContainerManagementProtocolProxy: Opening proxy : szq1.appadhoc.com:8041
15/09/23 17:47:38 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. szq1.appadhoc.com:61805
15/09/23 17:47:39 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@szq1.appadhoc.com:49069/user/Executor#1058701258]) with ID 1
15/09/23 17:47:39 INFO cluster.YarnClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
15/09/23 17:47:39 INFO cluster.YarnClusterScheduler: YarnClusterScheduler.postStartHook done
15/09/23 17:47:39 INFO dstream.ForEachDStream: Set context for org.apache.spark.streaming.dstream.ForEachDStream@3fa6b67b
15/09/23 17:47:39 INFO dstream.MappedDStream: Set context for org.apache.spark.streaming.dstream.MappedDStream@421046a8
15/09/23 17:47:39 INFO dstream.MappedDStream: Set context for org.apache.spark.streaming.dstream.MappedDStream@3a8ee17a
15/09/23 17:47:39 INFO dstream.StateDStream: Set context for org.apache.spark.streaming.dstream.StateDStream@7a752e9e
15/09/23 17:47:39 INFO dstream.MappedDStream: Set context for org.apache.spark.streaming.dstream.MappedDStream@2347c5e1
15/09/23 17:47:39 INFO dstream.FilteredDStream: Set context for org.apache.spark.streaming.dstream.FilteredDStream@3d1048df
15/09/23 17:47:39 INFO dstream.MapPartitionedDStream: Set context for org.apache.spark.streaming.dstream.MapPartitionedDStream@782047ca
15/09/23 17:47:39 INFO dstream.MappedDStream: Set context for org.apache.spark.streaming.dstream.MappedDStream@f495c63
15/09/23 17:47:39 INFO kafka.KafkaInputDStream: Set context for org.apache.spark.streaming.kafka.KafkaInputDStream@19506f6c
15/09/23 17:47:39 INFO dstream.ForEachDStream: Set context for org.apache.spark.streaming.dstream.ForEachDStream@6fc3286f
15/09/23 17:47:39 INFO dstream.MappedDStream: Set context for org.apache.spark.streaming.dstream.MappedDStream@7593c8bb
15/09/23 17:47:39 INFO dstream.MappedDStream: Set context for org.apache.spark.streaming.dstream.MappedDStream@395fa94b
15/09/23 17:47:39 INFO dstream.StateDStream: Set context for org.apache.spark.streaming.dstream.StateDStream@3763985a
15/09/23 17:47:39 INFO dstream.MappedDStream: Set context for org.apache.spark.streaming.dstream.MappedDStream@6ba7591e
15/09/23 17:47:39 INFO dstream.MapPartitionedDStream: Set context for org.apache.spark.streaming.dstream.MapPartitionedDStream@782047ca
15/09/23 17:47:39 INFO dstream.MappedDStream: Set context for org.apache.spark.streaming.dstream.MappedDStream@f495c63
15/09/23 17:47:39 INFO kafka.KafkaInputDStream: Set context for org.apache.spark.streaming.kafka.KafkaInputDStream@19506f6c
15/09/23 17:47:39 INFO dstream.ForEachDStream: Set context for org.apache.spark.streaming.dstream.ForEachDStream@37cbf7c9
15/09/23 17:47:39 INFO dstream.MappedDStream: Set context for org.apache.spark.streaming.dstream.MappedDStream@2ff8c9bd
15/09/23 17:47:39 INFO dstream.ShuffledDStream: Set context for org.apache.spark.streaming.dstream.ShuffledDStream@7684afe5
15/09/23 17:47:39 INFO dstream.MappedDStream: Set context for org.apache.spark.streaming.dstream.MappedDStream@138202d
15/09/23 17:47:39 INFO dstream.MappedDStream: Set context for org.apache.spark.streaming.dstream.MappedDStream@bc86afb
15/09/23 17:47:39 INFO dstream.FlatMappedDStream: Set context for org.apache.spark.streaming.dstream.FlatMappedDStream@12e78ac8
15/09/23 17:47:39 INFO dstream.MapPartitionedDStream: Set context for org.apache.spark.streaming.dstream.MapPartitionedDStream@782047ca
15/09/23 17:47:39 INFO dstream.MappedDStream: Set context for org.apache.spark.streaming.dstream.MappedDStream@f495c63
15/09/23 17:47:39 INFO kafka.KafkaInputDStream: Set context for org.apache.spark.streaming.kafka.KafkaInputDStream@19506f6c
15/09/23 17:47:39 INFO dstream.ForEachDStream: Set context for org.apache.spark.streaming.dstream.ForEachDStream@1f485a2a
15/09/23 17:47:39 INFO dstream.MappedDStream: Set context for org.apache.spark.streaming.dstream.MappedDStream@47178b71
15/09/23 17:47:39 INFO dstream.StateDStream: Set context for org.apache.spark.streaming.dstream.StateDStream@5ac3cc2d
15/09/23 17:47:39 INFO dstream.MappedDStream: Set context for org.apache.spark.streaming.dstream.MappedDStream@6c91cce8
15/09/23 17:47:39 INFO dstream.MappedDStream: Set context for org.apache.spark.streaming.dstream.MappedDStream@bc86afb
15/09/23 17:47:39 INFO dstream.FlatMappedDStream: Set context for org.apache.spark.streaming.dstream.FlatMappedDStream@12e78ac8
15/09/23 17:47:39 INFO dstream.MapPartitionedDStream: Set context for org.apache.spark.streaming.dstream.MapPartitionedDStream@782047ca
15/09/23 17:47:39 INFO dstream.MappedDStream: Set context for org.apache.spark.streaming.dstream.MappedDStream@f495c63
15/09/23 17:47:39 INFO kafka.KafkaInputDStream: Set context for org.apache.spark.streaming.kafka.KafkaInputDStream@19506f6c
15/09/23 17:47:39 INFO dstream.ForEachDStream: Set context for org.apache.spark.streaming.dstream.ForEachDStream@5823cb07
15/09/23 17:47:39 INFO dstream.ShuffledDStream: Set context for org.apache.spark.streaming.dstream.ShuffledDStream@15e665c6
15/09/23 17:47:39 INFO dstream.MappedDStream: Set context for org.apache.spark.streaming.dstream.MappedDStream@4923b565
15/09/23 17:47:39 INFO dstream.StateDStream: Set context for org.apache.spark.streaming.dstream.StateDStream@475a1f66
15/09/23 17:47:39 INFO dstream.MappedDStream: Set context for org.apache.spark.streaming.dstream.MappedDStream@6dc38622
15/09/23 17:47:39 INFO dstream.FlatMappedDStream: Set context for org.apache.spark.streaming.dstream.FlatMappedDStream@12e78ac8
15/09/23 17:47:39 INFO dstream.MapPartitionedDStream: Set context for org.apache.spark.streaming.dstream.MapPartitionedDStream@782047ca
15/09/23 17:47:39 INFO dstream.MappedDStream: Set context for org.apache.spark.streaming.dstream.MappedDStream@f495c63
15/09/23 17:47:39 INFO kafka.KafkaInputDStream: Set context for org.apache.spark.streaming.kafka.KafkaInputDStream@19506f6c
15/09/23 17:47:39 INFO dstream.ForEachDStream: Set context for org.apache.spark.streaming.dstream.ForEachDStream@3f9b5a7a
15/09/23 17:47:39 INFO dstream.MappedDStream: Set context for org.apache.spark.streaming.dstream.MappedDStream@70512035
15/09/23 17:47:39 INFO dstream.ShuffledDStream: Set context for org.apache.spark.streaming.dstream.ShuffledDStream@5e19b12e
15/09/23 17:47:39 INFO dstream.MappedDStream: Set context for org.apache.spark.streaming.dstream.MappedDStream@68bf7345
15/09/23 17:47:39 INFO dstream.StateDStream: Set context for org.apache.spark.streaming.dstream.StateDStream@475a1f66
15/09/23 17:47:39 INFO dstream.MappedDStream: Set context for org.apache.spark.streaming.dstream.MappedDStream@6dc38622
15/09/23 17:47:39 INFO dstream.FlatMappedDStream: Set context for org.apache.spark.streaming.dstream.FlatMappedDStream@12e78ac8
15/09/23 17:47:39 INFO dstream.MapPartitionedDStream: Set context for org.apache.spark.streaming.dstream.MapPartitionedDStream@782047ca
15/09/23 17:47:39 INFO dstream.MappedDStream: Set context for org.apache.spark.streaming.dstream.MappedDStream@f495c63
15/09/23 17:47:39 INFO kafka.KafkaInputDStream: Set context for org.apache.spark.streaming.kafka.KafkaInputDStream@19506f6c
15/09/23 17:47:39 INFO dstream.ForEachDStream: Set context for org.apache.spark.streaming.dstream.ForEachDStream@7128ecbc
15/09/23 17:47:39 INFO dstream.MappedDStream: Set context for org.apache.spark.streaming.dstream.MappedDStream@35ce64ae
15/09/23 17:47:39 INFO dstream.ShuffledDStream: Set context for org.apache.spark.streaming.dstream.ShuffledDStream@133f71b
15/09/23 17:47:39 INFO dstream.MappedDStream: Set context for org.apache.spark.streaming.dstream.MappedDStream@7aa40e3a
15/09/23 17:47:39 INFO dstream.StateDStream: Set context for org.apache.spark.streaming.dstream.StateDStream@28a240bd
15/09/23 17:47:39 INFO dstream.MappedDStream: Set context for org.apache.spark.streaming.dstream.MappedDStream@34484d22
15/09/23 17:47:39 INFO dstream.FlatMappedDStream: Set context for org.apache.spark.streaming.dstream.FlatMappedDStream@76c00dfe
15/09/23 17:47:39 INFO dstream.FlatMappedDStream: Set context for org.apache.spark.streaming.dstream.FlatMappedDStream@12e78ac8
15/09/23 17:47:39 INFO dstream.MapPartitionedDStream: Set context for org.apache.spark.streaming.dstream.MapPartitionedDStream@782047ca
15/09/23 17:47:39 INFO dstream.MappedDStream: Set context for org.apache.spark.streaming.dstream.MappedDStream@f495c63
15/09/23 17:47:39 INFO kafka.KafkaInputDStream: Set context for org.apache.spark.streaming.kafka.KafkaInputDStream@19506f6c
15/09/23 17:47:39 INFO dstream.ForEachDStream: Set context for org.apache.spark.streaming.dstream.ForEachDStream@3ad6db9a
15/09/23 17:47:39 INFO dstream.MappedDStream: Set context for org.apache.spark.streaming.dstream.MappedDStream@6feb79a3
15/09/23 17:47:39 INFO dstream.ShuffledDStream: Set context for org.apache.spark.streaming.dstream.ShuffledDStream@4c7ef5bc
15/09/23 17:47:39 INFO dstream.MappedDStream: Set context for org.apache.spark.streaming.dstream.MappedDStream@233730e0
15/09/23 17:47:39 INFO dstream.StateDStream: Set context for org.apache.spark.streaming.dstream.StateDStream@7c69d42f
15/09/23 17:47:39 INFO dstream.FlatMappedDStream: Set context for org.apache.spark.streaming.dstream.FlatMappedDStream@76c00dfe
15/09/23 17:47:39 INFO dstream.FlatMappedDStream: Set context for org.apache.spark.streaming.dstream.FlatMappedDStream@12e78ac8
15/09/23 17:47:39 INFO dstream.MapPartitionedDStream: Set context for org.apache.spark.streaming.dstream.MapPartitionedDStream@782047ca
15/09/23 17:47:39 INFO dstream.MappedDStream: Set context for org.apache.spark.streaming.dstream.MappedDStream@f495c63
15/09/23 17:47:39 INFO kafka.KafkaInputDStream: Set context for org.apache.spark.streaming.kafka.KafkaInputDStream@19506f6c
15/09/23 17:47:39 INFO dstream.ForEachDStream: Set context for org.apache.spark.streaming.dstream.ForEachDStream@7e99979
15/09/23 17:47:39 INFO dstream.MappedDStream: Set context for org.apache.spark.streaming.dstream.MappedDStream@6feb79a3
15/09/23 17:47:39 INFO dstream.ShuffledDStream: Set context for org.apache.spark.streaming.dstream.ShuffledDStream@4c7ef5bc
15/09/23 17:47:39 INFO dstream.MappedDStream: Set context for org.apache.spark.streaming.dstream.MappedDStream@233730e0
15/09/23 17:47:39 INFO dstream.StateDStream: Set context for org.apache.spark.streaming.dstream.StateDStream@7c69d42f
15/09/23 17:47:39 INFO dstream.FlatMappedDStream: Set context for org.apache.spark.streaming.dstream.FlatMappedDStream@76c00dfe
15/09/23 17:47:39 INFO dstream.FlatMappedDStream: Set context for org.apache.spark.streaming.dstream.FlatMappedDStream@12e78ac8
15/09/23 17:47:39 INFO dstream.MapPartitionedDStream: Set context for org.apache.spark.streaming.dstream.MapPartitionedDStream@782047ca
15/09/23 17:47:39 INFO dstream.MappedDStream: Set context for org.apache.spark.streaming.dstream.MappedDStream@f495c63
15/09/23 17:47:39 INFO kafka.KafkaInputDStream: Set context for org.apache.spark.streaming.kafka.KafkaInputDStream@19506f6c
15/09/23 17:47:39 INFO dstream.ForEachDStream: Set context for org.apache.spark.streaming.dstream.ForEachDStream@795adafd
15/09/23 17:47:39 INFO dstream.MappedDStream: Set context for org.apache.spark.streaming.dstream.MappedDStream@3bdf5649
15/09/23 17:47:39 INFO dstream.ShuffledDStream: Set context for org.apache.spark.streaming.dstream.ShuffledDStream@4399f154
15/09/23 17:47:39 INFO dstream.MappedDStream: Set context for org.apache.spark.streaming.dstream.MappedDStream@2fb2e478
15/09/23 17:47:39 INFO dstream.StateDStream: Set context for org.apache.spark.streaming.dstream.StateDStream@9b59abf
15/09/23 17:47:39 INFO dstream.MappedDStream: Set context for org.apache.spark.streaming.dstream.MappedDStream@71c27693
15/09/23 17:47:39 INFO dstream.FlatMappedDStream: Set context for org.apache.spark.streaming.dstream.FlatMappedDStream@15f6e73e
15/09/23 17:47:39 INFO dstream.FlatMappedDStream: Set context for org.apache.spark.streaming.dstream.FlatMappedDStream@12e78ac8
15/09/23 17:47:39 INFO dstream.MapPartitionedDStream: Set context for org.apache.spark.streaming.dstream.MapPartitionedDStream@782047ca
15/09/23 17:47:39 INFO dstream.MappedDStream: Set context for org.apache.spark.streaming.dstream.MappedDStream@f495c63
15/09/23 17:47:39 INFO kafka.KafkaInputDStream: Set context for org.apache.spark.streaming.kafka.KafkaInputDStream@19506f6c
15/09/23 17:47:39 INFO dstream.ForEachDStream: Set context for org.apache.spark.streaming.dstream.ForEachDStream@4c7a2b6
15/09/23 17:47:39 INFO dstream.MappedDStream: Set context for org.apache.spark.streaming.dstream.MappedDStream@4e8b5d2d
15/09/23 17:47:39 INFO dstream.ShuffledDStream: Set context for org.apache.spark.streaming.dstream.ShuffledDStream@1f925da4
15/09/23 17:47:39 INFO dstream.MappedDStream: Set context for org.apache.spark.streaming.dstream.MappedDStream@4241ca2d
15/09/23 17:47:39 INFO dstream.StateDStream: Set context for org.apache.spark.streaming.dstream.StateDStream@6d407256
15/09/23 17:47:39 INFO dstream.FlatMappedDStream: Set context for org.apache.spark.streaming.dstream.FlatMappedDStream@15f6e73e
15/09/23 17:47:39 INFO dstream.FlatMappedDStream: Set context for org.apache.spark.streaming.dstream.FlatMappedDStream@12e78ac8
15/09/23 17:47:39 INFO dstream.MapPartitionedDStream: Set context for org.apache.spark.streaming.dstream.MapPartitionedDStream@782047ca
15/09/23 17:47:39 INFO dstream.MappedDStream: Set context for org.apache.spark.streaming.dstream.MappedDStream@f495c63
15/09/23 17:47:39 INFO kafka.KafkaInputDStream: Set context for org.apache.spark.streaming.kafka.KafkaInputDStream@19506f6c
15/09/23 17:47:39 INFO dstream.ForEachDStream: Set context for org.apache.spark.streaming.dstream.ForEachDStream@2212a423
15/09/23 17:47:39 INFO dstream.MappedDStream: Set context for org.apache.spark.streaming.dstream.MappedDStream@4e8b5d2d
15/09/23 17:47:39 INFO dstream.ShuffledDStream: Set context for org.apache.spark.streaming.dstream.ShuffledDStream@1f925da4
15/09/23 17:47:39 INFO dstream.MappedDStream: Set context for org.apache.spark.streaming.dstream.MappedDStream@4241ca2d
15/09/23 17:47:39 INFO dstream.StateDStream: Set context for org.apache.spark.streaming.dstream.StateDStream@6d407256
15/09/23 17:47:39 INFO dstream.FlatMappedDStream: Set context for org.apache.spark.streaming.dstream.FlatMappedDStream@15f6e73e
15/09/23 17:47:39 INFO dstream.FlatMappedDStream: Set context for org.apache.spark.streaming.dstream.FlatMappedDStream@12e78ac8
15/09/23 17:47:39 INFO dstream.MapPartitionedDStream: Set context for org.apache.spark.streaming.dstream.MapPartitionedDStream@782047ca
15/09/23 17:47:39 INFO dstream.MappedDStream: Set context for org.apache.spark.streaming.dstream.MappedDStream@f495c63
15/09/23 17:47:39 INFO kafka.KafkaInputDStream: Set context for org.apache.spark.streaming.kafka.KafkaInputDStream@19506f6c
15/09/23 17:47:39 INFO dstream.ForEachDStream: Set context for org.apache.spark.streaming.dstream.ForEachDStream@75d6074e
15/09/23 17:47:39 INFO dstream.MappedDStream: Set context for org.apache.spark.streaming.dstream.MappedDStream@3679ce52
15/09/23 17:47:39 INFO dstream.ShuffledDStream: Set context for org.apache.spark.streaming.dstream.ShuffledDStream@76d8856e
15/09/23 17:47:39 INFO dstream.MappedDStream: Set context for org.apache.spark.streaming.dstream.MappedDStream@12035b7
15/09/23 17:47:39 INFO dstream.StateDStream: Set context for org.apache.spark.streaming.dstream.StateDStream@69a681f4
15/09/23 17:47:39 INFO dstream.MappedDStream: Set context for org.apache.spark.streaming.dstream.MappedDStream@329df05c
15/09/23 17:47:39 INFO dstream.FlatMappedDStream: Set context for org.apache.spark.streaming.dstream.FlatMappedDStream@1f0f41fa
15/09/23 17:47:39 INFO dstream.FlatMappedDStream: Set context for org.apache.spark.streaming.dstream.FlatMappedDStream@12e78ac8
15/09/23 17:47:39 INFO dstream.MapPartitionedDStream: Set context for org.apache.spark.streaming.dstream.MapPartitionedDStream@782047ca
15/09/23 17:47:39 INFO dstream.MappedDStream: Set context for org.apache.spark.streaming.dstream.MappedDStream@f495c63
15/09/23 17:47:39 INFO kafka.KafkaInputDStream: Set context for org.apache.spark.streaming.kafka.KafkaInputDStream@19506f6c
15/09/23 17:47:39 INFO dstream.ForEachDStream: Set context for org.apache.spark.streaming.dstream.ForEachDStream@22b49404
15/09/23 17:47:39 INFO dstream.MappedDStream: Set context for org.apache.spark.streaming.dstream.MappedDStream@7d61a468
15/09/23 17:47:39 INFO dstream.ShuffledDStream: Set context for org.apache.spark.streaming.dstream.ShuffledDStream@1570e827
15/09/23 17:47:39 INFO dstream.MappedDStream: Set context for org.apache.spark.streaming.dstream.MappedDStream@27996370
15/09/23 17:47:39 INFO dstream.StateDStream: Set context for org.apache.spark.streaming.dstream.StateDStream@474f625f
15/09/23 17:47:39 INFO dstream.FlatMappedDStream: Set context for org.apache.spark.streaming.dstream.FlatMappedDStream@1f0f41fa
15/09/23 17:47:39 INFO dstream.FlatMappedDStream: Set context for org.apache.spark.streaming.dstream.FlatMappedDStream@12e78ac8
15/09/23 17:47:39 INFO dstream.MapPartitionedDStream: Set context for org.apache.spark.streaming.dstream.MapPartitionedDStream@782047ca
15/09/23 17:47:39 INFO dstream.MappedDStream: Set context for org.apache.spark.streaming.dstream.MappedDStream@f495c63
15/09/23 17:47:39 INFO kafka.KafkaInputDStream: Set context for org.apache.spark.streaming.kafka.KafkaInputDStream@19506f6c
15/09/23 17:47:39 INFO dstream.ForEachDStream: Set context for org.apache.spark.streaming.dstream.ForEachDStream@2cc36f8c
15/09/23 17:47:39 INFO dstream.MappedDStream: Set context for org.apache.spark.streaming.dstream.MappedDStream@52d06749
15/09/23 17:47:39 INFO dstream.ShuffledDStream: Set context for org.apache.spark.streaming.dstream.ShuffledDStream@702d1418
15/09/23 17:47:39 INFO dstream.MappedDStream: Set context "
Bin Wang <wbin00@gmail.com>,"Thu, 24 Sep 2015 05:45:01 +0000",Get only updated RDDs from or after updateStateBykey,"""dev@spark.apache.org"" <dev@spark.apache.org>","I've read the source code and it seems to be impossible, but I'd like to
confirm it.

It is a very useful feature. For example, I need to store the state of
DStream into my database, in order to recovery them from next redeploy. But
I only need to save the updated ones. Save all keys into database is a lot
of waste.

Through the source code, I think it could be add easily: StateDStream can
get prevStateRDD so that it can make a diff. Is there any chance to add
this as an API of StateDStream? If so, I can work on this feature.

If not possible, is there any work around or hack to do this by myself?
"
Reynold Xin <rxin@databricks.com>,"Thu, 24 Sep 2015 00:27:25 -0700",[VOTE] Release Apache Spark 1.5.1 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version
1.5.1. The vote is open until Sun, Sep 27, 2015 at 10:00 UTC and passes if
a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.5.1
[ ] -1 Do not release this package because ...


The release fixes 81 known issues in Spark 1.5.0, listed here:
http://s.apache.org/spark-1.5.1

The tag to be voted on is v1.5.1-rc1:
https://github.com/apache/spark/commit/4df97937dbf68a9868de58408b9be0bf87dbbb94

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.5.1-rc1-bin/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release (1.5.1) can be found at:
*https://repository.apache.org/content/repositories/orgapachespark-1148/
<https://repository.apache.org/content/repositories/orgapachespark-1148/>*

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.5.1-rc1-docs/


=======================================
How can I help test this release?
=======================================
If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions.

================================================
What justifies a -1 vote for this release?
================================================
-1 vote should occur for regressions from Spark 1.5.0. Bugs already present
in 1.5.0 will not block this release.

===============================================================
What should happen to JIRA tickets still targeting 1.5.1?
===============================================================
Please target 1.5.2 or 1.6.0.
"
Shixiong Zhu <zsxwing@gmail.com>,"Thu, 24 Sep 2015 16:27:20 +0800",Re: Get only updated RDDs from or after updateStateBykey,Bin Wang <wbin00@gmail.com>,"For data that are not updated, where do you save? Or do you only want to
avoid accessing database for those that are not updated?

Besides,  the community is working on optimizing ""updateStateBykey""'s
performance. Hope it will be delivered soon.

Best Regards,
Shixiong Zhu

2015-09-24 13:45 GMT+08:00 Bin Wang <wbin00@gmail.com>:

"
Tathagata Das <tdas@databricks.com>,"Thu, 24 Sep 2015 01:33:32 -0700",Re: Checkpoint directory structure,Bin Wang <wbin00@gmail.com>,"Thanks for the log file. Unfortunately, this is insufficient as it does not
why the file does not exist. It could be that before failure somehow the
file was deleted. For that I need to see both the before failure and after
recovery logs. If this can be reproduced, could you generate the before and
after failure logs?


0b7d9f2/rdd-26909
0b7d9f2/rdd-26909
cala:45)
t.scala:1227)
t.scala:1227)
:147)
:108)
$1.apply(DStreamCheckpointData.scala:112)
$1.apply(DStreamCheckpointData.scala:109)
8)
8)
)
heckpointData.scala:109)
scala:487)
$2.apply(DStream.scala:488)
$2.apply(DStream.scala:488)
scala:488)
$2.apply(DStream.scala:488)
$2.apply(DStream.scala:488)
scala:488)
$2.apply(DStream.scala:488)
$2.apply(DStream.scala:488)
scala:488)
apply(DStreamGraph.scala:153)
apply(DStreamGraph.scala:153)
a:59)
h.scala:153)
:158)
StreamingContext.scala:837)
StreamingContext.scala:837)
.scala:837)
:57)
mpl.java:43)
ter.scala:525)
0b7d9f2/rdd-26909
ˆ24æ—¥å‘¨å›› ä¸Šåˆ9:45å†™é“ï¼š
er,
able
å‘¨ä¸‰ ä¸‹åˆ6:58å†™é“ï¼š
ms
e I
r?
"
Bin Wang <wbin00@gmail.com>,"Thu, 24 Sep 2015 09:26:49 +0000",Re: Get only updated RDDs from or after updateStateBykey,Shixiong Zhu <zsxwing@gmail.com>,"Data that are not updated should be saved earlier: while the data added to
the DStream at the first time, it should be considered as updated. So save
the same data again is a waste.

What are the community is doing? Is there any doc or discussion that I can
look for? Thanks.



Shixiong Zhu <zsxwing@gmail.com>äºŽ2015å¹´9æœˆ24æ—¥å‘¨å›› ä¸‹åˆ4:27å†™é“ï¼š

But
ot
n
"
Shixiong Zhu <zsxwing@gmail.com>,"Thu, 24 Sep 2015 17:37:03 +0800",Re: Get only updated RDDs from or after updateStateBykey,Bin Wang <wbin00@gmail.com>,"Could you write your update func like this?

    val updateFunc = (iterator: Iterator[(String, Seq[Int], Option[Int])])
=> {
      iterator.flatMap { case (key, values, stateOption) =>
        if (values.isEmpty) {
          // don't access database
        } else {
          // update to new state and save to database
        }
        // return new state
      }
    }

and use this overload:

def updateStateByKey[S: ClassTag](
      updateFunc: (Seq[V], Option[S]) => Option[S],
      partitioner: Partitioner
    ): DStream[(K, S)]

There is a JIRA: https://issues.apache.org/jira/browse/SPARK-2629 but
doesn't have a doc now...


Best Regards,
Shixiong Zhu

2015-09-24 17:26 GMT+08:00 Bin Wang <wbin00@gmail.com>:

o
e
n
¥å‘¨å›› ä¸‹åˆ4:27å†™é“ï¼š
o
 But
lot
 add
"
Bin Wang <wbin00@gmail.com>,"Thu, 24 Sep 2015 09:42:54 +0000",Re: Get only updated RDDs from or after updateStateBykey,Shixiong Zhu <zsxwing@gmail.com>,"It seems like a work around. But I don't know how to get the database
connection from the working nodes.

Shixiong Zhu <zsxwing@gmail.com>äºŽ2015å¹´9æœˆ24æ—¥å‘¨å›› ä¸‹åˆ5:37å†™é“ï¼š

)])
¥å‘¨å›› ä¸‹åˆ4:27å†™é“ï¼š
o
. But
 lot
o add
?
"
Shixiong Zhu <zsxwing@gmail.com>,"Thu, 24 Sep 2015 18:01:15 +0800",Re: Get only updated RDDs from or after updateStateBykey,Bin Wang <wbin00@gmail.com>,"You can create connection like this:

    val updateFunc = (iterator: Iterator[(String, Seq[Int], Option[Int])])
=> {
      val dbConnection = create a db connection
      iterator.flatMap { case (key, values, stateOption) =>
        if (values.isEmpty) {
          // don't access database
        } else {
          // update to new state and save to database
        }
        // return new state
      }
      TaskContext.get().addTaskCompletionListener(_ => db.disconnect())
    }


Best Regards,
Shixiong Zhu

2015-09-24 17:42 GMT+08:00 Bin Wang <wbin00@gmail.com>:

¥å‘¨å›› ä¸‹åˆ5:37å†™é“ï¼š
o
—¥å‘¨å›› ä¸‹åˆ4:27å†™é“ï¼š
f
y. But
a lot
to add
f?
"
Sean Owen <sowen@cloudera.com>,"Thu, 24 Sep 2015 14:44:14 +0100",Re: [VOTE] Release Apache Spark 1.5.1 (RC1),Reynold Xin <rxin@databricks.com>,"+1 non-binding. This is the first time I've seen all tests pass the
first time with Java 8 + Ubuntu + ""-Pyarn -Phadoop-2.6 -Phive
-Phive-thriftserver"". Clearly the test improvement efforts are paying
off.

As usual the license, sigs, etc are OK.


-------"
Bin Wang <wbin00@gmail.com>,"Thu, 24 Sep 2015 14:16:58 +0000",Re: Get only updated RDDs from or after updateStateBykey,Shixiong Zhu <zsxwing@gmail.com>,"Thanks, it seems good, though a little hack.

And here is another question. updateByKey compute on all the data from the
beginning, but in many situation, we just need to update the coming data.
This could be a big improve on speed and resource. Would this to be support
in the future?

Shixiong Zhu <zsxwing@gmail.com>äºŽ2015å¹´9æœˆ24æ—¥å‘¨å›› ä¸‹åˆ6:01å†™é“ï¼š

)])
¥å‘¨å›› ä¸‹åˆ5:37å†™é“ï¼š
d
So
—¥å‘¨å›› ä¸‹åˆ4:27å†™é“ï¼š
e
eploy.
e is a
m
 to add
"
shane knapp <sknapp@berkeley.edu>,"Thu, 24 Sep 2015 07:19:36 -0700","Re: JENKINS: downtime next week, wed and thurs mornings (9-23 and 9-24)","dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>, 
	Jon Kuroda <jkuroda@eecs.berkeley.edu>","this is happening now.


---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Thu, 24 Sep 2015 08:26:31 -0700","Re: JENKINS: downtime next week, wed and thurs mornings (9-23 and 9-24)","dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>, 
	Jon Kuroda <jkuroda@eecs.berkeley.edu>","...and we're finished and now building!


---------------------------------------------------------------------


"
Richard Hillegas <rhilleg@us.ibm.com>,"Thu, 24 Sep 2015 09:52:19 -0700",Re: [VOTE] Release Apache Spark 1.5.1 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","
-1 (non-binding)

I was able to build Spark cleanly from the source distribution using the
command in README.md:

    build/mvn -DskipTests clean package

However, while I was waiting for the build to complete, I started going
through the NOTICE file. I was confused about where to find licenses for
3rd party software bundled with Spark. About halfway through the NOTICE
file, starting with Java Collections Framework, there is a list of licenses
of the form

   license/*.txt

But there is no license subdirectory in the source distro. I couldn't find
the  *.txt license files for Java Collections Framework, Base64 Encoder, or
JZlib anywhere in the source distro. I couldn't find those files in license
subdirectories at the indicated home pages for those projects. (I did find
the license for JZLIB somewhere else, however:
http://www.jcraft.com/jzlib/LICENSE.txt.)

In addition, I couldn't find licenses for those projects in the master
LICENSE file.

Are users supposed to get licenses from the indicated 3rd party web sites?
Those online licenses could change. I would feel more comfortable if the
ASF were protected by our bundling the licenses inside our source distros.

After looking for those three licenses, I stopped reading the NOTICE file.
Maybe I'm confused about how to read the NOTICE file. Where should users
expect to find the 3rd party licenses?

Thanks,
-Rick

Reynold Xin <rxin@databricks.com> wrote on 09/24/2015 12:27:25 AM:

t:
/
8/
s/
================
================
=========================
=========================
========================================
========================================
"
Sean Owen <sowen@cloudera.com>,"Thu, 24 Sep 2015 18:00:52 +0100",Re: [VOTE] Release Apache Spark 1.5.1 (RC1),Richard Hillegas <rhilleg@us.ibm.com>,"Hi Richard, those are messages reproduced from other projects' NOTICE
files, not created by Spark. They need to be reproduced in Spark's
NOTICE file to comply with the license, but their text may or may not
apply to Spark's distribution. The intent is that users would track
this back to the source project if interested to investigate what the
upstream notice is about.

Requirements vary by license, but I do not believe there is additional
requirement to reproduce these other files. Their license information
is already indicated in accordance with the license terms.

What licenses are you looking for in LICENSE that you believe should be there?

Getting all this right is both difficult and important. I've made some
efforts over time to strictly comply with the Apache take on
licensing, which is at http://www.apache.org/legal/resolved.html  It's
entirely possible there's still a mistake somewhere in here (possibly
a new dependency, etc). Please point it out if you see such a thing.

But so far what you describe is ""working as intended"", as far as I
know, according to Apache.



---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Thu, 24 Sep 2015 10:24:25 -0700",Re: [VOTE] Release Apache Spark 1.5.1 (RC1),Sean Owen <sowen@cloudera.com>,"Hey Richard,

My assessment (just looked before I saw Sean's email) is the same as
his. The NOTICE file embeds other projects' licenses. If those
licenses themselves have pointers to other files or dependencies, we
don't embed them. I think this is standard practice.

- Patrick


---------------------------------------------------------------------


"
Richard Hillegas <rhilleg@us.ibm.com>,"Thu, 24 Sep 2015 10:45:51 -0700",Re: [VOTE] Release Apache Spark 1.5.1 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>,
        Sean Owen <sowen@cloudera.com>","
Hi Sean and Wendell,

I share your concerns about how difficult and important it is to get this
right. I think that the Spark community has compiled a very readable and
well organized NOTICE file. A lot of careful thought went into gathering
together 3rd party projects which share the same license text.

All I can offer is my own experience of having served as a release manager
for a sister Apache project (Derby) over the past ten years. The Derby
NOTICE file recites 3rd party licenses verbatim. This is also the approach
taken by the THIRDPARTYLICENSEREADME.txt in the JDK. I am not a lawyer.
However, I have great respect for the experience and legal sensitivities of
the people who compile that JDK license file.

Under your guidance, I would be happy to help compile a NOTICE file which
follows the pattern used by Derby and the JDK. This effort might proceed in
parallel with vetting 1.5.1 and could be targeted at a later release
vehicle. I don't think that the ASF's exposure is greatly increased by one
more release which follows the old pattern.

Another comment inline...

Patrick Wendell <pwendell@gmail.com> wrote on 09/24/2015 10:24:25 AM:


This may be where our perspectives diverge. I did not find those licenses
embedded in the NOTICE file. As I see it, the licenses are cited but not
included.

Thanks,
-Rick


e:
CE
ot

he
nal
on
ome
t's
ly
.
ng
the
going
E
file,
n't
find
Encoder, or
n
license
did
find
ster
b
sites?
f
the ASF
ros.
CE
file.

users

UTC
nd
at:
http://people.apache.org/~pwendell/spark-releases/spark-1.5.1-rc1-bin/
https://repository.apache.org/content/repositories/orgapachespark-1148/
http://people.apache.org/~pwendell/spark-releases/spark-1.5.1-rc1-docs/
=================
=================
ing

==========================
==========================
dy
=========================================
=========================================
--

"
Sean Owen <sowen@cloudera.com>,"Thu, 24 Sep 2015 18:51:00 +0100",Re: [VOTE] Release Apache Spark 1.5.1 (RC1),Richard Hillegas <rhilleg@us.ibm.com>,"
I'd prefer to use the ASF's preferred pattern, no? That's what we've
been trying to do and seems like we're even required to do so, not
follow a different convention. There is some specific guidance there
about what to add, and not add, to these files. Specifically, because
the AL2 requires downstream projects to embed the contents of NOTICE,
the guidance is to only include elements in NOTICE that must appear
there.

Put it this way -- what would you like to change specifically? (you
can start another thread for that)


Pretty sure that was meant to say that NOTICE embeds other projects'
""notices"", not licenses. And those notices can have all kinds of
stuff, including licenses.

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Thu, 24 Sep 2015 10:55:53 -0700","[Discuss] NOTICE file for transitive ""NOTICE""s",Sean Owen <sowen@cloudera.com>,"Richard,

Thanks for bringing this up and this is a great point. Let's start another
thread for it so we don't hijack the release thread.




"
Reynold Xin <rxin@databricks.com>,"Thu, 24 Sep 2015 10:56:10 -0700",Re: [VOTE] Release Apache Spark 1.5.1 (RC1),Sean Owen <sowen@cloudera.com>,"I forked a new thread for this. Please discuss NOTICE file related things
there so it doesn't hijack this thread.



"
Richard Hillegas <rhilleg@us.ibm.com>,"Thu, 24 Sep 2015 11:24:48 -0700","Re: [Discuss] NOTICE file for transitive ""NOTICE""s","""dev@spark.apache.org"" <dev@spark.apache.org>,
        Sean Owen <sowen@cloudera.com>","
Thanks for forking the new email thread, Reynold. It is entirely possible
that I am being overly skittish. I have posed a question for our legal
experts: https://issues.apache.org/jira/browse/LEGAL-226

To answer Sean's question on the previous email thread, I would propose
making changes like the following to the NOTICE file:

Replace a stanza like this...

""This product contains a modified version of 'JZlib', a re-implementation
of
zlib in pure Java, which can be obtained at:

  * LICENSE:
    * license/LICENSE.jzlib.txt (BSD Style License)
  * HOMEPAGE:
    * http://www.jcraft.com/jzlib/""

...with full license text like this

""This product contains a modified version of 'JZlib', a re-implementation
of
zlib in pure Java, which can be obtained at:

  * HOMEPAGE:
    * http://www.jcraft.com/jzlib/

The ZLIB license text follows:

JZlib 0.0.* were released under the GNU LGPL license.  Later, we have
switched
over to a BSD-style license.

------------------------------------------------------------------------------
Copyright (c) 2000-2011 ymnk, JCraft,Inc. All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are met:

  1. Redistributions of source code must retain the above copyright notice,
     this list of conditions and the following disclaimer.

  2. Redistributions in binary form must reproduce the above copyright
     notice, this list of conditions and the following disclaimer in
     the documentation and/or other materials provided with the
distribution.

  3. The names of the authors may not be used to endorse or promote
products
     derived from this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESSED OR IMPLIED
WARRANTIES,
INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY
AND
FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL JCRAFT,
INC. OR ANY CONTRIBUTORS TO THIS SOFTWARE BE LIABLE FOR ANY DIRECT,
INDIRECT,
INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
NOT
LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA,
OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
SOFTWARE,
EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.""

Thanks,
-Rick



Reynold Xin <rxin@databricks.com> wrote on 09/24/2015 10:55:53 AM:

e:

which
proceed in
e
 by
one


as
licenses
t
not

"
Hossein <falaki@gmail.com>,"Thu, 24 Sep 2015 11:36:51 -0700",Re: SparkR package path,"""Sun, Rui"" <rui.sun@intel.com>","Requiring users to download entire Spark distribution to connect to a
remote cluster (which is already running Spark) seems an over kill. Even
for most spark users who download Spark source, it is very unintuitive that
they need to run a script named ""install-dev.sh"" before they can run SparkR.

--Hossein


"
Sean Owen <sowen@cloudera.com>,"Thu, 24 Sep 2015 20:07:01 +0100","Re: [Discuss] NOTICE file for transitive ""NOTICE""s",Richard Hillegas <rhilleg@us.ibm.com>,"Have a look at http://www.apache.org/dev/licensing-howto.html#mod-notice
though, which makes a good point about limiting what goes into NOTICE
to what is required. That's what makes me think we shouldn't do this.


---------------------------------------------------------------------


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Thu, 24 Sep 2015 12:25:32 -0700",Re: SparkR package path,Hossein <falaki@gmail.com>,"I don't think the crux of the problem is about users who download the
source -- Spark's source distribution is clearly marked as something
that needs to be built and they can run `mvn -DskipTests -Psparkr
package` based on instructions in the Spark docs.

The crux of the problem is that with a source or binary R package, the
client side the SparkR code needs the Spark JARs to be available. So
we can't just connect to a remote Spark cluster using just the R
scripts as we need the Scala classes around to create a Spark context
etc.

But this is a use case that I've heard from a lot of users -- my take
is that this should be a separate package / layer on top of SparkR.
Dan Putler (cc'd) had a proposal on a client package for this and
maybe able to add more.

Thanks
Shivaram


---------------------------------------------------------------------


"
Richard Hillegas <rhilleg@us.ibm.com>,"Thu, 24 Sep 2015 12:36:42 -0700","Re: [Discuss] NOTICE file for transitive ""NOTICE""s","""dev@spark.apache.org"" <dev@spark.apache.org>","
Thanks for that pointer, Sean. It may be that Derby is putting the license
information in the wrong place, viz. in the NOTICE file. But the 3rd party
license text may need to go somewhere else. See for instance the advice a
little further up the page at
http://www.apache.org/dev/licensing-howto.html#permissive-deps

Thanks,
-Rick

Sean Owen <sowen@cloudera.com> wrote on 09/24/2015 12:07:01 PM:

ice


pose
"
Sean Owen <sowen@cloudera.com>,"Thu, 24 Sep 2015 20:40:12 +0100","Re: [Discuss] NOTICE file for transitive ""NOTICE""s",Richard Hillegas <rhilleg@us.ibm.com>,"Yes, the issue of where 3rd-party license information goes is
different, and varies by license. I think the BSD/MIT licenses are all
already listed in LICENSE accordingly. Let me know if you spy an
omission.


---------------------------------------------------------------------


"
Richard Hillegas <rhilleg@us.ibm.com>,"Thu, 24 Sep 2015 13:05:36 -0700","Re: [Discuss] NOTICE file for transitive ""NOTICE""s","""dev@spark.apache.org"" <dev@spark.apache.org>","
Hi Sean,

My reading would be that a separate copy of the BSD license, with copyright
years filled in, is required for each BSD-licensed dependency. Same for
MIT-licensed dependencies. Hopefully, we will receive some guidance on
https://issues.apache.org/jira/browse/LEGAL-226

Thanks,
-Rick



Sean Owen <sowen@cloudera.com> wrote on 09/24/2015 12:40:12 PM:

l
license
d
party
vice
a
http://www.apache.org/dev/licensing-howto.html#mod-notice
ICE
is.
com>
propose

"
Sean Owen <sowen@cloudera.com>,"Thu, 24 Sep 2015 21:11:33 +0100","Re: [Discuss] NOTICE file for transitive ""NOTICE""s",Richard Hillegas <rhilleg@us.ibm.com>,"Yes, but the ASF's reading seems to be clear:
http://www.apache.org/dev/licensing-howto.html#permissive-deps
""In LICENSE, add a pointer to the dependency's license within the
source tree and a short note summarizing its licensing:""

I'd be concerned if you get a different interpretation from the ASF. I
suppose it's OK to ask the question again, but for the moment I don't
see a reason to believe there's a problem.


---------------------------------------------------------------------


"
Hossein <falaki@gmail.com>,"Thu, 24 Sep 2015 14:09:26 -0700",Re: SparkR package path,shivaram@eecs.berkeley.edu,"Right now in sparkR.R the backend hostname is hard coded to ""localhost"" (
https://github.com/apache/spark/blob/master/R/pkg/R/sparkR.R#L156).

If we make that address configurable / parameterized, then a user can
connect a remote Spark cluster with no need to have spark jars on their
local machine. I have got this request from some R users. Their company has
a Spark cluster (usually managed by another team), and they want to connect
to it from their workstation (e.g., from within RStudio, etc).



--Hossein


"
Reynold Xin <rxin@databricks.com>,"Thu, 24 Sep 2015 14:54:06 -0700",Re: [VOTE] Release Apache Spark 1.5.1 (RC1),Sean Owen <sowen@cloudera.com>,"I'm going to +1 this myself. Tested on my laptop.




"
Xiangrui Meng <mengxr@gmail.com>,"Thu, 24 Sep 2015 15:10:22 -0700",Re: [VOTE] Release Apache Spark 1.5.1 (RC1),Reynold Xin <rxin@databricks.com>,"+1. Checked user guide and API doc, and ran some MLlib and SparkR
examples. -Xiangrui


---------------------------------------------------------------------


"
Hossein <falaki@gmail.com>,"Thu, 24 Sep 2015 16:02:36 -0700",Re: [VOTE] Release Apache Spark 1.5.1 (RC1),Xiangrui Meng <mengxr@gmail.com>,"+1 tested SparkR on Mac and Linux.

--Hossein


"
Luciano Resende <luckbr1975@gmail.com>,"Thu, 24 Sep 2015 17:24:48 -0700",Re: SparkR package path,Hossein <falaki@gmail.com>,"For host information, are you looking for something like this (which is
available today in Spark 1.5 already) ?

# Spark related configuration
Sys.setenv(""SPARK_MASTER_IP""=""127.0.0.1"")
Sys.setenv(""SPARK_LOCAL_IP""=""127.0.0.1"")

#Load libraries
library(""rJava"")
library(SparkR, lib.loc=""/...../spark-bin/R/lib"")

#Initalize  spark context
sc <- sparkR.init(sparkHome = ""/...../spark-bin"",
sparkPackages=""com.databricks:spark-csv_2.11:1.2.0"")






-- 
Luciano Resende
http://people.apache.org/~lresende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
Krishna Sankar <ksankar42@gmail.com>,"Thu, 24 Sep 2015 18:08:51 -0700",Re: [VOTE] Release Apache Spark 1.5.1 (RC1),Reynold Xin <rxin@databricks.com>,"+1 (non-binding, of course)

1. Compiled OSX 10.10 (Yosemite) OK Total time: 26:48 min
     mvn clean package -Pyarn -Phadoop-2.6 -DskipTests
2. Tested pyspark, mllib (iPython 4.0, FYI, notebook install is separate
â€œconda install pythonâ€ and then â€œc"
Reynold Xin <rxin@databricks.com>,"Thu, 24 Sep 2015 18:14:31 -0700",Re: [VOTE] Release Apache Spark 1.5.1 (RC1),Krishna Sankar <ksankar42@gmail.com>,"Krishna,

Thanks for testing every release!


:

pyterâ€)
"") OK
A
e
:
if
7dbbb94
===============
===============
========================
========================
=======================================
=======================================
"
"""Sun, Rui"" <rui.sun@intel.com>","Fri, 25 Sep 2015 01:49:40 +0000",RE: SparkR package path,Hossein <falaki@gmail.com>,"If  a user downloads Spark source, of course he needs to build it before running it. But a user can download pre-built Spark binary distributions, then he can directly use sparkR after deployment of the Spark cluster.

From: Hossein [mailto:falaki@gmail.com]
Sent: Friday, September 25, 2015 2:37 AM
To: Sun, Rui
Cc: shivaram@eecs.berkeley.edu; dev@spark.apache.org
Subject: Re: SparkR package path

Requiring users to download entire Spark distribution to connect to a remote cluster (which is already running Spark) seems an over kill. Even for most spark users who download Spark source, it is very unintuitive that they need to run a script named ""install-dev.sh"" before they can run SparkR.

--Hossein

On Wed, Sep 23, 2015 at 7:28 PM, Sun, Rui <rui.sun@intel.com<mailto:rui.sun@intel.com>> wrote:
SparkR package is not a standalone R package, as it is actually R API of Spark and needs to co-operate with a matching version of Spark, so exposing it in CRAN does not ease use of R users as they need to download matching Spark distribution, unless we expose a bundled SparkR package to CRAN (packageing with Spark), is this desirable? Actually, for normal users who are not developers, they are not required to download Spark source, build and install SparkR package. They just need to download a Spark distribution, and then use SparkR.

For using SparkR in Rstudio, there is a documentation at https://github.com/apache/spark/tree/master/R



From: Hossein [mailto:falaki@gmail.com<mailto:falaki@gmail.com>]
Sent: Thursday, September 24, 2015 1:42 AM
To: shivaram@eecs.berkeley.edu<mailto:shivaram@eecs.berkeley.edu>
Cc: Sun, Rui; dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: SparkR package path

Yes, I think exposing SparkR in CRAN can significantly expand the reach of both SparkR and Spark itself to a larger community of data scientists (and statisticians).

I have been getting questions on how to use SparkR in RStudio. Most of these folks have a Spark Cluster and wish to talk to it from RStudio. While that is a bigger task, for now, first step could be not requiring them to download Spark source and run a script that is named install-dev.sh. I filed SPARK-10776 to track this.


--Hossein

On Tue, Sep 22, 2015 at 7:21 PM, Shivaram Venkataraman <shivaram@eecs.berkeley.edu<mailto:shivaram@eecs.berkeley.edu>> wrote:
As Rui says it would be good to understand the use case we want to
support (supporting CRAN installs could be one for example). I don't
think it should be very hard to do as the RBackend itself doesn't use
the R source files. The RRDD does use it and the value comes from
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/api/r/RUtils.scala#L29
AFAIK -- So we could introduce a new config flag that can be used for
this new mode.

Thanks
Shivaram

On Mon, Sep 21, 2015 at 8:15 PMwrote:
> Hossein,
>
>
>
> Any strong reason to download and install SparkR source package separately
> from the Spark distribution?
>
> An R user can simply download the spark distribution, which contains SparkR
> source and binary package, and directly use sparkR. No need to install
> SparkR package at all.
>
>
>
> From: Hossein [mailto:falaki@gmail.com<mailto:falaki@gmail.com>]
> Sent: Tuesday, September 22, 2015 9:19 AM
> To: dev@spark.apache.org<mailto:dev@spark.apache.org>
> Subject: SparkR package path
>
>
>
> Hi dev list,
>
>
>
> SparkR backend assumes SparkR source files are located under
> ""SPARK_HOME/R/lib/."" This directory is created by running R/install-dev.sh.
> This setting makes sense for Spark developers, but if an R user downloads
> and installs SparkR source package, the source files are going to be in
> placed different locations.
>
>
>
> In the R runtime it is easy to find location of package files using
> path.package(""SparkR""). But we need to make some changes to R backend and/or
> spark-submit so that, JVM process learns the location of worker.R and
> daemon.R and shell.R from the R runtime.
>
>
>
> Do you think this change is feasible?
>
>
>
> Thanks,
>
> --Hossein


"
"""Sun, Rui"" <rui.sun@intel.com>","Fri, 25 Sep 2015 02:09:17 +0000",RE: SparkR package path,"Hossein <falaki@gmail.com>, ""shivaram@eecs.berkeley.edu""
	<shivaram@eecs.berkeley.edu>","Yes, the current implementation requires the backend to be on the same host as SparkR package. But this does not prevent SparkR from connecting to a remote Spark Cluster specified by a Spark master URL. The only thing needed is that there need be to a Spark JAR co-located with SparkR package on the same client machine. This is similar to any Spark application, which also depends on Spark JAR.

Theoritically, as SparkR package communicates with the backend via socket, the backend could be running on a different host. But this will make the launching of SparkR more complex, requiring not small change to spark-submit. Also additional network traffic overhead would be incurred.  I canâ€™t see any compelling demand for this.

From: Hossein [mailto:falaki@gmail.com]
Sent: Friday, September 25, 2015 5:09 AM
To: shivaram@eecs.berkeley.edu
Cc: Sun, Rui; dev@spark.apache.org; Dan Putler
Subject: Re: SparkR package path

Right now in sparkR.R the backend hostname is hard coded to ""localhost"" (https://github.com/apache/spark/blob/master/R/pkg/R/sparkR.R#L156).

If we make that address configurable / parameterized, then a user can connect a remote Spark cluster with no need to have spark jars on their local machine. I have got this request from some R users. Their company has a Spark cluster (usually managed by another team), and they want to connect to it from their workstation (e.g., from within RStudio, etc).



--Hossein

On Thu, Sep 24, 2015 at 12:25 PM, Shivaram Venkataraman <shivaram@eecs.berkeley.edu< the crux of the problem is about users who download the
source -- Spark's source distribution is clearly marked as something
that needs to be built and they can run `mvn -DskipTests -Psparkr
package` based on instructions in the Spark docs.

The crux of the problem is that with a source or binary R package, the
client side the SparkR code needs the Spark JARs to be available. So
we can't just connect to a remote Spark cluster using just the R
scripts as we need the Scala classes around to create a Spark context
etc.

But this is a use case that I've heard from a lot of users -- my take
is that this should be a separate package / layer on top of SparkR.
Dan Putler (cc'd) had a proposal on a client package for this and
maybe able to add more.

Thanks
Shivaram

On Thu, Sep 24, 2015 at 11:36 AM, Hossein <falaki@gmail.com<mailto:falaki@gmail.com>> wrote:
> Requiring users to download entire Spark distribution to connect to a remote
> cluster (which is already running Spark) seems an over kill. Even for most
> spark users who download Spark source, it is very unintuitive that they need
> to run a script named ""install-dev.sh"" before they can run SparkR.
>
> --Hossein
>
> On Wed, Sep 23, 2015 at 7:28 PM, Sun, Rui <rui.sun@intel.com<mailto:rui.sun@intel.com>> wrote:
>>
>> SparkR package is not a standalone R package, as it is actually R API of
>> Spark and needs to co-operate with a matching version of Spark, so exposing
>> it in CRAN does not ease use of R users as they need to download matching
>> Spark distribution, unless we expose a bundled SparkR package to CRAN
>> (packageing with Spark), is this desirable? Actually, for normal users who
>> are not developers, they are not required to download Spark source, build
>> and install SparkR package. They just need to download a Spark distribution,
>> and then use SparkR.
>>
>>
>>
>> For using SparkR in Rstudio, there is a documentation at
>> https://github.com/apache/spark/tree/master/R
>>
>>
>>
>>
>>
>>
>>
>> From: Hossein [mailto:falaki@gmail.com<mailto:falaki@gmail.com>]
>> Sent: Thursday, September 24, 2015 1:42 AM
>> To: shivaram@eecs.berkeley.edu<mailto:shivaram@eecs.berkeley.edu>
>> Cc: Sun, Rui; dev@spark.apache.org<mailto:dev@spark.apache.org>
>> Subject: Re: SparkR package path
>>
>>
>>
>> Yes, I think exposing SparkR in CRAN can significantly expand the reach of
>> both SparkR and Spark itself to a larger community of data scientists (and
>> statisticians).
>>
>>
>>
>> I have been getting questions on how to use SparkR in RStudio. Most of
>> these folks have a Spark Cluster and wish to talk to it from RStudio. While
>> that is a bigger task, for now, first step could be not requiring them to
>> download Spark source and run a script that is named install-dev.sh. I filed
>> SPARK-10776 to track this.
>>
>>
>>
>>
>> --Hossein
>>
>>
>>
>> On Tue, Sep 22, 2015 at 7:21 PM, Shivaram Venkataraman
>> <shivaram@eecs.berkeley.edu<mailto:shivaram@eecs.berkeley.edu>> wrote:
>>
>> As Rui says it would be good to understand the use case we want to
>> support (supporting CRAN installs could be one for example). I don't
>> think it should be very hard to do as the RBackend itself doesn't use
>> the R source files. The RRDD does use it and the value comes from
>>
>> https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/api/r/RUtils.scala#L29
>> AFAIK -- So we could introduce a new config flag that can be used for
>> this new mode.
>>
>> Thanks
>> Shivaram
>>
>>
>> On Mon, Sep 21, 2015 at 8:15 PM, Sun, Rui <rui.sun@intel.com<mailto:rui.sun@intel.com>> wrote:
>> > Hossein,
>> >
>> >
>> >
>> > Any strong reason to download and install SparkR source package
>> > separately
>> > from the Spark distribution?
>> >
>> > An R user can simply download the spark distribution, which contains
>> > SparkR
>> > source and binary package, and directly use sparkR. No need to install
>> > SparkR package at all.
>> >
>> >
>> >
>> > From: Hossein [mailto:falaki@gmail.com<mailto:falaki@gmail.com>]
>> > Sent: Tuesday, September 22, 2015 9:19 AM
>> > To: dev@spark.apache.org<mailto:dev@spark.apache.org>
>> > Subject: SparkR package path
>> >
>> >
>> >
>> > Hi dev list,
>> >
>> >
>> >
>> > SparkR backend assumes SparkR source files are located under
>> > ""SPARK_HOME/R/lib/."" This directory is created by running
>> > R/install-dev.sh.
>> > This setting makes sense for Spark developers, but if an R user
>> > downloads
>> > and installs SparkR source package, the source files are going to be in
>> > placed different locations.
>> >
>> >
>> >
>> > In the R runtime it is easy to find location of package files using
>> > path.package(""SparkR""). But we need to make some changes to R backend
>> > and/or
>> > spark-submit so that, JVM process learns the location of worker.R and
>> > daemon.R and shell.R from the R runtime.
>> >
>> >
>> >
>> > Do you think this change is feasible?
>> >
>> >
>> >
>> > Thanks,
>> >
>> > --Hossein
>>
>>
>
>

"
Fengdong Yu <fengdongy@everstring.com>,"Fri, 25 Sep 2015 11:12:00 +0800",How to get the HDFS path for each RDD,dev@spark.apache.org,"Hi,

I have  multiple files with JSON format, such as:

/data/test1_data/sub100/test.data
/data/test2_data/sub200/test.data


I can sc.textFile(â€œ/data/*/*â€)

but I want to add the {â€œsourceâ€ : â€œHDFS_LOCATIONâ€} to each line, then save it the one target HDFS location. 

how to do it, Thanks.






---------------------------------------------------------------------


"
Anchit Choudhry <anchit.choudhry@gmail.com>,"Thu, 24 Sep 2015 23:25:37 -0400",Re: How to get the HDFS path for each RDD,Fengdong Yu <fengdongy@everstring.com>,"Hi Fengdong,

Thanks for your question.

Spark already has a function called wholeTextFiles within sparkContext
which can help you with that:

Python

hdfs://a-hdfs-path/part-00000hdfs://a-hdfs-path/part-00001
...hdfs://a-hdfs-path/part-nnnnn

rdd = sparkContext.wholeTextFiles(â€œhdfs://a-hdfs-pathâ€)

(a-hdfs-path/part-00000, its content)
(a-hdfs-path/part-00001, its content)
...
(a-hdfs-path/part-nnnnn, its content)

More info: http://spark.apache.org/docs/latest/api/python/pyspark
.html?highlight=wholetext#pyspark.SparkContext.wholeTextFiles

------------

Scala

val rdd = sparkContext.wholeTextFile(""hdfs://a-hdfs-path"")

More info: https://spark.apache.org/docs/latest/api/scala/index.html#org.
apache.spark.SparkContext@wholeTextFiles(String,Int):RDD[(String,String)]

Let us know if this helps or you need more help.

Thanks,
Anchit Choudhry

:

â€} to each line, then save
"
Joseph Bradley <joseph@databricks.com>,"Thu, 24 Sep 2015 20:33:48 -0700",Re: [VOTE] Release Apache Spark 1.5.1 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1  Tested MLlib on Mac OS X


upyterâ€)
)
R
'"") OK
d
n
 if
87dbbb94
/
/>*
================
================
=========================
=========================
========================================
========================================
"
Fengdong Yu <fengdongy@everstring.com>,"Fri, 25 Sep 2015 11:44:46 +0800",Re: How to get the HDFS path for each RDD,Anchit Choudhry <anchit.choudhry@gmail.com>,"Hi Anchit, 

Thanks for the quick answer.

my exact question is : I want to add HDFS location into each line in my JSON  data.


which can help you with that:
)
<http://spark/>.apache.org/docs/latest/api/python/pyspark.html?highlight=wholetext#pyspark.SparkContext.wholeTextFiles
https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext@wholeTextFiles(String,Int):RDD[(String,String)]
â€} to each line, then save it the one target HDFS location.
<mailto:dev-unsubscribe@spark.apache.org>
<mailto:dev-help@spark.apache.org>

"
Anchit Choudhry <anchit.choudhry@gmail.com>,"Fri, 25 Sep 2015 03:52:25 +0000",Re: How to get the HDFS path for each RDD,Fengdong Yu <fengdongy@everstring.com>,"Sure. May I ask for a sample input(could be just few lines) and the output
you are expecting to bring clarity to my thoughts?


â€} to each line, then
"
Fengdong Yu <fengdongy@everstring.com>,"Fri, 25 Sep 2015 11:55:33 +0800",Re: How to get the HDFS path for each RDD,Anchit Choudhry <anchit.choudhry@gmail.com>,"
yes. such as I have two data sets:

date set A: /data/test1/dt=20100101
data set B: /data/test2/dt=20100202


all data has the same JSON format , such as:
{â€œkey1â€ : â€œvalue1â€, â€œkey2â€ : â€œvalue2â€ }


my output expected:
{â€œkey1â€ : â€œvalue1â€, â€œkey2â€ : â€œvalue2â€ , â€œsourceâ€ : â€œtest1â€, â€œdateâ€ : â€œ20100101""}
{â€œkey1â€ : â€œvalue1â€, â€œkey2â€ : â€œvalue2â€ , â€œsourceâ€ : â€œtest2â€, â€œdateâ€ : â€œ20100202""}


output you are expecting to bring clarity to my thoughts?
my JSON  data.
sparkContext which can help you with that:
<>hdfs-pathâ€)
<http://spark/>.apache.org/docs/latest/api/python/pyspark.html?highlight=wholetext#pyspark.SparkContext.wholeTextFiles
https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext@wholeTextFiles(String,Int):RDD[(String,String)]
â€œHDFS_LOCATIONâ€} to each line, then save it the one target HDFS location.
<mailto:dev-unsubscribe@spark.apache.org>
<mailto:dev-help@spark.apache.org>

"
Sean McNamara <Sean.McNamara@Webtrends.com>,"Fri, 25 Sep 2015 04:57:37 +0000",Re: [VOTE] Release Apache Spark 1.5.1 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","Ran tests + built/ran an internal spark streaming app /w 1.5.1 artifacts.

+1

Cheers,

Sean



Please vote on releasing the following candidate as Apache Spark version 1.5.1. The vote is open until Sun, Sep 27, 2015 at 10:00 UTC and passes if a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.5.1
[ ] -1 Do not release this package because ...


The release fixes 81 known issues in Spark 1.5.0, listed here:
http://s.apache.org/spark-1.5.1

The tag to be voted on is v1.5.1-rc1:
https://github.com/apache/spark/commit/4df97937dbf68a9868de58408b9be0bf87dbbb94

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.5.1-rc1-bin/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release (1.5.1) can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1148/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.5.1-rc1-docs/


=======================================
How can I help test this release?
=======================================
If you are a Spark user, you can help us test this release by taking an existing Spark workload and running on this release candidate, then reporting any regressions.

================================================
What justifies a -1 vote for this release?
================================================
-1 vote should occur for regressions from Spark 1.5.0. Bugs already present in 1.5.0 will not block this release.

===============================================================
What should happen to JIRA tickets still targeting 1.5.1?
===============================================================
Please target 1.5.2 or 1.6.0.




"
Anchit Choudhry <anchit.choudhry@gmail.com>,"Fri, 25 Sep 2015 01:12:43 -0400",Re: How to get the HDFS path for each RDD,Fengdong Yu <fengdongy@everstring.com>,"Hi Fengdong,

So I created two files in HDFS under a test folder.

test/dt=20100101.json
{ ""key1"" : ""value1"" }

test/dt=20100102.json
{ ""key2"" : ""value2"" }

Then inside PySpark shell

rdd = sc.wholeTextFiles('./test/*')
rdd.collect()
[(u'hdfs://localhost:9000/user/hduser/test/dt=20100101.json', u'{ ""key1"" :
""value1"" }), (u'hdfs://localhost:9000/user/hduser/test/dt=20100102.json',
u'{ ""key2"" : ""value2"" })]
import json
def editMe(y, x):
      j = json.loads(y)
      j['source'] = x
      return j

rdd.map(lambda (x,y): editMe(y,x)).collect()
[{'source': u'hdfs://localhost:9000/user/hduser/test/dt=20100101.json',
u'key1': u'value1'}, {u'key2': u'value2', 'source': u'hdfs://localhost
:9000/user/hduser/test/dt=20100102.json'}]

Similarly you could modify the function to return 'source' and 'date' with
some string manipulation per your requirements.

Let me know if this helps.

Thanks,
Anchit


:

 : â€œvalue2â€ }
 : â€œvalue2â€ , â€œsourceâ€ : â€œtest1â€, â€œdateâ€ :
 : â€œvalue2â€ , â€œsourceâ€ : â€œtest2â€, â€œdateâ€ :
t
)
.
]
Nâ€} to each line, then
"
Luciano Resende <luckbr1975@gmail.com>,"Thu, 24 Sep 2015 22:32:33 -0700",Re: [VOTE] Release Apache Spark 1.5.1 (RC1),Reynold Xin <rxin@databricks.com>,"+1 (non-binding)

Compiled in Mac OS with :
build/mvn -Pyarn,sparkr,hive,hive-thriftserver
-Phadoop-2.6 -Dhadoop.version=2.6.0 -DskipTests clean package

Checked around R
Looked into legal files

All looks good.





-- 
Luciano Resende
http://people.apac"
Sean Owen <sowen@cloudera.com>,"Fri, 25 Sep 2015 08:54:30 +0100","Re: [Discuss] NOTICE file for transitive ""NOTICE""s",Reynold Xin <rxin@databricks.com>,"Update: I *think* the conclusion was indeed that nothing needs to
happen with NOTICE.
However, along the way in
https://issues.apache.org/jira/browse/LEGAL-226 it emerged that the
BSD/MIT licenses should be inlined into LICENSE (or copied in the
distro somewhere). I can get on that -- just some grunt work to copy
and paste it all.


---------------------------------------------------------------------


"
Steve Loughran <stevel@hortonworks.com>,"Fri, 25 Sep 2015 09:06:39 +0000","Re: [Discuss] NOTICE file for transitive ""NOTICE""s",Sean Owen <sowen@cloudera.com>,"

Having looked at the notice, I actually see a lot more thorough that most ASF projects.

in contrast, here is the hadoop one: 

---
This product includes software developed by The Apache Software
Foundation (http://www.apache.org/).
---

regarding the spark one, I don't see that you need to refer to transitive dependencies for the non-binary distros, and, for any binaries, to bother listing the licensing of all the ASF dependencies. Things pulled in from elsewhere & pasted in, that's slightly more complex. I've just been dealing with the issue of taking an openstack-applied patch to the hadoop swift object store code -and, because the licenses are compatible, we're just going to stick it in as-is.

Uber-JARs, such as spark.jar, do contain lots of classes from everywhere. I don't know the status of them. You could probably get maven to work out the licensing if all the dependencies declare their license.

b/*.jar to the CP would allow codahale's ganglia support to come in just by dropping in the relevant LGPL JAR, avoiding the need to build a custom spark JAR tainted by the transitive dependency.

-Steve

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Fri, 25 Sep 2015 10:18:46 +0100","Re: [Discuss] NOTICE file for transitive ""NOTICE""s",Steve Loughran <stevel@hortonworks.com>,"rote:
 dependencies for the non-binary distros, and, for any binaries, to bother listing the licensing of all the ASF dependencies. Things pulled in from elsewhere & pasted in, that's slightly more complex.

The requirements for including source can be different. There's not
much of it. There's not really a ""transitive dependency"" for source,
as it is self-contained if copied into the project. I think the source
stuff is dealt with correctly in LICENSE.

Yes you also don't end up needing to repeat the licensing for ASF
dependencies. The issue is BSD/MIT here as far as I can tell
(so-called permissive licenses).


 I don't know the status of them. You could probably get maven to work out the licensing if all the dependencies declare their license.

Indeed, that's exactly why we have to deal with license stuff since
Spark does redistribute other code (not just depend on it). And yes,
using Maven to dig out this info is just what I have done :)

It's not that we missed dependencies, and it's not an issue of NOTICE,
but rather BSD/MIT licenses in LICENSE. The net-net is: inline them.


lib/*.jar to the CP would allow codahale's ganglia support to come in just by dropping in the relevant LGPL JAR, avoiding the need to build a custom spark JAR tainted by the transitive dependency.

(We still couldn't distribute the LGPL bits in Spark, but I don't
think you're suggesting that)

---------------------------------------------------------------------


"
Akhil Das <akhil@sigmoidanalytics.com>,"Fri, 25 Sep 2015 18:06:10 +0530",Re: unsubscribe,Nirmal R Kumar <nimmoadam@hotmail.com>,"Send an email to dev-unsubscribe@spark.apache.org instead of
dev@spark.apache.org

Thanks
Best Regards


"
Doug Balog <doug.sparkdev@dugos.com>,"Fri, 25 Sep 2015 08:55:59 -0400",Re: [VOTE] Release Apache Spark 1.5.1 (RC1),Reynold Xin <rxin@databricks.com>,"+1 (non-binding)

Tested on secure YARN cluster with HIVE.

Notes:  SPARK-10422, SPARK-10737 were causing us problems with 1.5.0. We see 1.5.1 as a big improvement. 

Cheers,

Doug


version 1.5.1. The vote is open until Sun, Sep 27, 2015 at 10:00 UTC and"
Eugene Zhulenev <eugene.zhulenev@gmail.com>,"Fri, 25 Sep 2015 08:58:03 -0400",Re: [VOTE] Release Apache Spark 1.5.1 (RC1),dev@spark.apache.org,"+1

Running latest build from 1.5 branch, SO much more stable than 1.5.0
release.


"
Sean Owen <sowen@cloudera.com>,"Fri, 25 Sep 2015 14:35:46 +0100","Re: [Discuss] NOTICE file for transitive ""NOTICE""s","Reynold Xin <rxin@databricks.com>, Richard Hillegas <rhilleg@us.ibm.com>","Work underway at ...

https://issues.apache.org/jira/browse/SPARK-10833
https://github.com/apache/spark/pull/8919




---------------------------------------------------------------------


"
vaquar khan <vaquar.khan@gmail.com>,"Fri, 25 Sep 2015 22:14:58 +0530",Re: [VOTE] Release Apache Spark 1.5.1 (RC1),Eugene Zhulenev <eugene.zhulenev@gmail.com>,"+1 (non-binding)

Regards,
Vaquar khan

"
Marcelo Vanzin <vanzin@cloudera.com>,"Fri, 25 Sep 2015 11:11:35 -0700",Re: RFC: packaging Spark without assemblies,Patrick Wendell <pwendell@gmail.com>,"
Thanks Patrick (and all the others) who commented on the document.

For BC, I think there are two main cases:

- People who ship the assembly with their application. As Matei
suggested (and I agree), that is kinda weird. But currently that is
the easiest way to embed Spark and get, for example, the YARN backend
working. There are ways around that but they are tricky. The code
changes I propose would make that much easier to do without the need
for an assembly.

- People who somehow depend on the layout of the Spark distribution.
Meaning they expect a ""lib/"" directory with an assembly in there
matching a specific file name pattern. Although I kinda consider that
to be an invalid use case (as in ""you're doing it wrong"").

unnecessary, but not get rid of them, at least at first. Maybe a build
profile or an argument in make-distribution.sh to enable or disable
them as desired.

-- 
Marcelo

---------------------------------------------------------------------


"
Tom Graves <tgraves_cs@yahoo.com.INVALID>,"Fri, 25 Sep 2015 18:09:55 +0000 (UTC)",Re: [VOTE] Release Apache Spark 1.5.1 (RC1),"Reynold Xin <rxin@databricks.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","+1. Tested Spark on Yarn on Hadoop 2.6 and 2.7.
Tom 


   

 Please vote on releasing the following candidate as Apache Spark version 1.5.1. The vote is open until Sun, Sep 27, 2015 at 10:00 UTC and passes if a majority of at least 3 +1 PMC votes are cast"
Marcelo Vanzin <vanzin@cloudera.com>,"Fri, 25 Sep 2015 11:32:22 -0700",Re: [VOTE] Release Apache Spark 1.5.1 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","Mostly for my education (I hope), but I was testing
""spark-1.5.1-bin-without-hadoop.tgz"" assuming it would contain
everything (including HiveContext support), just without the Hadoop
common jars in the assembly. But HiveContext is not there.

Is this expected?




-- 
Marcelo

---------------------------------------------------------------------


"
Marcelo Vanzin <vanzin@cloudera.com>,"Fri, 25 Sep 2015 13:27:01 -0700",Re: [VOTE] Release Apache Spark 1.5.1 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","Ignoring my previous question, +1. Tested several different jobs on
YARN and standalone with dynamic allocation on.




-- 
Marcelo

---------------------------------------------------------------------


"
Russell Spitzer <russell.spitzer@gmail.com>,"Sat, 26 Sep 2015 05:02:00 +0000",Dataframes: PrunedFilteredScan without Spark Side Filtering,dev@spark.apache.org,"Hi! First time poster, long time reader.

I'm wondering if there is a way to let cataylst know that it doesn't need
to repeat a filter on the spark side after a filter has been applied by the
Source Implementing PrunedFilterScan.


This is for a usecase in which we except a filter on a non-existant column
that serves as an entry point for our integration with a different system.
While the source can correctly deal with this, the secondary filter done on
the RDD itself wipes out the results because the column being filtered does
not exist.

In particular this is with our integration with Solr where we allow users
to pass in a predicate based on ""solr_query"" ala (""where solr_query='*:*')
there is no column ""solr_query"" so the rdd.filter( row.solr_query == ""*:*')
filters out all of the data since no row's will have that column.

I'm thinking about a few solutions to this but they all seem a little hacky
1) Try to manually remove the filter step from the query plan after our
source handles the filter
2) Populate the solr_query field being returned so they all automatically
pass

But I think the real solution is to add a way to create a PrunedFilterScan
which does not reapply filters if the source doesn't want it to. IE Giving
PrunedFilterScan the ability to trust the underlying source that the filter
will be accurately applied. Maybe changing the api to

PrunedFilterScan(requiredColumns: Array[String], filters: Array[Filter],
reapply: Boolean = true)

Where Catalyst can check the Reapply value and not add an RDD.filter if it
is false.

Thoughts?

Thanks for your time,
Russ
"
Dibyendu Bhattacharya <dibyendu.bhattachary@gmail.com>,"Sat, 26 Sep 2015 12:19:25 +0530","Re: Spark Streaming with Tachyon : Data Loss on Receiver Failure due
 to WAL error",N B <nb.nospam@gmail.com>,"Hi,

Recently I was working on a PR to use Tachyon as OFF_HEAP store for Spark
Streaming and make sure Spark Streaming can recover from Driver failure and
recover the blocks form Tachyon.

The The Motivation for this PR is  :

If Streaming application stores the blocks OFF_HEAP, it may not need any
WAL like feature to recover from Driver failure. As long as the writing of
blocks to Tachyon from Streaming receiver is durable, it should be
recoverable from Tachyon directly on Driver failure.
This can solve the issue of expensive WAL write and duplicating the blocks
both in MEMORY and also WAL and also guarantee end to end No-Data-Loss
channel using OFF_HEAP store.

https://github.com/apache/spark/pull/8817

This PR still under review . But having done various fail over testing in
my environment , I see this PR worked perfectly fine without any data loss
. Let see what TD and other have to say on this PR .

Below is the configuration I used to test this PR ..


Spark : 1.6 from Master
Tachyon : 0.7.1

SparkConfiguration Details :

SparkConf conf = new SparkConf().setAppName(""TestTachyon"")
.set(""spark.streaming.unpersist"", ""true"")
.set(""spark.local.dir"", ""/mnt1/spark/tincan"")
.set(""tachyon.zookeeper.address"",""10.252.5.113:2182"")
.set(""tachyon.usezookeeper"",""true"")
.set(""spark.externalBlockStore.url"", ""tachyon-ft://
ip-10-252-5-113.asskickery.us:19998"")
        .set(""spark.externalBlockStore.baseDir"", ""/sparkstreaming"")
        .set(""spark.externalBlockStore.folderName"",""pearson"")
        .set(""spark.externalBlockStore.dirId"", ""subpub"")


JavaStreamingContext jsc = new JavaStreamingContext(conf, new Duration(
10000));

String checkpointDirectory = ""hdfs://10.252.5.113:9000/user/hadoop/spark/wal
"";

jsc.checkpoint(checkpointDirectory);


//I am using the My Receiver Based Consumer (
https://github.com/dibbhatt/kafka-spark-consumer) . But
KafkaUtil.CreateStream will also work

JavaDStream<MessageAndMetadata> unionStreams = ReceiverLauncher.launch(
jsc, props, numberOfReceivers, StorageLevel.OFF_HEAP());




Regards,
Dibyendu


"
Fengdong Yu <fengdongy@everstring.com>,"Sat, 26 Sep 2015 17:27:59 +0800",Re: How to get the HDFS path for each RDD,Anchit Choudhry <anchit.choudhry@gmail.com>,"Hi Anchit,

this is not my expected, because you specified the HDFS directory in your code.
I've solved like this:

       val text = sc.hadoopFile(Args.input,
                   classOf[TextInputFormat], classOf[LongWritable], classOf[Text], 2)
        val hadoopRdd = text.asInstanceOf[HadoopRDD[LongWritable, Text]]

          hadoopRdd.mapPartitionsWithInputSplit((inputSplit, iterator) => {
              val file = inputSplit.asInstanceOf[FileSplit]
              terator.map ( tp => {tp._1, new Text(file.toString + â€œ,â€ + tp._2.toString)})
          }




""key1"" : ""value1"" }), (u'hdfs://localhost:9000/user/hduser/test/dt=20100102.json', u'{ ""key2"" : ""value2"" })]
u'hdfs://localhost:9000/user/hduser/test/dt=20100101.json', u'key1': u'value1'}, {u'key2': u'value2', 'source': u'hdfs://localhost:9000/user/hduser/test/dt=20100102.json'}]
with some string manipulation per your requirements.
 : â€œvalue2â€ }
 : â€œvalue2â€ , â€œsourceâ€ : â€œtest1â€, â€œdateâ€ : â€œ20100101""}
 : â€œvalue2â€ , â€œsourceâ€ : â€œtest2â€, â€œdateâ€ : â€œ20100202""}
output you are expecting to bring clarity to my thoughts?
my JSON  data.
sparkContext which can help you with that:
<>hdfs-pathâ€)
<http://spark/>.apache.org/docs/latest/api/python/pyspark.html?highlight=wholetext#pyspark.SparkContext.wholeTextFiles
https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext@wholeTextFiles(String,Int):RDD[(String,String)]
â€œHDFS_LOCATIONâ€} to each line, then save it the one target HDFS location.
---------------------------------------------------------------------
<mailto:dev-unsubscribe@spark.apache.org>
<mailto:dev-help@spark.apache.org>

"
Fengdong Yu <fengdongy@everstring.com>,"Sat, 26 Sep 2015 18:00:11 +0800",Re: How to get the HDFS path for each RDD,Anchit Choudhry <anchit.choudhry@gmail.com>,"Anchit,

please ignore my inputs. you are right. Thanks.



your code.
classOf[Text], 2)
Text]]
=> {
â€œ,â€ + tp._2.toString)})
""key1"" : ""value1"" }), (u'hdfs://localhost:9000/user/hduser/test/dt=20100102.json', u'{ ""key2"" : ""value2"" })]
u'hdfs://localhost:9000/user/hduser/test/dt=20100101.json', u'key1': u'value1'}, {u'key2': u'value2', 'source': u'hdfs://localhost:9000/user/hduser/test/dt=20100102.json'}]
with some string manipulation per your requirements.
 : â€œvalue2â€ }
 : â€œvalue2â€ , â€œsourceâ€ : â€œtest1â€, â€œdateâ€ : â€œ20100101""}
 : â€œvalue2â€ , â€œsourceâ€ : â€œtest2â€, â€œdateâ€ : â€œ20100202""}
output you are expecting to bring clarity to my thoughts?
my JSON  data.
sparkContext which can help you with that:
<>hdfs-pathâ€)
<http://spark/>.apache.org/docs/latest/api/python/pyspark.html?highlight=wholetext#pyspark.SparkContext.wholeTextFiles
https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext@wholeTextFiles(String,Int):RDD[(String,String)]
â€œHDFS_LOCATIONâ€} to each line, then save it the one target HDFS location.
---------------------------------------------------------------------
<mailto:dev-unsubscribe@spark.apache.org>
<mailto:dev-help@spark.apache.org>

"
Steve Loughran <stevel@hortonworks.com>,"Sat, 26 Sep 2015 10:18:58 +0000",Re: RFC: packaging Spark without assemblies,Marcelo Vanzin <vanzin@cloudera.com>,"

not wierd if you are bypassing bin/spark



well, spark-submit and spark-example shells do something close to this, though primarly as error checking against >1 artifact and classpath confusion

---------------------------------------------------------------------


"
Olivier Girardot <ssaboum@gmail.com>,"Sat, 26 Sep 2015 15:44:03 +0200",Re: ClassCastException using DataFrame only when num-executors > 2 ...,Reynold Xin <rxin@databricks.com>,"sorry for the delay, yes still.
I'm still trying to figure out if it comes from bad data and trying to
isolate the bug itself...

2015-09-11 0:28 GMT+02:00 Reynold Xin <rxin@databricks.com>:

etLong(rows.scala:41)
s.scala:220)
ala:85)
Projection.apply(Unknown
la:325)
la:252)
c1
heduler.scala:1267)
Scheduler.scala:1255)
Scheduler.scala:1254)
ala:59)
254)
apply(DAGScheduler.scala:684)
apply(DAGScheduler.scala:684)
r.scala:684)
Scheduler.scala:1480)
heduler.scala:1442)
heduler.scala:1431)
4)
la:147)
la:108)
ange.scala:156)
ange.scala:141)
48)
getLong(rows.scala:41)
ws.scala:220)
cala:85)
eProjection.apply(Unknown
ala:325)
ala:252)
indow.scala:265)
scala:272)
)
)
ly(RDD.scala:706)
ly(RDD.scala:706)
)
)
)
)
)
)
)
)
)
)
)
)
)
)
)
)
)
)
)
)
)
)
)
)
)
onsWithPreparationRDD.scala:46)
onsWithPreparationRDD.scala:46)
)
)
cala:88)
)
)
onsWithPreparationRDD.scala:46)
)
73)
41)
)
va:1145)
ava:615)
t)
ala:325)
"
Mike Hynes <91mbbh@gmail.com>,"Sat, 26 Sep 2015 12:27:52 -0400",Re: RDD API patterns,dev <dev@spark.apache.org>,"Hello Devs,

This email concerns some timing results for a treeAggregate in
computing a (stochastic) gradient over an RDD of labelled points, as
is currently done in the MLlib optimization routine for SGD.

In SGD, the underlying RDD is downsampled by a fraction f \in (0,1],
and the subgradients over all the instances in the downsampled RDD are
aggregated to the driver as a dense vector. However, we have noticed
some unusual behaviour when f < 1: it takes the same amount of time to
compute the stochastic gradient for a stochastic minibatch as it does
for a full batch (f = 1).

Attached are two plots of the mean task timing metrics for each level
in the aggregation, which has been performed with 4 levels (level 4 is
the final level, in which the results are communicated to the driver).
16 nodes are used, and the RDD has 256 partitions. We run in (client)
standalone mode. Here, the total time for the tasks is shown (\tau)
alongside the execution time (not counting GC),
serialization/deserialization time, the GC time, and the difference
between tau and all other times, assumed to be variable
IO/communication/waiting time. The RDD in this case is a labelled
point representation of the KDD Bridge to Algebra dataset, with 20M
(sparse) instances and a problem dimension of 30M. The sparsity of the
instances is very high---each individual instance vector may have only
a hundred nonzeros. All metrics have been taken from the JSON Spark
event logs.

The plot gradient_f1.pdf shows the times for a gradient computation
with f = 1, and gradient_f-3.pdf shows the same metrics with f = 1e-3.
For other f values in {1e-1 1e-2 ... 1e-5}, the same effect is
observed.

What I would like to mention about these plots, and ask if anyone has
experience with, is the following:
1. The times are essentially identical; I would have thought that
downsampling the RDD before aggregating the subgradients would at
least reduce the execution time required, if not the
communication/serialization times.
2. The serialization time in level 4 is almost entirely from the
result serialization to the driver, and not the task deserialization.
In each level of the treeAggregation, however, the local (dense)
gradients have to be communicated between compute nodes, so I am
surprised that it takes so much longer to return the vectors to the
driver.

I initially wondered if the large IO overhead in the last stage had
anything to do with client mode vs cluster mode, since, from what I
understand, only a single core is allocated to the driver thread in
client mode. However, when running tests in the two modes, I have
previously seen no appreciable difference in the running time for
other (admittedly smaller) problems. Furthermore, I am still very
confused about why the execution time for each task is just as large
for the downsampled RDD. It seems unlikely that sampling each
partition would be as expensive as the gradient computations, even for
sparse feature vectors.

If anyone has experience working with the sampling in minibatch SGD or
has tested the scalability of the treeAggregation operation for
vectors, I'd really appreciate your thoughts.

Thanks,
Mike

---------------------------------------------------------------------"
"""Evan R. Sparks"" <evan.sparks@gmail.com>","Sat, 26 Sep 2015 10:07:39 -0700",Re: RDD API patterns,Mike Hynes <91mbbh@gmail.com>,"Mike,

I believe the reason you're seeing near identical performance on the
gradient computations is twofold
1) Gradient computations for GLM models are computationally pretty cheap
from a FLOPs/byte read perspective. They are essentially a BLAS ""gemv"" call
in the dense case, which is well known to be bound by memory bandwidth on
modern processors. So, you're basically paying the cost of a scan of the
points you've sampled to do the gradient computation.
2) The default sampling mechanism used by the GradientDescent optimizer in
MLlib is implemented via RDD.sample, which does reservoir sampling on each
partition. This requires a full scan of each partition at every iteration
to collect the samples.

So - you're going to pay the cost of a scan to do the sampling anyway, and
the gradient computation is essentially free at this point (and can be
pipelined, etc.).

It is quite possible to improve #2 by coming up with a better sampling
randomly shuffled (or do that once) and then use the first
miniBatchFraction*partitionSize records on the first iteration, the second
set on the second set on the second iteration, and so on. You could
protoype this algorithm pretty easily by converting your data to an
RDD[Array[DenseVector]] and doing some bookkeeping at each iteration.

That said - eventually the overheads of the platform catch up to you. As a
rule of thumb I estimate about 50ms/iteration as a floor for things like
task serialization and other platform overheads. You've got to balance how
much computation you want to do vs. the amount of time you want to spend
waiting for the platform.

- Evan


"
Mike Hynes <91mbbh@gmail.com>,"Sat, 26 Sep 2015 13:20:31 -0400",treeAggregate timing / SGD performance with miniBatchFraction < 1,"""Evan R. Sparks"" <evan.sparks@gmail.com>","Hi Evan,

(I just realized my initial email was a reply to the wrong thread; I'm
very sorry about this).

Thanks for your email, and your thoughts on the sampling. That the
gradient computations are essentially the cost of a pass through each
element of the partition makes sense, especially given the sparsity of
the feature vectors.

Would you have any idea why the communication time is so much larger
in the final level of the aggregation, however? I can't immediately
see why it should take longer to transfer the local gradient vectors
in that level, since they are dense in every level. Furthermore, the
driver is receiving the result of only 4 tasks, which is relatively
small.

Mike




-- 
Thanks,
Mike

---------------------------------------------------------------------


"
"""Evan R. Sparks"" <evan.sparks@gmail.com>","Sat, 26 Sep 2015 10:45:55 -0700",Re: treeAggregate timing / SGD performance with miniBatchFraction < 1,Mike Hynes <91mbbh@gmail.com>,"Off the top of my head, I'm not sure, but it looks like virtually all the
extra time between each stage is accounted for with T_{io} in your plot,
which I'm guessing is time spent communicating results over the network? Is
your driver running on the master or is it on a different node? If you look
at the code for treeAggregate, the last stage uses a .reduce() for the
final combination, which happens on the driver. In this case, the size of
the gradients is O(1GB) so if you've got to go over a slow link for the
last portion this could really make a difference.


"
Mike Hynes <91mbbh@gmail.com>,"Sat, 26 Sep 2015 15:55:09 -0400",Re: treeAggregate timing / SGD performance with miniBatchFraction < 1,"""Evan R. Sparks"" <evan.sparks@gmail.com>","That is an interesting point; I run the driver as a background process
on the master node so that I can still pipe the stdout/stderr
filestreams to the (network) filesystem.
I should mention that the master is connected to the slaves with a 10
Gb link on the same managed switch that the slaves use.



-- 
Thanks,
Mike

---------------------------------------------------------------------


"
robineast <robin.east@xense.co.uk>,"Sat, 26 Sep 2015 13:36:30 -0700 (MST)",Re: [VOTE] Release Apache Spark 1.5.1 (RC1),dev@spark.apache.org,"+1


build/mvn clean package -DskipTests -Pyarn -Phadoop-2.6
OK
Basic graph tests
  Load graph using edgeListFile...SUCCESS
  Run PageRank...SUCCESS
Minimum Spanning Tree Algorithm
  Run basic Minimum Spanning Tree algorithm...SUCCESS
  Run Minimum Spanni"
Ratika Prasad <rprasad@couponsinc.com>,"Sun, 27 Sep 2015 04:50:22 +0000",Spark-Kafka Connector issue,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi All,

I am trying out the spark streaming and reading the messages from kafka topics which later would be created into streams as below...I have the kafka setup on a vm and topics created however when I try to run the program below from my spark vm as below I get an error even though the kafka server and zookeeper are up and running

./bin/spark-submit --class org.stream.processing.JavaKafkaStreamEventProcessing --master local spark-stream-processing-0.0.1-SNAPSHOT-jar-with-dependencies.jar 172.28.161.32:2181 redemption_inbound

Exception in thread ""main"" org.apache.spark.SparkException: java.io.EOFException: Received -1 when reading from channel, socket has likely been closed.
        at org.apache.spark.streaming.kafka.KafkaCluster$$anonfun$checkErrors$1.apply(KafkaCluster.scala:366)
        at org.apache.spark.streaming.kafka.KafkaCluster$$anonfun$checkErrors$1.apply(KafkaCluster.scala:366)
        at scala.util.Either.fold(Either.scala:97)
        at org.apache.spark.streaming.kafka.KafkaCluster$.checkErrors(KafkaCluster.scala:365)
        at org.apache.spark.streaming.kafka.KafkaUtils$.createDirectStream(KafkaUtils.scala:422)
        at org.apache.spark.streaming.kafka.KafkaUtils$.createDirectStream(KafkaUtils.scala:532)
        at org.apache.spark.streaming.kafka.KafkaUtils.createDirectStream(KafkaUtils.scala)
        at org.stream.processing.JavaKafkaStreamEventProcessing.main(JavaKafkaStreamEventProcessing.java:52)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:497)
        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)
        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)
        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

Program

public static void main(String[] args) {
    if (args.length < 2) {
      System.err.println(""Usage: DirectKafkaWordCount <brokers> <topics>\n"" +
          ""  <brokers> is a list of one or more Kafka brokers\n"" +
          ""  <topics> is a list of one or more kafka topics to consume from\n\n"");
      System.exit(1);
    }

    String brokers = args[0];
    String topics = args[1];

    // Create context with 2 second batch interval
    SparkConf sparkConf = new SparkConf().setAppName(""JavaKafkaStreamEventProcessing"");
    JavaStreamingContext jssc = new JavaStreamingContext(sparkConf, Durations.seconds(2));

    HashSet<String> topicsSet = new HashSet<String>(Arrays.asList(topics.split("","")));
    HashMap<String, String> kafkaParams = new HashMap<String, String>();
    kafkaParams.put(""metadata.broker.list"", brokers);

    // Create direct kafka stream with brokers and topics
    JavaPairInputDStream<String, String> messages = KafkaUtils.createDirectStream(
        jssc,
        String.class,
        String.class,
        StringDecoder.class,
        StringDecoder.class,
        kafkaParams,
        topicsSet
    );

    // Get the lines, split them into words, count the words and print
    JavaDStream<String> lines = messages.map(new Function<Tuple2<String, String>, String>() {
      public String call(Tuple2<String, String> tuple2) {
        return tuple2._2();
      }
    });
    JavaDStream<String> words = lines.flatMap(new FlatMapFunction<String, String>() {
      public Iterable<String> call(String x) {
        return Lists.newArrayList(SPACE.split(x));
      }
    });
    JavaPairDStream<String, Integer> wordCounts = words.mapToPair(
      new PairFunction<String, String, Integer>() {
        public Tuple2<String, Integer> call(String s) {
          return new Tuple2<String, Integer>(s, 1);
        }
      }).reduceByKey(
        new Function2<Integer, Integer, Integer>() {
        public Integer call(Integer i1, Integer i2) {
          return i1 + i2;
        }
      });
    wordCounts.print();
    System.out.println(""Word Counts are : "" + wordCounts.toString());

    // Start the computation
    jssc.start();
    jssc.awaitTermination();
  }
}
"
Yin Huai <yhuai@databricks.com>,"Sun, 27 Sep 2015 09:17:58 -0700",Re: [VOTE] Release Apache Spark 1.5.1 (RC1),robineast <robin.east@xense.co.uk>,"+1

Tested 1.5.1 SQL blockers.


"
Suresh Thalamati <suresh.thalamati@gmail.com>,"Sun, 27 Sep 2015 14:03:21 -0700",Re: [VOTE] Release Apache Spark 1.5.1 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1  (non-binding.)

Tested jdbc data source, and  some of the tpc-ds queries.
"
Michael Armbrust <michael@databricks.com>,"Sun, 27 Sep 2015 14:08:31 -0700",Re: Dataframes: PrunedFilteredScan without Spark Side Filtering,Russell Spitzer <russell.spitzer@gmail.com>,"We have to try and maintain binary compatibility here, so probably the
easiest thing to do here would be to add a method to the class.  Perhaps
something like:

def unhandledFilters(filters: Array[Filter]): Array[Filter] = filters

By default, this could return all filters so behavior would remain the
same, but specific implementations could override it.  There is still a
chance that this would conflict with existing methods, but hopefully that
would not be a problem in practice.

Thoughts?

Michael


"
Reynold Xin <rxin@databricks.com>,"Sun, 27 Sep 2015 14:25:26 -0700",Re: [VOTE] Release Apache Spark 1.5.1 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","Thanks everybody for voting. I'm going to close the vote now. The vote
passes with 17 +1 votes and 1 -1 vote. I will work on packaging this asap.


+1:
Reynold Xin*
Sean Owen
Hossein Falaki
Xiangrui Meng*
Krishna Sankar
Joseph Bradley
Sean McNamara*
Luciano Resende
Doug Balog
Eugene Zhulenev
Vaquar Khan
Tom Graves*
Michael Armbrust*
Marcelo Vanzin
Robin East
Yin Huai
Suresh Thalamati

0:

-1:
Richard Hillegas [see note]



Note: Richard Hillegas did say in a separate thread the issue he brought up
should not block the release. However, he did not explicitly amend his vote
so I'm including it as a -1 here.



"
Fengdong Yu <fengdongy@everstring.com>,"Mon, 28 Sep 2015 10:56:58 +0800",Re: How to get the HDFS path for each RDD,Anchit Choudhry <anchit.choudhry@gmail.com>,"Hi Anchit, 
cat you create more than one data in each dataset to test again?



your code.
classOf[Text], 2)
Text]]
iterator) => {
â€œ,â€ + tp._2.toString)})
""key1"" : ""value1"" }), (u'hdfs://localhost:9000/user/hduser/test/dt=20100102.json', u'{ ""key2"" : ""value2"" })]
u'hdfs://localhost:9000/user/hduser/test/dt=20100101.json', u'key1': u'value1'}, {u'key2': u'value2', 'source': u'hdfs://localhost:9000/user/hduser/test/dt=20100102.json'}]
'date' with some string manipulation per your requirements.
 : â€œvalue2â€ }
 : â€œvalue2â€ , â€œsourceâ€ : â€œtest1â€, â€œdateâ€ : â€œ20100101""}
 : â€œvalue2â€ , â€œsourceâ€ : â€œtest2â€, â€œdateâ€ : â€œ20100202""}
output you are expecting to bring clarity to my thoughts?
in my JSON  data.
sparkContext which can help you with that:
<>hdfs-pathâ€)
<http://spark/>.apache.org/docs/latest/api/python/pyspark.html?highlight=wholetext#pyspark.SparkContext.wholeTextFiles
https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext@wholeTextFiles(String,Int):RDD[(String,String)]
â€œHDFS_LOCATIONâ€} to each line, then save it the one target HDFS location.
---------------------------------------------------------------------
<mailto:dev-unsubscribe@spark.apache.org>
<mailto:dev-help@spark.apache.org>

"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 28 Sep 2015 03:09:43 +0000",Re: How to get the HDFS path for each RDD,"Fengdong Yu <fengdongy@everstring.com>, Anchit Choudhry <anchit.choudhry@gmail.com>","Shouldn't this discussion be held on the user list and not the dev list?
The dev list (this list) is for discussing development on Spark itself.

Please move the discussion accordingly.

Nick
2015ë…„ 9ì›” 27ì¼ (ì¼) ì˜¤í›„ 10:57, Fengdong Yu <fengdongy@everstring.com>ë‹˜ì´ ìž‘ì„±:

]
=> {
€œ,â€ +
""
on',
h
 : â€œvalue2â€ }
 : â€œvalue2â€ , â€œsourceâ€ : â€œtest1â€, â€œdateâ€ :
 : â€œvalue2â€ , â€œsourceâ€ : â€œtest2â€, â€œdateâ€ :
:
)
D
ONâ€} to each line, then
"
james <yiazhou@gmail.com>,"Mon, 28 Sep 2015 06:21:23 -0700 (MST)",Re: [VOTE] Release Apache Spark 1.5.1 (RC1),dev@spark.apache.org,"+1 

1) Build binary instruction: ./make-distribution.sh --tgz --skip-java-test
-Pyarn -Phadoop-2.6 -Dhadoop.version=2.6.0 -Phive -Phive-thriftserver
-DskipTests
2) Run Spark SQL with YARN client mode

This 1.5.1 RC1 package have better test results than "
Cody Koeninger <cody@koeninger.org>,"Mon, 28 Sep 2015 09:26:13 -0500",Re: Spark-Kafka Connector issue,Ratika Prasad <rprasad@couponsinc.com>,"This is a user list question not a dev list question.

Looks like your driver is having trouble communicating to the kafka
brokers.  Make sure the broker host and port is available from the driver
host (using nc or telnet); make sure that you're providing the _broker_
host and port to createDirectStream, not the zookeeper host; make sure the
topics in question actually exist on kafka and the names match what you're
providing to createDirectStream.






the kafka
er
y(KafkaCluster.scala:366)
y(KafkaCluster.scala:366)
cala:365)
s.scala:422)
s.scala:532)
.scala)
EventProcessing.java:52)
:62)
mpl.java:43)
$runMain(SparkSubmit.scala:665)
2)
();
ing,
ring,
"
Richard Hillegas <rhilleg@us.ibm.com>,"Mon, 28 Sep 2015 08:04:25 -0700","Re: [Discuss] NOTICE file for transitive ""NOTICE""s","""dev@spark.apache.org"" <dev@spark.apache.org>","Thanks, Sean!

Sean Owen <sowen@cloudera.com> wrote on 09/25/2015 06:35:46 AM:

another
<rhilleg@us.ibm.com>
proceed
release
by
as
but not"
Rohith P <rparameshwara@couponsinc.com>,"Mon, 28 Sep 2015 08:31:39 -0700 (MST)",using JavaRDD in spark-redis connector,dev@spark.apache.org,"Hi all,
      I am trying to work with spark-redis connector (redislabs) which
requires all transactions between redis and spark be in RDD's. The language 
I am using is Java but the connector does not accept JavaRDD's .So I tried
using Spark context in my code instead of JavaSparkContext. But when I
wanted to create a RDD using sc.parallelize , it asks for some scala related
parameters as opposed to lists in java.... when I tries to have both
javaSparkContext and sparkcontext(for connector) then Multiple contexts
cannot be opened was the error....
 The code that I have been trying ....


// initialize spark context
	private static RedisContext config() {
		conf = new SparkConf().setAppName(""redis-jedis"");
		sc2=new SparkContext(conf);
		RedisContext rc=new RedisContext(sc2);
		return rc;

	}
//write to redis which requires the data to be in RDD 
	private static void WriteUserTacticData(RedisContext rc, String userid,
String tacticsId, String value) {
		hostTup= calling(redisHost,redisPort);
		String key=userid+""-""+tacticsId;
		RDD<Tuple2&lt;String, String>> newTup=createTuple(key,value);
		rc.toRedisKV(newTup,hostTup);

// the createTuple where the RDD is to be created which will be inserted
into redis
	private static RDD<Tuple2&lt;String, String>> createTuple(String key,
String value) {
		sc=new JavaSparkContext(conf);
		ArrayList<Tuple2&lt;String,String>> list= new
ArrayList<Tuple2&lt;String,String>>();
		Tuple2<String,String> e= new Tuple2<String, String>(key,value);
		list.add(e);
		JavaRDD<Tuple2&lt;String,String>> javardd= sc.parallelize(list);
		RDD<Tuple2&lt;String,String>> newTupRdd=JavaRDD.toRDD(javardd); 
		sc.close();
		return newTupRdd;
	}



How would I create an RDD(not javaRDD) in java which will be accepted by
redis connector... Any kind of related to the topic would be
appretiated......





--

---------------------------------------------------------------------


"
Ratika Prasad <rprasad@couponsinc.com>,"Mon, 28 Sep 2015 18:20:06 +0000",RE: Spark-Kafka Connector issue,Cody Koeninger <cody@koeninger.org>,"Thanks for your reply.

I invoked my program with the broker ip and host and it triggered as expected but I see the below error

./bin/spark-submit --class org.stream.processing.JavaKafkaStreamEventProcessing --master local spark-stream-processing-0.0.1-SNAPSHOT-jar-with-dependencies.jar 172.28.161.32:9092 TestTopic
15/09/28 17:45:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
15/09/28 17:45:11 WARN StreamingContext: spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
Exception in thread ""main"" org.apache.spark.SparkException: java.nio.channels.ClosedChannelException
org.apache.spark.SparkException: Couldn't find leader offsets for Set([TestTopic,0])
        at org.apache.spark.streaming.kafka.KafkaCluster$$anonfun$checkErrors$1.apply(KafkaCluster.scala:366)
        at org.apache.spark.streaming.kafka.KafkaCluster$$anonfun$checkErrors$1.apply(KafkaCluster.scala:366)
        at scala.util.Either.fold(Either.scala:97)
        at org.apache.spark.streaming.kafka.KafkaCluster$.checkErrors(KafkaCluster.scala:365)
        at org.apache.spark.streaming.kafka.KafkaUtils$.createDirectStream(KafkaUtils.scala:422)
        at org.apache.spark.streaming.kafka.KafkaUtils$.createDirectStream(KafkaUtils.scala:532)
        at org.apache.spark.streaming.kafka.KafkaUtils.createDirectStream(KafkaUtils.scala)
        at org.stream.processing.JavaKafkaStreamEventProcessing.main(JavaKafkaStreamEventProcessing.java:52)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:497)
        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)
        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)
        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

Whene I ran the below to check the offsets I get this

bin/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --topic TestTopic --group test-consumer-group --zookeeper localhost:2181
Exiting due to: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /consumers/test-consumer-group/offsets/TestTopic /0.

Also I just added this below configs to my kafaka/config/consumer.properties and restarted kafka

auto.offset.reset=smallest
offsets.storage=zookeeper
offsets.channel.backoff.ms=1000
offsets.channel.socket.timeout.ms=10000
offsets.commit.max.retries=5
dual.commit.enabled=true

From: Cody Koeninger [mailto:cody@koeninger.org]
Sent: Monday, September 28, 2015 7:56 PM
To: Ratika Prasad <rprasad@couponsinc.com>
Cc: dev@spark.apache.org
Subject: Re: Spark-Kafka Connector issue

This is a user list question not a dev list question.

Looks like your driver is having trouble communicating to the kafka brokers.  Make sure the broker host and port is available from the driver host (using nc or telnet); make sure that you're providing the _broker_ host and port to createDirectStream, not the zookeeper host; make sure the topics in question actually exist on kafka and the names match what you're providing to createDirectStream.





On Sat, Sep 26, 2015 at 11:50 PM, Ratika Prasad <rprasad@couponsinc.com<maing out the spark streaming and reading the messages from kafka topics which later would be created into streams as belowâ€¦I have the kafka setup on a vm and topics created however when I try to run the program below from my spark vm as below I get an error even though the kafka server and zookeeper are up and running

./bin/spark-submit --class org.stream.processing.JavaKafkaStreamEventProcessing --master local spark-stream-processing-0.0.1-SNAPSHOT-jar-with-dependencies.jar 172.28.161.32:2181<http://172.28.161.32:2181> redemption_inbound

Exception in thread ""main"" org.apache.spark.SparkException: java.io.EOFException: Received -1 when reading from channel, socket has likely been closed.
        at org.apache.spark.streaming.kafka.KafkaCluster$$anonfun$checkErrors$1.apply(KafkaCluster.scala:366)
        at org.apache.spark.streaming.kafka.KafkaCluster$$anonfun$checkErrors$1.apply(KafkaCluster.scala:366)
        at scala.util.Either.fold(Either.scala:97)
        at org.apache.spark.streaming.kafka.KafkaCluster$.checkErrors(KafkaCluster.scala:365)
        at org.apache.spark.streaming.kafka.KafkaUtils$.createDirectStream(KafkaUtils.scala:422)
        at org.apache.spark.streaming.kafka.KafkaUtils$.createDirectStream(KafkaUtils.scala:532)
        at org.apache.spark.streaming.kafka.KafkaUtils.createDirectStream(KafkaUtils.scala)
        at org.stream.processing.JavaKafkaStreamEventProcessing.main(JavaKafkaStreamEventProcessing.java:52)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:497)
        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)
        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)
        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

Program

public static void main(String[] args) {
    if (args.length < 2) {
      System.err.println(""Usage: DirectKafkaWordCount <brokers> <topics>\n"" +
          ""  <brokers> is a list of one or more Kafka brokers\n"" +
          ""  <topics> is a list of one or more kafka topics to consume from\n\n"");
      System.exit(1);
    }

    String brokers = args[0];
    String topics = args[1];

    // Create context with 2 second batch interval
    SparkConf sparkConf = new SparkConf().setAppName(""JavaKafkaStreamEventProcessing"");
    JavaStreamingContext jssc = new JavaStreamingContext(sparkConf, Durations.seconds(2));

    HashSet<String> topicsSet = new HashSet<String>(Arrays.asList(topics.split("","")));
    HashMap<String, String> kafkaParams = new HashMap<String, String>();
    kafkaParams.put(""metadata.broker.list"", brokers);

    // Create direct kafka stream with brokers and topics
    JavaPairInputDStream<String, String> messages = KafkaUtils.createDirectStream(
        jssc,
        String.class,
        String.class,
        StringDecoder.class,
        StringDecoder.class,
        kafkaParams,
        topicsSet
    );

    // Get the lines, split them into words, count the words and print
    JavaDStream<String> lines = messages.map(new Function<Tuple2<String, String>, String>() {
      public String call(Tuple2<String, String> tuple2) {
        return tuple2._2();
      }
    });
    JavaDStream<String> words = lines.flatMap(new FlatMapFunction<String, String>() {
      public Iterable<String> call(String x) {
        return Lists.newArrayList(SPACE.split(x));
      }
    });
    JavaPairDStream<String, Integer> wordCounts = words.mapToPair(
      new PairFunction<String, String, Integer>() {
        public Tuple2<String, Integer> call(String s) {
          return new Tuple2<String, Integer>(s, 1);
        }
      }).reduceByKey(
        new Function2<Integer, Integer, Integer>() {
        public Integer call(Integer i1, Integer i2) {
          return i1 + i2;
        }
      });
    wordCounts.print();
    System.out.println(""Word Counts are : "" + wordCounts.toString());

    // Start the computation
    jssc.start();
    jssc.awaitTermination();
  }
}

"
Renyi Xiong <renyixiong0@gmail.com>,"Mon, 28 Sep 2015 16:36:10 -0700",failed to run spark sample on windows,dev@spark.apache.org,"I tried to run HdfsTest sample on windows spark-1.4.0

bin\run-sample org.apache.spark.examples.HdfsTest <file>

but got below exception, any body any idea what was wrong here?

15/09/28 16:33:56.565 ERROR SparkContext: Error initializing SparkContext.
java.lang.NullPointerException
        at java.lang.ProcessBuilder.start(ProcessBuilder.java:1010)
        at org.apache.hadoop.util.Shell.runCommand(Shell.java:445)
        at org.apache.hadoop.util.Shell.run(Shell.java:418)
        at
org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:650)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:739)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:722)
        at
org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:633)
        at
org.apache.hadoop.fs.FilterFileSystem.setPermission(FilterFileSystem.java:467)
        at
org.apache.spark.scheduler.EventLoggingListener.start(EventLoggingListener.scala:130)
        at org.apache.spark.SparkContext.<init>(SparkContext.scala:515)
        at org.apache.spark.examples.HdfsTest$.main(HdfsTest.scala:32)
        at org.apache.spark.examples.HdfsTest.main(HdfsTest.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at
org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:664)
        at
org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:169)
        at
org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:192)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:111)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
"
Ted Yu <yuzhihong@gmail.com>,"Mon, 28 Sep 2015 16:39:05 -0700",Re: failed to run spark sample on windows,Renyi Xiong <renyixiong0@gmail.com>,"What version of hadoop are you using ?

Is that version consistent with the one which was used to build Spark 1.4.0
?

Cheers


"
Siva <sbhavanari@gmail.com>,"Mon, 28 Sep 2015 16:45:14 -0700",Monitoring tools for spark streaming,"user@apache.spark.org, dev@spark.apache.org","Hi,

Could someone recommend the monitoring tools for spark streaming?

By extending StreamingListener we can dump the delay in processing of
batches and some alert messages.

But are there any Web UI tools where we can monitor failures, see delays in
processing, error messages and setup alerts etc.

Thanks
"
Stephen Boesch <javadba@gmail.com>,"Mon, 28 Sep 2015 17:39:26 -0700",Re: Using scala-2.11 when making changes to spark source,Ted Yu <yuzhihong@gmail.com>,"The effects of changing the pom.xml extend beyond cases in which we wish to
modify spark itself. In addition when git pull'ing from trunk we need to
either stash or roll back the changes before rebase'ing.

An effort to look into a better solution (possibly including evaluating Ted
Yu's suggested approach) might be considered?

2015-09-20 9:12 GMT-07:00 Ted Yu <yuzhihong@gmail.com>:

"
Rachana Srivastava <Rachana.Srivastava@markmonitor.com>,"Tue, 29 Sep 2015 02:01:31 +0000",spark-submit classloader issue...,"""user@spark.apache.org"" <user@spark.apache.org>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","Hello all,

Goal:  I want to use APIs from HttpClient library 4.4.1.  I am using maven shaded plugin to generate JAR.



Findings: When I run my program as a java application within eclipse everything works fine.  But when I am running the program using spark-submit I am getting following error:

URL content Could not initialize class org.apache.http.conn.ssl.SSLConnectionSocketFactory

java.lang.NoClassDefFoundError: Could not initialize class org.apache.http.conn.ssl.SSLConnectionSocketFactory



When I tried to get the referred JAR it is pointing to some Hadoop JAR,  I am assuming this is something set in spark-submit.



ClassLoader classLoader = HttpEndPointClient.class.getClassLoader();

URL resource = classLoader.getResource(""org/apache/http/message/BasicLineFormatter.class"");

Prints following jar:

jar:file:/usr/lib/hadoop/lib/httpcore-4.2.5.jar!/org/apache/http/message/BasicLineFormatter.class



After research I found that I can override --conf spark.files.userClassPathFirst=true --conf spark.yarn.user.classpath.first=true



But when I do that I am getting following error:

ERROR: org.apache.spark.executor.Executor - Exception in task 0.0 in stage 0.0 (TID 0)

java.io.InvalidClassException: org.apache.spark.scheduler.Task; local class incompatible: stream classdesc serialVersionUID = -4703555755588060120, local class serialVersionUID = -1589734467697262504

                at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:617)

                at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1622)

                at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1517)

                at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1622)

                at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1517)

                at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1771)

                at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)

                at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)

                at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:68)

                at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:94)

                at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:185)

                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)

                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

                at java.lang.Thread.run(Thread.java:745)



I am running on CDH 5.4  Here is my complete pom file.



<project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""

                xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd<http://maven.apache.org/POM/4.0.0%20http:/maven.apache.org/xsd/maven-4.0.0.xsd>"">

                <modelVersion>4.0.0</modelVersion>

                <groupId>test</groupId>

                <artifactId>test</artifactId>

                <version>0.0.1-SNAPSHOT</version>

                <dependencies>

                                <dependency>

                                                <groupId>org.apache.httpcomponents</groupId>

                                                <artifactId>httpcore</artifactId>

                                                <version>4.4.1</version>

                                </dependency>

                                <dependency>

                                                <groupId>org.apache.httpcomponents</groupId>

                                                <artifactId>httpclient</artifactId>

                                                <version>4.4.1</version>

                                </dependency>

                                <dependency>

                                                <groupId>org.apache.spark</groupId>

                                                <artifactId>spark-streaming-kafka_2.10</artifactId>

                                                <version>1.5.0</version>

                                                <exclusions>

    <exclusion>

     <artifactId>httpcore</artifactId>

      <groupId>org.apache.httpcomponents</groupId>

    </exclusion>

</exclusions>

                                </dependency>

                                <dependency>

                                                <groupId>org.apache.spark</groupId>

                                                <artifactId>spark-streaming_2.10</artifactId>

                                                <version>1.5.0</version>

                                                <exclusions>

    <exclusion>

     <artifactId>httpcore</artifactId>

      <groupId>org.apache.httpcomponents</groupId>

    </exclusion>

</exclusions>

                                </dependency>

                                <dependency>

                                                <groupId>org.apache.spark</groupId>

                                                <artifactId>spark-core_2.10</artifactId>

                                                <version>1.5.0</version>

                                                <exclusions>

    <exclusion>

     <artifactId>httpcore</artifactId>

      <groupId>org.apache.httpcomponents</groupId>

    </exclusion>

</exclusions>

                                </dependency>

                                <dependency>

                                                <groupId>org.apache.spark</groupId>

                                                <artifactId>spark-mllib_2.10</artifactId>

                                                <version>1.5.0</version>

                                                <exclusions>

    <exclusion>

     <artifactId>httpcore</artifactId>

      <groupId>org.apache.httpcomponents</groupId>

    </exclusion>

</exclusions>

                                </dependency>

                                <dependency>

                                                <groupId>org.json</groupId>

                                                <artifactId>json</artifactId>

                                                <version>20140107</version>

                                </dependency>

                                <dependency>

                                                <groupId>org.jsoup</groupId
                                                <artifactId>jsoup</artifactId>

                                                <version>1.8.3</version>

                                </dependency>

                </dependencies>

                <build>

                                <sourceDirectory>src/main/java</sourceDirectory>

                                <testSourceDirectory>src/test/java</testSourceDirectory>

                                <resources>

                                                <resource>

                                                                <directory>src/main/resources</directory>

                                                </resource>

                                </resources>

                                <plugins>

                                                <!-- download source code in Eclipse, best practice -->

                                                <plugin>

                                                                <groupId>org.apache.maven.plugins</groupId>

                                                                <artifactId
                                                                <version>2.9</version>

                                                                <configuration>

                                                                                <downloadSources>true</downloadSources>

                                                                                <downloadJavadocs>false</downloadJavadocs>

                                                                </configuration>

                                                </plugin>



                                                <!-- Set a compiler level -->

                                                <plugin>

                                                                <groupId>org.apache.maven.plugins</groupId>

                                                                <artifactId
                                                                <version>2.3.2</version>

                                                </plugin>

                                                <!-- Maven Shade Plugin -->

                                                <plugin>

                                                                <groupId>org.apache.maven.plugins</groupId>

                                                                <artifactId
                                                                <version>2.3</version>

                                                                <executions
                                                                                <!-- Run shade goal on package phase -->

                                                                                <execution>

                                                                                                <phase>package</phase>

                                                                                                <goals>

                                                                                                                <goal>shade</goal>

                                                                                                </goals>

                                                                                                <configuration>

                                                                                                                <filters>

                                                                                                                                <filter>

                                                                                                                                                <artifact>*:*</artifact>

                                                                                                                                                <excludes>

                                                                                                                                                                <exclude>META-INF/*.SF</exclude>

                                                                                                                                                                <exclude>META-INF/*.DSA</exclude>

                                                                                                                                                                <exclude>META-INF/*.RSA</exclude>

                                                                                                                                                </excludes>

                                                                                                                                </filter>

                                                                                                                </filters>

                                                                                                                <transformers>

                                                                                                                                <transformer

                                                                                                                                                implementation=""org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"">

                                                                                                                                </transformer>

                                                                                                                </transformers>

                                                                                                </configuration>

                                                                                </execution>

                                                                </executions>

                                                </plugin>

                                </plugins>

                </build>

</project>

Thanks,

Rachana

"
Jerry Lam <chilinglam@gmail.com>,"Mon, 28 Sep 2015 23:07:11 -0400",Re: [VOTE] Release Apache Spark 1.5.1 (RC1),james <yiazhou@gmail.com>,"Hi Spark Developers,

The Spark 1.5.1 documentation is already publicly accessible (
https://spark.apache.org/docs/latest/index.html) but the release is not. Is
it intentional?

Best Regards,

Jerry


"
Sean Owen <sowen@cloudera.com>,"Mon, 28 Sep 2015 23:09:28 -0400",Re: [VOTE] Release Apache Spark 1.5.1 (RC1),Jerry Lam <chilinglam@gmail.com>,"It's on Maven Central already. These various updates have to happen in
some order, and you'll probably see an inconsistent state for a day or
so while things get slowly updated. Consider it released when there's
an announcement, I suppose.


---------------------------------------------------------------------


"
Rachana Srivastava <Rachana.Srivastava@markmonitor.com>,"Tue, 29 Sep 2015 06:37:48 +0000",Where are logs for Spark Kafka Yarn on Cloudera,"""user@spark.apache.org"" <user@spark.apache.org>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","Hello all,

I am trying to test JavaKafkaWordCount on Yarn, to make sure Yarn is working fine I am saving the output to hdfs.  The example works fine in local mode but not on yarn mode.  I cannot see any output logged when I changed the mode to yarn-client or yarn-cluster or cannot find any errors logged.  For my application id I was looking for logs under /var/log/hadoop-yarn/containers (e.g /var/log/hadoop-yarn/containers/application_1439517792099_0010/container_1439517792099_0010_01_000003/stderr) but I cannot find anything useful information.   Is it the only location where application logs are logged.

Also tried setting log output to spark.yarn.app.container.log.dir but got access denied error.

Question:  Do we need to have some special setup to run spark streaming on Yarn?  How do we debug?  Where to find more details to test streaming on Yarn.

Thanks,

Rachana
"
sbiookag <sbiookag@asu.edu>,"Tue, 29 Sep 2015 11:36:56 -0700 (MST)",Spark Network Module behaviour,dev@spark.apache.org,"Dear All,

I am trying to understand how exactly spark network module works. Looking at
Netty package, I would like to intercept every server response for block
fetch. As I understood the place which is responsible for sending remote
blocks is ""TransportRequestHandler.processFetchRequest"". Im trying to
distinguish these flows from other flows in a switch using the information I
get from ""channel"" object in this function. These information are source and
destination IP and source and destination PORT. I get all these four numbers
from the channel object. The problem rises when I'm looking at the switch
log and I see extensive traffic going back and forth, which do not belong to
these flows I'm capturing. Now my question is, I am missing other places,
which are responsible for huge network traffic? Is the port and IP I get
from channel is not the correct one?

Thank,
Saman



--

---------------------------------------------------------------------


"
Archit Thakur <archit279thakur@gmail.com>,"Wed, 30 Sep 2015 00:36:26 +0530",Dynamic DAG use-case for spark streaming.,"user@spark.apache.org, dev@spark.apache.org","Hi,

 We are using spark streaming as our processing engine, and as part of
output we want to push the data to UI. Now there would be multiple users
accessing the system with there different filters on. Based on the filters
and other inputs we want to either run a SQL Query on DStream or do a
custom logic processing. This would need the system to read the
filters/query and generate the execution graph at runtime. I cant see any
support in spark streaming for generating the execution graph on the fly.
I think I can broadcast the query to executors and read the broadcasted
query at runtime but that would also limit my user to 1 at a time.

Do we not expect the spark streaming to take queries/filters from outside
world. Does output in spark streaming only means outputting to an external
source which could then be queried.

Thanks,
Archit Thakur.
"
"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Tue, 29 Sep 2015 19:23:53 +0000",Too many executors are created ,"""dev@spark.apache.org"" <dev@spark.apache.org>","Dear Spark developers,

I have created a simple Spark application for spark submit. It calls a machine learning library from Spark MLlib that is executed in a number of iterations that correspond to the same number of task in Spark. It seems that Spark creates an executor for each task and then removes it. The following messages indicate this in my log:

15/09/29 12:21:02 INFO AppClient$ClientEndpoint: Executor updated: app-20150929120924-0000/24463 is now RUNNING
15/09/29 12:21:02 INFO AppClient$ClientEndpoint: Executor updated: app-20150929120924-0000/24463 is now EXITED (Command exited with code 1)
15/09/29 12:21:02 INFO SparkDeploySchedulerBackend: Executor app-20150929120924-0000/24463 removed: Command exited with code 1
15/09/29 12:21:02 INFO SparkDeploySchedulerBackend: Asked to remove non-existent executor 24463
15/09/29 12:21:02 INFO AppClient$ClientEndpoint: Executor added: app-20150929120924-0000/24464 on worker-20150929120330-16.111.35.101-46374 (16.111.35.101:46374) with 12 cores
15/09/29 12:21:02 INFO SparkDeploySchedulerBackend: Granted executor ID app-20150929120924-0000/24464 on hostPort 16.111.35.101:46374 with 12 cores, 30.0 GB RAM
15/09/29 12:21:02 INFO AppClient$ClientEndpoint: Executor updated: app-20150929120924-0000/24464 is now LOADING
15/09/29 12:21:02 INFO AppClient$ClientEndpoint: Executor updated: app-20150929120924-0000/24464 is now RUNNING
15/09/29 12:21:02 INFO AppClient$ClientEndpoint: Executor updated: app-20150929120924-0000/24464 is now EXITED (Command exited with code 1)
15/09/29 12:21:02 INFO SparkDeploySchedulerBackend: Executor app-20150929120924-0000/24464 removed: Command exited with code 1
15/09/29 12:21:02 INFO SparkDeploySchedulerBackend: Asked to remove non-existent executor 24464
15/09/29 12:21:02 INFO AppClient$ClientEndpoint: Executor added: app-20150929120924-0000/24465 on worker-20150929120330-16.111.35.101-46374 (16.111.35.101:46374) with 12 cores
15/09/29 12:21:02 INFO SparkDeploySchedulerBackend: Granted executor ID app-20150929120924-0000/24465 on hostPort 16.111.35.101:46374 with 12 cores, 30.0 GB RAM
15/09/29 12:21:02 INFO AppClient$ClientEndpoint: Executor updated: app-20150929120924-0000/24465 is now LOADING
15/09/29 12:21:02 INFO AppClient$ClientEndpoint: Executor updated: app-20150929120924-0000/24465 is now EXITED (Command exited with code 1)
15/09/29 12:21:02 INFO SparkDeploySchedulerBackend: Executor app-20150929120924-0000/24465 removed: Command exited with code 1
15/09/29 12:21:02 INFO SparkDeploySchedulerBackend: Asked to remove non-existent executor 24465
15/09/29 12:21:02 INFO AppClient$ClientEndpoint: Executor added: app-20150929120924-0000/24466 on worker-20150929120330-16.111.35.101-46374 (16.111.35.101:46374) with 12 cores
15/09/29 12:21:02 INFO SparkDeploySchedulerBackend: Granted executor ID app-20150929120924-0000/24466 on hostPort 16.111.35.101:46374 with 12 cores, 30.0 GB RAM
15/09/29 12:21:02 INFO AppClient$ClientEndpoint: Executor updated: app-20150929120924-0000/24466 is now LOADING
15/09/29 12:21:02 INFO AppClient$ClientEndpoint: Executor updated: app-20150929120924-0000/24466 is now RUNNING

It end up creating and removing thousands of executors. Is this a normal behavior?

If I run the same code within spark-shell, this does not happen. Could you suggest what might be wrong in my setting?

Best regards, Alexander
"
Tathagata Das <tdas@databricks.com>,"Tue, 29 Sep 2015 13:07:16 -0700",Re: Dynamic DAG use-case for spark streaming.,Archit Thakur <archit279thakur@gmail.com>,"A very basic support that is there in DStream is DStream.transform() which
take arbitrary RDD => RDD function. This function can actually choose to do
different computation with time. That may be of help to you.


"
Renyi Xiong <renyixiong0@gmail.com>,"Tue, 29 Sep 2015 13:17:22 -0700",Re: failed to run spark sample on windows,Ted Yu <yuzhihong@gmail.com>,"not sure, so downloaded  again release 1.4.1 with Hadoop 2.6 and later
options from http://spark.apache.org/downloads.html assuming the version is
consistent and run the following on Windows 10

c:\spark-1.4.1-bin-hadoop2.6>bin\run-example HdfsTest <local_file>

still got similar exception below: (I heard there's permission config for
hdfs, if so how do I do that?)

15/09/29 13:03:26 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.NullPointerException
        at java.lang.ProcessBuilder.start(ProcessBuilder.java:1010)
        at org.apache.hadoop.util.Shell.runCommand(Shell.java:482)
        at org.apache.hadoop.util.Shell.run(Shell.java:455)
        at
org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:715)
        at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:873)
        at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:853)
        at org.apache.spark.util.Utils$.fetchFile(Utils.scala:465)
        at
org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:398)
        at
org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:390)
        at
scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
        at
scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
        at
scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
        at
scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
        at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
        at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
        at
scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
        at org.apache.spark.executor.Executor.org
$apache$spark$executor$Executor$$updateDependencies(Executor.scala:390)
        at
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:193)
        at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)


"
saurfang <forest.fang@outlook.com>,"Tue, 29 Sep 2015 13:22:02 -0700 (MST)",Re: failed to run spark sample on windows,dev@spark.apache.org,"See
http://stackoverflow.com/questions/26516865/is-it-possible-to-run-hadoop-jobs-like-the-wordcount-sample-in-the-local-mode,
https://issues.apache.org/jira/browse/SPARK-6961 and finally
https://issues.apache.org/jira/browse/HADOOP-10775. The easy solution is to
download a Windows Hadoop distribution and point %HADOOP_HOME% to that
location so winutils.exe can be picked up.



--

---------------------------------------------------------------------


"
Pala M Muthaia <mchettiar@rocketfuelinc.com.INVALID>,"Tue, 29 Sep 2015 15:43:08 -0700",Hive permanent functions are not available in Spark SQL,dev <dev@spark.apache.org>,"Hi,

I am trying to use internal UDFs that we have added as permanent functions
to Hive, from within Spark SQL query (using HiveContext), but i encounter
NoSuchObjectException, i.e. the function could not be found.

However, if i execute 'show functions' command in spark SQL, the permanent
functions appear in the list.

I am using Spark 1.4.1 with Hive 0.13.1. I tried to debug this by looking
at the log and code, but it seems both the show functions command as well
as udf query both go through essentially the same code path, but the former
can see the UDF but the latter can't.

Any ideas on how to debug/fix this?


Thanks,
pala
"
Akhil Das <akhil@sigmoidanalytics.com>,"Wed, 30 Sep 2015 12:27:30 +0530",Re: using JavaRDD in spark-redis connector,Rohith P <rparameshwara@couponsinc.com>,"You can create a JavaRDD as normal and then call the .rdd() to get the RDD.

Thanks
Best Regards


"
Yogs <mahajan.yogesh@gmail.com>,"Wed, 30 Sep 2015 13:25:02 +0530",CQs on WindowedStream created on running StreamingContext,dev@spark.apache.org,"Hi,

We intend to run adhoc windowed continuous queries on spark streaming data.
The queries could be registered/deregistered dynamically or can be
submitted through command line. Currently Spark streaming doesnâ€™t allow
adding any new inputs, transformations, and output operations after
starting a StreamingContext. But doing following code changes in
DStream.scala allows me to create an window on DStream even after
StreamingContext has started (in StreamingContextState.ACTIVE).

1) In DStream.validateAtInit()
Allowed adding new inputs, transformations, and output operations after
starting a streaming context
2) In DStream.persist()
Allowed to change storage level of an DStream after streaming context has
started

Ultimately the window api just does slice on the parentRDD and returns
allRDDsInWindow.
We create DataFrames out of these RDDs from this particular
WindowedDStream, and evaluate queries on those DataFrames.

1) Do you see any challenges and consequences with this approach ?
2) Will these on the fly created WindowedDStreams be accounted properly in
Runtime and memory management?
3) What is the reason we do not allow creating new windows with
StreamingContextState.ACTIVE state?
4) Does it make sense to add our own implementation of WindowedDStream in
this case?

- Yogesh
"
gsvic <victorasgs@gmail.com>,"Wed, 30 Sep 2015 02:21:44 -0700 (MST)",Task Execution,dev@spark.apache.org,"Concerning task execution, a worker executes its assigned tasks in parallel
or sequentially?



--

---------------------------------------------------------------------


"
Renyi Xiong <renyixiong0@gmail.com>,"Wed, 30 Sep 2015 09:37:19 -0700",Re: failed to run spark sample on windows,saurfang <forest.fang@outlook.com>,"thanks a lot, it works now after I set %HADOOP_HOME%


"
Richard Hillegas <rhilleg@us.ibm.com>,"Wed, 30 Sep 2015 09:53:32 -0700",Re: unsubscribe,sukesh kumar <s724788@gmail.com>,"
Hi Sukesh,

ease
send a message user-unsubscribe@spark.apache.org. Please see:
http://spark.apache.org/community.html#mailing-lists.

Thanks,
-Rick

sukesh kumar <s724788@gmail.com> wrote on 09/28/2015 11:39:01 PM:

"
"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Wed, 30 Sep 2015 22:55:47 +0000",GraphX PageRank keeps 3 copies of graph in memory,"""dev@spark.apache.org"" <dev@spark.apache.org>","Dear Spark developers,

I would like to understand GraphX caching behavior with regards to PageRank in Spark, in particular, the following implementation of PageRank:
https://github.com/apache/spark/blob/master/graphx/src/main/scala/org/apache/spark/graphx/lib/PageRank.scala

 un-cached:
1) Create new graph and cache it:
rankGraph = rankGraph.joinVertices(rankUpdates) {
        (id, oldRank, msgSum) => rPrb(src, id) + (1.0 - resetProb) * msgSum
      }.cache()
2) Unpersist the old one:
      prevRankGraph.vertices.unpersist(false)
      prevRankGraph.edges.unpersist(false)

According to the code, at the end of each iteration only one graph should be in memory, i.e. one EdgeRDD and one VertexRDD. During the iteration, exactly between the mentioned lines of code, there will be two graphs: old and new. It is two pairs of Edge and Vertex RDDs. However, when I run the example provided in Spark examples folder, I observe the different behavior.

Run the example (I checked that it runs the mentioned code):
$SPARK_HOME/bin/spark-submit --class ""org.apache.spark.examples.graphx.SynthBenchmark""  --master spark://mynode.net:7077 $SPARK_HOME/examples/target/spark-examples.jar

According to ""Storage"" and RDD DAG in Spark UI, 3 VertexRDDs and 3 EdgeRDDs are cached, even when all iterations are finished, given that the mentioned code suggests caching at most 2 (and only in particular stage of the iteration):
https://drive.google.com/file/d/0BzYMzvDiCep5WFpnQjFzNy0zYlU/view?usp=sharing
Edges (the green ones are cached):
https://drive.google.com/file/d/0BzYMzvDiCep5S2JtYnhVTlV1Sms/view?usp=sharing
Vertices (the green ones are cached):
https://drive.google.com/file/d/0BzYMzvDiCep5S1k4N2NFb05RZDA/view?usp=sharing

Could you explain, why 3 VertexRDDs and 3 EdgeRDDs are cached?

Is it OK that there is a double caching in code, given that joinVertices implicitly caches vertices and then the graph is cached in the PageRank code?

Best regards, Alexander
"
