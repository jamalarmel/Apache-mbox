"""Kazuaki Ishizaki"" <ISHIZAKI@jp.ibm.com>","Mon, 4 Jan 2016 12:52:01 +0900",Support off-loading computations to a GPU,dev@spark.apache.org,"Dear all,

We reopened the existing JIRA entry 
https://issues.apache.org/jira/browse/SPARK-3785 to support off-loading 
computations to a GPU by adding a description for our prototype. We are 
working to effectively and easily exploit GPUs on Spark at 
http://github.com/kiszk/spark-gpu. Please also visit our project page 
http://kiszk.github.io/spark-gpu/.

For now, we added a new format for a partition in an RDD, which is a 
column-based structure in an array format, in addition to the current 
Iterator[T] format with Seq[T]. This reduces data 
serialization/deserialization and copy overhead between CPU and GPU.

Our prototype achieved more than 3x performance improvement for a simple 
logistic regression program using a NVIDIA K40 card.

This JIRA entry (SPARK-3785) includes a link to a design document. We are 
very glad to hear valuable feedback/suggestions/comments and to have great 
discussions to exploit GPUs in Spark.

Best Regards,
Kazuaki Ishizaki

"
"""Allen Zhang"" <allenzhang010@126.com>","Mon, 4 Jan 2016 12:26:44 +0800 (CST)",Re:Support off-loading computations to a GPU,"""Kazuaki Ishizaki"" <ISHIZAKI@jp.ibm.com>","Hi Kazuaki,


I am looking at http://kiszk.github.io/spark-gpu/ , can you point me where is the kick-start scripts that I can give it a go?


to be more specifically, what does *""off-loading""* mean? aims to reduce the copy overhead between CPU and GPU?
I am a newbie for GPU, how can I specify how many GPU cores I want to use (like --executor-cores) ?






At 2016-01-04 11:52:01, ""Kazuaki Ishizaki"" <ISHIZAKI@jp.ibm.com> wrote:
Dear all,

We reopened the existing JIRA entry https://issues.apache.org/jira/browse/SPARK-3785to support off-loading computations to a GPU by adding a description for our prototype. We are working to effectively and easily exploit GPUs on Spark at http://github.com/kiszk/spark-gpu. Please also visit our project page http://kiszk.github.io/spark-gpu/.

For now, we added a new format for a partition in an RDD, which is a column-based structure in an array format, in addition to the current Iterator[T] format with Seq[T]. This reduces data serialization/deserialization and copy overhead between CPU and GPU.

Our prototype achieved more than 3x performance improvement for a simple logistic regression program using a NVIDIA K40 card.

This JIRA entry (SPARK-3785) includes a link to a design document. We are very glad to hear valuable feedback/suggestions/comments and to have great discussions to exploit GPUs in Spark.

Best Regards,
Kazuaki Ishizaki
"
"""Kazuaki Ishizaki"" <ISHIZAKI@jp.ibm.com>","Mon, 4 Jan 2016 23:06:59 +0900",Re: Support off-loading computations to a GPU,dev@spark.apache.org,"I created a new JIRA entry 
https://issues.apache.org/jira/browse/SPARK-12620 for this instead of 
reopening the existing JIRA based on the suggestion.

Best Regards,
Kazuaki Ishizaki



From:   Kazuaki Ishizaki/Japan/IBM@IBMJP
To:     dev@spark.apache.org
Date:   2016/01/04 12:54
Subject:        Support off-loading computations to a GPU



Dear all,

We reopened the existing JIRA entry 
https://issues.apache.org/jira/browse/SPARK-3785to support off-loading 
computations to a GPU by adding a description for our prototype. We are 
working to effectively and easily exploit GPUs on Spark at 
http://github.com/kiszk/spark-gpu. Please also visit our project page 
http://kiszk.github.io/spark-gpu/.

For now, we added a new format for a partition in an RDD, which is a 
column-based structure in an array format, in addition to the current 
Iterator[T] format with Seq[T]. This reduces data 
serialization/deserialization and copy overhead between CPU and GPU.

Our prototype achieved more than 3x performance improvement for a simple 
logistic regression program using a NVIDIA K40 card.

This JIRA entry (SPARK-3785) includes a link to a design document. We are 
very glad to hear valuable feedback/suggestions/comments and to have great 
discussions to exploit GPUs in Spark.

Best Regards,
Kazuaki Ishizaki


"
Michael Armbrust <michael@databricks.com>,"Mon, 4 Jan 2016 08:50:00 -0800",[ANNOUNCE] Announcing Spark 1.6.0,"""dev@spark.apache.org"" <dev@spark.apache.org>, user <user@spark.apache.org>","Hi All,

Spark 1.6.0 is the seventh release on the 1.x line. This release includes
patches from 248+ contributors! To download Spark 1.6.0 visit the downloads
page.  (It may take a while for all mirrors to update.)

A huge thanks go to all of the individuals and organizations involved in
development and testing of this release.

Visit the release notes [1] to read about the new features, or download [2]
the release today.

For errata in the contributions or release notes, please e-mail me
*directly* (not on-list).

Thanks to everyone who helped work on this release!

[1] http://spark.apache.org/releases/spark-release-1-6-0.html
[2] http://spark.apache.org/downloads.html
"
Rachana Srivastava <Rachana.Srivastava@markmonitor.com>,"Mon, 4 Jan 2016 20:56:35 +0000","Spark Streaming Application is Stuck Under Heavy Load  Due to
 DeadLock","""user@spark.apache.org"" <user@spark.apache.org>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","Hello All,

I am running my application on Spark cluster but under heavy load the system is hung due to deadlock.  I found similar issues resolved here https://datastax-oss.atlassian.net/browse/JAVA-555 in  Spark version 2.1.3.  But I am running on Spark 1.3 still getting the same issue.

Here is the stack trace for reference:

sun.misc.Unsafe.park(Native Method)
java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
org.apache.spark.streaming.ContextWaiter.waitForStopOrError(ContextWaiter.scala:63)
org.apache.spark.streaming.StreamingContext.awaitTermination(StreamingContext.scala:521)
org.apache.spark.streaming.api.java.JavaStreamingContext.awaitTermination(JavaStreamingContext.scala:592)

sun.misc.Unsafe.park(Native Method)
java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)
java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:994)
java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1303)
java.util.concurrent.Semaphore.acquire(Semaphore.java:317)
org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(AsynchronousListenerBus.scala:62)
org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply(AsynchronousListenerBus.scala:61)
org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply(AsynchronousListenerBus.scala:61)
org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1617)
org.apache.spark.util.AsynchronousListenerBus$$anon$1.run(AsynchronousListenerBus.scala:60)

sun.misc.Unsafe.park(Native Method)
java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
org.spark-project.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
org.spark-project.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
org.spark-project.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
org.spark-project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
java.lang.Thread.run(Thread.java:745)


Thanks,

Rachana

"
"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Mon, 4 Jan 2016 21:06:18 +0000",RE: SparkML algos limitations question.,"Yanbo Liang <ybliang8@gmail.com>, Joseph Bradley <joseph@databricks.com>","Hi Yanbo,

As long as two models fit into memory of a single machine, there should be no problems, so even 16GB machines can handle large models. (master should have more memory because it runs LBFGS) In my experiments, I‚Äôve trained the models 12M and 32M parameters without issues.

Best regards, Alexander

From: Yanbo Liang [mailto3 AM
To: Joseph Bradley
Cc: Eugene Morozov; user; dev@spark.apache.org
Subject: Re: SparkML algos limitations question.

Hi Eugene,

AFAIK, the current implementation of MultilayerPerceptronClassifier have some scalability problems if the model is very huge (such as >10M), although I think the limitation can cover many use cases already.

Yanbo

2015-12-16 6:00 GMT+08:00 Joseph Bradley <joseph@databricks.com<mailto:joseph@databricks.com>>:
Hi Eugene,

The maxDepth parameter exists because the implementation uses Integer node IDs which correspond to positions in the binary tree.  This simplified the implementation.  I'd like to eventually modify it to avoid depending on tree node IDs, but that is not yet on the roadmap.

There is not an analogous limit for the GLMs you listed, but I'm not very familiar with the perceptron implementation.

Joseph

On Mon, Dec 14, 2015 at 10:52 AM, Eugene Morozov <evgeny.a.morozov@gmail.com<mailto:evgeny.a.morozov@gmail.com>> wrote:
Hello!

I'm currently working on POC and try to use Random Forest (classification and regression). I also have to check SVM and Multiclass perceptron (other algos are less important at the moment). So far I've discovered that Random Forest has a limitation of maxDepth for trees and just out of curiosity I wonder why such a limitation has been introduced?

An actual question is that I'm going to use Spark ML in production next year and would like to know if there are other limitations like maxDepth in RF for other algorithms: Logistic Regression, Perceptron, SVM, etc.

Thanks in advance for your time.
--
Be well!
Jean Morozov


"
Shixiong Zhu <zsxwing@gmail.com>,"Mon, 4 Jan 2016 13:10:03 -0800",Re: Spark Streaming Application is Stuck Under Heavy Load Due to DeadLock,Rachana Srivastava <Rachana.Srivastava@markmonitor.com>,"Hye Rachana, could you provide the full jstack outputs? Maybe it's same as
https://issues.apache.org/jira/browse/SPARK-11104

Best Regards,
Shixiong Zhu

2016-01-04 12:56 GMT-08:00 Rachana Srivastava <
Rachana.Srivastava@markmonitor.com>:

"
"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Mon, 4 Jan 2016 21:11:37 +0000",RE: Support off-loading computations to a GPU,"Kazuaki Ishizaki <ISHIZAKI@jp.ibm.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","Hi Kazuaki,

Sounds very interesting! Could you elaborate on your benchmark with regards to logistic regression (LR)? Did you compare your implementation with the current implementation of LR in Spark?

Best regards, Alexander

From: Kazuaki Ishizaki [mailto:ISHIZAKI@jp.ibm.com]
Sent: Sunday, January 03, 2016 7:52 PM
To: dev@spark.apache.org
Subject: Support off-loading computations to a GPU

Dear all,

We reopened the existing JIRA entry https://issues.apache.org/jira/browse/SPARK-3785to support off-loading computations to a GPU by adding a description for our prototype. We are working to effectively and easily exploit GPUs on Spark at http://github.com/kiszk/spark-gpu. Please also visit our project page http://kiszk.github.io/spark-gpu/.

For now, we added a new format for a partition in an RDD, which is a column-based structure in an array format, in addition to the current Iterator[T] format with Seq[T]. This reduces data serialization/deserialization and copy overhead between CPU and GPU.

Our prototype achieved more than 3x performance improvement for a simple logistic regression program using a NVIDIA K40 card.

This JIRA entry (SPARK-3785) includes a link to a design document. We are very glad to hear valuable feedback/suggestions/comments and to have great discussions to exploit GPUs in Spark.

Best Regards,
Kazuaki Ishizaki
"
"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Mon, 4 Jan 2016 21:19:14 +0000",RE: Data and Model Parallelism in MLPC,Disha Shrivastava <dishu.905@gmail.com>,"Hi Disha,

Data is stacked into matrices to perform matrix-matrix multiplication (instead of matrix-vector) that is handled by native BLAS and one can get a speed-up. You can refer here for benchmarks https://github.com/fommil/netlib-java

With regards to your second question, data parallelism is handled by Spark RDD, i.e. each worker processes a subset of data partitions, and master serves the role of parameter server.

Best regards, Alexander

From: Disha Shrivastava [mailto:dishu.905@gmail.com]
Sent: Wednesday, December 30, 2015 4:03 AM
To: Ulanov, Alexander
Cc: dev@spark.apache.org
Subject: Re: Data and Model Parallelism in MLPC

Hi,
I went through the code for implementation of MLPC and couldn't understand why stacking/unstacking of the input data has been done. The description says "" Block size for stacking input data in matrices to speed up the computation. Data is stacked within partitions. If block size is more than remaining data in a partition then it is adjusted to the size of this data. Recommended size is between 10 and 1000. Default: 128"". I am not pretty sure what this means and how does this attain speed in computation?
Also, I couldn't find exactly how data parallelism as depicted in http://static.googleusercontent.com/media/research.google.com/hi//archive/large_deep_networks_nips2012.pdf<http://static.googleusercontent.com/media/research.google.com/hi/archive/large_deep_networks_nips2012.pdf> is incorporated in the existing code. There seems to be no notion of parameter server and optimization routine is also normal LBFGS not Sandblaster LBFGS. The only parallelism seems to be coming from the way input data is read and stored.
Please correct me if I am wrong and clarify my doubt.
Thanks and Regards,
Disha

On Tue, Dec 29, 2015 at 5:40 PM, Disha Shrivastava <dishu.905@gmail.com<mailto:dishu.905@gmail.com>> wrote:
Hi Alexander,
Thanks a lot for your response.Yes, I am considering the use case when the weight matrix is too large to fit into the main memory of a single machine.
Can you tell me ways of dividing the weight matrix? According to my investigations so far, we can do this by two ways:

1. By parallelizing the weight matrix RDD using sc.parallelize and then using suitable map functions in the forward and backward pass.
2. By using RowMatrix / BlockMatrix to represent the weight matrix and do calculations on it.
Which of these methods will be efficient to use ? Also, I came across an implementation using Akka where layer-by-layer partitioning of the network has been done (http://alexminnaar.com/implementing-the-distbelief-deep-neural-network-training-framework-with-akka.html) which I believe is model parallelism in the true sense.
Please suggest any other ways/implementation that can help. I would love to hear your remarks on the above.
Thanks and Regards,
Disha

On Wed, Dec 9, 2015 at 1:29 AM, Ulanov, Alexander <alexander.ulanov@hpe.com<mailto:alexander.ulanov@hpe.com>> wrote:
Hi Disha,

Which use case do you have in mind that would require model parallelism? It should have large number of weights, so it could not fit into the memory of a single machine. For example, multilayer perceptron topologies, that are used for speech recognition, have up to 100M of weights. Present hardware is capable of accommodating this in the main memory. That might be a problem for GPUs, but this is a different topic.

The straightforward way of model parallelism for fully connected neural networks is to distribute horizontal (or vertical) blocks of weight matrices across several nodes. That means that the input data has to be reproduced on all these nodes. The forward and the backward passes will require re-assembling the outputs and the errors on each of the nodes after each layer, because each of the node can produce only partial results since it holds a part of weights. According to my estimations, this is inefficient due to large intermediate traffic between the nodes and should be used only if the model does not fit in memory of a single machine. Another way of model parallelism would be to represent the network as the graph and use GraphX to write forward and back propagation. However, this option does not seem very practical to me.

Best regards, Alexander

From: Disha Shrivastava [mailto:dishu.905@gmail.com<mailto:dishu.905@gmail.com>]
Sent: Tuesday, December 08, 2015 11:19 AM
To: Ulanov, Alexander
Cc: dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: Data and Model Parallelism in MLPC

Hi Alexander,
Thanks for your response. Can you suggest ways to incorporate Model Parallelism in MPLC? I am trying to do the same in Spark. I got hold of your post http://apache-spark-developers-list.1001551.n3.nabble.com/Model-parallelism-with-RDD-td13141.html where you have divided the weight matrix into different worker machines. I have two basic questions in this regard:
1. How to actually visualize/analyze and control how nodes of the neural network/ weights are divided across different workers?
2. Is there any alternate way to achieve model parallelism for MPLC in Spark? I believe we need to have some kind of synchronization and control for the updation of weights shared across different workers during backpropagation.
Looking forward for your views on this.
Thanks and Regards,
Disha

On Wed, Dec 9, 2015 at 12:36 AM, Ulanov, Alexander <alexander.ulanov@hpe.com<mailto:alexander.ulanov@hpe.com>> wrote:
Hi Disha,

Multilayer perceptron classifier in Spark implements data parallelism.

Best regards, Alexander

From: Disha Shrivastava [mailto:dishu.905@gmail.com<mailto:dishu.905@gmail.com>]
Sent: Tuesday, December 08, 2015 12:43 AM
To: dev@spark.apache.org<mailto:dev@spark.apache.org>; Ulanov, Alexander
Subject: Data and Model Parallelism in MLPC

Hi,
I would like to know if the implementation of MLPC in the latest released version of Spark ( 1.5.2 ) implements model parallelism and data parallelism as done in the DistBelief model implemented by Google  http://static.googleusercontent.com/media/research.google.com/hi//archive/large_deep_networks_nips2012.pdf<http://static.googleusercontent.com/media/research.google.com/hi/archive/large_deep_networks_nips2012.pdf>
Thanks And Regards,
Disha



"
Yanbo Liang <ybliang8@gmail.com>,"Tue, 5 Jan 2016 11:17:23 +0800",Re: SparkML algos limitations question.,"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Hi Alexander,

That's cool! Thanks for the clarification.

Yanbo

2016-01-05 5:06 GMT+08:00 Ulanov, Alexander <alexander.ulanov@hpe.com>:

e
d
rained the
e
e
r
om
in
"
Jeff Zhang <zjffdu@gmail.com>,"Tue, 5 Jan 2016 12:36:05 +0800",How to execute non-hadoop command ?,dev <dev@spark.apache.org>,"I want to create service check for spark, but spark don't use hadoop script
as launch script. I found other component use ExecuteHadoop to launch
hadoop job to verify the service, I am wondering is there is there any api
for non-hadoop command ? BTW I check the source code of execute_hadoop.py
but don't find how it associates with hadoop

-- 
Best Regards

Jeff Zhang
"
Li Li <fancyerii@gmail.com>,"Tue, 5 Jan 2016 12:42:38 +0800",Re: running lda in spark throws exception,Joseph Bradley <joseph@databricks.com>,"anyone could help? the problem is very easy to reproduce. What's wrong?


---------------------------------------------------------------------


"
Jeff Zhang <zjffdu@gmail.com>,"Tue, 5 Jan 2016 12:49:25 +0800",Re: How to execute non-hadoop command ?,dev <dev@spark.apache.org>,"Sorry, wrong list





-- 
Best Regards

Jeff Zhang
"
Reynold Xin <rxin@databricks.com>,"Mon, 4 Jan 2016 23:17:07 -0800",[discuss] dropping Python 2.6 support,"""dev@spark.apache.org"" <dev@spark.apache.org>, user <user@spark.apache.org>","Does anybody here care about us dropping support for Python 2.6 in Spark
2.0?

Python 2.6 is ancient, and is pretty slow in many aspects (e.g. json
parsing) when compared with Python 2.7. Some libraries that Spark depend on
stopped supporting 2.6. We can still convince the library maintainers to
support 2.6, but it will be extra work. I'm curious if anybody still uses
Python 2.6 to run Spark.

Thanks.
"
Alexander Pivovarov <apivovarov@gmail.com>,"Mon, 4 Jan 2016 23:30:44 -0800",GraphX does not unpersist RDDs,dev@spark.apache.org,"// open spark-shell 1.5.2
// run

import org.apache.spark.graphx._

val vert = sc.parallelize(List((1L, 1), (2L, 2), (3L, 3)), 1)
val edges = sc.parallelize(List(Edge[Long](1L, 2L), Edge[Long](1L, 3L)), 1)

val g0 = Graph(vert, edges)
val g = g0.partitionBy(PartitionStrategy.EdgePartition2D, 2)
val cc = g.connectedComponents()

cc.unpersist()
g.unpersist()
g0.unpersist()
vert.unpersist()
edges.unpersist()

// open http://localhost:4040/storage/
// Spark UI 4040 Storage page still shows 2 items

// VertexRDD Memory Deserialized 1x Replicated 1 100% 1688.0 B 0.0 B 0.0 B
// EdgeRDD Memory Deserialized 1x Replicated 2 100% 4.7 KB 0.0 B 0.0 B
"
,"Tue, 5 Jan 2016 08:52:57 +0100",Re: [discuss] dropping Python 2.6 support,dev@spark.apache.org,"+1

no problem for me to remove Python 2.6 in 2.0.

Thanks
Regards
JB


-- 
jbonofre@apache.org
http://blog.nanthrax.net
Talend - http://www.talend.com

---------------------------------------------------------------------


"
Jian Feng Zhang <jzhang.chs@gmail.com>,"Tue, 5 Jan 2016 16:27:46 +0800",Re: [discuss] dropping Python 2.6 support,"Kushal Datta <kushal.datta@gmail.com>
	dev <dev@spark.apache.org>","+1

We use Python 2.7+ and 3.4+ to call PySpark.

2016-01-05 15:58 GMT+08:00 Kushal Datta <kushal.datta@gmail.com>:

et>
k
d
s


-- 
Best,
Jian Feng
"
yash datta <saucam@gmail.com>,"Tue, 5 Jan 2016 14:16:36 +0530",Re: [discuss] dropping Python 2.6 support,"Jian Feng Zhang <jzhang.chs@gmail.com>
	dev <dev@spark.apache.org>","+1


net>
rk
nd
rs
l



-- 
When events unfold with calm and ease
When the winds that blow are merely breeze
Learn from nature, from birds and bees
Live your life in love, and let joy not cease.
"
Sean Owen <sowen@cloudera.com>,"Tue, 5 Jan 2016 09:28:52 +0000",Re: [discuss] dropping Python 2.6 support,"Reynold Xin <rxin@databricks.com>, Juliet Hougland <juliet@cloudera.com>","+juliet for an additional opinion, but FWIW I think it's safe to say
that future CDH will have a more consistent Python story and that
story will support 2.7 rather than 2.6.


---------------------------------------------------------------------


"
Meethu Mathew <meethu.mathew@flytxt.com>,"Tue, 5 Jan 2016 15:41:45 +0530",Re: [discuss] dropping Python 2.6 support,Reynold Xin <rxin@databricks.com>,"+1
We use Python 2.7

Regards,

Meethu Mathew


"
"""Kazuaki Ishizaki"" <ISHIZAKI@jp.ibm.com>","Tue, 5 Jan 2016 19:35:05 +0900",Re:Support off-loading computations to a GPU,"""Allen Zhang"" <allenzhang010@126.com>","Hi Allen,
Thank you for having an interest.

For quick start, I prepared a new page ""Quick Start"" at 
https://github.com/kiszk/spark-gpu/wiki/Quick-Start. You can install the 
package with two lines and run a sample program with one line.

We mean that ""off-loading"" is to exploit GPU for a task execution of 
Spark. For this, it is necessary to map a task into GPU kernels (While the 
current version requires a programmer to write CUDA code, future versions 
will prepare GPU code from a Spark program automatically). To execute GPU 
kernels requires data copy between CPU and GPU. To reduce data copy 
overhead, our prototype keeps data as a binary representation in RDD using 
a column format.

The current version does not specify the number of CUDA cores for a job by 
using a command line option. There are two ways to specify resources in 
GPU.
1) to specify the number of GPU cards by setting CUDA_VISIBLE_DEVICES in 
conf/spark-env.sh (refer to 
http://devblogs.nvidia.com/parallelforall/cuda-pro-tip-control-gpu-visibility-cuda_visible_devices/
)
2) to specify the number of CUDA threads for processing a partition in a 
program as 
https://github.com/kiszk/spark-gpu/blob/dev/examples/src/main/scala/org/apache/spark/examples/SparkGPULR.scala#L89 
(Sorry for no documentation now).

We are glad to support requested features or to looking forward to getting 
pull requests.
 
Best Regard,
Kazuaki Ishizaki



From:   ""Allen Zhang"" <allenzhang010@126.com>
To:     Kazuaki Ishizaki/Japan/IBM@IBMJP
Cc:     dev@spark.apache.org
Date:   2016/01/04 13:29
Subject:        Re:Support off-loading computations to a GPU



Hi Kazuaki,

I am looking at http://kiszk.github.io/spark-gpu/ , can you point me where 
is the kick-start scripts that I can give it a go?

to be more specifically, what does *""off-loading""* mean? aims to reduce 
the copy overhead between CPU and GPU?
I am a newbie for GPU, how can I specify how many GPU cores I want to use 
(like --executor-cores) ?





Dear all,

We reopened the existing JIRA entry 
https://issues.apache.org/jira/browse/SPARK-3785to support off-loading 
computations to a GPU by adding a description for our prototype. We are 
working to effectively and easily exploit GPUs on Spark at 
http://github.com/kiszk/spark-gpu. Please also visit our project page 
http://kiszk.github.io/spark-gpu/.

For now, we added a new format for a partition in an RDD, which is a 
column-based structure in an array format, in addition to the current 
Iterator[T] format with Seq[T]. This reduces data 
serialization/deserialization and copy overhead between CPU and GPU.

Our prototype achieved more than 3x performance improvement for a simple 
logistic regression program using a NVIDIA K40 card.

This JIRA entry (SPARK-3785) includes a link to a design document. We are 
very glad to hear valuable feedback/suggestions/comments and to have great 
discussions to exploit GPUs in Spark.

Best Regards,
Kazuaki Ishizaki


 


"
"""Kazuaki Ishizaki"" <ISHIZAKI@jp.ibm.com>","Tue, 5 Jan 2016 19:55:38 +0900",RE: Support off-loading computations to a GPU,"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Hi Alexander,
Thank you for having an interest.

We used a LR derived from a Spark sample program 
https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/SparkLR.scala 
(not from mllib or ml). Here are scala source files for GPU and non-GPU 
versions.
GPU: 
https://github.com/kiszk/spark-gpu/blob/dev/examples/src/main/scala/org/apache/spark/examples/SparkGPULR.scala
non-GPU: 
https://github.com/kiszk/spark-gpu/blob/dev/examples/src/main/scala/org/apache/spark/examples/SparkLR.scala

Best Regards,
Kazuaki Ishizaki



From:   ""Ulanov, Alexander"" <alexander.ulanov@hpe.com>
To:     Kazuaki Ishizaki/Japan/IBM@IBMJP, ""dev@spark.apache.org"" 
<dev@spark.apache.org>
Date:   2016/01/05 06:13
Subject:        RE: Support off-loading computations to a GPU



Hi Kazuaki,
 
Sounds very interesting! Could you elaborate on your benchmark with 
regards to logistic regression (LR)? Did you compare your implementation 
with the current implementation of LR in Spark?
 
Best regards, Alexander
 
From: Kazuaki Ishizaki [mailto:ISHIZAKI@jp.ibm.com] 
Sent: Sunday, January 03, 2016 7:52 PM
To: dev@spark.apache.org
Subject: Support off-loading computations to a GPU
 
Dear all,

We reopened the existing JIRA entry 
https://issues.apache.org/jira/browse/SPARK-3785to support off-loading 
computations to a GPU by adding a description for our prototype. We are 
working to effectively and easily exploit GPUs on Spark at 
http://github.com/kiszk/spark-gpu. Please also visit our project page 
http://kiszk.github.io/spark-gpu/.

For now, we added a new format for a partition in an RDD, which is a 
column-based structure in an array format, in addition to the current 
Iterator[T] format with Seq[T]. This reduces data 
serialization/deserialization and copy overhead between CPU and GPU.

Our prototype achieved more than 3x performance improvement for a simple 
logistic regression program using a NVIDIA K40 card.

This JIRA entry (SPARK-3785) includes a link to a design document. We are 
very glad to hear valuable feedback/suggestions/comments and to have great 
discussions to exploit GPUs in Spark.

Best Regards,
Kazuaki Ishizaki


"
"""Allen Zhang"" <allenzhang010@126.com>","Tue, 5 Jan 2016 20:52:10 +0800 (CST)",Re: [discuss] dropping Python 2.6 support,"""Meethu Mathew"" <meethu.mathew@flytxt.com>","plus 1,


we are currently using python 2.7.2 in production environment.






‘⁄ 2016-01-05 18:11:45£¨""Meethu Mathew"" <meethu.mathew@flytxt.com> –¥µ¿£∫

+1
We use Python 2.7


Regards,
 
Meethu Mathew


On Tue, Jan 5, 2016 at 12:47 PM, Reynold Xin <rxin@databricks.com> wrote:

Does anybody here care about us dropping support for Python 2.6 in Spark 2.0? 


Python 2.6 is ancient, and is pretty slow in many aspects (e.g. json parsing) when compared with Python 2.7. Some libraries that Spark depend on stopped supporting 2.6. We can still convince the library maintainers to support 2.6, but it will be extra work. I'm curious if anybody still uses Python 2.6 to run Spark.


Thanks.





"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 05 Jan 2016 13:45:14 +0000",Re: [discuss] dropping Python 2.6 support,"Allen Zhang <allenzhang010@126.com>, Meethu Mathew <meethu.mathew@flytxt.com>","+1

Red Hat supports Python 2.6 on REHL 5 until 2020
<https://alexgaynor.net/2015/mar/30/red-hat-open-source-community/>, but
otherwise yes, Python 2.6 is ancient history and the core Python developers
stopped supporting it in 2013. REHL 5 is not a good e"
Rachana Srivastava <Rachana.Srivastava@markmonitor.com>,"Tue, 5 Jan 2016 16:14:50 +0000",Double Counting When Using Accumulators with Spark Streaming,"""user@spark.apache.org"" <user@spark.apache.org>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","I have a very simple two lines program.  I am getting input from Kafka and save the input in a file and counting the input received.  My code looks like this, when I run this code I am getting two accumulator count for each input.

HashMap<String, String> kafkaParams = new HashMap<String, String>();  kafkaParams.put(""metadata.broker.list"",    ""localhost:9092"");   kafkaParams.put(""zookeeper.connect"", ""localhost:2181"");
JavaPairInputDStream<String, String> messages = KafkaUtils.createDirectStream( jssc, String.class, String.class, StringDecoder.class, StringDecoder.class, kafkaParams, topicsSet);
final Accumulator<Integer> accum = jssc.sparkContext().accumulator(0);
JavaDStream<String> lines = messages.map(
new Function<Tuple2<String, String>, String>() {
               public String call(Tuple2<String, String> tuple2) { accum.add(1); return tuple2._2();
} });
lines.foreachRDD(new Function<JavaRDD<String>, Void>() {
public Void call(JavaRDD<String> rdd) throws Exception {
if(!rdd.isEmpty() || !rdd.partitions().isEmpty()){ rdd.saveAsTextFile(""hdfs://quickstart.cloudera:8020/user/cloudera/testDirJan4/test1.text"");}
System.out.println("" &&&&&&&&&&&&&&&&&&&&& COUNT OF ACCUMULATOR IS "" + accum.value()); return null;}
 });
 jssc.start();

If I comment rdd.saveAsTextFile I get correct count, but with rdd.saveAsTextFile for each input I am getting multiple accumulator count.

Thanks,

Rachana
"
Davies Liu <davies@databricks.com>,"Tue, 5 Jan 2016 10:49:11 -0800",Re: [discuss] dropping Python 2.6 support,Nicholas Chammas <nicholas.chammas@gmail.com>,"+1

on
it
n
txt.com> ÂÜôÈÅìÔºö
:
k
d on
o
es

---------------------------------------------------------------------


"
Juliet Hougland <juliet.hougland@gmail.com>,"Tue, 5 Jan 2016 10:52:00 -0800",Re: [discuss] dropping Python 2.6 support,Nicholas Chammas <nicholas.chammas@gmail.com>,"I don't see a reason Spark 2.0 would need to support Python 2.6. At this
point, Python 3 should be the default that is encouraged.
Most organizations acknowledge the 2.7 is common, but lagging behind the
version they should theoretically use. Dropping python 2.6
support sounds very reasonable to me.

m

rs
txt.com> ÂÜôÈÅìÔºö
:
k
d on
o
es
"
Koert Kuipers <koert@tresata.com>,"Tue, 5 Jan 2016 14:07:31 -0500",Re: [discuss] dropping Python 2.6 support,Juliet Hougland <juliet.hougland@gmail.com>,"rhel/centos 6 ships with python 2.6, doesnt it?

if so, i still know plenty of large companies where python 2.6 is the only
option. asking them for python 2.7 is not going to work

so i think its a bad idea


ers
:
ytxt.com> ÂÜôÈÅìÔºö
nd on
to
ses
"
Ted Yu <yuzhihong@gmail.com>,"Tue, 5 Jan 2016 11:10:23 -0800",Re: [discuss] dropping Python 2.6 support,Davies Liu <davies@databricks.com>,"+1

on
t
n
:
txt.com> ÂÜôÈÅìÔºö
e:
k
d on
o
es

---------------------------------------------------------------------


"
Julio Antonio Soto de Vicente <julio@esbet.es>,"Tue, 5 Jan 2016 20:26:58 +0100",Re: [discuss] dropping Python 2.6 support,Koert Kuipers <koert@tresata.com>,"Unfortunately, Koert is right.

I've been in a couple of projects using Spark (banking industry) where CentOS + Python 2.6 is the toolbox available. 

That said, I believe it should not be a concern for Spark. Python 2.6 is old and busted, which is totally opposite to the Spark philosophy IMO.


:
 option. asking them for python 2.7 is not going to work
oint, Python 3 should be the default that is encouraged.
ersion they should theoretically use. Dropping python 2.6
hon 2.6 is ancient history and the core Python developers stopped supporting it in 2013. REHL 5 is not a good enough reason to continue support for Python 2.6 IMO.
urrently do).
e:
ytxt.com> ÂÜôÈÅìÔºö
te:
rk 2.0? 
arsing) when compared with Python 2.7. Some libraries that Spark depend on stopped supporting 2.6. We can still convince the library maintainers to support 2.6, but it will be extra work. I'm curious if anybody still uses Python 2.6 to run Spark.
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 05 Jan 2016 22:19:34 +0000",Re: [discuss] dropping Python 2.6 support,"Julio Antonio Soto de Vicente <julio@esbet.es>, Koert Kuipers <koert@tresata.com>","As I pointed out in my earlier email, RHEL will support Python 2.6 until
2020. So I'm assuming these large companies will have the option of riding
out Python 2.6 until then.

Are we seriously saying that Spark should likewise support Python 2.6 for
the next several years? Even though the core Python devs stopped supporting
it in 2013?

If that's not what we're suggesting, then when, roughly, can we drop
support? What are the criteria?

I understand the practical concern here. If companies are stuck using 2.6,
it doesn't matter to them that it is deprecated. But balancing that concern
against the maintenance burden on this project, I would say that ""upgrade
to Python 2.7 or stay on Spark 1.6.x"" is a reasonable position to take.
There are many tiny annoyances one has to put up with to support 2.6.

I suppose if our main PySpark contributors are fine putting up with those
annoyances, then maybe we don't need to drop support just yet...

Nick
2016ÎÖÑ 1Ïõî 5Ïùº (Ìôî) Ïò§ÌõÑ 2:27, Julio Antonio Soto de Vicente <julio@esbet.es>ÎãòÏù¥
ÏûëÏÑ±:

≥:
y
m
lytxt.com> ÂÜôÈÅìÔºö
end on
 to
uses
"
Koert Kuipers <koert@tresata.com>,"Tue, 5 Jan 2016 17:33:26 -0500",Re: [discuss] dropping Python 2.6 support,Nicholas Chammas <nicholas.chammas@gmail.com>,"yeah, the practical concern is that we have no control over java or python
version on large company clusters. our current reality for the vast
majority of them is java 7 and python 2.6, no matter how outdated that is.

i dont like it either, but i cannot change it.

we currently don't use pyspark so i have no stake in this, but if we did i
can assure you we would not upgrade to spark 2.x if python 2.6 was dropped.
no point in developing something that doesnt run for majority of customers.

m

g
ng
,
rn
Julio Antonio Soto de Vicente <julio@esbet.es>ÎãòÏù¥
≥:
s
e
e
flytxt.com> ÂÜôÈÅìÔºö
pend on
s to
 uses
"
Josh Rosen <joshrosen@databricks.com>,"Tue, 5 Jan 2016 14:35:36 -0800",Re: [discuss] dropping Python 2.6 support,Koert Kuipers <koert@tresata.com>,"If users are able to install Spark 2.0 on their RHEL clusters, then I
imagine that they're also capable of installing a standalone Python
alongside that Spark version (without changing Python systemwide). For
instance, Anaconda/Miniconda make it really easy to install Python
2.7.x/3.x without impacting / changing the system Python and doesn't
require any special permissions to install (you don't need root / sudo
access). Does this address the Python versioning concerns for RHEL users?


n
.
i
d.
s.
ng
r
ing
to
.6.
e
 Julio Antonio Soto de Vicente <julio@esbet.es>ÎãòÏù¥
s
√≥:
@flytxt.com> ÂÜôÈÅìÔºö
n
epend on
rs to
l uses
"
Koert Kuipers <koert@tresata.com>,"Tue, 5 Jan 2016 17:49:42 -0500",Re: [discuss] dropping Python 2.6 support,Josh Rosen <joshrosen@databricks.com>,"i do not think so.

does the python 2.7 need to be installed on all slaves? if so, we do not
have direct access to those.

also, spark is easy for us to ship with our software since its apache 2
licensed, and it only needs to be present on the machine that launches the
app (thanks to yarn).
even if python 2.7 was needed only on this one machine that launches the
app we can not ship it with our software because its gpl licensed, so the
client would have to download it and install it themselves, and this would
mean its an independent install which has to be audited and approved and
now you are in for a lot of fun. basically it will never happen.


:

st
s.
f
l
ing
t
t
 to
2.6.
, Julio Antonio Soto de Vicente <julio@esbet.es>ÎãòÏù¥
MO.
√≥:
h
w@flytxt.com> ÂÜôÈÅìÔºö
park
y
 if
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 05 Jan 2016 22:57:41 +0000",Re: [discuss] dropping Python 2.6 support,"Koert Kuipers <koert@tresata.com>, Josh Rosen <joshrosen@databricks.com>","even if python 2.7 was needed only on this one machine that launches the
app we can not ship it with our software because its gpl licensed

Not to nitpick, but maybe this is important. The Python license is
GPL-compatible
but not GPL <https://docs.python.org/3/license.html>:

Note GPL-compatible doesn‚Äôt mean that we‚Äôre distributing Python under the
GPL. All Python licenses, unlike the GPL, let you distribute a modified
version without making your changes open source. The GPL-compatible
licenses make it possible to combine Python with other software that is
released under the GPL; the others don‚Äôt.

Nick
‚Äã


e
d
?
ast
is.
d
of
 of
at
at
n to
 2.6.
7, Julio Antonio Soto de Vicente <julio@esbet.es>ÎãòÏù¥
e
IMO.
,
n
gh
e
ew@flytxt.com>
n
Spark
ry
s if
"
Davies Liu <davies@databricks.com>,"Tue, 5 Jan 2016 14:58:33 -0800",Re: [discuss] dropping Python 2.6 support,Koert Kuipers <koert@tresata.com>,"Created JIRA: https://issues.apache.org/jira/browse/SPARK-12661

e
app
ent
its
re
te:
/3.x
ast
is.
d
of
il
ding
at
at
n to
 2.6.
7, Julio Antonio Soto de Vicente
e
IMO.
√≥:
,
d
ue
e
ew@flytxt.com> ÂÜôÈÅìÔºö
n
Spark
ry
s if

---------------------------------------------------------------------


"
Koert Kuipers <koert@tresata.com>,"Tue, 5 Jan 2016 18:02:12 -0500",Re: [discuss] dropping Python 2.6 support,Nicholas Chammas <nicholas.chammas@gmail.com>,"interesting i didnt know that!

m

mpatible
Python under the
he
e
ld
s?
:
vast
 is.
as
 of
n of
hat
hat
on to
t 2.6.
.
27, Julio Antonio Soto de Vicente <julio@esbet.es>ÎãòÏù¥
6
 IMO.
e
d
on
ugh
hew@flytxt.com>
 Spark
ary
us if
"
Koert Kuipers <koert@tresata.com>,"Tue, 5 Jan 2016 18:03:57 -0500",Re: [discuss] dropping Python 2.6 support,Nicholas Chammas <nicholas.chammas@gmail.com>,"if python 2.7 only has to be present on the node that launches the app
(does it?) than that could be important indeed.


ompatible
 Python under the
t
the
e
he
uld
d
rs?
 vast
t is.
was
y of
on of
6
g
that
that
ion to
rt 2.6.
..
:27, Julio Antonio Soto de Vicente <
sophy
t
2.6
/>,
hon
ough
thew@flytxt.com>
m
t Spark
rary
ous if
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 05 Jan 2016 23:05:06 +0000",Re: [discuss] dropping Python 2.6 support,Koert Kuipers <koert@tresata.com>,"I think all the slaves need the same (or a compatible) version of Python
installed since they run Python code in PySpark jobs natively.


ompatible
 Python under the
t
the
e
he
uld
d
rs?
 vast
t is.
was
y of
on of
6
g
that
that
ion to
rt 2.6.
..
:27, Julio Antonio Soto de Vicente <
sophy
t
2.6
/>,
hon
ough
thew@flytxt.com>
m
t Spark
rary
ous if
"
Josh Rosen <joshrosen@databricks.com>,"Tue, 5 Jan 2016 15:07:35 -0800",Re: [discuss] dropping Python 2.6 support,Nicholas Chammas <nicholas.chammas@gmail.com>,"Yep, the driver and executors need to have compatible Python versions. I
think that there are some bytecode-level incompatibilities between 2.6 and
2.7 which would impact the deserialization of Python closures, so I think
you need to be running the same 2.x version for all communicating Spark
processes. Note that you _can_ use a Python 2.7 `ipython` executable on the
driver while continuing to use a vanilla `python` executable on the
executors (we have environment variables which allow you to control these
separately).

m

e
compatible
g Python under
fied
2
 the
 so
is
oved
r
o
ers?
e vast
at is.
 was
ty of
ion of
opped
p
ancing
d say
with to
...
2:27, Julio Antonio Soto de Vicente <
osophy
 2.6
y/>,
thon
nough
m>
athew@flytxt.com>
6
.
at Spark
brary
ious if
"
Josh Rosen <joshrosen@databricks.com>,"Tue, 5 Jan 2016 15:08:39 -0800",Re: [discuss] dropping Python 2.6 support,Nicholas Chammas <nicholas.chammas@gmail.com>,"

Whoops, just to be clear, this should actually read ""while continuing to
use a vanilla `python` 2.7 executable"".

:

d
he
-compatible
ng Python under
ified
s
:
hes
, so
his
roved
I
or
do
sers?
he vast
hat is.
e
6 was
ity of
tion of
topped
lancing
ld say
e
 with to
h
t...
 2:27, Julio Antonio Soto de Vicente <
losophy
.
n 2.6
ty/>,
ython
enough
mathew@flytxt.com>
ries that
 the library
rious if
"
Koert Kuipers <koert@tresata.com>,"Tue, 5 Jan 2016 19:07:55 -0500",Re: [discuss] dropping Python 2.6 support,"""Carlile, Ken"" <carlilek@janelia.hhmi.org>","hey evil admin:)
i think the bit about java was from me?
if so, i meant to indicate that the reality for us is java is 1.7 on most
(all?) clusters. i do not believe spark prefers java 1.8. my point was that
even although java 1.7 is getting old as well it would be a major issue for
me if spark dropped java 1.7 support.


n
e
t
nd
k
the
e
n
L-compatible
ing Python under
dified
is
e
ches
d, so
this
proved
on
For
t
udo
users?
r
the vast
that is.
n 2.6 was
rity of
6
ption of
stopped
alancing
uld say
le
p with to
ust yet...
Ñ 2:27, Julio Antonio Soto de Vicente <
n
ilosophy
s
.
d.
on 2.6
ity/>,
Python
 enough
.
aries that
e the library
urious if
"
Josh Rosen <joshrosen@databricks.com>,"Tue, 5 Jan 2016 16:37:37 -0800",Re: [discuss] dropping Python 2.6 support,Koert Kuipers <koert@tresata.com>,"I don't think that we're planning to drop Java 7 support for Spark 2.0.

Personally, I would recommend using Java 8 if you're running Spark 1.5.0+
and are using SQL/DataFrames so that you can benefit from improvements to
code cache flushing in the Java 8 JVMs. Spark SQL's generated classes can
fill up the JVM's code cache, which causes JIT to stop working for new
bytecode. Empirically, it looks like the Java 8 JVMs have an improved
ability to flush this code cache, thereby avoiding this problem.

TL;DR: I'd prefer to run Java 8 with Spark if given the choice.


at
or
rn
ve
at
I
and
nk
 the
se
:
d
PL-compatible
ting Python under
odified
 is
o
hat
s
ed, so
 this
pproved
.
m
n
hon
 For
't
sudo
 users?
for the
dated that
on 2.6 was
ority of
the option
n
 stopped
balancing
ould say
ble
up with to
just yet...
Ñ 2:27, Julio Antonio Soto de Vicente <
)
Spark
work
ouraged.
hon 2.6
nity/>,
 Python
d enough
t.
raries that
ce the library
curious if
"
Juliet Hougland <juliet.hougland@gmail.com>,"Tue, 5 Jan 2016 17:18:48 -0800",Re: [discuss] dropping Python 2.6 support,Koert Kuipers <koert@tresata.com>,"Most admins I talk to about python and spark are already actively (or on
their way to) managing their cluster python installations. Even if people
begin using the system python with pyspark, there is eventually a user who
needs a complex dependency (like pandas or sklearn) on the cluster. No
admin would muck around installing libs into system python, so you end up
with other python installations.

Installing a non-system python is something users intending to use pyspark
on a real cluster should be thinking about, eventually, anyway. It would
work in situations where people are running pyspark locally or actively
managing python installations on a cluster. There is an awkward middle
point where someone has installed spark but not configured their cluster
(by installing non default python) in any other way. Most clusters I see
are RHEL/CentOS and have something other than system python used by spark.

What libraries stopped supporting python 2.6 and where does spark use them?
The ""ease of transitioning to pyspark onto a cluster"" problem may be an
easier pill to swallow if it only affected something like mllib or spark
sql and not parts of the core api. You end up hoping numpy or pandas are
installed in the runtime components of spark anyway. At that point people
really should just go install a non system python. There are tradeoffs to
using pyspark and I feel pretty fine explaining to people that managing
their cluster's python installations is something that comes with using
pyspark.

RHEL/CentOS is so common that this would probably be a little work for a
lot of people.

--Juliet


at
or
rn
ve
at
I
and
nk
 the
se
:
d
PL-compatible
ting Python under
odified
 is
o
hat
s
ed, so
 this
pproved
.
m
n
hon
 For
't
sudo
 users?
for the
dated that
on 2.6 was
ority of
the option
n
 stopped
balancing
ould say
ble
up with to
just yet...
Ñ 2:27, Julio Antonio Soto de Vicente <
)
Spark
work
ouraged.
hon 2.6
nity/>,
 Python
d enough
t.
raries that
ce the library
curious if
"
Soumitra Johri <soumitra.siddharth@gmail.com>,"Tue, 5 Jan 2016 20:21:23 -0500",UpdateStateByKey : Partitioning and Shuffle,"user <user@spark.apache.org>, dev@spark.apache.org","Hi,

I am relatively new to Spark and am using updateStateByKey() operation to
maintain state in my Spark Streaming application. The input data is coming
through a Kafka topic.

   1. I want to understand how are DStreams partitioned?
   2. How does the partitioning work with mapWithState() or
   updateStatebyKey() method?
   3. In updateStateByKey() does the old state and the new values against a
   given key processed on same node ?
   4. How frequent is the shuffle for updateStateByKey() method ?

The state I have to maintaining contains ~ 100000 keys and I want to avoid
shuffle every time I update the state , any tips to do it ?

Warm Regards
Soumitra
"
Jeff Zhang <zjffdu@gmail.com>,"Wed, 6 Jan 2016 09:40:15 +0800",Re: [discuss] dropping Python 2.6 support,Juliet Hougland <juliet.hougland@gmail.com>,"+1


o
k
.
e
rk
t
hat
for
y
arn
ave
hat
:
o
.6
I
g
able
 the
ese
s
ed
GPL-compatible
uting Python
ute a
er
that
gpl
as to be
y it will
ne Python
. For
n
n't
 sudo
L users?
 for the
tdated that
f
hon 2.6 was
jority of
 the option
hon devs
 balancing
would "
Priya Ch <learnings.chitturi@gmail.com>,"Wed, 6 Jan 2016 08:18:08 +0530",Re: java.io.FileNotFoundException(Too many open files) in Spark streaming,"""user@spark.apache.org"" <user@spark.apache.org>, dev@spark.apache.org","Yes, the fileinputstream is closed. May be i didn't show in the screen shot
.

As spark implements, sort-based shuffle, there is a parameter called
maximum merge factor which decides the number of files that can be merged
at once and this avoids too many open files. I am suspecting that it is
something related to this.

Can someone confirm on this ?


"
Tathagata Das <tdas@databricks.com>,"Tue, 5 Jan 2016 19:01:14 -0800",Re: UpdateStateByKey : Partitioning and Shuffle,Soumitra Johri <soumitra.siddharth@gmail.com>,"Both mapWithState and updateStateByKey by default uses the HashPartitioner,
and hashes the key in the key-value DStream on which the state operation is
applied. The new data and state is partition in the exact same partitioner,
so that same keys from the new data (from the input DStream) get shuffled
and colocated with the already partitioned state RDDs. So the new data is
brought to the corresponding old state in the same machine and then the
state mapping /updating function is applied. The state is not shuffled
every time, only the batches of new data is shuffled in every batch





"
Hamel Kothari <hamelkothari@gmail.com>,"Wed, 06 Jan 2016 03:09:03 +0000",Re: java.io.FileNotFoundException(Too many open files) in Spark streaming,"Priya Ch <learnings.chitturi@gmail.com>, 
	""user@spark.apache.org"" <user@spark.apache.org>, dev@spark.apache.org","The ""Too Many Files"" part of the exception is just indicative of the fact
that when that call was made, too many files were already open. It doesn't
necessarily mean that that line is the source of all of the open files,
that's just the point at which it hit its limit.

What I would recommend is to try to run this code again and use ""lsof"" on
one of the spark executors (perhaps run it in a for loop, writing the
output to separate files) until it fails and see which files are being
opened, if there's anything that seems to be taking up a clear majority
that might key you in on the culprit.


"
"""qinggangwang7@gmail.com"" <qinggangwang7@gmail.com>","Wed, 6 Jan 2016 11:36:34 +0800",executor lost when running sparksql,dev <dev@spark.apache.org>,"Hi all,
  I am running sparksql in hiveql dialect,  the sql is like ""select * from (select * from t1 order by t1.id desc) as ff"".  The sql succeed when it runs only once, but it failed when I run the sql five times at the same time.  It seemed that the thread is dumped and executors are lost.  The problem is not caused by memory or gc, the shufflle data is relative large, but the whole shuffle size is less than 3g onceand 15g five times.  Does anyone have a good idea?

Thanks



qinggangwang7@gmail.com
"
Priya Ch <learnings.chitturi@gmail.com>,"Wed, 6 Jan 2016 14:30:23 +0530",Re: java.io.FileNotFoundException(Too many open files) in Spark streaming,"""user@spark.apache.org"" <user@spark.apache.org>, dev@spark.apache.org","Running 'lsof' will let us know the open files but how do we come to know
the root cause behind opening too many files.

Thanks,
Padma CH


"
Hamel Kothari <hamelkothari@gmail.com>,"Wed, 06 Jan 2016 13:00:50 +0000",Re: java.io.FileNotFoundException(Too many open files) in Spark streaming,"Priya Ch <learnings.chitturi@gmail.com>, 
	""user@spark.apache.org"" <user@spark.apache.org>, dev@spark.apache.org","If you're filling up the number of open files, odds are there's one code
path that's opening most of these files. If that's the case, these files
will likely be named similarly and easy to pick out if you just sort the
output of ""lsof"" once you find the group that is clearly the largest, you
can then backtrack into the source and find the line of code that is
creating them by searching for the filename/folder name in the source.


"
Sean Owen <sowen@cloudera.com>,"Wed, 6 Jan 2016 16:10:47 +0000","Spark 2.x Java API issues: Optional, and Iterator/Iterable",dev <dev@spark.apache.org>,"Here are two interesting issues (with PRs) concerning Java APIs for
Spark 2.x. Details and discussion inside, and comments requested.


https://issues.apache.org/jira/browse/SPARK-3369

This concerns fixing Iterator/Iterable problems in some Java API
methods, to make them consistent with Scala. This allows real
streaming operation in, say, flatMap, but it's an API change that's
definitely going to make people change their code.


https://issues.apache.org/jira/browse/SPARK-4819

This concerns the fate of Guava Optional in the Java API, and what to
replace it with.

---------------------------------------------------------------------


"
Alexander Pivovarov <apivovarov@gmail.com>,"Wed, 6 Jan 2016 13:45:31 -0800",Re: GraphX does not unpersist RDDs,dev@spark.apache.org,"The same issue exists in spark-1.6.0

I've opened Jira ticket for that
https://issues.apache.org/jira/browse/SPARK-12655


"
Priya Ch <learnings.chitturi@gmail.com>,"Thu, 7 Jan 2016 09:09:38 +0530",Re: java.io.FileNotFoundException(Too many open files) in Spark streaming,"""user@spark.apache.org"" <user@spark.apache.org>, dev@spark.apache.org","The line of code which I highlighted in the screenshot is within the spark
source code. Spark implements sort-based shuffle implementation and the
spilled files are merged using the merge sort.

Here is the link
https://issues.apache.org/jira/secure/attachment/12655884/Sort-basedshuffledesign.pdf
which would convey the same.


"
Wail Alkowaileet <wael.y.k@gmail.com>,"Thu, 7 Jan 2016 09:27:35 +0300",Dataset throws: Task not serializable,dev@spark.apache.org,"Hey,

I got an error when trying to map a Dataset df.as[CLASS] when I have some
nested case classes
I'm not sure if it's a bug ... or I did something wrong... or I missed some
configuration.


I did the following:

*input snapshot*

{
  ""count"": ""string"",
  ""name"": [{
    ""addr_no"": ""string"",
    ""dais_id"": ""string"",
    ""display_name"": ""string"",
    ""first_name"": ""string"",
    ""full_name"": ""string"",
    ""last_name"": ""string"",
    ""r_id"": ""string"",
    ""reprint"": ""string"",
    ""role"": ""string"",
    ""seq_no"": ""string"",
    ""suffix"": ""string"",
    ""wos_standard"": ""string""
  }]
}

*Case classes:*

case class listType1(addr_no:String, dais_id:String,
display_name:String, first_name:String, full_name:String,
last_name:String, r_id:String, reprint:String, role:String,
seq_no:String, suffix:String, wos_standard:String)
case class DatasetType1(count:String, name:Array[listType1])

*Schema:*
root
 |-- count: string (nullable = true)
 |-- name: array (nullable = true)
 |    |-- element: struct (containsNull = true)
 |    |    |-- addr_no: string (nullable = true)
 |    |    |-- dais_id: string (nullable = true)
 |    |    |-- display_name: string (nullable = true)
 |    |    |-- first_name: string (nullable = true)
 |    |    |-- full_name: string (nullable = true)
 |    |    |-- last_name: string (nullable = true)
 |    |    |-- r_id: string (nullable = true)
 |    |    |-- reprint: string (nullable = true)
 |    |    |-- role: string (nullable = true)
 |    |    |-- seq_no: string (nullable = true)
 |    |    |-- suffix: string (nullable = true)
 |    |    |-- wos_standard: string (nullable = true)

*Scala code:*

import sqlContext.implicits._

val ds = df.as[DatasetType1]

//Taking first() works fine
println(ds.first().count)

//map() then first throws exception
println(ds.map(x => x.count).first())


*Exception Message:*
Exception in thread ""main"" org.apache.spark.SparkException: Task not
serializable
at
org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:304)
at
org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:294)
at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:122)
at org.apache.spark.SparkContext.clean(SparkContext.scala:2055)
at org.apache.spark.SparkContext.runJob(SparkContext.scala:1857)
at org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)
at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:927)
at
org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
at
org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
at org.apache.spark.rdd.RDD.collect(RDD.scala:926)
at org.apache.spark.sql.Dataset.collect(Dataset.scala:668)
at main.main$.testAsterixRDDWithSparkSQL(main.scala:63)
at main.main$.main(main.scala:70)
at main.main.main(main.scala)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:497)
at com.intellij.rt.execution.application.AppMain.main(AppMain.java:140)
Caused by: java.io.NotSerializableException:
scala.reflect.internal.Symbols$PackageClassSymbol
Serialization stack:
- object not serializable (class:
scala.reflect.internal.Symbols$PackageClassSymbol, value: package main)
- field (class: scala.reflect.internal.Types$ThisType, name: sym, type:
class scala.reflect.internal.Symbols$Symbol)
- object (class scala.reflect.internal.Types$UniqueThisType, main.type)
- field (class: scala.reflect.internal.Types$TypeRef, name: pre, type:
class scala.reflect.internal.Types$Type)
- object (class scala.reflect.internal.Types$TypeRef$$anon$6,
main.listType1)
- field (class:
org.apache.spark.sql.catalyst.ScalaReflection$$anonfun$org$apache$spark$sql$catalyst$ScalaReflection$$constructorFor$2,
name: elementType$1, type: class scala.reflect.api.Types$TypeApi)
- object (class
org.apache.spark.sql.catalyst.ScalaReflection$$anonfun$org$apache$spark$sql$catalyst$ScalaReflection$$constructorFor$2,
<function0>)
- field (class:
org.apache.spark.sql.catalyst.ScalaReflection$$anonfun$org$apache$spark$sql$catalyst$ScalaReflection$$constructorFor$2$$anonfun$apply$1,
name: $outer, type: class
org.apache.spark.sql.catalyst.ScalaReflection$$anonfun$org$apache$spark$sql$catalyst$ScalaReflection$$constructorFor$2)
- object (class
org.apache.spark.sql.catalyst.ScalaReflection$$anonfun$org$apache$spark$sql$catalyst$ScalaReflection$$constructorFor$2$$anonfun$apply$1,
<function1>)
- field (class: org.apache.spark.sql.catalyst.expressions.MapObjects, name:
function, type: interface scala.Function1)
- object (class org.apache.spark.sql.catalyst.expressions.MapObjects,
mapobjects(<function1>,cast(name#1 as
array<struct<display_name:string,first_name:string,full_name:string,reprint:string,role:string,wos_standard:string,last_name:string,dais_id:string,seq_no:string,suffix:string,r_id:string,addr_no:string>>),StructField(display_name,StringType,true),StructField(first_name,StringType,true),StructField(full_name,StringType,true),StructField(reprint,StringType,true),StructField(role,StringType,true),StructField(wos_standard,StringType,true),StructField(last_name,StringType,true),StructField(dais_id,StringType,true),StructField(seq_no,StringType,true),StructField(suffix,StringType,true),StructField(r_id,StringType,true),StructField(addr_no,StringType,true)))
- field (class: org.apache.spark.sql.catalyst.expressions.Invoke, name:
targetObject, type: class
org.apache.spark.sql.catalyst.expressions.Expression)
- object (class org.apache.spark.sql.catalyst.expressions.Invoke,
invoke(mapobjects(<function1>,cast(name#1 as
array<struct<display_name:string,first_name:string,full_name:string,reprint:string,role:string,wos_standard:string,last_name:string,dais_id:string,seq_no:string,suffix:string,r_id:string,addr_no:string>>),StructField(display_name,StringType,true),StructField(first_name,StringType,true),StructField(full_name,StringType,true),StructField(reprint,StringType,true),StructField(role,StringType,true),StructField(wos_standard,StringType,true),StructField(last_name,StringType,true),StructField(dais_id,StringType,true),StructField(seq_no,StringType,true),StructField(suffix,StringType,true),StructField(r_id,StringType,true),StructField(addr_no,StringType,true)),array,ObjectType(class
[Lmain.listType1;)))
- writeObject data (class: scala.collection.immutable.$colon$colon)
- object (class scala.collection.immutable.$colon$colon,
List(invoke(count#0,toString,ObjectType(class java.lang.String)),
invoke(mapobjects(<function1>,cast(name#1 as
array<struct<display_name:string,first_name:string,full_name:string,reprint:string,role:string,wos_standard:string,last_name:string,dais_id:string,seq_no:string,suffix:string,r_id:string,addr_no:string>>),StructField(display_name,StringType,true),StructField(first_name,StringType,true),StructField(full_name,StringType,true),StructField(reprint,StringType,true),StructField(role,StringType,true),StructField(wos_standard,StringType,true),StructField(last_name,StringType,true),StructField(dais_id,StringType,true),StructField(seq_no,StringType,true),StructField(suffix,StringType,true),StructField(r_id,StringType,true),StructField(addr_no,StringType,true)),array,ObjectType(class
[Lmain.listType1;))))
- field (class: org.apache.spark.sql.catalyst.expressions.NewInstance,
name: arguments, type: interface scala.collection.Seq)
- object (class org.apache.spark.sql.catalyst.expressions.NewInstance,
newinstance(class
main.DatasetType1,invoke(count#0,toString,ObjectType(class
java.lang.String)),invoke(mapobjects(<function1>,cast(name#1 as
array<struct<display_name:string,first_name:string,full_name:string,reprint:string,role:string,wos_standard:string,last_name:string,dais_id:string,seq_no:string,suffix:string,r_id:string,addr_no:string>>),StructField(display_name,StringType,true),StructField(first_name,StringType,true),StructField(full_name,StringType,true),StructField(reprint,StringType,true),StructField(role,StringType,true),StructField(wos_standard,StringType,true),StructField(last_name,StringType,true),StructField(dais_id,StringType,true),StructField(seq_no,StringType,true),StructField(suffix,StringType,true),StructField(r_id,StringType,true),StructField(addr_no,StringType,true)),array,ObjectType(class
[Lmain.listType1;)),false,ObjectType(class main.DatasetType1),None))
- field (class: org.apache.spark.sql.catalyst.encoders.ExpressionEncoder,
name: fromRowExpression, type: class
org.apache.spark.sql.catalyst.expressions.Expression)
- object (class org.apache.spark.sql.catalyst.encoders.ExpressionEncoder,
class[count[0]: string,
name#ExprId(4,879d493d-efa1-4799-8fc4-5872ccb3b07b):
array<struct<display_name:string,first_name:string,full_name:string,reprint:string,role:string,wos_standard:string,last_name:string,dais_id:string,seq_no:string,suffix:string,r_id:string,addr_no:string>>])
- field (class: org.apache.spark.sql.execution.MapPartitions, name:
tEncoder, type: class
org.apache.spark.sql.catalyst.encoders.ExpressionEncoder)
- object (class org.apache.spark.sql.execution.MapPartitions,
!MapPartitions <function1>, class[count[0]: string,
name#ExprId(4,879d493d-efa1-4799-8fc4-5872ccb3b07b):
array<struct<display_name:string,first_name:string,full_name:string,reprint:string,role:string,wos_standard:string,last_name:string,dais_id:string,seq_no:string,suffix:string,r_id:string,addr_no:string>>],
class[value[0]: string], [value#10]
+- ConvertToSafe
   +- Scan JSONRelation[count#0,name#1] InputPaths:
)
- field (class: org.apache.spark.sql.execution.MapPartitions$$anonfun$8,
name: $outer, type: class org.apache.spark.sql.execution.MapPartitions)
- object (class org.apache.spark.sql.execution.MapPartitions$$anonfun$8,
<function1>)
- field (class: org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1,
name: f$22, type: interface scala.Function1)
- object (class org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1,
<function0>)
- field (class:
org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$21,
name: $outer, type: class
org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1)
- object (class
org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$21,
<function3>)
- field (class: org.apache.spark.rdd.MapPartitionsRDD, name: f, type:
interface scala.Function3)
- object (class org.apache.spark.rdd.MapPartitionsRDD, MapPartitionsRDD[6]
at collect at main.scala:63)
- field (class: org.apache.spark.NarrowDependency, name: _rdd, type: class
org.apache.spark.rdd.RDD)
- writeObject data (class: scala.collection.immutable.$colon$colon)
- object (class scala.collection.immutable.$colon$colon,
- field (class: org.apache.spark.rdd.RDD, name:
org$apache$spark$rdd$RDD$$dependencies_, type: interface
scala.collection.Seq)
- object (class org.apache.spark.rdd.MapPartitionsRDD, MapPartitionsRDD[7]
at collect at main.scala:63)
- field (class: org.apache.spark.rdd.RDD$$anonfun$collect$1, name: $outer,
type: class org.apache.spark.rdd.RDD)
- object (class org.apache.spark.rdd.RDD$$anonfun$collect$1, <function0>)
- field (class: org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12,
name: $outer, type: class org.apache.spark.rdd.RDD$$anonfun$collect$1)
- object (class org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12,
<function1>)
at
org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:40)
at
org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:47)
at
org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:101)
at
org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:301)
... 19 more



*P.S* mapping by name with case classes doesn't work if the order of the
fields of a case class doesn't match with the order of the DataFrame's
schema.


-- 

*Regards,*
Wail Alkowaileet
"
Jacek Laskowski <jacek@japila.pl>,"Thu, 7 Jan 2016 08:08:13 +0100",BUILD FAILURE for Scala 2.11?,dev <dev@spark.apache.org>,"Hi,

I'm building the sources with Scala 2.11 and today the command fails:

‚ûú  spark git:(master) ‚úó ./build/mvn -Pyarn -Phadoop-2.6
-Dhadoop.version=2.7.1 -Dscala-2.11 -Phive -Phive-thriftserver
-DskipTests clean install
...
[INFO] Spark Project SQL .................................. FAILURE [  9.319 s]
...
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------

The reason being:

[error] /Users/jacek/dev/oss/spark/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JSONRelation.scala:66:
in class JSONRelation, multiple overloaded alternatives of constructor
JSONRelation define default arguments.
[error] private[sql] class JSONRelation(
[error]                    ^

Is anyone working on it already?

Pozdrawiam,
Jacek

Jacek Laskowski | https://medium.com/@jaceklaskowski/
Mastering Apache Spark
==> https://jaceklaskowski.gitbooks.io/mastering-apache-spark/
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Wed, 6 Jan 2016 23:09:04 -0800",Re: BUILD FAILURE for Scala 2.11?,Jacek Laskowski <jacek@japila.pl>,"I don't think so - please submit a fix. Thanks.



xecution/datasources/json/JSONRelation.scala:66:
"
Jacek Laskowski <jacek@japila.pl>,"Thu, 7 Jan 2016 08:10:33 +0100",Re: BUILD FAILURE for Scala 2.11?,Reynold Xin <rxin@databricks.com>,"
Pozdrawiam,
Jacek

Jacek Laskowski | https://medium.com/@jaceklaskowski/
Mastering Apache Spark
==> https://jaceklaskowski.gitbooks.io/mastering-apache-spark/
Follow me at https://twitter.com/jaceklaskowski


execution/datasources/json/JSONRelation.scala:66:

---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Thu, 7 Jan 2016 08:38:48 +0100",Re: BUILD FAILURE for Scala 2.11?,Reynold Xin <rxin@databricks.com>,"Hi,

Done. See https://github.com/apache/spark/pull/10636

Pozdrawiam,
Jacek

Jacek Laskowski | https://medium.com/@jaceklaskowski/
Mastering Apache Spark
==> https://jaceklaskowski.gitbooks.io/mastering-apache-spark/
Follow me at https://twitter.com/jaceklaskowski


:
-
-
/execution/datasources/json/JSONRelation.scala:66:

---------------------------------------------------------------------


"
=?UTF-8?Q?Mathieu_Br=C3=A9dif?= <mathieub@gmail.com>,"Thu, 7 Jan 2016 18:46:21 +0100",UserDefinedNumericType ??,dev@spark.apache.org,"Dear all,

I would like to define a user-defined numeric type to represent scaled
integers (with offsets) : (intVal * _scale + _offset) similarly to
DecimalType's (longVal / 10 ** _scale).

I managed to work it through up to this point :
https://github.com/IGNF/spark-iqmulus/blob/master/src/main/scala/fr/ign/spark/iqmulus/ScaledIntegerType.scala

However, I had to code this within ""package
org.apache.spark.sql.types"" to access private methods and I cannot
figure out how to make it aware of the arithmetic operators which are
not functions such as infix operators (+,*...). It seems that I have
to inherit from both NumericType and UserDefinedType...

I think my concern boils down to this : ""Could DecimalType have been
implemented as a userdefined type and if so, how?"" or more generally
""How to code a UserDefinedNumericType ?""

Any clue/advice?

Thanks
Mathieu

PS : Spark-IQmulus is a spark package for lidar point cloud
processing. For now it includes: LAS, PLY and XYZ Datasources, SQL
Strategies will be optimized using file header metadata such as
bounding boxes...

---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Thu, 7 Jan 2016 10:34:22 -0800",Re: Dataset throws: Task not serializable,Wail Alkowaileet <wael.y.k@gmail.com>,"Thanks for providing a great description.  I've opened
https://issues.apache.org/jira/browse/SPARK-12696

I'm actually getting a different error (running in notebooks though).
Something seems wrong either way.



We have tests for reordering
<https://github.com/apache/spark/blob/master/sql/core/src/test/scala/org/apache/spark/sql/DatasetSuite.scala#L97>
can
you provide a smaller reproduction of this problem?


 first_name:String, full_name:String, last_name:String, r_id:String, reprint:String, role:String, seq_no:String, suffix:String, wos_standard:String)
cala:304)
r$$clean(ClosureCleaner.scala:294)
:150)
:111)
:62)
mpl.java:43)
ql$catalyst$ScalaReflection$$constructorFor$2,
ql$catalyst$ScalaReflection$$constructorFor$2,
ql$catalyst$ScalaReflection$$constructorFor$2$$anonfun$apply$1,
ql$catalyst$ScalaReflection$$constructorFor$2)
ql$catalyst$ScalaReflection$$constructorFor$2$$anonfun$apply$1,
nt:string,role:string,wos_standard:string,last_name:string,dais_id:string,seq_no:string,suffix:string,r_id:string,addr_no:string>>),StructField(display_name,StringType,true),StructField(first_name,StringType,true),StructField(full_name,StringType,true),StructField(reprint,StringType,true),StructField(role,StringType,true),StructField(wos_standard,StringType,true),StructField(last_name,StringType,true),StructField(dais_id,StringType,true),StructField(seq_no,StringType,true),StructField(suffix,StringType,true),StructField(r_id,StringType,true),StructField(addr_no,StringType,true)))
nt:string,role:string,wos_standard:string,last_name:string,dais_id:string,seq_no:string,suffix:string,r_id:string,addr_no:string>>),StructField(display_name,StringType,true),StructField(first_name,StringType,true),StructField(full_name,StringType,true),StructField(reprint,StringType,true),StructField(role,StringType,true),StructField(wos_standard,StringType,true),StructField(last_name,StringType,true),StructField(dais_id,StringType,true),StructField(seq_no,StringType,true),StructField(suffix,StringType,true),StructField(r_id,StringType,true),StructField(addr_no,StringType,true)),array,ObjectType(class
nt:string,role:string,wos_standard:string,last_name:string,dais_id:string,seq_no:string,suffix:string,r_id:string,addr_no:string>>),StructField(display_name,StringType,true),StructField(first_name,StringType,true),StructField(full_name,StringType,true),StructField(reprint,StringType,true),StructField(role,StringType,true),StructField(wos_standard,StringType,true),StructField(last_name,StringType,true),StructField(dais_id,StringType,true),StructField(seq_no,StringType,true),StructField(suffix,StringType,true),StructField(r_id,StringType,true),StructField(addr_no,StringType,true)),array,ObjectType(class
nt:string,role:string,wos_standard:string,last_name:string,dais_id:string,seq_no:string,suffix:string,r_id:string,addr_no:string>>),StructField(display_name,StringType,true),StructField(first_name,StringType,true),StructField(full_name,StringType,true),StructField(reprint,StringType,true),StructField(role,StringType,true),StructField(wos_standard,StringType,true),StructField(last_name,StringType,true),StructField(dais_id,StringType,true),StructField(seq_no,StringType,true),StructField(suffix,StringType,true),StructField(r_id,StringType,true),StructField(addr_no,StringType,true)),array,ObjectType(class
nt:string,role:string,wos_standard:string,last_name:string,dais_id:string,seq_no:string,suffix:string,r_id:string,addr_no:string>>])
nt:string,role:string,wos_standard:string,last_name:string,dais_id:string,seq_no:string,suffix:string,r_id:string,addr_no:string>>],
,
,
21,
21,
]
s
]
,
lizationDebugger.scala:40)
lizer.scala:47)
er.scala:101)
cala:301)
"
Michael Armbrust <michael@databricks.com>,"Thu, 7 Jan 2016 11:03:25 -0800",Re: Dataset throws: Task not serializable,Wail Alkowaileet <wael.y.k@gmail.com>,"Were you running in the REPL?


apache/spark/sql/DatasetSuite.scala#L97> can
, first_name:String, full_name:String, last_name:String, r_id:String, reprint:String, role:String, seq_no:String, suffix:String, wos_standard:String)
scala:304)
er$$clean(ClosureCleaner.scala:294)
a:150)
a:111)
a:62)
Impl.java:43)
sql$catalyst$ScalaReflection$$constructorFor$2,
sql$catalyst$ScalaReflection$$constructorFor$2,
sql$catalyst$ScalaReflection$$constructorFor$2$$anonfun$apply$1,
sql$catalyst$ScalaReflection$$constructorFor$2)
sql$catalyst$ScalaReflection$$constructorFor$2$$anonfun$apply$1,
int:string,role:string,wos_standard:string,last_name:string,dais_id:string,seq_no:string,suffix:string,r_id:string,addr_no:string>>),StructField(display_name,StringType,true),StructField(first_name,StringType,true),StructField(full_name,StringType,true),StructField(reprint,StringType,true),StructField(role,StringType,true),StructField(wos_standard,StringType,true),StructField(last_name,StringType,true),StructField(dais_id,StringType,true),StructField(seq_no,StringType,true),StructField(suffix,StringType,true),StructField(r_id,StringType,true),StructField(addr_no,StringType,true)))
int:string,role:string,wos_standard:string,last_name:string,dais_id:string,seq_no:string,suffix:string,r_id:string,addr_no:string>>),StructField(display_name,StringType,true),StructField(first_name,StringType,true),StructField(full_name,StringType,true),StructField(reprint,StringType,true),StructField(role,StringType,true),StructField(wos_standard,StringType,true),StructField(last_name,StringType,true),StructField(dais_id,StringType,true),StructField(seq_no,StringType,true),StructField(suffix,StringType,true),StructField(r_id,StringType,true),StructField(addr_no,StringType,true)),array,ObjectType(class
int:string,role:string,wos_standard:string,last_name:string,dais_id:string,seq_no:string,suffix:string,r_id:string,addr_no:string>>),StructField(display_name,StringType,true),StructField(first_name,StringType,true),StructField(full_name,StringType,true),StructField(reprint,StringType,true),StructField(role,StringType,true),StructField(wos_standard,StringType,true),StructField(last_name,StringType,true),StructField(dais_id,StringType,true),StructField(seq_no,StringType,true),StructField(suffix,StringType,true),StructField(r_id,StringType,true),StructField(addr_no,StringType,true)),array,ObjectType(class
int:string,role:string,wos_standard:string,last_name:string,dais_id:string,seq_no:string,suffix:string,r_id:string,addr_no:string>>),StructField(display_name,StringType,true),StructField(first_name,StringType,true),StructField(full_name,StringType,true),StructField(reprint,StringType,true),StructField(role,StringType,true),StructField(wos_standard,StringType,true),StructField(last_name,StringType,true),StructField(dais_id,StringType,true),StructField(seq_no,StringType,true),StructField(suffix,StringType,true),StructField(r_id,StringType,true),StructField(addr_no,StringType,true)),array,ObjectType(class
,
,
int:string,role:string,wos_standard:string,last_name:string,dais_id:string,seq_no:string,suffix:string,r_id:string,addr_no:string>>])
int:string,role:string,wos_standard:string,last_name:string,dais_id:string,seq_no:string,suffix:string,r_id:string,addr_no:string>>],
$21,
$21,
)
alizationDebugger.scala:40)
alizer.scala:47)
zer.scala:101)
scala:301)
"
Juliet Hougland <juliet.hougland@gmail.com>,"Thu, 7 Jan 2016 11:55:38 -0800",Re: [discuss] dropping Python 2.6 support,Jeff Zhang <zjffdu@gmail.com>,"@ Reynold Xin @Josh Rosen: What is current maintenance burden of supporting
Python 2.6? What libraries are no longer supporting Python 2.6 and where
does Spark use them?



m
e
ho
p
It
rs
y
be
ark
e
o
 was
o set
hen
tand
r
s
.
2.6
 I
ng
table
n the
hese
."
Bryan Cutler <cutlerb@gmail.com>,"Fri, 8 Jan 2016 14:21:41 -0800",Re: running lda in spark throws exception,Li Li <fancyerii@gmail.com>,"Hi Li,

I tried out your code and sample data in both local mode and Spark
Standalone and it ran correctly with output that looks good.  Sorry, I
don't have a YARN cluster setup right now, so maybe the error you are
seeing is specific to that.  Btw, I am running the latest Spark code from
the master branch.  Hope that helps some!

Bryan


"
Li Li <fancyerii@gmail.com>,"Sat, 9 Jan 2016 14:38:39 +0800",Re: running lda in spark throws exception,Bryan Cutler <cutlerb@gmail.com>,"I am running it in 1.5.2. I will try running it in small standalone
cluster to see whether it's correct.


---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Sat, 9 Jan 2016 10:59:14 +0100",NoClassDefFoundError when starting standalone Master,dev <dev@spark.apache.org>,"Hi,

With today's sources I'm facing ""NoClassDefFoundError:
org/spark-project/guava/collect/Maps"" while starting standalone Master
using ./sbin/start-master.sh.

Anyone's working on it? File an issue?

Spark Command: /Library/Java/JavaVirtualMachines/Current/Contents/Home/bin/java
-cp /Users/jacek/dev/oss/spark/conf/:/Users/jacek/dev/oss/spark/assembly/target/scala-2.11/spark-assembly-2.0.0-SNAPSHOT-hadoop2.7.1.jar:/Users/jacek/dev/oss/spark/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/Users/jacek/dev/oss/spark/lib_managed/jars/datanucleus-core-3.2.10.jar:/Users/jacek/dev/oss/spark/lib_managed/jars/datanucleus-rdbms-3.2.9.jar
-Xms1g -Xmx1g org.apache.spark.deploy.master.Master --ip japila.local
--port 7077 --webui-port 8080
========================================
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel).
Exception in thread ""main"" java.lang.NoClassDefFoundError:
org/spark-project/guava/collect/Maps
        at org.apache.hadoop.metrics2.lib.MetricsRegistry.<init>(MetricsRegistry.java:42)
        at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.<init>(MetricsSystemImpl.java:94)
        at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.<init>(MetricsSystemImpl.java:141)
        at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.<init>(DefaultMetricsSystem.java:38)
        at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.<clinit>(DefaultMetricsSystem.java:36)
        at org.apache.hadoop.security.UserGroupInformation$UgiMetrics.create(UserGroupInformation.java:120)
        at org.apache.hadoop.security.UserGroupInformation.<clinit>(UserGroupInformation.java:236)
        at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2156)
        at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2156)
        at scala.Option.getOrElse(Option.scala:121)
        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2156)
        at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:214)
        at org.apache.spark.deploy.master.Master$.startRpcEnvAndEndpoint(Master.scala:1108)
        at org.apache.spark.deploy.master.Master$.main(Master.scala:1093)
        at org.apache.spark.deploy.master.Master.main(Master.scala)
Caused by: java.lang.ClassNotFoundException:
org.spark-project.guava.collect.Maps
        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        ... 15 more

Pozdrawiam,
Jacek

Jacek Laskowski | https://medium.com/@jaceklaskowski/
Mastering Apache Spark
==> https://jaceklaskowski.gitbooks.io/mastering-apache-spark/
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Sat, 9 Jan 2016 11:17:59 +0100",Re: NoClassDefFoundError when starting standalone Master,dev <dev@spark.apache.org>,"Hi,

I think the change is related:
https://github.com/apache/spark/commit/659fd9d04b988d48960eac4f352ca37066f43f5c
as it touches the dependency in pom.xml.

Pozdrawiam,
Jacek

Jacek Laskowski | https://medium.com/@jaceklaskowski/
Mastering Apache Spark
==> https://jaceklaskowski.gitbooks.io/mastering-apache-spark/
Follow me at https://twitter.com/jaceklaskowski


n/java
target/scala-2.11/spark-assembly-2.0.0-SNAPSHOT-hadoop2.7.1.jar:/Users/jacek/dev/oss/spark/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/Users/jacek/dev/oss/spark/lib_managed/jars/datanucleus-core-3.2.10.jar:/Users/jacek/dev/oss/spark/lib_managed/jars/datanucleus-rdbms-3.2.9.jar
================
egistry.java:42)
csSystemImpl.java:94)
csSystemImpl.java:141)
aultMetricsSystem.java:38)
efaultMetricsSystem.java:36)
ate(UserGroupInformation.java:120)
roupInformation.java:236)
y(Utils.scala:2156)
y(Utils.scala:2156)
56)
214)
Master.scala:1108)

---------------------------------------------------------------------


"
Steve Loughran <stevel@hortonworks.com>,"Sat, 9 Jan 2016 11:25:27 +0000",Re: [discuss] dropping Python 2.6 support,,"
> On 7 Jan 2016, at 19:55, Juliet Hougland <juliet.hougland@gmail.com> wrote:
> 
> @ Reynold Xin @Josh Rosen: What is current maintenance burden of supporting Python 2.6? What libraries are no longer supporting Python 2.6 and where does Spark use them?
> 

generally the cost comes in the test matrix: one more thing to test against. You can test for the extremes with the right VMs (me: kerberos-java7-linux) (windows-server+java-8) but you still need to keep those combinations down ‚Äîand be setup to locally debug/replicate problems. 


"
Jacek Laskowski <jacek@japila.pl>,"Sat, 9 Jan 2016 12:33:07 +0100",Re: NoClassDefFoundError when starting standalone Master,dev <dev@spark.apache.org>,"Figured it out and reported
https://issues.apache.org/jira/browse/SPARK-12736. Fix's coming...

Pozdrawiam,
Jacek

Jacek Laskowski | https://medium.com/@jaceklaskowski/
Mastering Apache Spark
==> https://jaceklaskowski.gitbooks.io/mastering-apache-spark/
Follow me at https://twitter.com/jaceklaskowski


f43f5c
in/java
/target/scala-2.11/spark-assembly-2.0.0-SNAPSHOT-hadoop2.7.1.jar:/Users/jacek/dev/oss/spark/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/Users/jacek/dev/oss/spark/lib_managed/jars/datanucleus-core-3.2.10.jar:/Users/jacek/dev/oss/spark/lib_managed/jars/datanucleus-rdbms-3.2.9.jar
================
Registry.java:42)
icsSystemImpl.java:94)
icsSystemImpl.java:141)
faultMetricsSystem.java:38)
DefaultMetricsSystem.java:36)
eate(UserGroupInformation.java:120)
GroupInformation.java:236)
ly(Utils.scala:2156)
ly(Utils.scala:2156)
156)
:214)
(Master.scala:1108)
)

---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Sat, 9 Jan 2016 12:37:50 +0100",Re: NoClassDefFoundError when starting standalone Master,dev <dev@spark.apache.org>,"Hi,

https://github.com/apache/spark/pull/10674

Please review and merge at your convenience. Thanks!

Pozdrawiam,
Jacek

Jacek Laskowski | https://medium.com/@jaceklaskowski/
Mastering Apache Spark
==> https://jaceklaskowski.gitbooks.io/mastering-apache-spark/
Follow me at https://twitter.com/jaceklaskowski

6f43f5c
:
bin/java
y/target/scala-2.11/spark-assembly-2.0.0-SNAPSHOT-hadoop2.7.1.jar:/Users/jacek/dev/oss/spark/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/Users/jacek/dev/oss/spark/lib_managed/jars/datanucleus-core-3.2.10.jar:/Users/jacek/dev/oss/spark/lib_managed/jars/datanucleus-rdbms-3.2.9.jar
=================
sRegistry.java:42)
ricsSystemImpl.java:94)
ricsSystemImpl.java:141)
efaultMetricsSystem.java:38)
(DefaultMetricsSystem.java:36)
reate(UserGroupInformation.java:120)
rGroupInformation.java:236)
ply(Utils.scala:2156)
ply(Utils.scala:2156)
2156)
a:214)
t(Master.scala:1108)
3)
)

---------------------------------------------------------------------


"
Sasha Kacanski <skacanski@gmail.com>,"Sat, 9 Jan 2016 07:36:18 -0500",Re: [discuss] dropping Python 2.6 support,Reynold Xin <rxin@databricks.com>,"+1
Companies that use stock python in redhat 2.6 will need to upgrade or
install fresh version wich is total of 3.5 minutes so no issues ...




-- 
Aleksandar Kacanski
"
Sean Owen <sowen@cloudera.com>,"Sat, 9 Jan 2016 12:48:17 +0000",Re: [discuss] dropping Python 2.6 support,Koert Kuipers <koert@tresata.com>,"Chiming in late, but my take on this line of argument is: these
companies are welcome to keep using Spark 1.x. If anything the
argument here is about how long to maintain 1.x, and indeed, it's
going to go dormant quite soon.

But using RHEL 6 (or any old-er version of any platform) and not
wanting to update already means you prefer stability more than change.
I don't receive an expectation that major releases of major things
support older major releases of other things.

Conversely: supporting something in Spark 2.x means making sure
nothing breaks compatibility with it for a couple years. This is
effort than can be spent elsewhere; this has to be weighed.

(For similar reasons I personally don't favor supporting Java 7 or
Scala 2.10 in Spark 2.x.)


---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Sat, 9 Jan 2016 14:12:44 +0100",Re: [discuss] dropping Python 2.6 support,Sean Owen <sowen@cloudera.com>,"

That reflects my sentiments as well. Thanks Sean for bringing that up!

Jacek

---------------------------------------------------------------------


"
Dmitry Kniazev <kniazev@tut.by>,"Sun, 10 Jan 2016 20:12:53 -0600",Re: [discuss] dropping Python 2.6 support,"Sasha Kacanski <skacanski@gmail.com>,
	Reynold Xin <rxin@databricks.com>","Sasha, it is more complicated than that: many RHEL 6 OS utilities rely on Python 2.6. Upgrading it to 2.7 breaks the system. For large enterprises migrating to another server OS means re-certifying (re-testing) hundreds of applications, so yes, they do prefer to stay where they are until the benefits of migrating outweigh the overhead. Long story short: you cannot simply upgrade built-in Python 2.6 in RHEL 6 and it will take years for enterprises to migrate to RHEL 7.

Having said that, I don't think that it is a problem though, because Python 2.6 and Python 2.7 can easily co-exist in the same environment. For example, we use virtualenv to run Spark with Python 2.7 and do not touch system Python 2.6.

Thank you,
Dmitry

09.01.2016, 06:36, ""Sasha Kacanski"" <skacanski@gmail.com>:

---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Mon, 11 Jan 2016 08:50:05 +0100",BUILD FAILURE...again?! :( Spark Project External Flume on fire,dev <dev@spark.apache.org>,"Hi,

It appears that the last commit [1] broke the build. Is anyone working
on it? I can when told so.

‚ûú  spark git:(master) ‚úó ./build/mvn -Pyarn -Phadoop-2.6
-Dhadoop.version=2.7.1 -Dscala-2.11 -Phive -Phive-thriftserver
-DskipTests clean install
...
[info] Compiling 8 Scala sources and 1 Java source to
/Users/jacek/dev/oss/spark/external/flume/target/scala-2.11/classes...
[error] /Users/jacek/dev/oss/spark/external/flume/src/main/scala/org/apache/spark/streaming/flume/FlumeInputDStream.scala:33:
object jboss is not a member of package org
[error] import org.jboss.netty.handler.codec.compression._
[error]            ^
[error] /Users/jacek/dev/oss/spark/external/flume/src/main/scala/org/apache/spark/streaming/flume/FlumeInputDStream.scala:31:
object jboss is not a member of package org
[error] import org.jboss.netty.channel.{ChannelPipeline,
ChannelPipelineFactory, Channels}
[error]            ^
[error] /Users/jacek/dev/oss/spark/external/flume/src/main/scala/org/apache/spark/streaming/flume/FlumeInputDStream.scala:32:
object jboss is not a member of package org
[error] import org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory
[error]            ^
[warn] Class org.jboss.netty.channel.ChannelFactory not found -
continuing with a stub.
[warn] Class org.jboss.netty.channel.ChannelFactory not found -
continuing with a stub.
[warn] Class org.jboss.netty.channel.ChannelPipelineFactory not found
- continuing with a stub.
[warn] Class org.jboss.netty.handler.execution.ExecutionHandler not
found - continuing with a stub.
[warn] Class org.jboss.netty.channel.ChannelFactory not found -
continuing with a stub.
[warn] Class org.jboss.netty.handler.execution.ExecutionHandler not
found - continuing with a stub.
[warn] Class org.jboss.netty.channel.group.ChannelGroup not found -
continuing with a stub.
[error] /Users/jacek/dev/oss/spark/external/flume/src/main/scala/org/apache/spark/streaming/flume/FlumeInputDStream.scala:149:
not found: type NioServerSocketChannelFactory
[error]       val channelFactory = new
NioServerSocketChannelFactory(Executors.newCachedThreadPool(),
[error]                                ^
[error] /Users/jacek/dev/oss/spark/external/flume/src/main/scala/org/apache/spark/streaming/flume/FlumeInputDStream.scala:196:
not found: type ChannelPipelineFactory
[error]   class CompressionChannelPipelineFactory extends
ChannelPipelineFactory {
[error]                                                   ^
[error] Class org.jboss.netty.channel.ChannelFactory not found -
continuing with a stub.
[error] Class org.jboss.netty.channel.ChannelPipelineFactory not found
- continuing with a stub.
[error] Class org.jboss.netty.handler.execution.ExecutionHandler not
found - continuing with a stub.
[error] /Users/jacek/dev/oss/spark/external/flume/src/main/scala/org/apache/spark/streaming/flume/FlumeInputDStream.scala:197:
not found: type ChannelPipeline
[error]     def getPipeline(): ChannelPipeline = {
[error]                        ^
[error] /Users/jacek/dev/oss/spark/external/flume/src/main/scala/org/apache/spark/streaming/flume/FlumeInputDStream.scala:198:
not found: value Channels
[error]       val pipeline = Channels.pipeline()
[error]                      ^
[error] /Users/jacek/dev/oss/spark/external/flume/src/main/scala/org/apache/spark/streaming/flume/FlumeInputDStream.scala:199:
not found: type ZlibEncoder
[error]       val encoder = new ZlibEncoder(6)
[error]                         ^
[error] /Users/jacek/dev/oss/spark/external/flume/src/main/scala/org/apache/spark/streaming/flume/FlumePollingInputDStream.scala:29:
object jboss is not a member of package org
[error] import org.jboss.netty.channel.socket.nio.NioClientSocketChannelFactory
[error]            ^
[error] /Users/jacek/dev/oss/spark/external/flume/src/main/scala/org/apache/spark/streaming/flume/FlumePollingInputDStream.scala:73:
not found: type NioClientSocketChannelFactory
[error]     new NioClientSocketChannelFactory(channelFactoryExecutor,
channelFactoryExecutor)
[error]         ^
[warn] Class org.jboss.netty.channel.ChannelFuture not found -
continuing with a stub.
[warn] Class org.jboss.netty.channel.ChannelFactory not found -
continuing with a stub.
[warn] Class org.jboss.netty.channel.ChannelFactory not found -
continuing with a stub.
[warn] Class org.jboss.netty.channel.ChannelFactory not found -
continuing with a stub.
[warn] Class org.jboss.netty.channel.ChannelFactory not found -
continuing with a stub.
[warn] Class org.jboss.netty.channel.ChannelUpstreamHandler not found
- continuing with a stub.
[error] Class org.jboss.netty.channel.ChannelFactory not found -
continuing with a stub.
[error] /Users/jacek/dev/oss/spark/external/flume/src/main/scala/org/apache/spark/streaming/flume/FlumeTestUtils.scala:33:
object jboss is not a member of package org
[error] import org.jboss.netty.channel.ChannelPipeline
[error]            ^
[error] /Users/jacek/dev/oss/spark/external/flume/src/main/scala/org/apache/spark/streaming/flume/FlumeTestUtils.scala:34:
object jboss is not a member of package org
[error] import org.jboss.netty.channel.socket.SocketChannel
[error]            ^
[error] /Users/jacek/dev/oss/spark/external/flume/src/main/scala/org/apache/spark/streaming/flume/FlumeTestUtils.scala:35:
object jboss is not a member of package org
[error] import org.jboss.netty.channel.socket.nio.NioClientSocketChannelFactory
[error]            ^
[error] /Users/jacek/dev/oss/spark/external/flume/src/main/scala/org/apache/spark/streaming/flume/FlumeTestUtils.scala:36:
object jboss is not a member of package org
[error] import org.jboss.netty.handler.codec.compression.{ZlibDecoder,
ZlibEncoder}
[error]            ^
[error] /Users/jacek/dev/oss/spark/external/flume/src/main/scala/org/apache/spark/streaming/flume/FlumeTestUtils.scala:107:
not found: type NioClientSocketChannelFactory
[error]     extends NioClientSocketChannelFactory {
[error]             ^
[error] /Users/jacek/dev/oss/spark/external/flume/src/main/scala/org/apache/spark/streaming/flume/FlumeTestUtils.scala:79:
overloaded method constructor NettyTransceiver with alternatives:
[error]   (x$1: java.net.InetSocketAddress,x$2:
org.jboss.netty.channel.ChannelFactory)org.apache.avro.ipc.NettyTransceiver
<and>
[error]   (x$1: java.net.InetSocketAddress,x$2:
Long)org.apache.avro.ipc.NettyTransceiver
[error]  cannot be applied to (java.net.InetSocketAddress,
FlumeTestUtils.this.CompressionChannelFactory)
[error]         new NettyTransceiver(testAddress, new
CompressionChannelFactory(6))
[error]         ^
[error] /Users/jacek/dev/oss/spark/external/flume/src/main/scala/org/apache/spark/streaming/flume/FlumeTestUtils.scala:109:
not found: type SocketChannel
[error]     override def newChannel(pipeline: ChannelPipeline):
SocketChannel = {
[error]                                                         ^
[error] /Users/jacek/dev/oss/spark/external/flume/src/main/scala/org/apache/spark/streaming/flume/FlumeTestUtils.scala:109:
not found: type ChannelPipeline
[error]     override def newChannel(pipeline: ChannelPipeline):
SocketChannel = {
[error]                                       ^
[error] /Users/jacek/dev/oss/spark/external/flume/src/main/scala/org/apache/spark/streaming/flume/FlumeTestUtils.scala:110:
not found: type ZlibEncoder
[error]       val encoder = new ZlibEncoder(compressionLevel)
[error]                         ^
[error] /Users/jacek/dev/oss/spark/external/flume/src/main/scala/org/apache/spark/streaming/flume/FlumeTestUtils.scala:113:
value newChannel is not a member of AnyRef
[error]       super.newChannel(pipeline)
[error]             ^
[warn] 13 warnings found
[error] 24 errors found
[error] Compile failed at Jan 11, 2016 8:43:02 AM [0.602s]
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO]
[INFO] Spark Project Parent POM ........................... SUCCESS [  5.427 s]
[INFO] Spark Project Test Tags ............................ SUCCESS [  4.430 s]
[INFO] Spark Project Launcher ............................. SUCCESS [  8.698 s]
[INFO] Spark Project Networking ........................... SUCCESS [ 16.379 s]
[INFO] Spark Project Shuffle Streaming Service ............ SUCCESS [  8.800 s]
[INFO] Spark Project Unsafe ............................... SUCCESS [ 11.254 s]
[INFO] Spark Project Core ................................. SUCCESS [02:04 min]
[INFO] Spark Project GraphX ............................... SUCCESS [ 16.071 s]
[INFO] Spark Project Streaming ............................ SUCCESS [ 35.991 s]
[INFO] Spark Project Catalyst ............................. SUCCESS [01:34 min]
[INFO] Spark Project SQL .................................. SUCCESS [01:08 min]
[INFO] Spark Project ML Library ........................... SUCCESS [01:18 min]
[INFO] Spark Project Tools ................................ SUCCESS [  5.614 s]
[INFO] Spark Project Hive ................................. SUCCESS [ 40.699 s]
[INFO] Spark Project Docker Integration Tests ............. SUCCESS [  2.102 s]
[INFO] Spark Project REPL ................................. SUCCESS [  6.258 s]
[INFO] Spark Project YARN Shuffle Service ................. SUCCESS [  6.547 s]
[INFO] Spark Project YARN ................................. SUCCESS [ 12.898 s]
[INFO] Spark Project Hive Thrift Server ................... SUCCESS [  9.361 s]
[INFO] Spark Project Assembly ............................. SUCCESS [ 40.149 s]
[INFO] Spark Project External Twitter ..................... SUCCESS [  7.137 s]
[INFO] Spark Project External Flume Sink .................. SUCCESS [  6.203 s]
[INFO] Spark Project External Flume ....................... FAILURE [  1.010 s]
[INFO] Spark Project External Flume Assembly .............. SKIPPED

[1] https://github.com/apache/spark/commit/3ab0138b0fe0f9208b4b476855294a7c729583b7

Pozdrawiam,
Jacek

Jacek Laskowski | https://medium.com/@jaceklaskowski/
Mastering Apache Spark
==> https://jaceklaskowski.gitbooks.io/mastering-apache-spark/
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Josh Rosen <joshrosen@databricks.com>,"Mon, 11 Jan 2016 00:24:20 -0800",Re: BUILD FAILURE...again?! :( Spark Project External Flume on fire,Jacek Laskowski <jacek@japila.pl>,"I've got a hotfix which should address it:
https://github.com/apache/spark/pull/10693




/streaming/flume/FlumeInputDStream.scala:33:
/streaming/flume/FlumeInputDStream.scala:31:
/streaming/flume/FlumeInputDStream.scala:32:
/streaming/flume/FlumeInputDStream.scala:149:
/streaming/flume/FlumeInputDStream.scala:196:
/streaming/flume/FlumeInputDStream.scala:197:
/streaming/flume/FlumeInputDStream.scala:198:
/streaming/flume/FlumeInputDStream.scala:199:
/streaming/flume/FlumePollingInputDStream.scala:29:
/streaming/flume/FlumePollingInputDStream.scala:73:
/streaming/flume/FlumeTestUtils.scala:33:
/streaming/flume/FlumeTestUtils.scala:34:
/streaming/flume/FlumeTestUtils.scala:35:
/streaming/flume/FlumeTestUtils.scala:36:
/streaming/flume/FlumeTestUtils.scala:107:
/streaming/flume/FlumeTestUtils.scala:79:
er
/streaming/flume/FlumeTestUtils.scala:109:
/streaming/flume/FlumeTestUtils.scala:109:
/streaming/flume/FlumeTestUtils.scala:110:
/streaming/flume/FlumeTestUtils.scala:113:
4
4
8
8
9583b7
"
Jacek Laskowski <jacek@japila.pl>,"Mon, 11 Jan 2016 09:26:24 +0100",Re: BUILD FAILURE...again?! :( Spark Project External Flume on fire,Josh Rosen <joshrosen@databricks.com>,"Thanks Josh. Leaving now so I can't give it a shot, but will report
results in a couple of hours. Thanks a lot!

Pozdrawiam,
Jacek

Jacek Laskowski | https://medium.com/@jaceklaskowski/
Mastering Apache Spark
==> https://jaceklaskowski.gitbooks.io/mastering-apache-spark/
Follow me at https://twitter.com/jaceklaskowski


e:
:
k/streaming/flume/FlumeInputDStream.scala:33:
k/streaming/flume/FlumeInputDStream.scala:31:
k/streaming/flume/FlumeInputDStream.scala:32:
k/streaming/flume/FlumeInputDStream.scala:149:
k/streaming/flume/FlumeInputDStream.scala:196:
k/streaming/flume/FlumeInputDStream.scala:197:
k/streaming/flume/FlumeInputDStream.scala:198:
k/streaming/flume/FlumeInputDStream.scala:199:
k/streaming/flume/FlumePollingInputDStream.scala:29:
k/streaming/flume/FlumePollingInputDStream.scala:73:
k/streaming/flume/FlumeTestUtils.scala:33:
k/streaming/flume/FlumeTestUtils.scala:34:
k/streaming/flume/FlumeTestUtils.scala:35:
k/streaming/flume/FlumeTestUtils.scala:36:
k/streaming/flume/FlumeTestUtils.scala:107:
k/streaming/flume/FlumeTestUtils.scala:79:
ver
k/streaming/flume/FlumeTestUtils.scala:109:
k/streaming/flume/FlumeTestUtils.scala:109:
k/streaming/flume/FlumeTestUtils.scala:110:
k/streaming/flume/FlumeTestUtils.scala:113:
04
34
08
18
29583b7

---------------------------------------------------------------------


"
Gaini Rajeshwar <raja.rajeshwar2006@gmail.com>,"Mon, 11 Jan 2016 15:14:28 +0530",XML column not supported in Database,"user <user@spark.apache.org>, dev@spark.apache.org","Hi All,

I am using PostgreSQL database. I am using the following jdbc call to
access a customer table (*customer_id int, event text, country text,
content xml)* in my database.

*val dataframe1 = sqlContext.load(""jdbc"", Map(""url"" ->
""jdbc:postgresql://localhost/customerlogs?user=postgres&password=postgres"",
""dbtable"" -> ""customer""))*

When i run above command in spark-shell i receive the following error.

*java.sql.SQLException: Unsupported type 1111*
* at
org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.org$apache$spark$sql$execution$datasources$jdbc$JDBCRDD$$getCatalystType(JDBCRDD.scala:103)*
* at
org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$$anonfun$1.apply(JDBCRDD.scala:140)*
* at
org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$$anonfun$1.apply(JDBCRDD.scala:140)*
* at scala.Option.getOrElse(Option.scala:120)*
* at
org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:139)*
* at
org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation.<init>(JDBCRelation.scala:91)*
* at
org.apache.spark.sql.execution.datasources.jdbc.DefaultSource.createRelation(DefaultSource.scala:60)*
* at
org.apache.spark.sql.execution.datasources.ResolvedDataSource$.apply(ResolvedDataSource.scala:158)*
* at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:119)*
* at org.apache.spark.sql.SQLContext.load(SQLContext.scala:1153)*
* at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:25)*
* at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:30)*
* at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:32)*
* at $iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:34)*
* at $iwC$$iwC$$iwC$$iwC.<init>(<console>:36)*
* at $iwC$$iwC$$iwC.<init>(<console>:38)*
* at $iwC$$iwC.<init>(<console>:40)*
* at $iwC.<init>(<console>:42)*
* at <init>(<console>:44)*
* at .<init>(<console>:48)*
* at .<clinit>(<console>)*
* at .<init>(<console>:7)*
* at .<clinit>(<console>)*
* at $print(<console>)*
* at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)*
* at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)*
* at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)*
* at java.lang.reflect.Method.invoke(Method.java:497)*
* at
org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)*
* at
org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)*
* at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)*
* at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)*
* at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)*
* at
org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:857)*
* at
org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:902)*
* at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:814)*
* at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:657)*
* at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:665)*
* at org.apache.spark.repl.SparkILoop.org
<http://org.apache.spark.repl.SparkILoop.org>$apache$spark$repl$SparkILoop$$loop(SparkILoop.scala:670)*
* at
org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply$mcZ$sp(SparkILoop.scala:997)*
* at
org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)*
* at
org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)*
* at
scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)*
* at org.apache.spark.repl.SparkILoop.org
<http://org.apache.spark.repl.SparkILoop.org>$apache$spark$repl$SparkILoop$$process(SparkILoop.scala:945)*
* at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1059)*
* at org.apache.spark.repl.Main$.main(Main.scala:31)*
* at org.apache.spark.repl.Main.main(Main.scala)*
* at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)*
* at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)*
* at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)*
* at java.lang.reflect.Method.invoke(Method.java:497)*
* at
org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)*
* at
org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)*
* at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)*
* at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)*
* at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)*

Is xml column type not supported yet in spark ? is there any way to fix
this issue ?

Thanks,
Rajeshwar Gaini.
"
,"Mon, 11 Jan 2016 13:59:03 +0100",Re: BUILD FAILURE...again?! :( Spark Project External Flume on fire,"""dev@spark.apache.org"" <dev@spark.apache.org>","I confirm: I have the same issue.

I tried Josh's PR and but the branch is not found:

git pull https://github.com/JoshRosen/spark netty-hotfix

As the issue is on Flume external, I'm not sure it's related.

Let me take a look and eventually provide a fix.

Regards
JB
-- 
jbonofre@apache.org
http://blog.nanthrax.net
Talend - http://www.talend.com

---------------------------------------------------------------------


"
,"Mon, 11 Jan 2016 14:05:15 +0100",Re: BUILD FAILURE...again?! :( Spark Project External Flume on fire,dev@spark.apache.org,"Heads up: I just updated my local copy, it looks better now (the build 
is so far so good). I keep you posted.

Regards
JB


-- 
jbonofre@apache.org
http://blog.nanthrax.net
Talend - http://www.talend.com

---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Mon, 11 Jan 2016 14:12:07 +0100",Re: BUILD FAILURE...again?! :( Spark Project External Flume on fire,,"Hi,

I've just git pull and it worked for me. Looks like
https://github.com/apache/spark/commit/f13c7f8f7dc8766b0a42406b5c3639d6be55cf33
fixed the issue (or something in-between).

Thanks for such a quick fix!

p.s. Had time for swimming :-)

Pozdrawiam,
Jacek

Jacek Laskowski | https://medium.com/@jaceklaskowski/
Mastering Apache Spark
==> https://jaceklaskowski.gitbooks.io/mastering-apache-spark/
Follow me at https://twitter.com/jaceklaskowski


 so

---------------------------------------------------------------------


"
,"Mon, 11 Jan 2016 15:48:08 +0100",Re: BUILD FAILURE...again?! :( Spark Project External Flume on fire,dev@spark.apache.org,"I confirm Jacek, it works fine now ;)

Regards
JB


-- 
jbonofre@apache.org
http://blog.nanthrax.net
Talend - http://www.talend.com

---------------------------------------------------------------------


"
=?UTF-8?B?RMW+ZW5hbiBTb2Z0acSH?= <dzeno10@gmail.com>,"Mon, 11 Jan 2016 15:52:09 +0100",Python custom partitioning,dev@spark.apache.org,"Hi,

I am trying to implement Gap statistics on Spark, which aims to determine
actual number of clusters for KMeans. Giving the range of possible Ks (e.g.
K = 10), Gap will run KMeans for each K in the range. Since computation of
Kmeans for K=10 takes more time than K=1,2,3,4 together, I would like to
partition computation so K=10 is on one node and K=1,2,3,4 is together on
another node.

As an example, if I want 3 partitions and if I have the following Ks
[1,2,3,4,5,6,7,8,9,10], I would like to get [1,2,3,4,5,6] [7,8] [9,10]
after partitionBy(3, partitionFunc).

Any suggestions how could I implement partition function for PartitionBy in
order to achieve something like this? I've implemented partition function
to divide Ks into 2 buckets, based on Greedy algorithm. But I don't get how
to control which element goes into which bucket from partition function.

Any suggestions would be very helpful.

Thank you.
"
David Chin <david.chin@drexel.edu>,"Mon, 11 Jan 2016 11:52:10 -0500",Re: [discuss] dropping Python 2.6 support,"""dev@spark.apache.org"" <dev@spark.apache.org>, user <user@spark.apache.org>","FWIW, RHEL 6 still uses Python 2.6, although 2.7.8 and 3.3.2 are available
through Red Hat Software Collections. See:
https://www.softwarecollections.org/en/

I run an academic compute cluster on RHEL 6. We do, however, provide Python
2.7.x and 3.5.x via modulefiles.

m

rs
txt.com> ÂÜôÈÅìÔºö
:
k
d on
o
es


-- 
David Chin, Ph.D.
david.chin@drexel.edu    Sr. Systems Administrator, URCF, Drexel U.
http://www.drexel.edu/research/urcf/
https://linuxfollies.blogspot.com/
+1.215.221.4747 (mobile)
https://github.com/prehensilecode
"
Wail Alkowaileet <wael.y.k@gmail.com>,"Mon, 11 Jan 2016 12:46:50 -0500",Re: Dataset throws: Task not serializable,Michael Armbrust <michael@databricks.com>,"Hello Michael,

Sorry for the late replay .. I was crossing the world the last few days.
I actually tried both ... REPEL and SparkApp. The reported exception was in
App.

Unfortunately the data I have is not for distribution ... sorry about that.
I saw it has been resolved.. I will try to reproduce the same error with
dummy data.

Thanks!


e's
/apache/spark/sql/DatasetSuite.scala#L97> can
g, first_name:String, full_name:String, last_name:String, r_id:String, reprint:String, role:String, seq_no:String, suffix:String, wos_standard:String)
.scala:304)
ner$$clean(ClosureCleaner.scala:294)
)
la:150)
la:111)
va:62)
rImpl.java:43)
$sql$catalyst$ScalaReflection$$constructorFor$2,
$sql$catalyst$ScalaReflection$$constructorFor$2,
$sql$catalyst$ScalaReflection$$constructorFor$2$$anonfun$apply$1,
$sql$catalyst$ScalaReflection$$constructorFor$2)
$sql$catalyst$ScalaReflection$$constructorFor$2$$anonfun$apply$1,
rint:string,role:string,wos_standard:string,last_name:string,dais_id:string,seq_no:string,suffix:string,r_id:string,addr_no:string>>),StructField(display_name,StringType,true),StructField(first_name,StringType,true),StructField(full_name,StringType,true),StructField(reprint,StringType,true),StructField(role,StringType,true),StructField(wos_standard,StringType,true),StructField(last_name,StringType,true),StructField(dais_id,StringType,true),StructField(seq_no,StringType,true),StructField(suffix,StringType,true),StructField(r_id,StringType,true),StructField(addr_no,StringType,true)))
rint:string,role:string,wos_standard:string,last_name:string,dais_id:string,seq_no:string,suffix:string,r_id:string,addr_no:string>>),StructField(display_name,StringType,true),StructField(first_name,StringType,true),StructField(full_name,StringType,true),StructField(reprint,StringType,true),StructField(role,StringType,true),StructField(wos_standard,StringType,true),StructField(last_name,StringType,true),StructField(dais_id,StringType,true),StructField(seq_no,StringType,true),StructField(suffix,StringType,true),StructField(r_id,StringType,true),StructField(addr_no,StringType,true)),array,ObjectType(class
rint:string,role:string,wos_standard:string,last_name:string,dais_id:string,seq_no:string,suffix:string,r_id:string,addr_no:string>>),StructField(display_name,StringType,true),StructField(first_name,StringType,true),StructField(full_name,StringType,true),StructField(reprint,StringType,true),StructField(role,StringType,true),StructField(wos_standard,StringType,true),StructField(last_name,StringType,true),StructField(dais_id,StringType,true),StructField(seq_no,StringType,true),StructField(suffix,StringType,true),StructField(r_id,StringType,true),StructField(addr_no,StringType,true)),array,ObjectType(class
rint:string,role:string,wos_standard:string,last_name:string,dais_id:string,seq_no:string,suffix:string,r_id:string,addr_no:string>>),StructField(display_name,StringType,true),StructField(first_name,StringType,true),StructField(full_name,StringType,true),StructField(reprint,StringType,true),StructField(role,StringType,true),StructField(wos_standard,StringType,true),StructField(last_name,StringType,true),StructField(dais_id,StringType,true),StructField(seq_no,StringType,true),StructField(suffix,StringType,true),StructField(r_id,StringType,true),StructField(addr_no,StringType,true)),array,ObjectType(class
]:
rint:string,role:string,wos_standard:string,last_name:string,dais_id:string,seq_no:string,suffix:string,r_id:string,addr_no:string>>])
rint:string,role:string,wos_standard:string,last_name:string,dais_id:string,seq_no:string,suffix:string,r_id:string,addr_no:string>>],
,
,
y$21,
y$21,
,
,
ializationDebugger.scala:40)
ializer.scala:47)
izer.scala:101)
.scala:301)
e's


-- 

*Regards,*
Wail Alkowaileet
"
shane knapp <sknapp@berkeley.edu>,"Mon, 11 Jan 2016 09:47:47 -0800","[build system] jenkins wedged, had to do a quick restart","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","jenkins looked to be wedged, and nothing was showing up in the logs.
i tried a restart, and am still looking in to the problem.

we should be back up and building shortly.  sorry for the inconvenience.

shane

---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Mon, 11 Jan 2016 09:48:55 -0800","Re: [build system] jenkins wedged, had to do a quick restart","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","...aaaaaaand we're back up and building.

shane


---------------------------------------------------------------------


"
Mark Grover <mark@apache.org>,"Mon, 11 Jan 2016 12:56:47 -0800",Write access to wiki,dev@spark.apache.org,"Hi all,
May I please get write access to the useful tools
<https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools#UsefulDeveloperTools-IntelliJ>
wiki page?

I did some investigation
<https://issues.apache.org/jira/browse/SPARK-12426?focusedCommentId=15083482&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15083482>
related to docker integration tests and want to list out the pre-requisites
required on the machine for those tests to pass, on that page.

maintain for setting up build slaves. If our Jenkins infra were wiped out,
how do we rebuild the slave?

Thanks in advance!

Mark
"
Sean Owen <sowen@cloudera.com>,"Mon, 11 Jan 2016 21:01:21 +0000",Re: Write access to wiki,"Mark Grover <mark@apache.org>, sknapp@berkeley.edu","... I forget who can give access -- is it INFRA at Apache or one of us?
I can apply any edit you need in the meantime.

Shane may be able to fill you in on how the Jenkins build is set up.


---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Mon, 11 Jan 2016 14:25:41 -0800",Re: Dataset throws: Task not serializable,Wail Alkowaileet <wael.y.k@gmail.com>,"I think this should be fixed in both master and branch-1.6 now.  We'll look
at doing 1.6.1 sometime in the near future.  Please let me know if you can
reproduce any issues there.


m
me's
g/apache/spark/sql/DatasetSuite.scala#L97> can
ng, first_name:String, full_name:String, last_name:String, r_id:String, reprint:String, role:String, seq_no:String, suffix:String, wos_standard:String)
r.scala:304)
aner$$clean(ClosureCleaner.scala:294)
2)
ala:150)
ala:111)
ava:62)
orImpl.java:43)
)
)
:
)
k$sql$catalyst$ScalaReflection$$constructorFor$2,
k$sql$catalyst$ScalaReflection$$constructorFor$2,
k$sql$catalyst$ScalaReflection$$constructorFor$2$$anonfun$apply$1,
k$sql$catalyst$ScalaReflection$$constructorFor$2)
k$sql$catalyst$ScalaReflection$$constructorFor$2$$anonfun$apply$1,
print:string,role:string,wos_standard:string,last_name:string,dais_id:string,seq_no:string,suffix:string,r_id:string,addr_no:string>>),StructField(display_name,StringType,true),StructField(first_name,StringType,true),StructField(full_name,StringType,true),StructField(reprint,StringType,true),StructField(role,StringType,true),StructField(wos_standard,StringType,true),StructField(last_name,StringType,true),StructField(dais_id,StringType,true),StructField(seq_no,StringType,true),StructField(suffix,StringType,true),StructField(r_id,StringType,true),StructField(addr_no,StringType,true)))
:
print:string,role:string,wos_standard:string,last_name:string,dais_id:string,seq_no:string,suffix:string,r_id:string,addr_no:string>>),StructField(display_name,StringType,true),StructField(first_name,StringType,true),StructField(full_name,StringType,true),StructField(reprint,StringType,true),StructField(role,StringType,true),StructField(wos_standard,StringType,true),StructField(last_name,StringType,true),StructField(dais_id,StringType,true),StructField(seq_no,StringType,true),StructField(suffix,StringType,true),StructField(r_id,StringType,true),StructField(addr_no,StringType,true)),array,ObjectType(class
print:string,role:string,wos_standard:string,last_name:string,dais_id:string,seq_no:string,suffix:string,r_id:string,addr_no:string>>),StructField(display_name,StringType,true),StructField(first_name,StringType,true),StructField(full_name,StringType,true),StructField(reprint,StringType,true),StructField(role,StringType,true),StructField(wos_standard,StringType,true),StructField(last_name,StringType,true),StructField(dais_id,StringType,true),StructField(seq_no,StringType,true),StructField(suffix,StringType,true),StructField(r_id,StringType,true),StructField(addr_no,StringType,true)),array,ObjectType(class
print:string,role:string,wos_standard:string,last_name:string,dais_id:string,seq_no:string,suffix:string,r_id:string,addr_no:string>>),StructField(display_name,StringType,true),StructField(first_name,StringType,true),StructField(full_name,StringType,true),StructField(reprint,StringType,true),StructField(role,StringType,true),StructField(wos_standard,StringType,true),StructField(last_name,StringType,true),StructField(dais_id,StringType,true),StructField(seq_no,StringType,true),StructField(suffix,StringType,true),StructField(r_id,StringType,true),StructField(addr_no,StringType,true)),array,ObjectType(class
0]:
print:string,role:string,wos_standard:string,last_name:string,dais_id:string,seq_no:string,suffix:string,r_id:string,addr_no:string>>])
print:string,role:string,wos_standard:string,last_name:string,dais_id:string,seq_no:string,suffix:string,r_id:string,addr_no:string>>],
)
ly$21,
ly$21,
rializationDebugger.scala:40)
rializer.scala:47)
lizer.scala:101)
r.scala:301)
me's
"
shane knapp <sknapp@berkeley.edu>,"Mon, 11 Jan 2016 14:50:26 -0800",Re: Write access to wiki,Sean Owen <sowen@cloudera.com>,"mark:  yes.  yes i can.  :)

currently, we have a set of bash scripts and binary packages on our
jenkins master that can turn a bare centos install in to a jenkins
worker.

i've also been porting over these bash tools in to ansible playbooks,
but a lot of development stopped on this after we lost our staging
instance due to a datacenter fire (yes, really) back in september.
we're getting a new staging instance (master + slaves) set up in the
next week or so, and THEN i can finish the ansible port.

these scripts are checked in to a private AMPLab github repo.

does this help?

shane

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Mon, 11 Jan 2016 15:25:10 -0800",Re: XML column not supported in Database,Gaini Rajeshwar <raja.rajeshwar2006@gmail.com>,"Can you file a JIRA ticket? Thanks.

The URL is issues.apache.org/jira/browse/SPARK


"
shane knapp <sknapp@berkeley.edu>,"Mon, 11 Jan 2016 15:36:54 -0800",Re: [discuss] dropping Python 2.6 support,David Chin <david.chin@drexel.edu>,"(this is a build system-specific reply, but quite pertinent to the conversation)

we currently test spark on a centos 6.X deployment, but in the next
~month will be bumping everything to centos 7.  by default, centos 7
comes w/python 2.7.5 installed as the system python.  for any builds
that need python 2.6 (1.5 and earlier), we'll be using anaconda
environments to manage them.

i'm generally VERY happy with how easy it is to manage our three
different python environments with anaconda, and don't plan on
changing that at all in the foreseeable future.

shane

e
on
:
ytxt.com> ÂÜôÈÅìÔºö
e:
rk
nd on
to
ses

---------------------------------------------------------------------


"
Mark Grover <mark@apache.org>,"Mon, 11 Jan 2016 17:15:47 -0800",Re: Write access to wiki,shane knapp <sknapp@berkeley.edu>,"Thanks Sean, I will send you the edit on the JIRA to keep email traffic
low:-)
Thanks Shane, comments in line.



Got it, thanks.


Ok, sounds good. I think it would be great, if you could add installing the
'docker-engine' package and starting the 'docker' service in there too. I
was planning to update the playbook if there were one in the apache/spark
repo but I didn't see one, hence my question.



Yes, it does. Thanks!


"
Gaini Rajeshwar <raja.rajeshwar2006@gmail.com>,"Tue, 12 Jan 2016 11:49:12 +0530",Re: XML column not supported in Database,Reynold Xin <rxin@databricks.com>,"Hi Reynold,

I did create a issue in JIRA. It is SPARK-12764
<https://issues.apache.org/jira/browse/SPARK-12764>


"
Nick Pentreath <nick.pentreath@gmail.com>,"Tue, 12 Jan 2016 12:52:08 +0200",Re: Write access to wiki,dev <dev@spark.apache.org>,"I'd also like to get Wiki write access - at the least it allows a few of us
to amend the ""Powered By"" and similar pages when those requests come
through (Sean has been doing a lot of that recently :)


"
Adam Roberts <AROBERTS@uk.ibm.com>,"Tue, 12 Jan 2016 15:01:35 +0000",Tungsten in a mixed endian environment,dev@spark.apache.org,"Hi all, I've been experimenting with DataFrame operations in a mixed 
endian environment - a big endian master with little endian workers. With 
tungsten enabled I'm encountering data corruption issues.

For example, with this simple test code:

import org.apache.spark.SparkContext
import org.apache.spark._
import org.apache.spark.sql.SQLContext

object SimpleSQL {
  def main(args: Array[String]): Unit = {
    if (args.length != 1) {
      println(""Not enough args, you need to specify the master url"")
    }
    val masterURL = args(0)
    println(""Setting up Spark context at: "" + masterURL)
    val sparkConf = new SparkConf
    val sc = new SparkContext(masterURL, ""Unsafe endian test"", sparkConf)

    println(""Performing SQL tests"")

    val sqlContext = new SQLContext(sc)
    println(""SQL context set up"")
    val df = sqlContext.read.json(""/tmp/people.json"")
    df.show()
    println(""Selecting everyone's age and adding one to it"")
    df.select(df(""name""), df(""age"") + 1).show()
    println(""Showing all people over the age of 21"")
    df.filter(df(""age"") > 21).show()
    println(""Counting people by age"")
    df.groupBy(""age"").count().show()
  }
} 

Instead of getting

+----+-----+
| age|count|
+----+-----+
|null|    1|
|  19|    1|
|  30|    1|
+----+-----+ 

I get the following with my mixed endian set up:

+-------------------+-----------------+
|                age|            count|
+-------------------+-----------------+
|               null|                1|
|1369094286720630784|72057594037927936|
|                 30|                1|
+-------------------+-----------------+ 

and on another run:

+-------------------+-----------------+
|                age|            count|
+-------------------+-----------------+
|                  0|72057594037927936|
|                 19|                1| 

Is Spark expected to work in such an environment? If I turn off tungsten (
sparkConf.set(""spark.sql.tungsten.enabled"", ""false""), in 20 runs I don't 
see any problems.

Unless stated otherwise above:
IBM United Kingdom Limited - Registered in England and Wales with number 
741598. 
Registered office: PO Box 41, North Harbour, Portsmouth, Hampshire PO6 3AU
"
David <themarchoffolly@protonmail.com>,"Tue, 12 Jan 2016 10:03:36 -0500",ROSE: Spark + R on the JVM.,"""user@spark.apache.org"" <user@spark.apache.org>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,

I'd like to share news of the recent release of a new Spark package, [ROSE](http://spark-packages.org/package/onetapbeyond/opencpu-spark-executor).

ROSE is a Scala library offering access to the full scientific computing power of the R programming language to Apache Spark batch and streaming applications on the JVM. Where Apache SparkR lets data scientists use Spark from R, ROSE is designed to let Scala and Java developers use R from Spark.

The project is available and documented [on GitHub](https://github.com/onetapbeyond/opencpu-spark-executor) and I would encourage you to [take a look](https://github.com/onetapbeyond/opencpu-spark-executor). Any feedback, questions etc very welcome.

David

""All that is gold does not glitter, Not all those who wander are lost."""
Ted Yu <yuzhihong@gmail.com>,"Tue, 12 Jan 2016 07:30:17 -0800",Re: Tungsten in a mixed endian environment,Adam Roberts <AROBERTS@uk.ibm.com>,"I logged SPARK-12778 where endian awareness in Platform.java should
help in mixed
endian set up.

There could be other parts of the code base which are related.

Cheers


"
Corey Nolet <cjnolet@gmail.com>,"Tue, 12 Jan 2016 12:32:16 -0500",Re: ROSE: Spark + R on the JVM.,David <themarchoffolly@protonmail.com>,"David,

Thank you very much for announcing this! It looks like it could be very
useful. Would you mind providing a link to the github?


"
David Russell <themarchoffolly@protonmail.com>,"Tue, 12 Jan 2016 12:37:47 -0500",Re: ROSE: Spark + R on the JVM.,Corey Nolet <cjnolet@gmail.com>,"Hi Corey,

> Would you mind providing a link to the github?

Sure, here is the github link you're looking for:

https://github.com/onetapbeyond/opencpu-spark-executor

David

""All that is gold does not glitter, Not all those who wander are lost.""



-------- Original Message --------
Subject: Re: ROSE: Spark + R on the JVM.
Local Time: January 12 2016 12:32 pm
UTC Time: January 12 2016 5:32 pm
From: cjnolet@gmail.com
To: themarchoffolly@protonmail.com
CC: user@spark.apache.org,dev@spark.apache.org



David,
Thank you very much for announcing this! It looks like it could be very useful. Would you mind providing a link to the github?



On Tue, Jan 12, 2016 at 10:03 AM, David <themarchoffolly@protonmail.com> wrote:

Hi all,

I'd like to share news of the recent release of a new Spark package, ROSE.

ROSE is a Scala library offering access to the full scientific computing power of the R programming language to Apache Spark batch and streaming applications on the JVM. Where Apache SparkR lets data scientists use Spark from R, ROSE is designed to let Scala and Java developers use R from Spark.

The project is available and documented on GitHub and I would encourage you to take a look. Any feedback, questions etc very welcome.

David

""All that is gold does not glitter, Not all those who wander are lost."""
shane knapp <sknapp@berkeley.edu>,"Tue, 12 Jan 2016 10:14:48 -0800",Re: Write access to wiki,Mark Grover <mark@apache.org>,"we currently have docker 1.5 running on the worker, and after the
Great Upgrade To CentOS7, we'll be running a much more modern version
of docker.

shane

---------------------------------------------------------------------


"
Lydia Ickler <icklerly@googlemail.com>,"Tue, 12 Jan 2016 19:28:02 +0100",Eigenvalue solver,"user@spark.apache.org,
 dev@spark.apache.org","Hi,

I wanted to know if there are any implementations yet within the Machine Learning Library or generally that can efficiently solve eigenvalue problems?
Or if not do you have suggestions on how to approach a parallel execution maybe with BLAS or Breeze?

Thanks in advance!
Lydia


Von meinem iPhone gesendet
---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Tue, 12 Jan 2016 10:49:02 -0800",Re: Tungsten in a mixed endian environment,Ted Yu <yuzhihong@gmail.com>,"How big of a deal this use case is in a heterogeneous endianness
environment? If we do want to fix it, we should do it when right before
Spark shuffles data to minimize performance penalty, i.e. turn big-endian
encoded data into little-indian encoded data before it goes on the wire.
This is a pretty involved change and given other things that might break
across heterogeneous endianness environments, I am not sure if it is high
priority enough to even warrant review bandwidth right now.





"
Alex Nastetsky <alex.nastetsky@vervemobile.com>,"Tue, 12 Jan 2016 13:55:58 -0500","Re: Enabling mapreduce.input.fileinputformat.list-status.num-threads
 in Spark?",Cheolsoo Park <piaozhexiu@gmail.com>,"Ran into this need myself. Does Spark have an equivalent of  ""mapreduce.
input.fileinputformat.list-status.num-threads""?

Thanks.


"
Robert Dodier <robert.dodier@gmail.com>,"Tue, 12 Jan 2016 12:36:46 -0800",Dependency on TestingUtils in a Spark package,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

I'm putting together a Spark package (in the spark-packages.org sense)
and I'd like to make use of the class
org.apache.spark.mllib.util.TestingUtils which appears in
mllib/src/test. Can I declare a dependency in my build.sbt to pull in
a suitable jar? I have searched around but I have not been able to
identify a jar which contains TestingUtils. I suppose I could cut 'n'
paste the relevant bits from the source code but I'd really rather
just declare a dependency. I looked at a few other packages at
spark-packages.org but I couldn't find an example of a project which
was doing something similar.

Thanks in advance for any light you can shed on this problem.

Robert Dodier

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Tue, 12 Jan 2016 12:50:21 -0800",Re: Dependency on TestingUtils in a Spark package,Robert Dodier <robert.dodier@gmail.com>,"There is no annotation in TestingUtils class indicating whether it is
suitable for consumption by external projects.

You should assume the class is not public since its methods may change in
future Spark releases.

Cheers


"
Reynold Xin <rxin@databricks.com>,"Tue, 12 Jan 2016 12:55:31 -0800",Re: Dependency on TestingUtils in a Spark package,Ted Yu <yuzhihong@gmail.com>,"If you need it, just copy it over to your own package. That's probably the
safest option.




"
Steve Loughran <stevel@hortonworks.com>,"Tue, 12 Jan 2016 21:13:07 +0000",Re: Tungsten in a mixed endian environment,,"
On 12 Jan 2016, at 10:49, Reynold Xin <rxin@databricks.com<mailto:rxin@databricks.com>> wrote:

How big of a deal this use case is in a heterogeneous endianness environment? If we do want to fix it, we should do it when right before Spark shuffles data to minimize performance penalty, i.e. turn big-endian encoded data into little-indian encoded data before it goes on the wire. This is a pretty involved change and given other things that might break across heterogeneous endianness environments, I am not sure if it is high priority enough to even warrant review bandwidth right now.




It's notable that Hadoop doesn't like mixed-endianness; there is work (primarily from Oracle) to have consistent byteswapping ‚Äîthat is: work reliably on big-endian systems  https://issues.apache.org/jira/browse/HADOOP-11505 ). There's no motivation to support mixed-endian clusters.


The majority of clusters x86, there's only 3 cpu families that are little endian: Spark, Power, Arm. Adam has clearly been playing with Power + x86, but I'd suspect that's experimentation, not production.

What is probably worth checking is mixed endian-ness between client apps submitting work and the servers: Java and Kryo serialization should handle that automatically.
"
David Hall <david.lw.hall@gmail.com>,"Tue, 12 Jan 2016 13:21:40 -0800",Re: Eigenvalue solver,Lydia Ickler <icklerly@googlemail.com>,"(I don't know anything spark specific, so I'm going to treat it like a
Breeze question...)

As I understand it, Spark uses ARPACK via Breeze for SVD, and presumably
the same approach can be used for EVD. Basically, you make a function that
multiplies your ""matrix"" (which might be represented
implicitly/distributed, whatever) by a breeze.linalg.DenseVector.

This is the Breeze implementation for sparse SVD (which is fully generic
and might be hard to follow if you're not used to Breeze/typeclass-heavy
Scala...)

https://github.com/dlwh/breeze/blob/aa958688c428db581d853fd92eb35e82f80d8b5c/math/src/main/scala/breeze/linalg/functions/svd.scala#L205-L205

The difference between SVD and EVD in arpack (to a first approximation) is
that you need to multiple by A.t * A * x for SVD, and just A * x for EVD.

The basic idea is to implement a Breeze UFunc eig.Impl2 implicit following
the svd code (or you could just copy out the body of the function and
specialize it.) The signature you're looking to implement is:

implicit def Eig_Sparse_Impl[Mat](implicit mul: OpMulMatrix.Impl2[Mat,
DenseVector[Double], DenseVector[Double]],
                                  dimImpl: dim.Impl[Mat, (Int, Int)])
  : eig.Impl3[Mat, Int, Double, EigenvalueResult] = {

The type parameters of Impl3 are: the matrix type, the number of
eigenvalues you want, and a tolerance, and a result type. If you implement
this signature, then you can call eig on anything that can be multiplied by
a dense vector and that implements dim (to get the number of outputs).

(You'll need to define the class eigenvalue result to be what you want. I
don't immediately know how to unpack ARPACK's answers, but you might look
at this scipy thing:
https://github.com/thomasnat1/cdcNewsRanker/blob/71b0ff3989d5191dc6a78c40c4a7a9967cbb0e49/venv/lib/python2.7/site-packages/scipy/sparse/linalg/eigen/arpack/arpack.py#L1049
)

I'm happy to help more if you decide to go this route, here, or on the
scala-breeze google group, or on github.

-- David



"
Robert Dodier <robert.dodier@gmail.com>,"Tue, 12 Jan 2016 13:27:33 -0800",Re: Dependency on TestingUtils in a Spark package,Reynold Xin <rxin@databricks.com>,"

OK, not a big deal, I was just hoping to avoid that, in part because the stuff
I'm working on is also proposed as a pull request, and it seems like it would
be a good idea to have a uniform testing environment between the PR
and the Spark package.

best,

Robert Dodier

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Tue, 12 Jan 2016 21:34:35 +0000",Re: Tungsten in a mixed endian environment,Steve Loughran <stevel@hortonworks.com>,"(x86 is little-endian and SPARC / POWER / ARM are big-endian; I'm sure
that was just a typo)

ote:
 work
on
,
e

---------------------------------------------------------------------


"
Steve Loughran <stevel@hortonworks.com>,"Tue, 12 Jan 2016 22:27:54 +0000",Re: Tungsten in a mixed endian environment,"""dev@spark.apache.org"" <dev@spark.apache.org>","
On 12 Jan 2016, at 10:49, Reynold Xin <rxin@databricks.com<mailto:rxin@databricks.com>> wrote:

How big of a deal this use case is in a heterogeneous endianness environment? If we do want to fix it, we should do it when right before Spark shuffles data to minimize performance penalty, i.e. turn big-endian encoded data into little-indian encoded data before it goes on the wire. This is a pretty involved change and given other things that might break across heterogeneous endianness environments, I am not sure if it is high priority enough to even warrant review bandwidth right now.



This is a classic problem in distributed computing, which has two common strategies


the SunOS RPC strategy: fixed order. For Sun, hence NFS, the order was that of the Motorola 68K, so cost-free on Sun workstations. As SPARC used the same byte ordering; again, free. For x86 parts wanting to play, inefficient at both sending and receiving. Protobuf has a fixed order, but here little-endian https://developers.google.com/protocol-buffers/docs/encoding.

Apollo RPC DCE strategy: packets declare byte order, recipient gets to deal with it. This is efficient in a homogenous cluster of either endianness, as x86-x86 would be zero-byteswapping. The Apollo design ended up in DCE, which is what Windows Distributed COM uses.  ( http://pubs.opengroup.org/onlinepubs/9629399/chap14.htm ). If you look at that spec, you can see its floating point marshalling that's most trouble.

recipient-makes-good is ideal for clusters where the systems all share the same endianness: the amount of marshalling is guaranteed to be zero if all CPU parts are the same. That's clearly the defacto strategy in Spark. On contrast, the one-network-fomat is guaranteed to have 0 byteswaps on CPUs whose endian matches the wire format, guaranteed to be two for the other part (one at each end). For mixed-endian RPC there'll be one bswap, so the cost is the same as for the apollo DCE.

Bits of hadoop core do byteswap stuff; for performance this is in native code; code which has to use assembly and builtin functions for max efficiency.

It's a big patch ‚Äîone that's designed for effective big-endian support, *ignoring heterogenous clusters*

https://issues.apache.org/jira/secure/attachment/12776247/HADOOP-11505.007.patch

All that stuff cropped up during Alan Burlinson sitting down to get Hadoop working properly on Sparc ‚Äîthat's a big enough project on its own that worrying about heterogenous systems isn't on his roadmap ‚Äîand nobody else appears to care.

I'd suggest the same to IBM: focus effort & testing on Power + AIX rather than worrying about heterogenous systems.

-Steve
"
Luciano Resende <luckbr1975@gmail.com>,"Tue, 12 Jan 2016 14:30:29 -0800",Spark and Export Classification,dev <dev@spark.apache.org>,"I was looking into the Apache export control page, and didn't see Spark
listed there, which from my initial investigation seemed ok because i
couldn't find any handling of cryptography in Spark code.

Could someone more familiar with the Spark dependency hierarchy confirm
that there is no specific crypto code/dependency on Spark.

Thank you

[1] http://www.apache.org/licenses/exports/

-- 
Luciano Resende
http://people.apache.org/~lresende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
"""Randy Swanberg"" <rswanber@us.ibm.com>","Tue, 12 Jan 2016 17:33:22 -0600",Re: Tungsten in a mixed endian environment,Sean Owen <sowen@cloudera.com>,"FWIW,  POWER is bi-endian.   AIX still runs big-endian on POWER,  but the 
latest Linux distros for POWER run little-endian (in fact Ubuntu for POWER 
only runs LE). 

> (x86 is little-endian and SPARC / POWER / ARM are big-endian; I'm sure
> that was just a typo)
> 
> On Tue, Jan 12, 2016 at 9:13 PM, Steve Loughran <stevel@hortonworks.com> 
wrote:
> > It's notable that Hadoop doesn't like mixed-endianness; there is work
> > (primarily from Oracle) to have consistent byteswapping ‚Äîthat is: work
> > reliably on big-endian systems
> > https://issues.apache.org/jira/browse/HADOOP-11505 ). There's no 
motivation
> > to support mixed-endian clusters.
> >
> >
> > The majority of clusters x86, there's only 3 cpu families that are 
little
> > endian: Spark, Power, Arm. Adam has clearly been playing with Power + 
x86,
> > but I'd suspect that's experimentation, not production.
> >
> > What is probably worth checking is mixed endian-ness between client 
apps
> > submitting work and the servers: Java and Kryo serialization should 
handle
> > that automatically.


Randy Swanberg




"
Cheolsoo Park <piaozhexiu@gmail.com>,"Tue, 12 Jan 2016 15:48:29 -0800","Re: Enabling mapreduce.input.fileinputformat.list-status.num-threads
 in Spark?",Alex Nastetsky <alex.nastetsky@vervemobile.com>,"Alex, see this jira-
https://issues.apache.org/jira/browse/SPARK-9926


"
Alex Nastetsky <alex.nastetsky@vervemobile.com>,"Tue, 12 Jan 2016 21:11:56 -0500","Re: Enabling mapreduce.input.fileinputformat.list-status.num-threads
 in Spark?",Cheolsoo Park <piaozhexiu@gmail.com>,"Thanks. I was actually able to get mapreduce.input.
fileinputformat.list-status.num-threads working in Spark against a regular
fileset in S3, in Spark 1.5.2 ... looks like the issue is isolated to Hive.


"
David Russell <themarchoffolly@protonmail.com>,"Tue, 12 Jan 2016 21:39:45 -0500",Re: ROSE: Spark + R on the JVM.,Richard Siebeling <rsiebeling@gmail.com>,"Hi Richard,

> Would it be possible to access the session API from within ROSE,
> to get for example the images that are generated by R / openCPU

Technically it would be possible although there would be some potentially significant runtime costs per task in doing so, primarily those related to extracting image data from the R session, serializing and then moving that data across the cluster for each and every image.

From a design perspective ROSE was intended to be used within Spark scale applications where R object data was seen as the primary task output. An output in a format that could be rapidly serialized and easily processed. Are there real world use cases where Spark scale applications capable of generating 10k, 100k, or even millions of image files would actually need to capture and store images? If so, how practically speaking, would these images ever be used? I'm just not sure. Maybe you could describe your own use case to provide some insights?

> and the logging to stdout that is logged by R?

If you are referring to the R console output (generated within the R session during the execution of an OCPUTask) then this data could certainly (optionally) be captured and returned on an OCPUResult. Again, can you provide any details for how you might use this console output in a real world application?

As an aside, for simple standalone Spark applications that will only ever run on a single host (no cluster) you could consider using an alternative library called fluent-r. This library is also available under my GitHub repo, [see here](https://github.com/onetapbeyond/fluent-r). The fluent-r library already has support for the retrieval of R objects, R console output and R graphics device image/plots. However it is not as lightweight as ROSE and it not designed to work in a clustered environment. ROSE on the other hand is designed for scale.

David

""All that is gold does not glitter, Not all those who wander are lost.""



-------- Original Message --------
Subject: Re: ROSE: Spark + R on the JVM.
Local Time: January 12 2016 6:56 pm
UTC Time: January 12 2016 11:56 pm
From: rsiebeling@gmail.com
To: mail@vijaykiran.com
CC: cjnolet@gmail.com,themarchoffolly@protonmail.com,user@spark.apache.org,dev@spark.apache.org



Hi,

this looks great and seems to be very usable.
Would it be possible to access the session API from within ROSE, to get for example the images that are generated by R / openCPU and the logging to stdout that is logged by R?

thanks in advance,
Richard



On Tue, Jan 12, 2016 at 10:16 PM, Vijay Kiran <mail@vijaykiran.com> wrote:

I think it would be this: https://github.com/onetapbeyond/opencpu-spark-executor

> On 12 Jan 2016, at 18:32, Corey Nolet <cjnolet@gmail.com> wrote:
>


> David,
>
> Thank you very much for announcing this! It looks like it could be very useful. Would you mind providing a link to the github?
>
> On Tue, Jan 12, 2016 at 10:03 AM, David <themarchoffolly@protonmail.com> wrote:
> Hi all,
>
> I'd like to share news of the recent release of a new Spark package, ROSE.
>
> ROSE is a Scala library offering access to the full scientific computing power of the R programming language to Apache Spark batch and streaming applications on the JVM. Where Apache SparkR lets data scientists use Spark from R, ROSE is designed to let Scala and Java developers use R from Spark.
>
> The project is available and documented on GitHub and I would encourage you to take a look. Any feedback, questions etc very welcome.
>
> David
>
> ""All that is gold does not glitter, Not all those who wander are lost.""
>




---------------------------------------------------------------------
To unsubscribe, e-mail: user-unsubscribe@spark.apache.org
For additional commands, e-mail: user-help@spark.apache.org"
Li Li <fancyerii@gmail.com>,"Wed, 13 Jan 2016 15:58:30 +0800",Re: running lda in spark throws exception,Bryan Cutler <cutlerb@gmail.com>,"I have set up a stand alone spark cluster and use the same codes. it
still failed with the same exception
I also preprocessed the data to lines of integers and use the scala
codes of lda example. it still failed.
the codes:

import org.apache.spark.mllib.clustering.{ LDA, DistributedLDAModel }

import org.apache.spark.mllib.linalg.Vectors

import org.apache.spark.SparkContext

import org.apache.spark.SparkContext._

import org.apache.spark.SparkConf


object TestLDA {

  def main(args: Array[String]) {

    if(args.length!=4){

      println(""need 4 args inDir outDir topic iternum"")

      System.exit(-1)

    }

    val conf = new SparkConf().setAppName(""TestLDA"")

    val sc = new SparkContext(conf)

    // Load and parse the data

    val data = sc.textFile(args(0))

    val parsedData = data.map(s => Vectors.dense(s.trim.split('
').map(_.toDouble)))

    // Index documents with unique IDs

    val corpus = parsedData.zipWithIndex.map(_.swap).cache()

    val topicNum=Integer.valueOf(args(2))

    val iterNum=Integer.valueOf(args(1))

    // Cluster the documents into three topics using LDA

    val ldaModel = new
LDA().setK(topicNum).setMaxIterations(iterNum).run(corpus)


    // Output topics. Each is a distribution over words (matching word
count vectors)

    println(""Learned topics (as distributions over vocab of "" +
ldaModel.vocabSize + "" words):"")

    val topics = ldaModel.topicsMatrix

    for (topic <- Range(0, topicNum)) {

      print(""Topic "" + topic + "":"")

      for (word <- Range(0, ldaModel.vocabSize)) { print("" "" +
topics(word, topic)); }

      println()

    }


    // Save and load model.

    ldaModel.save(sc, args(1))

  }


}

scripts to submit:

~/spark-1.5.2-bin-hadoop2.6/bin/spark-submit --class
com.mobvoi.knowledgegraph.scala_test.TestLDA \

    --master spark://master:7077 \

    --num-executors 10 \

    --executor-memory 4g \

    --executor-cores 3 \

    scala_test-1.0-jar-with-dependencies.jar \

    /test.txt \

    100 \

    5  \

    /lda_model

test.txt is in attachment


0 1 2 3 4 5 6 2 7 8 9 10 11 12 13 14 15 16 8 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 32 35 36 37 38 15 39 40 41 42 5 43 44 17 45 46 42 47 26 48 49
50 51 52 53 54 5 55 56 57 58 59
50 51 55 26 60 61 5 62 63 64 65 32 66 5 67 68 69 48 70 71 72 5 2 73 5 9 74 53 75 17 76 77 78 79 80 4 5
81 82 83 5 84 85 86 87 88 5 89 90 78 53 91 17 92 5
93 94 95 96 97 98 99 100 101 96
85 90 86 102 103 5 69 2 104 7 105 5 96
106 107 108 109 110 111 112 113 104 16 5
9 98 114 115 69 116 26 117 118 119 26 120
125 126 127 128 5 129 11 130 16 131 132 133 5 134 26 135 136 5 137 138 60 139 140 141 142 143 144 142 55 2 145 146 147 148 149 135 5 78 150 151 85 152 153 154 69 10 155 125 156 126 19 157 155
158 2 159 160 161 162 163 164 164 164 165 166 85 167 5 9 14 168 7 169 5 170 171
90 172 173 174 175 176 177 178 5 179 180 60 5 181
146 182 183 5 9 26 16 184 185 186 154 5 30 186 187 16 188 189 190
191 4 192 193 194 195 196 97 78 197 99 198 199 200 201
50 51 202 203 5 204 78 15 205 206 207 5 208 209 210 211 212 5 213 214 87 140 186 99
50 51 215 216 217 78 174 153 218 76 219 220 221 222 5 172 174 153 5 223 224
50 51 225 226 227 4 17 228 229 230 153
50 51 150 10 155 189 231 232 4 5 55 233
50 51 60 96 97 234 235 129 98 48 236 128 5 237 96
238 16 100 26 81 239 189 100 4 153
50 51 9 116 184 247 16 5 7 248 249 250 251
50 51 76 237 252 253 254 255 128 5 190 53 188
256 257 2 4 258 5 259 5 80 260 261 262 263 264 5 265 266 267 268
269 270 271 272 5 9 273 31 274 275 276
277 278 5 142 279 134 280 281 5 194 282 283 134 284 210 285 134 286 32 287 288 210 289 290 5 291 292 287 288 168 26 7 293 173 82 287 288 294 155 295 296 69 155 297 85 134 286 298 32 299 300 301 302 303 304 305 305 306 17 307 134 286 210 293 173 214 4 308 309 310 311 186 312 313 134 286 68 5 314 210 315 74 31 2 316 283 134 1 317 134 68 5 314 318 5 319 320 5 321 322 323 324 325 326 327 328 134 286 68 7 172 173 329 330 74 331 332 333 334 308 335 7 336 5 337 338 5 306 324 333 339 340 341 331 134 69 60
26 270 271 26 270 271 342 2 343 343 5 344 345
50 51 57 346 347 348 98 349 350 351 352 353 353 5 354 355 17 57 346 96
50 51 189 100 4 5 153 218 26 356 357 55 69 60 2 358 359 5 360
361 362 363 364 365 366 367 57 368 369 128 369 128 370 362 361 361 362 371 372 373 362 361 374 375 376 233 2 377 7 302 4 5 2 358 121 122 5 378 361 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 379 374 380 5 374 381 361 118 382 383 384 7 8 5 16 385 386 387 388 389 390 5 386 391 388 103 5 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 383 392 7 393 394 389 395 5 374 375 396 397 398 399 400 17 401 402 403 5 404 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 405 406 31 174 407 16 408 409 361 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 389 142 81 5 410 265 374 381 2 411 412 5 413 414 415 416 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 313 32 382 383 417 17 418 419 420 421 422 17 96 423 4 424 5 425 396 426 400 17 410 427 428 361
429 430 165 431 432 433 434 435 436 5 437 129 438 439
440 441 442 443 5 444 445 446 447 448 449 5 450 451 382 9 2 452 149 453 410 5 454 17 443 32 142 455 392 389 142 456 457 458 329 443 459 460 313 313 313 382 461 443 5 455 32 440 441 442 194 462 463 5 464 37 464 465 265 466 17
307 69 2 283 284 5 281 194 239 467 468 469 470 471 472 116 473 7 296 82 474 162 475 307 244 476 307 57 19 477 478 26 479 194 392 7 480 481 482 483 484 309 485 32 194 486 69 155 487 488 489 490 287 288 5 491 194 7 329 339 492 493 149 338 5 78 493 339 494 495 339 496 341 69 121 338
497 498 139 499 17 154 500 501 17 4 17 502 503 5 504 505 506 104 57 5 214 507 508 5 261 509 238 16 8 510 509 511 512 149 513 5 2 514 515 353 155 516 517 512
518 519 121 16 520 188 521 16 522 523 524
50 51 150 525 526 238 16 527 238 233 528 529
50 51 237 18 265 97 530 531 69 48 532
533 103 5 31 7 534 505 142 535 142 5 536 505 69 7 537 538 539 389 540 541 97 542 543
50 51 544 519 4 153 5 545 97 234 546 547 548 98 549 550 551 17 552 553 554 5 555 78 104 7 556 5 118 557 5 558 188 17 410 34 116 559 560 556 98 188 561 384 153 218 26 356 562 134 202 265 563 564 565 130 566 31 567 568 569 206 210 332 206 570 10 571 336 572 219 26 573 574 26 575 5 154
50 51 576 577 578 5 579 162 5 97 580 99 581 582 392 583 584 255 582 186 585 586 587 128 5 588 589 590 591 592 15 48 4 17
244 595 5 16 596 5 597 31 75 17 598 599 10 247 600
50 51 601 4 602 603 604 605 103 69 583
606 4 607 608 609 242 610 611 612 613 98 588 17 614 58 615 616 174 617 618 617 619 620 7 621 622 389 623 624 625 7 142 626 627 31 2 122 628 629 97 188 191 5 630
50 51 50 631 632 633 634 635 636 5 84 96 135 142
85 637 638 186 294 639 242 640 121 122 131 641 642 643 644 645 646 647 367 354 648 131 649 650 651 69 244
153 5 48 652 404 653 654 356 18 655 46 48 354 239
50 51 656 5 521 657 26 93 658 53 659
519 609 97 8 118 26 7 660 48 661
544 519 85 662 663 664 665 666 247 57 667 668 669 618 670 415 420 671 218 16 7 672 673 674 675 63 676 677 678 679 680 681 239 682 330 532 683 684 685 686 218 656 687 685 688 60 689 690 691 155 692 693 30 155 99 694 695 512 672 696 2 697 548 196 698 63 699 700 701 5 702 703 704 705
50 51 706 358 281 173 31 684 7 684 707 149 708 5 265 709 209 710 5 69 10 53 93 96 149 48 711 5 265 97 530 712 142 281 31 713 17 26 714 715 96 689 716 717 4 96
50 51 718 242 392 719 48 720 721 17 247 16 5
722 723 724 725 726 37 727 728 265 729 730 85 17 731 732 17 162 17 464 37 5 733 734 647 367 735 736 737 738 739 735 423 740 741 742 743 744 745 746 747 414 19 748 46 749 641 750 751 210 752 753 754 26 755
50 51 52 756 757 55 78 758 759 5 16 235 236 150 760
475 5 761 78 26 762 5 32 763 719 764 31 2 48 765 766 8 767 768 769 770 771 46 591 11 772 17 773 774 775 31 776 777 778 779 780 162 781 5 782 605 773
32 783 784 785 56 786 17 787 5 788 789 790 367 791
792 793 794 5 795 796 797 798 799 17 800 801 118 802 803 46 804 805 806 651 5 802 162 5 807 808 411 2 809 7 810 811 651 5 808 69 812 5 60 98 813 5 129 131 814 5 815 816 98 817 244 818 819 32 808 37 820 17 821 822 823 26 16
50 51 628 60 824 99 679 2 825 339 5 16 96
706 757 826 150 827 16 313 151 16 119 184
828 829 828 830 5 831 244 17 832 833 834 611 80 419 5 657 835 153 521 836 212 223 149 837 5 838 839 17 840 377 85 17 96
26 841 5 842 843 533 119 5 69 8
844 845 2 846 278 847 848 849 850 851 96 852 853 854 855
856 857 718 858 125 197 859 46 32 860
861 862 121 81 731 4 2 863 864 4 476 337 5 204 6 2 57 865 5 365 866 17 867 868 5 869 870 650 871 872 873 476 864 5 874 875 476 337 6 2 876 4 476 82 877 878 4 5 476 352 476 5 879 32 476 880 337 121 881 5 2 882 69 85 883 774 5 337 884 70 882 885 886 85 17 887 14 888 889 882 890 891 892 206 774 5 337 893 2 679 894 774 895 337 896 605 804 897 898 210 153 899 4 5 309 265 900 173 901 902 99 903 4 475 904 2 902 177 679 81 18 905 2 70 242 588 906 4 907 588 5 194 745 908 909 194 69 53 588 19 910 382 911 131 912 605 70 816 913 382 911 914 604 650 722 7 358 915 2 916 85 423 917 804 189 367 0 918 664 919 329 315 74 920 921 5 2 922 923 216 32 924 925 926 927 928 929 5 131 930 5 628 593 931 129 32 932 933 934 912 605 435 586 935 936 465 937 938 5 939 940 7 941 256 96 942 943 329 944 206 337 945 944 206 945 946 947 944 206 947 327 948 337 89 949 17 774 5 950 951 291 10 32 952 328 340 341 953 954 955 956 340 341 957 958 959 32 824 33 339 960 961 103 5 382 340 341 816 26 962 5 963 314 964 142 963 965 438 966 895 967 968 218 969 2 354 970 17 971 162 972 410 973 974 975 900 347 476 382 900 506 53 122 131 154 30 976 153 358 977 978 979 980 10 218 981 982 579 983 984 985 863 986 177 32 987 122 481 988 989 132 990 991 128 218 906 980 190 839 17 992 128 5 2 993 994 16 57 5 994 750 121 995 774 996 69 400 17 997 998 999 885 97 180 1000 914 142 1001 1002 1003 1004 103 5 1005 265 731 5 866 388 1006 722 863 1007 1008 1009 1010 4 17 916 916 920 921 5 2 140 314 899 314 2 1011 1012 1013 0 903 17 1014 604 605 1015 5 1011 1012 1016 367 329 1017 1018 1019 17 774 210 1007 863 774 1020 10 1021 774 1022 1023 1012 17 855 7 1007 1024 1025 218 1007 1026 218 165 1027 605 995 1028 314 2 1029 5 1030 1031 904 2 1032 804 266 1031 1033 1034 367 1022 1035 1036 1037 1038 1024 328 900 1039 1031 1040 367 1041 1042 1043 1042 1044 1007 1045 218 1046 68 1047 5 904 265 907 99 388 893 416 503 1048 1049 5 986 177 875 1050 1051 186 1052 162 802 5 1031 855
50 51 121 312 118 11 651 153 5 96 1053 30 2 11 1054
1055 32 1055 1056 37 168 7 1057 1058 210 1059 653 328 1060 309 1055 392 7 698 97 1061 622 1062
162 1063 80 1064 96 1065 1066 1067 5 1068 784 96 168 389 1069 1070 69 26 1071 96
1072 4 5 97 233 558 981 46 155 17 32 1073 311 1074 1075 603 32 1076 5 150 303 1077 603 1078 26 1079 382 531 210 1080 1081 719 80 1082 1083 392 1084 1085 1086 202 1087 922 1088 1089 533 5 1090 1091 210 1092 1093 1094 1095 1096 1097 1098
150 26 16 238 26 16 1099 26 16 1100 1101 10 1102
50 51 10 60 18 265 97 1103 530 17 34 4 6 46 1104 4 1105 2 4 206 97 1106 116 206 1107
50 51 100 26 81 153 17 1108 419 689 265 1109 354 1110 740 588 740 1110
85 1111 1051 1112 1113 1114 698 5 1115 210 1116 155 369 1117
475 5 761 48 1118 48 762 131 26 1119 1120 185 413 2 1121 1122 1123
1126 358 1127 774 32 1128 1129 5 1130 1131 5 1132 1133 2 397 5 802 31 2 1134 1134 96 78 1135 446 1136 265 48 229 1135 1137 5 1138 243 438 1139 1140 210 519 1083 683 1141 512 118 69 688 1131 397 17 1142 347 26 71 10 683 1141 131 392 1143 1144 1145 96
1146 6 2 940 60 5 26 7 1147 67 4 5 1148 324 363 804 1149 1150 97 1151 1152 26 233 465 358 1153 1154 411 155 1155
1065 774 861 298 5 1156 10 653 1157 5 1117 1158 1117 1117 1117
50 51 153 218 26 356 26 239 1159 1160 97 4 153 96
50 51 31 2 1161 31 2 1161 31 2 1161 244 521 96 1162 1163 96 1164 1165 465 46 142 1166 1167 5 1168 96 1169 907 837 1170 1171 1172 895 96 96 96
238 1173 150 151 1174 81 55 5 1175 18
723 724 1176 1177 69 1112 299 5 1178 1179 32 1180 1181 1182 142 1183 751 46 155 1184 214 1128 1185 5 1186 1187 1188 1189 118 1190 2 774 85 5 30 1191 552 1192 5 1178 1193 1194 31 1195 1196 5 552 252 1197 1198 722 1199 1200 17 1201 739 5 1202 774 1203 205 1204 1205 103 5 48 1206 97 5 122 1207 131 1208 68 1209 244 1210 248 249 1211 46 1212 26 143 5 1213 1214 774 735 1215 735 423 774 516 369 17 666 46 344 1216 4 17 1217 210 107 1021 889 5 204 1211 31 1021 774 4 5 354 1218 17 15 1219 1220 4
315 223 1221 95 131 392 128 5 1000 750 1222 26 233
50 51 238 10 155 150 26 60 1223 653 1224 4 5
50 51 863 1225 5 1083 60 97 53 99 10 1226 1227
1158 1228 17 1229 1229 177 32 475 5 1230 26 1231 31 377 1232 0 1233 5 32 1234 1235 1236 85 1111 1051 186 4 1237 1238 1117
521 1239 10 155 392 7 1240 1241 1242 186 713 26 1243 5 715 689 1244 4 5 97 205 1245 97 981 1246 1242 1241 17 810 1247 1248 1249 1250 1251 5 1252 1253 8 31 116 839 1254 10 155 265 1255 1256 7 10 1257 17 358 597 1021 1195
1258 912 465 1259 1260 1261 618 265 17 1262 26 16 69 2 1263 1264 1114 5 1265 1266 8 97 32 475 1267
50 51 711 60 855 528 1268 653 39 837 1269 1270
1154 1021 1271 475 1272 1273 1274 97 53 188 19 475 5 814 1275 1276 149 1277 5 313 87 5 1278 815 26 1279 1280 1281 1282 1283 382 1284 1285 210 1286 2 1287 210 1288 140 1177 189 1274 1289 1250 1290 1291 378 32 507 1278 770 32 1292 1293 129 32 1294 1294 1295 893 26 356 1296 774 210 1297 32 475 1294 162 17 1298 358 1299 5 1300 1301 122 5 1103 586 1260 1278 770 32 774 1292 5 107 177 1021 87 265 1302 1303 1304 1117 1117 774 210 1297 31 26 356 5 1305 1306 174 1307 1308 1309 1310 14 1311 1312 149 1313 5 9 69 32 475 1277 5 1278 1314 1315 210 1316 5 97 618 32 507 129 893
1317 206 530 1318 17 18 1087 1319 1320 32 531 1321 1322 5 1323 1324 1325 895 1274 1326 1327 452 1051 186 4 1328 895 1329 5 125 1330 48 837 118 78 4 17
55 247 16 5 265 1190 354 1331 255 1332 244 17 1333 1334 80 1335 1336 1337
7 1338 5 655 210 217 166 162 309 895 389 411 5 1339 1340 69 2 1051 1112 914 1341 611 25 1342 5 1343 10 174 1344 5 482 940 5 841 93 895
1142 544 519 94 189 100 4 1345 1346 232 1347 1348 119 94 1008 218 26 356 96 1346 232 119 121 1349 1296 1350 94 150 1351 94 1352 655 31 153 218 1353 356 96 96 1199 5 1354 1355 1356 1357 1358 94 0 1159 31 128 218 26 40 1359 1346 232 10 1174 1360 885 988 1361 1362 773 773
50 51 60 4 5 204 97 234 238 155 96
129 452 83 5 1363 1246 1364 1365 32 547 96 547 60 1366 1367 542 1368
10 244 411 1369 47 1370 4 5 6 2 242 78 1371 895 133 26 1372 547 98 1373 5 549 131 210 1374 78 7 1375 516 86 10 155 1376 129 1377 1378 1187 4 1374 118 628 210 547 79 4 86 135
1379 4 5 55 1380 238 60 1381 1382 750 60 265 672 1383 1384 127 519 1385 97 234 1386 804 1387
50 51 1388 1389 1390 1391 1392 1393 1394 1395 1396
50 51 570 10 60 706 358 314 972 904 610 1397 30 972 981 500 5 1398 142 419 1274 26 970 26 970 26 970 242 1399 1400 588 34 244 103 1401 599 1402 377 1220 4
121 16 593 1403 55 5 96 1404 593 11 30 405 1305 174 1405 1102 55
1406 1407 1408 26 1409 5 129 1410 210 163 5 1411 129 336 475 39 1412 261 5 1413 8 31 2 7 754 256 5 1414 210 1415 671 69 26 1416 5 129 638 125 69 2 235 1417 1418 575 1072 1419 1420 31 1421 389 1422 118 411 2 1423 48 1424 1425 5 1426 69 1427 244 1428 5 129 163 1410 5 1429 411 1430
475 139 5 97 8 1431 770 32 1432 5 129 26 60 475 5 1433 1434 1435 368 1220 243 1436 1437 618 32 734 69 2 1293 475 1438 2 48 446 1439 1440 5 1439 1440 5 9 186 446 1441 835 1439 180 174 1442 46 1442 17 1117
34 1443 1170 490 1444 774 70 1445 1446 69 155 1447 5 1448 1449
639 886 5 532 309 32 475 1450 17 80 1386 265 1451 1452 1453 134 596 1454 1455 202 1456 1107 26 16 131 1457 1458 1459 640 7 698 252 1460 1461 244 1462 69 244 1455 1296 244 458 8 97 31 48 1463 15 686 5 39 1464 1427 914
1465 5 1466 547 82 299 770 794 1467 1468 1469 69 162 388 1470 7 103 1471 1472 5 118 21 1423 1473 68 149 1474 5 1475 384 26 1398 420 69 7 1476 1477 437 1478 5 1479 1480 7 1481 1482
50 51 97 530 17 154 69 46 153 17 1483 223 118 863 107 1244 78 26 356 5
1484 26 16 1485 26 1471 595 1486 1487 1488 354 1326 1489 17 8 1490 734 31 7 1491
50 51 26 711 561 153 218 26 356 96 1246 561 82 1492 68 1493 5 939 937 1494 1495 17 1496 96
861 1497 19 1498 1499 2 149 1500 5 893 7 1501 1502 1503 697 5 129 17 1504 1504 722 1505 1506 1507 532 683 131 954 1508 7 1509 5 96 1142 956 1510 1511 1512 1513 228 1514 889
50 51 4 17 8 1515 1516 60 1517 1518 855 96
774 70 1072 4 39 600 96 731 4 5 204 2 1519 97 1520 99 614 129 5 34 1223 155 206 246 1521 1522 204 1523 37 5 1524 69 26 7 750 5 512 155 1525 1526 1527 118 1528 1529 1530
82 1531 344 1532 1533 48 618 1534 5 1535 37 17 1536 1537 382 172 173 347 1443 1538 5 172 173 131 1539 1540 1541 1542 1543 210 26 7 754 256 1544 5 1545 210 1546 180 26 1101 895
125 1547 512 79 1548 512 512 512 512 512 512
50 51 504 214 595 5 698 17 446 358 135 96 96 96 96 96 96 96 96 96 96 96 96 96 96
32 1549 1550 1551 80 1552 1553 5 1521 31 1554 533 128 5 26 841
964 1555 5 1556 1557 1558 920 926 1559 162 57 861 698 1515 17 130 26 346 4 128 1560 103 5 329 1561 1562 1563 14 982 46 516 110 1564 1565 465 1566 202 1567 1568 1569 1570 1571 1572 1573 1574 1575 5 1576 31 7 1274 773 46 233 982 773 1577 542 1578 1548 465 773
323 212 51 1021 5 1579 1580 134 1402 1581
109 128 5 1582 1583 1360 135 914 954 1584 0 97 1585 1586 354 233 1105 0 186 128 693 693
26 1587 5 850 1588 347 377 1589 131 162 17 1590 435 1591 1565 1592 5 670 265 1501 1593
382 1594 1595 1209 2 1596 1597 5 1598 1021 87 51 5 1596 969 1599 1600 1601 1602 716 87 1594 1595 265 1603 1604 5 1605 1606 1607 1021 1608 1609 1610 1611 31 32 446 1612 1613 1614 1615 69 438 1616 32 17 1617 1618 87 1614 2 210 1617 5 845 1619 1620 5 6 204 5 845 1289 1621 845 1296 1317 5 1615 1622 87 1623 1120 1624 1289 1625 1626 1123 1627 32 1628 389 494 1629 1630 5 1594 1595 1631 506 186 1632 1633 1189 87 1594 1595 265 1615 303 5 1634 547 1635 32 843 1636 31 7 1637 794 1245 1638 1639 5 204 46 863 1640 1641 1612 1642 5 1643 1644 31 155 937 1645 1646 1647 1496 1117
50 51 121 16 1648 246 1139 1649 1650 1050 5 298 1651 10 653 1220 4 5
50 51 121 711 238 60 265 392 1652 839 96
1653 475 31 2 1654 1655 5 1475 129 1656 476 5 1657 1658 7 1560 7 1659 1660 7 1661 1662 1663 336 720 1664 1665 17 465 1666 1653 722 106 5 1667 1668 1609 1669 1670 477 1106 1671 1672 1673 1022 1653 1024
1674 1675 1138 1676 26 1677 5 142 1678 1679 1021 98 1680 1681 26 1682 506 1683 1684 129 31 829 1685 1686 278 5 1687 2 253 1688 5 1689 1690 1691 212 5 556 31 1692 1677 1693 722 392 438 1694 1274 1245 17 358 1689 1695 1696 465 1697 18
956 1698 1699 603 354 1700 26 1701 1702 1703 1704 69 48 837
242 1705 588 1706 4 5 204 969 1707 67 69 186 195 1016 5 204 228 1530 392 116 184
774 954 1514 1021 1708 242 1709 400 69 1181 1710 96 96 214 382 50 10 837 96 96
26 841 5 645 1711 1712 379 17 1713 1694 5 16 9 1714 1715 808 1716
50 51 2 802 154 4 5 1717 31 187 1718 17 855
82 1719 1720 1721 1722 1723 475 1724 513 5 142 1725 265 722 1726 1727 1402 2 1728 37 149 708 5 1729 1380 96 1730 382 286 37 1731 5 527 1732 46 615 1733 5 1734 32 87 1735 1736 1737 1738 17 37 1739 1740 937 1741 2 286 5 303 69 1673 2 1728 5 1742 82 1743 162 1744 1745 1746 1747 1726 1727 1748 5 1749 46 244 1750 1458 1751 708 1752 87 1753 1177 5 681
382 443 1754 1466 5 309 1755 1756 78 1112 1694 1757 914 1758 1759 720 1760 242 5 309
382 309 1520 5 1761 1762 466 265 48 1083 18
717 85 5 97 969 530 17 802 0 1763 46 1764 734 1765 1496 118 1107 247 1042
1766 1767 5 1768 1453 97 1769 17 31 1770 1771 5 129 1087 1772 653 1773 87 1735 808 5
50 51 1589 1251 1142 530 97 153 17 1499 46 912 17 1774 78 26 60 5
491 1775 19 684 7 684 1687 684 7 684 1776 914
50 51 239 706 396 1777 1778 5 100 859 1779 1780 1141 174 153 1781 1548 1782
1783 32 1784 1785 142 1100 1786 5 1787 186 1788 1789 1790 539 4 1065 1791 682 1792 1793 1794 5 1795 1796 1107 1797 7 1798 5 1799 210 1800 37 5 670 26 1177 26 1801 69 1802 17 8 1803 1792 965 85 1804 1805 94 1806 94 1807 1808 7 1809 746 1810 10 247 1811 5 1812 1107 26 16 155 435 1813 1814 475 5 1808 7 8 1815 69 1816 32 475 1817 575 1818 823 26 16 78 940 60 142 1819 1273 5 309
50 51 121 60 5 9 655 1352 31 26 1095
1820 309 926 155 609 1821 5 9 97 234 155 1822 617 206 617 1821 895
382 808 1823 17 264 264 97 17 18 969 2 40 93 1600 1824 1825 914
50 51 194 238 60 1087 302 219 46 80 309 1826 134 53 1581
103 31 16 519 242 1341 155 32 1830 410 1831
1832 475 5 1833 1834 1835 18 265 252 586 594 129 26 236 131 212 1687 55 2 244 5 7 1836 1216 81 1837 6 713 239 1838 1617 7 1839 19 1376 189 1840 651 5 78 1841 1677 5 1216 53 16 1842 5 1843 1844 16 57 773 773 186 1845 1187 1846 773 773 48 354 239 1494 773 773
116 1121 1851 5 309 131 238 78 656 5 206 99 17 1156 85 86 194 69 2 60 5
50 51 191 16 1852 1853 85 1854 465 1855 670 69 688 1856 614 1857 31 2 1858 5 1859 1860 670 26 16 1861 368 1862 1863 1864 1236 26 40
1865 194 265 16 57 5 924 1866 1802 8 302 32 1694 682 5
1137 32 87 1008 5 1867 55 57 1868 1869 116 731 85 17 1274 1870 1871 581 1872 10 32 446 1873 1874 10 863 0 1021 956 1875 180 26 356 810 722 1876 999 997 1877 1878 1879 5 996 69 1880 214 953 1881 235 758 759 5 16 1882 5 235 186 39 1277 1189
50 51 16 95 206 17 10 1514 1517 206 1268 1220 4
50 51 55 26 40 150 69 26 16 1443 0 2 1883 1884 5 1884 5 1885 98 188 96 1296 10 244 99 5 1886 96 1622 87 1427 212 31 26 60 96
1887 1888 1889 5 1890 614 97 37 1891 155 4 792 1892 69 155 586 618 1893 1890 109 155 608 1154 1021 109 7 358 1894 5 1895 1617 1896 122 618 610 1897 7 358 1898 265 17 389 1899 94 1062 328 1900 223 1901 644 1902 702 18 734 1903 17 159 1904 1821 5 1905 1906 1907 1908 69 244 679 93 5 9 516 110 86 639 1909 1910 405 1785 162 1063 80 1064 5 309
579 162 5 29 1911 31 10 244 1912 252 1913 210 1914 131 328 0 128 1915 1759 1911 650 1608 1916 1917 46 31 1918 17 129 60 400 17 1919 1920 400 17 1921 1922 129 210 212 9 5 1923 89 1924 131 1271 1925 1054 1926 827 1927
50 51 238 16 1100 1705 1786 78 186 1928 53 1101 97 69 48 679 99 104 40 5
140 216 31 16 95 1929 1930 57 1931 1932 78 104 1933 5 1268 1220 1328 1934 187 1220 1204 18 1935 1936 5 1937 16 355 5 1938 26 653 1939 15 69 2 358 1940 5 1941 593 32 428 128 1891 5 204 792 5 2 482 86 1107 104 60 5 482 1942 7 1069 1943 17 130 476 1944 5 1056 110 104 7 750 5 206 17 142 339 7 1945 2 1946 1947 909 238 1948 5 189 217 4 206 26 7 1949 1950 76 78 26 503 4 5
50 51 189 367 140 100 4 5 153 5 26 1951 96
50 51 121 711 684 685 31 155 1100 1101 238 69 60 96
100 121 81 265 56 122 17 1952 45 46 369 1915
1107 78 416 5 1852 1953 19 46 39 7 673 79 617 619
50 51 10 60 265 97 698 914 392 69 1954 189 128 5 1955 5 533 5 16 588 855
129 214 1956 1957 1958 1959 1960 48 1961 1962 740 57 509 1963 479 135 96
792 1964 1418 4 57 1965 1807 1966 1967 5 1107 1968 1969 1970 5 1287 1944 1971 1972 361
50 51 60 119 26 120 1973 1347 1879 69 26 60
50 51 672 1974 1975 58 59 1427 388 550 247 122 5 189 217 4 60 562 69 104 1530 5 618 1976 265 98 1977 76 219 1974 78 812 5 104 16 5 405 1978 693 74 1687 1979 389 1805 1585 17 1514 833 10 218 981 328 1980 1647 17 1981 405 1254 477 39 859 1189 980 1121 57 5 455 1982 1983 618 78 26 1984 5
50 51 342 2 238 16 1985 74 16 96 1986 1101
1129 5 1987 7 1988 389 358 1930 1989 1990 107 1244 940 1991 1099 438 1992 1993 1994 1995 1996 7 1997 5 7 1998 5 1663 1999 2000 248 249 69 174 2001 2002 255 103 5 11 2003 46 7 358 247 16 5 599 1376 174 2004 5 9 863 55 2005 1244 46 16 17
8 2006 2007 2008 214 2009 31 99 1072 2010 162 2011 5 1420 210 2012 5 1410 329 1560 1922 361 97 1086 2013 2014 653 1463 2015 286 1611 5 97 361 2016 2017 2018 2019 396 2020 2021 538 149 392 7 142 2022 533 5 2023 2024 2025 1063 1159 1051 186 1204 5 96
50 51 60 404 247 101 5 96 679 2 6 2026 96
50 51 656 5 96 992 186 189 2027 210 2028 99 2029 5 2030 96 893 16 71 162 26 93 96 96
50 51 462 5 581 1222 285 246 17 2031 599 46 48 2032 912 2033 1780 15 2034 2035 244 2036 2037 653 2038 8 2039 5
23 17 2040 438 1184 194 610 1004 142 97 194 10 11 411 1273 14 4 2041 155 4 938 2042
2043 2044 2045 2046 2047 97 354 2048 740 16 618 17 246 2049 1639 2050 5 2047 7 2051 235 2052 2053 2054 260 97 31 2055 2056 2057 5 1719 228 2058
50 51 2 2059 4 5 242 26 1547 939 2060 17 8 26 60
50 51 247 1530 5 96 118 2061 98 188 302 188 96
2062 1284 2063 843 2064 2065 7 8 2066 628 893 416 2067 808 1117
761 2068 682 5 16 9 96 2069 280 2070 686 5 2071 210 2072 96 410 358 135 96
50 51 60 574 575 57 836 26 93 265 1577 1400 99 1379 2073 217 48 618 2074 91 32 836 37 46 2075 2076 1352 2077 19 655 1209 48 1949 774 100 2078 5 618 2074 37 186 2079 2080 742 5 48 2081 655 10 1021 774 2082 48 2083 1065 103 2084 2085 774 2086 2087 2088 586 2089 1480 2090 2091 1772 96 48 2092 5 97 69 1480 2093 774 684 7 684 2094 96
2095 265 2096 2097 131 813 747 265 358 1731 5 2098 2099 17 18 96
50 51 2100 107 400 5 603 48 229 2101 570 2102 2103 446 358 2104 18 722 11 2105 107 5 2036 2106 96
87 954 239 2107 2108 233 107 2109 78 2110 475 2111 1877 69 2112 125 478 48 2112 581 2113 600 5 1168 2113 478 2114 2115 10 60 313 118 2116 2117 180 682 2118 5 16 9
130 863 232 861 265 2119 146 2120 2121 2122 96 392 7 533 2123 5 78 2124 5
2125 526 1888 31 2 1952 2126 32 1063 1810 1420 1955 410 2127 2128 1875 5 181
371 1609 426 2129 2130 364 32 504 2131 206 17 2132 2133 2134 396 2135 396 2136 2137 2138 2139 1298 895 290 2140 2141 2142 2143 2144 2145 2146 2147 2148
129 104 16 5 313 180 2149 2150 26 312 2151 2152
1406 2 453 2153 505 85 313 313 2154 1096 17 265 2116 2117 98
2155 1284 790 367 1287 1944 2156 177 1378 121 745 2157 96
637 2158 97 2159 60 2160 96 63 330 2161 1421 96 2162 238 60 96 1480 2163 908 2164 2164 96
244 2165 2166 2167 2168 2169 32 32 2170 37 74 760 670 16 240 721 37 1527 742 110 26 60 1759 810 125 2171 1356 2172 2173
50 51 26 16 2174 2043 1185 2175 757 380 664 1759 378 1964 2176 618 610 1397 46 162 79 80 206
813 2178 2179 78 26 1802 367 850 5 2180 78 53 841 5 93 96
475 210 2181 155 2182 684 7 2183 914 313 31 7 1246 586 210 2184 2185 2186 5 2187 129
2188 2189 26 239 0 1347 5 2190 805 604 228 2191 804 5 122 2192 162 1617 1975 104 2193 996 5 1404 1250 26 81 2194 91 400 242 604 5 125 98 588 2195 2196 2197 2198 1780 10 1924
50 51 837 238 69 60 599 10 1102 5
50 51 2199 286 392 16 71 298 586 99 189 2200 5 1922 2201 4 532 2202 5 2201 53 2203 78 2204 5 2205 5 265 1608 2206 2203 2207
265 302 85 110 5 1072 247 2208 236 48 1442 2209 1065 1066 5 707 719 2210 2211 2212 2213 31 653 1952 2214 2215 2216 5 2217 2218 2219 2101 2220
774 2221 2222 51 17 2181 5 2223 313 679 2 2224 2225 313 405 210 774 1083 5 302 31 174 2226 1548 246 1377 2227 313 2228 515 388
1971 438 39 95 93 17 2229 2230 78 2231 2232 618 18 1496
2233 285 2234 2235 2236 2237 2237 2237 2237 2237 2237
354 1468 2238 2239 26 7 2240 2241 96 162 637 5 31 2 2242 410 96 7 1051 309 961 96
217 210 655 5 178 286 1979 1910 2243 835 1246 2210 223 31 153 1436 689 205 282 1030 5 1285 2244
2245 2246 2247 2248 4 1088 69 2249 96 162 17 637 2250 2251 96 2252 2252 1960 2253 2254 2255 2256 96 2249 2257 96 122 2247 8 96 31 2 2258 5 505 99 96 2259 903 96
1783 2260 2261 2262 2263 2264 299 2265 5 2126 1212 5 2 299 300 402 2266 2267 5 64 2268 2269 32 1251 1955 17 2270 2271 7 1087 1838 32 6 2272 774 774 2273 46 618 17 774 2274 17 69 244 99 2275 2060 17 410 2260 5 2276 46 344 1216 132 17
238 827 2174 2277 2278 2279 2 2180 2280 2281 1050 5 2282 2283 2284 2285 1267 31 155 1772 155 2286 2287 155 2288 2289
1190 1459 615 5 628 2290 206 1274 70 202 244 595 926 5 1121 16 1631 2 2291 17 142 2292 206 2293 2294 638 2295 69 1443 2293 1016 5 2296 835 435 2297 69 46 1443 206 2298 2294
775 4 476 166 4 909 194 96 2299 2300 633 281 194 5 2301 96
2302 2303 392 5 2304 2131 426 2129 2305 364 32 475 206 17 2306 2307 5 2308 165 183
4 475 1402 93 217 177 5 2309 2310 232 2311 189 2312 2313 85 2314 18 96
774 5 2315 265 2316 2317 1243 5 2318 97 2319 917 5 2320 794 2321 2322 17 1481 722 2 2323 2324 242 103 5 69 31 2325 17 69 2 1101 2315 893 26 16 798 2326 774 458 17 308 9 109 7 358 210 746 1083 5 9 2327 1289 103 17 158 130 78 1250 2328 5 2329 16 2330 5 2331 920 921 5 15 2 235 2332 17 46 32 663 303 832 2333 5 206 158 2334 10 653 937 2335 2336 1862 2337 5 204 2338 2339 5 2340 530 354 2341 618 605 69 174 1695 358 2342 475 628 60 599 1405 2343 99 2344 5
2031 2345 2346 411 265 1916 2347 2348 1554 2349 2345 5 2350 1114 2351 1514 2352 1723 17 2346 728 242 116 1000 2353 17 246 46 604 17 2354 2346 10 247 57 5 130 10 571 11 158 1008 2345 7 2346 2355 2356 194 69 7 142 2357 234 10 2358 2359 19 475 10 2360 247 236 2357 5 1382 69 8 8 31 2361 542 2362 435 698 796 2363 5 2357 571 1246 774 2364 367 17 389 140 653 2365 2366 2367 5 5 2368 1453 774 920 2369 389 2370 2370 5 2371 494 774 149 81 5 92 2372 1937 17 938 17 8 2060 210 2373 153 5 10 247 356 5
50 51 129 233 18 96 56 53 122 96 2374 10 155 96 265 2026 125 2375 1085 611 388 96 122 588 96
2376 2377 2378 2379 2380 2381 2382 2383 2384 122 415
50 51 2385 218 9 96 121 218 2124 1268 10 1102 405 2386 2387 1220 2388
309 26 1472 1115 2389 859 740 93 599 10 653 32 85 153
129 1158 96 10 653 1220 4 153 5 96 121 356 914 774 96
1065 286 2392 5 2393 2394 112 2395 2396 1639 540 299 465 2397 1782 46 32 2398 389 2399 2400
538 728 313 2401 367 313 186 2402 242 313 1465 618 17 313 722 48 229 392 2374 2403 1453 774 824 87 2402
1107 2404 17 2101 125 69 155 873 1268 10 1102 5
1600 2405 5 309 2295 2406 2214 1151 1152 2407 2408 835 2409 2410 2411 2412 5 2413 2414 298 988 465 155 2287 88 2415 69 155 2416 2417 2418 5 329 2419 242 298 2034 566 2420 155 400 189 2414 210 1151 5 2421 24 2422 400 1780 1190 653 2423
2424 2425 5 547 2426 2427 850 166 162 2428
382 845 2429 2430 2 146 2431 202 5 2432 2130 2433 1772 155 99 2434 410 34 1772 7 2435 69 155 188 446 410 1366 1221 2436 2437 1193 2438 5 2439 31 1813 2440 2441 32 382 1950 37 2338 2 1501 1948 154 2 979 5 1268 7 2251 1051 186 32 87 843 36 2243 457 1246 2210 1678 664 358 977 893 2442
339 2443 2444 2445 5 2446 242 838 2447 885 64 339 2 2448 1690 1420 2449 2450 347 1215 131 121 319 2451 2452 419 30 129 1008 218 226
16 5 2453 406 1051 186 2226 1548 437 129
1684 506 184 5 2454 1676 7 2455 2454 69 7 997 2456 547 747 69 2 2457 210 2455 5 2458 2459 7 30 1780 79 2407 854 185 2460 210 2461 5 2456 438 966
2462 2463 2464 2465 1965 19 2412 2466 2467 2468 5 2469 478 2470 2471 2472 80 2473 547 2466 671 745 2474 1600 2405 2475 210 2476 328 2477 5 2478 69 1754 2479 156 438 251 131 2480 5 1593 2481 242 426 2447 689 1104 604 1386 2175 2482 706 410 2483 2484 210 2485 2486 5 302 155 532 2031 2487 483 1144 1145
50 51 60 5 703 7 30 31 15 4 86 96
26 2488 2489 519 4 5 97 247 99 5 2490 1172 1900 2491 2182 717 1264 241 2492 239 2493 2082 2494 599 70 2162 2495 17 689 406 1853 4 82 376 162 1808 2496 1298 253 1722 1639 18 1296 26 2497 34 2 649 5 46 653 970 5 1163 2498 521 17
87 2499 2 1629 1114 69 2 142 547 32 460 2500 37 5 2501 5 961 2502 116 142 850 309 131 121 338
98 2503 5 750 1103 2504 1808 210 2505 140 2506 2505 2 2507 506 10 53 57 1551 7 184 5 2508 1813 746 1888 31 2 2509 155 26 2510 5 2281 2511 1069 2512 2513 1639 155 352 1808 5 746 1808 5 746 414 19 2514 1107 26 16 32 2515 2296 10 155 2516 8 2515 799 79 80 4
2425 5 2517 194 1629 937 2518 2519 17 5 2520 506 184 19 392 309 234 1622 2517 210 2521 2522 2523 2 233 5 2524 2525 2526 19 2521 893 416
2295 4 5 9 628 416 26 81 773 773
2527 1752 2528 2529 5 2530 2531 32 2532 2533 2534 46 32 2535 2536 5 2537 2538 2539 212 168 2540 2541 2542 2543 2544 2545 2 2535 5 2546 2547 2548 2549 210 1250 2550 2551 2552 2553 2554 2553 2555 2556 2557 2540 155 2558 420 628
1600 708 5 2463 2559 2 2560 2535 5 2426 162 475 155 1463 142 2561 5 2535
2562 2563 1096 4 5 204 2564 2565 2566 854 1676 668 2567 664 17 666 2568 1611 85 2569 5 2570 142 129 2137 2571 2572 265 2573 2574 96
2575 2565 2576 2577 462 854 131 319 2578 256 5 2579 78 1600 920 2369
142 282 2580 2581 1106 379 5 2582 961 2583 10 32 1273 2584 926 392 7 997 810 2126 2585 944 459 961 2586 2587 5 1541 2192 2588 1802 17 2589 2590 2591 2592 1979 392 149 338 5 2 142 2593 5 2594 2454 2526 26 7 2479 855
50 51 153 5 356 214 2595 2596 97 2597 512 400 190 5 581 151 591 512 2154
2598 367 2599 97 5 1535 2600 142 2180 2601 2602 2603 2604 5 16 9 2180 155 32 1251 2605 2223 2606 2607 586 5 2608 2609 2602 2 2180 2602 586 2610 115 5 9 96
638 2566 210 2611 5 214 118 157 60 1618 155 2612 1096 11 2613 773 773
50 51 26 1101 446 248 249 2614 246 771 2615 46 604 153 17 96
50 51 2616 97 530 2617 244 1313 5 2618 618 5 2619 229 82 299 794 907 349 46 1853 85 17 244 435 2620 55 2621 2622 237 265 11 2117 252 2623 103 5 244 2624 5
50 51 10 60 96 1589 392 16 71 96 129 26 93
1910 17 242 354 2625 17 2157 689 2626 115 2627 2628 150 2629 914 389 87 2630 186 1706 85 155 2631 67 2632 17 2566 2633 98 1096
1420 2634 2 774 2635 1008 298 206 5 503 96
2640 2641 2447 2642 155 240 981 91 82 2643 1184 1637 103 204 69 1451 2644 17
76 12"
Li Li <fancyerii@gmail.com>,"Wed, 13 Jan 2016 17:17:36 +0800",Re: running lda in spark throws exception,Bryan Cutler <cutlerb@gmail.com>,"I will try spark 1.6.0 to see it is the bug of 1.5.2.


---------------------------------------------------------------------


"
=?UTF-8?Q?Maciej_Bry=C5=84ski?= <maciek@brynski.pl>,"Wed, 13 Jan 2016 11:12:54 +0100",Spark 1.6.0 and HDP 2.2 - problem,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,
I/m trying to run Spark 1.6.0 on HDP 2.2
Everything was fine until I tried to turn on dynamic allocation.
According to instruction I need to add shuffle service to yarn classpath.
The problem is that HDP 2.2 has jackson 2.2.3 and Spark is using 2.4.4.
So connecting it gives error:

2016-01-11 16:56:51,222 INFO  containermanager.AuxServices
(AuxServices.java:addService(72)) - Adding auxiliary service
spark_shuffle, ""spark_shuffle""
2016-01-11 16:56:51,439 FATAL nodemanager.NodeManager
(NodeManager.java:initAndStartNodeManager(465)) - Error starting
NodeManager
java.lang.NoSuchMethodError:
com.fasterxml.jackson.core.JsonFactory.requiresPropertyOrdering()Z
        at com.fasterxml.jackson.databind.ObjectMapper.<init>(ObjectMapper.java:457)
        at com.fasterxml.jackson.databind.ObjectMapper.<init>(ObjectMapper.java:379)
        at org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.<clinit>(ExternalShuffleBlockResolver.java:57)
        at org.apache.spark.network.shuffle.ExternalShuffleBlockHandler.<init>(ExternalShuffleBlockHandler.java:56)
        at org.apache.spark.network.yarn.YarnShuffleService.serviceInit(YarnShuffleService.java:128)
        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices.serviceInit(AuxServices.java:143)
        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
        at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceInit(ContainerManagerImpl.java:237)
        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
        at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)
        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:253)
        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:462)
        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:509)


What can I do ?
I have following ideas:
1) Compile Spark 1.6.0 with modified pom.xml (change jackson version
to 2.2.3). I'm not sure if this will be working
2) I tried to put shuffle service from different version of Spark.
1.4.1 works on HDP 2.2.
Is it possible to run shuffle service from 1.4.1 with Spark 1.6.0 ?
3) Other ideas ?

Regards,
-- 
Maciek Bry≈Ñski

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Wed, 13 Jan 2016 02:25:42 -0800",Re: Spark 1.6.0 and HDP 2.2 - problem,=?utf-8?Q?Maciej_Bry=C5=84ski?= <maciek@brynski.pl>,"I would suggest trying option #1 first. 

Thanks

:
java:457)
java:379)
linit>(ExternalShuffleBlockResolver.java:57)
it>(ExternalShuffleBlockHandler.java:56)
nShuffleService.java:128)
ava:163)
rvices.serviceInit(AuxServices.java:143)
ava:163)
Service.java:107)
inerManagerImpl.serviceInit(ContainerManagerImpl.java:237)
ava:163)
Service.java:107)
t(NodeManager.java:253)
ava:163)
rtNodeManager(NodeManager.java:462)
anager.java:509)

---------------------------------------------------------------------


"
=?UTF-8?Q?Maciej_Bry=C5=84ski?= <maciek@brynski.pl>,"Wed, 13 Jan 2016 12:23:02 +0100",Re: Spark 1.6.0 and HDP 2.2 - problem,Ted Yu <yuzhihong@gmail.com>,"Thanks.
I successfully compiled Spark 1.6.0 with Jackson 2.2.3 from source.

I'll try to using it.

2016-01-13 11:25 GMT+01:00 Ted Yu <yuzhihong@gmail.com>:
te:
.
r.java:457)
r.java:379)
<clinit>(ExternalShuffleBlockResolver.java:57)
init>(ExternalShuffleBlockHandler.java:56)
arnShuffleService.java:128)
.java:163)
Services.serviceInit(AuxServices.java:143)
.java:163)
teService.java:107)
tainerManagerImpl.serviceInit(ContainerManagerImpl.java:237)
.java:163)
teService.java:107)
nit(NodeManager.java:253)
.java:163)
tartNodeManager(NodeManager.java:462)
eManager.java:509)



-- 
Maciek Bry≈Ñski

---------------------------------------------------------------------


"
David Russell <themarchoffolly@protonmail.com>,"Wed, 13 Jan 2016 09:05:52 -0500",Re: ROSE: Spark + R on the JVM.,Richard Siebeling <rsiebeling@gmail.com>,"Hi Richard,

Thanks for providing the background on your application.

> the user types or copy-pastes his R code,
> the system should then send this R code (using ROSE) to R

Unfortunately this type of ad hoc R analysis is not supported. ROSE supports the execution of any R function or script within an existing R package on CRAN, Bioc, or github. It does not support the direct execution of arbitrary blocks of R code as you described.

You may want to look at [DeployR](http://deployr.revolutionanalytics.com/), it's an open source R integration server that provides APIs in Java, JavaScript and .NET that can easily support your use case. The outputs of your DeployR integration could then become inputs to your data processing system.

David

""All that is gold does not glitter, Not all those who wander are lost.""



-------- Original Message --------
Subject: Re: ROSE: Spark + R on the JVM.
Local Time: January 13 2016 3:18 am
UTC Time: January 13 2016 8:18 am
From: rsiebeling@gmail.com
To: themarchoffolly@protonmail.com
CC: mail@vijaykiran.com,cjnolet@gmail.com,user@spark.apache.org,dev@spark.apache.org


Hi David,

the use case is that we're building a data processing system with an intuitive user interface where Spark is used as the data processing framework.
We would like to provide a HTML user interface to R where the user types or copy-pastes his R code, the system should then send this R code (using ROSE) to R, process it and give the results back to the user. The RDD would be used so that the data can be further processed by the system but we would like to also show or be able to show the messages printed to STDOUT and also the images (plots) that are generated by R. The plots seems to be available in the OpenCPU API, see below

Inline image 1

So the case is not that we're trying to process millions of images but rather that we would like to show the generated plots (like a regression plot) that's generated in R to the user. There could be several plots generated by the code, but certainly not thousands or even hundreds, only a few.

Hope that this would be possible using ROSE because it seems a really good fit,
thanks in advance,
Richard



On Wed, Jan 13, 2016 at 3:39 AM, David Russell <themarchoffolly@protonmail.com> wrote:

Hi Richard,


> Would it be possible to access the session API from within ROSE,
> to get for example the images that are generated by R / openCPU

Technically it would be possible although there would be some potentially significant runtime costs per task in doing so, primarily those related to extracting image data from the R session, serializing and then moving that data across the cluster for each and every image.

From a design perspective ROSE was intended to be used within Spark scale applications where R object data was seen as the primary task output. An output in a format that could be rapidly serialized and easily processed. Are there real world use cases where Spark scale applications capable of generating 10k, 100k, or even millions of image files would actually need to capture and store images? If so, how practically speaking, would these images ever be used? I'm just not sure. Maybe you could describe your own use case to provide some insights?


> and the logging to stdout that is logged by R?

If you are referring to the R console output (generated within the R session during the execution of an OCPUTask) then this data could certainly (optionally) be captured and returned on an OCPUResult. Again, can you provide any details for how you might use this console output in a real world application?

As an aside, for simple standalone Spark applications that will only ever run on a single host (no cluster) you could consider using an alternative library called fluent-r. This library is also available under my GitHub repo, [see here](https://github.com/onetapbeyond/fluent-r). The fluent-r library already has support for the retrieval of R objects, R console output and R graphics device image/plots. However it is not as lightweight as ROSE and it not designed to work in a clustered environment. ROSE on the other hand is designed for scale.


David

""All that is gold does not glitter, Not all those who wander are lost.""




-------- Original Message --------
Subject: Re: ROSE: Spark + R on the JVM.


Local Time: January 12 2016 6:56 pm
UTC Time: January 12 2016 11:56 pm
From: rsiebeling@gmail.com
To: mail@vijaykiran.com
CC: cjnolet@gmail.com,themarchoffolly@protonmail.com,user@spark.apache.org,dev@spark.apache.org



Hi,

this looks great and seems to be very usable.
Would it be possible to access the session API from within ROSE, to get for example the images that are generated by R / openCPU and the logging to stdout that is logged by R?

thanks in advance,
Richard



On Tue, Jan 12, 2016 at 10:16 PM, Vijay Kiran <mail@vijaykiran.com> wrote:

I think it would be this: https://github.com/onetapbeyond/opencpu-spark-executor

> On 12 Jan 2016, at 18:32, Corey Nolet <cjnolet@gmail.com> wrote:
>


> David,
>
> Thank you very much for announcing this! It looks like it could be very useful. Would you mind providing a link to the github?
>
> On Tue, Jan 12, 2016 at 10:03 AM, David <themarchoffolly@protonmail.com> wrote:
> Hi all,
>
> I'd like to share news of the recent release of a new Spark package, ROSE.
>
> ROSE is a Scala library offering access to the full scientific computing power of the R programming language to Apache Spark batch and streaming applications on the JVM. Where Apache SparkR lets data scientists use Spark from R, ROSE is designed to let Scala and Java developers use R from Spark.
>
> The project is available and documented on GitHub and I would encourage you to take a look. Any feedback, questions etc very welcome.
>
> David
>
> ""All that is gold does not glitter, Not all those who wander are lost.""
>




---------------------------------------------------------------------
To unsubscribe, e-mail: user-unsubscribe@spark.apache.org
For additional commands, e-mail: user-help@spark.apache.org"
Steve Loughran <stevel@hortonworks.com>,"Wed, 13 Jan 2016 17:00:27 +0000",Re: Spark 1.6.0 and HDP 2.2 - problem,"""dev@spark.apache.org"" <dev@spark.apache.org>","
> On 13 Jan 2016, at 03:23, Maciej Bry≈Ñski <maciek@brynski.pl> wrote:
> 
> Thanks.
> I successfully compiled Spark 1.6.0 with Jackson 2.2.3 from source.
> 
> I'll try to using it.
> 

This is the eternal classpath version problem, with Jackson turning out to be incredibly brittle. After one point update of the 1.x JAR broke things (it removed a method), there's ~0 enthusiasm for incrementing the version counters again.


1. Hadoop, even Hadoop trunk, is built with : <jackson2.version>2.2.3</jackson2.version>
 
this means that right now, the Spark 1.6 shuffle isn't going to work in *any* Hadoop cluster that hasn't been built with a compatible Jackson version: either rebuild spark 1.6 or hadoop-core itself.


2. There's a YARN JIRA for classpath isolation in aux services: https://issues.apache.org/jira/browse/YARN-4577 , with a longer term one for forking off the services entirely: https://issues.apache.org/jira/browse/YARN-1593 . there's a patch for the first one, which, if someone want's to apply and test locally, would be valued. It wouldn't ship in Hadoop until 2.9. . Patch #2 got assigned to someone last week, so maybe it'll surface in Hadoop 2.9 instead/as well.

3. I'm going to open a SPARK JIRA here, cross link it to the YARN ones -so at least there'll be a central record.  (Done: SPARK-12807)

4. I'll also add an ""upgrade jackson"" issue under HADOOP-9991, though like I said: enthusiasm will be low. 

5. You can D/L a version of spark 1.6 built against HDP 2.3: 
http://hortonworks.com/hadoop-tutorial/apache-spark-1-6-technical-preview-with-hdp-2-3/

This isn't likely to work against HDP 2.2 BTW; later Hadoop JAR versions. 

-I suspect for things to work on CDH there'll be something similar. For ASF Hadoop, rebuilding spark is all you have.

6. Looking @spark/master, it's been on jackson 2.5.3 since last month, from SPARK-12269. Which is just going to make versioning even more traumatic. And we know that amazon-aws has a back compatibility track record, so swapping things around there is going to be fun. You'd probably need to rebuild Hadoop-2.7.2+ with the later aws/s3 JARs to keep everything aligned

on #4: has anyone found any compatibility problems if they swap out Jackson 2.2.3 for Jackson 2.4.4 or 2.5.3 *without recompiling anything*? That's what we need to know for Hadoop JAR updates.

-Steve





> 2016-01-13 11:25 GMT+01:00 Ted Yu <yuzhihong@gmail.com>:
>> I would suggest trying option #1 first.
>> 
>> Thanks
>> 
>>> On Jan 13, 2016, at 2:12 AM, Maciej Bry≈Ñski <maciek@brynski.pl> wrote:
>>> 
>>> Hi,
>>> I/m trying to run Spark 1.6.0 on HDP 2.2
>>> Everything was fine until I tried to turn on dynamic allocation.
>>> According to instruction I need to add shuffle service to yarn classpath.
>>> The problem is that HDP 2.2 has jackson 2.2.3 and Spark is using 2.4.4.
>>> So connecting it gives error:
>>> 
>>> 2016-01-11 16:56:51,222 INFO  containermanager.AuxServices
>>> (AuxServices.java:addService(72)) - Adding auxiliary service
>>> spark_shuffle, ""spark_shuffle""
>>> 2016-01-11 16:56:51,439 FATAL nodemanager.NodeManager
>>> (NodeManager.java:initAndStartNodeManager(465)) - Error starting
>>> NodeManager
>>> java.lang.NoSuchMethodError:
>>> com.fasterxml.jackson.core.JsonFactory.requiresPropertyOrdering()Z
>>>       at com.fasterxml.jackson.databind.ObjectMapper.<init>(ObjectMapper.java:457)
>>>       at com.fasterxml.jackson.databind.ObjectMapper.<init>(ObjectMapper.java:379)
>>>       at org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.<clinit>(ExternalShuffleBlockResolver.java:57)
>>>       at org.apache.spark.network.shuffle.ExternalShuffleBlockHandler.<init>(ExternalShuffleBlockHandler.java:56)
>>>       at org.apache.spark.network.yarn.YarnShuffleService.serviceInit(YarnShuffleService.java:128)
>>>       at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
>>>       at org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices.serviceInit(AuxServices.java:143)
>>>       at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
>>>       at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)
>>>       at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceInit(ContainerManagerImpl.java:237)
>>>       at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
>>>       at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)
>>>       at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:253)
>>>       at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
>>>       at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:462)
>>>       at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:509)
>>> 
>>> 
>>> What can I do ?
>>> I have following ideas:
>>> 1) Compile Spark 1.6.0 with modified pom.xml (change jackson version
>>> to 2.2.3). I'm not sure if this will be working
>>> 2) I tried to put shuffle service from different version of Spark.
>>> 1.4.1 works on HDP 2.2.
>>> Is it possible to run shuffle service from 1.4.1 with Spark 1.6.0 ?
>>> 3) Other ideas ?
>>> 
>>> Regards,
>>> --
>>> Maciek Bry≈Ñski
>>> 
>>> ---------------------------------------------------------------------
>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>> For additional commands, e-mail: dev-help@spark.apache.org
>>> 
> 
> 
> 
> -- 
> Maciek Bry≈Ñski
> 
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org
"
=?UTF-8?Q?Maciej_Bry=C5=84ski?= <maciek@brynski.pl>,"Wed, 13 Jan 2016 19:15:28 +0100",Re: Spark 1.6.0 and HDP 2.2 - problem,Steve Loughran <stevel@hortonworks.com>,"Steve,
Thank you for the answer.
How Hortonworks deal with this problem internally ?
You have Spark 1.3.1 in HDP 2.3. Is it compilled with Jackson 2.2.3 ?

Regards,
Maciek

2016-01-13 18:00 GMT+01:00 Steve Loughran <stevel@hortonworks.com>:
o be incredibly brittle. After one point update of the 1.x JAR broke things (it removed a method), there's ~0 enthusiasm for incrementing the version counters again.
ckson2.version>
any* Hadoop cluster that hasn't been built with a compatible Jackson version: either rebuild spark 1.6 or hadoop-core itself.
ssues.apache.org/jira/browse/YARN-4577 , with a longer term one for forking off the services entirely: https://issues.apache.org/jira/browse/YARN-1593 . there's a patch for the first one, which, if someone want's to apply and test locally, would be valued. It wouldn't ship in Hadoop until 2.9. . Patch #2 got assigned to someone last week, so maybe it'll surface in Hadoop 2.9 instead/as well.
o at least there'll be a central record.  (Done: SPARK-12807)
e I said: enthusiasm will be low.
-with-hdp-2-3/
SF Hadoop, rebuilding spark is all you have.
om SPARK-12269. Which is just going to make versioning even more traumatic. And we know that amazon-aws has a back compatibility track record, so swapping things around there is going to be fun. You'd probably need to rebuild Hadoop-2.7.2+ with the later aws/s3 JARs to keep everything aligned
on 2.2.3 for Jackson 2.4.4 or 2.5.3 *without recompiling anything*? That's what we need to know for Hadoop JAR updates.
rote:
th.
.
er.java:457)
er.java:379)
.<clinit>(ExternalShuffleBlockResolver.java:57)
<init>(ExternalShuffleBlockHandler.java:56)
YarnShuffleService.java:128)
e.java:163)
xServices.serviceInit(AuxServices.java:143)
e.java:163)
iteService.java:107)
ntainerManagerImpl.serviceInit(ContainerManagerImpl.java:237)
e.java:163)
iteService.java:107)
Init(NodeManager.java:253)
e.java:163)
StartNodeManager(NodeManager.java:462)
deManager.java:509)



-- 
Maciek Bry≈Ñski

---------------------------------------------------------------------


"
"""Praveen Devarao"" <praveendrl@in.ibm.com>","Wed, 13 Jan 2016 16:54:26 +0530",IDE for spark development,"""dev@spark.apache.org""@d28av03.in.ibm.com","Hi,

        What is the best IDE for spark development? When I say development 
I would like to make changes in SPARK core and be able to contribute back.

        I know this question is been asked many times but I don't see a 
convincing answer anywhere. I use intelliJ but getting the environment up 
and running is quite tedious, as there is no known guide to just import 
the project into your workspace and then it starts running. I was able to 
setup with lots of ""Generate Sources and Update"" folder once and now when 
tried second time the same approach doesn't help. It just lands into a 
situation where there is compile error on console saying 'xyz' variable 
not defined but on editor it is able to navigate to the definition.


        Any suggestions on how to setup successfully on intelliJ or 
suggestions for alternate IDE (with steps please)?


Thanks

Praveen

"
Bryan Cutler <cutlerb@gmail.com>,"Wed, 13 Jan 2016 14:53:09 -0800",Re: running lda in spark throws exception,Li Li <fancyerii@gmail.com>,"I was now able to reproduce the exception using the master branch and local
mode.  It looks like the problem is the vectors of term counts in the
counts to the vocab size, it ran without the exception.

Joseph, I also tried calling describeTopics and noticed that with the
improper vector size, it will not throw an exception but the term indices
will start to be incorrect.  For a small number of iterations, it is ok,
but increasing iterations causes the indices to get larger also.  Maybe
that is what is going on in the JIRA you linked to?


"
Rachana Srivastava <Rachana.Srivastava@markmonitor.com>,"Wed, 13 Jan 2016 23:29:33 +0000",Random Forest FeatureImportance throwing NullPointerException,"""'user@spark.apache.org'"" <user@spark.apache.org>,
	""'dev@spark.apache.org'"" <dev@spark.apache.org>","I have a Random forest model for which I am trying to get the featureImportance vector.

Map<Object,Object> categoricalFeaturesParam = new HashMap<>();
scala.collection.immutable.Map<Object,Object> categoricalFeatures =  (scala.collection.immutable.Map<Object,Object>)
scala.collection.immutable.Map$.MODULE$.apply(JavaConversions.mapAsScalaMap(categoricalFeaturesParam).toSeq());
int numberOfClasses =2;
RandomForestClassifier rfc = new RandomForestClassifier();
RandomForestClassificationModel rfm = RandomForestClassificationModel.fromOld(model, rfc, categoricalFeatures, numberOfClasses);
System.out.println(rfm.featureImportances());

When I run above code I found featureImportance as null.  Do I need to set anything in specific to get the feature importance for the random forest model.

Thanks,

Rachana
"
Michael Armbrust <michael@databricks.com>,"Wed, 13 Jan 2016 17:39:28 -0800",Spark 1.6.1,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey All,

While I'm not aware of any critical issues with 1.6.0, there are several
corner cases that users are hitting with the Dataset API that are fixed in
branch-1.6.  As such I'm considering a 1.6.1 release.

At the moment there are only two critical issues targeted for 1.6.1:
 - SPARK-12624 - When schema is specified, we should treat undeclared
fields as null (in Python)
 - SPARK-12783 - Dataset map serialization error

When these are resolved I'll likely begin the release process.  If there
are any other issues that we should wait for please contact me.

Michael
"
Li Li <fancyerii@gmail.com>,"Thu, 14 Jan 2016 10:40:26 +0800",Re: running lda in spark throws exception,Bryan Cutler <cutlerb@gmail.com>,"It looks like the problem is the vectors of term counts in the corpus
are not always the vocabulary size.
Do you mean some integers not occured in the corpus?
for example, I have the dictionary is 0 - 9 (total 10 words).
The docs are:
0 2 4 6 8
1 3 5 7 9
Then it will be correct
If the docs are:
0 2 4 6 9
1 3 5 6 7
8 is not occured in any document, Then it will wrong?

So the workaround is to process the input to re-encode terms?


---------------------------------------------------------------------


"
Prabhu Joseph <prabhujose.gates@gmail.com>,"Thu, 14 Jan 2016 10:31:03 +0530","Spark on YARN job continuously reports ""Application does not exist in cache""","user <user@spark.apache.org>, dev@spark.apache.org","Hi All,

  When we submit Spark jobs on YARN, during RM failover, we see lot of jobs
reporting below error messages.


*2016-01-11 09:41:06,682 INFO
org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService:
Unregistering app attempt : appattempt_1450676950893_0280_000001*
2016-01-11 09:41:06,683 INFO
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl:
appattempt_1450676950893_0280_000001 State change from FINAL_SAVING to
FAILED
2016-01-11 09:41:06,683 INFO
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl:
application_1450676950893_0280 State change from RUNNING to ACCEPTED
2016-01-11 09:41:06,683 INFO
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler:
Application appattempt_1450676950893_0280_000001 is done. finalState=FAILED
2016-01-11 09:41:06,683 INFO
org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService:
Registering app attempt : appattempt_1450676950893_0280_000002
2016-01-11 09:41:06,683 INFO
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo:
Application application_1450676950893_0280 requests cleared
2016-01-11 09:41:06,683 INFO
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl:
appattempt_1450676950893_0280_000002 State change from NEW to SUBMITTED
2016-01-11 09:41:06,683 INFO
org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher:
Cleaning master appattempt_1450676950893_0280_000001
2016-01-11 09:41:06,683 INFO
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler:
Added Application Attempt appattempt_1450676950893_0280_000002 to scheduler
from user: glenm
2016-01-11 09:41:06,683 INFO
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl:
appattempt_1450676950893_0280_000002 State change from SUBMITTED to
SCHEDULED




*2016-01-11 09:41:06,747 ERROR
org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService:
AppAttemptId doesnt exist in cache appattempt_1450676950893_0280_000001*
ResourceManager has a ConcurrentMap where it puts applicationId during
resgistering of application attempt, and when there is
finishApplicationMaster request, it gets the entry from ConcurrentMap, if
there if no entry present, it throws that ERROR message. When there is
unregistering Application Attempt, it removes the entry.

So, after the unregistering application attempt, there are many
finishApplicationMaster request causing the ERROR.

Need your help to understand on what scenario the above happens.


JIRA's related are

https://issues.apache.org/jira/browse/SPARK-1032
https://issues.apache.org/jira/browse/SPARK-3072



Thanks,
Prabhu Joseph
"
Reynold Xin <rxin@databricks.com>,"Wed, 13 Jan 2016 22:29:09 -0800",[discuss] dropping Hadoop 2.2 and 2.3 support in Spark 2.0?,"""dev@spark.apache.org"" <dev@spark.apache.org>, user <user@spark.apache.org>","We've dropped Hadoop 1.x support in Spark 2.0.

There is also a proposal to drop Hadoop 2.2 and 2.3, i.e. the minimal
Hadoop version we support would be Hadoop 2.4. The main advantage is then
we'd be able to focus our Jenkins resources (and the associated maintenance
of Jenkins) to create builds for Hadoop 2.6/2.7. It is my understanding
that all Hadoop vendors have moved away from 2.2/2.3, but there might be
some users that are on these older versions.

What do you think about this idea?
"
praveen S <mylogin13@gmail.com>,"Thu, 14 Jan 2016 13:14:27 +0530",Usage of SparkContext within a Web container,"user@spark.apache.org, dev@spark.apache.org","Is use of SparkContext from a Web container a right way to process spark
jobs or should we use spark-submit in a processbuilder?

Are there any pros or cons of using SparkContext from a Web container..?

How does zeppelin trigger spark jobs from the Web context?
"
Sean Owen <sowen@cloudera.com>,"Thu, 14 Jan 2016 10:17:21 +0000",Re: [discuss] dropping Hadoop 2.2 and 2.3 support in Spark 2.0?,Reynold Xin <rxin@databricks.com>,"I personally support this. I had suggest drawing the line at Hadoop
2.6, but that's minor. More info:

Hadoop 2.7: April 2015
Hadoop 2.6: Nov 2014
Hadoop 2.5: Aug 2014
Hadoop 2.4: April 2014
Hadoop 2.3: Feb 2014
Hadoop 2.2: Oct 2013

CDH 5.0/5.1 = Hadoop 2.3 + backports
CDH 5.2/5.3 = Hadoop 2.5 + backports
CDH 5.4+ = Hadoop 2.6 + chunks of 2.7 + backports.

I can only imagine that CDH6 this year will be based on something
later still like 2.8 (no idea about the 3.0 schedule). In the sense
that 5.2 was released about a year and half ago, yes, this vendor has
moved on from 2.3 a while ago. These releases will also never contain
a different minor Spark release. For example 5.7 will have Spark 1.6,
I believe, and not 2.0.

Here, I listed some additional things we could clean up in Spark if
Hadoop 2.6 was assumed. By itself, not a lot:
https://github.com/apache/spark/pull/10446#issuecomment-167971026

Yes, we also get less Jenkins complexity. Mostly, the jar-hell that's
biting now gets a little more feasible to fix. And we get Hadoop fixes
as well as new APIs, which helps mostly for YARN.

My general position is that backwards-compatibility and supporting
older platforms needs to be a low priority in a major release; it's a
decision about what to support for users in the next couple years, not
the preceding couple years. Users on older technologies simply stay on
the older Spark until ready to update; they are in no sense suddenly
left behind otherwise.


---------------------------------------------------------------------


"
Eugene Morozov <evgeny.a.morozov@gmail.com>,"Thu, 14 Jan 2016 17:37:44 +0300",Re: Usage of SparkContext within a Web container,praveen S <mylogin13@gmail.com>,"Praveen,

Zeppelin uses Spark's REPL.

I'm currently writing an app that is a web service, which is going to run
spark jobs.
So, at the init stage I just create JavaSparkContext and then use it for
all users requests. Web service is stateless. The issue with stateless is
that it's possible to run several instances of web service, but each of
them will have separate JavaSparkContext, which means they are going to
compete for resources as different application. Although they have to look
as just one application. I'm pretty sure it's possible to use pools, but I
haven't tried it, yet. I see no other cons... or pros for that matter.

The way you're going to use it, I'd say, depends on if users are going to
provide their own code. If that's the case, then you probably better with
Zeppelin's way. If not - then my assumption is that using SparkContext for
processing is simpler.

--
Be well!
Jean Morozov


"
Rachana Srivastava <Rachana.Srivastava@markmonitor.com>,"Thu, 14 Jan 2016 16:12:46 +0000",RE: Random Forest FeatureImportance throwing NullPointerException,"""'user@spark.apache.org'"" <user@spark.apache.org>,
	""'dev@spark.apache.org'"" <dev@spark.apache.org>","Tried using 1.6 version of Spark that takes numberOfFeatures fifth argument in  the API but still getting featureImportance as null.

RandomForestClassifier rfc = getRandomForestClassifier( numTrees,  maxBinSize,  maxTreeDepth,  seed,  impurity);
RandomForestClassificationModel rfm = RandomForestClassificationModel.fromOld(model, rfc, categoricalFeatures, numberOfClasses,numberOfFeatures);
System.out.println(rfm.featureImportances());

Stack Trace:
Exception in thread ""main"" java.lang.NullPointerException
                at org.apache.spark.ml.tree.impl.RandomForest$.computeFeatureImportance(RandomForest.scala:1152)
                at org.apache.spark.ml.tree.impl.RandomForest$$anonfun$featureImportances$1.apply(RandomForest.scala:1111)
                at org.apache.spark.ml.tree.impl.RandomForest$$anonfun$featureImportances$1.apply(RandomForest.scala:1108)
                at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
                at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
                at org.apache.spark.ml.tree.impl.RandomForest$.featureImportances(RandomForest.scala:1108)
                at org.apache.spark.ml.classification.RandomForestClassificationModel.featureImportances$lzycompute(RandomForestClassifier.scala:237)
                at org.apache.spark.ml.classification.RandomForestClassificationModel.featureImportances(RandomForestClassifier.scala:237)
                at com.markmonitor.antifraud.ce.ml.CheckFeatureImportance.main(CheckFeatureImportance.java:49)

From: Rachana Srivastava
Sent: Wednesday, January 13, 2016 3:30 PM
To: 'user@spark.apache.org'; 'dev@spark.apache.org'
Subject: Random Forest FeatureImportance throwing NullPointerException

I have a Random forest model for which I am trying to get the featureImportance vector.

Map<Object,Object> categoricalFeaturesParam = new HashMap<>();
scala.collection.immutable.Map<Object,Object> categoricalFeatures =  (scala.collection.immutable.Map<Object,Object>)
scala.collection.immutable.Map$.MODULE$.apply(JavaConversions.mapAsScalaMap(categoricalFeaturesParam).toSeq());
int numberOfClasses =2;
RandomForestClassifier rfc = new RandomForestClassifier();
RandomForestClassificationModel rfm = RandomForestClassificationModel.fromOld(model, rfc, categoricalFeatures, numberOfClasses);
System.out.println(rfm.featureImportances());

When I run above code I found featureImportance as null.  Do I need to set anything in specific to get the feature importance for the random forest model.

Thanks,

Rachana
"
Steve Loughran <stevel@hortonworks.com>,"Thu, 14 Jan 2016 17:28:46 +0000",Re: [discuss] dropping Hadoop 2.2 and 2.3 support in Spark 2.0?,,"
> On 14 Jan 2016, at 02:17, Sean Owen <sowen@cloudera.com> wrote:
> 
> I personally support this. I had suggest drawing the line at Hadoop
> 2.6, but that's minor. More info:
> 
> Hadoop 2.7: April 2015
> Hadoop 2.6: Nov 2014
> Hadoop 2.5: Aug 2014
> Hadoop 2.4: April 2014
> Hadoop 2.3: Feb 2014
> Hadoop 2.2: Oct 2013
> 
> CDH 5.0/5.1 = Hadoop 2.3 + backports
> CDH 5.2/5.3 = Hadoop 2.5 + backports
> CDH 5.4+ = Hadoop 2.6 + chunks of 2.7 + backports.
> 
> I can only imagine that CDH6 this year will be based on something
> later still like 2.8 (no idea about the 3.0 schedule).

Hadoop 2.8 comes out in ~1-2 months. I've already been building & testing spark against it; no major issues.

> In the sense
> that 5.2 was released about a year and half ago, yes, this vendor has
> moved on from 2.3 a while ago. These releases will also never contain
> a different minor Spark release. For example 5.7 will have Spark 1.6,
> I believe, and not 2.0.
> 
> Here, I listed some additional things we could clean up in Spark if
> Hadoop 2.6 was assumed. By itself, not a lot:
> https://github.com/apache/spark/pull/10446#issuecomment-167971026
> 


> Yes, we also get less Jenkins complexity. Mostly, the jar-hell that's
> biting now gets a little more feasible to fix. And we get Hadoop fixes
> as well as new APIs, which helps mostly for YARN.
> 

2.6.x is still having active releases, likely through 2016. It'll be the only hadoop version where problems Spark encounters would get fixed

It's also the last iteration of interesting API features ‚Äîespecially in YARN: timeline server, registry, various other things

And it has s3a, which, for anyone using S3 for storage, is the only S3 filesystem binding I'd recommend. Hadoop 2.4 not only has s3n, it's got a broken one that (HADOOP-10589)

I believe 2.6 supportsr recent guava versions, even if it is frozen on 11.0 to avoid surprising people (i.e. all deprecated/removed classes should have been stripped)

Finally: it's the only version of Hadoop that works on Java 7, has patches to support Java8+kerberos (in fact, Java 7u80+ and kerberos).

For the reason of JVMs and guava alone, I'd abandon Hadoop < 2.6. Those versions won't work on secure Java 7 clusters, recent guava versions, and have lots of uncorrected issues.

Oh, and did I mention the test matrix? The later version of Hadoop you use, the less versions to test against. 

> My general position is that backwards-compatibility and supporting
> older platforms needs to be a low priority in a major release; it's a
> decision about what to support for users in the next couple years, not
> the preceding couple years. Users on older technologies simply stay on
> the older Spark until ready to update; they are in no sense suddenly
> left behind otherwise.


If they are running older versions of Hadoop, they generally have stable apps which they don't bother upgrading. New clusters => new versions => new apps.


"
Bryan Cutler <cutlerb@gmail.com>,"Thu, 14 Jan 2016 11:24:05 -0800",Re: running lda in spark throws exception,Li Li <fancyerii@gmail.com>,"What I mean is the input to LDA.run() is a RDD[(Long, Vector)] and the
Vector is a vector of counts of each term and should be the same size as
the vocabulary (so if the vocabulary, or dictionary has 10 words, each
vector should have a size of 10).  This probably means that there will be
some elements with zero counts, and a sparse vector might be a good way to
handle that.


"
Steve Loughran <stevel@hortonworks.com>,"Thu, 14 Jan 2016 19:26:13 +0000",Re: [discuss] dropping Hadoop 2.2 and 2.3 support in Spark 2.0?,,"
> On 14 Jan 2016, at 09:28, Steve Loughran <stevel@hortonworks.com> wrote:
>> 
> 
> 2.6.x is still having active releases, likely through 2016. It'll be the only hadoop version where problems Spark encounters would get fixed

Correction: minimum Hadoop version

Any problem reported against older versions will probably get a message saying ""upgrade""

> 
> It's also the last iteration of interesting API features ‚Äîespecially in YARN: timeline server, registry, various other things
> 
> And it has s3a, which, for anyone using S3 for storage, is the only S3 filesystem binding I'd recommend. Hadoop 2.4 not only has s3n, it's got a broken one that (HADOOP-10589)
> 
> I believe 2.6 supportsr recent guava versions, even if it is frozen on 11.0 to avoid surprising people (i.e. all deprecated/removed classes should have been stripped)
> 
> Finally: it's the only version of Hadoop that works on Java 7, has patches to support Java8+kerberos (in fact, Java 7u80+ and kerberos).
> 
> For the reason of JVMs and guava alone, I'd abandon Hadoop < 2.6. Those versions won't work on secure Java 7 clusters, recent guava versions, and have lots of uncorrected issues.
> 
> Oh, and did I mention the test matrix? The later version of Hadoop you use, the less versions to test against. 
> 
>> My general position is that backwards-compatibility and supporting
>> older platforms needs to be a low priority in a major release; it's a
>> decision about what to support for users in the next couple years, not
>> the preceding couple years. Users on older technologies simply stay on
>> the older Spark until ready to update; they are in no sense suddenly
>> left behind otherwise.
> 
> 
> If they are running older versions of Hadoop, they generally have stable apps which they don't bother upgrading. New clusters => new versions => new apps.
> 
> 
> BÔøΩKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKCBÔøΩÔøΩ[ÔøΩÔøΩXÔøΩÔøΩ‹öXÔøΩKK[XZ[ÔøΩ]ÔøΩ][ÔøΩÔøΩXÔøΩÔøΩ‹öXÔøΩPÔøΩ\ÔøΩÀò\XÔøΩKÔøΩ‹ôÔøΩBÔøΩÔøΩ‹àY][€ò[ÔøΩÔøΩ[X[ÔøΩÔøΩK[XZ[ÔøΩ]ÔøΩZ[ÔøΩ\ÔøΩÀò\XÔøΩKÔøΩ‹ôÔøΩBÔøΩ

I have no idea what this is or why it made it to the tail of my email. Maybe outlook has changed its signature for me.


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org
"
Steve Loughran <stevel@hortonworks.com>,"Thu, 14 Jan 2016 19:51:46 +0000",Re: Spark 1.6.0 and HDP 2.2 - problem,=?utf-8?B?TWFjaWVqIEJyecWEc2tp?= <maciek@brynski.pl>,"
> On 13 Jan 2016, at 10:15, Maciej Bry≈Ñski <maciek@brynski.pl> wrote:
> 
> Steve,
> Thank you for the answer.
> How Hortonworks deal with this problem internally ?

Looks like this problem is very brittle to directory loading: if all of a single jackson versions' set of classes is loaded first, then, irrespective of version, you don't see the problem. That's luck, nothing else.


> You have Spark 1.3.1 in HDP 2.3. Is it compilled with Jackson 2.2.3 ?
> 

it only surfaces in 1.6.x,, but I'm now inclined to believe that the problem exists everywhere. You just got to find it first


> Regards,
> Maciek




---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org
"
Bryan Cutler <cutlerb@gmail.com>,"Thu, 14 Jan 2016 14:18:48 -0800",Re: Random Forest FeatureImportance throwing NullPointerException,Rachana Srivastava <Rachana.Srivastava@markmonitor.com>,"Hi Rachana,

I got the same exception.  It is because computing the feature importance
depends on impurity stats, which is not calculated with the old
RandomForestModel in MLlib.  Feel free to create a JIRA for this if you
think it is necessary, otherwise I believe this problem will be eventually
solved as part of this JIRA
https://issues.apache.org/jira/browse/SPARK-12183

Bryan


"
Bryan Cutler <cutlerb@gmail.com>,"Thu, 14 Jan 2016 14:45:11 -0800",Re: Random Forest FeatureImportance throwing NullPointerException,Rachana.Srivastava@thomsonreuters.com,"If you are able to just train the RandomForestClassificationModel from ML
directly instead of training the old model and converting, then that would
be the way to go.


"
Reynold Xin <rxin@databricks.com>,"Thu, 14 Jan 2016 14:47:38 -0800",Re: [discuss] dropping Hadoop 2.2 and 2.3 support in Spark 2.0?,Steve Loughran <stevel@hortonworks.com>,"Thanks for chiming in. Note that an organization's agility in Spark
upgrades can be very different from Hadoop upgrades.

For many orgs, Hadoop is responsible for cluster resource scheduling (YARN)
and data storage (HDFS). These two are notorious difficult to upgrade. It
is all or nothing for a cluster. (You can't have a subset of the nodes
running Hadoop 2.2 and the other subset running Hadoop 2.6). For Spark, it
is a very different story. It is pretty easy to run multiple different
versions of Spark in different applications, even though they are all
running in a single cluster.

As a result, you might see a lot of orgs with really old Hadoop versions
and yet are willing to upgrade to Spark 2.x.






:
e
ially in
ld
e
=> new
KKKKKCBÔøΩ ÔøΩ
 ]ÔøΩ][ÔøΩÔøΩXÔøΩÔøΩ‹öXÔøΩP ÔøΩ \ÔøΩÀò\ XÔøΩ KÔøΩ‹ôÔøΩBÔøΩÔøΩ‹à Y  ] [€ò[  ÔøΩÔøΩ[X[ÔøΩ ÔøΩ
ÔøΩ‹ôÔøΩBÔøΩ
"
Li Li <fancyerii@gmail.com>,"Fri, 15 Jan 2016 10:02:57 +0800",Re: running lda in spark throws exception,Bryan Cutler <cutlerb@gmail.com>,"I got it. I mistakenly thought that each line is a wordid list.


---------------------------------------------------------------------


"
Pete Robbins <robbinspg@gmail.com>,"Fri, 15 Jan 2016 09:09:04 +0000",Elasticsearch sink for metrics,Dev <dev@spark.apache.org>,"Has anyone tried pushing Spark metrics into elasticsearch? We have other
metrics, eg some runtime information, going into ES and would like to be
able to combine this with the Spark metrics for visualization with Kibana.

I experimented with a new sink using ES's ElasticsearchReporter for the
Coda Hale metrics but have a few issues with default mappings.

Has anyone already implemented this before I start to dig deeper?

Cheers,
"
Nick Pentreath <nick.pentreath@gmail.com>,"Fri, 15 Jan 2016 11:18:33 +0200",Re: Elasticsearch sink for metrics,Dev <dev@spark.apache.org>,"I haven't come across anything, but could you provide more detail on what
issues you're encountering?




"
Tim Preece <tepreece@mail.com>,"Fri, 15 Jan 2016 02:30:05 -0700 (MST)",Re: Tungsten in a mixed endian environment,dev@spark.apache.org,"So if Spark does not support heterogeneous endianness clusters, should Spark
at least always support homogeneous endianess clusters ?

I ask because I just noticed
https://issues.apache.org/jira/browse/SPARK-12785 which appears to be
introducing a new feature designed for Little Endian only.





--

---------------------------------------------------------------------


"
Robert Kruszewski <robertk@palantir.com>,"Fri, 15 Jan 2016 16:54:52 +0000",[1.6] Coalesce/binary operator on casted named column,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Spark devs,

I have been debugging failing unit test in our application and it led me to believe that the bug is in spark itself. The exception I am getting is
org.apache.spark.sql.catalyst.analysis.UnresolvedException: Invalid call to dataType on unresolved object, tree: unresolvedalias(cast(string_field#164 as string))
	at org.apache.spark.sql.catalyst.analysis.UnresolvedAlias.dataType(unresolved.scala:295)
	at org.apache.spark.sql.catalyst.expressions.Coalesce$$anonfun$checkInputDataTypes$1.apply(nullExpressions.scala:49)
	at org.apache.spark.sql.catalyst.expressions.Coalesce$$anonfun$checkInputDataTypes$1.apply(nullExpressions.scala:49)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.catalyst.expressions.Coalesce.checkInputDataTypes(nullExpressions.scala:49)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:62)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:57)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:319)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:319)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:53)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:316)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:316)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:265)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
65)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
2)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:305)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:316)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:316)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:316)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:265)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
65)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
2)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:305)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:316)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:316)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:316)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:265)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
65)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
2)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:305)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:316)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:316)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:316)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:265)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
65)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
2)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:305)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:316)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:316)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:316)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:265)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
65)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
2)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:305)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:316)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionUp$1(QueryPlan.scala:107)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2(QueryPlan.scala:117)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:125)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
65)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
2)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:125)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:57)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:50)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:105)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:50)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:44)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:34)
	at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:133)
	at org.apache.spark.sql.DataFrame.org$apache$spark$sql$DataFrame$$withPlan(DataFrame.scala:2165)
	at org.apache.spark.sql.DataFrame.filter(DataFrame.scala:799)
	at org.apache.spark.sql.DataFrame.where(DataFrame.scala:825)
Binary operators seem to have the same problem.

org.apache.spark.sql.catalyst.analysis.UnresolvedException: Invalid call to dataType on unresolved object, tree: unresolvedalias(cast(string_field#219 as string))
	at org.apache.spark.sql.catalyst.analysis.UnresolvedAlias.dataType(unresolved.scala:295)
	at org.apache.spark.sql.catalyst.expressions.BinaryOperator.checkInputDataTypes(Expression.scala:467)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:62)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:57)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:319)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:319)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:53)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:316)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:316)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:265)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
65)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
2)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:305)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:316)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:316)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:316)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:265)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
65)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
2)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:305)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:316)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionUp$1(QueryPlan.scala:107)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2(QueryPlan.scala:117)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:125)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
65)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
2)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:125)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:57)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:50)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:105)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:50)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:44)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:34)
	at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:133)
	at org.apache.spark.sql.DataFrame.org$apache$spark$sql$DataFrame$$withPlan(DataFrame.scala:2165)
	at org.apache.spark.sql.DataFrame.filter(DataFrame.scala:799)
	at org.apache.spark.sql.DataFrame.where(DataFrame.scala:825)
You can reproduce it with following

val df1 = sc.makeRDD(1 to 5).map(i => (i, i * 2)).toDF(""single"", ""double‚Äù)
df1.where(df1.col(""single"").cast(""string"").equalTo(""1""))

What‚Äôs the expected behaviour of type checking unresolved expression?

- Robert


"
Renyi Xiong <renyixiong0@gmail.com>,"Fri, 15 Jan 2016 11:04:34 -0800",Spark Streaming KafkaUtils missing Save API?,dev@spark.apache.org,"Hi,

We noticed there's no Save method in KafkaUtils. we do have scenarios where
we want to save RDD back to Kafka queue to be consumed by down stream
streaming applications.

I wonder if this is a common scenario, if yes, any plan to add it?

Thanks,
Renyi.
"
Nong Li <nongli@gmail.com>,"Fri, 15 Jan 2016 11:10:30 -0800",Re: Tungsten in a mixed endian environment,Tim Preece <tepreece@mail.com>,"
I wouldn't say this is designed for little endian only. If anything, it is
designed to support both
but only little endian is implemented (and more importantly tested). I
don't think it would be
very hard to add big endian support and instead of producing wrong results,
it is going to
fail to run.


"
Benjamin Fradet <benjamin.fradet@gmail.com>,"Fri, 15 Jan 2016 21:00:54 +0100",Re: Spark Streaming KafkaUtils missing Save API?,Renyi Xiong <renyixiong0@gmail.com>,"There was a PR regarding this which was closed but the author of the PR
created a spark-package: https://github.com/cloudera/spark-kafka-writer.

I don't know exactly why it was decided not be incorporated into spark
however.

"
vonnagy <ivan@vadio.com>,"Fri, 15 Jan 2016 15:31:19 -0700 (MST)",Partitioned parquet files missing partition columns from data,dev@spark.apache.org,"When writing a DataFrame into partitioned parquet files, the partition
columns are removed from the data. 

For example:

 df.write.mode(SaveMode.Append).partitionBy('year','month','day',
'hour').parquet(somePath)

This creates a directory structure like:

events
  |->  2016
    |-> 1
      |-> 15
        |-> 10
          |-> part-r-00000-b6d0b99e-8673-4a12-8ab4-379421b008c8.gz.parquet

The column values for the partitions are represented in the directory
structure, but removed from the actual parquet file. When reading back, the
values will be NULL, unless PartitionDiscovery is enabled in Spark. We did
not realize this and missed 2 weeks of data analysis since those fields
where returned as NULL.

The logic that removes the columns from the data schema is in
ResolvedDataSource (~ line 182):

val dataSchema = StructType(data.schema.filterNot(f =>
partitionColumns.contains(f.name)))

When using this data in something like Apache Drill, it returns NULL for all
of the partition fields and thus makes it hard to work with.

Why does it doe this (Parquet Spec?) and would this be considered a defect
or at least have a configuration to not remove the columns from the data
schema?

Any help would be appreciated.

Thanks,

Ivan
            



--

---------------------------------------------------------------------


"
=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@questtec.nl>,"Sat, 16 Jan 2016 00:19:08 +0100",Are we running SparkR tests in Jenkins?,dev@spark.apache.org,"Hi all,

I just noticed the following log entry in Jenkins:

========================================================================
> Running SparkR tests
> ========================================================================
> Running R applications through 'sparkR' is not supported as of Spark 2.0.
> Use ./bin/spark-submit <R file>


Are we still running R tests? Or just saying that this will be deprecated?

Kind regards,

Herman van H√∂vell tot Westerflier
"
Reynold Xin <rxin@databricks.com>,"Fri, 15 Jan 2016 15:20:53 -0800",Re: Are we running SparkR tests in Jenkins?,=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@questtec.nl>,"+Shivaram

Ah damn - we should fix it.

This was broken by https://github.com/apache/spark/pull/10658 - which
removed a functionality that has been deprecated since Spark 1.0.






================================================
========================"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Fri, 15 Jan 2016 15:21:22 -0800",Re: Are we running SparkR tests in Jenkins?,=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@questtec.nl>,"Yes - we should be running R tests AFAIK. That error message is a
deprecation warning about the script `bin/sparkR` which needs to be
changed in https://github.com/apache/spark/blob/7cd7f2202547224593517b392f56e49e4c94cabc/R/run-tests.sh#L26
to bin/spark-submit.

Thanks
Shivaram

================================================
================================================
.
?

---------------------------------------------------------------------


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Fri, 15 Jan 2016 15:23:02 -0800",Re: Are we running SparkR tests in Jenkins?,Reynold Xin <rxin@databricks.com>,"Ah I see. I wasn't aware of that PR. We should do a find and replace
in all the documentation and rest of the repository as well.

Shivaram

=================================================
=================================================
0.
d?

---------------------------------------------------------------------


"
Jeff Zhang <zjffdu@gmail.com>,"Fri, 15 Jan 2016 15:29:22 -0800",Re: Are we running SparkR tests in Jenkins?,shivaram@eecs.berkeley.edu," Right, I forget the documentation, will create a follow up jira.


================================================
================================================


-- 
Best Regards

Jeff Zhang
"
Jeff Zhang <zjffdu@gmail.com>,"Fri, 15 Jan 2016 15:33:13 -0800",Re: Are we running SparkR tests in Jenkins?,shivaram@eecs.berkeley.edu,"Created https://issues.apache.org/jira/browse/SPARK-12846



:
r
================================================
================================================



-- 
Best Regards

Jeff Zhang
"
Felix Cheung <felixcheung_m@hotmail.com>,"Sun, 17 Jan 2016 00:26:01 -0800",RE: Are we running SparkR tests in Jenkins?,"Jeff Zhang <zjffdu@gmail.com>, ""shivaram@eecs.berkeley.edu""
	<shivaram@eecs.berkeley.edu>","I think that breaks sparkR, the commandline script, and Jenkins, in which run-test.sh is calling sparkR.
I'll work on this - since this also affects my PR #10652...
 
Date: Fri, 15 Jan 2016 15:33:13 -0800
Subject: Re: Are we running SparkR tests in Jenkins?
From: zjffdu@gmail.com
To: shivaram@eecs.berkeley.edu
CC: rxin@databricks.com; hvanhovell@questtec.nl; dev@spark.apache.org; shivaram.venkataraman@gmail.com

Created https://issues.apache.org/jira/browse/SPARK-12846

 Right, I forget the documentation, will create a follow up jira.  
Ah I see. I wasn't aware of that PR. We should do a find and replace

in all the documentation and rest of the repository as well.



Shivaram



ote:












r







=================================================


=================================================

0.




d?









---------------------------------------------------------------------


For additional commands, e-mail: dev-help@spark.apache.org





-- 
Best Regards

Jeff Zhang



-- 
Best Regards

Jeff Zhang
 		 	   		  "
Jia Zou <jacquelinezou@gmail.com>,"Sun, 17 Jan 2016 09:29:05 -0600",Reuse Executor JVM across different JobContext,"dev@spark.apache.org, ""user @spark"" <user@spark.apache.org>","Dear all,

Is there a way to reuse executor JVM across different JobContexts? Thanks.

Best Regards,
Jia
"
Reynold Xin <rxin@databricks.com>,"Sun, 17 Jan 2016 21:39:39 -0800",Re: [1.6] Coalesce/binary operator on casted named column,Robert Kruszewski <robertk@palantir.com>,"To close the loop:

JIRA filed: https://issues.apache.org/jira/browse/SPARK-12841

Patch created (to be merged): https://github.com/apache/spark/pull/10781





to dataType on unresolved object, tree: unresolvedalias(cast(string_field#164 as string))
olved.scala:295)
tDataTypes$1.apply(nullExpressions.scala:49)
tDataTypes$1.apply(nullExpressions.scala:49)
.scala:244)
.scala:244)
scala:59)
s(nullExpressions.scala:49)
alysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:62)
alysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:57)
pply(TreeNode.scala:319)
pply(TreeNode.scala:319)
e.scala:53)
la:318)
e.scala:316)
e.scala:316)
e.scala:265)
48)
103)
47)
:265)
252)
de.scala:305)
la:316)
e.scala:316)
e.scala:316)
e.scala:265)
48)
103)
47)
:265)
252)
de.scala:305)
la:316)
e.scala:316)
e.scala:316)
e.scala:265)
48)
103)
47)
:265)
252)
de.scala:305)
la:316)
e.scala:316)
e.scala:316)
e.scala:265)
48)
103)
47)
:265)
252)
de.scala:305)
la:316)
e.scala:316)
e.scala:316)
e.scala:265)
48)
103)
47)
:265)
252)
de.scala:305)
la:316)
(QueryPlan.scala:107)
talyst$plans$QueryPlan$$recursiveTransform$2(QueryPlan.scala:117)
lan.scala:125)
48)
103)
47)
:265)
252)
QueryPlan.scala:125)
alysis$1.apply(CheckAnalysis.scala:57)
alysis$1.apply(CheckAnalysis.scala:50)
:105)
sis(CheckAnalysis.scala:50)
r.scala:44)
cution.scala:34)
an(DataFrame.scala:2165)
to dataType on unresolved object, tree: unresolvedalias(cast(string_field#219 as string))
olved.scala:295)
taTypes(Expression.scala:467)
alysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:62)
alysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:57)
pply(TreeNode.scala:319)
pply(TreeNode.scala:319)
e.scala:53)
la:318)
e.scala:316)
e.scala:316)
e.scala:265)
48)
103)
47)
:265)
252)
de.scala:305)
la:316)
e.scala:316)
e.scala:316)
e.scala:265)
48)
103)
47)
:265)
252)
de.scala:305)
la:316)
(QueryPlan.scala:107)
talyst$plans$QueryPlan$$recursiveTransform$2(QueryPlan.scala:117)
lan.scala:125)
48)
103)
47)
:265)
252)
QueryPlan.scala:125)
alysis$1.apply(CheckAnalysis.scala:57)
alysis$1.apply(CheckAnalysis.scala:50)
:105)
sis(CheckAnalysis.scala:50)
r.scala:44)
cution.scala:34)
an(DataFrame.scala:2165)
ble‚Äù)
ion?
"
Pete Robbins <robbinspg@gmail.com>,"Mon, 18 Jan 2016 16:46:26 +0000",Fwd: Elasticsearch sink for metrics,Dev <dev@spark.apache.org>,"The issue I had was with the ElasticsearchReporter and how it maps eg a
Gauge in JSON. The ""value"" was typed to whatever the first Guage was, eg
int, which caused issues with some of my other guages which were double.

As I say I've just started looking at this and was wanting to see if this
was already implemented before continuing.


"
Scott walent <scottwalent@gmail.com>,"Mon, 18 Jan 2016 18:55:05 +0000",Spark Summit East - Full Schedule Available,"""dev@spark.apache.org"" <dev@spark.apache.org>, ""user@spark.apache.org"" <user@spark.apache.org>","Join the Apache Spark community at the 2nd annual Spark Summit East from
February 16-18, 2016 in New York City.

We will kick things off with a Spark update from Matei Zaharia followed by
over 60 talks that were selected by the program committee. The agenda this
year includes enterprise talks from Microsoft, Bloomberg and Comcast as
well as the popular developer, data science, research and application
tracks.  See the full agenda at https://spark-summit.org/east-2016/schedule.


If you are new to Spark or looking to improve on your knowledge of the
technology, we are offering three levels of Spark Training: Spark
Essentials, Advanced Exploring Wikipedia with Spark, and Data Science with
Spark. Visit https://spark-summit.org/east-2016/schedule/spark-training for
details.

Space is limited and we anticipate selling out, so register now! Use promo
code ""ApacheListEast"" to save 20% when registering before January 29, 2016.
Register at https://spark-summit.org/register.

We look forward to seeing you there.

Scott and the Summit Organizers
"
Hyukjin Kwon <gurwls223@gmail.com>,"Tue, 19 Jan 2016 14:06:33 +0900",Unable to compile and test Spark in IntelliJ,dev@spark.apache.org,"Hi all,

I usually have been working with Spark in IntelliJ.

Before this PR,
https://github.com/apache/spark/commit/7cd7f2202547224593517b392f56e49e4c94cabc
for
`[SPARK-12575][SQL] Grammar parity with existing SQL parser`. I was able to
just open the project and then run some tests with IntelliJ Run button.

However, it looks that PR adds some ANTLR files for parsing and I cannot
run the tests as I did. So, I ended up with doing this by mvn compile first
and then running some tests with IntelliJ.

I can still run some tests with sbt or maven in comment line but this is a
bit inconvenient. I just want to run some tests as I did in IntelliJ.

I followed this
https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools
several times but it still emits some exceptions such as

Error:(779, 34) not found: value SparkSqlParser
    case ast if ast.tokenType == SparkSqlParser.TinyintLiteral =>
                                 ^

and I still should run mvn compile or mvn test first for them.

Is there any good way to run some Spark tests within IntelliJ as I did
before?

Thanks!
"
Li Li <fancyerii@gmail.com>,"Tue, 19 Jan 2016 21:31:14 +0800",Re: running lda in spark throws exception,Bryan Cutler <cutlerb@gmail.com>,"I have modified my codes. I can get the total vocabulary size and
index array and freq array from the jsonobject.

        JsonArray idxArr = jo.get(""idxArr"").getAsJsonArray();

        JsonArray freqArr=jo.get(""freqArr"").getAsJsonArray();

        int total=jo.get(""vocabSize"").getAsInt();

        String url = jo.get(""webpageUrl"").getAsString();

        int[] idx = new int[idxArr.size()];

        double[] freq=new double[freqArr.size()];

        for(int i=0;i<idxArr.size();i++){

          idx[i]=idxArr.get(i).getAsInt();

          freq[i]=freqArr.get(i).getAsDouble();

        }



        return new VectorUrl(Vectors.sparse(total, idx, freq), url);

But when I run it, it throws exception Job aborted due to stage
failure: Total size of serialized results of 22 tasks (3.1 GB) is
bigger than spark.driver.maxResultSize (3.0 GB)
       I have set result to 3g but it still says not engouh.
       conf.set(""spark.driver.maxResultSize"", ""3g"");
       How much memory will it use?
I use the following script to submit job to yarn cluster.

bin/spark-submit --class xxx.yyyy.ReviewLDA \
    --master yarn-cluster \
    --num-executors 10 \
    --driver-memory 4g \
    --executor-memory 4g \
    --executor-cores 2


---------------------------------------------------------------------


"
doruchiulan <doru.chiulan@gmail.com>,"Tue, 19 Jan 2016 07:21:22 -0700 (MST)",Spark LDA model reuse with new set of data,dev@spark.apache.org,"Hi,

Just so you know, I am new to Spark, and also very new to ML (this is my
first contact with ML).

Ok, I am trying to write a DSL where you can run some commands.

I did a command that trains the Spark LDA and it produces the topics I want
and I saved it using the save method provided by the LDAModel.

Now I want to load this LDAModel and use it to predict on a new set of data.
I call the load method, obtain the LDAModel instance but here I am stuck.

Isnt this possible ? Am I wrong in the way I understood LDA and we cannot
reuse trained LDA to analyse new data ?

If its possible can you point me to some documentation, or give me a hint on
how should I do that.

Thx



--

---------------------------------------------------------------------


"
=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"Tue, 19 Jan 2016 17:08:46 +0100",Re: Removing the Mesos fine-grained mode,Timothy Chen <tnachen@gmail.com>,"It would be good to get to the bottom of this.

Adam, could you share the Spark app that you're using to test this?

iulian


s
y
b
:
t
ia


-- 

--
Iulian Dragos

------
Reactive Apps on the JVM
www.typesafe.com
"
Adam McElwee <adam@mcelwee.me>,"Tue, 19 Jan 2016 10:41:57 -0600",Re: Removing the Mesos fine-grained mode,=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"Sorry, I never got a chance to circle back with the master logs for this. I
definitely can't share the job code, since it's used to build a pretty core
dataset for my company, but let me see if I can pull some logs together in
the next couple days.

e.com>

,
ly
ob
r
y
d
:
%
lia
"
Elliot West <teabot@gmail.com>,"Tue, 19 Jan 2016 21:08:15 +0000",Fwd: Spark integration with HCatalog (specifically regarding partitions),dev@spark.apache.org,"(Cross posted from user@spark.apache.org)

Hello,

I am in the process of evaluating Spark (1.5.2) for a wide range of use
cases. In particular I'm keen to understand the depth of the integration
with HCatalog (aka the Hive Metastore). I am very encouraged when browsing
the source contained within the org.apache.spark.sql.hive package. My goals
are to evaluate how effectively Spark handles the following scenarios:

   1. Reading from an unpartitioned HCatalog table.
   2. Reading from a partitioned HCatalog table with partition pruning from
   filter pushdown.
   3. Writing to a new unpartitioned HCatalog table.
   4. Writing to a new partitioned HCatalog table.
   5. Adding a partition to a partitioned HCatalog table.

I found that the first three cases appear to function beautifully. However,
I cannot seem to effectively create new HCatalog aware partitions either in
a new table or on and existing table (cases 4 & 5). I suspect this may be
due to my inexperience with Spark so wonder if you could advise me on what
to try next. Here's what I have:

*Case 4: Writing to a new partitioned HCatalog table*

Create a source in Hive (could be plain data file also):


hive (default)> create table foobar ( id int, name string );
hive (default)> insert into table foobar values (1, ""xxx""), (2, ""zzz"");

Read the source with Spark, partition the data, and write to a new table:

sqlContext.sql(""select *
from foobar"").write.format(""orc"").partitionBy(""id"").saveAsTable(""raboof"")


Check for the new table in Hive, it is partitioned correctly although the
formats and schema are unexpected:

hive (default)> show table extended like 'raboof';
OK
tab_name
tableName: raboof
location:hdfs://host:port/user/hive/warehouse/raboof
inputformat:org.apache.hadoop.mapred.SequenceFileInputFormat
outputformat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
columns:struct columns { list<string> col}
partitioned:true
partitionColumns:struct partition_columns { i32 id}


Check for correctly partitioned data on HDFS, it appears to be there:

[me@host]$ hdfs dfs -ls -R /user/hive/warehouse/raboof
/user/hive/warehouse/raboof/_SUCCESS
/user/hive/warehouse/raboof/id=1
/user/hive/warehouse/raboof/id=1/part-r-00000-<uuid1>.orc
/user/hive/warehouse/raboof/id=2
/user/hive/warehouse/raboof/id=2/part-r-00000-<uuid2>.orc


Something is wrong however, no data is returned from this query and the
column names appear incorrect:

hive (default)> select * from default.raboof;
OK
col id

HCatalog reports no partitions for the table:

hive (default)> show partitions default.raboof;
OK
partition

*Case 5: Adding a partition to a partitioned HCatalog table*

Created partitioned source table in Hive:

hive (default)> create table foobar ( name string )
              > partitioned by ( id int )
              > stored as orc;
hive (default)> insert into table foobar PARTITION (id)
              > values (""xxx"", 1), (""yyy"", 2);


Created a source for a new record to add to new_record_source:

hive (default)> create table new_record_source ( id int, name string )
              > stored as orc;
hive (default)> insert into table new_record_source
              > values (3, ""zzz"");


Trying to add a partition with:

sqlContext.sql(""select *
from new_record_source"").write.mode(""append"").partitionBy(""id"").saveAsTable(""foobar"")


This almost did what I wanted:

hive (default)> show partitions default.foobar;
partition
id=1
id=2
id=__HIVE_DEFAULT_PARTITION__

hive (default)> select * from default.foobar;
name id
xxx 1
yyy 2
3 NULL

Any assistance would be greatly appreciated.

Many thanks - Elliot.
"
Boric Tan <it.news.trends@gmail.com>,"Tue, 19 Jan 2016 16:12:46 -0800",How Spark utilize low-level architecture features?,dev@spark.apache.org,"Hi there,

I am new to Spark, and would like to get some help to understand if Spark
can utilize the underlying architectures for better performance. If so, how
does it do it?

For example, assume there is a cluster built with machines of different
CPUs, will Spark check the individual CPU information and use some
machine-specific setting for the tasks assigned to that machine? Or is it
totally dependent on the underlying JVM implementation to run the JAR file,
and therefor the JVM is the place to check if certain CPU features can be
used?

Thanks,
Boric
"
"""Driesprong, Fokko"" <fokko@driesprong.frl>","Wed, 20 Jan 2016 13:11:21 +0100",Optimized toIndexedRowMatrix,dev@spark.apache.org,"Hi guys,

I've been working on an optimized implementation of the toIndexedRowMatrix
<https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/linalg/distributed/BlockMatrix.scala#L271>
of the BlockMatrix. I already created a ticket
<https://issues.apache.org/jira/browse/SPARK-12869> and submitted a pull
<https://github.com/apache/spark/pull/10839> request at Github. What has to
be done to get this accepted? All the tests are passing.

<https://github.com/Fokko/BlockMatrixToIndexedRowMatrix> to see how the
performance is affected, for dense matrices this is a speedup of almost 19
times. Also for sparse matrices it will most likely be more optimal, as the
current implementation requires a lot of shuffling and creates high volumes
of intermediate objects (unless it is super sparse, but then also a
BlockMatrix would not be very optimal).

I would appreciate suggestions or tips to get this accepted.

Cheers, Fokko Driesprong.
"
=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"Wed, 20 Jan 2016 18:14:14 +0100",Re: Removing the Mesos fine-grained mode,Adam McElwee <adam@mcelwee.me>,"That'd be great, thanks Adam!


:
o
g
),
n
s
y
or
t
ly
e
r
e
s
.
d


-- 

--
Iulian Dragos

------
Reactive Apps on the JVM
www.typesafe.com
"
Jacek Laskowski <jacek@japila.pl>,"Wed, 20 Jan 2016 20:46:37 +0100",BUILD FAILURE at Spark Project Test Tags for 2.11.7?,dev <dev@spark.apache.org>,"Hi,

Build is broken again for me :( I build for Scala 2.11.7 and use
maven. Is this a known issue? Anyone looking into it?

‚ûú  spark git:(master) ‚úó ./build/mvn -Pyarn -Phadoop-2.6
-Dhadoop.version=2.7.1 -Dscala-2.11 -Phive -Phive-thriftserver
-DskipTests clean install
...
[INFO] --- scala-maven-plugin:3.2.2:compile (scala-compile-first) @
spark-test-tags_2.11 ---
[INFO] Using zinc server for incremental compilation
[warn] Pruning sources from previous analysis, due to incompatible CompileSetup.
[info] Compiling 3 Java sources to
/Users/jacek/dev/oss/spark/tags/target/scala-2.11/classes...
[error] Cannot run program ""javac"": error=2, No such file or directory
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO]
[INFO] Spark Project Parent POM ........................... SUCCESS [  2.843 s]
[INFO] Spark Project Test Tags ............................ FAILURE [  0.321 s]


Pozdrawiam,
Jacek

Jacek Laskowski | https://medium.com/@jaceklaskowski/
Mastering Apache Spark
==> https://jaceklaskowski.gitbooks.io/mastering-apache-spark/
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Marcelo Vanzin <vanzin@cloudera.com>,"Wed, 20 Jan 2016 11:48:50 -0800",Re: BUILD FAILURE at Spark Project Test Tags for 2.11.7?,Jacek Laskowski <jacek@japila.pl>,"
That doesn't exactly look like a Spark problem.

-- 
Marcelo

---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Wed, 20 Jan 2016 22:23:41 +0100",Re: BUILD FAILURE at Spark Project Test Tags for 2.11.7?,Marcelo Vanzin <vanzin@cloudera.com>,"
You're right and moreover I just today upgraded Java 8 to the latest
release. Kafka compiles fine (they use gradle, though). Other apps
build fine too. Just a friendly heads-up.

Jacek

---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Wed, 20 Jan 2016 22:44:01 +0100",Re: BUILD FAILURE at Spark Project Test Tags for 2.11.7?,Marcelo Vanzin <vanzin@cloudera.com>,"
It *was* a Spark problem. The issue was that zinc was up while I
upgraded JDK and eventually it couldn't find proper binaries. When I
killed com.typesafe.zinc.Nailgun the build went fine.

I remember I saw the issue reported in the past and when I was
completely hopeless to figure it out without rebooting the machine the
idea of zinc being ""misconfigured"" came! `jps -lm` to the rescue!

Sorry for the noise.

Jacek

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Wed, 20 Jan 2016 22:47:57 +0100",Re: BUILD FAILURE at Spark Project Test Tags for 2.11.7?,Jacek Laskowski <jacek@japila.pl>,"That's not a Spark problem. Your compiler was not available.


---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Wed, 20 Jan 2016 23:14:38 +0100",Re: BUILD FAILURE at Spark Project Test Tags for 2.11.7?,Sean Owen <sowen@cloudera.com>,"
It's Spark to use zinc. It starts it for a built, but alas doesn't
stop it afterwards. If you happen to upgrade your JDK without
restarting zinc, you *will* run into the issue. If you say it's not a
Spark problem, whom could that be? I personally don't use zinc. I
can't seem to find anyone more guilty than Spark. Sorry.

Jacek

---------------------------------------------------------------------


"
"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Thu, 21 Jan 2016 02:05:03 +0000",RE: Using CUDA within Spark / boosting linear algebra,"Sam Halliday <sam.halliday@gmail.com>, John Canny <canny@berkeley.edu>","Hi Everyone,

I‚Äôve updated the benchmark and done experiments with new hardware with 2x Nvidia Tesla K80 (physically 4x Tesla K40) and 2x modern Haswell CPU Intel E5-2650 v3 @ 2.30GHz.

This time I computed average and median of 10 runs for each of experiment and approximated FLOPS.

Results are available at google docs (old experiments are in the other 2 sheets):
https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J5r7kwKSPkY/edit?usp=sharing
Benchmark code:
https://github.com/avulanov/scala-blas

Best regards, Alexander


From: Sam Halliday [mailto:sam.halliday@gmail.com]
Sent: Thursday, March 26, 2015 9:27 AM
To: John Canny
Cc: Xiangrui Meng; dev@spark.apache.org; Joseph Bradley; Evan R. Sparks; Ulanov, Alexander
Subject: Re: Using CUDA within Spark / boosting linear algebra


John, I have to disagree with you there. Dense matrices come up a lot in industry,  although your personal experience may be different.
On 26 Mar 2015 16:20, ""John Canny"" <canny@berkeley.edu<mailto:canny@berkeley.edu>> wrote:
I mentioned this earlier in the thread, but I'll put it out again. Dense BLAS are not very important for most machine learning workloads: at least for non-image workloads in industry (and for image processing you would probably want a deep learning/SGD solution with convolution kernels). e.g. it was only relevant for 1/7 of our recent benchmarks, which should be a reasonable sample. What really matters is sparse BLAS performance. BIDMat is still an order of magnitude faster there. Those kernels are only in BIDMat, since NVIDIAs sparse BLAS dont perform well on power-law data.

Its also the case that the overall performance of an algorithm is determined by the slowest kernel, not the fastest. If the goal is to get closer to BIDMach's performance on typical problems, you need to make sure that every kernel goes at comparable speed. So the real question is how much faster MLLib routines do on a complete problem with/without GPU acceleration. For BIDMach, its close to a factor of 10. But that required running entirely on the GPU, and making sure every kernel is close to its limit.

-John

If you think nvblas would be helpful, you should try it in some end-to-end benchmarks.
On 3/25/15, 6:23 PM, Evan R. Sparks wrote:
Yeah, much more reasonable - nice to know that we can get full GPU performance from breeze/netlib-java - meaning there's no compelling performance reason to switch out our current linear algebra library (at least as far as this benchmark is concerned).

Instead, it looks like a user guide for configuring Spark/MLlib to use the right BLAS library will get us most of the way there. Or, would it make sense to finally ship openblas compiled for some common platforms (64-bit linux, windows, mac) directly with Spark - hopefully eliminating the jblas warnings once and for all for most users? (Licensing is BSD) Or am I missing something?

On Wed, Mar 25, 2015 at 6:03 PM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
As everyone suggested, the results were too good to be true, so I double-checked them. It turns that nvblas did not do multiplication due to parameter NVBLAS_TILE_DIM from ""nvblas.conf"" and returned zero matrix. My previously posted results with nvblas are matrices copying only. The default NVBLAS_TILE_DIM==2048 is too big for my graphic card/matrix size. I handpicked other values that worked. As a result, netlib+nvblas is on par with BIDMat-cuda. As promised, I am going to post a how-to for nvblas configuration.

https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J5r7kwKSPkY/edit?usp=sharing



-----Original Message-----
From: Ulanov, Alexander
Sent: Wednesday, March 25, 2015 2:31 PM
To: Sam Halliday
Cc: dev@spark.apache.org<mailto:dev@spark.apache.org>; Xiangrui Meng; Joseph Bradley; Evan R. Sparks; jfcanny
Subject: RE: Using CUDA within Spark / boosting linear algebra

Hi again,

I finally managed to use nvblas within Spark+netlib-java. It has exceptional performance for big matrices with Double, faster than BIDMat-cuda with Float. But for smaller matrices, if you will copy them to/from GPU, OpenBlas or MKL might be a better choice. This correlates with original nvblas presentation on GPU conf 2013 (slide 21): http://on-demand.gputechconf.com/supercomputing/2013/presentation/SC3108-New-Features-CUDA%206%20-GPU-Acceleration.pdf

My results:
https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J5r7kwKSPkY/edit?usp=sharing

Just in case, these tests are not for generalization of performance of different libraries. I just want to pick a library that does at best dense matrices multiplication for my task.

P.S. My previous issue with nvblas was the following: it has Fortran blas functions, at the same time netlib-java uses C cblas functions. So, one needs cblas shared library to use nvblas through netlib-java. Fedora does not have cblas (but Debian and Ubuntu have), so I needed to compile it. I could not use cblas from Atlas or Openblas because they link to their implementation and not to Fortran blas.

Best regards, Alexander

-----Original Message-----
From: Ulanov, Alexander
Sent: Tuesday, March 24, 2015 6:57 PM
To: Sam Halliday
Cc: dev@spark.apache.org<mailto:dev@spark.apache.org>; Xiangrui Meng; Joseph Bradley; Evan R. Sparks
Subject: RE: Using CUDA within Spark / boosting linear algebra

Hi,

I am trying to use nvblas with netlib-java from Spark. nvblas functions should replace current blas functions calls after executing LD_PRELOAD as suggested in http://docs.nvidia.com/cuda/nvblas/#Usage without any changes to netlib-java. It seems to work for simple Java example, but I cannot make it work with Spark. I run the following:
export LD_LIBRARY_PATH=/usr/local/cuda-6.5/lib64
env LD_PRELOAD=/usr/local/cuda-6.5/lib64/libnvblas.so ./spark-shell --driver-memory 4G In nvidia-smi I observe that Java is to use GPU:
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0      8873    C   bash                                            39MiB |
|    0      8910    C   /usr/lib/jvm/java-1.7.0/bin/java                39MiB |
+-----------------------------------------------------------------------------+

In Spark shell I do matrix multiplication and see the following:
15/03/25 06:48:01 INFO JniLoader: successfully loaded /tmp/jniloader8192964377009965483netlib-native_system-linux-x86_64.so
So I am sure that netlib-native is loaded and cblas supposedly used. However, matrix multiplication does executes on CPU since I see 16% of CPU used and 0% of GPU used. I also checked different matrix sizes, from 100x100 to 12000x12000

Could you suggest might the LD_PRELOAD not affect Spark shell?

Best regards, Alexander



From: Sam Halliday [mailto:sam.halliday@gmail.com<mailto:sam.halliday@gmail.com>]
Sent: Monday, March 09, 2015 6:01 PM
To: Ulanov, Alexander
Cc: dev@spark.apache.org<mailto:dev@spark.apache.org>; Xiangrui Meng; Joseph Bradley; Evan R. Sparks
Subject: RE: Using CUDA within Spark / boosting linear algebra


Thanks so much for following up on this!

Hmm, I wonder if we should have a concerted effort to chart performance on various pieces of hardware...
On 9 Mar 2015 21:08, ""Ulanov, Alexander"" <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
Hi Everyone, I've updated the benchmark as Xiangrui suggested. Added the comment that BIDMat 0.9.7 uses Float matrices in GPU (although I see the support of Double in the current source code), did the test with BIDMat and CPU Double matrices. BIDMat MKL is indeed on par with netlib MKL.

https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J5r7kwKSPkY/edit?usp=sharing

Best regards, Alexander

-----Original Message-----
From: Sam Halliday [mailtoilto:sam.halliday@gmail.com<mailto:sam.halliday@gmail.com>>]
Sent: Tuesday, March 03, 2015 1:54 PM
To: Xiangrui Meng; Joseph Bradley
Cc: Evan R. Sparks; Ulanov, Alexander; dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.apache.org<mailto:dev@spark.apache.org>>
Subject: Re: Using CUDA within Spark / boosting linear algebra

BTW, is anybody on this list going to the London Meetup in a few weeks?

https://skillsmatter.com/meetups/6987-apache-spark-living-the-post-mapreduce-world#community

Would be nice to meet other people working on the guts of Spark! :-)


Xiangrui Meng <mengxr@gmail.com<mailto:mengxr@gmail.com><mailto:mengxr@gmail.com<mailto:mengxr@gmail.com>>> writes:

> Hey Alexander,
>
> I don't quite understand the part where netlib-cublas is about 20x
> slower than netlib-openblas. What is the overhead of using a GPU BLAS
> with netlib-java?
>
> CC'ed Sam, the author of netlib-java.
>
> Best,
> Xiangrui
>
> On Wed, Feb 25, 2015 at 3:36 PM, Joseph Bradley <joseph@databricks.com<mailto:joseph@databricks.com><mailto:joseph@databricks.com<mailto:joseph@databricks.com>>> wrote:
>> Better documentation for linking would be very helpful!  Here's a JIRA:
>> https://issues.apache.org/jira/browse/SPARK-6019
>>
>>
>> On Wed, Feb 25, 2015 at 2:53 PM, Evan R. Sparks
>> <evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>>
>> wrote:
>>
>>> Thanks for compiling all the data and running these benchmarks,
>>> Alex. The big takeaways here can be seen with this chart:
>>>
>>> https://docs.google.com/spreadsheets/d/1aRm2IADRfXQV7G2vrcVh4StF50uZ
>>> Hl6kmAJeaZZggr0/pubchart?oid=1899767119&format=interactive
>>>
>>> 1) A properly configured GPU matrix multiply implementation (e.g.
>>> BIDMat+GPU) can provide substantial (but less than an order of
>>> BIDMat+magnitude)
>>> benefit over a well-tuned CPU implementation (e.g. BIDMat+MKL or
>>> netlib-java+openblas-compiled).
>>> 2) A poorly tuned CPU implementation can be 1-2 orders of magnitude
>>> worse than a well-tuned CPU implementation, particularly for larger matrices.
>>> (netlib-f2jblas or netlib-ref) This is not to pick on netlib - this
>>> basically agrees with the authors own benchmarks (
>>> https://github.com/fommil/netlib-java)
>>>
>>> I think that most of our users are in a situation where using GPUs
>>> may not be practical - although we could consider having a good GPU
>>> backend available as an option. However, *ALL* users of MLlib could
>>> benefit (potentially tremendously) from using a well-tuned CPU-based
>>> BLAS implementation. Perhaps we should consider updating the mllib
>>> guide with a more complete section for enabling high performance
>>> binaries on OSX and Linux? Or better, figure out a way for the
>>> system to fetch these automatically.
>>>
>>> - Evan
>>>
>>>
>>>
>>> On Thu, Feb 12, 2015 at 4:18 PM, Ulanov, Alexander <
>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
>>>
>>>> Just to summarize this thread, I was finally able to make all
>>>> performance comparisons that we discussed. It turns out that:
>>>> BIDMat-cublas>>BIDMat
>>>> MKL==netlib-mkl==netlib-openblas-compiled>netlib-openblas-yum-repo=
>>>> =netlib-cublas>netlib-blas>f2jblas
>>>>
>>>> Below is the link to the spreadsheet with full results.
>>>>
>>>> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx
>>>> 378T9J5r7kwKSPkY/edit?usp=sharing
>>>>
>>>> One thing still needs exploration: does BIDMat-cublas perform
>>>> copying to/from machine‚Äôs RAM?
>>>>
>>>> -----Original Message-----
>>>> From: Ulanov, Alexander
>>>> Sent: Tuesday, February 10, 2015 2:12 PM
>>>> To: Evan R. Sparks
>>>> Cc: Joseph Bradley;
>>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.apache.org<mailto:dev@spark.apache.org>>
>>>> Subject: RE: Using CUDA within Spark / boosting linear algebra
>>>>
>>>> Thanks, Evan! It seems that ticket was marked as duplicate though
>>>> the original one discusses slightly different topic. I was able to
>>>> link netlib with MKL from BIDMat binaries. Indeed, MKL is
>>>> statically linked inside a 60MB library.
>>>>
>>>> |A*B  size | BIDMat MKL | Breeze+Netlib-MKL  from BIDMat|
>>>> Breeze+Netlib-OpenBlas(native system)| Breeze+Netlib-f2jblas |
>>>> +-----------------------------------------------------------------------+
>>>> |100x100*100x100 | 0,00205596 | 0,000381 | 0,03810324 | 0,002556 |
>>>> |1000x1000*1000x1000 | 0,018320947 | 0,038316857 | 0,51803557
>>>> |1,638475459 |
>>>> |10000x10000*10000x10000 | 23,78046632 | 32,94546697 |445,0935211 |
>>>> 1569,233228 |
>>>>
>>>> It turn out that pre-compiled MKL is faster than precompiled
>>>> OpenBlas on my machine. Probably, I‚Äôll add two more columns with
>>>> locally compiled openblas and cuda.
>>>>
>>>> Alexander
>>>>
>>>> From: Evan R. Sparks
>>>> [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
>>>> Sent: Monday, February 09, 2015 6:06 PM
>>>> To: Ulanov, Alexander
>>>> Cc: Joseph Bradley;
>>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.apache.org<mailto:dev@spark.apache.org>>
>>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>>>
>>>> Great - perhaps we can move this discussion off-list and onto a
>>>> JIRA ticket? (Here's one:
>>>> https://issues.apache.org/jira/browse/SPARK-5705)
>>>>
>>>> It seems like this is going to be somewhat exploratory for a while
>>>> (and there's probably only a handful of us who really care about
>>>> fast linear
>>>> algebra!)
>>>>
>>>> - Evan
>>>>
>>>> On Mon, Feb 9, 2015 at 4:48 PM, Ulanov, Alexander <
>>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>>> wrote:
>>>> Hi Evan,
>>>>
>>>> Thank you for explanation and useful link. I am going to build
>>>> OpenBLAS, link it with Netlib-java and perform benchmark again.
>>>>
>>>> Do I understand correctly that BIDMat binaries contain statically
>>>> linked Intel MKL BLAS? It might be the reason why I am able to run
>>>> BIDMat not having MKL BLAS installed on my server. If it is true, I
>>>> wonder if it is OK because Intel sells this library. Nevertheless,
>>>> it seems that in my case precompiled MKL BLAS performs better than
>>>> precompiled OpenBLAS given that BIDMat and Netlib-java are supposed to be on par with JNI overheads.
>>>>
>>>> Though, it might be interesting to link Netlib-java with Intel MKL,
>>>> as you suggested. I wonder, are John Canny (BIDMat) and Sam
>>>> Halliday
>>>> (Netlib-java) interested to compare their libraries.
>>>>
>>>> Best regards, Alexander
>>>>
>>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>><mailto:
>>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>>]
>>>> Sent: Friday, February 06, 2015 5:58 PM
>>>>
>>>> To: Ulanov, Alexander
>>>> Cc: Joseph Bradley;
>>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.apache.org<mailto:dev@spark.apache.org>><mailto:dev@spark<mailto:dev@spark>.
>>>> apache.org<http://apache.org><mailto:dev@spark.apache.org<mailto:dev@spark.apache.org>>>
>>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>>>
>>>> I would build OpenBLAS yourself, since good BLAS performance comes
>>>> from getting cache sizes, etc. set up correctly for your particular
>>>> hardware - this is often a very tricky process (see, e.g. ATLAS),
>>>> but we found that on relatively modern Xeon chips, OpenBLAS builds
>>>> quickly and yields performance competitive with MKL.
>>>>
>>>> To make sure the right library is getting used, you have to make
>>>> sure it's first on the search path - export
>>>> LD_LIBRARY_PATH=/path/to/blas/library.so will do the trick here.
>>>>
>>>> For some examples of getting netlib-java setup on an ec2 node and
>>>> some example benchmarking code we ran a while back, see:
>>>> https://github.com/shivaram/matrix-bench
>>>>
>>>> In particular - build-openblas-ec2.sh shows you how to build the
>>>> library and set up symlinks correctly, and scala/run-netlib.sh
>>>> shows you how to get the path setup and get that library picked up by netlib-java.
>>>>
>>>> In this way - you could probably get cuBLAS set up to be used by
>>>> netlib-java as well.
>>>>
>>>> - Evan
>>>>
>>>> On Fri, Feb 6, 2015 at 5:43 PM, Ulanov, Alexander <
>>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>>> wrote:
>>>> Evan, could you elaborate on how to force BIDMat and netlib-java to
>>>> force loading the right blas? For netlib, I there are few JVM
>>>> flags, such as
>>>> -Dcom.github.fommil.netlib.BLAS=com.github.fommil.netlib.F2jBLAS,
>>>> so I can force it to use Java implementation. Not sure I understand how to force use a specific blas (not specific wrapper for blas).
>>>>
>>>> Btw. I have installed openblas (yum install openblas), so I suppose
>>>> that netlib is using it.
>>>>
>>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>><mailto:
>>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>>]
>>>> Sent: Friday, February 06, 2015 5:19 PM
>>>> To: Ulanov, Alexander
>>>> Cc: Joseph Bradley;
>>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.apache.org<mailto:dev@spark.apache.org>><mailto:dev@spark<mailto:dev@spark>.
>>>> apache.org<http://apache.org><mailto:dev@spark.apache.org<mailto:dev@spark.apache.org>>>
>>>>
>>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>>>
>>>> Getting breeze to pick up the right blas library is critical for
>>>> performance. I recommend using OpenBLAS (or MKL, if you already have it).
>>>> It might make sense to force BIDMat to use the same underlying BLAS
>>>> library as well.
>>>>
>>>> On Fri, Feb 6, 2015 at 4:42 PM, Ulanov, Alexander <
>>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>>> wrote:
>>>> Hi Evan, Joseph
>>>>
>>>> I did few matrix multiplication test and BIDMat seems to be ~10x
>>>> faster than netlib-java+breeze (sorry for weird table formatting):
>>>>
>>>> |A*B  size | BIDMat MKL | Breeze+Netlib-java
>>>> |native_system_linux_x86-64|
>>>> Breeze+Netlib-java f2jblas |
>>>> +-----------------------------------------------------------------------+
>>>> |100x100*100x100 | 0,00205596 | 0,03810324 | 0,002556 |
>>>> |1000x1000*1000x1000 | 0,018320947 | 0,51803557 |1,638475459 |
>>>> |10000x10000*10000x10000 | 23,78046632 | 445,0935211 | 1569,233228
>>>> ||
>>>>
>>>> Configuration: Intel(R) Xeon(R) CPU E31240 3.3 GHz, 6GB RAM, Fedora
>>>> 19 Linux, Scala 2.11.
>>>>
>>>> Later I will make tests with Cuda. I need to install new Cuda
>>>> version for this purpose.
>>>>
>>>> Do you have any ideas why breeze-netlib with native blas is so much
>>>> slower than BIDMat MKL?
>>>>
>>>> Best regards, Alexander
>>>>
>>>> From: Joseph Bradley [mailto:joseph@databricks.com<mailto:joseph@databricks.com><mailto:joseph@databricks.com<mailto:joseph@databricks.com>><mailto:
>>>> joseph@databricks.com<mailto:joseph@databricks.com><mailto:joseph@databricks.com<mailto:joseph@databricks.com>>>]
>>>> Sent: Thursday, February 05, 2015 5:29 PM
>>>> To: Ulanov, Alexander
>>>> Cc: Evan R. Sparks;
>>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.apache.org<mailto:dev@spark.apache.org>><mailto:dev@spark<mailto:dev@spark>.
>>>> apache.org<http://apache.org><mailto:dev@spark.apache.org<mailto:dev@spark.apache.org>>>
>>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>>>
>>>> Hi Alexander,
>>>>
>>>> Using GPUs with Spark would be very exciting.  Small comment:
>>>> Concerning your question earlier about keeping data stored on the
>>>> GPU rather than having to move it between main memory and GPU
>>>> memory on each iteration, I would guess this would be critical to
>>>> getting good performance.  If you could do multiple local
>>>> iterations before aggregating results, then the cost of data
>>>> movement to the GPU could be amortized (and I believe that is done
>>>> in practice).  Having Spark be aware of the GPU and using it as another part of memory sounds like a much bigger undertaking.
>>>>
>>>> Joseph
>>>>
>>>> On Thu, Feb 5, 2015 at 4:59 PM, Ulanov, Alexander <
>>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>>> wrote:
>>>> Thank you for explanation! I‚Äôve watched the BIDMach presentation by
>>>> John Canny and I am really inspired by his talk and comparisons with Spark MLlib.
>>>>
>>>> I am very interested to find out what will be better within Spark:
>>>> BIDMat or netlib-java with CPU or GPU natives. Could you suggest a
>>>> fair way to benchmark them? Currently I do benchmarks on artificial
>>>> neural networks in batch mode. While it is not a ‚Äúpure‚Äù test of
>>>> linear algebra, it involves some other things that are essential to machine learning.
>>>>
>>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>><mailto:
>>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>>]
>>>> Sent: Thursday, February 05, 2015 1:29 PM
>>>> To: Ulanov, Alexander
>>>> Cc:
>>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.apache.org<mailto:dev@spark.apache.org>><mailto:dev@spark<mailto:dev@spark>.
>>>> apache.org<http://apache.org><mailto:dev@spark.apache.org<mailto:dev@spark.apache.org>>>
>>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>>>
>>>> I'd be surprised of BIDMat+OpenBLAS was significantly faster than
>>>> netlib-java+OpenBLAS, but if it is much faster it's probably due to
>>>> netlib-java+data
>>>> layout and fewer levels of indirection - it's definitely a
>>>> worthwhile experiment to run. The main speedups I've seen from
>>>> using it come from highly optimized GPU code for linear algebra. I
>>>> know that in the past Canny has gone as far as to write custom GPU
>>>> kernels for performance-critical regions of code.[1]
>>>>
>>>> BIDMach is highly optimized for single node performance or
>>>> performance on small clusters.[2] Once data doesn't fit easily in
>>>> GPU memory (or can be batched in that way) the performance tends to
>>>> fall off. Canny argues for hardware/software codesign and as such
>>>> prefers machine configurations that are quite different than what
>>>> we find in most commodity cluster nodes - e.g. 10 disk cahnnels and 4 GPUs.
>>>>
>>>> In contrast, MLlib was designed for horizontal scalability on
>>>> commodity clusters and works best on very big datasets - order of terabytes.
>>>>
>>>> For the most part, these projects developed concurrently to address
>>>> slightly different use cases. That said, there may be bits of
>>>> BIDMach we could repurpose for MLlib - keep in mind we need to be
>>>> careful about maintaining cross-language compatibility for our Java
>>>> and Python-users, though.
>>>>
>>>> - Evan
>>>>
>>>> [1] - http://arxiv.org/abs/1409.5402 [2] -
>>>> http://eecs.berkeley.edu/~hzhao/papers/BD.pdf<http://eecs.berkeley.edu/%7Ehzhao/papers/BD.pdf>
>>>>
>>>> On Thu, Feb 5, 2015 at 1:00 PM, Ulanov, Alexander <
>>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>><mailto:
>>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>>>> wrote:
>>>> Hi Evan,
>>>>
>>>> Thank you for suggestion! BIDMat seems to have terrific speed. Do
>>>> you know what makes them faster than netlib-java?
>>>>
>>>> The same group has BIDMach library that implements machine
>>>> learning. For some examples they use Caffe convolutional neural
>>>> network library owned by another group in Berkeley. Could you
>>>> elaborate on how these all might be connected with Spark Mllib? If
>>>> you take BIDMat for linear algebra why don‚Äôt you take BIDMach for optimization and learning?
>>>>
>>>> Best regards, Alexander
>>>>
>>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>><mailto:
>>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>><mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>><mailto:
>>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>>>]
>>>> Sent: Thursday, February 05, 2015 12:09 PM
>>>> To: Ulanov, Alexander
>>>> Cc: dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.apache.org<mailto:dev@spark.apache.org>><mailto:dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.apache.org<mailto:dev@spark.apache.org>>><mailto:
>>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.apache.org<mailto:dev@spark.apache.org>><mailto:dev@spark<mailto:dev@spark>.
>>>> apache.org<http://apache.org><mailto:dev@spark.apache.org<mailto:dev@spark.apache.org>>>>
>>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>>>
>>>> I'd expect that we can make GPU-accelerated BLAS faster than CPU
>>>> blas in many cases.
>>>>
>>>> You might consider taking a look at the codepaths that BIDMat (
>>>> https://github.com/BIDData/BIDMat) takes and comparing them to
>>>> netlib-java/breeze. John Canny et. al. have done a bunch of work
>>>> optimizing to make this work really fast from Scala. I've run it on
>>>> my laptop and compared to MKL and in certain cases it's 10x faster at matrix multiply.
>>>> There are a lot of layers of indirection here and you really want
>>>> to avoid data copying as much as possible.
>>>>
>>>> We could also consider swapping out BIDMat for Breeze, but that
>>>> would be a big project and if we can figure out how to get
>>>> breeze+cublas to comparable performance that would be a big win.
>>>>
>>>> On Thu, Feb 5, 2015 at 11:55 AM, Ulanov, Alexander <
>>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>><mailto:
>>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>>>> wrote:
>>>> Dear Spark developers,
>>>>
>>>> I am exploring how to make linear algebra operations faster within Spark.
>>>> One way of doing this is to use Scala Breeze library that is
>>>> bundled with Spark. For matrix operations, it employs Netlib-java
>>>> that has a Java wrapper for BLAS (basic linear algebra subprograms)
>>>> and LAPACK native binaries if they are available on the worker
>>>> node. It also has its own optimized Java implementation of BLAS. It
>>>> is worth mentioning, that native binaries provide better performance only for BLAS level 3, i.e.
>>>> matrix-matrix operations or general matrix multiplication (GEMM).
>>>> This is confirmed by GEMM test on Netlib-java page
>>>> https://github.com/fommil/netlib-java. I also confirmed it with my
>>>> experiments with training of artificial neural network
>>>> https://github.com/apache/spark/pull/1290#issuecomment-70313952.
>>>> However, I would like to boost performance more.
>>>>
>>>> GPU is supposed to work fast with linear algebra and there is
>>>> Nvidia CUDA implementation of BLAS, called cublas. I have one Linux
>>>> server with Nvidia GPU and I was able to do the following. I linked
>>>> cublas (instead of cpu-based blas) with Netlib-java wrapper and put
>>>> it into Spark, so Breeze/Netlib is using it. Then I did some
>>>> performance measurements with regards to artificial neural network
>>>> batch learning in Spark MLlib that involves matrix-matrix
>>>> multiplications. It turns out that for matrices of size less than
>>>> ~1000x780 GPU cublas has the same speed as CPU blas. Cublas becomes
>>>> slower for bigger matrices. It worth mentioning that it is was not a test for ONLY multiplication since there are other operations involved.
>>>> One of the reasons for slowdown might be the overhead of copying
>>>> the matrices from computer memory to graphic card memory and back.
>>>>
>>>> So, few questions:
>>>> 1) Do these results with CUDA make sense?
>>>> 2) If the problem is with copy overhead, are there any libraries
>>>> that allow to force intermediate results to stay in graphic card
>>>> memory thus removing the overhead?
>>>> 3) Any other options to speed-up linear algebra in Spark?
>>>>
>>>> Thank you, Alexander
>>>>
>>>> -------------------------------------------------------------------
>>>> -- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org><mailto:dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org>><mailto:
>>>> dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org><mailto:dev-unsubscribe@spark.apach<mailto:dev-unsubscribe@spark.apach>
>>>> e.org<http://e.org>>><mailto:dev-unsubscribe@spark.apac<mailto:dev-unsubscribe@spark.apac><mailto:dev-unsubscribe@sp<mailto:dev-unsubscribe@sp>
>>>> ark.apac> he.org<http://he.org><http://he.org>
>>>> <mailto:dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org><mailto:dev-unsubscribe@spa<mailto:dev-unsubscribe@spa>
>>>> rk.apache.org<http://rk.apache.org>>>> For additional commands, e-mail:
>>>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org><mailto:dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>><mailto:
>>>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org><mailto:dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>>><mailto:dev-help@spark.apache.org<mailto:dev-help@spark.apache.org><mailto:dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>><mailto:
>>>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org><mailto:dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>>>>
>>>>
>>>>
>>>>
>>>>
>>>

--
Best regards,
Sam


"
Boric Tan <it.news.trends@gmail.com>,"Wed, 20 Jan 2016 21:55:17 -0800",Re: How Spark utilize low-level architecture features?,dev@spark.apache.org,"Anyone could shed some light on this?

Thanks,
Boric


"
Stephen Boesch <javadba@gmail.com>,"Wed, 20 Jan 2016 23:51:47 -0800",Re: spark task scheduling delay,Renu Yadav <yrenu21@gmail.com>,"Which Resource Manager  are you using?

2016-01-20 21:38 GMT-08:00 Renu Yadav <yrenu21@gmail.com>:

"
Justin Uang <justin.uang@gmail.com>,"Thu, 21 Jan 2016 11:22:27 +0000",Spark SQL: Avoid shuffles when data is already partitioned on disk,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

If I had a df and I wrote it out via partitionBy(""id""), presumably, when I
load in the df and do a groupBy(""id""), a shuffle shouldn't be necessary
right? Effectively, we can load in the dataframe with a hash partitioner
already set, since each task can simply read all the folders where
id=<value> where hash(<value>) % reducer_count == reducer_id. Is this an
optimization that is on the radar? This will be a huge boon in terms of
reducing the number of shuffles necessary if we're always joining on the
same columns.

Best,

Justin
"
"""Kazuaki Ishizaki"" <ISHIZAKI@jp.ibm.com>","Thu, 21 Jan 2016 11:34:14 +0000",RE: Using CUDA within Spark / boosting linear algebra,"""dev@spark.apache.org"" <dev@spark.apache.org>,
        ""Ulanov, Alexander""
 <alexander.ulanov@hpe.com>,
        ""Joseph Bradley"" <joseph@databricks.com>","Dear all,

another part of memory sounds like a much bigger undertaking.

As Joseph pointed out before, there are two potential issues to 
efficiently exploit GPUs in Spark.
(1) the cost of data movement between CPU and GPU
(2) the cost of encoding/decoding between current row-format and 
GPU-friendly column format

Our prototype http://kiszk.github.io/spark-gpu/ addresses these two issues 
by supporting data partition caching in GPU device memory and by providing 
binary column storage for data partition. We really appreciate it if you 
would give us comments, suggestions, or feedback.

Best Regards
Kazuaki Ishizaki



From:   ""Ulanov, Alexander"" <alexander.ulanov@hpe.com>
To:     Sam Halliday <sam.halliday@gmail.com>, John Canny 
<canny@berkeley.edu>
Cc:     Xiangrui Meng <mengxr@gmail.com>, ""dev@spark.apache.org"" 
<dev@spark.apache.org>, Joseph Bradley <joseph@databricks.com>, ""Evan R. 
Sparks"" <evan.sparks@gmail.com>
Date:   2016/01/21 11:07
Subject:        RE: Using CUDA within Spark / boosting linear algebra



Hi Everyone,
 
I$B!G(Bve updated the benchmark and done experiments with new hardware with 2x 
Nvidia Tesla K80 (physically 4x Tesla K40) and 2x modern Haswell CPU Intel 
E5-2650 v3 @ 2.30GHz.
 
This time I computed average and median of 10 runs for each of experiment 
and approximated FLOPS.
 
Results are available at google docs (old experiments are in the other 2 
sheets):
https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J5r7kwKSPkY/edit?usp=sharing
Benchmark code:
https://github.com/avulanov/scala-blas
 
Best regards, Alexander
 
 
From: Sam Halliday [mailto:sam.halliday@gmail.com] 
Sent: Thursday, March 26, 2015 9:27 AM
To: John Canny
Cc: Xiangrui Meng; dev@spark.apache.org; Joseph Bradley; Evan R. Sparks; 
Ulanov, Alexander
Subject: Re: Using CUDA within Spark / boosting linear algebra
 
John, I have to disagree with you there. Dense matrices come up a lot in 
industry,  although your personal experience may be different. 
I mentioned this earlier in the thread, but I'll put it out again. Dense 
BLAS are not very important for most machine learning workloads: at least 
for non-image workloads in industry (and for image processing you would 
probably want a deep learning/SGD solution with convolution kernels). e.g. 
it was only relevant for 1/7 of our recent benchmarks, which should be a 
reasonable sample. What really matters is sparse BLAS performance. BIDMat 
is still an order of magnitude faster there. Those kernels are only in 
BIDMat, since NVIDIAs sparse BLAS dont perform well on power-law data. 

Its also the case that the overall performance of an algorithm is 
determined by the slowest kernel, not the fastest. If the goal is to get 
closer to BIDMach's performance on typical problems, you need to make sure 
that every kernel goes at comparable speed. So the real question is how 
much faster MLLib routines do on a complete problem with/without GPU 
acceleration. For BIDMach, its close to a factor of 10. But that required 
running entirely on the GPU, and making sure every kernel is close to its 
limit.

-John

If you think nvblas would be helpful, you should try it in some end-to-end 
benchmarks. 
Yeah, much more reasonable - nice to know that we can get full GPU 
performance from breeze/netlib-java - meaning there's no compelling 
performance reason to switch out our current linear algebra library (at 
least as far as this benchmark is concerned). 
 
Instead, it looks like a user guide for configuring Spark/MLlib to use the 
right BLAS library will get us most of the way there. Or, would it make 
sense to finally ship openblas compiled for some common platforms (64-bit 
linux, windows, mac) directly with Spark - hopefully eliminating the jblas 
warnings once and for all for most users? (Licensing is BSD) Or am I 
missing something?
 
As everyone suggested, the results were too good to be true, so I 
double-checked them. It turns that nvblas did not do multiplication due to 
parameter NVBLAS_TILE_DIM from ""nvblas.conf"" and returned zero matrix. My 
previously posted results with nvblas are matrices copying only. The 
default NVBLAS_TILE_DIM==2048 is too big for my graphic card/matrix size. 
I handpicked other values that worked. As a result, netlib+nvblas is on 
par with BIDMat-cuda. As promised, I am going to post a how-to for nvblas 
configuration.

https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J5r7kwKSPkY/edit?usp=sharing





Hi again,

I finally managed to use nvblas within Spark+netlib-java. It has 
exceptional performance for big matrices with Double, faster than 
BIDMat-cuda with Float. But for smaller matrices, if you will copy them 
to/from GPU, OpenBlas or MKL might be a better choice. This correlates 
with original nvblas presentation on GPU conf 2013 (slide 21): 
http://on-demand.gputechconf.com/supercomputing/2013/presentation/SC3108-New-Features-CUDA%206%20-GPU-Acceleration.pdf


My results:
https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J5r7kwKSPkY/edit?usp=sharing


Just in case, these tests are not for generalization of performance of 
different libraries. I just want to pick a library that does at best dense 
matrices multiplication for my task.

P.S. My previous issue with nvblas was the following: it has Fortran blas 
functions, at the same time netlib-java uses C cblas functions. So, one 
needs cblas shared library to use nvblas through netlib-java. Fedora does 
not have cblas (but Debian and Ubuntu have), so I needed to compile it. I 
could not use cblas from Atlas or Openblas because they link to their 
implementation and not to Fortran blas.

Best regards, Alexander

Hi,

I am trying to use nvblas with netlib-java from Spark. nvblas functions 
should replace current blas functions calls after executing LD_PRELOAD as 
suggested in http://docs.nvidia.com/cuda/nvblas/#Usage without any changes 
to netlib-java. It seems to work for simple Java example, but I cannot 
make it work with Spark. I run the following:
export LD_LIBRARY_PATH=/usr/local/cuda-6.5/lib64
env LD_PRELOAD=/usr/local/cuda-6.5/lib64/libnvblas.so ./spark-shell 
--driver-memory 4G In nvidia-smi I observe that Java is to use GPU:
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU 
Memory |
|  GPU       PID  Type  Process name                               Usage   
|
|=============================================================================|
|    0      8873    C   bash 39MiB |
|    0      8910    C   /usr/lib/jvm/java-1.7.0/bin/java 39MiB |
+-----------------------------------------------------------------------------+

In Spark shell I do matrix multiplication and see the following:
15/03/25 06:48:01 INFO JniLoader: successfully loaded 
/tmp/jniloader8192964377009965483netlib-native_system-linux-x86_64.so
So I am sure that netlib-native is loaded and cblas supposedly used. 
However, matrix multiplication does executes on CPU since I see 16% of CPU 
used and 0% of GPU used. I also checked different matrix sizes, from 
100x100 to 12000x12000

Could you suggest might the LD_PRELOAD not affect Spark shell?

Best regards, Alexander



From: Sam Halliday [mailto:sam.halliday@gmail.com]
Sent: Monday, March 09, 2015 6:01 PM
To: Ulanov, Alexander
Cc: dev@spark.apache.org; Xiangrui Meng; Joseph Bradley; Evan R. Sparks
Subject: RE: Using CUDA within Spark / boosting linear algebra


Thanks so much for following up on this!

Hmm, I wonder if we should have a concerted effort to chart performance on 
various pieces of hardware...
Hi Everyone, I've updated the benchmark as Xiangrui suggested. Added the 
comment that BIDMat 0.9.7 uses Float matrices in GPU (although I see the 
support of Double in the current source code), did the test with BIDMat 
and CPU Double matrices. BIDMat MKL is indeed on par with netlib MKL.

https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J5r7kwKSPkY/edit?usp=sharing


Best regards, Alexander

Subject: Re: Using CUDA within Spark / boosting linear algebra

BTW, is anybody on this list going to the London Meetup in a few weeks?

https://skillsmatter.com/meetups/6987-apache-spark-living-the-post-mapreduce-world#community


Would be nice to meet other people working on the guts of Spark! :-)


Xiangrui Meng <mengxr@gmail.com<mailto:mengxr@gmail.com>> writes:

matrices.
repo=
+-----------------------------------------------------------------------+
 with
to be on par with JNI overheads.
evan.sparks@gmail.com><mailto:

netlib-java.
how to force use a specific blas (not specific wrapper for blas).
evan.sparks@gmail.com><mailto:
it).
+-----------------------------------------------------------------------+
joseph@databricks.com><mailto:
another part of memory sounds like a much bigger undertaking.
ation by
Spark MLlib.
!I(B test of
machine learning.
evan.sparks@gmail.com><mailto:

GPUs.
terabytes.
alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:
ch for 
optimization and learning?
evan.sparks@gmail.com><mailto:
evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:
dev@spark.apache.org<mailto:dev@spark.apache.org>><mailto:

matrix multiply.
alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:
Spark.
only for BLAS level 3, i.e.
test for ONLY multiplication since there are other operations involved.
dev-unsubscribe@spark.apache.org><mailto:
dev-help@spark.apache.org<mailto:dev-help@spark.apache.org><mailto:

--
Best regards,
Sam
 
 


"
"""Allen Zhang"" <allenzhang010@126.com>","Thu, 21 Jan 2016 20:03:21 +0800 (CST)",RE: Using CUDA within Spark / boosting linear algebra,"""Kazuaki Ishizaki"" <ISHIZAKI@jp.ibm.com>","

Hi Kazuaki,


Jcuda is actually a wrapper of the **pure** CUDA, as your wiki page shows that 3.15x performance boost of logistic regression seems slower than BIDMat-cublas or pure CUDA.
Could you elaborate on why you chose Jcuda other then JNI to call CUDA directly?


Regards,
Allen Zhang








At 2016-01-21 19:34:14, ""Kazuaki Ishizaki"" <ISHIZAKI@jp.ibm.com> wrote:
Dear all,

>>>> Hi Alexander,
>>>>
>>>> Using GPUs with Spark would be very exciting.  Small comment:
>>>> Concerning your question earlier about keeping data stored on the
>>>> GPU rather than having to move it between main memory and GPU
>>>> memory on each iteration, I would guess this would be critical to
>>>> getting good performance.  If you could do multiple local
>>>> iterations before aggregating results, then the cost of data
>>>> movement to the GPU could be amortized (and I believe that is done
>>>> in practice).  Having Spark be aware of the GPU and using it as another part of memory sounds like a much bigger undertaking.
>>>>
>>>> Joseph

As Joseph pointed out before, there are two potential issues to efficiently exploit GPUs in Spark.
(1) the cost of data movement between CPU and GPU
(2) the cost of encoding/decoding between current row-format and GPU-friendly column format

Our prototype http://kiszk.github.io/spark-gpu/addresses these two issues by supporting data partition caching in GPU device memory and by providing binary column storage for data partition. We really appreciate it if you would give us comments, suggestions, or feedback.

Best Regards
Kazuaki Ishizaki



From:        ""Ulanov, Alexander"" <alexander.ulanov@hpe.com>
To:        Sam Halliday <sam.halliday@gmail.com>, John Canny <canny@berkeley.edu>
Cc:        Xiangrui Meng <mengxr@gmail.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>, Joseph Bradley <joseph@databricks.com>, ""Evan R. Sparks"" <evan.sparks@gmail.com>
Date:        2016/01/21 11:07
Subject:        RE: Using CUDA within Spark / boosting linear algebra




Hi Everyone,
 
I°Øve updated the benchmark and done experiments with new hardware with 2x Nvidia Tesla K80 (physically 4x Tesla K40) and 2x modern Haswell CPU Intel E5-2650 v3 @ 2.30GHz.
 
This time I computed average and median of 10 runs for each of experiment and approximated FLOPS.
 
Results are available at google docs (old experiments are in the other 2 sheets):
https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J5r7kwKSPkY/edit?usp=sharing
Benchmark code:
https://github.com/avulanov/scala-blas
 
Best regards, Alexander
 
 
From: Sa, March 26, 2015 9:27 AM
To: John Canny
Cc: Xiangrui Meng; dev@spark.apache.org; Joseph Bradley; Evan R. Sparks; Ulanov, Alexander
Subject: Re: Using CUDA within Spark / boosting linear algebra
 

John, I have to disagree with you there. Dense matrices come up a lot in industry,  although your personal experience may be different.
On 26 Mar 2015 16:20, ""John Canny"" <canny@berkeley.edu> wrote:
I mentioned this earlier in the thread, but I'll put it out again. Dense BLAS are not very important for most machine learning workloads: at least for non-image workloads in industry (and for image processing you would probably want a deep learning/SGD solution with convolution kernels). e.g. it was only relevant for 1/7 of our recent benchmarks, which should be a reasonable sample. What really matters is sparse BLAS performance. BIDMat is still an order of magnitude faster there. Those kernels are only in BIDMat, since NVIDIAs sparse BLAS dont perform well on power-law data.

Its also the case that the overall performance of an algorithm is determined by the slowest kernel, not the fastest. If the goal is to get closer to BIDMach's performance on typical problems, you need to make sure that every kernel goes at comparable speed. So the real question is how much faster MLLib routines do on a complete problem with/without GPU acceleration. For BIDMach, its close to a factor of 10. But that required running entirely on the GPU, and making sure every kernel is close to its limit.

-John

If you think nvblas would be helpful, you should try it in some end-to-end benchmarks.
On 3/25/15, 6:23 PM, Evan R. Sparks wrote:
Yeah, much more reasonable - nice to know that we can get full GPU performance from breeze/netlib-java - meaning there's no compelling performance reason to switch out our current linear algebra library (at least as far as this benchmark is concerned).
 
Instead, it looks like a user guide for configuring Spark/MLlib to use the right BLAS library will get us most of the way there. Or, would it make sense to finally ship openblas compiled for some common platforms (64-bit linux, windows, mac) directly with Spark - hopefully eliminating the jblas warnings once and for all for most users? (Licensing is BSD) Or am I missing something?
 
On Wed, Mar 25, 2015 at 6:03 PM, Ulanov, Alexander <alexander.ulanov@hp.com> wrote:
As everyone suggested, the results were too good to be true, so I double-checked them. It turns that nvblas did not do multiplication due to parameter NVBLAS_TILE_DIM from ""nvblas.conf"" and returned zero matrix. My previously posted results with nvblas are matrices copying only. The default NVBLAS_TILE_DIM==2048 is too big for my graphic card/matrix size. I handpicked other values that worked. As a result, netlib+nvblas is on par with BIDMat-cuda. As promised, I am going to post a how-to for nvblas configuration.

https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J5r7kwKSPkY/edit?usp=sharing



-----Original Message-----
From: Ulanov, Alexander
Sent: Wednesday, March 25, 2015 2:31 PM
To: Sam Halliday
Cc: dev@spark.apache.org; Xiangrui Meng; Joseph Bradley; Evan R. Sparks; jfcanny
Subject: RE: Using CUDA within Spark / boosting linear algebra

Hi again,

I finally managed to use nvblas within Spark+netlib-java. It has exceptional performance for big matrices with Double, faster than BIDMat-cuda with Float. But for smaller matrices, if you will copy them to/from GPU, OpenBlas or MKL might be a better choice. This correlates with original nvblas presentation on GPU conf 2013 (slide 21): http://on-demand.gputechconf.com/supercomputing/2013/presentation/SC3108-New-Features-CUDA%206%20-GPU-Acceleration.pdf

My results:
https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J5r7kwKSPkY/edit?usp=sharing

Just in case, these tests are not for generalization of performance of different libraries. I just want to pick a library that does at best dense matrices multiplication for my task.

P.S. My previous issue with nvblas was the following: it has Fortran blas functions, at the same time netlib-java uses C cblas functions. So, one needs cblas shared library to use nvblas through netlib-java. Fedora does not have cblas (but Debian and Ubuntu have), so I needed to compile it. I could not use cblas from Atlas or Openblas because they link to their implementation and not to Fortran blas.

Best regards, Alexander

-----Original Message-----
From: Ulanov, Alexander
Sent: Tuesday, March 24, 2015 6:57 PM
To: Sam Halliday
Cc: dev@spark.apache.org; Xiangrui Meng; Joseph Bradley; Evan R. Sparks
Subject: RE: Using CUDA within Spark / boosting linear algebra

Hi,

I am trying to use nvblas with netlib-java from Spark. nvblas functions should replace current blas functions calls after executing LD_PRELOAD as suggested in http://docs.nvidia.com/cuda/nvblas/#Usagewithout any changes to netlib-java. It seems to work for simple Java example, but I cannot make it work with Spark. I run the following:
export LD_LIBRARY_PATH=/usr/local/cuda-6.5/lib64
env LD_PRELOAD=/usr/local/cuda-6.5/lib64/libnvblas.so ./spark-shell --driver-memory 4G In nvidia-smi I observe that Java is to use GPU:
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0      8873    C   bash                                            39MiB |
|    0      8910    C   /usr/lib/jvm/java-1.7.0/bin/java                39MiB |
+-----------------------------------------------------------------------------+

In Spark shell I do matrix multiplication and see the following:
15/03/25 06:48:01 INFO JniLoader: successfully loaded /tmp/jniloader8192964377009965483netlib-native_system-linux-x86_64.so
So I am sure that netlib-native is loaded and cblas supposedly used. However, matrix multiplication does executes on CPU since I see 16% of CPU used and 0% of GPU used. I also checked different matrix sizes, from 100x100 to 12000x12000

Could you suggest might the LD_PRELOAD not affect Spark shell?

Best regards, Alexander



From: Sam Halliday [mailto:sam.halliday@gmail.com]
Sent: Monday, March 09, 2015 6:01 PM
To: Ulanov, Alexander
Cc: dev@spark.apache.org; Xiangrui Meng; Joseph Bradley; Evan R. Sparks
Subject: RE: Using CUDA within Spark / boosting linear algebra


Thanks so much for following up on this!

Hmm, I wonder if we should have a concerted effort to chart performance on various pieces of hardware...
On 9 Mar 2015 21:08, ""Ulanov, Alexander"" <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
Hi Everyone, I've updated the benchmark as Xiangrui suggested. Added the comment that BIDMat 0.9.7 uses Float matrices in GPU (although I see the support of Double in the current source code), did the test with BIDMat and CPU Double matrices. BIDMat MKL is indeed on par with netlib MKL.

https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J5r7kwKSPkY/edit?usp=sharing

Best regards, Alexander

-----Original Message-----
From: Sam Halliday [mai]
Sent: Tuesday, March 03, 2015 1:54 PM
To: Xiangrui Meng; Joseph Bradley
Cc: Evan R. Sparks; Ulanov, Alexander; dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: Using CUDA within Spark / boosting linear algebra

BTW, is anybody on this list going to the London Meetup in a few weeks?

https://skillsmatter.com/meetups/6987-apache-spark-living-the-post-mapreduce-world#community

Would be nice to meet other people working on the guts of Spark! :-)


Xiangrui Meng <mengxr@gmail.com<mailto:mengxr@gmail.com>> writes:

> Hey Alexander,
>
> I don't quite understand the part where netlib-cublas is about 20x
> slower than netlib-openblas. What is the overhead of using a GPU BLAS
> with netlib-java?
>
> CC'ed Sam, the author of netlib-java.
>
> Best,
> Xiangrui
>
> On Wed, Feb 25, 2015 at 3:36 PM, Joseph Bradley <joseph@databricks.com<mailto:joseph@databricks.com>> wrote:
>> Better documentation for linking would be very helpful!  Here's a JIRA:
>> https://issues.apache.org/jira/browse/SPARK-6019
>>
>>
>> On Wed, Feb 25, 2015 at 2:53 PM, Evan R. Sparks
>> <evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>
>> wrote:
>>
>>> Thanks for compiling all the data and running these benchmarks,
>>> Alex. The big takeaways here can be seen with this chart:
>>>
>>> https://docs.google.com/spreadsheets/d/1aRm2IADRfXQV7G2vrcVh4StF50uZ
>>> Hl6kmAJeaZZggr0/pubchart?oid=1899767119&format=interactive
>>>
>>> 1) A properly configured GPU matrix multiply implementation (e.g.
>>> BIDMat+GPU) can provide substantial (but less than an order of
>>> BIDMat+magnitude)
>>> benefit over a well-tuned CPU implementation (e.g. BIDMat+MKL or
>>> netlib-java+openblas-compiled).
>>> 2) A poorly tuned CPU implementation can be 1-2 orders of magnitude
>>> worse than a well-tuned CPU implementation, particularly for larger matrices.
>>> (netlib-f2jblas or netlib-ref) This is not to pick on netlib - this
>>> basically agrees with the authors own benchmarks (
>>> https://github.com/fommil/netlib-java)
>>>
>>> I think that most of our users are in a situation where using GPUs
>>> may not be practical - although we could consider having a good GPU
>>> backend available as an option. However, *ALL* users of MLlib could
>>> benefit (potentially tremendously) from using a well-tuned CPU-based
>>> BLAS implementation. Perhaps we should consider updating the mllib
>>> guide with a more complete section for enabling high performance
>>> binaries on OSX and Linux? Or better, figure out a way for the
>>> system to fetch these automatically.
>>>
>>> - Evan
>>>
>>>
>>>
>>> On Thu, Feb 12, 2015 at 4:18 PM, Ulanov, Alexander <
>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
>>>
>>>> Just to summarize this thread, I was finally able to make all
>>>> performance comparisons that we discussed. It turns out that:
>>>> BIDMat-cublas>>BIDMat
>>>> MKL==netlib-mkl==netlib-openblas-compiled>netlib-openblas-yum-repo=
>>>> =netlib-cublas>netlib-blas>f2jblas
>>>>
>>>> Below is the link to the spreadsheet with full results.
>>>>
>>>> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx
>>>> 378T9J5r7kwKSPkY/edit?usp=sharing
>>>>
>>>> One thing still needs exploration: does BIDMat-cublas perform
>>>> copying to/from machine°Øs RAM?
>>>>
>>>> -----Original Message-----
>>>> From: Ulanov, Alexander
>>>> Sent: Tuesday, February 10, 2015 2:12 PM
>>>> To: Evan R. Sparks
>>>> Cc: Joseph Bradley;
>>>> dev@spark.apache.org<mailto:dev@spark.apache.org>
>>>> Subject: RE: Using CUDA within Spark / boosting linear algebra
>>>>
>>>> Thanks, Evan! It seems that ticket was marked as duplicate though
>>>> the original one discusses slightly different topic. I was able to
>>>> link netlib with MKL from BIDMat binaries. Indeed, MKL is
>>>> statically linked inside a 60MB library.
>>>>
>>>> |A*B  size | BIDMat MKL | Breeze+Netlib-MKL  from BIDMat|
>>>> Breeze+Netlib-OpenBlas(native system)| Breeze+Netlib-f2jblas |
>>>> +-----------------------------------------------------------------------+
>>>> |100x100*100x100 | 0,00205596 | 0,000381 | 0,03810324 | 0,002556 |
>>>> |1000x1000*1000x1000 | 0,018320947 | 0,038316857 | 0,51803557
>>>> |1,638475459 |
>>>> |10000x10000*10000x10000 | 23,78046632 | 32,94546697 |445,0935211 |
>>>> 1569,233228 |
>>>>
>>>> It turn out that pre-compiled MKL is faster than precompiled
>>>> OpenBlas on my machine. Probably, I°Øll add two more columns with
>>>> locally compiled openblas and cuda.
>>>>
>>>> Alexander
>>>>
>>>> From: Evan R. Sparks
>>>> [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>]
>>>> Sent: Monday, February 09, 2015 6:06 PM
>>>> To: Ulanov, Alexander
>>>> Cc: Joseph Bradley;
>>>> dev@spark.apache.org<mailto:dev@spark.apache.org>
>>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>>>
>>>> Great - perhaps we can move this discussion off-list and onto a
>>>> JIRA ticket? (Here's one:
>>>> https://issues.apache.org/jira/browse/SPARK-5705)
>>>>
>>>> It seems like this is going to be somewhat exploratory for a while
>>>> (and there's probably only a handful of us who really care about
>>>> fast linear
>>>> algebra!)
>>>>
>>>> - Evan
>>>>
>>>> On Mon, Feb 9, 2015 at 4:48 PM, Ulanov, Alexander <
>>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
>>>> Hi Evan,
>>>>
>>>> Thank you for explanation and useful link. I am going to build
>>>> OpenBLAS, link it with Netlib-java and perform benchmark again.
>>>>
>>>> Do I understand correctly that BIDMat binaries contain statically
>>>> linked Intel MKL BLAS? It might be the reason why I am able to run
>>>> BIDMat not having MKL BLAS installed on my server. If it is true, I
>>>> wonder if it is OK because Intel sells this library. Nevertheless,
>>>> it seems that in my case precompiled MKL BLAS performs better than
>>>> precompiled OpenBLAS given that BIDMat and Netlib-java are supposed to be on par with JNI overheads.
>>>>
>>>> Though, it might be interesting to link Netlib-java with Intel MKL,
>>>> as you suggested. I wonder, are John Canny (BIDMat) and Sam
>>>> Halliday
>>>> (Netlib-java) interested to compare their libraries.
>>>>
>>>> Best regards, Alexander
>>>>
>>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:
>>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
>>>> Sent: Friday, February 06, 2015 5:58 PM
>>>>
>>>> To: Ulanov, Alexander
>>>> Cc: Joseph Bradley;
>>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
>>>> apache.org<mailto:dev@spark.apache.org>>
>>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>>>
>>>> I would build OpenBLAS yourself, since good BLAS performance comes
>>>> from getting cache sizes, etc. set up correctly for your particular
>>>> hardware - this is often a very tricky process (see, e.g. ATLAS),
>>>> but we found that on relatively modern Xeon chips, OpenBLAS builds
>>>> quickly and yields performance competitive with MKL.
>>>>
>>>> To make sure the right library is getting used, you have to make
>>>> sure it's first on the search path - export
>>>> LD_LIBRARY_PATH=/path/to/blas/library.so will do the trick here.
>>>>
>>>> For some examples of getting netlib-java setup on an ec2 node and
>>>> some example benchmarking code we ran a while back, see:
>>>> https://github.com/shivaram/matrix-bench
>>>>
>>>> In particular - build-openblas-ec2.sh shows you how to build the
>>>> library and set up symlinks correctly, and scala/run-netlib.sh
>>>> shows you how to get the path setup and get that library picked up by netlib-java.
>>>>
>>>> In this way - you could probably get cuBLAS set up to be used by
>>>> netlib-java as well.
>>>>
>>>> - Evan
>>>>
>>>> On Fri, Feb 6, 2015 at 5:43 PM, Ulanov, Alexander <
>>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
>>>> Evan, could you elaborate on how to force BIDMat and netlib-java to
>>>> force loading the right blas? For netlib, I there are few JVM
>>>> flags, such as
>>>> -Dcom.github.fommil.netlib.BLAS=com.github.fommil.netlib.F2jBLAS,
>>>> so I can force it to use Java implementation. Not sure I understand how to force use a specific blas (not specific wrapper for blas).
>>>>
>>>> Btw. I have installed openblas (yum install openblas), so I suppose
>>>> that netlib is using it.
>>>>
>>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:
>>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
>>>> Sent: Friday, February 06, 2015 5:19 PM
>>>> To: Ulanov, Alexander
>>>> Cc: Joseph Bradley;
>>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
>>>> apache.org<mailto:dev@spark.apache.org>>
>>>>
>>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>>>
>>>> Getting breeze to pick up the right blas library is critical for
>>>> performance. I recommend using OpenBLAS (or MKL, if you already have it).
>>>> It might make sense to force BIDMat to use the same underlying BLAS
>>>> library as well.
>>>>
>>>> On Fri, Feb 6, 2015 at 4:42 PM, Ulanov, Alexander <
>>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
>>>> Hi Evan, Joseph
>>>>
>>>> I did few matrix multiplication test and BIDMat seems to be ~10x
>>>> faster than netlib-java+breeze (sorry for weird table formatting):
>>>>
>>>> |A*B  size | BIDMat MKL | Breeze+Netlib-java
>>>> |native_system_linux_x86-64|
>>>> Breeze+Netlib-java f2jblas |
>>>> +-----------------------------------------------------------------------+
>>>> |100x100*100x100 | 0,00205596 | 0,03810324 | 0,002556 |
>>>> |1000x1000*1000x1000 | 0,018320947 | 0,51803557 |1,638475459 |
>>>> |10000x10000*10000x10000 | 23,78046632 | 445,0935211 | 1569,233228
>>>> ||
>>>>
>>>> Configuration: Intel(R) Xeon(R) CPU E31240 3.3 GHz, 6GB RAM, Fedora
>>>> 19 Linux, Scala 2.11.
>>>>
>>>> Later I will make tests with Cuda. I need to install new Cuda
>>>> version for this purpose.
>>>>
>>>> Do you have any ideas why breeze-netlib with native blas is so much
>>>> slower than BIDMat MKL?
>>>>
>>>> Best regards, Alexander
>>>>
>>>> From: Joseph Bradley [mailto:joseph@databricks.com<mailto:joseph@databricks.com><mailto:
>>>> joseph@databricks.com<mailto:joseph@databricks.com>>]
>>>> Sent: Thursday, February 05, 2015 5:29 PM
>>>> To: Ulanov, Alexander
>>>> Cc: Evan R. Sparks;
>>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
>>>> apache.org<mailto:dev@spark.apache.org>>
>>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>>>
>>>> Hi Alexander,
>>>>
>>>> Using GPUs with Spark would be very exciting.  Small comment:
>>>> Concerning your question earlier about keeping data stored on the
>>>> GPU rather than having to move it between main memory and GPU
>>>> memory on each iteration, I would guess this would be critical to
>>>> getting good performance.  If you could do multiple local
>>>> iterations before aggregating results, then the cost of data
>>>> movement to the GPU could be amortized (and I believe that is done
>>>> in practice).  Having Spark be aware of the GPU and using it as another part of memory sounds like a much bigger undertaking.
>>>>
>>>> Joseph
>>>>
>>>> On Thu, Feb 5, 2015 at 4:59 PM, Ulanov, Alexander <
>>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
>>>> Thank you for explanation! I°Øve watched the BIDMach presentation by
>>>> John Canny and I am really inspired by his talk and comparisons with Spark MLlib.
>>>>
>>>> I am very interested to find out what will be better within Spark:
>>>> BIDMat or netlib-java with CPU or GPU natives. Could you suggest a
>>>> fair way to benchmark them? Currently I do benchmarks on artificial
>>>> neural networks in batch mode. While it is not a °∞pure°± test of
>>>> linear algebra, it involves some other things that are essential to machine learning.
>>>>
>>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:
>>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
>>>> Sent: Thursday, February 05, 2015 1:29 PM
>>>> To: Ulanov, Alexander
>>>> Cc:
>>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
>>>> apache.org<mailto:dev@spark.apache.org>>
>>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>>>
>>>> I'd be surprised of BIDMat+OpenBLAS was significantly faster than
>>>> netlib-java+OpenBLAS, but if it is much faster it's probably due to
>>>> netlib-java+data
>>>> layout and fewer levels of indirection - it's definitely a
>>>> worthwhile experiment to run. The main speedups I've seen from
>>>> using it come from highly optimized GPU code for linear algebra. I
>>>> know that in the past Canny has gone as far as to write custom GPU
>>>> kernels for performance-critical regions of code.[1]
>>>>
>>>> BIDMach is highly optimized for single node performance or
>>>> performance on small clusters.[2] Once data doesn't fit easily in
>>>> GPU memory (or can be batched in that way) the performance tends to
>>>> fall off. Canny argues for hardware/software codesign and as such
>>>> prefers machine configurations that are quite different than what
>>>> we find in most commodity cluster nodes - e.g. 10 disk cahnnels and 4 GPUs.
>>>>
>>>> In contrast, MLlib was designed for horizontal scalability on
>>>> commodity clusters and works best on very big datasets - order of terabytes.
>>>>
>>>> For the most part, these projects developed concurrently to address
>>>> slightly different use cases. That said, there may be bits of
>>>> BIDMach we could repurpose for MLlib - keep in mind we need to be
>>>> careful about maintaining cross-language compatibility for our Java
>>>> and Python-users, though.
>>>>
>>>> - Evan
>>>>
>>>> [1] - http://arxiv.org/abs/1409.5402[2] -
>>>> http://eecs.berkeley.edu/~hzhao/papers/BD.pdf
>>>>
>>>> On Thu, Feb 5, 2015 at 1:00 PM, Ulanov, Alexander <
>>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:
>>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>>> wrote:
>>>> Hi Evan,
>>>>
>>>> Thank you for suggestion! BIDMat seems to have terrific speed. Do
>>>> you know what makes them faster than netlib-java?
>>>>
>>>> The same group has BIDMach library that implements machine
>>>> learning. For some examples they use Caffe convolutional neural
>>>> network library owned by another group in Berkeley. Could you
>>>> elaborate on how these all might be connected with Spark Mllib? If
>>>> you take BIDMat for linear algebra why don°Øt you take BIDMach for optimization and learning?
>>>>
>>>> Best regards, Alexander
>>>>
>>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:
>>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>><mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:
>>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>>]
>>>> Sent: Thursday, February 05, 2015 12:09 PM
>>>> To: Ulanov, Alexander
>>>> Cc: dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.apache.org<mailto:dev@spark.apache.org>><mailto:
>>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
>>>> apache.org<mailto:dev@spark.apache.org>>>
>>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>>>
>>>> I'd expect that we can make GPU-accelerated BLAS faster than CPU
>>>> blas in many cases.
>>>>
>>>> You might consider taking a look at the codepaths that BIDMat (
>>>> https://github.com/BIDData/BIDMat) takes and comparing them to
>>>> netlib-java/breeze. John Canny et. al. have done a bunch of work
>>>> optimizing to make this work really fast from Scala. I've run it on
>>>> my laptop and compared to MKL and in certain cases it's 10x faster at matrix multiply.
>>>> There are a lot of layers of indirection here and you really want
>>>> to avoid data copying as much as possible.
>>>>
>>>> We could also consider swapping out BIDMat for Breeze, but that
>>>> would be a big project and if we can figure out how to get
>>>> breeze+cublas to comparable performance that would be a big win.
>>>>
>>>> On Thu, Feb 5, 2015 at 11:55 AM, Ulanov, Alexander <
>>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:
>>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>>> wrote:
>>>> Dear Spark developers,
>>>>
>>>> I am exploring how to make linear algebra operations faster within Spark.
>>>> One way of doing this is to use Scala Breeze library that is
>>>> bundled with Spark. For matrix operations, it employs Netlib-java
>>>> that has a Java wrapper for BLAS (basic linear algebra subprograms)
>>>> and LAPACK native binaries if they are available on the worker
>>>> node. It also has its own optimized Java implementation of BLAS. It
>>>> is worth mentioning, that native binaries provide better performance only for BLAS level 3, i.e.
>>>> matrix-matrix operations or general matrix multiplication (GEMM).
>>>> This is confirmed by GEMM test on Netlib-java page
>>>> https://github.com/fommil/netlib-java. I also confirmed it with my
>>>> experiments with training of artificial neural network
>>>> https://github.com/apache/spark/pull/1290#issuecomment-70313952.
>>>> However, I would like to boost performance more.
>>>>
>>>> GPU is supposed to work fast with linear algebra and there is
>>>> Nvidia CUDA implementation of BLAS, called cublas. I have one Linux
>>>> server with Nvidia GPU and I was able to do the following. I linked
>>>> cublas (instead of cpu-based blas) with Netlib-java wrapper and put
>>>> it into Spark, so Breeze/Netlib is using it. Then I did some
>>>> performance measurements with regards to artificial neural network
>>>> batch learning in Spark MLlib that involves matrix-matrix
>>>> multiplications. It turns out that for matrices of size less than
>>>> ~1000x780 GPU cublas has the same speed as CPU blas. Cublas becomes
>>>> slower for bigger matrices. It worth mentioning that it is was not a test for ONLY multiplication since there are other operations involved.
>>>> One of the reasons for slowdown might be the overhead of copying
>>>> the matrices from computer memory to graphic card memory and back.
>>>>
>>>> So, few questions:
>>>> 1) Do these results with CUDA make sense?
>>>> 2) If the problem is with copy overhead, are there any libraries
>>>> that allow to force intermediate results to stay in graphic card
>>>> memory thus removing the overhead?
>>>> 3) Any other options to speed-up linear algebra in Spark?
>>>>
>>>> Thank you, Alexander
>>>>
>>>> -------------------------------------------------------------------
>>>> -- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org><mailto:
>>>> dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apach
>>>> e.org>><mailto:dev-unsubscribe@spark.apac<mailto:dev-unsubscribe@sp
>>>> ark.apac> he.org<http://he.org>
>>>> <mailto:dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spa
>>>> rk.apache.org>>> For additional commands, e-mail:
>>>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org><mailto:
>>>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>><mailto:dev-help@spark.apache.org<mailto:dev-help@spark.apache.org><mailto:
>>>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>>>
>>>>
>>>>
>>>>
>>>>
>>>

--
Best regards,
Sam
 
 

"
"""Driesprong, Fokko"" <fokko@driesprong.frl>","Thu, 21 Jan 2016 13:07:21 +0100",Re: How Spark utilize low-level architecture features?,Boric Tan <it.news.trends@gmail.com>,"Hi Boric,

For the Spark Mllib package, which is build on top of Breeze
<https://github.com/scalanlp/breeze>, which uses in turn netlib-java
<https://github.com/fommil/netlib-java>. This netlib-java library can be
optimized for each system by compiling the specific architecture:

*To get optimal performance for a specific machine, it is best to compile
locally by grabbing the latest ATLAS or the latest OpenBLAS and following
the compilation instructions.*

For the rest, Spark focusses on adding more machines instead of using very
specific optimization procedures. Also optimizing your jobs (decreasing
communication between workers e.d.) might do the trick.

Cheers, Fokko.

2016-01-21 6:55 GMT+01:00 Boric Tan <it.news.trends@gmail.com>:

"
Steve Loughran <stevel@hortonworks.com>,"Thu, 21 Jan 2016 18:56:28 +0000",Re: How Spark utilize low-level architecture features?,,"
 can utilize the underlying architectures for better performance. If so, how does it do it?
PUs, will Spark check the individual CPU information and use some machine-specific setting for the tasks assigned to that machine? Or is it totally dependent on the underlying JVM implementation to run the JAR file, and therefor the JVM is the place to check if certain CPU features can be used?

You can't control where work is done based on CPU parts. Ideally your cluster is homogenous, or at least vary only in CPU performance and memory.

If some of your systems have GPUs and some don't, then in a YARN cluster, label the GPU parts and then use yarn queues or spark-submit to schedule the work only on those GPU systems.

The native libraries you load into JVMs are generally where CPU checks and features (e.g. x86 AES opcodes for encrypt/decrypt) would go

---------------------------------------------------------------------


"
"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Thu, 21 Jan 2016 19:10:17 +0000",RE: Using CUDA within Spark / boosting linear algebra,"Kazuaki Ishizaki <ISHIZAKI@jp.ibm.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>, Joseph Bradley <joseph@databricks.com>","Hi Kazuaki,

Indeed, moving data to/from GPU is costly and this benchmark summarizes the costs for moving different data sizes with regards to matrices multiplication. These costs are paid for the convenience of using the standard BLAS API that Nvidia NVBLAS provides. The thing is that there are no code changes required (in Spark), one just needs to reference BLAS implementation with the system variable. Naturally, hardware-specific implementation will always be faster than default. The benchmark results show that fact by comparing jCuda (by means of BIDMat) and NVBLAS. However, it also shows that it worth using NVBLAS for large matrices because it can take advantage of several GPUs and it will be faster despite the copying overhead. That is also a known thing advertised by Nvidia.

By the way, I don't think that the column/row friendly format is an issue, because one can use transposed matrices to fit the required format. I believe that is just a software preference.

My suggestion with regards to your prototype would be to make comparisons with Spark's implementation of logistic regression (that does not take advantage of GPU) and also with BIDMach's (that takes advantage of GPUs). It will give the users a better understanding of your's implementation performance. Currently you compare it with Spark's example logistic regression implementation that is supposed to be a reference for learning Spark rather than benchmarking its performance.

Best regards, Alexander

From: Kazuaki Ishizaki [mailto:ISHIZAKI@jp.ibm.com]
Sent: Thursday, January 21, 2016 3:34 AM
To: dev@spark.apache.org; Ulanov, Alexander; Joseph Bradley
Cc: John Canny; Evan R. Sparks; Xiangrui Meng; Sam Halliday
Subject: RE: Using CUDA within Spark / boosting linear algebra

Dear all,

r part of memory sounds like a much bigger undertaking.

As Joseph pointed out before, there are two potential issues to efficiently exploit GPUs in Spark.
(1) the cost of data movement between CPU and GPU
(2) the cost of encoding/decoding between current row-format and GPU-friendly column format

Our prototype http://kiszk.github.io/spark-gpu/addresses these two issues by supporting data partition caching in GPU device memory and by providing binary column storage for data partition. We really appreciate it if you would give us comments, suggestions, or feedback.

Best Regards
Kazuaki Ishizaki



From:        ""Ulanov, Alexander"" <alexander.ulanov@hpe.com<mailto:alexander.ulanov@hpe.com>>
To:        Sam Halliday <sam.halliday@gmail.com<mailto:sam.halliday@gmail.com>>, John Canny <canny@berkeley.edu<mailto:canny@berkeley.edu>>
Cc:        Xiangrui Meng <mengxr@gmail.com<mailto:mengxr@gmail.com>>, ""dev@spark.apache.org<mailto:dev@spark.apache.org>"" <dev@spark.apache.org<mailto:dev@spark.apache.org>>, Joseph Bradley <joseph@databricks.com<mailto:joseph@databricks.com>>, ""Evan R. Sparks"" <evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>
Date:        2016/01/21 11:07
Subject:        RE: Using CUDA within Spark / boosting linear algebra
________________________________



Hi Everyone,

I've updated the benchmark and done experiments with new hardware with 2x Nvidia Tesla K80 (physically 4x Tesla K40) and 2x modern Haswell CPU Intel E5-2650 v3 @ 2.30GHz.

This time I computed average and median of 10 runs for each of experiment and approximated FLOPS.

Results are available at google docs (old experiments are in the other 2 sheets):
https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J5r7kwKSPkY/edit?usp=sharing
Benchmark code:
https://github.com/avulanov/scala-blas

Best regards, Alexander


From: Sam Halliday [mailto:sam.halliday@gmail.com]
Sent: Thursday, March 26, 2015 9:27 AM
To: John Canny
Cc: Xiangrui Meng; dev@spark.apache.org<mailto:dev@spark.apache.org>; Joseph Bradley; Evan R. Sparks; Ulanov, Alexander
Subject: Re: Using CUDA within Spark / boosting linear algebra


John, I have to disagree with you there. Dense matrices come up a lot in industry,  although your personal experience may be different.
I mentioned this earlier in the thread, but I'll put it out again. Dense BLAS are not very important for most machine learning workloads: at least for non-image workloads in industry (and for image processing you would probably want a deep learning/SGD solution with convolution kernels). e.g. it was only relevant for 1/7 of our recent benchmarks, which should be a reasonable sample. What really matters is sparse BLAS performance. BIDMat is still an order of magnitude faster there. Those kernels are only in BIDMat, since NVIDIAs sparse BLAS dont perform well on power-law data.

Its also the case that the overall performance of an algorithm is determined by the slowest kernel, not the fastest. If the goal is to get closer to BIDMach's performance on typical problems, you need to make sure that every kernel goes at comparable speed. So the real question is how much faster MLLib routines do on a complete problem with/without GPU acceleration. For BIDMach, its close to a factor of 10. But that required running entirely on the GPU, and making sure every kernel is close to its limit.

-John

If you think nvblas would be helpful, you should try it in some end-to-end benchmarks.
Yeah, much more reasonable - nice to know that we can get full GPU performance from breeze/netlib-java - meaning there's no compelling performance reason to switch out our current linear algebra library (at least as far as this benchmark is concerned).

Instead, it looks like a user guide for configuring Spark/MLlib to use the right BLAS library will get us most of the way there. Or, would it make sense to finally ship openblas compiled for some common platforms (64-bit linux, windows, mac) directly with Spark - hopefully eliminating the jblas warnings once and for all for most users? (Licensing is BSD) Or am I missing something?

As everyone suggested, the results were too good to be true, so I double-checked them. It turns that nvblas did not do multiplication due to parameter NVBLAS_TILE_DIM from ""nvblas.conf"" and returned zero matrix. My previously posted results with nvblas are matrices copying only. The default NVBLAS_TILE_DIM==2048 is too big for my graphic card/matrix size. I handpicked other values that worked. As a result, netlib+nvblas is on par with BIDMat-cuda. As promised, I am going to post a how-to for nvblas configuration.

https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J5r7kwKSPkY/edit?usp=sharing




Hi again,

I finally managed to use nvblas within Spark+netlib-java. It has exceptional performance for big matrices with Double, faster than BIDMat-cuda with Float. But for smaller matrices, if you will copy them to/from GPU, OpenBlas or MKL might be a better choice. This correlates with original nvblas presentation on GPU conf 2013 (slide 21): http://on-demand.gputechconf.com/supercomputing/2013/presentation/SC3108-New-Features-CUDA%206%20-GPU-Acceleration.pdf

My results:
https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J5r7kwKSPkY/edit?usp=sharing

Just in case, these tests are not for generalization of performance of different libraries. I just want to pick a library that does at best dense matrices multiplication for my task.

P.S. My previous issue with nvblas was the following: it has Fortran blas functions, at the same time netlib-java uses C cblas functions. So, one needs cblas shared library to use nvblas through netlib-java. Fedora does not have cblas (but Debian and Ubuntu have), so I needed to compile it. I could not use cblas from Atlas or Openblas because they link to their implementation and not to Fortran blas.

Best regards, Alexander


Hi,

I am trying to use nvblas with netlib-java from Spark. nvblas functions should replace current blas functions calls after executing LD_PRELOAD as suggested in http://docs.nvidia.com/cuda/nvblas/#Usagewithout any changes to netlib-java. It seems to work for simple Java example, but I cannot make it work with Spark. I run the following:
export LD_LIBRARY_PATH=/usr/local/cuda-6.5/lib64
env LD_PRELOAD=/usr/local/cuda-6.5/lib64/libnvblas.so ./spark-shell --driver-memory 4G In nvidia-smi I observe that Java is to use GPU:
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0      8873    C   bash                                            39MiB |
|    0      8910    C   /usr/lib/jvm/java-1.7.0/bin/java                39MiB |
+-----------------------------------------------------------------------------+

In Spark shell I do matrix multiplication and see the following:
15/03/25 06:48:01 INFO JniLoader: successfully loaded /tmp/jniloader8192964377009965483netlib-native_system-linux-x86_64.so
So I am sure that netlib-native is loaded and cblas supposedly used. However, matrix multiplication does executes on CPU since I see 16% of CPU used and 0% of GPU used. I also checked different matrix sizes, from 100x100 to 12000x12000

Could you suggest might the LD_PRELOAD not affect Spark shell?

Best regards, Alexander



From: Sam Halliday [mailto:sam.halliday@gmail.com<mailto:sam.halliday@gmail.com>]
Sent: Monday, March 09, 2015 6:01 PM
To: Ulanov, Alexander
Cc: dev@spark.apache.org<mailto:dev@spark.apache.org>; Xiangrui Meng; Joseph Bradley; Evan R. Sparks
Subject: RE: Using CUDA within Spark / boosting linear algebra


Thanks so much for following up on this!

Hmm, I wonder if we should have a concerted effort to chart performance on various pieces of hardware...
exander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanHi Everyone, I've updated the benchmark as Xiangrui suggested. Added the comment that BIDMat 0.9.7 uses Float matrices in GPU (although I see the support of Double in the current source code), did the test with BIDMat and CPU Double matrices. BIDMat MKL is indeed on par with netlib MKL.

https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J5r7kwKSPkY/edit?usp=sharing

Best regards, Alexander

Subject: Re: Using CUDA within Spark / boosting linear algebra

BTW, is anybody on this list going to the London Meetup in a few weeks?

https://skillsmatter.com/meetups/6987-apache-spark-living-the-post-mapreduce-world#community

Would be nice to meet other people working on the guts of Spark! :-)


Xiangrui Meng <mengxr@gmail.com<mailto:mengxr@gmail.com><mailto:mengxr@gmail.com<mailto:mengxr@gmail.com>>> writes:

ilto:joseph@databricks.com><mailto:joseph@databricks.com<mailto:joseph@datagmail.com<mailto:evan.sparks@gmail.com>>>
rices.
repo=
che.org<mailto:dev@spark.apache.org>>
--+
n.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
che.org<mailto:dev@spark.apache.org>>
er.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto be on par with JNI overheads.
gmail.com><mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>><mailto:
@gmail.com<mailto:evan.sparks@gmail.com>>>]
che.org<mailto:dev@spark.apache.org>><mailto:dev@spark<mailto:dev@spark>.
spark.apache.org>>>
netlib-java.
er.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailtow to force use a specific blas (not specific wrapper for blas).
gmail.com><mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>><mailto:
@gmail.com<mailto:evan.sparks@gmail.com>>>]
che.org<mailto:dev@spark.apache.org>><mailto:dev@spark<mailto:dev@spark>.
spark.apache.org>>>
t).
er.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto--+
ricks.com><mailto:joseph@databricks.com<mailto:joseph@databricks.com>><mailto:
bricks.com<mailto:joseph@databricks.com>>>]
che.org<mailto:dev@spark.apache.org>><mailto:dev@spark<mailto:dev@spark>.
spark.apache.org>>>
r part of memory sounds like a much bigger undertaking.
er.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailtopark MLlib.
chine learning.
gmail.com><mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>><mailto:
@gmail.com<mailto:evan.sparks@gmail.com>>>]
che.org<mailto:dev@spark.apache.org>><mailto:dev@spark<mailto:dev@spark>.
spark.apache.org>>>
GPUs.
bytes.
/%7Ehzhao/papers/BD.pdf>
er.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>><mailto:
er.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailtomization and learning?
gmail.com><mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>><mailto:
@gmail.com<mailto:evan.sparks@gmail.com>>><mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>><mailto:
@gmail.com<mailto:evan.sparks@gmail.com>>>>]
.apache.org<mailto:dev@spark.apache.org>><mailto:dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.apache.org<mailto:dev@spark.apache.org>>><mailto:
che.org<mailto:dev@spark.apache.org>><mailto:dev@spark<mailto:dev@spark>.
spark.apache.org>>>>
matrix multiply.
er.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>><mailto:
er.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailtork.
nly for BLAS level 3, i.e.
est for ONLY multiplication since there are other operations involved.
-unsubscribe@spark.apache.org><mailto:dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org>><mailto:
rg><mailto:dev-unsubscribe@spark.apach<mailto:dev-unsubscribe@spark.apach>
subscribe@spark.apac><mailto:dev-unsubscribe@sp<mailto:dev-unsubscribe@sp>
apache.org><mailto:dev-unsubscribe@spa<mailto:dev-unsubscribe@spa>
l:
-help@spark.apache.org<mailto:dev-help@spark.apache.org>><mailto:
-help@spark.apache.org<mailto:dev-help@spark.apache.org>>><mailto:dev-help@spark.apache.org<mailto:dev-help@spark.apache.org><mailto:dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>><mailto:
-help@spark.apache.org<mailto:dev-help@spark.apache.org>>>>

--
Best regards,
Sam


"
sara mustafa <eng.sara.mustafa@gmail.com>,"Thu, 21 Jan 2016 13:05:00 -0700 (MST)",Generate Amplab queries set,dev@spark.apache.org,"Hi,
I have downloaded the Amplab benchmark dataset from
s3n://big-data-benchmark/pavlo/text/tiny, but I don't know how to generate a
set of random mixed queries of different types like scan,aggregate and join. 

Thanks,



--

---------------------------------------------------------------------


"
borictan <it.news.trends@gmail.com>,"Thu, 21 Jan 2016 15:12:08 -0700 (MST)",Re: How Spark utilize low-level architecture features?,dev@spark.apache.org,"Thanks for the explanation, Steve. 

I don't want to control where the work is done. What I wanted to understand
is if Spark could take advantage of the underlying architecture features.
For example, if the CPUs on the nodes support some improved vector
instructions, can the Spark jobs (if they have a lot of vector operations)
benefit from this? If yes, how does it happen, inside Spark, or the JVM
where the the job TAR is running on?

Also, for the GPU part you mentioned, labeling the GPU nodes, and scheduling
work to those GPU-enabled system does not mean the GPU computation power
will be utilized, right? The user has to provide CUDE codes
(openCL/CUDA/etc) and somehow link them to the system. Is my understanding
correct? 


Thanks,
Boric



--

---------------------------------------------------------------------


"
borictan <it.news.trends@gmail.com>,"Thu, 21 Jan 2016 15:14:18 -0700 (MST)",Re: How Spark utilize low-level architecture features?,dev@spark.apache.org,"Thanks, Fokko.

the other hand, we are also looking for opportunities to harness the single
node hardware power to increase the single-node performance, which will help
the overall performance. 

Thanks,
Boric



--

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Thu, 21 Jan 2016 19:13:29 -0800",Re: Spark SQL: Avoid shuffles when data is already partitioned on disk,Justin Uang <justin.uang@gmail.com>,"It is not necessary if you are using bucketing available in Spark 2.0. For
partitioning, it is still necessary because we do not assume each partition
is small, and as a result there is no guarantee all the records for a
partition end up in a single Spark task partition.



"
Takeshi Yamamuro <linguin.m.s@gmail.com>,"Fri, 22 Jan 2016 15:54:53 +0900",Re: Spark SQL: Avoid shuffles when data is already partitioned on disk,Reynold Xin <rxin@databricks.com>,"You mean RDD#partitions are possibly split into multiple Spark task
partitions?
If so, the optimization below is wrong?

Without opt.:
####
== Physical Plan ==
TungstenAggregate(key=[col0#159],
functions=[(sum(col1#160),mode=Final,isDistinct=false),(avg(col2#161),mode=Final,isDistinct=false)],
output=[col0#159,sum(col1)#177,avg(col2)#178])
+- TungstenAggregate(key=[col0#159],
functions=[(sum(col1#160),mode=Partial,isDistinct=false),(avg(col2#161),mode=Partial,isDistinct=false)],
output=[col0#159,sum#200,sum#201,count#202L])
   +- TungstenExchange hashpartitioning(col0#159,200), None
      +- InMemoryColumnarTableScan [col0#159,col1#160,col2#161],
InMemoryRelation [col0#159,col1#160,col2#161], true, 10000,
StorageLevel(true, true, false, true, 1), ConvertToUnsafe, None

With opt.:
####
== Physical Plan ==
TungstenAggregate(key=[col0#159],
functions=[(sum(col1#160),mode=Complete,isDistinct=false),(avg(col2#161),mode=Final,isDistinct=false)],
output=[col0#159,sum(col1)#177,avg(col2)#178])
+- TungstenExchange hashpartitioning(col0#159,200), None
  +- InMemoryColumnarTableScan [col0#159,col1#160,col2#161],
InMemoryRelation [col0#159,col1#160,col2#161], true, 10000,
StorageLevel(true, true, false, true, 1), ConvertToUnsafe, None






-- 
---
Takeshi Yamamuro
"
Reynold Xin <rxin@databricks.com>,"Thu, 21 Jan 2016 23:34:05 -0800",Re: Spark SQL: Avoid shuffles when data is already partitioned on disk,Takeshi Yamamuro <linguin.m.s@gmail.com>,"The original email was asking about data partitioning (Hive style) for
files, not in memory caching.


"
=?gb2312?B?zfTR8w==?= <tiandiwoxin@icloud.com>,"Fri, 22 Jan 2016 16:53:48 +0800",Using distinct count in over clause,dev@spark.apache.org,"Hi,

Do we support distinct count in the over clause in spark sql? 

I ran a sql like this:

select a, count(distinct b) over ( order by a rows between unbounded preceding and current row) from table limit 10

Currently, it return an error says: expression °Æa' is neither present in the group by, nor is it an aggregate function. Add to group by or wrap in first() if you don't care which value you get.;

Yang

"
=?gb2312?B?zfTR8w==?= <tiandiwoxin@icloud.com>,"Fri, 22 Jan 2016 17:01:00 +0800",Re: Using distinct count in over clause,dev@spark.apache.org,"I think it cannot be right.

 <tiandiwoxin@icloud.com> –¥µ¿£∫
preceding and current row) from table limit 10
present in the group by, nor is it an aggregate function. Add to group by or wrap in first() if you don't care which value you get.;

"
"""Kazuaki Ishizaki"" <ISHIZAKI@jp.ibm.com>","Fri, 22 Jan 2016 09:36:04 +0000",RE: Using CUDA within Spark / boosting linear algebra,"""Allen Zhang"" <allenzhang010@126.com>","Hi Allen,
Thank you for your feedback.
An API to launch GPU kernels with JCuda is the our first step. A purpose 
to release our prototype is to get feedback. In the future, we may use 
other wrappers instead of JCuda.

We are very appreciate it if you would suggest or propose APIs to 
effectively exploit GPUs such as BIDMat in Spark.
If we would run BIDMat with our columnar strorage, the performance boost 
would be good as others reported.

Best Regards,
Kazuaki Ishizaki,



From:   ""Allen Zhang"" <allenzhang010@126.com>
To:     Kazuaki Ishizaki/Japan/IBM@IBMJP
Cc:     ""dev@spark.apache.org"" <dev@spark.apache.org>, ""Ulanov, Alexander"" 
<alexander.ulanov@hpe.com>, ""Joseph Bradley"" <joseph@databricks.com>, 
""John Canny"" <canny@berkeley.edu>, ""Evan R. Sparks"" 
<evan.sparks@gmail.com>, ""Xiangrui Meng"" <mengxr@gmail.com>, ""Sam 
Halliday"" <sam.halliday@gmail.com>
Date:   2016/01/21 21:05
Subject:        RE: Using CUDA within Spark / boosting linear algebra




Hi Kazuaki,

Jcuda is actually a wrapper of the **pure** CUDA, as your wiki page shows 
that 3.15x performance boost of logistic regression seems slower than 
BIDMat-cublas or pure CUDA.
Could you elaborate on why you chose Jcuda other then JNI to call CUDA 
directly?

Regards,
Allen Zhang






Dear all,

another part of memory sounds like a much bigger undertaking.

As Joseph pointed out before, there are two potential issues to 
efficiently exploit GPUs in Spark.
(1) the cost of data movement between CPU and GPU
(2) the cost of encoding/decoding between current row-format and 
GPU-friendly column format

Our prototype http://kiszk.github.io/spark-gpu/addresses these two issues 
by supporting data partition caching in GPU device memory and by providing 
binary column storage for data partition. We really appreciate it if you 
would give us comments, suggestions, or feedback.

Best Regards
Kazuaki Ishizaki



From:        ""Ulanov, Alexander"" <alexander.ulanov@hpe.com>
To:        Sam Halliday <sam.halliday@gmail.com>, John Canny <
canny@berkeley.edu>
Cc:        Xiangrui Meng <mengxr@gmail.com>, ""dev@spark.apache.org"" <
dev@spark.apache.org>, Joseph Bradley <joseph@databricks.com>, ""Evan R. 
Sparks"" <evan.sparks@gmail.com>
Date:        2016/01/21 11:07
Subject:        RE: Using CUDA within Spark / boosting linear algebra



Hi Everyone,
 
I$B!G(Bve updated the benchmark and done experiments with new hardware with 2x 
Nvidia Tesla K80 (physically 4x Tesla K40) and 2x modern Haswell CPU Intel 
E5-2650 v3 @ 2.30GHz.
 
This time I computed average and median of 10 runs for each of experiment 
and approximated FLOPS.
 
Results are available at google docs (old experiments are in the other 2 
sheets):
https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J5r7kwKSPkY/edit?usp=sharing

Benchmark code:
https://github.com/avulanov/scala-blas
 
Best regards, Alexander
 
 
From: Sam Halliday [mailto:sam.halliday@gmail.com] 
Sent: Thursday, March 26, 2015 9:27 AM
To: John Canny
Cc: Xiangrui Meng; dev@spark.apache.org; Joseph Bradley; Evan R. Sparks; 
Ulanov, Alexander
Subject: Re: Using CUDA within Spark / boosting linear algebra
 
John, I have to disagree with you there. Dense matrices come up a lot in 
industry,  although your personal experience may be different. 
I mentioned this earlier in the thread, but I'll put it out again. Dense 
BLAS are not very important for most machine learning workloads: at least 
for non-image workloads in industry (and for image processing you would 
probably want a deep learning/SGD solution with convolution kernels). e.g. 
it was only relevant for 1/7 of our recent benchmarks, which should be a 
reasonable sample. What really matters is sparse BLAS performance. BIDMat 
is still an order of magnitude faster there. Those kernels are only in 
BIDMat, since NVIDIAs sparse BLAS dont perform well on power-law data. 

Its also the case that the overall performance of an algorithm is 
determined by the slowest kernel, not the fastest. If the goal is to get 
closer to BIDMach's performance on typical problems, you need to make sure 
that every kernel goes at comparable speed. So the real question is how 
much faster MLLib routines do on a complete problem with/without GPU 
acceleration. For BIDMach, its close to a factor of 10. But that required 
running entirely on the GPU, and making sure every kernel is close to its 
limit.

-John

If you think nvblas would be helpful, you should try it in some end-to-end 
benchmarks. 
Yeah, much more reasonable - nice to know that we can get full GPU 
performance from breeze/netlib-java - meaning there's no compelling 
performance reason to switch out our current linear algebra library (at 
least as far as this benchmark is concerned). 
 
Instead, it looks like a user guide for configuring Spark/MLlib to use the 
right BLAS library will get us most of the way there. Or, would it make 
sense to finally ship openblas compiled for some common platforms (64-bit 
linux, windows, mac) directly with Spark - hopefully eliminating the jblas 
warnings once and for all for most users? (Licensing is BSD) Or am I 
missing something?
 
As everyone suggested, the results were too good to be true, so I 
double-checked them. It turns that nvblas did not do multiplication due to 
parameter NVBLAS_TILE_DIM from ""nvblas.conf"" and returned zero matrix. My 
previously posted results with nvblas are matrices copying only. The 
default NVBLAS_TILE_DIM==2048 is too big for my graphic card/matrix size. 
I handpicked other values that worked. As a result, netlib+nvblas is on 
par with BIDMat-cuda. As promised, I am going to post a how-to for nvblas 
configuration.

https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J5r7kwKSPkY/edit?usp=sharing





Hi again,

I finally managed to use nvblas within Spark+netlib-java. It has 
exceptional performance for big matrices with Double, faster than 
BIDMat-cuda with Float. But for smaller matrices, if you will copy them 
to/from GPU, OpenBlas or MKL might be a better choice. This correlates 
with original nvblas presentation on GPU conf 2013 (slide 21): 
http://on-demand.gputechconf.com/supercomputing/2013/presentation/SC3108-New-Features-CUDA%206%20-GPU-Acceleration.pdf


My results:
https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J5r7kwKSPkY/edit?usp=sharing


Just in case, these tests are not for generalization of performance of 
different libraries. I just want to pick a library that does at best dense 
matrices multiplication for my task.

P.S. My previous issue with nvblas was the following: it has Fortran blas 
functions, at the same time netlib-java uses C cblas functions. So, one 
needs cblas shared library to use nvblas through netlib-java. Fedora does 
not have cblas (but Debian and Ubuntu have), so I needed to compile it. I 
could not use cblas from Atlas or Openblas because they link to their 
implementation and not to Fortran blas.

Best regards, Alexander

Hi,

I am trying to use nvblas with netlib-java from Spark. nvblas functions 
should replace current blas functions calls after executing LD_PRELOAD as 
suggested in http://docs.nvidia.com/cuda/nvblas/#Usagewithout any changes 
to netlib-java. It seems to work for simple Java example, but I cannot 
make it work with Spark. I run the following:
export LD_LIBRARY_PATH=/usr/local/cuda-6.5/lib64
env LD_PRELOAD=/usr/local/cuda-6.5/lib64/libnvblas.so ./spark-shell 
--driver-memory 4G In nvidia-smi I observe that Java is to use GPU:
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU 
Memory |
|  GPU       PID  Type  Process name                               Usage   
|
|=============================================================================|
|    0      8873    C   bash 39MiB |
|    0      8910    C   /usr/lib/jvm/java-1.7.0/bin/java 39MiB |
+-----------------------------------------------------------------------------+

In Spark shell I do matrix multiplication and see the following:
15/03/25 06:48:01 INFO JniLoader: successfully loaded 
/tmp/jniloader8192964377009965483netlib-native_system-linux-x86_64.so
So I am sure that netlib-native is loaded and cblas supposedly used. 
However, matrix multiplication does executes on CPU since I see 16% of CPU 
used and 0% of GPU used. I also checked different matrix sizes, from 
100x100 to 12000x12000

Could you suggest might the LD_PRELOAD not affect Spark shell?

Best regards, Alexander



From: Sam Halliday [mailto:sam.halliday@gmail.com]
Sent: Monday, March 09, 2015 6:01 PM
To: Ulanov, Alexander
Cc: dev@spark.apache.org; Xiangrui Meng; Joseph Bradley; Evan R. Sparks
Subject: RE: Using CUDA within Spark / boosting linear algebra


Thanks so much for following up on this!

Hmm, I wonder if we should have a concerted effort to chart performance on 
various pieces of hardware...
Hi Everyone, I've updated the benchmark as Xiangrui suggested. Added the 
comment that BIDMat 0.9.7 uses Float matrices in GPU (although I see the 
support of Double in the current source code), did the test with BIDMat 
and CPU Double matrices. BIDMat MKL is indeed on par with netlib MKL.

https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J5r7kwKSPkY/edit?usp=sharing


Best regards, Alexander

Subject: Re: Using CUDA within Spark / boosting linear algebra

BTW, is anybody on this list going to the London Meetup in a few weeks?

https://skillsmatter.com/meetups/6987-apache-spark-living-the-post-mapreduce-world#community


Would be nice to meet other people working on the guts of Spark! :-)


Xiangrui Meng <mengxr@gmail.com<mailto:mengxr@gmail.com>> writes:

matrices.
repo=
+-----------------------------------------------------------------------+
 with
to be on par with JNI overheads.
evan.sparks@gmail.com><mailto:

netlib-java.
how to force use a specific blas (not specific wrapper for blas).
evan.sparks@gmail.com><mailto:
it).
+-----------------------------------------------------------------------+
joseph@databricks.com><mailto:
another part of memory sounds like a much bigger undertaking.
ation by
Spark MLlib.
!I(B test of
machine learning.
evan.sparks@gmail.com><mailto:

GPUs.
terabytes.
alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:
ch for 
optimization and learning?
evan.sparks@gmail.com><mailto:
evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:
dev@spark.apache.org<mailto:dev@spark.apache.org>><mailto:

matrix multiply.
alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:
Spark.
only for BLAS level 3, i.e.
test for ONLY multiplication since there are other operations involved.
dev-unsubscribe@spark.apache.org><mailto:
dev-help@spark.apache.org<mailto:dev-help@spark.apache.org><mailto:

--
Best regards,
Sam
 
 



 


"
"""Kazuaki Ishizaki"" <ISHIZAKI@jp.ibm.com>","Fri, 22 Jan 2016 09:42:28 +0000",RE: Using CUDA within Spark / boosting linear algebra,"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Hi Alexander,
important items is to effectively and easily enable highly-tuned libraries 
for GPU such as BIDMach.


not easy task to scaling BIDMach with current Spark. I expect that this 
talk would help us.
http://conferences.oreilly.com/strata/hadoop-big-data-ca/public/schedule/detail/47565

We appreciate your great feedback.

Best Regards,
Kazuaki Ishizaki, Ph.D., Senior research staff member, IBM Research - 
Tokyo



From:   ""Ulanov, Alexander"" <alexander.ulanov@hpe.com>
To:     Kazuaki Ishizaki/Japan/IBM@IBMJP, ""dev@spark.apache.org"" 
<dev@spark.apache.org>, Joseph Bradley <joseph@databricks.com>
Cc:     John Canny <canny@berkeley.edu>, ""Evan R. Sparks"" 
<evan.sparks@gmail.com>, Xiangrui Meng <mengxr@gmail.com>, Sam Halliday 
<sam.halliday@gmail.com>
Date:   2016/01/22 04:20
Subject:        RE: Using CUDA within Spark / boosting linear algebra



Hi Kazuaki,
 
Indeed, moving data to/from GPU is costly and this benchmark summarizes 
the costs for moving different data sizes with regards to matrices 
multiplication. These costs are paid for the convenience of using the 
standard BLAS API that Nvidia NVBLAS provides. The thing is that there are 
no code changes required (in Spark), one just needs to reference BLAS 
implementation with the system variable. Naturally, hardware-specific 
implementation will always be faster than default. The benchmark results 
show that fact by comparing jCuda (by means of BIDMat) and NVBLAS. 
However, it also shows that it worth using NVBLAS for large matrices 
because it can take advantage of several GPUs and it will be faster 
despite the copying overhead. That is also a known thing advertised by 
Nvidia.
 
By the way, I don$B!G(Bt think that the column/row friendly format is an 
issue, because one can use transposed matrices to fit the required format. 
I believe that is just a software preference.
 
My suggestion with regards to your prototype would be to make comparisons 
with Spark$B!G(Bs implementation of logistic regression (that does not take 
advantage of GPU) and also with BIDMach$B!G(Bs (that takes advantage of GPUs). 
It will give the users a better understanding of your$B!G(Bs implementation 
performance. Currently you compare it with Spark$B!G(Bs example logistic 
regression implementation that is supposed to be a reference for learning 
Spark rather than benchmarking its performance.
 
Best regards, Alexander
 
From: Kazuaki Ishizaki [mailto:ISHIZAKI@jp.ibm.com] 
Sent: Thursday, January 21, 2016 3:34 AM
To: dev@spark.apache.org; Ulanov, Alexander; Joseph Bradley
Cc: John Canny; Evan R. Sparks; Xiangrui Meng; Sam Halliday
Subject: RE: Using CUDA within Spark / boosting linear algebra
 
Dear all,

another part of memory sounds like a much bigger undertaking.

As Joseph pointed out before, there are two potential issues to 
efficiently exploit GPUs in Spark.
(1) the cost of data movement between CPU and GPU
(2) the cost of encoding/decoding between current row-format and 
GPU-friendly column format

Our prototype http://kiszk.github.io/spark-gpu/addresses these two issues 
by supporting data partition caching in GPU device memory and by providing 
binary column storage for data partition. We really appreciate it if you 
would give us comments, suggestions, or feedback.

Best Regards
Kazuaki Ishizaki



From:        ""Ulanov, Alexander"" <alexander.ulanov@hpe.com>
To:        Sam Halliday <sam.halliday@gmail.com>, John Canny <
canny@berkeley.edu>
Cc:        Xiangrui Meng <mengxr@gmail.com>, ""dev@spark.apache.org"" <
dev@spark.apache.org>, Joseph Bradley <joseph@databricks.com>, ""Evan R. 
Sparks"" <evan.sparks@gmail.com>
Date:        2016/01/21 11:07
Subject:        RE: Using CUDA within Spark / boosting linear algebra




Hi Everyone,
 
I$B!G(Bve updated the benchmark and done experiments with new hardware with 2x 
Nvidia Tesla K80 (physically 4x Tesla K40) and 2x modern Haswell CPU Intel 
E5-2650 v3 @ 2.30GHz.
 
This time I computed average and median of 10 runs for each of experiment 
and approximated FLOPS.
 
Results are available at google docs (old experiments are in the other 2 
sheets):
https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J5r7kwKSPkY/edit?usp=sharing

Benchmark code:
https://github.com/avulanov/scala-blas
 
Best regards, Alexander
 
 
From: Sam Halliday [mailto:sam.halliday@gmail.com] 
Sent: Thursday, March 26, 2015 9:27 AM
To: John Canny
Cc: Xiangrui Meng; dev@spark.apache.org; Joseph Bradley; Evan R. Sparks; 
Ulanov, Alexander
Subject: Re: Using CUDA within Spark / boosting linear algebra
 
John, I have to disagree with you there. Dense matrices come up a lot in 
industry,  although your personal experience may be different. 
I mentioned this earlier in the thread, but I'll put it out again. Dense 
BLAS are not very important for most machine learning workloads: at least 
for non-image workloads in industry (and for image processing you would 
probably want a deep learning/SGD solution with convolution kernels). e.g. 
it was only relevant for 1/7 of our recent benchmarks, which should be a 
reasonable sample. What really matters is sparse BLAS performance. BIDMat 
is still an order of magnitude faster there. Those kernels are only in 
BIDMat, since NVIDIAs sparse BLAS dont perform well on power-law data. 

Its also the case that the overall performance of an algorithm is 
determined by the slowest kernel, not the fastest. If the goal is to get 
closer to BIDMach's performance on typical problems, you need to make sure 
that every kernel goes at comparable speed. So the real question is how 
much faster MLLib routines do on a complete problem with/without GPU 
acceleration. For BIDMach, its close to a factor of 10. But that required 
running entirely on the GPU, and making sure every kernel is close to its 
limit.

-John

If you think nvblas would be helpful, you should try it in some end-to-end 
benchmarks. 
Yeah, much more reasonable - nice to know that we can get full GPU 
performance from breeze/netlib-java - meaning there's no compelling 
performance reason to switch out our current linear algebra library (at 
least as far as this benchmark is concerned). 
 
Instead, it looks like a user guide for configuring Spark/MLlib to use the 
right BLAS library will get us most of the way there. Or, would it make 
sense to finally ship openblas compiled for some common platforms (64-bit 
linux, windows, mac) directly with Spark - hopefully eliminating the jblas 
warnings once and for all for most users? (Licensing is BSD) Or am I 
missing something?
 
As everyone suggested, the results were too good to be true, so I 
double-checked them. It turns that nvblas did not do multiplication due to 
parameter NVBLAS_TILE_DIM from ""nvblas.conf"" and returned zero matrix. My 
previously posted results with nvblas are matrices copying only. The 
default NVBLAS_TILE_DIM==2048 is too big for my graphic card/matrix size. 
I handpicked other values that worked. As a result, netlib+nvblas is on 
par with BIDMat-cuda. As promised, I am going to post a how-to for nvblas 
configuration.

https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J5r7kwKSPkY/edit?usp=sharing





Hi again,

I finally managed to use nvblas within Spark+netlib-java. It has 
exceptional performance for big matrices with Double, faster than 
BIDMat-cuda with Float. But for smaller matrices, if you will copy them 
to/from GPU, OpenBlas or MKL might be a better choice. This correlates 
with original nvblas presentation on GPU conf 2013 (slide 21): 
http://on-demand.gputechconf.com/supercomputing/2013/presentation/SC3108-New-Features-CUDA%206%20-GPU-Acceleration.pdf


My results:
https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J5r7kwKSPkY/edit?usp=sharing


Just in case, these tests are not for generalization of performance of 
different libraries. I just want to pick a library that does at best dense 
matrices multiplication for my task.

P.S. My previous issue with nvblas was the following: it has Fortran blas 
functions, at the same time netlib-java uses C cblas functions. So, one 
needs cblas shared library to use nvblas through netlib-java. Fedora does 
not have cblas (but Debian and Ubuntu have), so I needed to compile it. I 
could not use cblas from Atlas or Openblas because they link to their 
implementation and not to Fortran blas.

Best regards, Alexander

Hi,

I am trying to use nvblas with netlib-java from Spark. nvblas functions 
should replace current blas functions calls after executing LD_PRELOAD as 
suggested in http://docs.nvidia.com/cuda/nvblas/#Usagewithout any changes 
to netlib-java. It seems to work for simple Java example, but I cannot 
make it work with Spark. I run the following:
export LD_LIBRARY_PATH=/usr/local/cuda-6.5/lib64
env LD_PRELOAD=/usr/local/cuda-6.5/lib64/libnvblas.so ./spark-shell 
--driver-memory 4G In nvidia-smi I observe that Java is to use GPU:
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU 
Memory |
|  GPU       PID  Type  Process name                               Usage   
|
|=============================================================================|
|    0      8873    C   bash 39MiB |
|    0      8910    C   /usr/lib/jvm/java-1.7.0/bin/java 39MiB |
+-----------------------------------------------------------------------------+

In Spark shell I do matrix multiplication and see the following:
15/03/25 06:48:01 INFO JniLoader: successfully loaded 
/tmp/jniloader8192964377009965483netlib-native_system-linux-x86_64.so
So I am sure that netlib-native is loaded and cblas supposedly used. 
However, matrix multiplication does executes on CPU since I see 16% of CPU 
used and 0% of GPU used. I also checked different matrix sizes, from 
100x100 to 12000x12000

Could you suggest might the LD_PRELOAD not affect Spark shell?

Best regards, Alexander



From: Sam Halliday [mailto:sam.halliday@gmail.com]
Sent: Monday, March 09, 2015 6:01 PM
To: Ulanov, Alexander
Cc: dev@spark.apache.org; Xiangrui Meng; Joseph Bradley; Evan R. Sparks
Subject: RE: Using CUDA within Spark / boosting linear algebra


Thanks so much for following up on this!

Hmm, I wonder if we should have a concerted effort to chart performance on 
various pieces of hardware...
Hi Everyone, I've updated the benchmark as Xiangrui suggested. Added the 
comment that BIDMat 0.9.7 uses Float matrices in GPU (although I see the 
support of Double in the current source code), did the test with BIDMat 
and CPU Double matrices. BIDMat MKL is indeed on par with netlib MKL.

https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J5r7kwKSPkY/edit?usp=sharing


Best regards, Alexander

Subject: Re: Using CUDA within Spark / boosting linear algebra

BTW, is anybody on this list going to the London Meetup in a few weeks?

https://skillsmatter.com/meetups/6987-apache-spark-living-the-post-mapreduce-world#community


Would be nice to meet other people working on the guts of Spark! :-)


Xiangrui Meng <mengxr@gmail.com<mailto:mengxr@gmail.com>> writes:

matrices.
repo=
+-----------------------------------------------------------------------+
 with
to be on par with JNI overheads.
evan.sparks@gmail.com><mailto:

netlib-java.
how to force use a specific blas (not specific wrapper for blas).
evan.sparks@gmail.com><mailto:
it).
+-----------------------------------------------------------------------+
joseph@databricks.com><mailto:
another part of memory sounds like a much bigger undertaking.
ation by
Spark MLlib.
!I(B test of
machine learning.
evan.sparks@gmail.com><mailto:

GPUs.
terabytes.
alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:
ch for 
optimization and learning?
evan.sparks@gmail.com><mailto:
evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:
dev@spark.apache.org<mailto:dev@spark.apache.org>><mailto:

matrix multiply.
alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:
Spark.
only for BLAS level 3, i.e.
test for ONLY multiplication since there are other operations involved.
dev-unsubscribe@spark.apache.org><mailto:
dev-help@spark.apache.org<mailto:dev-help@spark.apache.org><mailto:

--
Best regards,
Sam
 
 


"
Takeshi Yamamuro <linguin.m.s@gmail.com>,"Fri, 22 Jan 2016 18:57:52 +0900",Re: Spark SQL: Avoid shuffles when data is already partitioned on disk,Reynold Xin <rxin@databricks.com>,"My bad, thanks.




-- 
---
Takeshi Yamamuro
"
BrandonBradley <bradleytastic@gmail.com>,"Fri, 22 Jan 2016 07:29:13 -0700 (MST)",Re: Spark 1.6.1,dev@spark.apache.org,"I'd like more complete Postgres JDBC support for ArrayType before the next
release. Some of them are still broken in 1.6.0. It would save me much time.

Please see SPARK-12747 @ https://issues.apache.org/jira/browse/SPARK-12747

Cheers!
Brandon Bradley



--

---------------------------------------------------------------------


"
Sam Halliday <sam.halliday@gmail.com>,"Fri, 22 Jan 2016 18:14:01 +0000",Re: GPU Acceleration of Spark Logistic Regression and Other MLlib libraries,Rajesh Bordawekar <bordaw@us.ibm.com>,"Hi all,

(I'm author of netlib-java)

Interesting to see this discussion come to life again.

JNI is quite limiting: pinning (or critical array access) essentially
disables the GC for the whole JVM for the duration of the native call. I
can justify this for CPU heavy tasks because frankly there are not going to
be any free cycles to do anything other than BLAS when a dense matrix in
being crunched. For GPU tasks, you could get into some hairy problems and
achieve OOM just by doing basic work.

The other big problem with JNI is that this memory is either on the heap
(and subject to the whims of GC, large pause times in tenured cleanups) or
is a lightweight reference to a huge off-heap object and the GC might never
clean it up. There are hacks around this, but none are satisfactory.

More at my talk at Scala Exchange http://fommil.github.io/scalax14/#/

I have a roadmap to move netlib-java over to ByteBuffers as they solve all
the problems I have seen. It would be an effective rewrite (down to the
Fortran JVM compiler) and would change the java API in a systematic way,
but could support BLA-like GPUs at the same time. I would be willing to
migrate all the major libraries that are using netlib-java as part of this
effort.

However, I have no commercial incentive to perform this work, so I would be
seeking funding to do it. I will not be starting anything without funding.
Please contact me if you would be a willing stakeholder. I estimate it as a
6 month project: all major platforms, along with a CI build making it easy
to update, with testing.

,
of
 a
e
I
n
s
,
to
he
e
e
r,
n
an issue,
 take
f GPUs).
ation
ic
"
John Canny <canny@berkeley.edu>,"Fri, 22 Jan 2016 11:18:16 -0800","Re: GPU Acceleration of Spark Logistic Regression and Other MLlib
 libraries","Rajesh Bordawekar <bordaw@us.ibm.com>, alexander.ulanov@hpe.com","Hi Rajesh,
FYI, we are developing our own version of BIDMach integration with 
Spark, and achieving large gains over Spark MLLib for both CPU and GPU 
computation. You can find the project here:
https://github.com/BIDData/BIDMach_Spark

I'm not sure I follow your comment ""However, I think comparing 
end-to-end results would not be appropriate as we are affected by 
Spark's runtime costs; specifically, a single Spark function to convert 
RDD to arrays is very expensive and impacts our end-to-end performance 
severely (from 200+ gain for the GPU kernel to 25+ for the Spark library 
function). In contrast, BIDMach has a very light and efficient layer 
between their GPU kernel and the user program""

RDDs can be defined over any base class. In our Spark implementation, we 
use RDDs of our matrix objects and our own high-performance SerDe to 
pull data directly from HDFS, wrapped as a Sequencefile RDD. We get 
within 30% of the performance of BIDMach running on the native 
filesystem. e.g. for MNIST digit data, we get 300-500 GB/s per node from 
the filesystem. Spark helps with caching and data and code distribution, 
but we substitute the code that is compute or I/O intensive, and process 
data in large chunks to get near-roofline throughput overall. That said, 
I dont understand why it would take any longer to convert any other RDD 
format (once) to an RDD of matrices than to read the first RDD 
(converting and saving the latter with binary lz4 should be much faster 
than reading text or java serialized data). That read will always be a 
limit of performance, but I dont see why you wouldnt store data in HDFS 
or Spark filesystem in the faster format.

Some things to keep in mind with using GPUs in general for machine learning:
1. Dense BLAS account for a relatively small amount of overall data 
analysis in a web company (speaking as a half-timer at Yahoo right now). 
Sparse BLAS are far more important, and we've spent most of our time 
developing and improving support for sparse computation on GPUs. Its a 
lesser-known fact that GPUs have a significant main memory speed 
advantage which typically gives an order-of-magnitude advantage for 
sparse operations which dominate most algorithms.
2. To get full performance from the GPU you should do virtually all the 
computation there, so dense/sparse matrix operations, tensor operations, 
random number generation, transcendentals, slicing/dicing, math 
operators, sorting, merging etc. We have all those primitives, 
implemented both for CPU/GPU, dense and sparse matrices, single and 
double precision (while GPUs have significantly slower double-precision 
performance in most cases, *memory bandwidth* dominates sparse 
computation, and so sparse GPU double-precision arithmetic often has a 
*larger* speedup over CPUs than single precision), and many for integer 
and long fields. You can forget the CPU/GPU boundary if you think of the 
GPU as ""the computer"" and the rest of the machine as the IO system.
3. We have found a high-level API (In the style of numpy or Breeze, 
rather than BLAS-level) is very important for programmer productivity. 
BIDMach now has a very large suite of learning algorithms, and the cost 
to develop, test and deploy these is very low. Virtually all algorithms 
are written with abstract matix types and operations and have been 
tested to work on CPU/GPU, sparse or dense inputs, single or double 
precision arithmetic. With BIDMach_Spark we are working to erase the 
boundary between single-node and cluster execution by running a separate 
thread that synchronizes the local model in the background.
4. Memory management is a headache for GPUs. You can always do explicit 
storage management, but complex machine learning algorithms are, well, 
complex and its a distraction to have to figure out the lifetimes of 
dozens of matrix objects. The converse is to take a piece of Matlab or 
scipy code with ""a = b * c"" operations and run it as is. We strongly 
advocate for the latter which requires a memory management strategy. We 
have found that caching works great for iterative ML algorithms, whereas 
any simple version of GC doesnt. BIDMach was built from the ground up 
using this caching scheme. I'm not sure how you're handling this, since 
Spark was written assuming a GC to manage Breeze matrices.

Lastly, the single most important factor to improve Spark performance is 
fast distributed model updates. Spark has very limited support for this 
(via aggregation on the driver node), but it is already the bottleneck 
for many Spark algorithms. And in our experience, its not possible with 
Spark's batch algorithms to get comparable statistical efficiency 
(number of passes over the data for given accuracy) for logistic 
regression say on newsgroup classification, with a state-of-art 
minibatch implementation, e.g. VW or BIDMach. We have tried with Spark 
SGD Logistic regression and LBFGS, but have not been able to get close. 
It would be good to see what accuracies you are finding. Most competing 
systems, e.g. parameter server systems, are largely optimzed to do such 
minibatch model updates.  We recently published a rooflined Sparse 
allreduce, which you can think of as an optimized parameter server that 
uses the original nodes to synchronize models. Its a pure-java 
implementation which produces near-network-limit performance on EC2 and 
allows distributed model updates on a timescale of fractions of a second.
Huasha Zhao and John Canny *Kylix: A Sparse Allreduce for Commodity 
Clusters * Proc. Int. Conference on Parallel Processing (ICPP 2014) 
(PDF) <http://www.cs.berkeley.edu/%7Ejfc/papers/14/Kylix.pdf>
This is not integrated with our BIDMach_Spark system (yet), but that's 
the main goal of the project. With that in place, Spark should be able 
to achieve near-optimal multi-node speedups for most state-of-the-art ML 
algorithms. SO far we have implemented a batch algorithm (K-Means) which 
doesnt require minibatch updating to validate I/O and model and code 
distribution in Spark (which requires every BIDMach class to be 
serializable, including all the GPU matrix classes which we have done), 
and gives the numbers I mentioned above.

-John



"
Ryan Williams <ryan.blake.williams@gmail.com>,"Sat, 23 Jan 2016 21:40:42 +0000",Latency due to driver fetching sizes of output statuses,"""dev@spark.apache.org"" <dev@spark.apache.org>","I have a recursive algorithm that performs a few jobs on successively
smaller RDDs, and then a few more jobs on successively larger RDDs as the
recursion unwinds, resulting in a somewhat deeply-nested (a few dozen
levels) RDD lineage.

I am observing significant delays starting jobs while the
MapOutputTrackerMaster calculates the sizes of the output statuses for all
previous shuffles. By the end of my algorithm's execution, the driver
spends about a minute doing this before each job, during which time my
entire cluster is sitting idle. This output-status info is the same every
time it computes it, no executors have joined or left the cluster.

In this gist
<https://gist.github.com/ryan-williams/445ef8736a688bd78edb#file-job-108>
you can see two jobs stalling for almost a minute each between ""Starting
job:"" and ""Got job""; with larger input datasets my RDD lineages and this
latency would presumably only grow.

Additionally, the ""DAG Visualization"" on the job page of the web UI shows a
huge horizontal-scrolling lineage of thousands of stages, indicating that
the driver is tracking far more information than would seem necessary.

I'm assuming the short answer is that I need to truncate RDDs' lineage, and
the only way to do that is by checkpointing them to disk. I've done that
and it avoids this issue, but means that I am now serializing my entire
dataset to disk dozens of times during the course of execution, which feels
unnecessary/wasteful.

Is there a better way to deal with this scenario?

Thanks,

-Ryan
"
Mark Hamstra <mark@clearstorydata.com>,"Sat, 23 Jan 2016 13:53:49 -0800",Re: Latency due to driver fetching sizes of output statuses,Ryan Williams <ryan.blake.williams@gmail.com>,"Do all of those thousands of Stages end up being actual Stages that need to
be computed, or are the vast majority of them eventually ""skipped"" Stages?
If the latter, then there is the potential to modify the DAGScheduler to
avoid much of this behavior:
https://issues.apache.org/jira/browse/SPARK-10193
https://github.com/apache/spark/pull/8427


"
Ryan Williams <ryan.blake.williams@gmail.com>,"Sat, 23 Jan 2016 22:09:41 +0000",Re: Latency due to driver fetching sizes of output statuses,Mark Hamstra <mark@clearstorydata.com>,"yea, they're all skipped, here's a gif
<http://f.cl.ly/items/413l3k363u290U173W00/Screen%20Recording%202016-01-23%20at%2005.08%20PM.gif>
scrolling through the DAG viz.

Thanks for the JIRA pointer, I'll keep an eye on that one!


"
"""wangzhenhua (G)"" <wangzhenhua@huawei.com>","Sun, 24 Jan 2016 13:52:24 +0000",timeout in shuffle problem,dev <dev@spark.apache.org>,"Hi,

I have a problem of time out in shuffle, it happened after shuffle write and at the start of shuffle read,
logs on driver and executors are shown as below. Spark version is 1.5. Looking forward to your replys. Thanks!

logs on driver only have warnings:
WARN TaskSetManager: Lost task 38.0 in stage 27.0 (TID 127459, linux-162): FetchFailed(BlockManagerId(66, 172.168.100.12, 23028), shuffleId=9, mapId=55, reduceId=38, message=
org.apache.spark.shuffle.FetchFailedException: java.util.concurrent.TimeoutException: Timeout waiting for task.
        at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:321)
        at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:306)
        at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:173)
        at org.apache.spark.sql.execution.TungstenSort.org$apache$spark$sql$execution$TungstenSort$$executePartition$1(sort.scala:160)
        at org.apache.spark.sql.execution.TungstenSort$$anonfun$doExecute$4.apply(sort.scala:169)
        at org.apache.spark.sql.execution.TungstenSort$$anonfun$doExecute$4.apply(sort.scala:169)
        at org.apache.spark.rdd.MapPartitionsWithPreparationRDD.compute(MapPartitionsWithPreparationRDD.scala:64)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
        at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:99)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
        at org.apache.spark.rdd.MapPartitionsWithPreparationRDD.compute(MapPartitionsWithPreparationRDD.scala:63)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
        at org.apache.spark.scheduler.Task.run(Task.scala:88)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: java.util.concurrent.TimeoutException: Timeout waiting for task.
        at org.spark-project.guava.base.Throwables.propagate(Throwables.java:160)
        at org.apache.spark.network.client.TransportClient.sendRpcSync(TransportClient.java:196)
        at org.apache.spark.network.sasl.SaslClientBootstrap.doBootstrap(SaslClientBootstrap.java:76)
        at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:205)
        at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:156)
        at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:88)
        at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
        at org.apache.spark.network.shuffle.RetryingBlockFetcher.start(RetryingBlockFetcher.java:120)
        at org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:97)
        at org.apache.spark.storage.ShuffleBlockFetcherIterator.sendRequest(ShuffleBlockFetcherIterator.scala:152)
        at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:301)
        ... 57 more
Caused by: java.util.concurrent.TimeoutException: Timeout waiting for task.
        at org.spark-project.guava.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:276)
        at org.spark-project.guava.util.concurrent.AbstractFuture.get(AbstractFuture.java:96)
        at org.apache.spark.network.client.TransportClient.sendRpcSync(TransportClient.java:192)
        ... 66 more

logs on executor have errors like:
ERROR | [shuffle-client-1] | Still have 1 requests outstanding when connection from /172.168.100.12:23908 is closed | org.apache.spark.network.client.TransportResponseHandler.channelUnregistered(TransportResponseHandler.java:102)

________________________________
best regards,
-zhenhua
"
Ted Yu <yuzhihong@gmail.com>,"Sun, 24 Jan 2016 13:58:00 -0800",Re: timeout in shuffle problem,"""wangzhenhua (G)"" <wangzhenhua@huawei.com>","Cycling past bits:
http://search-hadoop.com/m/q3RTtU5CRU1KKVA42&subj=RE+shuffle+FetchFailedException+in+spark+on+YARN+job


"
Ewan Leith <ewan.leith@realitymine.com>,"Mon, 25 Jan 2016 12:04:30 +0000",RE: Spark 1.6.1,"BrandonBradley <bradleytastic@gmail.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","Hi Brandon,

It's relatively straightforward to try out different type options for this in the spark-shell, try pasting the attached code into spark-shell before you make a normal postgres JDBC connection.  

You can then experiment with the mappings without recompiling Spark or anything like that, and you can embed the same code in your own packages, outside of the main Spark releases.

Thanks,
Ewan

release. Some of them are still broken in 1.6.0. It would save me much time.

Please see SPARK-12747 @ https://issues.apache.org/jira/browse/SPARK-12747

Cheers!
Brandon Bradley



--
3.nabble.com/Spark-1-6-1-tp16009p16082.html
om.

---------------------------------------------------------------------
mands, e-mail: dev-help@spark.apache.org

import java.sql.{Connection, Types}

import org.apache.spark.sql.types._
import org.apache.spark.sql.jdbc
import org.apache.spark.sql.jdbc.JdbcDialects
import org.apache.spark.sql.jdbc.JdbcDialect
import org.apache.spark.sql.jdbc.JdbcType

def toCatalystType(typeName: String): Option[DataType] = typeName match {
    case ""bool"" => Some(BooleanType)
    case ""bit"" => Some(BinaryType)
    case ""int2"" => Some(ShortType)
    case ""int4"" => Some(IntegerType)
    case ""int8"" | ""oid"" => Some(LongType)
    case ""float4"" => Some(FloatType)
    case ""money"" | ""float8"" => Some(DoubleType)
    case ""text"" | ""varchar"" | ""char"" | ""cidr"" | ""inet"" | ""json"" | ""jsonb"" | ""uuid"" =>
      Some(StringType)
    case ""bytea"" => Some(BinaryType)
    case ""timestamp"" | ""timestamptz"" | ""time"" | ""timetz"" => Some(TimestampType)
    case ""date"" => Some(DateType)
    case ""numeric"" => Some(DecimalType.SYSTEM_DEFAULT)
    case _ => None
}

case object PostgresDialect extends JdbcDialect {
  override def canHandle(url: String): Boolean = url.startsWith(""jdbc:postgresql"")

  override def getCatalystType(
      sqlType: Int, typeName: String, size: Int, md: MetadataBuilder): Option[DataType] = {
    if (sqlType == Types.BIT && typeName.equals(""bit"") && size != 1) {
      Some(BinaryType)
    } else if (sqlType == Types.OTHER) {
      toCatalystType(typeName).filter(_ == StringType)
    } else if (sqlType == Types.ARRAY && typeName.length > 1 && typeName(0) == '_') {
      toCatalystType(typeName.drop(1)).map(ArrayType(_))
    } else None
  }
}

JdbcDialects.registerDialect(PostgresDialect)
---------------------------------------------------------------------"
"""james.green9@baesystems.com"" <james.green9@baesystems.com>","Tue, 26 Jan 2016 09:33:46 +0000",spark hivethriftserver problem on  1.5.0 -> 1.6.0 upgrade  ,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi

I posted this on the user list yesterday,  I am posting it here now because on further investigation I am pretty sure this is a bug:


On upgrade from 1.5.0 to 1.6.0 I have a problem with the hivethriftserver2, I have this code:

val hiveContext = new HiveContext(SparkContext.getOrCreate(conf));

val thing = hiveContext.read.parquet(""hdfs://dkclusterm1.imp.net:8020/user/jegreen1/ex208"")

thing.registerTempTable(""thing"")

HiveThriftServer2.startWithContext(hiveContext)


When I start things up on the cluster my hive-site.xml is found ‚Äì I can see that the metastore connects:


INFO  metastore - Trying to connect to metastore with URI thrift://dkclusterm2.imp.net:9083
INFO  metastore - Connected to metastore.


But then later on the thrift server seems not to connect to the remote hive metastore but to start a derby instance instead:

INFO  AbstractService - Service:CLIService is started.
INFO  ObjectStore - ObjectStore, initialize called
INFO  Query - Reading in results for query ""org.datanucleus.store.rdbms.query.SQLQuery@0"" since the connection used is closing
INFO  MetaStoreDirectSql - Using direct SQL, underlying DB is DERBY
INFO  ObjectStore - Initialized ObjectStore
INFO  HiveMetaStore - 0: get_databases: default
INFO  audit - ugi=jegreen1      ip=unknown-ip-addr      cmd=get_databases: default
INFO  HiveMetaStore - 0: Shutting down the object store...
INFO  audit - ugi=jegreen1      ip=unknown-ip-addr      cmd=Shutting down the object store...
INFO  HiveMetaStore - 0: Metastore shutdown complete.
INFO  audit - ugi=jegreen1      ip=unknown-ip-addr      cmd=Metastore shutdown complete.
INFO  AbstractService - Service:ThriftBinaryCLIService is started.
INFO  AbstractService - Service:HiveServer2 is started.

On 1.5.0 the same bit of the log reads:

INFO  AbstractService - Service:CLIService is started.
INFO  metastore - Trying to connect to metastore with URI thrift://dkclusterm2.imp.net:9083      ******* ie 1.5.0 connects to remote hive
INFO  metastore - Connected to metastore.
INFO  AbstractService - Service:ThriftBinaryCLIService is started.
INFO  AbstractService - Service:HiveServer2 is started.
INFO  ThriftCLIService - Starting ThriftBinaryCLIService on port 10000 with 5...500 worker threads



So if I connect to this with JDBC I can see all the tables on the hive server ‚Äì but not anything temporary ‚Äì I guess they are going to derby.

I see someone on the databricks website is also having this problem.


Thanks

James
Please consider the environment before printing this email. This message should be regarded as confidential. If you have received this email in error please notify the sender and destroy it immediately. Statements of intent shall only become binding when confirmed in hard copy by an authorised signatory. The contents of this email may relate to dealings with other companies under the control of BAE Systems Applied Intelligence Limited, details of which can be found at http://www.baesystems.com/Businesses/index.htm.
"
=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"Tue, 26 Jan 2016 17:11:34 +0100",Re: Unable to compile and test Spark in IntelliJ,Hyukjin Kwon <gurwls223@gmail.com>,"

I'm using Eclipse, but all I had to do in order to build in the IDE was to
add `target/generated-sources/antlr3` to the project sources, after
building once in Sbt. You probably have the sources there already.

iulian





-- 

--
Iulian Dragos

------
Reactive Apps on the JVM
www.typesafe.com
"
Yin Huai <yhuai@databricks.com>,"Tue, 26 Jan 2016 09:47:44 -0800",Re: spark hivethriftserver problem on 1.5.0 -> 1.6.0 upgrade,"""james.green9@baesystems.com"" <james.green9@baesystems.com>","Can you post more logs, specially lines around ""Initializing execution hive
..."" (this is for an internal used fake metastore and it is derby) and
""Initializing HiveMetastoreConnection version ..."" (this is for the real
metastore. It should be your remote one)? Also, those temp tables are
stored in the memory and are associated with a HiveContext. If you can not
see temp tables, it usually means that the HiveContext that you used with
JDBC was different from the one used to create the temp table. However, in
your case, you are using HiveThriftServer2.startWithContext(hiveContext).
So, it will be good to provide more logs and see what happened.

Thanks,

Yin


 I can
abases:
g down
re
ing to derby.
e
"
ndjido@gmail.com,"Tue, 26 Jan 2016 20:02:44 +0100",Issue with spark-shell in yarn mode,"user@spark.apache.org, dev@spark.apache.org","Hi folks,

ode:

1) sc.parallelize(Array(1,2,3,3,3,3,4)).collect()

2) sc.parallelize(Array(1,2,3,3,3,3,4)).map( x => (x, 1)).collect()

1) works well whereas 2) raises the following exception: 

Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
        at scala.Option.foreach(Option.scala:236)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
ve(DAGScheduler.scala:1640)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)
        at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1314)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
        at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
        at org.apache.spark.rdd.RDD.take(RDD.scala:1288)
        at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:28)
        at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:33)
        at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:35)
        at $iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:37)
        at $iwC$$iwC$$iwC$$iwC.<init>(<console>:39)
        at $iwC$$iwC$$iwC.<init>(<console>:41)
        at $iwC$$iwC.<init>(<console>:43)
        at $iwC.<init>(<console>:45)
        at <init>(<console>:47)
        at .<init>(<console>:51)
        at .<clinit>(<console>)
        at .<init>(<console>:7)
        at .<clinit>(<console>)
        at $print(<console>)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)
        at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)
        at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)
        at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)
        at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)
        at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:857)
        at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:902)
        at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:814)
        at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:657)
        at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:665)
        at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$loop(SparkILoop.scala:670)
        at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply$mcZ$sp(SparkILoop.scala:997)
        at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)
        at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)
        at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
        at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$process(SparkILoop.scala:945)
        at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1059)
        at org.apache.spark.repl.Main$.main(Main.scala:31)
        at org.apache.spark.repl.Main.main(Main.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)
        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.ClassNotFoundException: $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$1
        at org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:84)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:278)
        at org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:68)
        at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1612)
        at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1517)
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1771)
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
        at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
        at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:115)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
        at org.apache.spark.scheduler.Task.run(Task.scala:89)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
  

Any idea?

Cheers,

Ardo 
---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Tue, 26 Jan 2016 23:19:10 +0000",Re: Spark 2.0.0 release plan,Koert Kuipers <koert@tresata.com>,"I think it will come significantly later -- or else we'd be at code
freeze for 2.x in a few days. I haven't heard anyone discuss this
officially but had batted around May or so instead informally in
conversation. Does anyone have a particularly strong opinion on that?
That's basically an extra 3 month period.

https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage


---------------------------------------------------------------------


"
Joseph Bradley <joseph@databricks.com>,"Tue, 26 Jan 2016 15:44:33 -0800",Re: Spark LDA model reuse with new set of data,doruchiulan <doru.chiulan@gmail.com>,"Hi,

This is more a question for the user list, not the dev list, so I'll CC
user.

If you're using mllib.clustering.LDAModel (RDD API), then can you make sure
you're using a LocalLDAModel (or convert to it from DistributedLDAModel)?
You can then call topicDistributions() on the new data.

If you're using ml.clustering.LDAModel (DataFrame API), then you can call
transform() on new data.

Does that work?

Joseph


"
"""Mao, Wei"" <wei.mao@intel.com>","Wed, 27 Jan 2016 03:14:34 +0000",RE: Unable to compile and test Spark in IntelliJ,"""iulian.dragos@typesafe.com"" <iulian.dragos@typesafe.com>, Hyukjin Kwon
	<gurwls223@gmail.com>","I used to meet same compile error within Intellij, and resolved by click:

View --> Tool Windows --> Maven Projects --> Spark Project Catalyst --> Plugins --> antlr3, then remake project

Thanks,
William Mao

From: Iulian Drago»ô [mailto:iulian.dragos@typesafe.com]
Sent: Wednesday, January 27, 2016 12:12 AM
To: Hyukjin Kwon
Cc: dev@spark.apache.org
Subject: Re: Unable to compile and test Spark in IntelliJ



On Tue, Jan 19, 2016 at 6:06 AM, Hyukjin Kwon <gurwls223@gmail.com<mailto:gurwls223@gmail.com>> wrote:
Hi all,

I usually have been working with Spark in IntelliJ.
Before this PR, https://github.com/apache/spark/commit/7cd7f2202547224593517b392f56e49e4c94cabc for `[SPARK-12575][SQL] Grammar parity with existing SQL parser`. I was able to just open the project and then run some tests with IntelliJ Run button.

However, it looks that PR adds some ANTLR files for parsing and I cannot run the tests as I did. So, I ended up with doing this by mvn compile first and then running some tests with IntelliJ.

I can still run some tests with sbt or maven in comment line but this is a bit inconvenient. I just want to run some tests as I did in IntelliJ.

I followed this https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools several times but it still emits some exceptions such as

Error:(779, 34) not found: value SparkSqlParser
    case ast if ast.tokenType == SparkSqlParser.TinyintLiteral =>
                                 ^

and I still should run mvn compile or mvn test first for them.

Is there any good way to run some Spark tests within IntelliJ as I did before?

I'm using Eclipse, but all I had to do in order to build in the IDE was to add `target/generated-sources/antlr3` to the project sources, after building once in Sbt. You probably have the sources there already.

iulian


Thanks!



--

--
Iulian Dragos

------
Reactive Apps on the JVM
www.typesafe.com<http://www.typesafe.com>

"
Hyukjin Kwon <gurwls223@gmail.com>,"Wed, 27 Jan 2016 12:41:20 +0900",RE: Unable to compile and test Spark in IntelliJ,"""Mao, Wei"" <wei.mao@intel.com>","Thanks guys! That works good.

yst √† Plugins √†
:
94cabc for
to
st
a
o
"
Koert Kuipers <koert@tresata.com>,"Tue, 26 Jan 2016 23:28:51 -0500",Re: Spark 2.0.0 release plan,Sean Owen <sowen@cloudera.com>,"thanks thats all i needed


"
doruchiulan <doru.chiulan@gmail.com>,"Wed, 27 Jan 2016 00:07:40 -0700 (MST)",Re: Spark LDA model reuse with new set of data,dev@spark.apache.org,"Yes, just saw myself the same thing last night. Thanks



--

---------------------------------------------------------------------


"
Kashish Jain <Kashish.Jain@guavus.com>,"Wed, 27 Jan 2016 07:53:34 +0000","""Use Default"" in spark","""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,


I am trying to upgrade to spark-1.5.1 with hadoop 2.7.1 and hive 1.2.1 from spark-1.3.1.

But with the above spark-assembly, the ""USE DEFAULT"" functionality seems to be broken with the message

""

   Cannot recognize input near 'default' '<EOF>' '<EOF>' in switch database statement; line 1 pos 4

   NoViableAltException(81@[])

   at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.identifier(HiveParser_IdentifiersParser.java:11577)

   at org.apache.hadoop.hive.ql.parse.HiveParser.identifier(HiveParser.java:46055)

""


According to my analysis this may be due to the removal of ""KW_DEFAULT"" keyword from the grammar of Hive parser according to the bug https://issues.apache.org/jira/browse/HIVE-6617


Tried adding the KW_DEFAULT keyword back to the HiveParser.g and HiveLexer.g but I guess I am missing something.


Can someone confirm if this is indeed an issue or if there is any fix planned for this in the upcoming releases.


Thanks

Kashish Jain

"
Jacek Laskowski <jacek@japila.pl>,"Wed, 27 Jan 2016 09:21:35 +0100",BUILD FAILURE at spark-sql_2.11?!,dev <dev@spark.apache.org>,"Hi,

Tried to build the sources today with Scala 2.11 twice and it failed.
No local changes. Restarted zinc.

Can anyone else confirm it?

Since the error is buried in the logs I'm asking now without offering
more information (before I catch the cause) so I or *the issue* get
corrected (whatever first :)). Thanks!

Pozdrawiam,
Jacek

Jacek Laskowski | https://medium.com/@jaceklaskowski/
Mastering Apache Spark
==> https://jaceklaskowski.gitbooks.io/mastering-apache-spark/
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Wed, 27 Jan 2016 09:47:30 +0100",Re: BUILD FAILURE at spark-sql_2.11?!,dev <dev@spark.apache.org>,"Hi,

My very rough investigation has showed that the commit to may have
broken the build was
https://github.com/apache/spark/commit/555127387accdd7c1cf236912941822ba8af0a52
(nongli committed with rxin 7 hours ago).

Found a fix and building the source again...

Pozdrawiam,
Jacek

Jacek Laskowski | https://medium.com/@jaceklaskowski/
Mastering Apache Spark
==> https://jaceklaskowski.gitbooks.io/mastering-apache-spark/
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Wed, 27 Jan 2016 10:15:27 +0100",Re: BUILD FAILURE at spark-sql_2.11?!,dev <dev@spark.apache.org>,"Hi,

Pull request submitted
https://github.com/apache/spark/pull/10946/files. Please review and
merge.

Pozdrawiam,
Jacek

Jacek Laskowski | https://medium.com/@jaceklaskowski/
Mastering Apache Spark
==> https://jaceklaskowski.gitbooks.io/mastering-apache-spark/
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
,"Wed, 27 Jan 2016 10:29:16 +0100",Re: BUILD FAILURE at spark-sql_2.11?!,dev@spark.apache.org,"Thanks Jacek,

I have the same issue here.

Regards
JB


-- 
jbonofre@apache.org
http://blog.nanthrax.net
Talend - http://www.talend.com

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Wed, 27 Jan 2016 02:12:55 -0800",Re: BUILD FAILURE at spark-sql_2.11?!,Jacek Laskowski <jacek@japila.pl>,"Strangely both Jenkins jobs showed green status:

https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Compile/job/SPARK-master-COMPILE-sbt-SCALA-2.11/
https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Compile/job/SPARK-master-COMPILE-MAVEN-SCALA-2.11/


"
Akhil Das <akhil@sigmoidanalytics.com>,"Wed, 27 Jan 2016 17:32:38 +0530",Re: Generate Amplab queries set,sara mustafa <eng.sara.mustafa@gmail.com>,"Have a look at the TPC-H queries, I found this repository with the quries.
https://github.com/ssavvides/tpch-spark

Thanks
Best Regards


"
Akhil Das <akhil@sigmoidanalytics.com>,"Wed, 27 Jan 2016 17:55:16 +0530",Re: Using distinct count in over clause,=?UTF-8?B?5rGq5rSL?= <tiandiwoxin@icloud.com>,"Does it support over? I couldn't find it in the documentation
http://spark.apache.org/docs/latest/sql-programming-guide.html#supported-hive-features

Thanks
Best Regards


53ÔºåÊ±™Ê¥ã <tiandiwoxin@icloud.com> ÂÜôÈÅìÔºö
sent in
"
=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@questtec.nl>,"Wed, 27 Jan 2016 13:33:47 +0100",Re: Using distinct count in over clause,Akhil Das <akhil@sigmoidanalytics.com>,"Hi,

We currently do not support distinct clauses in window functions. Nor is
such functionality planned.

Spark 2.0 uses native spark UDAFs (instead of Hive window functions) and
allows you to use your own UDAFs, it is trivial to implement a distinct
count/sum in that case.

Kind regards,

Herman van H√∂vell

2016-01-27 13:25 GMT+01:00 Akhil Das <akhil@sigmoidanalytics.com>:

hive-features
:53ÔºåÊ±™Ê¥ã <tiandiwoxin@icloud.com> ÂÜôÈÅìÔºö
esent in
n
"
Hamel Kothari <hamelkothari@gmail.com>,"Wed, 27 Jan 2016 14:21:17 +0000",Re: timeout in shuffle problem,"Ted Yu <yuzhihong@gmail.com>, ""wangzhenhua (G)"" <wangzhenhua@huawei.com>","Are you running on YARN? Another possibility here is that your shuffle
managers are facing GC pain and becoming less responsive, thus missing
timeouts. Can you try increasing the memory on the node managers and see if
that helps?


"
"""james.green9@baesystems.com"" <james.green9@baesystems.com>","Wed, 27 Jan 2016 15:21:32 +0000",RE: spark hivethriftserver problem on 1.5.0 -> 1.6.0 upgrade,Yin Huai <yhuai@databricks.com>,"
Thanks Yin,  here are the logs:



INFO  SparkContext - Added JAR file:/home/jegreen1/mms/zookeeper-3.4.6.jar at http://10.39.65.122:38933/jars/zookeeper-3.4.6.jar with timestamp 1453907484092
INFO  SparkContext - Added JAR file:/home/jegreen1/mms/mms-http-0.2-SNAPSHOT.jar at http://10.39.65.122:38933/jars/mms-http-0.2-SNAPSHOT.jar with timestamp 1453907484093
INFO  Executor - Starting executor ID driver on host localhost
INFO  Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41220.
INFO  NettyBlockTransferService - Server created on 41220
INFO  BlockManagerMaster - Trying to register BlockManager
INFO  BlockManagerMasterEndpoint - Registering block manager localhost:41220 with 511.1 MB RAM, BlockManagerId(driver, localhost, 41220)
INFO  BlockManagerMaster - Registered BlockManager
INFO  HiveContext - Initializing execution hive, version 1.2.1
INFO  ClientWrapper - Inspected Hadoop version: 2.6.0
INFO  ClientWrapper - Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0
WARN  HiveConf - HiveConf of name hive.enable.spark.execution.engine does not exist
INFO  HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
INFO  ObjectStore - ObjectStore, initialize called
INFO  Persistence - Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
INFO  Persistence - Property datanucleus.cache.level2 unknown - will be ignored
WARN  HiveConf - HiveConf of name hive.enable.spark.execution.engine does not exist
INFO  ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes=""Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order""
INFO  Datastore - The class ""org.apache.hadoop.hive.metastore.model.MFieldSchema"" is tagged as ""embedded-only"" so does not have its own datastore table.
INFO  Datastore - The class ""org.apache.hadoop.hive.metastore.model.MOrder"" is tagged as ""embedded-only"" so does not have its own datastore table.
INFO  Datastore - The class ""org.apache.hadoop.hive.metastore.model.MFieldSchema"" is tagged as ""embedded-only"" so does not have its own datastore table.
INFO  Datastore - The class ""org.apache.hadoop.hive.metastore.model.MOrder"" is tagged as ""embedded-only"" so does not have its own datastore table.
INFO  MetaStoreDirectSql - Using direct SQL, underlying DB is DERBY
INFO  ObjectStore - Initialized ObjectStore
WARN  ObjectStore - Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
WARN  ObjectStore - Failed to get database default, returning NoSuchObjectException
INFO  HiveMetaStore - Added admin role in metastore
INFO  HiveMetaStore - Added public role in metastore
INFO  HiveMetaStore - No user is added in admin role, since config is empty
INFO  HiveMetaStore - 0: get_all_databases
INFO  audit - ugi=jegreen1      ip=unknown-ip-addr      cmd=get_all_databases
INFO  HiveMetaStore - 0: get_functions: db=default pat=*
INFO  audit - ugi=jegreen1      ip=unknown-ip-addr      cmd=get_functions: db=default pat=*
INFO  Datastore - The class ""org.apache.hadoop.hive.metastore.model.MResourceUri"" is tagged as ""embedded-only"" so does not have its own datastore table.
WARN  NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
INFO  SessionState - Created local directory: /tmp/9b102c97-c3f4-4d92-b722-0a2e257d3b5b_resources
INFO  SessionState - Created HDFS directory: /tmp/hive/jegreen1/9b102c97-c3f4-4d92-b722-0a2e257d3b5b
INFO  SessionState - Created local directory: /tmp/jegreen1/9b102c97-c3f4-4d92-b722-0a2e257d3b5b
INFO  SessionState - Created HDFS directory: /tmp/hive/jegreen1/9b102c97-c3f4-4d92-b722-0a2e257d3b5b/_tmp_space.db
WARN  HiveConf - HiveConf of name hive.enable.spark.execution.engine does not exist
INFO  HiveContext - default warehouse location is /user/hive/warehouse
INFO  HiveContext - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
INFO  ClientWrapper - Inspected Hadoop version: 2.6.0
INFO  ClientWrapper - Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0
WARN  HiveConf - HiveConf of name hive.enable.spark.execution.engine does not exist
INFO  metastore - Trying to connect to metastore with URI thrift://dkclusterm2.imp.net:9083
INFO  metastore - Connected to metastore.
INFO  SessionState - Created local directory: /tmp/7e230580-37af-47d3-81cc-eb4829b8da62_resources
INFO  SessionState - Created HDFS directory: /tmp/hive/jegreen1/7e230580-37af-47d3-81cc-eb4829b8da62
INFO  SessionState - Created local directory: /tmp/jegreen1/7e230580-37af-47d3-81cc-eb4829b8da62
INFO  SessionState - Created HDFS directory: /tmp/hive/jegreen1/7e230580-37af-47d3-81cc-eb4829b8da62/_tmp_space.db
INFO  ParquetRelation - Listing hdfs://dkclusterm1.imp.net:8020/user/jegreen1/ex208 on driver
INFO  SparkContext - Starting job: parquet at ThriftTest.scala:39
INFO  DAGScheduler - Got job 0 (parquet at ThriftTest.scala:39) with 32 output partitions
INFO  DAGScheduler - Final stage: ResultStage 0 (parquet at ThriftTest.scala:39)
INFO  DAGScheduler - Parents of final stage: List()
INFO  DAGScheduler - Missing parents: List()
INFO  DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at ThriftTest.scala:39), which has no missing parents
INFO  MemoryStore - Block broadcast_0 stored as values in memory (estimated size 65.5 KB, free 65.5 KB)
INFO  MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.9 KB, free 88.3 KB)
INFO  BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:41220 (size: 22.9 KB, free: 511.1 MB)
INFO  SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1006
INFO  DAGScheduler - Submitting 32 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at ThriftTest.scala:39)
INFO  TaskSchedulerImpl - Adding task set 0.0 with 32 tasks
INFO  TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 6528 bytes)
INFO  TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1,PROCESS_LOCAL, 6528 bytes)
INFO  TaskSetManager - Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2,PROCESS_LOCAL, 6528 bytes)
INFO  TaskSetManager - Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3,PROCESS_LOCAL, 6528 bytes)
INFO  TaskSetManager - Starting task 4.0 in stage 0.0 (TID 4, localhost, partition 4,PROCESS_LOCAL, 6528 bytes)
INFO  TaskSetManager - Starting task 5.0 in stage 0.0 (TID 5, localhost, partition 5,PROCESS_LOCAL, 6528 bytes)


From: Yin Huai [mailto:yhuai@databricks.com]
Sent: 26 January 2016 17:48
To: Green, James (UK Guildford)
Cc: dev@spark.apache.org
Subject: Re: spark hivethriftserver problem on 1.5.0 -> 1.6.0 upgrade

Can you post more logs, specially lines around ""Initializing execution hive ..."" (this is for an internal used fake metastore and it is derby) and ""Initializing HiveMetastoreConnection version ..."" (this is for the real metastore. It should be your remote one)? Also, those temp tables are stored in the memory and are associated with a HiveContext. If you can not see temp tables, it usually means that the HiveContext that you used with JDBC was different from the one used to create the temp table. However, in your case, you are using HiveThriftServer2.startWithContext(hiveContext). So, it will be good to provide more logs and see what happened.

Thanks,

Yin

On Tue, Jan 26, 2016 at 1:33 AM, james.green9@baesystems.com<mailto:james.green9@baesystems.com> <james.green9@baesystems.com<mailto:james.green9@baesystems.com>> wrote:
Hi

I posted this on the user list yesterday,  I am posting it here now because on further investigation I am pretty sure this is a bug:


On upgrade from 1.5.0 to 1.6.0 I have a problem with the hivethriftserver2, I have this code:

val hiveContext = new HiveContext(SparkContext.getOrCreate(conf));

val thing = hiveContext.read.parquet(""hdfs://dkclusterm1.imp.net:8020/user/jegreen1/ex208<http://dkclusterm1.imp.net:8020/user/jegreen1/ex208>"")

thing.registerTempTable(""thing"")

HiveThriftServer2.startWithContext(hiveContext)


When I start things up on the cluster my hive-site.xml is found ‚Äì I can see that the metastore connects:


INFO  metastore - Trying to connect to metastore with URI thrift://dkclusterm2.imp.net:9083<http://dkclusterm2.imp.net:9083>
INFO  metastore - Connected to metastore.


But then later on the thrift server seems not to connect to the remote hive metastore but to start a derby instance instead:

INFO  AbstractService - Service:CLIService is started.
INFO  ObjectStore - ObjectStore, initialize called
INFO  Query - Reading in results for query ""org.datanucleus.store.rdbms.query.SQLQuery@0<mailto:org.datanucleus.store.rdbms.query.SQLQuery@0>"" since the connection used is closing
INFO  MetaStoreDirectSql - Using direct SQL, underlying DB is DERBY
INFO  ObjectStore - Initialized ObjectStore
INFO  HiveMetaStore - 0: get_databases: default
INFO  audit - ugi=jegreen1      ip=unknown-ip-addr      cmd=get_databases: default
INFO  HiveMetaStore - 0: Shutting down the object store...
INFO  audit - ugi=jegreen1      ip=unknown-ip-addr      cmd=Shutting down the object store...
INFO  HiveMetaStore - 0: Metastore shutdown complete.
INFO  audit - ugi=jegreen1      ip=unknown-ip-addr      cmd=Metastore shutdown complete.
INFO  AbstractService - Service:ThriftBinaryCLIService is started.
INFO  AbstractService - Service:HiveServer2 is started.

On 1.5.0 the same bit of the log reads:

INFO  AbstractService - Service:CLIService is started.
INFO  metastore - Trying to connect to metastore with URI thrift://dkclusterm2.imp.net:9083<http://dkclusterm2.imp.net:9083>      ******* ie 1.5.0 connects to remote hive
INFO  metastore - Connected to metastore.
INFO  AbstractService - Service:ThriftBinaryCLIService is started.
INFO  AbstractService - Service:HiveServer2 is started.
INFO  ThriftCLIService - Starting ThriftBinaryCLIService on port 10000 with 5...500 worker threads



So if I connect to this with JDBC I can see all the tables on the hive server ‚Äì but not anything temporary ‚Äì I guess they are going to derby.

I see someone on the databricks website is also having this problem.


Thanks

James
Please consider the environment before printing this email. This message should be regarded as confidential. If you have received this email in error please notify the sender and destroy it immediately. Statements of intent shall only become binding when confirmed in hard copy by an authorised signatory. The contents of this email may relate to dealings with other companies under the control of BAE Systems Applied Intelligence Limited, details of which can be found at http://www.baesystems.com/Businesses/index.htm.

Please consider the environment before printing this email. This message should be regarded as confidential. If you have received this email in error please notify the sender and destroy it immediately. Statements of intent shall only become binding when confirmed in hard copy by an authorised signatory. The contents of this email may relate to dealings with other companies under the control of BAE Systems Applied Intelligence Limited, details of which can be found at http://www.baesystems.com/Businesses/index.htm.
"
Vinayak Agrawal <vinayakagrawal88@gmail.com>,"Wed, 27 Jan 2016 11:43:43 -0500",Adding Naive Bayes sample code in Documentation,dev@spark.apache.org,"Hi,
I was reading through Spark ML package and I couldn't find Naive Bayes
examples documented on the spark documentation page.
http://spark.apache.org/docs/latest/ml-classification-regression.html

However, the API exists and can be used.
https://spark.apache.org/docs/1.5.2/api/python/pyspark.ml.html#module-pyspark.ml.classification

Can the examples be added in the latest documentation?

-- 
Vinayak Agrawal


""To Strive, To Seek, To Find and Not to Yield!""
~Lord Alfred Tennyson
"
Jakob Odersky <jakob@odersky.com>,"Wed, 27 Jan 2016 10:39:21 -0800",Mutiple spark contexts,dev@spark.apache.org,"A while ago, I remember reading that multiple active Spark contexts
per JVM was a possible future enhancement.
I was wondering if this is still on the roadmap, what the major
obstacles are and if I can be of any help in adding this feature?

regards,
--Jakob

---------------------------------------------------------------------


"
Ashish Soni <asoni.learn@gmail.com>,"Wed, 27 Jan 2016 13:41:31 -0500",Re: Mutiple spark contexts,Jakob Odersky <jakob@odersky.com>,"There is a property you need to set which is
spark.driver.allowMultipleContexts=true

Ashish


"
=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@questtec.nl>,"Wed, 27 Jan 2016 19:42:51 +0100",Re: Mutiple spark contexts,Ashish Soni <asoni.learn@gmail.com>,"Just out of curiousity. What is the use case for having multiple active
contexts in a single JVM?

Kind regards,

Herman van H√∂vell

2016-01-27 19:41 GMT+01:00 Ashish Soni <asoni.learn@gmail.com>:

"
Reynold Xin <rxin@databricks.com>,"Wed, 27 Jan 2016 10:40:50 -0800",Re: Mutiple spark contexts,Jakob Odersky <jakob@odersky.com>,"There are no major obstacles, just a million tiny obstacles that would take
forever to fix.



"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 27 Jan 2016 18:50:53 +0000",Re: Mutiple spark contexts,"=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@questtec.nl>, 
	Ashish Soni <asoni.learn@gmail.com>","There is a lengthy discussion about this on the JIRA:
https://issues.apache.org/jira/browse/SPARK-2243


:
"
Michael Armbrust <michael@databricks.com>,"Wed, 27 Jan 2016 11:55:49 -0800",Re: Spark 2.0.0 release plan,Daniel Siegmann <daniel.siegmann@teamaol.com>,"We do maintenance releases on demand when there is enough to justify doing
one.  I'm hoping to cut 1.6.1 soon, but have not had time yet.


"
Niranda Perera <niranda.perera@gmail.com>,"Thu, 28 Jan 2016 08:37:32 +0530",spark job scheduling,"""dev@spark.apache.org"" <dev@spark.apache.org>","hi all,

I have a few questions on spark job scheduling.

1. As I understand, the smallest unit of work an executor can perform. In
the 'fair' scheduler mode, let's say  a job is submitted to the spark ctx
which has a considerable amount of work to do in a task. While such a 'big'
task is running, can we still submit another smaller job (from a separate
thread) and get it done? or does that smaller job has to wait till the
bigger task finishes and the resources are freed from the executor?

2. When a job is submitted without setting a scheduler pool, the default
scheduler pool is assigned to it, which employs FIFO scheduling. but what
happens when we have the spark.scheduler.mode as FAIR, and if I submit jobs
without specifying a scheduler pool (which has FAIR scheduling)? would the
jobs still run in FIFO mode with the default pool?
essentially, for us to really set FAIR scheduling, do we have to assign a
FAIR scheduler pool?

best

-- 
Niranda
@n1r44 <https://twitter.com/N1R44>
+94-71-554-8430
https://pythagoreanscript.wordpress.com/
"
"""wangzhenhua (G)"" <wangzhenhua@huawei.com>","Thu, 28 Jan 2016 03:12:25 +0000",Re: Re: timeout in shuffle problem,"Hamel Kothari <hamelkothari@gmail.com>, Ted Yu <yuzhihong@gmail.com>","external shuffle service is not enabled

________________________________
best regards,
-zhenhua

From: Hamel Kothari<mailto:hamelkothari@gmail.com>
Date: 2016-01-27 22:21
To: Ted Yu<mailto:yuzhihong@gmail.com>; wangzhenhua (G)<mailto:wangzhenhua@huawei.com>
CC: dev<mailto:dev@spark.apache.org>
Subject: Re: timeout in shuffle problem
Are you running on YARN? Another possibility here is that your shuffle managers are facing GC pain and becoming less responsive, thus missing timeouts. Can you try increasing the memory on the node managers and see if that helps?

On Sun, Jan 24, 2016 at 4:58 PM Ted Yu <yuzhihong@gmail.com<mailto:yuzhihong@gmail.com>> wrote:
Cycling past bits:
http://search-hadoop.com/m/q3RTtU5CRU1KKVA42&subj=RE+shuffle+FetchFailedException+in+spark+on+YARN+job

On Sun, Jan 24, 2016 at 5:52 AM, wangzhenhua (G) <wae:
Hi,

I have a problem of time out in shuffle, it happened after shuffle write and at the start of shuffle read,
logs on driver and executors are shown as below. Spark version is 1.5. Looking forward to your replys. Thanks!

logs on driver only have warnings:
WARN TaskSetManager: Lost task 38.0 in stage 27.0 (TID 127459, linux-162): FetchFailed(BlockManagerId(66, 172.168.100.12, 23028), shuffleId=9, mapId=55, reduceId=38, message=
org.apache.spark.shuffle.FetchFailedException: java.util.concurrent.TimeoutException: Timeout waiting for task.
        at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:321)
        at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:306)
        at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:173)
        at org.apache.spark.sql.execution.TungstenSort.org<http://org.apache.spark.sql.execution.TungstenSort.org>$apache$spark$sql$execution$TungstenSort$$executePartition$1(sort.scala:160)
        at org.apache.spark.sql.execution.TungstenSort$$anonfun$doExecute$4.apply(sort.scala:169)
        at org.apache.spark.sql.execution.TungstenSort$$anonfun$doExecute$4.apply(sort.scala:169)
        at org.apache.spark.rdd.MapPartitionsWithPreparationRDD.compute(MapPartitionsWithPreparationRDD.scala:64)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
        at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:99)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
        at org.apache.spark.rdd.MapPartitionsWithPreparationRDD.compute(MapPartitionsWithPreparationRDD.scala:63)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
        at org.apache.spark.scheduler.Task.run(Task.scala:88)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: java.util.concurrent.TimeoutException: Timeout waiting for task.
        at org.spark-project.guava.base.Throwables.propagate(Throwables.java:160)
        at org.apache.spark.network.client.TransportClient.sendRpcSync(TransportClient.java:196)
        at org.apache.spark.network.sasl.SaslClientBootstrap.doBootstrap(SaslClientBootstrap.java:76)
        at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:205)
        at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:156)
        at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:88)
        at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
        at org.apache.spark.network.shuffle.RetryingBlockFetcher.start(RetryingBlockFetcher.java:120)
        at org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:97)
        at org.apache.spark.storage.ShuffleBlockFetcherIterator.sendRequest(ShuffleBlockFetcherIterator.scala:152)
        at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:301)
        ... 57 more
Caused by: java.util.concurrent.TimeoutException: Timeout waiting for task.
        at org.spark-project.guava.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:276)
        at org.spark-project.guava.util.concurrent.AbstractFuture.get(AbstractFuture.java:96)
        at org.apache.spark.network.client.TransportClient.sendRpcSync(TransportClient.java:192)
        ... 66 more

logs on executor have errors like:
ERROR | [shuffle-client-1] | Still have 1 requests outstanding when connection from /172.168.100.12:23908<http://172.168.100.12:23908> is closed | org.apache.spark.network.client.TransportResponseHandler.channelUnregistered(TransportResponseHandler.java:102)

________________________________
best regards,
-zhenhua

"
Chayapan Khannabha <chayapan@gmail.com>,"Thu, 28 Jan 2016 10:17:48 +0700",Re: spark job scheduling,Niranda Perera <niranda.perera@gmail.com>,"I think the smallest unit of work is a ""Task"", and an ""Executor"" is
responsible for getting the work done? Would like to understand more about
the scheduling system too. Scheduling strategy like FAIR or FIFO do have
significant impact on a Spark cluster architecture design decision.

Best,

Chayapan (A)


"
Niranda Perera <niranda.perera@gmail.com>,"Thu, 28 Jan 2016 08:57:22 +0530",Re: spark job scheduling,Chayapan Khannabha <chayapan@gmail.com>,"Sorry I have made typos. let me rephrase

1. As I understand, the smallest unit of work an executor can perform, is a
'task'. In the 'FAIR' scheduler mode, let's say a job is submitted to the
spark ctx which has a considerable amount of work to do in a single task.
While such a 'big' task is running, can we still submit another smaller job
(from a separate thread) and get it done? or does that smaller job has to
wait till the bigger task finishes and the resources are freed from the
executor?
(essentially, what I'm asking is, in the FAIR scheduler mode, jobs are
scheduled fairly, but at the task granularity they are still FIFO?)

2. When a job is submitted without setting a scheduler pool, the 'default'
scheduler pool is assigned to it, which employs FIFO scheduling. but what
happens when we have the spark.scheduler.mode as FAIR, and if I submit jobs
without specifying a scheduler pool (which has FAIR scheduling)? would the
jobs still run in FIFO mode with the default pool?
essentially, for us to really set FAIR scheduling, do we have to assign a
FAIR scheduler pool also to the job?




-- 
Niranda
@n1r44 <https://twitter.com/N1R44>
+94-71-554-8430
https://pythagoreanscript.wordpress.com/
"
Chayapan Khannabha <chayapan@gmail.com>,"Thu, 28 Jan 2016 10:50:49 +0700",Re: spark job scheduling,Niranda Perera <niranda.perera@gmail.com>,"I would start at this wiki page
https://spark.apache.org/docs/1.2.0/job-scheduling.html

Although I'm sure this depends a lot on your cluster environment and the
deployed Spark version.

IMHO


"
Jakob Odersky <jakob@odersky.com>,"Wed, 27 Jan 2016 20:48:58 -0800",Re: spark job scheduling,Chayapan Khannabha <chayapan@gmail.com>,"Nitpick: the up-to-date version of said wiki page is
https://spark.apache.org/docs/1.6.0/job-scheduling.html (not sure how
much it changed though)


---------------------------------------------------------------------


"
=?UTF-8?Q?Maciej_Bry=C5=84ski?= <maciek@brynski.pl>,"Thu, 28 Jan 2016 10:14:16 +0100",Spark 1.6.0 + Hive + HBase,dev@spark.apache.org,"Hi,
I'm trying to run SQL query on Hive table which is stored on HBase.
I'm using:
- Spark 1.6.0
- HDP 2.2
- Hive 0.14.0
- HBase 0.98.4

I managed to configure working classpath, but I have following problems:

1) I have UDF defined in Hive Metastore (FUNCS table).
Spark cannot use it..

 File ""/opt/spark/python/lib/py4j-0.9-src.zip/py4j/protocol.py"", line 308,
in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o51.sql.
: org.apache.spark.sql.AnalysisException: undefined function
dwh.str_to_map_int_str; line 55 pos 30
        at
org.apache.spark.sql.hive.HiveFunctionRegistry$$anonfun$lookupFunction$2$$anonfun$1.apply(hiveUDFs.scala:69)
        at
org.apache.spark.sql.hive.HiveFunctionRegistry$$anonfun$lookupFunction$2$$anonfun$1.apply(hiveUDFs.scala:69)
        at scala.Option.getOrElse(Option.scala:120)
        at
org.apache.spark.sql.hive.HiveFunctionRegistry$$anonfun$lookupFunction$2.apply(hiveUDFs.scala:68)
        at
org.apache.spark.sql.hive.HiveFunctionRegistry$$anonfun$lookupFunction$2.apply(hiveUDFs.scala:64)
        at scala.util.Try.getOrElse(Try.scala:77)
        at
org.apache.spark.sql.hive.HiveFunctionRegistry.lookupFunction(hiveUDFs.scala:64)
        at
org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$12$$anonfun$applyOrElse$5$$anonfun$applyOrElse$24.apply(Analyzer.scala:574)
        at
org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$12$$anonfun$applyOrElse$5$$anonfun$applyOrElse$24.apply(Analyzer.scala:574)
        at
org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:48)
        at
org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$12$$anonfun$applyOrElse$5.applyOrElse(Analyzer.scala:573)
        at
org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$12$$anonfun$applyOrElse$5.applyOrElse(Analyzer.scala:570)


2) When I'm using SQL without this function Spark tries to connect to
Zookeeper on localhost.
I make a tunnel from localhost to one of the zookeeper servers but it's not
a solution.

16/01/28 10:09:18 INFO ZooKeeper: Client
environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
16/01/28 10:09:18 INFO ZooKeeper: Client environment:host.name=j4.jupyter1
16/01/28 10:09:18 INFO ZooKeeper: Client environment:java.version=1.8.0_66
16/01/28 10:09:18 INFO ZooKeeper: Client environment:java.vendor=Oracle
Corporation
16/01/28 10:09:18 INFO ZooKeeper: Client
environment:java.home=/usr/lib/jvm/java-8-oracle/jre
16/01/28 10:09:18 INFO ZooKeeper: Client
environment:java.class.path=/opt/spark/lib/mysql-connector-java-5.1.35-bin.jar:/opt/spark/lib/dwh-hbase-connector.jar:/opt/spark/lib/hive-hbase-handler-1.2.1.spark.jar:/opt/spark/lib/hbase-server.jar:/opt/spark/lib/hbase-common.jar:/opt/spark/lib/dwh-commons.jar:/opt/spark/lib/guava.jar:/opt/spark/lib/hbase-client.jar:/opt/spark/lib/hbase-protocol.jar:/opt/spark/lib/htrace-core.jar:/opt/spark/conf/:/opt/spark/lib/spark-assembly-1.6.0-hadoop2.6.0.jar:/opt/spark/lib/datanucleus-rdbms-3.2.9.jar:/opt/spark/lib/datanucleus-api-jdo-3.2.6.jar:/opt/spark/lib/datanucleus-core-3.2.10.jar:/etc/hadoop/conf/
16/01/28 10:09:18 INFO ZooKeeper: Client
environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
16/01/28 10:09:18 INFO ZooKeeper: Client environment:java.io.tmpdir=/tmp
16/01/28 10:09:18 INFO ZooKeeper: Client environment:java.compiler=<NA>
16/01/28 10:09:18 INFO ZooKeeper: Client environment:os.name=Linux
16/01/28 10:09:18 INFO ZooKeeper: Client environment:os.arch=amd64
16/01/28 10:09:18 INFO ZooKeeper: Client
environment:os.version=3.13.0-24-generic
16/01/28 10:09:18 INFO ZooKeeper: Client environment:user.name=mbrynski
16/01/28 10:09:18 INFO ZooKeeper: Client
environment:user.home=/home/mbrynski
16/01/28 10:09:18 INFO ZooKeeper: Client environment:user.dir=/home/mbrynski
16/01/28 10:09:18 INFO ZooKeeper: Initiating client connection,
connectString=localhost:2181 sessionTimeout=90000
watcher=hconnection-0x36079f06, quorum=localhost:2181, baseZNode=/hbase
16/01/28 10:09:18 INFO RecoverableZooKeeper: Process
identifier=hconnection-0x36079f06 connecting to ZooKeeper
ensemble=localhost:2181
16/01/28 10:09:18 INFO ClientCnxn: Opening socket connection to server
localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL
(unknown error)
16/01/28 10:09:18 INFO ClientCnxn: Socket connection established to
localhost/127.0.0.1:2181, initiating session
16/01/28 10:09:18 INFO ClientCnxn: Session establishment complete on server
localhost/127.0.0.1:2181, sessionid = 0x15254709ed3c8e1, negotiated timeout
= 40000
16/01/28 10:09:18 INFO ZooKeeperRegistry: ClusterId read in ZooKeeper is
null


3) After making tunel I'm getting NPE.

Caused by: java.lang.NullPointerException
        at
org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.getMetaReplicaNodes(ZooKeeperWatcher.java:269)
        at
org.apache.hadoop.hbase.zookeeper.MetaRegionTracker.blockUntilAvailable(MetaRegionTracker.java:241)
        at
org.apache.hadoop.hbase.client.ZooKeeperRegistry.getMetaRegionLocation(ZooKeeperRegistry.java:62)
        at
org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateMeta(ConnectionManager.java:1203)
        at
org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion(ConnectionManager.java:1164)
        at
org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas.getRegionLocations(RpcRetryingCallerWithReadReplicas.java:294)
        at
org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:130)
        at
org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:55)
        at
org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:201)
        ... 91 more

Do you have any ideas how to resolve those problems ?

Regards,
-- 
Maciek Bry≈Ñski
"
Ted Yu <yuzhihong@gmail.com>,"Thu, 28 Jan 2016 01:25:53 -0800",Re: Spark 1.6.0 + Hive + HBase,=?utf-8?Q?Maciej_Bry=C5=84ski?= <maciek@brynski.pl>,"For the last two problems, hbase-site.xml seems not to be on classpath. 

. 

Cheers

:
 in get_return_value
p_int_str; line 55 pos 30
nction$2$$anonfun$1.apply(hiveUDFs.scala:69)
nction$2$$anonfun$1.apply(hiveUDFs.scala:69)
nction$2.apply(hiveUDFs.scala:68)
nction$2.apply(hiveUDFs.scala:64)
iveUDFs.scala:64)
s$$anonfun$apply$12$$anonfun$applyOrElse$5$$anonfun$applyOrElse$24.apply(Analyzer.scala:574)
s$$anonfun$apply$12$$anonfun$applyOrElse$5$$anonfun$applyOrElse$24.apply(Analyzer.scala:574)
ckage.scala:48)
s$$anonfun$apply$12$$anonfun$applyOrElse$5.applyOrElse(Analyzer.scala:573)
s$$anonfun$apply$12$$anonfun$applyOrElse$5.applyOrElse(Analyzer.scala:570)
eeper on localhost.
t a solution.
.4.6-1569965, built on 02/20/2014 09:09 GMT
r1
66
orporation
jvm/java-8-oracle/jre
t/spark/lib/mysql-connector-java-5.1.35-bin.jar:/opt/spark/lib/dwh-hbase-connector.jar:/opt/spark/lib/hive-hbase-handler-1.2.1.spark.jar:/opt/spark/lib/hbase-server.jar:/opt/spark/lib/hbase-common.jar:/opt/spark/lib/dwh-commons.jar:/opt/spark/lib/guava.jar:/opt/spark/lib/hbase-client.jar:/opt/spark/lib/hbase-protocol.jar:/opt/spark/lib/htrace-core.jar:/opt/spark/conf/:/opt/spark/lib/spark-assembly-1.6.0-hadoop2.6.0.jar:/opt/spark/lib/datanucleus-rdbms-3.2.9.jar:/opt/spark/lib/datanucleus-api-jdo-3.2.6.jar:/opt/spark/lib/datanucleus-core-3.2.10.jar:/etc/hadoop/conf/
usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib

4-generic
ynski
nski
ing=localhost:2181 sessionTimeout=90000 watcher=hconnection-0x36079f06, quorum=localhost:2181, baseZNode=/hbase
ion-0x36079f06 connecting to ZooKeeper ensemble=localhost:2181
alhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
ost/127.0.0.1:2181, initiating session
r localhost/127.0.0.1:2181, sessionid = 0x15254709ed3c8e1, negotiated timeout = 40000
ull
caNodes(ZooKeeperWatcher.java:269)
vailable(MetaRegionTracker.java:241)
ocation(ZooKeeperRegistry.java:62)
lementation.locateMeta(ConnectionManager.java:1203)
lementation.locateRegion(ConnectionManager.java:1164)
s.getRegionLocations(RpcRetryingCallerWithReadReplicas.java:294)
(ScannerCallableWithReplicas.java:130)
(ScannerCallableWithReplicas.java:55)
ries(RpcRetryingCaller.java:201)
"
Tomas Kliegr <kliegr@gmail.com>,"Thu, 28 Jan 2016 10:29:13 +0100",FPGrowth: adding a stopping criterion (max. literal length or itemset count),dev@spark.apache.org,"Hi all,

Could anyone provide pointers on how to  extend the SPARK FPGrowth
implementation with either of the following stopping criteria:

* maximum number of generated itemsets,
* maximum length of generated itemsets (i.e. number of items in itemset).

The second criterion is e.g. available in the Christian Borgelt's FPG
implementation [1] through the -n# switch.

We have experience with apriori but not parallel FP-Growth, so any
guidance will be welcome.

The reason why we need this -  without these extra constraints we keep
running into combinatorial explosion problems,  as documented on the
UCI Audiology dataset [2].

Thanks,
-- 
Tomas

[1] http://www.borgelt.net/doc/fpgrowth/fpgrowth.html
[2] https://issues.apache.org/jira/browse/SPARK-12163

---------------------------------------------------------------------


"
"""=?gb18030?B?v6rQxNHTxOo=?="" <muyannian@qq.com>","Thu, 28 Jan 2016 19:27:19 +0800",Why Spark-sql miss TableScanDesc.FILTER_EXPR_CONF_STR params when I move Hive table to Spark?,"""=?gb18030?B?dXNlcg==?="" <user@spark.apache.org>, ""=?gb18030?B?ZGV2?="" <dev@spark.apache.org>","Dear spark
I am test StorageHandler on Spark-SQL.
but i find the TableScanDesc.FILTER_EXPR_CONF_STR is miss ,but i need it ,is three any where i could found it?
I really want to get some filter information from Spark Sql, so that I could make a pre filter by my Index ;
so where is the TableScanDesc.FILTER_EXPR_CONF_STR=hive.io.filter.expr.serialized? it is missing or replace by other method ,thanks every body ,thanks .


for example  I make a custorm StorageHandler like hive .

creat table xxx(...)
STORED BY 'cn.net.ycloud.ydb.handle.Ya100StorageHandler' 
TBLPROPERTIES(
""ya100.handler.master""=""101.200.130.48:8080"",
""ya100.handler.table.name""=""ydb_example_shu"",
""ya100.handler.columns.mapping""=""phonenum,usernick,ydb_sex,ydb_province,ydb_grade,ydb_age,ydb_blood,ydb_zhiye,ydb_earn,ydb_prefer,ydb_consume,ydb_day,content,ydbpartion,ya100_pipe""
)

in Ya100StorageHandler code .
I wang to use TableScanDesc.FILTER_EXPR_CONF_STR  like this

  String filterExprSerialized = conf.get(TableScanDesc.FILTER_EXPR_CONF_STR);
    if (filterExprSerialized == null) {
        return """";
//         throw new IOException(""can`t found filter condition in your Sql ,at least you must special a field as ydbpartion "");
    }else{
        LOG.info(filterExprSerialized);
        ExprNodeGenericFuncDesc filterExpr =    Utilities.deserializeExpression(filterExprSerialized);
        LOG.info(filterExpr);
        try {
            return Ya100Utils.parserFilter(filterExpr,info);
        } catch (Throwable e) {
            throw new IOException(e);
        }
    }"
=?UTF-8?Q?Maciej_Bry=C5=84ski?= <maciek@brynski.pl>,"Thu, 28 Jan 2016 12:56:43 +0100",Re: Spark 1.6.0 + Hive + HBase,Ted Yu <yuzhihong@gmail.com>,"Ted,
You're right.
hbase-site.xml resolved problems 2 and 3, but...

Problem 4)
Spark don't push down predicates for HiveTableScan, which means that every
query is full scan.

== Physical Plan ==
TungstenAggregate(key=[],
functions=[(count(1),mode=Final,isDistinct=false)],
output=[count#144L])
+- TungstenExchange SinglePartition, None
   +- TungstenAggregate(key=[],
functions=[(count(1),mode=Partial,isDistinct=false)],
output=[count#147L])
      +- Project
         +- Filter (added_date#141L >= 201601280000)
            +- HiveTableScan [added_date#141L], MetastoreRelation
dwh_diagnostics, sessions_hbase, None


Is there any magic option to make this work ?

Regards,
Maciek

2016-01-28 10:25 GMT+01:00 Ted Yu <yuzhihong@gmail.com>:

e:
,
$anonfun$1.apply(hiveUDFs.scala:69)
$anonfun$1.apply(hiveUDFs.scala:69)
apply(hiveUDFs.scala:68)
apply(hiveUDFs.scala:64)
ala:64)
$apply$12$$anonfun$applyOrElse$5$$anonfun$applyOrElse$24.apply(Analyzer.scala:574)
$apply$12$$anonfun$applyOrElse$5$$anonfun$applyOrElse$24.apply(Analyzer.scala:574)
a:48)
$apply$12$$anonfun$applyOrElse$5.applyOrElse(Analyzer.scala:573)
$apply$12$$anonfun$applyOrElse$5.applyOrElse(Analyzer.scala:570)
GMT
er1
_66
bin.jar:/opt/spark/lib/dwh-hbase-connector.jar:/opt/spark/lib/hive-hbase-handler-1.2.1.spark.jar:/opt/spark/lib/hbase-server.jar:/opt/spark/lib/hbase-common.jar:/opt/spark/lib/dwh-commons.jar:/opt/spark/lib/guava.jar:/opt/spark/lib/hbase-client.jar:/opt/spark/lib/hbase-protocol.jar:/opt/spark/lib/htrace-core.jar:/opt/spark/conf/:/opt/spark/lib/spark-assembly-1.6.0-hadoop2.6.0.jar:/opt/spark/lib/datanucleus-rdbms-3.2.9.jar:/opt/spark/lib/datanucleus-api-jdo-3.2.6.jar:/opt/spark/lib/datanucleus-core-3.2.10.jar:/etc/hadoop/conf/
lib64:/lib:/usr/lib
p
base
oKeeperWatcher.java:269)
etaRegionTracker.java:241)
oKeeperRegistry.java:62)
n.locateMeta(ConnectionManager.java:1203)
n.locateRegion(ConnectionManager.java:1164)
nLocations(RpcRetryingCallerWithReadReplicas.java:294)
llableWithReplicas.java:130)
llableWithReplicas.java:55)
tryingCaller.java:201)


-- 
Maciek Bry≈Ñski
"
Julio Antonio Soto de Vicente <julio@esbet.es>,"Thu, 28 Jan 2016 13:09:13 +0100",Re: Spark 1.6.0 + Hive + HBase,=?utf-8?Q?Maciej_Bry=C5=84ski?= <maciek@brynski.pl>,"Hi,

Indeed, Hive is not able to perform predicate pushdown through a HBase table. Nor Hive or Impala can.

Broadly speaking, if you need to query your  HBase table through a field other than de rowkey:

A) Try to ""encode"" as much info as possible in the rowkey field and use it as your predicate, or
B) Feel free to use other kind of storage system/create coprocessors in order to create a secondary index.


bi√≥:
 query is full scan.
=false)], output=[count#144L])
Distinct=false)], output=[count#147L])
nostics, sessions_hbase, None

ess. 
te:

8, in get_return_value
map_int_str; line 55 pos 30
pFunction$2$$anonfun$1.apply(hiveUDFs.scala:69)
pFunction$2$$anonfun$1.apply(hiveUDFs.scala:69)
pFunction$2.apply(hiveUDFs.scala:68)
pFunction$2.apply(hiveUDFs.scala:64)
(hiveUDFs.scala:64)
ons$$anonfun$apply$12$$anonfun$applyOrElse$5$$anonfun$applyOrElse$24.apply(Analyzer.scala:574)
ons$$anonfun$apply$12$$anonfun$applyOrElse$5$$anonfun$applyOrElse$24.apply(Analyzer.scala:574)
package.scala:48)
ons$$anonfun$apply$12$$anonfun$applyOrElse$5.applyOrElse(Analyzer.scala:573)
ons$$anonfun$apply$12$$anonfun$applyOrElse$5.applyOrElse(Analyzer.scala:570)
okeeper on localhost.
ot a solution.
3.4.6-1569965, built on 02/20/2014 09:09 GMT
ter1
0_66
e Corporation
b/jvm/java-8-oracle/jre
opt/spark/lib/mysql-connector-java-5.1.35-bin.jar:/opt/spark/lib/dwh-hbase-connector.jar:/opt/spark/lib/hive-hbase-handler-1.2.1.spark.jar:/opt/spark/lib/hbase-server.jar:/opt/spark/lib/hbase-common.jar:/opt/spark/lib/dwh-commons.jar:/opt/spark/lib/guava.jar:/opt/spark/lib/hbase-client.jar:/opt/spark/lib/hbase-protocol.jar:/opt/spark/lib/htrace-core.jar:/opt/spark/conf/:/opt/spark/lib/spark-assembly-1.6.0-hadoop2.6.0.jar:/opt/spark/lib/datanucleus-rdbms-3.2.9.jar:/opt/spark/lib/datanucleus-api-jdo-3.2.6.jar:/opt/spark/lib/datanucleus-core-3.2.10.jar:/etc/hadoop/conf/
/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
mp
-24-generic
i
brynski
rynski
tring=localhost:2181 sessionTimeout=90000 watcher=hconnection-0x36079f06, quorum=localhost:2181, baseZNode=/hbase
ction-0x36079f06 connecting to ZooKeeper ensemble=localhost:2181
ocalhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
lhost/127.0.0.1:2181, initiating session
ver localhost/127.0.0.1:2181, sessionid = 0x15254709ed3c8e1, negotiated timeout = 40000
 null
licaNodes(ZooKeeperWatcher.java:269)
lAvailable(MetaRegionTracker.java:241)
nLocation(ZooKeeperRegistry.java:62)
mplementation.locateMeta(ConnectionManager.java:1203)
mplementation.locateRegion(ConnectionManager.java:1164)
cas.getRegionLocations(RpcRetryingCallerWithReadReplicas.java:294)
ll(ScannerCallableWithReplicas.java:130)
ll(ScannerCallableWithReplicas.java:55)
etries(RpcRetryingCaller.java:201)
"
"""=?gb18030?B?v6rQxNHTxOo=?="" <muyannian@qq.com>","Thu, 28 Jan 2016 20:28:42 +0800","=?gb18030?B?u9i4tKO6V2h5IFNwYXJrLXNxbCBtaXNzIFRhYmxl?=
 =?gb18030?B?U2NhbkRlc2MuRklMVEVSX0VYUFJfQ09ORl9TVFIg?=
 =?gb18030?B?cGFyYW1zIHdoZW4gSSBtb3ZlIEhpdmUgdGFibGUg?=
 =?gb18030?B?dG8gU3Bhcms/?=","""=?gb18030?B?v6rQxNHTxOo=?="" <muyannian@qq.com>, ""=?gb18030?B?dXNlcg==?="" <user@spark.apache.org>, ""=?gb18030?B?ZGV2?="" <dev@spark.apache.org>","we always used Sql like below.

select count(*) from ydb_example_shu where ydbpartion='20151110' and (ydb_sex='' or ydb_province='LIAONING' or ydb_day>='20151217') limit 10

Spark don't push down predicates for TableScanDesc.FILTER_EXPR_CONF_STR, which means that every query is full scan can`t use the index (Something like HbaseStoreHandle).








------------------ ‘≠ º” º˛ ------------------
∑¢º˛»À: ""ø™–ƒ—”ƒÍ"";<muyannian@qq.com>;
∑¢ÀÕ ±º‰: 2016ƒÍ1‘¬28»’(–«∆⁄Àƒ) ÕÌ…œ7:27
 ’º˛»À: ""user""<user@spark.apache.org>; ""dev""<dev@spark.apache.org>; 

÷˜Ã‚: Why Spark-sql miss TableScanDesc.FILTER_EXPR_CONF_STR params when I move Hive table to Spark?



Dear spark
I am test StorageHandler on Spark-SQL.
but i find the TableScanDesc.FILTER_EXPR_CONF_STR is miss ,but i need it ,is three any where i could found it?
I really want to get some filter information from Spark Sql, so that I could make a pre filter by my Index ;
so where is the TableScanDesc.FILTER_EXPR_CONF_STR=hive.io.filter.expr.serialized? it is missing or replace by other method ,thanks every body ,thanks .


for example  I make a custorm StorageHandler like hive .

creat table xxx(...)
STORED BY 'cn.net.ycloud.ydb.handle.Ya100StorageHandler' 
TBLPROPERTIES(
""ya100.handler.master""=""101.200.130.48:8080"",
""ya100.handler.table.name""=""ydb_example_shu"",
""ya100.handler.columns.mapping""=""phonenum,usernick,ydb_sex,ydb_province,ydb_grade,ydb_age,ydb_blood,ydb_zhiye,ydb_earn,ydb_prefer,ydb_consume,ydb_day,content,ydbpartion,ya100_pipe""
)

in Ya100StorageHandler code .
I wang to use TableScanDesc.FILTER_EXPR_CONF_STR  like this

  String filterExprSerialized = conf.get(TableScanDesc.FILTER_EXPR_CONF_STR);
    if (filterExprSerialized == null) {
        return """";
//         throw new IOException(""can`t found filter condition in your Sql ,at least you must special a field as ydbpartion "");
    }else{
        LOG.info(filterExprSerialized);
        ExprNodeGenericFuncDesc filterExpr =    Utilities.deserializeExpression(filterExprSerialized);
        LOG.info(filterExpr);
        try {
            return Ya100Utils.parserFilter(filterExpr,info);
        } catch (Throwable e) {
            throw new IOException(e);
        }
    }"
"""=?gb18030?B?v6rQxNHTxOo=?="" <muyannian@qq.com>","Thu, 28 Jan 2016 20:42:33 +0800","=?gb18030?B?u9i4tKO6IFNwYXJrIDEuNi4wICsgSGl2ZSArIEhC?=
 =?gb18030?B?YXNl?=","""=?gb18030?B?SnVsaW8gQW50b25pbyBTb3RvIGRlIFZpY2VudGU=?="" <julio@esbet.es>, ""=?gb18030?B?TWFjaWVqIEJyeai9c2tp?="" <maciek@brynski.pl>","Is there any body can solve Problem 4)? thanks.
Problem 4)
Spark don't push down predicates for HiveTableScan, which means that every query is full scan.





------------------ ‘≠ º” º˛ ------------------
∑¢º˛»À: ""Julio Antonio Soto de Vicente"";<julio@esbet.es>;
∑¢ÀÕ ±º‰: 2016ƒÍ1‘¬28»’(–«∆⁄Àƒ) ÕÌ…œ8:09
 ’º˛»À: ""Maciej Bry®Ωski""<maciek@brynski.pl>; 
≥≠ÀÕ: ""Ted Yu""<yuzhihong@gmail.com>; ""dev""<dev@spark.apache.org>; 
÷˜Ã‚: Re: Spark 1.6.0 + Hive + HBase



Hi,


Indeed, Hive is not able to perform predicate pushdown through a HBase table. Nor Hive or Impala can.


Broadly speaking, if you need to query your  HBase table through a field other than de rowkey:


A) Try to ""encode"" as much info as possible in the rowkey field and use it as your predicate, or
B) Feel free to use other kind of storage system/create coprocessors in order to create a secondary index.



El 28 ene 2016, a las 12:56, Maciej Bry®Ωski <maciek@brynski.pl> escribi®Æ:


Ted,You're right.
hbase-site.xml resolved problems 2 and 3, but...


Problem 4)
Spark don't push down predicates for HiveTableScan, which means that every query is full scan.


== Physical Plan == TungstenAggregate(key=[], functions=[(count(1),mode=Final,isDistinct=false)], output=[count#144L]) +- TungstenExchange SinglePartition, None    +- TungstenAggregate(key=[], functions=[(count(1),mode=Partial,isDistinct=false)], output=[count#147L])       +- Project          +- Filter (added_date#141L >= 201601280000)             +- HiveTableScan [added_date#141L], MetastoreRelation dwh_diagnostics, sessions_hbase, None


Is there any magic option to make this work ?


Regards,
Maciek

2016-01-28 10:25 GMT+01:00 Ted Yu <yuzhihong@gmail.com>:

For the last two problems, hbase-site.xml seems not to be on classpath. 


Once hbase-site.xml is put on classpath, you should be able to make progress. 


Cheers

On Jan 28, 2016, at 1:14 AM, Maciej Bry®Ωski <maciek@brynski.pl> wrote:


Hi,I'm trying to run SQL query on Hive table which is stored on HBase.
I'm using:

- Spark 1.6.0
- HDP 2.2
- Hive 0.14.0
- HBase 0.98.4


I managed to configure working classpath, but I have following problems:


1) I have UDF defined in Hive Metastore (FUNCS table).
Spark cannot use it.. 


 File ""/opt/spark/python/lib/py4j-0.9-src.zip/py4j/protocol.py"", line 308, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o51.sql.
: org.apache.spark.sql.AnalysisException: undefined function dwh.str_to_map_int_str; line 55 pos 30
        at org.apache.spark.sql.hive.HiveFunctionRegistry$$anonfun$lookupFunction$2$$anonfun$1.apply(hiveUDFs.scala:69)
        at org.apache.spark.sql.hive.HiveFunctionRegistry$$anonfun$lookupFunction$2$$anonfun$1.apply(hiveUDFs.scala:69)
        at scala.Option.getOrElse(Option.scala:120)
        at org.apache.spark.sql.hive.HiveFunctionRegistry$$anonfun$lookupFunction$2.apply(hiveUDFs.scala:68)
        at org.apache.spark.sql.hive.HiveFunctionRegistry$$anonfun$lookupFunction$2.apply(hiveUDFs.scala:64)
        at scala.util.Try.getOrElse(Try.scala:77)
        at org.apache.spark.sql.hive.HiveFunctionRegistry.lookupFunction(hiveUDFs.scala:64)
        at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$12$$anonfun$applyOrElse$5$$anonfun$applyOrElse$24.apply(Analyzer.scala:574)
        at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$12$$anonfun$applyOrElse$5$$anonfun$applyOrElse$24.apply(Analyzer.scala:574)
        at org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:48)
        at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$12$$anonfun$applyOrElse$5.applyOrElse(Analyzer.scala:573)
        at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$12$$anonfun$applyOrElse$5.applyOrElse(Analyzer.scala:570)





2) When I'm using SQL without this function Spark tries to connect to Zookeeper on localhost.
I make a tunnel from localhost to one of the zookeeper servers but it's not a solution.


16/01/28 10:09:18 INFO ZooKeeper: Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
16/01/28 10:09:18 INFO ZooKeeper: Client environment:host.name=j4.jupyter1
16/01/28 10:09:18 INFO ZooKeeper: Client environment:java.version=1.8.0_66
16/01/28 10:09:18 INFO ZooKeeper: Client environment:java.vendor=Oracle Corporation
16/01/28 10:09:18 INFO ZooKeeper: Client environment:java.home=/usr/lib/jvm/java-8-oracle/jre
16/01/28 10:09:18 INFO ZooKeeper: Client environment:java.class.path=/opt/spark/lib/mysql-connector-java-5.1.35-bin.jar:/opt/spark/lib/dwh-hbase-connector.jar:/opt/spark/lib/hive-hbase-handler-1.2.1.spark.jar:/opt/spark/lib/hbase-server.jar:/opt/spark/lib/hbase-common.jar:/opt/spark/lib/dwh-commons.jar:/opt/spark/lib/guava.jar:/opt/spark/lib/hbase-client.jar:/opt/spark/lib/hbase-protocol.jar:/opt/spark/lib/htrace-core.jar:/opt/spark/conf/:/opt/spark/lib/spark-assembly-1.6.0-hadoop2.6.0.jar:/opt/spark/lib/datanucleus-rdbms-3.2.9.jar:/opt/spark/lib/datanucleus-api-jdo-3.2.6.jar:/opt/spark/lib/datanucleus-core-3.2.10.jar:/etc/hadoop/conf/
16/01/28 10:09:18 INFO ZooKeeper: Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
16/01/28 10:09:18 INFO ZooKeeper: Client environment:java.io.tmpdir=/tmp
16/01/28 10:09:18 INFO ZooKeeper: Client environment:java.compiler=<NA>
16/01/28 10:09:18 INFO ZooKeeper: Client environment:os.name=Linux
16/01/28 10:09:18 INFO ZooKeeper: Client environment:os.arch=amd64
16/01/28 10:09:18 INFO ZooKeeper: Client environment:os.version=3.13.0-24-generic
16/01/28 10:09:18 INFO ZooKeeper: Client environment:user.name=mbrynski
16/01/28 10:09:18 INFO ZooKeeper: Client environment:user.home=/home/mbrynski
16/01/28 10:09:18 INFO ZooKeeper: Cli9:18 INFO ZooKeeper: Initiating client connection, connectString=localhost:2181 sessionTimeout=90000 watcher=hconnection-0x36079f06, quorum=localhost:2181, baseZNode=/hbase
16/01/28 10:09:18 INFO RecoverableZooKeeper: Process identifier=hconnection-0x36079f06 connecting to ZooKeeper ensemble=localhost:2181
16/01/28 10:09:18 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
16/01/28 10:09:18 INFO ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
16/01/28 10:09:18 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x15254709ed3c8e1, negotiated timeout = 40000
16/01/28 10:09:18 INFO ZooKeeperRegistry: ClusterId read in ZooKeeper is null





3) After making tunel I'm getting NPE.


Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.getMetaReplicaNodes(ZooKeeperWatcher.java:269)
        at org.apache.hadoop.hbase.zookeeper.MetaRegionTracker.blockUntilAvailable(MetaRegionTracker.java:241)
        at org.apache.hadoop.hbase.client.ZooKeeperRegistry.getMetaRegionLocation(ZooKeeperRegistry.java:62)
        at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateMeta(ConnectionManager.java:1203)
        at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion(ConnectionManager.java:1164)
        at org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas.getRegionLocations(RpcRetryingCallerWithReadReplicas.java:294)
        at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:130)
        at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:55)
        at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:201)
        ... 91 more


Do you have any ideas how to resolve those problems ?


Regards,
-- 
Maciek Bry®Ωski
 


 








-- 
Maciek Bry®Ωski"
"""=?gb18030?B?v6rQxNHTxOo=?="" <muyannian@qq.com>","Thu, 28 Jan 2016 21:02:59 +0800","=?gb18030?B?u9i4tKO6V2h5IFNwYXJrLXNxbCBtaXNzIFRhYmxl?=
 =?gb18030?B?U2NhbkRlc2MuRklMVEVSX0VYUFJfQ09ORl9TVFIg?=
 =?gb18030?B?cGFyYW1zIHdoZW4gSSBtb3ZlIEhpdmUgdGFibGUg?=
 =?gb18030?B?dG8gU3Bhcms/?=","""=?gb18030?B?v6rQxNHTxOo=?="" <muyannian@qq.com>, ""=?gb18030?B?dXNlcg==?="" <user@spark.apache.org>, ""=?gb18030?B?ZGV2?="" <dev@spark.apache.org>","If we support TableScanDesc.FILTER_EXPR_CONF_STR like hive 

we may write sql LIKE this 

select ydb_sex from ydb_example_shu where ydbpartion='20151110' limit 10
select ydb_sex from ydb_example_shu where ydbpartion='20151110' and (ydb_sex='≈Æ' or ydb_province='¡…ƒ˛' or ydb_day>='20151217') limit 10
select count(*) from ydb_example_shu where ydbpartion='20151110' and (ydb_sex='≈Æ' or ydb_province='¡…ƒ˛' or ydb_day>='20151217') limit 10


If we may not  support TableScanDesc.FILTER_EXPR_CONF_STR like hive  we write Sql like this  

set ya100.spark.filter.ydb_example_shu=ydbpartion='20151110';
select ydb_sex from ydb_example_shu  limit 10

set ya100.spark.filter.ydb_example_shu=ydbpartion='20151110' and (ydb_sex='≈Æ' or ydb_province='¡…ƒ˛' or ydb_day>='20151217');
select ydb_sex from ydb_example_shu  limit 10

set ya100.spark.filter.ydb_example_shu=ydbpartion='20151110' and (ydb_sex='≈Æ' or ydb_province='¡…ƒ˛' or ydb_day>='20151217');
select count(*) from ydb_example_shu limit 10

set ya100.spark.filter.ydb_example_shu=ydbpartion='20151110' and (ydb_sex in ('ƒ–','≈Æ','’≈»˝','¿ÓÀƒ'));
select ydb_sex,ydb_province from ydb_example_shu   limit 10

set ya100.spark.filter.ydb_example_shu=ydbpartion='20151110';
select count(*) from ydb_example_shu   limit 10



------------------ ‘≠ º” º˛ ------------------
∑¢º˛»À: ""ø™–ƒ—”ƒÍ"";<muyannian@qq.com>;
∑¢ÀÕ ±º‰: 2016ƒÍ1‘¬28»’(–«∆⁄Àƒ) ÕÌ…œ8:28
 ’º˛»À: ""ø™–ƒ—”ƒÍ""<muyannian@qq.com>; ""user""<user@spark.apache.org>; ""dev""<dev@spark.apache.org>; 

÷˜Ã‚: ªÿ∏¥£∫Why Spark-sql miss TableScanDesc.FILTER_EXPR_CONF_STR params when I move Hive table to Spark?



we always used Sql like below.

select count(*) from ydb_example_shu where ydbpartion='20151110' and (ydb_sex='' or ydb_province='LIAONING' or ydb_day>='20151217') limit 10

Spark don't push down predicates for TableScanDesc.FILTER_EXPR_CONF_STR, which means that every query is full scan can`t use the index (Something like HbaseStoreHandle).








------------------ ‘≠ º” º˛ ------------------
∑¢º˛»À: ""ø™–ƒ—”ƒÍ"";<muyannian@qq.com>;
∑¢ÀÕ ±º‰: 2016ƒÍ1‘¬28»’(–«∆⁄Àƒ) ÕÌ…œ7:27
 ’º˛»À: ""user""<user@spark.apache.org>; ""dev""<dev@spark.apache.org>; 

÷˜Ã‚: Why Spark-sql miss TableScanDesc.FILTER_EXPR_CONF_STR params when I move Hive table to Spark?



Dear spark
I am test StorageHandler on Spark-SQL.
but i find the TableScanDesc.FILTER_EXPR_CONF_STR is miss ,but i need it ,is three any where i could found it?
I really want to get some filter information from Spark Sql, so that I could make a pre filter by my Index ;
so where is the TableScanDesc.FILTER_EXPR_CONF_STR=hive.io.filter.expr.serialized? it is missing or replace by other method ,thanks every body ,thanks .


for example  I make a custorm StorageHandler like hive .

creat table xxx(...)
STORED BY 'cn.net.ycloud.ydb.handle.Ya100StorageHandler' 
TBLPROPERTIES(
""ya100.handler.master""=""101.200.130.48:8080"",
""ya100.handler.table.name""=""ydb_example_shu"",
""ya100.handler.columns.mapping""=""phonenum,usernick,ydb_sex,ydb_province,ydb_grade,ydb_age,ydb_blood,ydb_zhiye,ydb_earn,ydb_prefer,ydb_consume,ydb_day,content,ydbpartion,ya100_pipe""
)

in Ya100StorageHandler code .
I wang to use TableScanDesc.FILTER_EXPR_CONF_STR  like this

  String filterExprSerialized = conf.get(TableScanDesc.FILTER_EXPR_CONF_STR);
    if (filterExprSerialized == null) {
        return """";
//         throw new IOException(""can`t found filter condition in your Sql ,at least you must special a field as ydbpartion "");
    }else{
        LOG.info(filterExprSerialized);
        ExprNodeGenericFuncDesc filterExpr =    Utilities.deserializeExpression(filterExprSerialized);
        LOG.info(filterExpr);
        try {
            return Ya100Utils.parserFilter(filterExpr,info);
        } catch (Throwable e) {
            throw new IOException(e);
        }
    }"
=?utf-8?Q?J=C3=B6rn_Franke?= <jornfranke@gmail.com>,"Thu, 28 Jan 2016 14:09:20 +0100","=?utf-8?Q?Re:_=E5=9B=9E=E5=A4=8D=EF=BC=9A_Spark_1.6.0_+_Hive_+_H?=
 =?utf-8?Q?Base?=",=?utf-8?B?5byA5b+D5bu25bm0?= <muyannian@qq.com>,"Probably a newer Hive version makes a lot of sense here - at least 1.2.1. What storage format are you using?
I think the old Hive version had a bug where it always scanned all partitions unless you limit it in the on clause of the query to a certain partition (eg on date=20201119)

 query is full scan.

es>;
òüÊúüÂõõ) Êôö‰∏ä8:09
org>;
le. Nor Hive or Impala can.
ther than de rowkey:
 as your predicate, or
der to create a secondary index.
ibi√≥:
y query is full scan.
t=false)], output=[count#144L])
sDistinct=false)], output=[count#147L])
gnostics, sessions_hbase, None
ress.
ote:
:
08, in get_return_value
_map_int_str; line 55 pos 30
upFunction$2$$anonfun$1.apply(hiveUDFs.scala:69)
upFunction$2$$anonfun$1.apply(hiveUDFs.scala:69)
upFunction$2.apply(hiveUDFs.scala:68)
upFunction$2.apply(hiveUDFs.scala:64)
n(hiveUDFs.scala:64)
ions$$anonfun$apply$12$$anonfun$applyOrElse$5$$anonfun$applyOrElse$24.apply(Analyzer.scala:574)
ions$$anonfun$apply$12$$anonfun$applyOrElse$5$$anonfun$applyOrElse$24.apply(Analyzer.scala:574)
(package.scala:48)
ions$$anonfun$apply$12$$anonfun$applyOrElse$5.applyOrElse(Analyzer.scala:573)
ions$$anonfun$apply$12$$anonfun$applyOrElse$5.applyOrElse(Analyzer.scala:570)
ookeeper on localhost.
 not a solution.
3.4.6-1569965, built on 02/20/2014 09:09 GMT
yter1
.0_66
le Corporation
ib/jvm/java-8-oracle/jre
/opt/spark/lib/mysql-connector-java-5.1.35-bin.jar:/opt/spark/lib/dwh-hbase-connector.jar:/opt/spark/lib/hive-hbase-handler-1.2.1.spark.jar:/opt/spark/lib/hbase-server.jar:/opt/spark/lib/hbase-common.jar:/opt/spark/lib/dwh-commons.jar:/opt/spark/lib/guava.jar:/opt/spark/lib/hbase-client.jar:/opt/spark/lib/hbase-protocol.jar:/opt/spark/lib/htrace-core.jar:/opt/spark/conf/:/opt/spark/lib/spark-assembly-1.6.0-hadoop2.6.0.jar:/opt/spark/lib/datanucleus-rdbms-3.2.9.jar:/opt/spark/lib/datanucleus-api-jdo-3.2.6.jar:/opt/spark/lib/datanucleus-core-3.2.10.jar:/etc/hadoop/conf/
/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
tmp
A>
0-24-generic
ki
mbrynski
brynski
String=localhost:2181 sessionTimeout=90000 watcher=hconnection-0x36079f06, quorum=localhost:2181, baseZNode=/hbase
ection-0x36079f06 connecting to ZooKeeper ensemble=localhost:2181
ocalhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
alhost/127.0.0.1:2181, initiating session
rver localhost/127.0.0.1:2181, sessionid = 0x15254709ed3c8e1, negotiated timeout = 40000
s null
plicaNodes(ZooKeeperWatcher.java:269)
ilAvailable(MetaRegionTracker.java:241)
onLocation(ZooKeeperRegistry.java:62)
Implementation.locateMeta(ConnectionManager.java:1203)
Implementation.locateRegion(ConnectionManager.java:1164)
icas.getRegionLocations(RpcRetryingCallerWithReadReplicas.java:294)
all(ScannerCallableWithReplicas.java:130)
all(ScannerCallableWithReplicas.java:55)
Retries(RpcRetryingCaller.java:201)
"
"""=?utf-8?B?5byA5b+D5bu25bm0?="" <muyannian@qq.com>","Thu, 28 Jan 2016 21:20:20 +0800","=?utf-8?B?5Zue5aSN77yaIOWbnuWkje+8miBTcGFyayAxLjYu?=
 =?utf-8?B?MCArIEhpdmUgKyBIQmFzZQ==?=","""=?utf-8?B?SsO2cm4gRnJhbmtl?="" <jornfranke@gmail.com>","This not hive`s bug .I test hive on my storage is ok. 
but when i test it on spark-sql is not pass TableScanDesc.FILTER_EXPR_CONF_STR params;

so that is the reason cause the full scan.

the source code in HiveHBaseTableInputFormat is as follows,that is the reason caused full scan.


 private Scan createFilterScan(JobConf jobConf, int iKey, int iTimestamp, boolean isKeyBinary)
      throws IOException {

    // TODO: assert iKey is HBaseSerDe#HBASE_KEY_COL

    Scan scan = new Scan();
    String filterObjectSerialized = jobConf.get(TableScanDesc.FILTER_OBJECT_CONF_STR);
    if (filterObjectSerialized != null) {
      HBaseScanRange range = Utilities.deserializeObject(filterObjectSerialized,
          HBaseScanRange.class);
      try {
        range.setup(scan, jobConf);
      } catch (Exception e) {
        throw new IOException(e);
      }
      return scan;
    }

    String filterExprSerialized = jobConf.get(TableScanDesc.FILTER_EXPR_CONF_STR);
    if (filterExprSerialized == null) {
      return scan;
    }

    ExprNodeGenericFuncDesc filterExpr =
      Utilities.deserializeExpression(filterExprSerialized);

    String keyColName = jobConf.get(serdeConstants.LIST_COLUMNS).split("","")[iKey];
    String colType = jobConf.get(serdeConstants.LIST_COLUMN_TYPES).split("","")[iKey];
    boolean isKeyComparable = isKeyBinary || colType.equalsIgnoreCase(""string"");

    String tsColName = null;
    if (iTimestamp >= 0) {
      tsColName = jobConf.get(serdeConstants.LIST_COLUMNS).split("","")[iTimestamp];
    }--------
Âèë‰ª∂‰∫∫: ""J√∂rn Franke"";<jornfranke@gmail.com>;
ÂèëÈÄÅÊó∂Èó¥: 2016Âπ¥1Êúà28Êó•(ÊòüÊúüÂõõ) Êôö‰∏ä9:09
Êî∂‰ª∂‰∫∫: ""ÂºÄÂøÉÂª∂Âπ¥""<muyannian@qq.com>; 
ÊäÑÈÄÅ: ""Julio Antonio Soto de Vicente""<julio@esbet.es>; ""Maciej Bry≈Ñski""<maciek@brynski.pl>; ""Ted Yu""<yuzhihong@gmail.com>; ""dev""<dev@spark.apache.org>; 
‰∏ªÈ¢ò: Re: ÂõûÂ§çÔºö Spark 1.6.0 + Hive + HBase



Probably a newer Hive version makes a lot of sense here - at least 1.2.1. What storage format are you using?
I think the old Hive version had a bug where it always scanned all partitions unless you limit it in the on clause of the query to a certain partition (eg on date=20201119)

On 28 Jan 2016, at 13:42, ÂºÄÂøÉÂª∂Âπ¥ <muyannian@qq.com> wrote:



Is there any body can solve Problem 4)? thanks.
Problem 4)
Spark don't push down predicates for HiveTableScan, which means that every query is full scan.





------------------tonio Soto de Vicente"";<julio@esbet.es>;
ÂèëÈÄÅÊó∂Èó¥: 2016Âπ¥1Êúà28Êó•(ÊòüÊúüÂõõ) Êôö‰∏ä8:09
Êî∂‰ª∂‰∫∫: ""Maciej Bry≈Ñski""<maciek@brynski.pl>; 
ÊäÑÈÄÅ: ""Ted Yu""<yuzhihong@gmail.com>; ""dev""<dev@spark.apache.org>; 
‰∏ªÈ¢ò: Re: Spark 1.6.0 + Hive + HBase



Hi,


Indeed, Hive is not able to perform predicate pushdown through a HBase table. Nor Hive or Impala can.


Broadly speaking, if you need to query your  HBase table through a field other than de rowkey:


A) Try to ""encode"" as much info as possible in the rowkey field and use it as your predicate, or
B) Feel free to use other kind of storage system/create coprocessors in order to create a secondary index.



El 28 ene 2016, a las 12:56, Maciej Bry≈Ñski <maciek@brynski.pl> escribi√≥:


Ted,You're right.
hbase-site.xml resolved problems 2 and 3, but...


Problem 4)
Spark don't push down predicates for HiveTableScan, which means that every query is full scan.


== Physical Plan == TungstenAggregate(key=[], functions=[(count(1),mode=Final,isDistinct=false)], output=[count#144L]) +- TungstenExchange SinglePartition, None    +- TungstenAggregate(key=[], functions=[(count(1),mode=Partial,isDistinct=false)], output=[count#147L])       +- Project          +- Filter (added_date#141L >= 201601280000)             +- HiveTableScan [added_date#141L], MetastoreRelation dwh_diagnostics, sessions_hbase, None


Is there any magic option to make this work ?


Regards,
Maciek

2016-01-28 10:25 GMT+01:00 Ted Yu <yuzhihong@gmail.com>:

For the last two problems, hbase-site.xml seems not to be on classpath. 


Once hbase-site.xml is put on classpath, you should be able to make progress. 


Cheers

On Jan 28, 2016, at 1:14 AM, Maciej Bry≈Ñski <maciek@brynski.pl> wrote:


Hi,I'm trying to run SQL query on Hive table which is stored on HBase.
I'm using:

- Spark 1.6.0
- HDP 2.2
- Hive 0.14.0
- HBase 0.98.4


I managed to configure working classpath, but I have following problems:


1) I have UDF defined in Hive Metastore (FUNCS table).
Spark cannot use it.. 


 File ""/opt/spark/python/lib/py4j-0.9-src.zip/py4j/protocol.py"", line 308, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o51.sql.
: org.apache.spark.sql.AnalysisException: undefined function dwh.str_to_map_int_str; line 55 pos 30
        at org.apache.spark.sql.hive.HiveFunctionRegistry$$anonfun$lookupFunction$2$$anonfun$1.apply(hiveUDFs.scala:69)
        at org.apache.spark.sql.hive.HiveFunctionRegistry$$anonfun$lookupFunction$2$$anonfun$1.apply(hiveUDFs.scala:69)
        at scala.Option.getOrElse(Option.scala:120)
        at org.apache.spark.sql.hive.HiveFunctionRegistry$$anonfun$lookupFunction$2.apply(hiveUDFs.scala:68)
        at org.apache.spark.sql.hive.HiveFunctionRegistry$$anonfun$lookupFunction$2.apply(hiveUDFs.scala:64)
        at scala.util.Try.getOrElse(Try.scala:77)
        at org.apache.spark.sql.hive.HiveFunctionRegistry.lookupFunction(hiveUDFs.scala:64)
        at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$12$$anonfun$applyOrElse$5$$anonfun$applyOrElse$24.apply(Analyzer.scala:574)
        at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$12$$anonfun$applyOrElse$5$$anonfun$applyOrElse$24.apply(Analyzer.scala:574)
        at org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:48)
        at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$12$$anonfun$applyOrElse$5.applyOrElse(Analyzer.scala:573)
        at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$12$$anonfun$applyOrElse$5.applyOrElse(Analyzer.scala:570)





2) When I'm using SQL without this function Spark tries to connect to Zookeeper on localhost.
I make a tunnel from localhost to one of the zookeeper servers but it's not a solution.


16/01/28 10:09:18 INFO ZooKeeper: Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
16/01/28 10:09:18 INFO ZooKeeper: Client environment:host.name=j4.jupyter1
16/01/28 10:09:18 INFO ZooKeeper: Client environment:java.version=1.8.0_66
16/01/28 10:09:18 INFO ZooKeeper: Client environment:java.vendor=Oracle Corporation
16/01/28 10:09:18 INFO ZooKeeper: Client environment:java.home=/usr/lib/jvm/java-8-oracle/jre
16/01/28 10:09:18 INFO ZooKeeper: Client environment:java.class.path=/opt/spark/lib/mysql-connector-java-5.1.35-bin.jar:/opt/spark/lib/dwh-hbase-connector.jar:/opt/spark/lib/hive-hbase-handler-1.2.1.spark.jar:/opt/spark/lib/hbase-server.jar:/opt/spark/lib/hbase-common.jar:/opt/spark/lib/dwh-commons.jar:/opt/spark/lib/guava.jar:/opt/spark/lib/hbase-client.jar:/opt/spark/lib/hbase-protocol.jar:/opt/spark/lib/htrace-core.jar:/opt/spark/conf/:/opt/spark/lib/spark-assembly-1.6.0-hadoop2.6.0.jar:/opt/spark/lib/datanucleus-rdbms-3.2.9.jar:/opt/spark/lib/datanucleus-api-jdo-3.2.6.jar:/opt/spark/lib/datanucleus-core-3.2.10.jar:/etc/hadoop/conf/
16/01/28 10:09:18 INFO ZooKeeper: Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
16/01/28 10:09:18 INFO ZooKeeper: Client environment:java.io.tmpdir=/tmp
16/01/28 10:09:18 INFO ZooKeeper: Client environment:java.compiler=<NA>
16/01/28 10:09:18 INFO ZooKeeper: Client environment:os.name=Linux
16/01/28 10:09:18 INFO ZooKeeper: Client environment:os.arch=amd64
16/01/28 10:09:18 INFO ZooKeeper: Client environment:os.version=3.13.0-24-generic
16/01/28 10:09:18 INFO ZooKeeper: Client environment:user.name=mbrynski
16/01/28 10:09:18 INFO ZooKeeper: Client environment:user.home=/home/mbrynski
16/01/28 10:09:18 INFO ZooKeeper: 0:09:18 INFO ZooKeeper: Initiating client connection, connectString=localhost:2181 sessionTimeout=90000 watcher=hconnection-0x36079f06, quorum=localhost:2181, baseZNode=/hbase
16/01/28 10:09:18 INFO RecoverableZooKeeper: Process identifier=hconnection-0x36079f06 connecting to ZooKeeper ensemble=localhost:2181
16/01/28 10:09:18 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
16/01/28 10:09:18 INFO ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
16/01/28 10:09:18 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x15254709ed3c8e1, negotiated timeout = 40000
16/01/28 10:09:18 INFO ZooKeeperRegistry: ClusterId read in ZooKeeper is null





3) After making tunel I'm getting NPE.


Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.getMetaReplicaNodes(ZooKeeperWatcher.java:269)
        at org.apache.hadoop.hbase.zookeeper.MetaRegionTracker.blockUntilAvailable(MetaRegionTracker.java:241)
        at org.apache.hadoop.hbase.client.ZooKeeperRegistry.getMetaRegionLocation(ZooKeeperRegistry.java:62)
        at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateMeta(ConnectionManager.java:1203)
        at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion(ConnectionManager.java:1164)
        at org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas.getRegionLocations(RpcRetryingCallerWithReadReplicas.java:294)
        at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:130)
        at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:55)
        at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:201)
        ... 91 more


Do you have any ideas how to resolve those problems ?


Regards,
-- 
Maciek Bry≈Ñski
 


 








-- 
Maciek Bry≈Ñski"
=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"Thu, 28 Jan 2016 14:21:34 +0100","build error: code too big: specialStateTransition(int, IntStream)","""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

Has anyone seen this error?

The code of method specialStateTransition(int, IntStream) is exceeding
the 65535 bytes limit    SparkSqlParser_IdentifiersParser.java:39907

The error is in ANTLR generated files and it‚Äôs (according to Stack
Overflow) due to state explosion in parser (or lexer). That seems
plausible, given that one file has >50000 lines of code. Some suggest that
refactoring the grammar would help.

I‚Äôm seeing this error only sometimes on the command line (building with
Sbt), but every time when building with Eclipse (which has its own Java
compiler, so it‚Äôs not surprising that it has a different behavior). Same
behavior with both Java 1.7 and 1.8.

Any ideas?

iulian
‚Äã
-- 

--
Iulian Dragos

------
Reactive Apps on the JVM
www.typesafe.com
"
Hamel Kothari <hamelkothari@gmail.com>,"Thu, 28 Jan 2016 14:43:27 +0000",Heuristics for Partitioning Non-Local Data,dev <dev@spark.apache.org>,"Hey spark-devs,

I'm in the process of writing a DataSource for what is essentially a java
web service. Each relation which we create will consist of a series of
queries to this webservice which returns a pretty much known amount of data
(eg. 2000 rows, 5 string columns or similar which we can calculate size in
bytes from).

Now in creating an RDD for this web service, I have to provide the
Partitions by which to break up the RDD. Naively, I can break it up over a
handful of the attributes we query by, but I was wondering if there are any
heuristics you guys would suggest considering that we know roughly how much
data will be fetched from the webservice during each call.

Using information from the SparkContext about executor size/number of
executors/free memory, etc, is there anything smart I could do with regards
to how I size my partitions? Has anyone done this before or is it a bad
idea?

Thanks,
Hamel
"
Ted Yu <yuzhihong@gmail.com>,"Thu, 28 Jan 2016 07:35:33 -0800",=?UTF-8?B?UmU6IOWbnuWkje+8miBTcGFyayAxLjYuMCArIEhpdmUgKyBIQmFzZQ==?=,=?UTF-8?B?5byA5b+D5bu25bm0?= <muyannian@qq.com>,"Under sql/hive/src/main/scala/org/apache/spark/sql/hive/execution , I only
see HiveTableScan and HiveNativeCommand
At the beginning of HiveTableScan :

 * The Hive table scan operator.  Column and partition pruning are both
handled.

Looks like filter pushdown hasn't been implemented.

As far as I know, Huawei's Astro can do filter pushdown :

 http://spark-packages.org/package/Huawei-Spark/Spark-SQL-on-HBase


-
•(ÊòüÊúüÂõõ) Êôö‰∏ä9:09
annian@qq.com>;
aciej Bry≈Ñski""<
+ HBase
y
-
et.es>;
•(ÊòüÊúüÂõõ) Êôö‰∏ä8:09
he.org>;
t
ibi√≥:
y
t=false)], output=[count#144L])
sDistinct=false)], output=[count#147L])
gnostics, sessions_hbase, None
te:
$$anonfun$1.apply(hiveUDFs.scala:69)
$$anonfun$1.apply(hiveUDFs.scala:69)
.apply(hiveUDFs.scala:68)
.apply(hiveUDFs.scala:64)
cala:64)
n$apply$12$$anonfun$applyOrElse$5$$anonfun$applyOrElse$24.apply(Analyzer.scala:574)
n$apply$12$$anonfun$applyOrElse$5$$anonfun$applyOrElse$24.apply(Analyzer.scala:574)
la:48)
n$apply$12$$anonfun$applyOrElse$5.applyOrElse(Analyzer.scala:573)
n$apply$12$$anonfun$applyOrElse$5.applyOrElse(Analyzer.scala:570)
 GMT
0_66
e
-bin.jar:/opt/spark/lib/dwh-hbase-connector.jar:/opt/spark/lib/hive-hbase-handler-1.2.1.spark.jar:/opt/spark/lib/hbase-server.jar:/opt/spark/lib/hbase-common.jar:/opt/spark/lib/dwh-commons.jar:/opt/spark/lib/guava.jar:/opt/spark/lib/hbase-client.jar:/opt/spark/lib/hbase-protocol.jar:/opt/spark/lib/htrace-core.jar:/opt/spark/conf/:/opt/spark/lib/spark-assembly-1.6.0-hadoop2.6.0.jar:/opt/spark/lib/datanucleus-rdbms-3.2.9.jar:/opt/spark/lib/datanucleus-api-jdo-3.2.6.jar:/opt/spark/lib/datanucleus-core-3.2.10.jar:/etc/hadoop/conf/
/lib64:/lib:/usr/lib
mp
i
hbase
ooKeeperWatcher.java:269)
MetaRegionTracker.java:241)
ooKeeperRegistry.java:62)
on.locateMeta(ConnectionManager.java:1203)
on.locateRegion(ConnectionManager.java:1164)
onLocations(RpcRetryingCallerWithReadReplicas.java:294)
allableWithReplicas.java:130)
allableWithReplicas.java:55)
etryingCaller.java:201)
"
"""=?utf-8?B?5byA5b+D5bu25bm0?="" <muyannian@qq.com>","Thu, 28 Jan 2016 23:42:15 +0800","=?utf-8?B?5Zue5aSN77yaIOWbnuWkje+8miBTcGFyayAxLjYu?=
 =?utf-8?B?MCArIEhpdmUgKyBIQmFzZQ==?=","""=?utf-8?B?VGVkIFl1?="" <yuzhihong@gmail.com>","Thanks Ted ,I will try on this version.




------------------ ÂéüÂßãÈÇÆ‰ª∂ ------------------
Âèë‰ª∂‰∫∫: ""Ted Yu"";<yuzhihong@gmail.com>;
ÂèëÈÄÅÊó∂Èó¥: 2016Âπ¥1Êúà28Êó•(ÊòüÊúüÂõõ) Êôö‰∏ä11:35
Êî∂‰ª∂‰∫∫: ""ÂºÄÂøÉÂª∂Âπ¥""<muyannian@qq.com>; 
ÊäÑÈÄÅ: ""J√∂rn Franke""<jornfranke@gmail.com>; ""Julio Antonio Soto de Vicente""<julio@esbet.es>; ""Maciej Bry≈Ñski""<maciek@brynski.pl>; ""dev""<dev@spark.apache.org>; 
‰∏ªÈ¢ò: Re: ÂõûÂ§çÔºö Spark 1.6.0 + Hive + HBase



Under sql/hive/src/main/scala/org/apache/spark/sql/hive/execution , I only see HiveTableScan and HiveNativeCommandAt the beginning of HiveTableScan :


 * The Hive table scan operator.  Column and partition pruning are both handled.


Looks like filter pushdown hasn't been implemented.


As far as I know, Huawei's Astro can do filter pushdown :


 http://spark-packages.org/package/Huawei-Spark/Spark-SQL-on-HBase



On Thu, Jan 28, 2016 at 5:20 AM, ÂºÄÂøÉÂª∂Âπ¥ <muyannian@qq.com> wrote:
This not hive`s bug .I test hive on my storage is ok. 
but when i test it on spark-sql is not pass TableScanDesc.FILTER_EXPR_CONF_STR params;

so that is the reason cause the full scan.

the source code in HiveHBaseTableInputFormat is as follows,that is the reason caused full scan.


 private Scan createFilterScan(JobConf jobConf, int iKey, int iTimestamp, boolean isKeyBinary)
      throws IOException {

    // TODO: assert iKey is HBaseSerDe#HBASE_KEY_COL

    Scan scan = new Scan();
    String filterObjectSerialized = jobConf.get(TableScanDesc.FILTER_OBJECT_CONF_STR);
    if (filterObjectSerialized != null) {
      HBaseScanRange range = Utilities.deserializeObject(filterObjectSerialized,
          HBaseScanRange.class);
      try {
        range.setup(scan, jobConf);
      } catch (Exception e) {
        throw new IOException(e);
      }
      return scan;
    }

    String filterExprSerialized = jobConf.get(TableScanDesc.FILTER_EXPR_CONF_STR);
    if (filterExprSerialized == null) {
      return scan;
    }

    ExprNodeGenericFuncDesc filterExpr =
      Utilities.deserializeExpression(filterExprSerialized);

    String keyColName = jobConf.get(serdeConstants.LIST_COLUMNS).split("","")[iKey];
    String colType = jobConf.get(serdeConstants.LIST_COLUMN_TYPES).split("","")[iKey];
    boolean isKeyComparable = isKeyBinary || colType.equalsIgnoreCase(""string"");

    String tsColName = null;
    if (iTimestamp >= 0) {
      tsColName = jobConf.get(serdeConstants.LIST_COLUMNS).split("","")[iTimestamp];
  -----------
Âèë‰ª∂‰∫∫: ""J√∂rn Franke"";<jornfranke@gmail.com>;
ÂèëÈÄÅÊó∂Èó¥: 2016Âπ¥1Êúà28Êó•(ÊòüÊúüÂõõ) Êôö‰∏ä9:09
Êî∂‰ª∂‰∫∫: ""ÂºÄÂøÉÂª∂Âπ¥""<muyannian@qq.com>; 
ÊäÑÈÄÅ: ""Julio Antonio Soto de Vicente""<julio@esbet.es>; ""Maciej Bry≈Ñski""<maciek@brynski.pl>; ""Ted Yu""<yuzhihong@gmail.com>; ""dev""<dev@spark.apache.org>; 
‰∏ªÈ¢ò: Re: ÂõûÂ§çÔºö Spark 1.6.0 + Hive + HBase



Probably a newer Hive version makes a lot of sense here - at least 1.2.1. What storage format are you using?
I think the old Hive version had a bug where it always scanned all partitions unless you limit it in the on clause of the query to a certain partition (eg on date=20201119)

On 28 Jan 2016, at 13:42, ÂºÄÂøÉÂª∂Âπ¥ <muyannian@qq.com> wrote:



Is there any body can solve Problem 4)? thanks.
Problem 4)
Spark don't push down predicates for HiveTableScan, which means that every query is full scan.





--------------- Antonio Soto de Vicente"";<julio@esbet.es>;
ÂèëÈÄÅÊó∂Èó¥: 2016Âπ¥1Êúà28Êó•(ÊòüÊúüÂõõ) Êôö‰∏ä8:09
Êî∂‰ª∂‰∫∫: ""Maciej Bry≈Ñski""<maciek@brynski.pl>; 
ÊäÑÈÄÅ: ""Ted Yu""<yuzhihong@gmail.com>; ""dev""<dev@spark.apache.org>; 
‰∏ªÈ¢ò: Re: Spark 1.6.0 + Hive + HBase



Hi,


Indeed, Hive is not able to perform predicate pushdown through a HBase table. Nor Hive or Impala can.


Broadly speaking, if you need to query your  HBase table through a field other than de rowkey:


A) Try to ""encode"" as much info as possible in the rowkey field and use it as your predicate, or
B) Feel free to use other kind of storage system/create coprocessors in order to create a secondary index.



El 28 ene 2016, a las 12:56, Maciej Bry≈Ñski <maciek@brynski.pl> escribi√≥:


Ted,You're right.
hbase-site.xml resolved problems 2 and 3, but...


Problem 4)
Spark don't push down predicates for HiveTableScan, which means that every query is full scan.


== Physical Plan == TungstenAggregate(key=[], functions=[(count(1),mode=Final,isDistinct=false)], output=[count#144L]) +- TungstenExchange SinglePartition, None    +- TungstenAggregate(key=[], functions=[(count(1),mode=Partial,isDistinct=false)], output=[count#147L])       +- Project          +- Filter (added_date#141L >= 201601280000)             +- HiveTableScan [added_date#141L], MetastoreRelation dwh_diagnostics, sessions_hbase, None


Is there any magic option to make this work ?


Regards,
Maciek

2016-01-28 10:25 GMT+01:00 Ted Yu <yuzhihong@gmail.com>:

For the last two problems, hbase-site.xml seems not to be on classpath. 


Once hbase-site.xml is put on classpath, you should be able to make progress. 


Cheers

On Jan 28, 2016, at 1:14 AM, Maciej Bry≈Ñski <maciek@brynski.pl> wrote:


Hi,I'm trying to run SQL query on Hive table which is stored on HBase.
I'm using:

- Spark 1.6.0
- HDP 2.2
- Hive 0.14.0
- HBase 0.98.4


I managed to configure working classpath, but I have following problems:


1) I have UDF defined in Hive Metastore (FUNCS table).
Spark cannot use it.. 


 File ""/opt/spark/python/lib/py4j-0.9-src.zip/py4j/protocol.py"", line 308, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o51.sql.
: org.apache.spark.sql.AnalysisException: undefined function dwh.str_to_map_int_str; line 55 pos 30
        at org.apache.spark.sql.hive.HiveFunctionRegistry$$anonfun$lookupFunction$2$$anonfun$1.apply(hiveUDFs.scala:69)
        at org.apache.spark.sql.hive.HiveFunctionRegistry$$anonfun$lookupFunction$2$$anonfun$1.apply(hiveUDFs.scala:69)
        at scala.Option.getOrElse(Option.scala:120)
        at org.apache.spark.sql.hive.HiveFunctionRegistry$$anonfun$lookupFunction$2.apply(hiveUDFs.scala:68)
        at org.apache.spark.sql.hive.HiveFunctionRegistry$$anonfun$lookupFunction$2.apply(hiveUDFs.scala:64)
        at scala.util.Try.getOrElse(Try.scala:77)
        at org.apache.spark.sql.hive.HiveFunctionRegistry.lookupFunction(hiveUDFs.scala:64)
        at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$12$$anonfun$applyOrElse$5$$anonfun$applyOrElse$24.apply(Analyzer.scala:574)
        at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$12$$anonfun$applyOrElse$5$$anonfun$applyOrElse$24.apply(Analyzer.scala:574)
        at org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:48)
        at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$12$$anonfun$applyOrElse$5.applyOrElse(Analyzer.scala:573)
        at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$12$$anonfun$applyOrElse$5.applyOrElse(Analyzer.scala:570)





2) When I'm using SQL without this function Spark tries to connect to Zookeeper on localhost.
I make a tunnel from localhost to one of the zookeeper servers but it's not a solution.


16/01/28 10:09:18 INFO ZooKeeper: Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
16/01/28 10:09:18 INFO ZooKeeper: Client environment:host.name=j4.jupyter1
16/01/28 10:09:18 INFO ZooKeeper: Client environment:java.version=1.8.0_66
16/01/28 10:09:18 INFO ZooKeeper: Client environment:java.vendor=Oracle Corporation
16/01/28 10:09:18 INFO ZooKeeper: Client environment:java.home=/usr/lib/jvm/java-8-oracle/jre
16/01/28 10:09:18 INFO ZooKeeper: Client environment:java.class.path=/opt/spark/lib/mysql-connector-java-5.1.35-bin.jar:/opt/spark/lib/dwh-hbase-connector.jar:/opt/spark/lib/hive-hbase-handler-1.2.1.spark.jar:/opt/spark/lib/hbase-server.jar:/opt/spark/lib/hbase-common.jar:/opt/spark/lib/dwh-commons.jar:/opt/spark/lib/guava.jar:/opt/spark/lib/hbase-client.jar:/opt/spark/lib/hbase-protocol.jar:/opt/spark/lib/htrace-core.jar:/opt/spark/conf/:/opt/spark/lib/spark-assembly-1.6.0-hadoop2.6.0.jar:/opt/spark/lib/datanucleus-rdbms-3.2.9.jar:/opt/spark/lib/datanucleus-api-jdo-3.2.6.jar:/opt/spark/lib/datanucleus-core-3.2.10.jar:/etc/hadoop/conf/
16/01/28 10:09:18 INFO ZooKeeper: Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
16/01/28 10:09:18 INFO ZooKeeper: Client environment:java.io.tmpdir=/tmp
16/01/28 10:09:18 INFO ZooKeeper: Client environment:java.compiler=<NA>
16/01/28 10:09:18 INFO ZooKeeper: Client environment:os.name=Linux
16/01/28 10:09:18 INFO ZooKeeper: Client environment:os.arch=amd64
16/01/28 10:09:18 INFO ZooKeeper: Client environment:os.version=3.13.0-24-generic
16/01/28 10:09:18 INFO ZooKeeper: Client environment:user.name=mbrynski
16/01/28 10:09:18 INFO ZooKeeper: Client environment:user.home=/home/mbrynski
16/01/28 10:09:18 INFO ZooKeepe8 10:09:18 INFO ZooKeeper: Initiating client connection, connectString=localhost:2181 sessionTimeout=90000 watcher=hconnection-0x36079f06, quorum=localhost:2181, baseZNode=/hbase
16/01/28 10:09:18 INFO RecoverableZooKeeper: Process identifier=hconnection-0x36079f06 connecting to ZooKeeper ensemble=localhost:2181
16/01/28 10:09:18 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
16/01/28 10:09:18 INFO ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
16/01/28 10:09:18 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x15254709ed3c8e1, negotiated timeout = 40000
16/01/28 10:09:18 INFO ZooKeeperRegistry: ClusterId read in ZooKeeper is null





3) After making tunel I'm getting NPE.


Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.getMetaReplicaNodes(ZooKeeperWatcher.java:269)
        at org.apache.hadoop.hbase.zookeeper.MetaRegionTracker.blockUntilAvailable(MetaRegionTracker.java:241)
        at org.apache.hadoop.hbase.client.ZooKeeperRegistry.getMetaRegionLocation(ZooKeeperRegistry.java:62)
        at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateMeta(ConnectionManager.java:1203)
        at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion(ConnectionManager.java:1164)
        at org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas.getRegionLocations(RpcRetryingCallerWithReadReplicas.java:294)
        at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:130)
        at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:55)
        at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:201)
        ... 91 more


Do you have any ideas how to resolve those problems ?


Regards,
-- 
Maciek Bry≈Ñski
 


 








-- 
Maciek Bry≈Ñski"
Ted Yu <yuzhihong@gmail.com>,"Thu, 28 Jan 2016 07:43:26 -0800","Re: build error: code too big: specialStateTransition(int, IntStream)",=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"After this change:
    [SPARK-12681] [SQL] split IdentifiersParser.g into two files

the biggest file under
sql/catalyst/src/main/antlr3/org/apache/spark/sql/catalyst/parser is
SparkSqlParser.g

Maybe split SparkSqlParser.g up as well ?

.com>

e 65535 bytes limit    SparkSqlParser_IdentifiersParser.java:39907
k
t
g with
). Same
"
=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"Thu, 28 Jan 2016 17:30:08 +0100","Re: build error: code too big: specialStateTransition(int, IntStream)",Ted Yu <yuzhihong@gmail.com>,"Thanks for the pointer. It seems to be really a pathological case, since
the file that's in error is part of the splinter file (the smaller one,
IndetifiersParser). I'll see if I can work around by splitting it some more.

iulian


kSqlParser.g
fe.com
he 65535 bytes limit    SparkSqlParser_IdentifiersParser.java:39907
ck
at
ng with
r). Same


-- 

--
Iulian Dragos

------
Reactive Apps on the JVM
www.typesafe.com
"
=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@questtec.nl>,"Thu, 28 Jan 2016 17:35:20 +0100","Re: build error: code too big: specialStateTransition(int, IntStream)",Ted Yu <yuzhihong@gmail.com>,"Hi,

I have only encountered 'code too large' errors when changing grammars. I
am using SBT/Idea, no Eclipse.

The size of an ANTLR Parser/Lexer is dependent on the rules inside the
source grammar and the rules it depends on. So we should take a look at the
IdentifiersParser.g/ExpressionParser.g; the problem is probably caused by
the nonReserved rule.

HTH

Kind regards,

Herman van H√∂vell


2016-01-28 16:43 GMT+01:00 Ted Yu <yuzhihong@gmail.com>:

kSqlParser.g
fe.com
he 65535 bytes limit    SparkSqlParser_IdentifiersParser.java:39907
ck
at
ng with
r). Same
"
satyajit vegesna <satyajit.apasprk@gmail.com>,"Thu, 28 Jan 2016 16:22:42 -0800",Data not getting printed in Spark Streaming with print().,"user@spark.apache.org, dev@spark.apache.org","HI All,

I am trying to run HdfsWordCount example from github.

https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/streaming/HdfsWordCount.scala

i am using ubuntu to run the program, but dont see any data getting printed
after ,
-------------------------------------------
Time: 1454026800000 ms
-------------------------------------------

I dont see any errors, the program just runs, but i do not see any output
of the data corresponding to the file used.

object HdfsStream {

  def main(args:Array[String]): Unit = {

    val sparkConf = new
SparkConf().setAppName(""SpoolDirSpark"").setMaster(""local[5]"")
    val ssc = new StreamingContext(sparkConf, Minutes(10))

    //val inputDirectory = ""hdfs://localhost:9000/SpoolDirSpark""
    //val inputDirectory = ""hdfs://localhost:9000/SpoolDirSpark/test.txt""
    val inputDirectory = ""file:///home/satyajit/jsondata/""

    val lines =
ssc.fileStream[LongWritable,Text,TextInputFormat](inputDirectory).map{case(x,y)=>
(x.toString,y.toString)}
    //lines.saveAsTextFiles(""hdfs://localhost:9000/SpoolDirSpark/datacheck"")
    lines.saveAsTextFiles(""file:///home/satyajit/jsondata/"")

    println(""check_data""+lines.print())

    ssc.start()
    ssc.awaitTermination()

Would like to know if there is any workaround, or if there is something i
am missing.

Thanking in advance,
Satyajit.
"
"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Thu, 28 Jan 2016 16:33:38 -0800",Re: Data not getting printed in Spark Streaming with print().,satyajit vegesna <satyajit.apasprk@gmail.com>,"processing only new files and ignore existing files in the directory. So
you need to ***move*** the files into the directory, otherwise it will
ignore existing files.

process all existing files.


"
Gireesh Puthumana <gireesh.puthumana@augmentiq.in>,"Fri, 29 Jan 2016 09:40:02 +0530",Persisting of DataFrames in transformation workflows,"dev@spark.apache.org, user@spark.apache.org","Hi All,

I am trying to run a series of transformation over 3 DataFrames. After each
transformation, I want to persist DF and save it to text file. The steps I
am doing is as follows.

*Step0:*
Create DF1
Create DF2
Create DF3
Create DF4
(no persist no save yet)

*Step1:*
Create RESULT-DF1 by joining DF1 and DF2
Persist it to disk and memory
Save it to text file

*Step2:*
Create RESULT-DF2 by joining RESULT-DF1 and DF3
Persist it to disk and memory
Save it to text file

*Step3:*
Create RESULT-DF3 by joining RESULT-DF2 and DF4
Persist it to disk and memory
Save it to text file

*Observation:*
Number of tasks created at Step1 is 601
Number of tasks created at Step2 is 1004 (Didn't skip anything)
Number of tasks created at Step3 is 1400 (Skipped 400 tasks)

As different approach, I broke above steps into three different runs. ie;

   - Start, Load DF1 and DF2, Do Step1, Save RESULT-DF1 & exit
   - Start, Load DF3, Load RESULT-DF1 from file, do Step2, save RESULT-DF2
   & exit
   - Start, Load DF4, Load RESULT-DF2 from file, do Step3, save RESULT-DF3
   & exit

Later approach runs faster.

*My question is:*

   1. Am missing something on the persisting side in first approach?
   2. Why Step2 run didn't just use result from Step1 without redoing all
   it's tasks even after persisting (with only 601 tasks instead of 1004)?
   3. What are some good reads about best practices, when implementing such
   series of transformation workflows?

Thanks in advance,
Gireesh
"
Lorena Reis <loreis1992@gmail.com>,"Fri, 29 Jan 2016 07:14:13 -0200",How is Spark built on top of the Akka framework?,"dev@spark.apache.org, rxin@databricks.com","Hi all,

I'd like to know, how is Spark built on top of the Akka framework? Are
there information about that?

Thanks in advance.
"
Reynold Xin <rxin@databricks.com>,"Fri, 29 Jan 2016 01:16:57 -0800",Re: How is Spark built on top of the Akka framework?,Lorena Reis <loreis1992@gmail.com>,"As of Spark 2.0 (not yet released), Spark does not use Akka any more.

See https://issues.apache.org/jira/browse/SPARK-5293


"
Joseph Bradley <joseph@databricks.com>,"Fri, 29 Jan 2016 11:48:53 -0800",Re: Adding Naive Bayes sample code in Documentation,Vinayak Agrawal <vinayakagrawal88@gmail.com>,"JIRA created!  https://issues.apache.org/jira/browse/SPARK-13089
Feel free to pick it up if you're interested.  : )
Joseph


"
Jakob Odersky <jakob@odersky.com>,"Fri, 29 Jan 2016 11:50:25 -0800",Re: Spark 2.0.0 release plan,Deenar Toraskar <deenar.toraskar@gmail.com>,"I'm not an authoritative source but I think it is indeed the plan to
move the default build to 2.11.

See this discussion for more detail
http://apache-spark-developers-list.1001551.n3.nabble.com/A-proposal-for-Spark-2-0-td15122.html


---------------------------------------------------------------------


"
Mark Hamstra <mark@clearstorydata.com>,"Fri, 29 Jan 2016 12:02:14 -0800",Re: Spark 2.0.0 release plan,Jakob Odersky <jakob@odersky.com>,"https://github.com/apache/spark/pull/10608


"
Michael Armbrust <michael@databricks.com>,"Fri, 29 Jan 2016 12:02:03 -0800",Re: Spark 2.0.0 release plan,Jakob Odersky <jakob@odersky.com>,"Its already underway: https://github.com/apache/spark/pull/10608


"
deenar <deenar.toraskar@thinkreactive.co.uk>,"Fri, 29 Jan 2016 13:16:01 -0700 (MST)",Re: Spark 1.6.1,dev@spark.apache.org,"Hi Michael

The Dataset aggregators do not appear to support complex Spark-SQL types. I
wasn't sure if I was doing something wrong or if this was a bug or a feature
not implemented yet. Having this in would be great. See below (reposting
this from the spark user list)

https://docs.cloud.databricks.com/docs/spark/1.6/index.html#examples/Dataset%20Aggregator.html

I have been converting my UDAFs to Dataset (Dataset's are cool BTW)
Aggregators. I have an ArraySum aggregator that does an element wise sum or
arrays. I have got the simple version working, but the Generic version fails
with the following error, not sure what I am doing wrong.

scala> import sqlContext.implicits._

scala> def arraySum[I, N : Numeric : Encoder](f: I => N): TypedColumn[I, N]
= new GenericArraySumAggregator(f).toColumn

<console>:34: error: Unable to find encoder for type stored in a Dataset. 
Primitive types (Int, String, etc) and Product types (case classes) are
supported by importing sqlContext.implicits._  Support for serializing other
types will be added in future releases.

         def arraySum[I, N : Numeric : Encoder](f: I => N): TypedColumn[I,
N] = new GenericArraySumAggregator(f).toColumn

                                                                                                                
^

object ArraySumAggregator extends  Aggregator[Seq[Float], Seq[Float],
Seq[Float]] with Serializable {
  def zero: Seq[Float] = Nil
  // The initial value.
  def reduce(currentSum: Seq[Float], currentRow: Seq[Float]) =
sumArray(currentSum, currentRow)
  def merge(sum: Seq[Float], row: Seq[Float]) = sumArray(sum, row)
  def finish(b: Seq[Float]) = b // Return the final result.
  def sumArray(a: Seq[Float], b: Seq[Float]): Seq[Float] = {
    (a, b) match {
      case (Nil, Nil) => Nil
      case (Nil, row) => row
      case (sum, Nil) => sum
      case (sum, row) => (a, b).zipped.map { case (a, b) => a + b }
    }
  }
}
class GenericArraySumAggregator[I, N : Numeric](f: I => N) extends
Aggregator[Seq[I], Seq[N], Seq[N]] with Serializable {
  val numeric = implicitly[Numeric[N]]
  override def zero: Seq[N] = Nil
  override def reduce(b: Seq[N], a: Seq[I]): Seq[N] = sumArray(b, a.map( x
=> f(x))) //numeric.plus(b, f(a))
  override def merge(b1: Seq[N],b2: Seq[N]): Seq[N] = sumArray(b1, b2)
  override def finish(reduction: Seq[N]): Seq[N] = reduction
  def sumArray(a: Seq[N], b: Seq[N]): Seq[N] = {
    (a, b) match {
      case (Nil, Nil) => Nil
      case (Nil, row) => row
      case (sum, Nil) => sum
      case (sum, row) => (a, b).zipped.map { case (a, b) => numeric.plus(a,
b) }
    }
  }
}



Regards
Deenar



--

---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Fri, 29 Jan 2016 12:29:26 -0800",Re: Spark 1.6.1,deenar <deenar.toraskar@thinkreactive.co.uk>,"I think this is fixed in branch-1.6 already.  If you can reproduce it there
can you please open a JIRA and ping me?


"
Reynold Xin <rxin@databricks.com>,"Sat, 30 Jan 2016 00:22:10 -0800",Scala 2.11 default build,"""dev@spark.apache.org"" <dev@spark.apache.org>","FYI - I just merged Josh's pull request to switch to Scala 2.11 as the
default build.

https://github.com/apache/spark/pull/10608
"
Yash Sharma <yash360@gmail.com>,"Sat, 30 Jan 2016 19:49:58 +1100",Spark not able to fetch events from Amazon Kinesis,dev@spark.apache.org,"Hi All,
I have a quick question if anyone has experienced this here.

I have been trying to get Spark read events from Kinesis recently but am
having problem in receiving the events. While Spark is able to connect to
Kinesis and is able to get metadata from Kinesis, Its not able to get
events from it. It always fetches zero elements back.

There are no errors, just empty results back. Spark is able to get metadata
(Eg. number of shards in kinesis etc).

I have used these [1 & 2] guides for getting it working but have not got
much luck yet. I have also tried couple of suggestions from SO [3]. The
cluster has sufficient resources/cores available.

We have seen a version conflict in Protobuf Version between Spark and
Kinesis which could also be a cause for this behavior. Spark uses protobuf-java
version 2.5.0 and kinesis probably uses protobuf-java-2.6.1.jar.

Just wondered if anyone has come across this behavior or, has got spark
working with kinesis.

Have tried with Spark 1.5.0, Spark 1.6.0.

Appreciate any pointers.

Best Regards,
Yash

1. http://spark.apache.org/docs/latest/streaming-kinesis-integration.html
2.
https://github.com/apache/spark/blob/master/extras/kinesis-asl/src/main/scala/org/apache/spark/examples/streaming/KinesisWordCountASL.scala

3.
http://stackoverflow.com/questions/26941844/apache-spark-kinesis-sample-not-working
"
"""Driesprong, Fokko"" <fokko@driesprong.frl>","Sat, 30 Jan 2016 11:51:39 +0100",Re: Scala 2.11 default build,Reynold Xin <rxin@databricks.com>,"Nice, good work!

I've been using a Docker container to compile against 2.11:
https://github.com/fokko/docker-spark

Cheers, Fokko


2016-01-30 9:22 GMT+01:00 Reynold Xin <rxin@databricks.com>:

"
Ted Yu <yuzhihong@gmail.com>,"Sat, 30 Jan 2016 03:04:51 -0800",Re: Scala 2.11 default build,"""Driesprong, Fokko"" <fokko@driesprong.frl>","Does this mean the following Jenkins builds can be disabled ?

https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Compile/job/SPARK-master-COMPILE-MAVEN-SCALA-2.11/
https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Compile/job/SPARK-master-COMPILE-sbt-SCALA-2.11/

Cheers


"
Ted Yu <yuzhihong@gmail.com>,"Sat, 30 Jan 2016 03:23:27 -0800",Re: Spark not able to fetch events from Amazon Kinesis,Yash Sharma <yash360@gmail.com>,"w.r.t. protobuf-java version mismatch, I wonder if you can rebuild Spark
with the following change (using maven):

http://pastebin.com/fVQAYWHM

Cheers


"
Yash Sharma <yash360@gmail.com>,"Sun, 31 Jan 2016 00:36:34 +1100",Re: Spark not able to fetch events from Amazon Kinesis,Ted Yu <yuzhihong@gmail.com>,"Thanks Ted, Rebuilding would not be possible for the setup unfortunately so
just wanted to check if the version mismatch is the primary issue here.
Wanted to know if anyone has hit across similar issue and how they have
solved this.

Thanks


"
Burak Yavuz <brkyvz@gmail.com>,"Sat, 30 Jan 2016 09:41:27 -0800",Re: Spark not able to fetch events from Amazon Kinesis,Yash Sharma <yash360@gmail.com>,"Hi Yash,

I've run into multiple problems due to version incompatibilities, either
due to protobuf or jackson. That may be your culprit. The problem is that
all failures by the Kinesis Client Lib is silent, therefore don't show up
on the logs. It's very hard to debug those buggers.

Best,
Burak


"
Acelot <acelot23@gmail.com>,"Sat, 30 Jan 2016 23:23:37 +0100",Proposal,dev@spark.apache.org,"Hi All,
As part of my final project at university I would try to build an 
alternative version of k-means algorithm, it's called k-modes introduced 
here: Improving the Accuracy and Efficiency of the k-means Clustering 
Algorithm paper (Link: 
http://www.iaeng.org/publication/WCE2009/WCE2009_pp308-312.pdf). I would 
like to know any related work. If someone is interested to work in this 
project contact with me,
Kind regards,

---------------------------------------------------------------------


"
Yash Sharma <yash360@gmail.com>,"Sun, 31 Jan 2016 12:11:15 +1100",Re: Spark not able to fetch events from Amazon Kinesis,Burak Yavuz <brkyvz@gmail.com>,"Thanks Burak,
By any chance were you able to work around these errors or get the setup
working ? Is there anything else that you might have tried ?

Regards


"
Nick Pentreath <nick.pentreath@gmail.com>,"Sun, 31 Jan 2016 08:31:41 +0200",Re: Proposal,dev@spark.apache.org,"Hi there

Sounds like a fun project :)

I'd recommend getting familiar with the existing k-means implementation as well as bisecting k-means in Spark, and then implementing yours based off that. You should focus on using the new ML pipelines API, and release it as a package on spark-packages.org.

If it got lots of use cases from there, it could be considered for inclusion in ML core in the future.

Good luck!

Sent from my iPhone

ive version of k-means algorithm, it's called k-modes introduced here: Improving the Accuracy and Efficiency of the k-means Clustering Algorithm paper (Link: http://www.iaeng.org/publication/WCE2009/WCE2009_pp308-312.pdf). I would like to know any related work. If someone is interested to work in this project contact with me,

---------------------------------------------------------------------


"
Justin Uang <justin.uang@gmail.com>,"Sun, 31 Jan 2016 21:37:30 +0000",Making BatchPythonEvaluation actually Batch,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey guys,

BLUF: sorry for the length of this email, trying to figure out how to batch
Python UDF executions, and since this is my first time messing with
catalyst, would like any feedback

My team is starting to use PySpark UDFs quite heavily, and performance is a
huge blocker. The extra roundtrip serialization from Java to Python is not
a huge concern if we only incur it ~once per column for most workflows,
since it'll be in the same order of magnitude as reading files from disk.
However, right now each Python UDFs lead to a single roundtrip. There is
definitely a lot we can do regarding this:

(all the prototyping code is here:
https://github.com/justinuang/spark/commit/8176749f8a6e6dc5a49fbbb952735ff40fb309fc
)

1. We can't chain Python UDFs.

    df.select(python_times_2(python_times_2(""col1"")))

throws an exception saying that the inner expression isn't evaluable. The
workaround is to do


df.select(python_times_2(""col1"").alias(""tmp"")).select(python_time_2(""tmp""))

This can be solved in ExtractPythonUDFs by always extracting the inner most
Python UDF first.

         // Pick the UDF we are going to evaluate (TODO: Support evaluating
multiple UDFs at a time)
         // If there is more than one, we will add another evaluation
operator in a subsequent pass.
-        udfs.find(_.resolved) match {
+        udfs.find { udf =>
+          udf.resolved && udf.children.map { child: Expression =>
+            child.find { // really hacky way to find if a child of a udf
has the PythonUDF node
+              case p: PythonUDF => true
+              case _ => false
+            }.isEmpty
+          }.reduce((x, y) => x && y)
+        } match {
           case Some(udf) =>
             var evaluation: EvaluatePython = null

2. If we have a Python UDF applied to many different columns, where they
don‚Äôt depend on each other, we can optimize them by collapsing them down
into a single python worker. Although we have to serialize and send the
same amount of data to the python interpreter, in the case where I am
applying the same function to 20 columns, the overhead/context_switches of
having 20 interpreters run at the same time causes huge performance hits. I
have confirmed this by manually taking the 20 columns, converting them to a
struct, and then writing a UDF that processes the struct at the same time,
and the speed difference is 2x. My approach to adding this to catalyst is
basically to write an optimizer rule called CombinePython which joins
adjacent EvaluatePython nodes that don‚Äôt depend on each other‚Äôs variables,
and then having BatchPythonEvaluation run multiple lambdas at once. I would
also like to be able to handle the case
df.select(python_times_2(‚Äúcol1‚Äù).alias(‚Äúcol1x2‚Äù)).select(F.col(‚Äúcol1x2‚Äù),
python_times_2(‚Äúcol1x2‚Äù).alias(‚Äúcol1x4‚Äù)). To get around that, I add a
PushDownPythonEvaluation optimizer that will push the optimization through
a select/project, so that the CombinePython rule can join the two.

3. I would like CombinePython to be able to handle UDFs that chain off of
each other.

    df.select(python_times_2(python_times_2(‚Äúcol1‚Äù)))

I haven‚Äôt prototyped this yet, since it‚Äôs a lot more complex. The way I‚Äôm
thinking about this is to still have a rule called CombinePython, except
that the BatchPythonEvaluation will need to be smart enough to build up the
dag of dependencies, and then feed that information to the python
interpreter, so it can compute things in the right order, and reuse the
in-memory objects that it has already computed. Does this seem right?
Should the code mainly be in BatchPythonEvaluation? In addition, we will
need to change up the protocol between the java and python sides to support
sending this information. What is acceptable?

Any help would be much appreciated! Especially w.r.t where to the design
choices such that the PR that has a chance of being accepted.

Justin
"
