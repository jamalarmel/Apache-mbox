Mike Hynes <91mbbh@gmail.com>,"Mon, 1 Feb 2016 02:01:14 -0500",sbt publish-local fails with 2.0.0-SNAPSHOT,dev@spark.apache.org,"Hi devs,

I used to be able to do some local development from the upstream
master branch and run the publish-local command in an sbt shell to
publish the modified jars to the local ~/.ivy2 repository.

I relied on this behaviour, since I could write other local packages
that had my local 1.X.0-SNAPSHOT dependencies in the build.sbt file,
such that I could run distributed tests from outside the spark source.

However, having just pulled from the upstream master on
2.0.0-SNAPSHOT, I can *not* run publish-local with sbt, with the
following error messages:

[...]
java.lang.RuntimeException: Undefined resolver 'local'
	at scala.sys.package$.error(package.scala:27)
	at sbt.IvyActions$$anonfun$publish$1.apply(IvyActions.scala:120)
	at sbt.IvyActions$$anonfun$publish$1.apply(IvyActions.scala:117)
	at sbt.IvySbt$Module$$anonfun$withModule$1.apply(Ivy.scala:155)
	at sbt.IvySbt$Module$$anonfun$withModule$1.apply(Ivy.scala:155)
	at sbt.IvySbt$$anonfun$withIvy$1.apply(Ivy.scala:132)
	at sbt.IvySbt.sbt$IvySbt$$action$1(Ivy.scala:57)
	at sbt.IvySbt$$anon$4.call(Ivy.scala:65)
	at xsbt.boot.Locks$GlobalLock.withChannel$1(Locks.scala:93)
	at xsbt.boot.Locks$GlobalLock.xsbt$boot$Locks$GlobalLock$$withChannelRetries$1(Locks.scala:78)
	at xsbt.boot.Locks$GlobalLock$$anonfun$withFileLock$1.apply(Locks.scala:97)
	at xsbt.boot.Using$.withResource(Using.scala:10)
	at xsbt.boot.Using$.apply(Using.scala:9)
	at xsbt.boot.Locks$GlobalLock.ignoringDeadlockAvoided(Locks.scala:58)
	at xsbt.boot.Locks$GlobalLock.withLock(Locks.scala:48)
	at xsbt.boot.Locks$.apply0(Locks.scala:31)
	at xsbt.boot.Locks$.apply(Locks.scala:28)
	at sbt.IvySbt.withDefaultLogger(Ivy.scala:65)
	at sbt.IvySbt.withIvy(Ivy.scala:127)
	at sbt.IvySbt.withIvy(Ivy.scala:124)
	at sbt.IvySbt$Module.withModule(Ivy.scala:155)
	at sbt.IvyActions$.publish(IvyActions.scala:117)
	at sbt.Classpaths$$anonfun$publishTask$1.apply(Defaults.scala:1298)
	at sbt.Classpaths$$anonfun$publishTask$1.apply(Defaults.scala:1297)
	at scala.Function3$$anonfun$tupled$1.apply(Function3.scala:35)
	at scala.Function3$$anonfun$tupled$1.apply(Function3.scala:34)
	at scala.Function1$$anonfun$compose$1.apply(Function1.scala:47)
	at sbt.$tilde$greater$$anonfun$$u2219$1.apply(TypeFunctions.scala:40)
	at sbt.std.Transform$$anon$4.work(System.scala:63)
	at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:226)
	at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:226)
	at sbt.ErrorHandling$.wideConvert(ErrorHandling.scala:17)
	at sbt.Execute.work(Execute.scala:235)
	at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:226)
	at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:226)
	at sbt.ConcurrentRestrictions$$anon$4$$anonfun$1.apply(ConcurrentRestrictions.scala:159)
	at sbt.CompletionService$$anon$2.call(CompletionService.scala:28)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
[...]
[error] (spark/*:publishLocal) Undefined resolver 'local'
[error] (hive/*:publishLocal) Undefined resolver 'local'
[error] (streaming-kafka-assembly/*:publishLocal) Undefined resolver 'local'
[error] (unsafe/*:publishLocal) Undefined resolver 'local'
[error] (streaming-twitter/*:publishLocal) Undefined resolver 'local'
[error] (streaming-flume/*:publishLocal) Undefined resolver 'local'
[error] (streaming-kafka/*:publishLocal) Undefined resolver 'local'
[error] (catalyst/*:publishLocal) Undefined resolver 'local'
[error] (streaming-akka/*:publishLocal) Undefined resolver 'local'
[error] (streaming-flume-sink/*:publishLocal) Undefined resolver 'local'
[error] (streaming-zeromq/*:publishLocal) Undefined resolver 'local'
[error] (test-tags/*:publishLocal) Undefined resolver 'local'
[error] (launcher/*:publishLocal) Undefined resolver 'local'
[error] (network-shuffle/*:publishLocal) Undefined resolver 'local'
[error] (streaming-mqtt-assembly/*:publishLocal) Undefined resolver 'local'
[error] (assembly/*:publishLocal) Undefined resolver 'local'
[error] (docker-integration-tests/*:publishLocal) Undefined resolver 'local'
[error] (tools/*:publishLocal) Undefined resolver 'local'
[error] (sketch/*:publishLocal) Undefined resolver 'local'
[error] (network-common/*:publishLocal) Undefined resolver 'local'
[error] (repl/*:publishLocal) Undefined resolver 'local'
[error] (core/*:publishLocal) Undefined resolver 'local'
[error] (streaming/*:publishLocal) Undefined resolver 'local'
[error] (graphx/*:publishLocal) Undefined resolver 'local'
[error] (sql/compile:doc) Scaladoc generation failed
[error] (streaming-mqtt/*:publishLocal) Undefined resolver 'local'
[error] (mllib/*:publishLocal) Undefined resolver 'local'
[error] (examples/*:publishLocal) Undefined resolver 'local'
[error] (streaming-flume-assembly/*:publishLocal) Undefined resolver 'local'

Could someone please tell me what has changed, and how I can publish
modified 2.X.0-SNAPSHOT jars to the local .iv2 & .m2 caches?

Thanks,
Mike

---------------------------------------------------------------------


"
Saisai Shao <sai.sai.shao@gmail.com>,"Mon, 1 Feb 2016 16:18:20 +0800",Re: sbt publish-local fails with 2.0.0-SNAPSHOT,Mike Hynes <91mbbh@gmail.com>,"I think it is due to our recent changes to override the external resolvers
in sbt building profile, I just created a JIRA (
https://issues.apache.org/jira/browse/SPARK-13109) to track this.



"
Prabhu Joseph <prabhujose.gates@gmail.com>,"Mon, 1 Feb 2016 14:02:44 +0530",Spark job does not perform well when some RDD in memory and some on Disk,"user <user@spark.apache.org>, dev@spark.apache.org","Hi All,


Sample Spark application which reads a logfile from hadoop (1.2GB - 5 RDD's
created each approx 250MB data) and there are two jobs. Job A gets the line
with ""a"" and the Job B gets the line with ""b"". The spark application is ran
multiple times, each time with
different executor memory, and enable/disable cache() function. Job A
performance is same in all the runs as it has to read the entire data first
time from Disk.

Spark Cluster - standalone mode with Spark Master, single worker node (12
cores, 16GB memory)

    val logData = sc.textFile(logFile, 2)
    var numAs = logData.filter(line => line.contains(""a"")).count()
    var numBs = logData.filter(line => line.contains(""b"")).count()


*Job B (which has 5 tasks) results below:*

*Run 1:* 1 executor with 2GB memory, 12 cores took 2 seconds [ran1 image]

    Since logData is not cached, the job B has to again read the 1.2GB data
from hadoop into memory and all the 5 tasks started parallel and each took
2 sec (29ms for GC) and the
 overall job completed in 2 seconds.

*Run 2:* 1 executor with 2GB memory, 12 cores and logData is cached took 4
seconds [ran2 image, ran2_cache image]

     val logData = sc.textFile(logFile, 2).cache()

     The Executor does not have enough memory to cache and hence again
needs to read the entire 1.2GB data from hadoop into memory.  But since the
cache() is used, leads to lot of GC pause leading to slowness in task
completion. Each task started parallel and
completed in 4 seconds (more than 1 sec for GC).

*Run 3: 1 executor with 6GB memory, 12 cores and logData is cached took 10
seconds [ran3 image]*

     The Executor has memory that can fit 4 RDD partitions into memory but
5th RDD it has to read from Hadoop. 4 tasks are started parallel and they
completed in 0.3 seconds without GC. But the 5th task which has to read RDD
from disk is started after 4 seconds, and gets completed in 2 seconds.
Analysing why the 5th task is not started parallel with other tasks or at
least why it is not started immediately after the other task completion.

*Run 4:* 1 executor with 16GB memory , 12 cores and logData is cached took
0.3 seconds [ran4 image]

     The executor has enough memory to cache all the 5 RDD. All 5 tasks are
started in parallel and gets completed within 0.3 seconds.


So Spark performs well when entire input data is in Memory or None. In case
of some RDD in memory and some from disk, there is a delay in scheduling
the fifth task, is it a expected behavior or a possible Bug.



Thanks,
Prabhu Joseph

---------------------------------------------------------------------"
Prabhu Joseph <prabhujose.gates@gmail.com>,"Mon, 1 Feb 2016 17:46:26 +0530",Spark Executor retries infinitely,"user <user@spark.apache.org>, dev@spark.apache.org","Hi All,

  When a Spark job (Spark-1.5.2) is submitted with a single executor and if
user passes some wrong JVM arguments with spark.executor.extraJavaOptions,
the first executor fails. But the job keeps on retrying, creating a new
executor and failing every tim*e, *until CTRL-C is pressed*. *Do we have
configuration to limit the retry attempts.

*Example:*

./spark-submit --class SimpleApp --master ""spark://10.10.72.145:7077""
--conf ""spark.executor.extraJavaOptions=-XX:+PrintGCDetails
-XX:+PrintGCTimeStamps -XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=35
-XX:ConcGCThreads=16"" /SPARK/SimpleApp.jar

Executor fails with

Error occurred during initialization of VM
Can't have more ConcGCThreads than ParallelGCThreads.

But the job does not exit, keeps on creating executors and retrying.
..........
16/02/01 06:54:28 INFO SparkDeploySchedulerBackend: *Granted executor ID
app-20160201065319-0014/2846* on hostPort 10.10.72.145:36558 with 12 cores,
2.0 GB RAM
16/02/01 06:54:28 INFO AppClient$ClientEndpoint: Executor updated:
app-20160201065319-0014/2846 is now LOADING
16/02/01 06:54:28 INFO AppClient$ClientEndpoint: Executor updated:
app-20160201065319-0014/2846 is now RUNNING
16/02/01 06:54:28 INFO AppClient$ClientEndpoint: Executor updated:
app-20160201065319-0014/2846 is now EXITED (Command exited with code 1)
16/02/01 06:54:28 INFO SparkDeploySchedulerBackend: Executor
app-20160201065319-0014/2846 removed: Command exited with code 1
16/02/01 06:54:28 INFO SparkDeploySchedulerBackend: Asked to remove
non-existent executor 2846
16/02/01 06:54:28 INFO AppClient$ClientEndpoint: *Executor added:
app-20160201065319-0014/2847* on worker-20160131230345-10.10.72.145-36558 (
10.10.72.145:36558) with 12 cores
16/02/01 06:54:28 INFO SparkDeploySchedulerBackend: Granted executor ID
app-20160201065319-0014/2847 on hostPort 10.10.72.145:36558 with 12 cores,
2.0 GB RAM
16/02/01 06:54:28 INFO AppClient$ClientEndpoint: Executor updated:
app-20160201065319-0014/2847 is now LOADING
16/02/01 06:54:28 INFO AppClient$ClientEndpoint: Executor updated:
app-20160201065319-0014/2847 is now EXITED (Command exited with code 1)
16/02/01 06:54:28 INFO SparkDeploySchedulerBackend: Executor
app-20160201065319-0014/2847 removed: Command exited with code 1
16/02/01 06:54:28 INFO SparkDeploySchedulerBackend: Asked to remove
non-existent executor 2847
16/02/01 06:54:28 INFO AppClient$ClientEndpoint:* Executor added:
app-20160201065319-0014/2848* on worker-20160131230345-10.10.72.145-36558 (
10.10.72.145:36558) with 12 cores
16/02/01 06:54:28 INFO SparkDeploySchedulerBackend: Granted executor ID
app-20160201065319-0014/2848 on hostPort 10.10.72.145:36558 with 12 cores,
2.0 GB RAM
16/02/01 06:54:28 INFO AppClient$ClientEndpoint: Executor updated:
app-20160201065319-0014/2848 is now LOADING
16/02/01 06:54:28 INFO AppClient$ClientEndpoint: Executor updated:
app-20160201065319-0014/2848 is now RUNNING
............



Thanks,
Prabhu Joseph
"
Steve Loughran <stevel@hortonworks.com>,"Mon, 1 Feb 2016 12:22:43 +0000",Re: Scala 2.11 default build,,"

FYI - I just merged Josh's pull request to switch to Scala 2.11 as the default build.

https://github.com/apache/spark/pull/10608



does this mean that Spark 2.10 compatibility & testing are no longer needed?
"
David Russell <themarchoffolly@gmail.com>,"Mon, 1 Feb 2016 07:23:43 -0500",[ANNOUNCE] New SAMBA Package = Spark + AWS Lambda,"user@spark.apache.org, dev@spark.apache.org","Hi all,

Just sharing news of the release of a newly available Spark package, SAMBA
<https://github.com/onetapbeyond/lambda-spark-executor>.
<http://spark-packages.org/package/onetapbeyond/opencpu-spark-executor>

https://github.com/onetapbeyond/lambda-spark-executor

SAMBA is an Apache Spark package offering seamless integration with the AWS
Lambda <https://aws.amazon.com/lambda/> compute service for Spark batch and
streaming applications on the JVM.

Within traditional Spark deployments RDD tasks are executed using fixed
compute resources on worker nodes within the Spark cluster. With SAMBA,
application developers can delegate selected RDD tasks to execute using
on-demand AWS Lambda compute infrastructure in the cloud.

Not unlike the recently released ROSE
<https://github.com/onetapbeyond/opencpu-spark-executor> package that
extends the capabilities of traditional Spark applications with support for
CRAN R analytics, SAMBA provides another (hopefully) useful extension for
Spark application developers on the JVM.

SAMBA Spark Package: https://github.com/onetapbeyond/lambda-spark-executor
<https://github.com/onetapbeyond/lambda-spark-executor>
ROSE Spark Package: https://github.com/onetapbeyond/opencpu-spark-executor
<https://github.com/onetapbeyond/opencpu-spark-executor>

Questions, suggestions, feedback welcome.

David

-- 
""*All that is gold does not glitter,** Not all those who wander are lost.""*
"
"""Praveen Devarao"" <praveendrl@in.ibm.com>","Mon, 1 Feb 2016 17:26:47 +0530",Guidelines for writing SPARK packages,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

        Is there any guidelines or specs to write a Spark package? I would 
like to implement a spark package and would like to know the way it needs 
to be structured (implement some interfaces etc) so that it can plug into 
Spark for extended functionality.

        Could any one help me point to docs or links on the above?

Thanking You

Praveen Devarao

"
Ted Yu <yuzhihong@gmail.com>,"Mon, 1 Feb 2016 07:02:34 -0800",Re: Scala 2.11 default build,Steve Loughran <stevel@hortonworks.com>,"The following jobs have been established for build against Scala 2.10:

https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Compile/job/SPARK-master-COMPILE-MAVEN-SCALA-2.10/
https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Compile/job/SPARK-master-COMPILE-sbt-SCALA-2.10/

FYI


"
Ted Yu <yuzhihong@gmail.com>,"Mon, 1 Feb 2016 07:29:09 -0800",Re: Spark 1.6.1,Michael Armbrust <michael@databricks.com>,"SPARK-12624 has been resolved.
According to Wenchen, SPARK-12783 is fixed in 1.6.0 release.

Are there other blockers for Spark 1.6.1 ?

Thanks


"
Hamel Kothari <hamelkothari@gmail.com>,"Mon, 01 Feb 2016 16:16:28 +0000",Re: Spark 1.6.1,"Ted Yu <yuzhihong@gmail.com>, Michael Armbrust <michael@databricks.com>","I noticed that the Jackson dependency was bumped to 2.5 in master for
something spark-streaming related. Is there any reason that this upgrade
can't be included with 1.6.1?

According to later comments on this thread:
https://issues.apache.org/jira/browse/SPARK-8332 and my personal experience
using with Spark with Jackson 2.5 hasn't caused any issues but it does have
some useful new features. It should be fully backwards compatible according
to the Jackson folks.


"
Jakob Odersky <jakob@odersky.com>,"Mon, 1 Feb 2016 11:33:50 -0800",Re: Scala 2.11 default build,Ted Yu <yuzhihong@gmail.com>,"Awesome!
+1 on Steve Loughran's question, how does this affect support for
2.10? Do future contributions need to work with Scala 2.10?

cheers


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Mon, 1 Feb 2016 11:37:53 -0800",Re: Scala 2.11 default build,Jakob Odersky <jakob@odersky.com>,"Yes they do. We haven't dropped 2.10 support yet. There are too many 2.10
active deployments out there.



"
Michael Armbrust <michael@databricks.com>,"Mon, 1 Feb 2016 11:59:36 -0800",Re: Spark 1.6.1,Hamel Kothari <hamelkothari@gmail.com>,"We typically do not allow changes to the classpath in maintenance releases.


"
eugene miretsky <eugene.miretsky@gmail.com>,"Mon, 1 Feb 2016 15:48:22 -0500",Encrypting jobs submitted by the client,dev@spark.apache.org,"Spark supports client authentication via shared secret or kerberos (on
YARN). However, the job itself is sent unencrypted over the network.  Is
there a way to encrypt the jobs the client submits to cluster?
The rational for this is very similar to  encrypting the HTTP file server
traffic - Jars may have sensitive data.

Cheers,
Eugene
"
eugene miretsky <eugene.miretsky@gmail.com>,"Mon, 1 Feb 2016 16:02:02 -0500",Secure multi tenancy on in stand alone mode,dev@spark.apache.org,"When having multiple users sharing the same Spark cluster, it's a good idea
to isolate the users - make sure that each users runs under a different
Linux account and prevent them from accessing data in jobs submitted by
other users. Is it currently possible to do with Spark?

The only thing I found about it online is
http://rnowling.github.io/spark/2015/04/07/multiuser-spark-mesos.html, and
some older Jira about adding support to YARN.

Cheers,
Eugene
"
Ted Yu <yuzhihong@gmail.com>,"Mon, 1 Feb 2016 13:26:26 -0800",Re: Secure multi tenancy on in stand alone mode,eugene miretsky <eugene.miretsky@gmail.com>,"w.r.t. running Spark on YARN, there are a few outstanding issues. e.g.

SPARK-11182 HDFS Delegation Token

See also the comments under SPARK-12279

FYI


"
Mike Hynes <91mbbh@gmail.com>,"Mon, 1 Feb 2016 17:45:30 -0500",Re: sbt publish-local fails with 2.0.0-SNAPSHOT,Saisai Shao <sai.sai.shao@gmail.com>,"Thank you Saisai for the JIRA/PR; I'm glad to see it is a one-line
fix, and will try this locally in the interim.
Mike



-- 
Thanks,
Mike

---------------------------------------------------------------------


"
Romi Kuntsman <romi@totango.com>,"Tue, 2 Feb 2016 13:16:54 +0200",Re: Spark 1.6.1,Michael Armbrust <michael@databricks.com>,"Hi Michael,
What about the memory leak bug?
https://issues.apache.org/jira/browse/SPARK-11293
Even after the memory rewrite in 1.6.0, it still happens in some cases.
Will it be fixed for 1.6.1?
Thanks,

*Romi Kuntsman*, *Big Data Engineer*
http://www.totango.com


"
Steve Loughran <stevel@hortonworks.com>,"Tue, 2 Feb 2016 12:46:39 +0000",Re: Encrypting jobs submitted by the client,,"
e:
RN). However, the job itself is sent unencrypted over the network.  Is there a way to encrypt the jobs the client submits to cluster? 


define submission? 

1. spark-submit submitting the YARN app for launch?  That you get it if you turn hadoop IPC encruption on, by settingo hadoop.rpc.protection=privacy across the cluster.
2. communications between spark driver and executor. that can use HTTPS

 traffic - Jars may have sensitive data. 


---------------------------------------------------------------------


"
Prabhu Joseph <prabhujose.gates@gmail.com>,"Tue, 2 Feb 2016 19:51:56 +0530",Spark saveAsHadoopFile stage fails with ExecutorLostfailure,"user <user@spark.apache.org>, dev@spark.apache.org","Hi All,

   Spark job stage having saveAsHadoopFile fails with ExecutorLostFailure
whenever the Executor is run with more cores. The stage is not memory
intensive, executor has 20GB memory. for example,

6 executors each with 6 cores, ExecutorLostFailure happens

10 executors each with 2 cores, saveAsHadoopFile runs fine.

What could be the reason for ExecutorLostFailure failing when cores per
executor is high.



Error: ExecutorLostFailure (executor 3 lost)

16/02/02 04:22:40 WARN TaskSetManager: Lost task 1.3 in stage 15.0 (TID
1318, hdnprd-c01-r01-14):



Thanks,
Prabhu Joseph
"
mkhaitman <mark.khaitman@chango.com>,"Tue, 2 Feb 2016 08:46:05 -0700 (MST)",Spark 1.6.0 Streaming + Persistance Bug?,dev@spark.apache.org,"Calling unpersist on an RDD in a spark streaming application does not
actually unpersist the blocks from memory and/or disk. After the RDD has
been processed in a .foreach(rdd) call, I attempt to unpersist the rdd since
it is no longer useful to store in memory/disk. This mainly causes a problem
with dynamic allocation where after the batch of data has been processed, we
want the executors to destroy their executors (giving the cores and memory
back to the cluster while waiting for the next batch processing attempt to
occur). 

Is this a known issue? It's not major in that it doesn't break anything...
just prevents dynamic allocation from working as well as it could if
streaming is combined with it.

Thanks,
Mark.



--

---------------------------------------------------------------------


"
mkhaitman <mark.khaitman@chango.com>,"Tue, 2 Feb 2016 09:22:23 -0700 (MST)",Re: Spark 1.6.0 Streaming + Persistance Bug?,dev@spark.apache.org,"Actually disregard! Forgot that
spark.dynamicAllocation.cachedExecutorIdleTimeout was defaulted to Infinity,
so lowering that should solve the problem :)

Mark.



--

---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Tue, 2 Feb 2016 10:10:36 -0800",Re: Spark 1.6.1,Romi Kuntsman <romi@totango.com>,"
I think we have enough issues queued up that I would not hold the release
for that, but if there is a patch we should try and review it.  We can
always do 1.6.2 when more issues have been resolved.  Is this an actual
issue that is affecting a production workload or are we concerned about an
edge case?
"
Mingyu Kim <mkim@palantir.com>,"Tue, 2 Feb 2016 18:43:19 +0000",Re: Spark 1.6.1,"Romi Kuntsman <romi@totango.com>, Michael Armbrust
	<michael@databricks.com>","Hi all,

Is there an estimated timeline for 1.6.1 release? Just wanted to check how
the release is coming along. Thanks!

Mingyu

From:  Romi Kuntsman <romi@totango.com>
Date:  Tuesday, February 2, 2016 at 3:16 AM
To:  Michael Armbrust <michael@databricks.com>
Cc:  Hamel Kothari <hamelkothari@gmail.com>, Ted Yu <yuzhihong@gmail.com>,
""dev@spark.apache.org"" <dev@spark.apache.org>
Subject:  Re: Spark 1.6.1

Hi Michael,
What about the memory leak bug?
https://issues.apache.org/jira/browse/SPARK-11293
<https://urldefense.proofpoint.com/v2/url?u=https-3A__issues.apache.org_jira
&r=ennQJq47pNnObsDh-88a9YUrUulcYQoV8giPASqXB84&m=tI8Pjfii7XuX3Suiky8mImD7S5B
oAq6fgOSdJ7rt2Wo&s=R_B4rDig-0VPE5Q4YeLEs2HUIg-A8St1OtDjD89d_zY&e=>
Even after the memory rewrite in 1.6.0, it still happens in some cases.
Will it be fixed for 1.6.1?
Thanks,

Romi Kuntsman, Big Data Engineer
http://www.totango.com
<https://urldefense.proofpoint.com/v2/url?u=http-3A__www.totango.com_&d=CwMF
ulcYQoV8giPASqXB84&m=tI8Pjfii7XuX3Suiky8mImD7S5BoAq6fgOSdJ7rt2Wo&s=Z4TgGF0h7
oetD4O6u_3qjrYbe0ZtW2g_In7V8tkByPg&e=>




"
Michael Armbrust <michael@databricks.com>,"Tue, 2 Feb 2016 10:48:12 -0800",Re: Spark 1.6.1,Mingyu Kim <mkim@palantir.com>,"I'm waiting for a few last fixes to be merged.  Hoping to cut an RC in the
next few days.


w
_jira_browse_SPARK-2D11293&d=CwMFaQ&c=izlc9mHr637UR4lpLEZLFFS3Vn2UXBrZ43Suiky8mImD7S5BoAq6fgOSdJ7rt2Wo&s=R_B4rDig-0VPE5Q4YeLEs2HUIg-A8St1OtDjD89d_zY&e=>
sDh-88a9YUrUulcYQoV8giPASqXB84&m=tI8Pjfii7XuX3Suiky8mImD7S5BoAq6fgOSdJ7rt2Wo&s=Z4TgGF0h7oetD4O6u_3qjrYbe0ZtW2g_In7V8tkByPg&e=>
e
rg_jira_browse_SPARK-2D8332&d=CwMFaQ&c=izlc9mHr637UR4lpLEZLFFS3Vn2UXBrZX3Suiky8mImD7S5BoAq6fgOSdJ7rt2Wo&s=i-ngQFHfxOmgkYx_5NiCaHdIlm7zi2LYpUxm9I3RfR4&e=>
sed
y
 are
"
Mingyu Kim <mkim@palantir.com>,"Tue, 2 Feb 2016 18:49:41 +0000",Re: Spark 1.6.1,Michael Armbrust <michael@databricks.com>,"Cool, thanks!

Mingyu

From:  Michael Armbrust <michael@databricks.com>
Date:  Tuesday, February 2, 2016 at 10:48 AM
To:  Mingyu Kim <mkim@palantir.com>
Cc:  Romi Kuntsman <romi@totango.com>, Hamel Kothari
<hamelkothari@gmail.com>, Ted Yu <yuzhihong@gmail.com>,
""dev@spark.apache.org"" <dev@spark.apache.org>, Punya Biswal
<pbiswal@palantir.com>, Robert Kruszewski <robertk@palantir.com>
Subject:  Re: Spark 1.6.1

I'm waiting for a few last fixes to be merged.  Hoping to cut an RC in the
next few days.




"
David Russell <themarchoffolly@gmail.com>,"Tue, 2 Feb 2016 14:25:40 -0500",Re: [ANNOUNCE] New SAMBA Package = Spark + AWS Lambda,Benjamin Kim <bbuild11@gmail.com>,"Hi Ben,


That may be true. Spark has first class support for Python which
fleshed out your ideas I'm sure folks on this mailing list can provide
helpful guidance based on their real world experience with Spark.


In a word, no. SAMBA is designed to extend-not-replace the traditional
Spark computation and deployment model. At it's most basic, the
traditional Spark computation model distributes data and computations
across worker nodes in the cluster.

SAMBA simply allows some of those computations to be performed by AWS
Lambda rather than locally on your worker nodes. There are I believe a
number of potential benefits to using SAMBA in some circumstances:

1. It can help reduce some of the workload on your Spark cluster by
moving that workload onto AWS Lambda, an infrastructure on-demand
compute service.

2. It allows Spark applications written in Java or Scala to make use
of libraries and features offered by Python and JavaScript (Node.js)
today, and potentially, more libraries and features offered by
additional languages in the future as AWS Lambda language support
evolves.

3. It provides a simple, clean API for integration with REST APIs that
may be a benefit to Spark applications that form part of a broader
data pipeline or solution.


You might find one of the hosted Spark platform solutions such as
Databricks or Amazon EMR that handle cluster management for you a good
place to start. At least in my experience, they got me up and running
without difficulty.

David

---------------------------------------------------------------------


"
eugene miretsky <eugene.miretsky@gmail.com>,"Tue, 2 Feb 2016 15:36:58 -0500",Re: Encrypting jobs submitted by the client,Steve Loughran <stevel@hortonworks.com>,"Thanks Steve!
1. spark-submit submitting the YARN app for launch?  That you get it if you
turn hadoop IPC encruption on, by settingo hadoop.rpc.protection=privacy
across the cluster.

2. communications between spark driver and executor. that can use HTTPS
driver, and SASL for block transfer. Is there anything else I'm missing?

Cheers,
Eugene






2. communications between spark driver and executor. that can use HTTPS
"
Ted Yu <yuzhihong@gmail.com>,"Tue, 2 Feb 2016 12:59:23 -0800",Re: Encrypting jobs submitted by the client,eugene miretsky <eugene.miretsky@gmail.com>,"For #1, a brief search landed the following:

core/src/main/scala/org/apache/spark/SparkConf.scala:
 DeprecatedConfig(""spark.rpc"", ""2.0"", ""Not used any more."")
core/src/main/scala/org/apache/spark/SparkConf.scala:
 ""spark.rpc.numRetries"" -> Seq(
core/src/main/scala/org/apache/spark/SparkConf.scala:
 ""spark.rpc.retry.wait"" -> Seq(
core/src/main/scala/org/apache/spark/SparkConf.scala:
 ""spark.rpc.askTimeout"" -> Seq(
core/src/main/scala/org/apache/spark/SparkConf.scala:
 ""spark.rpc.lookupTimeout"" -> Seq(
core/src/main/scala/org/apache/spark/SparkConf.scala:
 ""spark.rpc.message.maxSize"" -> Seq(
core/src/main/scala/org/apache/spark/SparkConf.scala:
 name.startsWith(""spark.rpc"") ||

There doesn't seem to be RPC protection for stand alone mode.


"
"""Wen Pei Yu"" <yuwenp@cn.ibm.com>","Wed, 3 Feb 2016 15:40:53 +0800",Lunch dev/run-tests on Windows,dev@spark.apache.org,"
Hi All

Have any one try launch dev/run-tests on Windows? I face some issues

1. `which` function didn't support file check without extension, like
""java"" vs ""java.exe"", ""R"" vs ""R.exe"".

2. Get error below in `run_cmd` function, major issues is some script file
failed run in windows.
	WindowsError: [Error 193] %1 is not a valid Win32 application

Thanks
Wenpei.
"
Steve Loughran <stevel@hortonworks.com>,"Wed, 3 Feb 2016 11:14:52 +0000",Re: Spark 1.6.1,"""dev@spark.apache.org"" <dev@spark.apache.org>","

I'm waiting for a few last fixes to be merged.  Hoping to cut an RC in the next few days.


I've just added https://issues.apache.org/jira/browse/SPARK-12807 to the list; there's a PR urgently in need of review.

Essentially: spark 1.6 network shuffle bundles a version of jackson 2.x incompatible with Hadoop's, leading to one of two outcomes: (a) shuffle broken (b) node managers not starting.

ongoing Hadoop work to have classpath isolation (and eventually, forked plugins) will fix this long term, short term; shade the imports

If this doesn't go in, then the release notes should warn that dynamic resource allocation won't work


Hi all,

Is there an estimated timeline for 1.6.1 release? Just wanted to check how the release is coming along. Thanks!

Mingyu

From: Romi Kuntsman <romi@totango.com<mailto:romi@totango.com>>
Date: Tuesday, February 2, 2016 at 3:16 AM
To: Michael Armbrust <michael@databricks.com<mailto:michael@databricks.com>Cc: Hamel Kothari <hamelkothari@gmail.com<mailto:hamelkothari@gmail.com>>, Ted Yu <yuzhihong@gmail.com<mailto:yuzhihong@gmail.com>>, ""dev@spark.apache.org<mailto:dev@spark.apache.org>"" <dev@spark.apache.org<mailto:dev@spark.apache.org>>
Subject: Re: Spark 1.6.1

Hi Michael,
What about the memory leak bug?
https://issues.apache.org/jira/browse/SPARK-11293<https://urldefense.proofpoint.com/v2/url?u=https-3A__issues.apache.org_jira_browse_SPARK-2D11293&dsDh-88a9YUrUulcYQoV8giPASqXB84&m=tI8Pjfii7XuX3Suiky8mImD7S5BoAq6fgOSdJ7rt2Wo&s=R_B4rDig-0VPE5Q4YeLEs2HUIg-A8St1OtDjD89d_zY&e=>
Even after the memory rewrite in 1.6.0, it still happens in some cases.
Will it be fixed for 1.6.1?
Thanks,

Romi Kuntsman, Big Data Engineer
http://www.totango.com<https://urldefense.proofpoint.com/v2/url?u=http-3Az8&r=ennQJq47pNnObsDh-88a9YUrUulcYQoV8giPASqXB84&m=tI8Pjfii7XuX3Suiky8mImD7S5BoAq6fgOSdJ7rt2Wo&s=Z4TgGF0h7oetD4O6u_3qjrYbe0ZtW2g_In7V8tkByPg&e=>

We typically do not allow changes to the classpath in maintenance releases.

I noticed that the Jackson dependency was bumped to 2.5 in master for something spark-streaming related. Is there any reason that this upgrade can't be included with 1.6.1?

According to later comments on this thread: https://issues.apache.org/jira/browse/SPARK-8332<https://urldefense.proofpoint.com/v2/url?u=https-3A__issues.apache.org_jira_browse_SPARK-2D8332&d=CwMFaQ&c=izlc9mHr637UR4lpLEZ=tI8Pjfii7XuX3Suiky8mImD7S5BoAq6fgOSdJ7rt2Wo&s=i-ngQFHfxOmgkYx_5NiCaHdIlm7zi2LYpUxm9I3RfR4&e=> and my personal experience using with Spark with Jackson 2.5 hasn't caused any issues but it does have some useful new features. It should be fully backwards compatible according to the Jackson folks.

SPARK-12624 has been resolved.
According to Wenchen, SPARK-12783 is fixed in 1.6.0 release.

Are there other blockers for Spark 1.6.1 ?

Thanks

Hey All,

While I'm not aware of any critical issues with 1.6.0, there are several corner cases that users are hitting with the Dataset API that are fixed in branch-1.6.  As such I'm considering a 1.6.1 release.

At the moment there are only two critical issues targeted for 1.6.1:
 - SPARK-12624 - When schema is specified, we should treat undeclared fields as null (in Python)
 - SPARK-12783 - Dataset map serialization error

When these are resolved I'll likely begin the release process.  If there are any other issues that we should wait for please contact me.

Michael





"
"""james.green9@baesystems.com"" <james.green9@baesystems.com>","Wed, 3 Feb 2016 12:17:35 +0000",RE: spark hivethriftserver problem on 1.5.0 -> 1.6.0 upgrade,"""dev@spark.apache.org"" <dev@spark.apache.org>","I have a workaround for this issue which is to go back to single session mode for the thrift server:

conf.set(""spark.sql.hive.thriftServer.singleSession"", ""true"")

This seems to mean that temp tables can be registered in 1.6.0 with a remote metastore.

Cheers

James



From: Yin Huai [mailto:yhuai@databricks.com]
Sent: 26 January 2016 17:48
To: Green, James (UK Guildford)
Cc: dev@spark.apache.org
Subject: Re: spark hivethriftserver problem on 1.5.0 -> 1.6.0 upgrade

Can you post more logs, specially lines around ""Initializing execution hive ..."" (this is for an internal used fake metastore and it is derby) and ""Initializing HiveMetastoreConnection version ..."" (this is for the real metastore. It should be your remote one)? Also, those temp tables are stored in the memory and are associated with a HiveContext. If you can not see temp tables, it usually means that the HiveContext that you used with JDBC was different from the one used to create the temp table. However, in your case, you are using HiveThriftServer2.startWithContext(hiveContext). So, it will be good to provide more logs and see what happened.

Thanks,

Yin

On Tue, Jan 26, 2016 at 1:33 AM, james.green9@baesystems.com<mailto:james.green9@baesystems.com> <james.green9@baesystems.com<mailto:james.green9@baesystems.com>> wrote:
Hi

I posted this on the user list yesterday,  I am posting it here now because on further investigation I am pretty sure this is a bug:


On upgrade from 1.5.0 to 1.6.0 I have a problem with the hivethriftserver2, I have this code:

val hiveContext = new HiveContext(SparkContext.getOrCreate(conf));

val thing = hiveContext.read.parquet(""hdfs://dkclusterm1.imp.net:8020/user/jegreen1/ex208<http://dkclusterm1.imp.net:8020/user/jegreen1/ex208>"")

thing.registerTempTable(""thing"")

HiveThriftServer2.startWithContext(hiveContext)


When I start things up on the cluster my hive-site.xml is found â€“ I can see that the metastore connects:


INFO  metastore - Trying to connect to metastore with URI thrift://dkclusterm2.imp.net:9083<http://dkclusterm2.imp.net:9083>
INFO  metastore - Connected to metastore.


But then later on the thrift server seems not to connect to the remote hive metastore but to start a derby instance instead:

INFO  AbstractService - Service:CLIService is started.
INFO  ObjectStore - ObjectStore, initialize called
INFO  Query - Reading in results for query ""org.datanucleus.store.rdbms.query.SQLQuery@0<mailto:org.datanucleus.store.rdbms.query.SQLQuery@0>"" since the connection used is closing
INFO  MetaStoreDirectSql - Using direct SQL, underlying DB is DERBY
INFO  ObjectStore - Initialized ObjectStore
INFO  HiveMetaStore - 0: get_databases: default
INFO  audit - ugi=jegreen1      ip=unknown-ip-addr      cmd=get_databases: default
INFO  HiveMetaStore - 0: Shutting down the object store...
INFO  audit - ugi=jegreen1      ip=unknown-ip-addr      cmd=Shutting down the object store...
INFO  HiveMetaStore - 0: Metastore shutdown complete.
INFO  audit - ugi=jegreen1      ip=unknown-ip-addr      cmd=Metastore shutdown complete.
INFO  AbstractService - Service:ThriftBinaryCLIService is started.
INFO  AbstractService - Service:HiveServer2 is started.

On 1.5.0 the same bit of the log reads:

INFO  AbstractService - Service:CLIService is started.
INFO  metastore - Trying to connect to metastore with URI thrift://dkclusterm2.imp.net:9083<http://dkclusterm2.imp.net:9083>      ******* ie 1.5.0 connects to remote hive
INFO  metastore - Connected to metastore.
INFO  AbstractService - Service:ThriftBinaryCLIService is started.
INFO  AbstractService - Service:HiveServer2 is started.
INFO  ThriftCLIService - Starting ThriftBinaryCLIService on port 10000 with 5...500 worker threads



So if I connect to this with JDBC I can see all the tables on the hive server â€“ but not anything temporary â€“ I guess they are going to derby.

I see someone on the databricks website is also having this problem.


Thanks

James
Please consider the environment before printing this email. This message should be regarded as confidential. If you have received this email in error please notify the sender and destroy it immediately. Statements of intent shall only become binding when confirmed in hard copy by an authorised signatory. The contents of this email may relate to dealings with other companies under the control of BAE Systems Applied Intelligence Limited, details of which can be found at http://www.baesystems.com/Businesses/index.htm.

Please consider the environment before printing this email. This message should be regarded as confidential. If you have received this email in error please notify the sender and destroy it immediately. Statements of intent shall only become binding when confirmed in hard copy by an authorised signatory. The contents of this email may relate to dealings with other companies under the control of BAE Systems Applied Intelligence Limited, details of which can be found at http://www.baesystems.com/Businesses/index.htm.
"
Daniel Darabos <daniel.darabos@lynxanalytics.com>,"Wed, 3 Feb 2016 14:16:08 +0100",Re: Spark 1.6.1,Michael Armbrust <michael@databricks.com>,"

The way we (Lynx Analytics) use RDDs, this affects almost everything we do
in production. Thankfully it does not cause any issues, it just logs a lot
of errors. I think the adverse effect may be that the memory manager does
not have a fully correct picture. But as long as the leak fits in the
""other"" (unmanaged) memory fraction this will not cause issues. We don't
see this as an urgent issue. Thanks!
"
Yiannis Gkoufas <johngouf85@gmail.com>,"Wed, 3 Feb 2016 15:26:56 +0000","SparkOscope: Enabling Spark Optimization through Cross-stack
 Monitoring and Visualization",dev@spark.apache.org,"Hi all,

I just wanted to introduce some of my recent work in IBM Research around
Spark and especially its Metric System and Web UI.
As a quick overview of our contributions:
We have a created a new type of Sink for the metrics ( HDFSSink ) which
captures the metrics into HDFS,
We have extended the metrics reported by the Executors to include OS-level
metrics regarding CPU, RAM, Disk IO, Network IO utilizing the Hyperic Sigar
library
We have extended the Web UI for the completed applications to visualize any
of the above metrics the user wants to.
The above functionalities can be configured in the metrics.properties and
spark-defaults.conf files.
We have recorded a small demo that shows those capabilities which you can
find here : https://ibm.app.box.com/s/vyaedlyb444a4zna1215c7puhxliqxdg
There is a blog post which gives more details on the functionality here:
*www.spark.tc/sparkoscope-enabling-spark-optimization-through-cross-stack-monitoring-and-visualization-2/*
<http://www.spark.tc/sparkoscope-enabling-spark-optimization-through-cross-stack-monitoring-and-visualization-2/>
and also there is a public repo where anyone can try it:
*https://github.com/ibm-research-ireland/sparkoscope*
<https://github.com/ibm-research-ireland/sparkoscope>

I would really appreciate any feedback or advice regarding this work.
Especially if you think it's worth it to upstream to the official Spark
repository.

Thanks a lot!
"
Antonio Piccolboni <antonio@piccolboni.info>,"Wed, 03 Feb 2016 18:17:05 +0000",Path to resource added with SQL: ADD FILE,dev <dev@spark.apache.org>,"Sorry if this is more appropriate for user list, I asked there on 12/17 and
got the silence treatment. I am writing a UDF that needs some additional
info to perform its task. This information is in a file that I reference in
a SQL ADD FILE statement. I expect that file to be accessible in the
working directory for the UDF, but it doesn't seem to work (aka, failure on
open(""./my resource file""). What is the correct way to access the added
resource? Thanks


Antonio
"
Andrew Lee <alee526@hotmail.com>,"Wed, 3 Feb 2016 18:57:50 +0000","Spark 1.6: Why Including hive-jdbc in assembly when -Phive-provided
 is set?","""dev@spark.apache.org"" <dev@spark.apache.org>","Hi All,


I have a question regarding the hive-jdbc library that is being included in the assembly JAR.


Build command.

mvn -U -X -Phadoop-2.6 -Phadoop-provided -Phive-provided -Pyarn -Phive-thriftserver -Psparkr -DskipTests install


In the pom.xml file, the scope for hive JARs are set to 'compile', however, there is one entry

https://github.com/apache/spark/blob/branch-1.6/pom.xml#L1414

[https://avatars1.githubusercontent.com/u/47359?v=3&s=400]<https://github.com/apache/spark/blob/branch-1.6/pom.xml#L1414>

apache/spark<https://github.com/apache/spark/blob/branch-1.6/pom.xml#L1414>
github.com
spark - Mirror of Apache Spark




that includes it again.


The assembly JAR shows the following content with 'jar tf'.


org/apache/hive/
org/apache/hive/jdbc/
org/apache/hive/jdbc/HiveDatabaseMetaData.class
org/apache/hive/jdbc/ZooKeeperHiveClientHelper.class
org/apache/hive/jdbc/ZooKeeperHiveClientHelper$DummyWatcher.class
org/apache/hive/jdbc/HiveQueryResultSet$Builder.class
org/apache/hive/jdbc/HiveResultSetMetaData.class
org/apache/hive/jdbc/HivePreparedStatement.class
org/apache/hive/jdbc/HiveStatement$1.class
org/apache/hive/jdbc/JdbcUriParseException.class
org/apache/hive/jdbc/HiveDataSource.class
org/apache/hive/jdbc/HttpBasicAuthInterceptor.class
org/apache/hive/jdbc/JdbcColumn.class
org/apache/hive/jdbc/Utils$JdbcConnectionParams.class
org/apache/hive/jdbc/HiveMetaDataResultSet.class
org/apache/hive/jdbc/HiveDriver.class
org/apache/hive/jdbc/JdbcTable.class
org/apache/hive/jdbc/HiveBaseResultSet.class
org/apache/hive/jdbc/HiveDatabaseMetaData$GetTablesComparator.class
org/apache/hive/jdbc/HiveDatabaseMetaData$1.class
org/apache/hive/jdbc/HiveStatement.class
org/apache/hive/jdbc/ZooKeeperHiveClientException.class
org/apache/hive/jdbc/HiveQueryResultSet$1.class
org/apache/hive/jdbc/Utils.class
org/apache/hive/jdbc/HiveConnection$1.class
org/apache/hive/jdbc/JdbcColumn$1.class
org/apache/hive/jdbc/HiveBaseResultSet$1.class
org/apache/hive/jdbc/HttpKerberosRequestInterceptor.class
org/apache/hive/jdbc/JdbcColumnAttributes.class
org/apache/hive/jdbc/HiveCallableStatement.class
org/apache/hive/jdbc/HiveDatabaseMetaData$GetColumnsComparator.class
org/apache/hive/jdbc/ClosedOrCancelledStatementException.class
org/apache/hive/jdbc/HiveQueryResultSet.class
org/apache/hive/jdbc/HttpRequestInterceptorBase.class
org/apache/hive/jdbc/HiveConnection.class
org/apache/hive/service/
org/apache/hive/service/server/
org/apache/hive/service/server/HiveServerServerOptionsProcessor.class


Would like to know why this is there and can we remove that? and link the hive-jdbc during runtime?




"
=?gb2312?B?zfTR8w==?= <tiandiwoxin@icloud.com>,"Thu, 04 Feb 2016 15:59:00 +0800",TakeOrderedAndProject operator may causes an OOM,dev@spark.apache.org,"Hi,

Currently the TakeOrderedAndProject operator in spark sql uses RDD¡¯s takeOrdered method. When we pass a large limit to operator, however, it will return partitionNum*limit number of records to the driver which may cause an OOM.

Are there any plans to deal with the problem in the community? 


Thanks.


Yang"
zzc <441586683@qq.com>,"Thu, 4 Feb 2016 01:41:32 -0700 (MST)",Re: Scala 2.11 default build,dev@spark.apache.org,"hi, rxin, in pom.xml file, 'scala.version' still is 2.10.5, does  it need to
be modified to 2.11.7?



--

---------------------------------------------------------------------


"
tiandiwoxin <tiandiwoxin@icloud.com>,"Thu, 4 Feb 2016 02:19:52 -0700 (MST)",TakeOrderedAndProject operator may causes an OOM,dev@spark.apache.org,"Hi,

Currently the TakeOrderedAndProject operator in spark sql uses RDDâ€™s
takeOrdered method. When we pass a large limit to operator, however, it will
return partitionNum*limit number of records to the driver which may cause an
OOM.

Are there any plans to deal with the problem in the community? 


Thanks.


Yang



--
3.nabble.com/TakeOrderedAndProject-operator-may-causes-an-OOM-tp16208.html
om.

---------------------------------------------------------------------


"
Prashant Sharma <scrapcodes@gmail.com>,"Thu, 4 Feb 2016 15:38:09 +0530",Re: Scala 2.11 default build,zzc <441586683@qq.com>,"Yes, That should be changed to 2.11.7. Mind sending a patch ?

Prashant Sharma




"
Prabhu Joseph <prabhujose.gates@gmail.com>,"Thu, 4 Feb 2016 16:03:22 +0530","Re: Spark job does not perform well when some RDD in memory and some
 on Disk","user <user@spark.apache.org>, dev@spark.apache.org","Okay, the reason for the task delay within executor when some RDD in memory
and some in Hadoop i.e, Multiple Locality Levels NODE_LOCAL and ANY, in
this case Scheduler waits
for *spark.locality.wait *3 seconds default. During this period, scheduler
waits to launch a data-local task before giving up and launching it on a
less-local node.

So after making it 0, all tasks started parallel. But learned that it is
better not to reduce it to 0.



"
Tao Lin <nblintao@gmail.com>,"Thu, 4 Feb 2016 03:46:43 -0700 (MST)",Interested in Contributing to Spark as GSoC 2016,dev@spark.apache.org,"Hi All, 
I am Tao Lin, a senior Computer Science student highly interested in Data
Science (Distributed Computing, Machine Learning, Visualization, etc.). I'd
like to join Google Summer of Code 2016 and contribute to Spark this year. 
When I was viewing the past GSoC projects, I was impressed by ""Enhance
MLlib's Python API"", completed by Manoj Kumar (Mentored by Xiangrui Meng)
last year. I look forward to writing something as meaningful and impactful
as what they did.
The organization list of GSoC 2016 hasn't been released yet, but I'd like to
join the community and make a stark on solving real problems asap. Is there
anyone who's going to sign up as a mentor for GSoC this year? Maybe you
could tell me about the projects you are going to mentor and give me some
suggestions about what issues I could fix now to get a start. Thanks!

Here is more information about myself and my related experiences:
I'm going to pursue my graduate study in the US after this summer. (I have
received an offer from U Wisconsinâ€“Madison, and I'm waiting for more
admissions.) Since I am vacant this spring and summer, I could put full
enthusiasm into the open-source development.
I am quite familiar with Spark. I did research on data visualization in the
Visual Analytics Group, State Key Lab of CAD&CG for more than two years.  I
administrated a cluster with more than 20 nodes for more than one year
there. I helped the whole group preprocess large datasets on the cluster
with Hadoop and Spark. In one of the projects, I used Spark to independently
process 14 billion trajectory records (about 1.8 TB) on the cluster.
The highlight of my professional experience has been working as a visiting
intern in the HKUST Multimedia Technology Research Center. Through that
experience, I have not only improved my programming skills, but I have also
learned how to work better in a large software engineering team by applying
software engineering techniques (like unit testing and code review) and how
to communicate cross-culturally.
As for programming languages, I'm good at Java and Python. And I believe I
could handle Scala or R in a short time if it is needed in the project.
(Besides, I'm experienced in C++ and JavaScript, which are unlikely to be
used in Spark projects.)
You could also view my CV at http://nblintao.github.io/pdf/Tao_Lin_CV.pdf

Thanks for your time!
Best Regards,
Tao



--
3.nabble.com/Interested-in-Contributing-to-Spark-as-GSoC-2016-tp16211.html
om.

---------------------------------------------------------------------


"
=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@questtec.nl>,"Thu, 4 Feb 2016 13:21:10 +0100",Re: Path to resource added with SQL: ADD FILE,Antonio Piccolboni <antonio@piccolboni.info>,"Hi Antonio,

I am not sure you got the silent treatment on the user list. Stackoverflow
is also a good place to ask questions.

Could you use an absolute path to add the jar file. So instead of './my
resource file' (which is a relative path; this depends on where you started
Spark), use something like this '/some/path/my resource file' or use an URI.

Kind regards,

Herman van HÃ¶vell


2016-02-03 19:17 GMT+01:00 Antonio Piccolboni <antonio@piccolboni.info>:

e
"
Luciano Resende <luckbr1975@gmail.com>,"Thu, 4 Feb 2016 05:20:27 -0800",Re: Scala 2.11 default build,Prashant Sharma <scrapcodes@gmail.com>,"There were few more issues, I have started tracking them at

https://issues.apache.org/jira/browse/SPARK-13189




-- 
Luciano Resende
http://people.apache.org/~lresende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
Max Grossman <jmg3@rice.edu>,"Thu, 4 Feb 2016 09:13:05 -0600",Re: Using CUDA within Spark / boosting linear algebra,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,

Iâ€™m jumping on this thread to point out another Spark+GPU project for people to take a look at: https://github.com/agrippa/spark-swat <https://github.com/agrippa/spark-swat>

SWAT (Spark with Accelerated Tasks) is a third-party JAR sitting on top of Spark that uses runtime code generation to convert user-written transformations into OpenCL kernels. SWATâ€™s lightweight runtime supports multi-GPU systems, managing each device and its memory automatically. You write your own Spark programs, and the runtime takes care of offloading your transformations to the GPUs in your system:

val rdd = CLWrapper.cl(sc.objectFile(inputPath))
val next = rdd.map(i => 2 * i).collect

SWAT primarily distinguishes itself in programmability: an explicit goal of this project is to have as few user-visible API changes as possible from what people have come to know and love in Spark. There are a number of fixed-function GPU libraries out there now, so we wanted to look instead at something that could be used to build new but still well-performing Spark apps.

SWAT is currently more of a research project than a production-ready system, so thereâ€™s a chance it wonâ€™t work out-of-the-box on some systems. With that said, it does have fairly comprehensive functional and code generation testing. If youâ€™re interested in trying it out and having trouble setting up, feel free to contact me directly. And of course, any questions or feedback from the community are always welcome.

Thanks,

Max

important items is to effectively and easily enable highly-tuned libraries for GPU such as BIDMach.
it is not easy task to scaling BIDMach with current Spark. I expect that this talk would help us.
http://conferences.oreilly.com/strata/hadoop-big-data-ca/public/schedule/detail/47565 <http://conferences.oreilly.com/strata/hadoop-big-data-ca/public/schedule/detail/47565>
Tokyo
<dev@spark.apache.org>, Joseph Bradley <joseph@databricks.com>
<evan.sparks@gmail.com>, Xiangrui Meng <mengxr@gmail.com>, Sam Halliday <sam.halliday@gmail.com>
summarizes the costs for moving different data sizes with regards to matrices multiplication. These costs are paid for the convenience of using the standard BLAS API that Nvidia NVBLAS provides. The thing is that there are no code changes required (in Spark), one just needs to reference BLAS implementation with the system variable. Naturally, hardware-specific implementation will always be faster than default. The benchmark results show that fact by comparing jCuda (by means of BIDMat) and NVBLAS. However, it also shows that it worth using NVBLAS for large matrices because it can take advantage of several GPUs and it will be faster despite the copying overhead. That is also a known thing advertised by Nvidia.
is an issue, because one can use transposed matrices to fit the required format. I believe that is just a software preference.
comparisons with Sparkâ€™s implementation of logistic regression (that does not take advantage of GPU) and also with BIDMachâ€™s (that takes advantage of GPUs). It will give the users a better understanding of yourâ€™s implementation performance. Currently you compare it with Sparkâ€™s example logistic regression implementation that is supposed to be a reference for learning Spark rather than benchmarking its performance.
<mailto:ISHIZAKI@jp.ibm.com>] 
done
another part of memory sounds like a much bigger undertaking.
efficiently exploit GPUs in Spark.
GPU-friendly column format
<http://kiszk.github.io/spark-gpu/>addresses these two issues by supporting data partition caching in GPU device memory and by providing binary column storage for data partition. We really appreciate it if you would give us comments, suggestions, or feedback.
<mailto:alexander.ulanov@hpe.com>>
<mailto:sam.halliday@gmail.com>>, John Canny <canny@berkeley.edu <mailto:canny@berkeley.edu>>
""dev@spark.apache.org <mailto:dev@spark.apache.org>"" <dev@spark.apache.org <mailto:dev@spark.apache.org>>, Joseph Bradley <joseph@databricks.com <mailto:joseph@databricks.com>>, ""Evan R. Sparks"" <evan.sparks@gmail.com <mailto:evan.sparks@gmail.com>>
hardware with 2x Nvidia Tesla K80 (physically 4x Tesla K40) and 2x modern Haswell CPU Intel E5-2650 v3 @ 2.30GHz.
experiment and approximated FLOPS.
2 sheets):
https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J5r7kwKSPkY/edit?usp=sharing <https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J5r7kwKSPkY/edit?usp=sharing>
<https://github.com/avulanov/scala-blas>
<mailto:sam.halliday@gmail.com>] 
Joseph Bradley; Evan R. Sparks; Ulanov, Alexander
in industry,  although your personal experience may be different. 
Dense BLAS are not very important for most machine learning workloads: at least for non-image workloads in industry (and for image processing you would probably want a deep learning/SGD solution with convolution kernels). e.g. it was only relevant for 1/7 of our recent benchmarks, which should be a reasonable sample. What really matters is sparse BLAS performance. BIDMat is still an order of magnitude faster there. Those kernels are only in BIDMat, since NVIDIAs sparse BLAS dont perform well on power-law data. 
determined by the slowest kernel, not the fastest. If the goal is to get closer to BIDMach's performance on typical problems, you need to make sure that every kernel goes at comparable speed. So the real question is how much faster MLLib routines do on a complete problem with/without GPU acceleration. For BIDMach, its close to a factor of 10. But that required running entirely on the GPU, and making sure every kernel is close to its limit.
end-to-end benchmarks. 
performance from breeze/netlib-java - meaning there's no compelling performance reason to switch out our current linear algebra library (at least as far as this benchmark is concerned). 
the right BLAS library will get us most of the way there. Or, would it make sense to finally ship openblas compiled for some common platforms (64-bit linux, windows, mac) directly with Spark - hopefully eliminating the jblas warnings once and for all for most users? (Licensing is BSD) Or am I missing something?
double-checked them. It turns that nvblas did not do multiplication due to parameter NVBLAS_TILE_DIM from ""nvblas.conf"" and returned zero matrix. My previously posted results with nvblas are matrices copying only. The default NVBLAS_TILE_DIM==2048 is too big for my graphic card/matrix size. I handpicked other values that worked. As a result, netlib+nvblas is on par with BIDMat-cuda. As promised, I am going to post a how-to for nvblas configuration.
https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J5r7kwKSPkY/edit?usp=sharing <https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J5r7kwKSPkY/edit?usp=sharing>
Joseph Bradley; Evan R. Sparks; jfcanny
exceptional performance for big matrices with Double, faster than BIDMat-cuda with Float. But for smaller matrices, if you will copy them to/from GPU, OpenBlas or MKL might be a better choice. This correlates with original nvblas presentation on GPU conf 2013 (slide 21): http://on-demand.gputechconf.com/supercomputing/2013/presentation/SC3108-New-Features-CUDA%206%20-GPU-Acceleration.pdf <http://on-demand.gputechconf.com/supercomputing/2013/presentation/SC3108-New-Features-CUDA%206%20-GPU-Acceleration.pdf>
https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J5r7kwKSPkY/edit?usp=sharing <https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J5r7kwKSPkY/edit?usp=sharing>
different libraries. I just want to pick a library that does at best dense matrices multiplication for my task.
blas functions, at the same time netlib-java uses C cblas functions. So, one needs cblas shared library to use nvblas through netlib-java. Fedora does not have cblas (but Debian and Ubuntu have), so I needed to compile it. I could not use cblas from Atlas or Openblas because they link to their implementation and not to Fortran blas.
Joseph Bradley; Evan R. Sparks
functions should replace current blas functions calls after executing LD_PRELOAD as suggested in http://docs.nvidia.com/cuda/nvblas/#Usage <http://docs.nvidia.com/cuda/nvblas/#Usage>without any changes to netlib-java. It seems to work for simple Java example, but I cannot make it work with Spark. I run the following:
--driver-memory 4G In nvidia-smi I observe that Java is to use GPU:
+-----------------------------------------------------------------------------+
Memory |
Usage      |
|=============================================================================|
 39MiB |
 39MiB |
+-----------------------------------------------------------------------------+
/tmp/jniloader8192964377009965483netlib-native_system-linux-x86_64.so
However, matrix multiplication does executes on CPU since I see 16% of CPU used and 0% of GPU used. I also checked different matrix sizes, from 100x100 to 12000x12000
<mailto:sam.halliday@gmail.com>]
Joseph Bradley; Evan R. Sparks
performance on various pieces of hardware...
<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com the comment that BIDMat 0.9.7 uses Float matrices in GPU (although I see the support of Double in the current source code), did the test with BIDMat and CPU Double matrices. BIDMat MKL is indeed on par with netlib MKL.
https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J5r7kwKSPkY/edit?usp=sharing <https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J5r7kwKSPkY/edit?usp=sharing>
<mailto:sam.halliday@gmail.com><mailto:sam.halliday@gmail.com <mailto:sam.halliday@gmail.com>>]
<mailto:dev@spark.apache.org><mailto:dev@spark.apache.org <mailto:dev@spark.apache.org>>
weeks?
https://skillsmatter.com/meetups/6987-apache-spark-living-the-post-mapreduce-world#community <https://skillsmatter.com/meetups/6987-apache-spark-living-the-post-mapreduce-world#community>
<mailto:mengxr@gmail.com><mailto:mengxr@gmail.com <mailto:mengxr@gmail.com>>> writes:
BLAS
<joseph@databricks.com <mailto:joseph@databricks.com><mailto:joseph@databricks.com JIRA:
<https://issues.apache.org/jira/browse/SPARK-6019>
<mailto:evan.sparks@gmail.com><mailto:evan.sparks@gmail.com <mailto:evan.sparks@gmail.com>>>
https://docs.google.com/spreadsheets/d/1aRm2IADRfXQV7G2vrcVh4StF50uZ <https://docs.google.com/spreadsheets/d/1aRm2IADRfXQV7G2vrcVh4StF50uZ>
magnitude
larger matrices.
this
<https://github.com/fommil/netlib-java>)
GPU
could
CPU-based
<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com MKL==netlib-mkl==netlib-openblas-compiled>netlib-openblas-yum-repo=
https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx <https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx>
<mailto:dev@spark.apache.org><mailto:dev@spark.apache.org <mailto:dev@spark.apache.org>>
to
+-----------------------------------------------------------------------+
|
|
columns with
<mailto:evan.sparks@gmail.com><mailto:evan.sparks@gmail.com <mailto:evan.sparks@gmail.com>>]
<mailto:dev@spark.apache.org><mailto:dev@spark.apache.org <mailto:dev@spark.apache.org>>
<https://issues.apache.org/jira/browse/SPARK-5705>)
while
<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com <mailto:alexander.ulanov@hp.com>><mailto:alexander.ulanov@hp.com <mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com run
I
Nevertheless,
than
supposed to be on par with JNI overheads.
MKL,
<mailto:evan.sparks@gmail.com><mailto:evan.sparks@gmail.com <mailto:evan.sparks@gmail.com>><mailto:
<mailto:evan.sparks@gmail.com><mailto:evan.sparks@gmail.com <mailto:evan.sparks@gmail.com>>>]
<mailto:dev@spark.apache.org><mailto:dev@spark.apache.org <mailto:dev@spark.apache.org>><mailto:dev@spark <mailto:dev@spark>.
<mailto:dev@spark.apache.org>>>
comes
particular
builds
here.
<https://github.com/shivaram/matrix-bench>
up by netlib-java.
<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com <mailto:alexander.ulanov@hp.com>><mailto:alexander.ulanov@hp.com <mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com to

understand how to force use a specific blas (not specific wrapper for blas).
suppose
<mailto:evan.sparks@gmail.com><mailto:evan.sparks@gmail.com <mailto:evan.sparks@gmail.com>><mailto:
<mailto:evan.sparks@gmail.com><mailto:evan.sparks@gmail.com <mailto:evan.sparks@gmail.com>>>]
<mailto:dev@spark.apache.org><mailto:dev@spark.apache.org <mailto:dev@spark.apache.org>><mailto:dev@spark <mailto:dev@spark>.
<mailto:dev@spark.apache.org>>>
have it).
BLAS
<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com <mailto:alexander.ulanov@hp.com>><mailto:alexander.ulanov@hp.com <mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com formatting):
+-----------------------------------------------------------------------+
1569,233228
Fedora
much
<mailto:joseph@databricks.com><mailto:joseph@databricks.com <mailto:joseph@databricks.com>><mailto:
<mailto:joseph@databricks.com><mailto:joseph@databricks.com <mailto:joseph@databricks.com>>>]
<mailto:dev@spark.apache.org><mailto:dev@spark.apache.org <mailto:dev@spark.apache.org>><mailto:dev@spark <mailto:dev@spark>.
<mailto:dev@spark.apache.org>>>
done
another part of memory sounds like a much bigger undertaking.
<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com <mailto:alexander.ulanov@hp.com>><mailto:alexander.ulanov@hp.com <mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com presentation by
with Spark MLlib.
Spark:
a
artificial
 test of
to machine learning.
<mailto:evan.sparks@gmail.com><mailto:evan.sparks@gmail.com <mailto:evan.sparks@gmail.com>><mailto:
<mailto:evan.sparks@gmail.com><mailto:evan.sparks@gmail.com <mailto:evan.sparks@gmail.com>>>]
<mailto:dev@spark.apache.org><mailto:dev@spark.apache.org <mailto:dev@spark.apache.org>><mailto:dev@spark <mailto:dev@spark>.
<mailto:dev@spark.apache.org>>>
to
I
GPU
to
and 4 GPUs.
terabytes.
address
Java
<http://arxiv.org/abs/1409.5402>[2] -
<http://eecs.berkeley.edu/%7Ehzhao/papers/BD.pdf>
<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com <mailto:alexander.ulanov@hp.com>><mailto:alexander.ulanov@hp.com <mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com <mailto:alexander.ulanov@hp.com>>><mailto:
<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com <mailto:alexander.ulanov@hp.com>><mailto:alexander.ulanov@hp.com <mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com If
BIDMach for optimization and learning?
<mailto:evan.sparks@gmail.com><mailto:evan.sparks@gmail.com <mailto:evan.sparks@gmail.com>><mailto:
<mailto:evan.sparks@gmail.com><mailto:evan.sparks@gmail.com <mailto:evan.sparks@gmail.com>>><mailto:evan.sparks@gmail.com <mailto:evan.sparks@gmail.com><mailto:evan.sparks@gmail.com <mailto:evan.sparks@gmail.com>><mailto:
<mailto:evan.sparks@gmail.com><mailto:evan.sparks@gmail.com <mailto:evan.sparks@gmail.com>>>>]
<mailto:dev@spark.apache.org><mailto:dev@spark.apache.org <mailto:dev@spark.apache.org>><mailto:dev@spark.apache.org <mailto:dev@spark.apache.org><mailto:dev@spark.apache.org <mailto:dev@spark.apache.org>>><mailto:
<mailto:dev@spark.apache.org><mailto:dev@spark.apache.org <mailto:dev@spark.apache.org>><mailto:dev@spark <mailto:dev@spark>.
<mailto:dev@spark.apache.org>>>>
<https://github.com/BIDData/BIDMat>) takes and comparing them to
on
faster at matrix multiply.
<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com <mailto:alexander.ulanov@hp.com>><mailto:alexander.ulanov@hp.com <mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com <mailto:alexander.ulanov@hp.com>>><mailto:
<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com <mailto:alexander.ulanov@hp.com>><mailto:alexander.ulanov@hp.com <mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com within Spark.
subprograms)
It
performance only for BLAS level 3, i.e.
<https://github.com/fommil/netlib-java>. I also confirmed it with my
<https://github.com/apache/spark/pull/1290#issuecomment-70313952>.
Linux
linked
put
network
becomes
not a test for ONLY multiplication since there are other operations involved.
back.
-------------------------------------------------------------------
<mailto:dev-unsubscribe@spark.apache.org><mailto:dev-unsubscribe@spark.apache.org <mailto:dev-unsubscribe@spark.apache.org>><mailto:
<mailto:dev-unsubscribe@spark.apache.org><mailto:dev-unsubscribe@spark.apach <mailto:dev-unsubscribe@spark.apach>
<mailto:dev-unsubscribe@spark.apac><mailto:dev-unsubscribe@sp <mailto:dev-unsubscribe@sp>
<mailto:dev-unsubscribe@spark.apache.org><mailto:dev-unsubscribe@spa <mailto:dev-unsubscribe@spa>
e-mail:
<mailto:dev-help@spark.apache.org><mailto:dev-help@spark.apache.org <mailto:dev-help@spark.apache.org>><mailto:
<mailto:dev-help@spark.apache.org><mailto:dev-help@spark.apache.org <mailto:dev-help@spark.apache.org>>><mailto:dev-help@spark.apache.org <mailto:dev-help@spark.apache.org><mailto:dev-help@spark.apache.org <mailto:dev-help@spark.apache.org>><mailto:
<mailto:dev-help@spark.apache.org><mailto:dev-help@spark.apache.org <mailto:dev-help@spark.apache.org>>>>

"
"""Allen Zhang"" <allenzhang010@126.com>","Thu, 4 Feb 2016 23:42:13 +0800 (CST)",Re: Using CUDA within Spark / boosting linear algebra,"""Max Grossman"" <jmg3@rice.edu>","Hi Max,

I will look at it tomorrow. but a quick question, does it support CUDA from Nvidia, not only OpenCL?

Thanks,
Allen







At 2016-02-04 23:13:05, ""Max Grossman"" <jmg3@rice.edu> wrote:
Hi all,


I¡¯m jumping on this thread to point out another Spark+GPU project for people to take a look at: https://github.com/agrippa/spark-swat


SWAT (Spark with Accelerated Tasks) is a third-party JAR sitting on top of Spark that uses runtime code generation to convert user-written transformations into OpenCL kernels. SWAT¡¯s lightweight runtime supports multi-GPU systems, managing each device and its memory automatically. You write your own Spark programs, and the runtime takes care of offloading your transformations to the GPUs in your system:


val rdd = CLWrapper.cl(sc.objectFile(inputPath))
val next = rdd.map(i => 2 * i).collect


SWAT primarily distinguishes itself in programmability: an explicit goal of this project is to have as few user-visible API changes as possible from what people have come to know and love in Spark. There are a number of fixed-function GPU libraries out there now, so we wanted to look instead at something that could be used to build new but still well-performing Spark apps.


SWAT is currently more of a research project than a production-ready system, so there¡¯s a chance it won¡¯t work out-of-the-box on some systems. With that said, it does have fairly comprehensive functional and code generation testing. If you¡¯re interested in trying it out and having trouble setting up, feel free to contact me directly. And of course, any questions or feedback from the community are always welcome.


Thanks,


Max


On Jan 22, 2016, at 3:42 AM, Kazuaki Ishizaki <ISHIZAKI@jp.ibm.com> wrote:


Hi Alexander,
The goal of our columnar to effectively drive GPUs in Spark. One of important items is to effectively and easily enable highly-tuned libraries for GPU such as BIDMach.

We will enable BIDMach with our columnar storage. On the other hand, it is not easy task to scaling BIDMach with current Spark. I expect that this talk would help us.
http://conferences.oreilly.com/strata/hadoop-big-data-ca/public/schedule/detail/47565

We appreciate your great feedback.

Best Regards,
Kazuaki Ishizaki, Ph.D., Senior research staff member, IBM Research - Tokyo



From:        ""Ulanov, Alexander"" <alexander.ulanov@hpe.com>
To:        Kazuaki Ishizaki/Japan/IBM@IBMJP, ""dev@spark.apache.org"" <dev@spark.apache.org>, Joseph Bradley <joseph@databricks.com>
Cc:        John Canny <canny@berkeley.edu>, ""Evan R. Sparks"" <evan.sparks@gmail.com>, Xiangrui Meng <mengxr@gmail.com>, Sam Halliday <sam.halliday@gmail.com>
Date:        2016/01/22 04:20
Subject:        RE: Using CUDA within Spark / boosting linear algebra




Hi Kazuaki,
 
Indeed, moving data to/from GPU is costly and this benchmark summarizes the costs for moving different data sizes with regards to matrices multiplication. These costs are paid for the convenience of using the standard BLAS API that Nvidia NVBLAS provides. The thing is that there are no code changes required (in Spark), one just needs to reference BLAS implementation with the system variable. Naturally, hardware-specific implementation will always be faster than default. The benchmark results show that fact by comparing jCuda (by means of BIDMat) and NVBLAS. However, it also shows that it worth using NVBLAS for large matrices because it can take advantage of several GPUs and it will be faster despite the copying overhead. That is also a known thing advertised by Nvidia.
 
By the way, I don¡¯t think that the column/row friendly format is an issue, because one can use transposed matrices to fit the required format. I believe that is just a software preference.
 
My suggestion with regards to your prototype would be to make comparisons with Spark¡¯s implementation of logistic regression (that does not take advantage of GPU) and also with BIDMach¡¯s (that takes advantage of GPUs). It will give the users a better understanding of your¡¯s implementation performance. Currently you compare it with Spark¡¯s example logistic regression implementation that is supposed to be a reference for learning Spark rather than benchmarking its performance.
 
Best regards, Alexander
 
From: Kazuaki Ishizaki [mailto:ISHIZAKI@jp.ibm.com]
Sent: Thursday, January 21, 2016 3:34 AM
To:dev@spark.apache.org; Ulanov, Alexander; Joseph Bradley
Cc: John Canny; Evan R. Sparks; Xiangrui Meng; Sam Halliday
Subject: RE: Using CUDA within Spark / boosting linear algebra
 
Dear all,

>>>> Hi Alexander,
>>>>
>>>> Using GPUs with Spark would be very exciting.  Small comment:
>>>> Concerning your question earlier about keeping data stored on the
>>>> GPU rather than having to move it between main memory and GPU
>>>> memory on each iteration, I would guess this would be critical to
>>>> getting good performance.  If you could do multiple local
>>>> iterations before aggregating results, then the cost of data
>>>> movement to the GPU could be amortized (and I believe that is done
>>>> in practice).  Having Spark be aware of the GPU and using it as another part of memory sounds like a much bigger undertaking.
>>>>
>>>> Joseph

As Joseph pointed out before, there are two potential issues to efficiently exploit GPUs in Spark.
(1) the cost of data movement between CPU and GPU
(2) the cost of encoding/decoding between current row-format and GPU-friendly column format

Our prototype http://kiszk.github.io/spark-gpu/addresses these two issues by supporting data partition caching in GPU device memory and by providing binary column storage for data partition. We really appreciate it if you would give us comments, suggestions, or feedback.

Best Regards
Kazuaki Ishizaki



From:        ""Ulanov, Alexander"" <alexander.ulanov@hpe.com>
To:        Sam Halliday <sam.halliday@gmail.com>, John Canny <canny@berkeley.edu>
Cc:        Xiangrui Meng <mengxr@gmail.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>, Joseph Bradley <joseph@databricks.com>, ""Evan R. Sparks"" <evan.sparks@gmail.com>
Date:        2016/01/21 11:07
Subject:        RE: Using CUDA within Spark / boosting linear algebra




Hi Everyone,

I¡¯ve updated the benchmark and done experiments with new hardware with 2x Nvidia Tesla K80 (physically 4x Tesla K40) and 2x modern Haswell CPU Intel E5-2650 v3 @ 2.30GHz.

This time I computed average and median of 10 runs for each of experiment and approximated FLOPS.

Results are available at google docs (old experiments are in the other 2 sheets):
https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J5r7kwKSPkY/edit?usp=sharing
Benchmark code:
https://github.com/avulanov/scala-blas

Best regards, Alexander


From: Sam Halliday [mailto:sam.halliday@gmail.com]
Sent: Thursday, March 26, 2015 9:27 AM
To: John Canny
Cc: Xiangrui Meng; dev@spark.apache.org; Joseph Bradley; Evan R. Sparks; Ulanov, Alexander
Subject: Re: Using CUDA within Spark / boosting linear algebra


John, I have to disagree with you there. Dense matrices come up a lot in industry,  although your personal experience may be different.
On 26 Mar 2015 16:20, ""John Canny"" <canny@berkeley.edu> wrote:
I mentioned this earlier in the thread, but I'll put it out again. Dense BLAS are not very important for most machine learning workloads: at least for non-image workloads in industry (and for image processing you would probably want a deep learning/SGD solution with convolution kernels). e.g. it was only relevant for 1/7 of our recent benchmarks, which should be a reasonable sample. What really matters is sparse BLAS performance. BIDMat is still an order of magnitude faster there. Those kernels are only in BIDMat, since NVIDIAs sparse BLAS dont perform well on power-law data.

Its also the case that the overall performance of an algorithm is determined by the slowest kernel, not the fastest. If the goal is to get closer to BIDMach's performance on typical problems, you need to make sure that every kernel goes at comparable speed. So the real question is how much faster MLLib routines do on a complete problem with/without GPU acceleration. For BIDMach, its close to a factor of 10. But that required running entirely on the GPU, and making sure every kernel is close to its limit.

-John

If you think nvblas would be helpful, you should try it in some end-to-end benchmarks.
On 3/25/15, 6:23 PM, Evan R. Sparks wrote:
Yeah, much more reasonable - nice to know that we can get full GPU performance from breeze/netlib-java - meaning there's no compelling performance reason to switch out our current linear algebra library (at least as far as this benchmark is concerned).

Instead, it looks like a user guide for configuring Spark/MLlib to use the right BLAS library will get us most of the way there. Or, would it make sense to finally ship openblas compiled for some common platforms (64-bit linux, windows, mac) directly with Spark - hopefully eliminating the jblas warnings once and for all for most users? (Licensing is BSD) Or am I missing something?

On Wed, Mar 25, 2015 at 6:03 PM, Ulanov, Alexander <alexander.ulanov@hp.com> wrote:
As everyone suggested, the results were too good to be true, so I double-checked them. It turns that nvblas did not do multiplication due to parameter NVBLAS_TILE_DIM from ""nvblas.conf"" and returned zero matrix. My previously posted results with nvblas are matrices copying only. The default NVBLAS_TILE_DIM==2048 is too big for my graphic card/matrix size. I handpicked other values that worked. As a result, netlib+nvblas is on par with BIDMat-cuda. As promised, I am going to post a how-to for nvblas configuration.

https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J5r7kwKSPkY/edit?usp=sharing



-----Original Message-----
From: Ulanov, Alexander
Sent: Wednesday, March 25, 2015 2:31 PM
To: Sam Halliday
Cc: dev@spark.apache.org; Xiangrui Meng; Joseph Bradley; Evan R. Sparks; jfcanny
Subject: RE: Using CUDA within Spark / boosting linear algebra

Hi again,

I finally managed to use nvblas within Spark+netlib-java. It has exceptional performance for big matrices with Double, faster than BIDMat-cuda with Float. But for smaller matrices, if you will copy them to/from GPU, OpenBlas or MKL might be a better choice. This correlates with original nvblas presentation on GPU conf 2013 (slide 21): http://on-demand.gputechconf.com/supercomputing/2013/presentation/SC3108-New-Features-CUDA%206%20-GPU-Acceleration.pdf

My results:
https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J5r7kwKSPkY/edit?usp=sharing

Just in case, these tests are not for generalization of performance of different libraries. I just want to pick a library that does at best dense matrices multiplication for my task.

P.S. My previous issue with nvblas was the following: it has Fortran blas functions, at the same time netlib-java uses C cblas functions. So, one needs cblas shared library to use nvblas through netlib-java. Fedora does not have cblas (but Debian and Ubuntu have), so I needed to compile it. I could not use cblas from Atlas or Openblas because they link to their implementation and not to Fortran blas.

Best regards, Alexander

-----Original Message-----
From: Ulanov, Alexander
Sent: Tuesday, March 24, 2015 6:57 PM
To: Sam Halliday
Cc: dev@spark.apache.org; Xiangrui Meng; Joseph Bradley; Evan R. Sparks
Subject: RE: Using CUDA within Spark / boosting linear algebra

Hi,

I am trying to use nvblas with netlib-java from Spark. nvblas functions should replace current blas functions calls after executing LD_PRELOAD as suggested in http://docs.nvidia.com/cuda/nvblas/#Usagewithout any changes to netlib-java. It seems to work for simple Java example, but I cannot make it work with Spark. I run the following:
export LD_LIBRARY_PATH=/usr/local/cuda-6.5/lib64
env LD_PRELOAD=/usr/local/cuda-6.5/lib64/libnvblas.so ./spark-shell --driver-memory 4G In nvidia-smi I observe that Java is to use GPU:
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0      8873    C   bash                                            39MiB |
|    0      8910    C   /usr/lib/jvm/java-1.7.0/bin/java                39MiB |
+-----------------------------------------------------------------------------+

In Spark shell I do matrix multiplication and see the following:
15/03/25 06:48:01 INFO JniLoader: successfully loaded /tmp/jniloader8192964377009965483netlib-native_system-linux-x86_64.so
So I am sure that netlib-native is loaded and cblas supposedly used. However, matrix multiplication does executes on CPU since I see 16% of CPU used and 0% of GPU used. I also checked different matrix sizes, from 100x100 to 12000x12000

Could you suggest might the LD_PRELOAD not affect Spark shell?

Best regards, Alexander



From: Sam Halliday [mailto:sam.halliday@gmail.com]
Sent: Monday, March 09, 2015 6:01 PM
To: Ulanov, Alexander
Cc: dev@spark.apache.org; Xiangrui Meng; Joseph Bradley; Evan R. Sparks
Subject: RE: Using CUDA within Spark / boosting linear algebra


Thanks so much for following up on this!

Hmm, I wonder if we should have a concerted effort to chart performance on various pieces of hardware...
On 9 Mar 2015 21:08, ""Ulanov, Alexander"" <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
Hi Everyone, I've updated the benchmark as Xiangrui suggested. Added the comment that BIDMat 0.9.7 uses Float matrices in GPU (although I see the support of Double in the current source code), did the test with BIDMat and CPU Double matrices. BIDMat MKL is indeed on par with netlib MKL.

https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J5r7kwKSPkY/edit?usp=sharing

Best regards, Alexander

-----Original Message-----
From: Sam Halliday [mailto:sam.halliday@gmail.com<mailto:sam.halliday@gmail.com>]
Sent: Tuesday, March 03, 2015 1:54 PM
To: Xiangrui Meng; Joseph Bradley
Cc: Evan R. Sparks; Ulanov, Alexander; dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: Using CUDA within Spark / boosting linear algebra

BTW, is anybody on this list going to the London Meetup in a few weeks?

https://skillsmatter.com/meetups/6987-apache-spark-living-the-post-mapreduce-world#community

Would be nice to meet other people working on the guts of Spark! :-)


Xiangrui Meng <mengxr@gmail.com<mailto:mengxr@gmail.com>> writes:

> Hey Alexander,
>
> I don't quite understand the part where netlib-cublas is about 20x
> slower than netlib-openblas. What is the overhead of using a GPU BLAS
> with netlib-java?
>
> CC'ed Sam, the author of netlib-java.
>
> Best,
> Xiangrui
>
> On Wed, Feb 25, 2015 at 3:36 PM, Joseph Bradley <joseph@databricks.com<mailto:joseph@databricks.com>> wrote:
>> Better documentation for linking would be very helpful!  Here's a JIRA:
>> https://issues.apache.org/jira/browse/SPARK-6019
>>
>>
>> On Wed, Feb 25, 2015 at 2:53 PM, Evan R. Sparks
>> <evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>
>> wrote:
>>
>>> Thanks for compiling all the data and running these benchmarks,
>>> Alex. The big takeaways here can be seen with this chart:
>>>
>>> https://docs.google.com/spreadsheets/d/1aRm2IADRfXQV7G2vrcVh4StF50uZ
>>> Hl6kmAJeaZZggr0/pubchart?oid=1899767119&format=interactive
>>>
>>> 1) A properly configured GPU matrix multiply implementation (e.g.
>>> BIDMat+GPU) can provide substantial (but less than an order of
>>> BIDMat+magnitude)
>>> benefit over a well-tuned CPU implementation (e.g. BIDMat+MKL or
>>> netlib-java+openblas-compiled).
>>> 2) A poorly tuned CPU implementation can be 1-2 orders of magnitude
>>> worse than a well-tuned CPU implementation, particularly for larger matrices.
>>> (netlib-f2jblas or netlib-ref) This is not to pick on netlib - this
>>> basically agrees with the authors own benchmarks (
>>> https://github.com/fommil/netlib-java)
>>>
>>> I think that most of our users are in a situation where using GPUs
>>> may not be practical - although we could consider having a good GPU
>>> backend available as an option. However, *ALL* users of MLlib could
>>> benefit (potentially tremendously) from using a well-tuned CPU-based
>>> BLAS implementation. Perhaps we should consider updating the mllib
>>> guide with a more complete section for enabling high performance
>>> binaries on OSX and Linux? Or better, figure out a way for the
>>> system to fetch these automatically.
>>>
>>> - Evan
>>>
>>>
>>>
>>> On Thu, Feb 12, 2015 at 4:18 PM, Ulanov, Alexander <
>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
>>>
>>>> Just to summarize this thread, I was finally able to make all
>>>> performance comparisons that we discussed. It turns out that:
>>>> BIDMat-cublas>>BIDMat
>>>> MKL==netlib-mkl==netlib-openblas-compiled>netlib-openblas-yum-repo=
>>>> =netlib-cublas>netlib-blas>f2jblas
>>>>
>>>> Below is the link to the spreadsheet with full results.
>>>>
>>>> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx
>>>> 378T9J5r7kwKSPkY/edit?usp=sharing
>>>>
>>>> One thing still needs exploration: does BIDMat-cublas perform
>>>> copying to/from machine¡¯s RAM?
>>>>
>>>> -----Original Message-----
>>>> From: Ulanov, Alexander
>>>> Sent: Tuesday, February 10, 2015 2:12 PM
>>>> To: Evan R. Sparks
>>>> Cc: Joseph Bradley;
>>>> dev@spark.apache.org<mailto:dev@spark.apache.org>
>>>> Subject: RE: Using CUDA within Spark / boosting linear algebra
>>>>
>>>> Thanks, Evan! It seems that ticket was marked as duplicate though
>>>> the original one discusses slightly different topic. I was able to
>>>> link netlib with MKL from BIDMat binaries. Indeed, MKL is
>>>> statically linked inside a 60MB library.
>>>>
>>>> |A*B  size | BIDMat MKL | Breeze+Netlib-MKL  from BIDMat|
>>>> Breeze+Netlib-OpenBlas(native system)| Breeze+Netlib-f2jblas |
>>>> +-----------------------------------------------------------------------+
>>>> |100x100*100x100 | 0,00205596 | 0,000381 | 0,03810324 | 0,002556 |
>>>> |1000x1000*1000x1000 | 0,018320947 | 0,038316857 | 0,51803557
>>>> |1,638475459 |
>>>> |10000x10000*10000x10000 | 23,78046632 | 32,94546697 |445,0935211 |
>>>> 1569,233228 |
>>>>
>>>> It turn out that pre-compiled MKL is faster than precompiled
>>>> OpenBlas on my machine. Probably, I¡¯ll add two more columns with
>>>> locally compiled openblas and cuda.
>>>>
>>>> Alexander
>>>>
>>>> From: Evan R. Sparks
>>>> [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>]
>>>> Sent: Monday, February 09, 2015 6:06 PM
>>>> To: Ulanov, Alexander
>>>> Cc: Joseph Bradley;
>>>> dev@spark.apache.org<mailto:dev@spark.apache.org>
>>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>>>
>>>> Great - perhaps we can move this discussion off-list and onto a
>>>> JIRA ticket? (Here's one:
>>>> https://issues.apache.org/jira/browse/SPARK-5705)
>>>>
>>>> It seems like this is going to be somewhat exploratory for a while
>>>> (and there's probably only a handful of us who really care about
>>>> fast linear
>>>> algebra!)
>>>>
>>>> - Evan
>>>>
>>>> On Mon, Feb 9, 2015 at 4:48 PM, Ulanov, Alexander <
>>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
>>>> Hi Evan,
>>>>
>>>> Thank you for explanation and useful link. I am going to build
>>>> OpenBLAS, link it with Netlib-java and perform benchmark again.
>>>>
>>>> Do I understand correctly that BIDMat binaries contain statically
>>>> linked Intel MKL BLAS? It might be the reason why I am able to run
>>>> BIDMat not having MKL BLAS installed on my server. If it is true, I
>>>> wonder if it is OK because Intel sells this library. Nevertheless,
>>>> it seems that in my case precompiled MKL BLAS performs better than
>>>> precompiled OpenBLAS given that BIDMat and Netlib-java are supposed to be on par with JNI overheads.
>>>>
>>>> Though, it might be interesting to link Netlib-java with Intel MKL,
>>>> as you suggested. I wonder, are John Canny (BIDMat) and Sam
>>>> Halliday
>>>> (Netlib-java) interested to compare their libraries.
>>>>
>>>> Best regards, Alexander
>>>>
>>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:
>>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
>>>> Sent: Friday, February 06, 2015 5:58 PM
>>>>
>>>> To: Ulanov, Alexander
>>>> Cc: Joseph Bradley;
>>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
>>>> apache.org<mailto:dev@spark.apache.org>>
>>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>>>
>>>> I would build OpenBLAS yourself, since good BLAS performance comes
>>>> from getting cache sizes, etc. set up correctly for your particular
>>>> hardware - this is often a very tricky process (see, e.g. ATLAS),
>>>> but we found that on relatively modern Xeon chips, OpenBLAS builds
>>>> quickly and yields performance competitive with MKL.
>>>>
>>>> To make sure the right library is getting used, you have to make
>>>> sure it's first on the search path - export
>>>> LD_LIBRARY_PATH=/path/to/blas/library.so will do the trick here.
>>>>
>>>> For some examples of getting netlib-java setup on an ec2 node and
>>>> some example benchmarking code we ran a while back, see:
>>>> https://github.com/shivaram/matrix-bench
>>>>
>>>> In particular - build-openblas-ec2.sh shows you how to build the
>>>> library and set up symlinks correctly, and scala/run-netlib.sh
>>>> shows you how to get the path setup and get that library picked up by netlib-java.
>>>>
>>>> In this way - you could probably get cuBLAS set up to be used by
>>>> netlib-java as well.
>>>>
>>>> - Evan
>>>>
>>>> On Fri, Feb 6, 2015 at 5:43 PM, Ulanov, Alexander <
>>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
>>>> Evan, could you elaborate on how to force BIDMat and netlib-java to
>>>> force loading the right blas? For netlib, I there are few JVM
>>>> flags, such as
>>>> -Dcom.github.fommil.netlib.BLAS=com.github.fommil.netlib.F2jBLAS,
>>>> so I can force it to use Java implementation. Not sure I understand how to force use a specific blas (not specific wrapper for blas).
>>>>
>>>> Btw. I have installed openblas (yum install openblas), so I suppose
>>>> that netlib is using it.
>>>>
>>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:
>>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
>>>> Sent: Friday, February 06, 2015 5:19 PM
>>>> To: Ulanov, Alexander
>>>> Cc: Joseph Bradley;
>>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
>>>> apache.org<mailto:dev@spark.apache.org>>
>>>>
>>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>>>
>>>> Getting breeze to pick up the right blas library is critical for
>>>> performance. I recommend using OpenBLAS (or MKL, if you already have it).
>>>> It might make sense to force BIDMat to use the same underlying BLAS
>>>> library as well.
>>>>
>>>> On Fri, Feb 6, 2015 at 4:42 PM, Ulanov, Alexander <
>>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
>>>> Hi Evan, Joseph
>>>>
>>>> I did few matrix multiplication test and BIDMat seems to be ~10x
>>>> faster than netlib-java+breeze (sorry for weird table formatting):
>>>>
>>>> |A*B  size | BIDMat MKL | Breeze+Netlib-java
>>>> |native_system_linux_x86-64|
>>>> Breeze+Netlib-java f2jblas |
>>>> +-----------------------------------------------------------------------+
>>>> |100x100*100x100 | 0,00205596 | 0,03810324 | 0,002556 |
>>>> |1000x1000*1000x1000 | 0,018320947 | 0,51803557 |1,638475459 |
>>>> |10000x10000*10000x10000 | 23,78046632 | 445,0935211 | 1569,233228
>>>> ||
>>>>
>>>> Configuration: Intel(R) Xeon(R) CPU E31240 3.3 GHz, 6GB RAM, Fedora
>>>> 19 Linux, Scala 2.11.
>>>>
>>>> Later I will make tests with Cuda. I need to install new Cuda
>>>> version for this purpose.
>>>>
>>>> Do you have any ideas why breeze-netlib with native blas is so much
>>>> slower than BIDMat MKL?
>>>>
>>>> Best regards, Alexander
>>>>
>>>> From: Joseph Bradley [mailto:joseph@databricks.com<mailto:joseph@databricks.com><mailto:
>>>> joseph@databricks.com<mailto:joseph@databricks.com>>]
>>>> Sent: Thursday, February 05, 2015 5:29 PM
>>>> To: Ulanov, Alexander
>>>> Cc: Evan R. Sparks;
>>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
>>>> apache.org<mailto:dev@spark.apache.org>>
>>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>>>
>>>> Hi Alexander,
>>>>
>>>> Using GPUs with Spark would be very exciting.  Small comment:
>>>> Concerning your question earlier about keeping data stored on the
>>>> GPU rather than having to move it between main memory and GPU
>>>> memory on each iteration, I would guess this would be critical to
>>>> getting good performance.  If you could do multiple local
>>>> iterations before aggregating results, then the cost of data
>>>> movement to the GPU could be amortized (and I believe that is done
>>>> in practice).  Having Spark be aware of the GPU and using it as another part of memory sounds like a much bigger undertaking.
>>>>
>>>> Joseph
>>>>
>>>> On Thu, Feb 5, 2015 at 4:59 PM, Ulanov, Alexander <
>>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
>>>> Thank you for explanation! I¡¯ve watched the BIDMach presentation by
>>>> John Canny and I am really inspired by his talk and comparisons with Spark MLlib.
>>>>
>>>> I am very interested to find out what will be better within Spark:
>>>> BIDMat or netlib-java with CPU or GPU natives. Could you suggest a
>>>> fair way to benchmark them? Currently I do benchmarks on artificial
>>>> neural networks in batch mode. While it is not a ¡°pure¡± test of
>>>> linear algebra, it involves some other things that are essential to machine learning.
>>>>
>>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:
>>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
>>>> Sent: Thursday, February 05, 2015 1:29 PM
>>>> To: Ulanov, Alexander
>>>> Cc:
>>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
>>>> apache.org<mailto:dev@spark.apache.org>>
>>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>>>
>>>> I'd be surprised of BIDMat+OpenBLAS was significantly faster than
>>>> netlib-java+OpenBLAS, but if it is much faster it's probably due to
>>>> netlib-java+data
>>>> layout and fewer levels of indirection - it's definitely a
>>>> worthwhile experiment to run. The main speedups I've seen from
>>>> using it come from highly optimized GPU code for linear algebra. I
>>>> know that in the past Canny has gone as far as to write custom GPU
>>>> kernels for performance-critical regions of code.[1]
>>>>
>>>> BIDMach is highly optimized for single node performance or
>>>> performance on small clusters.[2] Once data doesn't fit easily in
>>>> GPU memory (or can be batched in that way) the performance tends to
>>>> fall off. Canny argues for hardware/software codesign and as such
>>>> prefers machine configurations that are quite different than what
>>>> we find in most commodity cluster nodes - e.g. 10 disk cahnnels and 4 GPUs.
>>>>
>>>> In contrast, MLlib was designed for horizontal scalability on
>>>> commodity clusters and works best on very big datasets - order of terabytes.
>>>>
>>>> For the most part, these projects developed concurrently to address
>>>> slightly different use cases. That said, there may be bits of
>>>> BIDMach we could repurpose for MLlib - keep in mind we need to be
>>>> careful about maintaining cross-language compatibility for our Java
>>>> and Python-users, though.
>>>>
>>>> - Evan
>>>>
>>>> [1] - http://arxiv.org/abs/1409.5402[2] -
>>>> http://eecs.berkeley.edu/~hzhao/papers/BD.pdf
>>>>
>>>> On Thu, Feb 5, 2015 at 1:00 PM, Ulanov, Alexander <
>>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:
>>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>>> wrote:
>>>> Hi Evan,
>>>>
>>>> Thank you for suggestion! BIDMat seems to have terrific speed. Do
>>>> you know what makes them faster than netlib-java?
>>>>
>>>> The same group has BIDMach library that implements machine
>>>> learning. For some examples they use Caffe convolutional neural
>>>> network library owned by another group in Berkeley. Could you
>>>> elaborate on how these all might be connected with Spark Mllib? If
>>>> you take BIDMat for linear algebra why don¡¯t you take BIDMach for optimization and learning?
>>>>
>>>> Best regards, Alexander
>>>>
>>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:
>>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>><mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:
>>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>>]
>>>> Sent: Thursday, February 05, 2015 12:09 PM
>>>> To: Ulanov, Alexander
>>>> Cc: dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.apache.org<mailto:dev@spark.apache.org>><mailto:
>>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
>>>> apache.org<mailto:dev@spark.apache.org>>>
>>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>>>
>>>> I'd expect that we can make GPU-accelerated BLAS faster than CPU
>>>> blas in many cases.
>>>>
>>>> You might consider taking a look at the codepaths that BIDMat (
>>>> https://github.com/BIDData/BIDMat) takes and comparing them to
>>>> netlib-java/breeze. John Canny et. al. have done a bunch of work
>>>> optimizing to make this work really fast from Scala. I've run it on
>>>> my laptop and compared to MKL and in certain cases it's 10x faster at matrix multiply.
>>>> There are a lot of layers of indirection here and you really want
>>>> to avoid data copying as much as possible.
>>>>
>>>> We could also consider swapping out BIDMat for Breeze, but that
>>>> would be a big project and if we can figure out how to get
>>>> breeze+cublas to comparable performance that would be a big win.
>>>>
>>>> On Thu, Feb 5, 2015 at 11:55 AM, Ulanov, Alexander <
>>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:
>>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>>> wrote:
>>>> Dear Spark developers,
>>>>
>>>> I am exploring how to make linear algebra operations faster within Spark.
>>>> One way of doing this is to use Scala Breeze library that is
>>>> bundled with Spark. For matrix operations, it employs Netlib-java
>>>> that has a Java wrapper for BLAS (basic linear algebra subprograms)
>>>> and LAPACK native binaries if they are available on the worker
>>>> node. It also has its own optimized Java implementation of BLAS. It
>>>> is worth mentioning, that native binaries provide better performance only for BLAS level 3, i.e.
>>>> matrix-matrix operations or general matrix multiplication (GEMM).
>>>> This is confirmed by GEMM test on Netlib-java page
>>>> https://github.com/fommil/netlib-java. I also confirmed it with my
>>>> experiments with training of artificial neural network
>>>> https://github.com/apache/spark/pull/1290#issuecomment-70313952.
>>>> However, I would like to boost performance more.
>>>>
>>>> GPU is supposed to work fast with linear algebra and there is
>>>> Nvidia CUDA implementation of BLAS, called cublas. I have one Linux
>>>> server with Nvidia GPU and I was able to do the following. I linked
>>>> cublas (instead of cpu-based blas) with Netlib-java wrapper and put
>>>> it into Spark, so Breeze/Netlib is using it. Then I did some
>>>> performance measurements with regards to artificial neural network
>>>> batch learning in Spark MLlib that involves matrix-matrix
>>>> multiplications. It turns out that for matrices of size less than
>>>> ~1000x780 GPU cublas has the same speed as CPU blas. Cublas becomes
>>>> slower for bigger matrices. It worth mentioning that it is was not a test for ONLY multiplication since there are other operations involved.
>>>> One of the reasons for slowdown might be the overhead of copying
>>>> the matrices from computer memory to graphic card memory and back.
>>>>
>>>> So, few questions:
>>>> 1) Do these results with CUDA make sense?
>>>> 2) If the pr"
"
Prabhu Joseph <prabhujose.gates@gmail.com>,Thu"," 4 Feb 2016 21:50:26 +0530""","Re: Spark job does not perform well when some RDD in memory and some
 on Disk",Alonso Isidoro Roman <alonsoir@gmail.com>,"If spark.locality.wait is 0, then there are two performance issues:

   1. Task Scheduler won't wait to schedule the tasks as DATA_LOCAL, will
launch it immediately on some node even if it is less local. The
probability of tasks running as less local will be higher
and affect the overall Job Performance.
      2. In case of Executor having not enough heap memory, some tasks
which has RDD on cache and some other has on hadoop, and if
spark.locality.wait is 0, all the tasks will starts parallel and since the
Executor Process is both Memory and IO intensive, the GC will be high and
tasks will be slower.















"
Antonio Piccolboni <antonio@piccolboni.info>,"Thu, 04 Feb 2016 16:31:27 +0000",Re: Path to resource added with SQL: ADD FILE,=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@questtec.nl>,"Hi Herman,
thanks for your reply, I used an absolute path to add the file. I use a
relative path only to access it in the UDF. I am not sure what absolute
path I should use in the UDF, if any. The documentation
<https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Cli#LanguageManualCli-HiveResources>
I am referring to is Beeline for Hive's, I hope it's the relevant bit.
Therein I read

add FILE /tmp/tt.py;

select from networks a MAP a.networkid USING 'python tt.py' as nn
where a.ds = '2009-01-04' limit 10;

So it's just referring to the file by basename, which I think is
equivalent to ""./tt.py"". I tried to follow this example closely, just
using R instead of python and the name of the file is hard-coded, not
an argument. If what you mean is that I should provide the same
absolute path in the ADD FILE and in the SELECT, then I will try that.
Thanks


Antonio



w
I
le
"
Max Grossman <jmg3@rice.edu>,"Thu, 4 Feb 2016 10:53:08 -0600",Re: Using CUDA within Spark / boosting linear algebra,Allen Zhang <allenzhang010@126.com>,"Allen,

Currently it only supports OpenCL because the code generator weâ€™ve extended targeted OpenCL. Thereâ€™s no technical reason that CUDA couldnâ€™t be supported if people would be interested in that, but it would require a rewrite of some of the code generator as well as some ifdefs in the runtime to allow us to compile with either OpenCL or CUDA support. There are actually a few components that support both OpenCL and CUDA for when theyâ€™ve been reused for other projects that did use CUDA, just not all of them.

Thanks,

Max

from Nvidia, not only OpenCL?
project for people to take a look at: https://github.com/agrippa/spark-swat <https://github.com/agrippa/spark-swat>
top of Spark that uses runtime code generation to convert user-written transformations into OpenCL kernels. SWATâ€™s lightweight runtime supports multi-GPU systems, managing each device and its memory automatically. You write your own Spark programs, and the runtime takes care of offloading your transformations to the GPUs in your system:
goal of this project is to have as few user-visible API changes as possible from what people have come to know and love in Spark. There are a number of fixed-function GPU libraries out there now, so we wanted to look instead at something that could be used to build new but still well-performing Spark apps.
system, so thereâ€™s a chance it wonâ€™t work out-of-the-box on some systems. With that said, it does have fairly comprehensive functional and code generation testing. If youâ€™re interested in trying it out and having trouble setting up, feel free to contact me directly. And of course, any questions or feedback from the community are always welcome.
important items is to effectively and easily enable highly-tuned libraries for GPU such as BIDMach.
it is not easy task to scaling BIDMach with current Spark. I expect that this talk would help us.
http://conferences.oreilly.com/strata/hadoop-big-data-ca/public/schedule/detail/47565 <http://conferences.oreilly.com/strata/hadoop-big-data-ca/public/schedule/detail/47565>
Tokyo
<mailto:alexander.ulanov@hpe.com>>
<mailto:dev@spark.apache.org>"" <dev@spark.apache.org <mailto:dev@spark.apache.org>>, Joseph Bradley <joseph@databricks.com <mailto:joseph@databricks.com>>
<mailto:canny@berkeley.edu>>, ""Evan R. Sparks"" <evan.sparks@gmail.com <mailto:evan.sparks@gmail.com>>, Xiangrui Meng <mengxr@gmail.com <mailto:mengxr@gmail.com>>, Sam Halliday <sam.halliday@gmail.com <mailto:sam.halliday@gmail.com>>
summarizes the costs for moving different data sizes with regards to matrices multiplication. These costs are paid for the convenience of using the standard BLAS API that Nvidia NVBLAS provides. The thing is that there are no code changes required (in Spark), one just needs to reference BLAS implementation with the system variable. Naturally, hardware-specific implementation will always be faster than default. The benchmark results show that fact by comparing jCuda (by means of BIDMat) and NVBLAS. However, it also shows that it worth using NVBLAS for large matrices because it can take advantage of several GPUs and it will be faster despite the copying overhead. That is also a known thing advertised by Nvidia.
is an issue, because one can use transposed matrices to fit the required format. I believe that is just a software preference.
comparisons with Sparkâ€™s implementation of logistic regression (that does not take advantage of GPU) and also with BIDMachâ€™s (that takes advantage of GPUs). It will give the users a better understanding of yourâ€™s implementation performance. Currently you compare it with Sparkâ€™s example logistic regression implementation that is supposed to be a reference for learning Spark rather than benchmarking its performance.
<mailto:ISHIZAKI@jp.ibm.com>] 
Alexander; Joseph Bradley
the
to
done
another part of memory sounds like a much bigger undertaking.
efficiently exploit GPUs in Spark.
GPU-friendly column format
<http://kiszk.github.io/spark-gpu/>addresses these two issues by supporting data partition caching in GPU device memory and by providing binary column storage for data partition. We really appreciate it if you would give us comments, suggestions, or feedback.
<mailto:alexander.ulanov@hpe.com>>
<mailto:sam.halliday@gmail.com>>, John Canny <canny@berkeley.edu <mailto:canny@berkeley.edu>>
<mailto:mengxr@gmail.com>>, ""dev@spark.apache.org <mailto:dev@spark.apache.org>"" <dev@spark.apache.org <mailto:dev@spark.apache.org>>, Joseph Bradley <joseph@databricks.com <mailto:joseph@databricks.com>>, ""Evan R. Sparks"" <evan.sparks@gmail.com <mailto:evan.sparks@gmail.com>>
hardware with 2x Nvidia Tesla K80 (physically 4x Tesla K40) and 2x modern Haswell CPU Intel E5-2650 v3 @ 2.30GHz.
experiment and approximated FLOPS.
other 2 sheets):
https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J5r7kwKSPkY/edit?usp=sharing <https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J5r7kwKSPkY/edit?usp=sharing>
<https://github.com/avulanov/scala-blas>
<mailto:sam.halliday@gmail.com>] 
<mailto:dev@spark.apache.org>; Joseph Bradley; Evan R. Sparks; Ulanov, Alexander
in industry,  although your personal experience may be different. 
Dense BLAS are not very important for most machine learning workloads: at least for non-image workloads in industry (and for image processing you would probably want a deep learning/SGD solution with convolution kernels). e.g. it was only relevant for 1/7 of our recent benchmarks, which should be a reasonable sample. What really matters is sparse BLAS performance. BIDMat is still an order of magnitude faster there. Those kernels are only in BIDMat, since NVIDIAs sparse BLAS dont perform well on power-law data. 
determined by the slowest kernel, not the fastest. If the goal is to get closer to BIDMach's performance on typical problems, you need to make sure that every kernel goes at comparable speed. So the real question is how much faster MLLib routines do on a complete problem with/without GPU acceleration. For BIDMach, its close to a factor of 10. But that required running entirely on the GPU, and making sure every kernel is close to its limit.
end-to-end benchmarks. 
performance from breeze/netlib-java - meaning there's no compelling performance reason to switch out our current linear algebra library (at least as far as this benchmark is concerned). 
use the right BLAS library will get us most of the way there. Or, would it make sense to finally ship openblas compiled for some common platforms (64-bit linux, windows, mac) directly with Spark - hopefully eliminating the jblas warnings once and for all for most users? (Licensing is BSD) Or am I missing something?
double-checked them. It turns that nvblas did not do multiplication due to parameter NVBLAS_TILE_DIM from ""nvblas.conf"" and returned zero matrix. My previously posted results with nvblas are matrices copying only. The default NVBLAS_TILE_DIM==2048 is too big for my graphic card/matrix size. I handpicked other values that worked. As a result, netlib+nvblas is on par with BIDMat-cuda. As promised, I am going to post a how-to for nvblas configuration.
https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J5r7kwKSPkY/edit?usp=sharing <https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J5r7kwKSPkY/edit?usp=sharing>
Meng; Joseph Bradley; Evan R. Sparks; jfcanny
exceptional performance for big matrices with Double, faster than BIDMat-cuda with Float. But for smaller matrices, if you will copy them to/from GPU, OpenBlas or MKL might be a better choice. This correlates with original nvblas presentation on GPU conf 2013 (slide 21): http://on-demand.gputechconf.com/supercomputing/2013/presentation/SC3108-New-Features-CUDA%206%20-GPU-Acceleration.pdf <http://on-demand.gputechconf.com/supercomputing/2013/presentation/SC3108-New-Features-CUDA%206%20-GPU-Acceleration.pdf>
https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J5r7kwKSPkY/edit?usp=sharing <https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J5r7kwKSPkY/edit?usp=sharing>
of different libraries. I just want to pick a library that does at best dense matrices multiplication for my task.
blas functions, at the same time netlib-java uses C cblas functions. So, one needs cblas shared library to use nvblas through netlib-java. Fedora does not have cblas (but Debian and Ubuntu have), so I needed to compile it. I could not use cblas from Atlas or Openblas because they link to their implementation and not to Fortran blas.
Meng; Joseph Bradley; Evan R. Sparks
functions should replace current blas functions calls after executing LD_PRELOAD as suggested in http://docs.nvidia.com/cuda/nvblas/#Usage <http://docs.nvidia.com/cuda/nvblas/#Usage>without any changes to netlib-java. It seems to work for simple Java example, but I cannot make it work with Spark. I run the following:
--driver-memory 4G In nvidia-smi I observe that Java is to use GPU:
+-----------------------------------------------------------------------------+
GPU Memory |
Usage      |
|=============================================================================|
  39MiB |
  39MiB |
+-----------------------------------------------------------------------------+
/tmp/jniloader8192964377009965483netlib-native_system-linux-x86_64.so
However, matrix multiplication does executes on CPU since I see 16% of CPU used and 0% of GPU used. I also checked different matrix sizes, from 100x100 to 12000x12000
<mailto:sam.halliday@gmail.com>]
Meng; Joseph Bradley; Evan R. Sparks
performance on various pieces of hardware...
<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com the comment that BIDMat 0.9.7 uses Float matrices in GPU (although I see the support of Double in the current source code), did the test with BIDMat and CPU Double matrices. BIDMat MKL is indeed on par with netlib MKL.
https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J5r7kwKSPkY/edit?usp=sharing <https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J5r7kwKSPkY/edit?usp=sharing>
<mailto:sam.halliday@gmail.com><mailto:sam.halliday@gmail.com <mailto:sam.halliday@gmail.com>>]
<mailto:dev@spark.apache.org><mailto:dev@spark.apache.org <mailto:dev@spark.apache.org>>
weeks?
https://skillsmatter.com/meetups/6987-apache-spark-living-the-post-mapreduce-world#community <https://skillsmatter.com/meetups/6987-apache-spark-living-the-post-mapreduce-world#community>
<mailto:mengxr@gmail.com><mailto:mengxr@gmail.com <mailto:mengxr@gmail.com>>> writes:
BLAS
<joseph@databricks.com <mailto:joseph@databricks.com><mailto:joseph@databricks.com JIRA:
<https://issues.apache.org/jira/browse/SPARK-6019>
<mailto:evan.sparks@gmail.com><mailto:evan.sparks@gmail.com <mailto:evan.sparks@gmail.com>>>
https://docs.google.com/spreadsheets/d/1aRm2IADRfXQV7G2vrcVh4StF50uZ <https://docs.google.com/spreadsheets/d/1aRm2IADRfXQV7G2vrcVh4StF50uZ>
magnitude
larger matrices.
this
<https://github.com/fommil/netlib-java>)
GPUs
GPU
could
CPU-based
mllib
<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com MKL==netlib-mkl==netlib-openblas-compiled>netlib-openblas-yum-repo=
https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx <https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx>
<mailto:dev@spark.apache.org><mailto:dev@spark.apache.org <mailto:dev@spark.apache.org>>
though
to
+-----------------------------------------------------------------------+
|
|445,0935211 |
columns with
<mailto:evan.sparks@gmail.com><mailto:evan.sparks@gmail.com <mailto:evan.sparks@gmail.com>>]
<mailto:dev@spark.apache.org><mailto:dev@spark.apache.org <mailto:dev@spark.apache.org>>
<https://issues.apache.org/jira/browse/SPARK-5705>)
while
<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com <mailto:alexander.ulanov@hp.com>><mailto:alexander.ulanov@hp.com <mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com statically
run
true, I
Nevertheless,
than
supposed to be on par with JNI overheads.
MKL,
<mailto:evan.sparks@gmail.com><mailto:evan.sparks@gmail.com <mailto:evan.sparks@gmail.com>><mailto:
<mailto:evan.sparks@gmail.com><mailto:evan.sparks@gmail.com <mailto:evan.sparks@gmail.com>>>]
<mailto:dev@spark.apache.org><mailto:dev@spark.apache.org <mailto:dev@spark.apache.org>><mailto:dev@spark <mailto:dev@spark>.
<mailto:dev@spark.apache.org>>>
comes
particular
ATLAS),
builds
here.
and
<https://github.com/shivaram/matrix-bench>
up by netlib-java.
<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com <mailto:alexander.ulanov@hp.com>><mailto:alexander.ulanov@hp.com <mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com to
-Dcom.github.fommil.netlib.BLAS=com.github.fommil.netlib.F2jBLAS,
understand how to force use a specific blas (not specific wrapper for blas).
suppose
<mailto:evan.sparks@gmail.com><mailto:evan.sparks@gmail.com <mailto:evan.sparks@gmail.com>><mailto:
<mailto:evan.sparks@gmail.com><mailto:evan.sparks@gmail.com <mailto:evan.sparks@gmail.com>>>]
<mailto:dev@spark.apache.org><mailto:dev@spark.apache.org <mailto:dev@spark.apache.org>><mailto:dev@spark <mailto:dev@spark>.
<mailto:dev@spark.apache.org>>>
have it).
BLAS
<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com <mailto:alexander.ulanov@hp.com>><mailto:alexander.ulanov@hp.com <mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com formatting):
+-----------------------------------------------------------------------+
1569,233228
Fedora
much
<mailto:joseph@databricks.com><mailto:joseph@databricks.com <mailto:joseph@databricks.com>><mailto:
<mailto:joseph@databricks.com><mailto:joseph@databricks.com <mailto:joseph@databricks.com>>>]
<mailto:dev@spark.apache.org><mailto:dev@spark.apache.org <mailto:dev@spark.apache.org>><mailto:dev@spark <mailto:dev@spark>.
<mailto:dev@spark.apache.org>>>
the
to
done
another part of memory sounds like a much bigger undertaking.
<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com <mailto:alexander.ulanov@hp.com>><mailto:alexander.ulanov@hp.com <mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com presentation by
with Spark MLlib.
Spark:
a
artificial
 test of
to machine learning.
<mailto:evan.sparks@gmail.com><mailto:evan.sparks@gmail.com <mailto:evan.sparks@gmail.com>><mailto:
<mailto:evan.sparks@gmail.com><mailto:evan.sparks@gmail.com <mailto:evan.sparks@gmail.com>>>]
<mailto:dev@spark.apache.org><mailto:dev@spark.apache.org <mailto:dev@spark.apache.org>><mailto:dev@spark <mailto:dev@spark>.
<mailto:dev@spark.apache.org>>>
than
to
I
GPU
in
to
such
what
and 4 GPUs.
of terabytes.
address
be
Java
<http://arxiv.org/abs/1409.5402>[2] -
<http://eecs.berkeley.edu/%7Ehzhao/papers/BD.pdf>
<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com <mailto:alexander.ulanov@hp.com>><mailto:alexander.ulanov@hp.com <mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com <mailto:alexander.ulanov@hp.com>>><mailto:
<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com <mailto:alexander.ulanov@hp.com>><mailto:alexander.ulanov@hp.com <mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com Do
If
BIDMach for optimization and learning?
<mailto:evan.sparks@gmail.com><mailto:evan.sparks@gmail.com <mailto:evan.sparks@gmail.com>><mailto:
<mailto:evan.sparks@gmail.com><mailto:evan.sparks@gmail.com <mailto:evan.sparks@gmail.com>>><mailto:evan.sparks@gmail.com <mailto:evan.sparks@gmail.com><mailto:evan.sparks@gmail.com <mailto:evan.sparks@gmail.com>><mailto:
<mailto:evan.sparks@gmail.com><mailto:evan.sparks@gmail.com <mailto:evan.sparks@gmail.com>>>>]
<mailto:dev@spark.apache.org><mailto:dev@spark.apache.org <mailto:dev@spark.apache.org>><mailto:dev@spark.apache.org <mailto:dev@spark.apache.org><mailto:dev@spark.apache.org <mailto:dev@spark.apache.org>>><mailto:
<mailto:dev@spark.apache.org><mailto:dev@spark.apache.org <mailto:dev@spark.apache.org>><mailto:dev@spark <mailto:dev@spark>.
<mailto:dev@spark.apache.org>>>>
<https://github.com/BIDData/BIDMat>) takes and comparing them to
on
faster at matrix multiply.
want
<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com <mailto:alexander.ulanov@hp.com>><mailto:alexander.ulanov@hp.com <mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com <mailto:alexander.ulanov@hp.com>>><mailto:
<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com <mailto:alexander.ulanov@hp.com>><mailto:alexander.ulanov@hp.com <mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com within Spark.
Netlib-java
subprograms)
It
performance only for BLAS level 3, i.e.
(GEMM).
<https://github.com/fommil/netlib-java>. I also confirmed it with my
<https://github.com/apache/spark/pull/1290#issuecomment-70313952>.
Linux
linked
put
network
than
becomes
not a test for ONLY multiplication since there are other operations involved.
back.
-------------------------------------------------------------------
<mailto:dev-unsubscribe@spark.apache.org><mailto:dev-unsubscribe@spark.apache.org <mailto:dev-unsubscribe@spark.apache.org>><mailto:
<mailto:dev-unsubscribe@spark.apache.org><mailto:dev-unsubscribe@spark.apach <mailto:dev-unsubscribe@spark.apach>
<mailto:dev-unsubscribe@spark.apac><mailto:dev-unsubscribe@sp <mailto:dev-unsubscribe@sp>
<http://he.org/>>
<mailto:dev-unsubscribe@spark.apache.org><mailto:dev-unsubscribe@spa <mailto:dev-unsubscribe@spa>
commands, e-mail:
<mailto:dev-help@spark.apache.org><mailto:dev-help@spark.apache.org <mailto:dev-help@spark.apache.org>><mailto:
<mailto:dev-help@spark.apache.org><mailto:dev-help@spark.apache.org <mailto:dev-help@spark.apache.org>>><mailto:dev-help@spark.apache.org <mailto:dev-help@spark.apache.org><mailto:dev-help@spark.apache.org <mailto:dev-help@spark.apache.org>><mailto:
<mailto:dev-help@spark.apache.org><mailto:dev-help@spark.apache.org <mailto:dev-help@spark.apache.org>>>>

"
Reynold Xin <rxin@databricks.com>,"Thu, 4 Feb 2016 10:08:31 -0800",Re: Interested in Contributing to Spark as GSoC 2016,Tao Lin <nblintao@gmail.com>,"I will email you offline.


'd
.
l
re
e
ore
he
 I
g
so
ng
ow
I
ontributing-to-Spark-as-GSoC-2016-tp16211.html
"
Charles Wright <charliewright@live.ca>,"Thu, 4 Feb 2016 18:03:45 -0500",Building Spark with Custom Hadoop Version,dev@spark.apache.org,"Hello,

I have made some modifications to the YARN source code that I want to 
test with Spark, how do I do this? I know that I need to include my 
custom hadoop jar as a dependency but I don't know how to do this as I 
am not very familiar with maven.

Any help is appreciated.

Thanks,
Charles.

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Thu, 4 Feb 2016 15:11:53 -0800",Re: Building Spark with Custom Hadoop Version,Charles Wright <charliewright@live.ca>,"Assuming your change is based on hadoop-2 branch, you can use 'mvn install'
command which would put artifacts under 2.8.0-SNAPSHOT subdir in your local
maven repo.

Here is an example:
~/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.8.0-SNAPSHOT

Then you can use the following command to build Spark:

-Pyarn -Phadoop-2.4 -Dhadoop.version=2.8.0-SNAPSHOT

FYI


"
zzc <441586683@qq.com>,"Thu, 4 Feb 2016 19:03:18 -0700 (MST)",Re: Scala 2.11 default build,dev@spark.apache.org,"Hi Prashant Sharma, i saw that there already were some pr submitted by
Luciano Resende:
https://github.com/apache/spark/pull/11075
https://github.com/apache/spark/pull/11074
https://github.com/apache/spark/pull/11076
https://github.com/apache/spark/pull/11077



--

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Fri, 05 Feb 2016 05:04:34 +0000",Re: Scala 2.11 default build,"zzc <441586683@qq.com>, dev@spark.apache.org","Many of these changes are not needed - please see how the profiles work.
Please also open 1 PR for one logical change and not 4.


"
Kai Jiang <jiangkai@gmail.com>,"Thu, 4 Feb 2016 22:53:57 -0800",[GSoC] Interested in GSoC 2016 ideas,dev@spark.apache.org,"Hello All,

I am Kai Jiang, a master student majoring in Computer Science. Machine
Learning and Distributed
System are my interests. Due to that, I've been contributing to Spark
codebase since last year. My
Pull Requests are related to MLlib, PySpark and SQL.(
https://github.com/apache/spark/pulls/vectorijk)

This year, I really want to extend my future contribution with Spark into a
GSoC project. Although the
list of GSoC organizations this year hasn't been announced yet, it is
highly possible that Apache
Software Foundation would be accepted based on organization list before.
Thus, I was wondering if
there are some specific ideas, issues or suggestions regarding MLlib, SQL
or others could be
gathered into a project. Meanwhile, I also noticed that Spark 2.0 would be
a big version in the near
future. After looking into the MLlib 2.0 Roadmap
<https://issues.apache.org/jira/browse/SPARK-12626>, I found there are many
issues I am interested in (i.e
Python/SparkR API for ML, PMML export, etc.). If community has other ideas,
I am very willing to
work on some issues before GSoC and get started with something new during
GSoC.

Looking forward to hearing from you!


Best,
Kai.
github.com/vectorijk
"
Pete Robbins <robbinspg@gmail.com>,"Fri, 5 Feb 2016 09:09:08 +0000","Re: SparkOscope: Enabling Spark Optimization through Cross-stack
 Monitoring and Visualization",Yiannis Gkoufas <johngouf85@gmail.com>,"Yiannis,

I'm interested in what you've done here as I was looking for ways to allow
the Spark UI to display custom metrics in a pluggable way without having to
modify the Spark source code. It would be good to see if we could have
modify your code to add extension points into the UI so we could configure
sources of the additional metrics. So for instance rather than creating
events from your HDFS files I would like to have a module that is pulling
in system/jvm metrics that are in eg Elasticsearch.

Do any of the Spark committers have any thoughts on this?

Cheers,



"
Steve Loughran <stevel@hortonworks.com>,"Fri, 5 Feb 2016 10:09:25 +0000",Re: Building Spark with Custom Hadoop Version,,"
l' command which would put artifacts under 2.8.0-SNAPSHOT subdir in your local maven repo.

Better to choose the hadoop-2.6 profile, e.g.

mvn test -Pyarn,hadoop-2.6 -Dhadoop.version=2.7.1  -pl yarn -Dtest=m  -DwildcardSuites=org.apache.spark.deploy.yarn.YarnClusterSuite

(the -Dtest= assignment skips all java tests)

if you are playing with -SNAPSHOT sourcess

(a) rebuild them every morning
(b) never do a test run that spans midnight

---------------------------------------------------------------------


"
Steve Loughran <stevel@hortonworks.com>,"Fri, 5 Feb 2016 10:18:05 +0000",Re: Building Spark with Custom Hadoop Version,"""dev@spark.apache.org"" <dev@spark.apache.org>","
l' command which would put artifacts under 2.8.0-SNAPSHOT subdir in your local maven repo.


+ generally, unless you want to run all the hadoop tests, set the  -DskipTests on the mvn commands. The HDFS ones take a while and can use up all your file handles.

mvn install -DskipTests

here's the aliases I use


export MAVEN_OPTS=""-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m -Xms256m -Djava.awt.headless=true""
alias mi=""mvn install -DskipTests""
alias mci=""mvn clean install -DskipTests""
alias mvt=""mvn test""
alias mvct=""mvn clean test""
alias mvp=""mvn package -DskipTests""
alias mvcp=""mvn clean package -DskipTests""
alias mvnsite=""mvn site:site -Dmaven.javadoc.skip=true -DskipTests""
alias mvndep=""mvn dependency:tree -Dverbose""


mvndep > target/dependencies.txt is my command of choice to start working out where some random dependency is coming in from

---------------------------------------------------------------------


"
Rachana Srivastava <Rachana.Srivastava@markmonitor.com>,"Fri, 5 Feb 2016 17:38:18 +0000","Spark process failing to receive data from the Kafka queue in
 yarn-client mode.","""user@spark.apache.org"" <user@spark.apache.org>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","I am trying to run following code using yarn-client mode in but getting slow readprocessor error mentioned below but the code works just fine in the local mode.  Any pointer is really appreciated.

Line of code to receive data from the Kafka Queue:
JavaPairReceiverInputDStream<String, String> messages =  KafkaUtils.createStream(jssc, String.class, String.class, StringDecoder.class, StringDecoder.class, kafkaParams, kafkaTopicMap, StorageLevel.MEMORY_ONLY());

JavaDStream<String> lines = messages.map(new Function<Tuple2<String, String>, String>() {
      public String call(Tuple2<String, String> tuple2) {
                              LOG.info("" &&&&&&&&&&&&&&&&&&&& Input json stream data  "" +  tuple2._2);
        return tuple2._2();
      }
    });


Error Details:
016-02-05 11:44:00 WARN DFSClient:975 - Slow ReadProcessor read fields took 30
011ms (threshold=30000ms); ack: seqno: 1960 reply: 0 reply: 0 reply: 0 downstrea
mAckTimeNanos: 1227280, targets: [DatanodeInfoWithStorage[10.0.0.245:50010,DS-a5
5d9212-3771-4936-bbe7-02035e7de148,DISK], DatanodeInfoWithStorage[10.0.0.243:500
10,DS-231b9915-c2e2-4392-b075-8a52ba1820ac,DISK], DatanodeInfoWithStorage[10.0.0
.244:50010,DS-6b8b5814-7dd7-4315-847c-b73bd375af0e,DISK]]
2016-02-05 11:44:00 INFO BlockManager:59 - Removing RDD 1954
2016-02-05 11:44:00 INFO MapPartitionsRDD:59 - Removing RDD 1955 from persisten
"
Matt Cheah <mcheah@palantir.com>,"Fri, 5 Feb 2016 21:49:14 +0000",Preserving partitioning with dataframe select,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi everyone,

When using raw RDDs, it is possible to have a map() operation indicate that the partitioning for the RDD would be preserved by the map operation. This makes it easier to reduce the overhead of shuffles by ensuring that RDDs are co-partitioned when they are joined.

When I'm using Data Frames, I'm pre-partitioning the data frame by using DataFrame.partitionBy($""X""), but I will invoke a select statement after the partitioning before joining that dataframe with another. Roughly speaking, I'm doing something like this pseudo-code:

partitionedDataFrame = dataFrame.partitionBy(""$X"")
groupedDataFrame = partitionedDataFrame.groupBy($""X"").agg(aggregations)
// Rename ""X"" to ""Y"" to make sure columns are unique
groupedDataFrameRenamed = groupedDataFrame.withColumnRenamed(""X"", ""Y"")
// Roughly speaking, join on ""X == Y"" to get the aggregation results onto every row
joinedDataFrame = partitionedDataFrame.join(groupedDataFrame)

However the renaming of the columns maps to a select statement, and to my knowledge, selecting the columns is throwing off the partitioning which results in shuffle both the partitionedDataFrame and the groupedDataFrame.

I have the following questions given this example:

1) Is pre-partitioning the Data Frame effective? In other words, does the physical planner recognize when underlying RDDs are co-partitioned and compute more efficient joins by reducing the amount of data that is shuffled?
2) If the planner takes advantage of co-partitioning, is the renaming of the columns invalidating the partitioning of the grouped Data Frame? When I look at the planner's conversion from logical.Project to the physical plan, I only see it invoking child.mapPartitions without specifying the preservesPartitioning flag.

Thanks,

-Matt Cheah
"
Andrew Holway <andrew.holway@otternetworks.de>,"Sat, 6 Feb 2016 20:22:30 +0100",Fwd: Writing to jdbc database from SparkR (1.5.2),dev@spark.apache.org,"Hi,

I have a thread on user@spark.apache.org but I think this might require
developer attention.

I'm reading data from a database: This is working well.

database.foo.eu-west-1.rds.amazonaws.com:3306?user=user&password=pass
<http://database.foo.eu-west-1.rds.amazonaws.com:3306/?user=user&password=pass>
"")

When I try and write something back to the DB I see this following error:

database.foo.eu-west-1.rds.amazonaws.com:3306?user=user&password=pass"",
dbtable=""db.table"", mode=""append"")


16/02/06 19:05:43 ERROR RBackendHandler: save on 2 failed

Error in invokeJava(isStatic = FALSE, objId$id, methodName, ...) :

  java.lang.RuntimeException:
org.apache.spark.sql.execution.datasources.jdbc.DefaultSource does not
allow create table as select.

at scala.sys.package$.error(package.scala:27)

at
org.apache.spark.sql.execution.datasources.ResolvedDataSource$.apply(ResolvedDataSource.scala:200)

at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:146)

at org.apache.spark.sql.DataFrame.save(DataFrame.scala:1855)

at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

at java.lang.reflect.Method.invoke(Method.java:497)

at
org.apache.spark.api.r.RBackendHandler.handleMethodCall(RBackendHandler.scala:132)

at
org.apache.spark.api.r.RBackendHandler.channelRead0(RBackendHandler.scala:79)

at
org.apache.spark.api.r.RBackendHandler.channelRead0(RBackendHandler.scala:38)

at io.netty.channel.SimpleChannelIn


Any ideas on a workaround?


Thanks,


Andrew
"
Felix Cheung <felixcheung_m@hotmail.com>,"Sun, 7 Feb 2016 01:54:21 +0000",Re: Fwd: Writing to jdbc database from SparkR (1.5.2),"Andrew Holway <andrew.holway@otternetworks.de>, <dev@spark.apache.org>","Unfortunately I couldn't find a simple workaround. It seems to be an issue with DataFrameWriter.save() that does not work with jdbc source/format
For instance, this does not work in Scala eitherdf1.write.format(""jdbc"").mode(""overwrite"").option(""url"", ""jdbc:mysql://something.rds.amazonaws.com:3306?user=user&password=password"").option(""dbtable"", ""table"").save() Â  Â  Â  Â  Â  Â 
For Spark 1.5.x, it seems the best option would be to write a JVM wrapper and call it from R.

    _____________________________
From: Andrew Holway <andrew.holway@otternetworks.de>
Sent: Saturday, February 6, 2016 11:22 AM
Subject: Fwd: Writing to jdbc database from SparkR (1.5.2)
To:  <dev@spark.apache.org>


       Hi,       
          I have a thread on     user@spark.apache.org but I think this might require developer attention.    
         
             I'm reading data from a database: This is working well.     
     
                                             

oo.eu-west-1.rds.amazonaws.com:3306?user=user&password=pass"")                            
                          When I try and write something back to the DB I see this following error:                         
                      

tabase.foo.eu-west-1.rds.amazonaws.com:3306?user=user&password=pass"", dbtable=""db.table"", mode=""append"")       


        

16/02/06 19:05:43 ERROR RBackendHandler: save on 2 failed        

Error in invokeJava(isStatic = FALSE, objId$id, methodName, ...) :Â  

Â  java.lang.RuntimeException: org.apache.spark.sql.execution.datasources.jdbc.DefaultSource does not allow create table as select. 

 at scala.sys.package$.error(package.scala:27) 

 at org.apache.spark.sql.execution.datasources.ResolvedDataSource$.apply(ResolvedDataSource.scala:200) 

 at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:146) 

 at org.apache.spark.sql.DataFrame.save(DataFrame.scala:1855) 

 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 

 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 

 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 

 at java.lang.reflect.Method.invoke(Method.java:497) 

 at org.apache.spark.api.r.RBackendHandler.handleMethodCall(RBackendHandler.scala:132) 

 at org.apache.spark.api.r.RBackendHandler.channelRead0(RBackendHandler.scala:79) 

 at org.apache.spark.api.r.RBackendHandler.channelRead0(RBackendHandler.scala:38) 

 at io.netty.channel.SimpleChannelIn




Any ideas on a workaround?




Thanks,




Andrew                      


  "
Renyi Xiong <renyixiong0@gmail.com>,"Sat, 6 Feb 2016 18:27:38 -0800",pyspark worker concurrency,dev@spark.apache.org,"Hi,

is it a good idea to have 2 threads in pyspark worker? -  main thread
responsible for receive and send data over socket while the other thread is
calling user functions to process data?

since CPU is idle (?) during network I/O, this should improve concurrency
quite a bit.

can expert answer the question? what are the pros and cons here?

thanks,
Renyi.
"
"""Sun, Rui"" <rui.sun@intel.com>","Sun, 7 Feb 2016 07:57:12 +0000",RE: Fwd: Writing to jdbc database from SparkR (1.5.2),"Felix Cheung <felixcheung_m@hotmail.com>, Andrew Holway
	<andrew.holway@otternetworks.de>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","DataFrameWrite.jdbc() does not work?

From: Felix Cheung [mailto:felixcheung_m@hotmail.com]
Sent: Sunday, February 7, 2016 9:54 AM
To: Andrew Holway <andrew.holway@otternetworks.de>; dev@spark.apache.org
Subject: Re: Fwd: Writing to jdbc database from SparkR (1.5.2)

Unfortunately I couldn't find a simple workaround. It seems to be an issue with DataFrameWriter.save() that does not work with jdbc source/format

For instance, this does not work in Scala either
df1.write.format(""jdbc"").mode(""overwrite"").option(""url"", ""jdbc:mysql://something.rds.amazonaws.com<http://something.rds.amazonaws.com>:3306?user=user&password=password"").option(""dbtable"", ""table"").save()

For Spark 1.5.x, it seems the best option would be to write a JVM wrapper and call it from R.

_____________________________
From: Andrew Holway <andrew.holway@otternetworks.de<mailto:andrew.holway@otternetworks.de>>
Sent: Saturday, February 6, 2016 11:22 AM
Subject: Fwd: Writing to jdbc database from SparkR (1.5.2)
To: <dev@spark.apache.org<mailto:dev@spark.apache.org>>

Hi,

I have a thread on user@spark.apache.org<mailto:user@spark.apache.org> but I think this might require developer attention.

I'm reading data from a database: This is working well.

> df <- read.df(sqlContext, source=""jdbc"", url=""jdbc:mysql://database.foo.eu-west-1.rds.amazonaws.com:3306?user=user&password=pass<http://database.foo.eu-west-1.rds.amazonaws.com:3306/?user=user&password=pass>"")

When I try and write something back to the DB I see this following error:


> write.df(fooframe, path=""NULL"", source=""jdbc"", url=""jdbc:mysql://database.foo.eu-west-1.rds.amazonaws.com:3306?user=user&password=pass<http://database.foo.eu-west-1.rds.amazonaws.com:3306?user=user&password=pass>"", dbtable=""db.table"", mode=""append"")



16/02/06 19:05:43 ERROR RBackendHandler: save on 2 failed

Error in invokeJava(isStatic = FALSE, objId$id, methodName, ...) :

  java.lang.RuntimeException: org.apache.spark.sql.execution.datasources.jdbc.DefaultSource does not allow create table as select.

at scala.sys.package$.error(package.scala:27)

at org.apache.spark.sql.execution.datasources.ResolvedDataSource$.apply(ResolvedDataSource.scala:200)

at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:146)

at org.apache.spark.sql.DataFrame.save(DataFrame.scala:1855)

at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

at java.lang.reflect.Method.invoke(Method.java:497)

at org.apache.spark.api.r.RBackendHandler.handleMethodCall(RBackendHandler.scala:132)

at org.apache.spark.api.r.RBackendHandler.channelRead0(RBackendHandler.scala:79)

at org.apache.spark.api.r.RBackendHandler.channelRead0(RBackendHandler.scala:38)

at io.netty.channel.SimpleChannelIn



Any ideas on a workaround?



Thanks,



Andrew

"
Felix Cheung <felixcheung_m@hotmail.com>,"Sun, 7 Feb 2016 12:47:44 +0000",RE: Fwd: Writing to jdbc database from SparkR (1.5.2),"""Sun, Rui"" <rui.sun@intel.com>, Andrew Holway
	<andrew.holway@otternetworks.de>, <dev@spark.apache.org>","That does but it's a bit hard to call from R since it is not exposed.











DataFrameWrite.jdbc() does not work?

From: Felix Cheung [mailto:felixcheung_m@hotmail.com]
Sent: Sunday, February 7, 2016 9:54 AM
To: Andrew Holway <andrew.holway@otternetworks.de>; dev@spark.apache.org
Subject: Re: Fwd: Writing to jdbc database from SparkR (1.5.2)

Unfortunately I couldn't find a simple workaround. It seems to be an issue with DataFrameWriter.save() that does not work with jdbc source/format

For instance, this does not work in Scala either
df1.write.format(""jdbc"").mode(""overwrite"").option(""url"", ""jdbc:mysql://something.rds.amazonaws.com<http://something.rds.amazonaws.com>:3306?user=user&password=password"").option(""dbtable"", ""table"").save()

For Spark 1.5.x, it seems the best option would be to write a JVM wrapper and call it from R.

_____________________________
From: Andrew Holway <andrew.holway@otternetworks.de<mailto:andrew.holway@otternetworks.de>>
Sent: Saturday, February 6, 2016 11:22 AM
Subject: Fwd: Writing to jdbc database from SparkR (1.5.2)
To: <dev@spark.apache.org<mailto:dev@spark.apache.org>>

Hi,

I have a thread on user@spark.apache.org<mailto:user@spark.apache.org> but I think this might require developer attention.

I'm reading data from a database: This is working well.

se.foo.eu-west-1.rds.amazonaws.com:3306?user=user&password=pass<http://database.foo.eu-west-1.rds.amazonaws.com:3306/?user=user&password=pass>"")

When I try and write something back to the DB I see this following error:


l://database.foo.eu-west-1.rds.amazonaws.com:3306?user=user&password=pass<http://database.foo.eu-west-1.rds.amazonaws.com:3306?user=user&password=pass>"", dbtable=""db.table"", mode=""append"")



16/02/06 19:05:43 ERROR RBackendHandler: save on 2 failed

Error in invokeJava(isStatic = FALSE, objId$id, methodName, ...) :

  java.lang.RuntimeException: org.apache.spark.sql.execution.datasources.jdbc.DefaultSource does not allow create table as select.

at scala.sys.package$.error(package.scala:27)

at org.apache.spark.sql.execution.datasources.ResolvedDataSource$.apply(ResolvedDataSource.scala:200)

at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:146)

at org.apache.spark.sql.DataFrame.save(DataFrame.scala:1855)

at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

at java.lang.reflect.Method.invoke(Method.java:497)

at org.apache.spark.api.r.RBackendHandler.handleMethodCall(RBackendHandler.scala:132)

at org.apache.spark.api.r.RBackendHandler.channelRead0(RBackendHandler.scala:79)

at org.apache.spark.api.r.RBackendHandler.channelRead0(RBackendHandler.scala:38)

at io.netty.channel.SimpleChannelIn



Any ideas on a workaround?



Thanks,



Andrew

"
Felix Cheung <felixcheung_m@hotmail.com>,"Sun, 7 Feb 2016 12:50:07 +0000",RE: Fwd: Writing to jdbc database from SparkR (1.5.2),"""Sun, Rui"" <rui.sun@intel.com>, Andrew Holway
	<andrew.holway@otternetworks.de>, <dev@spark.apache.org>","I mean not exposed from the SparkR API.
Calling it from R without a SparkR API would require either a serializer change or a JVM wrapper function.








That does but it's a bit hard to call from R since it is not exposed.











DataFrameWrite.jdbc() does not work?

From: Felix Cheung [mailto:felixcheung_m@hotmail.com]
Sent: Sunday, February 7, 2016 9:54 AM
To: Andrew Holway <andrew.holway@otternetworks.de>; dev@spark.apache.org
Subject: Re: Fwd: Writing to jdbc database from SparkR (1.5.2)

Unfortunately I couldn't find a simple workaround. It seems to be an issue with DataFrameWriter.save() that does not work with jdbc source/format

For instance, this does not work in Scala either
df1.write.format(""jdbc"").mode(""overwrite"").option(""url"", ""jdbc:mysql://something.rds.amazonaws.com<http://something.rds.amazonaws.com>:3306?user=user&password=password"").option(""dbtable"", ""table"").save()

For Spark 1.5.x, it seems the best option would be to write a JVM wrapper and call it from R.

_____________________________
From: Andrew Holway <andrew.holway@otternetworks.de<mailto:andrew.holway@otternetworks.de>>
Sent: Saturday, February 6, 2016 11:22 AM
Subject: Fwd: Writing to jdbc database from SparkR (1.5.2)
To: <dev@spark.apache.org<mailto:dev@spark.apache.org>>

Hi,

I have a thread on user@spark.apache.org<mailto:user@spark.apache.org> but I think this might require developer attention.

I'm reading data from a database: This is working well.

se.foo.eu-west-1.rds.amazonaws.com:3306?user=user&password=pass<http://database.foo.eu-west-1.rds.amazonaws.com:3306/?user=user&password=pass>"")

When I try and write something back to the DB I see this following error:


l://database.foo.eu-west-1.rds.amazonaws.com:3306?user=user&password=pass<http://database.foo.eu-west-1.rds.amazonaws.com:3306?user=user&password=pass>"", dbtable=""db.table"", mode=""append"")



16/02/06 19:05:43 ERROR RBackendHandler: save on 2 failed

Error in invokeJava(isStatic = FALSE, objId$id, methodName, ...) :

  java.lang.RuntimeException: org.apache.spark.sql.execution.datasources.jdbc.DefaultSource does not allow create table as select.

at scala.sys.package$.error(package.scala:27)

at org.apache.spark.sql.execution.datasources.ResolvedDataSource$.apply(ResolvedDataSource.scala:200)

at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:146)

at org.apache.spark.sql.DataFrame.save(DataFrame.scala:1855)

at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

at java.lang.reflect.Method.invoke(Method.java:497)

at org.apache.spark.api.r.RBackendHandler.handleMethodCall(RBackendHandler.scala:132)

at org.apache.spark.api.r.RBackendHandler.channelRead0(RBackendHandler.scala:79)

at org.apache.spark.api.r.RBackendHandler.channelRead0(RBackendHandler.scala:38)

at io.netty.channel.SimpleChannelIn



Any ideas on a workaround?



Thanks,



Andrew

"
"""Sun, Rui"" <rui.sun@intel.com>","Sun, 7 Feb 2016 13:19:17 +0000",RE: Fwd: Writing to jdbc database from SparkR (1.5.2),"Felix Cheung <felixcheung_m@hotmail.com>, Andrew Holway
	<andrew.holway@otternetworks.de>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","This should be solved by your pending PR https://github.com/apache/spark/pull/10480, right?

From: Felix Cheung [mailto:felixcheung_m@hotmail.com]
Sent: Sunday, February 7, 2016 8:50 PM
To: Sun, Rui <rui.sun@intel.com>; Andrew Holway <andrew.holway@otternetworks.de>; dev@spark.apache.org
Subject: RE: Fwd: Writing to jdbc database from SparkR (1.5.2)

I mean not exposed from the SparkR API.
Calling it from R without a SparkR API would require either a serializer change or a JVM wrapper function.

On Sun, Feb 7, 2016 at 4:47 AM -0800, ""Felix Cheung"" <felixcheung_m@hotmail.com<mailto:felixcheung_m@hotmail.com>> wrote:
That does but it's a bit hard to call from R since it is not exposed.



On Sat, Feb 6, 2016 at 11:57 Pel.com>> wrote:

DataFrameWrite.jdbc() does not work?



From: Felix Cheung [mailto:felixcheung_m@hotmail.com]
Sent: Sunday, February 7, 2016 9:54 AM
To: Andrew Holway <andrew.holway@otternetworks.de<mailto:andrew.holway@otternetworks.de>>; dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: Fwd: Writing to jdbc database from SparkR (1.5.2)



Unfortunately I couldn't find a simple workaround. It seems to be an issue with DataFrameWriter.save() that does not work with jdbc source/format



For instance, this does not work in Scala either

df1.write.format(""jdbc"").mode(""overwrite"").option(""url"", ""jdbc:mysql://something.rds.amazonaws.com<http://something.rds.amazonaws.com>:3306?user=user&password=password"").option(""dbtable"", ""table"").save()



For Spark 1.5.x, it seems the best option would be to write a JVM wrapper and call it from R.



_____________________________
From: Andrew Holway <andrew.holway@otternetworks.de<mailto:andrew.holway@otternetworks.de>>
Sent: Saturday, February 6, 2016 11:22 AM
Subject: Fwd: Writing to jdbc database from SparkR (1.5.2)
To: <dev@spark.apache.org<mailto:dev@spark.apache.org>>

Hi,



I have a thread on user@spark.apache.org<mailto:user@spark.apache.org> but I think this might require developer attention.



I'm reading data from a database: This is working well.

> df <- read.df(sqlContext, source=""jdbc"", url=""jdbc:mysql://database.foo.eu-west-1.rds.amazonaws.com:3306?user=user&password=pass<http://database.foo.eu-west-1.rds.amazonaws.com:3306/?user=user&password=pass>"")



When I try and write something back to the DB I see this following error:



> write.df(fooframe, path=""NULL"", source=""jdbc"", url=""jdbc:mysql://database.foo.eu-west-1.rds.amazonaws.com:3306?user=user&password=pass<http://database.foo.eu-west-1.rds.amazonaws.com:3306?user=user&password=pass>"", dbtable=""db.table"", mode=""append"")



16/02/06 19:05:43 ERROR RBackendHandler: save on 2 failed

Error in invokeJava(isStatic = FALSE, objId$id, methodName, ...) :

  java.lang.RuntimeException: org.apache.spark.sql.execution.datasources.jdbc.DefaultSource does not allow create table as select.

at scala.sys.package$.error(package.scala:27)

at org.apache.spark.sql.execution.datasources.ResolvedDataSource$.apply(ResolvedDataSource.scala:200)

at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:146)

at org.apache.spark.sql.DataFrame.save(DataFrame.scala:1855)

at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

at java.lang.reflect.Method.invoke(Method.java:497)

at org.apache.spark.api.r.RBackendHandler.handleMethodCall(RBackendHandler.scala:132)

at org.apache.spark.api.r.RBackendHandler.channelRead0(RBackendHandler.scala:79)

at org.apache.spark.api.r.RBackendHandler.channelRead0(RBackendHandler.scala:38)

at io.netty.channel.SimpleChannelIn



Any ideas on a workaround?



Thanks,



Andrew


"
sim <sim@swoop.com>,"Sun, 7 Feb 2016 16:29:09 -0700 (MST)",Scala API: simplifying common patterns,dev@spark.apache.org,"The more Spark code I write, the more I hit the same use cases where the
Scala APIs feel a bit awkward. I'd love to understand if there are
historical reasons for these and whether there is opportunity + interest to
improve the APIs. Here are my top two:
1. registerTempTable() returns Unit
def cachedDF(path: String, tableName: String) = {  val df =
sqlContext.read.load(path).cache()  df.registerTempTable(tableName)  df}//
vs.def cachedDF(path: String, tableName: String) = 
sqlContext.read.load(path).cache().registerTempTable(tableName)
2. No toDF() implicit for creating a DataFrame from an RDD + schema
val schema: StructType = ...val rdd = sc.textFile(...)  .map(...) 
.aggregate(...)val df = sqlContext.createDataFrame(rdd, schema)// vs.val
schema: StructType = ...val df = sc.textFile(...)  .map(...) 
.aggregate(...)  .toDF(schema)
Have you encountered other examples where small, low-risk API tweaks could
make common use cases more consistent + simpler to code?
/Sim



--"
Reynold Xin <rxin@databricks.com>,"Sun, 7 Feb 2016 16:22:14 -0800",Re: Scala API: simplifying common patterns,sim <sim@swoop.com>,"Both of these make sense to add. Can you submit a pull request?



"
Felix Cheung <felixcheung_m@hotmail.com>,"Mon, 8 Feb 2016 01:26:56 +0000",RE: Fwd: Writing to jdbc database from SparkR (1.5.2),"""Sun, Rui"" <rui.sun@intel.com>, Andrew Holway
	<andrew.holway@otternetworks.de>, <dev@spark.apache.org>","Correct :)



    _____________________________
From: Sun, Rui <rui.sun@intel.com>
Sent: Sunday, February 7, 2016 5:19 AM
Subject: RE: Fwd: Writing to jdbc database from SparkR (1.5.2)
To:  <dev@spark.apache.org>, Felix Cheung <felixcheung_m@hotmail.com>, Andrew Holway <andrew.holway@otternetworks.de>


                     

This should be solved by your pending PR https://github.com/apache/spark/pull/10480, right?    

Â                

From: Felix Cheung [mailto:felixcheung_m@hotmail.com] 
 Sent: Sunday, February 7, 2016 8:50 PM
 To: Sun, Rui <rui.sun@intel.com>; Andrew Holway <andrew.holway@otternetworks.de>; dev@spark.apache.org
 Subject: RE: Fwd: Writing to jdbc database from SparkR (1.5.2)             

Â     

I mean not exposed from the SparkR API.
 Calling it from R without a SparkR API would require either a serializer change or a JVM wrapper function.
 
          


That does but it's a bit hard to call from R since it is not exposed.      

Â           


 
            

:                       

DataFrameWrite.jdbc() does not work?       

Â        

From: Felix Cheung [mailto:felixcheung_m@hotmail.com] 
 Sent: Sunday, February 7, 2016 9:54 AM
 To: Andrew Holway <andrew.holway@otternetworks.de>; dev@spark.apache.org
 Subject: Re: Fwd: Writing to jdbc database from SparkR (1.5.2)       

Â                         

Unfortunately I couldn't find a simple workaround. It seems to be an issue with DataFrameWriter.save() that does not work with jdbc source/format                         

Â                          

For instance, this does not work in Scala either                         

df1.write.format(""jdbc"").mode(""overwrite"").option(""url"", ""jdbc:mysql://something.rds.amazonaws.com:3306?user=user&password=password"").option(""dbtable"", ""table"").save() Â  Â  Â  Â  Â  Â                          

Â                          

For Spark 1.5.x, it seems the best option would be to write a JVM wrapper and call it from R.                         

Â                               

_____________________________
 From: Andrew Holway <andrew.holway@otternetworks.de>
 Sent: Saturday, February 6, 2016 11:22 AM
 Subject: Fwd: Writing to jdbc database from SparkR (1.5.2)
 To: <dev@spark.apache.org>                 

Hi,                    

Â                             

I have a thread on  user@spark.apache.org but I think this might require developer attention.                       

Â                                

I'm reading data from a database: This is working well.                                                                  

oo.eu-west-1.rds.amazonaws.com:3306?user=user&password=pass"")                                           

Â                                            

When I try and write something back to the DB I see this following error:                                            

Â                                         

tabase.foo.eu-west-1.rds.amazonaws.com:3306?user=user&password=pass"", dbtable=""db.table"", mode=""append"")             

Â              

16/02/06 19:05:43 ERROR RBackendHandler: save on 2 failed             

Error in invokeJava(isStatic = FALSE, objId$id, methodName, ...) :Â              

Â  java.lang.RuntimeException: org.apache.spark.sql.execution.datasources.jdbc.DefaultSource does not allow create table as select.             

at scala.sys.package$.error(package.scala:27)             

at org.apache.spark.sql.execution.datasources.ResolvedDataSource$.apply(ResolvedDataSource.scala:200)             

at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:146)             

at org.apache.spark.sql.DataFrame.save(DataFrame.scala:1855)             

at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)             

at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)             

at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)             

at java.lang.reflect.Method.invoke(Method.java:497)             

at org.apache.spark.api.r.RBackendHandler.handleMethodCall(RBackendHandler.scala:132)             

at org.apache.spark.api.r.RBackendHandler.channelRead0(RBackendHandler.scala:79)             

at org.apache.spark.api.r.RBackendHandler.channelRead0(RBackendHandler.scala:38)             

at io.netty.channel.SimpleChannelIn             

Â              

Any ideas on a workaround?             

Â              

Thanks,             

Â              

Andrew                                                          

Â                              


  "
sim <sim@swoop.com>,"Sun, 7 Feb 2016 19:14:09 -0700 (MST)",Re: Scala API: simplifying common patterns,dev@spark.apache.org,"Sure.



--

---------------------------------------------------------------------


"
sim <sim@swoop.com>,"Sun, 7 Feb 2016 20:05:06 -0700 (MST)",Re: Scala API: simplifying common patterns,dev@spark.apache.org,"Reynold, I just forked + built master and I'm getting lots of binary
compatibility errors when running the tests. 

https://gist.github.com/ssimeonov/69cb0b41750be7777776

Nothing in the dev tools section of the wiki on this. Any advice on how to
get green before I work on the PRs?

Thanks,
Sim



--

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Mon, 8 Feb 2016 12:55:19 +0900",Re: Scala API: simplifying common patterns,sim <sim@swoop.com>,"Not 100% sure what's going on, but you can try wiping your local ivy2 and
maven cache.





"
sim <sim@swoop.com>,"Sun, 7 Feb 2016 23:47:36 -0700 (MST)",Re: Scala API: simplifying common patterns,dev@spark.apache.org,"Same result with both caches cleared.



--

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Mon, 8 Feb 2016 16:02:33 +0900",Re: Scala API: simplifying common patterns,sim <sim@swoop.com>,"Yea I'm not sure what's going on either. You can just run the unit tests
through ""build/sbt sql/test"" without running mima.



"
Reynold Xin <rxin@databricks.com>,"Mon, 8 Feb 2016 16:11:58 +0900",Re: Preserving partitioning with dataframe select,Matt Cheah <mcheah@palantir.com>,"Matt,

Thanks for the email. Are you just asking whether it should work, or
reporting they don't work?

Internally, the way we track physical data distribution should make the
scenarios described work. If it doesn't, we should make them work.



"
sim <sim@swoop.com>,"Mon, 8 Feb 2016 00:51:19 -0700 (MST)",Re: Scala API: simplifying common patterns,dev@spark.apache.org,"24 test failures for sql/test:
https://gist.github.com/ssimeonov/89862967f87c5c497322



--

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Mon, 8 Feb 2016 18:28:41 +0900",Re: Scala API: simplifying common patterns,sim <sim@swoop.com>,"Can you create a pull request? It is difficult to know what's going on.



"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 8 Feb 2016 12:15:34 -0500",Welcoming two new committers,dev <dev@spark.apache.org>,"Hi all,

The PMC has recently added two new Spark committers -- Herman van Hovell and Wenchen Fan. Both have been heavily involved in Spark SQL and Tungsten, adding new features, optimizations and APIs. Please join me in welcoming Herman and Wenchen.

Matei
---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Mon, 8 Feb 2016 09:23:14 -0800",Re: Welcoming two new committers,Matei Zaharia <matei.zaharia@gmail.com>,"Congratulations, Herman and Wenchen.


"
shane knapp <sknapp@berkeley.edu>,"Mon, 8 Feb 2016 09:27:40 -0800","[build system] brief downtime, 8am PST thursday feb 10th","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","happy monday!

i will be bringing down jenkins and the workers thursday morning to
upgrade docker on all of the workers from 1.5.0-1 to 1.7.1-2.

as of december last year, docker 1.5 and older lost the ability to
pull from the docker hub.  since we're running centos 6.X on our
workers, and can't run the 3.X kernel, that limits our options to
docker 1.7.

this will allow us to close out https://github.com/apache/spark/pull/9893

i'll be sure to send updates as they happen.

shane

---------------------------------------------------------------------


"
Corey Nolet <cjnolet@gmail.com>,"Mon, 8 Feb 2016 12:35:25 -0500",Re: Welcoming two new committers,Ted Yu <yuzhihong@gmail.com>,"Congrats guys!


"
Xiao Li <gatorsmile@gmail.com>,"Mon, 8 Feb 2016 09:39:33 -0800",Re: Welcoming two new committers,Corey Nolet <cjnolet@gmail.com>,"Congratulations! Herman and Wenchen!  I am just so happy for you! You
absolutely deserve it!

2016-02-08 9:35 GMT-08:00 Corey Nolet <cjnolet@gmail.com>:

"
"""Dilip Biswal"" <dbiswal@us.ibm.com>","Mon, 8 Feb 2016 10:25:46 -0800",Re: Welcoming two new committers,Xiao Li <gatorsmile@gmail.com>,"Congratulations Wenchen and Herman !! 

Regards,
Dilip Biswal
Tel: 408-463-4980
dbiswal@us.ibm.com



From:   Xiao Li <gatorsmile@gmail.com>
To:     Corey Nolet <cjnolet@gmail.com>
Cc:     Ted Yu <yuzhihong@gmail.com>, Matei Zaharia 
<matei.zaharia@gmail.com>, dev <dev@spark.apache.org>
Date:   02/08/2016 09:39 AM
Subject:        Re: Welcoming two new committers



Congratulations! Herman and Wenchen!  I am just so happy for you! You 
absolutely deserve it!

2016-02-08 9:35 GMT-08:00 Corey Nolet <cjnolet@gmail.com>:
Congrats guys! 

Congratulations, Herman and Wenchen.

Hi all,

The PMC has recently added two new Spark committers -- Herman van Hovell 
and Wenchen Fan. Both have been heavily involved in Spark SQL and 
Tungsten, adding new features, optimizations and APIs. Please join me in 
welcoming Herman and Wenchen.

Matei
---------------------------------------------------------------------






"
Denny Lee <denny.g.lee@gmail.com>,"Mon, 08 Feb 2016 18:39:47 +0000",Re: Welcoming two new committers,"Dilip Biswal <dbiswal@us.ibm.com>, Xiao Li <gatorsmile@gmail.com>","Awesome - congratulations Herman and Wenchan!


"
Luciano Resende <luckbr1975@gmail.com>,"Mon, 8 Feb 2016 10:44:02 -0800",Re: Welcoming two new committers,Matei Zaharia <matei.zaharia@gmail.com>,"

Congratulations !!!

-- 
Luciano Resende
http://people.apache.org/~lresende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Mon, 8 Feb 2016 10:55:26 -0800",Re: Welcoming two new committers,"hvanhovell@questtec.nl, Wenchen Fan <wenchen@databricks.com>","Congrats!!! Herman and Wenchen!!!


"
Bhupendra Mishra <bhupendra.mishra@gmail.com>,"Tue, 9 Feb 2016 00:25:49 +0530",Re: Welcoming two new committers,Matei Zaharia <matei.zaharia@gmail.com>,"Congratulations to both. and welcome to group.


"
Andrew Or <andrew@databricks.com>,"Mon, 8 Feb 2016 10:59:17 -0800",Re: Welcoming two new committers,Bhupendra Mishra <bhupendra.mishra@gmail.com>,"Welcome!

2016-02-08 10:55 GMT-08:00 Bhupendra Mishra <bhupendra.mishra@gmail.com>:

"
Suresh Thalamati <suresh.thalamati@gmail.com>,"Mon, 8 Feb 2016 11:50:51 -0800",Re: Welcoming two new committers,Andrew Or <andrew@databricks.com>,"Congratulations Herman and Wenchen!


"
Amit Chavan <achavan1@gmail.com>,"Mon, 8 Feb 2016 15:05:40 -0500",Re: Welcoming two new committers,Suresh Thalamati <suresh.thalamati@gmail.com>,"Welcome.


"
Ram Sriharsha <sriharsha.ram@gmail.com>,"Mon, 8 Feb 2016 12:19:19 -0800",Re: Welcoming two new committers,Amit Chavan <achavan1@gmail.com>,"great job guys! congrats and welcome!




-- 
Ram Sriharsha
Architect, Spark and Data Science
Hortonworks, 2550 Great America Way, 2nd Floor
Santa Clara, CA 95054
Ph: 408-510-8635
email: harsha@apache.org

[image: https://www.linkedin.com/in/harsha340]
<https://www.linkedin.com/in/harsha340> <https://twitter.com/halfabrane>
<https://github.com/harsha2010/>
"
Joseph Bradley <joseph@databricks.com>,"Mon, 8 Feb 2016 13:58:13 -0800",Re: Welcoming two new committers,Ram Sriharsha <sriharsha.ram@gmail.com>,"Congrats & welcome!


"
Scott walent <scottwalent@gmail.com>,"Mon, 08 Feb 2016 22:12:34 +0000",Spark in Production - Use Cases,"""dev@spark.apache.org"" <dev@spark.apache.org>, ""user@spark.apache.org"" <user@spark.apache.org>","the highlights this year will focus on how Spark is being used across
businesses to solve both big and small data needs. Check out the full
agenda here: https://spark-summit.org/east-2016/schedule/

Use ""ApacheList"" for 30% off at registration.

We wanted to highlight a few talks, including keynotes from:
- Chris D'Agostino: Vice President Digital and US Card Servicing Technology
- Matei Zaharia:CTO and Co-founder from Databricks
- Seshu Adunuthula: Head of Analytics Infrastructure at eBay

The keynotes are just the start of the summit. The community submitted over
200 talks and we narrowed it down to 60 to be presented in NYC. Here is
just a sampling:
- Top 5 Mistakes When Writing Spark Applications from Cloudera
- Structuring Spark: DataFrames, Datasets, and Streaming from Databricks
- TopNotch: Systematically Quality Controlling Big Data from BlackRock
- Distributed Time Travel for Feature Generation by Netflix

This will be our only summit on the east coast this year, register today to
guarantee a seat! https://spark-summit.org/east-2016/
"
Renyi Xiong <renyixiong0@gmail.com>,"Mon, 8 Feb 2016 14:25:06 -0800",Re: pyspark worker concurrency,dev@spark.apache.org,"never mind,  I think pyspark is already doing async socket read / write,
but on scala side in PythonRDD.scala


"
Alexander Pivovarov <apivovarov@gmail.com>,"Mon, 8 Feb 2016 21:03:03 -0800",spark on yarn wastes one box (or 1 GB on each box) for am container,dev@spark.apache.org,"Lets say that yarn has 53GB memory available on each slave

spark.am container needs 896MB.  (512 + 384)

I see two options to configure spark:

1. configure spark executors to use 52GB and leave 1 GB on each box. So,
some box will also run am container. So, 1GB memory will not be used on all
slaves but one.

2. configure spark to use all 53GB and add additional 53GB box which will
run only am container. So, 52GB on this additional box will do nothing

I do not like both options. Is there a better way to configure yarn/spark?


Alex
"
Prabhu Joseph <prabhujose.gates@gmail.com>,"Tue, 9 Feb 2016 11:25:57 +0530","Re: Long running Spark job on YARN throws ""No AMRMToken""","user <user@spark.apache.org>, user@hadoop.apache.org, dev@spark.apache.org",#NAME?
Matt Cheah <mcheah@palantir.com>,"Tue, 9 Feb 2016 05:37:02 +0000",Re: Preserving partitioning with dataframe select,Reynold Xin <rxin@databricks.com>,"Interesting ­ I might be misinterpreting my Spark UI then, in terms of the
number of stages I¹m seeing in the job before and after I¹m doing the
pre-partitioning.

That said, I was mostly thinking about this when reading through the code.
In particular, under basicOperators.scala in org.apache.spark.sql.execution,
the Project gets compiled down to child.executor.mapPartitionsInternal
without passing the preservesPartitioning flag. Is this Projection being
moved around in the case that the optimizer wants to take advantage of
co-partitioning? Guidance on how to trace the planner¹s logic would be
appreciated!

-Matt Cheah

From:  Reynold Xin <rxin@databricks.com>
Date:  Sunday, February 7, 2016 at 11:11 PM
To:  Matt Cheah <mcheah@palantir.com>
Cc:  ""dev@spark.apache.org"" <dev@spark.apache.org>, Mingyu Kim
<mkim@palantir.com>
Subject:  Re: Preserving partitioning with dataframe select

Matt, 

Thanks for the email. Are you just asking whether it should work, or
reporting they don't work?

Internally, the way we track physical data distribution should make the
scenarios described work. If it doesn't, we should make them work.


at
s
are
he
, I'm
.
mpute
the
ook
only



"
Jonathan Kelly <jonathakamzn@gmail.com>,"Tue, 09 Feb 2016 06:49:11 +0000",Re: spark on yarn wastes one box (or 1 GB on each box) for am container,"Alexander Pivovarov <apivovarov@gmail.com>, dev@spark.apache.org","Alex,

That's a very good question that I've been trying to answer myself recently
too. Since you've mentioned before that you're using EMR, I assume you're
asking this because you've noticed this behavior on emr-4.3.0.

In this release, we made some changes to the maximizeResourceAllocation
(which you may or may not be using, but either way this issue is present),
including the accidental inclusion of somewhat of a bug that makes it not
reserve any space for the AM, which ultimately results in one of the nodes
being utilized only by the AM and not an executor.

However, as you point out, the only viable fix seems to be to reserve
enough memory for the AM on *every single node*, which in some cases might
actually be worse than wasting a lot of memory on a single node.

So yeah, I also don't like either option. Is this just the price you pay
for running on YARN?


~ Jonathan

"
Sean Owen <sowen@cloudera.com>,"Tue, 9 Feb 2016 06:53:21 +0000",Re: spark on yarn wastes one box (or 1 GB on each box) for am container,Jonathan Kelly <jonathakamzn@gmail.com>,"Typically YARN is there because you're mediating resource requests
from things besides Spark, so yeah using every bit of the cluster is a
little bit of a corner case. There's not a good answer if all your
nodes are the same size.

I think you can let YARN over-commit RAM though, and allocate more
memory than it actually has. It may be beneficial to let them all
think they have an extra GB, and let one node running the AM
technically be overcommitted, a state which won't hurt at all unless
you're really really tight on memory, in which case something might
get killed.


---------------------------------------------------------------------


"
Alexander Pivovarov <apivovarov@gmail.com>,"Tue, 9 Feb 2016 00:35:53 -0800",Re: spark on yarn wastes one box (or 1 GB on each box) for am container,Sean Owen <sowen@cloudera.com>,"If I add additional small box to the cluster can I configure yarn to select
small box to run am container?



"
Sean Owen <sowen@cloudera.com>,"Tue, 9 Feb 2016 08:39:54 +0000",Re: spark on yarn wastes one box (or 1 GB on each box) for am container,Alexander Pivovarov <apivovarov@gmail.com>,"If it's too small to run an executor, I'd think it would be chosen for
the AM as the only way to satisfy the request.


---------------------------------------------------------------------


"
Steve Loughran <stevel@hortonworks.com>,"Tue, 9 Feb 2016 11:26:16 +0000","Re: Long running Spark job on YARN throws ""No AMRMToken""",Spark dev list <dev@spark.apache.org>,"

+ Spark-Dev

Hi All,

    A long running Spark job on YARN throws below exception after running for few days.

yarn.ApplicationMaster: Reporter thread fails 1 time(s) in a row. org.apache.hadoop.yarn.exceptions.YarnException: No AMRMToken found for user prabhu at org.apache.hadoop.yarn.ipc.RPCUtil.getRemoteException(RPCUtil.java:45)

Do any of the below renew the AMRMToken and solve the issue

1. yarn-resourcemanager.delegation.token.max-lifetime increase from 7 days

2. Configuring Proxy user:

<property> <name>hadoop.proxyuser.yarn.hosts</name> <value>*</value> </property>
<property> <name>hadoop.proxyuser.yarn.groups</name> <value>*</value> </property>

wouldnt do that: security issues


3. Can Spark-1.4.0 handle with fix https://issues.apache.org/jira/browse/SPARK-5342

    spark.yarn.credentials.file



I'll say ""maybe"" there

How to renew the AMRMToken for a long running job on YARN?




AMRM token renewal should be automatic in AM; Yarn sends a message to the AM (actually an allocate() response with no containers but a new token at the tail of the message.

i don't see any logging in the Hadoopp code there (AMRMClientImpl); filed YARN-4682 to add a log statement

if someone other than me were to supply a patch to that JIRA to add a log statement *by the end of the day* I'll review it and get it in to Hadoop 2.8

-Steve
"
Steve Loughran <stevel@hortonworks.com>,"Tue, 9 Feb 2016 11:29:11 +0000","Re: spark on yarn wastes one box (or 1 GB on each box) for am
 container",dev <dev@spark.apache.org>,"
> On 9 Feb 2016, at 06:53, Sean Owen <sowen@cloudera.com> wrote:
> 
> 
> I think you can let YARN over-commit RAM though, and allocate more
> memory than it actually has. It may be beneficial to let them all
> think they have an extra GB, and let one node running the AM
> technically be overcommitted, a state which won't hurt at all unless
> you're really really tight on memory, in which case something might
> get killed.


from my test VMs

      <property>
        <description>Whether physical memory limits will be enforced for
          containers.
        </description>
        <name>yarn.nodemanager.pmem-check-enabled</name>
        <value>false</value>
      </property>

      <property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
      </property>


it does mean that a container can swap massively, hurting the performance of all containers around it as IO bandwidth gets soaked up â€”which is why the checks are on for shared clusters. If it's dedicated, you can overcommit"
praveen S <mylogin13@gmail.com>,"Tue, 9 Feb 2016 19:18:37 +0530",Re: spark on yarn wastes one box (or 1 GB on each box) for am container,Steve Loughran <stevel@hortonworks.com>,"How about running in client mode, so that the client from which it is run
becomes the driver.

Regards,
Praveen

 is why
mit
"
Steve Loughran <stevel@hortonworks.com>,"Tue, 9 Feb 2016 16:44:59 +0000","Re: Long running Spark job on YARN throws ""No AMRMToken""",Spark dev list <dev@spark.apache.org>,"



+ Spark-Dev

Hi All,

    A long running Spark job on YARN throws below exception after running for few days.

yarn.ApplicationMaster: Reporter thread fails 1 time(s) in a row. org.apache.hadoop.yarn.exceptions.YarnException: No AMRMToken found for user prabhu at org.apache.hadoop.yarn.ipc.RPCUtil.getRemoteException(RPCUtil.java:45)

Do any of the below renew the AMRMToken and solve the issue

1. yarn-resourcemanager.delegation.token.max-lifetime increase from 7 days

2. Configuring Proxy user:

<property> <name>hadoop.proxyuser.yarn.hosts</name> <value>*</value> </property>
<property> <name>hadoop.proxyuser.yarn.groups</name> <value>*</value> </property>

wouldnt do that: security issues


3. Can Spark-1.4.0 handle with fix https://issues.apache.org/jira/browse/SPARK-5342

    spark.yarn.credentials.file



I'll say ""maybe"" there

uprated to a no, having looked at the code more


How to renew the AMRMToken for a long running job on YARN?




AMRM token renewal should be automatic in AM; Yarn sends a message to the AM (actually an allocate() response with no containers but a new token at the tail of the message.

i don't see any logging in the Hadoopp code there (AMRMClientImpl); filed YARN-4682 to add a log statement

if someone other than me were to supply a patch to that JIRA to add a log statement *by the end of the day* I'll review it and get it in to Hadoop 2.8


like I said: I'll get this in to hadoop-2.8 if someone is timely with the diff

"
Jonathan Kelly <jonathakamzn@gmail.com>,"Tue, 09 Feb 2016 16:47:01 +0000",Re: spark on yarn wastes one box (or 1 GB on each box) for am container,"Alexander Pivovarov <apivovarov@gmail.com>, Sean Owen <sowen@cloudera.com>","Sean, I'm not sure if that's actually the case, since the AM would be
allocated before the executors are even requested (by the driver through
the AM), right? This must at least be the case with dynamicAllocation
enabled, but I would expect that it's true regardless.

However, Alex, yes, this would be possible on EMR if you use small CORE
instances and larger TASK instances. EMR is configured to run AMs only on
CORE instances, so if you don't need much HDFS space (HDFS is stored only
on CORE instances, not TASK instances), this might be a good option for
you. Note though that you would have to set spark.executor.memory yourself
though rather than using maximizeResourceAllocation because
maximizeResourceAllocation currently only considers the size of the CORE
instances when determining spark.{driver,executor}.memory.

~ Jonathan


"
Jonathan Kelly <jonathakamzn@gmail.com>,"Tue, 09 Feb 2016 16:51:52 +0000",Re: spark on yarn wastes one box (or 1 GB on each box) for am container,"Steve Loughran <stevel@hortonworks.com>, praveen S <mylogin13@gmail.com>","Praveen,

You mean cluster mode, right? That would still in a sense cause one box to
be ""wasted"", but at least it would be used a bit more to its full
potential, especially if you set spark.driver.memory to higher than its 1g
default. Also, cluster mode is not an option for some applications, such as
the spark-shell, pyspark shell, or Zeppelin.

~ Jonathan


e
h is why
mmit
"
Alexander Pivovarov <apivovarov@gmail.com>,"Tue, 9 Feb 2016 09:51:51 -0800",Re: spark on yarn wastes one box (or 1 GB on each box) for am container,Sean Owen <sowen@cloudera.com>,"Am container starts first and yarn selects random computer to run it.

Is it possible to configure yarn so that it selects small computer for am
container.

"
Marcelo Vanzin <vanzin@cloudera.com>,"Tue, 9 Feb 2016 09:54:36 -0800",Re: spark on yarn wastes one box (or 1 GB on each box) for am container,Alexander Pivovarov <apivovarov@gmail.com>,"You should be able to use spark.yarn.am.nodeLabelExpression if your
version of YARN supports node labels (and you've added a label to the
node where you want the AM to run).




-- 
Marcelo

---------------------------------------------------------------------


"
Alexander Pivovarov <apivovarov@gmail.com>,"Tue, 9 Feb 2016 09:56:13 -0800",Re: spark on yarn wastes one box (or 1 GB on each box) for am container,Marcelo Vanzin <vanzin@cloudera.com>,"I use hadoop 2.7.1

"
Alexander Pivovarov <apivovarov@gmail.com>,"Tue, 9 Feb 2016 10:41:56 -0800",Re: spark on yarn wastes one box (or 1 GB on each box) for am container,Marcelo Vanzin <vanzin@cloudera.com>,"I decided to do YARN over-commit and add 896
to yarn.nodemanager.resource.memory-mb
it was 54,272
now I set it to 54,272+896 = 55,168

Kelly, can I ask you couple questions
1. it is possible to add yarn label to particular instance group boxes on
EMR?
2. in addition to maximizeResourceAllocation it would be nice if we have
executorsPerBox setting in EMR.
I have a case when I need to run 2 or 4 executors on r3.2xlarge


"
Alexander Pivovarov <apivovarov@gmail.com>,"Tue, 9 Feb 2016 10:42:43 -0800",Re: spark on yarn wastes one box (or 1 GB on each box) for am container,Marcelo Vanzin <vanzin@cloudera.com>,"I mean Jonathan


"
Diwakar Dhanuskodi <diwakar.dhanuskodi@gmail.com>,"Wed, 10 Feb 2016 00:26:16 +0530","RE: spark on yarn wastes one box (or 1 GB on each box) for am
 container","Alexander Pivovarov <apivovarov@gmail.com>, dev@spark.apache.org","

Are you using Â Yarn Â  to Â run Â spark jobs only Â ?. Are you Â configuring Â spark Â properties in Â spark-submit parameters? . If Â soÂ 
did Â you Â try Â with Â --no - of - executors x*53 (where Â x is Â no of Â nodes ) --spark executor-memory 1g --spark-driver-memory 1g.

You Â might Â see Â yarn Â allocating Â resources Â to Â all executors except one because Â driver draws that Â memory . Trade off Â is Â that Â if Â there were Â not Â much Â data to Â process then Â many Â executors Â may Â run empty wasting Â up Â resources . In Â that Â case probably Â you Â might Â need Â to Â settle Â down Â with Â dynamic allocation Â enabled .



Sent from Samsung Mobile.
Sent from Samsung Mobile.

<div>-------- Original message --------</div><div>From: Alexander Pivovarov <apivovarov@gmail.com> </div><div>Date:09/02/2016  10:33  (GMT+05:30) </div><div>To: dev@spark.apache.org </div><div>Cc:  </div><div>Subject: spark on yarn wastes one box (or 1 GB on each box) for am container </div><div>
</div>Lets say that yarn has 53GB memory available on each slave

spark.am container needs 896MB.  (512 + 384)

I see two options to configure spark:

1. configure spark executors to use 52GB and leave 1 GB on each box. So, some box will also run am container. So, 1GB memory will not be used on all slaves but one.

2. configure spark to use all 53GB and add additional 53GB box which will run only am container. So, 52GB on this additional box will do nothing

I do not like both options. Is there a better way to configure yarn/spark?


Alex"
Michael Armbrust <michael@databricks.com>,"Tue, 9 Feb 2016 11:12:31 -0800",Re: Preserving partitioning with dataframe select,Matt Cheah <mcheah@palantir.com>,"RDD level partitioning information is not used to decide when to shuffle
for queries planned using Catalyst (since we have better information about
distribution from the query plan itself).  Instead you should be looking at
the logic in EnsureRequirements
<https://github.com/apache/spark/blob/06f0df6df204c4722ff8a6bf909abaa32a715c41/sql/core/src/main/scala/org/apache/spark/sql/execution/Exchange.scala#L272>
.

We don't yet reason about equivalence classes for attributes when deciding
if a given partitioning is valid, but #10844
<https://github.com/apache/spark/pull/10844> is a start at building that
infrastructure.
"
Hari Shreedharan <hshreedharan@cloudera.com>,"Tue, 9 Feb 2016 11:29:31 -0800","Re: Long running Spark job on YARN throws ""No AMRMToken""",Steve Loughran <stevel@hortonworks.com>,"The credentials file approach (using keytab for spark apps) will only
update HDFS tokens. YARN's AMRM tokens should be taken care of by YARN
internally.

Steve - correct me if I am wrong here: If the AMRM tokens are disappearing
it might be a YARN bug (does the AMRM token have a 7 day limit as well? I
thought that was only for HDFS).


Thanks,
Hari


"
rakeshchalasani <vnit.rakesh@gmail.com>,"Tue, 9 Feb 2016 12:55:23 -0700 (MST)",Error aliasing an array column.,dev@spark.apache.org,"Hi All:

I am getting an ""UnsupportedOperationException"" when trying to alias an
array column. The issue seems to be at ""CreateArray"" expression -> dataType,
which checks for nullability of its children, while aliasing is creating a
PrettyAttribute that does not implement nullability.  

Below is an example to reproduce it.



this throws the following exception:




--

---------------------------------------------------------------------


"
Jonathan Kelly <jonathakamzn@gmail.com>,"Tue, 09 Feb 2016 20:16:59 +0000",Re: spark on yarn wastes one box (or 1 GB on each box) for am container,"Marcelo Vanzin <vanzin@cloudera.com>, Alexander Pivovarov <apivovarov@gmail.com>","Interesting, I was not aware of spark.yarn.am.nodeLabelExpression.

We do use YARN labels on EMR; each node is automatically labeled with its
type (MASTER, CORE, or TASK). And we do
set yarn.app.mapreduce.am.labels=CORE in yarn-site.xml, but we do not set
spark.yarn.am.nodeLabelExpression.

Does Spark somehow not actually honor this? It seems weird that Spark would
have its own similar-sounding property (spark.yarn.am.nodeLabelExpression).
If spark.yarn.am.nodeLabelExpression is used
and yarn.app.mapreduce.am.labels ignored, I could be wrong about Spark AMs
only running on CORE instances in EMR.

I'm guessing though that spark.yarn.am.nodeLabelExpression would simply
override yarn.app.mapreduce.am.labels, so yarn.app.mapreduce.am.labels
would be treated as a default when it is set and
spark.yarn.am.nodeLabelExpression is not. Is that correct?

In short, Alex, you should not need to set any of the label-related
properties yourself if you do what I suggested regarding using small CORE
instances and large TASK instances. But if you want to do something
different, it would also be possible to add a TASK instance group with
small nodes and configured with some new label. Then you could set
spark.yarn.am.nodeLabelExpression to that label.

Thanks, Marcelo, for pointing out spark.yarn.am.nodeLabelExpression!

~ Jonathan


"
Alexander Pivovarov <apivovarov@gmail.com>,"Tue, 9 Feb 2016 12:36:29 -0800",Re: spark on yarn wastes one box (or 1 GB on each box) for am container,Jonathan Kelly <jonathakamzn@gmail.com>,"Thanks Jonathan

Actually I'd like to use maximizeResourceAllocation.

Ideally for me would be to add new instance group having single small box
labelled as AM
I'm not sure ""aws emr create-cluster"" supports setting custom LABELS , the
only settings awailable are:

InstanceCount=1,BidPrice=0.5,Name=sparkAM,InstanceGroupType=TASK,InstanceType=m3.xlarge


How can I specify yarn label AM for that box?




"
Ted Yu <yuzhihong@gmail.com>,"Tue, 9 Feb 2016 13:23:24 -0800",Re: Error aliasing an array column.,rakeshchalasani <vnit.rakesh@gmail.com>,"Do you mind pastebin'ning code snippet and exception one more time - I
couldn't see them in your original email.

Which Spark release are you using ?


"
Marcelo Vanzin <vanzin@cloudera.com>,"Tue, 9 Feb 2016 13:30:21 -0800",Re: spark on yarn wastes one box (or 1 GB on each box) for am container,Jonathan Kelly <jonathakamzn@gmail.com>,"
That sounds very mapreduce-specific, so I doubt Spark (or anything
non-MR) would honor it.

-- 
Marcelo

---------------------------------------------------------------------


"
Rakesh Chalasani <vnit.rakesh@gmail.com>,"Tue, 09 Feb 2016 21:38:48 +0000",Re: Error aliasing an array column.,Ted Yu <yuzhihong@gmail.com>,"Sorry, didn't realize the mail didn't show the code. Using Spark release
1.6.0

Below is an example to reproduce it.

import org.apache.spark.sql.SQLContext
val sqlContext = new SQLContext(sparkContext)
import sqlContext.implicits._
import org.apache.spark.sql.functions

case class Test(a:Int, b:Int)
val data = sparkContext.parallelize(Array.range(0, 10).map(x => Test(x,
x+1)))
val df = data.toDF()
val arrayCol = functions.array(df(""a""), df(""b"")).as(""arrayCol"")

this throws the following exception:
ava.lang.UnsupportedOperationException
        at
org.apache.spark.sql.catalyst.expressions.PrettyAttribute.nullable(namedExpressions.scala:289)
        at
org.apache.spark.sql.catalyst.expressions.CreateArray$$anonfun$dataType$3.apply(complexTypeCreator.scala:40)
        at
org.apache.spark.sql.catalyst.expressions.CreateArray$$anonfun$dataType$3.apply(complexTypeCreator.scala:40)
        at
scala.collection.IndexedSeqOptimized$$anonfun$exists$1.apply(IndexedSeqOptimized.scala:40)
        at
scala.collection.IndexedSeqOptimized$$anonfun$exists$1.apply(IndexedSeqOptimized.scala:40)
        at
scala.collection.IndexedSeqOptimized$class.segmentLength(IndexedSeqOptimized.scala:189)
        at
scala.collection.mutable.ArrayBuffer.segmentLength(ArrayBuffer.scala:47)
        at
scala.collection.GenSeqLike$class.prefixLength(GenSeqLike.scala:92)
        at scala.collection.AbstractSeq.prefixLength(Seq.scala:40)
        at
scala.collection.IndexedSeqOptimized$class.exists(IndexedSeqOptimized.scala:40)
        at
scala.collection.mutable.ArrayBuffer.exists(ArrayBuffer.scala:47)
        at
org.apache.spark.sql.catalyst.expressions.CreateArray.dataType(complexTypeCreator.scala:40)
        at
org.apache.spark.sql.catalyst.expressions.Alias.dataType(namedExpressions.scala:136)
        at
org.apache.spark.sql.catalyst.expressions.NamedExpression$class.typeSuffix(namedExpressions.scala:84)
        at
org.apache.spark.sql.catalyst.expressions.Alias.typeSuffix(namedExpressions.scala:120)
        at
org.apache.spark.sql.catalyst.expressions.Alias.toString(namedExpressions.scala:155)
        at
org.apache.spark.sql.catalyst.expressions.Expression.prettyString(Expression.scala:207)
        at org.apache.spark.sql.Column.toString(Column.scala:138)
        at java.lang.String.valueOf(String.java:2994)
        at scala.runtime.ScalaRunTime$.stringOf(ScalaRunTime.scala:331)
        at scala.runtime.ScalaRunTime$.replStringOf(ScalaRunTime.scala:337)
        at .<init>(<console>:20)
        at .<clinit>(<console>)
        at $print(<console>)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:497)
        at
org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)
        at
org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)
        at
org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)
        at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)
        at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)


"
Ted Yu <yuzhihong@gmail.com>,"Tue, 9 Feb 2016 13:52:45 -0800",Re: Error aliasing an array column.,Rakesh Chalasani <vnit.rakesh@gmail.com>,"How about changing the last line to:

scala> val df2 = df.select(functions.array(df(""a""),
df(""b"")).alias(""arrayCol""))
df2: org.apache.spark.sql.DataFrame = [arrayCol: array<int>]

scala> df2.show()
+--------+
|arrayCol|
+--------+
|  [0, 1]|
|  [1, 2]|
|  [2, 3]|
|  [3, 4]|
|  [4, 5]|
|  [5, 6]|
|  [6, 7]|
|  [7, 8]|
|  [8, 9]|
| [9, 10]|
+--------+

FYI


"
Rakesh Chalasani <vnit.rakesh@gmail.com>,"Tue, 09 Feb 2016 22:27:41 +0000",Re: Error aliasing an array column.,Ted Yu <yuzhihong@gmail.com>,"Do you mean using ""alias"" instead of ""as""? Unfortunately, that didn't help


still throws the error.

Surprisingly, doing the same thing inside a select works,

+--------+
|arrayCol|
+--------+
|  [0, 1]|
|  [1, 2]|
|  [2, 3]|
|  [3, 4]|
|  [4, 5]|
|  [5, 6]|
|  [6, 7]|
|  [7, 8]|
|  [8, 9]|
| [9, 10]|
+--------+




"
Ted Yu <yuzhihong@gmail.com>,"Tue, 9 Feb 2016 14:29:05 -0800",Re: Error aliasing an array column.,Rakesh Chalasani <vnit.rakesh@gmail.com>,"What's your plan of using the arrayCol ?
It would be part of some query, right ?


"
Michael Armbrust <michael@databricks.com>,"Tue, 9 Feb 2016 14:33:20 -0800",Re: Error aliasing an array column.,Rakesh Chalasani <vnit.rakesh@gmail.com>,"That looks like a bug in toString for columns.  Can you open a JIRA?


"
Rakesh Chalasani <vnit.rakesh@gmail.com>,"Tue, 09 Feb 2016 22:39:32 +0000",Re: Error aliasing an array column.,Ted Yu <yuzhihong@gmail.com>,"We are trying to dynamically create the query, with columns coming from
different places. We can over come this with a few more lines of code, but
it would be nice for us pass on the `alias` along (given that we can do so
for all the rest of the frame operations.)

Created JIRA here https://issues.apache.org/jira/browse/SPARK-13253

Thanks for the help.



"
Jonathan Kelly <jonathakamzn@gmail.com>,"Tue, 09 Feb 2016 23:28:36 +0000",Re: spark on yarn wastes one box (or 1 GB on each box) for am container,Marcelo Vanzin <vanzin@cloudera.com>,"Oh, sheesh, how silly of me. I copied and pasted that setting name without
even noticing the ""mapreduce"" in it. Yes, I guess that would mean that
Spark AMs are probably running even on TASK instances currently, which is
OK but not consistent with what we do for MapReduce. I'll make sure we
set spark.yarn.am.nodeLabelExpression appropriately in the next EMR release.

~ Jonathan


"
Alexander Pivovarov <apivovarov@gmail.com>,"Tue, 9 Feb 2016 15:37:53 -0800",Re: spark on yarn wastes one box (or 1 GB on each box) for am container,Jonathan Kelly <jonathakamzn@gmail.com>,"Can you add an ability to set custom yarn labels instead/in addition to?

"
Jonathan Kelly <jonathakamzn@gmail.com>,"Wed, 10 Feb 2016 00:02:11 +0000",Re: spark on yarn wastes one box (or 1 GB on each box) for am container,Alexander Pivovarov <apivovarov@gmail.com>,"You can set custom per-instance-group configurations (e.g.,
[""classification"":""yarn-site"",properties:{""yarn.nodemanager.labels"":""SPARKAM""}])
using the Configurations parameter of
http://docs.aws.amazon.com/ElasticMapReduce/latest/API/API_InstanceGroupConfig.html.
Unfortunately, it's not currently possible to specify per-instance-group
configurations via the CLI though, only cluster wide configurations.

~ Jonathan


"
Alexander Pivovarov <apivovarov@gmail.com>,"Tue, 9 Feb 2016 17:29:12 -0800",Re: spark on yarn wastes one box (or 1 GB on each box) for am container,Jonathan Kelly <jonathakamzn@gmail.com>,"Great! Thank you!


"
Rishitesh Mishra <rishi80.mishra@gmail.com>,"Wed, 10 Feb 2016 10:07:31 +0530",map-side-combine in Spark SQL,dev@spark.apache.org,"Can anybody confirm, whether ANY operator in Spark SQL uses
map-side-combine ? If not, is it safe to assume SortShuffleManager will
always use Serialized sorting in case of queries from Spark SQL ?
"
Li Ming Tsai <mailinglist@ltsai.com>,"Wed, 10 Feb 2016 05:59:45 +0000","Re: Kmeans++ using 1 core only Was: Slowness in Kmeans calculating
 fastSquaredDistance ","""dev@spark.apache.org"" <dev@spark.apache.org>","Forwarding to the dev list, hoping someone can chime in.


@mengxr?


________________________________
From: Li Ming Tsai <mailinglist@ltsai.com>
Sent: Wednesday, February 10, 2016 12:43 PM
To: user@spark.apache.org
Subject: Re: Slowness in Kmeans calculating fastSquaredDistance


Hi,


It looks like Kmeans++ is slow (SPARK-3424<https://issues.apache.org/jira/browse/SPARK-3424>) in the initialisation phase and is local to driver using 1 core only.


If I use random, the job completed in 1.5mins compared to 1hr+.


Should I move this to the dev list?


Regards,

Liming


________________________________
From: Li Ming Tsai <mailinglist@ltsai.com>
Sent: Sunday, February 7, 2016 10:03 AM
To: user@spark.apache.org
Subject: Re: Slowness in Kmeans calculating fastSquaredDistance


Hi,


I did more investigation and found out that BLAS.scala is calling the native reference architecture (f2jblas) for level 1 routines.


I even patched it to use nativeBlas.ddot but it has no material impact.


https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/linalg/BLAS.scala#L126


private def dot(x: DenseVector, y: DenseVector): Double = {

    val n = x.size

    f2jBLAS.ddot(n, x.values, 1, y.values, 1)

  }


Maybe Xiangrui can comment on this?



________________________________
From: Li Ming Tsai <mailinglist@ltsai.com>
Sent: Friday, February 5, 2016 10:56 AM
To: user@spark.apache.org
Subject: Slowness in Kmeans calculating fastSquaredDistance


Hi,


I'm using INTEL MKL on Spark 1.6.0 which I built myself with the -Pnetlib-lgpl flag.


I am using spark local[4] mode and I run it like this:
# export LD_LIBRARY_PATH=/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64
# bin/spark-shell ...

I have also added the following to /opt/intel/mkl/lib/intel64:
lrwxrwxrwx 1 root root        12 Feb  1 09:18 libblas.so -> libmkl_rt.so
lrwxrwxrwx 1 root root        12 Feb  1 09:18 libblas.so.3 -> libmkl_rt.so
lrwxrwxrwx 1 root root        12 Feb  1 09:18 liblapack.so -> libmkl_rt.so
lrwxrwxrwx 1 root root        12 Feb  1 09:18 liblapack.so.3 -> libmkl_rt.so


I believe (???) that I'm using Intel MKL because the warnings went away:

16/02/01 07:49:38 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS

16/02/01 07:49:38 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS

After collectAsMap, there is no progress but I can observe that only 1 CPU is being utilised with the following stack trace:

""ForkJoinPool-3-worker-7"" #130 daemon prio=5 os_prio=0 tid=0x00007fbf30ab6000 nid=0xbdc runnable [0x00007fbf12205000]

   java.lang.Thread.State: RUNNABLE

        at com.github.fommil.netlib.F2jBLAS.ddot(F2jBLAS.java:71)

        at org.apache.spark.mllib.linalg.BLAS$.dot(BLAS.scala:128)

        at org.apache.spark.mllib.linalg.BLAS$.dot(BLAS.scala:111)

        at org.apache.spark.mllib.util.MLUtils$.fastSquaredDistance(MLUtils.scala:349)

        at org.apache.spark.mllib.clustering.KMeans$.fastSquaredDistance(KMeans.scala:587)

        at org.apache.spark.mllib.clustering.KMeans$$anonfun$findClosest$1.apply(KMeans.scala:561)

        at org.apache.spark.mllib.clustering.KMeans$$anonfun$findClosest$1.apply(KMeans.scala:555)


This last few steps takes more than half of the total time for a 1Mx100 dataset.


The code is just:

val clusters = KMeans.train(parsedData, 1000, 1)


Shouldn't it utilising all the cores for the dot product? Is this a misconfiguration?


Thanks!


"
Prabhu Joseph <prabhujose.gates@gmail.com>,"Wed, 10 Feb 2016 13:47:16 +0530",Re: Spark Job on YARN accessing Hbase Table,"user <user@spark.apache.org>, dev@spark.apache.org","+ Spark-Dev

For a Spark job on YARN accessing hbase table, added all hbase client jars
into spark.yarn.dist.files, NodeManager when launching container i.e
executor, does localization and brings all hbase-client jars into executor
CWD, but still the exec"
Prabhu Joseph <prabhujose.gates@gmail.com>,"Wed, 10 Feb 2016 16:44:18 +0530",Re: Spark Job on YARN accessing Hbase Table,Ted Yu <yuzhihong@gmail.com>,"Yes Ted, spark.executor.extraClassPath will work if hbase client jars is
present in all Spark Worker / NodeManager machines.

spark.yarn.dist.files is the easier way, as hbase client jars can be copied
from driver machine or hdfs into container / spark-executor classpath
automatically. No need to manually copy hbase client jars into
spark.executor.extraClassPath of all Worker / NodeManager nodes.

 spark.yarn.dist.files includes the jars from driver machine or hdfs into
container / spark executor classpath, but launch-container.sh does not
include the CWD/* of container into the classpath in hadoop-2.5.1 and hence
spark.yarn.dist.files does not work with hadoop-2.5.1,
spark.yarn.dist.files works fine on hadoop-2.7.0, as CWD/* is included in
container classpath through some bug fix. Searching for the JIRA.

Thanks,
Prabhu Joseph




"
Tim Hunter <timhunter@databricks.com>,"Wed, 10 Feb 2016 09:13:59 -0800","Introducing spark-sklearn, a scikit-learn integration package for Spark","dev@spark.apache.org, user <user@spark.apache.org>","Hello community,
Joseph and I would like to introduce a new Spark package that should
be useful for python users that depend on scikit-learn.

Among other tools:
 - train and evaluate multiple scikit-learn models in parallel.
 - convert Spark's Dataframes seamlessly into numpy arrays
 - (experimental) distribute Scipy's sparse matrices as a dataset of
sparse vectors.

Spark-sklearn focuses on problems that have a small amount of data and
that can be run in parallel. Note this package distributes simple
tasks like grid-search cross-validation. It does not distribute
individual learning algorithms (unlike Spark MLlib).

If you want to use it, see instructions on the package page:
https://github.com/databricks/spark-sklearn

This blog post contains more details:
https://databricks.com/blog/2016/02/08/auto-scaling-scikit-learn-with-spark.html

Let us know if you have any questions. Also, documentation or code
contributions are much welcome (Apache 2.0 license).

Cheers

Tim

---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Wed, 10 Feb 2016 10:58:24 -0800","Re: [build system] brief downtime, 8am PST thursday feb 10th","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","reminder:  this is happening tomorrow morning.


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Wed, 10 Feb 2016 13:08:33 -0800",Re: map-side-combine in Spark SQL,Rishitesh Mishra <rishi80.mishra@gmail.com>,"I'm not 100% sure I understand your question, but yes, Spark (both the RDD
API and SQL/DataFrame) does partial aggregation.



"
Jacek Laskowski <jacek@japila.pl>,"Thu, 11 Feb 2016 14:51:06 +0100",SPARK_WORKER_MEMORY in Spark Standalone - conf.getenv vs System.getenv?,dev <dev@spark.apache.org>,"Hi,

Is there a reason to use conf to read SPARK_WORKER_MEMORY not
System.getenv as for the other env vars?

https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/worker/WorkerArguments.scala#L45

Pozdrawiam,
Jacek

Jacek Laskowski | https://medium.com/@jaceklaskowski/
Mastering Apache Spark
==> https://jaceklaskowski.gitbooks.io/mastering-apache-spark/
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Prabhu Joseph <prabhujose.gates@gmail.com>,"Thu, 11 Feb 2016 20:54:18 +0530","Re: Long running Spark job on YARN throws ""No AMRMToken""",Hari Shreedharan <hshreedharan@cloudera.com>,"Steve,


      When ResourceManager is submitted with an application, AMLauncher
creates the token YARN_AM_RM_TOKEN (token used between RM and AM). When
ApplicationMaster
is launched, it tries to contact RM for registering request, allocate
request to receive containers, finish request. In all the requests,
ResourceManager does the
authorizeRequest, where it checks if the Current User has the token
YARN_AM_RM_TOKEN, if not throws the *""No AMRMToken"". *

       ResourceManager for every
yarn.resourcemanager.am-rm-tokens.master-key-rolling-interval-sec rolls the
master key, before rolling it, it has a period
of 1.5 *  yarn.am.liveness-monitor.expiry-interval-ms during which if AM
contacts RM with allocate request, RM checks if the AM has the
YARN_AM_RM_TOKEN
prepared using the previous master key, if so, it updates the AM user with
YARN_AM_RM_TOKEN prepared using new master key.

     If AM contacts with an YARN_AM_RM_TOKEN which is neither constructed
using current master key nor previous master key, then *""Invalid AMRMToken""*
message is thrown. This
error is the one will happen if AM has not been updated with new RM master
key. [YARN-3103 and YARN-2212 ]

Need your help to find scenario where ""No AMRMToken"" will happen, an user
added with a token but later that token is missing. Is token removed since
expired?


Thanks,
Prabhu Joseph


"
shane knapp <sknapp@berkeley.edu>,"Thu, 11 Feb 2016 07:35:07 -0800","Re: [build system] brief downtime, 8am PST thursday feb 10th","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","reminder:  this is happening in ~30 minutes



---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Thu, 11 Feb 2016 08:10:46 -0800","Re: [build system] brief downtime, 8am PST thursday feb 10th","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","this is now done.


---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Thu, 11 Feb 2016 10:19:46 -0800",[build system] additional jenkins downtime next thursday,"amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","there's a big security patch coming out next week, and i'd like to
upgrade our jenkins installation so that we're covered.  it'll be
around 8am, again, and i'll send out more details about the upgrade
when i get them.

thanks!

shane

---------------------------------------------------------------------


"
Davies Liu <davies@databricks.com>,"Thu, 11 Feb 2016 11:26:44 -0800",Re: Making BatchPythonEvaluation actually Batch,Justin Uang <justin.uang@gmail.com>,"Had a quick look in your commit, I think that make sense, could you
send a PR for that, then we can review it.

In order to support 2), we need to change the serialized Python
function from `f(iter)` to `f(x)`, process one row at a time (not a
partition),
then we can easily combine them together:

for f1(f2(x))  and g1(g2(x)), we can do this in Python:

for row in reading_stream:
   x1, x2 = row
   y1 = f1(f2(x1))
   y2 = g1(g2(x2))
   yield (y1, y2)

For RDD, we still need to use `f(iter)`, but for SQL UDF, use `f(x)`.

ch
 a
t a
ce
er,
y a
f40fb309fc)
))
st
ng
em down
ame
20
,
€™s variables,
ld
€)).select(F.col(â€œcol1x2â€),
. To get around that, I add a
h a
lex. The way Iâ€™m
he
uld
ng

---------------------------------------------------------------------


"
"""Franklyn D'souza"" <franklyn.dsouza@shopify.com>","Thu, 11 Feb 2016 16:42:03 -0500",Operations on DataFrames with User Defined Types in pyspark,dev@spark.incubator.apache.org,"I'm using the UDT api to work with a custom Money datatype in dataframes.
heres how i have it setup

class StringUDT(UserDefinedType):


    @classmethod
    def sqlType(self):
        return StringType()

    @classmethod
    def module(cls):
        return cls.__module__

    @classmethod
    def scalaUDT(cls):
        return ''

    def serialize(self, obj):
        return str(obj)

    def deserialize(self, datum):
        return Money(datum)


class MoneyUDT(StringUDT):
    pass

Money.__UDT__ = MoneyUDT()

I then create a DataFrame like so

df = sc.sql.createDataFrame([[Money(""25.0"")], [Money(""100.0"")]], spark_schema)

However i've run into a few snags with this. DFs created using this
UDT can not be orderedBy the UDT column and i can't Union two DFs that
have this UDT on one of their columns.

Is this expected behaviour ? or is my UDT setup wrong ?.
"
Reynold Xin <rxin@apache.org>,"Thu, 11 Feb 2016 14:52:39 -0800",Spark Summit San Francisco 2016 call for presentations (CFP),"""dev@spark.apache.org"" <dev@spark.apache.org>, user <user@spark.apache.org>","FYI,

Call for presentations is now open for Spark Summit. The event will take
place on June 6-8 in San Francisco. Submissions are welcome across a
variety of Spark-related topics, including applications, development, data
science, business value, spark ecosystem and research. Please submit by
February 29th to be considered.

Link to submission: https://spark-summit.org/2016/
"
Charlie Wright <charliewright@live.ca>,"Thu, 11 Feb 2016 20:15:45 -0500","Building Spark with a Custom Version of Hadoop: HDFS
 ClassNotFoundException","""dev@spark.apache.org"" <dev@spark.apache.org>","I am having issues trying to run a test job on a built version of Spark with a custom Hadoop JAR. My custom hadoop version runs without issues and I can run jobs from a precompiled version of Spark (with Hadoop) no problem. 
However, whenever I try to run the same Spark example on the Spark version with my custom hadoop JAR - I get this error:""Exception in thread ""main"" java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.Hdfs not found""
Does anybody know why this is happening?
Thanks,Charles.
 		 	   		  "
Ted Yu <yuzhihong@gmail.com>,"Thu, 11 Feb 2016 17:29:00 -0800",Re: Building Spark with a Custom Version of Hadoop: HDFS ClassNotFoundException,Charlie Wright <charliewright@live.ca>,"Hdfs class is in hadoop-hdfs-XX.jar

Can you check the classpath to see if the above jar is there ?

Please describe the command lines you used for building hadoop / Spark.

Cheers


"
Le Tien Dung <tiendzung.le@gmail.com>,"Fri, 12 Feb 2016 16:23:57 +0100",Spark SQL performance: version 1.6 vs version 1.5,dev@spark.apache.org,"Hi folks,

I have compared the performance of Spark SQL version 1.6.0 and version
1.5.2. In a simple case, Spark 1.6.0 is quite faster than Spark 1.5.2.
However in a more complex query - in our case it is an aggregation query
with grouping sets, Spark SQL version 1.6.0 is very much slower than Spark
SQL version 1.5. Could any of you kindly let us know a workaround for this
performance regression ?

Here is our test scenario:

case class Toto(
                 a: String = f""${(math.random*1e6).toLong}%06.0f"",
                 b: String = f""${(math.random*1e6).toLong}%06.0f"",
                 c: String = f""${(math.random*1e6).toLong}%06.0f"",
                 n: Int = (math.random*1e3).toInt,
                 m: Double = (math.random*1e3))

val data = sc.parallelize(1 to 1e6.toInt).map(i => Toto())
val df: org.apache.spark.sql.DataFrame = sqlContext.createDataFrame( data )

df.registerTempTable( ""toto"" )
val sqlSelect = ""SELECT a, b, COUNT(1) AS k1, COUNT(DISTINCT n) AS k2,
SUM(m) AS k3""
val sqlGroupBy = ""FROM toto GROUP BY a, b GROUPING SETS ((a,b),(a),(b))""
val sqlText = s""$sqlSelect $sqlGroupBy""

val rs1 = sqlContext.sql( sqlText )
rs1.saveAsParquetFile( ""rs1"" )

The query is executed from a spark-shell in local mode with
--driver-memory=1G. Screenshots from Spark UI are accessible at
http://i.stack.imgur.com/VujQY.png (Spark 1.5.2) and
http://i.stack.imgur.com/Hlg95.png (Spark 1.6.0). The DAG on Spark 1.6.0
can be viewed at http://i.stack.imgur.com/u3HrG.png.

Many thanks and looking forward to hearing from you,
Tien-Dung Le
"
=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@questtec.nl>,"Fri, 12 Feb 2016 16:51:07 +0100",Re: Spark SQL performance: version 1.6 vs version 1.5,Le Tien Dung <tiendzung.le@gmail.com>,"Hi Tien-Dung,

1.6 plans single distinct aggregates like multiple distinct aggregates;
this inherently causes some overhead but is more stable in case of high
cardinalities. You can revert to the old behavior by setting the
spark.sql.specializeSingleDistinctAggPlanning option to false. See also:
https://github.com/apache/spark/blob/branch-1.6/sql/core/src/main/scala/org/apache/spark/sql/SQLConf.scala#L452-L462

HTH

Kind regards,

Herman van HÃ¶vell


2016-02-12 16:23 GMT+01:00 Le Tien Dung <tiendzung.le@gmail.com>:

k
s
ta )
""
"
Le Tien Dung <tiendzung.le@gmail.com>,"Fri, 12 Feb 2016 17:41:01 +0100",Re: Spark SQL performance: version 1.6 vs version 1.5,=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@questtec.nl>,"Hi Herman,

We are very happy to receive your mail. Indeed, we can revert to the
old behaviour of Spark SQL (the performance and the DAG are the same in
both version).

Many thanks and have a nice weekend,
Tien-Dung

PS: In order to revert, the setting value should be ""true"".


rg/apache/spark/sql/SQLConf.scala#L452-L462
rk
is
ata
)""
"
gstvolvr <g.antoliver@gmail.com>,"Fri, 12 Feb 2016 12:34:01 -0700 (MST)",Saving a Pipeline with DecisionTreeModel Spark ML,dev@spark.apache.org,"Hi all,

I noticed that I cannot save a Pipeline containing a DecisionTree model
similar to the way I can save one with a LogisticRegression model. 
It looks like DecisionTreeClassificationModel does not implement MLWritable.

I describe a use case in  this post
<http://stackoverflow.com/questions/35368414/saving-a-pipeline-with-decisiontreemodel-spark-ml> 
.

Is there another way of doing this or should I open a JIRA? 

Thanks,
Gustavo





--

---------------------------------------------------------------------


"
Rakesh Chalasani <vnit.rakesh@gmail.com>,"Fri, 12 Feb 2016 21:36:58 +0000",Re: Saving a Pipeline with DecisionTreeModel Spark ML,"gstvolvr <g.antoliver@gmail.com>, dev@spark.apache.org","There is already JIRA tracking this
https://issues.apache.org/jira/browse/SPARK-11888


"
Jacek Laskowski <jacek@japila.pl>,"Fri, 12 Feb 2016 22:39:01 +0100",Re: SPARK_WORKER_MEMORY in Spark Standalone - conf.getenv vs System.getenv?,dev <dev@spark.apache.org>,"Hi devs,

Following up on this, it appears that spark.worker.ui.port can only be
set in --properties-file. I wonder why conf/spark-defaults.conf is
*not* used for the spark.worker.ui.port property? Any reason for the
decision?

Pozdrawiam,
Jacek

Jacek Laskowski | https://medium.com/@jaceklaskowski/
Mastering Apache Spark
==> https://jaceklaskowski.gitbooks.io/mastering-apache-spark/
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Fri, 12 Feb 2016 22:08:10 +0000",Re: SPARK_WORKER_MEMORY in Spark Standalone - conf.getenv vs System.getenv?,Jacek Laskowski <jacek@japila.pl>,"I think that difference in the code is just an oversight. They
actually do the same thing.

Why do you say this property can only be set in a file?


---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Sat, 13 Feb 2016 00:11:38 +0100",Re: SPARK_WORKER_MEMORY in Spark Standalone - conf.getenv vs System.getenv?,Sean Owen <sowen@cloudera.com>,"
Correct. Just meant to know the reason if there was any.


I said that conf/spark-defaults.conf can *not* be used to set
spark.worker.ui.port property and wondered why is so? It'd be nice to
have it for settings (not use workarounds like
SPARK_WORKER_OPTS=-Dspark.worker.ui.port=21212). Just spot it and
thought I'd ask if it needs to be cleaned up or improved.

Jacek

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Sat, 13 Feb 2016 07:38:21 +0000",Re: SPARK_WORKER_MEMORY in Spark Standalone - conf.getenv vs System.getenv?,Jacek Laskowski <jacek@japila.pl>,"Yes you said it is only set in a props file, but why do you say that?
because the resolution of your first question is that this is not
differently handled.


---------------------------------------------------------------------


"
Steve Loughran <stevel@hortonworks.com>,"Sat, 13 Feb 2016 15:13:10 +0000","Re: Long running Spark job on YARN throws ""No AMRMToken""",Prabhu Joseph <prabhujose.gates@gmail.com>,"

Steve,


      When ResourceManager is submitted with an application, AMLauncher creates the token YARN_AM_RM_TOKEN (token used between RM and AM). When ApplicationMaster
is launched, it tries to contact RM for registering request, allocate request to receive containers, finish request. In all the requests,

yes, see

https://github.com/steveloughran/hadoop-trunk/blob/HADOOP-12649-security/YARN-4653-yarn/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/markdown/YarnApplicationSecurity.md


ResourceManager does the
authorizeRequest, where it checks if the Current User has the token YARN_AM_RM_TOKEN, if not throws the ""No AMRMToken"".

yes; prior to YARN-3103 it did the login user


       ResourceManager for every yarn.resourcemanager.am-rm-tokens.master-key-rolling-interval-sec rolls the master key, before rolling it, it has a period
of 1.5 *  yarn.am.liveness-monitor.expiry-interval-ms during which if AM contacts RM with allocate request, RM checks if the AM has the YARN_AM_RM_TOKEN
prepared using the previous master key, if so, it updates the AM user with YARN_AM_RM_TOKEN prepared using new master key.

     If AM contacts with an YARN_AM_RM_TOKEN which is neither constructed using current master key nor previous master key, then ""Invalid AMRMToken"" message is thrown. This
error is the one will happen if AM has not been updated with new RM master key. [YARN-3103 and YARN-2212 ]

Need your help to find scenario where ""No AMRMToken"" will happen, an user added with a token but later that token is missing. Is token removed since expired?


...or there's some confusion about the current user

I've got a java class to help with credential creation and diagnostics, not yet ported to hadoop core, which can do some listing & dumping of credentials

https://github.com/apache/incubator-slider/blob/develop/slider-core/src/main/java/org/apache/slider/core/launch/CredentialUtils.java

you may be able to copy that code and use it to print out what tokens the current user has; otherwise I don't know. I've never personally hit the message
"
Jong Wook Kim <ilikekjw@gmail.com>,"Sat, 13 Feb 2016 19:57:00 -0500",Re: Spark 1.6.1,Daniel Darabos <daniel.darabos@lynxanalytics.com>,"Is 1.6.1 going to be ready this week? I see that the two last unresolved
issues targeting 1.6.1 are fixed
<https://github.com/apache/spark/pull/11131> now
<https://github.com/apache/spark/pull/10539>.


"
chutium <tengqiu@gmail.com>,"Mon, 15 Feb 2016 07:45:39 -0700 (MST)",Re: Spark 1.6.0 + Hive + HBase,dev@spark.apache.org,"anyone took a look at this issue:
https://issues.apache.org/jira/browse/HIVE-11166

i got same exception by inserting into hbase table



--

---------------------------------------------------------------------


"
Koert Kuipers <koert@tresata.com>,"Mon, 15 Feb 2016 10:12:29 -0500",Dataset in spark 2.0.0-SNAPSHOT missing columns,"""dev@spark.apache.org"" <dev@spark.apache.org>","i noticed some things stopped working on datasets in spark 2.0.0-SNAPSHOT,
and with a confusing error message (cannot resolved some column with input
columns []).

for example in 1.6.0-SNAPSHOT:
scala> val ds = sc.parallelize(1 to 10).toDS
ds: org.apache.spark.sql.Dataset[Int] = [value: int]

scala> ds.map(x => Option(x))
res0: org.apache.spark.sql.Dataset[Option[Int]] = [value: int]

and same commands in 2.0.0-SNAPSHOT:
scala> val ds = sc.parallelize(1 to 10).toDS
ds: org.apache.spark.sql.Dataset[Int] = [value: int]

scala> ds.map(x => Option(x))
org.apache.spark.sql.AnalysisException: cannot resolve 'value' given input
columns: [];
  at
org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
  at
org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:60)
  at
org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:57)
  at
org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:284)
  at
org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:284)
  at
org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
  at
org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:283)
  at
org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionUp$1(QueryPlan.scala:162)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.org
$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2(QueryPlan.scala:172)
  at
org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2$1.apply(QueryPlan.scala:176)
  at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245)
  at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245)
  at scala.collection.immutable.List.foreach(List.scala:381)
  at scala.collection.TraversableLike$class.map(TraversableLike.scala:245)
  at scala.collection.immutable.List.map(List.scala:285)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.org
$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2(QueryPlan.scala:176)
  at
org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:181)
  at scala.collection.Iterator$$anon$11.next(Iterator.scala:370)
  at scala.collection.Iterator$class.foreach(Iterator.scala:742)
  at scala.collection.AbstractIterator.foreach(Iterator.scala:1194)
  at
scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
  at
scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
  at
scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
  at scala.collection.AbstractIterator.to(Iterator.scala:1194)
  at
  at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1194)
  at
  at scala.collection.AbstractIterator.toArray(Iterator.scala:1194)
  at
org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:181)
  at
org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:57)
  at
org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:50)
  at
org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:122)
  at
org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:121)
  at
org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:121)
  at scala.collection.immutable.List.foreach(List.scala:381)
  at
org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:121)
  at
org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:50)
  at
org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:46)
  at
org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.resolve(ExpressionEncoder.scala:322)
  at org.apache.spark.sql.Dataset.<init>(Dataset.scala:81)
  at org.apache.spark.sql.Dataset.<init>(Dataset.scala:92)
  at org.apache.spark.sql.Dataset.mapPartitions(Dataset.scala:339)
  at org.apache.spark.sql.Dataset.map(Dataset.scala:323)
  ... 43 elided

i observed similar issues with user defined types
(org.apache.spark.sql.types.UserDefinedType) in Dataset. trying to insert a
UserDefinedType in Dataset[Row] fails with input columns [].
"
Jayesh Thakrar <j_thakrar@yahoo.com.INVALID>,"Mon, 15 Feb 2016 16:01:50 +0000 (UTC)",Subscribe,"""user@spark.apache.org"" <user@spark.apache.org>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>",Subscribe
Reynold Xin <rxin@databricks.com>,"Mon, 15 Feb 2016 12:52:03 -0800",Re: Dataset in spark 2.0.0-SNAPSHOT missing columns,Koert Kuipers <koert@tresata.com>,"Looks like a bug. I'm also not sure whether we support Option yet. (If not,
we should definitely support that in 2.0.)

Can you file a JIRA ticket?



"
Koert Kuipers <koert@tresata.com>,"Mon, 15 Feb 2016 16:04:21 -0500",Re: Dataset in spark 2.0.0-SNAPSHOT missing columns,Reynold Xin <rxin@databricks.com>,"yeah i was surprised with the Option. it works in 1.6.0-SNAPSHOT, and its a
pretty neat way to indicate nullability i guess.

i will file jira. i saw similar behavior with other types than Option. this
was just the easiest to show.


"
Michael Armbrust <michael@databricks.com>,"Mon, 15 Feb 2016 15:50:33 -0800",Re: Spark 1.6.1,Jong Wook Kim <ilikekjw@gmail.com>,"I'm not going to be able to do anything until after the Spark Summit, but I
will kick off RC1 after that (end of week).  Get your patches in before
then!


"
Deepak Gopalakrishnan <dgkris@gmail.com>,"Tue, 16 Feb 2016 15:47:00 +0530",Call wholeTextFiles to read gzip files,dev@spark.apache.org,"Hello,

I'm reading S3 files using wholeTextFiles() . My files are gzip format but
the names of the files does not end with a "".gz"". I cannot force the names
of these files to end with a "".gz"" . Is there a way to specify the
InputFormat as Gzip when using wholeTextFiles()
?

-- 
Regards,
*Deepak Gopalakrishnan*
*Mobile*:+918891509774
*Skype* : deepakgk87
http://myexps.blogspot.com
"
Ted Yu <yuzhihong@gmail.com>,"Tue, 16 Feb 2016 04:56:56 -0800",Re: Call wholeTextFiles to read gzip files,Deepak Gopalakrishnan <dgkris@gmail.com>,"Have you seen this thread ?

http://stackoverflow.com/questions/24402737/how-to-read-gz-files-in-spark-using-wholetextfiles


"
Igor Costa <igorcosta@apache.org>,"Wed, 17 Feb 2016 11:44:32 +1300",Re: SPARK_WORKER_MEMORY in Spark Standalone - conf.getenv vs System.getenv?,dev <dev@spark.apache.org>,"Actually answering the first question:

Is there a reason to use conf to read SPARK_WORKER_MEMORY not
System.getenv as for the other env vars?

You can use the properties file to change the amount, System.getenv would
be bad when you have for example other things running on the JVM which will
cause conflict on some parts.
 Defined usage in properties files is more convenience for custom UI to be
made available.


"
Igor Costa <igorcosta@apache.org>,"Wed, 17 Feb 2016 11:45:57 +1300",Re: Welcoming two new committers,dev <dev@spark.apache.org>,"Congratulations Herman and Wenchen.


"
Raffael Bottoli Schemmer <spock2f@gmail.com>,"Tue, 16 Feb 2016 21:20:18 -0200",Re: Welcoming two new committers,Igor Costa <igorcosta@apache.org>,"Congratulations Herman and Wenchen,

2016-02-16 20:45 GMT-02:00 Igor Costa <igorcosta@apache.org>:

"
Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Wed, 17 Feb 2016 05:28:54 +0100",DataFrame API and Ordering,dev@spark.apache.org,"I am not sure if I've missed something obvious but as far as I can tell
DataFrame API doesn't provide a clearly defined ordering rules excluding
NaN handling. Methods like DataFrame.sort or sql.functions like min /
max provide only general description. Discrepancy between functions.max
(min) and GroupedData.max where the latter one supports only numeric
makes current situation even more confusing. With growing number of
orderable types I believe that documentation should clearly define
ordering rules including:

- NULL behavior
- collation
- behavior on complex types (structs, arrays)

While this information can extracted from the source it is not easily
accessible and without explicit specification it is not clear if current
behavior is contractual. It can be also confusing if user expects an
order depending on a current locale (R).

Best,
Maciej

"
shane knapp <sknapp@berkeley.edu>,"Wed, 17 Feb 2016 10:47:09 -0800",Re: [build system] additional jenkins downtime next thursday,"amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","the security release has been delayed until next wednesday morning,
and i'll be doing the upgrade first thing thursday morning.

i'll update everyone when i get more information.

thanks!

shane


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Wed, 17 Feb 2016 08:16:15 +0000",Re: SPARK_WORKER_MEMORY in Spark Standalone - conf.getenv vs System.getenv?,Igor Costa <igorcosta@apache.org>,"Here he's referring to a line of code that calls SparkConf.getenv vs
System.getenv, but the former calls the latter. In neither case does
it read from a props file.


---------------------------------------------------------------------


"
Stavros Kontopoulos <stavros.kontopoulos@typesafe.com>,"Wed, 17 Feb 2016 21:25:28 +0200","Re: SparkOscope: Enabling Spark Optimization through Cross-stack
 Monitoring and Visualization",Pete Robbins <robbinspg@gmail.com>,"Cool work! I will have a look to the project.

Cheers




-- 



<http://www.typesafe.com>
"
shane knapp <sknapp@berkeley.edu>,"Wed, 17 Feb 2016 14:19:48 -0800",FYI: github is getting DDOSed,"amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","this may cause builds to timeout on the git fetch much more than usual[1].

https://status.github.com/messages

just thought people might want to know...

shane

1 -- this actually happens pretty often, sadly.

---------------------------------------------------------------------


"
Cheng Lian <lian.cs.zju@gmail.com>,"Thu, 18 Feb 2016 11:03:37 +0800",Re: Welcoming two new committers,"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>, hvanhovell@questtec.nl,
 Wenchen Fan <wenchen@databricks.com>","Awesome! Congrats and welcome!!


"
Reynold Xin <rxin@databricks.com>,"Wed, 17 Feb 2016 22:18:46 -0500",pull request template,"""dev@spark.apache.org"" <dev@spark.apache.org>","Github introduced a new feature today that allows projects to define
templates for pull requests. I pushed a very simple template to the
repository:

https://github.com/apache/spark/blob/master/.github/PULL_REQUEST_TEMPLATE


Over time I think we can see how this works and perhaps add a small
checklist to the pull request template so contributors are reminded every
time they submit a pull request the important things to do in a pull
request (e.g. having proper tests).



## What changes were proposed in this pull request?

(Please fill in changes proposed in this fix)


## How was the this patch tested?

(Please explain how this patch was tested. E.g. unit tests, integration
tests, manual tests)


(If this patch involves UI changes, please attach a screenshot; otherwise,
remove this)
"
Cheng Lian <lian.cs.zju@gmail.com>,"Thu, 18 Feb 2016 11:26:34 +0800",Re: Welcoming two new committers,"Wenchen Fan <wenchen@databricks.com>, hvanhovell@questtec.nl","Awesome! Congrats and welcome!!

Cheng


"
Ashish Soni <asoni.learn@gmail.com>,"Thu, 18 Feb 2016 10:13:32 -0500",SPARK-9559,"user <user@spark.apache.org>, dev@spark.apache.org","Hi All ,

Just wanted to know if there is any work around or resolution for below
issue in Stand alone mode

https://issues.apache.org/jira/browse/SPARK-9559

Ashish
"
Daniel Darabos <daniel.darabos@lynxanalytics.com>,"Thu, 18 Feb 2016 16:21:20 +0100",Re: SPARK-9559,Ashish Soni <asoni.learn@gmail.com>,"YARN may be a workaround.


"
Cody Koeninger <cody@koeninger.org>,"Thu, 18 Feb 2016 10:59:33 -0600",Kafka connector mention in Matei's keynote,"""dev@spark.apache.org"" <dev@spark.apache.org>","I saw this slide:
http://image.slidesharecdn.com/east2016v2matei-160217154412/95/2016-spark-summit-east-keynote-matei-zaharia-5-638.jpg?cb=1455724433

Didn't see the talk - was this just referring to the existing work on the
spark-streaming-kafka subproject, or is someone actually working on making
Kafka Connect ( http://docs.confluent.io/2.0.0/connect/ ) play nice with
Spark?
"
Reynold Xin <rxin@databricks.com>,"Thu, 18 Feb 2016 12:24:31 -0500",Re: Kafka connector mention in Matei's keynote,Cody Koeninger <cody@koeninger.org>,"I think Matei was referring to the Kafka direct streaming source added in
2015.



"
Reynold Xin <rxin@databricks.com>,"Thu, 18 Feb 2016 18:35:01 -0500",Re: DataFrame API and Ordering,Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"You are correct and we should document that.

Any suggestions on where we should document this? In DoubleType and
FloatType?


"
Jason White <jason.white@shopify.com>,"Thu, 18 Feb 2016 19:07:41 -0700 (MST)",How to run PySpark tests?,dev@spark.apache.org,"Hi,

I'm trying to finish up a PR (https://github.com/apache/spark/pull/10089)
which is currently failing PySpark tests. The instructions to run the test
suite seem a little dated. I was able to find these:
https://cwiki.apache.org/confluence/display/SPARK/PySpark+Internals
http://spark.apache.org/docs/latest/building-spark.html

I've tried running `python/run-tests`, but it fails hard at the ORC tests. I
suspect it has to do with the external libraries not being compiled or put
in the right location.
I've tried running `SPARK_TESTING=1 ./bin/pyspark
python/pyspark/streaming/tests.py` as suggested, but this doesn't work on
Spark 2.0.
I've tried running `SPARK_TESTING=1 ./bin/spark-submit
python/pyspark/streaming/tests.py`and that worked a little better, but it
failed at `pyspark.streaming.tests.KafkaStreamTests`, with
`java.lang.ClassNotFoundException:
org.apache.spark.streaming.kafka.KafkaTestUtils`. I suspect the same issue
with external libraries.

I've compiling Spark with `build/mvn -Pyarn -Phadoop-2.4
-Dhadoop.version=2.4.0 -DskipTests clean package` with no trouble.

Is there any better documentation somewhere about how to run the PySpark
tests?



--

---------------------------------------------------------------------


"
=?UTF-8?B?5YiY55WF?= <liuchang0812@gmail.com>,"Fri, 19 Feb 2016 10:21:59 +0800",Re: Welcoming two new committers,Cheng Lian <lian.cs.zju@gmail.com>,"Awesome! Congrats and welcome!!

2016-02-18 11:26 GMT+08:00 Cheng Lian <lian.cs.zju@gmail.com>:

"
Hyukjin Kwon <gurwls223@gmail.com>,"Fri, 19 Feb 2016 11:25:55 +0900",Ability to auto-detect input data for datasources (by file extension).,dev@spark.apache.org,"Hi all,

I am planning to submit a PR for
https://issues.apache.org/jira/browse/SPARK-8000.

Currently, file format is not detected by the file extension unlike
compression codecs are being detected.

I am thinking of introducing another interface (a function) at
DataSourceRegister just like shortName() at in order to specify possible
file exceptions so that we can detect datasources by file extensions just
like Hadoop does for compression codecs.

Since adding an interface should be carefully done, I want to first ask if
this approach looks appropriate.

Could you please give me some feedback for this?


Thanks!
"
Reynold Xin <rxin@databricks.com>,"Thu, 18 Feb 2016 21:27:32 -0500",Re: Ability to auto-detect input data for datasources (by file extension).,Hyukjin Kwon <gurwls223@gmail.com>,"Thanks for the email.

Don't make it that complicated. We just want to simplify the common cases
(e.g. csv/parquet), and don't need this to work for everything out there.



"
Holden Karau <holden@pigscanfly.ca>,"Thu, 18 Feb 2016 18:56:56 -0800",Re: How to run PySpark tests?,Jason White <jason.white@shopify.com>,"I've run into some problems with the Python tests in the past when I
haven't built with hive support, you might want to build your assembly with
hive support and see if that helps.



-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
Jason White <jason.white@shopify.com>,"Thu, 18 Feb 2016 21:34:39 -0700 (MST)",Re: How to run PySpark tests?,dev@spark.apache.org,"Compiling with `build/mvn -Pyarn -Phadoop-2.4 -Phive -Dhadoop.version=2.4.0
-DskipTests clean package` followed by `python/run-tests` seemed to do the
trick! Thanks!



--

---------------------------------------------------------------------


"
Holden Karau <holden@pigscanfly.ca>,"Thu, 18 Feb 2016 20:45:28 -0800",Re: How to run PySpark tests?,Jason White <jason.white@shopify.com>,"Great - I'll update the wiki.




-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
Prabhu Joseph <prabhujose.gates@gmail.com>,"Fri, 19 Feb 2016 11:21:35 +0530",Concurreny does not improve for Spark Jobs with Same Spark Context,"user <user@spark.apache.org>, Spark dev list <dev@spark.apache.org>","Hi All,

   When running concurrent Spark Jobs on YARN (Spark-1.5.2) which share a
single Spark Context, the jobs take more time to complete comparing with
when they ran with different Spark Context.
The spark jobs are submitted on different threads.

Test Case:

    A.  3 spark jobs submitted serially
    B.  3 spark jobs submitted concurrently and with different SparkContext
    C.  3 spark jobs submitted concurrently and with same Spark Context
    D.  3 spark jobs submitted concurrently and with same Spark Context and
tripling the resources.

A and B takes equal time, But C and D are taking 2-3 times longer than A,
which shows concurrency does not improve with shared Spark Context. [Spark
Job Server]

Thanks,
Prabhu Joseph
"
=?utf-8?Q?J=C3=B6rn_Franke?= <jornfranke@gmail.com>,"Fri, 19 Feb 2016 07:36:40 +0100",Re: Concurreny does not improve for Spark Jobs with Same Spark Context,Prabhu Joseph <prabhujose.gates@gmail.com>,"How did you configure YARN queues? What scheduler? Preemption ?

:
ingle Spark Context, the jobs take more time to complete comparing with when they ran with different Spark Context.
t
d tripling the resources.
hich shows concurrency does not improve with shared Spark Context. [Spark Job Server]

---------------------------------------------------------------------


"
Prabhu Joseph <prabhujose.gates@gmail.com>,"Fri, 19 Feb 2016 12:50:17 +0530",Re: Concurreny does not improve for Spark Jobs with Same Spark Context,=?UTF-8?B?SsO2cm4gRnJhbmtl?= <jornfranke@gmail.com>,"Fair Scheduler, YARN Queue has the entire cluster resource as maxResource,
preemption does not come into picture during test case, all the spark jobs
got the requested resource.

The concurrent jobs with different spark context runs fine, so suspecting
on resource contention is not a correct one.

The performace degrades only for concurrent jobs on shared spark context.
Is SparkContext has any critical section, which needs locking, and jobs
waiting to read that. I know Spark and Scala is not a old thread model, it
uses Actor Model, where locking does not happen, but still want to verify
is java old  threading is used somewhere.



:

h
"
=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"Fri, 19 Feb 2016 11:33:58 +0100",Re: pull request template,Reynold Xin <rxin@databricks.com>,"It's a good idea. I would add in there the spec for the PR title. I always
get wrong the order between Jira and component.

Moreover, CONTRIBUTING.md is also lacking them. Any reason not to add it
there? I can open PRs for both, but maybe you want to keep that info on the
wiki instead.

iulian




-- 

--
Iulian Dragos

------
Reactive Apps on the JVM
www.typesafe.com
"
Sean Owen <sowen@cloudera.com>,"Fri, 19 Feb 2016 10:36:01 +0000",Re: pull request template,=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"All that seems fine. All of this is covered in the contributing wiki,
which is linked from CONTRIBUTING.md (and should be from the
template), but people don't seem to bother reading it. I don't mind
duplicating some key points, and even a more explicit exhortation to
read the whole wiki, before considering opening a PR. We spend way too
much time asking people to fix things they should have taken 60
seconds to do correctly in the first place.

s
he
E
y
uest
e,

---------------------------------------------------------------------


"
Holden Karau <holden@pigscanfly.ca>,"Fri, 19 Feb 2016 07:42:43 -0800",Re: Write access to wiki,shane knapp <sknapp@berkeley.edu>,"Any chance I could also get write access to the wiki? I'd like to update
some of the PySpark documentation in the wiki.




-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
Holden Karau <holden@pigscanfly.ca>,"Fri, 19 Feb 2016 07:43:28 -0800",Re: How to run PySpark tests?,Jason White <jason.white@shopify.com>,"Or wait I don't have access to the wiki - if anyone can give me wiki access
I'll update the instructions.





-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
Reynold Xin <rxin@databricks.com>,"Fri, 19 Feb 2016 11:53:33 -0800",Re: pull request template,Sean Owen <sowen@cloudera.com>,"We can add that too - just need to figure out a good way so people don't
leave a lot of the unnecessary ""guideline"" messages in the template.

The contributing guide is great, but unfortunately it is not as noticeable
and is often ignored. It's good to have this full-fledged contributing
guide, and then have a very lightweight version of that in the form of
templates to force contributors to think about all the important aspects
outlined in the contributing guide.





t
n
"
Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Sat, 20 Feb 2016 01:09:42 +0100",Re: DataFrame API and Ordering,Reynold Xin <rxin@databricks.com>,"I am not sure. Spark SQL, DataFrames and Datasets Guide already has a
section about NaN semantics. This could be a good place to add at least
some basic description.

For the rest InterpretedOrdering could be a good choice.

 /
c

ly
n

"
ahaider3 <ahaider3@hawk.iit.edu>,"Sat, 20 Feb 2016 21:05:21 -0700 (MST)",Using Encoding to reduce GraphX's static graph memory consumption,dev@spark.apache.org,"Hi,
I have been looking through the GraphX source code, dissecting the reason
for its high memory consumption compared to the on-disk size of the graph. I
have found that there may be room to reduce the memory footprint of the
graph structures. I think the biggest savings can come from the localSrcIds
and localDstIds in EdgePartitions. 

In particular, instead of storing both a source and destination local ID for
each edge, we could store only the destination id. For example after sorting
edges by global source id, we can map each of the source vertices first to
local values followed by unmapped global destination ids. This would make
localSrcIds sorted starting from 0 to n, where n is the number of distinct
global source ids. Then instead of actually storing the local source id for
each edge, we can store an array of size n, with each element storing an
index into localDstIds.  From my understanding, this would also eliminate
the need for storing an index for indexed scanning, since each element in
localSrcIds would be the start of a cluster. From some extensive testing,
this along with some delta encoding strategies on localDstIds and the
mapping structures can reduce memory consumption of the graph by nearly
half. 

However, I am not entirely sure if there is any reason for storing both
localSrcIds and localDstIds for each edge in terms of integration of future
functionalities, such as graph mutations. I noticed there was another post
similar to this one as well, but it had not replies.

The idea is quite similar to  Netflix graph library
<https://github.com/Netflix/netflix-graph>   and would be happy to open a
jira on this issue with partial improvements. But, I may not be completely
correct with my thinking! 




--

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Mon, 22 Feb 2016 06:35:49 +0000",Re: Using Encoding to reduce GraphX's static graph memory consumption,"ahaider3 <ahaider3@hawk.iit.edu>, dev@spark.apache.org",#NAME?
Reynold Xin <rxin@databricks.com>,"Mon, 22 Feb 2016 06:36:26 +0000",Re: DataFrame API and Ordering,Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"InterpretedOrdering I think is internal, so it's not useful to add it there
for public docs. We should definitely add a small section to the guide.




"
Sean Owen <sowen@cloudera.com>,"Mon, 22 Feb 2016 09:17:11 +0000",How do we run that PR auto-close script again?,dev <dev@spark.apache.org>,"I know Patrick told us at some point, but I can't find the email or
wiki that describes how to run the script that auto-closes PRs with
""do you mind closing this PR"". Does anyone know? I think it's been a
long time since it was run.

---------------------------------------------------------------------


"
Akhil Das <akhil@sigmoidanalytics.com>,"Mon, 22 Feb 2016 16:14:52 +0530",Re: How do we run that PR auto-close script again?,Sean Owen <sowen@cloudera.com>,"This?
http://apache-spark-developers-list.1001551.n3.nabble.com/Automated-close-of-PR-s-td15862.html

Thanks
Best Regards


"
Sean Owen <sowen@cloudera.com>,"Mon, 22 Feb 2016 10:48:30 +0000",Re: How do we run that PR auto-close script again?,Akhil Das <akhil@sigmoidanalytics.com>,"That's what I'm talking about, yes, but I'm looking for the actual
script. I'm sure there was a discussion about where it was and how to
run it somewhere. Really just looking to have it run again.


---------------------------------------------------------------------


"
"""=?gb18030?B?v6rQxNHTxOo=?="" <muyannian@qq.com>","Mon, 22 Feb 2016 19:14:09 +0800",a new FileFormat 5x~100x  faster than parquet,"""=?gb18030?B?dXNlcg==?="" <user@spark.apache.org>, ""=?gb18030?B?ZGV2?="" <dev@spark.apache.org>","Ya100 is a FileFormat 5x~100x  faster than parquet¡£
we can get ya100 from this link https://github.com/ycloudnet/ya100/tree/master/v1.0.8




 
1.we used the inverted index£¬so we skip the rows that we does need.

  for example  the trade log search SQL

     

        select 
 
£¨1£©phonenum,usernick,ydb_grade,ydb_age,ydb_blood,ydb_zhiye,ydb_earn,ydb_day,amtlong  
 
        from spark_txt where   
 
£¨2£©tradeid=' 2014012213870282671'
 
        limit 10;  
 





     this sql is compose by two part

     (1)the part 1 is return the result which has 9 columns

     (2) the part 2 is the filter condition ,filter by tradeid

      


      let guess which plan is faster

     plan A :first read all the 9 columns result then filter by tradeid

     plan B: first filter by tradeid ,then we read the match 9 columns result.




    Ya100 choose plan B




     contrast  performance Ya100`index with parquet








 2.TOP N sort ,the non sort column we doesn`t read it until the last 


  for example  we sort by the logtime

 

    select 
 
£¨1£©phonenum,usernick,ydb_grade,ydb_age,ydb_blood,ydb_zhiye,ydb_earn,ydb_day,amtlong  
 
    from spark_txt 
 
£¨2£©order by logtime desc 
 
    limit 10;  
 


  this sql is compose by two part
     (1)the part 1 is return the result which has 9 columns

     (2) the part 2 is the column need to sort

      


      let guess which plan is faster
     plan A :first read all the 9 columns result then sort by logtime
     plan B: first sort by logtime  ,then we read the match 9 columns result.




       Ya100 choose plan B




     contrast  performance Ya100`lazy read with parquet


3.we used label instead of the original value for grouping and sorting



1).General situation,the data has a lot of repeat value,for exampe the sex file ,the age field .
2).if we store the original value ,that will weast a lot of storage.
so we make a small modify at original  value, Additional add a new filed called label.
make a unique value sort by fields, and then gave each term a unique  Number from begin to end  .
3).we use number value(we called label) instead of original  value.lable is store by fixed length. the file could be read by random read.
4).the label`s order is the same with dictionary  order .so if we do some calculation like order by or group by only need to  read the label. we don`t need to read the original value.
5).some field like sex field ,only have 2 different values.so we only use 2 bits(not 2 bytes) to store the label, it will save a lot of Disk io.
 when we finish all of the calculation, we translate label to original  value by a dictionary.
6)if a lots of rows have the same original value ,the original value we only store once,only read once.
Solve the problem:
1)ya100`s data is quite big we don`t have enough memory to load all Values to memory.
2)on realtime mode ,data is change Frequent , The cache is invalidated Frequent by append or update. build Cache will take a lot of times and io;
3)the Original value  is a string type.  whene sorting or grouping ,thed string value need a lot of memory and need lot of cpu time to calculate hashcode \compare \equals ,But label is number  is fast.
4)the label is number ,it`s type maybe short ,or maybe a byte ,or may be integer whitch depending on the max number of the label.

two-phase search
Original:
1)group by order by use original value,the real value may be is a string type,may be more larger ,the real value maybe  need a lot of io 
2)compare by string is slowly then compare by integer
Our improve:
1)we split one search into multy-phase search
2)the first search we only search the field that use for order by ,group by
3)the first search we doesn`t need to read the original value(the real value),we only need to read the docid and label for order by group by.
4)when we finish all the order by and group by ,may be we only need to return Top n records .so we start next to search to get the Top n records original value.
Solve the problem:
5)reduce io ,read original take a lot of disk io
6)reduce network io (for merger)
7)most of the field has repeated value, the repeated only need to read once
the group by filed only need to read the origin once by label where display to user.
8)most of the search only need to display on Top n (n<=100) results, so most of the original value could be skip.











 
How to install ya100 
 
 
1)Add the depend jar 
 
export SPARK_CLASSPATH=$SPARK_CLASSPATH:/data/ycloud/spark_ydb/ya100.jar
 
You can get the ya100 jar from this link



 
https://github.com/ycloudnet/ya100/blob/master/v1.0.8/ya100-1.0.8.jar



 
 
 
2)Start thriftserver 
 
./start-thriftserver.sh     --master yarn-client  --executor-memory 990m --executor-cores 2 --num-executors 16
 
 
 
Our suggest spark version is 6 
 
3)Config the ya100 function 
 
create  function Yfilter as 'cn.net.ycloud.ydb.handle.fun.Yfilter';   
 
create  function Ytop10000 as 'cn.net.ycloud.ydb.handle.fun.Ytop10000';  
 
create  function Ycombine as 'cn.net.ycloud.ydb.handle.fun.Ycombine'; 
 
create  function Ycount as 'cn.net.ycloud.ydb.handle.fun.Ycount';  
 
create  function Ymax as 'cn.net.ycloud.ydb.handle.fun.Ymax'; 
 
create  function Ymin as 'cn.net.ycloud.ydb.handle.fun.Ymin';   
 
create  function Yavg as 'cn.net.ycloud.ydb.handle.fun.Yavg';    
 
create  function Ysum as 'cn.net.ycloud.ydb.handle.fun.Ysum'; 
 
create  function Ymaxstring as 'cn.net.ycloud.ydb.handle.fun.Ymaxstring'; 
 
create  function Yminstring as 'cn.net.ycloud.ydb.handle.fun.Yminstring¡®; 
 
 
YA100 support data type 
 

 
How to create ya100 table on spark Sql 
 
CREATE external  table spark_ya100(  
 
 phonenum bigint, usernick string, ydb_sex string,  ydb_province string, ydb_grade string, ydb_age string, ydb_blood string, ydb_zhiye string, ydb_earn string, ydb_prefer string, ydb_consume string, ydb_day string, amtdouble double,amtlong int, content string, ydbpartion string, ya100_pipe string 
 
) partitioned by (dt string) 
 
STORED BY 'cn.net.ycloud.ydb.handle.Ya100StorageHandler' 
 
LOCATION  '/data/ydb/shu_ya100' 
 
TBLPROPERTIES( 
 
""ya100.handler.table.name""=""ydb_example_shu"", 
 
""ya100.handler.schema""=""phonenum long,usernick string,ydb_sex string,ydb_province string,ydb_grade string,ydb_age string,ydb_blood string,ydb_zhiye string,ydb_earn string,ydb_prefer string,ydb_consume string,ydb_day string, amtdouble double,amtlong int,content textcjk,ydbpartion string,ya100_pipe string"" 
 
)  
 
//×¢£¬ydbpartionÎªydbµÄ·ÖÇø×Ö¶ÎÓëya100_pipe Îªya100µÄ¹ÜµÀ×Ö¶Î£¬±ØÐëÒª¶¼Òª´´½¨£¬·ñÔòÓÐÐ©¹¦ÄÜ»á±»ÏÞÖÆÊ¹ÓÃ¡£
 
ya100.handler.table.nameÎªÔ¤ÁôµÄkey,²»Í¬µÄ±íÖ®¼äÒªÇø·Ö¿ªÀ´
 
ya100.handler.schema ÎªÄÚÖÃµÄË÷ÒýµÄÊý¾ÝÀàÐÍ£¬Ãû×ÖÒªÓësparkµÄ×Ö¶ÎÃû³ÆÒ»ÖÂ¡£ 
 

 
How to Import data into ya100 table


 

1)Set up import params 
 
set hive.mapred.supports.subdirectories=true; 
 
set mapred.min.split.size=2147483648;
 
×¢£º 
 
ya100ÒòÎªÉú³ÉµÄ×îÖÕ²»ÊÇÒ»¸öÎÄ¼þ£¬¶øÊÇÒ»¸öÄ¿Â¼£¬Õâ¸öÄ¿Â¼ÏÂ»áÓÐË÷Òý£¬ËùÒÔÒªÍ¨¹ýsubdirectories²ÎÊýÉèÖÃsparkÖ§³Ö×ÓÄ¿Â¼£¬·ñÔòÏò±íÀïµ¼ÈëÊý¾Ý»á±¨´íÊ§°Ü¡£ 
 
set mapred.min.split.size=2147483648 ÊÇÎªÁË¿ØÖÆmapµÄ¸öÊý£¬·ÀÖ¹Éú³ÉµÄË÷ÒýÎÄ¼þÊýÁ¿Ì«¶à¡£ 
 
 
 
2)Begin import  
 
insert into table spark_ya100 partition (dt='1200million')
 
 select phonenum,usernick,ydb_sex,ydb_province,ydb_grade,ydb_age,ydb_blood,ydb_zhiye,ydb_earn,ydb_prefer,ydb_consume,ydb_day,amtdouble,amtlong,content,2,1 
 
from spark_txt where dt='1200million'; 
 
 
How to use ya100 to filter data 
 
 
1),basic example 1 
 
set ya100.spark.filter.ydb_example_shu=ydb_sex='Å®' or ydb_province='ÁÉÄþ' or ydb_day>='20151217'; 
 
ÓÐÐ©Ê±ºò£¬»áÓöµ½×ªÒåµÄÎÊÌâ£¬ÕâÀïµÄvalueÖµ£¬¿ÉÒÔ½øÐÐurlencode´¦Àí¡£
 
Èç£º
 
set ya100.spark.filter.ydb_example_shu=ydbpartion%3D%2720151110%27+and+%28ydb_sex%3D%27%E5%A5%B3%27+or+ydb_province%3D%27%E8%BE%BD%E5%AE%81%27+or+ydb_day%3E%3D%2720151217%27%29;
 
 
 
2)Filter example 2 
 
set ya100.spark.filter.ydb_example_shu=ydb_sex='Å®' or ydb_province='ÁÉÄþ' or ydb_day>='20151217'; 
 
select * from spark_ya100 where dt=¡®1200million¡¯ limit 10; 
 
 
 
 
 
 
Other filter example use case  
 
1)equal 
 
qq=¡®165162897¡¯
 
2)Support in 
 
Èç£ºindexnum in (1,2,3) 
 
3)>,<,>=,<=, 
 
clickcount >=10 and clickcount <=11 
 
4) range 
 
indexnum like '({0 TO 11}) '      ²»°üº¬±ß½çÖµ
 
indexnum like '([10 TO 11] ) '    °üº¬±ß½çÖµ
 
5) Unequal 
 
label<>'l_14' and label<>'l_15'  
 
6) 
 
indexnum='1' or indexnum='2' or (clickcount >=5 and clickcount <=11) 
 
 
 



 
How to use ya100 to make a top N sort 
 
set ya100.spark.top10000.ydb_example_shu=ydb_age desc limit 10; 
 
set ya100.spark.top10000.ydb_example_shu=ydb_sex desc,ydb_province limit 100£» 
 
set ya100.spark.top10000.ydb_example_shu=* limit 10; 
 
 
 
×¢Òâ£¬µ±Ç°°æ±¾limitµÄ×î´óÖµÎª10000 £¨Ã¿¸öË÷Òý×î´ó·µ»Ø1000£© 
 
ÁÐµÄÃû×Ö Ð´*±íÊ¾ ²»ÐèÒªÅÅÐò£¬µ«Ö»·µ»ØÇ°NÌõ¾Í¿ÉÒÔÁË 
 
 
 
SQL example  
 
 
 
set ya100.spark.top10000.ydb_example_shu=phonenum desc limit 10; 
 
select * from spark_ya100 where dt='1200million' order by phonenum desc limit 10; 
 
 
 

 
How to make group by or stat by ya100 
 
Example 1 
 
set ya100.spark.combine.ydb_example_shu=*,ydb_age; 
 
select ydb_province,Ycount('*',ya100_pipe), Ycount('ydb_age',ya100_pipe), Ymaxstring('ydb_age',ya100_pipe), Yminstring('ydb_age',ya100_pipe) 
 
 from spark_ya100  group by ydb_province limit 10 
 
  
 
Example 2 
 
set ya100.spark.combine.ydb_example_shu=amtlong; 
 
select ydb_sex, ydb_province,Ysum('amtlong',ya100_pipe) as cnt from spark_ya100  group by ydb_sex, ydb_province  limit 10 
 
 
 

 
Other SQL EXAMPLE


 
6¦1EXAMPLE COUNT(*) 
 
set ya100.spark.filter.ydb_example_shu=phonenum='13870282671'  and usernick='½­¾þÎõ'; 
 
set ya100.spark.combine.ydb_example_shu=*; 
 
set ya100.spark.top10000.ydb_example_shu=; 
 
select Ycount('*',ya100_pipe) from spark_ya100 where dt='100million' limit 10; 
 6¦1TOP N SORT EXAMPLE
 6¦1set ya100.spark.filter.ydb_example_shu= amtlong like '([1090 TO 1100] )' and amtdouble like '([1090 TO 1100] )'; 
 
set ya100.spark.combine.ydb_example_shu=; 
 
set ya100.spark.top10000.ydb_example_shu=phonenum desc limit 10; 
 
select * from spark_ya100 where dt='100million' order by phonenum desc limit 10;
 
SUM(amtlong)
 
set ya100.spark.filter.ydb_example_shu= amtlong like '([1090 TO 1100] )' and amtdouble like '([1090 TO 1100] )'; 
 
set ya100.spark.combine.ydb_example_shu=amtlong; 
 
set ya100.spark.top10000.ydb_example_shu=; 
 
select Ycount(¡®amtlong¡¯,ya100_pipe) ,Ysum(¡®amtlong¡¯,ya100_pipe) ,Yavg(¡®amtlong¡¯,ya100_pipe), Ymax(¡®amtlong¡¯,ya100_pipe), Ymin(¡®amtlong¡¯,ya100_pipe)  from spark_ya100 where dt=¡®100million¡¯  limit 10; 
 
 6¦1Top n sort 
 
set ya100.spark.filter.ydb_example_shu= amtlong like '([1090 TO 1100] )' and amtdouble like '([1090 TO 1100] )'; 
 
set ya100.spark.combine.ydb_example_shu=; 
 
set ya100.spark.top10000.ydb_example_shu= ydb_age desc, ydb_sex; 
 
select ydb_sex, ydb_age from spark_ya100 where dt='100million'  order  by  ydb_age desc, ydb_sex  limit 10;  
 6¦1Group by 
 
set ya100.spark.filter.ydb_example_shu=; 
 
set ya100.spark.combine.ydb_example_shu=ydb_age,ydb_blood,ydb_zhiye,ydb_earn,ydb_prefer,ydb_consume,ydb_day,amtlong; 
 
set ya100.spark.top10000.ydb_example_shu=; 
 
select ydb_sex,ydb_province,ydb_grade, Ymaxstring(¡®ydb_age¡¯,ya100_pipe),Ymaxstring(¡®ydb_blood¡¯,ya100_pipe),Ymaxstring(¡®ydb_zhiye¡¯,ya100_pipe),Ymaxstring(¡®ydb_earn¡¯,ya100_pipe),Ymaxstring(¡®ydb_prefer¡¯,ya100_pipe),Ymaxstring(¡®ydb_consume¡¯,ya100_pipe),Ymaxstring(¡®ydb_day¡¯,ya100_pipe),Ysum(¡®amtlong¡¯,ya100_pipe) as cnt from spark_ya100 where dt=¡®100million¡¯  group by ydb_sex,ydb_province,ydb_grade order by cnt desc limit 10 
 

 
Connect Ya100 to ydb  make real time data 
CREATE external  table spark_ydb(
 phonenum bigint, usernick string, ydb_sex string,  ydb_province string, ydb_grade string, ydb_age string, ydb_blood string, ydb_zhiye string, ydb_earn string, ydb_prefer string, ydb_consume string, ydb_day string, amtdouble double,amtlong int, content string, ydbpartion string, ya100_pipe string
)
STORED BY 'cn.net.ycloud.ydb.handle.Ya100StorageHandler'
LOCATION  '/data/ydb/shu_ydb'
TBLPROPERTIES(
""ya100.handler.table.name""=""ydb_example_shu"",
""ya100.handler.master""=""101.200.130.48:8080"",
""ya100.handler.columns.mapping""=""phonenum,usernick,ydb_sex,ydb_province,ydb_grade,ydb_age,ydb_blood,ydb_zhiye,ydb_earn,ydb_prefer,ydb_consume,ydb_day,amtdouble,amtlong,content,ydbpartion,ya100_pipe"")

×¢

ydbpartionÎªydbµÄ·ÖÇø×Ö¶Î£¬²éÑ¯µÄÊ±ºò±ØÐëÖ´ÐÐydbpartion,¶øspark±íÔòÃ»ÓÐ·ÖÇø×Ö¶Î 
 
ya100_pipe Îªya100µÄ¹ÜµÀ×Ö¶Î£¬±ØÐëÒª¶¼Òª´´½¨£¬·ñÔòÓÐÐ©¹¦ÄÜ»á±»ÏÞÖÆÊ¹ÓÃ¡£
 
ya100.handler.table.nameÎªydbÏµÍ³ÀïµÄ±íÃû×Ö
 
ya100.handler.columns.mapping ÎªydbÏµÍ³ÄÚµÄ±íÓësparkµÄ±íÖ®¼äµÄÓ³Éä¡£"
"""=?gb18030?B?yfLR9NHT1MbUxrzGy+M=?="" <1820150327@qq.com>","Mon, 22 Feb 2016 19:47:11 +0800","=?gb18030?B?u9i4tKO6YSBuZXcgRmlsZUZvcm1hdCA1eH4xMDB4?=
 =?gb18030?B?ICBmYXN0ZXIgdGhhbiBwYXJxdWV0?=","""=?gb18030?B?dXNlcg==?="" <user@spark.apache.org>, ""=?gb18030?B?ZGV2?="" <dev@spark.apache.org>","mark




Ya100 is a FileFormat 5x~100x  faster than parquet¡£
we can get ya100 from this link https://github.com/ycloudnet/ya100/tree/master/v1.0.8




 
1.we used the inverted index£¬so we skip the rows that we does need.

  for example  the trade log search SQL

     

        select 
 
£¨1£©phonenum,usernick,ydb_grade,ydb_age,ydb_blood,ydb_zhiye,ydb_earn,ydb_day,amtlong  
 
        from spark_txt where   
 
£¨2£©tradeid=' 2014012213870282671'
 
        limit 10;  
 





     this sql is compose by two part

     (1)the part 1 is return the result which has 9 columns

     (2) the part 2 is the filter condition ,filter by tradeid

      


      let guess which plan is faster

     plan A :first read all the 9 columns result then filter by tradeid

     plan B: first filter by tradeid ,then we read the match 9 columns result.




    Ya100 choose plan B




     contrast  performance Ya100`index with parquet








 2.TOP N sort ,the non sort column we doesn`t read it until the last 


  for example  we sort by the logtime

 

    select 
 
£¨1£©phonenum,usernick,ydb_grade,ydb_age,ydb_blood,ydb_zhiye,ydb_earn,ydb_day,amtlong  
 
    from spark_txt 
 
£¨2£©order by logtime desc 
 
    limit 10;  
 


  this sql is compose by two part
     (1)the part 1 is return the result which has 9 columns

     (2) the part 2 is the column need to sort

      


      let guess which plan is faster
     plan A :first read all the 9 columns result then sort by logtime
     plan B: first sort by logtime  ,then we read the match 9 columns result.




       Ya100 choose plan B




     contrast  performance Ya100`lazy read with parquet


3.we used label instead of the original value for grouping and sorting



1).General situation,the data has a lot of repeat value,for exampe the sex file ,the age field .
2).if we store the original value ,that will weast a lot of storage.
so we make a small modify at original  value, Additional add a new filed called label.
make a unique value sort by fields, and then gave each term a unique  Number from begin to end  .
3).we use number value(we called label) instead of original  value.lable is store by fixed length. the file could be read by random read.
4).the label`s order is the same with dictionary  order .so if we do some calculation like order by or group by only need to  read the label. we don`t need to read the original value.
5).some field like sex field ,only have 2 different values.so we only use 2 bits(not 2 bytes) to store the label, it will save a lot of Disk io.
 when we finish all of the calculation, we translate label to original  value by a dictionary.
6)if a lots of rows have the same original value ,the original value we only store once,only read once.
Solve the problem:
1)ya100`s data is quite big we don`t have enough memory to load all Values to memory.
2)on realtime mode ,data is change Frequent , The cache is invalidated Frequent by append or update. build Cache will take a lot of times and io;
3)the Original value  is a string type.  whene sorting or grouping ,thed string value need a lot of memory and need lot of cpu time to calculate hashcode \compare \equals ,But label is number  is fast.
4)the label is number ,it`s type maybe short ,or maybe a byte ,or may be integer whitch depending on the max number of the label.

two-phase search
Original:
1)group by order by use original value,the real value may be is a string type,may be more larger ,the real value maybe  need a lot of io 
2)compare by string is slowly then compare by integer
Our improve:
1)we split one search into multy-phase search
2)the first search we only search the field that use for order by ,group by
3)the first search we doesn`t need to read the original value(the real value),we only need to read the docid and label for order by group by.
4)when we finish all the order by and group by ,may be we only need to return Top n records .so we start next to search to get the Top n records original value.
Solve the problem:
5)reduce io ,read original take a lot of disk io
6)reduce network io (for merger)
7)most of the field has repeated value, the repeated only need to read once
the group by filed only need to read the origin once by label where display to user.
8)most of the search only need to display on Top n (n<=100) results, so most of the original value could be skip.











 
How to install ya100 
 
 
1)Add the depend jar 
 
export SPARK_CLASSPATH=$SPARK_CLASSPATH:/data/ycloud/spark_ydb/ya100.jar
 
You can get the ya100 jar from this link



 
https://github.com/ycloudnet/ya100/blob/master/v1.0.8/ya100-1.0.8.jar



 
 
 
2)Start thriftserver 
 
./start-thriftserver.sh     --master yarn-client  --executor-memory 990m --executor-cores 2 --num-executors 16
 
 
 
Our suggest spark version is 6 
 
3)Config the ya100 function 
 
create  function Yfilter as 'cn.net.ycloud.ydb.handle.fun.Yfilter';   
 
create  function Ytop10000 as 'cn.net.ycloud.ydb.handle.fun.Ytop10000';  
 
create  function Ycombine as 'cn.net.ycloud.ydb.handle.fun.Ycombine'; 
 
create  function Ycount as 'cn.net.ycloud.ydb.handle.fun.Ycount';  
 
create  function Ymax as 'cn.net.ycloud.ydb.handle.fun.Ymax'; 
 
create  function Ymin as 'cn.net.ycloud.ydb.handle.fun.Ymin';   
 
create  function Yavg as 'cn.net.ycloud.ydb.handle.fun.Yavg';    
 
create  function Ysum as 'cn.net.ycloud.ydb.handle.fun.Ysum'; 
 
create  function Ymaxstring as 'cn.net.ycloud.ydb.handle.fun.Ymaxstring'; 
 
create  function Yminstring as 'cn.net.ycloud.ydb.handle.fun.Yminstring¡®; 
 
 
YA100 support data type 
 

 
How to create ya100 table on spark Sql 
 
CREATE external  table spark_ya100(  
 
 phonenum bigint, usernick string, ydb_sex string,  ydb_province string, ydb_grade string, ydb_age string, ydb_blood string, ydb_zhiye string, ydb_earn string, ydb_prefer string, ydb_consume string, ydb_day string, amtdouble double,amtlong int, content string, ydbpartion string, ya100_pipe string 
 
) partitioned by (dt string) 
 
STORED BY 'cn.net.ycloud.ydb.handle.Ya100StorageHandler' 
 
LOCATION  '/data/ydb/shu_ya100' 
 
TBLPROPERTIES( 
 
""ya100.handler.table.name""=""ydb_example_shu"", 
 
""ya100.handler.schema""=""phonenum long,usernick string,ydb_sex string,ydb_province string,ydb_grade string,ydb_age string,ydb_blood string,ydb_zhiye string,ydb_earn string,ydb_prefer string,ydb_consume string,ydb_day string, amtdouble double,amtlong int,content textcjk,ydbpartion string,ya100_pipe string"" 
 
)  
 
//×¢£¬ydbpartionÎªydbµÄ·ÖÇø×Ö¶ÎÓëya100_pipe Îªya100µÄ¹ÜµÀ×Ö¶Î£¬±ØÐëÒª¶¼Òª´´½¨£¬·ñÔòÓÐÐ©¹¦ÄÜ»á±»ÏÞÖÆÊ¹ÓÃ¡£
 
ya100.handler.table.nameÎªÔ¤ÁôµÄkey,²»Í¬µÄ±íÖ®¼äÒªÇø·Ö¿ªÀ´
 
ya100.handler.schema ÎªÄÚÖÃµÄË÷ÒýµÄÊý¾ÝÀàÐÍ£¬Ãû×ÖÒªÓësparkµÄ×Ö¶ÎÃû³ÆÒ»ÖÂ¡£ 
 

 
How to Import data into ya100 table


 

1)Set up import params 
 
set hive.mapred.supports.subdirectories=true; 
 
set mapred.min.split.size=2147483648;
 
×¢£º 
 
ya100ÒòÎªÉú³ÉµÄ×îÖÕ²»ÊÇÒ»¸öÎÄ¼þ£¬¶øÊÇÒ»¸öÄ¿Â¼£¬Õâ¸öÄ¿Â¼ÏÂ»áÓÐË÷Òý£¬ËùÒÔÒªÍ¨¹ýsubdirectories²ÎÊýÉèÖÃsparkÖ§³Ö×ÓÄ¿Â¼£¬·ñÔòÏò±íÀïµ¼ÈëÊý¾Ý»á±¨´íÊ§°Ü¡£ 
 
set mapred.min.split.size=2147483648 ÊÇÎªÁË¿ØÖÆmapµÄ¸öÊý£¬·ÀÖ¹Éú³ÉµÄË÷ÒýÎÄ¼þÊýÁ¿Ì«¶à¡£ 
 
 
 
2)Begin import  
 
insert into table spark_ya100 partition (dt='1200million')
 
 select phonenum,usernick,ydb_sex,ydb_province,ydb_grade,ydb_age,ydb_blood,ydb_zhiye,ydb_earn,ydb_prefer,ydb_consume,ydb_day,amtdouble,amtlong,content,2,1 
 
from spark_txt where dt='1200million'; 
 
 
How to use ya100 to filter data 
 
 
1),basic example 1 
 
set ya100.spark.filter.ydb_example_shu=ydb_sex='Å®' or ydb_province='ÁÉÄþ' or ydb_day>='20151217'; 
 
ÓÐÐ©Ê±ºò£¬»áÓöµ½×ªÒåµÄÎÊÌâ£¬ÕâÀïµÄvalueÖµ£¬¿ÉÒÔ½øÐÐurlencode´¦Àí¡£
 
Èç£º
 
set ya100.spark.filter.ydb_example_shu=ydbpartion%3D%2720151110%27+and+%28ydb_sex%3D%27%E5%A5%B3%27+or+ydb_province%3D%27%E8%BE%BD%E5%AE%81%27+or+ydb_day%3E%3D%2720151217%27%29;
 
 
 
2)Filter example 2 
 
set ya100.spark.filter.ydb_example_shu=ydb_sex='Å®' or ydb_province='ÁÉÄþ' or ydb_day>='20151217'; 
 
select * from spark_ya100 where dt=¡®1200million¡¯ limit 10; 
 
 
 
 
 
 
Other filter example use case  
 
1)equal 
 
qq=¡®165162897¡¯
 
2)Support in 
 
Èç£ºindexnum in (1,2,3) 
 
3)>,<,>=,<=, 
 
clickcount >=10 and clickcount <=11 
 
4) range 
 
indexnum like '({0 TO 11}) '      ²»°üº¬±ß½çÖµ
 
indexnum like '([10 TO 11] ) '    °üº¬±ß½çÖµ
 
5) Unequal 
 
label<>'l_14' and label<>'l_15'  
 
6) 
 
indexnum='1' or indexnum='2' or (clickcount >=5 and clickcount <=11) 
 
 
 



 
How to use ya100 to make a top N sort 
 
set ya100.spark.top10000.ydb_example_shu=ydb_age desc limit 10; 
 
set ya100.spark.top10000.ydb_example_shu=ydb_sex desc,ydb_province limit 100£» 
 
set ya100.spark.top10000.ydb_example_shu=* limit 10; 
 
 
 
×¢Òâ£¬µ±Ç°°æ±¾limitµÄ×î´óÖµÎª10000 £¨Ã¿¸öË÷Òý×î´ó·µ»Ø1000£© 
 
ÁÐµÄÃû×Ö Ð´*±íÊ¾ ²»ÐèÒªÅÅÐò£¬µ«Ö»·µ»ØÇ°NÌõ¾Í¿ÉÒÔÁË 
 
 
 
SQL example  
 
 
 
set ya100.spark.top10000.ydb_example_shu=phonenum desc limit 10; 
 
select * from spark_ya100 where dt='1200million' order by phonenum desc limit 10; 
 
 
 

 
How to make group by or stat by ya100 
 
Example 1 
 
set ya100.spark.combine.ydb_example_shu=*,ydb_age; 
 
select ydb_province,Ycount('*',ya100_pipe), Ycount('ydb_age',ya100_pipe), Ymaxstring('ydb_age',ya100_pipe), Yminstring('ydb_age',ya100_pipe) 
 
 from spark_ya100  group by ydb_province limit 10 
 
  
 
Example 2 
 
set ya100.spark.combine.ydb_example_shu=amtlong; 
 
select ydb_sex, ydb_province,Ysum('amtlong',ya100_pipe) as cnt from spark_ya100  group by ydb_sex, ydb_province  limit 10 
 
 
 

 
Other SQL EXAMPLE


 
6¦1EXAMPLE COUNT(*) 
 
set ya100.spark.filter.ydb_example_shu=phonenum='13870282671'  and usernick='½­¾þÎõ'; 
 
set ya100.spark.combine.ydb_example_shu=*; 
 
set ya100.spark.top10000.ydb_example_shu=; 
 
select Ycount('*',ya100_pipe) from spark_ya100 where dt='100million' limit 10; 
 6¦1TOP N SORT EXAMPLE
 6¦1set ya100.spark.filter.ydb_example_shu= amtlong like '([1090 TO 1100] )' and amtdouble like '([1090 TO 1100] )'; 
 
set ya100.spark.combine.ydb_example_shu=; 
 
set ya100.spark.top10000.ydb_example_shu=phonenum desc limit 10; 
 
select * from spark_ya100 where dt='100million' order by phonenum desc limit 10;
 
SUM(amtlong)
 
set ya100.spark.filter.ydb_example_shu= amtlong like '([1090 TO 1100] )' and amtdouble like '([1090 TO 1100] )'; 
 
set ya100.spark.combine.ydb_example_shu=amtlong; 
 
set ya100.spark.top10000.ydb_example_shu=; 
 
select Ycount(¡®amtlong¡¯,ya100_pipe) ,Ysum(¡®amtlong¡¯,ya100_pipe) ,Yavg(¡®amtlong¡¯,ya100_pipe), Ymax(¡®amtlong¡¯,ya100_pipe), Ymin(¡®amtlong¡¯,ya100_pipe)  from spark_ya100 where dt=¡®100million¡¯  limit 10; 
 
 6¦1Top n sort 
 
set ya100.spark.filter.ydb_example_shu= amtlong like '([1090 TO 1100] )' and amtdouble like '([1090 TO 1100] )'; 
 
set ya100.spark.combine.ydb_example_shu=; 
 
set ya100.spark.top10000.ydb_example_shu= ydb_age desc, ydb_sex; 
 
select ydb_sex, ydb_age from spark_ya100 where dt='100million'  order  by  ydb_age desc, ydb_sex  limit 10;  
 6¦1Group by 
 
set ya100.spark.filter.ydb_example_shu=; 
 
set ya100.spark.combine.ydb_example_shu=ydb_age,ydb_blood,ydb_zhiye,ydb_earn,ydb_prefer,ydb_consume,ydb_day,amtlong; 
 
set ya100.spark.top10000.ydb_example_shu=; 
 
select ydb_sex,ydb_province,ydb_grade, Ymaxstring(¡®ydb_age¡¯,ya100_pipe),Ymaxstring(¡®ydb_blood¡¯,ya100_pipe),Ymaxstring(¡®ydb_zhiye¡¯,ya100_pipe),Ymaxstring(¡®ydb_earn¡¯,ya100_pipe),Ymaxstring(¡®ydb_prefer¡¯,ya100_pipe),Ymaxstring(¡®ydb_consume¡¯,ya100_pipe),Ymaxstring(¡®ydb_day¡¯,ya100_pipe),Ysum(¡®amtlong¡¯,ya100_pipe) as cnt from spark_ya100 where dt=¡®100million¡¯  group by ydb_sex,ydb_province,ydb_grade order by cnt desc limit 10 
 

 
Connect Ya100 to ydb  make real time data 
CREATE external  table spark_ydb(
 phonenum bigint, usernick string, ydb_sex string,  ydb_province string, ydb_grade string, ydb_age string, ydb_blood string, ydb_zhiye string, ydb_earn string, ydb_prefer string, ydb_consume string, ydb_day string, amtdouble double,amtlong int, content string, ydbpartion string, ya100_pipe string
)
STORED BY 'cn.net.ycloud.ydb.handle.Ya100StorageHandler'
LOCATION  '/data/ydb/shu_ydb'
TBLPROPERTIES(
""ya100.handler.table.name""=""ydb_example_shu"",
""ya100.handler.master""=""101.200.130.48:8080"",
""ya100.handler.columns.mapping""=""phonenum,usernick,ydb_sex,ydb_province,ydb_grade,ydb_age,ydb_blood,ydb_zhiye,ydb_earn,ydb_prefer,ydb_consume,ydb_day,amtdouble,amtlong,content,ydbpartion,ya100_pipe"")

×¢

ydbpartionÎªydbµÄ·ÖÇø×Ö¶Î£¬²éÑ¯µÄÊ±ºò±ØÐëÖ´ÐÐydbpartion,¶øspark±íÔòÃ»ÓÐ·ÖÇø×Ö¶Î 
 
ya100_pipe Îªya100µÄ¹ÜµÀ×Ö¶Î£¬±ØÐëÒª¶¼Òª´´½¨£¬·ñÔòÓÐÐ©¹¦ÄÜ»á±»ÏÞÖÆÊ¹ÓÃ¡£
 
ya100.handler.table.nameÎªydbÏµÍ³ÀïµÄ±íÃû×Ö
 
ya100.handler.columns.mapping ÎªydbÏµÍ³ÄÚµÄ±íÓësparkµÄ±íÖ®¼äµÄÓ³Éä¡£"
Akhil Das <akhil@sigmoidanalytics.com>,"Mon, 22 Feb 2016 17:42:46 +0530",Re: a new FileFormat 5x~100x faster than parquet,=?UTF-8?B?5byA5b+D5bu25bm0?= <muyannian@qq.com>,"Would be good to see the source code and the documentation in English.

Thanks
Best Regards


d.
amtlong
amtlong
x
s
;
by
ce
r
â€˜;
­—æ®µä¸Žya100_pipe ä¸ºya100çš„ç®¡é“å­—æ®µï¼Œå¿…é¡»è¦éƒ½è¦åˆ›å»ºï¼Œå¦åˆ™æœ‰äº›åŠŸèƒ½ä¼šè¢«é™åˆ¶ä½¿ç”¨ã€‚
åŒçš„è¡¨ä¹‹é—´è¦åŒºåˆ†å¼€æ¥
•çš„æ•°æ®ç±»åž‹ï¼Œåå­—è¦ä¸Žsparkçš„å­—æ®µåç§°ä¸€è‡´ã€‚
¸æ˜¯ä¸€ä¸ªæ–‡ä»¶ï¼Œè€Œæ˜¯ä¸€ä¸ªç›®å½•ï¼Œè¿™ä¸ªç›®å½•ä¸‹ä¼šæœ‰ç´¢å¼•ï¼Œæ‰€ä»¥è¦é€šè¿‡subdirectorieså‚æ•°è®¾ç½®spark
å‘è¡¨é‡Œå¯¼å…¥æ•°æ®ä¼šæŠ¥é”™å¤±è´¥ã€‚
§åˆ¶mapçš„ä¸ªæ•°ï¼Œé˜²æ­¢ç”Ÿæˆçš„ç´¢å¼•æ–‡ä»¶æ•°é‡å¤ªå¤šã€‚
iye,ydb_earn,ydb_prefer,ydb_consume,ydb_day,amtdouble,amtlong,content,2,1
vince='è¾½å®'
è½¬ä¹‰çš„é—®é¢˜ï¼Œè¿™é‡Œçš„valueå€¼ï¼Œå¯ä»¥è¿›è¡Œurlencodeå¤„ç†ã€‚
db_sex%3D%27%E5%A5%B3%27+or+ydb_province%3D%27%E8%BE%BD%E5%AE%81%27+or+ydb_day%3E%3D%2720151217%27%29;
vince='è¾½å®'
10;
•Œå€¼
€¼
11)
t
š„æœ€å¤§å€¼ä¸º10000 ï¼ˆæ¯ä¸ªç´¢å¼•æœ€å¤§è¿”å›ž1000ï¼‰
éœ€è¦æŽ’åºï¼Œä½†åªè¿”å›žå‰Næ¡å°±å¯ä»¥äº†
O 1100] )'
'
gâ€™,ya100_pipe) ,Yavg(â€˜amtlongâ€™,ya100_pipe),
™,ya100_pipe)  from spark_ya100
'
by
n,ydb_prefer,ydb_consume,ydb_day,amtlong;
™,ya100_pipe),
b_zhiyeâ€™,ya100_pipe),
_preferâ€™,ya100_pipe),
ydb_dayâ€™,ya100_pipe),Ysum(â€˜amtlongâ€™,ya100_pipe)
pe
,ydb_grade,ydb_age,ydb_blood,ydb_zhiye,ydb_earn,ydb_prefer,ydb_consume,ydb_day,amtdouble,amtlong,content,ydbpartion,ya100_pipe"")
ŒæŸ¥è¯¢çš„æ—¶å€™å¿…é¡»æ‰§è¡Œydbpartion,è€Œsparkè¡¨åˆ™æ²¡æœ‰åˆ†åŒºå­—æ®µ
¼Œå¿…é¡»è¦éƒ½è¦åˆ›å»ºï¼Œå¦åˆ™æœ‰äº›åŠŸèƒ½ä¼šè¢«é™åˆ¶ä½¿ç”¨ã€‚
è¡¨åå­—
š„è¡¨ä¸Žsparkçš„è¡¨ä¹‹é—´çš„æ˜ å°„ã€‚
"
=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"Mon, 22 Feb 2016 13:27:41 +0100",Builds are failing,"""dev@spark.apache.org"" <dev@spark.apache.org>","Just in case you missed this:
https://issues.apache.org/jira/browse/SPARK-13431

Builds are failing with 'Method code too large' in the ""shading"" step with
Maven.

iulian

-- 

--
Iulian Dragos

------
Reactive Apps on the JVM
www.typesafe.com
"
"""=?gb18030?B?v6rQxNHTxOo=?="" <muyannian@qq.com>","Mon, 22 Feb 2016 20:27:34 +0800","=?gb18030?B?u9i4tKO6IGEgbmV3IEZpbGVGb3JtYXQgNXh+MTAw?=
 =?gb18030?B?eCBmYXN0ZXIgdGhhbiBwYXJxdWV0?=","""=?gb18030?B?QWtoaWwgRGFz?="" <akhil@sigmoidanalytics.com>","if apache enjoy this project , of course we provider the source code .

BUt if apache dislike the porject , we had continue to improve the project by myself .

ya100 and ydb  max process data is 180billions rows data per day for neary realtime import .

because of index ,we make the search 10 secondes return in 1800billion (10days) rows data.




------------------ Ô­Ê¼ÓÊ¼þ ------------------
·¢¼þÈË: ""Akhil Das"";<akhil@sigmoidanalytics.com>;
·¢ËÍÊ±¼ä: 2016Äê2ÔÂ22ÈÕ(ÐÇÆÚÒ») ÍíÉÏ8:42
ÊÕ¼þÈË: ""¿ªÐÄÑÓÄê""<muyannian@qq.com>; 
³­ËÍ: ""user""<user@spark.apache.org>; ""dev""<dev@spark.apache.org>; 
Ö÷Ìâ: Re: a new FileFormat 5x~100x faster than parquet



Would be good to see the source code and the documentation in English.


ThanksBest Regards



 
On Mon, Feb 22, 2016 at 4:44 PM, ¿ªÐÄÑÓÄê <muyannian@qq.com> wrote:
Ya100 is a FileFormat 5x~100x  faster than parquet¡£
we can get ya100 from this link https://github.com/ycloudnet/ya100/tree/master/v1.0.8




 
1.we used the inverted index£¬so we skip the rows that we does need.

  for example  the trade log search SQL

     

        select 
 
£¨1£©phonenum,usernick,ydb_grade,ydb_age,ydb_blood,ydb_zhiye,ydb_earn,ydb_day,amtlong  
 
        from spark_txt where   
 
£¨2£©tradeid=' 2014012213870282671'
 
        limit 10;  
 





     this sql is compose by two part

     (1)the part 1 is return the result which has 9 columns

     (2) the part 2 is the filter condition ,filter by tradeid

      


      let guess which plan is faster

     plan A :first read all the 9 columns result then filter by tradeid

     plan B: first filter by tradeid ,then we read the match 9 columns result.




    Ya100 choose plan B




     contrast  performance Ya100`index with parquet








 2.TOP N sort ,the non sort column we doesn`t read it until the last 


  for example  we sort by the logtime

 

    select 
 
£¨1£©phonenum,usernick,ydb_grade,ydb_age,ydb_blood,ydb_zhiye,ydb_earn,ydb_day,amtlong  
 
    from spark_txt 
 
£¨2£©order by logtime desc 
 
    limit 10;  
 


  this sql is compose by two part
     (1)the part 1 is return the result which has 9 columns

     (2) the part 2 is the column need to sort

      


      let guess which plan is faster
     plan A :first read all the 9 columns result then sort by logtime
     plan B: first sort by logtime  ,then we read the match 9 columns result.




       Ya100 choose plan B




     contrast  performance Ya100`lazy read with parquet


3.we used label instead of the original value for grouping and sorting



1).General situation,the data has a lot of repeat value,for exampe the sex file ,the age field .
2).if we store the original value ,that will weast a lot of storage.
so we make a small modify at original  value, Additional add a new filed called label.
make a unique value sort by fields, and then gave each term a unique  Number from begin to end  .
3).we use number value(we called label) instead of original  value.lable is store by fixed length. the file could be read by random read.
4).the label`s order is the same with dictionary  order .so if we do some calculation like order by or group by only need to  read the label. we don`t need to read the original value.
5).some field like sex field ,only have 2 different values.so we only use 2 bits(not 2 bytes) to store the label, it will save a lot of Disk io.
 when we finish all of the calculation, we translate label to original  value by a dictionary.
6)if a lots of rows have the same original value ,the original value we only store once,only read once.
Solve the problem:
1)ya100`s data is quite big we don`t have enough memory to load all Values to memory.
2)on realtime mode ,data is change Frequent , The cache is invalidated Frequent by append or update. build Cache will take a lot of times and io;
3)the Original value  is a string type.  whene sorting or grouping ,thed string value need a lot of memory and need lot of cpu time to calculate hashcode \compare \equals ,But label is number  is fast.
4)the label is number ,it`s type maybe short ,or maybe a byte ,or may be integer whitch depending on the max number of the label.

two-phase search
Original:
1)group by order by use original value,the real value may be is a string type,may be more larger ,the real value maybe  need a lot of io 
2)compare by string is slowly then compare by integer
Our improve:
1)we split one search into multy-phase search
2)the first search we only search the field that use for order by ,group by
3)the first search we doesn`t need to read the original value(the real value),we only need to read the docid and label for order by group by.
4)when we finish all the order by and group by ,may be we only need to return Top n records .so we start next to search to get the Top n records original value.
Solve the problem:
5)reduce io ,read original take a lot of disk io
6)reduce network io (for merger)
7)most of the field has repeated value, the repeated only need to read once
the group by filed only need to read the origin once by label where display to user.
8)most of the search only need to display on Top n (n<=100) results, so most of the original value could be skip.











 
How to install ya100 
 
 
1)Add the depend jar 
 
export SPARK_CLASSPATH=$SPARK_CLASSPATH:/data/ycloud/spark_ydb/ya100.jar
 
You can get the ya100 jar from this link



 
https://github.com/ycloudnet/ya100/blob/master/v1.0.8/ya100-1.0.8.jar



 
 
 
2)Start thriftserver 
 
./start-thriftserver.sh     --master yarn-client  --executor-memory 990m --executor-cores 2 --num-executors 16
 
 
 
Our suggest spark version is 6 
 
3)Config the ya100 function 
 
create  function Yfilter as 'cn.net.ycloud.ydb.handle.fun.Yfilter';   
 
create  function Ytop10000 as 'cn.net.ycloud.ydb.handle.fun.Ytop10000';  
 
create  function Ycombine as 'cn.net.ycloud.ydb.handle.fun.Ycombine'; 
 
create  function Ycount as 'cn.net.ycloud.ydb.handle.fun.Ycount';  
 
create  function Ymax as 'cn.net.ycloud.ydb.handle.fun.Ymax'; 
 
create  function Ymin as 'cn.net.ycloud.ydb.handle.fun.Ymin';   
 
create  function Yavg as 'cn.net.ycloud.ydb.handle.fun.Yavg';    
 
create  function Ysum as 'cn.net.ycloud.ydb.handle.fun.Ysum'; 
 
create  function Ymaxstring as 'cn.net.ycloud.ydb.handle.fun.Ymaxstring'; 
 
create  function Yminstring as 'cn.net.ycloud.ydb.handle.fun.Yminstring¡®; 
 
 
YA100 support data type 
 

 
How to create ya100 table on spark Sql 
 
CREATE external  table spark_ya100(  
 
 phonenum bigint, usernick string, ydb_sex string,  ydb_province string, ydb_grade string, ydb_age string, ydb_blood string, ydb_zhiye string, ydb_earn string, ydb_prefer string, ydb_consume string, ydb_day string, amtdouble double,amtlong int, content string, ydbpartion string, ya100_pipe string 
 
) partitioned by (dt string) 
 
STORED BY 'cn.net.ycloud.ydb.handle.Ya100StorageHandler' 
 
LOCATION  '/data/ydb/shu_ya100' 
 
TBLPROPERTIES( 
 
""ya100.handler.table.name""=""ydb_example_shu"", 
 
""ya100.handler.schema""=""phonenum long,usernick string,ydb_sex string,ydb_province string,ydb_grade string,ydb_age string,ydb_blood string,ydb_zhiye string,ydb_earn string,ydb_prefer string,ydb_consume string,ydb_day string, amtdouble double,amtlong int,content textcjk,ydbpartion string,ya100_pipe string"" 
 
)  
 
//×¢£¬ydbpartionÎªydbµÄ·ÖÇø×Ö¶ÎÓëya100_pipe Îªya100µÄ¹ÜµÀ×Ö¶Î£¬±ØÐëÒª¶¼Òª´´½¨£¬·ñÔòÓÐÐ©¹¦ÄÜ»á±»ÏÞÖÆÊ¹ÓÃ¡£
 
ya100.handler.table.nameÎªÔ¤ÁôµÄkey,²»Í¬µÄ±íÖ®¼äÒªÇø·Ö¿ªÀ´
 
ya100.handler.schema ÎªÄÚÖÃµÄË÷ÒýµÄÊý¾ÝÀàÐÍ£¬Ãû×ÖÒªÓësparkµÄ×Ö¶ÎÃû³ÆÒ»ÖÂ¡£ 
 

 
How to Import data into ya100 table


 

1)Set up import params 
 
set hive.mapred.supports.subdirectories=true; 
 
set mapred.min.split.size=2147483648;
 
×¢£º 
 
ya100ÒòÎªÉú³ÉµÄ×îÖÕ²»ÊÇÒ»¸öÎÄ¼þ£¬¶øÊÇÒ»¸öÄ¿Â¼£¬Õâ¸öÄ¿Â¼ÏÂ»áÓÐË÷Òý£¬ËùÒÔÒªÍ¨¹ýsubdirectories²ÎÊýÉèÖÃsparkÖ§³Ö×ÓÄ¿Â¼£¬·ñÔòÏò±íÀïµ¼ÈëÊý¾Ý»á±¨´íÊ§°Ü¡£ 
 
set mapred.min.split.size=2147483648 ÊÇÎªÁË¿ØÖÆmapµÄ¸öÊý£¬·ÀÖ¹Éú³ÉµÄË÷ÒýÎÄ¼þÊýÁ¿Ì«¶à¡£ 
 
 
 
2)Begin import  
 
insert into table spark_ya100 partition (dt='1200million')
 
 select phonenum,usernick,ydb_sex,ydb_province,ydb_grade,ydb_age,ydb_blood,ydb_zhiye,ydb_earn,ydb_prefer,ydb_consume,ydb_day,amtdouble,amtlong,content,2,1 
 
from spark_txt where dt='1200million'; 
 
 
How to use ya100 to filter data 
 
 
1),basic example 1 
 
set ya100.spark.filter.ydb_example_shu=ydb_sex='Å®' or ydb_province='ÁÉÄþ' or ydb_day>='20151217'; 
 
ÓÐÐ©Ê±ºò£¬»áÓöµ½×ªÒåµÄÎÊÌâ£¬ÕâÀïµÄvalueÖµ£¬¿ÉÒÔ½øÐÐurlencode´¦Àí¡£
 
Èç£º
 
set ya100.spark.filter.ydb_example_shu=ydbpartion%3D%2720151110%27+and+%28ydb_sex%3D%27%E5%A5%B3%27+or+ydb_province%3D%27%E8%BE%BD%E5%AE%81%27+or+ydb_day%3E%3D%2720151217%27%29;
 
 
 
2)Filter example 2 
 
set ya100.spark.filter.ydb_example_shu=ydb_sex='Å®' or ydb_province='ÁÉÄþ' or ydb_day>='20151217'; 
 
select * from spark_ya100 where dt=¡®1200million¡¯ limit 10; 
 
 
 
 
 
 
Other filter example use case  
 
1)equal 
 
qq=¡®165162897¡¯
 
2)Support in 
 
Èç£ºindexnum in (1,2,3) 
 
3)>,<,>=,<=, 
 
clickcount >=10 and clickcount <=11 
 
4) range 
 
indexnum like '({0 TO 11}) '      ²»°üº¬±ß½çÖµ
 
indexnum like '([10 TO 11] ) '    °üº¬±ß½çÖµ
 
5) Unequal 
 
label<>'l_14' and label<>'l_15'  
 
6) 
 
indexnum='1' or indexnum='2' or (clickcount >=5 and clickcount <=11) 
 
 
 



 
How to use ya100 to make a top N sort 
 
set ya100.spark.top10000.ydb_example_shu=ydb_age desc limit 10; 
 
set ya100.spark.top10000.ydb_example_shu=ydb_sex desc,ydb_province limit 100£» 
 
set ya100.spark.top10000.ydb_example_shu=* limit 10; 
 
 
 
×¢Òâ£¬µ±Ç°°æ±¾limitµÄ×î´óÖµÎª10000 £¨Ã¿¸öË÷Òý×î´ó·µ»Ø1000£© 
 
ÁÐµÄÃû×Ö Ð´*±íÊ¾ ²»ÐèÒªÅÅÐò£¬µ«Ö»·µ»ØÇ°NÌõ¾Í¿ÉÒÔÁË 
 
 
 
SQL example  
 
 
 
set ya100.spark.top10000.ydb_example_shu=phonenum desc limit 10; 
 
select * from spark_ya100 where dt='1200million' order by phonenum desc limit 10; 
 
 
 

 
How to make group by or stat by ya100 
 
Example 1 
 
set ya100.spark.combine.ydb_example_shu=*,ydb_age; 
 
select ydb_province,Ycount('*',ya100_pipe), Ycount('ydb_age',ya100_pipe), Ymaxstring('ydb_age',ya100_pipe), Yminstring('ydb_age',ya100_pipe) 
 
 from spark_ya100  group by ydb_province limit 10 
 
  
 
Example 2 
 
set ya100.spark.combine.ydb_example_shu=amtlong; 
 
select ydb_sex, ydb_province,Ysum('amtlong',ya100_pipe) as cnt from spark_ya100  group by ydb_sex, ydb_province  limit 10 
 
 
 

 
Other SQL EXAMPLE


 
6¦1EXAMPLE COUNT(*) 
 
set ya100.spark.filter.ydb_example_shu=phonenum='13870282671'  and usernick='½­¾þÎõ'; 
 
set ya100.spark.combine.ydb_example_shu=*; 
 
set ya100.spark.top10000.ydb_example_shu=; 
 
select Ycount('*',ya100_pipe) from spark_ya100 where dt='100million' limit 10; 
 6¦1TOP N SORT EXAMPLE
 6¦1set ya100.spark.filter.ydb_example_shu= amtlong like '([1090 TO 1100] )' and amtdouble like '([1090 TO 1100] )'; 
 
set ya100.spark.combine.ydb_example_shu=; 
 
set ya100.spark.top10000.ydb_example_shu=phonenum desc limit 10; 
 
select * from spark_ya100 where dt='100million' order by phonenum desc limit 10;
 
SUM(amtlong)
 
set ya100.spark.filter.ydb_example_shu= amtlong like '([1090 TO 1100] )' and amtdouble like '([1090 TO 1100] )'; 
 
set ya100.spark.combine.ydb_example_shu=amtlong; 
 
set ya100.spark.top10000.ydb_example_shu=; 
 
select Ycount(¡®amtlong¡¯,ya100_pipe) ,Ysum(¡®amtlong¡¯,ya100_pipe) ,Yavg(¡®amtlong¡¯,ya100_pipe), Ymax(¡®amtlong¡¯,ya100_pipe), Ymin(¡®amtlong¡¯,ya100_pipe)  from spark_ya100 where dt=¡®100million¡¯  limit 10; 
 
 6¦1Top n sort 
 
set ya100.spark.filter.ydb_example_shu= amtlong like '([1090 TO 1100] )' and amtdouble like '([1090 TO 1100] )'; 
 
set ya100.spark.combine.ydb_example_shu=; 
 
set ya100.spark.top10000.ydb_example_shu= ydb_age desc, ydb_sex; 
 
select ydb_sex, ydb_age from spark_ya100 where dt='100million'  order  by  ydb_age desc, ydb_sex  limit 10;  
 6¦1Group by 
 
set ya100.spark.filter.ydb_example_shu=; 
 
set ya100.spark.combine.ydb_example_shu=ydb_age,ydb_blood,ydb_zhiye,ydb_earn,ydb_prefer,ydb_consume,ydb_day,amtlong; 
 
set ya100.spark.top10000.ydb_example_shu=; 
 
select ydb_sex,ydb_province,ydb_grade, Ymaxstring(¡®ydb_age¡¯,ya100_pipe),Ymaxstring(¡®ydb_blood¡¯,ya100_pipe),Ymaxstring(¡®ydb_zhiye¡¯,ya100_pipe),Ymaxstring(¡®ydb_earn¡¯,ya100_pipe),Ymaxstring(¡®ydb_prefer¡¯,ya100_pipe),Ymaxstring(¡®ydb_consume¡¯,ya100_pipe),Ymaxstring(¡®ydb_day¡¯,ya100_pipe),Ysum(¡®amtlong¡¯,ya100_pipe) as cnt from spark_ya100 where dt=¡®100million¡¯  group by ydb_sex,ydb_province,ydb_grade order by cnt desc limit 10 
 

 
Connect Ya100 to ydb  make real time data 
CREATE external  table spark_ydb(
 phonenum bigint, usernick string, ydb_sex string,  ydb_province string, ydb_grade string, ydb_age string, ydb_blood string, ydb_zhiye string, ydb_earn string, ydb_prefer string, ydb_consume string, ydb_day string, amtdouble double,amtlong int, content string, ydbpartion string, ya100_pipe string
)
STORED BY 'cn.net.ycloud.ydb.handle.Ya100StorageHandler'
LOCATION  '/data/ydb/shu_ydb'
TBLPROPERTIES(
""ya100.handler.table.name""=""ydb_example_shu"",
""ya100.handler.master""=""101.200.130.48:8080"",
""ya100.handler.columns.mapping""=""phonenum,usernick,ydb_sex,ydb_province,ydb_grade,ydb_age,ydb_blood,ydb_zhiye,ydb_earn,ydb_prefer,ydb_consume,ydb_day,amtdouble,amtlong,content,ydbpartion,ya100_pipe"")

×¢

ydbpartionÎªydbµÄ·ÖÇø×Ö¶Î£¬²éÑ¯µÄÊ±ºò±ØÐëÖ´ÐÐydbpartion,¶øspark±íÔòÃ»ÓÐ·ÖÇø×Ö¶Î 
 
ya100_pipe Îªya100µÄ¹ÜµÀ×Ö¶Î£¬±ØÐëÒª¶¼Òª´´½¨£¬·ñÔòÓÐÐ©¹¦ÄÜ»á±»ÏÞÖÆÊ¹ÓÃ¡£
 
ya100.handler.table.nameÎªydbÏµÍ³ÀïµÄ±íÃû×Ö
 
ya100.handler.columns.mapping ÎªydbÏµÍ³ÄÚµÄ±íÓësparkµÄ±íÖ®¼äµÄÓ³Éä¡£"
Patrick Woody <patrick.woody1@gmail.com>,"Mon, 22 Feb 2016 08:19:45 -0500",Re: Spark 1.6.1,Michael Armbrust <michael@databricks.com>,"Hey Michael,

Any update on a first cut of the RC?

Thanks!
-Pat


"
"""=?gb18030?B?v6rQxNHTxOo=?="" <muyannian@qq.com>","Mon, 22 Feb 2016 22:12:02 +0800","=?gb18030?B?u9i4tKO6ILvYuLSjuiBhIG5ldyBGaWxlRm9ybWF0?=
 =?gb18030?B?IDV4fjEwMHggZmFzdGVyIHRoYW4gcGFycXVldA==?=","""=?gb18030?B?R2F2aW4gWXVl?="" <yue.yuanyuan@gmail.com>","1.ya100 is not only the invert index  ,but also include the TOP N sort lazy read,also include label .
2.our test on ya100 and parquet is on this link address https://github.com/ycloudnet/ya100/blob/master/v1.0.8/ya100%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.docx?raw=true

3.you are right ,the load into ya100 is longer then parquet ,but the query time is very fast.
4. the code is already available £¬but i`m sorry ya100 and ydb is a          Commercial product now by our company (called ycloud),but i think if apache like it ,the source code is not a problem.

our invert index indeed is apache lucene-but a quite different useage, such as solr or es.
we can make a test on ya100(we support java jar),it is really very faster than  parquet          especially                     on sort ¡¢group by ¡¢filter and so on.


provide more information? what any thing other infomation do you want ,i can provide it  tomorrow.


------------------ Ô­Ê¼ÓÊ¼þ ------------------
·¢¼þÈË: ""Gavin Yue"";<yue.yuanyuan@gmail.com>;
·¢ËÍÊ±¼ä: 2016Äê2ÔÂ22ÈÕ(ÐÇÆÚÒ») ÍíÉÏ9:33
ÊÕ¼þÈË: ""¿ªÐÄÑÓÄê""<muyannian@qq.com>; 
³­ËÍ: ""Akhil Das""<akhil@sigmoidanalytics.com>; ""user""<user@spark.apache.org>; ""dev""<dev@spark.apache.org>; 
Ö÷Ìâ: Re: »Ø¸´£º a new FileFormat 5x~100x faster than parquet




I recommend you provide more information. Using inverted index certainly speed up the query time if hitting the index, but it would take longer to create and insert.  


Is the source code not available at this moment? 


Thanks 
Gavin 

On Feb 22, 2016, at 20:27, ¿ªÐÄÑÓÄê <muyannian@qq.com> wrote:


if apache enjoy this project , of course we provider the source code .

BUt if apache dislike the porject , we had continue to improve the project by myself .

ya100 and ydb  max process data is 180billions rows data per day for neary realtime import .

because of index ,we make the search 10 secondes return in 1800billion (10days) rows data.




------------------ Ô­Ê¼ÓÊ¼þ ------------------
·¢¼þÈË: ""Akhil Das"";<akhil@sigmoidanalytics.com>;
·¢ËÍÊ±¼ä: 2016Äê2ÔÂ22ÈÕ(ÐÇÆÚÒ») ÍíÉÏ8:42
ÊÕ¼þÈË: ""¿ªÐÄÑÓÄê""<muyannian@qq.com>; 
³­ËÍ: ""user""<user@spark.apache.org>; ""dev""<dev@spark.apache.org>; 
Ö÷Ìâ: Re: a new FileFormat 5x~100x faster than parquet



Would be good to see the source code and the documentation in English.


ThanksBest Regards



 
On Mon, Feb 22, 2016 at 4:44 PM, ¿ªÐÄÑÓÄê <muyannian@qq.com> wrote:
Ya100 is a FileFormat 5x~100x  faster than parquet¡£
we can get ya100 from this link https://github.com/ycloudnet/ya100/tree/master/v1.0.8


<A2D2FBB6@C4EFD745.B6FECA56>

 
1.we used the inverted index£¬so we skip the rows that we does need.

  for example  the trade log search SQL

     

        select 
 
£¨1£©phonenum,usernick,ydb_grade,ydb_age,ydb_blood,ydb_zhiye,ydb_earn,ydb_day,amtlong  
 
        from spark_txt where   
 
£¨2£©tradeid=' 2014012213870282671'
 
        limit 10;  
 





     this sql is compose by two part

     (1)the part 1 is return the result which has 9 columns

     (2) the part 2 is the filter condition ,filter by tradeid

      


      let guess which plan is faster

     plan A :first read all the 9 columns result then filter by tradeid

     plan B: first filter by tradeid ,then we read the match 9 columns result.




    Ya100 choose plan B




     contrast  performance Ya100`index with parquet



<60C62790@C4EFD745.B6FECA56>




 2.TOP N sort ,the non sort column we doesn`t read it until the last 


  for example  we sort by the logtime

 

    select 
 
£¨1£©phonenum,usernick,ydb_grade,ydb_age,ydb_blood,ydb_zhiye,ydb_earn,ydb_day,amtlong  
 
    from spark_txt 
 
£¨2£©order by logtime desc 
 
    limit 10;  
 


  this sql is compose by two part
     (1)the part 1 is return the result which has 9 columns

     (2) the part 2 is the column need to sort

      


      let guess which plan is faster
     plan A :first read all the 9 columns result then sort by logtime
     plan B: first sort by logtime  ,then we read the match 9 columns result.




       Ya100 choose plan B




     contrast  performance Ya100`lazy read with parquet
<88FABE61@C4EFD745.B6FECA56>

3.we used label instead of the original value for grouping and sorting

<C2F6865D@C4EFD745.B6FECA56>

1).General situation,the data has a lot of repeat value,for exampe the sex file ,the age field .
2).if we store the original value ,that will weast a lot of storage.
so we make a small modify at original  value, Additional add a new filed called label.
make a unique value sort by fields, and then gave each term a unique  Number from begin to end  .
3).we use number value(we called label) instead of original  value.lable is store by fixed length. the file could be read by random read.
4).the label`s order is the same with dictionary  order .so if we do some calculation like order by or group by only need to  read the label. we don`t need to read the original value.
5).some field like sex field ,only have 2 different values.so we only use 2 bits(not 2 bytes) to store the label, it will save a lot of Disk io.
 when we finish all of the calculation, we translate label to original  value by a dictionary.
6)if a lots of rows have the same original value ,the original value we only store once,only read once.
Solve the problem:
1)ya100`s data is quite big we don`t have enough memory to load all Values to memory.
2)on realtime mode ,data is change Frequent , The cache is invalidated Frequent by append or update. build Cache will take a lot of times and io;
3)the Original value  is a string type.  whene sorting or grouping ,thed string value need a lot of memory and need lot of cpu time to calculate hashcode \compare \equals ,But label is number  is fast.
4)the label is number ,it`s type maybe short ,or maybe a byte ,or may be integer whitch depending on the max number of the label.

two-phase search
Original:
1)group by order by use original value,the real value may be is a string type,may be more larger ,the real value maybe  need a lot of io 
2)compare by string is slowly then compare by integer
Our improve:
1)we split one search into multy-phase search
2)the first search we only search the field that use for order by ,group by
3)the first search we doesn`t need to read the original value(the real value),we only need to read the docid and label for order by group by.
4)when we finish all the order by and group by ,may be we only need to return Top n records .so we start next to search to get the Top n records original value.
Solve the problem:
5)reduce io ,read original take a lot of disk io
6)reduce network io (for merger)
7)most of the field has repeated value, the repeated only need to read once
the group by filed only need to read the origin once by label where display to user.
8)most of the search only need to display on Top n (n<=100) results, so most of the original value could be skip.
<D328C2B9@C4EFD745.B6FECA56>


<6694C613@C4EFD745.B6FECA56>


<F21D52B8@C4EFD745.B6FECA56>

<2CBE257B@C4EFD745.B6FECA56>


 
How to install ya100 
 
 
1)Add the depend jar 
 
export SPARK_CLASSPATH=$SPARK_CLASSPATH:/data/ycloud/spark_ydb/ya100.jar
 
You can get the ya100 jar from this link



 
https://github.com/ycloudnet/ya100/blob/master/v1.0.8/ya100-1.0.8.jar



 
 
 
2)Start thriftserver 
 
./start-thriftserver.sh     --master yarn-client  --executor-memory 990m --executor-cores 2 --num-executors 16
 
 
 
Our suggest spark version is 6 
 
3)Config the ya100 function 
 
create  function Yfilter as 'cn.net.ycloud.ydb.handle.fun.Yfilter';   
 
create  function Ytop10000 as 'cn.net.ycloud.ydb.handle.fun.Ytop10000';  
 
create  function Ycombine as 'cn.net.ycloud.ydb.handle.fun.Ycombine'; 
 
create  function Ycount as 'cn.net.ycloud.ydb.handle.fun.Ycount';  
 
create  function Ymax as 'cn.net.ycloud.ydb.handle.fun.Ymax'; 
 
create  function Ymin as 'cn.net.ycloud.ydb.handle.fun.Ymin';   
 
create  function Yavg as 'cn.net.ycloud.ydb.handle.fun.Yavg';    
 
create  function Ysum as 'cn.net.ycloud.ydb.handle.fun.Ysum'; 
 
create  function Ymaxstring as 'cn.net.ycloud.ydb.handle.fun.Ymaxstring'; 
 
create  function Yminstring as 'cn.net.ycloud.ydb.handle.fun.Yminstring¡®; 
 
 
YA100 support data type 
 <30907791@C4EFD745.B6FECA56>

 
How to create ya100 table on spark Sql 
 
CREATE external  table spark_ya100(  
 
 phonenum bigint, usernick string, ydb_sex string,  ydb_province string, ydb_grade string, ydb_age string, ydb_blood string, ydb_zhiye string, ydb_earn string, ydb_prefer string, ydb_consume string, ydb_day string, amtdouble double,amtlong int, content string, ydbpartion string, ya100_pipe string 
 
) partitioned by (dt string) 
 
STORED BY 'cn.net.ycloud.ydb.handle.Ya100StorageHandler' 
 
LOCATION  '/data/ydb/shu_ya100' 
 
TBLPROPERTIES( 
 
""ya100.handler.table.name""=""ydb_example_shu"", 
 
""ya100.handler.schema""=""phonenum long,usernick string,ydb_sex string,ydb_province string,ydb_grade string,ydb_age string,ydb_blood string,ydb_zhiye string,ydb_earn string,ydb_prefer string,ydb_consume string,ydb_day string, amtdouble double,amtlong int,content textcjk,ydbpartion string,ya100_pipe string"" 
 
)  
 
//×¢£¬ydbpartionÎªydbµÄ·ÖÇø×Ö¶ÎÓëya100_pipe Îªya100µÄ¹ÜµÀ×Ö¶Î£¬±ØÐëÒª¶¼Òª´´½¨£¬·ñÔòÓÐÐ©¹¦ÄÜ»á±»ÏÞÖÆÊ¹ÓÃ¡£
 
ya100.handler.table.nameÎªÔ¤ÁôµÄkey,²»Í¬µÄ±íÖ®¼äÒªÇø·Ö¿ªÀ´
 
ya100.handler.schema ÎªÄÚÖÃµÄË÷ÒýµÄÊý¾ÝÀàÐÍ£¬Ãû×ÖÒªÓësparkµÄ×Ö¶ÎÃû³ÆÒ»ÖÂ¡£ 
 

 
How to Import data into ya100 table


 

1)Set up import params 
 
set hive.mapred.supports.subdirectories=true; 
 
set mapred.min.split.size=2147483648;
 
×¢£º 
 
ya100ÒòÎªÉú³ÉµÄ×îÖÕ²»ÊÇÒ»¸öÎÄ¼þ£¬¶øÊÇÒ»¸öÄ¿Â¼£¬Õâ¸öÄ¿Â¼ÏÂ»áÓÐË÷Òý£¬ËùÒÔÒªÍ¨¹ýsubdirectories²ÎÊýÉèÖÃsparkÖ§³Ö×ÓÄ¿Â¼£¬·ñÔòÏò±íÀïµ¼ÈëÊý¾Ý»á±¨´íÊ§°Ü¡£ 
 
set mapred.min.split.size=2147483648 ÊÇÎªÁË¿ØÖÆmapµÄ¸öÊý£¬·ÀÖ¹Éú³ÉµÄË÷ÒýÎÄ¼þÊýÁ¿Ì«¶à¡£ 
 
 
 
2)Begin import  
 
insert into table spark_ya100 partition (dt='1200million')
 
 select phonenum,usernick,ydb_sex,ydb_province,ydb_grade,ydb_age,ydb_blood,ydb_zhiye,ydb_earn,ydb_prefer,ydb_consume,ydb_day,amtdouble,amtlong,content,2,1 
 
from spark_txt where dt='1200million'; 
 
 
How to use ya100 to filter data 
 
 
1),basic example 1 
 
set ya100.spark.filter.ydb_example_shu=ydb_sex='Å®' or ydb_province='ÁÉÄþ' or ydb_day>='20151217'; 
 
ÓÐÐ©Ê±ºò£¬»áÓöµ½×ªÒåµÄÎÊÌâ£¬ÕâÀïµÄvalueÖµ£¬¿ÉÒÔ½øÐÐurlencode´¦Àí¡£
 
Èç£º
 
set ya100.spark.filter.ydb_example_shu=ydbpartion%3D%2720151110%27+and+%28ydb_sex%3D%27%E5%A5%B3%27+or+ydb_province%3D%27%E8%BE%BD%E5%AE%81%27+or+ydb_day%3E%3D%2720151217%27%29;
 
 
 
2)Filter example 2 
 
set ya100.spark.filter.ydb_example_shu=ydb_sex='Å®' or ydb_province='ÁÉÄþ' or ydb_day>='20151217'; 
 
select * from spark_ya100 where dt=¡®1200million¡¯ limit 10; 
 
 
 
 
 
 
Other filter example use case  
 
1)equal 
 
qq=¡®165162897¡¯
 
2)Support in 
 
Èç£ºindexnum in (1,2,3) 
 
3)>,<,>=,<=, 
 
clickcount >=10 and clickcount <=11 
 
4) range 
 
indexnum like '({0 TO 11}) '      ²»°üº¬±ß½çÖµ
 
indexnum like '([10 TO 11] ) '    °üº¬±ß½çÖµ
 
5) Unequal 
 
label<>'l_14' and label<>'l_15'  
 
6) 
 
indexnum='1' or indexnum='2' or (clickcount >=5 and clickcount <=11) 
 
 
 

<EE93F720@C4EFD745.B6FECA56>

 
How to use ya100 to make a top N sort 
 
set ya100.spark.top10000.ydb_example_shu=ydb_age desc limit 10; 
 
set ya100.spark.top10000.ydb_example_shu=ydb_sex desc,ydb_province limit 100£» 
 
set ya100.spark.top10000.ydb_example_shu=* limit 10; 
 
 
 
×¢Òâ£¬µ±Ç°°æ±¾limitµÄ×î´óÖµÎª10000 £¨Ã¿¸öË÷Òý×î´ó·µ»Ø1000£© 
 
ÁÐµÄÃû×Ö Ð´*±íÊ¾ ²»ÐèÒªÅÅÐò£¬µ«Ö»·µ»ØÇ°NÌõ¾Í¿ÉÒÔÁË 
 
 
 
SQL example  
 
 
 
set ya100.spark.top10000.ydb_example_shu=phonenum desc limit 10; 
 
select * from spark_ya100 where dt='1200million' order by phonenum desc limit 10; 
 
 
 

 
How to make group by or stat by ya100 
 
Example 1 
 
set ya100.spark.combine.ydb_example_shu=*,ydb_age; 
 
select ydb_province,Ycount('*',ya100_pipe), Ycount('ydb_age',ya100_pipe), Ymaxstring('ydb_age',ya100_pipe), Yminstring('ydb_age',ya100_pipe) 
 
 from spark_ya100  group by ydb_province limit 10 
 
  
 
Example 2 
 
set ya100.spark.combine.ydb_example_shu=amtlong; 
 
select ydb_sex, ydb_province,Ysum('amtlong',ya100_pipe) as cnt from spark_ya100  group by ydb_sex, ydb_province  limit 10 
 
 
 

 
Other SQL EXAMPLE


 
6¦1EXAMPLE COUNT(*) 
 
set ya100.spark.filter.ydb_example_shu=phonenum='13870282671'  and usernick='½­¾þÎõ'; 
 
set ya100.spark.combine.ydb_example_shu=*; 
 
set ya100.spark.top10000.ydb_example_shu=; 
 
select Ycount('*',ya100_pipe) from spark_ya100 where dt='100million' limit 10; 
 6¦1TOP N SORT EXAMPLE
 6¦1set ya100.spark.filter.ydb_example_shu= amtlong like '([1090 TO 1100] )' and amtdouble like '([1090 TO 1100] )'; 
 
set ya100.spark.combine.ydb_example_shu=; 
 
set ya100.spark.top10000.ydb_example_shu=phonenum desc limit 10; 
 
select * from spark_ya100 where dt='100million' order by phonenum desc limit 10;
 
SUM(amtlong)
 
set ya100.spark.filter.ydb_example_shu= amtlong like '([1090 TO 1100] )' and amtdouble like '([1090 TO 1100] )'; 
 
set ya100.spark.combine.ydb_example_shu=amtlong; 
 
set ya100.spark.top10000.ydb_example_shu=; 
 
select Ycount(¡®amtlong¡¯,ya100_pipe) ,Ysum(¡®amtlong¡¯,ya100_pipe) ,Yavg(¡®amtlong¡¯,ya100_pipe), Ymax(¡®amtlong¡¯,ya100_pipe), Ymin(¡®amtlong¡¯,ya100_pipe)  from spark_ya100 where dt=¡®100million¡¯  limit 10; 
 
 6¦1Top n sort 
 
set ya100.spark.filter.ydb_example_shu= amtlong like '([1090 TO 1100] )' and amtdouble like '([1090 TO 1100] )'; 
 
set ya100.spark.combine.ydb_example_shu=; 
 
set ya100.spark.top10000.ydb_example_shu= ydb_age desc, ydb_sex; 
 
select ydb_sex, ydb_age from spark_ya100 where dt='100million'  order  by  ydb_age desc, ydb_sex  limit 10;  
 6¦1Group by 
 
set ya100.spark.filter.ydb_example_shu=; 
 
set ya100.spark.combine.ydb_example_shu=ydb_age,ydb_blood,ydb_zhiye,ydb_earn,ydb_prefer,ydb_consume,ydb_day,amtlong; 
 
set ya100.spark.top10000.ydb_example_shu=; 
 
select ydb_sex,ydb_province,ydb_grade, Ymaxstring(¡®ydb_age¡¯,ya100_pipe),Ymaxstring(¡®ydb_blood¡¯,ya100_pipe),Ymaxstring(¡®ydb_zhiye¡¯,ya100_pipe),Ymaxstring(¡®ydb_earn¡¯,ya100_pipe),Ymaxstring(¡®ydb_prefer¡¯,ya100_pipe),Ymaxstring(¡®ydb_consume¡¯,ya100_pipe),Ymaxstring(¡®ydb_day¡¯,ya100_pipe),Ysum(¡®amtlong¡¯,ya100_pipe) as cnt from spark_ya100 where dt=¡®100million¡¯  group by ydb_sex,ydb_province,ydb_grade order by cnt desc limit 10 
 

 
Connect Ya100 to ydb  make real time data 
CREATE external  table spark_ydb(
 phonenum bigint, usernick string, ydb_sex string,  ydb_province string, ydb_grade string, ydb_age string, ydb_blood string, ydb_zhiye string, ydb_earn string, ydb_prefer string, ydb_consume string, ydb_day string, amtdouble double,amtlong int, content string, ydbpartion string, ya100_pipe string
)
STORED BY 'cn.net.ycloud.ydb.handle.Ya100StorageHandler'
LOCATION  '/data/ydb/shu_ydb'
TBLPROPERTIES(
""ya100.handler.table.name""=""ydb_example_shu"",
""ya100.handler.master""=""101.200.130.48:8080"",
""ya100.handler.columns.mapping""=""phonenum,usernick,ydb_sex,ydb_province,ydb_grade,ydb_age,ydb_blood,ydb_zhiye,ydb_earn,ydb_prefer,ydb_consume,ydb_day,amtdouble,amtlong,content,ydbpartion,ya100_pipe"")

×¢

ydbpartionÎªydbµÄ·ÖÇø×Ö¶Î£¬²éÑ¯µÄÊ±ºò±ØÐëÖ´ÐÐydbpartion,¶øspark±íÔòÃ»ÓÐ·ÖÇø×Ö¶Î 
 
ya100_pipe Îªya100µÄ¹ÜµÀ×Ö¶Î£¬±ØÐëÒª¶¼Òª´´½¨£¬·ñÔòÓÐÐ©¹¦ÄÜ»á±»ÏÞÖÆÊ¹ÓÃ¡£
 
ya100.handler.table.nameÎªydbÏµÍ³ÀïµÄ±íÃû×Ö
 
ya100.handler.columns.mapping ÎªydbÏµÍ³ÄÚµÄ±íÓësparkµÄ±íÖ®¼äµÄÓ³Éä¡£ 
 
 
 

<5C00AFEC@C4EFD745.B6FECA56>
<CDC3E8B4@C4EFD745.B6FECA56>

 
 <10C4E788@C4EFD745.B6FECA56>"
"""=?gb18030?B?v6rQxNHTxOo=?="" <muyannian@qq.com>","Mon, 22 Feb 2016 22:03:36 +0800","=?gb18030?B?u9i4tKO6ILvYuLSjuiBhIG5ldyBGaWxlRm9ybWF0?=
 =?gb18030?B?IDV4fjEwMHggZmFzdGVyIHRoYW4gcGFycXVldA==?=","""=?gb18030?B?R2F2aW4gWXVl?="" <yue.yuanyuan@gmail.com>","1.ya100 is not only the invert index  ,but also include the TOP N sort lazy read,also include label .
2.on the attache file is our test on ya100 and parquet
3.you are right ,the load into ya100 is longer then parquet ,but the query time is very fast.
4. the code is already available £¬but i`m sorry ya100 and ydb is a          Commercial product now by our company (called ycloud),but i think if apache like it ,the source code is not a problem.

our invert index indeed is apache lucene-but a quite different useage, such as solr or es.
we can make a test on ya100(we support java jar),it is really very faster than  parquet          especially                     on sort ¡¢group by ¡¢filter and so on.


provide more information? what any thing other infomation do you want ,i can provide it  tomorrow.


------------------ Ô­Ê¼ÓÊ¼þ ------------------
·¢¼þÈË: ""Gavin Yue"";<yue.yuanyuan@gmail.com>;
·¢ËÍÊ±¼ä: 2016Äê2ÔÂ22ÈÕ(ÐÇÆÚÒ») ÍíÉÏ9:33
ÊÕ¼þÈË: ""¿ªÐÄÑÓÄê""<muyannian@qq.com>; 
³­ËÍ: ""Akhil Das""<akhil@sigmoidanalytics.com>; ""user""<user@spark.apache.org>; ""dev""<dev@spark.apache.org>; 
Ö÷Ìâ: Re: »Ø¸´£º a new FileFormat 5x~100x faster than parquet




I recommend you provide more information. Using inverted index certainly speed up the query time if hitting the index, but it would take longer to create and insert.  


Is the source code not available at this moment? 


Thanks 
Gavin 

On Feb 22, 2016, at 20:27, ¿ªÐÄÑÓÄê <muyannian@qq.com> wrote:


if apache enjoy this project , of course we provider the source code .

BUt if apache dislike the porject , we had continue to improve the project by myself .

ya100 and ydb  max process data is 180billions rows data per day for neary realtime import .

because of index ,we make the search 10 secondes return in 1800billion (10days) rows data.




------------------ Ô­Ê¼ÓÊ¼þ ------------------
·¢¼þÈË: ""Akhil Das"";<akhil@sigmoidanalytics.com>;
·¢ËÍÊ±¼ä: 2016Äê2ÔÂ22ÈÕ(ÐÇÆÚÒ») ÍíÉÏ8:42
ÊÕ¼þÈË: ""¿ªÐÄÑÓÄê""<muyannian@qq.com>; 
³­ËÍ: ""user""<user@spark.apache.org>; ""dev""<dev@spark.apache.org>; 
Ö÷Ìâ: Re: a new FileFormat 5x~100x faster than parquet



Would be good to see the source code and the documentation in English.


ThanksBest Regards



 
On Mon, Feb 22, 2016 at 4:44 PM, ¿ªÐÄÑÓÄê <muyannian@qq.com> wrote:
Ya100 is a FileFormat 5x~100x  faster than parquet¡£
we can get ya100 from this link https://github.com/ycloudnet/ya100/tree/master/v1.0.8


<A2D2FBB6@C4EFD745.B6FECA56>

 
1.we used the inverted index£¬so we skip the rows that we does need.

  for example  the trade log search SQL

     

        select 
 
£¨1£©phonenum,usernick,ydb_grade,ydb_age,ydb_blood,ydb_zhiye,ydb_earn,ydb_day,amtlong  
 
        from spark_txt where   
 
£¨2£©tradeid=' 2014012213870282671'
 
        limit 10;  
 





     this sql is compose by two part

     (1)the part 1 is return the result which has 9 columns

     (2) the part 2 is the filter condition ,filter by tradeid

      


      let guess which plan is faster

     plan A :first read all the 9 columns result then filter by tradeid

     plan B: first filter by tradeid ,then we read the match 9 columns result.




    Ya100 choose plan B




     contrast  performance Ya100`index with parquet



<60C62790@C4EFD745.B6FECA56>




 2.TOP N sort ,the non sort column we doesn`t read it until the last 


  for example  we sort by the logtime

 

    select 
 
£¨1£©phonenum,usernick,ydb_grade,ydb_age,ydb_blood,ydb_zhiye,ydb_earn,ydb_day,amtlong  
 
    from spark_txt 
 
£¨2£©order by logtime desc 
 
    limit 10;  
 


  this sql is compose by two part
     (1)the part 1 is return the result which has 9 columns

     (2) the part 2 is the column need to sort

      


      let guess which plan is faster
     plan A :first read all the 9 columns result then sort by logtime
     plan B: first sort by logtime  ,then we read the match 9 columns result.




       Ya100 choose plan B




     contrast  performance Ya100`lazy read with parquet
<88FABE61@C4EFD745.B6FECA56>

3.we used label instead of the original value for grouping and sorting

<C2F6865D@C4EFD745.B6FECA56>

1).General situation,the data has a lot of repeat value,for exampe the sex file ,the age field .
2).if we store the original value ,that will weast a lot of storage.
so we make a small modify at original  value, Additional add a new filed called label.
make a unique value sort by fields, and then gave each term a unique  Number from begin to end  .
3).we use number value(we called label) instead of original  value.lable is store by fixed length. the file could be read by random read.
4).the label`s order is the same with dictionary  order .so if we do some calculation like order by or group by only need to  read the label. we don`t need to read the original value.
5).some field like sex field ,only have 2 different values.so we only use 2 bits(not 2 bytes) to store the label, it will save a lot of Disk io.
 when we finish all of the calculation, we translate label to original  value by a dictionary.
6)if a lots of rows have the same original value ,the original value we only store once,only read once.
Solve the problem:
1)ya100`s data is quite big we don`t have enough memory to load all Values to memory.
2)on realtime mode ,data is change Frequent , The cache is invalidated Frequent by append or update. build Cache will take a lot of times and io;
3)the Original value  is a string type.  whene sorting or grouping ,thed string value need a lot of memory and need lot of cpu time to calculate hashcode \compare \equals ,But label is number  is fast.
4)the label is number ,it`s type maybe short ,or maybe a byte ,or may be integer whitch depending on the max number of the label.

two-phase search
Original:
1)group by order by use original value,the real value may be is a string type,may be more larger ,the real value maybe  need a lot of io 
2)compare by string is slowly then compare by integer
Our improve:
1)we split one search into multy-phase search
2)the first search we only search the field that use for order by ,group by
3)the first search we doesn`t need to read the original value(the real value),we only need to read the docid and label for order by group by.
4)when we finish all the order by and group by ,may be we only need to return Top n records .so we start next to search to get the Top n records original value.
Solve the problem:
5)reduce io ,read original take a lot of disk io
6)reduce network io (for merger)
7)most of the field has repeated value, the repeated only need to read once
the group by filed only need to read the origin once by label where display to user.
8)most of the search only need to display on Top n (n<=100) results, so most of the original value could be skip.
<D328C2B9@C4EFD745.B6FECA56>


<6694C613@C4EFD745.B6FECA56>


<F21D52B8@C4EFD745.B6FECA56>

<2CBE257B@C4EFD745.B6FECA56>


 
How to install ya100 
 
 
1)Add the depend jar 
 
export SPARK_CLASSPATH=$SPARK_CLASSPATH:/data/ycloud/spark_ydb/ya100.jar
 
You can get the ya100 jar from this link



 
https://github.com/ycloudnet/ya100/blob/master/v1.0.8/ya100-1.0.8.jar



 
 
 
2)Start thriftserver 
 
./start-thriftserver.sh     --master yarn-client  --executor-memory 990m --executor-cores 2 --num-executors 16
 
 
 
Our suggest spark version is 6 
 
3)Config the ya100 function 
 
create  function Yfilter as 'cn.net.ycloud.ydb.handle.fun.Yfilter';   
 
create  function Ytop10000 as 'cn.net.ycloud.ydb.handle.fun.Ytop10000';  
 
create  function Ycombine as 'cn.net.ycloud.ydb.handle.fun.Ycombine'; 
 
create  function Ycount as 'cn.net.ycloud.ydb.handle.fun.Ycount';  
 
create  function Ymax as 'cn.net.ycloud.ydb.handle.fun.Ymax'; 
 
create  function Ymin as 'cn.net.ycloud.ydb.handle.fun.Ymin';   
 
create  function Yavg as 'cn.net.ycloud.ydb.handle.fun.Yavg';    
 
create  function Ysum as 'cn.net.ycloud.ydb.handle.fun.Ysum'; 
 
create  function Ymaxstring as 'cn.net.ycloud.ydb.handle.fun.Ymaxstring'; 
 
create  function Yminstring as 'cn.net.ycloud.ydb.handle.fun.Yminstring¡®; 
 
 
YA100 support data type 
 <30907791@C4EFD745.B6FECA56>

 
How to create ya100 table on spark Sql 
 
CREATE external  table spark_ya100(  
 
 phonenum bigint, usernick string, ydb_sex string,  ydb_province string, ydb_grade string, ydb_age string, ydb_blood string, ydb_zhiye string, ydb_earn string, ydb_prefer string, ydb_consume string, ydb_day string, amtdouble double,amtlong int, content string, ydbpartion string, ya100_pipe string 
 
) partitioned by (dt string) 
 
STORED BY 'cn.net.ycloud.ydb.handle.Ya100StorageHandler' 
 
LOCATION  '/data/ydb/shu_ya100' 
 
TBLPROPERTIES( 
 
""ya100.handler.table.name""=""ydb_example_shu"", 
 
""ya100.handler.schema""=""phonenum long,usernick string,ydb_sex string,ydb_province string,ydb_grade string,ydb_age string,ydb_blood string,ydb_zhiye string,ydb_earn string,ydb_prefer string,ydb_consume string,ydb_day string, amtdouble double,amtlong int,content textcjk,ydbpartion string,ya100_pipe string"" 
 
)  
 
//×¢£¬ydbpartionÎªydbµÄ·ÖÇø×Ö¶ÎÓëya100_pipe Îªya100µÄ¹ÜµÀ×Ö¶Î£¬±ØÐëÒª¶¼Òª´´½¨£¬·ñÔòÓÐÐ©¹¦ÄÜ»á±»ÏÞÖÆÊ¹ÓÃ¡£
 
ya100.handler.table.nameÎªÔ¤ÁôµÄkey,²»Í¬µÄ±íÖ®¼äÒªÇø·Ö¿ªÀ´
 
ya100.handler.schema ÎªÄÚÖÃµÄË÷ÒýµÄÊý¾ÝÀàÐÍ£¬Ãû×ÖÒªÓësparkµÄ×Ö¶ÎÃû³ÆÒ»ÖÂ¡£ 
 

 
How to Import data into ya100 table


 

1)Set up import params 
 
set hive.mapred.supports.subdirectories=true; 
 
set mapred.min.split.size=2147483648;
 
×¢£º 
 
ya100ÒòÎªÉú³ÉµÄ×îÖÕ²»ÊÇÒ»¸öÎÄ¼þ£¬¶øÊÇÒ»¸öÄ¿Â¼£¬Õâ¸öÄ¿Â¼ÏÂ»áÓÐË÷Òý£¬ËùÒÔÒªÍ¨¹ýsubdirectories²ÎÊýÉèÖÃsparkÖ§³Ö×ÓÄ¿Â¼£¬·ñÔòÏò±íÀïµ¼ÈëÊý¾Ý»á±¨´íÊ§°Ü¡£ 
 
set mapred.min.split.size=2147483648 ÊÇÎªÁË¿ØÖÆmapµÄ¸öÊý£¬·ÀÖ¹Éú³ÉµÄË÷ÒýÎÄ¼þÊýÁ¿Ì«¶à¡£ 
 
 
 
2)Begin import  
 
insert into table spark_ya100 partition (dt='1200million')
 
 select phonenum,usernick,ydb_sex,ydb_province,ydb_grade,ydb_age,ydb_blood,ydb_zhiye,ydb_earn,ydb_prefer,ydb_consume,ydb_day,amtdouble,amtlong,content,2,1 
 
from spark_txt where dt='1200million'; 
 
 
How to use ya100 to filter data 
 
 
1),basic example 1 
 
set ya100.spark.filter.ydb_example_shu=ydb_sex='Å®' or ydb_province='ÁÉÄþ' or ydb_day>='20151217'; 
 
ÓÐÐ©Ê±ºò£¬»áÓöµ½×ªÒåµÄÎÊÌâ£¬ÕâÀïµÄvalueÖµ£¬¿ÉÒÔ½øÐÐurlencode´¦Àí¡£
 
Èç£º
 
set ya100.spark.filter.ydb_example_shu=ydbpartion%3D%2720151110%27+and+%28ydb_sex%3D%27%E5%A5%B3%27+or+ydb_province%3D%27%E8%BE%BD%E5%AE%81%27+or+ydb_day%3E%3D%2720151217%27%29;
 
 
 
2)Filter example 2 
 
set ya100.spark.filter.ydb_example_shu=ydb_sex='Å®' or ydb_province='ÁÉÄþ' or ydb_day>='20151217'; 
 
select * from spark_ya100 where dt=¡®1200million¡¯ limit 10; 
 
 
 
 
 
 
Other filter example use case  
 
1)equal 
 
qq=¡®165162897¡¯
 
2)Support in 
 
Èç£ºindexnum in (1,2,3) 
 
3)>,<,>=,<=, 
 
clickcount >=10 and clickcount <=11 
 
4) range 
 
indexnum like '({0 TO 11}) '      ²»°üº¬±ß½çÖµ
 
indexnum like '([10 TO 11] ) '    °üº¬±ß½çÖµ
 
5) Unequal 
 
label<>'l_14' and label<>'l_15'  
 
6) 
 
indexnum='1' or indexnum='2' or (clickcount >=5 and clickcount <=11) 
 
 
 

<EE93F720@C4EFD745.B6FECA56>

 
How to use ya100 to make a top N sort 
 
set ya100.spark.top10000.ydb_example_shu=ydb_age desc limit 10; 
 
set ya100.spark.top10000.ydb_example_shu=ydb_sex desc,ydb_province limit 100£» 
 
set ya100.spark.top10000.ydb_example_shu=* limit 10; 
 
 
 
×¢Òâ£¬µ±Ç°°æ±¾limitµÄ×î´óÖµÎª10000 £¨Ã¿¸öË÷Òý×î´ó·µ»Ø1000£© 
 
ÁÐµÄÃû×Ö Ð´*±íÊ¾ ²»ÐèÒªÅÅÐò£¬µ«Ö»·µ»ØÇ°NÌõ¾Í¿ÉÒÔÁË 
 
 
 
SQL example  
 
 
 
set ya100.spark.top10000.ydb_example_shu=phonenum desc limit 10; 
 
select * from spark_ya100 where dt='1200million' order by phonenum desc limit 10; 
 
 
 

 
How to make group by or stat by ya100 
 
Example 1 
 
set ya100.spark.combine.ydb_example_shu=*,ydb_age; 
 
select ydb_province,Ycount('*',ya100_pipe), Ycount('ydb_age',ya100_pipe), Ymaxstring('ydb_age',ya100_pipe), Yminstring('ydb_age',ya100_pipe) 
 
 from spark_ya100  group by ydb_province limit 10 
 
  
 
Example 2 
 
set ya100.spark.combine.ydb_example_shu=amtlong; 
 
select ydb_sex, ydb_province,Ysum('amtlong',ya100_pipe) as cnt from spark_ya100  group by ydb_sex, ydb_province  limit 10 
 
 
 

 
Other SQL EXAMPLE


 
6¦1EXAMPLE COUNT(*) 
 
set ya100.spark.filter.ydb_example_shu=phonenum='13870282671'  and usernick='½­¾þÎõ'; 
 
set ya100.spark.combine.ydb_example_shu=*; 
 
set ya100.spark.top10000.ydb_example_shu=; 
 
select Ycount('*',ya100_pipe) from spark_ya100 where dt='100million' limit 10; 
 6¦1TOP N SORT EXAMPLE
 6¦1set ya100.spark.filter.ydb_example_shu= amtlong like '([1090 TO 1100] )' and amtdouble like '([1090 TO 1100] )'; 
 
set ya100.spark.combine.ydb_example_shu=; 
 
set ya100.spark.top10000.ydb_example_shu=phonenum desc limit 10; 
 
select * from spark_ya100 where dt='100million' order by phonenum desc limit 10;
 
SUM(amtlong)
 
set ya100.spark.filter.ydb_example_shu= amtlong like '([1090 TO 1100] )' and amtdouble like '([1090 TO 1100] )'; 
 
set ya100.spark.combine.ydb_example_shu=amtlong; 
 
set ya100.spark.top10000.ydb_example_shu=; 
 
select Ycount(¡®amtlong¡¯,ya100_pipe) ,Ysum(¡®amtlong¡¯,ya100_pipe) ,Yavg(¡®amtlong¡¯,ya100_pipe), Ymax(¡®amtlong¡¯,ya100_pipe), Ymin(¡®amtlong¡¯,ya100_pipe)  from spark_ya100 where dt=¡®100million¡¯  limit 10; 
 
 6¦1Top n sort 
 
set ya100.spark.filter.ydb_example_shu= amtlong like '([1090 TO 1100] )' and amtdouble like '([1090 TO 1100] )'; 
 
set ya100.spark.combine.ydb_example_shu=; 
 
set ya100.spark.top10000.ydb_example_shu= ydb_age desc, ydb_sex; 
 
select ydb_sex, ydb_age from spark_ya100 where dt='100million'  order  by  ydb_age desc, ydb_sex  limit 10;  
 6¦1Group by 
 
set ya100.spark.filter.ydb_example_shu=; 
 
set ya100.spark.combine.ydb_example_shu=ydb_age,ydb_blood,ydb_zhiye,ydb_earn,ydb_prefer,ydb_consume,ydb_day,amtlong; 
 
set ya100.spark.top10000.ydb_example_shu=; 
 
select ydb_sex,ydb_province,ydb_grade, Ymaxstring(¡®ydb_age¡¯,ya100_pipe),Ymaxstring(¡®ydb_blood¡¯,ya100_pipe),Ymaxstring(¡®ydb_zhiye¡¯,ya100_pipe),Ymaxstring(¡®ydb_earn¡¯,ya100_pipe),Ymaxstring(¡®ydb_prefer¡¯,ya100_pipe),Ymaxstring(¡®ydb_consume¡¯,ya100_pipe),Ymaxstring(¡®ydb_day¡¯,ya100_pipe),Ysum(¡®amtlong¡¯,ya100_pipe) as cnt from spark_ya100 where dt=¡®100million¡¯  group by ydb_sex,ydb_province,ydb_grade order by cnt desc limit 10 
 

 
Connect Ya100 to ydb  make real time data 
CREATE external  table spark_ydb(
 phonenum bigint, usernick string, ydb_sex string,  ydb_province string, ydb_grade string, ydb_age string, ydb_blood string, ydb_zhiye string, ydb_earn string, ydb_prefer string, ydb_consume string, ydb_day string, amtdouble double,amtlong int, content string, ydbpartion string, ya100_pipe string
)
STORED BY 'cn.net.ycloud.ydb.handle.Ya100StorageHandler'
LOCATION  '/data/ydb/shu_ydb'
TBLPROPERTIES(
""ya100.handler.table.name""=""ydb_example_shu"",
""ya100.handler.master""=""101.200.130.48:8080"",
""ya100.handler.columns.mapping""=""phonenum,usernick,ydb_sex,ydb_province,ydb_grade,ydb_age,ydb_blood,ydb_zhiye,ydb_earn,ydb_prefer,ydb_consume,ydb_day,amtdouble,amtlong,content,ydbpartion,ya100_pipe"")

×¢

ydbpartionÎªydbµÄ·ÖÇø×Ö¶Î£¬²éÑ¯µÄÊ±ºò±ØÐëÖ´ÐÐydbpartion,¶øspark±íÔòÃ»ÓÐ·ÖÇø×Ö¶Î 
 
ya100_pipe Îªya100µÄ¹ÜµÀ×Ö¶Î£¬±ØÐëÒª¶¼Òª´´½¨£¬·ñÔòÓÐÐ©¹¦ÄÜ»á±»ÏÞÖÆÊ¹ÓÃ¡£
 
ya100.handler.table.nameÎªydbÏµÍ³ÀïµÄ±íÃû×Ö
 
ya100.handler.columns.mapping ÎªydbÏµÍ³ÄÚµÄ±íÓësparkµÄ±íÖ®¼äµÄÓ³Éä¡£ 
 
 
 

<5C00AFEC@C4EFD745.B6FECA56>
<CDC3E8B4@C4EFD745.B6FECA56>

 
 <10C4E788@C4EFD745.B6FECA56>
---------------------------------------------------------------------"
Adnan Haider <ahaider3@hawk.iit.edu>,"Mon, 22 Feb 2016 08:16:14 -0600",Re: Using Encoding to reduce GraphX's static graph memory consumption,Reynold Xin <rxin@databricks.com>,"Yes, sounds good. I can submit the pull request.

"
Ted Yu <yuzhihong@gmail.com>,"Mon, 22 Feb 2016 07:09:45 -0800","=?UTF-8?B?UmU6IOWbnuWkje+8miBhIG5ldyBGaWxlRm9ybWF0IDV4fjEwMHggZmFzdGVyIHRoYW4gcA==?=
	=?UTF-8?B?YXJxdWV0?=",=?UTF-8?B?5byA5b+D5bu25bm0?= <muyannian@qq.com>,"The referenced benchmark is in Chinese. Please provide English version so
that more people can understand.

For item 7, looks like the speed of ingest is much slower compared to using
Parquet.

Cheers


3%BD%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.docx?raw=true
y
a
he
 on.
-
¥(æ˜ŸæœŸä¸€) æ™šä¸Š9:33
annian@qq.com>;
~100x faster than parquet
t
y
-
¥(æ˜ŸæœŸä¸€) æ™šä¸Š8:42
annian@qq.com>;
he.org>;
ed.
,amtlong
,amtlong
e
e
o;
s
o
ar
;
â€˜;
­—æ®µä¸Žya100_pipe ä¸ºya100çš„ç®¡é“å­—æ®µï¼Œå¿…é¡»è¦éƒ½è¦åˆ›å»ºï¼Œå¦åˆ™æœ‰äº›åŠŸèƒ½ä¼šè¢«é™åˆ¶ä½¿ç”¨ã€‚
åŒçš„è¡¨ä¹‹é—´è¦åŒºåˆ†å¼€æ¥
•çš„æ•°æ®ç±»åž‹ï¼Œåå­—è¦ä¸Žsparkçš„å­—æ®µåç§°ä¸€è‡´ã€‚
¸æ˜¯ä¸€ä¸ªæ–‡ä»¶ï¼Œè€Œæ˜¯ä¸€ä¸ªç›®å½•ï¼Œè¿™ä¸ªç›®å½•ä¸‹ä¼šæœ‰ç´¢å¼•ï¼Œæ‰€ä»¥è¦é€šè¿‡subdirectorieså‚æ•°è®¾ç½®spark
å‘è¡¨é‡Œå¯¼å…¥æ•°æ®ä¼šæŠ¥é”™å¤±è´¥ã€‚
§åˆ¶mapçš„ä¸ªæ•°ï¼Œé˜²æ­¢ç”Ÿæˆçš„ç´¢å¼•æ–‡ä»¶æ•°é‡å¤ªå¤šã€‚
hiye,ydb_earn,ydb_prefer,ydb_consume,ydb_day,amtdouble,amtlong,content,2,1
ovince='è¾½å®'
è½¬ä¹‰çš„é—®é¢˜ï¼Œè¿™é‡Œçš„valueå€¼ï¼Œå¯ä»¥è¿›è¡Œurlencodeå¤„ç†ã€‚
ydb_sex%3D%27%E5%A5%B3%27+or+ydb_province%3D%27%E8%BE%BD%E5%AE%81%27+or+ydb_day%3E%3D%2720151217%27%29;
ovince='è¾½å®'
 10;
ç•Œå€¼
å€¼
=11)
it
š„æœ€å¤§å€¼ä¸º10000 ï¼ˆæ¯ä¸ªç´¢å¼•æœ€å¤§è¿”å›ž1000ï¼‰
éœ€è¦æŽ’åºï¼Œä½†åªè¿”å›žå‰Næ¡å°±å¯ä»¥äº†
c
,
TO 1100]
)'
ngâ€™,ya100_pipe) ,Yavg(â€˜amtlongâ€™,ya100_pipe),
™,ya100_pipe)  from spark_ya100
)'
 by
rn,ydb_prefer,ydb_consume,ydb_day,amtlong;
™,ya100_pipe),
db_zhiyeâ€™,ya100_pipe),
b_preferâ€™,ya100_pipe),
˜ydb_dayâ€™,ya100_pipe),
 dt=â€˜100millionâ€™
ipe
e,ydb_grade,ydb_age,ydb_blood,ydb_zhiye,ydb_earn,ydb_prefer,ydb_consume,ydb_day,amtdouble,amtlong,content,ydbpartion,ya100_pipe"")
¼ŒæŸ¥è¯¢çš„æ—¶å€™å¿…é¡»æ‰§è¡Œydbpartion,è€Œsparkè¡¨åˆ™æ²¡æœ‰åˆ†åŒºå­—æ®µ
ï¼Œå¿…é¡»è¦éƒ½è¦åˆ›å»ºï¼Œå¦åˆ™æœ‰äº›åŠŸèƒ½ä¼šè¢«é™åˆ¶ä½¿ç”¨ã€‚
è¡¨åå­—
š„è¡¨ä¸Žsparkçš„è¡¨ä¹‹é—´çš„æ˜ å°„ã€‚
"
Michael Armbrust <michael@databricks.com>,"Mon, 22 Feb 2016 10:24:48 -0800",Re: Spark 1.6.1,Patrick Woody <patrick.woody1@gmail.com>,"I will cut the RC today.  Sorry for the delay!


"
shane knapp <sknapp@berkeley.edu>,"Mon, 22 Feb 2016 13:42:49 -0800",[build system] jenkins restarted,"amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","i noticed that jenkins was quiet...  too quiet...  and not building
anything.  i looked in the logs and it looks like things stopped
building after the backup at midnight.  looking through the logs, it
looks like we had some jenkins java processes stacktrace.  i'll keep a
close eye on things this week and see if we have any ongoing problems.

anyways, a service restart seemed to get jenkins back in the mood to
build, and we're working through the backlog now.

shane

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Mon, 22 Feb 2016 13:47:43 -0800",Re: [build system] jenkins restarted,shane knapp <sknapp@berkeley.edu>,"Thanks for keeping an eye on this!


"
shane knapp <sknapp@berkeley.edu>,"Mon, 22 Feb 2016 14:01:28 -0800",Re: [build system] jenkins restarted,Reynold Xin <rxin@databricks.com>,"not a problem!  :)


---------------------------------------------------------------------


"
Yash Sharma <yash360@gmail.com>,"Tue, 23 Feb 2016 12:58:41 +1100",Re: Spark not able to fetch events from Amazon Kinesis,Burak Yavuz <brkyvz@gmail.com>,"Answering my own Question -

I have got some success with Spark Kinesis integration, and the key being
the unionStreams.foreachRDD.

There are 2 versions of the foreachRDD available
- unionStreams.foreachRDD
- unionStreams.foreachRDD ((rdd: RDD[Array[Byte]], time: Time)

For some reason the first one is not able to get me the results but
changing to the second one fetches me the results as expected. Yet to
explore the reason.

Adding a code snippet below for reference.

Hope it helps someone :)

Thanks everyone for help.

<code>

unionStreams.foreachRDD ((rdd: RDD[Array[Byte]], time: Time) => { // Works,


working ? Is there anything else that you might have tried ?
due to protobuf or jackson. That may be your culprit. The problem is that
all failures by the Kinesis Client Lib is silent, therefore don't show up
on the logs. It's very hard to debug those buggers.
unfortunately so just wanted to check if the version mismatch is the
primary issue here. Wanted to know if anyone has hit across similar issue
and how they have solved this.
Spark with the following change (using maven):
am having problem in receiving the events. While Spark is able to connect
to Kinesis and is able to get metadata from Kinesis, Its not able to get
events from it. It always fetches zero elements back.
metadata (Eg. number of shards in kinesis etc).
got much luck yet. I have also tried couple of suggestions from SO [3]. The
cluster has sufficient resources/cores available.
Kinesis which could also be a cause for this behavior. Spark uses
protobuf-java version 2.5.0 and kinesis probably uses
protobuf-java-2.6.1.jar.
spark working with kinesis.
http://spark.apache.org/docs/latest/streaming-kinesis-integration.html
https://github.com/apache/spark/blob/master/extras/kinesis-asl/src/main/scala/org/apache/spark/examples/streaming/KinesisWordCountASL.scala
http://stackoverflow.com/questions/26941844/apache-spark-kinesis-sample-not-working
"
Michael Armbrust <michael@databricks.com>,"Mon, 22 Feb 2016 18:28:02 -0800",Re: Spark 1.6.1,"""dev@spark.apache.org"" <dev@spark.apache.org>","I've kicked off the build.  Please be extra careful about merging into
branch-1.6 until after the release.


"
"""Pierson, Oliver C"" <ocp@gatech.edu>","Tue, 23 Feb 2016 02:45:07 +0000",Opening a JIRA for QuantileDiscretizer bug,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hello,

  I've discovered a bug in the QuantileDiscretizer estimator.  Specifically, for large DataFrames QuantileDiscretizer will only create one split (i.e. two bins).


The error happens in lines 113 and 114 of QuantileDiscretizer.scala:


    val requiredSamples = math.max(numBins * numBins, 10000)

    val fraction = math.min(requiredSamples / dataset.count(), 1.0)


After the first line, requiredSamples is an Int.  Therefore, if requiredSamples > dataset.count() then fraction is always 0.0.


The problem can be simply fixed by replacing the first with:


  val requiredSamples = math.max(numBins * numBins, 10000.0)


I've implemented this change in my fork and all tests passed (except for docker integration, but I think that's another issue).  I'm happy to submit a PR if it will ease someone else's workload.  However, I'm unsure of how to create a JIRA.  I've created an account on the issue tracker (issues.apache.org) but when I try to create an issue it asks me to choose a ""Service Desk"".  Which one should I be choosing?


Thanks much,

Oliver Pierson


"
Ted Yu <yuzhihong@gmail.com>,"Mon, 22 Feb 2016 19:07:58 -0800",Re: Opening a JIRA for QuantileDiscretizer bug,"""Pierson, Oliver C"" <ocp@gatech.edu>","When you click on Create, you're brought to 'Create Issue' dialog where you
choose Project Spark.
Component should be MLlib.

Please see also:
http://search-hadoop.com/m/q3RTtmsshe1W6cH22/spark+pull+template&subj=pull+request+template



"
Michael Armbrust <michael@databricks.com>,"Mon, 22 Feb 2016 21:08:08 -0800",Re: Spark 1.6.1,"""dev@spark.apache.org"" <dev@spark.apache.org>","An update: people.apache.org has been shut down so the release scripts are
broken. Will try again after we fix them.


"
Luciano Resende <luckbr1975@gmail.com>,"Mon, 22 Feb 2016 22:07:30 -0800",Re: Spark 1.6.1,Michael Armbrust <michael@databricks.com>,"
If you skip uploading to people.a.o, it should still be available in nexus
for review.

The other option is to add the RC into
https://dist.apache.org/repos/dist/dev/



-- 
Luciano Resende
http://people.apache.org/~lresende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
Romi Kuntsman <romi@totango.com>,"Tue, 23 Feb 2016 09:32:06 +0200",Re: Spark 1.6.1,Luciano Resende <luckbr1975@gmail.com>,"Is it possible to make RC versions available via Maven? (many projects do
that)
That will make integration much easier, so many more people can test the
version before the final release.
Thanks!

*Romi Kuntsman*, *Big Data Engineer*
http://www.totango.com


"
Reynold Xin <rxin@databricks.com>,"Mon, 22 Feb 2016 23:34:35 -0800",Re: Spark 1.6.1,Romi Kuntsman <romi@totango.com>,"We usually publish to a staging maven repo hosted by the ASF (not maven
central).




"
Romi Kuntsman <romi@totango.com>,"Tue, 23 Feb 2016 09:37:07 +0200",Re: Spark 1.6.1,Reynold Xin <rxin@databricks.com>,"Sounds fair. Is it to avoid cluttering maven central with too many
intermediate versions?

What do I need to add in my pom.xml <repositories> section to make it work?

*Romi Kuntsman*, *Big Data Engineer*
http://www.totango.com


"
Reynold Xin <rxin@databricks.com>,"Mon, 22 Feb 2016 23:39:55 -0800",Re: Spark 1.6.1,Romi Kuntsman <romi@totango.com>,"Yes, we don't want to clutter maven central.


The staging repo is included in the release candidate voting thread.

See the following for an example:

http://apache-spark-developers-list.1001551.n3.nabble.com/VOTE-Release-Apache-Spark-1-6-0-RC1-td15424.html


"
lonely Feb <lonely8658@gmail.com>,"Tue, 23 Feb 2016 16:16:45 +0800",spark core api vs. google cloud dataflow,dev@spark.apache.org,"oogle Cloud Dataflow provides distributed dataset which called PCollection,
and syntactic sugar based on PCollection is provided in the form of
""apply"". Note that ""apply"" is different from spark api ""map"" which passing
each element of the source through a function func. I wonder can spark
support this kind of syntactic sugar, if not, why?
"
Reynold Xin <rxin@databricks.com>,"Tue, 23 Feb 2016 00:24:10 -0800",Re: spark core api vs. google cloud dataflow,lonely Feb <lonely8658@gmail.com>,"That's the just transform function in DataFrame

  /**
   * Concise syntax for chaining custom transformations.
   * {{{
   *   def featurize(ds: DataFrame) = ...
   *
   *   df
   *     .transform(featurize)
   *     .transform(...)
   * }}}
   * @since 1.6.0
   */
  def transform[U](t: DataFrame => DataFrame): DataFrame = t(this)


Note that while this is great for chaining, having *only* this leads to
pretty bad user experience, especially in interactive analysis when it is
not obvious what operations are available.




"
Vasanth Bhat <vasbhat@gmail.com>,"Tue, 23 Feb 2016 14:45:48 +0530",Re: Accessing Web UI,dev@spark.apache.org,"Hi,
-configured.html

    I am unable to connect to the web UI after I bring up the master daemon
using sbin/start-master.sh   It looks like this is an issue due to the
threadpool settings used by  the embedded in the master daemon.

 on the
ver
n
r
in-hadoop2.6/lib/spark-assembly-1.6.0-hadoop2.6.0.jar:/usr/local/spark-1.6.0-bin-hadoop2.6/lib/datanucleus-api-jdo-3.2.6.jar:/usr/local/spark-1.6.0-bin-hadoop2.6/lib/datanucleus-rdbms-3.2.9.jar:/usr/local/spark-1.6.0-bin-hadoop2.6/lib/datanucleus-core-3.2.10.jar
==================
,
op
er
r
b-UI-tp23029p26276.html
--
"
Sean Owen <sowen@cloudera.com>,"Tue, 23 Feb 2016 09:58:14 +0000",Re: Opening a JIRA for QuantileDiscretizer bug,"""Pierson, Oliver C"" <ocp@gatech.edu>","Good catch, though probably very slightly simpler to write

math.min(requiredSamples.toDouble ...

Make sure you're logged in to JIRA maybe. If you have any trouble I'll
open it for you. You can file it as a minor bug against ML.

This is how you open a PR and everything else
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark


---------------------------------------------------------------------


"
Vasanth Bhat <vasbhat@gmail.com>,"Tue, 23 Feb 2016 16:21:42 +0530",Re: Accessing Web UI,Gourav Sengupta <gourav.sengupta@gmail.com>,"Hi Gourav,

     The spark version is   spark-1.6.0-bin-hadoop2.6 .     The  Java
version is JDK 8.  I have  also tried with JDK 7 also, but the results are
same.

Thanks
Vasanth




s-configured.html
o
e
:
e on the
rver
P
:
er
bin-hadoop2.6/lib/spark-assembly-1.6.0-hadoop2.6.0.jar:/usr/local/spark-1.6.0-bin-hadoop2.6/lib/datanucleus-api-jdo-3.2.6.jar:/usr/local/spark-1.6.0-bin-hadoop2.6/lib/datanucleus-rdbms-3.2.9.jar:/usr/local/spark-1.6.0-bin-hadoop2.6/lib/datanucleus-core-3.2.10.jar
A
==================
or
eb-UI-tp23029p26276.html
---
"
James Barney <jamesbarney71@gmail.com>,"Tue, 23 Feb 2016 09:05:31 -0500",ORC file writing hangs in pyspark,dev@spark.apache.org,"I'm trying to write an ORC file after running the FPGrowth algorithm on a
dataset of around just 2GB in size. The algorithm performs well and can
display results if I take(n) the freqItemSets() of the result after
converting that to a DF.

I'm using Spark 1.5.2 on HDP 2.3.4 and Python 3.4.2 on Yarn.

I get the results from querying a Hive table, also ORC format, running a
number of maps, joins, and filters on the data.

When the program attempts to write the files:
    result.write.orc('/data/staged/raw_result')
  size_1_buckets.write.orc('/data/staged/size_1_results')
  filter_size_2_buckets.write.orc('/data/staged/size_2_results')

The first path, /data/staged/raw_result, is created with a _temporary
folder, but the data is never written. The job hangs at this point,
apparently indefinitely.

Additionally, no logs are recorded or available for the jobs on the history
server.

What could be the problem?
"
=?UTF-8?Q?Sergio_Ram=c3=adrez?= <sramirezga@ugr.es>,"Tue, 23 Feb 2016 18:33:34 +0100",Modify text in spark-packages,dev <dev@spark.apache.org>,"Hello,

I have some problems in modifying the description of some of my packages 
in spark-packages.com. However, I haven't been able to change anything. 
I've written to the e-mail direction in charge of managing this page, 
but I got no answer.

Any clue?

Thanks

---------------------------------------------------------------------


"
Adnan Haider <ahaider3@hawk.iit.edu>,"Tue, 23 Feb 2016 15:13:08 -0600",Re: Using Encoding to reduce GraphX's static graph memory consumption,Reynold Xin <rxin@databricks.com>,"Hi
I have created a jira for this issue here.
<https://issues.apache.org/jira/browse/SPARK-13460?jql=project%20%3D%20SPARK%20AND%20created%3E%3D-1w%20ORDER%20BY%20created%20DESC>
As
for the pull request, my implementation is based on removing localSrcIds
and storing an array of offsets into localDstIds. I am running into issues
with this method when testing operations which create partitions from
existing edge partitions. The other method for implementing this would be
to use a hashmap from local id to offset/length pairs. This seems to work
fine but there is more storage overhead associated with this since each
source vertex requires space for three integers.

Thanks, Adnan Haider
B.S Candidate, Computer Science
Illinois Institute of Technology
www.adnanhaider.com


"
Dushyant Rajput <dushyant.0311@gmail.com>,"Tue, 23 Feb 2016 13:46:41 -0800",Fwd: HANA data access from SPARK,dev@spark.apache.org,"Hi,

I am writting a python app to load data from SAP HANA.

dfr = DataFrameReader(sqlContext)
df =
dfr.jdbc(url='jdbc:sap://ip_hana:30015/?user=<user>&password=<pwd>',table=table)
df.show()

It throws a
â€‹ serialization errorâ€‹
:

y4j.protocol.Py4JJavaError: An error occurred while calling o59.showString.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task
not serializable: java.io.NotSerializableException:
com.sap.db.jdbc.topology.Host
Serialization stack:
- object not serializable (class: com.sap.db.jdbc.topology.Host, value:
<ip>:30015)
- writeObject data (class: java.util.ArrayList)
- object (class java.util.ArrayList, [])
- writeObject data (class: java.util.Hashtable)
- field (class:
org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$$anonfun$getConnector$1,
name: properties$1, type: class java.util.Properties)
- object (class
org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$$anonfun$getConnector$1,
<function0>)
- field (class: org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD,
name:
org$apache$spark$sql$execution$datasources$jdbc$JDBCRDD$$getConnection,
type: interface scala.Function0)
- object (class org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD,
JDBCRDD[5] at showString at NativeMethodAccessorImpl.java:-2)
- field (class: org.apache.spark.NarrowDependency, name: _rdd, type: class
org.apache.spark.rdd.RDD)
- writeObject data (class: scala.collection.immutable.$colon$colon)
- object (class scala.collection.immutable.$colon$colon,
- field (class: org.apache.spark.rdd.RDD, name:
org$apache$spark$rdd$RDD$$dependencies_, type: interface
scala.collection.Seq)
- object (class org.apache.spark.rdd.MapPartitionsRDD, MapPartitionsRDD[6]
at showString at NativeMethodAccessorImpl.java:-2)
- field (class: scala.Tuple2, name: _1, type: class java.lang.Object)
- object (class scala.Tuple2, (MapPartitionsRDD[6] at showString at
NativeMethodAccessorImpl.java:-2,<function2>))
at org.apache.spark.scheduler.DAGScheduler.org
$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)
at
org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)
at
org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)
at
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
at
org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)
at org.apache.spark.scheduler.DAGScheduler.org
$apache$spark$scheduler$DAGScheduler$$submitMissingTasks(DAGScheduler.scala:865)
at org.apache.spark.scheduler.DAGScheduler.org
$apache$spark$scheduler$DAGScheduler$$submitStage(DAGScheduler.scala:772)
at
org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:757)
at
duler.scala:1466)
at
org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)
at
org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)
at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)
at org.apache.spark.SparkContext.runJob(SparkContext.scala:1824)
at org.apache.spark.SparkContext.runJob(SparkContext.scala:1837)
at org.apache.spark.SparkContext.runJob(SparkContext.scala:1850)
at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:215)
at
org.apache.spark.sql.execution.Limit.executeCollect(basicOperators.scala:207)
at
org.apache.spark.sql.DataFrame$$anonfun$collect$1.apply(DataFrame.scala:1385)
at
org.apache.spark.sql.DataFrame$$anonfun$collect$1.apply(DataFrame.scala:1385)

Rgds,
Dushyant.
"
"""Joseph E. Gonzalez"" <joseph.e.gonzalez@gmail.com>","Tue, 23 Feb 2016 14:53:46 -0800",Re: Using Encoding to reduce GraphX's static graph memory consumption,Adnan Haider <ahaider3@hawk.iit.edu>,"Actually another improvement would be to use something like compressed sparse row encoding which can be used to store A and A^T relatively efficiently (I think using 5 arrays instead of 6).  There is an option to also be more cache aware using something like a block compressed sparse row encoding.

Ankur also at some point looked at renumbering the vertex ids on each edge to be consecutive with respect to the edge partition.  This is a pretty common technique in most graph processing systems but in our early experiments we didnâ€™t see much of a gain.   In theory, however this should allow for lower bit precision encoding and a more dense representation and eliminate the need to use a hash lookup when joining vertices with edges. 

Joey





<https://issues.apache.org/jira/browse/SPARK-13460?jql=project%20%3D%20SPARK%20AND%20created%3E%3D-1w%20ORDER%20BY%20created%20DESC> As for the pull request, my implementation is based on removing localSrcIds and storing an array of offsets into localDstIds. I am running into issues with this method when testing operations which create partitions from existing edge partitions. The other method for implementing this would be to use a hashmap from local id to offset/length pairs. This seems to work fine but there is more storage overhead associated with this since each source vertex requires space for three integers. 
request?
reason
graph. I
the
localSrcIds
ID for
sorting
first to
make
distinct
id for
an
eliminate
in
testing,
nearly
both
future
post
<https://github.com/Netflix/netflix-graph>>   and would be happy to open a
completely
http://apache-spark-developers-list.1001551.n3.nabble.com/Using-Encoding-to-reduce-GraphX-s-static-graph-memory-consumption-tp16373.html <http://apache-spark-developers-list.1001551.n3.nabble.com/Using-Encoding-to-reduce-GraphX-s-static-graph-memory-consumption-tp16373.html>
Nabble.com.
<mailto:dev-unsubscribe@spark.apache.org>
<mailto:dev-help@spark.apache.org>

"
Prabhu Joseph <prabhujose.gates@gmail.com>,"Wed, 24 Feb 2016 06:49:53 +0530",Spark Job on YARN Hogging the entire Cluster resource,"yarn-dev@hadoop.apache.org, Spark dev list <dev@spark.apache.org>","Hi All,

 A YARN cluster with 352 Nodes (10TB, 3000cores) and has Fair Scheduler
with root queue having 230 queues.

    Each Queue is configured with maxResources equal to Total Cluster
Resource. When a Spark job is submitted into a queue A, it is given with
10TB, 3000 cores according to instantaneous Fair Share and it is holding
the entire resource without releasing. After some time, when another job is
submitted into other queue B, it will get the Fair Share 45GB and 13 cores
i.e (10TB,3000 cores)/230 using Preemption. Now if some more jobs are
submitted into queue B, all the jobs in B has to share the 45GB and 13
cores. Whereas the job which is in queue A holds the entire cluster
resource affecting the other jobs.
     This kind of issue often happens when a Spark job submitted first
which holds the entire cluster resource. What is the best way to fix this
issue. Can we make preemption to happen for instantaneous fair share
instead of fair share, will it help.

Note:

1. We do not want to give weight for particular queue. Because all the 240
queues are critical.
2. Changing the queues into nested does not solve the issue.
3. Adding maxResource to queue  won't allow the first job to pick entire
cluster resource, but still configuring the optimal maxResource for 230
queue is difficult and also the first job can't use the entire cluster
resource when the cluster is idle.
4. We do not want to handle it in Spark ApplicationMaster, then we need to
check for other new YARN application type with similar behavior. We want
YARN to control this behavior by killing the resources which is hold by
first job for longer period.


Thanks,
Prabhu Joseph
"
Jeff Zhang <zjffdu@gmail.com>,"Wed, 24 Feb 2016 10:22:55 +0800",Re: ORC file writing hangs in pyspark,James Barney <jamesbarney71@gmail.com>,"Have you checked the live spark UI and yarn app logs ?





-- 
Best Regards

Jeff Zhang
"
Zhan Zhang <zzhang@hortonworks.com>,"Wed, 24 Feb 2016 02:28:29 +0000",Re: ORC file writing hangs in pyspark,James Barney <jamesbarney71@gmail.com>,"Hi James,

You can try to write with other format, e.g., parquet to see whether it is a orc specific issue or more generic issue.

Thanks.

Zhan Zhang


I'm trying to write an ORC file after running the FPGrowth algorithm on a dataset of around just 2GB in size. The algorithm performs well and can display results if I take(n) the freqItemSets() of the result after converting that to a DF.

I'm using Spark 1.5.2 on HDP 2.3.4 and Python 3.4.2 on Yarn.

I get the results from querying a Hive table, also ORC format, running a number of maps, joins, and filters on the data.

When the program attempts to write the files:
    result.write.orc('/data/staged/raw_result')
  size_1_buckets.write.orc('/data/staged/size_1_results')
  filter_size_2_buckets.write.orc('/data/staged/size_2_results')

The first path, /data/staged/raw_result, is created with a _temporary folder, but the data is never written. The job hangs at this point, apparently indefinitely.

Additionally, no logs are recorded or available for the jobs on the history server.

What could be the problem?

"
Yin Yang <yy201602@gmail.com>,"Wed, 24 Feb 2016 02:09:27 -0800",Re: Spark 1.6.1,dev@spark.apache.org,"Looks like access to people.apache.org has been restored.

FYI


"
Hamel Kothari <hamelkothari@gmail.com>,"Wed, 24 Feb 2016 16:33:58 +0000",Re: Spark Job on YARN Hogging the entire Cluster resource,"Prabhu Joseph <prabhujose.gates@gmail.com>, yarn-dev@hadoop.apache.org, 
	Spark dev list <dev@spark.apache.org>","If all queues are identical, this behavior should not be happening.
Preemption as designed in fair scheduler (IIRC) takes place based on the
instantaneous fair share, not the steady state fair share. The fair
scheduler docs
<https://hadoop.apache.org/docs/r2.7.1/hadoop-yarn/hadoop-yarn-site/FairScheduler.html>
aren't super helpful on this but it does say in the Monitoring section that
preemption won't take place if you're less than your instantaneous fair
share (which might imply that it would occur if you were over your inst.
fair share and someone had requested resources). The code for
FairScheduler.resToPreempt
<http://grepcode.com/file/repo1.maven.org/maven2/org.apache.hadoop/hadoop-yarn-server-resourcemanager/2.7.1/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java#FairScheduler.resToPreempt%28org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue%2Clong%29>
also seems to use getFairShare rather than getSteadyFairShare() for
preemption so that would imply that it is using instantaneous fair share
rather than steady state.

Could you share your YARN site/fair-scheduler and Spark configurations?
Could you also share the YARN Scheduler UI (specifically the top of of the
RM which shows how many resources are in use)?

Since it's not likely due to steady state fair share, some other possible
reasons why this might be taking place (this is not remotely conclusive but
with no information this is what comes to mind):
- You're not reaching
yarn.scheduler.fair.preemption.cluster-utilization-threshold. Perhaps due
to core/memory ratio inconsistency with the cluster.
- Your second job doesn't have a sufficient level of parallelism to request
more executors than what it is recieving (perhaps there are fewer than 13
tasks at any point in time) and you don't have
spark.dynamicAllocation.minExecutors set?

-Hamel


is
s
0
o
"
Minudika Malshan <minudika001@gmail.com>,"Thu, 25 Feb 2016 00:58:19 +0530",Build fails,dev@spark.apache.org,"Hi,

I am trying to build from spark source code which was cloned from
https://github.com/apache/spark.git.
But it fails with following error.

[ERROR] Failed to execute goal
org.apache.maven.plugins:maven-enforcer-plugin:1.4.1:enforce
(enforce-versions) on project spark-parent_2.11: Some Enforcer rules have
failed. Look above for specific messages explaining why the rule failed. ->
[Help 1]

Please help me to get it fixed.

Thanks and regards..
Minudika


Minudika Malshan
Undergraduate
Department of Computer Science and Engineering
University of Moratuwa.
"
Marcelo Vanzin <vanzin@cloudera.com>,"Wed, 24 Feb 2016 11:29:43 -0800",Re: Build fails,Minudika Malshan <minudika001@gmail.com>,"Well, did you do what the message instructed you to do and looked
above the message you copied for more specific messages for why the
build failed?




-- 
Marcelo

---------------------------------------------------------------------


"
Yin Yang <yy201602@gmail.com>,"Wed, 24 Feb 2016 11:30:59 -0800",Re: Build fails,Marcelo Vanzin <vanzin@cloudera.com>,"I encountered similar warning recently.

Please check the version of maven you're using: it should be 3.3.9


"
Minudika Malshan <minudika001@gmail.com>,"Thu, 25 Feb 2016 01:07:20 +0530",Re: Build fails,Yin Yang <yy201602@gmail.com>,"Here is the full stack trace..
@Yin : yeah it seems like a problem with maven version. I am going to
update maven.
@ Marcelo :  Yes, couldn't decide what's wrong at first :)

Thanks for your help!


[INFO] Scanning for projects...
[INFO]
------------------------------------------------------------------------
[INFO] Reactor Build Order:
[INFO]
[INFO] Spark Project Parent POM
[INFO] Spark Project Test Tags
[INFO] Spark Project Sketch
[INFO] Spark Project Launcher
[INFO] Spark Project Networking
[INFO] Spark Project Shuffle Streaming Service
[INFO] Spark Project Unsafe
[INFO] Spark Project Core
[INFO] Spark Project GraphX
[INFO] Spark Project Streaming
[INFO] Spark Project Catalyst
[INFO] Spark Project SQL
[INFO] Spark Project ML Library
[INFO] Spark Project Tools
[INFO] Spark Project Hive
[INFO] Spark Project Docker Integration Tests
[INFO] Spark Project REPL
[INFO] Spark Project Assembly
[INFO] Spark Project External Twitter
[INFO] Spark Project External Flume Sink
[INFO] Spark Project External Flume
[INFO] Spark Project External Flume Assembly
[INFO] Spark Project External Akka
[INFO] Spark Project External MQTT
[INFO] Spark Project External MQTT Assembly
[INFO] Spark Project External ZeroMQ
[INFO] Spark Project External Kafka
[INFO] Spark Project Examples
[INFO] Spark Project External Kafka Assembly
[INFO]

[INFO]
------------------------------------------------------------------------
[INFO] Building Spark Project Parent POM 2.0.0-SNAPSHOT
[INFO]
------------------------------------------------------------------------
[INFO]
[INFO] --- maven-clean-plugin:3.0.0:clean (default-clean) @
spark-parent_2.11 ---
[INFO]
[INFO] --- maven-enforcer-plugin:1.4.1:enforce (enforce-versions) @
spark-parent_2.11 ---
[WARNING] Rule 0: org.apache.maven.plugins.enforcer.RequireMavenVersion
failed with message:
Detected Maven Version: 3.3.3 is not in the allowed range 3.3.9.
[INFO]
------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO]
[INFO] Spark Project Parent POM ........................... FAILURE [
 0.811 s]
[INFO] Spark Project Test Tags ............................ SKIPPED
[INFO] Spark Project Sketch ............................... SKIPPED
[INFO] Spark Project Launcher ............................. SKIPPED
[INFO] Spark Project Networking ........................... SKIPPED
[INFO] Spark Project Shuffle Streaming Service ............ SKIPPED
[INFO] Spark Project Unsafe ............................... SKIPPED
[INFO] Spark Project Core ................................. SKIPPED
[INFO] Spark Project GraphX ............................... SKIPPED
[INFO] Spark Project Streaming ............................ SKIPPED
[INFO] Spark Project Catalyst ............................. SKIPPED
[INFO] Spark Project SQL .................................. SKIPPED
[INFO] Spark Project ML Library ........................... SKIPPED
[INFO] Spark Project Tools ................................ SKIPPED
[INFO] Spark Project Hive ................................. SKIPPED
[INFO] Spark Project Docker Integration Tests ............. SKIPPED
[INFO] Spark Project REPL ................................. SKIPPED
[INFO] Spark Project Assembly ............................. SKIPPED
[INFO] Spark Project External Twitter ..................... SKIPPED
[INFO] Spark Project External Flume Sink .................. SKIPPED
[INFO] Spark Project External Flume ....................... SKIPPED
[INFO] Spark Project External Flume Assembly .............. SKIPPED
[INFO] Spark Project External Akka ........................ SKIPPED
[INFO] Spark Project External MQTT ........................ SKIPPED
[INFO] Spark Project External MQTT Assembly ............... SKIPPED
[INFO] Spark Project External ZeroMQ ...................... SKIPPED
[INFO] Spark Project External Kafka ....................... SKIPPED
[INFO] Spark Project Examples ............................. SKIPPED
[INFO] Spark Project External Kafka Assembly .............. SKIPPED
[INFO]
------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO]
------------------------------------------------------------------------
[INFO] Total time: 1.721 s
[INFO] Finished at: 2016-02-25T01:03:12+05:30
[INFO] Final Memory: 28M/217M
[INFO]
------------------------------------------------------------------------
[ERROR] Failed to execute goal
org.apache.maven.plugins:maven-enforcer-plugin:1.4.1:enforce
(enforce-versions) on project spark-parent_2.11: Some Enforcer rules have
failed. Look above for specific messages explaining why the rule failed. ->
[Help 1]
[ERROR]
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e
switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR]
[ERROR] For more information about the errors and possible solutions,
please read the following articles:
[ERROR] [Help 1]
http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException


Minudika Malshan
Undergraduate
Department of Computer Science and Engineering
University of Moratuwa.




"
Marcelo Vanzin <vanzin@cloudera.com>,"Wed, 24 Feb 2016 11:38:48 -0800",Re: Build fails,Minudika Malshan <minudika001@gmail.com>,"The error is right there. Just read the output more carefully.


---------------------------------------------------------------------


"
Prabhu Joseph <prabhujose.gates@gmail.com>,"Thu, 25 Feb 2016 01:22:34 +0530",Re: Spark Job on YARN Hogging the entire Cluster resource,Hamel Kothari <hamelkothari@gmail.com>,"Hi Hamel,

    Thanks for looking into the issue. What i am not understanding is,
after preemption what is the share that the second queue gets in case if
the first queue holds the entire cluster resource without releasing, is it
instantaneous fair share or fair share.

     Queue A and B are there (total 230 queues), total cluster resource is
10TB, 3000 cores. If a job submitted into queue A, it will get 10TB, 3000
cores and it is not releasing any resource. Now if a second job submitted
into queue B, so preemption definitely will happen, but what is the share
queue B will get after preemption. *Is it  <10 TB , 3000> / 2 or
<10TB,3000> / 230*

We find, after preemption queue B gets only <10TB,3000> / 230, because the
first job is holding the resource. In case if first job releases the
resource, the second queue will get <10TB,3000> /2 based on higher priority
and reservation.

The question is how much preemption tries to preempt the queue A if it
holds the entire resource without releasing? Could not able to share the
actual configuration, but the answer to the question here will help us.


Thanks,
Prabhu Joseph





cheduler.html>
at
-yarn-server-resourcemanager/2.7.1/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java#FairScheduler.resToPreempt%28org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue%2Clong%29>
e
ut
 is
es
s
ant
"
Hamel Kothari <hamelkothari@gmail.com>,"Wed, 24 Feb 2016 19:58:56 +0000",Re: Spark Job on YARN Hogging the entire Cluster resource,Prabhu Joseph <prabhujose.gates@gmail.com>,"The instantaneous fair share is what Queue B should get according to the
code (and my experience). Assuming your queues are all equal it would be
10TB/2.

I can't help much more unless I can see your config files and ideally also
the YARN Scheduler UI to get an idea of what your queues/actual resource
usage is like. Logs from each of your Spark applications would also be
useful. Basically the more info the better.


t
s
e
ty
Scheduler.html>
hat
p-yarn-server-resourcemanager/2.7.1/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java#FairScheduler.resToPreempt%28org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue%2Clong%29>
he
e
but
r
m>
h
g
b is
res
is
e
want
"
James Barney <jamesbarney71@gmail.com>,"Wed, 24 Feb 2016 16:17:47 -0500",Re: ORC file writing hangs in pyspark,Zhan Zhang <zzhang@hortonworks.com>,"Thank you for the suggestions. We looked at the live spark UI and yarn app
logs and found what we think to be the issue: in spark 1.5.2, the FPGrowth
algorithm doesn't require you to specify the number of partitions in your
input data. Without specifying, FPGrowth puts all of its data into one
partition however. Subsequently, only one executor is responsible for
writing the ORC file from the resultant dataframe that FPGrowth puts out.
That's what was causing it to hang.

After specifying the number of partitions in FPGrowth, upon writing, the
writing step continues and finishes quickly.

Thank you again for the suggestions


"
Reynold Xin <rxin@apache.org>,"Wed, 24 Feb 2016 13:50:18 -0800","Spark Summit (San Francisco, June 6-8) call for presentation due in
 less than week","""dev@spark.apache.org"" <dev@spark.apache.org>, user <user@spark.apache.org>","Just want to send a reminder in case people don't know about it. If you are
working on (or with, using) Spark, consider submitting your work to Spark
Summit, coming up in June in San Francisco.

https://spark-summit.org/2016/call-for-presentations/

Cheers.
"
Nezih Yigitbasi <nyigitbasi@netflix.com.INVALID>,"Wed, 24 Feb 2016 22:08:30 +0000",how about a custom coalesce() policy?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Spark devs,

I have sent an email about my problem some time ago where I want to merge a
large number of small files with Spark. Currently I am using Hive with the
CombineHiveInputFormat and I can control the size of the output files with
the max split size parameter (which is used for coalescing the input splits
by the CombineHiveInputFormat). My first attempt was to use coalesce(), but
since coalesce only considers the target number of partitions the output
file sizes were varying wildly.

What I think can be useful is to have an optional PartitionCoalescer
parameter (a new interface) in the coalesce() method (or maybe we can add a
new method ?) that the callers can implement for custom coalescing
strategies â€” for my use case I have already implemented a
SizeBasedPartitionCoalescer that coalesces partitions by looking at their
sizes and by using a max split size parameter, similar to the
CombineHiveInputFormat (I also had to expose HadoopRDD to get access to the
individual split sizes etc.).

What do you guys think about such a change, can it be useful to other users
as well? Or do you think that there is an easier way to accomplish the same
merge logic? If you think it may be useful, I already have an
implementation and I will be happy to work with the community to contribute
it.

Thanks,
Nezih
â€‹
"
Prabhu Joseph <prabhujose.gates@gmail.com>,"Thu, 25 Feb 2016 04:17:50 +0530",Re: Spark Job on YARN Hogging the entire Cluster resource,Hamel Kothari <hamelkothari@gmail.com>,"You are right, Hamel. It should get 10 TB /2. And In hadoop-2.7.0, it is
working fine. But in hadoop-2.5.1, it gets only 10TB/230. The same
configuration used in both versions.
So i think a JIRA could have fixed the issue after hadoop-2.5.1.


o
it
s
ity
e
rScheduler.html>
that
.
op-yarn-server-resourcemanager/2.7.1/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java#FairScheduler.resToPreempt%28org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue%2Clong%29>
e
the
er
r
th
ng
ob is
ores
his
 for
ster
d
 want
y
"
Dushyant Rajput <dushyant.0311@gmail.com>,"Wed, 24 Feb 2016 15:34:39 -0800",Spark HANA jdbc connection issue,dev@spark.apache.org,"Hi,

Will this be resolved in any forthcoming release?

https://issues.apache.org/jira/browse/SPARK-10625

Rgds,
Dushyant.
"
Michael Armbrust <michael@databricks.com>,"Wed, 24 Feb 2016 17:04:59 -0800",Re: Spark 1.6.1,Yin Yang <yy201602@gmail.com>,"Unfortunately I don't think thats sufficient as they don't seem to support
sftp in the same way they did before.  We'll still need to update our
release scripts.


"
Yin Yang <yy201602@gmail.com>,"Wed, 24 Feb 2016 17:44:36 -0800",Re: Spark 1.6.1,Michael Armbrust <michael@databricks.com>,"Have you tried using scp ?

scp file id@people.apache.org

Thanks


"
shane knapp <sknapp@berkeley.edu>,"Wed, 24 Feb 2016 18:08:39 -0800",Re: [build system] additional jenkins downtime next thursday,"amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","the security update has been released, and it's a doozy!

https://wiki.jenkins-ci.org/display/SECURITY/Security+Advisory+2016-02-24

i will be putting jenkins in to quiet mode ~7am PST tomorrow morning
for the upgrade, and expect to be back up and building by 9am PST at
the latest.

amp-jenkins-worker-08 will also be getting a reboot to test out a fix for:
https://github.com/apache/spark/pull/9893

shane


---------------------------------------------------------------------


"
Prabhu Joseph <prabhujose.gates@gmail.com>,"Thu, 25 Feb 2016 12:06:28 +0530",Re: Spark Job on YARN Hogging the entire Cluster resource,Hamel Kothari <hamelkothari@gmail.com>,"YARN-2026 has fixed the issue.


m>
f
 it
is
e
rity
e
he
irScheduler.html>
 that
r
t.
oop-yarn-server-resourcemanager/2.7.1/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java#FairScheduler.resToPreempt%28org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue%2Clong%29>
re
?
 the
wer
ith
ing
job is
cores
3
t
this
e
e for
uster
or. We
hold
"
Zee Chen <zeocio@gmail.com>,"Thu, 25 Feb 2016 00:24:25 -0800",Bug in DiskBlockManager subDirs logic?,dev@spark.apache.org,"Hi,

I am debugging a situation where SortShuffleWriter sometimes fail to
create a file, with the following stack trace:

16/02/23 11:48:46 ERROR Executor: Exception in task 13.0 in stage
47827.0 (TID 1367089)
java.io.FileNotFoundException:
/tmp/spark-9dd8dca9-6803-4c6c-bb6a-0e9c0111837c/executor-129dfdb8-9422-4668-989e-e789703526ad/blockmgr-dda6e340-7859-468f-b493-04e4162d341a/00/temp_shuffle_69fe1673-9ff2-462b-92b8-683d04669aad
(No such file or directory)
        at java.io.FileOutputStream.open0(Native Method)
        at java.io.FileOutputStream.open(FileOutputStream.java:270)
        at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
        at org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:88)
        at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.insertAll(BypassMergeSortShuffleWriter.java:110)
        at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:73)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
        at org.apache.spark.scheduler.Task.run(Task.scala:88)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)


I checked the linux file system (ext4) and saw the /00/ subdir is
missing. I went through the heap dump of the
CoarseGrainedExecutorBackend jvm proc and found that
DiskBlockManager's subDirs list had more non-null 2-hex subdirs than
present on the file system! As a test I created all 64 2-hex subdirs
by hand and then the problem went away.

So had anybody else seen this problem? Looking at the relevant logic
in DiskBlockManager and it hasn't changed much since the fix to
https://issues.apache.org/jira/browse/SPARK-6468

My configuration:
spark-1.5.1, hadoop-2.6.0, standalone, oracle jdk8u60

Thanks,
Zee

---------------------------------------------------------------------


"
lgieron <lgieron@gmail.com>,"Thu, 25 Feb 2016 06:55:49 -0700 (MST)","Eclipse: Wrong project dependencies in generated by ""sbt eclipse""",dev@spark.apache.org,"The Spark projects generated by sbt eclipse plugin have incorrect dependent
projects (as visible on Properties -> Java Build Path -> Projects tab). All
dependent project are missing the ""_2.11"" suffix (for example, it's
""spark-core"" instead of correct ""spark-core_2.11""). This of course causes
the build to fail.

I am using sbteclipse-plugin version 4.0.0.

Has anyone encountered this problem and found a fix?

Thanks,
Lukasz





--

---------------------------------------------------------------------


"
"""Allen Zhang"" <allenzhang010@126.com>","Thu, 25 Feb 2016 22:04:56 +0800 (CST)","Re:Eclipse: Wrong project dependencies in generated by ""sbt
 eclipse""",lgieron <lgieron@gmail.com>,"why not use maven








At 2016-02-25 21:55:49, ""lgieron"" <lgieron@gmail.com> wrote:
>The Spark projects generated by sbt eclipse plugin have incorrect dependent
>projects (as visible on Properties -> Java Build Path -> Projects tab). All
>dependent project are missing the ""_2.11"" suffix (for example, it's
>""spark-core"" instead of correct ""spark-core_2.11""). This of course causes
>the build to fail.
>
>I am using sbteclipse-plugin version 4.0.0.
>
>Has anyone encountered this problem and found a fix?
>
>Thanks,
>Lukasz
>
>
>
>
>
>--
>View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Eclipse-Wrong-project-dependencies-in-generated-by-sbt-eclipse-tp16436.html
>Sent from the Apache Spark Developers List mailing list archive at Nabble.com.
>
>---------------------------------------------------------------------
>To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>For additional commands, e-mail: dev-help@spark.apache.org
>
"
=?UTF-8?B?xYF1a2FzeiBHaWVyb8WE?= <lgieron@gmail.com>,"Thu, 25 Feb 2016 15:13:11 +0100","Re: Eclipse: Wrong project dependencies in generated by ""sbt eclipse""",Allen Zhang <allenzhang010@126.com>,"I've just checked, and ""mvn eclipse:eclipse"" generates incorrect projects
as well.


"
"""Allen Zhang"" <allenzhang010@126.com>","Thu, 25 Feb 2016 22:05:51 +0800 (CST)","Re:Eclipse: Wrong project dependencies in generated by ""sbt
 eclipse""",lgieron <lgieron@gmail.com>,"dev/change-scala-version 2.10 may help you?








At 2016-02-25 21:55:49, ""lgieron"" <lgieron@gmail.com> wrote:
>The Spark projects generated by sbt eclipse plugin have incorrect dependent
>projects (as visible on Properties -> Java Build Path -> Projects tab). All
>dependent project are missing the ""_2.11"" suffix (for example, it's
>""spark-core"" instead of correct ""spark-core_2.11""). This of course causes
>the build to fail.
>
>I am using sbteclipse-plugin version 4.0.0.
>
>Has anyone encountered this problem and found a fix?
>
>Thanks,
>Lukasz
>
>
>
>
>
>--
>View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Eclipse-Wrong-project-dependencies-in-generated-by-sbt-eclipse-tp16436.html
>Sent from the Apache Spark Developers List mailing list archive at Nabble.com.
>
>---------------------------------------------------------------------
>To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>For additional commands, e-mail: dev-help@spark.apache.org
>
"
"""Allen Zhang"" <allenzhang010@126.com>","Thu, 25 Feb 2016 22:14:44 +0800 (CST)","Re: Eclipse: Wrong project dependencies in generated by ""sbt
 eclipse""",=?UTF-8?Q?=C5=81ukasz_Giero=C5=84?= <lgieron@gmail.com>,"
well, I am using IDEA to import the code base.





At 2016-02-25 22:13:11, ""Åukasz GieroÅ„"" <lgieron@gmail.com> wrote:

I've just checked, and ""mvn eclipse:eclipse"" generates incorrect projects as well.



On Thu, Feb 25, 2016 at 3:04 PM, Allen Zhang <allenzhang010@126.com> wrote:

why not use maven








At 2016-02-25 21:55:49, ""lgieron"" <lgieron@gmail.com> wrote:
>The Spark projects generated by sbt eclipse plugin have incorrect dependent
>projects (as visible on Properties -> Java Build Path -> Projects tab). All
>dependent project are missing the ""_2.11"" suffix (for example, it's
>""spark-core"" instead of correct ""spark-core_2.11""). This of course causes
>the build to fail.
>
>I am using sbteclipse-plugin version 4.0.0.
>
>Has anyone encountered this problem and found a fix?
>
>Thanks,
>Lukasz
>
>
>
>
>
>--
>View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Eclipse-Wrong-project-dependencies-in-generated-by-sbt-eclipse-tp16436.html
>Sent from the Apache Spark Developers List mailing list archive at Nabble.com.
>
>---------------------------------------------------------------------
>To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>For additional commands, e-mail: dev-help@spark.apache.org
>





 


"
Yin Yang <yy201602@gmail.com>,"Thu, 25 Feb 2016 06:22:16 -0800","Re: Eclipse: Wrong project dependencies in generated by ""sbt eclipse""",=?UTF-8?B?xYF1a2FzeiBHaWVyb8WE?= <lgieron@gmail.com>,"In yarn/.classpath , I see:
  <classpathentry kind=""src"" path=""/spark-core_2.11""/>

Here is the command I used:

build/mvn clean -Phive -Phive-thriftserver -Pyarn -Phadoop-2.6
-Dhadoop.version=2.7.0 package -DskipTests eclipse:eclipse

FYI


dent
 All
es
51.n3.nabble.com/Eclipse-Wrong-project-dependencies-in-generated-by-sbt-eclipse-tp16436.html
le.com.
"
shane knapp <sknapp@berkeley.edu>,"Thu, 25 Feb 2016 08:15:31 -0800",Re: [build system] additional jenkins downtime next thursday,"amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","this is happening now.


---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Thu, 25 Feb 2016 08:24:28 -0800",Re: [build system] additional jenkins downtime next thursday,"amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","alright, the update is done and worker-08 rebooted.  we're back up and
building already!


---------------------------------------------------------------------


"
=?UTF-8?B?xYF1a2FzeiBHaWVyb8WE?= <lgieron@gmail.com>,"Thu, 25 Feb 2016 19:17:36 +0100","Re: Eclipse: Wrong project dependencies in generated by ""sbt eclipse""",,"Thank you, your version of the mvn invocation (as opposed to mine bare ""mvn
eclipse:eclipse"") worked perfectly.


s
ndent
. All
ses
551.n3.nabble.com/Eclipse-Wrong-project-dependencies-in-generated-by-sbt-eclipse-tp16436.html
ble.com.
"
Reynold Xin <rxin@databricks.com>,"Thu, 25 Feb 2016 15:23:33 -0800",[discuss] DataFrame vs Dataset in Spark 2.0,"""dev@spark.apache.org"" <dev@spark.apache.org>","When we first introduced Dataset in 1.6 as an experimental API, we wanted
to merge Dataset/DataFrame but couldn't because we didn't want to break the
pre-existing DataFrame API (e.g. map function should return Dataset, rather
than RDD). In Spark 2.0, one of the main API changes is to merge DataFrame
and Dataset.

Conceptually, DataFrame is just a Dataset[Row]. In practice, there are two
ways to implement this:

Option 1. Make DataFrame a type alias for Dataset[Row]

Option 2. DataFrame as a concrete class that extends Dataset[Row]


I'm wondering what you think about this. The pros and cons I can think of
are:


Option 1. Make DataFrame a type alias for Dataset[Row]

+ Cleaner conceptually, especially in Scala. It will be very clear what
libraries or applications need to do, and we won't see type mismatches
(e.g. a function expects DataFrame, but user is passing in Dataset[Row]
+ A lot less code
- Breaks source compatibility for the DataFrame API in Java, and binary
compatibility for Scala/Java


Option 2. DataFrame as a concrete class that extends Dataset[Row]

The pros/cons are basically the inverse of Option 1.

+ In most cases, can maintain source compatibility for the DataFrame API in
Java, and binary compatibility for Scala/Java
- A lot more code (1000+ loc)
- Less cleaner, and can be confusing when users pass in a Dataset[Row] into
a function that expects a DataFrame


The concerns are mostly with Scala/Java. For Python, it is very easy to
maintain source compatibility for both (there is no concept of binary
compatibility), and for R, we are only supporting the DataFrame operations
anyway because that's more familiar interface for R users outside of Spark.
"
Chester Chen <chester@alpinenow.com>,"Thu, 25 Feb 2016 15:30:47 -0800",Re: [discuss] DataFrame vs Dataset in Spark 2.0,Reynold Xin <rxin@databricks.com>,"vote for Option 1.
  1)  Since 2.0 is major API, we are expecting some API changes,
  2)  It helps long term code base maintenance with short term pain on Java
side
  3) Not quite sure how large the code base is using Java DataFrame APIs.






"
Michael Malak <michaelmalak@yahoo.com.INVALID>,"Thu, 25 Feb 2016 23:31:50 +0000 (UTC)",Re: [discuss] DataFrame vs Dataset in Spark 2.0,"Reynold Xin <rxin@databricks.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","Would it make sense (in terms of feasibility, code organization, and politically) to have a JavaDataFrame, as a way to isolate the 1000+ extra lines to a Java compatibility layer/class?

      From: Reynold Xin <rxin@databricks.com>
 To: ""dev@spark.apache.org"" <dev@spark.apache.org> 
 Sent: Thursday, February 25, 2016 4:23 PM
 Subject: [discuss] DataFrame vs Dataset in Spark 2.0
   
When we first introduced Dataset in 1.6 as an experimental API, we wanted to merge Dataset/DataFrame but couldn't because we didn't want to break the pre-existing DataFrame API (e.g. map function should return Dataset, rather than RDD). In Spark 2.0, one of the main API changes is to merge DataFrame and Dataset.
Conceptually, DataFrame is just a Dataset[Row]. In practice, there are two ways to implement this:
Option 1. Make DataFrame a type alias for Dataset[Row]
Option 2. DataFrame as a concrete class that extends Dataset[Row]

I'm wondering what you think about this. The pros and cons I can think of are:

Option 1. Make DataFrame a type alias for Dataset[Row]
+ Cleaner conceptually, especially in Scala. It will be very clear what libraries or applications need to do, and we won't see type mismatches (e.g. a function expects DataFrame, but user is passing in Dataset[Row]
+ A lot less code- Breaks source compatibility for the DataFrame API in Java, and binary compatibility for Scala/Java

Option 2. DataFrame as a concrete class that extends Dataset[Row]
The pros/cons are basically the inverse of Option 1.
+ In most cases, can maintain source compatibility for the DataFrame API in Java, and binary compatibility forÂ Scala/Java- A lot more code (1000+ loc)- Less cleaner, and can be confusing when users pass in a Dataset[Row] into a function that expects a DataFrame

The concerns are mostly with Scala/Java. For Python, it is very easy to maintain source compatibility for both (there is no concept of binary compatibility), and for R, we are only supporting the DataFrame operations anyway because that's more familiar interface for R users outside of Spark.



  "
Reynold Xin <rxin@databricks.com>,"Thu, 25 Feb 2016 16:25:39 -0800",Re: [discuss] DataFrame vs Dataset in Spark 2.0,Michael Malak <michaelmalak@yahoo.com>,"It might make sense, but this option seems to carry all the cons of Option
2, and yet doesn't provide compatibility for Java?


"
Koert Kuipers <koert@tresata.com>,"Thu, 25 Feb 2016 19:50:13 -0500",Re: [discuss] DataFrame vs Dataset in Spark 2.0,Reynold Xin <rxin@databricks.com>,"since a type alias is purely a convenience thing for the scala compiler,
does option 1 mean that the concept of DataFrame ceases to exist from a
java perspective, and they will have to refer to Dataset<Row>?


"
Reynold Xin <rxin@databricks.com>,"Thu, 25 Feb 2016 16:52:09 -0800",Re: [discuss] DataFrame vs Dataset in Spark 2.0,Koert Kuipers <koert@tresata.com>,"Yes - and that's why source compatibility is broken.

Note that it is not just a ""convenience"" thing. Conceptually DataFrame is a
Dataset[Row], and for some developers it is more natural to think about
""DataFrame"" rather than ""Dataset[Row]"".

If we were in C++, DataFrame would've been a type alias for Dataset[Row]
too, and some methods would return DataFrame (e.g. sql method).




"
"""Sun, Rui"" <rui.sun@intel.com>","Fri, 26 Feb 2016 07:37:35 +0000",RE: [discuss] DataFrame vs Dataset in Spark 2.0,"Reynold Xin <rxin@databricks.com>, Koert Kuipers <koert@tresata.com>","Vote for option 2.
Source compatibility and binary compatibility are very important from userâ€™s perspective.
It â€˜s unfair for Java developers that they donâ€™t have DataFrame abstraction. As you said, sometimes it is more natural to think about DataFrame.

I am wondering if conceptually there is slight subtle difference between DataFrame and Dataset[Row]? For example,
Dataset[T] joinWith Dataset[U]  produces Dataset[(T, U)]
So,
Dataset[Row] joinWith Dataset[Row]  produces Dataset[(Row, Row)]

While
DataFrame join DataFrame is still DataFrame of Row?

From: Reynold Xin [mailto:rxin@databricks.com]
Sent: Friday, February 26, 2016 8:52 AM
To: Koert Kuipers <koert@tresata.com>
Cc: dev@spark.apache.org
Subject: Re: [discuss] DataFrame vs Dataset in Spark 2.0

Yes - and that's why source compatibility is broken.

Note that it is not just a ""convenience"" thing. Conceptually DataFrame is a Dataset[Row], and for some developers it is more natural to think about ""DataFrame"" rather than ""Dataset[Row]"".

If we were in C++, DataFrame would've been a type alias for Dataset[Row] too, and some methods would return DataFrame (e.g. sql method).



On Thu, Feb 25, 2016 at 4:50 PM, Koert Kuipers <koert@tresata.com<mailto:koert@tresata.com>> wrote:
since a type alias is purely a convenience thing for the scala compiler, does option 1 mean that the concept of DataFrame ceases to exist from a java perspective, and they will have to refer to Dataset<Row>?

On Thu, Feb 25, 2016 at 6:23 PM, Reynold Xin <rxin@databricks.com<mailto:rxin@databricks.com>> wrote:
When we first introduced Dataset in 1.6 as an experimental API, we wanted to merge Dataset/DataFrame but couldn't because we didn't want to break the pre-existing DataFrame API (e.g. map function should return Dataset, rather than RDD). In Spark 2.0, one of the main API changes is to merge DataFrame and Dataset.

Conceptually, DataFrame is just a Dataset[Row]. In practice, there are two ways to implement this:

Option 1. Make DataFrame a type alias for Dataset[Row]

Option 2. DataFrame as a concrete class that extends Dataset[Row]


I'm wondering what you think about this. The pros and cons I can think of are:


Option 1. Make DataFrame a type alias for Dataset[Row]

+ Cleaner conceptually, especially in Scala. It will be very clear what libraries or applications need to do, and we won't see type mismatches (e.g. a function expects DataFrame, but user is passing in Dataset[Row]
+ A lot less code
- Breaks source compatibility for the DataFrame API in Java, and binary compatibility for Scala/Java


Option 2. DataFrame as a concrete class that extends Dataset[Row]

The pros/cons are basically the inverse of Option 1.

+ In most cases, can maintain source compatibility for the DataFrame API in Java, and binary compatibility for Scala/Java
- A lot more code (1000+ loc)
- Less cleaner, and can be confusing when users pass in a Dataset[Row] into a function that expects a DataFrame


The concerns are mostly with Scala/Java. For Python, it is very easy to maintain source compatibility for both (there is no concept of binary compatibility), and for R, we are only supporting the DataFrame operations anyway because that's more familiar interface for R users outside of Spark.




"
Reynold Xin <rxin@databricks.com>,"Thu, 25 Feb 2016 23:55:17 -0800",Re: [discuss] DataFrame vs Dataset in Spark 2.0,"""Sun, Rui"" <rui.sun@intel.com>","The join and joinWith are just two different join semantics, and is not
about Dataset vs DataFrame.

join is the relational join, where fields are flattened; joinWith is more
like a tuple join, where the output has two fields that are nested.

So you can do

Dataset[A] joinWith Dataset[B] = Dataset[(A, B)]

DataFrame[A] joinWith DataFrame[B] = Dataset[(Row, Row)]

Dataset[A] join Dataset[B] = Dataset[Row]

DataFrame[A] join DataFrame[B] = Dataset[Row]




aFrame
he
er
e
o
s
k.
"
Reynold Xin <rxin@databricks.com>,"Fri, 26 Feb 2016 00:01:22 -0800",Re: how about a custom coalesce() policy?,Nezih Yigitbasi <nyigitbasi@netflix.com.invalid>,"I think this can be useful.

The only thing is that we are slowly migrating to the Dataset/DataFrame
API, and leave RDD mostly as is as a lower level API. Maybe we should do
both? In either case it would be great to discuss the API on a pull
request. Cheers.


te
"
Reynold Xin <rxin@databricks.com>,"Fri, 26 Feb 2016 00:02:48 -0800",Re: how about a custom coalesce() policy?,Nezih Yigitbasi <nyigitbasi@netflix.com>,"Using the right email for Nezih



e
e
ute
"
"""Sun, Rui"" <rui.sun@intel.com>","Fri, 26 Feb 2016 10:06:03 +0000",RE: [discuss] DataFrame vs Dataset in Spark 2.0,Reynold Xin <rxin@databricks.com>,"Thanks for the explaination.

What confusing me is the different internal semantic of Dataset on non-Row type (primitive types for example) and Row type:

Dataset[Int] is internally actually Dataset[Row(value:Int)]

scala> val ds = sqlContext.createDataset(Seq(1,2,3))
ds: org.apache.spark.sql.Dataset[Int] = [value: int]

scala> ds.schema.json
res17: String = {""type"":""struct"",""fields"":[{""name"":""value"",""type"":""integer"",""nullable"":false,""metadata"":{}}]}

But obviously Dataset[Row] is not internally in@databricks.com]
Sent: Friday, February 26, 2016 3:55 PM
To: Sun, Rui <rui.sun@intel.com>
Cc: Koert Kuipers <koert@tresata.com>; dev@spark.apache.org
Subject: Re: [discuss] DataFrame vs Dataset in Spark 2.0

The join and joinWith are just two different join semantics, and is not about Dataset vs DataFrame.

join is the relational join, where fields are flattened; joinWith is more like a tuple join, where the output has two fields that are nested.

So you can do

Dataset[A] joinWith Dataset[B] = Dataset[(A, B)]

DataFrame[A] joinWith DataFrame[B] = Dataset[(Row, Row)]

Dataset[A] join Dataset[B] = Dataset[Row]

DataFrame[A] join DataFrame[B] = Dataset[Row]



On Thu, Feb 25, 2016 at 11:37 PM, Sun, Rui <rui.suption 2.
Source compatibility and binary compatibility are very important from userâ€™s perspective.
It â€˜s unfair for Java developers that they donâ€™t have DataFrame abstraction. As you said, sometimes it is more natural to think about DataFrame.

I am wondering if conceptually there is slight subtle difference between DataFrame and Dataset[Row]? For example,
Dataset[T] joinWith Dataset[U]  produces Dataset[(T, U)]
So,
Dataset[Row] joinWith Dataset[Row]  produces Dataset[(Row, Row)]

While
DataFrame join DataFrame is still DataFrame of Row?

From: Ricks.com>]
Sent: Friday, February 26, 2016 8:52 AM
To: Koert Kuipers <koert@tresata.com<mailto:koert@tresata.com>>
Cc: dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: [discuss] DataFrame vs Dataset in Spark 2.0

Yes - and that's why source compatibility is broken.

Note that it is not just a ""convenience"" thing. Conceptually DataFrame is a Dataset[Row], and for some developers it is more natural to think about ""DataFrame"" rather than ""Dataset[Row]"".

If we were in C++, DataFrame would've been a type alias for Dataset[Row] too, and some methods would return DataFrame (e.g. sql method).



On Thu, Feb 25, 2016 at 4:50 PM, Koert Kuipers <koert@tresata.com<mailto:koert@tresata.com>> wrote:
since a type alias is purely a convenience thing for the scala compiler, does option 1 mean that the concept of DataFrame ceases to exist from a java perspective, and they will have to refer to Dataset<Row>?

On Thu, Feb 25, 2016 at 6:23 PM, om>> wrote:
When we first introduced Dataset in 1.6 as an experimental API, we wanted to merge Dataset/DataFrame but couldn't because we didn't want to break the pre-existing DataFrame API (e.g. map function should return Dataset, rather than RDD). In Spark 2.0, one of the main API changes is to merge DataFrame and Dataset.

Conceptually, DataFrame is just a Dataset[Row]. In practice, there are two ways to implement this:

Option 1. Make DataFrame a type alias for Dataset[Row]

Option 2. DataFrame as a concrete class that extends Dataset[Row]


I'm wondering what you think about this. The pros and cons I can think of are:


Option 1. Make DataFrame a type alias for Dataset[Row]

+ Cleaner conceptually, especially in Scala. It will be very clear what libraries or applications need to do, and we won't see type mismatches (e.g. a function expects DataFrame, but user is passing in Dataset[Row]
+ A lot less code
- Breaks source compatibility for the DataFrame API in Java, and binary compatibility for Scala/Java


Option 2. DataFrame as a concrete class that extends Dataset[Row]

The pros/cons are basically the inverse of Option 1.

+ In most cases, can maintain source compatibility for the DataFrame API in Java, and binary compatibility for Scala/Java
- A lot more code (1000+ loc)
- Less cleaner, and can be confusing when users pass in a Dataset[Row] into a function that expects a DataFrame


The concerns are mostly with Scala/Java. For Python, it is very easy to maintain source compatibility for both (there is no concept of binary compatibility), and for R, we are only supporting the DataFrame operations anyway because that's more familiar interface for R users outside of Spark.





"
Teng Qiu <tengqiu@gmail.com>,"Fri, 26 Feb 2016 11:43:20 +0100",Re: DirectFileOutputCommiter,"Takeshi Yamamuro <linguin.m.s@gmail.com>, dev@spark.apache.org","Hi, thanks :) performance gain is huge, we have a INSERT INTO query, ca.
30GB in JSON format will be written to s3 at the end, without
DirectOutputCommitter and our hack in hive and InsertIntoHiveTable.scala,
it took more than 40min, with our changes, only 15min then.

DirectOutputCommitter works for SparkContext and SqlContext, but for
HiveContext, it only solved the problem with ""staging folder"" in target
table, problem for HiveContext is here:
https://github.com/apache/spark/blob/v1.6.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/InsertIntoHiveTable.scala#L132

besides staging folder created by Committer, Hive will use a temp location
as well... so we made some hack on this:
https://github.com/apache/spark/compare/branch-1.6...zalando:branch-1.6-zalando#diff-d579db9a8f27e0bbef37720ab14ec3f6R134

mainly idea is, we added an internal var runID, and use HiveConf
spark.hive.insert.skip.temp to disable Hive to use temp location, but with
this hack, we need to change Hive's implementation... we put our Hive.java
file under
sql/hive/src/main/java/org/apache/hadoop/hive/ql/metadata/Hive.java

you can find the full change using this link:
https://github.com/apache/spark/compare/branch-1.6...zalando:branch-1.6-zalando#diff-d579db9a8f27e0bbef37720ab14ec3f6R134


i would like to forward this discuss to spark-dev, hope spark team can
think about it, and hope there will be a better solution for this, like
some more official hack :D


2016-02-26 7:24 GMT+01:00 Takeshi Yamamuro <linguin.m.s@gmail.com>:

"
Jeff Zhang <zjffdu@gmail.com>,"Fri, 26 Feb 2016 18:44:30 +0800",Is spark.driver.maxResultSize used correctly ?,"""user@spark.apache.org"" <user@spark.apache.org>, dev <dev@spark.apache.org>","My job get this exception very easily even when I set large value of
spark.driver.maxResultSize. After checking the spark code, I found
spark.driver.maxResultSize is also used in Executor side to decide whether
DirectTaskResult/InDirectTaskResult sent. This doesn't make sense to me.
Using  spark.driver.maxResultSize / taskNum might be more proper. Because
if  spark.driver.maxResultSize is 1g and we have 10 tasks each has 200m
output. Then even the output of each task is less than
 spark.driver.maxResultSize so DirectTaskResult will be sent to driver, but
the total result size is 2g which will cause exception in driver side.


16/02/26 10:10:49 INFO DAGScheduler: Job 4 failed: treeAggregate at
LogisticRegression.scala:283, took 33.796379 s

Exception in thread ""main"" org.apache.spark.SparkException: Job aborted due
to stage failure: Total size of serialized results of 1 tasks (1085.0 MB)
is bigger than spark.driver.maxResultSize (1024.0 MB)


-- 
Best Regards

Jeff Zhang
"
=?UTF-8?B?SmnFmcOtIFN5cm92w70=?= <syrovy.jiri@gmail.com>,"Fri, 26 Feb 2016 15:11:19 +0100",Fwd: Aggregation + Adding static column + Union + Projection = Problem,dev@spark.apache.org,"Hi,

I've recently noticed a bug in Spark (branch 1.6) that appears if you do
the following

Let's have some DataFrame called df.

1) Aggregation of multiple columns on the Dataframe df and store result as
result_agg_1
2) Do another aggregation of multiple columns, but on one less grouping
columns and store the result as result_agg_2
3) Align the result of second aggregation by adding missing grouping column
with value empty lit("""")
4) Union result_agg_1 and result_agg_2
5) Do the projection from ""sum(count_column)"" to ""count_column"" for all
aggregated columns.

The result is structurally inconsistent DataFrame that has all the data
coming from result_agg_1 shifted.

An example of stripped down code and example result can be seen here:

https://gist.github.com/xjrk58/e0c7171287ee9bdc8df8
https://gist.github.com/xjrk58/7a297a42ebb94f300d96

Best,
Jiri Syrovy
"
=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@questtec.nl>,"Fri, 26 Feb 2016 15:15:28 +0100",Re: Aggregation + Adding static column + Union + Projection = Problem,jrk@hkfree.org,"Hi JiÅ™Ã­,

Thanks for your mail.

Could you create a JIRA ticket for this:
 https://issues.apache.org/jira/browse/SPARK/?selectedTab=com.atlassian.jira.jira-projects-plugin:summary-panel
<https://issues.apache.org/jira/browse/SPARK/?selectedTab=com.atlassian.jira.jira-projects-plugin:summary-panel>

<https://issues.apache.org/jira/browse/SPARK/?selectedTab=com.atlassian.jira.jira-projects-plugin:summary-panel>
?

Kind regards,

Herman van HÃ¶vell


2016-02-26 15:11 GMT+01:00 JiÅ™Ã­ SyrovÃ½ <syrovy.jiri@gmail.com>:

s
"
Jong Wook Kim <ilikekjw@gmail.com>,"Fri, 26 Feb 2016 10:47:13 -0500",make-distribution.sh fails because tachyon-project was renamed to Alluxio,dev@spark.apache.org,"Hi,

Spark's packaging script downloads tachyon from tachyon-project.org
<https://github.com/apache/spark/blob/master/make-distribution.sh#L38>
which is now redirected to alluxio.org.

I guess the url should be changed to
http://alluxio.org/downloads/files/0.8.2/ is it right?

Jong Wook
"
Hamel Kothari <hamelkothari@gmail.com>,"Fri, 26 Feb 2016 16:44:33 +0000",More Robust DataSource Parameters,dev <dev@spark.apache.org>,"Hi devs,

Has there been any discussion around changing the DataSource parameters
arguments be something more sophisticated than Map[String, String]? As you
write more complex DataSources there are likely to be a variety of
parameters of varying formats which are needed and having to coerce them to
be strings becomes suboptimal pretty fast.

Quite often I see this combated by people specifying parameters which take
in Json strings and then parse them into the parameter objects that they
actually need. Unfortunately having people write Json strings can be a
really error prone process so to ensure compile time safety people write
convenience functions written which take in actual POJOs as parameters,
serialize them to json so they can be passed into the data source API and
then deserialize them in the constructors of their data sources. There's
also no real story around discoverability of options with the current
Map[String, String] setup other than looking at the source code of the
datasource and hoping that they specified constants somewhere.

Rather than doing all of the above, we could adapt the DataSource API to
have RelationProviders be templated on a parameter class which could be
create the appropriate configuration object and provide that object to the
DataFrameReader.parameters call and it would be possible to guarantee that
enough parameters were provided to construct a DataFrame in that case.

The key challenge I see with this approach is that I'm not sure how to make
the above changes in a backwards compatible way that doesn't involve
duplicating a bunch of methods.

Do people have thoughts regarding this approach? I'm happy to file a JIRA
and have the discussion there if it makes sense.

Best,
Hamel
"
Ted Yu <yuzhihong@gmail.com>,"Fri, 26 Feb 2016 08:55:40 -0800",Re: Hbase in spark,Renu Yadav <yrenu21@gmail.com>,"In hbase, there is hbase-spark module which supports bulk load.
This module is to be backported in the upcoming 1.3.0 release.

There is some pending work, such as HBASE-15271 .

FYI


"
Sean Owen <sowen@cloudera.com>,"Fri, 26 Feb 2016 17:03:48 +0000",Re: make-distribution.sh fails because tachyon-project was renamed to Alluxio,Jong Wook Kim <ilikekjw@gmail.com>,"Yes, though more broadly, should this just be removed for 2.x? I had
this sense Tachyon was going away, or at least being put into a corner
of the project. There's probalby at least no need for special builds
for it.


---------------------------------------------------------------------


"
=?UTF-8?B?SmnFmcOtIMWgaW3FoWE=?= <jiri@alluxio.com>,"Fri, 26 Feb 2016 09:19:37 -0800",Re: make-distribution.sh fails because tachyon-project was renamed to Alluxio,Jong Wook Kim <ilikekjw@gmail.com>,"Hi Jong,

Thank you for pointing that out. I am one of the maintainers of the Alluxio
project, formerly known as Tachyon, and will make sure that the old
download links still work. I will update this thread when it is fixed.

future version) as there is no longer a compile time dependency.

Best,

JiÅ™Ã­ Å imÅ¡a
Alluxio, Inc. <http://alluxio.com/>


"
Ted Malaska <ted.malaska@cloudera.com>,"Fri, 26 Feb 2016 12:23:02 -0500",Re: Hbase in spark,Ted Yu <yuzhihong@gmail.com>,"Yes, and I have used HBASE-15271 and successful loaded over 20 billion
records into HBase even with node failures.


"
Mark Grover <mark@apache.org>,"Fri, 26 Feb 2016 09:46:17 -0800",Upgrading to Kafka 0.9.x,dev@kafka.apache.org,"Hi Kafka devs,
I come to you with a dilemma and a request.

Based on what I understand, users of Kafka need to upgrade their brokers to
Kafka 0.9.x first, before they upgrade their clients to Kafka 0.9.x.

However, that presents a problem to other projects that integrate with
Kafka (Spark, Flume, Storm, etc.). From here on, I will speak for Spark +
Kafka, since that's the one I am most familiar with.

In the light of compatibility (or the lack thereof) between 0.8.x and
0.9.x, Spark is faced with a problem of what version(s) of Kafka to be
compatible with, and has 2 options (discussed in this PR
<https://github.com/apache/spark/pull/11143>):
1. We either upgrade to Kafka 0.9, dropping support for 0.8. Storm and
Flume are already on this path.
2. We introduce complexity in our code to support both 0.8 and 0.9 for the
entire duration of our next major release (Apache Spark 2.x).

I'd love to hear your thoughts on which option, you recommend.

Long term, I'd really appreciate if Kafka could do something that doesn't
make Spark having to support two, or even more versions of Kafka. And, if
there is something that I, personally, and Spark project can do in your
next release candidate phase to make things easier, please do let us know.

Thanks!
Mark
"
=?UTF-8?B?SmnFmcOtIMWgaW3FoWE=?= <jiri@alluxio.com>,"Fri, 26 Feb 2016 11:08:40 -0800",Re: make-distribution.sh fails because tachyon-project was renamed to Alluxio,Jong Wook Kim <ilikekjw@gmail.com>,"Hi Jong, the download links should be fixed now.

Best,


ld
:


-- 
JiÅ™Ã­ Å imÅ¡a
Alluxio, Inc. <http://alluxio.com>
"
Reynold Xin <rxin@databricks.com>,"Fri, 26 Feb 2016 11:49:07 -0800","External dependencies in public APIs (was previously: Upgrading to
 Kafka 0.9.x)",dev <dev@spark.apache.org>,"Dropping Kafka list since this is about a slightly different topic.

Every time we expose the API of a 3rd party application as a public Spark
API has caused some problems down the road. This goes from Hadoop, Tachyon,
Kafka, to Guava. Most of these are used for input/output.

The good thing is that in Spark 2.0 we are removing most of those
exposures, and in the new DataFrame/Dataset API we are providing an unified
input/output API for end-users so the internals of the 3rd party
dependencies are no longer exposed directly to users. Unfortunately, some
Spark APIs still depend on Hadoop.

It is important to keep this in mind as we develop Spark. We should avoid
to the best degree possible exposing other projects' APIs for the long term
stability of Spark APIs.



"
Reynold Xin <rxin@databricks.com>,"Fri, 26 Feb 2016 11:51:01 -0800",Re: [discuss] DataFrame vs Dataset in Spark 2.0,"""Sun, Rui"" <rui.sun@intel.com>","That's actually not Row vs non-Row.

It's just primitive vs non-primitive. Primitives get automatically
flattened, to avoid having to type ._1 all the time.


w
lse,""metadata"":{}}]}
aFrame
he
er
e
o
s
k.
"
Reynold Xin <rxin@databricks.com>,"Fri, 26 Feb 2016 14:02:12 -0800",Re: More Robust DataSource Parameters,Hamel Kothari <hamelkothari@gmail.com>,"Thanks for the email. This sounds great in theory, but might run into two
major problems:

1. Need to support 4+ programming languages (SQL, Python, Java, Scala)

2. API stability (both backward and forward)




"
Jakob Odersky <jakob@odersky.com>,"Fri, 26 Feb 2016 15:24:12 -0800",Re: [discuss] DataFrame vs Dataset in Spark 2.0,Reynold Xin <rxin@databricks.com>,"I would recommend (non-binding) option 1.

Apart from the API breakage I can see only advantages, and that sole
disadvantage is minimal for a few reasons:

1. the DataFrame API has been ""Experimental"" since its implementation,
so no stability was ever implied
2. considering that the change is for a major release some
incompatibilities are to be expected
3. using type aliases may break code now, but it will remove the
possibility of library incompatibilities in the future (see Reynold's
second point ""[...] and we won't see type mismatches (e.g. a function
expects DataFrame, but user is passing in Dataset[Row]"")

ow
alse,""metadata"":{}}]}
e
taFrame
s
t
:
java
:
d
the
her
me
wo
f
e.g.
ns
rk.

---------------------------------------------------------------------


"
Mark Grover <mark@apache.org>,"Fri, 26 Feb 2016 16:22:47 -0800",Re: Upgrading to Kafka 0.9.x,dev@kafka.apache.org,"Thanks Jay. Yeah, if we were able to use the old consumer API from 0.9
clients to work with 0.8 brokers that would have been super helpful here. I
am just trying to avoid a scenario where Spark cares about new features
from every new major release of Kafka (which is a good thing) but ends up
having to keep multiple profiles/artifacts for it - one for 0.8.x, one for
0.9.x and another one, once 0.10.x gets released.

So, anything that the Kafka community can do to alleviate the situation
down the road would be great. Thanks again!


"
Joel Koshy <jjkoshy.w@gmail.com>,"Fri, 26 Feb 2016 16:40:29 -0800",Re: Upgrading to Kafka 0.9.x,"""dev@kafka.apache.org"" <dev@kafka.apache.org>","The 0.9 release still has the old consumer as Jay mentioned but this
specific release is a little unusual in that it also provides a completely
new consumer client.

Based on what I understand, users of Kafka need to upgrade their brokers to



However, that presents a problem to other projects that integrate with


This is true and we faced a similar issue at LinkedIn - there are scenarios
where it is useful/necessary to allow the client to be upgraded before the
broker. This improvement
<https://cwiki.apache.org/confluence/display/KAFKA/KIP-35+-+Retrieving+protocol+version>
can help with that although if users want to leverage newer server-side
features they would obviously need to upgrade the brokers.

Thanks,

Joel


"
Josh Rosen <joshrosen@databricks.com>,"Sat, 27 Feb 2016 02:44:52 +0000",Re: Spark 1.6.1,"Yin Yang <yy201602@gmail.com>, Michael Armbrust <michael@databricks.com>","I updated the release packaging scripts to use SFTP via the *lftp* client:
https://github.com/apache/spark/pull/11350

I'm starting the process of cutting a 1.6.1-RC1 tag and release artifacts
right now, so please be extra careful about merging into branch-1.6 until
the list to start a vote thread.

- Josh


"
Koert Kuipers <koert@tresata.com>,"Sat, 27 Feb 2016 00:54:58 -0500",some joins stopped working with spark 2.0.0 SNAPSHOT,"""dev@spark.apache.org"" <dev@spark.apache.org>","dataframe df1:
schema:
StructType(StructField(x,IntegerType,true))
explain:
== Physical Plan ==
MapPartitions <function1>, obj#135: object, [if (input[0, object].isNullAt)
null else input[0, object].get AS x#128]
+- MapPartitions <function1>, createexternalrow(if (isnull(x#9)) null else
x#9), [input[0, object] AS obj#135]
   +- WholeStageCodegen
      :  +- Project [_1#8 AS x#9]
      :     +- Scan ExistingRDD[_1#8]
show:
+---+
|  x|
+---+
|  2|
|  3|
+---+


dataframe df2:
schema:
StructType(StructField(x,IntegerType,true), StructField(y,StringType,true))
explain:
== Physical Plan ==
MapPartitions <function1>, createexternalrow(x#2, if (isnull(y#3)) null
else y#3.toString), [if (input[0, object].isNullAt) null else input[0,
object].get AS x#130,if (input[0, object].isNullAt) null else
staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType,
fromString, input[0, object].get, true) AS y#131]
+- WholeStageCodegen
   :  +- Project [_1#0 AS x#2,_2#1 AS y#3]
   :     +- Scan ExistingRDD[_1#0,_2#1]
show:
+---+---+
|  x|  y|
+---+---+
|  1|  1|
|  2|  2|
|  3|  3|
+---+---+


i run:
df1.join(df2, Seq(""x"")).show

i get:
java.lang.UnsupportedOperationException: No size estimation available for
objects.
at org.apache.spark.sql.types.ObjectType.defaultSize(ObjectType.scala:41)
at
org.apache.spark.sql.catalyst.plans.logical.UnaryNode$$anonfun$6.apply(LogicalPlan.scala:323)
at
org.apache.spark.sql.catalyst.plans.logical.UnaryNode$$anonfun$6.apply(LogicalPlan.scala:323)
at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245)
at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245)
at scala.collection.immutable.List.foreach(List.scala:381)
at scala.collection.TraversableLike$class.map(TraversableLike.scala:245)
at scala.collection.immutable.List.map(List.scala:285)
at
org.apache.spark.sql.catalyst.plans.logical.UnaryNode.statistics(LogicalPlan.scala:323)
at
org.apache.spark.sql.execution.SparkStrategies$CanBroadcast$.unapply(SparkStrategies.scala:87)

now sure what changed, this ran about a week ago without issues (in our
internal unit tests). it is fully reproducible, however when i tried to
minimize the issue i could not reproduce it by just creating data frames in
the repl with the same contents, so it probably has something to do with
way these are created (from Row objects and StructTypes).

best, koert
"
Reynold Xin <rxin@databricks.com>,"Sat, 27 Feb 2016 00:49:08 -0800",Re: some joins stopped working with spark 2.0.0 SNAPSHOT,Koert Kuipers <koert@tresata.com>,"Can you file a JIRA ticket?


"
Reynold Xin <rxin@databricks.com>,"Sat, 27 Feb 2016 00:51:21 -0800",Re: Is spark.driver.maxResultSize used correctly ?,Jeff Zhang <zjffdu@gmail.com>,"But sometimes you might have skew and almost all the result data are in one
or a few tasks though.


"
Tarek Elgamal <tarek.elgamal@gmail.com>,"Sat, 27 Feb 2016 02:58:48 -0600",Spark Checkpointing behavior,dev <dev@spark.apache.org>,"Hi,

I am trying to understand the behavior of rdd.checkpoint() in Spark. I am
running the JavaPageRank
<https://github.com/apache/spark/blob/master/examples/src/main/java/org/apache/spark/examples/JavaPageRank.java>
example on a 1 GB graph and I am checkpointing the *ranks *rdd inside each
iteration (between line 125 and 126 in the given link). Spark execution
starts when it hits the *collect()* action. I am expecting that after each
iteration the intermediate ranks will be materialized and written in the
checkpoint dir but, it seems that the rdd is only written once in the end
of the program, although I am invoking ranks.checkpoint() inside the for
loop. Is that the default behavior ?

Note that I am caching the rdd before checkpointing in order to avoid
recomputing

Best Regards,
Tarek
"
longsonr <longsonr@gmail.com>,"Sat, 27 Feb 2016 09:27:45 -0700 (MST)",beeline and spark-defaults.conf,dev@spark.apache.org,"I'd like to be able to pass a java define to beeline (to configure jline). I
tried setting

spark.driver.extraJavaOptions

in my spark-defaults.conf file, however beeline does not seem to read this
file, it picks up settings from the SPARK_JAVA_OPTS environment variable
instead. 

If I changed the SparkClassCommandBuilder.java to read from
spark-defaults.conf (as well as the environment variable), would that be a
reasonable thing to do? 

I'm happy to provide a patch or a merge request if I'm doing something
reasonable here.

Best regards

Robert

patch.txt
<http://apache-spark-developers-list.1001551.n3.nabble.com/file/n16479/patch.txt>  




--

---------------------------------------------------------------------


"
Hamel Kothari <hamelkothari@gmail.com>,"Sat, 27 Feb 2016 18:12:15 +0000",Re: More Robust DataSource Parameters,Reynold Xin <rxin@databricks.com>,"Thanks for the flags Reynold.

1. For the 4+ languages, these are just on the consumption side (i.e. you
can't write a data source in Python or SQL, correct), right? ? If this is
correct and you can only write data sources in the JVM languages than that
the configuration object is JSON deserializable.

Then on the consumption side (ie. from sqlContext.read):
 - From Java/Scala these objects can be passed through to the DataSource
natively since it's in the same JVM and people have access to the concrete
parameter classes.
deserialized and could be forced to generate explicit serialization
failures when insufficient options are provided. The datasource provide
could even (optionally) provide a python object which performs validation
on the python side to make this easier for consumers.
- In the SQL instance, since these objects are JSON serializable, we can
alter the OPTIONS keyword to allow nested maps to create the JSON object.

In all of these cases the solution proposed still worst case degrades to
something equivalent to the Map[String, String] (except that it has nesting
support), but in the best cases we have POJOs and optionally provided
python objects which help facilitate this in a first class fashion.

2. Yeah agree this is a big problem, which is why I flagged it in the
initial email. I'll put some more thought into how this can be done in a
reasonable fashion (although any sugguestions wouild be greatly
appreciated).

With the above answer to #1 and contingent on finding a solution to the API
stability part of it, would you be supportive of a change to do this? If
so, I'll submit a JIRA first and solicit/brainstorm some ideas on how to do
#2 in a more sane way.


"
Koert Kuipers <koert@tresata.com>,"Sat, 27 Feb 2016 14:06:48 -0500",Re: some joins stopped working with spark 2.0.0 SNAPSHOT,Reynold Xin <rxin@databricks.com>,"https://issues.apache.org/jira/browse/SPARK-13531


"
Jonathan Kelly <jonathakamzn@gmail.com>,"Sat, 27 Feb 2016 19:26:33 +0000",Re: some joins stopped working with spark 2.0.0 SNAPSHOT,"Koert Kuipers <koert@tresata.com>, Reynold Xin <rxin@databricks.com>","If you want to find what commit caused it, try out the ""git bisect"" command.

"
Prabhu Joseph <prabhujose.gates@gmail.com>,"Sun, 28 Feb 2016 02:10:27 +0530",Spark log4j fully qualified class name,Spark dev list <dev@spark.apache.org>,"Hi All,

    When i change the spark log4j.properties conversion pattern to know the
fully qualified class name, all the logs has the FQCN as
org.apache.spark.Logging. The actual fully qualified class name is
overwritten.

log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p
*%C:* %m%n


16/02/27 15:34:40 INFO org.apache.spark.Logging$class: Successfully started
service 'sparkDriver' on port 49482.
16/02/27 15:34:40 DEBUG org.apache.spark.Logging$class: Using serializer:
class org.apache.spark.serializer.JavaSerializer
16/02/27 15:34:40 INFO org.apache.spark.Logging$class: Registering
MapOutputTracker
16/02/27 15:34:40 INFO org.apache.spark.Logging$class: Registering
BlockManagerMaster
16/02/27 15:34:40 ERROR org.apache.spark.Logging$class: Failed to create
dir in maprfs:///var/mapr/local/prabhuFS2/spark. Ignoring this directory.
16/02/27 15:34:40 ERROR org.apache.spark.Logging$class: Failed to create
any local dir.
16/02/27 15:34:40 INFO org.apache.spark.Logging$class: Shutdown hook called
16/02/27 15:34:40 INFO org.apache.spark.Logging$class: Deleting directory
/tmp/spark-5544c349-0393-4bd0-8aab-c20331a9a1cf


Thanks,
Prabhu Joseph
"
Alexander Pivovarov <apivovarov@gmail.com>,"Sat, 27 Feb 2016 13:31:36 -0800","spark yarn exec container fails if yarn.nodemanager.local-dirs value
 starts with file://",dev <dev@spark.apache.org>,"Spark yarn executor container fails if yarn.nodemanager.local-dirs starts
with file://

   <property>
     <name>yarn.nodemanager.local-dirs</name>
     <value>file:///data01/yarn/nm,file:///data02/yarn/nm</value>
   </property>

other application, e.g. Hadoop MR and Hive work normally

Spark works only if yarn.nodemanager.local-dirs does not have file:// prefix
e.g.
    <value>/data01/yarn/nm,/data02/yarn/nm</value>

https://issues.apache.org/jira/browse/SPARK-13532
"
Ted Yu <yuzhihong@gmail.com>,"Sat, 27 Feb 2016 14:16:13 -0800",Re: Spark log4j fully qualified class name,Prabhu Joseph <prabhujose.gates@gmail.com>,"Looking at
https://logging.apache.org/log4j/1.2/apidocs/org/apache/log4j/PatternLayout.html

*WARNING* Generating the caller class information is slow. Thus, use should
be avoided unless execution speed is not an issue.


"
Prabhu Joseph <prabhujose.gates@gmail.com>,"Sun, 28 Feb 2016 04:28:49 +0530",Re: Spark log4j fully qualified class name,Ted Yu <yuzhihong@gmail.com>,"Ted,

     That was a useful information. But i want it to use in development
is not printed instead org.apache.spark.Logging overwrites. Is it a
intended change in Spark.




"
Minudika Malshan <minudika001@gmail.com>,"Mon, 29 Feb 2016 00:02:05 +0530",Implementing Bagging ensemble method using spark.mlLib,dev@spark.apache.org,"Hi,

I found out that ml Lib supports two ensemble algorithms, GBT and Random
Forest.
I want to implement Bagging method using ml Lib features.
Can you give me the location that contains the implementation of GBT and
Random Forest methods in the repository.
Also I'm grateful if you can give me some resources to getting started with
implementation of Bagging method using ml Lib functionalities.

Thanks and regards.
Minudika

Minudika Malshan
Undergraduate
Department of Computer Science and Engineering
University of Moratuwa.
"
Jeff Zhang <zjffdu@gmail.com>,"Mon, 29 Feb 2016 08:17:01 +0800",Re: Is spark.driver.maxResultSize used correctly ?,Reynold Xin <rxin@databricks.com>,"data skew might be possible, but not the common case. I think we should
design for the common case, for the skew case, we may can set some
parameter of fraction to allow user to tune it.




-- 
Best Regards

Jeff Zhang
"
Niranda Perera <niranda.perera@gmail.com>,"Mon, 29 Feb 2016 11:20:27 +0530",Control the stdout and stderr streams in a executor JVM,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,

Is there any possibility to control the stdout and stderr streams in an
executor JVM?

I understand that there are some configurations provided from the spark
conf as follows
spark.executor.logs.rolling.maxRetainedFiles
spark.executor.logs.rolling.maxSize
spark.executor.logs.rolling.strategy
spark.executor.logs.rolling.time.interval

But is there a possibility to have more fine grained control over these,
like we do in a log4j appender, with a property file?

Rgds
-- 
Niranda
@n1r44 <https://twitter.com/N1R44>
+94-71-554-8430
https://pythagoreanscript.wordpress.com/
"
Jeff Zhang <zjffdu@gmail.com>,"Mon, 29 Feb 2016 14:54:25 +0800",Re: Control the stdout and stderr streams in a executor JVM,Niranda Perera <niranda.perera@gmail.com>,"You can create log4j.properties for executors, and use ""--files
log4j.properties"" when submitting spark jobs.





-- 
Best Regards

Jeff Zhang
"
Anuruddha Premalal <anuruddhapremalal@gmail.com>,"Mon, 29 Feb 2016 15:24:47 +0530",Re: Control the stdout and stderr streams in a executor JVM,dev@spark.apache.org,"Hi,

You can create log4j.properties for executors, and use ""--files



In the case when we are initializing spark context via java, how can we
pass the same parameter?

jsc = new JavaSparkContext(conf);

Is it possible to set this parameter in spark-defaults.conf?

spark jobs.



Regards,
-- 
*Anuruddha Premalala (MIEEE)Mobile : +94717213122E-mail  :
anuruddhapremalal@gmail.com <anuruddhapremalal@gmail.com>web      :
www.anuruddha.org <http://www.anuruddha.org>Sri Lanka.*
"
Steve Loughran <stevel@hortonworks.com>,"Mon, 29 Feb 2016 10:09:52 +0000",Re: Spark log4j fully qualified class name,Spark dev list <dev@spark.apache.org>,"
On 27 Feb 2016, at 20:40, Prabhu Joseph <prabhujose.gates@gmail.com<mailto:prabhujose.gates@gmail.com>> wrote:

Hi All,

    When i change the spark log4j.properties conversion pattern to know the fully qualified class name, all the logs has the FQCN as org.apache.spark.Logging. The actual fully qualified class name is overwritten.

log4j.appender.console.layout.ConversionPattern=%d{yy/MM

log4j.appender.stdout.layout.ConversionPattern=%d{ISO8601} [%t] %-5p %c (%F:%M(%L)) - %m%n

Gives me things like

2016-02-23 18:54:24,645 [EventPoster] DEBUG org.apache.spark.deploy.history.yarn.YarnHistoryService (Logging.scala:logDebug(58)) - entity successfully posted
2016-02-23 18:54:24,645 [EventPoster] DEBUG org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter (FileSystemTimelineWriter.java:flush(286)) - Flushing cache
2016-02-23 18:54:24,646 [EventPoster] DEBUG org.apache.spark.deploy.history.yarn.YarnHistoryService (Logging.scala:logDebug(58)) - Queue stopped
2016-02-23 18:54:24,646 [EventPoster] DEBUG org.apache.spark.deploy.history.yarn.YarnHistoryService (Logging.scala:logDebug(58)) - Queue shutdown, time limit= 18:54
2016-02-23 18:54:24,646 [EventPoster] INFO  org.apache.spark.deploy.history.yarn.YarnHistoryService (Logging.scala:logInfo(54)) - Stopping dequeue service, final queue size is 0; outstanding events to post count: 0
2016-02-23 18:54:24,646 [EventPoster] DEBUG org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter (FileSystemTimelineWriter.java:close(276)) - Closing cache
2016-02-23 18:54:24,648 [ScalaTest-main-running-IncompleteApplicationSuite] INFO  org.apache.spark.deploy.history.yarn.testtools.YarnTestUtils (Logging.scala:logInfo(54)) - awaitServiceThreadStopped

Getting that thread in there is invaluable when there are lots of threads about, having the line number in lets you track down where the code is.  Here you can see that there was a background thread [EventPoster], alongside the test thread itself


As ted points out, this crawls in production, as it creates an exception on every log, so that the stack trace can be examined â€”you would only use it in production in an emergency. For test runs though, wonderful.
"
Deepak Gopalakrishnan <dgkris@gmail.com>,"Mon, 29 Feb 2016 21:15:28 +0530",Mapper side join with DataFrames API,dev@spark.apache.org,"Hello,

I'm trying to join 2 dataframes A and B with a

sqlContext.sql(""SELECT * FROM A INNER JOIN B ON A.a=B.a"");

Now what I have done is that I have registeredTempTables for A and B after
loading these DataFrames from different sources. I need the join to be
really fast and I was wondering if there is a way to use the SQL statement
and then being able to do a mapper side join ( say my table B is small) ?

I read some articles on using broadcast to do mapper side joins. Could I do
something like this and then execute my sql statement to achieve mapper
side join ?

DataFrame B = sparkContext.broadcast(B);
B.registerTempTable(""B"");



-- 
Regards,
*Deepak Gopalakrishnan*
*Mobile*:+918891509774
*Skype* : deepakgk87
http://myexps.blogspot.com
"
Deepak Gopalakrishnan <dgkris@gmail.com>,"Mon, 29 Feb 2016 22:44:36 +0530",Re: Mapper side join with DataFrames API,dev@spark.apache.org,"Hello All,

Just to add to this question a bit more context....

I have a join as stated above and I see in my executor logs the below :

16/02/29 17:02:35 INFO TaskSetManager: Finished task 198.0 in stage 7.0
(TID 1114) in 20354 ms on localhost (196/200)

16/02/29 17:02:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty
blocks out of 200 blocks

16/02/29 17:02:35 INFO ShuffleBlockFetcherIterator: Started 0 remote
fetches in 0 ms

16/02/29 17:02:35 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty
blocks out of 128 blocks

16/02/29 17:02:35 INFO ShuffleBlockFetcherIterator: Started 0 remote
fetches in 0 ms

16/02/29 17:03:03 INFO Executor: Finished task 199.0 in stage 7.0 (TID
1115). 2511 bytes result sent to driver

16/02/29 17:03:03 INFO TaskSetManager: Finished task 199.0 in stage 7.0
(TID 1115) in 27621 ms on localhost (197/200)

*16/02/29 17:07:06 INFO UnsafeExternalSorter: Thread 124 spilling sort data
of 256.0 KB to disk (0  time so far)*


Now, I have around 10G of executor memory and my memory faction should be
the default ( 0.75 as per the documentation) and my memory usage is < 1.5G(
obtained from the Storage tab on Spark dashboard), but still it says
spilling sort data. I'm a little surprised why this happens even when I
have enough memory free.


Any inputs will be greatly appreciated!


Thanks







-- 
Regards,
*Deepak Gopalakrishnan*
*Mobile*:+918891509774
*Skype* : deepakgk87
http://myexps.blogspot.com
"
Alexander Pivovarov <apivovarov@gmail.com>,"Mon, 29 Feb 2016 11:12:50 -0800",What should be spark.local.dir in spark on yarn?,dev <dev@spark.apache.org>,"I have Spark on yarn

I defined yarn.nodemanager.local-dirs to be /data01/yarn/nm,/data02/yarn/nm

when I look at yarn executor container log I see that blockmanager files
created in /data01/yarn/nm,/data02/yarn/nm

But output files to upload to s3 still created in /tmp on slaves

I do not want Spark write heavy files to /tmp because /tmp is only 5GB

spark slaves have two big additional disks /disk01 and /disk02 attached

Probably I can set spark.local.dir to be /data01/tmp,/data02/tmp

But spark master also writes some files to spark.local.dir
But my master box has only one additional disk /data01

So, what should I use for  spark.local.dir the
spark.local.dir=/data01/tmp
or
spark.local.dir=/data01/tmp,/data02/tmp

?
"
Jeff Zhang <zjffdu@gmail.com>,"Tue, 1 Mar 2016 07:44:53 +0800",Re: What should be spark.local.dir in spark on yarn?,Alexander Pivovarov <apivovarov@gmail.com>,"In yarn mode, spark.local.dir is yarn.nodemanager.local-dirs for shuffle
data and block manager disk data. What do you mean ""But output files to
upload to s3 still created in /tmp on slaves"" ? You should have control on
where to store your output data if that means your job's output.





-- 
Best Regards

Jeff Zhang
"
